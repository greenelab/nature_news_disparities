---
title: "Mention_v_citation_analysis"
author: "Natalie Davidson"
date: "3/31/2021"
output: rmarkdown::github_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)

require(data.table)
require(here)
require(ggplot2)
require(pheatmap)
require(tidytext)
require(stringr)


proj_dir = here()
source(file.path(proj_dir, "/analysis_scripts/analysis_utils.R"))

MIN_ART = 20
MIN_PROP = 0.05

data(stop_words)

```


## Data Description

This analysis will compare token frequencies between two types of Nature News articles.
To identify the types of articles, we first identify which countries are cited more than mentioned and which countries are mentioned more than cited.
This comparison is done on a per year basis.
After this, we will take the most exemplary of the 2 country classes (top mentions > cited: Class M & top mentions < cited: Class C).
We will compare the token frequencies between a mention of Class C v M.

The source data file for a bootstrap estimate of country mentions and citations: `/data/author_data/all_author_country_95CI.tsv`

The all source text is here: `/data/scraped_data/downloads/*.json`

The country mention to source articles id map here: `/data/scraped_data/location_table_raw_YEAR_ARTICLE-TYPE.tsv`

## Get Top Class C and M Countries

#### Read in the raw country counts.

```{r read_in_raw_counts, fig.align='center'}
# get the project directory, everything is set relative to this
proj_dir = here()

# read in the raw location article counts
raw_file = file.path(proj_dir, "/data/author_data/all_author_country.tsv")
raw_df = fread(raw_file)
raw_df = subset(raw_df, address.country_code != "" & !is.na(address.country_code))
raw_df = subset(raw_df, corpus %in% c("naturenews_mentions", "naturenews_citations"))

# get UN info
un_info = get_country_info()
raw_df = merge(un_info, raw_df)

head(raw_df)



# get the total number of mentions and citations
# for each country per year
# we only want to evaluate when we have > 10 in either citations or mentions
mention_total = unique(subset(raw_df, 
                              corpus == "naturenews_mentions", 
                              select=c(file_id, year, address.country_code)) )
tot_country_mention = mention_total %>% 
                group_by(year, address.country_code) %>% 
                summarise(n()) 
tot_country_mention$corpus = "naturenews_mentions"
colnames(tot_country_mention)[3] = "total"

citation_total = unique(subset(raw_df, 
                               corpus == "naturenews_citations", 
                               select=c(file_id, year, address.country_code)) )
tot_country_citation = citation_total %>% 
                group_by(year, address.country_code) %>% 
                summarise(n()) 
tot_country_citation$corpus = "naturenews_citations"
colnames(tot_country_citation)[3] = "total"

# show the spread of the mentions and citations
raw_sum_df = rbind(tot_country_citation, tot_country_mention)
ggplot(raw_sum_df, aes(x=as.factor(year), y=log10(total+1), fill=corpus)) +
    geom_boxplot() + theme_bw() + geom_hline(yintercept = log10(10), color="black") +
    xlab("Year") + ylab("log10(# Articles +1)") +
    ggtitle("log10 # of articles in each Corpus for all countries") + 
    scale_fill_brewer(palette="Set2")
    
# now get the country + year pairings in a format easy to join on later
raw_sum_df = reshape2::dcast(raw_sum_df, year+address.country_code ~ corpus, value.var="total")
raw_sum_df[is.na(raw_sum_df)] = 0
colnames(raw_sum_df)[3:4] = c("tot_citations", "tot_mentions")

```

#### Read in the bootstrapped estimate of % of articles with country counts.


```{r read_in_bootstrap, fig.align='center', warning = FALSE}

# read in the cited author data
ci_file = file.path(proj_dir, "/data/author_data/all_author_country_95CI.tsv")
ci_df = fread(ci_file)
ci_df = subset(ci_df, country != "" & !is.na(country))

# get UN info
un_info = get_country_info()
ci_df = merge(un_info, ci_df)

head(ci_df)

# now filter for only the country-year pairings that have enough counts
ci_df = merge(raw_sum_df, ci_df)

# show the spread of the mentions and citations
ggplot(ci_df, aes(x=as.factor(year), y=as.numeric(mean), fill=corpus)) +
    geom_boxplot(position="dodge") + theme_bw() + 
    xlab("Year") + ylab("Est. % of articles") +
    ggtitle("Est. % of articles in each Corpus for all countries and years") + 
    scale_fill_brewer(palette="Set2")

# show the spread where a mentions or citations >20 
ggplot(subset(ci_df, tot_citations > MIN_ART | tot_mentions > MIN_ART), 
       aes(x=as.factor(year), y=as.numeric(mean), fill=corpus)) +
    geom_boxplot(position="dodge") + theme_bw() + 
    xlab("Year") + ylab("Est. % of articles") +
    ggtitle("Est. % of articles in each Corpus, cutoff > 20 for either mention or citation") + 
    scale_fill_brewer(palette="Set2")

# dcast the folder so we can compare mentions to citations
ci_df_cast = reshape2::dcast(ci_df, 
                             year+country+address.country_code+tot_citations+tot_mentions ~ corpus, 
                             value.var="mean")

# calculate the difference between mentions + citations
ci_df_cast$M_C = ci_df_cast$naturenews_mentions - ci_df_cast$naturenews_citations

# show the spread of the difference mentions and citations
ggplot(subset(ci_df_cast, tot_citations > MIN_ART | tot_mentions > MIN_ART), 
       aes(x=as.numeric(year), y=as.numeric(M_C))) +
    geom_point() + theme_bw() + 
    geom_hline(yintercept = MIN_PROP, color="red") +
    geom_hline(yintercept = -1*MIN_PROP, color="red") +
    xlab("Corpus") + ylab("Mention % - Citation % for each country+year") +
    ggtitle("Diff. between mentions and citations for each country and year") + 
    scale_fill_brewer(palette="Set2")


# final dataframe with all filters
top_diff_MC = subset(ci_df_cast, tot_citations > MIN_ART | tot_mentions > MIN_ART)
top_diff_MC = subset(top_diff_MC, M_C > MIN_PROP | M_C < -1*MIN_PROP)


```

#### Plot the top country-year pairings have a large difference in citations vs mentions

```{r plot_filtered_countries, fig.align='center', fig.width=10, fig.height=10, warning = FALSE}

make_heatmap_res <- function(in_df, value_col){
    plot_matr_MC = reshape2::dcast(in_df, 
                             country ~ year, 
                             value.var=value_col)
    row.names(plot_matr_MC) = plot_matr_MC$country
    plot_matr_MC = plot_matr_MC[,-1]
    #plot_matr_MC[is.na(plot_matr_MC)] = 0
    
    max_val = max(abs(plot_matr_MC), na.rm = T)
    breaks = c(seq(-1*max_val, max_val, by = 0.01))
    color_pmap <- colorRampPalette(c("yellow", "white", "blue"))(length(breaks))

    if(max_val > 1){
        breaks = c(seq(1, max_val, by = 1))
        color_pmap <- colorRampPalette(c("white", "blue"))(length(breaks))

    }

    res = list(plot_matr = plot_matr_MC,
               color_pmap = color_pmap,
               breaks = breaks)
    return(res)
}

# plot the Top ptoportion differences
res_MC = make_heatmap_res(top_diff_MC, value_col="M_C")
pheatmap(res_MC$plot_matr, cluster_rows = F, 
         cluster_cols = F, display_numbers = T, 
         main = "Top (Mention - Citation) Proportions",
         color = res_MC$color_pmap, breaks = res_MC$breaks)

# OF the Top proportion differences, only plot the raw # citations
res_cite = make_heatmap_res(top_diff_MC, value_col="tot_citations")
pheatmap(res_cite$plot_matr, cluster_rows = F, 
         cluster_cols = F, display_numbers = T, 
         main = "Top (Mention - Citation), total citations",
         color = res_cite$color_pmap, breaks = res_cite$breaks)

# OF the Top proportion differences, only plot the raw # mentions
res_mention = make_heatmap_res(top_diff_MC, value_col="tot_mentions")
pheatmap(res_mention$plot_matr, cluster_rows = F, 
         cluster_cols = F, display_numbers = T, 
         main = "Top (Mention - Citation), total mentions",
         color = res_mention$color_pmap, breaks = res_mention$breaks)


```

## Compare tokens from Class C and M Country-Year pairs

#### Get the raw text ids for each class

```{r get_file_ids, fig.align='center', fig.width=10, fig.height=10, warning = FALSE}

# for every country + year pair in our filtered table (top_diff_MC), get the associated raw file ids
class_c_counts = subset(top_diff_MC, M_C < 0, select=c("address.country_code", "year") )
class_c_counts$class = "class_c" 
class_m_counts = subset(top_diff_MC, M_C > 0, select=c("address.country_code", "year") )
class_m_counts$class = "class_m" 
class_m_counts$idx = paste(class_m_counts$address.country_code,
                          class_m_counts$year, sep="_")


# find all location estimates
all_loc_files = list.files(file.path(proj_dir, "/data/scraped_data/"), 
                            pattern="location_table_raw",
                            recursive=F,
                            full.names=T)

# read in all the file id's
full_loc_df = NA
for(loc_file in all_loc_files){

    loc_df = read_corenlp_location_files(loc_file)
    loc_df$year = str_extract(loc_file, "[1-9][0-9]+") # curr_year
    loc_df$type = substring(basename(loc_file), 
                            25, nchar(basename(loc_file))-4)
    full_loc_df = rbind(full_loc_df, loc_df)
}
full_loc_df = full_loc_df[-1,]
full_loc_df = subset(full_loc_df, est_un_region != "" & 
                                        est_un_subregion != "" &
                                        est_un_region != "NO_EST" & 
                                        est_un_subregion != "NO_EST")
colnames(full_loc_df)[1] = c("address.country_code")


## filter for countries where they are mentioned more than once just for highest accuracy
loc_dups = data.frame(table(full_loc_df$file_id, full_loc_df$address.country_code))
loc_keep = subset(loc_dups, Freq > 1)
full_loc_df$freq_idx = paste(full_loc_df$file_id, full_loc_df$address.country_code, sep="_")
freq_pass = paste(loc_keep$Var1, loc_keep$Var2, sep="_")
full_loc_df_no_mention_dup = subset(full_loc_df, freq_idx %in% freq_pass)


# now get the file ids per class
country_year_pass = unique(class_m_counts[,c("year", "address.country_code")])
full_loc_df_pass = merge(country_year_pass, full_loc_df_no_mention_dup)




```

#### Get the cited text ids for each class

```{r get_cited_file_ids, fig.align='center', fig.width=10, fig.height=10, warning = FALSE}

    # all the cited articles
    cited_country_file = file.path(proj_dir, 
                                    "/data/author_data/cited_author_country.tsv")
    cited_country_df = data.frame(fread(cited_country_file))
    cited_country_df = subset(cited_country_df, country != "")
    cited_country_df$country = format_country_names(cited_country_df$country)
    
    # format the countries
    cited_country_df_formatted = get_author_country(cited_country_df)
    cited_country_df_formatted = unique(cited_country_df_formatted)

    # we only care about if a country was cited in an article, 
    # not how many times it was cited
    cited_country_df_formatted$num_entries = 1

```


#### Now seperate the articles with a citation and with a mention

```{r merge_mention_citation_file_ids, fig.align='center', fig.width=10, fig.height=10, warning = FALSE}

    # all the cited articles
    cited_loc = merge(unique(full_loc_df[,c("file_id", "year", "type")]),
                      cited_country_df_formatted)

    cited_loc$idx = paste(cited_loc$address.country_code,
                          cited_loc$year, sep="_")
    class_m_citations = subset(cited_loc, idx %in% 
                                   class_m_counts$idx)
    

```



#### Function to calculate word frequencies from raw text for each class

```{r method_word_freq_raw_text, fig.align='center', fig.width=10, fig.height=10, warning = FALSE}


#' Get the word frequencies across multiple JSON files containing
#' for all years and news types
#'
#' @param class_ids This contains at a minimum a file_id - country - 
#' article_type - year mapping
#' @param class_str name of country class of interest
#' @return word_freq a dataframe of word counts across all relevant articles
get_word_freq_per_class <- function(class_ids, class_str){
        
    # get the word frequencies for class C articles
    class_word_freq = NA
    for(curr_year in unique(class_ids$year)){
        # subset for each year
        curr_year_df = subset(class_ids, year == curr_year)
        
        for(curr_type in unique(curr_year_df$type)){
            # subset for each type within a year
            curr_year_type_df = subset(curr_year_df, type == curr_type)
    
            # make the file name
            curr_file = file.path(proj_dir, "/data/scraped_data/downloads/",
                                  paste("links_crawled_", 
                                        curr_year, "_", 
                                        curr_type, ".json", sep=""))
            
            # calculate the word frequency
            curr_word_freq = calc_word_freq(curr_file, unique(curr_year_type_df$file_id))
            curr_word_freq = as.data.frame(curr_word_freq)
            curr_word_freq$year = curr_year
            curr_word_freq$type = curr_type
            class_word_freq = rbind(class_word_freq, curr_word_freq)
        }
    }
    class_word_freq_total = class_word_freq[-1,]
    class_word_freq = class_word_freq[-1,]
    
    # sum word frequencies across the different JSON files
    class_word_freq = class_word_freq %>%
                    select("word", "n") %>%
                    group_by(word) %>% 
                    summarise(sum(n)) 
    col_id = paste(class_str, "_count", sep="")
    colnames(class_word_freq)[2] = col_id
    
    # sort
    class_word_freq = class_word_freq[
                            order(class_word_freq[,col_id], 
                                  decreasing=T),]
     
    return(class_word_freq)
}

```

#### Calculate word frequencies for Class C

```{r calc_word_freq_citations, fig.align='center', fig.width=10, fig.height=10, warning = FALSE, message = FALSE}


# get the word frequencies for each class of country
class_all_word_freq = get_word_freq_per_class(full_loc_df_pass, class_str = "class_all")

class_m_citations = subset(class_m_citations, address.country_code %in% class_m_ids$address.country_code)
top_words_c = list()
top_words_m = list()
# write out top words for each country
for(curr_country in unique(class_m_citations$address.country_code)){
    class_c_word_freq = get_word_freq_per_class(
                            subset(class_m_citations, address.country_code == curr_country), 
                            class_str = "class_c")
     class_m_word_freq = get_word_freq_per_class(
                            subset(class_m_ids, address.country_code == curr_country), 
                            class_str = "class_m")
     per_class_word_freq = merge(data.table(class_c_word_freq), 
                                data.table(class_m_word_freq), by="word")
     per_class_word_freq = merge(data.table(per_class_word_freq), 
                                data.table(class_all_word_freq), by="word")
    
        
    # it should be more than 100 in at least one corpus
    per_class_word_freq = subset(per_class_word_freq, class_all_count > 100)
        
    per_class_word_freq$ratioC = per_class_word_freq$class_c_count / 
                                per_class_word_freq$class_all_count
    per_class_word_freq$ratioM = per_class_word_freq$class_m_count / 
                                per_class_word_freq$class_all_count
    per_class_word_freq$ratio = per_class_word_freq$ratioC / 
                                per_class_word_freq$ratioM    
    
    per_class_word_freq = per_class_word_freq[order(per_class_word_freq$ratio, decreasing=T),]
    print(knitr::kable(head(per_class_word_freq,15), 
                 caption = paste(curr_country, "Class Citation, top terms")))
    top_words_c[[curr_country]] = per_class_word_freq$word[1:min(nrow(per_class_word_freq), 100)]
    
    
    per_class_word_freq = per_class_word_freq[order(per_class_word_freq$ratio, decreasing=F),]
    print(knitr::kable(head(per_class_word_freq,15), 
                 caption = paste(curr_country, "Class Mention, top terms")))
    top_words_m[[curr_country]] = per_class_word_freq$word[1:min(nrow(per_class_word_freq), 100)]

}
top_words_c_freq = data.frame(sort(table(unlist(top_words_c)), decreasing=T))
top_words_c_freq$prop = top_words_c_freq$Freq / length(top_words_c)
print(knitr::kable(head(top_words_c_freq,15), 
                       caption = "Overall Class Citation, top terms"))

top_words_m_freq = data.frame(sort(table(unlist(top_words_m)), decreasing=T))
top_words_m_freq$prop = top_words_m_freq$Freq / length(top_words_m)
print(knitr::kable(head(top_words_m_freq,15), 
                       caption = "Overall Class Mention, top terms"))

```

#### Calculate word frequencies for Class M

```{r calc_word_freq_mentions, fig.align='center', fig.width=10, fig.height=10, warning = FALSE, message = FALSE}

# write out top words for each country
top_words_m = list()
for(curr_country in unique(class_m_ids$est_country)){
    class_m_word_freq = get_word_freq_per_class(
                            subset(class_m_ids, est_country == curr_country), 
                            class_str = "class_m")
    per_class_word_freq = merge(data.table(class_m_word_freq), 
                                data.table(class_all_word_freq), by="word")
    
        
    # it should be more than 100 in at least one corpus
    per_class_word_freq = subset(per_class_word_freq, class_all_count > 100)
        
    per_class_word_freq$ratio = per_class_word_freq$class_m_count / 
                                per_class_word_freq$class_all_count
    
    per_class_word_freq = per_class_word_freq[order(per_class_word_freq$ratio, decreasing=T),]
    print(knitr::kable(head(per_class_word_freq,15), 
                       caption = paste(curr_country, "Class Mention, top terms")))
    top_words_m[[curr_country]] = per_class_word_freq$word[1:min(nrow(per_class_word_freq), 100)]

}
top_words_m_freq = data.frame(sort(table(unlist(top_words_m)), decreasing=T))
top_words_m_freq$prop = top_words_m_freq$Freq / length(top_words_m)
print(knitr::kable(head(top_words_m_freq,15), 
                       caption = "Overall Class Mention, top terms"))


```