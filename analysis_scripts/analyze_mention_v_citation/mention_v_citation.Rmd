---
title: "Mention_v_ciation_analysis"
author: "Natalie Davidson"
date: "3/31/2021"
output: rmarkdown::github_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)

require(data.table)
require(here)
require(ggplot2)
require(pheatmap)
require(tidytext)


proj_dir = here()
source(file.path(proj_dir, "/analysis_scripts/analysis_utils.R"))

MIN_ART = 20
MIN_PROP = 0.05

data(stop_words)

```


## Data Description

This analysis will compare token frequencies between two types of Nature News articles.
To identify the types of articles, we first identify which countries are cited more than mentioned and which countries are mentioned more than cited.
This comparison is done on a per year basis.
After this, we will take the most exemplary of the 2 country classes (top mentions > cited: Class M & top mentions < cited: Class C).
We will compare the token frequencies between a mention of Class C v M.

The source data file for a bootstrap estimate of country mentions and citations: `/data/author_data/all_author_country_95CI.tsv`

The all source text is here: `/data/scraped_data/downloads/*.json`

The country mention to source articles id map here: `/data/scraped_data/location_table_raw_YEAR_ARTICLE-TYPE.tsv`

## Get Top Class C and M Countries

#### Read in the raw country counts.

```{r read_in_raw_counts, fig.align='center'}
# get the project directory, everything is set relative to this
proj_dir = here()

# read in the raw location article counts
raw_file = file.path(proj_dir, "/data/author_data/all_author_country.tsv")
raw_df = fread(raw_file)
raw_df = subset(raw_df, address.country_code != "" & !is.na(address.country_code))
raw_df = subset(raw_df, corpus %in% c("naturenews_mentions", "naturenews_citations"))

# get UN info
un_info = get_country_info()
raw_df = merge(un_info, raw_df)

head(raw_df)

# get the total number of mentions and citations
# for each country per year
# we only want to evaluate when we have > 10 in either citations or mentions
mention_total = unique(subset(raw_df, 
                              corpus == "naturenews_mentions", 
                              select=c(file_id, year, address.country_code)) )
tot_country_mention = mention_total %>% 
                group_by(year, address.country_code) %>% 
                summarise(n()) 
tot_country_mention$corpus = "naturenews_mentions"
colnames(tot_country_mention)[3] = "total"

citation_total = unique(subset(raw_df, 
                               corpus == "naturenews_citations", 
                               select=c(file_id, year, address.country_code)) )
tot_country_citation = citation_total %>% 
                group_by(year, address.country_code) %>% 
                summarise(n()) 
tot_country_citation$corpus = "naturenews_citations"
colnames(tot_country_citation)[3] = "total"

# show the spread of the mentions and citations
raw_sum_df = rbind(tot_country_citation, tot_country_mention)
ggplot(raw_sum_df, aes(x=as.factor(year), y=log10(total+1), fill=corpus)) +
    geom_boxplot() + theme_bw() + geom_hline(yintercept = log10(10), color="black") +
    xlab("Year") + ylab("log10(# Articles +1)") +
    ggtitle("log10 # of articles in each Corpus for all countries") + 
    scale_fill_brewer(palette="Set2")
    
# now get the country + year pairings in a format easy to join on later
raw_sum_df = reshape2::dcast(raw_sum_df, year+address.country_code ~ corpus, value.var="total")
raw_sum_df[is.na(raw_sum_df)] = 0
colnames(raw_sum_df)[3:4] = c("tot_citations", "tot_mentions")

```

#### Read in the bootstrapped estimate of % of articles with country counts.


```{r read_in_bootstrap, fig.align='center'}

# read in the cited author data
ci_file = file.path(proj_dir, "/data/author_data/all_author_country_95CI.tsv")
ci_df = fread(ci_file)
ci_df = subset(ci_df, country != "" & !is.na(country))

# get UN info
un_info = get_country_info()
ci_df = merge(un_info, ci_df)

head(ci_df)

# now filter for only the country-year pairings that have enough counts
ci_df = merge(raw_sum_df, ci_df)

# show the spread of the mentions and citations
ggplot(ci_df, aes(x=as.factor(year), y=as.numeric(mean), fill=corpus)) +
    geom_boxplot(position="dodge") + theme_bw() + 
    xlab("Year") + ylab("Est. % of articles") +
    ggtitle("Est. % of articles in each Corpus for all countries and years") + 
    scale_fill_brewer(palette="Set2")

# show the spread where a mentions or citations >20 
ggplot(subset(ci_df, tot_citations > MIN_ART | tot_mentions > MIN_ART), 
       aes(x=as.factor(year), y=as.numeric(mean), fill=corpus)) +
    geom_boxplot(position="dodge") + theme_bw() + 
    xlab("Year") + ylab("Est. % of articles") +
    ggtitle("Est. % of articles in each Corpus, cutoff > 20 for either mention or citation") + 
    scale_fill_brewer(palette="Set2")

# dcast the folder so we can compare mentions to citations
ci_df_cast = reshape2::dcast(ci_df, 
                             year+country+address.country_code+tot_citations+tot_mentions ~ corpus, 
                             value.var="mean")

# calculate the difference between mentions + citations
ci_df_cast$M_C = ci_df_cast$naturenews_mentions - ci_df_cast$naturenews_citations

# show the spread of the difference mentions and citations
ggplot(subset(ci_df_cast, tot_citations > MIN_ART | tot_mentions > MIN_ART), 
       aes(x=as.numeric(year), y=as.numeric(M_C))) +
    geom_point() + theme_bw() + 
    geom_hline(yintercept = MIN_PROP, color="red") +
    geom_hline(yintercept = -1*MIN_PROP, color="red") +
    xlab("Corpus") + ylab("Mention % - Citation % for each country+year") +
    ggtitle("Diff. between mentions and citations for each country and year") + 
    scale_fill_brewer(palette="Set2")


# final dataframe with all filters
top_diff_MC = subset(ci_df_cast, tot_citations > MIN_ART | tot_mentions > MIN_ART)
top_diff_MC = subset(top_diff_MC, M_C > MIN_PROP | M_C < -1*MIN_PROP)


```

#### Plot the top country-year pairings have a large difference in citations vs mentions

```{r plot_filtered_countries, fig.align='center', fig.width=10, fig.height=10}

make_heatmap_res <- function(in_df, value_col){
    plot_matr_MC = reshape2::dcast(in_df, 
                             country ~ year, 
                             value.var=value_col)
    row.names(plot_matr_MC) = plot_matr_MC$country
    plot_matr_MC = plot_matr_MC[,-1]
    #plot_matr_MC[is.na(plot_matr_MC)] = 0
    
    max_val = max(abs(plot_matr_MC), na.rm = T)
    breaks = c(seq(-1*max_val, max_val, by = 0.01))
    color_pmap <- colorRampPalette(c("yellow", "white", "blue"))(length(breaks))

    if(max_val > 1){
        breaks = c(seq(1, max_val, by = 1))
        color_pmap <- colorRampPalette(c("white", "blue"))(length(breaks))

    }

    res = list(plot_matr = plot_matr_MC,
               color_pmap = color_pmap,
               breaks = breaks)
    return(res)
}

# plot the Top ptoportion differences
res_MC = make_heatmap_res(top_diff_MC, value_col="M_C")
pheatmap(res_MC$plot_matr, cluster_rows = F, 
         cluster_cols = F, display_numbers = T, 
         main = "Top (Mention - Citation) Proportions",
         color = res_MC$color_pmap, breaks = res_MC$breaks)

# OF the Top proportion differences, only plot the raw # citations
res_cite = make_heatmap_res(top_diff_MC, value_col="tot_citations")
pheatmap(res_cite$plot_matr, cluster_rows = F, 
         cluster_cols = F, display_numbers = T, 
         main = "Top (Mention - Citation), total citations",
         color = res_cite$color_pmap, breaks = res_cite$breaks)

# OF the Top proportion differences, only plot the raw # mentions
res_mention = make_heatmap_res(top_diff_MC, value_col="tot_mentions")
pheatmap(res_mention$plot_matr, cluster_rows = F, 
         cluster_cols = F, display_numbers = T, 
         main = "Top (Mention - Citation), total mentions",
         color = res_mention$color_pmap, breaks = res_mention$breaks)


```

## Compare tokens from Class C and M Country-Year pairs

#### Get the raw text ids for each class

```{r get_file_ids, fig.align='center', fig.width=10, fig.height=10}

# for every country + year pair in our filtered table (top_diff_MC), get the associated raw file ids
class_c_counts = subset(top_diff_MC, M_C < 0 )
class_c_counts$idx = paste(class_c_counts$address.country_code, 
                             class_c_counts$year, sep="_")
class_m_counts = subset(top_diff_MC, M_C > 0 )
class_m_counts$idx = paste(class_m_counts$address.country_code, 
                             class_m_counts$year, sep="_")

# find all location estimates
all_loc_files = list.files(file.path(proj_dir, "/data/scraped_data/"), 
                            pattern="location_table_raw",
                            recursive=F,
                            full.names=T)

# read in all the file id's
full_loc_df = NA
for(loc_file in all_loc_files){

    loc_df = read_corenlp_location_files(loc_file)
    loc_df$year = str_extract(loc_file, "[1-9][0-9]+") # curr_year
    loc_df$type = substring(basename(loc_file), 
                            25, nchar(basename(loc_file))-4)
    full_loc_df = rbind(full_loc_df, loc_df)
}
full_loc_df = full_loc_df[-1,]
full_loc_df = subset(full_loc_df, est_un_region != "" & 
                                        est_un_subregion != "" &
                                        est_un_region != "NO_EST" & 
                                        est_un_subregion != "NO_EST")

# now get the file ids per class
country_year_pass = unique(top_diff_MC[,c("year", "address.country_code")])
colnames(full_loc_df)[1] = c("address.country_code")
full_loc_df_pass = merge(country_year_pass, full_loc_df) 
full_loc_df_pass$idx = paste(full_loc_df_pass$address.country_code, 
                             full_loc_df_pass$year, sep="_")

class_c_ids = subset(full_loc_df_pass, idx %in% unique(class_c_counts$idx))
class_m_ids = subset(full_loc_df_pass, idx %in% unique(class_m_counts$idx))

head(class_c_ids)
head(class_m_ids)

# remove any files that are in the overlap
files_in_both = intersect(class_c_ids$file_id, class_m_ids$file_id)
class_c_ids = subset(class_c_ids, !file_id %in% files_in_both)
class_m_ids = subset(class_m_ids, !file_id %in% files_in_both)


```


#### Calculate word frequencies from raw text for each class

```{r calc_word_freq_raw_text, fig.align='center', fig.width=10, fig.height=10}

# function to calculate the word frequencies from each article of interest
calc_word_freq <- function(json_file, file_ids_pass){
    
        # read in the json
        all_articles_df = read_json(curr_file)
        
        # get the articles we are interested in
        all_articles_df = subset(all_articles_df, file_id %in% file_ids_pass)
        all_articles_df = data.table(all_articles_df)
        
        # tokenize
        all_articles_df <- all_articles_df %>%
                unnest_tokens(word, body)
        
        # remove stop words
        all_articles_df <- all_articles_df %>%
                anti_join(stop_words)
        
        # make sure its ASCII
        all_articles_df$word = iconv(all_articles_df$word, from = "UTF-8", to = "ASCII", sub = "")
        
        # remove punctuations
        all_articles_df$word = gsub('[[:punct:] ]+', '', all_articles_df$word)
        
        # make sure its not a number
        non_num = which(is.na(as.numeric(all_articles_df$word)))
        all_articles_df = all_articles_df[non_num,]
        
        # make sure its not an empty string
        non_whitespace = which(trimws(all_articles_df$word)!="")
        all_articles_df = all_articles_df[non_whitespace,]

        # get counts
        word_freq = all_articles_df %>%
                count(word, sort = TRUE)
        
        return(word_freq)
}

# get the word frequencies for class C articles
class_c_word_freq = NA
for(curr_year in unique(class_c_ids$year)){
    # subset for each year
    curr_year_df = subset(class_c_ids, year == curr_year)
    
    for(curr_type in unique(curr_year_df$type)){
        # subset for each type within a year
        curr_year_type_df = subset(curr_year_df, type == curr_type)

        # make the file name
        curr_file = file.path(proj_dir, "/data/scraped_data/downloads/",
                              paste("links_crawled_", 
                                    curr_year, "_", 
                                    curr_type, ".json", sep=""))
        
        # calculate the word frequency
        curr_word_freq = calc_word_freq(curr_file, unique(curr_year_type_df$file_id))
        curr_word_freq = as.data.frame(curr_word_freq)
        curr_word_freq$year = curr_year
        curr_word_freq$type = curr_type
        class_c_word_freq = rbind(class_c_word_freq, curr_word_freq)
    }
}
class_c_word_freq_total = class_c_word_freq[-1,]
class_c_word_freq = class_c_word_freq[-1,]

# sum word frequencies across the different JSON files
class_c_word_freq = class_c_word_freq %>%
                select("word", "n") %>%
                group_by(word) %>% 
                summarise(sum(n)) 
colnames(class_c_word_freq)[2] = "class_c_count"

# sort
class_c_word_freq = class_c_word_freq[
                        order(class_c_word_freq$class_c_count, 
                              decreasing=T),]
 

# get the word frequencies for class M articles
class_m_word_freq = NA
for(curr_year in unique(class_m_ids$year)){
    # subset for each year
    curr_year_df = subset(class_m_ids, year == curr_year)
    
    for(curr_type in unique(curr_year_df$type)){
        # subset for each type within a year
        curr_year_type_df = subset(curr_year_df, type == curr_type)

        # make the file name
        curr_file = file.path(proj_dir, "/data/scraped_data/downloads/",
                              paste("links_crawled_", 
                                    curr_year, "_", 
                                    curr_type, ".json", sep=""))
        
        # calculate the word frequency
        curr_word_freq = calc_word_freq(curr_file, unique(curr_year_type_df$file_id))
        curr_word_freq = as.data.frame(curr_word_freq)
        curr_word_freq$year = curr_year
        curr_word_freq$type = curr_type
        class_m_word_freq = rbind(class_m_word_freq, curr_word_freq)
    }
}
class_m_word_freq_total = class_m_word_freq[-1,]
class_m_word_freq = class_m_word_freq[-1,]

# sum word frequencies across the different JSON files
class_m_word_freq = class_m_word_freq %>%
                select("word", "n") %>%
                group_by(word) %>% 
                summarise(sum(n)) 
colnames(class_m_word_freq)[2] = "class_m_count"


# sort
class_m_word_freq = class_m_word_freq[
                        order(class_m_word_freq$class_m_count, 
                              decreasing=T),]
 
# merge the classes together
# only take words that are in both
per_class_word_freq = merge(data.table(class_c_word_freq), data.table(class_m_word_freq), by="word")

# it should be more than 100 in at least one corpus
per_class_word_freq = subset(per_class_word_freq, 
                             class_c_count > 50 & class_m_count > 50)

# now calculate the log odds ratio
per_class_word_freq$ratio = per_class_word_freq$class_c_count / 
                            per_class_word_freq$class_m_count

per_class_word_freq = per_class_word_freq[order(per_class_word_freq$ratio),]
print(head(per_class_word_freq,50))
print(tail(per_class_word_freq,50))


```