[
{"file_id": "527290a", "url": "https://www.nature.com/articles/527290a", "year": 2015, "authors": [{"name": "Ron Cowen"}], "parsed_as_year": "2006_or_before", "body": "Many physicists believe that entanglement is the essence of quantum weirdness \u2014 and some now suspect that it may also be the essence of space-time geometry. In early 2009, determined to make the most of his first sabbatical from teaching, Mark Van Raamsdonk decided to tackle one of the deepest mysteries in physics: the relationship between quantum mechanics and gravity. After a year of work and consultation with colleagues, he submitted a paper on the topic to the  Journal of High Energy Physics . In April 2010, the journal sent him a rejection\u00a0\u2014 with a referee\u2019s report implying that Van Raamsdonk, a physicist at the University of British Columbia in Vancouver, was a crackpot. His next submission, to  General Relativity and Gravitation , fared little better: the referee\u2019s report was scathing, and the journal\u2019s editor asked for a complete rewrite. But by then, Van Raamsdonk had entered a shorter version of the paper into a prestigious annual essay contest run by the Gravity Research Foundation in Wellesley, Massachusetts. Not only did he win first prize, but he also got to savour a particularly satisfying irony: the honour included guaranteed publication in  General Relativity and Gravitation . The journal published the shorter essay 1  in June 2010. Still, the editors had good reason to be cautious. A successful unification of quantum mechanics and gravity has eluded physicists for nearly a century. Quantum mechanics governs the world of the small\u00a0\u2014 the weird realm in which an atom or particle can be in many places at the same time, and can simultaneously spin both clockwise and anticlockwise. Gravity governs the Universe at large\u00a0\u2014 from the fall of an apple to the motion of planets, stars and galaxies\u00a0\u2014 and is described by Albert Einstein\u2019s general theory of relativity, announced 100\u00a0years ago this month. The theory holds that gravity is geometry: particles are deflected when they pass near a massive object not because they feel a force, said Einstein, but because space and time around the object are curved. Both theories have been abundantly verified through experiment, yet the realities they describe seem utterly incompatible. And from the editors\u2019 standpoint, Van Raamsdonk\u2019s approach to resolving this incompatibility was strange. All that\u2019s needed, he asserted, is \u2018entanglement\u2019: the phenomenon that many physicists believe to be the ultimate in quantum weirdness. Entanglement lets the measurement of one particle instantaneously determine the state of a partner particle, no matter how far away it may be\u00a0\u2014 even on the other side of the Milky Way. Einstein loathed the idea of entanglement, and famously derided it as \u201cspooky action at a distance\u201d. But  it is central to quantum theory . And Van Raamsdonk, drawing on work by like-minded physicists going back more than a decade, argued for the ultimate irony\u00a0\u2014 that, despite Einstein\u2019s objections, entanglement might be the basis of geometry, and thus of Einstein\u2019s geometric theory of gravity. \u201cSpace-time,\u201d he says, \u201cis just a geometrical picture of how stuff in the quantum system is entangled.\u201d This idea is a long way from being proved, and is hardly a complete theory of quantum gravity. But independent studies have reached much the same conclusion, drawing intense interest from major theorists. A small industry of physicists is now working to expand the geometry\u2013entanglement relationship, using all the modern tools developed for quantum computing and quantum information theory. \u201cI would not hesitate for a minute,\u201d says physicist Bart\u0142omiej Czech of Stanford University in California, \u201cto call the connections between quantum theory and gravity that have emerged in the last ten years revolutionary.\u201d \n               Gravity without gravity \n             Much of this work rests on a discovery 2  announced in 1997 by physicist Juan Maldacena, now at the Institute for Advanced Study in Princeton, New Jersey. Maldacena\u2019s research had led him to consider the relationship between two seemingly different model universes. One is a cosmos similar to our own. Although it neither expands nor contracts, it has three dimensions, is filled with quantum particles and obeys Einstein\u2019s equations of gravity. Known as anti-de Sitter space (AdS), it is commonly referred to as the bulk. The other model is also filled with elementary particles, but it has one dimension fewer and doesn\u2019t recognize gravity. Commonly known as the boundary, it is a mathematically defined membrane that lies an infinite distance from any given point in the bulk, yet completely encloses it, much like the 2D surface of a balloon enclosing a 3D volume of air. The boundary particles obey the equations of a quantum system known as conformal field theory (CFT). Maldacena discovered that the boundary and the bulk are completely equivalent. Like the 2D circuitry of a computer chip that encodes the 3D imagery of a computer game, the relatively simple, gravity-free equations that prevail on the boundary contain the same information and describe the same physics as the more complex equations that rule the bulk. \u201cIt\u2019s kind of a miraculous thing,\u201d says Van Raamsdonk. Suddenly, he says, Maldacena\u2019s duality gave physicists a way to think about quantum gravity in the bulk without thinking about gravity at all: they just had to look at the equivalent quantum state on the boundary. And in the years since, so many have rushed to explore this idea that Maldacena\u2019s paper is now one of the most highly cited articles in physics. Among the enthusiasts was Van Raamsdonk, who started his sabbatical by pondering one of the central unsolved questions posed by Maldacena\u2019s discovery: exactly how does a quantum field on the boundary produce gravity in the bulk? There had already been hints 3  that the answer might involve some sort of relation between geometry and entanglement. But it was unclear how significant these hints were: all the earlier work on this idea had dealt with special cases, such as a bulk universe that contained a black hole. So Van Raamsdonk decided to settle the matter, and work out whether the relationship was true in general, or was just a mathematical oddity. He first considered an empty bulk universe, which corresponded to a single quantum field on the boundary. This field, and the quantum relationships that tied various parts of it together, contained the only entanglement in the system. But now, Van Raamsdonk wondered, what would happen to the bulk universe if that boundary entanglement were removed? He was able to answer that question using mathematical tools 4  introduced in 2006 by Shinsei Ryu, now at the University of Illinois at Urbana\u2013Champaign, and Tadashi Takanagi, now at the Yukawa Institute for Theoretical Physics at Kyoto University in Japan. Their equations allowed him to model a slow and methodical reduction in the boundary field\u2019s entanglement, and to watch the response in the bulk, where he saw space-time steadily elongating and pulling apart (see \u2018The entanglement connection\u2019). Ultimately, he found, reducing the entanglement to zero would break the space-time into disjointed chunks, like chewing gum stretched too far. The geometry\u2013entanglement relationship was general, Van Raamsdonk realized. Entanglement is the essential ingredient that knits space-time together into a smooth whole\u00a0\u2014 not just in exotic cases with black holes, but always. \u201cI felt that I had understood something about a fundamental question that perhaps nobody had understood before,\u201d he recalls: \u201cEssentially, what is space-time?\u201d \n               Entanglement and Einstein \n             Quantum entanglement as geometric glue\u00a0\u2014 this was the essence of Van Raamsdonk\u2019s rejected paper and winning essay, and an idea that has increasingly resonated among physicists. No one has yet found a rigorous proof, so the idea still ranks as a conjecture. But many independent lines of reasoning support it. In 2013, for example, Maldacena and Leonard Susskind of Stanford published 5  a related conjecture that they dubbed ER\u2009=\u2009EPR, in honour of two landmark papers from 1935. ER, by Einstein and American-Israeli physicist Nathan Rosen, introduced 6  what is now called a wormhole: a tunnel through space-time connecting two black holes. (No real particle could actually travel through such a wormhole, science-fiction films notwithstanding: that would require moving faster than light, which is impossible.) EPR, by Einstein, Rosen and American physicist Boris Podolsky, was the first paper to clearly articulate what is now called entanglement 7 . Maldacena and Susskind\u2019s conjecture was that these two concepts are related by more than a common publication date. If any two particles are connected by entanglement, the physicists suggested, then they are effectively joined by a wormhole. And vice versa: the connection that physicists call a wormhole is equivalent to entanglement. They are different ways of describing the same underlying reality. No one has a clear idea of what this under\u00adlying reality is. But physicists are increasingly convinced that it must exist. Maldacena, Susskind and others have been testing the ER\u2009=\u2009EPR hypothesis to see if it is mathematically consistent with everything else that is known about entanglement and wormholes\u00a0\u2014 and so far, the answer is yes. \n               Hidden connections \n             Other lines of support for the geometry\u2013entanglement relationship have come from condensed-matter physics and quantum information theory: fields in which entanglement already plays a central part. This has allowed researchers from these disciplines to attack quantum gravity with a whole array of fresh concepts and mathematical tools. Tensor networks, for example, are a technique developed by condensed-matter physicists to track the quantum states of huge numbers of subatomic particles. Brian Swingle was using them in this way in 2007, when he was a graduate student at the Massachusetts Institute of Technology (MIT) in Cambridge, calculating how groups of electrons interact in a solid mat\u00aderial. He found that the most useful network for this purpose started by linking adjacent pairs of electrons, which are most likely to interact with each other, then linking larger and larger groups in a pattern that resembled the hierarchy of a family tree. But then, during a course in quantum field theory, Swingle learned about Maldacena\u2019s bulk\u2013boundary correspondence and noticed an intriguing pattern: the mapping between the bulk and the boundary showed exactly the same tree-like network. Swingle wondered whether this resemblance might be more than just coincidence. And in 2012, he published 8  calculations showing that it was: he had independently reached much the same conclusion as Van Raamsdonk, thereby adding strong support to the geometry\u2013entanglement idea. \u201cYou can think of space as being built from entanglement in this very precise way using the tensors,\u201d says Swingle, who is now at Stanford and has seen tensor networks become a frequently used tool to explore the geometry\u2013entanglement correspondence. Another prime example of cross-fertilization is the theory of quantum error-correcting codes, which physicists invented to aid the  construction of quantum computers . These machines encode information not in bits but in \u2018qubits\u2019: quantum states, such as the up or down spin of an electron, that can take on values of 1 and 0 simultaneously. In principle, when the qubits interact and become entangled in the right way, such a device could perform calculations that an ordinary computer could not finish in the lifetime of the Universe. But in practice, the process can be incredibly fragile: the slightest disturbance from the outside world will disrupt the qubits\u2019 delicate entanglement and destroy any possibility of quantum computation. That need inspired quantum error-correcting codes, numerical strategies that repair corrupted correlations between the qubits and make the computation more robust. One hallmark of these codes is that they are always \u2018non-local\u2019: the information needed to restore any given qubit has to be spread out over a wide region of space. Otherwise, damage in a single spot could destroy any hope of recovery. And that non-locality, in turn, accounts for the fascination that many quantum information theorists feel when they first encounter Maldacena\u2019s bulk\u2013boundary correspondence: it shows a very similar kind of non-locality. The information that corresponds to a small region of the bulk is spread over a vast region of the boundary. \u201cAnyone could look at AdS\u2013CFT and say that it\u2019s sort of vaguely analogous to a quantum error-correcting code,\u201d says Scott Aaronson, a computer scientist at MIT. But in work published in June 9 , physicists led by Daniel Harlow at Harvard University in Cambridge and John Preskill of the California Institute of Technology in Pasadena argue for something stronger: that the Maldacena duality is itself a quantum error-correcting code. They have demonstrated that this is mathematically correct in a simple model, and are now trying to show that the assertion holds more generally. \u201cPeople have been saying for years that entanglement is somehow important for the emergence of the bulk,\u201d says Harlow. \u201cBut for the first time, I think we are really getting a glimpse of how and why.\u201d \n               Beyond entanglement \n             That prospect seems to be enticing for the Simons Foundation, a philanthropic organization in New York City that announced in August that it would provide US$2.5 million per year for at least 4 years to help researchers to move forward on the gravity\u2013quantum information connection. \u201cInformation theory provides a powerful way to structure our thinking about fundamental physics,\u201d says Patrick Hayden, the Stanford physicist who is directing the programme. He adds that the Simons sponsorship will support 16 main researchers at 14 institutions worldwide, along with students, postdocs and a series of workshops and schools. Ultimately, one major goal is to build up a comprehensive dictionary for translating geometric concepts into quantum language, and vice versa. This will hopefully help physicists to find their way to the complete theory of quantum gravity. Still, researchers face several challenges. One is that the bulk\u2013boundary correspondence does not apply in our Universe, which is neither static nor bounded; it is expanding and apparently infinite. Most researchers in the field do think that calculations using Maldacena\u2019s correspondence are telling them something true about the real Universe, but there is little agreement as yet on exactly how to translate results from one regime to the other. Another challenge is that the standard definition of entanglement refers to particles only at a given moment. A complete theory of quantum gravity will have to add time to that picture. \u201cEntanglement is a big piece of the story, but it\u2019s not the whole story,\u201d says Susskind. He thinks physicists may have to embrace another concept from quantum information theory:  computational complexity , the number of logical steps, or operations, needed to construct the quantum state of a system. A system with low complexity is analogous to a quantum computer with almost all the qubits on zero: it is easy to define and to build. One with high complexity is analogous to a set of qubits encoding a number that would take aeons to compute. Susskind\u2019s road to computational complexity began about a decade ago, when he noticed that a solution to Einstein\u2019s equations of general relativity allowed a wormhole in AdS space to get longer and longer as time went on. What did that correspond to on the boundary, he wondered? What was changing there? Susskind knew that it couldn\u2019t be entanglement, because the correlations that produce entanglement between different particles on the boundary reach their maximum in less than a second 10 . In an article last year 11 , however, he and Douglas Stanford, now at the Institute for Advanced Study, showed that as time progressed, the quantum state on the boundary would vary in exactly the way expected from computational complexity. \u201cIt appears more and more that the growth of the interior of a black hole is exactly the growth of computational complexity,\u201d says Susskind. If quantum entanglement knits together pieces of space, he says, then computational complexity may drive the growth of space\u00a0\u2014 and thus bring in the elusive element of time. One potential consequence, which he is just beginning to explore, could be a link between the growth of computational complexity and the expansion of the Universe. Another is that, because the insides of black holes are the very regions where quantum gravity is thought to dominate, computational complexity may have a key role in a complete theory of quantum gravity. Despite the remaining challenges, there is a sense among the practitioners of this field that they have begun to glimpse something real and very important. \u201cI didn\u2019t know what space was made of before,\u201d says Swingle. \u201cIt wasn\u2019t clear that question even had meaning.\u201d But now, he says, it is becoming increasingly apparent that the question does make sense. \u201cAnd the answer is something that we understand,\u201d says Swingle. \u201cIt\u2019s made of entanglement.\u201d As for Van Raamsdonk, he has written some 20 papers on quantum entanglement since 2009. All of them, he says, have been accepted for publication. \n                 Tweet \n                 Facebook \n                 LinkedIn \n                 Weibo \n                 Wechat \n               \n                     History: Einstein was no lone genius 2015-Nov-16 \n                   \n                     Quantum \u2018spookiness\u2019 passes toughest test yet 2015-Aug-27 \n                   \n                     Physics: Quantum computer quest 2014-Dec-03 \n                   \n                     Theoretical physics: Complexity on the horizon 2014-May-28 \n                   \n                     Simulations back up theory that Universe is a hologram 2013-Dec-10 \n                   \n                     Theoretical physics: The origins of space and time 2013-Aug-28 \n                   \n                     Nature  special: General relativity at 100 \n                   \n                     Mark Van Raamsdonk: Gravity and entanglement\u00a0 \n                   \n                     Juan Maldacena: Quantum mechanics and the geometry of spacetime \n                   \n                     Leonard Susskind: ER = EPR\u00a0 \n                   \n                     Brian Swingle: Einstein's equations starting from qubits \n                   Reprints and Permissions"},
{"file_id": "528178a", "url": "https://www.nature.com/articles/528178a", "year": 2015, "authors": [{"name": "Meredith Wadman"}], "parsed_as_year": "2006_or_before", "body": "The use of aborted fetal tissue has sparked controversy in the United States, but many scientists say it is essential for studies of HIV, development and more. Every month, Lishan Su receives a small test tube on ice from a company in California. In it is a piece of liver from a human fetus aborted at between 14 and 19 weeks of pregnancy. Su and his staff at the University of North Carolina at Chapel Hill carefully grind the liver, centrifuge it and then extract and purify liver- and blood-forming stem cells. They inject the cells into the livers of newborn mice, and allow those mice to mature. The resulting animals are the only \u2018humanized\u2019 mice with both functioning human liver and immune cells and, for Su, they are invaluable in his work on hepatitis B and C, allowing him to probe how the viruses evade the human immune system and cause chronic liver diseases. \u201cUsing fetal tissue is not an easy choice, but so far there is no better choice,\u201d says Su, who has tried, and failed, to make a humanized mouse with other techniques. \u201cMany, many biomedical researchers depend on fetal tissue research to really save human lives,\u201d he says. \u201cAnd I think many of them feel the same way.\u201d Meredith Wadman explains how fetal tissue is used in research, and how undercover filming has thrown such research into the public eye. An explosive climate has surrounded US research with fetal tissues since July, when an anti-abortion group called the Center for Medical Progress in Irvine, California, released covertly filmed videos in which senior physicians from the Planned Parenthood Federation of America bluntly and dispassionately discussed their harvesting of fetal organs from abortions for use in research. Planned Parenthood is a non-profit women\u2019s health provider that received US$528\u00a0million of government money in 2014, much of it in reimbursements for services ranging from contraception to cancer screenings, which it provides largely to poor women. Abortions, which are performed at about half of Planned Parenthood\u2019s 700 clinics, constitute 3% of its services. A handful of clinics in two states supply fetal tissue for research. The videos provoked a furore that has intensified over the past few weeks. On 3 December, the Republican-led US Senate voted to strip Planned Parenthood of government funding. This is despite the fact that fetal tissue research is legal, the US National Institutes of Health (NIH) has been funding it for decades and President Obama is sure to veto the bill, should it reach his desk. A few days earlier, on 27 November, a gunman shot dead three people at a Planned Parenthood clinic in Colorado Springs, Colorado. In a post-arrest interview, the suspect is reported to have said \u201cno more baby parts\u201d. The episode has shone a spotlight on a little-discussed arm of biomedical research, raising the questions of why, how and how widely fetal tissue is used. To find out,  Nature  turned to an NIH database of research grants funded in 2014 to find those using fresh human fetal tissue, and in October contacted 18 researchers working with it. Su was one of only two who were willing to be interviewed. Most requests were declined or went unanswered; a public-affairs officer at one major Texas university refused to have a researcher speak to  Nature  to keep that person \u201csafe\u201d. The figures show that in 2014, the NIH funded 164 projects using the tissue, at a cost of $76 million. This is slightly less than half of what the agency spent on work with human embryonic stem cells (ES cells), which has also been highly controversial, and 0.27% of the $27.9 billion it spent on all research. (By comparison, the UK Medical Research Council spent 0.16% \u2014 \u00a31.24 million ($1.9 million) \u2014 of its total spending on research on five projects involving fetal tissue in the 12 months up to 31 March 2015.) Analysis of the NIH projects shows that the tissue is used most heavily for research on infectious diseases, especially HIV/AIDS; in the study of retinal function and disease; and in studies of normal and anomalous fetal development (see \u2018Fetal tissue research by discipline\u2019). Opponents argue that the work is not necessary because other model systems and techniques can be used. \u201cThis is antiquated science,\u201d says David Prentice, the vice-president and research director at the Charlotte Lozier Institute, the research arm of the Susan B. Anthony List, which is an anti-abortion organization in Washington DC. \u201cThere are better and, frankly, more successful alternatives.\u201d But supporters of the research counter that fetal tissue is legally obtained, that it would otherwise be destroyed, that such work has already led to major medical advances and that, if there were better alternatives, they would turn to them. \u201cFetal tissue is a flexible, less-differentiated tissue. It grows readily and adapts to new environments, allowing researchers to study basic biology or use it as a tool in a way that can\u2019t be replicated with adult tissue,\u201d says Carrie Wolinetz, the NIH\u2019s associate director for science policy. \u201cI get very frustrated when misinformed people go on about how it can all be done with computer models or cell cultures or stem cells or animals,\u201d says Paul Fowler, a reproductive biologist at the University of Aberdeen Institute of Medical Sciences, UK, who in January published a study using livers from aborted fetuses to probe the impacts of maternal smoking on liver development 1 . \u201cIn some areas, the human is absolutely dramatically different than rodents.\u201d Some argue that the entire episode represents a thinly cloaked attempt to attack and limit access to abortion by eroding support and funding for Planned Parenthood. \u201cPeople are talking about fetal tissue, but really what this discussion is about is abortion,\u201d says Shari Gelber, a specialist in maternal\u2013fetal medicine at Weill-Cornell Medical College in New York City, who has argued for the value of the research. \n               Laboratory lines \n             Cell lines derived from aborted fetal tissue have been fairly commonplace in research and medicine since the creation in the 1960s of the  WI-38 cell strain , which was derived at the Wistar Institute in Philadelphia, Pennsylvania, and MRC-5, which came from a Medical Research Council laboratory in London (see  Nature 498, 422\u2013426; 2013 ). Viruses multiply readily in these cells, and they are used to manufacture many globally important vaccines, including those against measles, rubella, rabies, chicken pox, shingles and hepatitis A. Companies have shipped at least 5.8 billion vaccines made with these two cell lines which, with others, have become standard laboratory tools in studies of ageing and drug toxicity. (Research with such lines is not covered by US regulations governing the use of fresh fetal cells and tissue nor captured in the NIH database.) In the past 25 years, fetal cell lines have been used in a roster of medical advances, including the production of a blockbuster arthritis drug and therapeutic proteins that fight cystic fibrosis and haemophilia. But off-the-shelf fetal cell lines are of limited use for scientists because they do not faithfully mimic native tissue and represent only a subset of cell types: WI-38 and MRC-5, for example, were derived from fetal lungs. The lines can also accumulate mutations after replicating  in vitro  over time. And creating humanized mice such as Su\u2019s requires whole pieces of fetal organs to provide sufficient numbers of stem cells. For all of these reasons, researchers turn to fresh tissue. In the United States, this is collected at medical centres and clinics that perform abortions under a patchwork of laws and regulations governing consent, tissue collection and transfer (see \u2018Fetal tissue and the law\u2019). US law says that clinics can recover \u201creasonable payments\u201d to offset the costs of providing the tissue, but it makes it a felony to profit from doing so. Planned Parenthood officials say that its clinics obtain full and informed consent from women choosing to donate fetal remains for research, and the organization announced in October that its clinics will no longer recover costs of $45\u201360 per specimen for collecting the tissue. \n               boxed-text \n             From the clinics, fetal tissue is then often passed to biological-research supply companies, which act as intermediaries and process the tissue before selling it to researchers. Su pays $830 for each sample of fetal liver tissue supplied to his lab by one of the most widely used suppliers, Advanced Bioscience Resources in Alameda, California. \n               HIV and AIDS \n             The category of fetal tissue work that draws most NIH funding is the study of HIV and AIDS: it accounts for 64 of the 164 NIH grants. Researchers in this field have long struggled with the paucity of effective models for this uniquely human disease. The standard models, macaques, are expensive to breed, are infected with SIV instead of HIV and have immune responses that are different from those of people. The flexibility and adaptability of fetal tissue \u2014 and its richness as a source of stem cells \u2014 has allowed the creation of a number of mice with humanized immune systems. Prominent among these is the BLT (bone marrow\u2013liver\u2013thymus) mouse, which was created in 2006 2 . This model is made by destroying the animal\u2019s immune system and then surgically transplanting liver and thymus tissue fragments from a human fetus into the mouse. The immune system is further humanized with a bone-marrow transplant, using blood-forming stem cells from the same fetal liver. The animal enables studies of, for instance, immune responses that are key to developing an effective HIV vaccine. The mouse has \u201caccelerated the study of HIV pathogenesis and novel approaches to harness anti-viral immunity to control HIV\u201d, reads a recent review by several NIH-funded scientists who are using the mouse 3 . The mouse has also helped to demonstrate that prophylactic drugs may prevent vaginal HIV infection \u2014 a strategy that is now in late-stage human trials. The animal is currently being used to examine how genital infection with herpes simplex virus alters immunity at the vaginal mucosa, making it easier for HIV to infect. In a similar vein, Su is now using his humanized mouse to examine the mechanisms by which hepatitis C and HIV co-infection can hasten liver disease. There are drawbacks: the BLT mouse\u2019s average lifespan is relatively short, at only around 8.5 months, because the animals tend to develop cancers of the thymus. And the humanized immune system is not inherited, so the model must be created again and again \u2014 leading to the constant demand for fetal tissue that so disturbs abortion opponents. \n               Human development \n             In some research areas, fetal tissue may, in time, be replaced by other materials and methods: alternative, flexible cell types, including human ES cells and induced pluripotent stem (iPS) cells, and organoids, which are  lab-created cellular structures that resemble tissue from normal organs . But there is one area in which, scientists say, fetal tissue is needed by definition: studies of early human development, and why it sometimes goes wrong. \u201cHuman fetal tissue is likely never going to be replaced in some areas of research, particularly relative to fetal development,\u201d says Wolinetz. And the application of such work goes far beyond understanding developmental disorders such as congenital heart disease or other malformations, says Neil Hanley, an endocrinologist at the University of Manchester, UK. \u201cFor a wide range, now, of adult diseases and disorders, we know that they have their origins during very early human development,\u201d he says \u2014 type 2 diabetes and schizophrenia are both cases in point. \u201cAnd unless you understand normal you\u2019re not going to understand abnormal.\u201d The 30 developmental-biology grants involving fetal tissue that were awarded by the NIH in 2014 range from a study of the differentiation of myoblasts, which are the embryonic precursors of muscle cells, to several examinations of development of the urogenital tract \u2014 studies with relevance, for instance, to hypospadias, a common condition in which the urethra fails to close and the underside of the penis is incompletely formed. One project is creating a three-dimensional atlas of gene expression in the genital tubercle, the precursor of the penis. Another is probing gene activity in cells lining the fetal intestine to help explain excessive intestinal inflammation in premature babies. Hanley says that such studies are important, particularly because gene regulation \u2014 the finely tuned symphony that controls when and where genes are active \u2014 can vary strikingly between species, so findings in other animals often do not hold true in humans. More than half of the 30 grants are for studies of brain development, and many of these projects are seeking advances in combating maladies such as autism, schizophrenia and Alzheimer\u2019s disease. Larry Goldstein, a neurobiologist at the University of California San Diego School of Medicine in La Jolla, uses cells called astrocytes from the brains of aborted fetuses to nourish neurons that he has derived from iPS cells and that have mutations associated with Alzheimer\u2019s disease. The astrocytes are thought to secrete factors that keep the neurons healthy in culture, and he uses the system to study the pathogenesis of the disease and to test potential drugs. Goldstein hopes eventually to derive the astrocytes, too, from iPS cells. But \u201cthe human fetal astrocytes that we get at present are the gold standard that we use, and will use, to compare astrocytes that we make by differentiation\u201d, he says. He has also used neurons from aborted fetal brains to compare with the neurons made from iPS cells 4 . \u201cAs long as fetal tissue is available, this is a very valuable use of it,\u201d he says. Another 23 of the NIH grants using fetal tissue involve eye development and disease. Damage to the retinal pigment epithelium (RPE), a single layer of cells at the back of the eye, has a key role in a number of eye diseases, including age-related macular degeneration, the most common cause of blindness in adults in the developed world. The 2000s saw advances in ways to create cell cultures with RPE dissected from the eyes of fetuses, allowing scientists to study the function of these cells in a dish. And although some scientists have turned to stem cells to generate RPE, like Goldstein they continue to use fetal tissue as a benchmark of normal development and function. Goldstein agreed to speak to  Nature , he says, because \u201csomebody has to speak up responsibly\u201d. He stressed that he and his colleagues think hard about the ethics of their work. \u201cWe are not happy about how the material became available, but we would not be willing to see it wasted and just thrown away.\u201d Occasionally, fetal tissue is used for clinical work. Last year, a company called Neuralstem in Germantown, Maryland, in collaboration with scientists at the University of California, San Diego, launched a trial in which stem cells from fetal spinal cord were implanted to treat spinal-cord injuries. In May, researchers in the United Kingdom and Sweden launched a study in which dopaminergic neurons from aborted fetuses are transplanted into the brains of patients with Parkinson\u2019s disease (see  Nature 510,195\u2013196; 2014 ). Research with fetal tissue is less controversial in countries where abortion is more widely accepted. \n               Uncomfortable viewing \n             The Planned Parenthood videos caused even some supporters of fetal tissue research to feel uncomfortable. In one video, physician Deborah Nucatola, the group\u2019s senior director of medical services, describes how she crushes fetuses above and below key organs to preserve them intact for research. She also described turning a fetus into a breech presentation to deliver the head last, when the cervix is more dilated, thus preserving the brain. This raised the question of whether physicians are altering abortion techniques to accommodate research requests, violating a widely held precept of research ethics. Arthur Caplan, a bioethicist at the New York University School of Medicine, dismisses the videos as \u201cpure politics\u201d, but some of the footage \u201cdid get my eyebrow to arch\u201d, he says. \u201cYou can\u2019t use a different approach to the abortion to try to preserve something. Those are just no-no\u2019s.\u201d Planned Parenthood spokeswoman Amanda Harrington says that the organization is not aware of any instances in which the method of an abortion has been changed to preserve organs. But, she adds, \u201cif minor adjustments that have no bearing on the woman\u2019s health and safety are done when the woman has expressed a desire to donate tissue, that is entirely appropriate and ethical and legal\u201d. Women\u2019s health and safety, she says, \u201cis always the number one priority\u201d. The question for many scientists is what the fallout of the controversy will be. On the heels of the Colorado shootings, some Republicans in Congress backed off earlier attempts to defund Planned Parenthood, and President Obama is expected to veto any bill that does so. This means that the lasting damage of the videos may end up being inflicted not on Planned Parenthood\u2019s budget, but on science. Since July, four bills that would criminalize or otherwise restrict the research have been introduced in the US Congress, and lawmakers have launched similar efforts in a dozen state legislatures. (Missouri, Arizona and North Dakota already ban the research.) Su felt the climate for his research grow colder when, on 1\u00a0October, a new North Carolina law was signed that makes it a felony to sell fetal tissue for any amount within the state. Su receives the tissue he uses from outside the state, but the message behind the new law concerns him. \u201cI hope this current controversy, or possible congressional interventions, won\u2019t slow down biomedical research,\u201d he says. \u201cThe benefit is bigger than the drawback on this.\u201d The controversy \u201cabsolutely puts fetal tissue research at risk\u201d, says Caplan. \u201cYoung scientists are unlikely to enter a field riven with controversy, where funding is uncertain and physical threats are a real possibility.\u201d Caplan says that parallels could emerge with events in the early 2000s, when the use of human ES cells in US research became politically fraught. Then, tight federal regulations governing NIH funding of the research were adopted, but some states, including California and Massachusetts, responded by pouring money into the science all the same. \u201cTo move ahead, the reality is that fetal tissue research need not be funded or permitted everywhere,\u201d Caplan says. \u201cIt needs to be allowed somewhere.\u201d \n                 Tweet \n                 Follow @NatureNews \n               See Editorial  page 163 . \n                     Fetal-cell revival for Parkinson\u2019s 2014-Jun-11 \n                   \n                     A culture of consent 2013-Jun-26 \n                   \n                     Medical research: Cell division 2013-Jun-26 \n                   \n                     Planned Parenthood \n                   \n                     National Institutes of Health \n                   Reprints and Permissions"},
{"file_id": "528022a", "url": "https://www.nature.com/articles/528022a", "year": 2015, "authors": [{"name": "Julie Gould"}], "parsed_as_year": "2006_or_before", "body": "There are too many PhD students for too few academic jobs \u2014 but with imagination, the problem could be solved. \u201cSince 1977, we've been recommending that graduate departments partake in birth control, but no one has been listening,\u201d said Paula Stephan to more than 200 postdocs and PhD students at a symposium in Boston, Massachusetts, in October this year. Stephan is a renowned labour economist at Georgia State University in Atlanta who has spent much of her career trying to understand the relationships between economics and science, particularly biomedical science. And the symposium, 'Future of Research', discussed the issue to which Stephan finds so many people deaf: the academic research system is generating progeny at a startling rate. In biomedicine, said Stephan. \u201cWe are definitely producing many more PhDs than there is demand for them in research positions.\u201d Julie Gould picks apart the problems with the PhD system The numbers show newly minted PhD students  flooding out of the academic pipeline . In 2003, 21,343 science graduate students in the United States received a doctorate. By 2013, this had increased by almost 41% \u2014 and the life sciences showed the greatest growth. That trend is mirrored elsewhere. According to a 2014 report looking at the 34 countries that make up the Organisation for Economic Co-operation and Development, the proportion of people who leave tertiary education with a doctorate has doubled from 0.8% to 1.6% over the past 17 years. Not all of these students want to pursue academic careers  \u2014 but many do, and they find it tough because there has been no equivalent growth in secure academic positions. The growing gap between the numbers of PhD graduates and available jobs has attracted particular attention in the United States, where students increasingly end up stuck in  lengthy, insecure postdoctoral research positions . Although the unemployment rate for people with science doctorates is relatively low, in 2013 some 42% of US life-sciences PhD students graduated without a job commitment of any kind, up from 28% a decade earlier. \u201cBut still students continue to enrol in PhD programmes,\u201d Stephan wrote in her 2012 book  How Economics Shapes Science . \u201cWhy? Why, given such bleak job prospects, do people continue to come to graduate school?\u201d One reason is that there is little institutional incentive to turn them away. Faculty members rely on cheap PhD students and postdocs because they are trying to get the most science out of stretched grants. Universities, in turn, know that PhD students help faculty members to produce the world-class research on which their reputations rest. \u201cThe biomedical research system is structured around a large workforce of graduate students and postdocs,\u201d says Michael Teitelbaum, a labour economist at Harvard Law School in Cambridge, Massachusetts. \u201cMany find it awkward to talk about change.\u201d But there are signs that the issue is becoming less taboo. In September, a group of high-profile US scientists (Harold Varmus, Marc Kirschner, Shirley Tilghman and Bruce Alberts, colloquially known as 'the Quartet') launched Rescuing Biomedical Research, a website where scientists can make recommendations on how to 'fix' different aspects of the broken biomedical research system in the United States \u2014 the PhD among them. \u201cHow can we improve graduate education so as to produce a more effective scientific workforce, while also reducing the ever-expanding PhD workforce in search of biomedical research careers?\u201d the site asks. Nature  put a similar question to 33 PhD students, scientists, postdocs and labour economists and uncovered a range of opinions on how to build a better PhD system, from small adjustments to major overhauls. All agreed on one thing: change is urgent. \u201cAcademia really is going to have to be dragged kicking and screaming into the twenty-first century,\u201d says Gary McDowell, a postdoctoral fellow at Tufts University in Medford, Massachusetts, and a leader of the group behind the Future of Research symposium. The renovation needs to happen now, says Jon Lorsch, director of the US National Institute of General Medical Sciences in Bethesda, Maryland. \u201cWe need to transform graduate education within five years. It's imperative. There's a lot at stake for scientists, and hence for science.\u201d \n               Track the PhD \n             One place to begin is with hard facts: show prospective students and supervisors data on trainees' chances of moving into academic research or other careers. Prospective students \u201caren't thinking strategically about what they really want to do or what they're best suited for\u201d, says Patricia Labosky, a programme director for scientific training at the US National Institutes of Health (NIH) in Bethesda, Maryland. A  2015  Nature  survey of more than 3,400 science graduate students  around the world suggested that many were overly optimistic about their chances in academia. About 78% of respondents said that they were \u201clikely\u201d or \u201cvery likely\u201d to follow an academic career, and 51% thought that they would land some type of permanent job in one to three years. In reality, only about 26% of PhD students in the United States move into tenured or tenure-track positions, and getting there can take much longer than this (see 'Ups and downs of PhDs'). But although some data exist about career paths, there are key gaps relating to the range of job opportunities, earnings, time spent as a postdoc and long-term career trajectories, says Julia Lane, an economist at New York University. A January report on post-PhD careers by the US Council of Graduate Schools in Washington DC found that there are no standardized ways to collect information on graduates after they have left their educational institution; only around one-third of universities in the United States and Canada formally compile such data. In October, Stanford University in California published the results of a major effort to track graduates either 5 or 10 years after their PhD. It showed that the number of bioscience PhD students progressing to postdoctoral positions had dropped from 41% to 31% in the more recent graduate group, and that many were moving into business, government or non-profit positions. This probably reflects the growing bottleneck in academic jobs and booming opportunities in business. Lane is leading a more comprehensive effort to track career outcomes in research called UMETRICS, which is based at the University of Michigan in Ann Arbor. By combining anonymized human-resource and administrative data from universities with US Census Bureau data on earnings, places of work and job titles, UMETRICS will be able to produce campus-level reports on the career outcomes of graduate students. A student interested in a chemistry PhD, for example, could scan a campus report and see what previous graduates went on to do, where they went and how much they earn. It will take several years before the first data sets are released, Lane says \u2014 but when they are, \u201cthe students opting in to graduate schools will go in with eyes wide open\u201d. \n               Revamp the PhD \n             Many PhD students enjoy the intellectual freedom of a PhD for a few years and then successfully  move on to other things . But a lot of students want more preparation and training for that step \u2014 such as  building skills in management, budgeting or negotiation . \u201cApparently, you have to learn these things somewhere on the side, since you are supposed to spend all your time as a PhD and postdoc doing research,\u201d says Joanna Klementowicz, a postdoc at the University of California, San Francisco (UCSF). The current graduate education system in many countries is based on an apprenticeship model, wherein lab heads train younger researchers in the craft of research. This system has been prominent since the 1800s, when the first 'modern' PhD was awarded by the University of Berlin. Although the scientific enterprise has changed dramatically since then,  the PhD system has not . Modernizing the PhD could improve training in areas of research ranging from reproducibility to experimental design and entrepreneurship. It could also help to solve the bottleneck problem by equipping doctorate holders with soft skills that make them more employable wherever they go. \u201cWe need to tailor graduate education to meet the needs of students without violating what it means to be a scientist,\u201d says Alan Leshner, chief executive emeritus of the American Association for the Advancement of Science in Washington DC. Some funding bodies and research institutions have  already taken this on board . In 2013, the NIH started the Broadening Experiences in Scientific Training (BEST) initiative \u2014 a US$3.7-million programme that is designed to improve training for biomedical PhDs and postdocs. \u201cWe got a lot of feedback from [employers] that the graduates weren't ready for careers outside of academia,\u201d says Labosky, who heads the programme. At UCSF, PhD students on the BEST programme spend nine months training in areas such as management, interviewing and networking, and are put into groups that work together to explore career objectives. \u201cThe programme made me practical: I learned to look out for what I can apply for, what my skills were matched to and what people with a PhD like mine go on to do,\u201d says Klementowicz, who took the programme as a postdoc. Some scientists would like to see particular emphasis put on teamwork to reflect the increasingly collaborative nature of research. David Golan, dean of graduate education at Harvard Medical School in Boston, Massachusetts, is considering how to ingrain teamwork more deeply into the graduate-school experience. \u201cWe have toyed with the idea of having students form a team before they apply to grad school,\u201d he says. They might then be given a project to work on together throughout their training \u2014 and perhaps even be examined together. \n               Split the PhD \n             There may be too many PhD graduates for academia, but there is plenty of demand for highly educated, scientifically minded workers elsewhere. So some scientists propose that the PhD should be split into two: one for future academics and a second to train those who would like in-depth science education for use in other careers. Biologist Anthony Hyman, director of the Max Planck Institute of Molecular Cell Biology and Genetics in Dresden, Germany, is one of those who thinks that a split PhD might work. Students in the academic-track PhD would focus on blue-skies research and discovery, he says. A vocational PhD would be more structured and directed towards specific careers in areas such as radiography, machine learning or mouse-model development. A similar concept already exists in engineering: students in the United Kingdom, the United States, France and Germany can choose to study for either an academic-style PhD in engineering or a doctorate in engineering (EngD), which is designed with industrial careers in mind and often involves a supervisor in industry alongside one in academia. David Stanley, who manages an EngD programme that focuses on nuclear engineering at the University of Manchester, UK, says that the programme is aimed at supplying industry with employees. \u201cGraduates with an EngD are highly valued in industry, more than those with PhDs, because of their extended training,\u201d he says. Elsewhere, industrial PhDs are taking shape in the biomedical sciences. One of the oldest government-organized industrial PhD schemes is run by Innovation Fund Denmark, which supports students who are simultaneously enrolled at a Danish university and employed (and paid) by a private-sector company. Melanie Sinche, director of education at the Jackson Laboratory for Genomic Medicine in Farmington, Connecticut, is enthusiastic about the idea of a vocational PhD at her institute, where it might fulfil a need for more expert computational biologists. \u201cThe number of people qualified to do this is small, and there are lots of employers competing for this small pool of candidates,\u201d she says. But the split PhD could face challenges if the two tracks are valued in different ways: academics could view a vocational PhD as second-class, whereas tech companies could view an academic PhD as too abstruse for the real world. That could end up limiting the career options of doctorates rather than broadening them, says Hyman. Stanley counters that EngD students do not have that problem. \u201cA couple of students a year find their way back into academia to conduct research,\u201d he says. \n               Skip the PhD \n             Some scientists call for more drastic measures \u2014 cutting down the number of people who pursue a PhD. Siphoning off more students into master's programmes is one way to reduce PhD numbers, says Bruce Alberts, professor of biochemistry and biophysics in the department of medicine at UCSF. A master's can offer advanced scientific training that is sufficient for many careers, as well as a taste of research, in one or two years rather than the four or five eaten up by a typical PhD. \u201cIn an ideal world, everyone would go in for a master's,\u201d Alberts says. Master's degrees are already common across Europe. In the Netherlands, students are required to complete a master's before embarking on a PhD. \u201cThere are many who don't want to be in academia who leave with a master's to work in government institutions, companies, in publishing,\u201d says Frank Miedema, professor and head of immunology at the University Medical Center Utrecht in the Netherlands. \u201cAnd a master's is not considered a failure for those who can't make it to a PhD.\u201d Victoria Evans graduated with a master's degree in astrophysics from Cardiff University, UK, in 2012. \u201cThe research project in the master's gave me an insight into what a PhD project would be like,\u201d she says, \u201cand I came to the conclusion that it wasn't what I wanted to do.\u201d She now works as a nuclear-safety engineer for EDF Energy on the west coast of Scotland. \u201cThe problem-solving and analytical skills that I learned during my master's were more than sufficient for me to work in this field.\u201d In the United States, the science master's has often had a lower status than the PhD \u2014 but universities are now launching more of them. Between 2000 and 2011, the number of science and engineering master's degrees available increased by 57%, compared with a 38% increase in doctoral degrees, according to the US National Science Foundation. Part of that growth has been in the professional science master's degree, a programme developed in the late 1990s as a graduate degree that would simultaneously develop scientific and workplace skills. Last year, Harvard Medical School introduced a two-year master's in immunology aimed at students who want additional classroom and research experience to help them decide whether to continue on to a PhD or MD, or to transition to industry. But master's programmes are no panacea. Unlike most doctoral students, master's students in the United States and Europe are often required to pay for their tuition, and that could dissuade many from signing up. \u201cThis does create a social access problem,\u201d said neuroscientist Eve Marder of Brandeis University in Waltham, Massachusetts, at last month's Future of Research meeting. \n               Cut the PhDs \n             Labour economists have been advocating for a reduction in the number of graduate students who enter biomedical sciences for several decades. Yet there is enormous resistance to change. That's what the Quartet found, when it proposed gradually reducing the numbers of PhD students as part of its efforts to rescue biomedical research. \u201cThis idea has had the most opposition from our colleagues,\u201d says Alberts. Faculty members and research institutions may be especially reluctant to give up the cheap workers who power their research when government funding for biomedicine has fallen, as it has in the United States for the past decade or so. And some scientists argue that fewer PhD graduates would be a loss to science and society as a whole. \u201cThe draconian measures of restricting access to graduate school is detrimental to science,\u201d said Marder at the Future of Research meeting. \u201cIt means we would restrict the imagination in our workforce.\u201d Cuts to PhD programmes haven't gone down well. When the Canadian Institutes of Health Research cancelled its 30-year-old MD/PhD programme earlier this year owing to budget tightening, academics and students reacted with horror. But other fields regulate the flow of students into courses to match supply to demand. The American Bar Association, which oversees the legal system in the United States, attempts to regulate the number of qualified lawyers by exerting strict control over the number of law schools. And bar associations set fiendishly difficult examinations for would-be lawyers to get into law school in the first place. Stiffer entrance assessments for those who want to pursue a PhD could cut down entrant numbers \u2014 if the right criteria can be found. In the United States, Graduate Records Examinations (GREs) are used as a way of selecting entrants for graduate school, but the system is hardly perfect: one survey showed that 37% of US biology PhD students drop out before completing their degree. When Orion Weiner, a molecular biologist at UCSF, did a small, retrospective study of graduate students admitted onto one of his university's biology PhD programmes, he found that previous experience in research and the subject-specific GRE results (but not the analytical, verbal or quantitative elements) were good indicators of future success in graduate school. A broader entrance assessment could look at students' experience in communication, management, teamwork and career goals. That could be used to filter students with a passion for academic or industrial research towards PhD programmes and send others into a master's or other types of training, says Bill Lindstaedt, executive director for career advancement at UCSF. Stephan believes that funding bodies should have a major role in limiting the number of biomedical PhD places to better match supply and demand, and she also proposes that students should contribute to their training costs. \u201cWhen we have to pay something out of pocket, we think a little more clearly about whether that is a good fit for us,\u201d she says. Such ideas may be controversial \u2014 but many people say that they have to be considered. At the heart of the problem, say scientists, is that the community is not discussing the PhD problem enough. \u201cThere is a reluctance from supervisors to tell undergrads and grad students the reality of the system,\u201d says postdoc McDowell. \u201cThe misinformation exists because the system is worried about deflecting smart people from entering.\u201d Although principal investigators acknowledge the difficulty of securing an academic position, the system worked for them and so it is tempting to tell students that they can do it too \u2014 just another experiment, another publication or another year, and you'll get there. Grass-roots groups such as Future of Research are calling attention to the issue, as are efforts such as Rescuing Biomedical Research. Meanwhile, some experts say that the onus falls partly on prospective and current PhD students to  make sure their eyes are open . They should arm themselves with as much information as possible, says Labosky, so that \u201cthey are aware of their alternative options and can make plans\u201d. Stephan does see some prospect that her call for PhD birth control will be heard. She says that change might happen naturally, as more information becomes available on career outcomes, and that flat funding streams could prevent further growth in biomedical PhDs. \u201cIndividuals might become less focused on PhD production, and universities and faculty are more likely to pay attention to these recommendations.\u201d Teitelbaum, for his part, does not favour a large cut in biomedical PhDs, and instead prefers a more considered approach. \u201cFind out why people start PhDs and what they think their career prospects are from the very beginning,\u201d he says. \u201cLike ballet dancers or actors, if they chose to take it on knowing their chances of becoming a successful professor, then let them carry on.\u201d See Editorial  page 7 \n                 Tweet \n                 Follow @NatureNews \n               \n                     Fellowships are the future 2015-Dec-02 \n                   \n                     Make the most of PhDs 2015-Dec-02 \n                   \n                     Graduate survey: Uncertain futures 2015-Oct-21 \n                   \n                     The future of the postdoc 2015-Apr-07 \n                   \n                     How not to deal with the PhD glut 2014-Oct-22 \n                   \n                     There is life after academia 2014-Sep-03 \n                   \n                     Life outside the lab: The ones who got away 2014-Sep-03 \n                   \n                     Employment: PhD overdrive 2014-Jul-09 \n                   \n                     Fix the PhD 2011-Apr-20 \n                   \n                     Education: The PhD factory 2011-Apr-20 \n                   \n                     Education: Rethinking PhDs 2011-Apr-20 \n                   \n                     Reform the PhD system or close it down 2011-Apr-20 \n                   \n                     What is a PhD really worth? 2011-Apr-20 \n                   \n                     Nature  special: The future of the PhD \n                   \n                     Blog post: Tracking PhDs \n                   \n                     Future of Research \n                   \n                     Rescuing Biomedical Research \n                   \n                     Council of Graduate Schools:  Understanding PhD Career Pathways for Program Improvement  (PDF) \n                   \n                     NSF Survey of Doctorate Recipients \n                   Reprints and Permissions"},
{"file_id": "527436a", "url": "https://www.nature.com/articles/527436a", "year": 2015, "authors": [{"name": "Jeff Tollefson"}], "parsed_as_year": "2006_or_before", "body": "Countries have pledged to limit global warming to 2\u2009\u00b0C, and climate models say that is still possible. But only with heroic\u00a0\u2014\u00a0and unlikely\u00a0\u2014\u00a0efforts. The year is 2100 and the world looks nothing like it did when global leaders gathered for the historic climate summit in Paris at the end of 2015. Nearly 8.8\u00a0billion people now crowd the planet. Energy consumption has nearly doubled, and economic production has increased more than sevenfold. Vast disparities in wealth remain, but governments have achieved one crucial goal: limiting global warming to 2\u2009\u00b0C above pre-industrial temperatures. The United Nations meeting in Paris proved to be a turning point. After forging a climate treaty, governments immediately moved to halt tropical deforestation and to expand forests around the globe. By 2020, plants and soils were stockpiling more than 17\u00a0billion tonnes of extra carbon dioxide each year, offsetting 50% of global CO 2  emissions. Several million wind turbines were installed, and thousands of nuclear power plants were built. The solar industry ballooned, overtaking coal as a source of energy in the waning years of the twenty-first century. But it took more than this. Governments had to drive emissions into negative terri\u00adtory\u00a0\u2014\u00a0essentially sucking greenhouse gases from the skies\u00a0\u2014\u00a0by vastly increasing the use of bioenergy, capturing the CO 2  generated and then pumping it underground on truly massive scales. These efforts pulled Earth back from the brink. Atmospheric CO 2  concentrations peaked in 2060, below the target of 450\u00a0parts per million (p.p.m.)\u00a0and continue to fall. ******** That scenario for conquering global warming is one possible\u00a0\u2014 if optimistic\u00a0\u2014 vision of the future. It was developed by modellers at the Joint Global Change Research Institute in College Park, Maryland, as part of a broad effort by climate scientists to chart possible paths for  limiting global warming to 2\u2009\u00b0C , a target enshrined in the UN climate convention that will produce the Paris treaty. Climate modellers have developed dozens of rosy 2\u2009\u00b0C scenarios over several years, and these fed into the latest assessment by the  Intergovernmental Panel on Climate Change (IPCC) . The panel seeks to be policy-neutral and has never formally endorsed the 2-degree target, but its official message, delivered in April 2014, was clear: the goal is ambitious but achievable. This work has fuelled hope among policymakers and environ\u00admentalists, and it will provide a foundation for debate as governments negotiate a new climate agreement at the UN\u2019s 2015 Paris Climate Conference starting on 30 November. Despite broad agreement that the  emissions-reduction commitments  that countries have offered up so far are insufficient, policy\u00admakers continue to talk about bending the emissions curve downwards to remain on the path to 2\u00a0degrees that was laid out by the IPCC. But take a closer look, some scientists argue, and the 2\u2009\u00b0C scenarios that define that path seem so optimistic and detached from current political realities that they verge on the farcical. Although the caveats and uncertainties are all spelled out in the scientific literature, there is concern that the 2\u2009\u00b0C modelling effort has distorted the political debate by obscuring the scale of the challenge. In particular, some researchers have questioned the viability of large-scale bioenergy use with carbon capture and storage (CCS), on which many models now rely as a relatively cheap way to provide substantial negative emissions. The entire exercise has opened up a rift in the scientific community, with some people raising ethical questions about whether scientists are bending to the will of politicians and government funders who want to maintain 2\u2009\u00b0C as a viable political target. \u201cNobody dares say it\u2019s impossible,\u201d says Oliver Geden, head of the European Union Research Division at the German Institute for Inter\u00adnational and Security Affairs in Berlin. \u201cEverybody is sort of underwriting the 2-degree cheque, but scientists have to think about the credibility of climate science.\u201d Modellers are first to acknowledge the limits of their work, and say that the effort is designed to explore options, not predict the future. \u201cWe\u2019ll tell you how many nuclear power plants you need, or how much CCS, but we can\u2019t tell you whether society is going to be willing to do that or not,\u201d says Leon Clarke, a senior scientist and modeller at the Joint Global Change Research Institute. \u201cThat\u2019s a different question.\u201d \n               One trillion tonnes \n             The idea of limiting global warming to 2\u2009\u00b0C dates back to 1975, when economist William Nordhaus of Yale University in New Haven, Connecticut, proposed that more than 2 or 3 degrees of warming would push the planet outside the temperature range of the past several hundred thousand years. In 1996, the EU adopted that limit, and the Group of 8 (G8) nations signed on in 2009. The parties to the UN convention on climate change affirmed the target in 2009 at their Copenhagen summit, and then formally adopted it a year later in Canc\u00fan, Mexico. The move caught scientists off guard. Before 2009, most modellers had focused on scenarios in which atmospheric CO 2  concentrations stabilized around 550 p.p.m.\u00a0\u2014\u00a0double the pre-industrial level\u00a0\u2014\u00a0which would probably limit warming to a little less than 3\u2009\u00b0C. But as political interest in the 2\u2009\u00b0C target grew, a few started exploring the implications. In April 2009, a team led by Myles Allen, a climate scientist at the University of Oxford, UK, published 1  a study concluding that humans would have to  limit their total cumulative carbon emissions to 1\u00a0trillion tonnes \u00a0\u2014\u00a0more than half of which had already been dumped into the atmosphere \u2014 to maintain a chance of limiting warming to 2\u2009\u00b0C. This  trillion-tonne carbon budget  provided a scientific baseline for what was now a politically important target, and many modellers shifted gears. \u201cThere were very few scenarios with stringent targets such as 2\u2009\u00b0C, and then sponsors started demanding it,\u201d says Massimo Tavoni, deputy coordinator of climate-change programmes at the Eni Enrico Mattei Foundation in Milan, Italy. The flurry of modelling efforts that followed split into two main camps: pay early or pay late (see \u2018Two paths to 2 \u00b0C\u2019). In the former, nations need to slash greenhouse-gas emissions immediately; in the latter, they can buy time for a slower phase-out by developing a massive infrastructure to suck CO 2  out of the air. \u201cModels that have these negative emissions really do let you continue to party on now, because you have these options later,\u201d says John Reilly, co-director of the Joint Program on the Science and Policy of Global Change at the Massachusetts Institute of Technology (MIT) in Cambridge. In the pay-later approach, most models rely on a combination of bioenergy and CCS. The system starts with planting crops that are harvested and either processed to make biofuels or burnt to generate electricity, which provide carbon-neutral power because the plants absorb CO 2  as they grow. The CO 2  created when the plants are processed is captured and pumped underground, and the process as a whole eats up more emissions than it creates. A consortium sponsored by the US Department of Energy has tested such a system at one facility that produces bioethanol fuel in Illinois, but neither bioenergy nor CCS has been demonstrated on anywhere near the scales imagined by the models. \u201cIt\u2019s just simple arithmetic: the carbon budget is so small that you need to go negative, or at least you need to offset some of your emissions in order to get to zero,\u201d says Tavoni. \u201cWe tried to be honest, and pretty agnostic about whether these transformations are easily achievable.\u201d On the basis of those models and other information, the IPCC estimates that climate mitigation would reduce the projected global consumption in 2100 by 3\u201311%\u00a0\u2014\u00a0a relatively modest amount that would allow the global economy to keep growing overall. But remove either bioenergy or CCS from the scenarios and the costs increase substantially. If mitigation is delayed or bioenergy and CCS are constrained, most models simply can\u2019t limit warming to 2\u2009\u00b0C. The question is whether any of those models accurately reflect technical and social challenges. MIT has a model that tends to project costs two or three times the average reported by the IPCC, in part because it tries to reflect difficulties in scaling up any technology, such as the availability of skilled labour and natural resources in different regions. And then there are the technical hurdles. Capturing CO 2 from power plants has proved more difficult\u00a0and expensive\u00a0than many had hoped. Just one commercial project is currently operating, at the Boundary Dam Power Station in Saskatchewan, Canada. Moreover, Reilly says, the number of models that actually completed 2\u2009\u00b0C scenarios remains relatively small, and they probably project lower mitigation costs than those that are not able to generate these low-emissions scenarios. \u201cIt\u2019s a very self-selecting set of models.\u201d Although the caveats are listed in the IPCC assessment, the report does not adequately highlight economic and technical challenges or modelling uncertainties, says David Victor, a political scientist at the University of California, San Diego, who participated in the IPCC assessment. Victor does not place all the blame on scientists glossing over the problems: when researchers drafted the assessment\u2019s chapter on emissions scenarios and costs, he says, they included clear statements about the difficulty of achieving the 2\u2009\u00b0C goal. But the governments\u00a0\u2014\u00a0led by the EU and a bloc of developing countries\u00a0\u2014\u00a0pushed for a more optimistic assessment in the final IPCC report. \u201cWe got a lot of pushback, and the text basically got mangled,\u201d Victor says. For all of the concerns and criticisms, however, modellers say that the exercises have illuminated important research questions, such as how much bioenergy and CCS will cost and what effects they will have on land use, food systems and water availability. One 2014 study 2  in  Earth\u2019s Future , for instance, found that it would be difficult to grow enough bioenergy crops, even with second-generation cellulosic biofuels, which are made not only from a plant\u2019s sugars but also from the carbon in its stem and woody materials. The effort would require significant boosts in crop yields and the use of 77% more nitrogen fertilizer by 2100. The bio\u00adenergy would also need to be produced in centralized facilities that capture the bulk of the emissions. Unless everything goes right, scaling up to the level projected in many models would be difficult without significantly reducing food production or clearing large swathes of natural ecosystems for farmland. \u201cIf we need to ramp up such a large infrastructure, we need to investigate what that implies,\u201d says Sabine Fuss, an environmental scientist at the Mercator Research Institute on Global Commons and Climate Change in Berlin. Fuss led a commentary 3  in  Nature Climate Change  in October 2014 calling for a transdisciplinary research agenda on negative emissions. One of the first outgrowths of that work, led by co-author Peter Smith, a biologist at the University of Aberdeen, UK, is an upcoming assessment of carbon-negative strategies and potential limitations. Strategies include bioenergy with CCS, as well as other ways of absorbing carbon, such as planting forests, using chemical scrubbers to capture CO 2  directly from the air and crushing rocks to enhance geological weathering that consumes the gas. \u201cThe science behind these technologies is probably a bit behind the models,\u201d Smith says. \u201cThis sort of provides a road map for where we need to go in the next two or three years.\u201d \n               Risk factor \n             Modellers are also digging into real-world complexities. Most models assume that participation in climate mitigation will be global, that countries will put a common price on carbon, that technological solutions will be widely available and that this combination will drive investment towards relatively cheap mitigation options in developing nations. But the reality could be more complicated. A team at the Joint Global Change Research Institute worked with Victor and others to investigate the risks of making investments in developing countries due to political instability and the relatively poor quality of many public institutions there. Their model showed 4  that investors would probably shun developing countries and pour money into developed ones, driving up costs and making it harder to curb rapidly rising emissions in developing nations. \u201cThe models have taught us that with unrealistic assumptions anything is possible, and with realistic assumptions it will be very hard to cut emissions to meet goals like 2 degrees,\u201d Victor says. \u201cThat\u2019s an important result because it forces\u00a0\u2014\u00a0or should force\u00a0\u2014\u00a0some sobriety about what can be achieved.\u201d One message that modellers have delivered quite clearly is that without collective and aggressive action by all countries, costs invariably increase, and the chance of hitting the 2\u2009\u00b0C goal plummets. This is precisely the situation heading into the Paris summit. Most countries, and all of the major greenhouse-gas emitters, have submitted pledges to reduce their emissions, but these vary widely in ambition. As it stands, the world is on a path to nearly 3\u2009\u00b0C of warming by the end of the century, and even that assumes substantial emissions reductions in the future. If nations do not go beyond their Paris pledges, the world could be on track to use up its 2\u2009\u00b0C carbon budget as early as 2032. If the models are correct, world leaders may have to either accept extra warming or plan for a Herculean negative-emissions campaign. In the event that they choose the latter\u00a0\u2014\u00a0and succeed\u00a0\u2014\u00a0the entire debate will change. \u201cIt\u2019s a completely different game,\u201d says Nebojsa Nakicenovic, an economic modeller and deputy director-general of the International Institute for Applied Systems Analysis in Laxenburg, Austria. \u201cIf that is technically possible, then we could also go below 2 degrees.\u201d Fast-forward to 2100 once more. The bioenergy industry is now one of the largest and most powerful on Earth. People are pulling roughly as much CO 2  out of the atmosphere as they were emitting at the time of the historic Paris conference. Humanity has asserted control over the atmosphere, and governments face a new and difficult question at the 108th anniversary of the UN climate convention: how low should they set the global thermostat? \n                 Tweet \n                 Facebook \n                 LinkedIn \n                 Weibo \n                 Wechat \n               \n                     A \u2018perfect\u2019 agreement in Paris is not essential 2015-Nov-25 \n                   \n                     Global climate agreement: After the talks 2015-Nov-25 \n                   \n                     The way forward is through Paris 2015-Nov-25 \n                   \n                     The fragile framework 2015-Nov-24 \n                   \n                     Combined climate pledges of 146 nations fall short of 2\u2009\u00b0C target 2015-Oct-30 \n                   \n                     India unveils climate-change pledge ahead of global talks 2015-Oct-02 \n                   \n                     Policy: Climate advisers must maintain integrity 2015-May-06 \n                   \n                     UN gets first pledges on road to Paris climate talks 2015-Mar-31 \n                   \n                     Global-warming limit of 2\u2009\u00b0C hangs in the balance 2015-Mar-27 \n                   \n                     Lima talks map out path to climate treaty 2014-Dec-14 \n                   \n                     Climate policy: Ditch the 2\u00a0\u00b0C warming goal 2014-Oct-01 \n                   \n                     Nature  special: 2015 Paris climate talks \n                   \n                     United Nations Framework Convention on Climate Change \n                   \n                     Intergovernmental Panel on Climate Change \n                   Reprints and Permissions"},
{"file_id": "527427a", "url": "https://www.nature.com/articles/527427a", "year": 2015, "authors": [{"name": "Richard Monastersky"}, {"name": "Nick Sousanis"}], "parsed_as_year": "2006_or_before", "body": "A  Nature  comic examines the 25-year quest for a climate treaty. Can nations unite to save Earth\u2019s climate? When the world\u2019s nations gather in Paris this December to negotiate a climate treaty, their efforts will cap a 25-year-long journey plagued by detours and dead ends. The quest started in 1990 when the United Nations launched talks aimed at producing the first global climate agreement. In June 1992, more than 170 nations gathered for the Earth Summit in Rio de Janeiro, Brazil. In Rio, they adopted the United Nations Framework Convention on Climate Change (UNFCCC), which declared: \u201cThe ultimate objective of this Convention \u2026 is to achieve \u2026 stabilization of greenhouse gas concentrations in the atmosphere at a level that would prevent dangerous anthropogenic interference with the climate system.\u201d The Rio convention was a historic step, but it contained no binding commitments to slow global warming. Would the world act in time? \u201cWe don\u2019t have another 20 years to squander,\u201d said Maurice Strong, organizer of the Rio summit. More than 20 years later, greenhouse-gas emissions continue their relentless rise. What is the greenhouse effect? Water vapour, carbon dioxide and other gases in the atmosphere keep the planet warmer than it would otherwise be. Solar radiation warms Earth\u2019s surface, which radiates infrared energy. Greenhouse gases absorb and re-emit infrared radiation. Some of that energy warms the atmosphere. By adding extra CO 2 , methane and other pollutants, humans are strengthening the greenhouse effect. In 1896, the Swedish scientist Svante Arrhenius calculated how changes in the amount of CO 2  in the atmosphere could warm or cool Earth. He later suggested humans were raising the planet\u2019s temperature and it would become noticeable in a few centuries. The changes came much faster than Arrhenius anticipated. On 23 June 1988, NASA scientist James Hansen told a US Senate hearing that humans were having a clear impact by burning fossil fuels such as coal, oil and natural gas. \u201cThe greenhouse effect has been detected, and it is changing our climate now,\u201d he said. It was a wake-up call to the world. That very day, the temperature in Washington DC hit a record high of 98\u2009\u00b0F (37\u2009\u00b0C). The country was suffering one of its worst droughts ever, and uncontrollable fires raged in Yellowstone. Fossil fuels are not the only cause of warming. Deforestation also contributes by releasing the CO 2  stored in trees. On 22 December 1988, the assassination of Brazilian activist Chico Mendes drew attention to the rampant destruction of the Amazon forest. Alarmed by the growing problem, the United Nations created the Intergovernmental Panel on Climate Change (IPCC) in 1988 to assess the issue. At the IPCC\u2019s first meeting, the director of the United Nations Environment Programme, Mostafa Tolba, implored scientists to use the time left in the century \u2014 just 4,000 days \u2014 to deal with climate change. Hopes were high because the world had already taken steps to solve one environmental disaster. In 1987, nations adopted a treaty to protect the ozone layer. But reaching that agreement was relatively easy because only a handful of companies in a few countries produced ozone-destroying compounds. In the case of global warming, everyone has a hand in the problem because so many activities generate greenhouse gases. So curbing greenhouse-gas pollution is infinitely more difficult.\u00a0 In its first report, the IPCC forecasted that if current trends continue until 2100, the world would be 4\u2009\u00b0C warmer than it was in 1850. Swelling oceans would be a major problem because half of humanity inhabits coastal regions. A monstrous cyclone drove that point home in 1991 when it killed more than 140,000 people in Bangladesh. The Rio treaty was clearly not enough, so nations gathered in 1995 in Berlin to negotiate a stronger accord. But the assembled countries couldn\u2019t agree on specifics. Island nations wanted rich countries \u2014 the source of most of the problem \u2014 to cut emissions by 20%. The US demanded that developing nations agree to future commitments. China said developed nations had to act first because they had caused the problem. During all-night negotiations, Germany\u2019s environment minister, Angela Merkel, brokered a deal. Countries would have two years to agree on emissions limits for developed nations. In December 1997, countries gathered in Kyoto, Japan, to hash out a new treaty. But they couldn\u2019t agree on how much developed nations should trim their emissions. The European Union called for a 15% cut. Island nations demanded a 20% cut. Japan proposed a 5% cut. And the US wanted developing countries to act, too. After working through the final night, negotiators reached an agreement called the Kyoto Protocol. It was the first time that countries promised to rein in greenhouse-gas pollution by specific amounts. The Kyoto Protocol split the world in two: industrialized countries with emissions limits and developing countries without. The protocol also allowed for flexibility in how countries met their commitments. Developed nations could get credit for reducing emissions in poorer ones. Developed countries promised to cut their overall emissions to 5.2% below 1990 levels for the period 2008\u201312. Each country had its own target. But the cracks in the treaty were clear from the start. The US refused to ratify the pact because of concerns that its economy would suffer while developing nations increased their pollution without limits. In 2001, US President George W. Bush rejected the agreement, saying: \u201cThe Kyoto Protocol was fatally flawed in fundamental ways.\u201d Soon, world events made clear how limited the protocol was. In 2006, China passed the US to become the world\u2019s largest carbon emitter. Canada formally withdrew from Kyoto in 2011. Meanwhile, global temperatures continued to soar. Throughout the climate negotiations, scientists have tried to show what kind of world awaits future generations if global warming continues. Such forecasts come from complex climate system models, which divide the globe into millions of cells and simulate the atmosphere, oceans and biosphere in 3D. Researchers have confidence in their models because they can reproduce features of past and current climates. Looking forward, the models see red. Although scientists agreed humans were warming the planet, some politicians denied that fact. \u201cWith all of the hysteria, all of the fear, all of the phony science, could it be that man-made global warming is the greatest hoax ever perpetrated on the American people?\u201d asked US Senator James Inhofe, in 2003. Other politicians defended climate science. \u201cWe should investigate the well-funded effort by certain oil companies to manufacture controversy and cast doubt on the reality of global warming,\u201d said US Representative Henry Waxman in 2006. In 2003, Europe suffered a prolonged heat wave that killed an estimated 70,000 people. The impacts were getting clearer. The IPCC declared in 2007: \u201cWarming of the climate system is unequivocal.\u201d Later that year, the IPCC was awarded the Nobel Peace Prize for its efforts. While the science raced ahead, the negotiations dragged on. Nations met in Bali in December 2007. The talks were so fractious that at one point, the chairman broke down in tears. But a deadline was set for a treaty in 2009 that would include new commitments by developed countries. For the first time, developing nations agreed to \u201cmitigation actions\u201d of their own choosing to limit climate change. And participants decided the next treaty would address deforestation. In the run-up to the 2009 Copenhagen summit, hopes for a new climate treaty surged. But so did concerns about the scale of the problem. Studies suggested that carbon emissions should not exceed 1\u00a0trillion tonnes if nations wanted to limit global warming to 2\u2009\u00b0C, the goal that would avert catastrophic change. Yet the world had already used up more than half of that budget, and was burning through the rest at 9 billion tonnes a year. In 2009, activists organized demonstrations around the world to push for tighter emissions caps. Some demanded reducing CO 2  levels to 350\u2009p.p.m., which would limit future warming and lessen the risks of dangerous impacts like extreme sea-level rise and mega-droughts. Despite the frenzy of attention, the Copenhagen negotiations failed to deliver a treaty. Western nations blamed China for blocking substantive emissions limits. Developing countries charged that they had been left out of crucial discussions. A compromise was brokered by a handful of major emitters, but it was so controversial that much of the world didn\u2019t accept it. A year later, nations met in Canc\u00fan and adopted the provisional accord reached in Copenhagen. The Canc\u00fan agreements set a target to limit warming to no more than 2\u2009\u00b0C. They also created a Green Climate Fund, supported by developed countries to help poorer ones reduce emissions and adapt to climate change. Countries are expected to contribute US$100 billion a year to the fund by 2020. And for the first time, all countries agreed to reduce emissions according to their different responsibilities and capacities. As the negotiations have crawled along, the world has hurtled through changes. In October 2011, the global population topped 7 billion people. A heat wave in Russia in 2010 killed roughly 55,000 people and stoked wildfires across the country. In 2012, 97% of the Greenland ice sheet showed signs of melting \u2014 the first time such an extensive area had thawed. In November 2013, Superstorm Haiyan became the strongest on record to make landfall when it slammed into the Philippines with winds of 315 kilometres per hour. And the Arctic Sea set a record in 2012 for the least amount of ice coverage ever documented. At climate talks in 2013, Philippine delegate Naderev Sano broke down over his country\u2019s devastation and the lack of progress in negotiations. \u201cBy failing to meet the objective of the convention, we may have ratified the doom of vulnerable countries,\u201d he said. On 9 May 2013, daily atmospheric measurements of CO 2  at a station in Hawaii topped 400\u00a0p.p.m. for the first time in, perhaps, millions of years. James Hansen, who had sounded the alarm over global warming, was arrested several times for protests over the proposed Keystone XL pipeline. In September 2013, the IPCC reported that \u201cHuman influence on the climate system is clear.\u201d In 2014, China overtook the European Union in per capita emissions of CO 2 . The US and China brokered a historic climate deal in November 2014. The US pledged to cut greenhouse-gas emissions 26\u201328% below 2005 levels by 2025. China said its emissions would peak around 2030. These became the basis for what the two countries submitted to the UNFCCC as their proposed emissions targets in advance of the Paris summit. So far, more than 160 countries have submitted their pledges. With nearly one-third of its population still lacking electricity, India says it cannot yet cut its CO 2  emissions. Prime Minister Narendra Modi\u2019s government pledged to substantially increase energy efficiency. 2014 set the record as the hottest year ever recorded for global land and sea surface temperatures. Nations will face a test again when they meet in Paris in December: can they take significant steps to limit climate change? The calls for action \u2014 and the warning signs \u2014 grow stronger. \u201cClimate change is a global problem with grave implications \u2026. It represents one of the principal challenges facing humanity in our day,\u201d said Pope Francis. Once again, global temperatures will set a record. 2015 is on pace to top 2014. \u201cAlthough we\u2019re moving in the right direction, it is clearly not enough,\u201d says Christiana Figueres, executive secretary of the UNFCCC. Fires in Indonesia ravaged the country and pumped half a billion tonnes of carbon into the air, more than Japan produces in a year. The Paris pledges will probably limit warming to below 3\u2009\u00b0C. But much stronger action is needed to stay under 2\u2009\u00b0C. Countries will probably blow through the trillion-tonne carbon budget before 2040. CO 2  continues to thicken the skies. Concentrations may never again drop below 400\u00a0p.p.m. Even if global warming does not pass 2\u2009\u00b0C, the world might still face calamitous impacts, like parts of the Antarctic ice sheet sliding into the ocean within a few centuries. The world has come a long way since Kyoto. This year, all countries are setting their own goals, which may make the targets more realistic. And nations may agree to step up their commitments periodically. The framework to fix the planet is coming together, but it is fragile and far too small. The job of finishing the task will fall to future generations. \n               boxed-text \n             \n                 Tweet \n                 Follow @NatureNews \n               \n                     All together now 2015-Nov-25 \n                   \n                     A \u2018perfect\u2019 agreement in Paris is not essential 2015-Nov-25 \n                   \n                     Global climate agreement: After the talks 2015-Nov-25 \n                   \n                     Is the 2\u2009\u00b0C world a fantasy? 2015-Nov-24 \n                   \n                     Combined climate pledges of 146 nations fall short of 2\u2009\u00b0C target 2015-Oct-30 \n                   \n                     India unveils climate-change pledge ahead of global talks 2015-Oct-02 \n                   \n                     Obama acts alone on climate 2015-Jan-27 \n                   \n                     The Kyoto Protocol: Hot air 2012-Nov-28 \n                   \n                     Nature  special: 2015 Paris climate talks \n                   \n                     Nature  special: Outlook for Earth \n                   \n                     Nature  special: After Kyoto \n                   \n                     Nature  special: Climate showdown in Durban \n                   \n                     Nature  special: Road to Copenhagen \n                   \n                     United Nations Framework Convention on Climate Change \n                   \n                     Intergovernmental Panel on Climate Change \n                   Reprints and Permissions"},
{"file_id": "527425a", "url": "https://www.nature.com/articles/527425a", "year": 2015, "authors": [], "parsed_as_year": "2006_or_before", "body": "After 25 years of negotiations, all countries are finally set to take steps to limit global warming. A special issue examines the path to the Paris climate summit, and the road beyond. When more than 190 nations gather in Paris on 30\u00a0November to broker an agreement to mitigate climate change, it will be a turning point for the planet. A successor to the 1997 Kyoto Protocol has been a long time coming. A previous attempt to shape a global agreement fell apart in 2009, at talks in Copenhagen. Now the world is ready to try again, and for the first time, all countries are poised to take action (see  page 418 ). But the history here is sobering: the quest to build a global climate treaty has hit many obstacles over the past 25 years. Its dramatic story is chronicled in a comic starting on  page 427 . Although the United Nations aims to limit global warming to 2\u2009\u00b0C, a News Feature on  page\u00a0436  reveals that this will be much harder than many studies have indicated. Part of the difficulty will be ensuring that any treaty leads to actions with lasting global momentum, say climate-policy experts David Victor and James Leape (see  page 439 ). But Johan Rockstr\u00f6m, director of the Stockholm Resilience Centre, argues on  page\u00a0411  that Paris will be a success if it shows that the world is serious about addressing the climate problem. To explore the backstory to the talks, historian Adam Rome reviews seminal books on sustainability from the 1960s and 1970s (see  page 443 ). A News story explores the challenges facing the Green Climate Fund, a UN mechanism to help developing nations adapt to climate change (see p age 419 ). Online,  Nature  presents videos about the climate summit, as well as other unique material, at  www.nature.com/parisclimate . We will also cover the talks as they happen. Any agreement reached in Paris will not solve the climate problem, but it could lay a solid foundation for collective action (see  page 409 ). The quest to save the planet will continue for many more years. \n                 Tweet \n                 Follow @NatureNews \n               \n                     Combined climate pledges of 146 nations fall short of 2\u2009\u00b0C target 2015-Oct-30 \n                   \n                     India unveils climate-change pledge ahead of global talks 2015-Oct-02 \n                   \n                     Obama acts alone on climate 2015-Jan-27 \n                   \n                     The Kyoto Protocol: Hot air 2012-Nov-28 \n                   \n                     Nature  Tumblr special: Paris climate talks 2015 \n                   \n                     Nature  special: 2015 Paris climate talks \n                   \n                     Nature  special: Outlook for Earth \n                   \n                     Nature  special: After Kyoto \n                   \n                     Nature  special: Climate showdown in Durban \n                   \n                     Nature  special: Road to Copenhagen \n                   \n                     United Nations Framework Convention on Climate Change \n                   \n                     Intergovernmental Panel on Climate Change \n                   Reprints and Permissions"},
{"file_id": "528182a", "url": "https://www.nature.com/articles/528182a", "year": 2015, "authors": [{"name": "Corie Lok"}], "parsed_as_year": "2006_or_before", "body": "Researchers are gaining insight into the causes of a devastating form of muscle wasting that is often the final stage of cancer and other diseases. As a palliative-care researcher, Susan McClement has talked to many people dying of cancer and their families \u2014 and some of their stories are burned into her brain. One man was so concerned by the sight of his emaciated wife, whose body had been ravaged by metastatic breast cancer, that he resorted to force feeding her \u2014 pinching her nose and slipping in a spoonful of food when she opened her mouth. Convinced that food would give her the energy to fight the cancer, his daily visits became protracted battles. She died a few weeks later. McClement, who works at the University of Manitoba in Winnipeg, Canada, says that nutritional conflicts can become a source of regret for relatives. \u201cThey said, \u2018You know, if I could do it over again, I would have spent much less time fighting about tapioca pudding and much more time telling my wife that I loved her.\u2019\u201d Adam Levy investigates the challenges in treating the wasting condition, cachexia The woman in this case had cachexia, a metabolic disorder that affects some 9\u00a0million people worldwide, including as many as 80% of people with advanced cancer. It typically involves extreme weight- and muscle-loss, makes routine activities difficult and increases the risk of deadly complications such as infections. Adding calories doesn\u2019t reverse cachexia, and McClement says that the disorder sometimes provokes extreme reactions from family members because it serves as visual confirmation of their worst fears. \u201cIt\u2019s a constant reminder that the person is sick and is not going to get better,\u201d says McClement. Cachexia is seen in the late stages of almost every major chronic illness, affecting 16\u201342% of people with heart failure, 30% of those with chronic obstructive pulmonary disease and up to 60% of people with kidney disease. But for many years it was overlooked, as physicians and researchers focused their attention on the primary illness instead. Now, scientists are increasingly viewing cachexia as a distinct, treatable condition. Basic research has revealed how it is driven by inflammation and metabolic imbalances, and has generated drug targets, says Stefan Anker, a cardiologist and cachexia specialist at the University Medical Center G\u00f6ttingen in Germany. \u201cNow we have quite a number of powerful options to test,\u201d he says. This has spurred investment from drug developers who aim to reduce suffering, and possibly give patients the strength to withstand chemotherapy or surgery. But some high-profile clinical trials in the past two years have produced disappointing results, prompting much self-reflection in the young field. \u201cI\u2019m a little bit worried that if we don\u2019t see a successful clinical trial in the next five years, the dollars from the pharmaceutical industry to develop a treatment will go somewhere else,\u201d says Jose Garcia, a clinical researcher focused on wasting disorders at the Michael E.\u00a0DeBakey Veterans Affairs Medical Center in Houston, Texas. \u201cIn my view, that would be a missed opportunity.\u201d \n               Wasted energy \n             The term cachexia is derived from the Greek  kakos  and  hexis , meaning \u2018bad condition\u2019. It is thought that Hippocrates recognized the syndrome \u2014 but it took until 2006 for the cachexia field to start working up a formal definition, which includes a loss of 5% or more of body weight over 12 months, and reduced muscle strength. In the clinic, it remains under-recognized by oncologists, says Egidio Del Fabbro, a palliative-care physician and researcher at Virginia Commonwealth University in Richmond. There are no standard guidelines for treatment. In the past decade, researchers have made strides in learning about the causes of cachexia, thanks to funding from the US National Cancer Institute and some advocacy groups. New international conferences (including one that wrapped up  this week in Paris ) and the launch of a research journal \u2014 the  Journal of Cachexia, Sarcopenia and Muscle  \u2014 have also drummed up interest in the field. It is now clear that a key mechanism underlying cachexia is the increased breakdown of muscle protein, along with dampened protein synthesis, which leads to overall muscle loss. Studies in 2001 helped to jump-start the field when they identified genes that were more active in atrophying rodent muscles than in normal ones 1 , 2 . These genes encode enzymes called E3 ubiquitin ligases, which tag proteins for destruction in the cell. Mice without these enzymes were resistant to muscle loss. Muscle cells seem to make more of these ligases when hit with certain inflammatory signals from tumours or from immune cells responding to cancer or other illness. Abnormalities in apoptosis (programmed cell death) and in the muscle cell\u2019s  energy-producing organelles, mitochondria , have also been implicated. Several drug-makers have homed in on the protein myostatin, which blocks muscle growth. In a 2010 paper 3  that got many people excited about a possible cachexia drug, researchers from biotechnology company Amgen in Thousand Oaks, California, showed that they could  reverse muscle loss and extend the lives of mice  with tumours and cachexia by blocking signalling through the myostatin pathway. Research since then suggests that cachexia is more than a muscle disease. Studies 4  have identified problems in the brain\u2019s regulation of appetite and feeding, and even ways in which the liver might be contributing to the energy imbalance that sees the body burn its own tissue to sustain itself. Others have looked at fat tissue, which can also waste away in cachexia. They showed that inflammation 5  and molecules made by tumours 6  cause  white fat cells to turn into brown fat cells , which burn more energy to generate heat than white fat cells. The question that researchers are now tackling is how tissues and organs \u2014 muscle, brain, fat, even bone \u2014 are communicating with one another. A paper published last week 7  suggests that fat signalling could be involved in muscle atrophy. All this research has brought more representatives of biotechnology and pharmaceutical companies to cachexia meetings in recent years, says Denis Guttridge, a cell biologist at the Ohio State University in Columbus, who organizes one such conference. \u201cThat\u2019s exciting for a basic scientist like myself,\u201d he says. \u201cI can see the increase in the translational pipeline.\u201d \n               Drug disappointment \n             Despite the excitement in labs, clinical research has so far proved disappointing. In 2011, biotech firm GTx of Memphis, Tennessee, launched two late-stage clinical trials of enobosarm, a molecule that binds to the same receptor as testosterone but only in muscle and bone, mimicking the hormone\u2019s ability to stimulate muscle build-up but without its undesirable side effects. Results from earlier, smaller trials looked promising: people taking the drug had increased lean body mass and improved physical function, as measured by their speed at climbing stairs 8 . But in the larger tests of the drug, on people with advanced lung cancer, the benefits in function disappeared. The firm has since abandoned muscle wasting, and is instead testing larger doses of enobosarm to treat breast cancer. A pair of unpublished studies on people with lung cancer and cachexia tested a compound called anamorelin, which mimics ghrelin, an appetite-stimulating peptide hormone produced mainly by the stomach. The trials were sponsored by pharmaceutical company Helsinn in Lugano, Switzerland, which reported that participants in the treatment group put on weight and muscle mass compared with those taking a placebo, but showed no difference in hand grip strength. Still, the company announced last week that the European Medicines Agency is reviewing its drug for approval. There is a lot of debate about why the trials failed to show functional improvements. Some researchers say that the teams did not use the most clinically relevant measures of muscle function. \u201cWe don\u2019t really know what is the best test for this,\u201d says Garcia. \u201cIf you can climb up a set of stairs one second faster, what does that mean?\u201d This confusion about trial design is a problem for the field, says Anker. \u201cWe need to reach consensus on endpoints and what to aim for in our treatments.\u201d Another problem is that animal data on cachexia may not translate into humans. Some work has tried to make a case that the mechanisms found in rodents might be similar to those in humans, by looking at human tissue samples, says Vickie Baracos, a clinical translational researcher in muscle wasting at the University of Alberta in Edmonton, Canada. \u201cBut held up to scrutiny, this clinical evidence is often rather sketchy.\u201d Researchers in the field lament the dearth of human data and clinical samples. Baracos says that studies are needed that follow people with cachexia over time, collecting blood and muscle samples along the way. \u201cA cachexia data repository with a biobank would sure be a great thing,\u201d she says. Perhaps the biggest challenge is that the field has to compete for funding and recognition with research into other major diseases, says Anker. \u201cCachexia is competing for internal resources within big companies, fighting with cancer, cardiology,\u201d he says. Few companies have dedicated cachexia groups or departments. GTx stopped its work on muscle wasting in part because insurers did not seem interested in covering a medication that was only going to target cachexia and not cancer, says Mary Ann Johnston, the company\u2019s vice-president for clinical development. \u201cThere\u2019s a lack of interest in supportive care.\u201d But an effective treatment would be transformative, says Garcia. It might spur physicians to talk more to patients and their families about the troubling symptoms of cachexia. Without the tools to treat the syndrome, many doctors don\u2019t address it, he says. And that vacuum of information can be distressing. McClement, for her part, has been interviewing more families of people with cachexia. She hopes to find ways to better inform them about the condition and help them to cope. Given the absence of pharmacological interventions, such psychosocial ones are important, she says. \u201cThat\u2019s all we\u2019ve got.\u201d \n                 Tweet \n                 Follow @NatureNews \n               \n                     The cannabis experiment 2015-Aug-19 \n                   \n                     Drug flexes muscle against cancer 2010-Aug-19 \n                   \n                     Boosting 'good' fat to burn off the bad 2008-Aug-20 \n                   Reprints and Permissions"},
{"file_id": "528026a", "url": "https://www.nature.com/articles/528026a", "year": 2015, "authors": [{"name": "Elizabeth Gibney"}], "parsed_as_year": "2006_or_before", "body": "Researchers want to wire the human body with sensors that could harvest reams of data \u2014 and transform health care. G\u00f6ran Gustafsson looks at people and thinks of cars \u2014 the ageing models that rolled off assembly lines a few decades ago. Today, says Gustafsson, cars are packed with cutting-edge sensors, computers and sophisticated communications systems that warn of problems when they are still easy to fix, which is why modern vehicles rarely surprise their drivers with catastrophic breakdowns. \u201cWhy don't we have a similar vision for our bodies?\u201d wonders Gustafsson, an engineer whose team at the Swedish electronics company Acreo, based in Kista, is one of many around the world trying to make such a vision possible. Instead of letting health problems go undetected until a person ends up in hospital \u2014 the medical equivalent of a roadside breakdown \u2014 these teams foresee a future in which humans are wired up like cars, with sensors that form a similar early-warning system. Working with researchers at Link\u00f6ping University in Sweden, Gustafsson's team has developed skin-surface and implanted sensors, as well as an in-body intranet that can link devices while keeping them private. Other groups are developing technologies ranging from skin patches that sense arterial stiffening \u2014 a signal of a looming heart attack \u2014 to devices that detect epileptic fits and automatically deliver drugs directly to affected areas of the brain. These next-generation devices are designed to function alongside tissue, rather than be isolated from it like most pacemakers and other electronic devices already used in the body. But making this integration work is no easy feat, especially for materials scientists, who must shrink circuits radically, make flexible and stretchable electronics that are imperceptible to tissue, and find innovative ways to create interfaces with the body. Achieving Gustafsson's vision \u2014 in which devices monitor and treat the body day in, day out \u2014 will also require both new power sources and new ways of transmitting information. Still, the potential to improve health care substantially while reducing its costs has drawn both researchers and physicians to the challenge, says John Rogers, a materials scientist at the University of Illinois at Urbana\u2013Champaign. \u201cI haven't found any clinical folks who say 'That's pie in the sky, come back to me in 20 years,'\u201d he says. \u201cThey say, 'Wow, that's cool. Here are three ways we can use it today, and how do we get started on a collaboration?'.\u201d Sensors woven into the body are a  natural extension of handheld smartphones and wearable devices , says Rogers. \u201cI think electronics is coming at you,\u201d he says. \u201cIt's migrating closer and closer and I think it's a very natural thing to imagine that they will eventually become intimately integrated with the body.\u201d \n               Skin deep \n             The first step beyond wearables will be wireless sensors mounted directly on the skin, where they can pick up a host of vital signs, including temperature, pulse and breathing rate. Unfortunately, says Rogers, \u201cbiology involves bending, stretching and swelling\u201d, which makes conventional electronics built from stiff silicon wafers a very poor choice for such sensors. His team has developed 'epidermal electronics': flexible, biodegradable stick-on patches that are crammed with sensors but almost imperceptible to the user. Attached like temporary tattoos, the patches use normal silicon electronics, but thinned down and transferred to a flexible backing using a rubber stamp 1 . The patches draw power either from nearby magnetic fields or by harvesting radio waves, using S-shaped wires and antennas designed to stretch, twist and bend. \u201cThey adopt a wavy kind of geometry, so when you stretch, the wave shapes can change, like accordion bellows,\u201d says Rogers. Rogers has co-founded a spin-off company \u2014 MC10, based in Lexington, Massachusetts \u2014 that next year will start marketing versions of the device as BioStamps: temporary patches that measure heart electrical activity, hydration, body temperature and exposure to ultraviolet light. The patches will be available to consumers first, says Rogers, but his real target is medicine. Results are expected soon from a trial at the neonatal intensive-care unit at Carle Foundation Hospital in Urbana, where doctors are using the patches to monitor the vital signs of newborn babies without the need for intrusive cables and scanners. MC10 is also collaborating with Brussels-based pharmaceutical company UCB on tests of a patch that monitors tremors in people with Parkinson's disease, to track their illness and whether they are taking their medication. Rogers' patches are relatively small, but at the University of Tokyo, engineer Takao Someya has created a sensor-laden electronic skin that can be made in much larger pieces 2 . His latest film is just 1 micrometre thick, and so light that it floats like a feather, yet it is robust enough to cope with the stretching and crumpling needed to flex with an elbow or knee. It can provide readouts on temperature \u2014 heat in a wound can signal infection \u2014 moisture, pulse and oxygen concentration in the blood. Someya achieves this by ditching silicon altogether, and instead using inherently soft organic components made of carbon-based polymers and other materials. Organic circuits can be printed onto a plastic film, making them cheap and easy to produce in large quantities. And they are versatile: they work in both high-temperature and water-based environments. Skin also inspires Zhenan Bao, an engineer at Stanford University in California. Her team creates thin pressure sensors by sandwiching micrometre-scale rubber pyramids between films 3 . Even a slight touch will compress the pyramids' tips, changing how electric current flows between the films. The sensors can be used in heart monitors that track how fast pressure waves pass through arteries. This can reveal increased stiffness in the vessels \u2014 a predictor of heart attacks. Last year, the US Food and Drug Administration approved a wireless pressure sensor that can be implanted inside the hearts of people with advanced heart disease; Bao's device could do a similar job from the surface of the skin. As useful as skin-mounted patches might be, much more information is available deeper in the body. \u201cThere's a reason why at the hospital, they draw your blood,\u201d says Michael Strano, a chemical engineer at the Massachusetts Institute of Technology (MIT) in Cambridge. \u201cThere are markers in blood that are exquisitely good at predicting disease.\u201d But delving deeper brings fresh challenges. Ideally, says Strano, sensors under the skin should be not only non-toxic, but also stable enough to function inside the body for years at a time if need be, and biocompatible \u2014 meaning that they don't trigger the body's immune response. Yet most current devices fall short on one score or another. For example, sensors that detect chemical signals in the blood called biomarkers often use biological materials that degrade very quickly. This is a severe limitation for the advanced, real-time sensors that are currently used to monitor glucose in people with diabetes, says Strano: the devices detect glucose with an enzyme reaction that produces hydrogen peroxide. This degrades the sensors so quickly that they must be replaced within weeks. To get around that, Strano's lab has developed synthetic,  long-lived detector materials  that can be mixed with a water-based gel and injected under the skin like a tattoo. The 'ink' for this tattoo consists of carbon nanotubes coated with dangling polymer strands, which have a lock-and-key chemical structure that recognizes biomarkers by dictating which molecules can dock with them 4 . When biomarkers bind to the polymer, they subtly change the optical properties of the nanotube: shine a light on the tattoo, and a glow reveals the presence of the biomarker. Strano and his team have developed carbon-nanotube sensors to monitor nitric oxide in blood 5  \u2014 an inflammatory marker that can indicate infection or even cancer \u2014 and are working on glucose and cortisol, a stress biomarker that may prove useful for monitoring post-traumatic stress disorder and anxiety disorders. The nitric oxide sensor worked for 400 days in mice, which to Strano's knowledge is the longest any implanted chemical sensor has been in place, and did so without provoking any immune response. For many other kinds of device, the jury is still out. \u201cFor electronic materials, especially plastic-based and organics, it's still unknown what their long-term effects are,\u201d says Bao. Now Strano is starting work with MIT engineer Daniel Anderson on devices that could combine sensors with drug-delivery systems. They hope to adapt microchips pioneered by fellow MIT engineer  Robert Langer  to respond to a range of triggers by releasing the appropriate drugs, encased in polymer capsules. The first human trial of a drug-delivering 'pharmacy on a chip' \u2014 without the sensors \u2014 was in 2012, in eight women with osteoporosis 6 . It will be a long time before such devices can be used to detect diseases reliably and treat them automatically, except perhaps for diabetes, which has been extensively studied. Strano's devices are good at binding only with their target molecules, but big questions remain about what fluctuations in biomarker signals actually mean in terms of health, he says. His team is modelling biomarkers in the body, to help to decide where the sensor needs to be and how quickly it should to react to give useful information. \u201cOften you need to rely on many different sensory parameters to make a decision. It's not enough that one chemical is over-expressed,\u201d says Magnus Berggren, an electronic engineer at Link\u00f6ping University who is collaborating with Gustafsson. \n               Moving target \n             Some researchers' targets lie still deeper in the body, and for them, flexibility and biocompatibility are even more important. If a rigid sensor rubs against a moving organ such as the heart or the brain, in which the cells shift slight as the animal breathes, the body will quickly surround it with a wall of scar tissue. And if sensors move relative to the organ, the results will be unreliable in any case. Bioelectronics engineer George Malliaras at the \u00c9cole Nationale Sup\u00e9rieure des Mines de Saint-\u00c9tienne in Gardanne, France, and his colleagues are among those developing flexible replacements for the relatively rigid sensors currently used to track distinctive electrical patterns in the brains of people with epilepsy or Parkinson's disease. Made of organic, conducting polymers, these flexible electronics respond to chemical signals \u2014 the flow of ions that generates the electrical patterns. This not only increases sensitivity, but also lets researchers \u201cinterface with biology in a wholly different fashion\u201d, he says. The team's latest device, tested in rats as well as in two humans undergoing surgery for epilepsy 7 , has detected the firing of individual neurons, says Malliaras. And if the process is reversed, he adds, the sensor can be used to deliver drugs. Devices known as organic electronic ion pumps respond to an applied voltage by forcing drugs \u2014 small charged particles \u2014 out of a reservoir. Working with the group at Link\u00f6ping University and the French National Institute of Health and Medical Research in Marseilles, Malliaras's team is coupling his epilepsy sensor to an ion pump that responds to seizures by releasing epilepsy drugs into the correct part of the brain 8 . Berggren and the Link\u00f6ping team have used a similar technique to develop a 'pacemaker for pain' that delivers analgesics directly to the spinal cord 9 . \n               Keep it going \n             Any electrical device is limited by its need for power. Devices that sit on or near the skin can incorporate antennas that harvest power wirelessly \u2014 as long as an external source is nearby. But sensors deeper in the body often have to rely on batteries, which are bulky and need replacing. And some, such as Berggren's pain-relief pump, need to have wires threaded through the overlying tissues \u2014 an arrangement that is both cumbersome and a potential route for infection (see 'Wired for life'). To get around such problems, Zhong Lin Wang, a nanoscientist at the Georgia Institute of Technology in Atlanta, has spent the past decade trying to harvest the tiny amounts of mechanical energy generated when people walk or even breathe. \u201cWe started thinking, how do we convert body motion into electricity?\u201d he says. His latest design uses static electricity \u2014 long thought of as a nuisance \u2014 to convert the movement of inhaling and exhaling into enough energy to power a pacemaker 10 . The generator uses two different polymer surfaces, sandwiched between electrodes and connected in a circuit. When the user breathes in and out, the surfaces touch and separate, swapping electrons \u2014 the same thing that happens when a balloon is stroked with a wool cloth. The build-up of charge causes current to flow through the wire. \u201cInhale and exhale, move back and forth or drive up and down and you generate power,\u201d says Wang. Starting in 2014, Wang began testing the system in rats, creating milliwatts of energy from a device the thickness of a few sheets of paper. Now his team is testing the same technology in pigs. Rogers' team has created 11  a  biodegradable battery  using electrodes made of magnesium and other metals that are safe in low concentrations and that slowly dissolve in the body. \u201cSome devices you want to last the life of the patient. In others, you only need and want the device to be  temporary ,\u201d says Rogers. \n               Personal privacy \n             The technology could be revolutionary, but the vision of a wired-up body that sends data to an outside computer or medical centre faces a threat that already troubles the wearables industry:  hacking . \u201cWhen a semiconductor chip is introduced inside the body, hacking is a truly serious issue,\u201d says Someya. One solution is to analyse data on the device itself, reducing the amount that gets sent over the airwaves. Another is to avoid the airwaves altogether. In as-yet-unpublished work, the Swedish team has developed an in-body intranet that transmits signals at low frequency using the body's water as its wires. To send information between devices, or from a device to a smartphone, users must physically touch the objects with their hands. This keeps the signals low-power and private, and avoids clogging up the data-transmitting frequencies that are already squabbled over by mobile phones and wireless routers. \u201cIt's only transmitted and exposed within your body,\u201d adds Berggren, who says that the system can already exchange data between electronically labelled objects through the body to a smartphone, and will soon integrate on-skin sensors. However good the devices, pioneers of new materials will also struggle against a tide of medical regulation, says Malliaras. That, along with the concerns of chemical suppliers who are afraid that failing devices could leave them vulnerable to lawsuits, \u201cputs a big brake on the adoption of new materials\u201d, he says. Berggren and his collaborators at Acreo are among the first to try to connect a range of devices by wiring up humans. But they readily acknowledge that making the vision a reality will require multiple companies and research teams, as well as the involvement of insurance companies and health-care providers. Berggren knows that there are big hurdles. \u201cThe challenge is to put everything together,\u201d he says. \u201cBut they did it for the car industry and it's impressive. You rarely see cars standing along the side of the road waiting for repair. Whether it's possible to do this also for humans is still a question mark, but it's definitely worth trying.\u201d Malliaras agrees. \u201cA car you usually keep for less than ten years,\u201d he says. \u201cA body you want to keep for 80 or 90 years; it's a lot more precious.\u201d \n                 Tweet \n                 Follow @NatureNews \n               \n                     What could derail the wearables revolution? 2015-Sep-01 \n                   \n                     Nanotube implants show diagnostic potential 2015-Aug-21 \n                   \n                     Electroceuticals spark interest 2014-Jul-02 \n                   \n                     Biodegradable battery could melt inside the body 2014-Mar-24 \n                   \n                     Biodegradable electronics here today, gone tomorrow 2012-Sep-27 \n                   Reprints and Permissions"},
{"file_id": "528322a", "url": "https://www.nature.com/articles/528322a", "year": 2015, "authors": [{"name": "Megan Scudellari"}], "parsed_as_year": "2006_or_before", "body": "False beliefs and wishful thinking about the human experience are common. They are hurting people \u2014 and holding back science. In 1997, physicians in southwest Korea began to offer ultrasound screening for early detection of thyroid cancer. News of the programme spread, and soon physicians around the region began to offer the service. Eventually it went nationwide, piggybacking on a government initiative to screen for other cancers. Hundreds of thousands took the test for just US$30\u201350. James Harkin, a researcher for the British TV trivia show QI, talks to Adam Levy about how he finds facts and myths for the show \u2014 and then runs a mini-quiz to see whether the Podcast team can discern science fact from science fiction Across the country, detection of thyroid cancer soared, from 5 cases per 100,000 people in 1999 to 70 per 100,000 in 2011. Two-thirds of those diagnosed had their thyroid glands removed and were placed on lifelong drug regimens, both of which carry risks. Such a costly and extensive public-health programme might be expected to save lives. But this one did not. Thyroid cancer is now the most common type of cancer diagnosed in South Korea, but the number of people who die from it has remained exactly the same \u2014 about 1 per 100,000. Even when some physicians in Korea realized this, and suggested that thyroid screening be stopped in 2014, the Korean Thyroid Association, a professional society of endocrinologists and thyroid surgeons, argued that screening and treatment were basic human rights. In Korea, as elsewhere, the idea that the early detection of any cancer saves lives had become an unshakeable belief. This blind faith in cancer screening is an example of how ideas about human biology and behaviour can persist among people \u2014 including scientists \u2014 even though the scientific evidence shows the concepts to be false. \u201cScientists think they're too objective to believe in something as folklore-ish as a myth,\u201d says Nicholas Spitzer, director of the Kavli Institute for Brain and Mind at the University of California, San Diego. Yet they do. These myths often blossom from a seed of a fact \u2014 early detection does save lives for some cancers \u2014 and thrive on human desires or anxieties, such as a fear of death. But they can do harm by, for instance, driving people to pursue unnecessary treatment or spend money on unproven products. They can also derail or forestall promising research by distracting scientists or monopolizing funding. And dispelling them is tricky. Scientists should work to discredit myths, but they also have a responsibility to try to prevent new ones from arising, says Paul Howard-Jones, who studies neuroscience and education at the University of Bristol, UK. \u201cWe need to look deeper to understand how they come about in the first place and why they're so prevalent and persistent.\u201d Some dangerous myths get plenty of air time: vaccines cause autism, HIV doesn't cause AIDS. But many others swirl about, too, harming people, sucking up money, muddying the scientific enterprise \u2014 or simply getting on scientists' nerves. Here,  Nature  looks at the origins and repercussions of five myths that refuse to die. \n               Myth 1: Screening saves lives for all types of cancer \n             Regular screening might be beneficial for some groups at risk of certain cancers, such as lung, cervical and colon, but this isn't the case for all tests. Still, some patients and clinicians defend the ineffective ones fiercely. The belief that early detection saves lives originated in the early twentieth century, when doctors realized that they got the best outcomes when tumours were identified and treated just after the onset of symptoms. The next logical leap was to assume that the earlier a tumour was found, the better the chance of survival. \u201cWe've all been taught, since we were at our mother's knee, the way to deal with cancer is to find it early and cut it out,\u201d says Otis Brawley, chief medical officer for the American Cancer Society. But evidence from large randomized trials for cancers such as thyroid, prostate and breast has shown that early screening is not the lifesaver it is often advertised as. For example, a Cochrane review of five randomized controlled clinical trials totalling 341,342 participants found that screening did not significantly decrease deaths due to prostate cancer 1 . \u201cPeople seem to imagine the mere fact that you found a cancer so-called early must be a benefit. But that isn't so at all,\u201d says Anthony Miller at the University of Toronto in Canada. Miller headed the Canadian National Breast Screening Study, a 25-year study of 89,835 women aged 40\u201359 years old 2  that found that annual mammograms did not reduce mortality from breast cancer. That's because some tumours will lead to death irrespective of when they are detected and treated. Meanwhile, aggressive early screening has a slew of negative health effects. Many cancers grow slowly and will do no harm if left alone, so people end up having unnecessary thyroidectomies, mastectomies and prostatectomies. So on a population level, the benefits (lives saved) do not outweigh the risks (lives lost or interrupted by unnecessary treatment). Still, individuals who have had a cancer detected and then removed are likely to feel that their life was saved, and these personal experiences help to keep the misconception alive. And oncologists routinely debate what ages and other risk factors would benefit from regular screening. Focusing so much attention on the current screening tests comes at a cost for cancer research, says Brawley. \u201cIn breast cancer, we've spent so much time arguing about age 40 versus age 50 and not about the fact that we need a better test,\u201d such as  one that could detect fast-growing rather than slow-growing tumours . And existing diagnostics should be rigorously tested to prove that they actually save lives, says epidemiologist John Ioannidis of the Stanford Prevention Research Center in California, who this year reported that very few screening tests for 19 major diseases actually reduced mortality 3 . Changing behaviours will be tough. Gilbert Welch at the Dartmouth Institute for Health Policy and Clinical Practice in Lebanon, New Hampshire, says that individuals would rather be told to get a quick test every few years than be told to eat well and exercise to prevent cancer. \u201cScreening has become an easy way for both doctor and patient to think they are doing something good for their health, but their risk of cancer hasn't changed at all.\u201d \n               Myth 2: Antioxidants are good and free radicals are bad \n             In December 1945, chemist Denham Harman's wife suggested that he read an article in  Ladies' Home Journal  entitled 'Tomorrow You May Be Younger'. It sparked his interest in ageing, and years later, as a research associate at the University of California, Berkeley, Harman had a thought \u201cout of the blue\u201d, as he later recalled. Ageing, he proposed, is caused by free radicals, reactive molecules that build up in the body as by-products of metabolism and lead to cellular damage. Scientists rallied around the free-radical theory of ageing, including the corollary that antioxidants, molecules that neutralize free radicals, are good for human health. By the 1990s, many people were taking antioxidant supplements, such as  vitamin C  and \u03b2-carotene. It is \u201cone of the few scientific theories to have reached the public: gravity, relativity and that free radicals cause ageing, so one needs to have antioxidants\u201d, says Siegfried Hekimi, a biologist at McGill University in Montreal, Canada. Yet in the early 2000s, scientists trying to build on the theory encountered bewildering results: mice genetically engineered to overproduce free radicals lived just as long as normal mice 4 , and those engineered to overproduce antioxidants didn't live any longer than normal 5 . It was the first of an onslaught of negative data, which initially proved difficult to publish. The free-radical theory \u201cwas like some sort of creature we were trying to kill. We kept firing bullets into it, and it just wouldn't die,\u201d says David Gems at University College London, who started to publish his own negative results in 2003 (ref.  6 ). Then, one study in humans 7  showed that antioxidant supplements prevent the health-promoting effects of exercise, and another associated them with higher mortality 8 . None of those results has slowed the global antioxidant market, which ranges from food and beverages to livestock feed additives. It is projected to grow from US$2.1 billion in 2013 to $3.1 billion in 2020. \u201cIt's a massive racket,\u201d says Gems. \u201cThe reason the notion of oxidation and ageing hangs around is because it is perpetuated by people making money out of it.\u201d Today, most researchers working on ageing agree that free radicals can cause cellular damage, but that this seems to be a normal part of the body's reaction to stress. Still, the field has wasted time and resources as a result. And the idea still holds back publications on possible benefits of free radicals, says Michael Ristow, a metabolism researcher at the Swiss Federal Institute of Technology in Zurich, Switzerland. \u201cThere is a significant body of evidence sitting in drawers and hard drives that supports this concept, but people aren't putting it out,\u201d he says. \u201cIt's still a major problem.\u201d Some researchers also question the broader assumption that molecular damage of any kind causes ageing. \u201cThere's a question mark about whether really the whole thing should be chucked out,\u201d says Gems. The trouble, he says, is that \u201cpeople don't know where to go now\u201d. \n               Myth 3: Humans have exceptionally large brains \n             The human brain \u2014 with its remarkable cognition \u2014 is often considered to be the pinnacle of brain evolution. That dominance is often attributed to the brain's exceptionally large size in comparison to the body, as well as its density of neurons and supporting cells, called glia. None of that, however, is true. \u201cWe cherry-pick the numbers that put us on top,\u201d says Lori Marino, a neuroscientist at Emory University in Atlanta, Georgia. Human brains are about seven times larger than one might expect relative to similarly sized animals. But mice and dolphins have about the same proportions, and some birds have a larger ratio. \u201cHuman brains respect the rules of scaling. We have a scaled-up primate brain,\u201d says Chet Sherwood, a biological anthropologist at George Washington University in Washington DC. Even cell counts have been inflated: articles, reviews and textbooks often state that the human brain has 100 billion neurons. More accurate measures suggest that the number is closer to 86 billion. That may sound like a rounding error, but 14 billion neurons is roughly the equivalent of two macaque brains. Human brains are different from those of other primates in other ways:  Homo sapiens  evolved an expanded cerebral cortex \u2014 the part of the brain involved in functions such as thought and language \u2014 and unique changes in neural structure and function in other areas of the brain. The myth that our brains are unique because of an exceptional number of neurons has done a disservice to neuroscience because other possible differences are rarely investigated, says Sherwood, pointing to the examples of energy metabolism, rates of brain-cell development and long-range connectivity of neurons. \u201cThese are all places where you can find human differences, and they seem to be relatively unconnected to total numbers of neurons,\u201d he says. The field is starting to  explore these topics . Projects such as the US National Institutes of Health's Human Connectome Project and the Swiss Federal Institute of Technology in Lausanne's Blue Brain Project are now working to understand brain function through wiring patterns rather than size. \n               Myth 4: Individuals learn best when taught in their preferred learning style \n             People attribute other mythical qualities to their unexceptionally large brains. One such myth is that individuals learn best when they are taught in the way they prefer to learn. A verbal learner, for example, supposedly learns best through oral instructions, whereas a visual learner absorbs information most effectively through graphics and other diagrams. There are two truths at the core of this myth: many people have a preference for how they receive information, and evidence suggests that teachers achieve the best educational outcomes when they present information in multiple sensory modes. Couple that with people's desire to learn and be considered unique, and conditions are ripe for myth-making. \u201cLearning styles has got it all going for it: a seed of fact, emotional biases and wishful thinking,\u201d says Howard-Jones. Yet just like sugar, pornography and television, \u201cwhat you prefer is not always good for you or right for you,\u201d says Paul Kirschner, an educational psychologist at the Open University of the Netherlands. In 2008, four cognitive neuroscientists reviewed the scientific evidence for and against learning styles. Only a few studies had rigorously put the ideas to the test and most of those that did showed that teaching in a person's preferred style had no beneficial effect on his or her learning. \u201cThe contrast between the enormous popularity of the learning-styles approach within education and the lack of credible evidence for its utility is, in our opinion, striking and disturbing,\u201d the authors of one study wrote 9 . That hasn't stopped a lucrative industry from pumping out books and tests for some 71 proposed learning styles. Scientists, too, perpetuate the myth, citing learning styles in more than 360 papers during the past 5 years. \u201cThere are groups of researchers who still adhere to the idea, especially folks who developed questionnaires and surveys for categorizing people. They have a strong vested interest,\u201d says Richard Mayer, an educational psychologist at the University of California, Santa Barbara. In the past few decades, research into educational techniques has started to show that there are  interventions that do improve learning , including getting students to summarize or explain concepts to themselves. And it seems almost all individuals, barring those with learning disabilities, learn best from a mixture of words and graphics, rather than either alone. Yet the learning-styles myth makes it difficult to get these evidence-backed concepts into classrooms. When Howard-Jones speaks to teachers to dispel the learning-styles myth, for example, they often don't like to hear what he has to say. \u201cThey have disillusioned faces. Teachers invested hope, time and effort in these ideas,\u201d he says. \u201cAfter that, they lose interest in the idea that science can support learning and teaching.\u201d \n               Myth 5: The human population is growing exponentially (and we're doomed) \n             Fears about overpopulation began with Reverend Thomas Malthus in 1798, who predicted that unchecked exponential population growth would lead to famine and poverty. But the human population has not and  is not growing exponentially  and is unlikely to do so, says Joel Cohen, a populations researcher at the Rockefeller University in New York City. The world\u2019s population is now growing at just half the rate it was before 1965. Today there are an estimated 7.3 billion people, and that is projected to reach 9.7 billion by 2050. Yet beliefs that the rate of population growth will lead to some doomsday scenario have been continually perpetuated. Celebrated physicist Albert Bartlett, for example, gave more than 1,742 lectures on exponential human population growth and the dire consequences starting in 1969. The world's population also has enough to eat. According to the Food and Agriculture Organization of the United Nations, the rate of global food production outstrips the growth of the population. People grow enough calories in cereals alone to feed between 10 billion and 12 billion people. Yet hunger and malnutrition persist worldwide. This is because about 55% of the food grown is divided between feeding cattle, making fuel and other materials or going to waste, says Cohen. And what remains is not evenly distributed \u2014 the rich have plenty, the poor have little. Likewise,  water is not scarce on a global scale , even though 1.2 billion people live in areas where it is. \u201cOverpopulation is really not overpopulation. It's a question about poverty,\u201d says Nicholas Eberstadt, a demographer at the American Enterprise Institute, a conservative think tank based in Washington DC. Yet instead of examining why poverty exists and how to sustainably support a growing population, he says, social scientists and biologists talk past each other, debating definitions and causes of overpopulation. Cohen adds that \u201ceven people who know the facts use it as an excuse not to pay attention to the problems we have right now\u201d, pointing to the example of economic systems that favour the wealthy. Like others interviewed for this article, Cohen is less than optimistic about the chances of dispelling the idea of overpopulation and other ubiquitous myths (see \u2018Myths that persist\u2019), but he agrees that it is worthwhile to try to prevent future misconceptions. Many myths have emerged after one researcher extrapolated beyond the narrow conclusions of another's work, as was the case for free radicals. That \u201cinterpretation creep\u201d, as Spitzer calls it, can lead to misconceptions that are hard to excise. To prevent that, \u201cwe can make sure an extrapolation is justified, that we're not going beyond the data\u201d, suggests Spitzer. Beyond that, it comes down to communication, says Howard-Jones. Scientists need to be effective at communicating ideas and get away from simple, boiled-down messages. \n               boxed-text \n             Once a myth is here, it is often here to stay. Psychological studies suggest that the very act of attempting to dispel a myth leads to stronger attachment to it. In one experiment, exposure to pro-vaccination messages reduced parents' intention to vaccinate their children in the United States. In another, correcting misleading claims from politicians increased false beliefs among those who already held them. \u201cMyths are almost impossible to eradicate,\u201d says Kirschner. \u201cThe more you disprove it, often the more hard core it becomes.\u201d \n                 Tweet \n                 Follow @NatureNews \n               \n                     Why we are teaching science wrong, and how to make it right 2015-Jul-15 \n                   \n                     Rethinking the brain 2015-Mar-24 \n                   \n                     World population unlikely to stop growing this century 2014-Sep-18 \n                   \n                     Antioxidants speed cancer in mice 2014-Jan-29 \n                   \n                     Cancer: Missing the mark 2011-Mar-23 \n                   \n                     Language: Disputed definitions 2008-Oct-22 \n                   Reprints and Permissions"},
{"file_id": "526310a", "url": "https://www.nature.com/articles/526310a", "year": 2015, "authors": [{"name": "Heidi Ledford"}], "parsed_as_year": "2006_or_before", "body": "A look at the legal landscape suggests where human genome editing might be used in research or reproduction. They are meeting in China; they are meeting in the United Kingdom; and they met in the United States last week. Around the world, scientists are gathering to discuss the promise and perils of editing the genome of a human embryo. Should it be allowed \u2014 and if so, under what circumstances? The meetings have been prompted by an explosion of interest in the powerful technology known as  CRISPR/Cas9 , which has brought unprecedented ease and precision to genetic engineering. This tool, and others like it, could be used to manipulate the DNA of embryos in a dish to learn about the earliest stages of human development. In theory, genome editing could also be used to 'fix' the mutations responsible for heritable human diseases. If done in embryos, this could prevent such diseases from being passed on. The prospects have prompted  widespread concern  and  discussion  among scientists, ethicists and patients. Fears loom that if genome editing becomes acceptable in the clinic to stave off disease, it will inevitably come to be used to introduce, enhance or eliminate traits for non-medical reasons. Ethicists are concerned that unequal access to such technologies could lead to genetic classism. And targeted changes to a person's genome would be passed on for generations, through the germ line (sperm and eggs), fuelling fears that embryo editing could have lasting, unintended consequences. Adding to these concerns, the regulations in many countries have not kept pace with the science. Nature  has tried to capture a snapshot of the legal landscape by querying experts and government agencies in 12 countries with histories of well-funded biological research. The responses reveal a wide range of approaches. In some countries, experimenting with human embryos at all would be a criminal offence, whereas in others, almost anything would be permissible. Concerns over the manipulation of human embryos are nothing new. Rosario Isasi, a legal scholar at McGill University in Montreal, Canada, points to two key waves of legislation over the years: one sparked by concerns about the derivation of embryonic stem cells, which was largely deemed acceptable; the other about reproductive cloning, which was largely prohibited for safety reasons. The current regulatory mosaic is their legacy. Tetsuya Ishii, a bioethicist at Hokkaido University in Sapporo, Japan, spent nearly a year analysing relevant legislation and guidelines in 39 countries, and found that 29 have rules that could be interpreted as restricting genome editing for clinical use (  M. Araki and T. Ishii  Reprod. Biol. Endocrinol.   12 , 108; 2014). But the 'bans' in several of these countries \u2014 including Japan, China and India \u2014 are not legally binding. \u201cThe truth is, we have guidelines but some people never follow them,\u201d said Qi Zhou, a developmental biologist at the Chinese Academy of Sciences Institute of Zoology in Beijing, at a meeting hosted by the US National Academy of Sciences in Washington DC last week. Ishii considers the rules in nine other countries \u2014 among them Russia and Argentina \u2014 to be \u201cambiguous\u201d. The United States, he notes, prohibits federal funding for research involving human embryos, and would probably require regulatory approval for human gene editing, but does not officially ban the use of the technique in the clinic. In countries where clinical use is banned, such as France and Australia, research is usually allowed as long as it meets certain restrictions and does not attempt to generate a live birth (see 'CRISPR embryos and the law'). Many researchers long for international guidelines that, even if not enforceable, could guide national lawmakers. Developing such a framework is one of the aims of ongoing discussions; the US National Academy, for example, plans to hold an international summit in December and then produce recommendations for responsible use of the technique in 2016. But the research has already begun, and more is coming. Scientists in China announced in April that they had used CRISPR to  alter the genomes of human embryos , albeit ones incapable of producing a live baby (  P. Liang  et al .  Protein Cell   6 , 363\u2013372; 2015). Xiao-Jiang Li, a neuroscientist at Emory University in Atlanta, Georgia, who has used the technique in monkeys, says he has heard rumours that several other Chinese laboratories are already doing such experiments. And in September, developmental biologist Kathy Niakan of the Francis Crick Institute in London applied to the UK Human Fertilisation and Embryology Authority for permission to use the technique to  study errors in embryo development  that can contribute to infertility and miscarriage. No one so far has declared an interest in producing live babies with edited genomes, and initial experiments would suggest that it is not yet safe. But some suspect that it is only a matter of time. Ishii predicts that countries with high rates of  in vitro  fertilization will be the first to attempt clinical applications. Japan, he says, has one of the highest numbers of fertility clinics in the world, and has no enforceable rules on germline modification. The same is true for India. Guoping Feng, a neuroscientist at the Massachusetts Institute of Technology in Cambridge, hopes that with improvement, the technique could eventually be used to prevent genetic disease. But he argues that it is much too soon to be trying it in the clinic. \u201cNow is not the time to do human-embryo manipulation,\u201d he says. \u201cIf we do the wrong thing, we can send the wrong message to the public \u2014 and then the public will not support scientific research anymore.\u201d  \n                 Tweet \n                 Follow @NatureNews \n               \n                     Alternative CRISPR system could improve genome editing 2015-Sep-25 \n                   \n                     UK scientists apply for licence to edit genes in human embryos 2015-Sep-18 \n                   \n                     Biohackers gear up for genome editing 2015-Aug-26 \n                   \n                     CRISPR, the disruptor 2015-Jun-03 \n                   \n                     US science academies take on human-genome editing 2015-May-18 \n                   \n                     Nature  special: CRISPR \n                   Reprints and Permissions"},
{"file_id": "526312a", "url": "https://www.nature.com/articles/526312a", "year": 2015, "authors": [{"name": "Peter Andrey Smith"}], "parsed_as_year": "2006_or_before", "body": "Neuroscientists are probing the idea that intestinal microbiota might influence brain development and behaviour. Nearly a year has passed since Rebecca Knickmeyer first met the participants in her latest study on brain development. Knickmeyer, a neuroscientist at the University of North Carolina School of Medicine in Chapel Hill, expects to see how 30 newborns have grown into crawling, inquisitive one-year-olds, using a battery of behavioural and temperament tests. In one test, a child's mother might disappear from the testing suite and then reappear with a stranger. Another ratchets up the weirdness with some Halloween masks. Then, if all goes well, the kids should nap peacefully as a noisy magnetic resonance imaging machine scans their brains. \u201cWe try to be prepared for everything,\u201d Knickmeyer says. \u201cWe know exactly what to do if kids make a break for the door.\u201d Knickmeyer is excited to see something else from the children \u2014 their faecal microbiota, the array of bacteria, viruses and other microbes that inhabit their guts. Her project (affectionately known as 'the poop study') is part of a  small but growing effort by neuroscientists  to see whether the microbes that colonize the gut in infancy can alter brain development. The project comes at a crucial juncture. A growing body of data, mostly from animals raised in sterile, germ-free conditions, shows that microbes in the gut influence behaviour and can alter brain physiology and neurochemistry. In humans, the data are more limited. Researchers have drawn links between gastrointestinal pathology and psychiatric neurological conditions such as anxiety, depression, autism, schizophrenia and neurodegenerative disorders \u2014 but they are just links. \u201cIn general, the problem of causality in microbiome studies is substantial,\u201d says Rob Knight, a microbiologist at the University of California, San Diego. \u201cIt's very difficult to tell if microbial differences you see associated with diseases are causes or consequences.\u201d There are many outstanding questions. Clues about the mechanisms by which gut bacteria might interact with the brain are starting to emerge, but no one knows how important these processes are in human development and health. That has not prevented some companies in the supplements industry from claiming that probiotics \u2014 bacteria that purportedly aid with digestive issues \u2014 can support emotional well-being. Pharmaceutical firms, hungry for new leads in treating neurological disorders,  are beginning to invest in research  related to gut microbes and the molecules that they produce. Scientists and funders are looking for clarity. Over the past two years, the US National Institute of Mental Health (NIMH) in Bethesda, Maryland, has funded seven pilot studies with up to US$1 million each to examine what it calls the 'microbiome\u2013gut\u2013brain axis' (Knickmeyer's research is one of these studies). This year, the US Office of Naval Research in Arlington, Virginia, agreed to pump around US$14.5 million over the next 6\u20137 years into work examining the gut's role in cognitive function and stress responses. And the European Union has put \u20ac9 million (US$10.1 million) towards a five-year project called MyNewGut, two main objectives of which target brain development and disorders. The latest efforts aim to move beyond basic observations and correlations \u2014 but preliminary results hint at complex answers. Researchers are starting to uncover a vast, varied system in which gut microbes influence the brain through hormones, immune molecules and the specialized metabolites that they produce. \u201cThere's probably more speculation than hard data now,\u201d Knickmeyer says. \u201cSo there's a lot of open questions about the gold standard for methods you should be applying. It's very exploratory.\u201d \n               Gut reactions \n             Microbes and the brain have rarely been thought to interact except in instances when pathogens penetrate the blood\u2013brain barrier \u2014 the cellular fortress protecting the brain against infection and inflammation. When they do, they can have strong effects: the virus that causes rabies elicits aggression, agitation and even a fear of water. But for decades, the vast majority of the body's natural array of microbes was largely uncharacterized, and the idea that it could influence neurobiology was hardly considered mainstream. That is slowly changing. Studies on community outbreaks were one key to illuminating the possible connections. In 2000, a flood in the Canadian town of Walkerton contaminated the town's drinking water with pathogens such as  Escherichia coli  and  Campylobacter jejuni . About 2,300 people suffered from severe gastrointestinal infection, and many of them developed chronic irritable bowel syndrome (IBS) as a direct result. During an eight-year study 1  of Walkerton residents, led by gastroenterologist Stephen Collins at McMaster University in Hamilton, Canada, researchers noticed that psychological issues such as depression and anxiety seemed to be a risk factor for persistent IBS. Premysl Bercik, another McMaster gastroenterologist, says that this interplay triggered intriguing questions. Could psychiatric symptoms be driven by lingering inflammation, or perhaps by a microbiome thrown out of whack by infection? The McMaster group began to look for answers in mice. In a 2011 study 2 , the team transplanted gut microbiota between different strains of mice and showed that behavioural traits specific to one strain transmitted along with the microbiota. Bercik says, for example, that \u201crelatively shy\u201d mice would exhibit more exploratory behaviour when carrying the microbiota of more-adventurous mice. \u201cI think it is surprising. The microbiota is really driving the behavioural phenotype of host. There's a marked difference,\u201d Bercik says. Unpublished research suggests that taking faecal bacteria from humans with both IBS and anxiety and transplanting it into mice induces anxiety-like behaviour, whereas transplanting bacteria from healthy control humans does not. Such results can be met with scepticism. As the field has developed, Knight says, microbiologists have had to learn from behavioural scientists that how animals are handled and caged can affect things such as social hierarchy, stress and even the microbiome. And these experiments and others like them start with a fairly unnatural model: germ-free \u2014 or 'gnotobiotic' \u2014 mice. These animals are delivered by Caesarean section to prevent them from picking up microbes that reside in their mothers' birth canals. They are then raised inside sterile isolators, on autoclaved food and filtered air. The animals are thus detached from many of the communal microbes that their species has evolved with for aeons. In 2011, immunologist Sven Pettersson and neuroscientist Rochellys Diaz Heijtz, both at the Karolinska Institute in Stockholm, showed that in lab tests, germ-free mice demonstrated less-anxious behaviour than mice colonized with natural indigenous microbes 3 . (Less anxiety is not always a good thing, evolutionarily speaking, for a small mammal with many predators.) When the Karolinska team examined the animals' brains, they found that one region in germ-free mice, the striatum, had higher turnover of key neurochemicals that are associated with anxious behaviour, including the neurotransmitter serotonin. The study also showed that introducing adult germ-free mice to conventional, non-sterile environments failed to normalize their behaviour, but the offspring of such 'conventionalized' mice showed some return to normal behaviour, suggesting that there is a critical window during which microbes have their strongest effects. By this time, many researchers were intrigued by the mounting evidence, but results stemmed mostly from fields other than neuroscience. \u201cThe groups working on this are primarily gut folks, with a few psychology-focused people collaborating,\u201d says Melanie Gareau, a physiologist at the University of California, Davis. \u201cSo the findings tended to describe peripheral and behavioural changes rather than changes to the central nervous system.\u201d But Pettersson and Diaz Heijtz's research galvanized the field, suggesting that researchers could get past observational phenomenology and into the mechanisms affecting the brain. Nancy Desmond, a programme officer involved in grant review at the NIMH, says that the paper sparked interest at the funding agency soon after its publication and, in 2013, the NIMH formed a study section devoted to neuroscience research that aims to unravel functional mechanisms and develop drugs or non-invasive treatments for psychological disorders. Judith Eisen, a neuroscientist at the University of Oregon in Eugene, earned a grant to study germ-free zebrafish, whose transparent embryos allow researchers to easily visualize developing brains. \u201cOf course, 'germ-free' is a completely unnatural situation,\u201d Eisen says. \u201cBut it provides the opportunity to learn which microbial functions are important for development of any specific organ or cell type.\u201d \n               Chemical exploration \n             Meanwhile, researchers were starting to uncover ways that bacteria in the gut might be able to get signals through to the brain. Pettersson and others revealed that in adult mice, microbial metabolites influence the basic physiology of the blood\u2013brain barrier 4 . Gut microbes break down complex carbohydrates into short-chain fatty acids with an array of effects: the fatty acid butyrate, for example, fortifies the blood\u2013brain barrier by tightening connections between cells (see 'The gut\u2013brain axis'). Recent studies also demonstrate that gut microbes directly alter neurotransmitter levels, which may enable them to communicate with neurons. For example, Elaine Hsiao, a biologist now at the University of California, Los Angeles, published research 5  this year examining how certain metabolites from gut microbes promote serotonin production in the cells lining the colon \u2014 an intriguing finding given that some antidepressant drugs work by promoting serotonin at the junctions between neurons. These cells account for 60% of peripheral serotonin in mice and more than 90% in humans. Like the Karolinska group, Hsiao found that germ-free mice have significantly less serotonin floating around in their blood, and she also showed that levels could be restored by introducing to their guts spore-forming bacteria (dominated by  Clostridium , which break down short-chain fatty acids). Conversely, mice with natural microbiota, when given antibiotics, had reduced serotonin production. \u201cAt least with those manipulations, it's quite clear there's a cause\u2013effect relationship,\u201d Hsiao says. But it remains unclear whether these altered serotonin levels in the gut trigger a cascade of molecular events, which in turn affect brain activity \u2014 and whether similar events take place in humans, too. \u201cIt will be important to replicate previous findings, and translate these findings into human conditions to really make it to the textbooks,\u201d Hsiao says. For John Cryan, a neuroscientist at University College Cork in Ireland, there is little question that they will. His lab has demonstrated 6  that germ-free mice grow more neurons in a specific brain region as adults than do conventional mice. He has been promoting the gut\u2013brain axis to neuroscientists, psychiatric-drug researchers and the public. \u201cIf you look at the hard neuroscience that has emerged in the last year alone, all the fundamental processes that neuroscientists spend their lives working on are now all shown to be regulated by microbes,\u201d he says, pointing to research on the regulation of the blood\u2013brain barrier, neurogenesis in mice and the activation of microglia, the immune-like cells that reside in the brain and spinal cord. At the 2015 Society for Neuroscience meeting in Chicago, Illinois, this month, Cryan and his colleagues plan to present research showing that  myelination  \u2014 the formation of fatty sheathing that insulates nerve fibres \u2014 can also be influenced by gut microbes, at least in a specific part of the brain. Unrelated work 7  has shown that germ-free mice are protected from an experimentally induced condition similar to multiple sclerosis, which is characterized by demyelination of nerve fibres. At least one company, Symbiotix Biotherapies in Boston, Massachusetts, is already investigating whether a metabolite produced by certain types of gut bacterium might one day be used to stem the damage in humans with multiple sclerosis. \n               A move to therapy \n             Tracy Bale, a neuroscientist at the University of Pennsylvania in Philadelphia, suspects that simple human interventions may already be warranted. Bale heard about Cryan's work on the radio programme  Radiolab  three years ago. At the time, she was researching the placenta, but wondered how microbes might fit into a model of how maternal stress affects offspring. In research published this year 8 , Bale subjected pregnant mice to stressful stimuli. She found that it noticeably reduced the levels of  Lactobacilli  present in the mice's vaginas, which are the main source of the microbes that colonize the guts of offspring. These microbial shifts carried over to pups born vaginally, and Bale detected signs that microbiota might affect neurodevelopment, especially in males. In work that her group plans to present at the  Society for Neuroscience meeting , Bale has shown that by feeding vaginal microbiota from stressed mice to Caesarean-born infant mice, they can recapitulate the neurodevelopmental effects of having a stressed mother. Bale and her colleagues are now wrapping up research investigating whether they can treat mice from stressed mums with the vaginal microbiota of non-stressed mice. The work, Bale says, has \u201cimmediate translational effects\u201d. She points to a project headed by Maria Dominguez-Bello, a microbiologist at the New York University School of Medicine, in which babies born by means of Caesarean section are swabbed on the mouth and skin with gauze taken from their mothers' vaginas. Her team wants to see whether these offspring end up with microbiota similar to babies born vaginally. \u201cIt's not standard of care,\u201d Bale says, \u201cbut I will bet you, one day, it will be.\u201d Many are still sceptical about the link between microbes and behaviour and whether it will prove important in human health \u2014 but scientists seem more inclined to entertain the idea now than they have in past. In 2007, for example, Francis Collins, now director of the US National Institutes of Health, suggested that the Human Microbiome Project, a large-scale study of the microbes that colonize humans, might help to unravel mental-health disorders. \u201cIt did surprise a few people who assumed we were talking about things that are more intestinal than cerebral,\u201d Collins says. \u201cIt was a little bit of leap, but it's been tentatively backed up.\u201d Funding agencies are supporting the emerging field, which spans immunology, microbiology and neuroscience, among other disciplines. The NIMH has offered seed funding for work on model systems and in humans to probe whether the area is worth more-substantial investment, a move that has already brought more researchers into the fold. The MyNewGut project in Europe has an even more optimistic view of the value of such research, specifically seeking concrete dietary recommendations that might alleviate brain-related disorders. Today, Knickmeyer's project on infants represents what she calls \u201ca messy take-all-comers kind of sample\u201d. Among the brain regions that Knickmeyer is scanning, the amygdala and prefrontal cortex hold her highest interest; both have been affected by microbiota manipulations in rodent models. But putting these data together with the dozens of other infant measures that she is taking will be a challenge. \u201cThe big question is how you deal with all the confounding factors.\u201d The children's diets, home lives and other environmental exposures can all affect their microbiota and their neurological development, and must be teased apart. Knickmeyer speculates that tinkering with microbes in the human gut to treat mental-health disorders could fail for other reasons. Take, for instance, how microbes might interact with the human genome. Even if scientists were to find the therapeutic version of a \u201cgold Cadillac of microbiota\u201d, she points out, \u201cmaybe your body rejects that and goes back to baseline because your own genes promote certain types of bacteria.\u201d There is much more to unravel, she says. \u201cI'm always surprised. It's very open. It's a little like a Wild West out there.\u201d  \n                 Tweet \n                 Follow @NatureNews \n               \n                     Microbiome: Microbial mystery 2015-May-13 \n                   \n                     Drugging the gut microbiome 2015-Mar-06 \n                   \n                     Mental Health: Thinking from the Gut 2015-Feb-25 \n                   \n                     Gut\u2013brain link grabs neuroscientists 2014-Nov-12 \n                   \n                     Microbiome therapy gains market traction 2014-May-13 \n                   \n                     Bacterium can reverse autism-like behaviour in mice 2013-Dec-05 \n                   \n                     Nature  special: Human microbiota \n                   \n                     Innovations in the microbiome \n                   Reprints and Permissions"},
{"file_id": "526624a", "url": "https://www.nature.com/articles/526624a", "year": 2015, "authors": [{"name": "Kenneth R. Weiss"}], "parsed_as_year": "2006_or_before", "body": "The island nation of Kiribati is one of the world's most vulnerable to rising sea levels. But residents may have to leave well before the ocean claims their homes. High tide left its mark on the houses like a dirty ring in a bathtub. The flood crept into the village of Teaoraereke under the cover of darkness, sending filthy seawater sloshing through pigsties and shallow graves, and into people's homes. Teaoraereke residents scrambled to retreat, hoisting sleeping children, sodden bedding and other belongings to higher ground. But some stayed put, including Rerema Kauria, a 63-year-old grandmother who was marooned just inches above the floodwaters on a raised platform bed. She was still there by mid-morning as the water receded, her possessions tucked into the rafters of her traditional house of wooden poles and thatch. She knew that when high tide returned that afternoon it would bring more flooding, but she gave a roaring laugh when asked if she had considered leaving. \u201cWhere would I go?\u201d The uncertain future of people such as Kauria has drawn attention to a collection of atolls in the central Pacific Ocean that make up the Republic of Kiribati (pronounced Keer-re-bahs). The average height of the country's 33 islands is little more than 2 metres above the ocean, which makes Kiribati  acutely vulnerable to climate change . By the end of the century, melting polar ice and the thermal expansion of warmer seawater is expected to  raise global ocean levels  by perhaps 1 metre. That upsurge would, according to some predictions, displace many from Kiribati and millions of others around the world \u2014 and the water will keep going up. For years, Kiribati President Anote Tong has  sounded the alarm over his nation's plight , warning that residents would soon have to abandon their homeland. The flooding that hit Teaoraereke last year reinforces those dire predictions. Although it is impossible to know how much, if at all, climate change contributed to the flooding, village residents say that they have never before seen such inundation. To some of them, it seemed as if the swelling seas were starting to consume Kiribati and the end of the atoll might come sooner than they had thought. But researchers who study Kiribati say that the situation is not a simple story of rising seas swallowing low-lying islands. In fact, some coastal experts dispute the idea that Kiribati will soon sink beneath the waves like a modern Atlantis. They have gathered evidence that many of these islands have been gaining ground in recent decades by capturing sediments from surrounding coral reefs. \u201cIt's just plain wrong to assume that all atolls are washing away,\u201d says Arthur Webb, a coastal geomorphologist affiliated with the University of Wollongong in Australia who has spent two decades living and working in the Pacific Islands. \u201cIt's also wrong to sugar-coat the sobering facts that rising sea levels will ultimately seal the fate of low-lying islands and their limited soils and groundwater. The confusion isn't surprising. It's just more complicated than many expect.\u201d Even if Kiribati does not drown in the near future, its residents may soon need an exit strategy. Poverty, overcrowding and poor sanitation are galloping ahead of rising seas to deplete the islands' resources, especially their supply of clean fresh water. And residents' habits of altering the shoreline and removing coastal protections can magnify the impacts of the swelling oceans, leaving villages more exposed to flooding. The story playing out on these tiny islands shows how difficult it is to tease out the impact of climate change from other human and environmental pressures. And what happens to the Kiribati people has implications for the hundreds of millions in low-lying coastal areas across the globe who will be threatened with flooding and displacement in coming decades. But unlike the residents of Miami, Guangzhou or Mumbai, the Kiribati people have no option of retreating inland or up-slope as their vulnerable flyspecks of land become uninhabitable. As Kauria says: where would they go? \n               At the mercy of the tides \n             From the air, Kiribati's Tarawa atoll emerges from the Pacific as narrow strands of land that join to form a wispy V shape. On the outside of the V is the deep blue of the ocean; inside are the aquamarine and turquoise waters of the shallow lagoon. Tarawa is the capital of Kiribati, which is one of the most remote countries on Earth, located on the equator about halfway between Australia and Hawaii. Its atolls are scattered across a patch of the Pacific the size of India, and yet they have a total of just 811 square kilometres of land, about half the size of Greater London. When a plane lands on Tarawa, a crowd gathers at the airport, drawn by the excitement of the jet making the three-hour flight from Fiji. Aside from occasional freighters bringing canned food, this twice-weekly Fiji Airways flight provides the primary connection to the outside world. The airport was built on relatively high ground, an elevation of 3 metres, in one of the atoll's widest sections. It happens to sit above the main subsurface reservoir, a freshwater layer floating on top of the seawater that presses against the porous island from all sides. Although the sea presents an existential threat, the more immediate problem is not too much water, but too little \u2014 of the fresh, clean kind. A dozen of Kiribati's islands are deserted, too arid to support human habitation. Without enough replenishing rains, their thin lenses of groundwater turned brackish. On Tarawa, groundwater is heavily overdrawn and contaminated by the local practice of defecating on the beach or in the bushes. With little land, residents bury their relatives and raise pigs next to their homes, which also contributes to groundwater pollution. Next to the road leading to the airport, a buried white plastic pipe that carries fresh water from the reservoir has been exposed in places, owing to erosion by waves and tides. Public workers have fought back by using old tyres filled with concrete to hold it in place. They have had less success keeping locals from tapping illegally into the waterline, directing the flow into hand-dug wells for their homes. Water supplies are so limited that authorities turn on the airport's groundwater pumps for only a couple of hours every other day. The perils of water, both sweet and salty, are intertwined with Tarawa's history, says George Fraser, high commissioner to Kiribati from Australia, which is the biggest provider of international support to the developing nation. In one of the bloodier clashes in the Pacific during the Second World War, US commanders misjudged the tides and landing craft got stuck on the reef, forcing marines to wade through chest-deep water under heavy Japanese fire. Fraser deconstructs that infamous battle as he takes a tour of the island the day before the king tide that flooded Teaoraereke. A fast and confident driver, Fraser weaves his small sport utility vehicle around wobbly wheeled trucks, dodging potholes. In narrow spots, the atoll's main road is soaked with seawater and he swerves to avoid a wave splashing over a concrete berm. \u201cSome people use calendars to get through the week,\u201d he says. \u201cWe use tide charts.\u201d The road is the only paved one on the atoll. It crosses a series of causeways that have been battered by wheels and waves; road crews repair cavities, stuffing them with as much concrete and sand as possible to slow the decay. The Australian government has bankrolled much of a repaving project along the length of South Tarawa. As Australia's top representative, Fraser has a keen sense of the various challenges that this poor country faces in coming decades, and how they stack up. \u201cIf you look at rising sea levels as the train coming down the track, it's a couple of kilometres away,\u201d he says. \u201cIf you look at what's 100 metres down the track, it's no water, and right behind it is no food.\u201d More than half of Kiribati's 110,000 residents live on Tarawa, and their numbers are rapidly increasing as more arrive from outer islands seeking jobs, cash and better schools. Many were subsistence fishers and farmers on their home islands, struggling with depleted fisheries and poor soil damaged by periodic over-wash of salt water. When they get to Tarawa, they often end up jobless or underemployed. The Kiribati culture is communal, with families accustomed to bedding down together on woven mats on the floor. It is taboo to refuse the request of a relative, so households often pack dozens of extended family members from other islands under one roof. That has made South Tarawa one of the most densely packed places in the Pacific; its clusters of shanties resemble slums in the poorest capitals of Africa and Asia. Factoring in high birth rates and ongoing urbanization, the government projects that the population of the island will almost double in 15 years. The new Battle of Tarawa will be over where all these people will live. \n               The incredible shrinking island \n             Today's scientific debate about whether Kiribati is growing or shrinking can be traced to Charles Darwin \u2014 who first worked out how coral atolls form. While sailing the Pacific on the HMS  Beagle  in the 1830s, he theorized that these curiously shaped sand islands are produced by coral reefs that sprouted on the slopes of volcanic islands and have continued to grow as the volcanoes sink into the abyss. He was proved right a century later, when scientists drilled into an atoll and hit volcanic rock. Over the millennia, the exoskeletons of millions of tiny coral animals fuse with coralline algae and the shells of molluscs and other sea creatures to form limestone reefs, often arranged in a circle with a shallow lagoon in the middle. Living corals grow on the fringes of these limestone platforms. As the crest of the living reef reaches close to the ocean surface, waves break some of it into rubble and sand that gets deposited on the dead limestone platform to form land. The atolls that exist today are the survivors, ones in which coral reefs kept pace with rising seas and the subsidence of the undersea volcano. The pressing issue is, what will become of those atolls as sea levels start rising faster? Researchers wonder whether corals can keep up, given the host of environmental problems they face. In many places, overfishing and nutrient pollution have triggered the growth of coral-killing bacteria and algae. Abnormally warm seawater is causing  'bleaching' die-offs throughout the tropics , and as ocean water takes up more carbon dioxide and acidifies, it will be harder for coral polyps to build rugged exoskeletons. Around Tarawa, the coral reefs are in particularly poor shape, says Simon Donner, a climatologist at the University of British Columbia in Vancouver, Canada, who has done diving surveys. \u201cCoral cover is lower than you'd expect around the island,\u201d he says. \u201cThat's the legacy of pollution, sewage mostly, and frequent bleaching events in the past 20 years.\u201d To help predict how corals may fare in the future, Dennis Hubbard, a geologist at Oberlin college in Ohio, and his colleagues have been peering into the past, amassing a database of sediment core samples obtained by drilling into limestone beneath coral reefs. With carbon dating, they can determine how quickly these reefs have grown: in yet-to-be-published work, they have found that more than half of the world's coral reefs grew more slowly over the past 10,000 years than sea levels are rising today. Extrapolating forward, those results suggest that only half of all atolls in existence today have a chance of keeping pace with rising seas under the best of conditions, he says. \u201cGiven that this was in a time with no human impact, we feel this is the most optimistic scenario possible.\u201d \n               Not so fast \n             Kiribati and other low-lying island nations have long been held up as the countries most susceptible to the ravages of rising seas. In 2001, the Intergovernmental Panel on Climate Change (IPCC) highlighted 1  predictions that two-thirds of Kiribati and the nearby Marshall Islands would be inundated by a sea rise of 80 centimetres. But the idea that these atolls will disappear any time soon has been challenged by Paul Kench, a coastal geomorphologist at the University of Auckland. He and his colleagues have pored over satellite images, comparing new and old aerial photographs to see how such islands have changed. In a 2010 study, he and Webb determined 2  that 23 out of 27 atoll islands scattered across Kiribati, Tuvalu and the Federated States of Micronesia had either increased in area or remained stable in recent decades. The results, they reported, \u201ccontradict widespread perceptions that all reef islands are eroding in response to recent sea level rise\u201d. The researchers concluded that these islands are more \u201cresilient landforms\u201d than previously thought. The study created a media stir in the region and beyond. It has been widely cited by climate-change sceptics seeking to punch holes in research on global warming and its impacts. Kench recognizes the powerful forces of climate change but complains that too many scientists and activists focus solely on rising sea levels while ignoring the other part of the equation: how the land responds. \u201cThere are a lot of claims that islands are passive geological entities that will sit there and drown,\u201d he says. \u201cOur work shows that they are anything but static. They are dynamic. They move around and they can grow. So just because sea level is rising, it doesn't mean doom and gloom for all atolls.\u201d He also believes that most scientists make a mistake by tethering the fate of atoll islands to the health of surrounding coral reefs. Even if reefs die, he says, they can provide sufficient sediment to maintain islands for a century or more. But Hubbard considers Kench's views shortsighted. \u201cIf you run out of reefs, you run out of sediment, and once you run out of sediment, you run out of islands,\u201d he says. \u201cA lot of this is a semantics issue, challenging when the reef island is going to be physically underwater. Those reef islands are going to be abandoned long before that because they are uninhabitable.\u201d On Tarawa and other Kiribati islands (see 'Isolated islands'), most people do not dwell on such matters, going about their daily lives just like residents of other countries. But their president has earned international recognition for speaking out on the threats of climate change. In an interview, Tong dismisses those who suggest that atolls are resilient to rising seas, saying that they have the luxury of \u201ctalking from the top of a mountain\u201d and not putting their lives on the line. \u201cThese people are not living here. Their grandchildren will not be living here. If they believe that, let them come here,\u201d he says, pounding his fist on a chair armrest for emphasis. \u201cI'd rather plan for the worst and hope for the best.\u201d Tong has told his people that they must prepare to leave, seeding the idea of an early \u201cmigration with dignity\u201d, rather than fleeing as refugees when storm-generated waves wash over the islands. Last year, his government completed an US$8-million purchase of 22 square kilometres of hilly land in Fiji, to grow food and provide possible refuge for some of his people \u2014 although it will not accommodate all of them. He does not know when people will need to migrate, but he wants to purchase more land in Australia and New Zealand, saying that it is much cheaper than trying to build sea walls and other defences. \u201cIf we build up these lands, it's going to cost billions of dollars,\u201d he says. \u201cWe might as well be buying land for millions of dollars elsewhere.\u201d \n               Developing challenges \n             For nearly a week after Teaoraereke flooded, resident Matua Kamori worked alongside his neighbours to build a makeshift sea wall where the high tide had breached a sand berm on the beach. Villagers piled up chunks of coral scavenged from the shore and grouted them together with cement donated by a local church. Kamori, 33, lives in the village with his wife and 4 kids on a small parcel of land given to him by his wife's uncle in exchange for looking after one of his sons. To prepare the land, Kamori spent months scouring sand and coral gravel off the beach and hauling it to the site with nothing more than a rice sack. Over time, he fashioned a building pad half a metre high and constructed a hand-hewn traditional house of wood and thatch on it. Such beach mining is rampant on Tarawa, according to household surveys. Government studies 3  show that it increases the likelihood of flooding by lowering the protective sand berm that keeps the highest tides at bay. In the case of the recent inundation, Kamori says that he fared better than most: the water reached calf-deep in his house, rather than thigh-high. But nothing could be done to stop the briny stew of salt water, mixed with human and animal waste, from polluting his well or killing his garden of vegetables and banana and papaya trees. Kamori says that he settled on the land because he had nowhere else to go. As crowding increases, new settlements are pushing into vulnerable lowlands, places they historically would have avoided. The individual actions of settlers such as Kamori are only part of the problem on Tarawa. Large-scale construction projects over the years have also exacerbated flooding and erosion, says Naomi Biribo, Kiribati's secretary of fisheries and marine resources development. Biribo earned a PhD in Australia by examining the impact of the sea walls and other human structures on Tarawa. The construction of causeways, rather than bridges, to connect the islets had the effect of closing channels and disrupting the flow of sediment that normally resupply some eroding coastlines, she found 4 . Reclamation projects that create new land are another problem: although such efforts have added hundreds of hectares to Tarawa, they accelerate erosion elsewhere, says Biribo. For Tarawa residents, she says, the thin ribbons of land leave little room to move. \u201cIn many places on Tarawa, you can stand in the middle and you can see the ocean on one side and still see lagoon on the other side,\u201d she says. \u201cIf we retreat from the ocean side, and institute a setback, we will fall into the lagoon.\u201d Biribo's work suggests that sea-level rise may be having a small influence on the shoreline changes happening today, but nothing compared to human activities and the seasonal variations in erosive tides and waves in the Pacific that come with the  El Ni\u00f1o periods of warming and La Ni\u00f1a cooling . Donner agrees that climate change has been dwarfed by other factors so far. \u201cYou cannot blame the flooding on sea-level rise,\u201d he says. \u201cAt least not yet.\u201d Where does this leave residents of Kiribati? Webb has long wrestled with that question. He is married to a woman from Tarawa, and they own a house there, where they live with their children for part of the year. Webb was also a lead author of the small-islands chapter 5  of the IPCC's fifth assessment report in 2014, which found that rising seas present \u201csevere sea flood and erosion risks for low-lying coastal areas and atoll islands\u201d. It highlighted one projection 6  that a 50-centimetre rise in sea level could displace 1.2 million people from low-lying islands in the Caribbean Sea and the Indian and Pacific oceans; that number almost doubles if the sea level rises by 2 metres. And yet, the latest assessment steered clear of the IPCC's previous assertion that an 80-centimetre rise would inundate two-thirds of Kiribati. Scientific understanding of atoll geology has sharpened since that earlier projection. Webb expects some remnants of Tarawa to remain a century or two from now, but probably no more than some wave-washed gravel banks \u2014 and by that point, everyone will have long gone. The geological evidence does not get to the key human question about the destiny of these Pacific islanders. That leaves Webb facing a difficult question \u2014 one he hears from his own Kiribati-born teenagers. \u201cHow long do we have?\u201d they ask. To that, he replies: \u201cYour children will not grow old in the atolls.\u201d \n                 Tweet \n                 Follow @NatureNews \n               \n                     Human adaptation: Manage climate-induced resettlement 2015-Jan-14 \n                   \n                     Climate science: Rising tide 2013-Sep-18 \n                   \n                     Adapting to a warmer world: No going back 2012-Nov-28 \n                   \n                     \"Too late\" to save Pacific island nation from submersion 2008-Jun-06 \n                   \n                     Nature  special: Outlook for Earth \n                   \n                     Kiribati Climate Change \n                   \n                     IPCC Fifth Assessment Report \n                   Reprints and Permissions"},
{"file_id": "526490a", "url": "https://www.nature.com/articles/526490a", "year": 2015, "authors": [{"name": "Quirin Schiermeier"}], "parsed_as_year": "2006_or_before", "body": "As a massive El Ni\u00f1o warming builds in the equatorial Pacific Ocean, researchers hope to make the most of their chance to study this havoc-wreaking phenomenon. The tropical Pacific seemed out of sorts this August, as oceanographer Kelvin Richards and his team cruised along the equator east of the Marshall Islands. Six tropical cyclones had barrelled across the ocean in the previous month, and more were spinning up as Richards' research expedition got under way. The sea surface across the region was abnormally warm, with water temperatures at least 1 \u00b0C higher than expected. And when the oceanographers peered below the surface, they found signs of intense turbulence extending hundreds of metres down. The team had found itself cruising through a spectacular El Ni\u00f1o warming event \u2014 one that may become the  strongest ever recorded . Big El Ni\u00f1os can turn climate conditions in the Pacific upside down and disrupt weather around the globe. The impacts of this one have already been felt. Indonesia has suffered through a withering drought that has intensified fires in forests and agricultural land, and Pacific corals are experiencing one of the  worst bleaching events on record . Peru has declared a state of emergency in some regions in expectation of flooding, and farmers in Australia have been put on alert for expected drought. The last time a major El Ni\u00f1o developed, in 1997\u201398, extreme weather and flooding killed thousands and left a quarter of a billion people in Asia homeless. It also helped to jack up global temperatures to a point never recorded before. For Richards, a researcher at the University of Hawaii at Manoa, the arrival of the latest El Ni\u00f1o turned out to be good timing. Such warmings develop only once or twice a decade, following no regular schedule, so researchers are eager to learn how to predict when an El Ni\u00f1o will hit and how powerful it will grow. That means that they must keep close tabs on the atmosphere and ocean, from the surface waters to the cooler layers hundreds of metres below. But getting the necessary data can be difficult. It takes years to plan research cruises, so it is hard to get a ship into the heart of the Pacific in time to study an unpredictable event. When Richards applied for ship time back in 2012, he had no idea that his trip would happen right as a warming episode was gathering strength. \u201cIt just happened to coincide with our expedition, and we gladly took the opportunity,\u201d he says. Most oceanographers are not lucky enough to be out at sea this year, but they are taking advantage of their colleagues' data, as well as information flowing in from research buoys and other sources. One key question that they want to answer is why every El Ni\u00f1o behaves differently. \u201cEl Ni\u00f1os are not made from a cookie cutter,\u201d says Michael McPhaden, an oceanographer with the US National Oceanic and Atmospheric Administration (NOAA) in Seattle, Washington. The strength and impact of each El Ni\u00f1o seem to depend in part on which region of the Pacific warms up first, but predicting the pattern of temperature anomalies is tough. \u201cWe would really like to better understand what's causing the diversity, and how far in advance it might be possible to predict what type of event we need to prepare for,\u201d says McPhaden. That would help forecasters to give warning of coming droughts and floods months before they hit. \n               Bait and switch \n             The current El Ni\u00f1o is a glaring reminder of  how much scientists need to learn . When it first started to take shape in 2014, it developed like many others. There was a weakening in the easterly trade winds that normally flow from South America towards Asia, carrying heat and moisture to the western part of the basin. This allowed warmth to spread eastwards, and researchers expected the pattern to be reinforced by westerly wind bursts that would help to push warm water eastwards (see 'Unruly ocean'). When enough warm water accumulates off the coast of South America, it prevents the normal upwelling of cool, nutrient-rich water from deeper layers. That, in turn, alters fish populations and typically ruins the anchovy harvest off the coast of Peru. But in 2014, the warming along the equator was less pronounced than in most El Ni\u00f1o years, and the westerly wind bursts did not appear as expected. By mid-year, the anticipated El Ni\u00f1o had completely vanished. What had stopped the show, and why the Pacific warming spectacularly resurfaced 12 months later, are questions that are puzzling ocean researchers and meteorologists. The mysteriously reborn El Ni\u00f1o is a fantastic opportunity for researchers to combine observations and models to find out what has happened, and perhaps to improve forecasting systems, says Axel Timmermann, an oceanographer at the University of Hawaii. One possible explanation, he says, is that the expected westerly wind bursts came too early last year, so they did not pile up enough warm water in the eastern Pacific to inhibit upwelling. That would have stopped El Ni\u00f1o in its tracks. But there is also a chance that an overlooked mechanism enabled cool water from deep layers to reach the surface. Or it might simply be that the erratic nature of the El Ni\u00f1o Southern Oscillation (ENSO) \u2014 the irregular sequence of warm El Ni\u00f1o and cold 'La Ni\u00f1a' phases \u2014 is down to the randomness of the weather. To test these hypotheses, researchers will need many forms of data, including measurements of ocean temperature over time, upwelling rates, water density and the strength of currents. And it will be important to compare El Ni\u00f1o years to neutral years and times when La Ni\u00f1a appears, as well as years when an event seems to be looming and then fails to materialize, says Matthew England, a climate scientist at the University of New South Wales in Sydney, Australia. The problem is made even more difficult because ENSO behaviour may be shifting as a result of climate change. Warmer surface waters make it easier for an El Ni\u00f1o to start, so researchers expect the events to become more frequent. Last year, a model-based study by Wenju Cai, a physical oceanographer with the Commonwealth Scientific and Industrial Research Organisation in Aspendale, Australia, in which Timmermann was involved, suggests that by the end of the century, extreme El Ni\u00f1os such as the 1997\u201398 event will occur twice as often as they have in recent decades (W. Cai  et al .  Nature Clim. Change   4 , 111\u2013116; 2014) . \n               Problem child \n             The Pacific warming was first described in the late 1880s by a Peruvian Navy captain who reported on an unusually warm ' corriente del Ni\u00f1o ' (ocean current of the Christ Child), so named because it appeared around Christmas time. For a long time, El Ni\u00f1o was thought to be a local phenomenon off Peru and Ecuador. But measurement campaigns during the International Geophysical Year 1957\u201358, which coincided with a major El Ni\u00f1o, revealed that the phenomenon spans the whole Pacific Ocean. Over the decades since, research on El Ni\u00f1o and La Ni\u00f1a has shown how conditions in the ocean and atmosphere reinforce each other to produce warming and cooling. Although El Ni\u00f1o/La Ni\u00f1a events can cause powerful changes to weather, science-funding agencies have been reluctant to sponsor expensive research expeditions to study them because they are so hard to forecast. Researchers instead rely to a large extent on data from the Tropical Atmosphere Ocean network of buoys strung across the Pacific, which is jointly operated by NOAA and the Japan Agency for Marine-Earth Science and Technology (JAMSTEC). Temperature and salinity data from the array of 70 or so moored buoys allow researchers to detect unusual ocean warming and track the large waves that push warm water eastwards. But the array is not without problems. Many buoys have failed in recent years, temporarily leaving scientists with data from just 40% of the network. Thanks to repair work, the system is currently back at 80% capacity. But budget cuts in 2012 forced NOAA to decommission a ship, the RV  Ka'imimoana , which was used for regular maintenance of the array. Over its 16-year service with NOAA, the ship had also made itself invaluable to the El Ni\u00f1o research community by collecting data on factors including water temperature, salinity and density as it made its servicing runs to the buoys. With the ship no longer available, data collected from buoys and autonomous floats are not sufficient to study the subtle changes in currents and ocean mixing that may be involved in El Ni\u00f1o evolution, says Timmermann. Researchers have other vessels available, such as the RV  Falkor , on which Richards made his trip. The RV  Kilo Moana  hosted a second University of Hawaii team in the equatorial Pacific in August and September. Oceanographers Brian Popp and Jeffrey Drazen had an unexpected research opportunity: they had planned to study mercury accumulation in marine organisms in a region with strong equatorial upwelling, but the data they collected during their expedition will allow them to examine how the effects of a strong El Ni\u00f1o might ripple through the marine food web. And yet, says Cai, this year's El Ni\u00f1o \u2014 possibly a once-in-a-generation event \u2014 is a missed opportunity with respect to going out and documenting the breadth of physical, chemical and biological changes that might occur in the ocean. \u201cIt's a pity we can't have more ships at sea,\u201d he says. Help might be on the way. By 2020, NOAA and JAMSTEC hope to have launched a sustained Tropical Pacific Observing System of buoys and satellite instruments to advance understanding of ocean variability and improve weather and climate prediction. That will be too late to help with the current El Ni\u00f1o, which is expected to peak late this year or early next. In recent months, it has been keeping pace with the most powerful El Ni\u00f1os on record, and westerly wind outbreaks in early October promised to keep the warming going. As a result, forecasters are warning many parts of the globe to prepare for some wild and crazy weather over the next several months. \n                 Tweet \n                 Follow @NatureNews \n               \n                     Playing hide and seek with El Ni\u00f1o 2015-Aug-17 \n                   \n                     Developing El Ni\u00f1o could be strongest on record 2015-Aug-14 \n                   \n                     El Ni\u00f1o tests forecasters 2014-Apr-02 \n                   \n                     El Ni\u00f1o monitoring system in failure mode 2014-Jan-23 \n                   \n                     Frequency of extreme El Ni\u00f1os to double as globe warms 2014-Jan-19 \n                   \n                     NOAA El Ni\u00f1o portal \n                   \n                     NOAA El Ni\u00f1o theme page \n                   \n                     Tropical Pacific Observing System \n                   Reprints and Permissions"},
{"file_id": "526492a", "url": "https://www.nature.com/articles/526492a", "year": 2015, "authors": [{"name": "Helen Pearson"}], "parsed_as_year": "2006_or_before", "body": "Armed with 850,000 diaries, an Oxford centre is trying to find out why modern life seems so hectic. In 1961, when more and more people were buying television sets to go with their radios, the BBC wanted to work out the best times to air its programmes. So its audience-research department decided to ask a sample of people across the United Kingdom to record what they were doing every half hour of the day, and to indicate whether the TV or radio was on. The result was a trove of 2,363 diaries filled with the everyday details of British lives. \u201c8 a.m., Eating breakfast,\u201d read one; \u201c8.30 a.m., Taking children to school; 9 a.m., Cleaning away, washing up and listening to Housewives' Choice\u201d \u2014 a popular radio record-request programme of the day. Today, these files are part of the biggest collection of time-use diaries in the world, kept by the Centre for Time Use Research at the University of Oxford, UK. The centre's holdings have been gathered from nearly 30 countries, span more than 50 years and cover some 850,000 person-days in total. They offer the most detailed portrait ever created of when people work, sleep, play and socialize \u2014 and of how those patterns have changed over time. \u201cIt certainly is unique,\u201d says Ignace Glorieux, a sociologist at the Dutch-speaking Free University of Brussels. \u201cIt started quite modest, and now it's a huge archive.\u201d The collection is helping to solve a slew of scientific and societal puzzles \u2014 not least, a paradox about modern life. There is a widespread perception in Western countries that life today is much busier than it once was, thanks to the unending demands of work, family, chores, smartphones and e-mails. But the diaries tell a different story: \u201cWe do not get indicators at all that people are more frantic,\u201d says John Robinson, a sociologist who works with time-use diaries at the University of Maryland, College Park. In fact, when paid and unpaid work are totted up, the average number of hours worked every week has not changed much since the 1980s in most countries of the developed world. Epidemiologists, meanwhile, are mining the diaries to explain how lifestyle changes are contributing to a rise in many chronic diseases. The diaries \u201cwere the greatest asset I could possibly have\u201d, says physiologist Edward Archer at the University of Alabama at Birmingham, who used the data in a 2013 study 1  of obesity. Now, the Oxford centre is testing a major update to its 50-year-old methods. In addition to asking people to complete a handwritten diary, it last year began giving them an electronic fitness tracker and a small camera that snaps a stream of pictures of their day (see 'The gadget guinea pig'). \u201cThe idea is to be a bit more adventurous,\u201d says Teresa Harms, a sociology research fellow who is leading the project. \u201cAre new technologies better than what we've been doing all these years?\u201d \n               Time management \n             Ironically for a scientific institute dedicated to time use, the researchers at the Oxford centre are no better at time management than anyone else. If anything, they are worse. One day in July, students were playing a game of croquet on the lawn outside the centre's home: a stone building in the placid grounds of St Hugh's College. But inside, things were more fraught. One flustered postdoc had slept through her alarm and arrived at 10.33 a.m. \u2014 more than an hour late for her meeting. The centre's ebullient founder and co-director, sociologist Jonathan Gershuny, cheerily admitted that his own time management is \u201cterrible\u201d \u2014 shortly before locking himself out of his office without his keys, ensuring that he would arrive for his next appointment catastrophically late. None of this seems to have slowed down Gershuny, who can trace the origins of the centre to the 1970s, when he was starting his career at the University of Sussex in Brighton, UK. Gershuny wanted to predict what society and the economy would look like in future decades \u2014 but he realized that there was very little empirical evidence showing how people actually spend their time. Gershuny started to search for surveys in which people had been asked to record their daily activities. Among his first discoveries were the BBC diaries. Another thousand or so journals, recorded in the 1930s, turned up in a mouldering old tea chest at the university. As Gershuny's diary collection grew, it became obvious that the records had been gathered by many different investigators around the world for many different purposes. To align the data and make meaningful comparisons, he would have to put them into a standardized form. So in the 1980s \u2014 by then working at the University of Bath, UK \u2014 he developed the Multinational Time Use Study: a system in which every activity is given one of 41 codes (gardening, 9; sleeping, 16; relaxing, 36). In an early project that assessed diaries from the United States and the United Kingdom, Gershuny and Robinson showed in 1988 that women in both nations were spending less time on domestic work, whereas men were doing slightly more \u2014 a consequence of women's increasing entry into paid employment and of shifting societal norms 2 . By the early 2000s, many countries had started to collect standardized time-use data; the US Bureau of Labor Statistics started gathering them annually in 2003. These efforts were driven by a growing global interest in understanding the impacts of time use on economies and on well-being. But the diary bank still remained something of a side line for Gershuny until 2008, when he won funding to develop a centre at Oxford dedicated to time-use research. Then, in 2013, the centre was awarded two major grants: \u20ac2.5 million (US$2.8 million) from the European Research Council and \u00a33.7 million (US$5.7 million) from the UK Economic and Social Research Council (ESRC), to exploit the diary bank and to launch a major collection of time-use diaries in the United Kingdom. \u201cWe were paid suddenly to do the things that I'd been agitating to do,\u201d Gershuny says. And one of those things has been to find out why some people feel so busy all the time. \n               No time \n             In 1930, the economist John Maynard Keynes wrote an essay predicting life 100 years ahead. The United States and Europe would be so prosperous that people would work just 15 hours a week, he said, and the main concern for \u201cour grandchildren\u201d would be how to fill their copious leisure time. That's not quite how things are turning out \u2014 something that Gershuny started to think about in the early 2000s. He was feeling desperately busy \u2014 more so than in the past \u2014 and people around him were complaining that they were stressed out and working harder as well. Books on the matter were proliferating, with titles such as  Fighting for Time 3 ,  Busier than Ever 4  and  Work Without End 5 . Survey data hinted at the problem too: in the United States, the proportion of people reporting 6  that they 'always' felt rushed was 24% in 1965 but 34% in 2004. Yet when researchers used diary data to look into the matter, a different picture emerged. Analyses showed that people in many countries routinely overestimate the amount of time that they spend working \u2014 in the United States, by some 5\u201310% on average 7  (see 'The truth about time'). But those who work longer hours tend to overestimate by the most: people who guess that they work 75-hour weeks, for example, can be over by more than 50%, and those of certain professions \u2014 teachers, lawyers, police officers \u2014 overestimate by more than 20%. (Scientists were not the worst exaggerators: they estimate working close to 42 hours per week on average, whereas diaries clock them at 39 hours) 8 . In a 2005 study 9 , Gershuny compared the BBC diaries from 1961 with UK diaries collected in 1983\u201384 and 2001, adding up the number of minutes per day spent on paid work, unpaid work (such as chores around the house) and other activities. He wanted to know whether people were actually working longer hours than they did 40 years ago. The answer was that it depends. Men had reduced the number of hours they spent on paid work, increased those in unpaid work and overall came out ahead, with just under 50 minutes more free time per day. Women were doing more paid work \u2014 again reflecting their movement into the workplace over the decades \u2014 and less unpaid work, producing little change overall. Studies in the United States and western European countries have shown similar patterns: little overall change in work time and, at least in some studies and groups, a slight growth in leisure time. All in all, there is little support for the idea that everyone is working harder than ever before. \u201cWhen you look at national averages of time-use data, it doesn't really show up,\u201d says Oriel Sullivan, a sociologist who now co-directs the centre with Gershuny. But certain groups have experienced a different trend. According to analyses by Gershuny, Sullivan and other time-use researchers, two demographic groups are, in fact, working harder. One consists of employed, single parents, who put in exceptionally long hours compared to the average; the other comprises well-educated professionals 10 , particularly those who also have small children. People in this latter group find themselves pushed to work hard and under societal pressure to spend quality time with their kids. \u201cThe combination of those pressures has meant that there is this group for which time pressure is particularly pertinent,\u201d Sullivan says. These findings, the researchers say, could help to explain why there is a widespread perception that life is busier for everyone. Sullivan and Gershuny propose that the time-squeezed professional group includes many of the academics who study and discuss the phenomenon, as well as the journalists who write about it \u2014 in other words, the people in society with a loud voice. But Gershuny suggests that changing attitudes to work and leisure may also play a part. In nineteenth-century Europe, having ample leisure time signified a person of high social status: one philosopher described the literary types in Paris around 1840, who had such an abundance of time that it was fashionable to walk a turtle on a leash through the arcades. In the twenty-first century, the situation has reversed, so that being busy is a signal of a privileged social position \u2014 and therefore an impression that some people are keen to give. Gershuny calls being busy in the modern day a \u201cbadge of honour\u201d, and Glorieux agrees. \u201cThe first thing we say when we meet people is 'I'm busy',\u201d he says. \u201cSuppose we say, 'I'm not busy; I have nothing to do; I was watching some TV.' It's not what people want to say.\u201d People might also feel busier because of an increase in multitasking, especially with computers and smartphones. The US time-use diaries are poor at recording how long people engage with their devices, says Robinson \u2014 in part, he suspects, because they have become so prevalent that people don't even report when they are on them. \n               Family time \n             The diary bank at Oxford has revealed other changes in how people use their time. In 2011, Oxford centre postdoc Evrim Altinta\u015f drew on diaries collected in the United States between 1965 and 2013 to examine how much time parents spend on 'developmental childcare' \u2014 engaging with children through reading, talking and helping with homework 11 . Activities of this type are strongly associated with better educational scores, behaviour and other positive outcomes in later life. Altinta\u015f found that parents overall were spending more time on developmental childcare in the 2000s than they were in the 1960s and 1970s, but she found a bigger increase among parents who both had university degrees than for those who had high-school diplomas or less. She estimated that a child born to a highly educated mother in the early 2000s would receive 27 minutes more developmental childcare per day than one born to less-educated parents \u2014 adding up to 657 extra hours of focused attention for that child during the first four years of life. \u201cThat puts the children who are born to less-educated parents at a real disadvantage,\u201d Altinta\u015f says. The diaries have also exposed trends that could affect the health of adults. In his study on obesity 1 , Archer analysed more than 50,000 diary days collected between 1965 and 2010 and divided women's time into paid work, household work, personal care and free time. Then he calculated what that meant for the amount of energy they were burning up. The results showed that women in 2010 were spending around 12 hours less per week on cooking, cleaning, laundry and other domestic work than women in 1965, and that had shifted towards more-sedentary pursuits such as using a computer. As a result, the team estimated that working women today are burning some 130 kilocalories per day less than those in the 1960s, and they proposed that this could be one explanation for the rise of obesity in the United States. (Archer stresses that he is not saying that women should do more housework; rather, the work reinforces public-health advice encouraging more physical activity of any kind.) At Oxford, Gershuny and Harms are attempting to carry out more-detailed analyses of energy use in collaboration with researchers at the US National Cancer Institute in Rockville, Maryland. Harms has been matching up entries in a selection of diaries to a list, widely used in research, of more than 800 activities alongside estimates of the energy burned doing each. (It includes entries as specific as playing darts, coalmining, whirlpool-sitting and casino gambling.) The study, which is still under way, has so far shown that a gym session or other structured work-out accounts for only a small fraction of the energy that a person typically burns each day. Activities such as paid work and childcare often burn more \u2014 because even if they are less physically intense, they take up longer periods of time. \u201cThe real metabolic activity is built up during the working day,\u201d Harms says. \n               Next-generation diaries \n             Since Gershuny started his diary bank, time-use research has become a thriving industry: there are now several hundred researchers in the field. But time-use diaries have weaknesses, and the biggest is that they can be wrong: people quickly forget what they were doing and record their days inaccurately \u2014 or they can lie. The desire to improve accuracy is one of the motivations behind CAPTURE-24: Gershuny and Harms's project to collect a new generation of diaries using some of the latest gadgets around. So far, about 150 people have each spent 24 hours wearing a watch-like accelerometer strapped to their wrists and a small camera, which takes around three pictures per minute, hung around their necks. They also jot down what they are doing for every 10-minute slot of the day in a conventional paper diary. The aim is to see whether the devices can provide more-useful information for the researchers than a standard paper diary alone. The accelerometer should collect more-accurate data on body movement and energy use, one reason that the project has earned support from biomedical-research funders the British Heart Foundation and the Wellcome Trust, both in London, as well as the ESRC. The photos could keep a more faithful record of when and what people eat \u2014 food diaries are notoriously unreliable \u2014 or reveal important nuances in people's interactions with children. (It is harder to say that you were focusing on childcare when the camera pictures show that you were checking your phone.) In her preliminary analyses, Harms has found that gadget diaries and paper diaries show the same sequence of events, but that the gadgets reveal details that paper diaries missed. Most researchers in the field agree that the future lies in collecting data through phones and other devices. \u201cMaybe this will bring a new boost to time-use research,\u201d Glorieux says. He anticipates a situation in which reams of diary data \u2014 such as location, heart rate, calories burned and even ambient noise \u2014 are collected through phones and linked-up gadgets. The researchers at Oxford are keen to grow their diary collection in other ways. They recently added ones from China, South Korea and India, and they are trying to include more from Eastern European and developing countries. And Gershuny holds out hope that there are more tea chests of old diaries still waiting to be found. Then, scientists can begin to examine cultural differences in how people from different regions work, rest and play. So many questions, so much data, so little time. Clearly, doing all this is going to take a lot more than Keynes's 15 hours a week. But the scientists hope to get there, by taking it one day at a time.\n \n                 Tweet \n                 Follow @NatureNews \n               \n                     Interdisciplinarity: How to catalyse collaboration 2015-Sep-16 \n                   \n                     Work ethic: The 24/7 lab 2011-Aug-31 \n                   \n                     Epidemiology: Every bite you take 2011-Feb-16 \n                   \n                     Nature  special: Interdisciplinarity \n                   Reprints and Permissions"},
{"file_id": "528459a", "url": "https://www.nature.com/articles/528459a", "year": 2015, "authors": [], "parsed_as_year": "2006_or_before", "body": "Ten people who mattered this year. Christiana Figueres: Climate guardian | Junjiu Huang: Embryo editor | Alan Stern: Pluto hunter | Zhenan Bao: Master of materials | Ali Akbar Salehi: Nuclear diplomat | Joan Schmelz: A voice for women | David Reich: Genome archaeologist | Mikhail Eremets: Super conductor | Christina Smolke: Fermenting revolution | Brian Nosek: Bias blaster | Ones to watch \n               CHRISTIANA FIGUERES: Climate guardian \n             \n               A dynamic leader charted the path to a new global climate agreement. \n               By Jeff Tollefson \n             Hours after the world\u2019s governments adopted a  landmark climate accord this month , Christiana Figueres was all smiles on the dance floor of a boisterous night club in Paris. As the leader of the United Nations climate convention, she had spent five long years travelling the world to rally support among environmentalists, businesses and govern\u00adments for the accord, in which 195 countries pledged to keep global warming to well below 2\u2009\u00b0C. But now here she was, leading conga lines and dancing to the Village People\u2019s classic \u2018Y.M.C.A.\u2019. Asked whether she ever had any doubts, she flashed a smile, pulled her hands together as if in prayer and pointed skyward. \u201cThe stars are guiding us,\u201d she said. Born into a politically powerful family in Costa Rica, Figueres came by her activism naturally. Her father led the republic\u2019s 1948 revolution and served as its first president. Her brother followed suit, with a term as president in the 1990s, and her mother served in the congress. Friends and colleagues credit Figueres for breaking out of her comfort zone in Costa Rica and jumping into the international environmental arena. \u201cIn this country, being a Figueres means something,\u201d says Monica Araya, a former climate negotiator who founded Nivela, an environmental think tank based in Heredia, Costa Rica. \u201cShe built a whole career outside Costa Rica, and in a very important way she chose climate change as her activity.\u201d Figueres attributes her environmental activism to the demise of a toad  that disappeared from Costa Rica\u2019s Monteverde Cloud Forest Reserve. She saw one when she was young, but her daughters missed the chance. \u201cThat was a real awakening for me,\u201d she says, because rising temperatures have been linked to the toad\u2019s extinction. \u201cI started reading into the topic, and before I knew it I was devoting my life to climate change.\u201d In 1995, after stints in the Costa Rican government at home and abroad, Figueres created a non-profit organization in Washington DC to encourage Latin American engagement in the newly minted UN climate convention. In parallel, she represented Costa Rica as a non-governmental climate negotiator \u2014 a move, Araya says, that helped to pave the way for other members of civil society to join the Costa Rican delegation. Over time, she became increasingly active in the governing secretariat of the UN convention and built up a reputation for getting things done. When Figueres was interviewed for her current post in 2010, she was asked what she would do if she were overruled by her boss. She offered up a quick joke: \u201cWell, to begin with, I would fire him.\u201d \u201cShe is brilliant, way above average, and she has a very well-developed sense of humour,\u201d says Marco Gonzalez, a friend and fellow Costa Rican who formerly headed the UN treaty organization that was built to phase out chemicals that damage the stratospheric ozone layer. \u201cShe brings success in her backpack.\u201d Figueres took charge of an organization and a process that she describes as \u201cin the garbage can\u201d after the  diplomatic meltdown at the Copenhagen climate conference in 2009 . The secretariat had previously concerned itself mostly with national governments, but Figueres expanded its sphere by reaching out to local and regional governments as well as the business sector. \u201cHer fingerprint is all over the intense presence of cities and businesses in Paris,\u201d says David Waskow, director of the International Climate Initiative at the World Resources Institute in Washington DC. Figueres used all of her political skills to help  herd governments towards the Paris agreement  \u2014 and her roots in a developing country helped her to bridge the gulf between rich and poor nations, a division that had plagued past negotiations. Although current climate pledges fall  short of the accord\u2019s ultimate goal , all nations have now committed to the battle against global warming. Throughout the process, Figueres says she has been driven by the same sense of duty that spurred her father: the desire to protect and expand opportunities for those who are less fortunate. \u201cI happened to choose a different battleground at the global level, but it\u2019s the same thing,\u201d she says. \u201cWe have a huge moral responsibility to do everything that we can to improve that situation.\u201d \n               JUNJIU HUANG: Embryo editor \n             \n               A modest biologist sparked global debate with an experiment to edit the genes of human embryos. \n               By David Cyranoski \n             In April, Junjiu Huang published the world\u2019s first report of  human embryos altered by gene editing . The news thrust rapid developments in gene-editing technology into the spotlight and ignited a huge debate about the ethical use of such tools. But Huang, a modest and soft-spoken molecular biologist at Sun Yat-sen University in Guangzhou, chose to stay out of the limelight. Huang and his team used a powerful technique known as CRISPR\u2013Cas9, which can be programmed to precisely alter DNA at specific sequences and has  swept through biology labs  in the past few years. He told  Nature  in April that he wanted to edit the genes of embryos because: \u201cIt can show genetic problems related to cancer or diabetes, and can be used to study gene function in embryonic development.\u201d In his study, he modified the gene responsible for the blood disorder \u03b2-thalassaemia. Huang used spare embryos \u2014 from fertility clinics \u2014\u00a0that could not progress to a live birth. And he expected his paper, which showed that the process created many unexpected mutations, to steer people away from the technology until it had been proved safe. \u201cWe wanted to show our data to the world so people know what really happened with this model,\u201d he said at the time. \u201cWe wanted to avoid ethical debate.\u201d But  the opposite happened : the ensuing discussion polarized the scientific community and nucleated several high-powered forums, including an international summit held in December in Washington DC. The general consensus is that gene editing is not yet ready for altering human embryos for reproductive purposes \u2014 and there are concerns that it could be adopted prematurely by rogue fertility clinics. Some scientists argue that the technique is permissible for research, whereas others say that this too should be forbidden for fear of a slippery slope. Huang has been notably absent from the debate, and refused to be interviewed for this article. \u201cOur paper was just basic research, which told people the risk of gene editing,\u201d he wrote in an e-mail. \u201cIt\u2019s like he\u2019s hiding,\u201d says Tetsuya Ishii, a bioethicist at Hokkaido University in Sapporo, Japan, who was at the US summit. \u201cThat\u2019s strange because there was nothing really ethically problematic about his research. He raised the issue, and that kind of drove discussions on the topic at the summit. That\u2019s a good thing.\u201d But Ishii says that Huang does \u201chave some responsibility to address his critics\u201d, perhaps by discussing cases in which clinical use of gene editing could be worthwhile in the future. Because of the risks, Huang predicted when his paper was published that it could take 50 or 100 years before the world saw a live-born, gene-edited baby. \u201cBut who knows, a decade ago, no one knew of CRISPR,\u201d he said. \u201cWe don\u2019t know what will happen.\u201d \n               ALAN STERN: Pluto hunter \n             \n               A single-minded planetary scientist brought the dwarf planet into focus. \n               By Alexandra Witze \n             Alan Stern, planetary scientist and workaholic, doesn\u2019t sleep much at the best of times. In the days approaching 14 July \u2014 as the  spacecraft he had dreamed about , worked for and slaved over for a quarter of a century neared its target \u2014 he was down to roughly three hours a night. Stern, of the Southwest Research Institute in Boulder, Colorado, is the principal investigator for NASA\u2019s New Horizons mission,  which in July became the first probe to visit Pluto . It whizzed just 12,504 kilometres above the dwarf planet\u2019s surface, in an extraordinarily choreographed fly-by that grabbed images, spectra and other scientific data \u2014 as well as headlines around the world. Stern had been preparing for the day since 1989, when he and other young researchers hatched plans to visit the distant world. They submitted their proposal to NASA, and kept their hopes alive even when the agency killed plans for a Pluto mission in 2000 over budget concerns. After Congress revived funding for the concept, and NASA restarted the competition for proposals, Stern\u2019s team won with a lean design that would carry a few key instruments. \u201cThat meant a laser focus on getting it there,\u201d he says. Stern is nothing if not laser focused. Under his leadership, New Horizons blasted off in January 2006 at a cost of US$720 million, much less than earlier multibillion-dollar missions to the outer Solar System. His three children went through high school and into university with 14 July 2015 imprinted on their brains. When the day arrived, Stern and the rest of Earth got to see  Pluto up close for the first time . Among his  favourite discoveries : ice mountains that tower as high as 4 kilometres, dune fields that may ripple across Pluto\u2019s surface, and skies that are tinted blue by atmospheric haze. A heart-shaped feature that showed up on images was a \u201cpublic-relations bonanza\u201d, he says, inspiring people around the world to connect with the dwarf planet. Stern\u2019s drive to explore new worlds is also reflected in his focus on public relations, says David Grinspoon, a researcher with the Planetary Science Institute in Tucson, Arizona, who is working with Stern on a book about the mission. Stern convened an eclectic group of artists, writers and visionaries in New York City months before the fly-by to pick their brains about ways to connect with the general public. \u201cIt wasn\u2019t your normal outreach team,\u201d Grinspoon says. Stern pursues public engagement with a singular passion. He is known for seeking out \u2014 and scrutinizing \u2014 media coverage. Even during the most intense stages of the mission, Stern was tweeting prolifically and posting to Facebook while overseeing press releases.\u00a0 After the fly-by, Stern found himself swamped with speaking invitations. At an astronomy conference in Vermont, he talked for an hour, took questions for an hour and then met Pluto fans individually. Two university students told him that New Horizons was the best thing that had happened in their lifetime. Months after the Pluto visit, some members of the team experienced a post-fly-by depression. Not Stern. He drives ahead as always, working on the data that will dribble back from the spacecraft until late 2016. He is also resuming work on the  European Space Agency cometary mission Rosetta , on which he has an ultraviolet spectro\u00admeter instrument, and on plans to fly research payloads on suborbital spacecraft. He has a little more time for sleep these days, but not much. And in October and November, New Horizons ignited its engines to set it on course to visit a second Kuiper belt object, this one on New Year\u2019s Day in 2019. If NASA approves the extended mission, Stern says, \u201cI\u2019m looking forward to finishing what we started\u201d. \n               ZHENAN BAO: Master of materials \n             \n               A chemical engineer is merging electronics with the human body. \n               By Erika Check Hayden \n             Zhenan Bao rummages through a plastic box on her desk, eagerly pulling out samples of materials developed in her lab. She finds a thin, nearly weightless patch made of carbon nanotubes that attaches to the wrist like a sticking plaster and monitors the wearer\u2019s heart rate. Then she picks up an artificial skin that uses tiny carbon-nanotube sensors to detect touch; and a version of it that even features hair-like structures to more closely mimic real skin. Bao, a chemical engineer at Stanford University in California and a founder of the field of thin, flexible organic electronics, shines a laser pointer through a sample of the nanotube material used in many of these devices. She laughs as the beam is diffracted into a spray of green dots on the wall, just as it would be when passing through a crystalline material. \u201cThat\u2019s how we know it has regular structure,\u201d she says. Innovations in her field are often inspired by nature, she says: \u201cIf we can understand how to design materials with the same degree of complexity, we will be able to address real-world problems.\u201d A prime example is the creation of  medical devices that can be worn or implanted to monitor blood sugar, send sensory signals and more . Progress towards that goal has taken off this year, with Bao\u2019s lab among the leaders. In October, her team showed that its artificial skin could mimic the sense of touch ( B. C.-L. Tee  et al .  Science    350,  313\u2013316; 2015 ). The researchers took inspiration from human skin, in which specialized nerves fire more rapidly as pressure increases, producing a code that the brain interprets as touch. Previous artificial touch sensors required power-hungry external devices to generate that code. But in Bao\u2019s sensors, pressure alters the oscillating frequency of microscopic circuits made from carbon nanotubes to generate the right kind of signals automatically. Although Bao calls the final design \u201csimple\u201d, it was a major accomplishment, says Polina Anikeeva, a neural-interfaces and materials scientist at the Massachusetts Institute of Technology in Cambridge. She notes that Bao has been working on perfecting these materials for years, and that her lab \u2014 which comprises around 40 chemists, chemical engineers and materials scientists \u2014 is highly interdisciplinary. \u201cIt\u2019s not just one idea,\u201d she says, \u201cmany ideas came together and made this possible.\u201d \u201cWe have many years of work to do,\u201d says Bao, who hopes that the treasures she keeps in the plastic box will one day help to revolutionize health care. \u201cBut generally, the path is laid out.\u201d \n               ALI AKBAR SALEHI: Nuclear diplomat \n             \n               The head of Iran\u2019s nuclear programme helped to forge a pact to keep it peaceful. \n               By Davide Castelvecchi \n             On 14 July 2015, Iran signed an agreement with six world powers to limit the country\u2019s nuclear development in exchange for lifted international-trade sanctions. If the deal is implemented successfully \u2014 still far from certain \u2014 it could ease years of tension over  Iran\u2019s alleged efforts to build nuclear weapons  and so allow the country to become  a major player in global science . That an accord was reached at all, however, was due in no small measure to nuclear engineer Ali Akbar Salehi, who is head of the Atomic Energy Organization of Iran. He worked closely with his US counterpart, energy secretary Ernest Moniz, to iron out the deal\u2019s technical aspects. Educated at the American University of Beirut and the Massachusetts Institute of Technology in Cambridge, Salehi returned to Iran after the Islamic revolution of 1979 and quickly rose to top posts in both academia and the government. By the 2000s, he had become the international face of Iran\u2019s nuclear programme \u2014 a man described as fiercely loyal to his country, but also a voice of reason to whom negotiators could appeal in times of crisis. Salehi is said to be a deeply spiritual person who has the trust \u2014 and the ear \u2014 of the country\u2019s supreme leader, Ayatollah Ali Khamenei. And he is one of very few people to have held senior posts in both hardline and comparatively liberal governments. This talent for building bridges is what enabled Salehi to work so effectively with Moniz during the negotiations, says Reza Mansouri, an astronomer at the Institute for Research in Fundamental Sciences in Tehran and a former deputy science minister of Iran; they shared the language of science. Mansouri, who has known Salehi for more than three decades, says that he has the modern, rational frame of mind that enables people to \u201cagree on how to talk to each other\u201d. \n               JOAN SCHMELZ: A voice for women \n             \n               A senior astronomer worked to unmask a prominent sexual harasser. \n               By Alexandra Witze \n             They came forward, one by one. Young female astronomers sought out Joan Schmelz and confided in her about the sexual harassment that they had endured. Schmelz, a solar physicist and chair of the American Astronomical Society\u2019s Committee on the Status of Women in Astronomy from 2009 to 2015, heard too many of these stories \u2014 and a lot of them involved the same man. Schmelz told the women that they were not alone, and asked whether they wanted to talk to others who were in the same situation. Thanks in part to those introductions, four women eventually filed complaints. Their actions, which became public this year, led to  the resignation of Geoff Marcy , a well-known exoplanet hunter at the University of California, Berkeley. It was one of the most dramatic episodes in a string of gender-equality controversies this year, including Nobel laureate Tim Hunt\u2019s  dismissive comments about women working in the laboratory . In astronomy, Schmelz\u2019s behind-the-scenes efforts to expose sexual harassment set the stage for a sea change in community understanding, says Meg Urry, an astronomer at Yale University in New Haven, Connecticut, and president of the astronomical society. After Marcy was outed, astronomy departments at universities and other institutions began frank discussions about unacceptable behaviour. \u201cWithout Joan, I don\u2019t think we would have seen this remarkable change,\u201d says Urry. Women were comfortable sharing their stories with Schmelz because she had been through the same thing. Early in her career, Schmelz had found herself the target of harassment by her supervisor. \u201cI was very isolated, and I didn\u2019t have anyone to confide in,\u201d she says. She only began to realize what had happened to her years later, in 1991, when attorney Anita Hill accused Clarence Thomas, a judge nominated for the US Supreme Court, of sexual harassment. In 2011, Schmelz went public, through a blog post on the website of the Committee on the Status of Women in Astronomy. Then the Marcy stories started pouring in. \u201cFor a while I kept trying out how we could move forward \u2014 I contacted a lot of people, players in the community, to see if there was anything we could do for these women,\u201d she says. Eventually the option emerged of filing complaints under the legislation known as Title IX, which prohibits sexual discrimination on campuses that receive federal funding. In July 2014, the first complaints hit Berkeley. \u201cI wasn\u2019t sure it would ever happen,\u201d says Schmelz. All this intense work took place as Schmelz led a busy career in solar astronomy. In June this year, she took a job as deputy director of Arecibo Observatory in Puerto Rico. Months later,  the director resigned , leaving Schmelz in charge of the world\u2019s largest single-dish radio telescope. She now lives just a block from the beach, which offers a much-needed respite when she can spare the time. But Schmelz knows that her work on harassment is not over. She would like to press universities to keep long-term records of complaints. In most institutions, there is no method for tracking whether there have been one, two or ten incidents reported against a given person over time. \u201cLet\u2019s find ways to take the pressure off the young women, so they can work on their science, write a thesis, without all of this extra added burden on them,\u201d says Schmelz. \u201cLet\u2019s change the system.\u201d \n               DAVID REICH: Genome archaeologist \n             \n               A big thinker helped to turn ancient genomics from niche pursuit to industrial process. \n               By Ewen Callaway \n             For most of its 30-year history, the field of ancient genetics has revolved around discovering exceedingly rare samples \u2014 a bone, a tooth \u2014 that harbour enough intact DNA to study. This year, population geneticist David Reich proved that it\u2019s possible to explore human history by powering through ancient genomes en masse. Reich\u2019s genome factory has revealed mass migrations, the spread of farming and the roots of languages. Last month, his group at Harvard Medical School in Boston, Massachusetts, reported genome data from 230\u00a0people  who lived in Europe and the Middle East over the past 8,000\u00a0years , tracking changes in skin colour, immunity and other traits ( I.\u00a0Mathiesonetal.Naturehttp://doi.org/9rb;2015 ). At university, \u201cI think I was sort of idealistic\u201d, Reich says. \u201cI was interested in grand unifying theories.\u201d For his first degree, he switched from sociology to physics. During his second, in biochemistry, he fell for human population genetics, and soon built a reputation for scientific rigour. In the late 2000s, plummeting sequencing costs and other advances made it easier to extract and analyse ancient DNA. Reich realized that by analysing the genomes of large numbers of people, he could see how immigration and interbreeding changed the genetics of entire regions. In 2013, Reich opened his own lab devoted to sequencing ancient remains. Its scale was industrial from day one: the first human samples came from 66 individuals who had lived in what is now Russia, including members of a Bronze Age culture called the Yamnaya. In June, the team described a massive migration of Yamnaya people into Western Europe, some 5,000 years ago ( W.\u00a0Haak  et al. Nature    522,  207\u2013211; 2015 ). It is not the only group powering through ancient genomes: the lab of Eske Willerslev at the Natural History Museum of Denmark in Copenhagen reached a similar conclusion ( M. E. Allentoft  et al .  Nature    522,  167\u2013172; 2015 ). Reich\u2019s team argued that the Yamnaya migration might also explain the radiation of Indo-European languages across Europe and Asia \u2014  advancing a problem that has vexed linguists for decades . By exploring the consequences of genetics for other fields, Reich \u201cis trying to do something that a lot of geneticists might not\u201d, says David Anthony, an archaeologist at Hartwick College in Oneonta, New York. Reich is eager to see genetics inform other debates, such as those about the  peopling of the Americas  and the prehistory of India. \u201cThe invention of ancient DNA as a tool for studying the past is like the invention of a new scientific instrument, like a microscope,\u201d he says. \u201cYou can see into things that you couldn\u2019t see before.\u201d \n               MIKHAIL EREMETS: Super conductor \n             \n               Decades of diligence earned one physicist a record for resistance-free electricity. \n               By Edwin Cartlidge \n             As a young researcher during the 1970s and 1980s, Mikhail Eremets proved to have a temperament well suited to life at the Institute for High Pressure Physics outside Moscow. The facilities were often abysmal, but the soft-spoken Belarusian was prepared to work around them \u2014 even dialling the same telephone number 100\u00a0times just to get a working line. \u201cIf I want to do something I am happy to repeat it many, many times,\u201d says Eremets, who is now at the Max Planck Institute for Chemistry in Mainz, Germany. That doggedness has served him well in his quest to understand how materials behave at pressures close to those of Earth\u2019s core \u2014 conditions that he recreates by squeezing tiny samples between the tips of two diamond \u2018anvils\u2019. These experiments have been painstaking and repetitive, with results that never troubled the Nobel committee. Until late 2014, that is, when Eremets and his colleagues reported hints that pressurized hydrogen sulfide \u2014 the compound responsible for the smell of rotting eggs \u2014  can become a superconductor , allowing electricity to flow without resistance at a record-breaking 190\u2009kelvin (\u221283\u2009\u00b0C) ( A.\u00a0P.\u00a0Drozdovet\u00a0al.Preprintathttp://arxiv.org/abs/1412.0460;2014 ). He and others  published conclusive evidence \u00a0\u2014\u00a0and measured an even higher temperature \u2014 in August ( A.\u00a0P.\u00a0Drozdov  et\u00a0al.    Nature   525,  73\u201376; 2015 ). The advance has been hailed as a giant step towards the long-sought goal of room-temperature superconductivity and the promise of loss-free electrical transmission. It has certainly  rocked the physics community , says Igor Mazin of the Naval Research Laboratory in Washington DC. Other materials have produced superconductivity at high temperatures, but the mechanism by which hydrogen sulfide operates has  never achieved superconductivity above 40\u2009kelvin . No independent group has confirmed the result entirely, but Eremets is already planning experiments to see whether hydrides doped with chemicals can superconduct at normal, atmospheric pressure \u2014 an essential step towards practical use. Having done most of his important work since turning 50, he feels he has plenty of research left in him. \u201cIn that sense I am still a young, growing scientist,\u201d he says. \n               CHRISTINA SMOLKE: Fermenting revolution \n             \n               A synthetic biologist won a breakneck race to produce opioids in yeast. \n               By Erika Check Hayden \n             Early this year, synthetic biologist Christina Smolke was in a dead-heat race with a handful of other labs to engineer a yeast strain capable of making opioids. These powerful pain-killing drugs are crucial in medicine, but they come solely from opium poppy crops that can have unpredictable yields. Scientists were seeking a more stable production method but faced a daunting hurdle: no one had been able to identify an enzyme that converts reticuline \u2014 a chemical building block of morphine and other narcotics \u2014 from one form to another. Most other labs hunting for the enzyme were working to isolate it from poppies directly. But Smolke and her team at Stanford University, California, took a different approach: they combed through genetic databases, looking for snippets of sequence that looked as if they might be involved in reticuline metabolism. When they found a hit from several different poppy species, they ordered a synthetic version of the gene that had been built letter-by-letter by a machine. They plugged it into yeast and it worked. \u201cI was super excited, really proud and also relieved,\u201d Smolke says. \u201cIt was a bit of a Hail Mary.\u201d The discovery enabled Smolke\u2019s lab to stitch together a pathway of 23\u00a0different genes from plants, mammals, bacteria and yeast to produce the world\u2019s first narcotic through synthetic biology ( S.\u00a0Galanie  et\u00a0al. Science    349,  1095\u20131100; 2015 ). It was a crowning achievement for a biological wunderkind who started her own lab at the California Institute of Technology in Pasadena at the age of just 28. The opioid-producing yeast cells contain the most complex synthetic-biology pathway developed so far, and mark a turning point for the field by showing how step-by-step engineering can turn microbes into drug factories. \u201cThis will significantly impact our future ability to produce many more chemicals through biotech\u00adnology,\u201d says Jens Nielsen, a synthetic biologist at the Chalmers University of Technology in Gothenburg, Sweden. Much of the news coverage of the work, however, stirred fears about how it could foster new ways to  easily manufacture illegal drugs  \u2014 and some scientists have  argued for tighter regulation  of the growing field. Smolke counters that existing regulations already restrict the production and distribution of narcotics; any lab that wishes to work with the yeast strain reported in her paper, for instance, must be licensed by the US Drug Enforcement Administration. So far, no one has requested the strain. In a bid to ground the debate in reality, Smolke, her husband \u2014 fellow Stanford synthetic biologist Drew Endy \u2014 and another colleague this year attempted to brew opioids using her lab\u2019s strain and standard beer-making equipment ( D.\u00a0Endyet\u00a0al.PreprintatbioRxivhttp://doi.org/9t2;2015 ). The set-up produced only a trace amount of reticuline and none of the downstream chemical, thebaine, that is used to synthesize commercial drugs such as oxycodone and oxymorphone \u2014 suggesting that it would be difficult for the average home-brewer to start making these pharmaceuticals. (The scientists\u2019 positive fermentation control, an English ale, was \u201cpalatable\u201d, the manuscript notes.) Smolke co-founded a company, Antheia, based in Palo Alto, to produce opiate drugs in yeast commercially, and specialists in the field suspect that more will follow. But some onlookers are circumspect. Plant biologist Ian Graham at the University of York, UK, says that it will be hard to beat poppies. \u201cWhere plants already do it very well, the arguments for taking a synthetic-biology route are much less convincing,\u201d he says. For Smolke, the goal is not merely to copy plants, but to engineer opioids that are free of side effects such as dependency and addiction. Sitting in the office of a Palo Alto incubator space, wearing jeans and grey Converse sneakers to a meeting with the co-founders of Antheia, Smolke can appear casual \u2014 but the intensity that has propelled her to the pinnacle of her field is tangible. For her, the year\u2019s accomplishments are just part of a quest to understand and improve on opioids, which are among the most complex natural chemicals . \u201cIt\u2019s a very powerful approach to take inspiration from nature and go beyond it,\u201d she says. \n               BRIAN NOSEK: Bias blaster \n             \n               A psychologist pledged to improve reproducibility in science. \n               By Brendan Maher \n             When Brian Nosek was a graduate student in experimental psychology, he started working on the implicit-association test, which reveals people\u2019s unconscious prejudices with the push of a button. Tap right every time a male name appears on a screen, for example, and left for a female name. That\u2019s easy \u2014 but add some stereotypically male or female roles into the mix and things get interesting. Even the most liberal minds will sometimes stall when asked to press the same button for the word \u2018executive\u2019 and for the name \u2018Susan\u2019. The tests are challenging, informative and kind of fun. So in 1998 Nosek convinced his mentors, who had developed the test, to put it online. It was a success: about a million people per year now take the test for research, corporate training and other reasons. \u201cIt really spread the word about what unconscious bias is,\u201d says Betsy Levy Paluck, a social psychologist at Princeton University, New Jersey. For Nosek, a key demographic still needs to be educated about their biases: scientists. Nosek is convinced that researchers are unconsciously influenced by their hypotheses, that  these biases can be seen in common practices that distort the interpretation of data  such as  p -value hacking , and that they are major drivers of the much-discussed  crisis in research reproducibility . In 2013, Nosek took leave from his post at the University of Virginia in Charlottesville to co-found the Center for Open Science (COS), a non-profit company that builds tools to facilitate better research methodology. It hit several milestones this year, accumulating US$18\u00a0million in funding and a staff of 68. Nosek also co-authored a set of guidelines for transparency and openness that more than 500 journals have signed up to ( B.\u00a0A.\u00a0Nosek  et\u00a0al. Science    348,  1422\u20131425; 2015 ). But the COS\u2019s most visible output in 2015 was the Reproducibility Project, an ambitious attempt to re-test seminal findings in 100\u00a0psychology papers ( OpenScienceCollaborationSciencehttp://doi.org/68c;2015 ). The decision to run the project \u201cwas quite brave of him\u201d, says Dorothy Bishop, a neuropsychologist at the University of Oxford, UK, because poor results could tarnish the field\u2019s reputation. In the end,  61 of the findings could not be replicated  \u2014 but the outcome was mostly received well, something for which many psychologists credit Nosek\u2019s careful diplomacy and can-do approach. Nosek is pushing researchers to adopt practices that will improve reproducibility, including preregistering studies, tracking the results in an open way and publishing them whether they are positive or negative. It will be a dramatic culture change, says Bishop, who has begun using systems developed by the COS for her own research. \u201cYes, it creates a lot more work. You have to document and check it very thoroughly. But it\u2019s not a bad thing to be slowed down a bit.\u201d A second reproducibility project that is focused on findings in cancer biology should begin releasing results next year, and Nosek says that negotiations are in the works for similar projects in ecology and computer science. No one operates completely free of bias, he says, and that includes him. \u201cI try to have some humility and understanding that I am as prone to these behaviours as anyone else.\u201d \n               Ones to watch in 2016 \n             \n               Fabiola Gianotti, Director-general of CERN \n             Gianotti will take charge  at the European lab as its Large Hadron Collider clocks up record high-energy particle collisions\u2014 and as hopes of the next big discovery soar. \n               Gabriela Gonz\u00e1lez, Spokesperson at Advanced LIGO \n             If rumours that this observatory  has detected gravitational waves  prove true, one of the most elusive predictions of the general theory of relativity would be confirmed. \n               Kathy Niakan. Stem-cell biologist, Francis Crick Institute \n             By  applying for approval to edit the genomes of human embryos , Niakan has placed herself at the front of the fast-moving, controversial CRISPR\u2013Cas9 field. \n               Demis Hassabis, Co-founder, DeepMind \n             There is intense curiosity about what will emerge next from Hassabis\u2019s efforts to  combine neuroscience and machine learning  at the Google-owned firm. \n               Yang Wei, Head, National Natural Science Foundation of China \n             Yang will be influential at this growing basic-research agency as China  overhauls its funding systems  and sets its next 5-year plan. \n                 Tweet \n                 Follow @NatureNews \n               \n                 Weibo \n               \n                     2015 Editors' choice 2015-Dec-23 \n                   \n                     365 days: The science events that shaped 2015 2015-Dec-17 \n                   \n                     365 days: The best science images of 2015 2015-Dec-17 \n                   \n                     The inside story on wearable electronics 2015-Dec-01 \n                   \n                     The fragile framework 2015-Nov-24 \n                   \n                     US astronomers rally to end sexual harassment 2015-Oct-21 \n                   \n                     Over half of psychology studies fail reproducibility test 2015-Aug-27 \n                   \n                     Superconductivity record sparks wave of follow-up physics 2015-Aug-17 \n                   \n                     Vibrant Pluto stuns scientists 2015-Jul-21 \n                   \n                     Iranian researchers welcome nuclear deal 2015-Jul-15 \n                   \n                     Drugs: Regulate 'home-brew' opiates 2015-May-18 \n                   \n                     Chinese scientists genetically modify human embryos 2015-Apr-22 \n                   \n                     Steppe migration rekindles debate on language origin 2015-Feb-18 \n                   \n                     Nature  special: 2015 the year in review \n                   \n                     Paris climate talks \n                   Reprints and Permissions"},
{"file_id": "527026a", "url": "https://www.nature.com/articles/527026a", "year": 2015, "authors": [{"name": "Richard Monastersky"}], "parsed_as_year": "2006_or_before", "body": "Seven centuries ago, tens of thousands of people fled their homes in the American Southwest. Archaeologists are trying to work out why. Vultures carve lazy circles in the sky as a stream of tourists marches down a walkway into Colorado's Spruce Canyon. Watching their steps, the visitors file along a series of switchbacks leading to one of the more improbable villages in North America \u2014 a warren of living quarters, storage rooms, defensive towers and ceremonial spaces all tucked into a large cleft in the face of a cliff. When ancient farmers built these structures around the year 1200, they had nothing like the modern machinery that constructed the tourist walkway. Instead, the residents had to haul thousands of tonnes of sandstone blocks, cut timber and other materials down precarious paths to build the settlement, known as Spruce Tree House, in Mesa Verde National Park. \u201cWhy would people live here? That's an important question. It's not an easy place to reach,\u201d says Donna Glowacki, an archaeologist now at the University of Notre Dame in Indiana, as she walks among the ruins. Even more perplexing is what happened after they settled there. The villagers occupied their cliffside houses for just a short time before everyone suddenly picked up and left. So did all the other farmers living in the Four Corners region of the American Southwest, where the modern states of Colorado, New Mexico, Utah and Arizona meet (see 'Turbulent times'). All together, nearly 30,000 people disappeared from this area between the mid-1200s and 1285, making it one of the greatest vanishing acts documented in human history. What had been one of the most populous parts of North America became almost instantly a ghost land. Archaeologists have long puzzled over what drove these farmers, the ancestors of the Pueblo people, from their homes and fields. \u201cThat is one of the iconic problems of southwestern \u2014 and world \u2014 prehistory,\u201d says archaeologist Mark Varien, who is executive vice-president of the Crow Canyon Research Institute in Cortez, Colorado. Early scholars blamed nomads, the ancestors of the Apache and Navajo, for violently displacing the farmers. Over the past couple of decades, the main explanation has shifted to climate \u2014 a profound drought and cold snap that hit in the 1270s. But a series of studies by Glowacki, Varian and other researchers reveals a much more complex answer. The scientists have used detailed archaeological analysis, fine-grained climatic reconstructions and computer models to simulate how ancestral Pueblo families would have responded to their environment. The interdisciplinary strategy has enabled the researchers to examine prehistoric societal changes at a level unattainable in most other regions. \u201cWe have enormous detail on this archaeologically. Unparalleled detail,\u201d says Steve Lekson, an archaeologist at the University of Colorado Boulder. The emerging picture is one of a society rocked by troubles until it eventually toppled. More than a century before the Mesa Verde villages emptied out, political disruptions and a monster drought destabilized the entire ancestral Pueblo world. Thousands of people moved into the Mesa Verde region from nearby areas, straining the agricultural capacity of the landscape and eroding established cultural traditions. This led to violent conflicts that further undermined the society, spurring some people to leave. When another drought hit in the late 1200s, the remaining population departed en masse. Political instability, cultural conflict, violence, overcrowding and drought. Many of the challenges encountered by the ancestral Pueblo seem all too familiar in 2015, as hundreds of thousands of  migrants flee from the Middle East and Africa  towards Europe. When Glowacki looks at the events of more than seven centuries ago at Spruce Tree House, she sees many similarities. \u201cThere was a splintering that went on and an implosion of this political system. It was a rejection, them saying, 'We can't live that way anymore. There has to be a better way'.\u201d \n               Stone work \n             It was chance that first carried Glowacki into the world of the ancestral Pueblo. Before starting graduate school, she ended up in a summer job as a ranger at Mesa Verde National Park, where she fell for the landscape and its archaeology. She has spent the past 23 years, on and off, researching the region's ancient populations. At Spruce Tree House, Glowacki pulls out a map showing the latest results of an architectural analysis that she is helping the park to carry out. The work is laborious \u2014 researchers sometimes sit in front of a wall of sandstone blocks for days, studying the mortar and rocks to work out how the structure was first built and then altered over time. Gradually, a history of the village has taken shape, showing that people assembled the first set of rooms in the alcove around the year 1200, and added more right up until the last residents abandoned the site around 85 years later. The researchers can narrow construction dates to within a year or two by analysing tree-ring patterns in the wooden support beams in the ceilings and then matching them to an established tree-ring chronology for the region. Despite the tedious nature of the work, Glowacki says that it never loses its appeal. \u201cThere are rooms that are fully intact, and you can stand in them \u2014 and they were built in the 1240s. In this country, being able to stand in something that was built at that time is really pretty magical.\u201d The cliff dwellings were a last resort for the park's prehistoric Pueblo residents. When farmers first arrived in the region around  AD  600, they settled on the fertile highlands above the canyons, which gave them easier access to their fields. But by 1200, something began to force them over the edge into the giant alcoves that naturally form in the sandstone cliffs. Insights into that shift are emerging thanks to a major interdisciplinary effort called the Village Ecodynamics Project (VEP), which launched in 2002. Funded by the US National Science Foundation, the nearly US$2.5-million initiative is assessing how social and environmental factors influenced the populations of prehistoric Pueblo farmers from about 600 to 1300, says Tim Kohler, the VEP's principal investigator and an archaeologist at Washington State University in Pullman. In one strand of research, the team drew on the rich history of archaeology in the region to compile a database of 18,000 prehistoric sites, which allowed them to measure the population and how it shifted over time 1 . With such a massive database, the researchers could look at population changes in narrow time bands averaging about 40 years (see 'All gone'). \u201cThere are not many places in the world where archaeologists can look at changes in such discrete slices of time,\u201d says Varien, who is a co-principal investigator of the VEP. The analysis 1  suggested that people started leaving the Mesa Verde region at least 15 years before the drought hit. \u201cIt looks as though the final depopulation began with a trickle and ended with a flood,\u201d says Scott Ortman, an archaeologist at the University of Colorado Boulder who developed the model for the project's population analysis. Another part of the VEP looked at how the farmers fed themselves. The researchers used temperature and precipitation estimates from tree-ring data to create a model of where the communities could have grown maize (corn) each year, which was their main source of food. The calculations of this 'maize niche' did a good job of explaining how many people settled in different regions, says Kohler. The team's latest data show that when growing conditions improved, the population density spiked, more than doubling in some regions. But one place defied that pattern: Mesa Verde National Park. When farming became easier, people actually moved out of that area. And, paradoxically, when times grew tough, more people moved in. Kohler and his colleagues suggest that these movement patterns have to do with topography. The park stands higher than the surrounding landscape, so it gets more precipitation. And because the highlands tilt to the south, cold air drains off, leaving Mesa Verde warmer than the surrounding lowlands. So when the region faced drought or a cold spell, farmers congregated in the more-reliable Mesa Verde area \u2014 something researchers had not appreciated before now, says Kohler. \u201cPeople have been working in this area for 100 years, and I don't think they ever realized it,\u201d he says of such a climate pattern. \n               Virtual reality \n             The VEP researchers have also conjured up a virtual version of the past. The team constructed a computer model of the landscape and then seeded it with households that could grow maize, hunt, collect water and wood and move to new sites if they failed to secure enough resources. By comparing the simulations to the archaeological record, the researchers can examine factors that might have driven ancient populations to migrate. \u201cIt's really a new way of doing archaeology,\u201d says Varien. Kohler says that he sometimes switches on the graphics during a simulation to watch the behaviour of the dots that represent households. Scattered randomly at first, they scurry around until their inhabitants can harvest enough resources. Then, they form into settlements, which grow rapidly to a point when they can no longer sustain themselves \u2014 and so the households move again. But there is a limit to how much Kohler can watch. \u201cEven on modern, fast processors, when the agents get into the thousands, it slows down and it's no longer fun,\u201d he says. By comparing the simulations to the actual population data, the researchers discovered 2  some interesting discrepancies during the 1100s and 1200s. In the model, the farmers spread out farther across the landscape than they actually did in reality. So something seems to have caused the real ancestral Pueblo to huddle together more tightly than expected. Kohler and his colleagues wondered whether fear might have been a factor. To find out, they surveyed the archaeological literature and tracked levels of violence in the area through time by tallying how many skeletons had broken arm bones, fractured skulls or other signs consistent with acts of aggression. Some had apparently died in massacres, and there was even evidence of cannibalism at certain sites. Between 600 and 1000, the Mesa Verde region was relatively peaceful, but rates of violence rose in the mid-1000s and spiked again in the late 1200s, right before the ancient Pueblo left, the researchers reported last year 3 . \u201cWhat we found was that people were more clumped up than the model predicted precisely in times when there was a lot of violence on the landscape,\u201d says Kohler. There has been some scepticism among archaeologists about the use of agent-based modelling, but Kohler says that it has been useful in this case: the inconsistency between the simulations and the real data prompted the researchers to look at violence in a new way. \u201cThat disjunction identifies for us interesting questions,\u201d he says. Most researchers think that the majority of violent acts occurred within ancestral Pueblo communities: one village attacking another over food resources or neighbours turning on each other. More than half the skeletons from some periods bore signs of trauma. \u201cThey are one of the most violent societies we've ever studied,\u201d says Kohler. But not all of their troubles came from within. Some unusual-looking projectile points have turned up at massacre sites that date to just before the Pueblo people left the Mesa Verde region, so invading nomads might have had a role in forcing the farmers from their homes. In the next stage of the VEP project, researchers plan to look at how food shortages might have contributed to violence. The new version of the agent-based model is more sophisticated than the last, allowing households to form social groups that compete with each other for access to agricultural lands. Leaders can emerge, fighting can erupt between groups and people can migrate away from Mesa Verde to an area farther south in New Mexico, where many ancestral Pueblo are thought to have resettled. This all amounts to a huge step up in processing, so the team will graduate to a supercomputer for future simulations, which are planned for later this year or early next year. Nothing of this scale has been done before in the field, says Kohler. \u201cArchaeologists do not have the reputation of being users of high-performance computing environments,\u201d he says. \u201cBut I don't think we'll be the end of the road for this kind of work.\u201d Among the ruins at Spruce Tree House, Glowacki takes a different approach. As a collaborator on the VEP project, she does not discount the importance of drought and short growing seasons. But she focuses on some of the other factors that also stressed the ancestral Pueblo society. The signs are in the houses that fill the Spruce Canyon alcove. The architectural-documentation project has taught Glowacki that the residents there updated their homes just as much as people in New York or London today. \u201cEven when they were living there, they were making changes and adding walls and doors and doing all of this remodelling.\u201d \n               Culture clash \n             Some of these alterations point to dramatic events. In the mid-1200s, structures associated with one of the founding families were burned: fire damage can be seen in one room and in a kiva, a circular depression that served as the family's ceremonial space. The fire does not seem to be accidental, Glowacki says. Rather, it could have been part of a ritual changeover in ownership or it might reflect someone forcing out one of the original clans. \u201cAt the very least, that suggests there were some significant changes in the clans or families that were using the structures \u2014 or in part of the leadership there.\u201d Other rooms in the alcove were also burned, including a tower that may have served as a defensive structure. Taken together, the architectural evidence provides a detailed view of friction in the village, she says. \u201cThere was some sort of conflict and people left, presumably, and new people came in and remade these spaces.\u201d Around the Pueblo region, there are many signs of cultural change leading up to and during the 1200s. Glowacki, along with some other archaeologists, thinks that such adjustments had to do with shifting political allegiances in that part of the world. During the mid-1000s and early 1100s, the centre of power among the Pueblo people was located about 150 kilometres south of the Mesa Verde area, in New Mexico's Chaco Canyon. In the 1100s, an extension of the Chaco political order rose up at a site now called Aztec Ruins National Monument, midway to Mesa Verde. The Chaco\u2013Aztec culture was socially stratified, with massive residences in which the elites lived. Smaller versions of the elite 'great houses' have been found in villages to the north, which reveals the broad influence of the Chaco\u2013Aztec political order. Then, an awful drought between 1130 and 1150 apparently weakened that order, and new types of practice emerged. In the Mesa Verde region, some communities built more-inclusive spaces, such as open plazas, and they removed the roofs from some large kivas, allowing broader participation in rituals 4 . The changes in public and ceremonial spaces demonstrate the waning influence of the Chaco\u2013Aztec polity, which had previously unified the Pueblo world. \u201cWhat is happening is you have this dissolution and splintering,\u201d Glowacki says. That may have contributed to the increased violence and served to drive farmers from their highland villages towards the more-secure alcoves along the cliff faces. These political upheavals may also partially explain why people started to abandon the Mesa Verde area decades before the drought of the mid-1270s hit. The combination of political instability, social upheaval and then a rotten climate was too much to take, she says. \u201cIt got really bad and really nasty, and they wanted to get away from it.\u201d Kohler sees parallels with the  collapse of the classic Mayan civilization  in the ninth century, as well as with  events in the Middle East today . In the case of the Mesa Verde exodus, researchers can look in detail not only at why and when people left, but also at what happened afterwards. \u201cWe need to understand migration streams better,\u201d he says. \u201cWe have the advantage of the long view.\u201d \n               Finding peace \n             Whatever forced the Pueblo to uproot themselves, tens of thousands of people left the Four Corners region in search of something better. And many apparently found what they were looking for. When the exodus began, the ancestral Pueblo migrated in several different directions: some to the southwest into Arizona and some to southern New Mexico. Archaeologists have long suspected that many settled along the Rio Grande river in northern New Mexico, a couple of hundred kilometres southeast of the Mesa Verde region. That hypothesis is supported by population data, which show that the Rio Grande region became more crowded; VEP studies 5  have indicated that between 1250 and 1300, the population in this area swelled from 8,000 to 18,000 people. By the early decades of the 1300s, it was close to 25,000, Ortman says. When they settled in their new home, the Mesa Verde people made a clear break from their former lives. Analysis by Kohler, Ortman and their colleagues 3  shows that rates of violence were much lower than before. And the Pueblo made social changes as well. \u201cThe migrants do not appear to be trying to continue with the society and traditions of the Four Corners. They were trying to leave them behind,\u201d says Ortman. The Pueblo villages that grew up after 1300 reflect a much more communal type of society, in which multiple families shared kivas and residents gathered in open ceremonial spaces. There was also a political change, says Lekson, who has studied the elite residences at Chaco Canyon and Aztec Ruins. \u201cThey shucked off all the nobles and the kings, and they never had them again. They figured out how to run villages without that apparatus.\u201d Even today, southwestern Pueblo villages continue to embrace an egalitarian society. Ortman finds inspiration in the evolution of Pueblo culture after the collapse. \u201cPueblo people had to create those values and institutions that reflect them as a result of their past struggles,\u201d he says. And that system has been remarkably successful. Pueblo villages have retained their culture and languages to a much stronger degree than most other Native American communities, he says. \u201cSome of the Pueblos that emerged after the Mesa Verde migration have been able to withstand 500 years of European colonization,\u201d says Ortman. \u201cOne could say that those communities have weathered European colonization better than almost any other society in the world \u2014 certainly within the United States.\u201d At Spruce Tree House, Glowacki has seen how strong those traditions still are. Just a few weeks earlier, she took part in a workshop that included some teachers who are Pueblo and who demonstrated how they grind maize. Even that mundane chore took on spiritual dimensions as the teachers made offerings to their ancestors who once inhabited the cliff dwelling. To the modern Pueblo, the centuries-old structures are not abandoned ruins but still echo with the spirits of those who came before. \u201cIt was a really beautiful moment,\u201d says Glowacki. \u201cWhat I think makes Pueblo culture really interesting and perhaps unique is the long arc of Pueblo history. There's a lot we can learn about how a society faces really difficult times, adversities \u2014 and fundamentally reorganizes and transforms their culture.\u201d\n \n                 Tweet \n                 Follow @NatureNews \n               Reprints and Permissions"},
{"file_id": "526628a", "url": "https://www.nature.com/articles/526628a", "year": 2015, "authors": [{"name": "XiaoZhi Lim"}], "parsed_as_year": "2006_or_before", "body": "Researchers hope to show that using the gas as a raw material could make an impact on climate change. On 29 September, the XPRIZE Foundation based in Culver City, California, announced a 4\u00bd-year competition that will award US$20 million to the research team that can come up with the best way to turn carbon dioxide from a liability into an asset. With gigatonnes of the gas pouring into the atmosphere each year, and with the consequences for global climate becoming increasingly obvious, the Carbon XPRIZE would reward technologies that can convert CO 2  emissions from coal and natural-gas power plants into useful products such as alternative building materials, fuels and raw material for the manufacture of plastics and other chemicals. The invitation should have plenty of takers: a growing number of companies and research chemists are already pursuing that goal. In Reykjavik, what looks like an overgrown playground climbing frame is actually a small chemical plant that turns CO 2  into methanol: a fuel that can also be used to manufacture products ranging from paints to wrinkle-resistant textiles. In Houston, Texas, another small plant is turning CO 2  into materials that go on to become coatings and adhesives. And in Tokyo, Japan, Asahi Kasei Chemicals is widely licensing its technique for turning CO 2  into the polycarbonate plastics used in bulletproof glass, spectacle lenses and electronic parts. Using this greenhouse gas as a raw material is an idea that many scientists once dismissed as hopeless, says Chunshan Song, a chemical engineer at Pennsylvania State University, University Park. As a practical matter, he says, \u201clots of people believed that nothing could be done with CO 2  utilization\u201d after the stuff went up the smokestack. As a source of carbon, sceptics argued, the gas was far more difficult and expensive to obtain than the petroleum, coal and natural gas that now provide the raw material for most chemical manufacturing. And even if CO 2  could be captured cheaply enough, converting such a stable molecule into more-useful chemicals would generally require lots of energy, which might well come from fossil-fuel plants. The conversion could cost a fortune and make more CO 2  than it consumed. But the balance is starting to shift. New conversion technologies are allowing the energy-hungry chemical reactions to proceed more efficiently. Renewable sources such as solar and wind can increasingly supply that energy at a competitive cost without the carbon penalty. And solutions to the capture problem may be forthcoming if governments follow the recommendations of the Intergovernmental Panel on Climate Change (IPCC) and mandate carbon capture and storage \u2014 grabbing the CO 2  coming out of power plants and other industries, and locking it away underground. But rather than simply burying it, companies could defray the cost of capture by putting the gas to productive use (see   Nature   526 , 306\u2013307; 2015). Michele Aresta, a chemist at the University of Bari in Italy, estimates that if currently known processes were deployed most efficiently and at the greatest possible scale, they could directly use some 300 million tonnes of CO 2  per year, while indirectly reducing emissions by around a gigatonne per year \u2014 roughly 5% of the total net emissions 1 . This is hardly a total solution to the CO 2  problem, he says, but it is substantial nonetheless. And that is not counting the additional benefits, which include a shift away from relying on fossil fuels as a source of carbon for materials and chemicals, as well as the use of CO 2 -derived fuels to store and transport energy from wind, solar and other intermittent sources. The Carbon XPRIZE may well accelerate these developments. But research into CO 2  conversion has already been receiving support from funding agencies worldwide. Since 2009, for example, the German Ministry of Education and Research has pumped in around \u20ac100 million (US$110 million) to support research on the use of CO 2  in chemical and synthetic-fuel production. In 2011, the US Department of Energy invested $106 million in various CO 2 -utilization projects. The European Union has lined up a 2020 Horizon Prize worth \u20ac1.5 million for a technology that demonstrates viable CO 2  reuse. And over the next five years, China is expected to invest some 30 billion yuan ($4.7 billion) in CO 2  recycling in the coal, steel, cement and paper industries. \n               Low-hanging fruit \n             Industrial uses of CO 2  have been around for generations. Every year, about 20 million tonnes of the gas are used 'as is': for example, it's the 'fizz' in fizzy drinks, and it can also be used as a high-pressure solvent to clean electronics. But gas used in this way quickly returns to the atmosphere, where it contributes to global warming. Another 114 million tonnes of CO 2  go into the production of urea fertilizer every year, accounting for more than 60% of the total worldwide consumption of the gas. But urea is usually made by reacting CO 2  with ammonia \u2014 most of which is made with hydrogen derived from fossil fuels. Researchers such as Peter Styring, a chemical engineer at the University of Sheffield, UK, are working with industrial partners to develop catalysts and reaction conditions that will allow the production of urea directly from nitrogen, water and CO 2 . But currently, the urea industry is still a net source of CO 2 . In the quest to make more materials from CO 2  without adding to the greenhouse problem, chemists' first challenge is to overcome the molecule's stability. Unlike a carbon atom in isolation, which has four electrons that are available to form bonds, the carbon in CO 2  has given up all four to the tight grip of the oxygen atoms. That grip has to be loosened before the carbon can form new bonds \u2014 and that takes energy (see 'The Carbon Challenge'). \u201cIf it [energy] comes from fossil fuels, it's pointless,\u201d says Styring. \u201cEverything has to be from renewable energy.\u201d This energy cost of separating the carbon and oxygen atoms is so exorbitant that some researchers in the field simply sidestep the problem. They focus on products such as calcium carbonate (CaCO 3 ), which can be made by combining CO 2  with calcium compounds in low-energy reactions that leave the carbon\u2013oxygen bonds relatively undisturbed. Calcium carbonate also happens to have a large potential market: it is used as a construction material and for whitening factory-produced paper. And because it is more inert than CO 2 , it could be used to store the gas over long time scales \u2014 something that already happens naturally in limestone deposits and coral reefs. \u201cNature has used mineralization to store millions of billions of tonnes of CO 2 ,\u201d says Martin Devenney, chief operating officer at Calera, a green-energy company in Los Gatos, California. Most of the calcium carbonate now used in industry, an estimated 15 billion tonnes per year, comes from mining such deposits. But since 2013, Calera has been operating a demonstration facility that produces the material from CO 2  and carbide residue: an industrial waste that comes from the production of polyvinyl chloride (PVC). Calera first adds water to the residue to extract calcium hydroxide, then bubbles CO 2 -rich flue gas from a nearby industrial plant through the solution to obtain pure calcium carbonate, which is turned into fibre cement boards that can be used in construction. After factoring in energy costs, says Devenney, the company's process captures about 170 kilograms of CO 2  for every tonne of calcium carbonate manufactured \u2014 versus zero kilograms for every tonne that is mined. In Asia, meanwhile, manufacturers licensing technology developed by Asahi Kasei Chemicals are using CO 2  to produce about 660,000 tonnes of polycarbonates annually, roughly 14% of the global total. Polycarbonates are plastics used in products ranging from water bottles to spectacle lenses. They are usually made in a reaction involving phosgene (COCl 2 ): a toxic compound that is infamous for its use as a poison gas in the First World War. It is typically produced by combining chlorine with carbon monoxide made by burning coal. The Asahi Kasei process replaces phosgene with CO 2  \u2014 overcoming the molecule's stability by putting it through three intermediate transformations. Asahi Kasei's process is still unable to make polycarbonate chains that are long enough for applications such as water bottles. But it is cost-competitive with the standard method, and according to Aresta's estimates, reduces indirect CO 2  emissions from 6 tonnes for every tonne of polycarbonate produced to 1 tonne. Yet another promising area for low-energy CO 2  utilization is in the production of polyols: a group of sugar-like molecules that are often used as the raw material for polymers such as polyurethanes, which are then used in mattresses, adhesives, coatings, refrigerator insulation and Spandex. Polyols are mostly made from precursor molecules known as epoxides. But Novomer, a green-chemistry company in Waltham, Massachusetts, has developed a cobalt-based catalyst that allows half of the epoxides to be replaced with CO 2 . \u201cOur catalyst will not work without the carbon dioxide,\u201d says Peter Shepard, executive vice-president of Novomer. The Novomer process is currently being tested at a plant in Houston, he says, where it is producing 3,000\u20134,000 tonnes of polyols per year while generating about one-third of the CO 2  that would be produced through conventional methods. And the company has begun designing a facility with a capacity of 100,000 tonnes per year, scheduled for completion in 2017. \n               An uphill task \n             Despite these successes, the demand for low-energy products such as polycarbonates and inorganic carbonates is relatively limited. The big money is in hydrocarbons, which consist of one or more carbon atoms, often arranged in long chains or complex rings, surrounded by hydrogen atoms. As well as their many other uses, hydrocarbons are ubiquitous in petrol, diesel, jet fuel and just about every other liquid energy source on Earth \u2014 a market roughly 14 times larger than that for non-fuel chemicals. Making hydrocarbons out of CO 2  is a \u201ccompletely different story\u201d from making low-energy products, says Aresta. In addition to the energy required to break the carbon atom free from oxygen so that it can link up with other carbons, the process needs a source of cheap hydrogen \u2014 which currently comes from fossil fuels. And it is not easy to control the reactions to get a target hydrocarbon molecule instead of a related molecule that is one carbon shorter or longer. \u201cThis is tricky,\u201d says Song. Still, researchers are making progress. Since 2010, for example, Song has developed a series of catalysts based on iron and cobalt that can bring CO 2  molecules close together with hydrogen molecules at 300 \u00b0C and about 10 atmospheres, forming ethylene, propylene and butane 2 . Because his experiments are still being done in the laboratory, Song cannot yet say how much his process could reduce greenhouse-gas emissions when carried out on an industrial scale. But any headway could have a large effect: these three chemicals are among the most heavily used in the chemical industry, says Song, with demand measured in the hundreds of millions of tonnes per year. Song is also developing palladium\u2013copper catalysts that can efficiently turn CO 2  and hydrogen into methanol. \u201cMother Nature makes biomass and it takes months and years,\u201d he says. \u201cWe can do it in seconds.\u201d As with other efforts to make fuels from CO 2 , however, the challenge is to do it efficiently: even in the laboratory, the known methods consume far more energy than the resulting fuels can provide. Among the most vexing problems is how to obtain the hydrogen. Right now, the choices are to extract the gas from fossil fuels, a source that is cheap but hardly green, or to make it by splitting water, a process that is both energy intensive and costly. \u201cWe need at least 15 years of good research\u201d to do better, estimates Aresta. And even then, he says, the conversion will probably not have a net benefit for climate unless the manufacturing plants can make extensive use of cheap energy from solar, wind and other renewables. \u201cIf we don't use these kinds of energy, we will never be able to go to large-scale deployment,\u201d he says. \n               Fuels and future \n             Some companies are already trying to do just that. In Reykjavik, for example, Carbon Recycling International (CRI) makes methanol from CO 2  and hydrogen using the emissions and electricity from a neighbouring geothermal power plant. The CO 2  in this case comes not from fossil fuel but from carbonate rocks baking in Earth's heat deep underground. The hydrogen comes from the electrolysis of water \u2014 a source that would be prohibitively expensive if the power plant's electricity were not so abundant. CRI currently produces about 5 million litres of methanol per year, or about 0.005% of the global production of methanol, which has 95% less CO 2  emissions than petrol. The advantage for Iceland is that methanol, being a liquid fuel, is a much easier way to export the country's wealth of geothermal energy than, say, laying an underground electricity cable to Europe. But for CRI, the plant is a test bed for wider deployment of its CO 2 -to-fuels technology, which it plans to market in other countries such as Germany as a way to capture and store large amounts of renewable energy at times of low demand. \u201cIt's a matter of accessing energy at the right time,\u201d says K.-C. Tran, cofounder and chief executive of CRI. In most places, because renewable energy isn't widely and cheaply available, the fuels from CO 2  are still not competitive with those currently being made from crude oil, says Jim Yang Lee, a chemical engineer at the National University of Singapore. \u201cIt solves certain, localized problems, assuming you have certain resources,\u201d he says, \u201cbut it's not solving the global CO 2  problem.\u201d The response from proponents is that it does not have to solve the problem all on its own: if some researchers manage to make progress with CO 2 -to-fuels conversion, they argue, while CO 2 -based chemical manufacturing continues to expand, CO 2  utilization could make a noticeable dent in greenhouse-gas emissions in the near future. This year, Styring evaluated various scenarios of CO 2  utilization 3 . If 100% of urea, 30% of minerals, 20% of specific chemicals and polymers, 10% of methane and 5% of diesel and aviation fuels were supplied by currently known CO 2 -utilization methods, he estimates that around 1.34 gigatonnes of CO 2  would be consumed per year. This equals around 83% of the IPPC's 2030 global target for emissions reductions through CO 2  capture and storage. \u201cThese are very conservative estimates,\u201d says Styring. \u201cIt is likely that the impact will be much greater.\u201d For this to happen, researchers acknowledge that it will take far more than clever chemistry. Governments and industries worldwide might have to consider CO 2  utilization along with storage, or even implement a carbon tax to help synthetic fuels stay competitive. \u201cIf you have to pay for CO 2  to go into the atmosphere, then the story will change,\u201d says Song. Aresta is optimistic. \u201cIf we are able, in 20 years from now, to use solar and wind energy in a concrete way,\u201d he says, \u201cand use it to bring back CO 2  into fuels, we can very, very easily reduce the emissions of CO 2  by greater than 10%.\u201d \n                 Tweet \n                 Follow @NatureNews \n               Reprints and Permissions"},
{"file_id": "527152a", "url": "https://www.nature.com/articles/527152a", "year": 2015, "authors": [{"name": "Sara Reardon"}], "parsed_as_year": "2006_or_before", "body": "Gene-editing technologies have breathed life into the languishing field of xenotransplantation. Pale on its bed of crushed ice, the lung looks like offal from a butcher\u2019s counter. Just six hours ago, surgeons at the University of Maryland\u2019s medical school in Baltimore removed it from a hefty adult pig and, with any luck, it will soon be coaxed back to life, turning a rich red and resuming its work in the chest of a six-year-old baboon. An assistant brings the lung to Lars Burdorf and his fellow surgeons, who currently have their hands in the baboon\u2019s splayed chest. The team then begins the painstaking process of connecting the organ to the baboon\u2019s windpipe and stitching together the appropriate arteries and blood vessels. But this 5-hour, US$50,000 operation is just one data point in a much longer experiment \u2014 one that involves dozens of labs and decades of immunological research and genetic engineering to produce a steady and safe source of organs for human transplantation. If the baboon\u2019s immune system tolerates this replacement lung, it will be a sign that the team is on the right track. Robin Pierson heads the Maryland lab, which has performed about 50 pig-to-primate transplants like this one to test different combinations of genetic modifications in the pig and immune-suppressing drugs in the primate. Even so, the team has not had a primate survive for longer than a few days. The complexities of the immune system and the possibility of infection by pig viruses are formidable and drove large companies out of the field in the early 2000s. That trend may now be reversing, thanks to improved immunosuppressant drugs and  advances in genome-editing technologies such as CRISPR/Cas9 . These techniques allow scientists to edit pig genes, which could cause rejection or infection, much more quickly and accurately than has been possible in the past. In October, eGenesis, a life-sciences company in Boston, Massachusetts, announced that it had edited the pig genome in 62 places at once. Some researchers now expect to see human trials with solid organs such as kidneys from genetically modified pigs within the next few years (see \u2018Choice cuts\u2019). United Therapeutics, a biotechnology company in Silver Spring, Maryland, has spent $100 million in the past year to speed up the process of making transgenic pigs for lung transplants \u2014 the first major industry investment in more than a decade. It says that it wants pig lungs in clinical trials by 2020. But others think that the timeline is unrealistic, not least because regulators are uneasy about safety and the risk of pig organs transmitting diseases to immunosuppressed humans. \u201cI think we\u2019re getting closer, in terms of science,\u201d says transplant surgeon Jeremy Chapman of the University of Sydney\u2019s Westmead Hospital in Australia. \u201cBut I\u2019m not yet convinced we\u2019ve surpassed all the critical issues that are ahead of us. Xenotransplantation has had a long enduring reality that every time we knock down a barrier, there\u2019s another one just a few steps on.\u201d \n               Long history \n             Surgeons have been attempting to put baboon and chimpanzee kidneys into humans since at least the 1960s. They had little success\u00a0\u2014\u00a0patients died within a few months, usually because the immune system attacked and rejected the organ. But the idea of xenotransplantation persisted. It could, proponents say, help to save the lives of the tens of thousands of people around the world who die each year while waiting for a suitable human donor. And having a steady supply of farm-grown organs would allow doctors to place recipients on immunosuppressant drugs days ahead of surgery, which should improve survival rates. When details about why non-human organs are rejected began to emerge in the 1990s, the transplantation field was ready to listen. In 1993, surgeon David Cooper of the University of Pittsburgh in Pennsylvania and his colleagues discovered that most of the human immune reaction was directed at a single pig antigen: a sugar molecule called \u03b1-1,3-galactose, or \u03b1-gal, on cell surfaces that can cause organ rejection within minutes 1 . An enzyme called \u03b1-1,3-galactosyltransferase is necessary for producing this sugar, and knocking out the gene that produces the enzyme should temper the reaction. This discovery and other advances in transplantation medicine made the problem seem more tractable to big pharmaceutical companies. In 1996, Novartis in Basel, Switzerland, began to invest heavily in xenotransplantation research, says Geoffrey MacKay, who was the firm\u2019s business director for transplants and immunology at the time and oversaw the xenotransplantation effort. \u201cThey wanted to not only put a dent into the organ shortage but really solve it via transgenic pigs.\u201d MacKay is currently interim chief executive at eGenesis. Novartis initially planned to spend more than $1 billion on xenotransplantation, including both scientific research and planning the infrastructure that would be needed to grow pigs in germ-free facilities around the world. Other companies put some skin in the game, including Boston-based Genzyme and PPL Therapeutics, the British company that collaborated in the creation of Dolly, the first cloned sheep. Regulators such as the US Food and Drug Administration (FDA) began to draw up the guidance and standards that companies would need to meet before the technology could be moved into people. But the immune system turned out to be much more complex than anticipated, and baboons that received pig organs never survived longer than a few weeks, even when the researchers were able to suppress \u03b1-gal production with drugs. A second major concern, especially to regulators, was the risk of infection. Even if pigs could be kept entirely sterile, the pig genome is sprinkled with dozens of dormant porcine endogenous retroviruses (PERVs), and studies conflicted as to whether these could become active in humans. The challenges proved too daunting, and in the early 2000s Novartis killed its xenotransplantation programme, reshuffling or laying off its researchers. Other companies followed suit. It became, Pierson says, \u201cthe third rail of biotech to discuss xenotransplantation as a business plan\u201d. For the next ten years, the business side of the field went dark, at least as far as solid-organ transplants were concerned. Meanwhile, a few research teams and start-up companies began pursuing pig tissue transplants: a much simpler goal than using solid organs because the immune response is not as severe. In April, Chinese regulators approved the use of pig corneas from which all the cells have been removed 2 . Also on the near horizon are pig insulin-producing islet cells that might be transplanted into people with diabetes. The first commercially available islets are likely to come from technology designed by Living Cell Technologies (LCT), a biotech company based in Auckland, New Zealand, that has developed a process to encapsulate pig islet cells in a gelatinous \u2018dewdrop\u2019 that protects them from the human immune system. The product, called DIABECELL, is currently in late-stage clinical trials in several countries. Patients implanted with the cells have survived more than nine years without evidence of immune rejection or infection 3 . \u201cI think people are coming around to look at xenotransplantation in a more-favourable light knowing that we have strong safety data,\u201d says LCT research lead Jackie Lee. Diatranz Otsuka Limited, in Auckland, is now running the programme. Solid organs still pose a challenge. The handful of researchers who have continued to work with them have solved some of the problems that vexed Novartis, such as identifying other key pig antigens and the correct combinations of immunosuppressant drugs. But different organs have different problems: kidneys may be safer than hearts, for instance. Lungs, as Pierson\u2019s team has discovered, are extremely difficult to transplant, because they have extensive networks of blood vessels, which provides more opportunities for primate blood to meet pig proteins and to coagulate. Pierson\u2019s current trials use lungs from an \u03b1-gal-knockout pig that includes five human genes. The baboon is treated with a combination of four immunosuppressant drugs. Most US researchers, including Pierson and Cooper, have relied on pigs made by a regenerative-medicine company called Revivicor in Blacksburg, Virginia, that spun-out from PPL Therapeutics. In 2003, Revivicor co-founder David Ayares and his colleagues created the first cloned pig genetically modified to delete \u03b1-gal 4 . The company has since been experimenting with altering other protein antigens that trigger the immune system or cause human blood to coagulate. These modifications have greatly lengthened the time that an organ can survive in a baboon. In one trial, surgeon Muhammad Mohiuddin at the National Heart, Lung, and Blood Institute in Bethesda, Maryland, and his colleagues took the heart from an \u03b1-gal-free pig that had two human genes that protect from coagulation and sewed it into the abdomen of a baboon 5 . The organ did not replace the baboon\u2019s heart, but the animal lived with the implant for two and a half years. Mohiuddin says that the group is now attempting a \u2018life-supporting\u2019 transplant by replacing the baboon\u2019s heart with a pig heart. The longest life-supporting transplant was published in June 6 , when Cooper\u2019s group announced that a kidney transplant from a Revivicor pig with six modified genes supported a baboon for 136 days. \n               Generation game \n             But the process is slow, Cooper says. It generally takes several generations of breeding to knock out both copies of just one given gene in a pig. Deleting multiple genes or swapping them for their human counterparts takes many more generations, because every litter contains pigs with different combinations of the modified genes. That is why so many are excited about precise genome-editing tools such as CRISPR/Cas9, which can precisely cut both copies of a gene \u2014 or genes \u2014 straight from a pig embryo in one go. \u201cOur first [\u03b1-]gal-knockout pig took three full years,\u201d says Joseph Tector, a transplant surgeon at Indiana University in Indianapolis. \u201cNow we can make a new pig from scratch in 150 days.\u201d His group recently used CRISPR to knock out two pig genes simultaneously 7 . The researchers are now beginning to transplant CRISPR-modified pig organs into macaques, one of which has survived for more than three months. Eventually, gene editing might even eliminate the need for immunosuppression, says Bernhard Hering, a transplant surgeon at the University of Minnesota in Minneapolis. His group is using CRISPR to create pig islets that could be transplanted without the need for drugs. Partly because of LCT\u2019s success with encapsulated islets, many are hopeful that islet cells will be the first genetically modified tissue to make it into clinical trials, paving the regulatory pathway for the more-difficult organs. A non-profit organization has built a germ-free facility in which to raise Hering\u2019s pigs. \n               Technology revival \n             The gene-editing advances have brought new investment into the field. In 2011, United Therapeutics acquired Revivicor for about $8\u00a0million and announced an ambitious plan to start clinical trials of gene-edited pig lungs by the end of the decade. The company\u2019s co-chief executive, Martine Rothblatt, secured land in North Carolina for a farm that could produce 1,000 pig organs per year and says she expects to break ground by 2017. The facility\u2019s elaborate plans include solar panels and helicopter landing pads to help speed fresh organs to those in need. In 2014, United Therapeutics formed a $50-million partnership with the biotech firm Synthetic Genomics (SGI) in La Jolla, California, founded by genome-sequencing pioneer Craig Venter. Rather than simply knocking out antigens, SGI is also engineering tissues that sidestep rejection in a different way \u2014 such as pig cells that produce surface receptors that act as \u2018molecular sponges\u2019 and sop up human immune signalling factors that would otherwise attack the organ. CRISPR and other methods also allow the researchers to make tweaks such as lowering a gene\u2019s expression rather than deleting it completely, says Sean Stevens, head of SGI\u2019s mammalian synthetic-biology group. In September, United Therapeutics committed another $50 million. Peter Cowan, an immunologist at St Vincent\u2019s Hospital in Melbourne, Australia, is taking a different approach. His group has made pigs that generate antibodies against human immune cells. In their design, the antibodies would be made only by transplanted liver cells, ensuring that the immune system is suppressed just around the organ. eGenesis was founded in April by bioengineer Luhan Yang and geneticist George Church of the Wyss Institute and Harvard University in Cambridge, Massachusetts. MacKay says that the firm plans to begin transplanting organs into primates next year. To that end, Church says that the company has made embryos that have more than 20 genetic alterations to cell-surface antigens and other factors and is ready to implant the embryos into female pigs. One of its first publications  used CRISPR to inactivate 62 occurrences of PERV genes in pig kidney cells 8 . The researchers have since transferred the cells\u2019 nuclei into pig embryos. Incidentally, few researchers in the field see the PERV problem as a major safety concern. The virus replicates poorly in human tissues and the risk of spreading it is virtually non-existent, says Jay Fishman, an infectious-disease specialist at Massachusetts General Hospital in Boston. He says that researchers have tracked dozens of people who received unregulated porcine skin grafts, and none seems to have developed disease. But dealing with PERVs may be a regulatory necessity. The FDA said in an e-mail to  Nature  that it is still concerned about the possibility of disease caused by PERVs. There are other pathogens to worry about, too. Most major epidemics start with an animal pathogen that jumps to humans, warns Peter Collignon, an infectious-disease scientist at the Australian National University in Canberra. \u201cIf you want to do the perfect experiment for finding new novel viruses and letting them multiply, this is it.\u201d Unless xenotransplants are proved to be extremely safe, the FDA suggests that they be limited to people with life-threatening conditions who have no other options. It will be even harder to get organs from genetically modified pigs to market, the agency says, because regulators must approve both the genetic construct used to make the animal and the organ itself. Even if safety can be assured, questions remain about whether pig organs would work correctly in their new home, Chapman says. It is unclear whether a pig kidney would, for instance, respond to the human hormones that regulate urination, or whether proteins produced by a pig liver would interact correctly with human systems. And because pigs live for only about ten years, their organs might not survive a human lifetime. Even using a xenotransplant as a \u2018bridge\u2019 until a suitable human donor is found will be difficult. After a heart transplant, for instance, fibrous tissue forms around the new organ, making second transplants very difficult, Chapman says. Given the long list of known hurdles, the surprise setbacks that researchers encounter along the way can be particularly disheartening. About half an hour after its surgery at the University of Maryland, the baboon with a pig\u2019s lung woke up in a cage wearing a small vest that monitored its vital signs. The lung functioned well overnight and was even able to provide enough oxygen to the animal when blood flow to its other lung was temporarily blocked. But the next day, the animal became ill and had to be killed. That was unexpected, Pierson says, because the pig\u2019s multiple genetic modifications seem to have worked well with the baboon\u2019s immune system. A post-mortem examination revealed that fluid had accumulated in the lung and the organ had developed blood clots. Like so many other aspects of xenotransplantation, Pierson says, \u201cthis is a problem that we are still learning about\u201d. \n                 Tweet \n                 Follow @NatureNews \n               \n                     Gene-editing record smashed in pigs 2015-Oct-06 \n                   \n                     Gene-edited 'micropigs' to be sold as pets at Chinese institute 2015-Sep-29 \n                   \n                     An inside look at the first pig biobank 2015-Mar-25 \n                   \n                     Tissue engineering: How to build a heart 2013-Jul-03 \n                   Reprints and Permissions"},
{"file_id": "527022a", "url": "https://www.nature.com/articles/527022a", "year": 2015, "authors": [{"name": "Linda Geddes"}], "parsed_as_year": "2006_or_before", "body": "A London lab is deploying every technology it can to understand infant brains, and what happens when development goes awry. Baby Ezra is sitting on his mother's lap and staring at the computer screen with the amazement of someone still new to the world. The five-month-old's eyes rest on a series of pictures: three dancing women, four black circles, then a face among random objects. Ezra studies the screen with fascination \u2014 although now and then, his attention wanders. He lets out a gurgle, and moments later, a short cry. He is chewing a sock. Below the screen, a box is shining infrared light at his cornea, and then capturing and processing the reflected light to work out the direction of his gaze. Behind a curtain, postdoc Jannath Begum Ali checks the data streaming in on her monitor. This set-up is part of a sophisticated experiment to understand the early development of the human mind in the Babylab at Birkbeck, University of London. The scientists here will closely monitor Ezra's brain and behaviour at visits over the next two and a half years. Oblivious to his important role in science, Ezra furrows his brow into a frown. What happens next is apparent only to his mother, who turns him around and checks his behind. With just half of a planned 15-minute observation complete, Ezra has defecated. At that point, everyone takes a break. How do you get into the mind of a human being who cannot speak, does not follow instructions and rudely interrupts your experiments? That is the challenge embraced by scientists at the Babylab. The brain undergoes more change during the first two years of life than at any other time: consciousness, traits of personality, temperament and ability all become apparent, as do the first signs that development could be drifting off course. But this period is also the most difficult to explore, because many of the standard tools of human neuroscience are useless: babies will not lie awake and still in an imaging machine, and they cannot answer questions or do as they are told. Researchers have measured infants' interest and attention mostly by tracking their gaze \u2014 but even this method has been criticized as crude. \u201cThere are many studies where someone tries to prove that the baby understands goals, causality, number \u2014 and in 99% of those studies the only measure they look at is a change in looking time,\u201d says Jerome Kagan, a psychologist at Harvard University in Cambridge, Massachusetts. The field is now becoming more sophisticated, thanks in part to the Birkbeck lab. Scientists there have pioneered techniques such as infant near-infrared spectrometry (NIRS), which measures brain activity by recording the colour, and therefore the oxygenation, of blood. They are also trying to strengthen conclusions by combining multiple techniques. Among the handful of baby labs around the world, this makes the London one stand out. \u201cThey are doing research on babies using every single technique you could imagine,\u201d says Richard Aslin, an infant-behaviour researcher and director of the Rochester Center for Brain Imaging in New York. The lab has used such tools to reveal a series of 'firsts' about the infant mind: that babies prefer to look at faces that are looking directly at them, rather than away from them; that they respond to such direct gaze with enhanced neural processing 1 ; and that changes in this brain response may be associated with the later emergence of autism \u2014 the first evidence that a measure of brain function might be used to predict the condition 2 . In 2013, the Babylab started the flagship project of which Ezra is part: an effort to study infants from 12 weeks old who are at high risk of autism spectrum disorder or attention deficit hyperactivity disorder (ADHD), alongside a control group, in order to detect more early signs of these conditions and find behavioural therapies that might help. \u201cIt's an exciting, and emerging, field,\u201d says Mark Johnson, director of the Babylab. And, like its subjects, the London lab is growing up. In 2014, Johnson received \u00a32.3 million (US$3.5 million) from a trio of foundations to establish a toddler lab at Birkbeck, in which children aged 18 months to 3 or 4 years old will be attached to wireless forms of electroencephalography (EEG), NIRS and eye-tracking technology as they walk around, play and interact with other children. The aim is to understand the brain during toddlerhood, the time when children start to appreciate the difference between self and other, complex language develops and long-term memories are first laid down. \u201cIn child development in general, but also in our brain-development work, the terrible twos are a major black hole,\u201d Johnson says. \n               Look and learn \n             There is a well-worn adage in show business that you should never work with children or animals. Johnson built his career doing both. For his PhD project in the 1980s, he investigated whether day-old chicks formed social attachments to any object placed in their pen, or if they preferred ones that resembled a mother hen. (The chicks were particularly drawn to objects with hen-like necks and faces, but weren't too fussy about the rest of their looks 3 .) But Johnson was more interested in human development, so after his PhD he took a research-scientist position in London to begin studying infants. \u201cIn some ways that's not as big a jump as it sounds,\u201d he says. \u201cIn both cases you're trying to develop tasks and get information from non-verbal creatures.\u201d Scientists have been attempting practical research with babies since the middle of the twentieth century. One of the first to do so was Jean Piaget, a Swiss psychologist who used detailed observations of infants and older children to gain insight into how they understand the world \u2014 including, famously, by hiding an object to see whether infants try to find it. He concluded that babies cannot grasp the concept that an object still exists when it is out of sight until they are around eight months old. Piaget went on to develop the theory that babies are essentially born as blank slates, but possess the machinery that motivates them to explore the world and allows them to assimilate knowledge. Infant neuroscience leapt forward in the early 1960s, when the US developmental psychologist Robert Fantz started measuring the amount of time babies spent looking at something as a way to gauge how interested in it they were. Fantz reported that a two-month-old baby spent twice as long looking at a sketch of the human face as at a bullseye, for instance. Experiments based on gaze measurements have been the field's workhorse ever since. \u201cIt is no exaggeration to say that without looking-time measures, we would know very little about nearly any aspect of infant development,\u201d says Aslin. Gaze experiments have led some researchers to conclude that, far from being blank slates, babies are born with an innate appreciation of number and human faces, as well as the ability to recognize when their mother's native language is being spoken \u2014 a familiarity proposed to develop through hearing speech while in the womb. \u201cThere have been literally thousands of experiments done with these looking-time methods,\u201d Aslin says, \u201cand by and large it is a pretty reliable technique; you can have two labs running the same experiment and you get the same results.\u201d But Aslin and Kagan are two of a growing number of researchers who think that such infant studies should be viewed with caution: it can be dangerous to infer too much about the workings of a baby's mind from just their fleeting glance \u2014 and they worry that some labs do not control for confounding factors as well as they should. \u201cLooking time is under the control of so many conditions,\u201d Kagan says. \u201cWhat are the physical features of the stimulus? Are its lines mainly curved or straight? What colours are present? How much contrast in lighting is there?\u201d Babies' brains are growing and developing at an extraordinary pace, which makes comparisons between different ages difficult: a newborn's gaze might reflect innate abilities, but a seven-month-old's will also be influenced by what he or she is starting to learn and remember about the world. \u201cAn infant may look longer in order to relate the event to what it already knows,\u201d says Kagan. \u201cThe main point is that no single measure is able to supply all the evidence required for conclusions about what infants know.\u201d That was the opinion that Johnson quickly reached when he began infant research: the reliance on looking time and observations alone were unsatisfying. He established a baby lab at University College London (UCL) in 1993, and it moved to more spacious premises at Birkbeck in 1998. From the start, Johnson wanted to take a more high-tech approach to investigating brain development than were the handful of other similar labs. In 2005, Johnson and his colleagues combined observations of looking time with electrical measurements of brain activity to investigate Piaget's claim that infants younger than nine months do not understand the permanence of an object that has vanished. When adults view an object disappearing, they tend to show an increase in a particular type of neural oscillation over the right temporal cortex. Johnson, working with colleagues Gergely Csibra and Jordy Kaufman, showed that six-month-old babies show a similar pattern \u2014 suggesting that they do keep hidden objects in mind. The same pattern was not observed when the object disintegrated instead of being hidden 4 . Studies such as these have convinced Johnson that babies are not born blank slates, but neither do they possess adult-like concepts about things like number. \u201cMy work, I think, goes for a middle ground,\u201d he says. He argues that the newborn has basic attention preferences for things such as faces and speech, and that these preferences shape the brain as it develops 5 . Johnson's observation that young babies prefer direct eye contact is one such example; this sets them up to focus on socially relevant parts of their surroundings, which in turn enables them to learn about language and other social cues such as facial expressions. \n               Happy baby \n             Working with babies requires specialized kit \u2014 particularly for a laboratory that can see as many as 14 in a day. The Babylab kitchen hosts a bottle-warmer, and bathrooms are well stocked with wet-wipes. The waiting room is brightly decorated and scattered with easy-to-clean toys. The laboratories, however, are largely empty and painted a dull battleship grey \u2014 a deliberate choice, because babies are easily distracted. \u201cWe try to make it as boring as possible, except for the thing we need them to focus on,\u201d says Leslie Tucker, coordinator of the Centre for Brain and Cognitive Development, of which the Babylab is part. Hungry or tired babies do not make for good experiments, so everything is carefully planned around meals and naps. In the waiting room, Caitlin \u2014 a four-month-old in stripy blue dungarees \u2014 is receiving a last-minute breastfeed before being ushered into a lab. She is participating in a study to assess the development of mimicry in babies: the unconscious tendency of people to frown when someone else frowns, or smile when they smile. \u201cMimicry serves important social functions in adults and has even been suggested to be the 'social glue' that binds us together,\u201d says Carina de Klerk, who is leading that study at Birkbeck. But very little is known about how, and when, it develops. Some researchers think that it is something babies are born with \u2014 newborns have been observed to stick their tongues out in response to an adult doing the same 6 . But \u201cit's not clear if the baby is actually copying, or perhaps they just stick out their tongue whenever something exciting happens\u201d, de Klerk says. She sings to baby Caitlin while sticking electrodes on her temples, cheeks and under her chin. The baby seems unsure, so a research assistant appears, brandishing a garish musical telephone. The art of distraction is a fundamental skill that anyone working in a baby lab must quickly master. \u201cResearchers from other fields come down here and are often horrified at the lack of controls,\u201d says Tucker. \u201cYou're going to interrupt the experiment if you have to, or make noises to distract them if they look like they're going to cry.\u201d It works: Caitlin is now cooing and smiling. The researchers pause for a moment, while Caitlin's mother takes a photo of her \u201cscience baby\u201d on her phone. Then Caitlin is shown a series of video sequences of a woman raising her eyebrows or opening and closing her mouth, interspersed with static pictures of farm animals. The mimicry experiment is a prime example of the Babylab's mixed-methods approach. Baby Caitlin stares intently at the screen; she does not seem to be copying the woman's actions. But the electrodes on her face may tell a different story: the technique, called electromyography (EMG), picks up electrical activity in her facial muscles, which will indicate if Caitlin is activating her eyebrow area \u2014 even if she is not overtly moving it \u2014 in response to the woman raising hers. Later in the day, Caitlin is shown the same video sequence while hooked up to NIRS. NIRS is transforming the ability of researchers to peer into the minds of babies. It was originally adopted by medical physicists at UCL as a technique to help predict the risk of stroke in premature babies. They then began working with Birkbeck researchers to adapt it to answer more fundamental questions 7 . By tracking the flow of oxygenated blood, NIRS allows scientists to see which brain areas become more active in response to external events. For instance, a 2009 study from the Babylab revealed that the brains of five-month-olds already show an adult-like pattern of activation in response to social stimuli, such as a woman playing peek-a-boo with them 8 . In the mimicry study, the researchers want to see if the babies' brains show a similar pattern to those of adults who are mimicking others, which should help to explain if mimicry is partly innate. But NIRS is not perfect, in part because it cannot measure what is happening in important inner brain regions such as the hippocampus or the amygdala. \u201cThe brain is a complex connected circuit. If you only measure a superficial part of that circuit, you can come to the wrong conclusions,\u201d Kagan says. To assess these deeper areas, researchers need a technique such as functional magnetic resonance imaging (fMRI), which has yielded huge insight into the adult brain. But fMRI is highly sensitive to movement, so babies can be scanned only if they are sedated or asleep, which has severely limited the technique's use. \n               An eye on autism \n             Looking time remains an important tool at Birkbeck and elsewhere \u2014 although these days, it is assessed not by human observation but by precise eye-tracking technology, such as that being used on baby Ezra. Ezra is a control for the autism and ADHD study: he does not have an older sibling with one of the disorders, so is not considered at high risk. As his attention flits between the apparently random objects on the screen, the reflected infrared light allows psychologist Emily Jones \u2014 who directs the project \u2014 to gauge precisely what he is looking at, and in which order. \u201cWhat we tend to find is that typically developing babies will always look first, and longer, at the face, before looking at the other objects,\u201d she says. Autism and ADHD have become a major focus of the Babylab as the prevalence and awareness of the conditions have risen in the past two decades \u2014 they are now believed to affect around 4% of the UK population. Last year, in a study of 104 infants, the Birkbeck team showed that infants at high risk of autism were drawn towards the face first, but they seemed to spend less time overall than 'neurotypical' babies in looking at any of the objects \u2014 and those that went on to develop autism had the shortest looking time of all 9 . A separate eye-tracking study published by the group earlier this year revealed that nine-month-olds who went on to develop symptoms of autism were more likely to spot the odd-one-out among a group of letters on a screen 10 . It is not completely clear why this is, but the working hypothesis is that these infants are more attentive to the details of what they see, says Teodora Gliga, who led the odd-one-out study. The downside of this could be that children who go on to develop autism find it harder to draw general conclusions about what they are seeing, she says. The study of which Ezra is part aims to extend this work by collecting more-detailed measures from over 400 families \u2014 and to identify those features that are strongly associated with the later onset of a developmental disorder. During the five visits that Ezra will make to the Babylab as he grows up, he will be tested using EEG, NIRS and EMG, and his parents will be given extensive questionnaires to assess his language skills, social development, temperament and sleeping patterns. The team hopes that early brain differences could some day provide indicators \u2014 or biomarkers \u2014 of autism, which isn't usually diagnosed until close to a child's third birthday. They also hope to find ways to steer brain development back towards a more typical course. One clinical trial at the Babylab already suggests that early intervention can have an effect. Babies in 28 families with an older sibling with autism were randomly assigned to a group in which they were visited by a therapist at least six times between the ages of seven and ten months, and were compared with a group of high-risk babies who received no therapy. The therapist showed parents videos of them interacting with their child to help understand how their baby was trying to communicate with them, and how to respond. After five months, the team saw hints of improvements in the babies' engagement, attention and social behaviour, compared with controls. But the team acknowledged that many of the results had wide confidence intervals and that it is too early to say whether the intervention will have long-term effects 11 . Johnson hopes that investigations in the toddler lab, when they start, might also eventually find a practical use, helping researchers to devise ways to boost cognitive, attention and memory skills. \u201cI believe we are now at a unique point of convergence between this basic science and the clinical science,\u201d he says. Meanwhile, the techniques continue to evolve. Jones is currently piloting 'gaze-contingent' tasks, which enable babies to become active participants in experiments. \u201cIf they can focus their attention on a butterfly flying across the screen, and not get distracted by other things that are happening, then the butterfly keeps flying, so they get rewarded for controlling their attention,\u201d Jones says. A more distant goal is to develop ways of using fMRI so that it could be used on awake babies. And there are still so many questions that demand answers. How do differences in the temperaments of babies develop into more complex personality traits as children age? And why can't people remember their earliest months and years? Baby Ezra will certainly not remember his day in the lab. By late afternoon, his mother is tucking him into the pushchair for his journey home \u2014 a 1-hour 45-minute journey to Bristol by train. The trip was worth it, she says, because she was curious to learn what goes on at the Babylab. \u201cI was interested in how Ezra would respond, but also in why those tasks were being done,\u201d she says. Ezra and his mother now have souvenirs of their day: some photos, a certificate of participation and a baby-sized T-shirt. \u201cI'm an infant scientist,\u201d it reads. \n                 Tweet \n                 Follow @NatureNews \n               \n                     Poverty shrinks brains from birth    2015-Mar-30 \n                   \n                     Neuroscience: The brain, interrupted 2015-Feb-03 \n                   \n                     Autism symptoms seen in babies 2013-Nov-06 \n                   \n                     Babies learn to babble like birds learn to sing 2013-May-29 \n                   \n                     Babies' brains may be tuned to language before birth 2013-Feb-25 \n                   \n                     Nature  special: The autism enigma \n                   \n                     Birkbeck Babylab \n                   Reprints and Permissions"},
{"file_id": "527148a", "url": "https://www.nature.com/articles/527148a", "year": 2015, "authors": [{"name": "Lauren Morello"}], "parsed_as_year": "2006_or_before", "body": "Social media is shaking up how scientists talk about gender issues. When Fiona Ingleby took to Twitter last April to vent about a journal\u2019s peer-review process, she didn\u2019t expect much of a response. With only around 100 followers on the social-media network, Ingleby \u2014 an evolutionary geneticist at the University of Sussex near Brighton, UK \u2014 guessed that she might receive a few messages of support or commiseration from close colleagues. What she got was an overwhelming wave of reaction. In four pointed tweets, Ingleby detailed her frustration with a  PLoS ONE  reviewer who tried to explain away her findings on gender disparities in the transition from PhD to postdoc. He suggested that men had \u201cmarginally better health and stamina\u201d, and that adding \u201cone or two male biologists\u201d as co-authors would improve the analysis. The response was a full-fledged \u2018Twitterstorm\u2019 that spawned more than 5,000\u00a0retweets, a popular hashtag \u2014 #addmaleauthorgate \u2014 and a public apology from the journal. \u201cThings went really mental,\u201d Ingleby says. \u201cI had to turn off the Twitter notifications on my e-mail.\u201d Yet her experience is not as unusual as it may seem. Social media has enabled an increasingly public discussion about the persistent problem of sexism in science. When a male scientist with the European Space Agency (ESA) wore a shirt patterned with half-naked women to a major media event in November 2014, Twitter blazed with criticism. The site was where the first reports surfaced in June of Nobel Prizewinning biologist Tim Hunt\u2019s self-confessed \u201ctrouble with girls\u201d in laboratories. And in mid-October, many astronomers took to Twitter to register their anger and disappointment when the news broke that Geoffrey Marcy, an exoplanet hunter at the University of California, Berkeley,  was found to have sexually harassed female subordinates for at least a decade . \u201cI have been in [the] field for 15 years,\u201d  wrote  Sarah H\u00f6rst, a planetary scientist at Johns Hopkins University in Baltimore, Maryland. \u201cIt is my field now too & we are not going to do things this way anymore if I have anything to do w/ it.\u201d Scientists studying the rise of social media are still trying to understand the factors that can whip an online debate into a raging Twitterstorm. Such events often have far-reaching and unpredictable consequences \u2014 for participants as well as targets. Sometimes this continuing public discussion prompts action:  PLoS ONE  is re-reviewing Ingleby\u2019s paper, and  its original editor and reviewer no longer work for the journal , for example. But women who speak out about sexism often face a vicious backlash, ranging from insults to threats of physical violence. Although it is not yet clear whether the social-media conversation about sexism in science will help to create lasting change, some scientists think that it may provide a sense of solidarity for women across disciplines. \u201cYou may not be changing minds, but you may be finding people who have your back,\u201d says Brooke Foucault Welles, a communications scientist at Northeastern University in Boston, Massachusetts. \u201cAnd that\u2019s powerful.\u201d \n               Brave new world \n             On 12 November 2014, the ESA Rosetta mission  landed a spacecraft on a comet  \u2014 a milestone for space exploration. But in certain corners of the Internet, Rosetta\u2019s landing day may be best remembered for the scantily clad women on Matt Taylor\u2019s shirt. Taylor, a Rosetta project scientist, sported the Hawaiian-style garment as he gave interviews to reporters at mission headquarters in Darmstadt, Germany, and answered questions on an ESA webcast. (His comments were also suggestive: Rosetta \u201cis sexy, but I never said she was easy\u201d, he told viewers.) It wasn\u2019t long before people following the historic comet landing took notice \u2014 and took to Twitter. \u201cWhat a lost opportunity to encourage girls into science,\u201d  tweeted  Fernanda Foertter, a computer programmer at Oak Ridge National Laboratory in Tennessee. Others approached it with a bit more snark: \u201cNo no women are toooootally welcome in our community, just ask the dude in the shirt,\u201d  wrote  New York-based science journalist Rose Eveleth, who linked to a  Nature  video interview with Taylor . What started as a trickle of tweets soon became a flood. By 14\u00a0November, the day that Taylor gave a tearful public apology on another ESA webcast, Twitter users had posted more than 3,100 messages using the #shirtstorm hashtag (see \u2018Anatomy of a Twitterstorm\u2019). In many ways, #shirtstorm and other Twitter conversations about sexism are not new. It is only the venue that has changed, says Hope Jahren, a geobiologist at the University of Hawaii at Manoa who is active on Twitter. \u201cGuys have been wearing girly shirts forever,\u201d she says. \u201cThe women around them have been rolling their eyes and going home and saying, \u2018What a buffoon. I\u2019m so sick of this crap.\u2019 They\u2019ve been doing it in the women\u2019s room and doing it in the coffee room.\u201d But now, Jahren says, \u201cTwitter is that thought under your breath.\u201d The social-media service is also an enormous megaphone that claims to have 320\u00a0million active users each month. Research suggests that hashtag-driven Twitter conversations can help to amplify the voices of people who are not powerful by conventional measures. One example comes from Foucault Welles and her colleagues\u2019 analysis of a hashtag that arose after police in Ferguson, Missouri, shot an unarmed African American teenager in August 2014. The killing quickly became a national news story, and the #ferguson hashtag became part of a broader US debate over police violence. Yet more than a year later, one of the most retweeted #ferguson contributors was a teenager from the Ferguson area. \u201cPeople who don\u2019t have power really can have their voices heard,\u201d Foucault Welles says. \u201cThey can reframe the story.\u201d And that can make Twitter an important outlet for younger scientists, who often don\u2019t know how to respond to instances of sexism or sexual harassment. One 2014 survey of 666 scientists \u2014 including 516 women \u2014 found that 64% had experienced sexual harassment, and only 20% of that group said that they knew how to report such behaviour 1 . Most were students or postdoctoral researchers at the time they were harassed. When scientists talk about sexism and harassment on Twitter, it presents younger researchers with a model for confronting such issues. \u201cThis way, they can see other people are going through it, and there is a positive effect to speaking out,\u201d says Zuleyka Zevallos, a sociologist who manages the Science Australia Gender Equity Project at the Australian Academy of Science in Canberra. For Ingleby, venting about her sexist journal review on Twitter paid unexpected dividends. She and her co-author, both postdocs, had waited three weeks for  PLoS ONE  to decide whether to grant their appeal and re-examine their paper. By making their plight public, Ingleby drew public support from other scientists \u2014 and, privately, invaluable advice from more-experienced researchers about how to deal with the journal. \u201cI did get some messages that called me a feminazi and all that stuff,\u201d Ingleby says, \u201cbut that was by far the minority.\u201d She has one crucial piece of advice for those who may follow in her footsteps: \u201cBe a bit more prepared for things going viral. Maybe pick a few quiet days in your calendar.\u201d \n               In the eye of the hurricane \n             Determining which factors can fan a handful of messages into an Internet firestorm, or what gives a hashtag staying power, is tricky. One study 2 , published in 2012 by researchers at the University of Pennsylvania in Philadelphia, suggests that Internet content goes viral when it elicits a strong emotional reaction. Marketing researcher Jonah Berger and decision scientist Katherine Milkman analysed the popularity of 6,956 news stories posted to the  New York Times  homepage between 30 August and 30 November 2008. The pair found that stories that inspired intense positive emotions, such as awe or amusement, were the most likely to go viral; anger, anxiety and other strong negative feelings also propelled articles to wide readership, but sadness seemed to reduce the chance that a reader would share a story with others. The recent science Twitterstorms, which are often fuelled by a combination of frustration, anger and black humour, fit with those ideas. Yet an element of randomness is also at play. Joseph Reagle, a communications researcher at Northeastern University, sees this in the  story of Cecil , a lion killed by an American tourist in Hwange National Park in Zimbabwe in July. The animal\u2019s death became an international cause c\u00e9l\u00e8bre, inspiring a hashtag (#CeciltheLion) that racked up 1.2 million tweets in one month \u2014 despite the fact that hunters kill dozens of lions in Zimbabwe each year 3 . To Reagle, Cecil\u2019s tale also suggests that \u2018hashtag activism\u2019 is here to stay. \u201cWe are seeing the emergence of a genre,\u201d he says. \u201cAnd we will see it repeated.\u201d The conversations sparked by popular hashtags can shift the focus of media coverage and broader public discussion. The #YesAllWomen hashtag began in May 2014, in response to a shooting spree in California in which the killer said that his motivation was a hatred of women. Women used the hashtag to connect this violent misogyny to examples of everyday sexism and harassment \u2014 giving rise to a new wave of media coverage 4 . \u201cThat\u2019s one of the really interesting things that starts to happen with some hashtags \u2014 they become news in their own right,\u201d says Samantha Thrift, a feminist media scholar at the University of Calgary in Canada. Hunt learned about the amplifying power of social media the hard way on 8 June. \u201cYou fall in love with them, they fall in love with you, and when you criticize them, they cry,\u201d he said in a speech at the World Conference of Science Journalists in Seoul. His comments were tweeted by audience members, creating an Internet furore that  quickly hit mainstream news outlets . On 10 June, Hunt told BBC Radio 4 that he was \u201creally sorry\u201d. But in later comments to  The Observer , he said that he had been \u201chung out to dry\u201d and forced to resign an honorary post at University College London. \u201cIt has done me lasting damage,\u201d he said. \u201cWhat they did was unacceptable.\u201d For those in positions of power, such as Hunt, finding themselves at the centre of a Twitterstorm can be deeply unsettling, given social media\u2019s ability to upend traditional hierarchies. But many women who talk about sexism, feminism and gender issues online face a harsher reception, from abusive comments to threats of physical harm. When Eveleth tweeted her criticism of Taylor\u2019s shirt, she received death threats. When others joined the fray, such as Jacquelyn Gill, a palaeoecologist at the University of Maine in Orono, they became targets, too. \u201cI stand with @roseveleth and others who are calling out sexism despite online harassment,\u201d she  tweeted . \u201cI\u2019m reporting abusive tweets as I\u2019m able.\u201d She added: \u201cFree-speech apparently only applies to dudes threatening violence to women with an opinion \u2014 not the women with an opinion. #shirtstorm\u201d. The reaction to her commentary was swift and punishing. \u201cFor the next 72 hours I got death and rape threats,\u201d Gill says. \u201cIt was a non-stop barrage of people trolling those hashtags.\u201d As the stream of vitriol became overwhelming, some of Gill\u2019s colleagues wrote a computer program to scan Twitter for threatening messages that mentioned her username. That spared Gill from constantly monitoring her account for serious threats. But no program could spare her from the awkward conversations that she had with University of Maine officials after realizing that some of her harassers on Twitter were discussing how to get her fired in retaliation for her \u2018Shirtgate\u2019 activism. \u201cI\u2019ve run up against the real-world consequences of speaking as a woman on the Internet,\u201d she says. This problem is not limited to science: in a study of 2,849 Internet users, the Pew Research Center in Washington DC reported that 40% had been harassed online. Although men are more likely to be called offensive names or purposefully embarrassed, women are more likely to be stalked or sexually harassed as a result of their Internet use. The survey also found that social media is the place where women are most vulnerable to harassment of all types, ranging from stalking to physical threats. Faced with such attacks, some scientists have begun to rethink how they participate in online discussions about sexism. Some retreat entirely; others, wary of being silenced by abuse, try to find safer ways to engage online. One female researcher who has suffered Internet harassment now tweets about feminist issues under a pseudonym while also maintaining an active Twitter account under her real name. \u201cIt makes me feel safer,\u201d says the researcher, who asked not to be named. \u201cAlthough, in a lot of these cases, if someone wants to find you, they will.\u201d \n               Making sense of a moving target \n             Researchers tracking the rise of social media are trying to understand whether intense discussions online translate into real-world change. The difficulty lies in deciding how to measure such effects. One approach draws on network analysis. A team of computer scientists at Carnegie Mellon University in Pittsburgh, Pennsylvania, tracked Twitter users\u2019 interactions before, during and after 20 Twitterstorms between 2011 and 2014 \u2014 most centred on targets of broad interest, such as US late-night television host Stephen Colbert and fast-food chain McDonald\u2019s 5 . The researchers found that these events did not create lasting links between participants, as measured by who these users follow or message on Twitter. This suggests that Internet dust-ups do not usually lead to sustained discussion or greater awareness of a given issue. But other studies show that intense Twitter discussions may affect contributors in ways that are harder to quantify. Mindi Foster, a social psychologist at Wilfrid Laurier University in Waterloo, Canada, decided to investigate the psychological effects of tweeting on the basis of her own experience using social media. After hearing an anti-Semitic remark on a television programme one night, Foster joined Twitter to vent her anger \u2014 and it felt good. Nature\u2019s Lauren Morello tells Adam Levy how Twitter is helping academics to call out sexism. Foster\u2019s research seems to confirm her hunch: that when women tweet about sexism, it improves their sense of well-being 6 . The study involved 93 female university students who were presented with information about sexism in academia, politics and the media. One group of students was asked to tweet publicly about what they had learned, another to tweet privately and a third to tweet about the weather. (A fourth group was told to do nothing.) During the three-day study, each participant filled out a daily questionnaire on her emotional state. Those who were assigned to tweet publicly reported a greater sense of well-being, on average, by the end of the experiment; those in the other groups showed no change. These results, although preliminary, are in line with earlier research that shows that expressive writing \u2014 such as writing in a diary \u2014 can provide similar benefits. But Foster speculates that public tweeting may confer an extra boost because it spurs writers to think more deeply about what they are saying. Twitter can also help to build a sense of community among scientists in different disciplines who are confronting sexism and sexual harassment. Sometimes these bonds grow out of dark humour, such as the #distractinglysexy hashtag birthed in reaction to Hunt\u2019s comments. Thousands of female researchers posted pictures of themselves in labs and at field sites, up to their knees in mud or swathed in shapeless biosafety suits. \u201cFilter mask protects me from hazardous chemicals and muffles my woman cries,\u201d  wrote  Amelia Cervera, a biochemist at the University of Valencia in Spain, who shared a photo of herself wearing the face-obscuring gear. Gill, a palaeoecologist, says that she has begun to connect with researchers in astronomy, anthropology, engineering and computer science, among other fields. Such links can help researchers to learn from each other\u2019s experiences of confronting sexism. \u201cSome of our disciplines have been better at gender equality than others,\u201d she notes. \u201cSome of us have been having these discussions for a long time.\u201d But the ongoing Twitter conversation about sexism is also limited in some important ways. It often ignores the concerns of women whose experiences with sexism are exacerbated by discrimination on the basis of race, sexual orientation or disability. For example, a US survey of 557\u00a0female scientists from ethnic minority groups found that two-thirds felt pressure to prove themselves over and over again \u2014 beyond what was asked of white colleagues. And 48% of African American respondents said that they had been mistaken for janitors (caretakers) or administrative staff in their workplaces. \u201cIf you are a minority within a minority, you are actually dealing with multiple problems,\u201d says Zevallos. That is just as true on Twitter as it is in the lab or office. And this can make women who are dealing with the effects of multiple forms of discrimination feel excluded from conversations that focus on sexism or sexual harassment. Such concerns surfaced recently in the wake of the Marcy sexual-harassment case, which had prompted a vigorous online debate under the #astroSH hashtag. \u201cIf you are not talking about and confronting racism with same vigilance as sexism, might as well hang \u2018no Blacks\u2019 signs,\u201d  tweeted  Chanda Prescod-Weinstein, an astrophysicist at the Massachusetts Institute of Technology (MIT) in Cambridge. \u201cAnd I say that as a victim of both sexual assault and sexual harassment.\u201d Sarah Ballard, also an MIT astrophysicist,  echoed the sentiment : \u201cWe can\u2019t rely on crowdsourcing meting out of justice- (Mostly white) crowds will stand up for white women, *crickets* otherwise.\u201d And although social media can help to create a community discussion about sexism and other forms of discrimination, fighting for equality requires the real-world cooperation of universities, governments and other institutions. Some of these have taken action in response to sexist incidents that online discussions helped to bring to wider attention. But although Twitter may be hard to ignore, it does not have the authority to set and enforce expectations for fair treatment. Despite those caveats, Thrift finds great value in the ongoing social-media conversations among scientists, which she sees as a form of public education \u2014 and the first step towards concrete change. \u201cThat\u2019s hugely important,\u201d she says. \u201cIf we don\u2019t name something as sexist, as harassment, as misogyny, it will continue unchecked.\u201d\n \n                 Tweet \n                 Facebook \n                 LinkedIn \n                 Weibo \n                 Wechat \n               This article appeared in print under the title  We can # it. \n                     \u2018Pretty curious\u2019 campaign draws criticism 2015-Oct-01 \n                   \n                     Universities highlight gender-equality policies after sexism row 2015-Jul-10 \n                   \n                     Sexism has no place in science 2015-Jun-15 \n                   \n                     Sexist review causes Twitter storm 2015-May-01 \n                   \n                     Inequality quantified: Mind the gender gap 2013-Mar-06 \n                   \n                     Nature  Special: Women in science \n                   \n                     Joseph Reagle \n                   \n                     Brooke Foucault Welles \n                   \n                     Samantha Thrift \n                   \n                     Mindi Foster \n                   Reprints and Permissions"},
{"file_id": "526178a", "url": "https://www.nature.com/articles/526178a", "year": 2015, "authors": [{"name": "Davide Castelvecchi"}], "parsed_as_year": "2006_or_before", "body": "A Japanese mathematician claims to have solved one of the most important problems in his field. The trouble is, hardly anyone can work out whether he's right. Sometime on the morning of 30 August 2012, Shinichi Mochizuki quietly posted four papers on his website. The papers were huge \u2014 more than 500 pages in all \u2014 packed densely with symbols, and the culmination of more than a decade of solitary work. They also had the potential to be an academic bombshell. In them,  Mochizuki claimed to have solved the  abc  conjecture , a 27-year-old problem in number theory that no other mathematician had even come close to solving. If his proof was correct, it would be one of the most astounding achievements of mathematics this century and would completely revolutionize the study of equations with whole numbers. Mochizuki, however, did not make a fuss about his proof. The respected mathematician, who works at Kyoto University's Research Institute for Mathematical Sciences (RIMS) in Japan, did not even announce his work to peers around the world. He simply posted the papers, and waited for the world to find out. Probably the first person to notice the papers was Akio Tamagawa, a colleague of Mochizuki's at RIMS. He, like other researchers, knew that Mochizuki had been working on the conjecture for years and had been finalizing his work. That same day, Tamagawa e-mailed the news to one of his collaborators, number theorist Ivan Fesenko of the University of Nottingham, UK. Fesenko immediately downloaded the papers and started to read. But he soon became \u201cbewildered\u201d, he says. \u201cIt was impossible to understand them.\u201d Fesenko e-mailed some top experts in Mochizuki's field of arithmetic geometry, and word of the proof quickly spread. Within days, intense chatter began on mathematical blogs and online forums (see  Nature   http://doi.org/725 ; 2012 ). But for many researchers, early elation about the proof quickly turned to scepticism. Everyone \u2014 even those whose area of expertise was closest to Mochizuki's \u2014 was just as flummoxed by the papers as Fesenko had been. To complete the proof, Mochizuki had invented a new branch of his discipline, one that is astonishingly abstract even by the standards of pure maths. \u201cLooking at it, you feel a bit like you might be reading a paper from the future, or from outer space,\u201d number theorist Jordan Ellenberg, of the University of Wisconsin\u2013Madison, wrote on his blog a few days after the paper appeared. Three years on, Mochizuki's proof remains in mathematical limbo \u2014 neither debunked nor accepted by the wider community. Mochizuki has estimated that it would take a maths graduate student about 10 years to be able to understand his work, and Fesenko believes that it would take even an expert in arithmetic geometry some 500 hours. So far, only four mathematicians say that they have been able to read the entire proof. Reporter Adam Levy learns about the maths proof that\u2019s proven too complex to be verified. Adding to the enigma is Mochizuki himself. He has so far lectured about his work only in Japan, in Japanese, and despite being fluent in English, he has declined invitations to talk about it elsewhere. He does not speak to journalists; several requests for an interview for this story went unanswered. Mochizuki has replied to e-mails from other mathematicians and been forthcoming to colleagues who have visited him, but his only public input has been sporadic posts on his website. In December 2014, he wrote that to understand his work, there was a \u201cneed for researchers to deactivate the thought patterns that they have installed in their brains and taken for granted for so many years\u201d. To mathematician Lieven Le Bruyn of the University of Antwerp in Belgium, Mochizuki's attitude sounds defiant. \u201cIs it just me,\u201d he wrote on his blog earlier this year, \u201cor is Mochizuki really sticking up his middle finger to the mathematical community\u201d. Now, that community is attempting to sort the situation out. In December, the first workshop on the proof outside of Asia will take place in Oxford, UK. Mochizuki will not be there in person, but he is said to be willing to answer questions from the workshop through Skype. The organizers hope that the discussion will motivate more mathematicians to invest the time to familiarize themselves with his ideas \u2014 and potentially move the needle in Mochizuki's favour. In his latest verification report, Mochizuki wrote that the status of his theory with respect to arithmetic geometry \u201cconstitutes a sort of faithful miniature model of the status of pure mathematics in human society\u201d. The trouble that he faces in communicating his abstract work to his own discipline mirrors the challenge that mathematicians as a whole often face in communicating their craft to the wider world. \n               Primal importance \n             The  abc  conjecture refers to numerical expressions of the type  a  +  b  =  c . The statement, which comes in several slightly different versions, concerns the prime numbers that divide each of the quantities  a ,  b  and  c . Every whole number, or integer, can be expressed in an essentially unique way as a product of prime numbers \u2014 those that cannot be further factored out into smaller whole numbers: for example, 15 = 3 \u00d7 5 or 84 = 2 \u00d7 2 \u00d7 3 \u00d7 7. In principle, the prime factors of  a  and  b  have no connection to those of their sum,  c . But the  abc  conjecture links them together. It presumes, roughly, that if a lot of small primes divide  a  and  b  then only a few, large ones divide  c . This possibility was first mentioned in 1985, in a rather off-hand remark about a particular class of equations by French mathematician Joseph Oesterl\u00e9 during a talk in Germany. Sitting in the audience was David Masser, a fellow number theorist now at the University of Basel in Switzerland, who recognized the potential importance of the conjecture, and later publicized it in a more general form. It is now credited to both, and is often known as the Oesterl\u00e9\u2013Masser conjecture. A few years later, Noam Elkies, a mathematician at Harvard University in Cambridge, Massachusetts, realized that the  abc  conjecture, if true, would have profound implications for the study of equations concerning whole numbers \u2014 also known as Diophantine equations after Diophantus, the ancient-Greek mathematician who first studied them. Elkies found that a proof of the  abc  conjecture would solve a huge collection of famous and unsolved Diophantine equations in one stroke. That is because it would put explicit bounds on the size of the solutions. For example,  abc  might show that all the solutions to an equation must be smaller than 100. To find those solutions, all one would have to do would be to plug in every number from 0 to 99 and calculate which ones work. Without  abc , by contrast, there would be infinitely many numbers to plug in. Elkies's work meant that the  abc  conjecture could supersede the most important breakthrough in the history of Diophantine equations: confirmation of a conjecture formulated in 1922 by the US mathematician Louis Mordell, which said that the vast majority of Diophantine equations either have no solutions or have a finite number of them. That conjecture was proved in 1983 by German mathematician Gerd Faltings, who was then 28 and within three years would win a Fields Medal, the most coveted mathematics award, for the work. But if  abc  is true, you don't just know how many solutions there are, Faltings says, \u201cyou can list them all\u201d. Soon after Faltings solved the Mordell conjecture, he started teaching at Princeton University in New Jersey \u2014 and before long, his path crossed with that of Mochizuki. Born in 1969 in Tokyo, Mochizuki spent his formative years in the United States, where his family moved when he was a child. He attended an exclusive high school in New Hampshire, and his precocious talent earned him an undergraduate spot in Princeton's mathematics department when he was barely 16. He quickly became legend for his original thinking, and moved directly into a PhD. People who know Mochizuki describe him as a creature of habit with an almost supernatural ability to concentrate. \u201cEver since he was a student, he just gets up and works,\u201d says Minhyong Kim, a mathematician at the University of Oxford, UK, who has known Mochizuki since his Princeton days. After attending a seminar or colloquium, researchers and students would often go out together for a beer \u2014 but not Mochizuki, Kim recalls. \u201cHe's not introverted by nature, but he's so much focused on his mathematics.\u201d Faltings was Mochizuki's adviser for his senior thesis and for his doctoral one, and he could see that Mochizuki stood out. \u201cIt was clear that he was one of the brighter ones,\u201d he says. But being a Faltings student couldn't have been easy. \u201cFaltings was at the top of the intimidation ladder,\u201d recalls Kim. He would pounce on mistakes, and when talking to him, even eminent mathematicians could often be heard nervously clearing their throats. Faltings's research had an outsized influence on many young number theorists at universities along the US eastern seaboard. His area of expertise was algebraic geometry, which since the 1950s had been transformed into a highly abstract and theoretical field by Alexander Grothendieck \u2014 often described as  the greatest mathematician of the twentieth century . \u201cCompared to Grothendieck,\u201d says Kim, \u201cFaltings didn't have as much patience for philosophizing.\u201d His style of maths required \u201ca lot of abstract background knowledge \u2014 but also tended to have as a goal very concrete problems. Mochizuki's work on  abc  does exactly this\u201d. \n               Single\u2014track mind \n             After his PhD, Mochizuki spent two years at Harvard and then in 1994 moved back to his native Japan, aged 25, to a position at RIMS. Although he had lived for years in the United States, \u201che was in some ways uncomfortable with American culture\u201d, Kim says. And, he adds, growing up in a different country may have compounded the feeling of isolation that comes from being a mathematically gifted child. \u201cI think he did suffer a little bit.\u201d Mochizuki flourished at RIMS, which does not require its faculty members to teach undergraduate classes. \u201cHe was able to work on his own for 20 years without too much external disturbance,\u201d Fesenko says. In 1996, he boosted his international reputation when he solved a conjecture that had been stated by Grothendieck; and in 1998, he gave an invited talk at the International Congress of Mathematicians in Berlin \u2014 the equivalent, in this community, of an induction to a hall of fame. But even as Mochizuki earned respect, he was moving away from the mainstream. His work was reaching higher levels of abstraction and he was writing papers that were increasingly impenetrable to his peers. In the early 2000s he stopped venturing to international meetings, and colleagues say that he rarely leaves the Kyoto prefecture any more. \u201cIt requires a special kind of devotion to be able to focus over a period of many years without having collaborators,\u201d says number theorist Brian Conrad of Stanford University in California. Mochizuki did keep in touch with fellow number theorists, who knew that he was ultimately aiming for  abc . He had next to no competition: most other mathematicians had steered clear of the problem, deeming it intractable. By early 2012, rumours were flying that Mochizuki was getting close to a proof. Then came the August news: he had posted his papers online. The next month, Fesenko became the first person from outside Japan to talk to Mochizuki about the work he had quietly unveiled. Fesenko was already due to visit Tamagawa, so he went to see Mochizuki too. The two met on a Saturday in Mochizuki's office, a spacious room offering a view of nearby Mount Daimonji and with neatly arranged books and papers. It is \u201cthe tidiest office of any mathematician I've ever seen in my life\u201d, Fesenko says. As the two mathematicians sat in leather armchairs, Fesenko peppered Mochizuki with questions about his work and what might happen next. Fesenko says that he warned Mochizuki to be mindful of the experience of another mathematician: the Russian topologist  Grigori Perelman , who shot to fame in 2003 after solving the century-old Poincar\u00e9 conjecture (see  Nature   427 , 388; 2004 ) and  then retreated and became increasingly estranged  from friends, colleagues and the outside world. Fesenko knew Perelman, and saw that the two mathematicians' personalities were very different. Whereas Perelman was known for his awkward social skills (and for letting his fingernails grow unchecked), Mochizuki is universally described as articulate and friendly \u2014 if intensely private about his life outside of work. Normally after a major proof is announced, mathematicians read the work \u2014 which is typically a few pages long \u2014 and can understand the general strategy. Occasionally, proofs are longer and more complex, and years may then pass for leading specialists to fully vet it and reach a consensus that it is correct. Perelman's work on the Poincar\u00e9 conjecture became accepted in this way. Even in the case of Grothendieck's highly abstract work, experts were able to relate most of his new ideas to mathematical objects they were familiar with. Only once the dust has settled does a journal typically publish the proof. But almost everyone who tackled Mochizuki's proof found themselves floored. Some were bemused by the sweeping \u2014 almost messianic \u2014 language with which Mochizuki described some of his new theoretical instructions: he even called the field that he had created 'inter-universal geometry'. \u201cGenerally, mathematicians are very humble, not claiming that what they are doing is a revolution of the whole Universe,\u201d says Oesterl\u00e9, at the Pierre and Marie Curie University in Paris, who made little headway in checking the proof. The reason is that Mochizuki's work is so far removed from anything that had gone before. He is attempting to reform mathematics from the ground up, starting from its foundations in the theory of sets (familiar to many as Venn diagrams). And most mathematicians have been reluctant to invest the time necessary to understand the work because they see no clear reward: it is not obvious how the theoretical machinery that Mochizuki has invented could be used to do calculations. \u201cI tried to read some of them and then, at some stage, I gave up. I don't understand what he's doing,\u201d says Faltings. Fesenko has studied Mochizuki's work in detail over the past year, visited him at RIMS again in the autumn of 2014 and says that he has now verified the proof. (The other three mathematicians who say they have corroborated it have also spent considerable time working alongside Mochizuki in Japan.) The overarching theme of inter-universal geometry, as Fesenko describes it, is that one must look at whole numbers in a different light \u2014 leaving addition aside and seeing the multiplication structure as something malleable and deformable. Standard multiplication would then be just one particular case of a family of structures, just as a circle is a special case of an ellipse. Fesenko says that Mochizuki compares himself to the mathematical giant Grothendieck \u2014 and it is no immodest claim. \u201cWe had mathematics before Mochizuki's work \u2014 and now we have mathematics after Mochizuki's work,\u201d Fesenko says. But so far, the few who have understood the work have struggled to explain it to anyone else. \u201cEverybody who I'm aware of who's come close to this stuff is quite reasonable, but afterwards they become incapable of communicating it,\u201d says one mathematician who did not want his name to be mentioned. The situation, he says, reminds him of the  Monty Python  skit about a writer who jots down the world's funniest joke. Anyone who reads it dies from laughing and can never relate it to anyone else. And that, says Faltings, is a problem. \u201cIt's not enough if you have a good idea: you also have to be able to explain it to others.\u201d Faltings says that if Mochizuki wants his work to be accepted, then he should reach out more. \u201cPeople have the right to be eccentric as much as they want to,\u201d he says. \u201cIf he doesn't want to travel, he has no obligation. If he wants recognition, he has to compromise.\u201d \n               Edge of reason \n             For Mochizuki, things could begin to turn around later this year, when the Clay Mathematics Institute will host the long-awaited workshop in Oxford. Leading figures in the field are expected to attend, including Faltings. Kim, who along with Fesenko is one of the organizers, says that a few days of lectures will not be enough to expose the entire theory. But, he says, \u201chopefully at the end of the workshop enough people will be convinced to put more of their effort into reading the proof\u201d. Most mathematicians expect that it will take many more years to find some resolution. (Mochizuki has said that he has submitted his papers to a journal, where they are presumably still under review.) Eventually, researchers hope, someone will be willing not only to understand the work, but also to make it understandable to others \u2014 the problem is, few want to be that person. Looking ahead, researchers think that it is unlikely that future open problems will be as complex and intractable. Ellenberg points out that theorems are generally simple to state in new mathematical fields, and the proofs are quite short. The question now is whether Mochizuki's proof will edge towards acceptance, as Perelman's did, or find a different fate. Some researchers see a cautionary tale in that of Louis de Branges, a well-established mathematician at Purdue University in West Lafayette, Indiana. In 2004, de Branges released a purported solution to the Riemann hypothesis, which many consider the most important open problem in maths. But mathematicians have remained sceptical of that claim; many say that they are turned off by his unconventional theories and his idiosyncratic style of writing, and the proof has slipped out of sight. For Mochizuki's work, \u201cit's not all or nothing\u201d, Ellenberg says. Even if the proof of the  abc  conjecture does not work out, his methods and ideas could still slowly percolate through the mathematical community, and researchers might find them useful for other purposes. \u201cI do think, based on my knowledge of Mochizuki, that the likelihood that there's interesting or important math in those documents is pretty high,\u201d Ellenberg says. But there is still a risk that it could go the other way, he adds. \u201cI think it would be pretty bad if we just forgot about it. It would be sad.\u201d \n                 Tweet \n                 Facebook \n                 LinkedIn \n                 weibo \n               \n                     Alexander Grothendieck (1928\u20132014) 2015-Jan-14 \n                   \n                     First proof that infinitely many prime numbers come in pairs 2013-May-14 \n                   \n                     Proof claimed for deep connection between primes 2012-Sep-10 \n                   \n                     Mathematics: The reluctant celebrity 2004-Jan-29 \n                   \n                     RIMS \n                   \n                     \u201cIUT Theory of Shinichi Mochizuki\u201d Clay Mathematics Institute workshop \n                   Reprints and Permissions"},
{"file_id": "525022a", "url": "https://www.nature.com/articles/525022a", "year": 2015, "authors": [{"name": "Kat Austen"}], "parsed_as_year": "2006_or_before", "body": "Electronic gadgets on \u2014 and in \u2014 our bodies are multiplying fast, but transmitting all their data safely will be a challenge. Tom is late for his train and doesn't know the way to the station. Racing around a corner, he runs into a plaza full of tourists snapping and uploading photos to Instagram and Facebook. Which way should he go? He tells his Internet-connected contact lenses to load a map, meanwhile tapping at his smartwatch to pull up his ticket and platform information. An alarm flashes in his peripheral vision, only 15 minutes until the train departs, but the map is not loading. He looks around in dismay, frantically yelling \u201crefresh\u201d to his lenses against the clamour of the street. An alert scrolls across his vision: \u201cYou're feeling stressed. Take a breath. Have a hug!\u201d But with all the tourists accessing the Internet, Tom has no hope of getting his much-needed map. Welcome to the chaotic future of wearable electronics: devices that promise to connect real to digital lives seamlessly. These gadgets are rapidly multiplying, and within five years there could be half a billion devices strapped onto, or even embedded in, human bodies. Today, the most familiar gadgets are fitness trackers and smart watches, which monitor health and provide ready access to online services. But there are already devices that claim to do more than monitor, such as headbands that alert wearers when they become distracted or wristbands that administer electric shocks to smokers who want help quitting. Electronics companies promise to transform medicine with wearables that can treat symptoms or manage care. Devices are emerging that alert people with epilepsy to incipient seizures, help prevent anxiety attacks, and enable blind people to navigate. But the potential of wearables crucially depends on the large amounts of data they access and generate. And that leads to two problems that researchers and technology developers are struggling to solve: finding improved ways to transmit data to and from wearables, and  keeping all that information safe . With everything from toasters to cars now connecting wirelessly to the Internet, demands on a finite bandwidth are rapidly straining the system. Nearly half a billion new devices started chattering over mobile broadband last year alone, pushing mobile traffic to 25 times what it was just 5 years ago. And wearables are leading to new security concerns, from the use of highly personal data to track people's activity to maliciously attacking their online presence. \u201cIt's a clich\u00e9 that whenever there's a new technology we start talking about Huxley and  A Brave New World , but with wearables \u2014 and what's loosely termed the Internet of Things \u2014 we truly are entering into a new era, and we have to start thinking of these issues,\u201d says Anupam Joshi, head of the Center for Cybersecurity at the University of Maryland, Baltimore County. \n               Traffic jam \n             By the end of 2014, global mobile-data traffic reached 2.5 exabytes (2.5 billion gigabytes) per month according to the networking-technology company Cisco Systems. Of that, the world's 100 million or so wearable devices were generating 15 million gigabytes of monthly traffic on what is a physically finite portion of the electromagnetic spectrum, with their number expected to increase fivefold by 2019 (see 'The catch with gadgets'). On top of the surge in those devices, there will be even greater chances for gridlock, as more people start wearing headsets that deliver data-hungry virtual and augmented reality experiences, says Robert Heath, a professor in electrical engineering at the University of Texas at Austin. All these devices clog up the airwaves, impairing performance and threatening essential internet traffic. To help ease congestion in the United States, the government pledged in 2010 to free up an extra 500 megaherz (MHz) within ten years, a doubling of the bandwidth available for mobile devices at the time. But even this is unlikely to be enough, according to a more recent report prepared for CTIA-The Wireless Association, a communications industry group based in Washington DC. It estimates that 350 MHz will need to be added from 2015 onwards to keep up with US demand by the end of 2019, 150 MHz more than the government estimate for that period. And limited bandwidth is a global problem, with each country dealing with it in its own way. In India, where users have access to just one-tenth of the bandwidth available to people in the United States, there are calls for spectrum sharing and the freeing up of channels currently devoted to the military. In the United Kingdom, the government has approved the use of old analogue TV bandwidths; the first networks of smart devices using these frequencies could be rolled out by the end of the year. For their part, telecom companies need to make more efficient use of the spectrum. One way is to look beyond the crowded parts of the airwaves in the radio and television bands. Data from all the wearables on one person could flow through a body-area network designed to use a completely different part of the spectrum, such as the millimetre wavelengths. Then just one device would use the more congested bands to communicate all the data to the Internet. This creates its own problems, however, because shorter wavelengths demand more power and can be blocked by people's bodies. So researchers such as Heath are trying to get around those difficulties by, for example, optimizing antennas to reduce interference and power consumption. Improvements in steerable communication beams could also lead to better ways of transmitting millimetre-wavelength signals. Also promising is the idea of taking wireless communications into the visible-light realm using light-emitting dioides (LEDs) \u2014 which produce light and can act as photoreceptors \u2014 to communicate either between wearables or to talk directly to the Internet. Wearables that incorporate LEDs could use visible light to wrap a person in a body-area network. That would sense every movement and communicate the information to the light fittings in a room, which would be connected to the Internet through their power wiring. Although this technology relies on visible wavelengths, the signals are imperceptible. \u201cLEDs blink so fast that the human eye cannot tell,\u201d says Daniele Puccinelli, an electrical engineer at the University of Applied Sciences of Southern Switzerland in Manno, who studies visible-light communications. Harald Haas, who researches mobile communications at the University of Edinburgh, UK, plans to test a visible-light system in hospitals within the next year. Patients will wear wristbands that monitor their temperature and relay the data using LEDs that communicate with the hospital's lighting. A broader approach might have wearable devices from many people relaying information to each other rather than having each connect to the Internet. This concept underpins the multitiered networks promised by the much-vaunted fifth-generation (5G) mobile-communication systems that are predicted to be up and running in many parts of the world by 2020. In situations where crowds of people are trying to access the same content \u2014 travel information after a sports match, for instance \u2014 one device could act as a 'seed', distributing the data to others in this network, which would reduce the number of times the data need to be downloaded from the Internet. One of the most attractive approaches makes devices smarter about when and how they use communication channels. These 'cognitive radios' sniff out underused regions of bandwidth and opportunistically hop into those gaps, speeding up communications. To reach their optimum potential, bandwidths would need to be more open, so that devices could jump onto a licensed frequency to communicate, and then drop off the spectrum when someone with higher priority enters. Although techniques based on this principle have been used for decades, cognitive radio will take it to a new level of efficiency, with devices smart enough to negotiate with each other to divvy up the available spectrum. Cognitive radios have great potential , but their development in the wearables realm is being held back by a lack of accepted standards and protocols for how this frequency hopping might work in practice, says Ekram Hossain, an electrical engineer at the University of Manitoba, Canada. \u201cUntil there is a standard, there won't be products,\u201d says Hossain, who adds that the research needed to establish these standards is under way. \n               Keeping safe \n             When 176,000 people swarmed through the Consumer Electronics Show in Las Vegas in January, some of the hottest items were the crop of new wearable devices, ranging from watches and glasses to the Pacifi-i, a smart pacifier, or baby soother, that monitors an infant's temperature and transmits the data to a parent's phone. And if those parents were stressed out, they could try the Melomind headset, which is advertised to measure the brain's electrical activity, beam it to a phone and then select the most appropriate music to help the wearer relax. Despite all the hype about wearables, there is also considerable scepticism about the gadgets available today. \u201cLots of people view wearables as just toys\u201d, says Puccinelli. But signs point to them being much more useful in the near future, particularly in the medical arena. Wearables are increasingly measuring aspects of human physiology, providing electrical stimulation to the brain and even injecting medication. These applications come with potential risks for users. A key hurdle for the wearable revolution arises from the  wealth of personal data  they gather about their users. Surveys show that users worry about how these devices invade their privacy, as they upload intimate data to potentially vulnerable servers owned by companies that could change their terms of service, be bought out or go out of business. When the Pew Research Center, an independent fact-gathering organization in Washington DC, canvassed 1,600 experts in 2014 about the future of the Internet, many expressed similar worries. \u201cThe realities of this data-drenched world raise substantial concerns about privacy and people's abilities to control their own lives,\u201d according to the report. Those concerns have been compounded by some high-profile incidents, such as when users of Fitbit activity trackers allowed their activity logs to be publicly accessible, unwittingly revealing when they had sex. When that was realized in 2011, Fitbit quickly took action to fix the problem. In another high-profile incident, the introduction of Google Glass headsets two years ago triggered concerns that users would capture images of passers-by without their knowledge. Researchers at the Center for Cybersecurity took this opportunity to apply their work on computer codes that enforce privacy policies. They built the wryly named FaceBlock app, which blocks out the faces of people who have requested privacy from photographs taken by Google Glass. But for this to work, a Google Glass owner would have to opt in by installing the app. So the only way for such a system to reliably provide privacy would be for manufacturers to make it standard and implement it with dedicated hardware, says Joshi. \u201cLet's say that Google was to build in a feature like this into every Google Glass so that it would automatically obey these kinds of commands \u2014 then it would work.\u201d Security concerns go hand in hand with privacy. Although encryption is becoming more pervasive and advanced, it is sometimes not used in low-cost wearable devices. Last year, researchers at the California-based information-management company Symantec, revealed that the location of many health monitors, including some from market leaders, can be easily tracked. And some of them wirelessly communicate passwords in clear text, which makes them vulnerable to hacking. Even if a health monitor is encrypted, the smartphone or hub device that links it to the Internet could also be a weak point, either because of unnecessarily broad permissions or because of malware. \u201cIf you're not encrypting the data you're definitely not secure,\u201d says Bogdan Carbunar, a security researcher at Florida International University in Miami. \u201cEven if you're encrypting the data you can still not be secure.\u201d Carbunar worked with a team, including a researcher from IBM, on security holes in two popular low-cost wearable fitness devices, the Fitbit Ultra and the Garmin Forerunner. They found that by impersonating the devices' trusted webservers, they could fool the gadgets into uploading false data \u2014 even nonsensical numbers such as millions of steps in one day (see M. Rahman  et al .  IEEE Trans. Mobile Comput.   http://doi.org/636 ; 2015 ). The researchers also found that they could inject data onto a tracker of their own, which would compromise data accuracy, something that could become a problem if fitness data are tied to health-insurance premiums, as they have been in some companies. Fitbit told  Nature  that it had been aware of the problem, which has been addressed in subsequent products. Garmin did not respond to requests for comment. According to Carbunar, security adds costs for manufacturers in terms of money, development time, device size and power consumption. But researchers are pushing to minimize those costs. After working out how to hack the devices, Carbunar and his team devised a way to keep them safe. They developed SensCrypt, an encryption protocol designed specifically for low-energy fitness trackers that reduces communications costs. It uses a procedure called symmetric key encryption to protect against remote attacks and to provide some security even if the device is stolen and tampered with. The researchers were unable to implement it on Fitbit or Garmin devices because they use closed-source code, but have tested their system on an open-source proxy. Even with high levels of encryption, devices could still be vulnerable to attack, says Bart Preneel, a cryptographer at the KU Leuven and iMinds research centre in Belgium. Preneel specializes in understanding and preventing side-channel attacks: attempts by hackers to infiltrate mobile devices by detecting fluctuations in the power usage and using these to calculate encryption keys and other secure information. \u201cThese attacks can be made at a distance of 10 or 20 metres,\u201d he says. This type of attack was discovered around 20 years ago in relation to bank cards, but ways of preventing it are not implemented in many wearable devices, particularly implanted medical technology. Some companies have tried to improve security on mobile devices and wearables by equipping them with biometric devices such as fingerprint readers and iris scanners. But even these are insecure: researchers and hackers have shown how high-resolution cameras can capture someone's iris from a distance and how to steal a fingerprint using a phone's camera. But Preneel says that biometrics are promising for encryption if designers focus on measures that are not so easy to discover. There are already wearables that authenticate users on the basis of their heartbeat pattern. In the long run, Preneel envisages using internal signals from the body, such as DNA or the internal microbial community, to pair with wearable gadgets so that the devices would unlock only when in close proximity to the owner. With these kinds of improved security \u2014 and many upgrades in communications networks \u2014 a lost tourist in the future would stand a better chance of getting their wearables to work in a crowded plaza. Tom would easily be able to summon a map of the city on his lenses and would know his personal data were safely encrypted. Following the highlighted route, he might even make it to the station with enough time to get a coffee and charge his gadgets. It may not be the technological utopia imagined by some wearables enthusiasts, but at least he will catch his maglev. \n                 Tweet \n               \n                     Graphene booms in factories but lacks a killer app 2015-Jun-17 \n                   \n                     Environmental science: Pollution patrol 2015-Jan-07 \n                   \n                     Physiological data must remain confidential 2014-Jan-15 \n                   \n                     The Consumer Electronics Association \n                   \n                     CTIA-The Wireless Association \n                   \n                     Pew Research Center Survey on the Internet of Things \n                   Reprints and Permissions"},
{"file_id": "525018a", "url": "https://www.nature.com/articles/525018a", "year": 2015, "authors": [{"name": "Mark Peplow"}], "parsed_as_year": "2006_or_before", "body": "Inspired by biology, chemists have created a cornucopia of molecular parts that act as switches, motors and ratchets. Now it is time to do something useful with them. The robot moves slowly along its track, pausing regularly to reach out an arm that carefully scoops up a component. The arm connects the component to an elaborate construction on the robot's back. Then the robot moves forward and repeats the process \u2014 systematically stringing the parts together according to a precise design. It might be a scene from a high-tech factory \u2014 except that  this assembly line is just a few nanometres long . The components are amino acids, the product is a small peptide and the robot, created by chemist David Leigh at the University of Manchester, UK, is one of the most complex molecular-scale machines ever devised. It is not alone. Leigh is part of a growing band of molecular architects who have been inspired to emulate the machine-like biological molecules found in living cells \u2014 kinesin proteins that stride along the cell's microscopic scaffolding, or the  ribosome  that constructs proteins by reading genetic code. Over the past 25 years, these researchers have devised an impressive array of switches, ratchets, motors, rods, rings, propellers and more \u2014 molecular mechanisms that can be plugged together as if they were nanoscale Lego pieces. And progress is accelerating, thanks to improved analytical-chemistry tools and reactions that make it easier to build big organic molecules. Now the field has reached a turning point. \u201cWe've made 50 or 60 different motors,\u201d says Ben Feringa, a chemist at the University of Groningen in the Netherlands. \u201cI'm less interested in making another motor than actually using it.\u201d That message was heard clearly in June, when one of the influential US Gordon conferences focused for the first time on molecular machines and their potential applications, a clear sign that the field has come of age, says the meeting's organizer, chemist Rafal Klajn of the Weizmann Institute of Science in Rehovot, Israel. \u201cIn 15 years' time,\u201d says Leigh, \u201cI think they will be seen as a core part of chemistry and materials design.\u201d Getting there will not be easy. Researchers must learn how to make billions of molecular machines work in concert to produce measurable macroscopic effects such as changing the shape of a material so that it acts as an artificial muscle. They must also make the machines easier to control, and ensure that they can carry out countless operations without breaking. That is why many in the field do not expect the first applications to involve elaborate constructs. Instead, they predict that the basic components of molecular machines will be used in diverse areas of science: as light-activated switches that can release targeted drugs, for example, or as smart materials that can store energy or expand and contract in response to light. That means that molecular architects need to reach out to researchers who work in fields that might benefit from their machine parts, says Klajn. \u201cWe need to convince them that these molecules are really exciting.\u201d \n               Shuttle launch \n             Many of today's molecular machines trace their origins to a relatively simple device built in 1991 by Fraser Stoddart, a chemist now at Northwestern University in Evanston, Illinois. It was an arrangement known as a rotaxane, in which a ring-shaped molecule is threaded onto an 'axle', a linear molecule capped by bulky stoppers at each end. Included in this particular axle, towards either end of the chain, were two chemical groups that could bind to the ring. Stoddart found 1  that the ring could hop back and forth between these two sites, creating the first molecular shuttle. By 1994, Stoddart had modified the design so that the axle had two different binding sites 2 . The shuttle existed in solution; changing the acidity of this liquid forced the ring to hop from one site to the other, making the shuttle into a reversible switch. Similar molecular switches could one day be used in sensors that respond to heat, light or specific chemicals, or that open the hatch of a nanoscale container to deliver a cargo of drug molecules at precisely the right time and to exactly the correct place in a person's body. Stoddart's switches displayed two properties that would come up again and again in the molecular machines that followed. First, the links between the ring and the axle's binding sites were not the strong covalent bonds that knit atoms into molecules. Instead, they were weaker electrostatic attractions between slightly positive and negative regions of the two components. This meant that the bonds could be readily formed and broken, much like zipping and unzipping the hydrogen bonds that link the two strands of DNA. Second, the shuttles did not need an external energy source to zip back and forth. They were powered by collisions with other molecules in the solution, a jostling effect called Brownian motion. A plethora of other switches soon followed. Some were controlled with light or changes in temperature, whereas others worked by binding specific ions or molecules from solution, in a similar way to how ion channels work in cell membranes, opening or closing in response to chemical signals. Stoddart, however, took his research in a different direction. Working with James Heath at the California Institute of Technology in Pasadena, he used millions of rotaxanes to make a  memory device 3 . Sandwiched between silicon and titanium electrodes, the rotaxanes could be electrically switched from one state to another and used to record data. This molecular abacus, roughly 13 micrometres across, contained 160,000 bits, each composed of a few hundred rotaxanes \u2014 a density of roughly 100 gigabits per square centimetre, comparable to the best commercial hard drives available today. Using 24 of the best-performing bits, Stoddart's team stored and retrieved the letters 'CIT' (for the California Institute of Technology). But the switches were not very robust, typically falling apart after fewer than 100 cycles. One promising solution is to load them into tough, porous crystals known as  metal-organic frameworks (MOFs) , which protect the switches and organize them into a precise 3D array. Earlier this year, Robert Schurko and Stephen Loeb of the University of Windsor, Canada, showed that they could pack about 10 21  molecular shuttles into each cubic centimetre of a MOF 4 . And last month, Stoddart unveiled 5  a different MOF that contained switchable rotaxanes. The MOF was mounted on an electrode, and the rotaxanes could be switched en masse by changing the voltage. Researchers working on these MOFs hope that the 3D, solid scaffolds will offer a greater density of switches than conventional silicon transistors, and make the molecules easier to switch in a controllable way, potentially offering vast amounts of data storage. \u201cThe sci-fi way to think about it would be to address each molecule as a bit,\u201d says Loeb. But more realistically, he says, a speck of the MOF containing hundreds of switches could act as one bit. As long as most of the switches in the speck function properly, he says, they will collectively and reliably encode data. Others have used rotaxanes to make switchable catalysts. In 2012, Leigh described 6  a system with a nitrogen atom in the middle of the rotaxane's axle, where it is normally covered by a ring. Add an acid, and the ring moves to one side, exposing the nitrogen atom so that it can catalyse a common chemical reaction. It goes further: last November, Leigh reported 7  a rotaxane system with two different catalytic sites. Moving the ring from one to the other allowed the chemists to switch the rotaxane's activity, so that it could stitch together a mixture of molecules in two different ways. Leigh is now working on putting several different switchable catalysts into the same solution, where they could be toggled on and off in a sequence to build target molecules into complex products, in much the same way as enzymes do in a cell. \n               Nano motors \n             In 1999, after early experiments with shuttles and switches, the field took a big step forward with the creation of the first synthetic molecular motor 8 . Built by Feringa's team, it was a single molecule containing two identical 'paddle' units connected by a carbon\u2013carbon double bond. This fixed the paddles in place until a burst of light broke part of the bond, allowing the paddles to rotate. Crucially, the shape of the paddles meant that they could turn in only one direction \u2014 and as long as there was a supply of light and some heat, the motor would just keep spinning. Feringa went on to use similar molecular motors to create a four-wheel-drive 'nanocar' 9 . He also showed 10  that the motors could give liquid crystals enough of a twist to  slowly rotate a glass rod  sitting on top of them. The rod was 28 micrometres long \u2014 thousands of times the size of the motors. Some chemists argue that although these motors are cute, they are ultimately useless by themselves. \u201cI've always been a bit sceptical of artificial motors \u2014 they're too difficult to make, too difficult to scale up,\u201d says Dirk Trauner, a chemist at the Ludwig Maximilian University in Munich, Germany. But the chemical principles behind them might be very useful indeed. Using the same light-activated mechanism, researchers have developed around 100 drug-like compounds that can be switched on or off in response to light. In July, for example, a team led by Trauner reported 11  a light-switchable version of combretastatin A-4, a potent anticancer compound that comes with some serious side effects, because it indiscriminately attacks tumour cells and healthy tissue alike. The team's switchable drug could drastically reduce system-wide side effects: it contains a nitrogen\u2013nitrogen double bond that holds two sections of the molecule apart and renders it inactive. Only under blue light will the bond break and allow the sections to rotate into the molecule's active form. Trauner says that an area of tissue just 10 micrometres wide can be specifically targeted in this way, using light delivered through a flexible tube or by an implanted device. Trauner is planning mouse studies to test the compound's effectiveness against cancer. He also hopes to use photoswitchable compounds to restore vision in people with macular degeneration or retinitis pigmentosa, conditions that damage the eye's light-sensing rod and cone cells. \u201cIt's low-hanging fruit \u2014 because it's in the eye, you don't have to worry about how to get the light in,\u201d he says. Last year, he showed 12  that one injection of a photoswitchable molecule called DENAQ into the eyes of blind mice partially restored their vision for several days, allowing the animals to distinguish between light and dark. The team is now trying the same technique in primates, and hopes to begin human trials in two years' time. Trauner and Klajn both agree that the main challenge will be to convince the cautious pharmaceutical industry that photoswitchable drugs have potential, even though they have no track record in humans. \u201cWe need to get the pharmaceutical industry excited about photopharmacy,\u201d says Trauner. \u201cOnce they see the value, we'll be in good shape.\u201d \n               Walk the line \n             Long before any creature had evolved to move on dry land, cells were using legs as part of their cellular machinery. Prime examples are the two-pronged proteins called kinesins, which put one 'foot' in front of the other as they carry molecular cargo along the cell's stiff scaffolding of microtubules. Inspired by kinesin, researchers have built artificial walkers from DNA. The molecules typically have feet that are anchored in place by binding to complementary strands of DNA laid out on a track; adding a competing DNA strand can free the foot, allowing it to take a step forward. One of the most striking examples was described 13  in 2010 by Nadrian Seeman at New York University. His DNA walker had four 'feet' and three 'hands', with which it could pick up gold nanoparticles as it moved around a tile made of folded DNA. DNA walkers \u2014 and variants that soon trundled out of other labs \u2014 would wander aimlessly if they did not have a built-in ratchet system to stop them taking a step backward. For many walkers, that ratchet lies in the relative rates of the chemical reactions that are involved in binding and releasing their feet, with the pummelling of Brownian motion driving the released foot forward 14 . Over the past few years, detailed chemical studies and molecular dynamics simulations have shown that this 'Brownian ratchet' concept underlies all chemically driven molecular machines, including many biological motors. In 2013, for example, a team led by Nils Walter, a chemical biologist at the University of Michigan in Ann Arbor, found 15  the same mechanism at work in the spliceosome, a cellular machine that snips sections out of RNA before genetic information is translated to make proteins. \u201cKinesin uses it, the ribosome uses it and the spliceosome uses it,\u201d says Walter. That shows that the same principles underlie biological machines and synthetic molecular machines, so researchers working in the two areas could share knowledge. \u201cBy and large, they're quite separate fields right now,\u201d says Walter. \u201cI think the next breakthroughs will come if we all sit at the same table.\u201d \n               Rocket science \n             Meanwhile, inspired by the microscopic medical submarine of the cult 1966 film  Fantastic Voyage , chemists have created an array of micrometre-sized particles and tubes that can zip through liquids like rockets. Some of these motors carry a catalyst that generates thrust by producing a stream of bubbles from the liquid around them \u2014 often hydrogen peroxide. Others get their power directly from light or from external electric and magnetic fields, which can also be used to steer the vessels. \u201cThese nanomotors can go over 1,000 times their own length per second, it's incredible,\u201d says nanoengineer Joseph Wang of the University of California, San Diego. He thinks that the most promising applications lie in fast drug delivery, or low-cost clean-up of environmental pollutants \u2014 although many in the field caution that it is too early to tell whether nanomotors would trump conventional methods. Hydrogen peroxide, a powerful oxidizing agent, is hardly conducive to  in vivo  use. \u201cWhen all the work was based on peroxide there was a lot of scepticism,\u201d Wang admits. But in December last year, he reported 16  a microscale motor suitable for testing in live animals. Made of a plastic tube roughly 20 micrometres long, it contains a core of zinc that reacts with stomach acid to generate propulsive bubbles of hydrogen. The tubes safely zipped around inside a mouse's stomach for about 10 minutes. Wang used them to carry gold nanoparticles into surrounding stomach tissue; mice dosed with plain nanoparticles ended up with three times less gold in their stomach lining than mice dosed with the tubes. Wang suggests that loading drugs or imaging compounds onto the rockets could deliver them into stomach tissue rapidly and effectively. \u201cIn the next five years we will move to practical  in vivo  applications,\u201d he says. \u201cIt really is the fantastic voyage.\u201d At the moment, there is limited crossover between research on these rockets and the molecular machines. \u201cBut we could bring a lot,\u201d says Klajn. For example, coating a micromotor with light-responsive molecular switches could offer extra control over its movement, he suggests. \n               Pump it up \n             In their quest to forge molecular machines that can actually do something useful, researchers are starting to integrate several different components into a single device. In May this year, Stoddart unveiled 17  an artificial molecular pump that pulls two ring molecules out of solution onto a storage chain. Each ring slips over a stopper at one end of the chain, attracted to a switchable binding point. Flipping that switch pushes the ring over a second barrier farther along the chain, where it reaches a holding area (see 'Nano machines'). The system is not able to pump any other type of molecule, and it took a lot of trial and error to build. \u201cIt's been a long road,\u201d sighs Stoddart. But it proves that molecular machines can be used to concentrate molecules, pushing a chemical system out of equilibrium in the same way that biology can build up a store of potential energy by forcing ions or molecules up a concentration gradient. \u201cWe're learning how to design an energy ratchet,\u201d he says. Stoddart says that such developments could enable the field to progress in two major directions: stay nano, giving the machines molecular-scale jobs that cannot be achieved in any other way; or go macro, using trillions of them together to reshape materials or move substantial cargoes, like an army of ants. Perhaps the prime example of the nano approach is  Leigh's molecular assembly line 18 . Inspired by the ribosome, it is based on a rotaxane system that picks up amino acids from its axle and adds them to a growing peptide chain. But the devices could have macro applications. Over 36 hours, 10 18  of them working together can produce a few milligrams of peptide. \u201cIt doesn't do anything that you can't do in the lab in half an hour,\u201d says Leigh. \u201cYet it shows that you can have a machine that moves down a track and picks up molecular building blocks and puts them together.\u201d Leigh is now working on other versions of the machine to make sequenced polymers, with tailored material properties. Conversely, trillions of molecular machines working together could change the properties of materials in the macroscopic world. Gels that expand or contract in response to light or chemicals, for example, could act as adjustable lenses or sensors. \u201cIn the next five years, I bet you'll get the first smart materials where you have switches incorporated,\u201d says Feringa. Rotaxane-like molecules are already starting to see commercial applications. The Nissan Scratch Shield iPhone case, launched in 2012 and based on work by Kohzo Ito at the University of Tokyo, is made of polymer strands threaded through pairs of barrel-shaped cyclodextrin molecules connected in a figure-of-eight shape. Pressure on a normal polymer coating would break the connections between the chains, leaving a scratch. But the cyclodextrin rings act like the wheels of a pulley system, allowing the polymer strands to  slip through without breaking 19 . The films can even protect a brittle screen from a  sustained beating with a hammer . For Stoddart, this shows that the components developed by molecular architects are already ripe for application. \u201cThis field has come a long way,\u201d says Stoddart. \u201cNow we have to start showing it's useful.\u201d \n                 Tweet \n                 Facebook \n                 LinkedIn \n                 Weibo \n                 Wechat \n               \n                     Materials science: The hole story 2015-Apr-08 \n                   \n                     Molecular robot mimics life's protein-builder 2013-Jan-10 \n                   \n                     Elusive protein factory mapped at last 2010-Nov-25 \n                   \n                     Nanotech comes alive 2008-Oct-07 \n                   \n                     High-density memory: A switch in time 2007-Jan-24 \n                   \n                     Gordon Research Conference on Artificial Molecular Switches and Motors \n                   \n                     David Leigh \n                   \n                     Rafal Klajn \n                   \n                     Ben Feringa \n                   \n                     Dirk Trauner \n                   \n                     Fraser Stoddart \n                   \n                     Stephen Loeb \n                   \n                     Nadrian Seeman \n                   \n                     Dean Astumian \n                   \n                     Nils Walter \n                   \n                     Joe Wang \n                   \n                     Kohzo Ito \n                   Reprints and Permissions"},
{"file_id": "525176a", "url": "https://www.nature.com/articles/525176a", "year": 2015, "authors": [{"name": "Emma Marris"}], "parsed_as_year": "2006_or_before", "body": "Archaeology is moving underwater and along riverbanks to find clues left by the people who colonized the New World. On 17 September, a catamaran will set off into the Pacific Ocean on a week-long cruise back to the Pleistocene. Laden with sonar instruments, the research vessel  Shearwater  will probe the ocean bottom to find places that were beaches and dry land more than 13,000 years ago, when the sea level was around 100 metres lower. The researchers are hunting for evidence that ancient people lived along this now-sunken coastline as they colonized the New World. Meanwhile, other archaeologists are digging in the intertidal zone on a remote island off the shore of British Columbia in Canada, where the sea level has barely changed since the ice-age glaciers began to retreat. Since late last year, that team has found footprints and a tool that date back 13,200 years, making them some of the oldest human marks on the continent. Whoever left them had to have reached the island by boat. Welcome to the newest wave of American archaeology: the idea that the first residents of the Americas came by sea, hugging the Pacific coast as they went south. This theory marks a sharp departure from the once-dominant hypothesis that Pleistocene hunters from Siberia migrated by foot across a land bridge to Alaska and then south into the heart of North America. This route opened up only when the vast sheets of ice covering the continent had melted enough to permit passage. It was thought that these first migrants made the distinctive stone spear tips called Clovis points, which began appearing at sites in the interior of North America around 13,000 years ago. There has long been evidence that others reached the New World at least 1,000 years earlier. But only in the past decade have archaeologists accumulated enough evidence to abandon the Clovis-first model (see  Nature   485 , 30\u201332; 2012 ). Some of the  earliest human sites in the Americas  date to well before a corridor opened up between the ice sheets, which is forcing researchers to explore the idea that New World colonizers skirted the coastline. Travelling by boat, these early people could have hopscotched their way south of the ice sheets, subsisting on the rich marine resources of the ice-free strip along the shore. The search for these sea-going settlers will not be easy. Much of the evidence that archaeologists seek is deep underwater \u2014 or was smashed long ago by the Pacific's legendary waves. But momentum is building to find those earliest settlers. \u201cPeople are just more optimistic,\u201d says Quentin Mackie, an archaeologist at the University of Victoria in Canada. Amanda Evans, a marine archaeologist at the ocean-survey company Tesla Offshore in Prairieville, Louisiana, says that prehistoric underwater archaeology in general is having a moment. \u201cThis year just seems to be the year that everybody was pushing the ball uphill and it finally crested.\u201d \n               Tools of the trade \n             Loren Davis, an archaeologist at Oregon State University in Corvallis, is searching for the ancient seafarers in an unusual spot \u2014 at a site in Idaho called Cooper's Ferry, which is on a bank of the Salmon River, hundreds of kilometres away from the coast. At the dig site in August, Davis examines a piece of rock brought to him by one of his field crew. He turns it over to see whether it was shaped by human hands, perhaps by early toolmakers who littered the ground with flakes of rock as they worked. Although Cooper's Ferry is far inland, Davis suggests that it is part of the coastal story. The Salmon is a tributary of the mighty Columbia River, which would have been the first large waterway encountered by people who made it south of the ice sheets during the last glacial epoch. At that time, valleys farther north would have been covered by glaciers. For a water-adapted culture, he says, \u201cthe first off-ramp south of the ice is the Columbia River\u201d. Having considered the stone, Davis hands it back to his colleague and says, \u201cI think it is a flake.\u201d His archaeological pits, which the crew has shaped into a series of neat holes, are full of flakes and finished 'western stemmed' points up to 13,200 years old 1 . Whereas the Clovis points are shaped like miniature surfboards, the western stemmed points from Cooper's Ferry are smaller and look like Christmas trees. Points resembling the western stemmed variety have been found throughout the western United States and in Siberia \u2014 a connection that suggests they were brought over to the New World by early hunters. Davis's crew is quietly intent, and the air is filled with the gentle sound of trowels scraping earth, along with a rock wren's distinctive call. The peace is occasionally broken by shouts between diggers and data recorders: \u201cBone!\u201d, \u201cFire-cracked rock!\u201d or \u201cDeb!\u201d (short for debitage, or flakes). The position of each artefact is precisely recorded, then it is bagged up and stored in one of many boxes that are piling up in a nearby trailer. Precise dates will be assigned later, in the laboratory. A sense of expectation hangs over the dig. If the team uncovers particularly old western stemmed points that definitively pre-date the Clovis era, that would strongly suggest that the first Americans carried these points there by sea and river. \u201cYou get a gambler's mentality,\u201d Davis says. The hunt obsesses the crew, who spend weeks here, camped out and digging for hours each day. Sarah Skinner, an Oregon State student who supervises pit B, says that she wakes up clenching her fists around dream trowels. \u201cWhen I close my eyes, I see artefacts,\u201d she says. \n               Hinge-point hunt \n             Signs of early inhabitants are also starting to appear along the coast, particularly in spots where the swelling seas have not covered ancient shorelines since the end of the last glacial period. The western coast of Canada, for example, was pressed down by the Pleistocene ice and has been rebounding upwards since the glaciers melted. In some spots \u2014 hinge points \u2014 that post-glaciation rebound almost exactly cancels out the rising sea level 2 . One of those hinge points is Calvert Island, where a 13,200-year-old footprint was found late last year and another was discovered this summer. Daryl Fedje and Duncan McLaren, both archaeologists at the University of Victoria in British Columbia, plan to continue working the site to look for signs of the earliest Americans (see 'Welcome to America'). The Hakai Institute on Calvert Island, founded by Canadian entrepreneur Eric Peterson, is supporting that work. \u201cAs a fourth-generation British Columbian,\u201d Peterson says, \u201cI am intensely interested in the rich history of humans on our coastline, which we now realize goes way, way back. How far back? Thirteen thousand years? Fifteen thousand years? That's what we want to find out.\u201d Mackie thinks that Calvert Island and similar hinge points will produce results much faster than underwater work, which is technically challenging and expensive. \u201cYou might as well just stand on your boat and burn $100 bills,\u201d he says. But despite the enormous cost and technical challenges, he and others agree that underwater locations may hold tremendous potential and that the time has finally come for archaeologists of American prehistory to explore the Pacific Ocean. There have been a few projects over the years. In the late 1990s, Mackie and Fedje did some sea-floor mapping around Haida Gwaii, an island off the British Columbia coast 3 . They took samples of sea-floor sediment and hauled up a barnacle-encrusted flake tool that they suggest dates from 10,000 years ago, when the sea floor on which it was found was dry land. More recently, the duo used an autonomous underwater vehicle to explore and found what they suggest could be a fishing weir \u2014 a trap made from rocks \u2014 that dates back 13,800 years. Archaeologist James Dixon of the University of New Mexico in Albuquerque has done marine surveys of the now-submerged land that once connected Asia and North America. And Jon Erlandson, an archaeologist at the University of Oregon in Eugene, has worked for years on the Channel Islands off Southern California, piecing together evidence for his theory that people followed a 'kelp highway' down the coast. This route would have offered abundant food \u2014 fish, shellfish and marine mammals \u2014 supported by the kelp forest ecosystems. But the offshore studies so far have been limited, and most of the discoveries have not been reliably dated. \u201cThere's very few of us, and we are spread over vast, vast areas,\u201d says Dixon. There has been a hesitation to join \u2014 or fund \u2014 the chancy and expensive underwater search. \u201cYou can go out there and be totally skunked by the weather,\u201d he says. \u201cIt is a tough thing.\u201d That has led many researchers to discount the coastal hypothesis in the past, notes Mackie. \u201cPeople thought 'well, all the information is deep underwater and we'll never find it',\u201d he says. And so far, nothing older than the Clovis era has been found along the Pacific Coast \u2014 either on the sea floor or on land. \n               Spark of interest \n             What snapped the field out of its funk was not a charismatic leader or a spectacular find \u2014 it was federal bureaucracy. The US Bureau of Ocean Energy Management (BOEM) was formed in 2010 to regulate energy development on the continental shelf. The bureau is bound by the National Historic Preservation Act, which requires it to make sure that valuable archaeological sites will not be destroyed by any development that requires a federal permit. As  interest in offshore renewable-energy projects  has increased in recent years, BOEM has scrambled to improve methods for identifying prehistoric sites. In 2011, it commissioned a sweeping study of possible archaeological sites on the Pacific outer continental shelf. Davis and a colleague at Oregon State, archaeologist Alex Nyers, worked on the report, using existing ocean-depth data and estimates of sea-level rise to decipher where previous shorelines would have been 4 . They then modelled where prehistoric sites might be clustered: presumably on gentle, south-facing (and thus warmer) slopes and near lakes, rivers, bays and islands, all now submerged. That report came out in 2013, and led directly to a US$600,000 grant from BOEM to seek out evidence for the predictions about prehistoric environments. On a series of cruises off California and Oregon over the next three years, researchers will use a variety of sonar instruments to survey the ocean floor and sediments below. If they identify a possible estuary, beach or other ancient shoreline feature, they will take core samples and carbon date biological material from the various layers of sediment to confirm the find. Principal investigator Todd Braje, an anthropologist at San Diego State University in California, is trying to expand the project by encouraging the US National Oceanic and Atmospheric Administration and other potential funders to add more cruises. But even at its current size, he says, \u201cThis is probably the biggest effort to identify submerged sites along the Pacific coast.\u201d  This is probably the biggest effort to identify submerged sites along the Pacific coast.  The investment may be big, but Braje is trying to keep expectations modest. He insists that the goal is to learn how to identify environments in which humans might have camped or settled up to 20,000 years ago; the team is not expecting to find the remains of any settlements, and certainly not ones older than those of the Clovis settlers. \u201cThe idea that we are going to hit on a 15,000-year-old site that is underwater is probably unrealistic in the near future,\u201d says Braje. \u201cYou get to those first migrants into the New World and the archaeological footprint they left is very small.\u201d The project will build on Davis's model of submerged environments, using coring and imaging to test whether his projections actually lead them to the right sorts of sites. Davis is a co-principal investigator and will join the Oregon cruises next year. In the meantime, he is digging in Idaho. It is near the end of the field season, and he and his crew are working on their day off to finish as much as possible. He has bribed them with gourmet cheese, and he lays it out with no fewer than five cheese knives. Combined with the trowels, brushes, scrapers and spoons used by the crew, the site is bristling with tools. Given all the difficulties of this work, those involved in investigating the ocean-migration hypothesis stress that expectations should remain modest for many years as researchers improve their search methods. If the theory is correct, the first definitive older-than-Clovis find along the coast \u2014 the green light for the theory that everyone seems to be hoping for \u2014 could still be far off. \u201cIt could happen this summer, next summer, it could be ten years,\u201d says Erlandson. Or it could happen right now in the sweltering pit at Cooper's Ferry, with the very next scrape of a trowel. \n                 Tweet \n                 Follow @NatureNews \n               \n                     Bone DNA reveals humanity\u2019s trek into South America 2015-Apr-29 \n                   \n                     In Search of Sunken Treasure 2014-Dec-16 \n                   \n                     The first South Americans: Extreme living 2014-Oct-01 \n                   \n                     First Australians may have been migrants rather than drifters 2013-Apr-24 \n                   \n                     Ancient migration: Coming to America 2012-May-02 \n                   \n                     Nature special: Peopling the planet \n                   \n                     Bureau of Ocean Energy Management \n                   Reprints and Permissions"},
{"file_id": "525172a", "url": "https://www.nature.com/articles/525172a", "year": 2015, "authors": [{"name": "Ewen Callaway"}], "parsed_as_year": "2006_or_before", "body": "Move over X-ray crystallography. Cryo-electron microscopy is kicking up a storm by revealing the hidden machinery of the cell. In a basement room, deep in the bowels of a steel-clad building in Cambridge, a major insurgency is under way. A hulking metal box, some three metres tall, is quietly beaming terabytes\u2019 worth of data through thick orange cables that disappear off through the ceiling. It is one of the world\u2019s most advanced cryo-electron microscopes: a device that uses electron beams to photograph frozen biological molecules and lay bare their molecular shapes. The microscope is so sensitive that a shout can ruin an experiment, says Sjors Scheres, a structural biologist at the UK Medical Research Council Laboratory of Molecular Biology (LMB), as he stands dwarfed beside the \u00a35-million (US$7.7-million) piece of equipment. \u201cThe UK needs many more of these, because there\u2019s going to be a boom,\u201d he predicts. In labs around the world, cryo-electron microscopes such as this one are sending tremors through the field of structural biology. In the past three years, they have revealed exquisite details of protein-making ribosomes, quivering membrane proteins and other key cell molecules, discoveries that leading journals are publishing at a rapid clip. Structural biologists say\u00a0\u2014\u00a0without hyperbole\u00a0\u2014\u00a0that their field is in the midst of a revolution: cryo-electron microscopy (cryo-EM) can quickly create high-resolution models of molecules that have resisted X-ray crystallography and other approaches, and labs that won Nobel prizes on the back of earlier techniques are racing to learn this upstart method. The new models reveal precisely how the essential machinery of the cell operates and how molecules involved in disease might be targeted with drugs. \u201cThere\u2019s a huge range of very important biological problems that are now open to being tackled in a way that they could never before,\u201d says David Agard, a structural cell biologist at the University of California, San Francisco. Scheres was recruited to the LMB several years ago to help push cryo-EM technology to its limits \u2014 and he and his colleagues have done just that. Last month, they reported one of the burgeoning field\u2019s most impressive feats: a startlingly clear picture of an enzyme implicated in Alzheimer\u2019s disease, showing the position of its 1,200 or so amino acids down to a resolution of a few tenths of a nanometre 1 .\u00a0 Biologists are now pushing the technique further to deduce ever more detailed structures of small and shape-shifting molecules \u2014 a challenge even for cryo-EM. \u201cWhether you call it revolution or a quantum leap, the fact is that the gates have opened,\u201d says Eva Nogales, a structural biologist at the University of California, Berkeley. \n               Crystal coaxing \n             Spend a bit of time with a structural biologist and they will probably mention their field\u2019s unofficial motto: \u2018structure is function\u2019. Only by knowing the atom-by-atom arrangement of a biomolecule can researchers grasp how it works\u00a0\u2014\u00a0how, for instance, the ribosome reads strands of messenger RNA to manufacture proteins, or how molecular pores flip open and shut. For decades, one technique enjoyed a near monopoly in elucidating protein structures to this level of detail: X-ray crystallography, in which scientists persuade proteins to form into crystals, then blast X-rays at them and decipher the protein\u2019s structure from patterns that the X-rays make when they bounce off (see \u2018Structure solvers\u2019). Of the more than 100,000 entries in the Protein Data Bank, a popular repository of protein structures, about 90% were solved by this technique. It has contributed to more than a dozen Nobel prizes, including the one awarded in 1962 for revealing DNA\u2019s double helix. But although X-ray crystallography has been structural biologists\u2019 best tool, it also has major limitations. It can take researchers years to find ways of forming some recalcitrant proteins into large crystals that are suitable for analysis, and many fundamentally important molecules\u00a0\u2014\u00a0such as proteins that are embedded in cell membranes or that make up complex molecular machines\u00a0\u2014\u00a0have defied crystallization. X-ray crystallography was certainly king when biologist Richard Henderson arrived at the LMB in 1973 to study a protein called bacteriorhodopsin, which uses light energy to pump protons across a membrane. Henderson and his colleague Nigel Unwin had managed to make two-dimensional crystals from the protein, but they were unsuitable for X-ray diffraction. So the pair decided to try electron microscopy instead. At the time, electron microscopy was used to study viruses or slices of tissue that had been treated with heavy-metal stains. A beam of electrons is fired at a sample, and the emerging electrons are detected and used to map out the structure of the materials they smashed into. This approach produced the first detailed image of a virus\u00a0\u2014\u00a0a tobacco pathogen\u00a0\u2014\u00a0but the stain made it difficult to see individual proteins, let alone the atomic details that the X-rays were revealing. \u201cIt was blobby stuff or negative-stained, and you would see outlines of molecules,\u201d says Agard. In a pivotal step, Henderson and Unwin omitted the stain when they used electron microscopy to image crystal sheets of bacteriorhodopsin\u00a0\u2014\u00a0instead, they placed the crystals on metallic grids to make the protein stand out. \u201cYou were looking at the atoms in the protein,\u201d says Henderson, who, with Unwin, published 2  the structure of bacteriorhodopsin in 1975. \u201cThat was such a huge step forward,\u201d Agard says. \u201cThat said, \u2018OK, it will be possible to solve protein structures by EM\u2019.\u201d The cryo-EM field developed through the 1980s and 1990s; a key advance was the use of liquid ethane to flash-freeze proteins in solution and hold them still 3 , which is how the \u2018cryo\u2019 came to cryo-EM. But still the technique could generally resolve structures only to more than 10\u00a0\u00c5ngstr\u00f6ms (1\u2009\u00c5 is one-tenth of a nanometre)\u00a0\u2014\u00a0nothing to rival the better than 4-\u00c5 models of X-ray crystallography, and nowhere near what was needed to use the structures for drug design. While funders such as the US National Institutes of Health were ploughing hundreds of millions of dollars into ambitious crystallography initiatives, support for cryo-EM lagged far behind. In 1997, when Henderson attended the annual Gordon Research Conference on 3D electron microscopy, a colleague opened the meeting with a provocative statement: cryo-EM was a \u201cniche\u201d method, he said, unlikely to ever supplant X-ray crystallography. But Henderson could see a different future, and he fired back a salvo in the next talk. \u201cI said we should go for global domination of cryo-EM over all the structural methods,\u201d he recalls. \n               The revolution starts here \n             In the years that followed, Henderson, Agard and other cryo-EM evangelists worked methodically on technical improvements to electron microscopes\u00a0\u2014\u00a0in particular, on better ways to sense electrons. Long after digital cameras had taken the world by storm, many electron microscopists still preferred old-fashioned film because it recorded electrons more efficiently than did digital sensors. But, working with microscope manufacturers, the researchers developed a new generation of \u2018direct electron detectors\u2019 that vastly outperforms both film and digital-camera detectors. Available since about 2012, the detectors can capture quick-fire images of an individual molecule at dozens of frames per second. Researchers such as Scheres, meanwhile, have written sophisticated software programs to morph thousands of 2D images into sharp 3D models that, in many cases, match the quality of those deciphered with crystallography. Cryo-EM is suited to large, stable molecules that can withstand electron bombardment without jiggling around\u00a0\u2014\u00a0so molecular machines, often built from dozens of proteins, are good targets. None has proved more suitable than ribosomes, which are braced by rigid twists of RNA. The solution of ribosome structures by X-ray crystallography won three chemists the 2009 Nobel Prize in Chemistry\u00a0\u2014\u00a0but those efforts took decades. In the past couple of years, \u2018ribosomania\u2019 has gripped cryo-EM researchers, and various teams have quickly determined and published dozens of cryo-EM structures of ribosomes from a multitude of organisms, including the first high-resolution models of human ribosomes 4 , 5 . X-ray crystallography has largely fallen by the wayside in the LMB laboratory of Venki Ramakrishnan, who shared the 2009 Nobel. For large molecules, \u201cit\u2019s safe to predict that cryo-EM will largely supersede crystallography\u201d, he says. The rocketing number of cryo-EM publications suggests this to be true: in 2015 alone, the technique has so far been used to map the structures of more than 100 molecules. And, unlike X-ray crystallography, in which crystals lock proteins in a single, static pose, researchers can use cryo-EM to calculate the structure of a protein that has been flash-frozen in several conformations and so deduce the mechanisms by which it works. In May, structural biologist John Rubinstein at the University of Toronto, Canada, and his colleagues used around 100,000 cryo-EM images to create a \u2018molecular movie\u2019 of a rotor-shaped enzyme called V-ATPase, which pumps protons in and out of cell vacuoles by burning ATP 6 . \u201cWhat we saw is that everything is flexible,\u201d Rubinstein says. \u201cIt\u2019s bending and twisting and deforming.\u201d He thinks that the enzyme\u2019s flexibility helps it to efficiently transmit energy released by ATP to the pump. And when a team led by Nogales in 2013 pieced together cryo-EM images of a complex that orchestrates the transcription of DNA into RNA, they discovered that an entire arm swings 100\u2009\u00c5 around the DNA strand like a crane, potentially influencing whether a gene is transcribed 7 . \u201cI\u00a0think this is beautiful,\u201d says Nogales. \u201cIt\u2019s a true insight into how these biological machines work.\u201d \n               Small and beautiful \n             Now that cryo-EM has hit its stride, experts are looking for grander challenges. For many, the most coveted targets are smaller proteins sandwiched in cellular membranes. These tend to be linchpins in cellular signalling pathways, as well as popular drug targets. They are also notoriously difficult to crystallize,\u00a0and imaging individual proteins with cryo-EM is tough because it is harder to extract the signal from the background noise. These hurdles did not stop Yifan Cheng, a biophysicist at the University of California, San Francisco (UCSF), from attempting cryo-EM on a small membrane protein called TRPV1, which detects the molecule that gives chilli peppers their burn and is closely related to other pain-sensing proteins. A team led by his collaborator David Julius, a UCSF physiologist, had failed to crystallize the protein. The cryo-EM project was slow-going at first, but the same technical advances that drove ribosomania produced a 3.4-\u00c5 structure of TRPV1 in late 2013. The report 8  was a thunderbolt to the field, because it showed that cryo-EM could conquer small, medically important molecules. \u201cI literally lost an entire night\u2019s sleep when I saw that,\u201d says Rubinstein. More sleepless nights are likely to follow. \u201cThere\u2019s going to be a huge explosion in the number of membrane-protein structures that get solved,\u201d says Agard. One such solution was that published last month 1  by Scheres, structural biologist Yigong Shi of Tsinghua University in Beijing and their team. They produced a model of \u03b3-secretase \u2014 a protein that makes the amyloid-\u03b2 molecule that is linked to Alzheimer\u2019s disease. The 3.4-\u00c5-resolution map reveals that \u03b3-secretase mutations that cause rare inherited forms of Alzheimer\u2019s map to two \u2018hotspots\u2019 in the enzyme and seem to influence its ability to form toxic amyloid-\u03b2 particles. The structure could help researchers to understand why drugs that inhibit the enzyme have failed in past clinical trials, and help them to design new pharmaceuticals. \u201cStunning\u201d is how Cheng describes the structure. Results such as these are attracting the attention of drug companies hoping to study medically important proteins that have resisted crystallography. Scheres is working with New York-based pharmaceutical giant Pfizer on ion channels, a broad class of membrane protein that includes pain-sensing molecules and neurotransmitter receptors. \u201cI\u2019ve been contacted by almost everybody,\u201d says Nogales of the drug companies lining up at her door. But despite the advances, many in the field see room for further improvement. They hope to devise better electron detectors and better methods for preparing protein samples. This would allow scientists to image proteins that are even smaller and more dynamic, and at even greater resolution than before. A 2.2-\u00c5 structure of a bacterial enzyme published in May 9  showed just how sharp cryo-EM structures can get. Like any burgeoning field, this one has growing pains. Some experts worry that researchers rushing to use the technique could produce problematic results. A 2013 structure of an HIV surface protein 10  was questioned by scientists who said that the images used to build the model were white noise 11 . Since then, X-ray and cryo-EM models generated by other teams have challenged the original model, but the researchers have stood by their result 12 . This June, at the field\u2019s Gordon conference, researchers wanting more quality control passed a resolution urging journals to provide referees with details of how cryo-EM structures were created. Costs could slow the spread of the technology. Scheres estimates that the LMB spends around \u00a33,000 per day running its cryo-EM facility, plus another \u00a31,000 on electricity, most of it for computers needed to store and process the images. \u201cYou\u2019re \u00a34,000 per day lighter if you want to do this. That, for many places, is a very high cost,\u201d he says. To make cryo-EM more accessible, some funders have established shared facilities at which researchers can book time. The Howard Hughes Medical Institute (HHMI) operates a cryo-EM lab on its Janelia Farm Campus in Virginia that is open to HHMI-funded investigators based elsewhere. In the United Kingdom, a national cryo-EM facility funded by the government and the Wellcome Trust opened this year in Didcot, near Oxford. \u201cThere is a real tidal wave of people wanting to learn about it,\u201d says Helen Saibil, a structural biologist at Birkbeck, University of London, who helped to establish the UK facility. Riding the wave is Rod MacKinnon, a biophysicist at Rockefeller University in New York City, who shared the 2003 Nobel Prize in Chemistry for determining the crystal structure of certain ion channels, but who is now deep into cryo-EM. \u201cI\u2019m on a steep slope of a learning curve, which always thrills me,\u201d says MacKinnon, who hopes to use the method to study how ion channels open and close. Henderson\u2019s tongue may have been firmly in his cheek when he declared back in 1997 that cryo-EM could rule the structural-biology world. But nearly 20 years later, his prediction is looking less like hyperbole than it did then. \u201cIf it carries on, and all the technical problems are solved, cryo-EM could indeed become, not just a first choice, but a dominant technology,\u201d he says. \u201cWe are probably halfway there.\u201d \n                 Tweet \n                 Follow @NatureNews \n               \n                     Guts of giant virus imaged in 3D 2015-Mar-02 \n                   \n                     365 days: Nature's 10 2014-Dec-17 \n                   \n                     Crystallography: Atomic secrets 2014-Jan-29 \n                   \n                     Rift widens over structure of HIV\u2019s molecular anchor 2013-Oct-31 \n                   \n                     Centenary: The birth of X-ray crystallography 2012-Nov-07 \n                   \n                     Cell signalling: It's all about the structure 2011-Aug-24 \n                   \n                     Protein structures: Structures of desire 2009-May-06 \n                   \n                     Nature  Crystallography special \n                   \n                     Richard Henderson \n                   \n                     Sjors Scheres \n                   \n                     David Agard \n                   \n                     Eva Nogales \n                   \n                     Yifan Cheng \n                   \n                     John Rubinstein \n                   \n                     UK electron Bio-Imaging Centre \n                   \n                     HHMI Cryo-EM Facility \n                   Reprints and Permissions"},
{"file_id": "525305a", "url": "https://www.nature.com/articles/525305a", "year": 2015, "authors": [], "parsed_as_year": "2006_or_before", "body": "Scientists must work together to save the world. A special issue asks how they can scale disciplinary walls. To solve the grand challenges facing society \u2014 energy, water, climate, food, health \u2014 scientists and social scientists must work together. But research that transcends conventional academic boundaries is harder to fund, do, review and publish \u2014 and those who attempt it struggle for recognition and advancement (see World View,  page 291 ). This special issue examines what governments, funders, journals, universities and academics must do to make interdisciplinary work a joy rather than a curse. A News Feature on  page 308  asks where the modern trend for interdisciplinary research came from \u2014 and finds answers in the proliferation of disciplines in the twentieth century, followed by increasingly urgent calls to bridge them. An analysis of publishing data explores which fields and countries are embracing interdisciplinary research the most, and what impact such research has ( page 306 ). On  page 313 , Rick Rylance, head of Research Councils UK and himself a researcher with one foot in literature and one in neuroscience, explains why interdisciplinarity will be the focus of a 2015\u201316 report from the Global Research Council. Around the world, government funding agencies want to know what it is, whether they should they invest in it, whether they are doing so effectively and, if not, what must change. How can scientists successfully pursue research outside their comfort zone? Some answers come from Rebekah Brown, director of Monash University\u2019s Monash Sustainability Institute in Melbourne, Australia, and her colleagues. They set out five principles for successful interdisciplinary working that they have distilled from years of encouraging researchers of many stripes to seek sustainability solutions ( page\u00a0315 ). Similar ideas help scientists, curators and humanities scholars to work together on a collection that includes clay tablets, papyri, manuscripts and e-mail archives at the John Rylands Research Institute in Manchester, UK, reveals its director, Peter Pormann, on  page 318 . Finally, on  page 319 , Clare Pettitt reassesses the multidisciplinary legacy of Richard Francis Burton \u2014 Victorian explorer, ethnographer, linguist and enthusiastic amateur natural scientist who got some things very wrong, but contributed vastly to knowledge of other cultures and continents. Today\u2019s would-be interdisciplinary scientists can draw many lessons from those of the past \u2014 and can take our polymathy quiz online at  nature.com/inter . \n                 Tweet \n                 Follow @NatureNews \n               \n                     Mind meld 2015-Sep-16 \n                   \n                     Trailblazing cancer\u2013physics project accused of losing ambition 2015-Aug-05 \n                   \n                     Europe\u2019s superlab: Sir Paul\u2019s cathedral 2015-Jun-23 \n                   \n                     Climate change: Embed the social sciences in climate policy 2015-Apr-01 \n                   \n                     Arizona's big bet: The research rethink 2014-Oct-15 \n                   \n                     Science funding: Science for the masses 2010-May-26 \n                   \n                     Nature  special: Interdisciplinarity \n                   Reprints and Permissions"},
{"file_id": "525308a", "url": "https://www.nature.com/articles/525308a", "year": 2015, "authors": [{"name": "Heidi Ledford"}], "parsed_as_year": "2006_or_before", "body": "Interdisciplinarity has become all the rage as scientists tackle climate change and other intractable issues. But there is still strong resistance to crossing borders. Asking for US$40 million is never easy, but Theodore Brown knew his pitch would be a particularly tough sell. As vice-chancellor for research at the University of Illinois at Urbana\u2013Champaign in the early 1980s, Brown had been tasked with soliciting a major donation from wealthy chemist and entrepreneur Arnold Beckman, a graduate of the university. Beckman was hesitant, believing that the university should receive most of its support from the state. So Brown decided to devise a project like nothing he had ever seen before. In 1983, he and his colleagues put together a proposal for an institute that had little chance of being funded through normal channels. It would defy the powerful disciplinary cartography that defines many modern universities, bringing together members of different departments and inducing them to work together on common projects. Brown argued that it would allow faculty members to tackle bigger scientific and societal questions than they normally could. \u201cThe problems challenging us today, the ones really worth working on, are complex, require sophisticated equipment and intellectual tools, and just don't yield to a narrow approach,\u201d he says. \u201cThe traditional structure of university departments and colleges was not conducive to cooperative, interdisciplinary work.\u201d It was an early example of the push for interdisciplinary research that is now sweeping universities around the globe. Although Brown was not completely alone \u2014 the interdisciplinary Santa Fe Institute in New Mexico was founded around the same time \u2014 he was advocating crossing boundaries before it became fashionable. And his proposal met strong resistance. Department heads fretted that faculty members \u2014 and their grants \u2014 would be snatched away. Some colleagues scorned Brown's idea of creating open office spaces to foster interactions between graduate students: surely the din would make it impossible to get serious work done. And then there was the stigma. \u201cInterdisciplinary research is for people who aren't good enough to make it in their own field,\u201d an illustrious physicist chided. But Beckman liked the idea and committed the full $40-million asking price \u2014 at that time, the largest-ever private donation to a US public university. A few hectic years later, the 29,000-square-metre Beckman Institute for Advanced Science and Technology was born. The institute struggled to recruit a qualified director willing to take a chance on the new model, so Brown took the helm. Soon, large grants from organizations such as the Department of Defense and the National Science Foundation poured in, hushing many critics. By the time Brown left the institute in 1993, other leading universities were sending delegations there to learn from the model. Researchers from Beckman \u2014 which now has more than 200 affiliated faculty members \u2014 have achieved attention-grabbing results, including helping to create one of the first graphical web browsers. Since the Beckman was founded, the interdisciplinary model has spread around the world, countering the trend towards specialization that had dominated science since the Second World War. Cross-cutting institutes have sprouted up in the United States, Europe, Japan, China and Australia, among other places, as researchers seek to solve complex problems such as climate change, sustainability and public-health issues. The interdisciplinary trend can be seen in publication data, where more than  one-third of the references in scientific papers now point to other disciplines . \u201cThe problems in the world are not within-discipline problems,\u201d says Sharon Derry, an educational psychologist at the University of North Carolina at Chapel Hill who studies interdisciplinarity. \u201cWe have to bring people with different kinds of skills and expertise together. No one has everything that's needed to deal with the issues that we're facing.\u201d Even so, supporters of interdisciplinary research say that it has been slow to catch on, and those who do cross academic disciplines face major challenges when applying for grants, seeking promotions or submitting papers to high-impact journals. In many cases, scientists say, the trend is nothing more than a fashionable label. \u201cThere's a huge push to call your work interdisciplinary,\u201d says David Wood, a bioengineer at the University of Minnesota in Minneapolis. \u201cBut there's still resistance to doing actual interdisciplinary science.\u201d \n               Highly disciplined \n             The idea of dividing academic inquiry into discrete categories dates back to Plato and Aristotle, but by the sixteenth century, Francis Bacon and other philosophers were mourning the fragmentation of knowledge. One problem lay in the rapid growth of science: there was too much information spread across the disciplines for any one person to handle. Science historian Peter Weingart of Bielefeld University in Germany points to Carl Linnaeus's taxonomic treatise  Systema Naturae  as an example: between its first edition in 1735 and its last in 1768, the catalogue swelled from 10 pages to 2,300, covering 7,000 species. In the nineteenth century, the disciplinary boundaries of the modern university started to take root. The disciplines surged in number and power after the Second World War, as nations, particularly the United States, boosted their research support. \u201cIt's the moment when universities increased exponentially,\u201d says Vincent Larivi\u00e8re, an information scientist at the University of Montreal in Canada. \u201cAnd the size of the university increased by creating more departments.\u201d Tensions between the United States and the Soviet Union also played a part, says Weingart. The Soviets boasted a research programme geared towards solving societal problems, for example improving agriculture to boost food security. By contrast, US President Dwight Eisenhower argued that basic research should be untethered. \u201cIn the field of intellectual exploration, true freedom can and must be practised,\u201d he said in a 1959 speech. And although basic research need not necessarily be disciplinary, it does not have the same pressure towards interdisciplinarity as does applied research. Specialities proliferated as individual disciplines were repeatedly subdivided. Biology was split into botany and zoology, then into evolutionary biology, molecular biology, microbiology, biochemistry, biophysics, bioengineering and more. Late last year, Jerry Jacobs, a sociologist at the University of Pennsylvania in Philadelphia, counted the number of biology-related departments at Michigan State University in East Lansing. There were nearly 40. From this thicket, the term 'interdisciplinary' emerged. The earliest citation in the  Oxford English Dictionary  dates back to December 1937, in a sociology journal. But even at that time, some believed that the word was already overused. In a report to the US Social Science Research Council in August that year, a sociologist at the University of Chicago in Illinois lumped 'interdisciplinarity' in with other \u201ccatch phrases and slogans which were not sufficiently critically examined\u201d ( R. Frank  Items   40,  73\u201378; 1988 ). As an academic movement, interdisciplinarity caught on during the 1970s and has been growing ever since, says Larivi\u00e8re. He credits that rise in part to libraries, which began to stockpile subscriptions and improved researchers' access to journals in alternative fields. A particle physicist could more easily browse biology journals, say. Furthermore, the US focus began to shift from basic research and scientific liberty back to societal problems such as environmental protection, which can rarely be tackled by a single discipline. The United States was not alone: in 1994, an influential book partially sponsored by the Swedish Council for Planning and Coordination of Research called  The New Production of Knowledge  (Sage) predicted, among other things, an increasingly interdisciplinary future as science seeks to solve socially relevant questions. That book had an impact, says Larivi\u00e8re, particularly in the European Union's Fifth Framework funding programme, which ran from 1998 to 2002 and emphasized interdisciplinary, problem-oriented research. Soon, interdisciplinary institutes began to sprout up around the world, each with its own unique structure and purpose. One of the first, the Santa Fe Institute, founded in 1984, focused on applying advanced mathematics and computational skills to a range of disciplines. Others, such as the Massachusetts Institute of Technology's David H. Koch Institute for Integrative Cancer Research in Cambridge, or the neuroscience-focused Janelia Research Campus in Ashburn, Virginia, tackle questions within a specific discipline but draw in work from other fields. And some, such as the Monash Sustainability Institute in Clayton, Australia, focus on specific problems. Even as the trend gained momentum, interdisciplinary researchers continued to hit the same hurdles that Brown had encountered. In 1998, chemist Richard Zare at Stanford University in California helped to launch the interdisciplinary institute Bio-X. But an influential colleague urged him not to move his lab into the Bio-X building. Doing so would essentially take Zare away from the chemistry department and his committee and teaching duties there, the colleague argued, weakening the department. Although he was well established, Zare worried about going against the establishment. \u201cIt was very serious,\u201d he says. The risk is even greater for young professors seeking tenure, he notes. In 2004, in response to the growing interest in interdisciplinary work \u2014 and the challenges that face those who attempt it \u2014 the US National Academies released a report called  Facilitating Interdisciplinary Research . The authors advised institutions to lower barriers, for example by making budgets flexible so that costs could be shared across departments. The publication drew a large audience. It has been downloaded more than 7,600 times and had impact beyond US shores. At Durham University, UK, says physicist Tom McLeish, administrators referred to the report when they were forging a series of on-campus interdisciplinary centres. Around that time, McLeish was serving as pro-vice-chancellor of research, and saw interdisciplinarity as a way to make the small university shine on the world stage. He battled with department chairs who feared that the centres would reduce their budgets, and he worked to set up a promotion system that rewards investigators on large team grants in the same way as those on single-investigator grants. The university now has interdisciplinary centres on topics ranging from resilience \u2014 both ecological and psychological \u2014 to the history of medieval science. The interdisciplinary trend is also growing in Asia. In 2000, the National Natural Science Foundation of China (NSFC) laid out a plan for interdisciplinary research, and universities have launched several cross-cutting centres over the past decade, including the Academy for Advanced Interdisciplinary Studies at Peking University in Beijing. The NSFC plans to launch further interdisciplinary projects in the coming years, says Yonghe Zheng, deputy director-general of the foundation's Bureau of Science Policy. \u201cChina is a developing country,\u201d he says. \u201cSo the universities and institutes can quickly set up some new centres which reflect the new trend in interdisciplinary research.\u201d Nanyang Technological University in Singapore established its Interdisciplinary Graduate School in 2012; it already has 335 students, out of a total graduate-school population of 2,000. Nanyang's interdisciplinary graduate programme, which bills itself as the first of its kind in Asia, was designed in part to expand the university's fundraising options, says Bo Liedberg, dean of the programme. Because industry is often focused on real-world problems that cross disciplines, an interdisciplinary programme could foster more collaborations with business, he reasons. That focus on interdisciplinarity as a revenue stream is widespread, says Merlin Crossley, a molecular biologist and dean of the faculty of life sciences at the University of New South Wales in Sydney, Australia. \u201cThere is constant pressure on me to make a cross-faculty, cross-institution alliance,\u201d he says. \u201cIf I want to build a new building, the more allies I have, the easier it is to raise the money.\u201d Arizona State University in Tempe saw its federal funding rise by 162% from 2003 to 2012 as it  promoted interdisciplinarity across its campus . Despite this pressure, interdisciplinarity's reach remains modest. For every Nanyang or Durham, there are hundreds of universities that have not embraced significant change. Departmental dividers remain in place \u2014 and in power \u2014 at most institutions, says Nancy Andreasen, a neuroscientist at the University of Iowa in Iowa City who co-chaired the committee that wrote the National Academies report more than a decade ago. \u201cIt has been an enormous disappointment.\u201d \n               Team work \n             For institutions or programmes that have embraced interdisciplinarity, the transition has not always been easy. The most common mistake is underestimating the depth of commitment and personal relationships needed for a successful interdisciplinary project, says Laura Meagher, a consultant based near St Andrews, UK, who coaches interdisciplinary teams. \u201cYou see people who think it's not much more than stapling a bunch of CVs to the back of a proposal,\u201d she says. \u201cThey don't realize that it takes time to build a relationship.\u201d When the push for collaboration comes from the top, some of that focus on personal relationships could be lost \u2014 leaving the project to suffer, she says. The UK Energy Research Centre (UKERC) in London, which since 2004 has coordinated and carried out sustainable-energy research, learned how delicate interdisciplinary relationships can be, says Mark Winskel, a social and political scientist at the University of Edinburgh who evaluated the centre's first decade. Its initial five-year phase went well, he says, and culminated in a key publication:  Energy 2050 , which synthesized the institution's results and translated them into recommendations. But the next five-year phase failed to produce a similar achievement. Winskel surveyed members and found that changes in the UKERC's structure designed to open it to a wider community \u2014 for example by offering several rounds of fresh grants in the middle of phase two \u2014 had upset some established long-term relationships. \u201cWe became a more diverse community of scholars and disciplines,\u201d he says. \u201cBut that also means you become less cohesive.\u201d The UKERC learned from the experience: its third phase, launched in May 2014, aims to provide more stability for collaborative relationships. Social scientists in particular often face that lack of cohesion , says Thomas Heberlein, a social psychologist at the University of Wisconsin\u2013Madison. When funders emphasize the societal impacts of the work they support, social scientists are often called in to assess the broader implications of a project. But, he says, it is obvious \u2014 and insulting \u2014 when a social scientist is asked to join a project as a way to tick a box, without a true commitment to incorporating the discipline into the project. \n               Social struggle \n             Several UK studies have found that social scientists are less likely than researchers in other disciplines to want to participate in interdisciplinary projects. For Heberlein, who has long collaborated with ecologists and environmental scientists, one of the stumbling blocks is what he calls \u201cthe hegemony of the natural sciences\u201d. Those disciplines tend to be held in higher esteem than more qualitative fields such as the social sciences, and they are  deemed more rigorous by funders and researchers , he says. That imbalance leads to frustration and undermines collaboration. Heberlein, whose speciality is in conducting surveys of public opinions, says that natural scientists often naively suggest that they can design and execute surveys themselves using an Internet tool such as SurveyMonkey. Heberlein disagrees: \u201cIt's really hard to do the stuff we do,\u201d he says. \u201cOur measurements are complicated.\u201d Lack of respect can run in many directions when different kinds of researchers come together. Wood says that bioengineers are always cautioned against having their grants reviewed by panels of biologists, who may be dismissive of engineering research goals and measurements. But he has also served on review panels in which engineers have recoiled at the limitations of clinical research. As more researchers become involved with interdisciplinary work, the mutual suspicion has started to ease. There have also been some signs of success in the funding arena. The US National Institutes of Health (NIH), for example, says that interdisciplinary proposals fare as well as, or slightly better than, more conventional applications. The European Research Council, by contrast, has noted that interdisciplinary grant proposals on average do not fare as well in review panels as projects that are narrower in scope. The atmosphere for publishing is also mixed. Interdisciplinary researchers have long complained that it is difficult to get their papers into top-tier disciplinary journals. Heberlein says that the rise of interdisciplinary journals has helped in his field, but he worries about the standard of some of the papers they publish. And he questions the wisdom of training graduate students across disciplines before they have immersed themselves in the rigours of one area. \u201cYou've got to develop your disciplinary skills first,\u201d he says. \u201cThe bad news is the quality of this research is pretty bad and may be getting worse.\u201d Many view the institutional push for interdisciplinarity as an experiment in progress. \u201cThe celebrations have begun, but the actual data on what kind of difference this makes are not in,\u201d says Scott Frickel, a sociologist at Brown University in Providence, Rhode Island. As more institutions adopt new ways to organize research, some are also trying to rethink their assessment processes, says McLeish. In July, Veronica Strang at Durham and McLeish released a report called  Evaluating Interdisciplinary Research , and he was surprised when academic societies and funders flocked to learn more. \u201cWe didn't anticipate that we'd be launching this report into an atmosphere where everyone wants to know this,\u201d he says. And the pace of change varies across the globe. In the United States, the NIH ran a programme to stimulate interdisciplinary research from 2004 to 2012. It resulted in some changes, such as starting to recognize multiple principal investigators on what had been considered single-investigator grants \u2014 a switch that removed a disincentive to collaborate. Since then, the agency has not perceived a need to follow up with any other incentives, noting that there are more than 4,000 active NIH-funded research projects that bill themselves as interdisciplinary. \u201cOur general sense is that interdisciplinary research has become a very standard way of doing science,\u201d says Betsy Wilder, head of the NIH Office of Strategic Coordination. \u201cIt really pervades NIH funding.\u201d In some other countries, the experiment has just begun. Chemist Ayyappanpillai Ajayaghosh, director of the National Institute for Interdisciplinary Science and Technology in Thiruvananthapuram, India, says that momentum is building in his country to promote more interdisciplinary projects. In Japan, theoretical physicist Tetsuo Hatsuda left the University of Tokyo in part because he felt that the boundaries between disciplines were too heavily enforced there. In 2013, he joined the RIKEN research institute in Wako, Japan, and launched an interdisciplinary team of theoretical physicists, chemists and biologists to work out techniques that will accelerate all three fields. He hopes that the effort will stimulate more interdisciplinary work in the country. \u201cJapan is a little behind other countries,\u201d he says. \u201cTheoretical science is a good starting point because it is easy for us to interact.\u201d Some 25 years after it opened, the Beckman Institute's experiment in interdisciplinary research has been a success, says Brown. The centre continues to attract distinguished faculty members and large team grants \u2014 last year it won a research contract worth up to $12.7 million from the federal government's Intelligence Advanced Research Projects Activity programme \u2014 even though competition for such money has increased as more universities build interdisciplinary teams. And Brown bristles at the suggestion that the global push for interdisciplinarity might be a fad. \u201cThe answer is a resounding 'no',\u201d he says. \u201cThings have changed \u2014 now people focus on big problems, and if you go for a big problem you need to be interdisciplinary.\u201d \n                 Tweet \n                 Follow @NatureNews \n               \n                 See Editorial \n                 page 289 \n               \n                     Why interdisciplinary research matters 2015-Sep-16 \n                   \n                     Mind meld 2015-Sep-16 \n                   \n                     Interdisciplinary research by the numbers 2015-Sep-16 \n                   \n                     Trailblazing cancer\u2013physics project accused of losing ambition 2015-Aug-05 \n                   \n                     Europe\u2019s superlab: Sir Paul\u2019s cathedral 2015-Jun-23 \n                   \n                     Climate change: Embed the social sciences in climate policy 2015-Apr-01 \n                   \n                     Arizona's big bet: The research rethink 2014-Oct-15 \n                   \n                     Science funding: Science for the masses 2010-May-26 \n                   \n                     Nature  special: Interdisciplinarity \n                   \n                     US National Research Council: Facilitating Interdisciplinary Research \n                   Reprints and Permissions"},
{"file_id": "525306a", "url": "https://www.nature.com/articles/525306a", "year": 2015, "authors": [{"name": "Richard Van Noorden"}], "parsed_as_year": "2006_or_before", "body": "An analysis reveals the extent and impact of research that bridges disciplines. Interdisciplinary work is considered crucial by scientists, policymakers and funders \u2014 but how widespread is it really, and what impact does it have? Scholars say that the concept is complex to define and measure, but efforts to map papers by the disciplines of the journals they appear in and by their citation patterns are \u2014 tentatively\u00a0\u2014\u00a0revealing the growth and influence of interdisciplinary research. \n               INTERDISCIPLINARY RESEARCH IS ON THE RISE \n             References  Since the mid-1980s, research papers have increasingly cited work outside their own disciplines. The analysis shown here used journal names to assign more than 35 million papers in the Web of Science to 14 major conventional disciplines (such as biology or physics) and 143 specialities. The fraction of paper references that point to work in other disciplines is increasing in both the natural and the social sciences. The fraction that points to another speciality in the same discipline (for example, a genetics paper pointing to zoology) shows a slight decline. Rhetoric  Discourse about interdisciplinary research is increasing. The fraction of papers that mention interdisciplinarity in their title has fluctuated, perhaps reflecting the priorities of funders, but the twenty-first century saw that proportion reach an all-time high. \n               INTERDISCIPLINARY RESEARCH TAKES TIME TO HAVE AN IMPACT \n             Whether interdisciplinary research gains more citations than disciplinary research is contentious. Over three years, papers with diverse references tend to pick up fewer citations than the norm, but over 13\u00a0years they gain more. Some studies suggest that a little interdisciplinarity is better than a lot: papers that combine very disparate fields tend to earn fewer citations. But interdisciplinary work can have broad societal and economic impacts that are not captured by citations. \n               SOME FIELDS ARE MORE INTERDISCIPLINARY THAN OTHERS\u2009\u2026 \n             In this chart, more interdisciplinary fields are in the top-right quadrant. From 1950 to 2014, a field\u2019s position is determined by how much its papers cite outside disciplines ( x -axis), and by how much outside disciplines subsequently cite its papers ( y -axis). (Some years, certain fields have too few references to be plotted.) As a whole, interdisciplinarity declines until the mid-1970s, then rises. Over time, disciplines tend to cluster towards the diagonal. That\u2019s partly a statistical artefact: emerging fields are small with few references from other fields, and then settle into position as they become larger and established.  boxed-text \n               \u2026\u2009AND SO ARE SOME COUNTRIES \n             A 2015 study by researchers with the publisher Elsevier defined interdisciplinary papers as those that reference journals that are rarely cited together. The report looked only at countries that routinely publish more than 30,000 papers per year to find the \u2018most interdisciplinary\u2019 countries for 2013. A separate analysis counted the proportion of a paper\u2019s references that are in other disciplines. After totting up all the papers for each country, and normalizing the results (so that average interdisciplinarity = 1), similar nations emerge on top for 2013. \n                 Tweet \n                 Follow @NatureNews \n               \n                     Why interdisciplinary research matters 2015-Sep-16 \n                   \n                     Mind meld 2015-Sep-16 \n                   \n                     How to solve the world's biggest problems 2015-Sep-16 \n                   \n                     Trailblazing cancer\u2013physics project accused of losing ambition 2015-Aug-05 \n                   \n                     Europe\u2019s superlab: Sir Paul\u2019s cathedral 2015-Jun-23 \n                   \n                     Climate change: Embed the social sciences in climate policy 2015-Apr-01 \n                   \n                     Arizona's big bet: The research rethink 2014-Oct-15 \n                   \n                     Science funding: Science for the masses 2010-May-26 \n                   \n                     Nature  special: Interdisciplinarity \n                   \n                     US National Research Council: Facilitating Interdisciplinary Research \n                   Reprints and Permissions"},
{"file_id": "526024a", "url": "https://www.nature.com/articles/526024a", "year": 2015, "authors": [{"name": "Alexandra Witze"}], "parsed_as_year": "2006_or_before", "body": "Plans to build one of the world's biggest telescopes on Mauna Kea in Hawaii are mired in conflict. Four people involved in the fight explain their diverse views. A low, mournful note rings across the broad summit of Mauna Kea, the highest peak in Hawaii. Joshua Lanakila Mangauil lowers his conch-shell horn and begins to walk over the craggy volcanic rock. Behind him come a dozen other Native Hawaiian men and women, many carrying the red, white and blue state flag. Together, they sing traditional chants as they hike up a volcanic ridge, headed for the top of the mountain. On this July day, Mangauil is leading a reverent but short visit to the 4,200-metre-high summit. Native Hawaiian tradition calls for visitors to pay their respects to the sacred  mauna , or mountain, and then leave. The group planned to stay a few hours, then get back into its pickup trucks and drive to a camp farther down. There they would resume the task that has consumed Mangauil and others for the greater part of a year: protecting the  mauna  from an effort to build a massive telescope. An international consortium plans to construct the Thirty Meter Telescope (TMT) on top of Mauna Kea on Hawaii's Big Island. A cutting-edge astronomical facility, it would have a light-gathering mirror 3 times bigger than any of the 13 other telescopes already on the mountain, which include some of the largest and most scientifically productive observatories in the world. TMT construction began in April and stopped almost immediately when demonstrators led by Mangauil blocked the vehicles from reaching the summit. They say that the TMT would violate both a fragile ecosystem and indigenous rights that have not been properly valued by astronomers. \u201cBefore you look into space, you need to respect this place,\u201d Mangauil says. The battle that erupted this year echoes previous clashes. Native Hawaiians, environmentalists, scientists and other interest groups  have wrangled for decades  over the environmental and cultural impacts of the summit telescopes (see 'Peak passions'). But the fight over the TMT has grown larger and more divisive than past debates. Thanks to a renaissance of Native Hawaiian pride and the flashpoint potential of social media, the protests have tapped into broader anger against the US government and its past behaviour towards the islands and their native peoples. \n               boxed-text \n             Mangauil is adamant that the telescope should not be built \u2014 but others have equally passionate, divergent perspectives. Here,  Nature  talks to four people living in Hawaii and involved in the controversy, whose differing viewpoints reveal the complexity of finding common ground and securing the future of astronomy there. \n               The opponent \n             When Mangauil is not paying tribute on Mauna Kea's summit, he can often be found about 1,400 metres below, at a stopping point that includes a visitor's centre and a makeshift camp set up by demonstrators in March. As one of the leaders of the protest group there, Mangauil frequently talks to supporters, tourists, journalists \u2014 anyone who stops by. In late July, the camp consisted of a small thatched hut, of traditional design, in which a Native Hawaiian man wearing a loincloth and shoulder cape was waiting to describe cultural customs to any visitors. Half a dozen other protesters stood around talking to each other quietly. Next to the hut, in a large tarpaulin enclosure amid camping beds and food supplies, Mangauil was working on his mobile phone, wearing a T-shirt and hoodie. He broke away to talk to a group of children who had travelled from the neighbouring island of Maui to see him. Many wore the bright-red  k\u016b kia'i mauna , or 'guardians of the mountain', T-shirts that have become popular among TMT protestors. Mangauil bent over to greet several of the students, touching his nose to theirs in the traditional manner that symbolizes an exchange of breath. \u201cOur creation is connected to this mountain,\u201d he explained, as some of the kids recorded him on their smartphones. In Hawaiian history, the sky father and the earth mother came together to create the Big Island, and Mauna Kea is the centre. \u201cThis mountain is the oldest sibling that watches over all of us,\u201d Mangauil said. \u201cIt collects the clouds, it channels the water, it gives us life.\u201d Mangauil has felt this connection since his childhood in a rainy town beneath Mauna Kea's northern slopes. \u201cThe river behind my house comes from this place,\u201d he says. \u201cThis has always been my mountain.\u201d Like many in the younger generation of Native Hawaiians \u2014 Mangauil is 28 \u2014 he attended a Hawaiian-language immersion school. Such institutions have helped to reinvigorate Hawaiian culture after a long period during which it was suppressed. In Mangauil's grandparents' generation, children at school were beaten if they spoke their native language, and heard more about George Washington than about their own Kamehameha the Great, who united the Hawaiian islands two centuries ago. But beginning in the 1970s, activists began to push back. Now, Hawaiian students can choose from a variety of immersion programmes in public and charter schools across the islands. Mangauil says that he was too young to be involved when the TMT began the seven-year process of obtaining state permits to build on Mauna Kea, but one of his teachers was a long-time activist fighting development on the mountain. Through her, he grew familiar with the issues surrounding Mauna Kea. After graduating, he returned to his school to work as a teacher. As his interest in cultural matters grew, Mangauil set up a business to consult on Native Hawaiian issues and began spending more time on Mauna Kea. In October 2014, when TMT officials organized a groundbreaking ceremony with visiting dignitaries, Mangauil surprised them \u2014 and himself \u2014 by jumping in front of the cameras and denouncing the project. \u201cThat was not planned,\u201d he says. \u201cI was upset.\u201d Six months later, Mangauil was again in the front of a protest group blocking the path of TMT construction trucks. He and 30 other demonstrators were arrested, booked and released. (A wealthy descendant of Hawaii's monarchy has put up much of the bail money.) Now Mangauil spends most of his time in the role of mountain guardian: leading protests, testifying at hearings and travelling to other islands to meet with activists. He helps to ignite demonstrations through his popular Facebook pages. He is also dipping his toe into politics, by running for a seat in a newly formed group that aims to build a governing base for a future Hawaiian nation. (Hawaii's monarchy was overthrown in 1893 by pro-US interests; it became a US state in 1959.) Mangauil's more-immediate goal is simple: to stop the TMT from being built on Mauna Kea. \u201cWe are fighting for the rights of the mountain,\u201d he says. \u201cI have nothing against astronomy \u2014 just don't put it up there.\u201d Looking ahead, Mangauil sees a day when astronomers will leave Mauna Kea. The existing telescopes are legally allowed to operate there until their lease to the site expires in 2033. At that point, Mangauil argues, they should all be dismantled. \u201cThen the mountain can rest.\u201d \n               The supporter \n             As Mangauil sounds his conch on Mauna Kea's summit, Alexis Acohido stands nearby trying to explain her feelings about the protest movement. \u201cI'm conflicted,\u201d she says finally. \u201cI'm really conflicted.\u201d Acohido, aged 22 and part Native Hawaiian, is of Mangauil's generation but not his mindset. Growing up on the island of Oahu near the heart of Honolulu, Acohido was always drawn towards science. \u201cIn high school, I wanted to be a biologist, but when I got to college I had all these math credits and thought, why don't I get my degree in math?\u201d she says. Acohido first heard of the TMT several years ago, while sitting in an orientation lecture for a summer astronomy internship. A project scientist spoke about how the TMT would see stars and galaxies with unprecedented clarity, better even than today's views from the Hubble Space Telescope. \u201cWhat they had planned sounded really awesome,\u201d she says. \u201cI thought it would be cool to have for Hawaii.\u201d So when TMT protests began to spread this spring, she decided to become more active. In a debate during a philosophy class, she spoke out in favour of the TMT, and a fellow student asked her to write an opinion piece for the university's newspaper. After that appeared, the public-relations firm that has been handling TMT affairs asked her to expand on her ideas for Honolulu's major newspaper, and so she published a commentary there in April. Acohido argued that the TMT should be built and that it would bring opportunities to students in Hawaii, a state that has typically ranked below US averages in school performance. (Among other contributions, the TMT has set up an educational fund that awards US$1 million annually to Big Island students in technical fields.) Her article did not go down well. Many people told her \u201cHow could you? You need to get back to your roots\u201d, she says. Some people who commented online claimed she had no cultural authority to speak about Native Hawaiian issues and had been brainwashed by TMT leaders. \u201cI've been called a bad Hawaiian so much it's not funny,\u201d she says. Acohido says that she even faced opposition from some members of her family. But others have offered encouragement, including her Native-Hawaiian grandmother. \u201cShe's been super-supportive,\u201d Acohido says. \u201cShe'll always save any news piece that comes out, for me to read when I get home.\u201d In many ways, Acohido represents the next generation of Hawaiian scientists that the TMT and other observatories hope to foster. She graduated with her mathematics degree earlier this year and now works as a communications intern in the Hilo offices of Gemini Observatory, which runs an 8-metre telescope on Mauna Kea. The day she saw Mangauil was her first time at the summit. She handed over her smartphone so she could get a photo of herself with the gleaming Gemini dome behind her. Watching the demonstrators, Acohido talked about the conflict she feels. \u201cIt's important to uphold your cultures and traditions, but I also think it's important to pick your battles,\u201d she says. \u201cA lot of their anger is misplaced.\u201d In her view, they should focus their wrath on the University of Hawaii (UH), which has managed the mountain observatories since the 1960s. \u201cIf they are going to be mad at anyone,\u201d she says, \u201cthey should be mad at UH.\u201d \n               The battle-scarred astronomer \n             The UH's Institute for Astronomy sits above the urban bustle of Honolulu in a neighbourhood of quiet winding streets. Inside, on a muggy August morning, astronomer Bob McLaren sighs at the current chaos engulfing Hawaiian astronomy. \u201cThe core disagreement is simple, but there's no easy solution,\u201d he says. McLaren knows these battles all too well. As the university representative tasked with developing astronomy facilities, he has taken part in some of the most contentious fights over the future of Mauna Kea. He came to Hawaii from Canada's University of Toronto in 1982, drawn to Mauna Kea as the best site to do infrared astronomy. Observing variable stars with the 4-metre Canada-France-Hawaii Telescope, he helped to recalibrate the distances to many nearby galaxies. But as larger telescopes started to come online, McLaren saw a role in helping to manage how astronomy was conducted on the mountain. He moved into his UH administration job and onto the front lines of public battles. Some of his most painful professional memories are from the 1990s, when university and state officials were working to adopt a master plan for the mountain's future. In a blistering 1998 report, auditors slammed the university for failing to balance telescope development with Mauna Kea's archaeological, cultural and environmental resources. Among its criticisms, the assessment noted that Native Hawaiian cultural practitioners \u2014 generally older men and women who travel to the mountain's summit to carry out personal devotions \u2014 charged that rubbish and development had desecrated it. Relations between astronomers and Native Hawaiians deteriorated so much that the late Senator Daniel Inouye, a legendary figure in Hawaiian politics, had to intervene. He forced nine people from each side, including McLaren as the institute's associate director, to sit down and talk out their differences. \u201cIt was kind of awkward at first, but it actually worked,\u201d McLaren says. \u201cWe could discuss why we thought certain things and why they thought certain things, and what we were going to do about it. But it took time.\u201d Those 1999 talks helped to get a comprehensive plan for the mountain approved. And some of the Native Hawaiians on the panel formed a cultural advisory council that provides input into the management of Mauna Kea. It was a rare example of people with different interests managing to have a productive dialogue about the mountain's future, McLaren says. Despite all the conversations, however, people were uncomfortable with the concept of future development, so the final master plan failed to lay out a clear sense of whether and how big telescopes could be built on the mountain. \u201cWe weren't able to achieve that,\u201d says McLaren. \u201cThat was a bit disappointing.\u201d In 2006, in the wake of the master-plan controversy, associated lawsuits and strong opposition, NASA pulled funding for a project that would have added up to six 'outrigger' telescopes to the twin 10-metre Keck telescopes, currently the largest atop Mauna Kea. Even though the master plan did not go as far as McLaren had wanted, he says that it has helped to shape the current project. When the TMT team decided in 2009 to build on Mauna Kea, it worked within the guidelines of the plan to minimize the telescope's impact on the mountain. Physically, the dome is slated to sit about 150 metres below the summit ridge, making it less prominent. Project officials consulted with a number of Native Hawaiian groups, including the Mauna Kea cultural advisory council, and made plans to limit traffic to the summit and have local voices deeply involved in all stages of construction. The TMT is also the first Mauna Kea telescope with more than a token rent; it will pay $1 million a year for its space, with most of that flowing directly to mountain stewardship. That made it all the more surprising \u2014 at least to astronomers \u2014 when Mangauil and others jumped in front of the cameras at the groundbreaking ceremony last October. \u201cI can't explain what suddenly got all of these new people involved,\u201d says McLaren. In the past, \u201cthere were elements of this sovereignty movement and disenfranchisement in there, but they were secondary\u201d, he says. \u201cNow it's a lot more complicated. Many of the people we're hearing speak would like to have a society that feels more Hawaiian to them, in the spirit of things they've been taught in school.\u201d McLaren is frustrated by what he sees as changing cultural values among all the groups with a stake in the mountain. \u201cI'd like people to tell me, if they got in a time machine and went back to 1964, what they would have done differently,\u201d he says. \u201cDon't just criticize what's on the mountain now \u2014 tell us what we did wrong, in the context of what actually happened.\u201d Most days, McLaren is optimistic that the TMT will be built in Hawaii. Other days, he cannot quite see a way through the conflict. \u201cPeople like me get a little cynical,\u201d he says. \u201cWe've seen this movie before.\u201d \n               The bridge builder \n             The discord might be familiar, but Doug Simons refuses to give up hope. Simons has worked in Hawaii for three decades, including stints directing Gemini and now the Canada-France-Hawaii Telescope. He specialized in developing instruments to study the Universe in infrared wavelengths, and is now thinking of ways in which the diverse observatories on Mauna Kea can work together more closely. Simons's ties to the mountain go far beyond his work. As someone who hunts birds on Mauna Kea, he wants to see its environment preserved and he has even made plans to have his ashes scattered nearby. \u201cIt tears me up to see my community being torn apart,\u201d he says. \u201cAt the end of the day, it's not me the astronomer, it's me the Big Island resident that has made me commit to finding a solution for my neighbours.\u201d So Simons has been meeting with anyone who wants to talk to him: Native Hawaiians whose families are divided, businessmen who wonder whether Hawaii will invest in high-tech industries, secondary-school students who desire local well-paying technical jobs and those who want the telescopes removed. He does it the old-fashioned way, one face-to-face sit-down at a time. \u201cI'm kind of old-timey,\u201d he says. \u201cI'm not a Facebook guy.\u201d Sometimes these conversations happen by chance: Simons recently ran into Mangauil at an airport and the two shared a beer. The discussions are always deeply personal. \u201cYou have to come out of your comfort zone as a scientist and get into the emotional arena to make the connection,\u201d he says. When speaking with Native Hawaiians about Mauna Kea, he opens up about his quiet Catholic faith, his daily prayers. \u201cIt's my spiritual component trying to map out to theirs,\u201d he says. He hopes and believes that these conversations will make a difference. \u201cThere is no way to make everybody happy on the mountain,\u201d he says. \u201cHistorically, these things have been worked through some sort of give-and-take process \u2014 I don't see why that can't happen here.\u201d Some compromises have already been found. The UH is working through long-held plans to decommission some of the older telescopes on Mauna Kea and return the land to its natural state.  Two are slated to be removed  soon, and a third demolition is expected to be announced by the end of this year. Meanwhile, legal challenges to the TMT continue to wend their way through Hawaii's courts. But the state's governor has said that the project has the authority to proceed, and unless the highest court rules otherwise, the TMT can begin construction again. For now, the TMT and the demonstrators remain in an uneasy stand-off. Most recently, in mid-September, Mangauil and his colleagues agreed to leave their encampment partway up Mauna Kea, which they had occupied continuously since March. As  Nature  went to press, the TMT had not announced when it might try to resume construction. In the meantime, Simons continues to try building bridges whenever possible. In early August, he was one of more than 3,000 astronomers in Honolulu for a meeting of the International Astronomical Union. Most were not from Hawaii, and Simons arranged for Mangauil to give a private presentation to around 30 interested astronomers \u2014 \u201cwhat I would call a one-hour classroom tutorial in his perspective\u201d, Simons says. But Mangauil did more than that. A week before his presentation,  he gathered some three dozen demonstrators  outside the convention centre where the meeting was taking place. He and a UH historian held a press conference there, describing their grievances against the state, the university and the TMT in particular. After the talks were over and journalists had asked their questions, someone broke out a ukulele and led the crowd in traditional song. About 20 metres away, some astronomers peered out curiously from inside the glass-walled conference centre. Most continued to travel up and down the main escalator on their way to poster sessions and talks about the Galactic Centre and the origin of the Universe. One of the very few scientists who left the building to mingle with the demonstrators was Simons. He walked among them, head bowed in conversation. \n                 Tweet \n                 Follow @NatureNews \n               \n                     Hawaiian telescope project sparks protests at astronomy meeting 2015-Aug-04 \n                   \n                     Hawaii prunes Mauna Kea telescope hub 2015-Jun-02 \n                   \n                     Hawaiian telescope fight prompts new rules for Mauna Kea 2015-May-27 \n                   \n                     Hawaiian telescope project seeks way forward amid protests 2015-Apr-22 \n                   \n                     Q&A: Mountain guardian 2013-Jul-24 \n                   \n                       Mauna Kea Observatories \n                   \n                       Thirty Meter Telescope \n                   \n                       \u2018Oiwi TV microsite on Mauna Kea \n                   Reprints and Permissions"},
{"file_id": "525440a", "url": "https://www.nature.com/articles/525440a", "year": 2015, "authors": [{"name": "Erika Check Hayden"}], "parsed_as_year": "2006_or_before", "body": "The data contained in tax returns, health and welfare records could be a gold mine for scientists \u2014 but only if they can protect people's identities. In 2011, six US economists tackled a question at the heart of education policy: how much does great teaching help children in the long run? They started with the records of more than 11,500 Tennessee schoolchildren who, as part of an experiment in the 1980s, had been randomly assigned to high- and average-quality teachers between the ages of five and eight. Then they gauged the children's earnings as adults from federal tax returns filed in the 2000s. The analysis 1  showed that the benefits of a good early education last for decades: each year of better teaching in childhood boosted an individual's annual earnings by some 3.5% on average. Other data showed the same individuals besting their peers on measures such as university attendance, retirement savings, marriage rates and home ownership. Erika Check Hayden discusses the conflict between private data and research The economists' work was widely hailed in education-policy circles, and US President Barack Obama cited it in his 2012 State of the Union address when he called for more investment in teacher training. But for many social scientists, the most impressive thing was that the authors had been able to examine US federal tax returns: a closely guarded data set that was then available to researchers only with tight restrictions. This has made the study an emblem for both the challenges and the enormous potential power of 'administrative data' \u2014 information collected during routine provision of services, including tax returns, records of welfare benefits, data on visits to doctors and hospitals, and criminal records. Unlike Internet searches, social-media posts and the rest of the digital trails that people establish in their daily lives, administrative data cover entire populations with minimal self-selection effects: in the US census, for example, everyone sampled is required by law to respond and tell the truth. This puts administrative data sets at the frontier of social science, says John Friedman, an economist at Brown University in Providence, Rhode Island, and one of the lead authors of the education study 1 . \u201cThey allow researchers to not just get at old questions in a new way,\u201d he says, \u201cbut to come at problems that were completely impossible before.\u201d \n               Probing the population \n             In the past few years, administrative data have been used to investigate issues ranging from the side effects of vaccines 2  to the lasting impact of a child's neighbourhood on his or her ability to earn and prosper as an adult 3 . Proponents say that these rich information sources could greatly improve how governments measure the effectiveness of social programmes such as providing stipends to help families move to more resource-rich neighbourhoods. But there is also concern that the rush to use these data could pose new threats to citizens' privacy. \u201cThe types of protections that we're used to thinking about have been based on the twin pillars of anonymity and informed consent, and neither of those hold in this new world,\u201d says Julia Lane, an economist at New York University. In 2013, for instance, researchers showed that they could  uncover the identities of supposedly anonymous participants in a genetic study  simply by cross-referencing their data with publicly available genealogical information. Many people are looking for ways to address these concerns without inhibiting research. Suggested solutions include policy measures, such as an international code of conduct for data privacy, and technical methods that allow the use of the data while protecting privacy. Crucially, notes Lane, although preserving privacy sometimes complicates researchers' lives, it is necessary to uphold the public trust that makes the work possible. \u201cDifficulty in access is a feature, not a bug,\u201d she says. \u201cIt should be hard to get access to data, but it's very important that such access be made possible.\u201d Many nations collect administrative data on a massive scale, but only a few, notably in northern Europe, have so far made it easy for researchers to use those data. In Denmark, for instance, every newborn child is assigned a unique identification number that tracks his or her lifelong interactions with the country's free health-care system and almost every other government service. In 2002, researchers used data gathered through this identification system to retrospectively analyse the vaccination and health status of almost every child born in the country from 1991 to 1998 \u2014 537,000 in all. At the time, it was the largest study ever to disprove 2  the now-debunked link between measles vaccination and autism. Other countries have begun to catch up. In 2012, for instance, Britain launched the unified UK Data Service to facilitate research access to data from the country's census and other surveys. A year later, the service added a new Administrative Data Research Network, which has centres in England, Scotland, Northern Ireland and Wales to provide secure environments for researchers to access anonymized administrative data. In the United States, the Census Bureau has been expanding its network of Research Data Centers, which currently includes 19 sites around the country at which researchers with the appropriate permissions can access confidential data from the bureau itself, as well as from other agencies. \u201cWe're trying to explore all the available ways that we can expand access to these rich data sets,\u201d says Ron Jarmin, the bureau's assistant director for research and methodology. In January, a group of federal agencies, foundations and universities created the Institute for Research on Innovation and Science at the University of Michigan in Ann Arbor to combine university and government data and measure the impact of research spending on economic outcomes. And in July, the US House of Representatives passed a bipartisan bill to study whether the federal government should provide a central clearing house of statistical administrative data. Yet vast swathes of administrative data are still inaccessible, says George Alter, director of the Inter-university Consortium for Political and Social Research based at the University of Michigan, which serves as a data repository for approximately 760 institutions. \u201cHealth systems, social-welfare systems, financial transactions, business records \u2014 those things are just not available in most cases because of privacy concerns,\u201d says Alter. \u201cThis is a big drag on research.\u201d \n               Unsought intimacy \n             Feeding those concerns is the rising public unease about online privacy in general. Private companies known as data brokers operate on a vast scale, collecting and selling information about Internet searches, online purchases and other data streams that can be combined to draw surprisingly intimate conclusions. In one famous example, the US retailer Target inferred that a teenage girl was pregnant based on her purchases there, and it began sending her coupons for baby products; her father was alerted to his impending grandchild only when the coupons arrived at the family's home. In a 2014 study 4  of data brokers, the US Federal Trade Commission pointed out the many ways in which this kind of information could harm consumers. People who buy products such as blood-sugar monitors, for instance, might be placed into a 'diabetes risk' marketing category that could be used by an insurance company to pinpoint a potential customer as high risk. Many researchers argue, however, that there are  legitimate scientific uses for such data . Jarmin says that the Census Bureau is exploring the use of data from credit-card companies to monitor economic activity. And researchers funded by the US National Science Foundation are studying how to use public Twitter posts to keep track of trends in phenomena such as unemployment. But not everyone makes the distinction between commerce and academia, says Lane. \u201cPeople conflate the concern about big data being used for private-sector purposes to make money with big data being used for research.\u201d In March 2014, for instance, while aiming to significantly boost consumer privacy through a new data-protection regulation, the European Parliament proposed limiting the use of personal health data for research without specific consent, which would have severely curtailed researchers' access to those data. After objections from organizations such as the London-based biomedical-research charity the Wellcome Trust, the proposal looks likely to be jettisoned, but its fate will not become clear until 2016, when the final text of the regulation comes up for approval. One solution to the privacy concerns has been to keep data under lock and key, tightly restricting who can access it. At the US research data centres, for instance, investigators are not allowed to take smartphones or flash drives into the rooms where they will use the centre's computer terminals. The computers themselves contain no data, but only link remotely to secure servers. \n               Technical answers \n             Computer scientists and cryptographers are experimenting with technological solutions. One, called differential privacy, adds a small amount of distortion to a data set, so that querying the data gives a roughly accurate result without revealing the identity of the individuals involved. The US Census Bureau uses this approach for its OnTheMap project, which tracks workers' daily commutes. Researchers at the bureau use actual data to build a statistical model based on where individual workers commute each day. They then build a synthetic data set that fits the model, but does not contain the actual data. This synthetic data set is released to the public, allowing users to draw accurate conclusions about transport and economic trends without tracking the exact movements of real individuals. Researchers are still learning to trust synthetic data, however, so few papers that have been published on this subject go beyond demonstrating the methods. In any case, although synthetic data potentially solve the privacy problem, there are some research applications that cannot tolerate any noise in the data. A good example is the work showing the effect of neighbourhood on earning potential 3 , which was carried out by Raj Chetty, an economist at Harvard University in Cambridge, Massachusetts. Chetty needed to track specific individuals to show that the areas in which children live their early lives correlate with their ability to earn more or less than their parents. In subsequent studies 5 , Chetty and his colleagues showed that moving children from resource-poor to resource-rich neighbourhoods can boost their earnings in adulthood, proving a causal link. Secure multiparty computation is a technique that attempts to address this issue by allowing multiple data holders to analyse parts of the total data set, without revealing the underlying data to each other. Only the results of the analyses are shared. For instance, in 2010, the US Defense Advanced Research Projects Agency (DARPA) asked a team of cryptographers to develop a  secure multiparty computation protocol to analyse the paths of commercial satellites  and head off costly collisions. Currently, companies do this by sharing their orbit data, which they consider proprietary, to a trusted third party that performs the analysis. But DARPA concluded that secure multiparty computation could be used to predict possible collisions just as effectively, albeit a little more slowly. In 2015, the Estonian company Cybernetica, based in Tallinn, said that it had used similar techniques to analyse financial filings of companies to detect tax fraud. It is also jointly analysing records from the country's tax and education ministries to explore whether university students who hold jobs fail their courses more often than those who focus exclusively on their studies. There are still some problems in need of technical solutions \u2014 especially as government agencies look beyond their own walls. For instance, the Census Bureau wants to combine its internal data on the formation and activities of companies with public data on patents to examine the factors that drive corporate innovation. But it could be relatively easy to unmask the identities of companies included in the analysis by matching them to information in the public patent database. Jarmin's team has not yet worked out an approach that adequately protects privacy. But for the most part, technical solutions are now being put in place. Increasingly, what looks likely to hold up the research is a lack of clear ethical and legal guidance about how data on individuals can be used \u2014 for all purposes, including research. Pam Dixon, executive director of the World Privacy Forum in San Diego, California, points to programmes such as India's national identification-card system, launched in 2010. This effort provided more than 900 million people with biometric identity cards that were linked to photographs, fingerprints and iris scans. The cards were supposed to be voluntary, and were used to identify rightful recipients of social benefits such as fuel and unemployment aid. But the country did not create a legislative framework to govern the use of the cards. They were soon discussed as gateways for a variety of essential services, such as salary payment and marriage registrations. This violated the original spirit of the programme, critics contended, because data from the cards was not supposed to be coerced from individuals. The Indian Supreme Court ruled such uses of the system illegal on 11 August, but the country's Parliament has still not enacted a governing framework. Likewise, in 2013, the United Kingdom launched the  care.data programme  to link records from patients' visits to general practitioners with their records from other parts of the health-care system, but there was  no clear guidance on how the project's data were to be used . After it was revealed that the database designed to distribute patient data had inappropriately released some information to private entities \u2014 such as actuaries, which aid insurers in setting insurance rates \u2014 care.data came under fire. On 2 September, the National Health Service (NHS) said that the government will conduct a review of the security of NHS data and develop new opt-out and consent provisions. The system is intended to be available to all patients by 2016. In the meantime, says Nicola Perrin, head of policy at the Wellcome Trust, the fallout has created huge delays in existing research projects, including clinical trials and health evaluation, audit and service research. Researchers in charge of SABRE, a large cohort study examining how diabetes and heart disease affect people of different ethnicities, have not received patient updates since March 2014; as a result, they risk sending requests for information to families whose loved ones may have died. The episode serves, for Perrin, as a cautionary tale about how the power of data could backfire if social unease with its uses is not addressed as soon as possible. \u201cThe lesson is to not underestimate public concerns,\u201d she says. \u201cPublic trust is very fragile \u2014 it's difficult to build and easy to break.\u201d \n                 Tweet \n                 Follow @NatureNews \n               \n                     Power to the people 2014-Jan-15 \n                   \n                     UK push to open up patients\u2019 data 2013-Oct-15 \n                   \n                     Privacy protections: The genome hacker 2013-May-08 \n                   \n                     US Census Bureau: OnTheMap \n                   \n                     US Statistics of Income Joint Statistical Research Program \n                   \n                     NHS England: care.data \n                   Reprints and Permissions"},
{"file_id": "525444a", "url": "https://www.nature.com/articles/525444a", "year": 2015, "authors": [{"name": "Garry Hamilton"}], "parsed_as_year": "2006_or_before", "body": "The powerhouses of the cell may have more roles than expected. Could that generate problems for mitochondrial replacement therapies? In the 1990s, French scientists wanted to see what happened to a mouse brain when they messed with the creature's mitochondria, the structures that generate energy inside most complex cells. The team looked at two mouse strains, called H and N, that carry slightly different mitochondrial-DNA sequences. It was clear that the H mice learned to navigate mazes faster than their N cousins, but when the team swapped the mitochondria \u2014 creating H mice with N mitochondria and N mice with H mitochondria \u2014 their performance changed. Mitochondria from N seemed to slow down the learning process for H mice. N mice, meanwhile, improved slightly with H mitochondria 1 . And the team, led by geneticist Pierre Roubertoux at INSERM, the French National Institute for Health and Medical Research in Marseilles, found other changes in behaviour, and in brain anatomy, too. The results came as a surprise, because such differences between mitochondrial genomes were seen as neutral \u2014 having no biological effect. \u201cThe long-held view was that the genetic variation we find within the mitochondrial genome doesn't affect function,\u201d says Damian Dowling, an evolutionary biologist at Monash University in Melbourne, Australia. That view has been changing. A growing body of evidence suggests that mitochondria do not just produce energy, but also influence a wide range of cellular processes, from cell death to immune responses, and that variations in the organelle matter very much. Variants in mitochondrial DNA are now linked to many common human conditions, including neurodegenerative diseases, cancer and ageing. The effects of these variants may come about through the organelle's long-evolved partnership with the much-larger nuclear genome. Studies in a handful of organisms have shown that just as for H and N mice, swapping healthy mitochondria between closely related strains can cause a mismatch between the genomes and can change important traits. The evidence, say Dowling and others, should raise questions about the safety of a procedure that will soon be used in humans. In February, the UK government  approved mitochondrial replacement therapy , a technique that would allow a woman with a mitochondrial disorder to give birth to healthy children by pairing her nuclear DNA with the healthy mitochondria from a donor's egg. The approval came after a 3.5-year effort to review the safety and ethics of creating individuals with DNA from three people (what some refer to as three-parent babies). And although many scientists lauded the decision, some worry that it is premature. \u201cThey're not looking at the bigger picture,\u201d says Ted Morrow, an evolutionary biologist at the University of Sussex in Brighton, UK, who is arguing for more-rigorous safety testing. A common refrain in favour of the therapy is that the genetic contribution from mitochondria is very small. And against the 3 billion base pairs of DNA and 20,000 genes found in the human nucleus, the mitochondrial genome can seem pretty insignificant (see 'A complicated relationship'). Inherited solely through a mother's egg, it comprises fewer than 17,000 base pairs and just 37 genes. But one cell can have thousands of copies of the mitochondrial genome, compared with just two of the nuclear genome \u2014 one from mum and one from dad. Mitochondrial DNA also accumulates mutations incredibly fast, at about ten times the rate of nuclear DNA \u2014 and geneticists can use the resulting variation as  a sort of molecular clock . The clock has allowed scientists to create a human family tree that shows several broadly related mitochondrial genomes, known as haplogroups, emerging in Africa somewhere around 150,000 years ago, including two that gave rise to the thousands of smaller haplogroups now found around the world. The standing view was that the genetic differences between mitochondria in these groups were little more than a reflection of past migrations. But during the 1980s, researchers began to challenge that assumption. \u201cMitochondria control a central component of metabolism,\u201d says David Rand, an evolutionary biologist at Brown University in Providence, Rhode Island. \u201cSo it followed that this variation ought to be very interesting.\u201d One way to examine whether mitochondria in one population work differently from those in another is to swap them. Such experiments would be unethical in people and impractical in many other animals, so Rand turned to fruit flies. He cross-bred two fly strains with different mitochondria and then repeatedly back-crossed them until the mitochondria from one were neatly paired with the nucleus of the other. He then put fruit flies with similar nuclear genomes but different mitochondria together in a cage, and found that flies with specific mitochondrial genomes would quickly come to dominate the population 2 . Something in the mitochondria was giving them a survival advantage. Subsequent work by Rand, Dowling and others has shown that it is not just the mitochondrial genome, but rather its interaction with the nuclear one that seems to be affecting a range of traits, including lifespan, reproductive success, rate of development, ageing, growth, movement, morphology and behaviour. The findings extended beyond inbred laboratory animals such as fruit flies and mice. Over the past two decades, Ron Burton at the Scripps Institution of Oceanography in La Jolla, California, has found that cross-breeding closely related populations of tiny crustaceans known as copepods from tide pools on the Pacific coast often leads to a massive fitness breakdown for the animals 3 . Two clues led Burton to suspect that the reason was a mismatch between nuclear and mitochondrial DNA. First, the populations had very different mitochondrial genomes. Second, energy production was at the heart of all the sickly organisms' deficiencies. The clincher came when Burton chose females from the unhealthy animals and mated them with males from the same population as the females' mothers. The resulting offspring, which once again had a natural combination of mitochondrial and nuclear genomes, were healthy. \u201cThat's pretty striking,\u201d says Burton. \u201cAnd we did it with multiple different crosses.\u201d Extending these results to mammals has been difficult: Roubertoux's mitochondrially mismatched mouse lines took more than 20 generations and 12 years to develop. But there are a few studies that have found similar results. Douglas Wallace, who heads the Center for Mitochondrial and Epigenomic Medicine at the Children's Hospital of Philadelphia, combined the nucleus from a lab-mouse strain with mitochondria from a mouse known to contain two different, but normal, mitochondrial genomes. His group found that the modified mice had altered circadian rhythms \u2014 the natural oscillations that follow a roughly 24-hour cycle \u2014 performed worse in mazes and seemed more stressed in certain experimental conditions, compared with unmodified animals 4 . In humans, there is only indirect evidence that the common variation found in the mitochondrial genomes of healthy individuals could have biological effects. Certain mitochondrial haplotypes have been linked to disorders such as type-2 diabetes,  Parkinson's disease  and cancer, and normal variation in the mitochondria is thought to influence general physical traits such as longevity and elite athleticism 5 . \u201cCorrelations are just correlations,\u201d says G\u00f6ran Arnqvist, an evolutionary biologist at Uppsala University in Sweden, \u201cbut there's now a large enough number of them to in itself provide ample evidence that there's something going on with mitochondrial DNA\u201d. \n               Powerhouse pairing \n             The question remains exactly how these variations could affect such a broad range of biological functions. Part of the answer seems to lie in their ties with the nuclear genome. Roughly 1,500 nuclear genes are involved in mitochondrial function, including around 76 that encode proteins which bind to mitochondrially derived peptides. Common variants could alter how these proteins interact. If a mitochondrially derived protein needs to fit snugly against a nuclear counterpart, even tiny changes in one partner could disrupt that binding, a possibility supported by 3D modelling 6 , 7 . A study published in 2009 compared mitochondria from two common human European lineages, called haplogroups J and H, in cells with the same nuclear DNA 8 . It showed that cells with haplogroup J mitochondria contained more than twice as many copies of mitochondrial DNA as those with haplogroup H, a difference that would be expected to have a big influence on the production of mitochondrial proteins. Such effects could alter the rate at which mitochondria supply energy, with consequences for many cellular activities. But emerging evidence points to other ways that mitochondria could have broad biological implications. Various molecules created during the production of energy, such as free radicals, may have a direct influence on processes involved in ageing, inflammation and in some basic cell functions. And in May, a team of researchers led by Gerald Shadel at Yale University in New Haven, Connecticut, showed in mice that mitochondrial DNA can itself trigger an innate immune response against viral infection 9 . \u201cThey're not just power factories,\u201d says Rand. \u201cThey're also in a sense a nerve centre, a thermostat for the cell and how it's doing.\u201d Researchers have also found evidence for a new class of mitochondrially derived peptide that might be encoded by sequences in other mitochondrial genes. One of these is humanin, a small peptide discovered by Japanese researchers in 2001 that increases sensitivity to insulin in diabetes-prone rats and mice 10 . The gene that encodes it is thought to reside in the mitochondrial gene for 16S ribosomal RNA. In March, researchers in the United States found a second potential example, MOTS-c, which is encoded by a small stretch of DNA tucked away in another gene. MOTS-c functions like a hormone, and when injected into mice helps to enhance insulin sensitivity and protect against obesity 11 . Some researchers now suspect that mitochondrial DNA produces a vast array of biologically active molecules \u2014 other small peptides as well as short stretches of RNA \u2014 that are part of a network of cross-communication between the mitochondrial and nuclear genomes. \u201cThe very viability of complex life \u2014 eukaryote life \u2014 depends on a really coordinated, intimate set of interactions between these two genomes,\u201d Dowling says. It is a partnership that has shaped and been shaped by aeons of evolution. Given how well evolution has tuned this communication, many biologists are concerned about disrupting it in mitochondrial replacement therapy. The results of mitochondria-swapping experiments in other organisms, they say, should not be overlooked. \u201cWe haven't seen anything fundamentally different between flies and humans in terms of interactions between the mitochondria and the nucleus,\u201d says Klaus Reinhardt, an evolutionary biologist at the University of T\u00fcbingen in Germany. The health effects may not be dramatic, says Burton, and they might not become apparent until decades after birth. \u201cBut I think there's a definite possibility that you'd see things like disrupted fertility function, various forms of metabolic syndromes and changes in things that relate to metabolism in general.\u201d \n               Call for caution \n             Reinhardt, Dowling and Morrow outlined their concerns in a 2013 paper 12  in  Science . They called for studies aimed at addressing how mammals born after mitochondrial replacement fare in adulthood, and argued that scientists should at least look into haplotype matching \u2014 ensuring that the mitochondria from the donor and recipient come from the same haplogroup before transplant. Moving ahead at this juncture, they argued, \u201cwould place an experimental risk on families\u201d. But other researchers disagree. Scientists at Newcastle University, UK, and at Oregon Health & Science University (OHSU) in Beaverton, two institutions that pioneered mitochondrial replacement therapies, pointed to perfectly healthy macaque monkeys born at OHSU in 2009 after the procedure 13 . They also pointed out that most of the evidence for risk stems from studies that used strains of flies and mice that had been highly inbred \u2014 a process that would increase the genetic differences between the strains and therefore produce a greater 'mismatch' when the mitochondria are swapped. They argued that such studies have little relevance for human populations that interbreed all the time. The \u201clack of any reliable evidence of mitochondrial\u2013nuclear interaction as a cause of disease in human outbred populations\u201d, they wrote, \u201cprovides the necessary reassurance to proceed\u201d. Doug Turnbull, who heads the Newcastle group, also argues that correlations between different human mitochondrial haplotypes and common diseases are not definitive. \u201cIf we're struggling to find a signal,\u201d he says, \u201cis that really something that's likely to cause major difficulties?\u201d Ultimately, government approval hinged on a 2014 report prepared by a scientific review panel set up by the Human Fertilisation and Embryology Authority (HFEA), the body that regulates assisted-reproduction treatments in the United Kingdom. The panel's chair, Andy Greenfield of the Medical Research Council, would not comment for this story, but the HFEA provided a written response to questions. It stated that deliberations were \u201ctime-consuming and as complex as the data themselves\u201d, adding that most respondents presenting evidence to the panel viewed these issues as \u201cat best minor or non-existent\u201d. In its final report, the panel recommended that haplogroup matching be considered \u201cas a precautionary step\u201d. But it also stated that the benefits of doing so are \u201clikely to be minimal\u201d. Some of the critics of the decision grant that mitochondrial replacement may be worth the risks for women who want to avoid passing rare and devastating disorders on to their children. Many, however, think that more time is needed to assess the risks. There is also concern that proponents of the therapy trivialized the role of mitochondria \u2014 particularly by likening mitochondrial replacement to changing the batteries in a camera. Critics argue that a failure to appreciate all the other processes in which the organelle is involved could lead to inadequate controls and wider application of mitochondrial replacement in fertility clinics. \u201cYou may have a few thousand people who suffer from mitochondrial diseases,\u201d says David Keefe, a reproductive biologist at New York University's Langone Medical Center. \u201cThere are tens of millions of women who have infertility who may see this as a way to have the batteries charged in their eggs.\u201d At least one clinic in the United States has used cytoplasm from donor eggs to  'normalize' the eggs of women being treated for infertility , starting in the late 1990s. The procedure, which probably transferred mitochondria as well, resulted in 17 births before the US Food and Drug Administration requested safety studies and the clinic stopped offering the procedure in 2001. Little is known about the health of the children born as a result of the procedure. Turnbull rejects the slippery-slope argument. \u201cIn the UK, the legislation is very clear that mitochondrial donation can only be used to prevent serious mitochondrial disease,\u201d he says. \u201cI do not think there is any good evidence it would be useful for anything else.\u201d Although no one knows what the rapidly growing field of mitochondrial research will uncover next, both sides agree that there is no way to say for sure what will happen when doctors swap mitochondria in humans, short of actually doing it. For Dowling, at least, it is one scientific debate that he would rather not win. \u201cI'd like to see this work so female sufferers of mitochondrial disease can have unaffected children,\u201d he says. \u201cSo I hope we're wrong.\u201d \n                 Tweet \n                 Follow @NatureNews \n               \n                 See Editorial \n                 page 425 \n               \n                     Power play 2015-Sep-23 \n                   \n                     Scientists cheer vote to allow three-person embryos 2015-Feb-03 \n                   \n                     Reproductive medicine: The power of three 2014-May-21 \n                   \n                     Regulators weigh benefits of \u2018three-parent\u2019 fertilization 2013-Oct-15 \n                   \n                     Biodiversity: On the origin of bar codes 2009-Nov-18 \n                   \n                     https://doi.org/10.1038/nature.2012.11651 \n                   \n                     When will \u201cthree-parent\u201d babies come to the United States? \n                   Reprints and Permissions"},
{"file_id": "523520a", "url": "https://www.nature.com/articles/523520a", "year": 2015, "authors": [{"name": "Cassandra Willyard"}], "parsed_as_year": "2006_or_before", "body": "Biologists are building banks of 'organoids', and learning a lot about human development on the way. It was an otherwise normal day in November when Madeline Lancaster realized that she had accidentally grown a brain. For weeks, she had been trying to get human embryonic stem cells to form neural rosettes, clusters of cells that can become many different types of neuron. But for some reason her cells refused to stick to the bottom of the culture plate. Instead they floated, forming strange, milky-looking spheres. \u201cI didn't really know what they were,\u201d says Lancaster, who was then a postdoc at the Institute of Molecular Biotechnology in Vienna. That day in 2011, however, she spotted an odd dot of pigment in one of her spheres. Looking under the microscope, she realized that it was the dark cells of a developing retina, an outgrowth of the developing brain. And when she sliced one of the balls open, she could pick out a variety of neurons. Lancaster realized that the cells had assembled themselves into something unmistakably like an embryonic brain, and she went straight to her adviser, stem-cell biologist J\u00fcrgen Knoblich, with the news. \u201cI've got something amazing,\u201d she told him. \u201cYou've got to see it.\u201d Kerri Smith investigates the growing field of growing miniature organs Lancaster and her colleagues were not the first to grow a brain in a dish. In 2008, researchers in Japan reported 1  that they had prompted embryonic stem cells from mice and humans to form layered balls reminiscent of a cerebral cortex. Since then, efforts to grow stem cells into rudimentary organs have taken off. Using carefully timed chemical cues, researchers around the world have produced three-dimensional structures that resemble tissue from the eye, gut, liver, kidney, pancreas, prostate, lung, stomach and breast. These bits of tissue, called organoids because they mimic some of the structure and function of real organs, are furthering knowledge of human development, serving as disease models and drug-screening platforms, and might eventually be used to rescue damaged organs (see \u2018The organoid bank\u2019). \u201cIt's probably the most significant development in the stem-cell field in the last five or six years,\u201d says Austin Smith, director of the Wellcome Trust/MRC Stem Cell Institute at the University of Cambridge, UK. The current crop of organoids isn't perfect. Some lack key cell types; others imitate only the earliest stages of organ development or vary from batch to batch. So researchers are toiling to refine their organoids \u2014 to make them more complex, more mature and more reproducible. Still, biologists have been amazed at how little encouragement cells need to self-assemble into elaborate structures. \u201cIt doesn't require any super-sophisticated bioengineering,\u201d says Knoblich. \u201cWe just let the cells do what they want to do, and they make a brain.\u201d \n               Growing a gut \n             This shouldn't come as a major surprise, says molecular biologist Melissa Little at the University of Queensland, Australia. \u201cThe embryo itself is incredibly able to self-organize; it doesn't need a template or a map.\u201d That has been known since the early 1900s, when embryologists showed that sponges that had been broken up into single cells could reassemble themselves. But such work fell out of fashion, and modern biologists have focused their attention on purifying cells and growing them in culture \u2014 often in flat layers that do little to mimic normal human tissue. Studying these cells to understand how an organ functions is like studying a pile of bricks to understand the function of a house, says Mina Bissell, a cancer researcher at the Lawrence Berkeley National Laboratory in California. \u201cWe should just begin to make the house,\u201d she says. Bissell's work on cultures of breast cells helped to propagate the idea that cells behave differently in 3D cultures than in conventional flat ones. By the mid-2000s, the idea was catching on. The burst of enthusiasm was fuelled by Yoshiki Sasai, a stem-cell biologist at the RIKEN Center for Developmental Biology in Kobe, Japan, who turned heads when he grew a cerebral cortex 1 , followed by a rudimentary optic cup 2  and pituitary gland 3  (see  Nature   488 , 444\u2013446; 2012 ). Just a year after Sasai announced his layered cortex, Hans Clevers, a stem-cell researcher at the Hubrecht Institute in Utrecht, the Netherlands, reported the creation of a mini-gut 4 . The breakthrough stemmed from a discovery in 2007, when Clevers and his colleagues had identified intestinal stem cells in mice. In the body, these cells seemed to have an unlimited capacity to divide and replenish the intestinal lining, and one of Clevers' postdocs, Toshiro Sato, was tasked with culturing them in the lab. Rather than growing the cells flat, the pair decided to embed them in matrigel, a soft jelly that resembles the extracellular matrix, the mesh of molecules that surrounds cells. \u201cWe were just trying things,\u201d Clevers says. \u201cWe hoped that we would make maybe a sphere or a blob of cells.\u201d Several months later, when Clevers put his eye to Sato's microscope, he saw more than blobs. The cells had divided, differentiated into multiple types, and formed hollow spheres that were dotted with knobby protrusions. Inside, the team found structures that resembled the intestine's nutrient-absorbing villi as well as the deep valleys between them called crypts. \u201cThe structures, to our total astonishment, looked like real guts,\u201d Clevers says. \u201cThey were beautiful.\u201d The mini-guts, reported in 2009, may prove to be a powerful tool in personalized medicine. Clevers and his team are using them to study the effectiveness of drugs in people with cystic fibrosis, who have genetic defects that affect ion channels and disrupt the movement of water in and out of the cells lining the lungs and intestine. The researchers take rectal biopsies from people with the disease, use the cells to create personalized gut organoids and then apply a potential drug. If the treatment opens the ion channels, then water can flow inwards and the gut organoids swell up. \u201cIt's a black-and-white assay,\u201d Clevers says, one that could prove quicker and cheaper than trying drugs in people to see whether they work. He has already used the system to assess whether a drug called Kalydeco (ivacaftor), and 5 other cystic-fibrosis drugs, will work in about 100 patients; at least 2 of them are now taking Kalydeco as a result. Organoids may also help physicians to choose the best therapies for people with cancer. Earlier this year, Clevers revealed that he had grown a bank of organoids from cells extracted from colorectal tumours 5 , and David Tuveson, a cancer researcher at Cold Spring Harbor Laboratory in New York, worked with Clevers to generate pancreas organoids using biopsies taken from people with pancreatic cancer 6 . In both cases, the organoids could be used to find drugs that work best on particular tumours. \u201cWhat patients are looking for is a logical approach to their cancer,\u201d Tuveson says. \u201cI'm very excited about what we're learning.\u201d \n               The organoid bank \n               \n               The small-scale stomach \n             That excitement is shared by developmental biologist James Wells, who last year reported that he and his team had created an organoid that resembled part of a human stomach 7 . Wells started with a different raw material to Clevers, whose organoids arise from adult stem cells that can generate only a limited number of cell types. Wells, who is at the Cincinnati Children's Hospital Medical Center in Ohio, and his colleagues craft organoids from embryonic stem cells, which have the ability to become almost any type of cell. As a result, they have been able to create mini-organs that are more complex. A decade ago, Wells and his colleagues began trying to coax human embryonic stem cells to form intestinal cells. When the team manipulated two key signalling pathways, the layer of cells produced tiny round buds. Wells noticed that these 'spheroids' mimicked sections of the primitive gut tube, which forms four weeks after conception. This was thrilling, because he realized that he now had a starting point from which to develop a variety of organoids. \u201cEvery organ from your mouth down to your anus \u2014 oesophagus, lungs, trachea, stomach, pancreas, liver, intestine, bladder \u2014 all of them come from this very primitive tube,\u201d he says. Wells and his colleagues mined the literature and their own experience to determine what chemical cues might send these gut tubes down the developmental path toward a specific organ. Using this strategy, in 2011 the team developed its first human organoid 8 , an intestine about the size of a sesame seed. But growing a stomach was a bigger challenge. In humans, the organ has two key areas: the fundus at the top, which churns out acid, and the antrum towards the base, which produces many key digestive hormones \u2014 and the signalling pathways that lead to one versus the other were unknown. What is more, \u201cthe human stomach is different from the stomachs of most animals that we use in the lab\u201d, so there is no good animal model, says Kyle McCracken, a former graduate student of Wells and now a medical student at the centre. The researchers went for a trial-and-error approach: they made some educated guesses and painstakingly tested different combinations of growth factors. Eventually, the effort paid off. In a 2014 paper 7 , Wells and his team revealed that they had created organoids that resembled the antrum. Using these as a model system, the team says that it has figured out the chemical trigger that prompts the development of a fundus. Now the researchers are working to answer other basic questions about stomach development and physiology, such as which factors regulate acid secretion, and they are trying to generate other mini-organs from their primitive gut tubes. This newfound ability to examine human development excites Daniel St Johnston, a developmental geneticist at the University of Cambridge's Gurdon Institute. \u201cYou can actually watch how the cells organize themselves to make complicated structures,\u201d he says \u2014 something that is impossible in a human embryo. But most organoids are still single tissues, which limits what developmental biologists can learn, he says. \u201cThere are certain questions you can't really address because they depend upon the physiology of the whole organism.\u201d \n               The baby kidney \n             Melissa Little has spent more than a decade marvelling at the complexity of the kidney. \u201cIt has, in an adult, probably 25\u201330 different cell types, each doing different jobs,\u201d she says. Tubular structures called nephrons filter fluid from the blood and produce urine. The surrounding space, called the interstitium, holds an intricate network of blood vessels and the plumbing that carries urine away. In 2010, Little and her colleagues started trying to turn embryonic stem cells into a progenitor cell that gives rise to nephrons. For three years, they tried various combinations and timings of growth factors. \u201cIt really took a lot of mucking around to make progress,\u201d she says. But finally, in 2013, the team landed on just the right mixture. Little had been aiming to produce just the progenitor cells. But when she looked in the dish she saw two cell types spontaneously patterning themselves as they would in an embryo. \u201cThere was a moment of, 'Oh wow. Isn't that amazing',\u201d she says. This organoid resembles an embryonic kidney rather than an adult one: it has a mix of nephron progenitors and the cells that give rise to urine-collecting ducts 9 . \u201cIf you want to get them to mature further, that's where the challenge really lies,\u201d Little says. So her team has been working to grow a more-sophisticated version \u2014 with blood vessels and interstitium. The hope then is to transplant the mini-organs into mice to see if they will mature and produce urine. \u201cI'm pretty excited about what we can build,\u201d Little says. Because the kidney plays a key part in drug metabolism and excretion, Little thinks that her mini-kidneys could be useful for testing drug candidates for toxicity before they reach clinical trials. And researchers say that other human organoids, such as heart and liver, could similarly be used to screen drug candidates for toxic effects \u2014 offering a better read-out on the response of an organ than is possible with standard tissue culture or animal testing. But Michael Shen, a stem-cell researcher at Columbia University in New York who has created a prostate organoid, is sceptical that these model systems could completely replace lab animals. Animals can show how a therapy affects the immune system, for example, something that organoid systems cannot currently do. \u201cYou want to be able to validate your experimental findings in an  in vivo  system,\u201d he says. \u201cI view that as a rigorous test.\u201d \n               Little livers \n             Takanori Takebe was inspired to grow a liver after a chilling spell in New York. While working in the organ-transplantation division at Columbia University in 2010, Takebe saw people die from liver failure owing to a lack of organs. \u201cThat was a sad situation,\u201d he says. When he looked into tissue engineering, he thought that the usual methods \u2014 seeding cells onto an artificial scaffold \u2014 seemed destined to fail. Part of the problem, he says, is that adult liver cells are very difficult to grow. \u201cWe cannot maintain it in culture for even a couple of hours.\u201d Takebe, who took up a research position at Yokohama City University in Japan, decided to work on induced pluripotent stem (iPS) cells, adult cells that have been reprogrammed to behave like embryonic stem cells. He coaxed human iPS cells into forming liver-cell precursors, or hepatoblasts. In the embryo, hepatoblasts rely on a complex symphony of signals from other nearby cells to mature, and Takebe suspected that these support cells would also be necessary to develop a liver in a dish. He and his colleagues mixed hepatoblasts with such cells \u2014 called mesenchymal and endothelial cells \u2014 and it worked. The team managed to create 'liver buds', structures no bigger than a lentil that resemble the liver of a six-week-old human embryo 10 . The researchers went on to find that, unlike mature liver cells, such structures can survive in culture for as long as two months. A liver bud is still a far cry from an entire liver \u2014 a hefty, multi-lobed organ composed of tens of billions of hepatocytes. But Takebe hopes that if he can infuse many thousands of buds into a failing organ, he might be able to rescue enough of its function to make a transplant unnecessary. The process seems to work in mice. When Takebe and his group transplanted a dozen of the buds into mouse abdomens, they saw dramatic effects. Within two days, the buds had connected up with the mouse's blood supply, and the cells went on to develop into mature liver cells that were able to make liver-specific proteins and to metabolize drugs. To mimic liver failure, the team wiped out the animals' natural liver function with a toxic drug. After a month, most of the control mice had died, but most of those that received liver bud transplants had survived. Takebe and his team hope to start human trials in four years. \u201cWe will target the children that critically need a liver transplant,\u201d he says. He and his colleagues are currently working to make the liver buds smaller and produce them in huge quantities that they can infuse through the large portal vein that feeds the liver. Takebe thinks that the timeline is \u201cdoable\u201d. But Smith says that the process seems rushed, and that the basic biology of these organs needs to be well understood before they are used in the clinic. \u201cIt's like running before you can walk,\u201d he says. Biologists know that their mini-organs are still a crude mimic of their life-sized counterparts. But that gives them something to aim for, says Anthony Atala, director of the Wake Forest Institute for Regenerative Medicine in Winston-Salem, North Carolina. \u201cThe long-term goal is that you will be able to replicate more and more of the functionality of a human organ.\u201d Already, the field has brought together developmental biologists, stem-cell biologists and clinical scientists. Now the aim is to build more-elaborate organs \u2014 ones that are larger and that integrate more cell types. And Wells says that even today's rudimentary organoids are facilitating discoveries that would have been difficult to make in an animal model, in which the molecular signals are hard to manipulate. \u201cIn a Petri dish it's easy,\u201d he says. \u201cWe have chemicals and proteins that we can just dump onto these cells.\u201d \n                     \u2018Organs-on-chips\u2019 go mainstream 2015-Jul-15 \n                   \n                     Tissue engineering: Organs from the lab 2015-Jun-17 \n                   \n                     Q&A: Hans Clevers 2015-May-13 \n                   \n                     Tiny human stomachs grown in the lab 2014-Oct-29 \n                   \n                     Stem-cell scientists mourn loss of brain engineer 2014-Aug-05 \n                   \n                     Developmental neuroscience: Miniature human brains 2013-Aug-28 \n                   \n                     Stem cells mimic human brain 2013-Aug-28 \n                   \n                     Miniature human liver grown in mice 2013-Jul-03 \n                   \n                     Tissue engineering: How to build a heart 2013-Jul-03 \n                   \n                     Tissue engineering: The brainmaker 2012-Aug-22 \n                   \n                     James Wells \n                   \n                     Melissa Little \n                   \n                     J\u00fcrgen Knoblich \n                   \n                     Hans Clevers \n                   \n                     Madeline Lancaster won the 2014 Eppendorf Young European Investigator award for her research. \n                   Reprints and Permissions"},
{"file_id": "524022a", "url": "https://www.nature.com/articles/524022a", "year": 2015, "authors": [{"name": "Declan Butler"}], "parsed_as_year": "2006_or_before", "body": "The world is ill-prepared for the next epidemic or pandemic. But the horror of the Ebola outbreak in West Africa may drive change. If there was one point last year when public-health experts held their breath, it was when a Liberian man infected with Ebola virus flew to Lagos, Nigeria, in July. Ebola was already raging uncontrolled through impoverished countries in West Africa, killing half of those it infected. Now a vomiting man had carried it straight to the heart of Africa's largest megacity \u2014 with 21 million inhabitants, many of whom live in slums. Experts were horrified at the prospect that the virus might rip through the city \u2014 and then, because Lagos is an international travel hub, spread farther afield. \u201cThe last thing anyone in the world wants to hear is the two words, 'Ebola' and 'Lagos' in the same sentence,\u201d said Jeffrey Hawkins, the US consul general in Nigeria, at the time. In the end, this apocalyptic scenario did not play out. Because  Nigeria is a focus of global efforts to eradicate polio , it has a decent infrastructure of virology labs and epidemiologists and the capacity to run large public-awareness campaigns. Authorities quickly repurposed this toolbox to tackle Ebola, and the outbreak was contained with just 20 cases in all. The number of infections from Ebola in Guinea, Liberia, and Sierra Leone has dropped from its peak of hundreds of cases per week, to 20 or 30. But what has not faded is the fear that, at some point in the future, the world will face an outbreak of a deadly disease that spreads much more easily between people than Ebola does, and so results in an epidemic or pandemic that is even more terrible than that in West Africa. Quite what that disease will be, no one knows. One  worst-case scenario  is that of an influenza virus as deadly as the one behind the 1918 pandemic, which raced across the world killing as many as 50 million people. Other virus families also keep researchers awake. Poxviruses are one: smallpox was eradicated in 1980 after killing some 300 million people in the twentieth century, but there are many animal poxviruses that could evolve to replace it. Paramyxoviruses are another major worry: the family includes Nipah virus and Hendra virus, both of which have triggered small outbreaks that caused serious illness and death. But uncertainty prevails. \u201cSecond on the list is the one we haven't thought of, and at the very top is the one we can't imagine,\u201d says infectious-disease specialist David Morens at the US National Institute of Allergy and Infectious Diseases in Bethesda, Maryland. With the Ebola epidemic apparently on the wane, Kerri Smith investigates the lessons that have been learned. The Ebola epidemic has spurred researchers and public-health experts to call for a major overhaul of the world's approach to epidemic threats. What's needed, they argue, is  better monitoring for the emergence and re-emergence of pathogens , and beefed-up health systems in the many poor countries that are often on the frontline of epidemics. They want to see nimble task forces that are able to respond rapidly and forcefully to outbreaks, and a  multibillion-dollar global fund  to quickly develop countermeasures such as drugs and vaccines. At the same time, the risks need to be kept in perspective, say researchers. History shows that new pathogens that pose a large epidemic threat are \u201cvery rare\u201d, says Adrian Hill, a specialist in infectious diseases and director of the Jenner Institute in Oxford, UK. So are those that quickly kill many of those infected \u2014 the type that  film plots thrive on . Many emerging epidemics, such as that of multidrug-resistant tuberculosis, move more slowly, yet cumulatively can kill many more people than the acute outbreaks that attract most of the media and political attention. But when they happen, large, acute epidemics can cause devastating loss of life and major economic damage, and  the panic and chaos they generate  can do more harm than the pathogen itself. The Ebola epidemic is not over, and there are concerns it could spike again. \u201cEbola has been a wake-up call, not just for Africa, but for the world,\u201d said Margaret Chan, director-general of the World Health Organization (WHO) in March. \u201cThe world must never again find itself in such a position.\u201d The greatest new epidemic threats are unknown pathogens that spread easily \u2014 for example, through the air \u2014 and to which humans have little or no immunity. The world's last brush with anything coming close was in late 2002, when the virus causing severe acute respiratory syndrome (SARS) caused a outbreak in humans in Guangdong province, China, then quickly fanned out into 29 countries \u2014 infecting at least 8,098 people and killing 774 of them \u2014 before a massive international response brought it under control. If that virus had spread just a bit more easily, it might have killed many more. \u201cSARS probably came close to becoming an out-of-control pandemic,\u201d says Morens. \u201cI think of SARS as one of our scariest close calls.\u201d \n               How to detect threats \n             Like SARS, which is thought to have originated in bats, most future infectious diseases will come from animals; some three-quarters of new human diseases have emerged this way. Scientists suspect that the current Ebola outbreak originated when  the virus passed from fruit bats to a two-year-old boy  playing in a forested region of southern Guinea; Middle East respiratory syndrome (MERS), a viral disease that emerged in 2012, is  probably transmitted by camels . And just last month,  researchers reported  that three squirrel breeders in Germany who had died of encephalitis were killed by a novel bornavirus that had been carried by the animals. In theory, this knowledge could help the world to prepare. Scientists could carefully monitor viruses in animal populations and in people living nearby to identify potential threats, such as any that show some ability to cross the species barrier. Such basic research might allow scientists to get a head start on developing vaccines and drugs. But the science of predicting such threats is in its infancy. Scientists know little about what allows an animal pathogen to infect humans or to then spread between them, processes that depend on many factors, including its ability to enter human cells and replicate there. \u201cOf all our gaps in knowledge, the worst gap is how little we know about the mechanisms of emergence,\u201d says Morens. To make matters worse, the vast majority of  infectious-disease research and surveillance is in developed countries, but most emerging and re-emerging diseases are in the developing world . \u201cWe need to be where the diseases are, and where they are likely to emerge, studying them at their source, not sitting in labs in US science buildings,\u201d says Morens, who is currently working on Ebola in Guinea. Robert Garry, a virologist at Tulane University in New Orleans, Louisiana, is working with African scientists in an international project \u2014 the  African Center of Excellence for Genomics of Infectious Diseases , based at Redeemer's University in Redemption City, Nigeria. The project, which began in May last year, is taking blood samples from villagers in the region who have fevers, and using next-generation genetic sequencing of the samples to discover new pathogens, as well as developing diagnostics for both new and known ones. Supported by the US National Institutes of Health and the World Bank, it has an initial four-year budget of around US$8 million. Researchers do have some clues to guide their search for threats. They know that factors such as geography, climate and culture can help to identify hotspots of disease emergence, with most at lower latitudes. And it is clear that a major driver is contact between animals and humans. The  EcoHealth Alliance , an international network of scientists centred in New York City, and the US Agency for International Development's  Emerging Pandemic Threats programme  are carrying out viral sampling from animals and people in hotspots across the world, and trying to tease out how farming, trade, deforestation and hunting and consumption of bushmeat influence the emergence of diseases. Such projects have led to the discovery of hundreds of viruses including arenaviruses, phleboviruses, coronaviruses and rhabdoviruses \u2014 and are likely to yield many more in the future, says Garry. But even when researchers do find new viruses, it is difficult to say which of them might pose a major threat. Few people would have anticipated that HIV/AIDS, the world's largest recent pandemic, would be caused by a retrovirus, part of a viral family that had not previously been associated with major infectious disease, Garry says (see 'Emerging threats'). Some hints can be found by examining the affinity of viruses for receptors on human cells and assessing how well they spread between animals in the lab. These approaches are perhaps most advanced for flu viruses, which cause pandemics every few decades, of varying severity. Researchers around the world try to rank the potential pandemic risk of flu viruses using a  battery of criteria , including the pathogens' ability to infect or transmit between ferrets, whether they can bind to human receptors, and to what extent the human population has immunity. This information is used to prioritize the development of vaccines against those that seem more threatening. But it cannot predict which flu viruses might go pandemic. Researchers know that more could and should be done. One of the most important tasks is to establish local medical and research systems that can quickly analyse what is going on when a cluster of people suddenly comes down with serious disease. Such systems, which are often underdeveloped in poorer countries, require a trained local workforce of microbiologists, epidemiologists and clinical scientists, and diagnostics laboratories capable of testing clinical samples for a wide range of diseases. These could be implemented in a low-income country for as little as $12 million annually,  according to Jeremy Farrar , director of the UK biomedical charity the Wellcome Trust, who helped to establish such a system in Vietnam. But right now, surveillance systems are just as limited as scientists' knowledge of emerging threats. So the current reality is that we will probably be alerted to the next human epidemic or pandemic  only once it is well under way. \n               How to respond \n             At that point, the world must respond \u2014 fast.  For Ebola, it did not . The initial outbreaks occurred in December 2013, but Ebola was only identified as the cause at the end of March 2014, by which point the outbreak had already spread.  Early alarms by the humanitarian organization M\u00e9dicins Sans Fronti\u00e8res  (MSF; also known as Doctors Without Borders) were ignored, and the international response did not kick into high gear until September (see  Nature   513 , 469; 2014 ). \u201cEbola spun out of control because of a lack of political leadership, will and accountability \u2014 not because of insufficient funding, early-warning systems, coordination or medical technologies,\u201d Joanne Liu, international president of MSF,  told a gathering of health leaders in May . This was not how it was meant to be. In 2005, all 196 countries adopted a set of laws called the  International Health Regulations , which were designed to improve the response to disease outbreaks. The regulations \u2014 effectively the world's emergency action plan \u2014 were spurred by the SARS epidemic, and by outbreaks of H5N1 avian flu virus. But Ebola revealed  how weak the regulations are  [PDF]. They mostly tasked individual countries with dealing with outbreaks \u2014 setting targets for them to reinforce their capacities for disease surveillance and response by 2012 \u2014 but did not include support to help the poorest countries reach those goals. This weakness has long been recognized, but not acted on \u2014 an \u201celephant in the emergency room\u201d, says David Fidler, a specialist in international and national security law at Indiana University Bloomington. Ten years after the treaty was adopted, two-thirds of its signatories have yet to meet the targets. The regulations also failed to create an international rapid-response group to deal with a major outbreak. The WHO has never had outbreak-response teams on the scale needed to deal with an epidemic as large as Ebola, says Fidler, and what capacity it had has been slashed by budget and staff cuts. \u201cWhat we are seeing in the Ebola crisis is the lack of a global public-health expeditionary capability that can handle something on a country or regional scale,\u201d he says. Governments and international organizations are now considering a raft of proposals to prevent the next serious outbreak from growing into an epidemic. These include boosting financial support for surveillance and outbreak response in low- and middle-income countries, and reform of the WHO,  which has come under fire for its slow response to Ebola . One idea is to create a Centre for Emergency Preparedness and Response within the WHO but autonomous from it to avoid the agency's notorious politicization and bureaucracy. The body would link to other United Nations' agencies, the World Bank, philanthropic organizations, non-governmental organizations and industry. It would create an international reserve force that could be rapidly deployed to an outbreak, and be able to call up the planes and helicopters often needed to quickly ship large amounts of medical equipment to regions in need. The World Bank, the WHO and other organizations are also working on the idea of a  Pandemic Emergency Facility  that could swiftly send contingency funds to cover the efforts of the WHO, governments and other bodies in the event of a serious outbreak. The question now is whether these grand plans will become a reality. Many people hoped that these and other measures to reinforce outbreak preparedness and response would receive  firm pledges at the June summit of G7 industrialized countries  in Germany. But although the summit produced supportive language, it did not make concrete decisions, something that disappoints Manica Balasegaram, executive director of MSF's Access Campaign in Geneva, Switzerland. \u201cWe need money put on the table, we need political commitment and funding,\u201d he says. But Farrar says that the high-level political attention is a good sign. He notes that the G7 has previously delivered on major public-health initiatives, such as helping to create the multibillion-dollar Global Fund to Fight AIDS, Tuberculosis and Malaria in 2002, two years after it was first proposed. What emerged from the G7 this year \u201chas to be seen as setting a tone and a direction,\u201d Farrar says. \u201cWhat's key is what then comes out of the language.\u201d \n               How to get vaccines and drugs \n             Even if the world reacts quickly to an emerging outbreak, it has to have effective tools to deploy. A vaccine could have stopped Ebola in its tracks, but the only ones available  had not been tested in humans. Drugs, too, were stuck in the experimental phase . In this and other outbreaks, health-care workers often have to rely on centuries-old public-health measures,  such as quarantine , chemical disinfection and encouraging hand washing \u2014 essential, but often not enough. If a worst-case epidemic hit tomorrow, the script would probably be the same. The problem, say public-health officials, lies in how global drug and vaccine development is set up. The process is left largely to major pharmaceutical companies, which are geared towards treating those who can pay \u2014 developed-world inhabitants with mostly developed-world diseases \u2014 rather than to addressing the most pressing global health needs, which are often infectious diseases in the developing world. \u201cWhat humanity actually needs isn't part of the equation,\u201d says Morens. \u201cIt's what can make big bucks.\u201d That there were even candidate vaccines and drugs for Ebola was largely down to spending on biodefence rather than concerns about global health, says Balasegaram. And there are few, if any, effective drugs and vaccines for a host of other epidemic threats and neglected diseases ranging from SARS to dengue \u2014 leaving the world defenceless against almost all the pathogens most likely to cause the next epidemic. After Ebola, \u201cthere is a real opportunity to change the status quo\u201d, says Jean-Fran\u00e7ois Alesandrini, a spokesman for the  Drugs for Neglected Diseases Initiative  (DNDi), a non-profit body working on long-ignored diseases such as leishmaniasis. In a paper published in May, leading researchers and public-health officials proposed the creation of an international not-for-profit pharmaceutical body, bringing together research organizations, governments, charities and private pharmaceutical companies that would research, develop and manufacture medical countermeasures for the many global-health threats for which there is little or no market (  M. Balasegaram  et al .  PLoS Med.   12 , e1001831; 2015). Such efforts have precedent in public\u2013private partnerships (PPPs) that have sprung up over the past 15 years, including the DNDi. The proposed initiative would be similar, but writ large: with proposed funding of $10 billion annually, it would focus not only on emerging epidemic threats, but also on existing neglected diseases and developing much-needed new antibiotics. This would share limited resources, ensure sustained financing, and allow more coherent long-term planning. \u201cThere is no PPP for outbreak pathogens. It is time to create one,\u201d says Hill. \u201cIf this doesn't happen soon, the opportunity will be lost as global attention moves on.\u201d Pharmaceutical companies are generally supportive of the proposal \u2014 which is crucial, because such ventures typically need access to the vast drug libraries, vaccine-technology platforms and manufacturing capacity that only industry possesses. Within such a scheme, Hill favours the immediate and accelerated development of vaccines against priority threats such as MERS and  Marburg  \u2014 a virus from the same family as Ebola that kills most of those it infects. He suggests foregoing the slow, costly animal studies that require high biosafety and biosecurity labs to contain the viruses, and instead developing small batches of vaccine that could be put directly through phase I safety and dosage testing in humans. If the vaccines were safe, and generated a good immune response, it is likely they would work, he says. Stockpiles could then be created, ready for phase II efficacy trials to start as soon as an outbreak occurs \u2014 so that it \u201ccan be nipped in the bud\u201d, Hill says. Researchers are encouraged by the announcement this week that a clinical trial of an Ebola vaccine has had positive results (see  page 13 ). But that still leaves the unknown pathogens, which are harder to prepare for. One option in such an outbreak would be to  transfuse patients with the plasma of survivors , whose blood is often rich in antibodies specific to the virus, says Ian Lipkin, a virologist and outbreak specialist at Columbia University in New York. In many cases, this technique could provide a quick, ready-made therapy to an unknown pathogen, bypassing the years of research it can take to find drugs or vaccines. The approach gained prominence during the Ebola outbreak:  clinical trials of 'convalescent plasma' for Ebola  began in West Africa in December, and results are expected in coming months. Lipkin would like to see the infrastructure for collecting and processing blood and plasma improved in poorer countries, where it is often lacking. Ideally, say researchers,  clinical-trial designs would also be approved by regulators before an outbreak  so that  a trial could launch straight away . This is already being done by researchers in the International Severe Acute Respiratory and Emerging Infection Consortium, an international network of outbreak specialists based in Oxford that aims to develop generic clinical-trial protocols that can be adapted to any epidemic threat. Reforming the world's epidemic response systems is not going to be easy, and public-health specialists are well aware that impetus might be lost as the Ebola epidemic fades from the limelight. But they also think that the shocking events in West Africa \u2014 bodies on the streets, nationwide quarantines, economies collapsing \u2014 have left an indelible mark. The West African epidemic has been a \u201cgame-changer\u201d in how the world prepares for a serious epidemic, says Morens. The era after Ebola, he hopes, will be very different from the one before it. \n                     Disease outbreak: Finish the fight against Ebola 2015-Aug-05 \n                   \n                     Ebola: Embed research in outbreak response 2015-Aug-05 \n                   \n                     Trial and triumph 2015-Aug-05 \n                   \n                     How Ebola-vaccine success could reshape clinical-trial policy 2015-Aug-04 \n                   \n                     Researchers frustrated by failure to roll out 'game-changing' Ebola test 2015-Jun-26 \n                   \n                     Hidden African typhoid epidemic traced to drug-resistant bacteria 2015-May-11 \n                   \n                     Ebola raises profile of blood-based therapy 2014-Dec-23 \n                   \n                     Flu surveillance lacking 2012-Mar-28 \n                   \n                     H5N1 surveillance: Shift expertise to where it matters 2012-Mar-28 \n                   \n                     Emerging disease: Looking for trouble 2009-Dec-09 \n                   \n                     Disease monitors 'looking in the wrong places' 2008-Feb-20 \n                   \n                     Nature  special: Ebola outbreak \n                   \n                     Africa Center Of Excellence for Genomics of Infectious Diseases \n                   \n                     EcoHealth Alliance \n                   \n                     USAID Emerging Pandemic Threats programme \n                   Reprints and Permissions"},
{"file_id": "524020a", "url": "https://www.nature.com/articles/524020a", "year": 2015, "authors": [{"name": "XiaoZhi Lim"}], "parsed_as_year": "2006_or_before", "body": "Slow, solid-state reactions used by lichens and Renaissance pigment-makers could help to make chemistry greener. Cristina Mottillo is in no rush. She pours finely ground white powder into a Petri dish, carefully rolls it flat with the side of a small glass vial, then seals it into a chamber where the heat and humidity are like those on a sweltering summer day in the tropics. \u201cNow,\u201d she says, \u201cwe wait.\u201c Over the next four days, with no further effort from Mottillo, the three chemicals in that powder will gradually turn into ZIF-8: a stable, porous compound called a metal\u2013organic framework that could find widespread use in carbon capture and storage, and that is worth more than 100 times the raw materials' original value. \u201cThe reactants do all the work,\u201d says Mottillo, a chemistry PhD student at McGill University in Montreal, Canada. This is a radical departure from standard chemical-synthesis methods, which typically involve dissolving, heating and stirring ingredients in a solution to encourage them to react quickly. These techniques are fast and well understood, but they tend to consume large amounts of chemicals and energy, and pose a major environmental challenge. An estimated 50\u201380% of all chemical waste produced by industry and university labs consists of solvents left over from synthesis, separation and purification. For around two decades, a worldwide 'green chemistry' movement has been trying to find ways to minimize these toxic waste streams. But Mottillo is one of a handful of scientists starting to adopt an approach that is radical even by the movement's standards. Her PhD supervisor, McGill chemist Tomislav Fri\u0161\u010di\u0107, describes it as \u201clazy man's chemistry\u201d: let a mix of solid reactants sit around undisturbed while they spontaneously transform themselves. More properly called slow chemistry, or even just ageing, the approach requires few, if any, hazardous solvents and uses minimal energy. If planned properly, it also consumes all the reagents in the mix, so that there is no waste and no need for chemical-intensive purification. Such processes have been known for millennia: rusting iron is a familiar example, as is the decades-long weathering process that produced the Statue of Liberty's green patina. But only now are scientists starting to understand these processes and learn how to control them to obtain the products they want. In the past decade, research groups have used such techniques to produce valuable products, including organometallic complexes, pharmaceuticals, simple organic compounds and photoluminescent materials. Proponents such as Fri\u0161\u010di\u0107 are hoping to make many more. \u201cThe ultimate goal,\u201d he says, \u201cis really to clean up the chemical manufacturing industry.\u201c \n               Slow and steady \n             That could take a while: even the most ardent advocates agree that ageing faces an uphill struggle for credibility. Students are taught that good chemistry frequently starts with the right solvent: molecules in solution can react much faster than they otherwise would, because they are free to tumble and collide with one another, which facilitates the making and breaking of bonds. But slow chemistry happens in solids, where, by definition, everything is held rigidly in place. \u201cPeople tend to think of a stone as a grave for molecules,\u201d says Dario Braga, a solid-state chemist at the University of Bologna, Italy. But that is not true. Solid-state reactions can take months or years, but they do exist in nature. In Western Australia, deposits of bird guano reacting with copper sulfide minerals in rocks have formed moolooite: an uncommon green copper oxalate mineral. Lichens living on rock often secrete a mixture of simple, weak organic acids that slowly react with minerals to produce complex metal\u2013organic materials, which give the lichens some protection from invasive microorganisms. Well into the nineteenth century, ageing was used to produce lead white, a pigment that is among the most widely used in art history. Manufacturers placed rolled-up sheets of lead over buckets containing a small amount of vinegar, and then left the buckets on a bed of manure in a shed. The metal would slowly react with water vapour in the air and carbon dioxide from the manure, turning into a white material now known to be a mixture of lead carbonate and lead hydroxide. The vinegar acted as a catalyst, and the decomposing manure kept the shed warm enough for the process to proceed at a reasonable rate. After about three months, the pigment was scraped off, washed and ground into a fine powder. It was used in paintings such as Leonardo da Vinci's  Mona Lisa  (around 1506) and Johannes Vermeer's  Girl with a Pearl Earring  (1665). But the recent surge in slow chemistry has nothing to do with art. One factor has been interest from the pharmaceutical industry, which would like better control over the ageing processes that can slowly degrade drugs in pill form (see \u2018Slow, slower, slowest\u2019). Another is that solid-state chemistry is no longer the mystery it once was. Reactions in solids tend to be much more complex than those in liquids, where molecules quickly diffuse into a uniform mixture. Solids are often poorly mixed agglomerations of very different particles, and are riven with cracks and other structural defects, where chemical reactions can take place in different ways and at different rates. But rapid improvements in imaging techniques such as X-ray crystallography, nuclear magnetic resonance scanning and electron microscopy are now giving chemists a better understanding of how those reactions proceed in real time, and what they eventually produce. \n               boxed-text \n             Such insights, in turn, have helped proponents to streamline and improve on natural ageing processes, while countering the perception that ageing is too slow and unpredictable to be of practical use. \u201cIt's not slow if you plan in advance,\u201d insists Fri\u0161\u010di\u0107, whose group is trying to better understand and exploit ageing reactions. Mottillo's experiments in the green synthesis of metal\u2013organic frameworks, for example, are an attempt to accelerate the chemistry between minerals and lichen acids. \n               Practical magic \n             Another student in Fri\u0161\u010di\u0107's group has used a different ageing process to synthesize various metal\u2013organic materials from oxides of main-group metals, transition metals and lanthanides \u2014 solids that tend to have very high melting points and low solubility. The researchers found that each metal oxide ages at a different speed 1 , so they have patented this as a way to isolate the metals from one another: the ageing products are less dense than the oxides, so they will float in an intermediate-density liquid while the remaining oxides sink. Metal oxides, says Fri\u0161\u010di\u0107, are ideal reagents because they are cheap, safe, widely available and produce only water as a by-product. Other metal salts, such as chlorides or nitrates, produce acids that end up as toxic waste. Furthermore, many metals occur naturally as oxides that have to be leached from ore with strong acids; with ageing, says Fri\u0161\u010di\u0107, one could bypass that step, and make valuable metal\u2013organic frameworks directly from rocks. He and his team are working on scaling this process up to bring it to the metal extraction and separation industry. As for speed, says Fri\u0161\u010di\u0107, \u201cwe can get reactivity going if we use a few tricks\u201d \u2014 most of them quite straightforward. One is to put the samples in a humid atmosphere: water vapour can migrate through holes in the solid structures, acting as a lubricant to help atoms or molecules inside the solid to diffuse, react or even rearrange into new structures. Another technique is to increase the temperature to, say, 45 \u00b0C \u2014 a far cry from the hundreds of degrees typical of industrial reaction vessels, but enough to make the ageing process run faster. \u201cIf we were living in India, we could potentially do it outside,\u201d says Mottillo. And a third trick is to do what the lichens cannot, and grind the reactants together into a fine, homogeneous mixture to increase the surface area of the particles, where they touch each other and can react. That is how Mottillo was able to complete her ZIF-8 synthesis in days instead of weeks 2 . Braga and his group have used ageing, or vapour digestion as they call it, to make a variety of materials by exposing solid reactants in a vial to solvent vapour. For example, by letting solid copper( i ) iodide sit with an organic compound for about a week in water, acetonitrile or toluene vapour, they obtained three new copper-based polymers that glow after exposure to ultraviolet light 3 . Such compounds could be used in light-emitting diodes and screen displays. But even more important, says Braga, is that copper(I) iodide is notoriously difficult to dissolve in common solvents; vapour digestion offers a way to make it and other insoluble materials more accessible to chemistry. Braga and Fri\u0161\u010di\u0107 are still not sure exactly what is happening in these reactions. But Dominik Cin\u010di\u0107, an assistant professor at the University of Zagreb, is applying their method to organic synthesis, a branch of chemistry that conventionally relies on energy- and chemical-intensive methods. Cin\u010di\u0107's group has demonstrated 4  that vapour digestion can be used to synthesize Schiff bases, small organic molecules containing a carbon\u2013nitrogen double bond. The team's next goal is to use the method in a one-step synthesis of amines: nitrogen-containing organic compounds that are used in many dyes and drugs, and that typically require two or three synthesis steps. Everyone working on ageing-based synthesis concedes that there is a long way to go. The mechanisms are not yet well understood and there are no good computational models to speed up research. Furthermore, sceptics doubt that the chemical industry can ever do without solvents entirely. Walter Leitner, a green chemist at RWTH Aachen University in Germany, points out that ageing research has had the most success in inorganic synthesis \u2014 which has historically had a much smaller environmental impact than organic synthesis, where the most solvents are used. In organic synthesis, he says, the most practical target a green chemist can aim for is to find ways to replace toxic solvents with environmentally benign ones such as water. Still, such objections have not discouraged Fri\u0161\u010di\u0107. \u201cEverything you can do in solution, you can do with ageing, and more,\u201d he declares. At the moment, he is exploring the mechanisms behind ageing by monitoring reactions as they happen. \u201cAll that one needs to do,\u201d he says, \u201cis to explore.\u201d \n                     Materials science: The hole story 2015-Apr-08 \n                   \n                     Organic synthesis: The robo-chemist 2014-Aug-06 \n                   \n                     Materials chemistry: Space invaders 2007-Aug-15 \n                   \n                     Tomislav Fri\u0161\u010di\u0107 \n                   \n                     Dario Braga \n                   \n                     Dominik Cin\u010di\u0107 \n                   Reprints and Permissions"},
{"file_id": "524406a", "url": "https://www.nature.com/articles/524406a", "year": 2015, "authors": [{"name": "Melinda Wenner Moyer"}], "parsed_as_year": "2006_or_before", "body": "Scientists have no shortage of ideas about how to stop tick-borne illnesses. What is holding them back? On a balmy day in late June, Scott Williams waits for a white-footed mouse ( Peromyscus leucopus ) to fall asleep. Williams, a wildlife biologist with the Connecticut Agricultural Experiment Station in New Haven, has just transferred the animal from a trap to a plastic bag containing a cotton ball doused in anaesthetic. As soon as the mouse's breathing slows to one breath per second, Williams will take it out, draw blood, weigh it, put an ear tag on it for identification and check the animal for ticks, saving any that are engorged with blood. He must work quickly. The mouse will wake up in about two minutes, and she might be grumpy. Williams is testing whether vaccinating mice against  Borrelia burgdorferi , the bacterium that causes Lyme disease in the United States, can reduce the proportion of ticks that are infected. Health officials are looking on with interest. Connecticut has one of the highest rates of human Lyme disease in the country, and June is peak time for transmission.  Borrelia burgdorferi  infects an estimated 329,000 people in the United States each year, according to the US Centers for Disease Control and Prevention (CDC) in Atlanta, Georgia. And although most people who get prompt treatment recover quickly \u2014 Williams has had Lyme three times \u2014 up to one in five develops long-term and potentially life-threatening symptoms, including heart, vision or memory problems, or debilitating joint pain. Williams's approach is one of several strategies being tested in an attempt to thwart the spread of tick-borne diseases. Some, like the mouse vaccine, interrupt the pathogen's ecological circuitry by targeting the wild animals that pass along and amplify the disease. Others, such as efforts to revive a human Lyme vaccine, aim to protect people from infection directly. A more radical approach could hamper the ability of ticks to bite humans or animals, potentially protecting against dozens of illnesses spreading across the United States, Europe, Africa and Asia. That the field needs creative solutions is clear. Many long-recommended interventions, such as pesticide application or controlling populations of deer, which are an important host for adult ticks, have had mixed success in scientific studies. Even the time-honoured protective strategies that most people use are not evidence-based. \u201cWe tell people to wear repellents, to do tick checks and to shower if they've been in the field, but there's very little data to show that these things reduce human illness,\u201d explains Ben Beard, chief of the CDC's bacterial-diseases branch in the division of vector-borne diseases. Diseases spread by ticks are on the rise around the world, spurred by a combination of factors, including shifting climates and population sprawl into rural areas. Reported cases of Lyme, the most common US tick-borne illness, have nearly tripled in the country since 1992, although some of the increase could be due to heightened awareness. Lyme is also a growing problem in parts of Europe, Mongolia and China. Yet as bad as it is, there are nastier threats on the rise. In parts of Africa, the Middle East, Asia and southern Europe, ticks can spread Crimean\u2013Congo haemorrhagic fever, which is fatal in 40% of cases. And a tick-borne relapsing fever afflicts as many as 1 in 20 residents in parts of Senegal. In the United States, ticks spread at least  16 illnesses , including anaplasmosis, babesiosis, ehrlichiosis and Rocky Mountain spotted fever, all \u201cserious, life-threatening infections\u201d, Beard says. And many are increasing in incidence more quickly than Lyme. In a  July 2015 position statement , the Entomological Society of America argued for a national strategy to combat tick-borne diseases. \u201cThe recent confluence of environmental, ecological, sociological, and human demographic factors,\u201d it said, \u201chas created a near 'perfect storm' leading to more ticks in more places throughout North America.\u201d \n               Backyard battlegrounds \n             Williams tags, weighs and releases his mouse just in time. It has no ticks to bring back to the lab for further analysis, but there will be other opportunities. Members of 32 Connecticut households have volunteered to place traps around their properties, and some will also get boxes of mouse treats laden with vaccine. The hope is that, over time, fewer mice and ticks will harbour the bacteria at the sites with the vaccine bait. The plan is unconventional, because most Lyme-control measures focus on white-tailed deer ( Odocoileus virginianus ), which have exploded in number in the United States over the past century as young forests have become increasingly fragmented by human development and  large predators have been all but eradicated . Adult blacklegged ticks ( Ixodes scapularis ) typically feed and mate on deer, so many scientists have argued that the only way to get rid of Lyme is to get rid of the deer. But such efforts have had \u201can incredibly spotty record\u201d, says Richard Ostfeld, a disease ecologist at the Cary Institute of Ecosystem Studies in Millbrook, New York, who has been studying tick-borne diseases for decades. When Sam Telford, an epidemiologist at Tufts University in North Grafton, Massachusetts, and his colleagues cut the deer population on Great Island in Cape Cod by 50% in the early 1980s, they saw no drop in tick numbers \u2014 the number of tick larvae on the island actually increased 1 . Ostfeld argues that you do not need many deer to maintain a large tick population. When deer numbers drop, ticks can either crowd in on the remaining deer or find other hosts. Only when almost all of the deer on Great Island had been eliminated did tick populations plummet. But, says Telford, \u201cit is a nightmare trying to get the deer population down that low\u201d. And anywhere that is not an island, keeping populations down is practically impossible. \n               Danger mouse \n             Ostfeld and others contend that mice are a major driver for both the tick problem and the disease problem. Mice, like deer, flourish in fragmented woodlands \u2014 in part because predators such as foxes and opossums get displaced. Ticks then thrive on the rodents, which are poor groomers. Studies suggest that larval ticks have a 50% chance of surviving when they feed on mice, but only a 3.5% chance on opossums 2 . And mice are typically where ticks pick up  B. burgdorferi . Most mice in Lyme-endemic areas get infected with the bacterium at a young age and, for reasons that are not completely clear, they are particularly good at transmitting it to other ticks. Almost all young ticks that feed on white-footed mice become infected, compared with a mere 1% of ticks that feed on deer. Interrupting the tick\u2013mouse infection cycle, says Ostfeld, could make ticks a lot less dangerous. Maria Gomes-Solecki, a medical microbiologist at the University of Tennessee Health Science Center in Memphis, agrees \u2014 which is why she invented the mouse vaccine that Williams is testing. It primes the mice to make antibodies against outer surface protein A (OspA), a molecule that  B. burgdorferi  expresses when it is in a tick's gut. A mouse eats the vaccine, then starts to produce OspA antibodies. The next time a tick feeds on the mouse, the antibodies attack the bacteria in its gut, clearing the infection. As the proportion of ticks infected with  B. burgdorferi  drops, it becomes less likely that the next generation of mice will pick up the parasite, even without vaccination. Ostfeld and his colleagues reported the first field tests 3  of Gomes-Solecki's vaccine in 2014, and found that although only 28% of the mice in an area that they targeted for 5 years developed protective levels of OspA antibodies, the prevalence of infected blacklegged-tick nymphs (the life stage between larvae and adults) dropped by 75%. The bait-based vaccine is also attractive because it is less ecologically destructive than other strategies \u2014 it does not kill animals or even ticks, just the pathogens. Gomes-Solecki, who licensed her technology to a company she founded, US Biologic in Memphis, would like to see homeowners putting walk-through bait boxes for mice around their gardens. Or, she says, local governments could disperse the bait in parks or forests, much as they do with bait-based rabies vaccines for raccoons and coyotes. \u201cThe rodents seem to love them,\u201d Williams says of the vaccine-laced treats. One of his colleagues calls them \u201cFritos for mice\u201d. Other scientists argue for more a direct means of protecting people against Lyme, ideally with a human vaccine. When vaccine researcher Stanley Plotkin's son was 35, he fell ill with Lyme disease. As often happens with the infection, a doctor missed the diagnosis and the young man went untreated for months. Bacteria invaded his heart and he collapsed one day while walking his dog. Plotkin, now an emeritus professor at the University of Pennsylvania in Philadelphia, says that when paramedics arrived, his son's heart rate was dangerously low. He has since recovered, but the experience \u201cfurther convinced me, if I needed any convincing\u201d, Plotkin says, \u201cthat the lack of a Lyme-disease vaccine was a public-health tragedy\u201d. Plotkin worked on a vaccine in the 1990s. Ultimately, a competing product called LYMErix, manufactured by UK-based pharmaceutical company SmithKline Beecham (now GlaxoSmithKline), was approved by the US Food and Drug Administration in 1998. It reduced the risk of Lyme caused by US strains of  Borrelia  by 76% in clinical trials 4 . But it faced problems from the start. First, it garnered lukewarm support from health officials in the United States and was recommended only for people aged 15 to 70 in regions where Lyme is endemic. Then, some recipients complained of autoimmune-related side effects such as arthritis and filed lawsuits against SmithKline Beecham. The company  voluntarily shelved LYMErix  in 2002. Plotkin maintains that this was a mistake. \u201cThe vaccine was safe,\u201d he says. Now, a new and potentially improved vaccine has completed safety trials 5 . Developed by researchers at Stony Brook University and Brookhaven National Laboratory in New York, and licensed to Baxter Innovations in Vienna, the vaccine is similar to LYMErix in that it targets OspA, but it does not contain the protein segment that some scientists and consumers feared could cause an autoimmune reaction. It also contains several variants of OspA, so it protects against many  Borrelia  species known to cause Lyme in humans, including those that affect people in Europe. Nevertheless, the vaccine's future is uncertain: in 2014, Pfizer bought the rights to sell many of Baxter's vaccine products, but not the Lyme candidate. Baxter is now in talks with Great Plains Biotechnology of Roca, Nebraska, which has expressed interest in purchasing and developing the Lyme vaccine. Richard Marconi, a microbiologist and vaccinologist at Virginia Commonwealth University in Richmond, says that he and his colleagues are working on an even better vaccine. One downside of an OspA vaccine is that it requires frequent boosters, because OspA antibodies have to be circulating constantly in the blood if they are to attack  B. burgdorferi  inside a biting tick. Marconi's team is developing a vaccine against immunologically relevant portions of the surface protein OspC, which  B. burgdorferi  expresses when it is inside mammals. On being bitten by infected ticks, vaccinated individuals can produce OspC antibodies from immunological memory; the antibodies do not have to be circulating already. Marconi and his colleagues have already licensed a version of the vaccine for use in dogs, and \u201cthe success of the canine vaccine and the uniqueness of the approach suggests that it's going to be highly effective in humans\u201d, he says. In light of the problems faced by LYMErix, however, the question remains whether health officials and consumers will embrace a human vaccine. \u201cI think, maybe optimistically, that the emotional situation has changed over the last 10 or 15 years \u2014 that is, that more people are convinced of the importance of Lyme disease,\u201d Plotkin says. But it is hard to know whether fears about Lyme will trump fears about the vaccine. Mouse vaccines would not raise such concerns, but some researchers, including Plotkin, are sceptical about whether they could dose enough mice to reduce Lyme rates. And both vaccine approaches are limited because they combat only one tick-borne disease, when more than a dozen others are spreading throughout the world (see 'Reality bites'). \n               Tick spit \n             There is one strategy that could conquer them all, and it involves turning one of the tick's most ingenious tools \u2014 its saliva \u2014 against it. When a tick bites a host, molecules in its saliva help it to evade detection and start to feed by blocking pain, inflammation and immune signals. If a vaccine could raise an immune response to key salivary proteins, it could make tick bites more noticeable or block the tick's ability to feed. Ostfeld himself is a proof-of-concept for this approach. He has been bitten more than 100 times, and his body now reacts to tick saliva. \u201cI realize when a tick is biting me because I get a burning sensation. It's pretty intense,\u201d he explains. Ostfeld has ample time to remove the tick before it can transfer an infection \u2014 if it even survives the experience. Often, Ostfeld says, he will remove a tick only to discover that, for unknown reasons, it is already dead. A European Commission-funded consortium called ANTIDotE (Anti-tick Vaccines to Prevent Tick-borne Diseases in Europe) is characterizing the tick salivary proteins that could be targeted to thwart feeding. In 2011, a member of the group reported 6  a technique to rapidly identify those proteins that react with the blood serum of tick-immune animals. When the team vaccinated rabbits against three salivary proteins that it had identified \u2014 including one that ticks use to inhibit blood coagulation and one that inhibits the host's immune response \u2014 it found that ticks had trouble getting blood from them. Researchers in the group are also working to identify the salivary genes involved in  B. burgdorferi  transmission. \u201cWe think that an anti-tick vaccine could be immensely useful in protecting both humans and animals,\u201d says Hein Sprong, an ANTIDotE leader at the National Institute for Public Health and the Environment in Bilthoven, the Netherlands. US Biologic also plans to develop a bait-based vaccine for mice that could thwart tick feeding, thereby protecting against multiple diseases. That could reduce overall tick numbers, too, because it would make it difficult for larval ticks to get the meals that they need to survive into adulthood and reproduce. But these approaches are hardly around the corner. Part of the problem, scientists say, is that  funding is scarce . The stereotype of Lyme and other US tick-borne diseases as primarily 'yuppie' illnesses does not help; Ostfeld says he has seen comments to this effect on reviews of his grant proposals. \u201cThey say something like, 'Is it really worth spending taxpayer dollars on a disease of the affluent in the northeastern United States, when there are so many diseases of people who live in poverty overseas?',\u201d he says. \u201cIn one sense, I think that's a legitimate point, but in another, I think it underestimates the impact of this disease on a vast number of citizens, not all of whom are affluent, not even close.\u201d Another potential reason for low funding is that in the United States Lyme and similar infections are only rarely fatal. Each year, more people in the United States are diagnosed with Lyme than with prostate cancer, but research funding for the latter from the National Institutes of Health was more than ten times that for Lyme in 2014. Until an all-encompassing solution becomes available, controlling tick-borne diseases will probably require an array of smaller-scale approaches that attack the problem, bit by bit, on a number of levels. That an arsenal of such weapons might be needed to hold back the enemy is not particularly surprising, considering the complexity of tick-borne-disease ecology, how drastically humans have been changing it, and how close people live to these disease-carrying parasites. \u201cWe've disrupted the balance of nature,\u201d Telford says. Steadying the scales again will be no small feat. \n                 Tweet \n               \n                     Resurrecting the 'yuppie vaccine' 2014-Jul-07 \n                   \n                     Lyme bacterium's possible ancestor found in ancient tick 2014-Jun-06 \n                   \n                     Lyme bacteria show that evolvability is evolvable 2013-Nov-14 \n                   \n                     Antibodies linked to long-term Lyme symptoms 2011-Aug-05 \n                   \n                     Scientists push for Lyme disease trials 2010-Oct-14 \n                   \n                     The chronic debate over Lyme disease 2008-Nov-01 \n                   \n                     US CDC reference manual on tick-borne diseases (PDF) \n                   Reprints and Permissions"},
{"file_id": "524402a", "url": "https://www.nature.com/articles/524402a", "year": 2015, "authors": [{"name": "Hannah Hoag"}], "parsed_as_year": "2006_or_before", "body": "Rising temperatures are threatening urban areas, but efforts to cool them may not work as planned. The greenhouses that sprawl across the coastline of southeastern Spain are so bright that they gleam in satellite photos. Since the 1970s, farmers have been expanding this patchwork of buildings in Almer\u00eda province to grow produce such as tomatoes, peppers and watermelons for export. To keep the plants from overheating in the summer, they paint the roofs with white lime to reflect the sunlight. That does more than just cool the crops. Over the past 30 years, the surrounding region has warmed by 1 \u00b0C, but the average air temperature in the greenhouse area has dropped by 0.7 \u00b0C (ref.  1 ). It's an effect that cities around the world would like to mimic. As Earth's climate changes over the coming decades, global warming  will hit metropolitan areas especially hard  because their buildings and pavements readily absorb sunlight and raise local temperatures, a phenomenon known as the urban heat island effect. Cities, as a result, stand a greater chance of extreme hot spells that can kill. \u201cHeat-related deaths in the United States outpace \u2014 over the last 30 years \u2014 all other types of mortality from extreme weather causes,\u201d says Kim Knowlton, a health scientist at Columbia University in New York. \u201cThis is not an issue that is going away.\u201d Some cities hope to stave off that sizzling future. Many are planting trees and building parks, but they have focused the most attention on rooftops \u2014 vast areas of unused space that absorb heat from the Sun. In 2009, Toronto, Canada, became the first city in North America to adopt a green-roof policy. It requires new buildings above a certain size to be topped with plants in the hope that they will retain storm water and keep temperatures down. Los Angeles, California, mandated in 2014 that new and renovated homes install 'cool roofs' made of light-coloured materials that reflect sunlight. A French law approved in March calls for the rooftops of new buildings in commercial zones to be partially covered in plants or solar panels. But the rush to act is speeding ahead of the science . Although cool roofs and green roofs can strongly curb temperatures at the tops of buildings, they do not always yield benefits at the street level, and they may trigger unwanted effects, such as reducing rainfall in some places. \u201cThere was a notion that the community had reached a conclusion and there was a one-size-fits-all solution,\u201d says Matei Georgescu, a sustainability scientist at Arizona State University in Tempe. \u201cBut that is not the case.\u201d On top of that, it is unclear whether the limited programmes currently in place will have a measurable effect on temperature \u2014 and citizen health \u2014 and whether cities will expand their efforts enough to produce results. \u201cIf you're just putting green roofs on city hall and schools, it's not going to move the needle,\u201d says Brian Stone Jr, an urban scientist at the Georgia Institute of Technology in Atlanta. \n               Hot time in the city \n             For ten days in August 2003, an  unprecedented heatwave stifled Western Europe , breaking records reaching back five centuries. Daytime temperatures in Paris shot up to 40 \u00b0C and nights remained torrid. By the end of August, the death toll from dehydration, hyperthermia, heat stroke and respiratory problems for all of Europe surpassed 70,000, with many fatalities in the urbanized areas around Paris and Moscow. This is just a taste of conditions to come. Regional climate models indicate that by 2050, week-long heat spells on par with the August 2003 event may strike once a decade in Eastern Europe and every 15 years in Western Europe 2 . Across the globe, the number, duration and frequency of heatwaves is projected to increase. \u201cThis is one of the few extreme events where all of the models agree with each other,\u201d says Dan Li, a climate modeller at Princeton University in New Jersey. And when temperatures rise, cities suffer disproportionately because of the way they are built. Dark roofs, roads and other construction materials absorb incoming short-wave radiation from the Sun and re-radiate it as long-wave energy, warming the atmosphere nearby. Air conditioning adds to the problem by pulling heat from inside buildings and vehicles and dumping it outside, further driving up urban temperatures. In the absence of interventions, heat islands will only grow: by 2050, urban surface area in the United States is expected to expand by one-third. At the same time, the global population is projected to grow to 9.6 billion, with two-thirds living in urban areas, compared with just over half today. It all adds up to more heat-trapping potential and more people affected by extreme heat. And yet despite the risks, few cities have plans in place to address urban heat directly. In the United States, says Stone, \u201cmost cities are ignoring the climate issue\u201d. Los Angeles is not. The city has seen its annual average temperature rise by more than 2 \u00b0C since 1878. By mid-century, its downtown area could face 22 days of extreme heat annually (temperatures exceeding 35 \u00b0C), nearly four times the long-term average 3 . To counter the warming, the city aims to convert 10,000 dark-coloured roofs to cool roofs by 2017. By pairing this effort with street plantings and reflective pavements, it intends to shave 1.65 \u00b0C off the urban heat island effect by 2035. Chicago, Illinois, has also become a leader on this issue: it hopes to prevent the kind of mass deaths seen during the city's 5-day heatwave in 1995, when 700 people died. Since that disaster, it has added cool roofs, green roofs and street plantings \u2014 and transformed black-top playgrounds into grass fields. Incentives have helped to trigger the construction of more than 516,000 square metres of green roofs on 509 buildings. Toronto is rapidly catching up. It requires new buildings taller than six storeys and with more than 2,000 m 2  of roof space to cover 20\u201360% of that with plants. Since 2010, the city has added 260 green roofs covering 196,000 m 2 . Some forms of cool roofs can be comparable in price to regular ones, but green roofs are more expensive to install, and they have higher maintenance costs. They offer other benefits, however, such as slowing storm-water run-off, providing habitat for pollinating insects and making cities more beautiful. \n               Seeing green \n             On the roof of the University of Toronto's architecture building, bumblebees flit from one yellow flower to another. Located in the city's downtown, the building is topped by a patchwork of 33 rectangular, raised garden beds planted with native grasses, flowers or non-native sedums \u2014 plants with waxy, water-storing leaves. Each bed has a different combination of plants, soil and irrigation techniques \u2014 all of which are monitored by 270 sensors measuring air temperature, soil temperature, soil moisture and rainwater run-off. The garden beds are part of the Green Roof Innovation Technology Laboratory (GRIT Lab), the only facility of its kind in Canada to test the performance of green roofs and other strategies to mitigate climate change. Green roofs reflect more sunlight than conventional tar or gravel roofs, but they get much of their cooling power from moisture in the plants and soil. As water in leaves and soil evaporates, it carries heat to the atmosphere and lowers air temperature nearby, just as athletes cool off when their sweat evaporates. Compared with a black roof, a green roof can be 40 \u00b0C cooler on a hot summer day. Green roofs also act as insulators and reduce energy costs associated with cooling. The coolest test bed on the GRIT Lab roof is irrigated and contains organic soil and a thick mat of sedum that flows over the box edges. Its neighbour has a patchy lawn of meadow grasses growing in an unirrigated box lined with a porous rocky medium that is widely used on green roofs. At its surface, the sedum box is 4 \u00b0C cooler than the air temperature, but the sparsely planted meadow-grass box can be 14 \u00b0C hotter, notes Liat Margolis, GRIT Lab's director. \u201cYou might as well not have it on the roof,\u201d she says. Experiments such as this show how difficult it can be to find the right combination of substrate, vegetation and irrigation to have an impact on the temperature of one rooftop, Margolis adds. And even with the best green roofs, no one knows how much this approach can cool a whole city. Only a few simulations have evaluated green roofs at that scale. A decade-old report prepared for the city of Toronto suggests that if the city's entire 50 square kilometres of available rooftop were converted to green roofs, the ambient air temperature would reduce by 0.5\u20132 \u00b0C. But the area of green roofs added since the by-law came into effect amounts to less than 0.5% of the city's available roof space. In the Baltimore\u2013Washington metropolitan area, a 2014 study projected that 90% of the rooftops had to be planted to decrease the daily maximum air temperature by 0.5 \u00b0C during a 3-day heatwave 4 . Because there is less evaporation when the Sun goes down, green roofs do not cool as well at night \u2014 a deadly time during heatwaves. And rooftop plantings release stored heat after the Sun goes down. \u201cIf you try to mitigate the urban heat by putting up green roofs, it will do some good for reducing temperatures during the day, but it might increase at night,\u201d says David Sailor, an urban climate scientist at Portland State University in Oregon. It's also unclear how well green roofs many storeys up offer relief to people below. A modelling study 5  that replicated a day during the 2003 European heatwave found that planting 25% of the roof area along a street in the centre of Arnhem in the Netherlands had no effect on street-level temperatures because the wind blew away the cooler air before it could reach the ground. Observations of green roofs in Chicago have also raised questions about their benefits. Comparing satellite images from 1995 and 2009, researchers checked how surface temperatures had changed at spots within the city where dark roofs and pavement had been replaced by vegetation or brighter coatings. The green roofs did not alter temperatures significantly, but cool roofs did 6 . They increased the city's reflectivity, or albedo, by 1.6% \u2014 equivalent to the cooling power of 65,000 large, window-sized air conditioning units operating at full capacity over the summer. Planting trees and converting paved areas to grass were also more effective than green roofs. \n               Downsides of cool \n             The effects in Almer\u00eda are even bigger. Each summer, after farmers whitewash the greenhouse roofs with slaked lime, they reflect 35% of the incident sunlight. The pastures nearby reflect just 15%, on par with most cities. Simulations show that by whitening roads and roofs, cities could cool down considerably. Doubling the rooftop albedo of Los Angeles, which is on average 17%, could trim temperatures by 0.5 \u00b0C in some areas and by as much as 2 \u00b0C in others, says Haider Taha, an urban atmospheric modeller and president of Altostratus, a meteorological company in Martinez, California. \u201cOne degree Celsius isn't too much to ask for, and according to the models, it's doable,\u201d he says. But cool roofs could also produce some unfavourable effects, depending on the location. If the albedo is increased too much, it could slow local sea breezes, reduce air quality or warm downwind areas, says Taha. \u201cEach city has a threshold, and if you go beyond that, some things start looking bad.\u201d And cool roofs could also inhibit rain. In many regions, heating of the ground during the day causes moist air to rise, driving cloud formation and precipitation. \u201cIf we don't have that, then we don't have precipitation,\u201d says Georgescu. In a modelling study 7 , he found that if cool roofs were widely implemented in urban areas from Florida to the northeastern United States, daily summertime precipitation could decrease by 2\u20134 millimetres by 2100. Despite the uncertainties, many scientists say that cities are not pursuing cooling strategies quickly enough, given the pace of climate change and urban growth. Proponents say that both green roofs and cool roofs have helped in some situations and that careful implementation could improve their efficiency. Stuart Gaffin, an urban-climate scientist at Columbia University in New York, warns against placing too much stock in modelling studies that forecast unwanted side effects such as reduced cloud cover and rainfall. Clouds are among the most complex things to model, and cities already enhance rainfall because of the particulates they produce, he says. Despite all of the heat-related risks that cities face in the future, few have put heat-management plans in place. Louisville in Kentucky is one: it will soon become the first major US city to develop an urban heat-adaptation plan, says Stone, who is leading the project. The effort is driven by necessity. Louisville has the fastest warming urban heat island in the United States, and temperatures there have climbed by more than 4 \u00b0C since 1961. Part of the problem is that the city has lost 54,000 trees per year to insects, ice storms and lack of care. Stone is now collecting the baseline data that most cities lacked before embracing cooling steps. He is travelling around Louisville measuring tree cover, finding hot spots and identifying areas with vulnerable residents. The next step is to create a blueprint that combines cool roofs, green roofs, tree plantings and cool paving materials that could change the fate of the city's most at-risk residents. Stone is starting with modest but realistic assumptions in his modelling: the conversion of just 100 buildings to green roofs, for example. At the same time, the city hopes to increase its number of trees. If Louisville implements the strategies that Stone recommends, it could become a testing ground that will reveal how changes to a city's physical surface alter the urban heat island \u2014 and its pioneering programme could point the way for other cities to follow. \u201cWe're already crossing thresholds that are pretty sensitive,\u201d says Stone. \u201cCities are going to be contemplating more aggressive action. But cities can measurably slow the rate at which they're warming over a decade or two.\u201d And that's pretty quick, he adds, because even if we eliminated greenhouse-gas emissions tomorrow, \u201cwe're still going to warm for a couple of hundred years\u201d. \n                 Tweet \n               \n                     Speedy study claims climate change doubled chances of European heatwave 2015-Jul-13 \n                   \n                     Russian summer tops 'universal' heatwave index 2014-Oct-29 \n                   \n                     Heatwaves blamed on global warming 2012-Aug-07 \n                   \n                     How green is your roof? 2008-Aug-19 \n                   \n                     Human activity implicated in Europe's 2003 heat wave 2004-Dec-01 \n                   \n                     GRIT Lab \n                   \n                     Matei Georgescu \n                   \n                     Urban Climate Lab \n                   \n                     Berkeley Lab Heat Island Group \n                   Reprints and Permissions"},
{"file_id": "526021a", "url": "https://www.nature.com/articles/526021a", "year": 2015, "authors": [{"name": "Andy Extance"}], "parsed_as_year": "2006_or_before", "body": "The digital currency has caused any number of headaches for law enforcement. Now entrepreneurs and academics are scrambling to build a better version. When the digital currency Bitcoin came to life in January 2009, it was noticed by almost no one apart from the handful of programmers who followed cryptography discussion groups. Its origins were shadowy: it had been conceived the previous year by a still-mysterious person or group known only by the alias Satoshi Nakamoto 1 . And its purpose seemed quixotic:  Bitcoin was to be a 'cryptocurrency' , in which strong encryption algorithms were exploited in a new way to secure transactions. Users' identities would be shielded by pseudonyms. Records would be completely decentralized. And no one would be in charge \u2014 not governments, not banks, not even Nakamoto. Yet the idea caught on. Today, there are some 14.6 million Bitcoin units in circulation. Called bitcoins with a lowercase 'b', they have a collective market value of around US$3.4 billion. Some of this growth is attributable to criminals taking advantage of the anonymity for drug trafficking and worse. But the system is also drawing interest from financial institutions such as JP Morgan Chase, which think it could streamline their internal payment processing and cut international transaction costs. It has inspired the creation of some 700 other cryptocurrencies. And on 15 September, Bitcoin officially came of age in academia with the launch of  Ledger , the first journal dedicated to cryptocurrency research. Noah Baker investigates what the future may hold for digital currencies What fascinates academics and entrepreneurs alike is the innovation at Bitcoin's core. Known as the block chain, it serves as the official online ledger of every Bitcoin transaction, dating back to the beginning. It is also the data structure that allows those records to be updated with minimal risk of hacking or tampering \u2014 even though the block chain is copied across the entire network of computers running Bitcoin software, and the owners of those computers do not necessarily know or trust one another. Many people see this block-chain architecture as the template for a host of other applications, including self-enforcing contracts and secure systems for online voting and crowdfunding. This is the goal of Ethereum, a block-chain-based system launched in July by the non-profit Ethereum Foundation, based in Baar, Switzerland. And it is the research agenda of the Initiative for CryptoCurrencies and Contracts (IC3), an academic consortium also launched in July, and led by Cornell University in Ithaca, New York. Nicolas Courtois, a cryptographer at University College London, says that the Bitcoin block chain could be \u201cthe most important invention of the twenty-first century\u201d \u2014 if only Bitcoin were not constantly shooting itself in the foot. Several shortcomings have become apparent in Bitcoin's implementation of the block-chain idea. Security, for example, is far from perfect: there have been more than 40 known thefts and seizures of bitcoins, several incurring losses of more than $1 million apiece. Cryptocurrency firms and researchers are attacking the problem with tools such as game theory and advanced cryptographic methods. \u201cCryptocurrencies are unlike many other systems, in that extremely subtle mathematical bugs can have catastrophic consequences,\u201d says Ari Juels, co-director of IC3. \u201cAnd I think when weaknesses surface there will be a need to appeal to the academic community where the relevant expertise resides.\u201d Academic interest in cryptocurrencies and their predecessors goes back at least two decades, with much of the early work spearheaded by cryptographer David Chaum. While working at the National Research Institute for Mathematics and Computer Science in Amsterdam, the Netherlands, Chaum wanted to give buyers privacy and safety. So in 1990 he founded one of the earliest digital currencies, DigiCash, which offered users anonymity through cryptographic protocols of his own devising. DigiCash went bankrupt in 1998 \u2014 partly because it had a centralized organization akin to a traditional bank, yet never managed to fit in with the financial industry and its regulations. But aspects of its philosophy re-emerged ten years later in Nakamoto's design for Bitcoin. That design also incorporated crowdsourcing and peer-to-peer networking \u2014 both of which help to avoid centralized control. Anyone is welcome to participate: it is just a matter of going online and running the open-source Bitcoin software. Users' computers form a network in which each machine is home to one constantly updated copy of the block chain. Nakamoto's central challenge with this wide-open system was the need to make sure that no one could find a way to rewrite the ledger and spend the same bitcoins twice \u2014 in effect, stealing bitcoins. His solution was to turn the addition of new transactions to the ledger into a competition: an activity that has come to be known as mining (see 'The Bitcoin game'). Mining starts with incoming Bitcoin transactions, which are continuously broadcast to every computer on the network. These are collected by 'miners' \u2014 the groups or individuals who choose to participate \u2014 who start competing for the right to bundle transactions into a new block. The winner is the first to broadcast a 'proof of work' \u2014 a solution showing that he or she has solved an otherwise meaningless mathematical puzzle that involves encrypted data from the previous block, and lots of computerized trial and error. The winning block is broadcast through the Bitcoin network and added to the block chain, with the proof of work providing an all but unbreakable link. The block chain is currently almost 400,000 blocks long. In principle, this competition keeps the block chain secure because the puzzle is too hard for any one miner to solve every time. This means that no one will ever gain access to the encrypted links in the block chain and the ability to rewrite the ledger. Mining is also a way to steadily increase the bitcoin supply: the miner who wins each block gets a reward, currently 25 new bitcoins. That is worth almost $6,000 at today's prices. Nakamoto's design controls the supply increase by automatically adjusting the difficulty of the puzzle so that a new block is added roughly every ten minutes. In addition, the reward for creating a block decreases by half roughly every four years. The goal is to limit the supply to a maximum of 21 million bitcoins. The network cannot determine the value of bitcoins relative to standard currencies, or real-world goods and services.  That has been left to market forces , with people trading bitcoins on online exchanges. One result is that the market price has gyrated spectacularly \u2014 especially in 2013, when the asking price soared from $13 per bitcoin in January to around $1,200 in December. That would have made the first real-world products ever paid for with the cryptocurrency \u2014 a pair of Papa John's pizzas, purchased for 10,000 bitcoins on 22 May 2010 \u2014 worth almost $12 million. \n               Puzzle solutions \n             It did not take long for the problems with Bitcoin to become apparent. For example, because users are allowed to mask their identity with pseudonyms, the currency is perfect for screening criminal activity. That was behind the success of the online black market Silk Road, which the FBI shut down in 2013; its founder was sentenced to life in prison in May this year. But Bitcoin also had a key role in funding the whistle-blowing website WikiLeaks \u2014 an outcome that some would call beneficial. It is difficult for society to work out a legal framework to differentiate between good and bad uses of this technology, says Arvind Narayanan, a computer scientist at Princeton University in New Jersey. \u201cHow do you regulate around Bitcoin without banning the technology itself?\u201d he asks. Other issues surfaced with Bitcoin's mining procedure. As the currency has gained value, for example, mining competition has become fiercer, with increasingly specialized computers solving the puzzles ever faster. Courtois, who has found ways to streamline the puzzle-solving process 2 , says that at one point he was successfully earning $200 a day through mining. The rivalry has driven the establishment of large Bitcoin-mining centres in Iceland, where cooling for the computers is cheap. According to one estimate from 2014, Bitcoin miners collectively consumed as much power as the whole of Ireland 3 . \n               Working together \n             Intensified Bitcoin mining has also led individual miners to pool their computational resources. Last year, the largest mining pool, GHash.IO, briefly exceeded 50% of total Bitcoin mining power \u2014 which is problematic because anyone who controls more than half of the mining power could start beating everyone else in the race to add blocks. This would effectively give them control of the transaction ledger and allow them to spend the same bitcoins over and over again. This is not just a theoretical possibility. Successful '51% attacks' \u2014 efforts to dominate mining power \u2014 have already been mounted against smaller cryptocurrencies such as Terracoin and Coiledcoin; the latter was so badly damaged that it ceased operation. To reduce the threat from mining pools, some existing cryptocurrencies, such as Litecoin, use puzzles that call more on computer memory than on processing power \u2014 a shift that tends to make it more costly to build the kind of specialized computers that the pools favour. Another approach, developed by IC3 co-director Elaine Shi and her collaborators 4 , enlists a helpful kind of theft. \u201cWe are cryptographically ensuring that pool members can always steal the reward for themselves without being detected,\u201d explains Shi. Their supposition is that miners would not trust each other enough to form into pools if their fellow pool members could easily waltz off with the rewards without sharing. They have built a prototype of the algorithm, and are hoping to see it tested in Bitcoin and other cryptocurrencies. Another problem is the profligate amount of electricity used in Bitcoin mining. To reduce wastage, researchers including Shi and Juels have proposed a currency called Permacoin 5 . Its proof of work would require miners to create a distributed archive for  valuable data such as medical records , or the output of a gene-sequencing centre. This would not save energy, but would at least put it to better use. The security of cryptocurrencies is another huge concern. The many thefts of bitcoins do not result from the block-chain structure, says Narayanan, but from Bitcoin's use of standard digital-signature technology. In digital signatures, he explains, people have two numeric keys: a public one that they give to others as an address to send money to, and a private one that they use to approve transactions. But the security of that private key is only as good as the security of the machine that stores it, he says. \u201cIf somebody hacks your computer, for example, and steals your private keys, then essentially all of your bitcoins are lost.\u201d Security is such a concern for consumers that Narayanan thinks Bitcoin is unlikely to find widespread use. So his team is working on a better security scheme that splits private keys across several different devices, such as an individual's desktop computer and smartphone, and requires a certain proportion of the fragments to approve a payment 6 . \u201cNeither reveals their share of the key to each other,\u201d says Narayanan. \u201cIf one machine gets hacked, you're still OK because the hacker would need to hack the others to steal your private key. You'll hopefully notice the hack happened before they have the chance.\u201d Other thefts have occurred because the private key needs to be combined with a random number to create a transaction signature. Some software \u2014 such as Bitcoin apps developed for Android smartphones \u2014 has generated random numbers improperly, making them easier to guess. This has allowed hackers to steal somewhere between several thousand and several million dollars' worth of bitcoins, says Courtois, who has been investigating such vulnerabilities 7 . \u201cIt's embarrassing,\u201d admits David Schwartz, chief cryptographer at cryptocurrency developer Ripple Labs in San Francisco, California. \u201cWe as an industry just seem to keep screwing up.\u201d \n               Into the ether \n             The block chain is a remarkably powerful idea that could be applied to much more than just transaction records, says Gavin Wood, co-founder of Ethereum and chief technology officer of its foundation. One use might be to develop computerized, self-enforcing contracts that make a payment automatically when a task is complete. Others might include voting systems, crowdfunding platforms, and even other cryptocurrencies. Wood says that Ethereum is best used in situations for which central control is a weakness \u2014 for example, when users do not necessarily trust one another. In 2014, to make it easier to develop such applications, Wood and fellow programmer Vitalik Buterin devised a way to combine the block chain with a programming language. Ethereum raised 30,000 bitcoins through crowdfunding to commercialize this system. To prevent the basic cryptography-related mistakes that have plagued Bitcoin, Ethereum has recruited academic experts to audit its protocol. Shi and Juels are looking for ways that Ethereum could be abused by criminals 8 . \u201cThe technology itself is morally neutral, but we should figure out how to shape it so that it can support policies designed to limit the amount of harm it can do,\u201d says Juels. Like Bitcoin, Ethereum is not under anyone's direct control, so it operates outside national laws, says Wood. However, he adds that technologies such as music taping and the Internet were also considered extralegal at first, and seemed threatening to the status quo. How Bitcoin, Ethereum and their successors sit legally is therefore \u201csomething that, as a culture and society, we're going to have to come together to deal with\u201d, he says. Juels suspects that Bitcoin, at least, will not last as an independent, decentralized entity. He points out how music streaming has moved from the decentralized model of peer-to-peer file-sharing service Napster to commercial operations such as Spotify and Apple Music. \u201cOne could imagine a similar trajectory for cryptocurrencies: when banks see they're successful, they'll want to create their own,\u201d he says. Courtois disagrees. He calls Bitcoin \u201cthe Microsoft of cryptocurrency\u201d, and maintains that its size and dominance mean that it is here to stay. As soon as any new innovations come along, he suggests, Bitcoin can adopt them and retain its leading position. Whatever the future holds for Bitcoin, Narayanan emphasizes that the community of developers and academics behind it is unique. \u201cIt's a remarkable body of knowledge, and we're going to be teaching this in computer science classes in 20 years, I'm certain of that.\u201d \n                 Tweet \n                 Follow @NatureNews \n               \n                     Unpatients\u2014why patients should own their medical data 2015-Sep-08 \n                   \n                     BitCoin meets Google Trends and Wikipedia: Quantifying the relationship between phenomena of the Internet era 2013-Dec-04 \n                   \n                     Currency without Borders 2011-Nov-15 \n                   \n                     Currency without Borders 2011-Nov-15 \n                   \n                     Bitcoin \n                   \n                     Coindesk's Bitcoin price index \n                   \n                     Ripple \n                   Reprints and Permissions"},
{"file_id": "524280a", "url": "https://www.nature.com/articles/524280a", "year": 2015, "authors": [{"name": "Daniel Cressey"}], "parsed_as_year": "2006_or_before", "body": "As marijuana use becomes more acceptable, researchers are scrambling to answer key questions about the drug. In 2013, Beau Kilmer took on a pretty audacious head count. Citizens in the state of Washington had just voted to legalize marijuana for recreational use, and the state's liquor control board, which would regulate the nascent industry, was anxious to understand how many people were using the drug \u2014 and importantly, how much they were consuming. The task was never going to be straightforward. Users of an illicit substance, particularly heavy users, often under-report the amounts they take. So Kilmer, co-director of the RAND Drug Policy Research Center in Santa Monica, California, led a team to develop a web-based survey that would ask people how often they had used cannabis in the past month and year. To help them gauge the amounts, the surveys included scaled pictures showing different quantities of weed. The survey, along with other data the team had collected, revealed a rift between perception and reality. Based on prior data, state officials had estimated use at about 85 tonnes per year; Kilmer's research suggested that it was actually double that, about 175 tonnes 1 . The take-home message, says Kilmer, was \u201cwe're going to have to start collecting more data\u201d. Scientists around the world would echo that statement. Laws designed to legalize cannabis or lessen the penalties associated with it are taking effect around the world. They are sweeping the sale of the drug out of stairwells and shady alleys and into modern shopfronts under full view of the authorities. In 2013, Uruguay became the first nation to legalize marijuana trade. And several countries in Europe \u2014 Spain and Italy among them \u2014 have moved away from tough penalties for use and possession.  Thirty-nine US states plus Washington DC  have at least some provisions for medicinal use of the drug. Washington, Colorado, Alaska and Oregon have gone further, legalizing the drug for recreational consumption. A handful of other states including California and Massachusetts are expected to vote on similar recreational-use measures by the end of 2016. But the rapid shift has caught researchers on the back foot. \u201cBroadly speaking, there's about 100 times as many studies on tobacco or alcohol as there are on illegal substances,\u201d says Christian Hopfer, a psychiatry researcher at the University of Colorado School of Medicine in Aurora. \u201cI don't think it's the priority it should be.\u201d Despite claims that range from its being a treatment for seizures to a cause of schizophrenia, the evidence for marijuana's effects on health and behaviour is limited and at times conflicting. Researchers struggle to answer even the most basic questions about cannabis use, its risks, its benefits and the effect that legalization will have. The quick shifts in policies should provide a plethora of natural experiments, but the window will not be open for long. \u201cThere's an opportunity here. Some of the most informative research we can do is right at the moment the market changes,\u201d says Robert MacCoun, a social psychologist and public-policy researcher at Stanford Law School in California who worked with Kilmer on the research done in Washington. \n               What are the negative effects? \n             For years, the debate over the drug's safety has been polarized. Those seeking legalization claim that it is basically harmless. Yet governments around the world have placed cannabis among the most-dangerous illegal drugs, running hard-hitting campaigns warning of the threats that it poses to mental health and social well-being. Scientists are fairly sure about some things, particularly when it comes to the short-term effects. They know, for instance, that it impairs memory and coordination, and can cause paranoia and psychosis 2 . These are some of the classic symptoms of being 'high' and can have major health effects in and of themselves. Studies have found, for example, that drivers are between twice and seven times as likely to crash if they have recently smoked the drug 3 , 4 . In the longer term, effects are less clear, but there are a few that most scientists agree on. Contrary to many popular arguments, there is evidence that cannabis is addictive. Around 9% of users become dependent on the drug, showing signs of addiction such as developing tolerance or experiencing withdrawal symptoms when they stop using. Beyond that, however, long-term effects have been difficult to pin down. Cannabis is often smoked, and this can raise the risk of respiratory problems and possibly lung cancer. A 2008 study in New Zealand found that smoking pot increased the risk of lung cancer by 8% for each 'joint-year' (the equivalent of smoking a joint per day for one year), even after taking tobacco use into account 5 . But other studies have found little to no correlation with lung cancer, even for heavy users 6 . Other health outcomes are even more difficult to disentangle from confounding factors. Some researchers have found links to poor educational performance, low social attainment \u2014 such as job status \u2014 and altered brain development. For example, the Christchurch Health and Development Study \u2014 which followed almost 1,300 children born in New Zealand in 1977 \u2014 found that people who used cannabis daily are around 50% more likely to have psychotic symptoms 7  than are non-users and are at greater risk of not finishing school 8 . And another study from New Zealand, which followed 1,000 people in Dunedin from birth to age 38, shows that persistent cannabis use, especially if started young, correlates with steeper  declines in IQ  in later life and with problems with memory and reasoning compared with people who have never used the drug 9 . Hall says that the association with negative social and mental-health outcomes has been consistently observed, but the debate \u201cis how we explain that association\u201d, which he says will probably involve a combination of factors. The difficulty, says Valerie Curran, a psychopharmacologist at University College London, lies in teasing apart correlation and causation, because \u201cthere are so many confounders\u201d. For example, adolescents who use cannabis are probably also drinking excessive amounts of alcohol and engaging in other risky activities. Attributing the effects to one particular substance or behaviour is therefore very difficult. Similar problems abound in the hotly contested link between cannabis and schizophrenia. Multiple studies have shown an increased risk of this mental-health disorder in people who use cannabis versus people who do not. A study of 50,000 Swedish men aged 18\u201320 found that heavy users were around three times more likely to develop schizophrenia than those who had never used the drug 10 . Although the increase in risk was significant, the overall risk is still low \u2014 just 1.4% of men who reported using cannabis developed the disorder, compared with 0.6% of those who said they had never tried the drug. Some cannabis advocates suggest that the link may be down to people with such problems 'self-medicating', but this is difficult to prove. Many of the negative health outcomes seem to be exacerbated if the drug is used in adolescence, leading to suggestions that cannabis is adversely affecting developing brains. And effects may also be linked to the drug's potency, which in itself is hard to pin down. As cannabis use becomes legal, the data may become easier to collect. But the drug's use is still low compared with alcohol and tobacco, says Wayne Hall, an addiction researcher at the University of Queensland in Brisbane, Australia, so it is hard to draw firm conclusions. Marijuana may be the most popular illegal drug, he says \u2014 about 44% of US adults have used it at some point in their lives according to one source \u2014 but only about one in ten have used it in the past year. By contrast, around 70% drank alcohol in that time. \u201cThe number of people who use it with any regularity for a long time is pretty small. The longer-term consequences are really understudied,\u201d says Hall. \n               How strong is it? \n             A major question for researchers \u2014 and a complication in interpreting the evidence \u2014 is dosing. There are more than 85 cannabinoid chemicals in pot. The one of most interest to researchers \u2014 and users \u2014 is tetrahydrocannabinol (THC). Growers have been able to breed high concentrations of the chemical into strains of the plant meant for recreational and medicinal use. A potency- monitoring programme run by the University of Mississippi for the US National Institute on Drug Abuse (NIDA) found that THC levels have steadily increased in the United States 11 , from 2\u20133% in 1985\u201395 to 4.9% in 2010. The increase is even starker for imported cannabis seized by law-enforcement officials. For these drugs, potency has gone from less than 4% in the late 1980s and early 1990s to more than 12% in 2013. But it is hard to determine the amounts of THC being consumed by the average customer. It is unclear, for example, whether users 'titrate' their doses, adjusting their intake according to the potency. Nicotine users are known to do this with cigarettes, but nicotine does not impair judgement in the same way that cannabis does. And the effects of THC are less immediate, especially for edible forms. The escalating potency raises questions about previous research because users in older studies may have been consuming lower-potency cannabis, and the effects may be different (see 'Research gaps'). A study published earlier this year, for example, linked high-potency cannabis to a threefold-increased risk of psychosis versus non-use but found no association with lower-potency forms 12 . And many researchers have complained that the pot approved for study in experiments funded by NIDA is a poor match for what is used recreationally or medicinally. In tandem with changing laws, the Colorado Department of Public Health and Environment (CDPHE) is establishing reference labs to check the potency of what is sold. And the US government is  expanding the varieties of marijuana that researchers with federal funding can obtain . In places where the drug is legal, existing labelling standards may also be inadequate. A survey done between August and October last year found that only 17% of edible cannabis products in San Francisco, Los Angeles and Seattle had accurate labels. More than half had less THC than claimed, and some contained significantly more 13 . \u201cA lot of people get a rude surprise,\u201d says MacCoun. \n               Are there medical benefits? \n             Although states are starting to ease restrictions on recreational use of marijuana, what got the ball rolling in changing public perceptions and the legal landscape for pot were the arguments for its medical use. Colorado introduced its rules allowing medical marijuana more than a decade before it allowed recreational use. The amendment to the state's constitution listed eight conditions for which marijuana was approved: cancer, glaucoma, HIV/AIDS, cachexia (a progressive wasting syndrome), persistent muscle spasms, seizures, severe nausea and severe pain. But, says Larry Wolk, executive director and chief medical officer of the CDPHE, \u201cthose are dictated by the constitution and not necessarily by medical research\u201d. Although there is a huge amount of anecdotal evidence \u2014 and well-organized advocacy groups that campaign for easier access to medical marijuana \u2014 there is little conclusive scientific evidence for many of the claimed medical benefits. One of the reasons for this dearth of evidence is that money generally has been obtainable only for research on the negative effects of cannabis. That is beginning to change. When Colorado first legalized the drug, its public-health department began collecting fees from patients who applied to purchase pot at medical dispensaries. By 2014, the state had amassed more than US$9 million, most of which was ploughed back into a medical marijuana research programme selected by the CDPHE. Among the projects funded by the Colorado millions, there are two investigating whether cannabinoids can help to mitigate seizures in childhood epilepsy. Similar research is being pursued in the United Kingdom and elsewhere in the United States. Another, more-established use is for people with multiple sclerosis. A cannabis-based spray has been approved in 27 countries for treatment of muscle problems associated with the disease, such as spasms. Other claimed benefits of marijuana, such as  boosting appetite  in people with AIDS, are supported by more-limited evidence. If positive effects can be clearly demonstrated, it would be a huge vindication for marijuana advocates. It might also go some way towards justifying medical-marijuana legislation. In the meantime, however, scientists are watching the emerging cannabis frontier with wary eyes. \u201cI think it's an experiment,\u201d says Robert Booth, a psychiatry researcher at the University of Colorado. \u201cWhen this study is all said and done, we'll know a whole lot about the effects of marijuana.\u201d \n               What happens when you make it legal? \n             One of the biggest questions is how legalization will change usage patterns. One place in which researchers are looking for answers is Europe, where cannabis regulation tends to be much lighter than it is in the United States (see 'Reefer madness'). In the United Kingdom, some police forces overlook cannabis use and small-scale growing operations. Spain allows private consumption, but still has restrictions on sales. The most extreme and long-standing example is the Netherlands, which decriminalized the possession and sale of small quantities of cannabis in 1976. But although some streets of Amsterdam have been transformed into pungent tourism hotspots, the country as a whole has not changed its habits much. Although hard data on cannabis use in Europe is patchy, the Netherlands does not have hugely more users than other nations. Data aggregated by the United Nations Office on Drugs and Crime put use in the Netherlands at about 7%. That is more than in Germany (5%) and Norway (5%), about the same as in the United Kingdom and less than in the United States (15%). Nor has the Netherlands seen a huge spike in use of harder drugs, dampening fears that marijuana serves as a gateway to more-dangerous substances such as heroin and cocaine. The message from the Netherlands, says Franz Trautmann, a drugs-policy researcher at the Trimbos Institute in Utrecht, the Netherlands, is that \u201ca very liberal policy doesn't lead to a skyrocketing prevalence\u201d. Rather, cannabis is endemic, he says. \u201cWe can't control this through prohibition. This is something which more and more is recognized.\u201d But the lesson from the Netherlands may be limited because the drug is still illegal, and growing and selling large quantities is still punishable by law. Colorado has gone further by legalizing not merely the drug's use, but the whole production chain, and that could have fundamentally different effects on the economics of pot. \u201cLegalized production really raises the prospect of a dramatic drop in price,\u201d says MacCoun. \u201cIt's conceivable marijuana prices could drop 75\u201380% in a fully legalized model.\u201d (Although Uruguay legalized the drug in 2013, it reportedly has struggled to regulate production and to set up working dispensaries.) The effects of a sharp drop in cost are unknown. Taxation may also have unintended consequences. If states tax by weight, users might look to higher-potency strains to save money. And once cannabis is a business, it gains a business lobby. Cannabis researchers already talk of being bombarded with e-mails from pro-cannabis groups if they make negative comments about the drug. \u201cMarijuana research is like tobacco research in the '60s,\u201d says Hopfer. \u201cAny study about harms is challenged. It's really something.\u201d Many fear that the big money now to be found in cannabis will drive attempts to obfuscate the risks. \u201cIf the commercial interests are too big, then the profit interest is prevailing above the health interest. This is what I'm afraid of,\u201d says Trautmann. Legalization provides an opportunity to answer some important questions. In a few years, Colorado, Washington and others will know (if only roughly) how legalization affects usage patterns, the number of car crashes and the number of people seeking help for drug dependency. The CDPHE-funded programmes will have added to the knowledge of beneficial effects. And continuing long-term studies of large groups of users will provide more evidence for statisticians who are attempting to disentangle correlation and causation on the negative impacts. \u201cWhen a jurisdiction changes its marijuana laws, that provides an opportunity for greater leverage on the questions of cause and effect,\u201d says MacCoun. But, he adds, the signals will only really be clear if the laws result in a dramatic increase in use \u2014 something that is neither a given, nor necessarily desirable. \u201cObviously, we don't want marijuana use to rise just to allow us to answer our questions, but if it does, we'll be poring over all the data.\u201d \n                 Tweet \n               \n                     Marijuana gears up for production high in US labs 2015-Mar-17 \n                   \n                     Marijuana flips appetite switch in brain 2015-Feb-18 \n                   \n                     E-cigarettes: The lingering questions 2014-Aug-26 \n                   \n                     How marijuana makes you forget 2012-Mar-01 \n                   \n                     Key ingredient staves off marijuana memory loss 2010-Oct-01 \n                   \n                     Cannabis study shows small MS benefit 2003-Nov-07 \n                   \n                     Blog post: Weed sequenced. No really \u2014 weed \n                   Reprints and Permissions"},
{"file_id": "524150a", "url": "https://www.nature.com/articles/524150a", "year": 2015, "authors": [{"name": "Jeff Tollefson"}], "parsed_as_year": "2006_or_before", "body": "A new generation of economists is trying to transform global development policy through the power of randomized controlled trials. In 70 local health clinics run by the Indian state of Haryana, the parents of a child who starts the standard series of vaccinations can walk away with a free kilogram of sugar. And if the parents make sure that the child finishes the injections, they also get to take home a free litre of cooking oil. These simple gifts are part of massive trial testing whether rewards can boost the stubbornly low immunization rates for poor children in the region. Following the model of the randomized controlled trials (RCTs) that are commonly used to test the effectiveness of drugs, scientists randomly assigned clinics in the seven districts with the lowest immunization rates to either give the gifts or not. Initial results are expected next year. But smaller-scale experiments suggest that the incentives have a good chance of working. In a pilot study conducted in India and published in 2010, the establishment of monthly medical camps saw vaccination rates triple, and adding on incentives that offered families a kilogram of lentils and a set of plates increased completion rates by more than sixfold 1 . \u201cWe have learned something about why immunization rates are low,\u201d says Esther Duflo, an economist at the Massachusetts Institute of Technology (MIT) in Cambridge, who was involved in the 2010 experiment and is working with Haryana on its latest venture. The problem is not necessarily that people are opposed to immunization, she says. It is that certain obstacles, such as lack of time or money, are making it difficult for them to attend the clinics. \u201cAnd you can balance that difficulty with a little incentive,\u201d she says. This is one of a flood of insights from researchers who are revolutionizing the field of economics with experiments designed to rigorously test how well social programmes work. Their targets range from education programmes to the prevention of traffic accidents. Their preferred method is the randomized trial. And so they have come to be known as the 'randomistas'. The randomistas have been particularly welcomed in the global development arena. Despite some US$16 trillion in aid having flowed to the developing world since the Second World War, there are little empirical data on whether that money improves the recipients' lives (see  page 144 ). The randomistas see their experiments as a way to generate such data and to give governments tools to promote development, relieve poverty and focus money on things that work. Not everyone is convinced. Sceptics argue that the randomistas' focus on evaluating specific aid programmes can lead them to lose sight of things such as energy, infrastructure, trade and corruption \u2014 macroeconomic issues that are central to a country's ability to prosper, but that are effectively impossible to randomize. \u201cDevelopment is ultimately about politics,\u201d says Angus Deaton, an economist at Princeton University in New Jersey. Nonetheless, the randomista movement is gaining momentum (see 'Scale the heights'). Universities are pumping out more economics graduate students with experience in RCTs every year. Organizations ranging from the UK Department for International Development to the Bill & Melinda Gates Foundation in Seattle, Washington, are throwing their financial support behind the technique. \u201cThere are hundreds and hundreds of randomized trials going on, and ten years ago that just wasn't the case,\u201d says economist Dean Karlan at Yale University in New Haven, Connecticut, who is at the forefront of the movement. \u201cWe've changed the conversation.\u201d Demand is only rising. This September, governments will gather in New York under the auspices of the United Nations to approve a new set of Sustainable Development Goals, which are intended to guide investments over the coming decade. And in December, questions about financial aid will be high on the agenda at the UN climate summit in Paris, where governments expect to sign a new climate agreement that will probably include commitments by industrialized nations to funnel money into sustainable development in poorer countries. In both cases, the effectiveness of the programmes is likely to be a key concern. \u201cThis is front and centre on a lot of people's agenda,\u201d says Ann Mei Chang, who is executive director of the Global Development Lab at the US Agency for International Development (USAID) in Washington DC. \u201cWhere do we get the biggest bang for our buck?\u201d \n               Progress and opportunities \n             RCTs have been used to test the effectiveness of social programmes at least since the 1960s. But the modern era began in 1997, when one of the most famous and influential RCTs in public policy began in Mexico. The experiment had its origins three years earlier, when Mexican President Ernesto Zedillo assumed office in the middle of an economic crisis and assigned economist Santiago Levy to devise a programme to help poor people. Sceptical of the conventional approach \u2014 subsidies for products such as tortillas and energy \u2014 Levy designed a system that would provide cash payments to poor families if they met certain requirements, such as visiting health clinics and keeping their children in school. \u201cAnd because people were very critical about what I was doing,\u201d says Levy, who now leads strategic development planning at the Inter-American Development Bank in Washington DC, \u201cI wanted to ensure that we had numbers so that we could have an informed debate.\u201d As it happened, Levy had a natural control group for his experiment. The government was rolling out its payment programme in stages, so he could collect data on families in villages that were included in the initial roll-out, and in comparable villages that were not. Within a few years, his team had data suggesting that the programme, dubbed  PROGRESA , was working remarkably well. Visitation to health clinics was 60% higher in participating communities than in the control group. Children in those communities also had a 23% reduction in illness and an 18% reduction in anaemia. Overnight hospital visits halved across several age ranges. These data helped to solidify support for the programme. Now known as  Prospera , it covers almost all of Mexico's poorest citizens and has inspired similar initiatives across Latin America and into Africa. \u201c PROGRESA  was one of the first major national programmes of its kind to get a rigorous evaluation,\u201d says William Savedoff, who works on aid effectiveness and health policy at the Center for Global Development, a think tank in Washington DC. \u201cToday conditional cash-transfer programmes are some of the most heavily evaluated programmes in the world, and that is I think a direct consequence of the Mexican experience.\u201d The idea of developing hard evidence to test public policies was bubbling up in parallel in the United States. One of the first trials began in 1994 with a small initiative to analyse the effect of supplying textbooks and uniforms as well as basic classroom improvements to a group of schools in Kenya. Economist Michael Kremer at Harvard University in Cambridge had taught in Kenya years earlier. A friend of his who worked for a non-profit group was initiating the programme, and Kremer suggested that the group roll it out as an experiment. \u201cI didn't necessarily expect anything to come of this,\u201d he says. Working with the group, Kremer collected data on students in 14 schools, half of which received the intervention. School attendance increased, but test scores did not. Similar results came from an experiment in 1995 that involved 100 schools. That trial suggested that providing textbooks had little effect on average test scores 2 , owing perhaps to language challenges \u2014 the textbooks were in English, which was not the native language for many students. Students who were already scoring higher than their peers, however, pulled further ahead if they had the books. Kremer continued to run RCTs of other programmes, but it was Duflo \u2014 then a student of his \u2014 who pushed the idea into the mainstream. Duflo's 1999 dissertation looked in part at an education initiative in Indonesia that had built 61,000 primary schools over 6 years in the 1970s. She wanted to test a common concern that such a rapid expansion would lead to a decline in the quality of education, thereby offsetting any gains. Running an experiment was impossible, but Duflo was able to use data on the differences across regions to show that the programme had, in fact, increased educational opportunities as well as wages. This and other early work inspired Duflo to look at RCTs as a way to generate data and definitively measure the effectiveness of policies and programmes. \u201cAs soon as I had a longer time horizon and some money I started working on setting some up,\u201d she says. One of Duflo's early papers 3 , published in 2004, capitalized on a 1993 amendment to India's constitution that devolved more power over public investments to local councils and reserved the leadership of one-third of those councils, to be chosen at random, for women. Duflo realized that this effectively created a RCT that could test the effect of having women-led councils. In analysing the data, she found that councils led by women boosted political engagement by other women and directed investment towards issues raised by them. In some areas, women are in charge of obtaining drinking water, for instance, and councils led by women typically invested more in water infrastructure than did those run by men. \u201cThe scale of the policy and the topic were at the time unusual,\u201d Duflo says. \u201cIt gave me a sense of the range of things that the tool could possibly cover.\u201d By the early 2000s, the randomistas were on the upswing. In 2002, Karlan, one of Duflo's students, joined with her and other researchers to form Development Innovations \u2014 now known as Innovations for Poverty Action \u2014 in New Haven. The following year, Duflo co-founded what is now known as the Abdul Latif Jameel Poverty Action Lab (J-PAL) in Cambridge with fellow MIT economists Abhijit Banerjee and Sendhil Mullainathan. The work quickly expanded, and J-PAL has now run nearly 600 evaluations in 62 countries, and trained more than 6,600 people. One of Duflo's latest projects will revisit her dissertation on education in Indonesia, only this time with secondary schools and randomized control groups. \u201cWe will have a randomized version of a paper on the benefits to education soon I hope,\u201d Duflo says. \n               Venture capital \n             One enthusiastic convert to the randomista philosophy is Rajiv Shah, a Gates Foundation official who became head of USAID in 2010. Once there he created a fund called Development Innovation Ventures (DIV) to test and scale up solutions to development problems, and he enlisted Kremer as its scientific director. The goal, Shah said, was to \u201cmove development into a new realm\u201d through the use of evidence. Since then DIV has invested in more than 100 development projects, and nearly half involve RCTs. One, conducted in Kenya by a pair of researchers from Georgetown University in Washington DC, tested a simple method for reducing traffic accidents that involve minibuses \u2014 collisions that Kremer calls major and increasing killers. \u201cTwo of them crash into each other, and 40 people die,\u201d he says. In 2008, the researchers worked with more than 1,000 drivers to place stickers on buses that urged passengers to speak up about reckless driving 4 . They then collected information from four major insurance companies and found that claims for serious accidents had dropped by 50% on buses with stickers compared with those without. DIV provided a grant to conduct a larger trial \u2014 which found that claims dropped by 25\u201333% \u2014 and a second grant of nearly $3 million to help to scale up the project throughout Kenya. \u201cThe really big win is when developing countries, or firms or NGOs [non-governmental organizations] change their policies,\u201d Kremer says. But one question now facing DIV is whether such a strategy \u2014 or indeed any project that proves effective in one setting \u2014 can be repackaged and deployed in other countries, where different cultural factors are at play (see   Nature   523 , 516\u2013518; 2015). \n               Scale up \n             Effecting policy change is the precise aim of the Global Innovation Fund, which was launched in September 2014 with $200 million over 5 years from the UK Department for International Development, USAID and others, and which follows the DIV model of rigorous testing. Interim director Jeffrey Brown, who is on loan from USAID, says that the fund has already received more than 1,800 applications for projects in 110 different countries and will be announcing its first suite of grants later this year. \u201cWe are essentially trying to become a bridge over the valley of death for good development ideas,\u201d he says. But such organizations still provide only a tiny fraction of the billions of dollars that are spent each year on development aid, let alone the trillions of dollars that are spent by governments on domestic social programmes. Even at lending institutions that have taken this evidence-based framework on board, the portion of investments that is covered by rigorous evaluations is small. At the World Bank, which started a Development Impact Evaluation division in 2005, the number of projects receiving formal impact evaluations \u2014 through RCTs or other means \u2014 rose from fewer than 20 in 2003 to 193 in 2014, mostly covering things such as agriculture, health and education. But that still represents just 15% of the bank's projects, says evaluation-division head Arianna Legovini, who leads a team of 23 full-time staff and has an annual budget of roughly $18 million. Although many of these evaluations more than pay for themselves over the long term, one constraint is the up-front cost: the average price of an impact evaluation is around $500,000. \u201cIf I did not have donor funding,\u201d she says, \u201cthese studies just would not happen.\u201d The World Bank is trying to make the most of its resources by working directly with developing countries on implementation. More than 3,000 people have attended its workshops and training sessions since 2005, most of whom were government officials in developing countries that are receiving funds from the bank. The bank is also making efforts to assess the impact-evaluation programme itself \u2014 although the analysis is based largely on whether payments for projects are made on time as a proxy for implementation of the initiatives. An analysis by Legovini and two of her team suggests that development projects that undergo a formal impact analysis are more likely to be implemented on time than are those that do not have evaluations, probably because of the extra attention that is given to initial set-up, roll-out and monitoring 5 . This finding is good news for individual projects, but it is also a potential thorn in the side of many RCTs. Positive effects seen in a trial setting may disappear when the programme is scaled up, governments take over and all the extra attention disappears (see   Nature   523 , 146\u2013148; 2015). \u201cThe fad now is let's pilot it, and if it works we'll take it to scale,\u201d says Annette Brown, who heads the Washington DC office of the International Initiative for Impact Evaluation, an organization that funds impact evaluations as well as meta-analyses of existing studies. Brown says that researchers and governments should probably conduct rigorous studies when any programme is scaled up to ensure that the results continue to hold true \u2014 just as the government in Haryana is doing now. \n               Randomization bias \n             From a political perspective, the strongest argument in favour of well constructed RCTs \u2014 that they do not lie \u2014 may also be the biggest factor working against them. Local politicians often want to cut ribbons and release money into communities, whereas international donors, including governments and NGOs, want flagship programmes that show how they are improving the world. They do not welcome results showing that initiatives are not working. Even in Mexico, Levy says, some of the subsidies that he fought against when he created  PROGRESA  have regained political favour. But the randomistas have been accused of succumbing to their own biases. Some fear that their insistence on the RCT has skewed research towards smaller policy questions and given short-shrift to larger, macroeconomic questions. One example comes from Martin Ravallion. An economist at Georgetown University and a former research director at the World Bank, he cites an antipoverty programme in China that received $464 million from the bank in the 1990s. Although the programme involved road construction, housing, education, health and even conditional cash payments for poor families, a study based on data collected in 2005, 4 years after disbursement ended, found minimal average impact on citizens 6 . \u201cThat was the only long-term study of integrated rural development, which is the most common form of development assistance,\u201d Ravallion says. Yet some families did benefit, and by combining statistics with economic modelling, he and his team showed that the difference lay in basic issues, such as education level. For Ravallion, the message is that aid is best targeted at the literate poor, or more broadly at issues such as literacy. \u201cGovernments need to know these things,\u201d he says. \u201cThey can't just know about the subset of things that are amenable to randomization.\u201d To Alexis Diamond, a former student of Duflo's who manages project evaluations at the International Finance Corporation, the private-sector development arm of the World Bank in Washington DC, the debate between the randomistas and the old-guard economists is in many ways about status and clout. The latter have spent their careers delving into ever more complex and abstract models, he says. And then \u201cthe randomistas came along and said 'We don't care about any of that. This is about who has a seat at the table'.\u201d Diamond says that he tries to strike a balance at his organization, where most evaluations still rely on a mixture of quantitative and qualitative data, including expert judgement. Duflo shrugs off the debate and says that she is merely trying to provide government officials with the information \u2014 and tools \u2014 that they need to help them spend their money more wisely. \u201cThe best use of international aid money should be to generate evidence and lessons for national governments,\u201d she says. She points to a anti-pollution programme in industrial plants in the Indian state of Gujarat. Partnering with a group of US researchers, the state ran an experiment in 2009 that divided nearly 500 plants into 2 groups. Those in the control group continued with the conventional system, in which industries hire their own auditors to check compliance with pollution regulations. The others tested a scheme in which independent auditors were paid a fixed price from a common pool. The hope was that this would eliminate auditors' fear of being black-balled for filing honest reports. And it did: independent auditors were 80% less likely to falsely give plants a passing grade, and many of the industrial plants covered by those audits responded by curbing their pollution. In January, regulators rolled out the programme across the state. \u201cMy hope, in a best-case scenario, is that in the next ten years you are going to have many, many of these projects run as a matter of course by governments in the spaces where they want to learn,\u201d Duflo says. \n                 Tweet \n                 Follow @NatureNews \n               See Editorial  page 135 \n                     Science in the community 2015-Aug-12 \n                   \n                     Millennium Villages Project launches retrospective analysis 2015-Aug-12 \n                   \n                     Policy: Development goals should enable decision-making 2015-Jul-08 \n                   \n                     Aid burst lifts people out of extreme poverty 2015-May-14 \n                   \n                     Poverty project opens to scrutiny 2012-Jun-12 \n                   \n                     Development: Harvest of hope 2006-Jul-05 \n                   \n                     Poverty Action Lab \n                   \n                     Innovations for Poverty Action \n                   \n                     World Bank's Development Impact Evaluation \n                   Reprints and Permissions"},
{"file_id": "523272a", "url": "https://www.nature.com/articles/523272a", "year": 2015, "authors": [{"name": "M. Mitchell Waldrop"}], "parsed_as_year": "2006_or_before", "body": "Active problem-solving confers a deeper understanding of science than does a standard lecture. But some university lecturers are reluctant to change tack. Outbreak alert: six students at the Chicago State Polytechnic University in Illinois have been hospitalized with severe vomiting, diarrhoea and stomach pain, as well as wheezing and difficulty in breathing. Some are in a critical condition. And the university's health centre is fielding dozens of calls from students with similar symptoms. This was the scenario that 17 third- and fourth-year undergraduates dealt with as part of an innovative virology course led by biologist Tammy Tobin at Susquehanna University in Selinsgrove, Pennsylvania. The students took on the role of federal public-health officials, and were tasked with identifying the pathogen, tracking how it spreads and figuring out how to contain and treat it \u2014 all by the end of the semester. Although the Chicago school and the cases were fictitious, says Tobin, \u201cwe tried to make it as real as possible\u201d. If students decided to run a blood test or genetic assay, Tobin would give them results consistent with enterovirus D68, a real respiratory virus. (To keep the students from just getting the answer from the Internet, she portrayed the virus as an emergent strain with previously unreported symptoms.) If they decided to send a team to Chicago, Tobin would make them look at real flight schedules and confirm that there were enough seats. In the end, the students pinpointed the virus, but they also made mistakes: six people died, for example, in part because the students did not pay enough attention to treatment. However, says Tobin, \u201cthat doesn't affect their grade so long as they present what they did, how it worked or didn't work, and how they'd do it differently\u201d. What matters is that the students got totally wrapped up in the problem, remembered what they learned and got a handle on a range of disciplines. \u201cWe looked at the intersection of politics, sociology, biology, even some economics,\u201d she says. Tobin's approach is just one of a diverse range of methods that have been sweeping through the world's undergraduate science classes. Some are complex, immersive exercises similar to Tobin's. But there are also team-based exercises on smaller problems, as well as simple, carefully tailored questions that students in a crowded lecture hall might respond to through hand-held 'clicker' devices. What the methods share is an outcome confirmed in hundreds of empirical studies: students gain a much deeper understanding of science when they actively grapple with questions than when they passively listen to answers. \u201cWe find up to 20% better grades over usual methods,\u201d says Tom Duff, a computer scientist who developed a team-based learning approach at the University of the West of Scotland in Paisley, UK. Other active-learning proponents have found similar gains. Last year, a group led by biologist Scott Freeman at the University of Washington in Seattle published an analysis of 225 studies of active learning in science, technology, engineering and mathematics (STEM) and found that active learning cut course failure rates by around one-third 1 . \u201cAt this point it is unethical to teach any other way,\u201d declares Clarissa Dirks, a microbiologist at the Evergreen State College in Olympia, Washington, and co-chair of the US National Academies Scientific Teaching Alliance, an initiative to reform undergraduate STEM education. Active learning is winning support from university administrators, who are facing demands for accountability: students and parents want to know why they should pay soaring tuition rates when so many lectures are now freely available online. It has also earned the attention of foundations, funding agencies and scientific societies, which see it as a way to patch the leaky pipeline for science students. In the United States, which keeps the most detailed statistics on this phenomenon, about 60% of students who enrol in a STEM field switch to a non-STEM field or drop out 2  (see 'A persistence problem'). That figure is roughly 80% for those from minority groups and for women. \n               Tough sell \n             Not everyone embraces the idea. Active learning can be a tough sell to faculty members who thrived on standard lectures during their own student years, and who wonder whether the benefits of active learning \u2014 which requires substantially more preparation than do standard lectures \u2014 could possibility justify the time that the approach would take away from their research. Understanding and addressing the resistance has become one of the reformers' prime concerns. Robert Lue, the other co-chair of the teaching alliance and director of the Derek Bok Center for Teaching and Learning at Harvard University in Cambridge, Massachusetts, says that he is \u201chell bent on erasing this sense that research is where you apply your intellect, and teaching is a rote skill\u201d. Scientists need to approach teaching with the same rigour and appreciation for evidence that they exercise in the laboratory, he says. \u201cIt's at the frontier of research. And the more people we get involved, the faster that research will go.\u201d On the surface, active-learning classes can seem to differ little from more conventional approaches. Undergraduate students have always had discussion sessions to ask about the course material, and laboratory classes in which they would carry out experiments. But if you look more closely, says Tobin, these are often just 'cookbook' exercises. The typical approach is 'read that and be prepared to talk about these questions', or 'follow that procedure and you'll get this result'. In an active-learning class such as hers, she says, the students take charge of their own education. \u201cThey are framing the questions themselves.\u201d The same is true for active learning in first-year courses, in which the teachers often do supply the questions \u2014 but frame them in a way that asks for more than a rote recitation of facts. It is the difference between 'name the sensory nerves of the leg', and what neuroscientist Sarah Leupen asks of her introductory physiology class at the University of Maryland, Baltimore County (UMBC): \n               You're innocently walking down the street when aliens zap away the sensory neurons in your legs. What happens? \n             a) Your walking movements show no significant change . b) You can no longer walk . c) You can walk, but the pace changes . d) You can walk, but clumsily . \u201cWe usually get lots of vigorous debate on this one,\u201d says Leupen, who spends most of her class time firing such questions at her students. \u201cIt's lovely to experience.\u201d What makes those questions special is that the students cannot answer them simply by reading the course material \u2014 although they are expected to have done that before attending class. Instead, they have to apply what they have learned, which they do by clustering around tables in small teams and arguing over the options. That struggle is the real pay-off, says Leupen, who eventually explains the right answer (in this case, d). And if a team gets it wrong, she says, \u201cthat's usually a good thing \u2014 because then they really remember it\u201d. Evidence has been accumulating for decades that students who actively engage with course material will end up retaining it for much longer than they would have otherwise, and they will be better able to apply their knowledge broadly. But the evidence began to draw widespread attention only around the turn of the century \u2014 not least thanks to Carl Wieman, who suddenly became one of the movement's most visible champions when he was awarded the 2001 Nobel Prize in Physics for his co-discovery of Bose\u2013Einstein condensates. \u201cI started way before the Nobel prize,\u201d says Wieman, who is now at Stanford University in California. \u201cIt's just that people didn't pay attention to me until then.\u201d Wieman's conversion began in the late 1980s, when he noticed something about the graduate students coming into his atomic-physics lab \u2014 then at the University of Colorado Boulder. \u201cThey had done really well as undergraduates, but couldn't do research,\u201d he says. Over the years, they learned how to be good scientists, \u201cbut that had little to do with how well they had done in their courses\u201d. In trying to figure out why, Wieman came across the already huge body of empirical research on learning \u2014 most of it totally unknown to science departments. Among the most striking findings, he says, was one 3  that explained his own observation. It showed that in the traditional way of teaching, students could pass the test, but did not get a basic conceptual model of the subject, he says. Other scientists were coming to much the same realization, and they were starting to experiment with other ways to teach. By the time Wieman's Nobel shone a spotlight on the efforts, many fields had started what is now known as Discipline-Based Education Research: investigations into active methods for teaching concepts specific to each branch of science 4 . Other powerful advocates included biologist Bruce Alberts, then president of the US National Academy of Sciences. In 2004, Alberts consolidated several academy panels into the Board on Science Education: a group of senior scientists, initially chaired by Wieman, that has gone on to release a series of reports on education reform. The most recent of those, published in January 2015, is essentially a how-to manual for applying active learning in undergraduate settings 5 . \n               Culture shock \n             Yet there is still plenty of scepticism, says Linda Hodges, a biochemist and head of the Faculty Development Center at the UMBC, and author of a forthcoming book on overcoming obstacles to education reform. One big reason, she says, is that for many scientists, active learning is sharply at odds with their beliefs about teaching. Researchers often feel that a teacher's job is simply to communicate content: the factual knowledge covered in the course. That is a big stumbling block for active learning, because time spent on team discussions and the like can seem like time taken away from that content. Getting past that requires compromise, says Jeff Leips, a geneticist who teaches ecology and evolution at the UMBC. \u201cYou have to accept that you can't cover everything to the same level.\u201d But the pay-off is that the students retain much more of the material that is covered, and are able to use that knowledge much more effectively. Another common belief, says Hodges, is that it is the professor who should be in control of the classroom. \u201cA lot of these pedagogies ask you to relinquish some of your control and hand it over to a bunch of novice students,\u201d she says. \u201cAnd that sounds odd to a lot of faculty.\u201d It definitely requires a different set of skills, agrees Leips. \u201cA lecture is like a performance: you know the script.\u201d But an active-learning class is more like improvisational acting, he says. \u201cYou have to go with the flow\u201d, responding to questions and situations as they arise. And not everyone is comfortable with that. Adding to the resistance is that many faculty members who try active learning hastily back off when the techniques do not seem to work, or when students start to turn in teacher evaluations that say, 'I had to teach myself!' or, 'Just tell me what I need to know!'. One faculty member, who asked not to be named so that she could speak freely about her institution, tells the story of a chemistry instructor who told his students to 'work together', and then spent the rest of the class time reading. \u201cActive learning done badly is worse than a good lecture,\u201d says Leips. A 2011 survey of biology teachers at 77 US universities found that, even though most of them claimed to be using at least some active-learning methods, few of them were doing it properly 6 . Proponents of active learning say that change will come only when innovations are made at every level of a university system (see  page 282 ). To help interested biology teachers to do better, the US National Academies has been running a series of five-day workshops every summer since 2004. \u201cWe've had about 1,000 people go through by now,\u201d says William Wood, a biologist at the University of Colorado Boulder, and co-director of the programme for its first ten years. One of the big lessons, says Wood, is that teachers should develop their lesson plans in the same way as they design experiments. Instead of following a textbook or syllabus, they should start with a clear goal \u2014 the concepts and skills that they want the students to learn. Then they should choose the instructional methods that will achieve that goal, as well as the methods they are going to use to assess the students' progress. The summer institutes have undoubtedly done a lot to raise people's awareness, says Leupen, but simply attending a workshop is not enough. \u201cIf they come back to the same department full of the same people doing the same things \u2014 they will go right back to teaching the old way.\u201d Continuing the support back home is crucial, says Lue. \u201cScience is a team sport. If you're a neuroscientist or a soft-matter physicist, you have seminars, colloquia, luncheons \u2014 places you can go on campus to meet with like-minded people and trade best practices.\u201d His centre at Harvard is trying to create that kind of community for teaching and learning, he says, and many other US universities are doing the same. As helpful as such efforts are, they do not get at what many regard as the biggest challenges for active learning. One is the lack of coordination across borders. Educational innovations are clearly happening across the globe (see  page 276 ) and interest in active learning is high. Dirks has received an enthusiastic response to workshops she ran on pedagogy and the responsible conduct of science in Jordan, Turkey, Egypt, India and Malaysia. And leaders such as Wieman say that they regularly get invitations to talk in Europe, Asia and Australia. Even so, the international flow of ideas is only a fraction of what it could be. The education systems differ between nations, and it is not obvious how lessons learned in one place can be applied elsewhere. What is more, researchers who are trying to get innovations into universities tend to publish in their own languages. And then there is what Wieman and others regard as the most fundamental obstacle: the university incentive system. Too often, they say, publications and funding are the only things measured for promotion and tenure decisions, which in effect penalizes time spent on classroom innovation. \u201cUntil we commit to having teaching be a key role in tenure decisions,\u201d says Lue, \u201cwe're just paying lip service.\u201d Some scientists say that the increased intellectual respectability of good teaching is beginning to make itself felt. Many small universities such as the UMBC are making it a key part of hiring and promotion decisions. \u201cIn our department, you don't get teaching points in tenure decisions unless you've been innovating,\u201d says Leupen. Even large research-focused universities such as Harvard are beginning to place more emphasis on instruction. \u201cBecause there is a movement, and programmes to impart this knowledge,\u201d says Dirks, \u201cpeople are starting to get it.\u201d \n                     Reading, writing and high-energy physics 2015-Jul-15 \n                   \n                     First robust genetic links to depression emerge 2015-Jul-15 \n                   \n                     University learning: Improve undergraduate science education 2015-Jul-15 \n                   \n                     The scientist of the future 2015-Jul-15 \n                   \n                     Lifelong learning: Science professors need leadership training 2015-Jul-15 \n                   \n                     Why we are teaching science wrong, and how to make it right 2015-Jul-15 \n                   \n                     The university experiment: Campus as laboratory 2014-Oct-15 \n                   \n                     Education online: The virtual lab 2013-Jul-17 \n                   \n                     Online learning: Campus 2.0 2013-Mar-13 \n                   \n                     Nature  special: Building the 21st century scientist \n                   \n                     The Secret to Raising Smart Kids \n                   \n                     Scientific American  collection on learning and the mind \n                   \n                     National Research Council's Board on Science Education \n                   \n                     Center for Scientific Teaching at Yale \n                   \n                     The National Academies Summer Institutes on Undergraduate Education \n                   Reprints and Permissions"},
{"file_id": "526182a", "url": "https://www.nature.com/articles/526182a", "year": 2015, "authors": [{"name": "Regina Nuzzo"}], "parsed_as_year": "2006_or_before", "body": "Humans are remarkably good at self-deception. But growing concern about reproducibility is driving many researchers to seek ways to fight their own worst instincts. In 2013, five years after he co-authored a paper showing that Democratic candidates in the United States could get more votes by moving slightly to the right on economic policy 1 , Andrew Gelman, a statistician at Columbia University in New York City, was chagrined to learn of an error in the data analysis. In trying to replicate the work, an undergraduate student named Yang Yang Hu had discovered that Gelman had got the sign wrong on one of the variables. Gelman immediately published a three-sentence correction, declaring that everything in the paper's crucial section should be considered wrong until proved otherwise. Reflecting today on how it happened, Gelman traces his error back to the natural fallibility of the human brain: \u201cThe results seemed perfectly reasonable,\u201d he says. \u201cLots of times with these kinds of coding errors you get results that are just ridiculous. So you know something's got to be wrong and you go back and search until you find the problem. If nothing seems wrong, it's easier to miss it.\u201d This is the big problem in science that no one is talking about: even an honest person is a master of self-deception. Our brains evolved long ago on the African savannah, where jumping to plausible conclusions about the location of ripe fruit or the presence of a predator was a matter of survival. But a smart strategy for evading lions does not necessarily translate well to a modern laboratory, where tenure may be riding on the analysis of terabytes of multidimensional data. In today's environment, our talent for jumping to conclusions makes it all too easy to find false patterns in randomness, to ignore alternative explanations for a result or to accept 'reasonable' outcomes without question \u2014 that is, to ceaselessly lead ourselves astray without realizing it. Failure to understand our own biases has helped to create a  crisis of confidence about the reproducibility  of published results, says statistician  John Ioannidis , co-director of the Meta-Research Innovation Center at Stanford University in Palo Alto, California. The issue goes well beyond cases of fraud. Earlier this year, a large project that attempted to replicate 100 psychology studies managed to reproduce only  slightly more than one-third 2 . In 2012, researchers at biotechnology firm Amgen in Thousand Oaks, California, reported that they could replicate only 6 out of 53 landmark studies in oncology and haematology 3 . And in 2009, Ioannidis and his colleagues described how they had been able to fully reproduce only 2 out of 18 microarray-based gene-expression studies 4 . Although it is impossible to document how often researchers fool themselves in data analysis, says Ioannidis, findings of irreproducibility beg for an explanation. The study of 100 psychology papers is a case in point: if one assumes that the vast majority of the original researchers were honest and diligent, then a large proportion of the problems can be explained only by unconscious biases. \u201cThis is a great time for  research on research ,\u201d he says. \u201cThe massive growth of science allows for a massive number of results, and a massive number of errors and biases to study. So there's good reason to hope we can find better ways to deal with these problems.\u201d \u201cWhen crises like this issue of reproducibility come along, it's a good opportunity to advance our scientific tools,\u201d says Robert MacCoun, a social scientist at Stanford. That has happened before, when scientists in the mid-twentieth century realized that experimenters and subjects often unconsciously changed their behaviour to match expectations. From that insight, the double-blind standard was born. \u201cPeople forget that when we talk about the scientific method, we don't mean a finished product,\u201d says Saul Perlmutter, an astrophysicist at the University of California, Berkeley. \u201cScience is an ongoing race between our inventing ways to fool ourselves, and our inventing ways to avoid fooling ourselves.\u201d So researchers are trying a variety of creative ways to debias data analysis \u2014 strategies that involve collaborating with academic rivals, getting papers accepted before the study has even been started and working with strategically faked data. \n               The problem \n             Although the human brain and its cognitive biases have been the same for as long as we have been doing science, some important things have changed, says psychologist Brian Nosek, executive director of the non-profit Center for Open Science in Charlottesville, Virginia, which works to increase the transparency and reproducibility of scientific research. Today's academic environment is more competitive than ever. There is an emphasis on  piling up publications with statistically significant results  \u2014 that is, with data relationships in which a commonly used measure of statistical certainty, the  p -value, is 0.05 or less. \u201cAs a researcher, I'm not trying to produce misleading results,\u201d says Nosek. \u201cBut I do have a stake in the outcome.\u201d And that gives the mind excellent motivation to find what it is primed to find. Another reason for concern about cognitive bias is the advent of staggeringly large multivariate data sets, often harbouring only a faint signal in a sea of random noise. Statistical methods have barely caught up with such data, and our brain's methods are even worse, says Keith Baggerly, a statistician at the University of Texas MD Anderson Cancer Center in Houston. As he told a conference on challenges in bioinformatics last September in Research Triangle Park, North Carolina, \u201cOur intuition when we start looking at 50, or hundreds of, variables sucks.\u201d Andrew King, a management specialist at Dartmouth College in Hanover, New Hampshire, says that the widespread use of point-and-click data-analysis software has made it easy for researchers to sift through massive data sets without fully understanding the methods, and to find small  p -values that may not actually mean anything. \u201cI believe we are in the steroids era of social science,\u201d he says. \u201cI've been guilty of using some of these performance-enhancing practices myself. My sense is that most researchers have fallen at least once.\u201d Just as in competitive sport, says Hal Pashler, a psychologist at the University of California, San Diego, this can set up a vicious circle of chasing increasingly better results. When a few studies in behavioural neuroscience started reporting improbably strong correlations of 0.85, Pashler says, researchers who had more moderate (and plausible) results started to worry: \u201cGee, I just got a 0.4, so maybe I'm not really doing this very well.\u201d \n               boxed-text \n             \n               Hypothesis myopia \n             One trap that awaits during the early stages of research is what might be called hypothesis myopia: investigators fixate on collecting evidence to support just one hypothesis; neglect to look for evidence against it; and fail to consider other explanations. \u201cPeople tend to ask questions that give 'yes' answers if their favoured hypothesis is true,\u201d says Jonathan Baron, a psychologist at the University of Pennsylvania in Philadelphia. For example, says Baron, studies have tried to show how disgust influences moral condemnation, \u201cby putting the subject in a messy room, or a room with 'fart spray' in the air\u201d. The participants are then asked to judge how to respond to moral transgressions; if those who have been exposed to clutter or smells favour harsher punishments, researchers declare their 'disgust hypothesis' to be supported 5 . But they have not considered competing explanations, he says, and so they ignore the possibility that participants are lashing out owing to anger at their foul treatment, not simply disgust. By focusing on one hypothesis, researchers might be missing the real story entirely. Courtrooms face a similar problem. In 1999, a woman in Britain called Sally Clark was found guilty of murdering two of her sons, who had died suddenly as babies. A factor in her conviction was the presentation of statistical evidence that the chances of two children in the same family dying of sudden infant death syndrome (SIDS) were only 1 in 73 million \u2014 a figure widely interpreted as fairly damning. Yet considering just one hypothesis leaves out an important part of the story. \u201cThe jury needs to weigh up two competing explanations for the babies' deaths: SIDS or murder,\u201d wrote statistician Peter Green on behalf of the Royal Statistical Society in 2002 (see  go.nature.com/ochsja ). \u201cThe fact that two deaths by SIDS is quite unlikely is, taken alone, of little value. Two deaths by murder may well be even more unlikely. What matters is the relative likelihood of the deaths under each explanation, not just how unlikely they are under one explanation.\u201d Mathematician Ray Hill of the University of Salford, UK, later estimated 6  that a double SIDS death would occur in roughly 1 out of 297,000 families, whereas two children would be murdered by a parent in roughly 1 out of 2.7 million families \u2014 a likelihood ratio of 9 to 1 against murder. In 2003, Clark's conviction was overturned on the basis of new evidence. The Attorney General for England and Wales went on to release two other women who had been convicted of murdering their children on similar statistical grounds. \n               The Texas sharpshooter \n             A cognitive trap that awaits during data analysis is illustrated by the fable of the Texas sharpshooter: an inept marksman who fires a random pattern of bullets at the side of a barn, draws a target around the biggest clump of bullet holes, and points proudly at his success. His bullseye is obviously laughable \u2014 but the fallacy is not so obvious to gamblers who believe in a 'hot hand' when they have a streak of wins, or to people who see supernatural significance when a lottery draw comes up as all odd numbers. Nor is it always obvious to researchers. \u201cYou just get some encouragement from the data and then think, well, this is the path to go down,\u201d says Pashler. \u201cYou don't realize you had 27 different options and you picked the one that gave you the most agreeable or interesting results, and now you're engaged in something that's not at all an unbiased representation of the data.\u201d Psychologist Uri Simonsohn at the University of Pennsylvania, gives an explicit nod to this naivety in his definition of ' p -hacking': \u201cExploiting \u2014 perhaps unconsciously \u2014 researcher degrees of freedom until  p  < 0.05.\u201d In 2012, a study of more than 2,000 US psychologists 7  suggested how common  p -hacking is. Half had selectively reported only studies that 'worked', 58% had peeked at the results and then decided whether to collect more data, 43% had decided to throw out data only after checking its impact on the  p -value and 35% had reported unexpected findings as having been predicted from the start, a practice that psychologist Norbert Kerr of Michigan State University in East Lansing has called HARKing, or hypothesizing after results are known. Not only did the researchers admit to these  p -hacking practices, but they defended them. This May, a journalist described how he had teamed up with a German documentary filmmaker and demonstrated that creative  p -hacking, carried out over one \u201cbeer-fueled\u201d weekend, could be used to 'prove' that eating chocolate leads to weight loss, reduced cholesterol levels and improved well-being (see  go.nature.com/blkpke ). They gathered 18 different measurements \u2014 including weight, blood protein levels and sleep quality \u2014 on 15 people, a handful of whom had eaten some extra chocolate for a few weeks. With that many comparisons, the odds were better than 50\u201350 that at least one of them would look statistically significant just by chance. As it turns out, three of them did \u2014 and the team cherry-picked only those to report. \n               Asymmetric attention \n             The data-checking phase holds another trap: asymmetric attention to detail. Sometimes known as disconfirmation bias, this happens when we give expected results a relatively free pass, but we rigorously check non-intuitive results. \u201cWhen the data don't seem to match previous estimates, you think, 'Oh, boy! Did I make a mistake?'\u201d MacCoun says. \u201cWe don't realize that probably we would have needed corrections in the other situation as well.\u201d The evidence suggests that scientists are more prone to this than one would think. A 2004 study 8  observed the discussions of researchers from 3 leading molecular-biology laboratories as they worked through 165 different lab experiments. In 88% of cases in which results did not align with expectations, the scientists blamed the inconsistencies on how the experiments were conducted, rather than on their own theories. Consistent results, by contrast, were given little to no scrutiny. In 2011, an analysis of over 250 psychology papers found 9  that more than 1 in 10 of the  p -values was incorrect \u2014 and that when the errors were big enough to change the statistical significance of the result, more than 90% of the mistakes were in favour of the researchers' expectations, making a non-significant finding significant. \n               Just-so storytelling \n             As data-analysis results are being compiled and interpreted, researchers often fall prey to just-so storytelling \u2014 a fallacy named after the Rudyard Kipling tales that give whimsical explanations for things such as how the leopard got its spots. The problem is that post-hoc stories can be concocted to justify anything and everything \u2014 and so end up truly explaining nothing. Baggerly says that he has seen such stories in genetics studies, when an analysis implicates a huge number of genes in a particular trait or outcome. \u201cIt's akin to a Rorschach test,\u201d he said at the bioinformatics conference. Researchers will find a story, he says, \u201cwhether it's there or not. The problem is that occasionally it ain't real.\u201d Another temptation is to rationalize why results should have come up a certain way but did not \u2014 what might be called JARKing, or justifying after results are known. Matthew Hankins, a statistician at King's College London, has collected more than 500 creative phrases that researchers use to convince readers that their non-significant results are worthy of attention (see  go.nature.com/pwctoq ). These include \u201cflirting with conventional levels of significance ( p  > 0.1)\u201d, \u201con the very fringes of significance ( p  = 0.099)\u201d and \u201cnot absolutely significant but very probably so ( p  > 0.05)\u201d. \n               The solutions \n             In every one of these traps, cognitive biases are hitting the accelerator of science: the process of spotting potentially important scientific relationships. Countering those biases comes down to strengthening the 'brake': the ability to slow down, be sceptical of findings and eliminate false positives and dead ends. One solution that is piquing interest revives an old tradition: explicitly considering competing hypotheses, and if possible working to develop experiments that can distinguish between them. This approach, called strong inference 10 , attacks hypothesis myopia head on. Furthermore, when scientists make themselves explicitly list alternative explanations for their observations, they can reduce their tendency to tell just-so stories. In 2013, researchers reported 11  using strong-inference techniques in a study of what attracts female t\u00fangara frogs ( Engystomops pustulosus ) during mating calls. The existing data could be explained equally well by two competing theories \u2014 one in which females have a preset neural template for mating calls, and another in which they flexibly combine auditory cues and visual signals such as the appearance of the males' vocal sacs. So the researchers developed an experiment for which the two theories had opposing predictions. The results showed that females can use multisensory cues to judge attractiveness. \n               Transparency \n             Another solution that has been gaining traction is open science. Under this philosophy, researchers share their methods, data, computer code and results in central repositories, such as the Center for Open Science's Open Science Framework, where they can choose to make various parts of the project subject to outside scrutiny. Normally, explains Nosek, \u201cI have enormous flexibility in how I analyse my data and what I choose to report. This creates a conflict of interest. The only way to avoid this is for me to tie my hands in advance. Precommitment to my analysis and reporting plan mitigates the influence of these cognitive biases.\u201d An even more radical extension of this idea is the introduction of registered reports: publications in which scientists present their research plans for peer review before they even do the experiment. If the plan is approved, the researchers get an 'in-principle' guarantee of publication, no matter how strong or weak the results turn out to be. This should reduce the unconscious temptation to warp the data analysis, says Pashler. At the same time, he adds, it should keep peer reviewers from discounting a study's results or complaining after results are known. \u201cPeople are evaluating methods without knowing whether they're going to find the results congenial or not,\u201d he says. \u201cIt should create a much higher level of honesty among referees.\u201d More than 20 journals are offering or plan to offer some format of registered reports. \n               Team of rivals \n             When it comes to replications and controversial topics, a good debiasing approach is to bypass the typical academic back-and-forth and instead invite your academic rivals to work with you. An adversarial collaboration has many advantages over a conventional one, says Daniel Kahneman, a psychologist at Princeton University in New Jersey. \u201cYou need to assume you're not going to change anyone's mind completely,\u201d he says. \u201cBut you can turn that into an interesting argument and intelligent conversation that people can listen to and evaluate.\u201d With competing hypotheses and theories in play, he says, the rivals will quickly spot flaws such as hypothesis myopia, asymmetric attention or just-so storytelling, and cancel them out with similar slants favouring the other side. Psychologist Eric-Jan Wagenmakers of the University of Amsterdam has engaged in this sort of proponent\u2013sceptic collaboration, when he teamed up with another group in an attempt 12  to replicate its research suggesting that horizontal eye movements help people to retrieve events from their memory. It is often difficult to get researchers whose original work is under scrutiny to agree to this kind of adversarial collaboration, he says. The invitation is \u201cabout as attractive as putting one's head on a guillotine \u2014 there is everything to lose and not much to gain\u201d. But the group that he worked with was eager to get to the truth, he says. In the end, the results were not replicated. The sceptics remained sceptical, and the proponents were not convinced by a single failure to replicate. Yet this was no stalemate. \u201cAlthough our adversarial collaboration has not resolved the debate,\u201d the researchers wrote, \u201cit has generated new testable ideas and has brought the two parties slightly closer.\u201d Wagenmakers suggests several ways in which this type of collaboration could be encouraged, including a prize for best adversarial collaboration, or special sections for such collaborations in top journals. \n               Blind data analysis \n             One debiasing procedure has a solid history in physics but is little known in other fields: blind data analysis (see  page 187 ). The idea is that researchers who do not know how close they are to desired results will be less likely to find what they are unconsciously looking for 13 . One way to do this is to write a program that creates alternative data sets by, for example, adding random noise or a hidden offset, moving participants to different experimental groups or hiding demographic categories. Researchers handle the fake data set as usual \u2014 cleaning the data, handling outliers, running analyses \u2014 while the computer faithfully applies all of their actions to the real data. They might even write up the results. But at no point do the researchers know whether their results are scientific treasures or detritus. Only at the end do they lift the blind and see their true results \u2014 after which, any further fiddling with the analysis would be obvious cheating. Perlmutter used this method for his team's work on the Supernova Cosmology Project in the mid-2000s. He knew that the potential for the researchers to fool themselves was huge. They were using new techniques to replicate estimates of two crucial quantities in cosmology \u2014 the relative abundances of matter and of dark energy \u2014 which together reveal whether the Universe will expand forever or eventually collapse into a Big Crunch. So their data were shifted by an amount known only to the computer, leaving them with no idea what their findings implied until everyone agreed on the analyses and the blind could be safely lifted. After the big reveal, not only were the researchers pleased to confirm earlier findings of an expanding Universe 14 , Perlmutter says, but they could be more confident in their conclusions. \u201cIt's a lot more work in some sense, but I think it leaves you feeling much safer as you do your analysis,\u201d he says. He calls blind data analysis \u201cintellectual hygiene, like washing your hands\u201d. Data blinding particularly appeals to young researchers, Perlmutter says \u2014 not least because of the sense of suspense it gives. He tells the story of a recent graduate student who had spent two years under a data blind as she analysed pairs of supernova explosions. After a long group meeting, Perlmutter says, the student presented all her analyses and said that she was ready to unblind if everyone agreed. \u201cIt was 6 o'clock in the evening and time for dinner,\u201d says Perlmutter. And everyone in the audience said, \u201cIf the result comes out wrong, it's going to be a very disappointing evening, and she's going to have to think really hard about what she's going to do with her PhD thesis. Maybe we should wait until morning.\u201d \u201cAnd we all looked at each other, and we said, 'Nah! Let's unblind now!' So we unblinded, and the results looked great, and we all cheered and applauded.\u201d See Comment   P.187  &  P.189 \n                 Tweet \n                 Follow @NatureNews \n               \n                     First results from psychology\u2019s largest reproducibility test 2015-Apr-30 \n                   \n                     Metascience could rescue the \u2018replication crisis\u2019 2014-Nov-04 \n                   \n                     Scientific method: Statistical errors 2014-Feb-12 \n                   \n                     Reproducibility: Six red flags for suspect work 2013-May-22 \n                   \n                     Replication studies: Bad copy 2012-May-16 \n                   \n                     Center for Open Science \n                   \n                     Robert MacCoun \n                   \n                     Retraction Watch \n                   Reprints and Permissions"},
{"file_id": "523271a", "url": "https://www.nature.com/articles/523271a", "year": 2015, "authors": [], "parsed_as_year": "2006_or_before", "body": "A special issue of  Nature  examines what is needed to grow the next generation of scientists. What does it take to be a successful scientist in the modern world? The obvious answers are deep knowledge of a discipline and mastery of the scientific method. But there are other key requirements, such as the ability to think critically and solve problems creatively and collaboratively. Communication skills are a must, and mastery of modern technology helps. For generations, classes in science, technology, engineering and maths (STEM) have been focused almost exclusively on building knowledge alone. A steady diet of lecture-based learning was designed to fill students up with facts and test their ability to memorize them. Teaching the other skills was too often given short shrift. Now educators and education researchers are calling for change. They argue that creative thinking, problem solving, motivation, persistence and other 'twenty-first-century skills' can, and should, be taught and fostered through well-designed courses. Developing these skills enhances students' abilities to master and retain knowledge; many hope that focusing on them will help to curb the alarming rate at which students interested in STEM abandon the subjects. The Organisation for Economic Co-operation and Development deems STEM education as crucial to powering innovation and economic growth, and has strongly encouraged investment in education strategies that focus on twenty-first-century skills. Now  Nature , in collaboration with  Scientific American , is taking a look at the challenges in STEM education (a full listing of content is available at  nature.com/stem ). A News Feature on  page 272  discusses the move towards 'active learning' rather than passive lecturing in the undergraduate classroom, but finds that encouraging innovative methods requires a change in incentives. A Comment article by representatives of the Association of American Universities and the Research Corporation for Science Advancement Cottrell Scholars on  page 282  offers a road map for the institutional changes that will be required to shift the status quo. Those teaching science in primary and secondary schools face different constraints, but have no shortage of innovative practices. A News Feature on  page 276  looks at some of the most creative STEM education programmes around the world, for preschoolers up to teens. On  page 286 , leading design practitioners explain how nature itself aids early child development, and how architecture and play spaces are best engineered for learning. At the other end of the spectrum, senior researchers should brush up their leadership skills, says a Comment piece on  page 279 . Finally,  Nature  polled some of the leading thinkers in science and education for what it takes to make an effective scientist in the twenty-first century. With answers on  page 371  that range from the practical to the philosophical, it is clear that the science classroom is in for a radical change.  \n                     Nature  special: Building the 21st century scientist \n                   Reprints and Permissions"},
{"file_id": "522274a", "url": "https://www.nature.com/articles/522274a", "year": 2015, "authors": [{"name": "Elizabeth Gibney"}], "parsed_as_year": "2006_or_before", "body": "A wave of innovative flat materials is following in the wake of graphene \u2014 but the most exciting applications could come from stacking them into 3D devices. Physicists have used almost every superlative they can think of to describe graphene. This gossamer, one-atom-thick sheet of carbon is flexible, transparent, stronger than steel, more conductive than copper and so thin that it is effectively two-dimensional (2D). No sooner was it isolated in 2004 than it became an obsession for researchers around the world. But not for Andras Kis. As miraculous as graphene was, says Kis, \u201cI felt there had to be more than carbon.\u201d So in 2008, when he got the chance to start his own research group in nanoscale electronics at the Swiss Federal Institute of Technology in Lausanne (EPFL), Kis focused his efforts on a class of super-flat materials that had been languishing in graphene's shadow. These materials had an ungainly name \u2014 transition-metal dichalcogenides (TMDCs) \u2014 but a 2D form that was quite simple. A single sheet of transition-metal atoms such as molybdenum or tungsten was sandwiched between equally thin layers of chalcogens: elements, such as sulfur and selenium, that lie below oxygen in the periodic table. TMDCs were almost as thin, transparent and flexible as graphene, says Kis, but \u201csomehow they got a reputation as not that interesting. I thought they deserved a second chance.\u201d He was right. Work by his team and a handful of others soon showed that different combinations of the basic ingredients could produce TMDCs with a wide range of electronic and optical properties. Unlike graphene, for example, many TMDCs are semiconductors, meaning that they have the potential to be made into molecular-scale digital processors that are much more energy efficient than anything possible with silicon. Within a few years, laboratories around the world had joined the 2D quest. \u201cAt first it was one, then two or three, and suddenly it became whole zoo of 2D materials,\u201d says Kis. From a scattering of publications in 2008, 2D TMDCs alone now generate six publications each day. Physicists think that there may be around 500 2D materials, including not just graphene and TMDCs, but also single layers of metal oxides, and single-element materials such as silicene and phosphorene. \u201cIf you want a 2D material with a given set of properties,\u201d says Jonathan Coleman, a physicist at Trinity College Dublin, \u201cyou will find one.\u201d Ironically, one of the most exciting frontiers in 2D materials is stacking them into structures that are still very thin, but definitely 3D. By taking advantage of the vastly different properties of various super-flat materials, it should be possible to build entire digital circuits out of atomically thick components, creating previously unimagined devices. Applications are already being touted in fields from energy harvesting to quantum communications \u2014 even though physicists are just beginning to learn the materials' potential. \u201cEach one is like a Lego brick,\u201d says Kis. \u201cIf you put them together, maybe you can build something completely new.\u201d \n               Adventures in Flatland \n             A material that is just a few atoms thick can have very different fundamental properties from a material made of the same molecules in solid form. \u201cEven if the bulk material is an old one, if you can get it into 2D form it opens up new opportunities,\u201d says Yuanbo Zhang, an experimental condensed-matter physicist at Fudan University in Shanghai, China. Carbon is the classic example, as physicists Andre Geim and Konstantin Novoselov found in 2004 when they first reported isolating graphene 1  in their laboratory at the University of Manchester, UK. Their technique was almost absurdly simple. The basic step is to press a strip of sticky tape onto a flake of graphite, then peel it off, bringing with it a few of the atom-thick layers that make up the bulk material. By repeating this process until they had single layers \u2014 which many theorists had said could not exist in isolation \u2014 Geim and Novoselov were able to start investigating graphene's remarkable properties. Their work won them the 2010 Nobel Prize in Physics. Physicists were soon hurrying to exploit those properties for applications ranging from flexible screens to energy storage (see  page 268 ). Unfortunately, graphene proved to be a poor fit for digital electronics. The ideal material for that application is a semiconductor \u2014 a material that does not conduct electricity unless its electrons are boosted with a certain amount of energy from heat, light or an external voltage. The amount of energy needed varies with the material, and is known as the band gap. Turning the material's conductivity on and off creates the 1s and 0s of the digital world. But graphene in its pure form does not have a band gap \u2014 it conducts all the time. Still, Geim and Novoselov's success in making graphene spurred them, Kis and many others to start investigating alternative 2D materials that could have a band gaps 2 . They began with TMDCs, which had been studied in bulk form since the 1960s. By 2010, Kis's team had built its first single-layer transistor 3  from the TMDC molybdenum disulfide (MoS 2 ; see 'Flat-pack assembly'), speculating that such devices could one day offer flexible electronics whose small component size and low voltage requirements would mean that they consumed much less power than conventional silicon transistors. Being semiconducting was not their only advantage. Studies in 2010 showed that MoS 2  could both absorb and emit light 4 , 5  efficiently, making it \u2014 and probably other TMDCs \u2014 attractive for use in solar cells and photodetectors. A single layer of TMDCs can capture more than 10% of incoming photons, an incredible figure for a material three atoms thick, says Bernhard Urbaszek, a physicist at the Physics and Chemistry of Nano-Objects Laboratory in Toulouse, France. This also helps them in another task: converting light into electricity. When an incoming photon hits the three-layer crystal, it boosts an electron past the band gap, allowing it to move through an external circuit. Each freed electron leaves behind a kind of empty space in the crystal \u2014 a positively charged 'hole' where an electron ought to be. Apply a voltage, and these holes and electrons circulate in opposite directions to produce a net flow of electric current. This process can also be reversed to turn electricity into light. If electrons and holes are injected into the TMDC from an outside circuit; when they meet, they recombine and give up their energy as photons. This ability to convert light to electricity and vice versa makes TMDCs promising candidates for applications that involve transmitting information using light, as well as for use in tiny, low-power light sources and even lasers. This year, four different teams demonstrated the ultimate control over light emission, showing that the TMDC tungsten diselenide (WSe 2 ) could absorb and release individual photons 6 , 7 , 8 , 9 . Quantum cryptography and communications, which encode information in one photon at a time, need emitters like this, where you \u201cpress a button and get a photon now\u201d, says Urbaszek. Existing single-photon emitters are often made of bulk semiconductors, but 2D materials could prove smaller and easier to integrate with other devices. Their emitters are necessarily on the surface, which could also make them more efficient and easier to control. Even as researchers were getting to grips with TMDCs, theorists were seeking other materials that could be engineered in two dimensions. One obvious candidate was silicon, which sits right below carbon in the periodic table, forms chemical bonds in a similar way, has a natural band gap and is already widely used in the electronics industry. Calculations suggested that, unlike graphene, a sheet of atomically thick silicon would have a ridged structure that could be squashed and stretched to create a tunable band gap. But like graphene, this 'silicene' would be a much faster conductor of electrons than most TMDCs. Unfortunately, theory also suggested that a 2D sheet of silicene would be highly reactive and completely unstable in air. Nor could it be ripped from a crystal like other 2D materials: natural silicon exists only in a 3D form analogous to a diamond crystal, with nothing resembling the layered sheets of carbon found in graphite. \u201cPeople said it was insane and would never work,\u201d says Guy Le Lay, a physicist at Aix-Marseille University in France. But Le Lay, who had been growing metals on silicon surfaces for years, saw a way to make silicene by doing the reverse \u2014 growing atomically thin sheets of silicon on metal. And in 2012 he reported success 10 : he had grown layers of silicene on silver, which has an atomic structure that matches the 2D material perfectly (see  Nature   495 , 152\u2013153; 2013 ). Buoyed by that effort, Le Lay and others have since moved down the carbon column of the periodic table. Last year, he demonstrated a similar technique to grow a 2D mesh of germanium atoms \u2014 germanene \u2014 on a substrate of gold 11 . His next target is stanene: a 2D lattice of tin atoms. Stanene should have a band gap larger than either silicene and germanene, which would allow its devices to work at higher temperatures and voltages. And it is predicted to carry charges only on its outside edges, so it should conduct with super efficiency. But Le Lay has competition. Although no one has yet reported growing stanene successfully, research groups in China are rumoured to be close. \n               Elemental shift \n             Others are exploring different parts of the periodic table. Zhang's team and another led by Peide Ye at Purdue University in West Lafayette, Indiana, last year described 12 , 13  stripping 2D layers of phosphorene from black phosphorus, a bulk form of the element that has been studied for a century. Like graphene, phosphorene conducts electrons swiftly. But unlike graphene, it has a natural band gap \u2014 and it is more stable than silicene. Phosphorene has enjoyed a meteoric rise. At the 2013 meeting of the American Physical Society, it was the subject of a single talk by members of Zhang's group; by 2015, the meeting had three entire sessions devoted to it. But like its fellow pure-element 2D materials, phosphorene reacts very strongly with oxygen and water. If it is to last longer than a few hours, it needs to be sandwiched between layers of other materials. This natural instability makes fabricating devices with the 'enes' difficult; Le Lay estimates that around 80% of the papers about them are still theoretical. Nonetheless, both Zhang and Ye succeeded in making phosphorene transistors. This year, the first transistor from silicene emerged 14 , although it survived for only a few minutes. Still, Le Lay is optimistic that these issues are not insurmountable. Just two years ago, he points out, Geim and other physicists were saying that a silicene transistor could not be made with current technology. \u201cSo it's always dangerous to predict the future,\u201d laughs Le Lay. \n               The next dimension \n             Even as some physicists search for new 2D materials and try to understand their properties, others are already sandwiching them together. \u201cInstead of trying to pick one and say this is the best, maybe the best thing to do is to combine them in such a way that all their different advantages are properly utilized,\u201d says Kis. This could mean stacking components made of different 2D materials to make tiny, dense 3D circuits. Materials could also be layered inside components \u2014 something that chip designers already do when they grow layers of different semiconductors on top of one another to make devices such as the lasers inside DVD players. In standard devices, this is tricky: each layer has to chemically bond with the next, and only certain combinations can be matched. Otherwise, the strain between the different crystal lattices in each layer makes bonding impossible. With 2D materials, that problem goes away: the atoms in each layer bond only very weakly to the neighbouring sheets, so the strain is minimal. Many layers of semiconductors, insulators and conductors can be stacked to form complex devices known generically as van der Waals heterostructures, after the weak bonds that bind the layers. Already, for example, graphene has been used alongside MoS2 and WSe2 to create the junctions at the heart of solar cells 15  and photodetectors 16 , exploiting the semiconductor's abilities to absorb photons and graphene's swift ability to carry the freed electrons away. In February this year, Novoselov and his team reversed the solar-cell concept to make a light-emitting diode 17  from MoS 2  and other TMDCs between graphene electrodes. By selecting different TMDCs, the team could choose the wavelength of the photons released. Better still, sandwiching together different 2D layers can allow physicists to fine-tune their devices. Although the bonds between layers are weak, the close proximity of their atoms means that they can affect each other's properties in subtle ways, says Wang Yao, a physicist at the University of Hong Kong. Stacking order, spacing and orientation all control device behaviour. \u201cModelling this gives theorists like me a headache, but the new physical properties are definitely there,\u201d says Yao. Even graphene can get a leg up from other 2D materials, says Marco Polini, a physicist at the National Enterprise for Nanoscience and Nanotechnology (NEST) in Pisa, Italy. His team has been working on devices in which graphene is sandwiched between 2D layers of the insulator boron nitride 18 . When laser light is focused on the device, it gets compressed and channelled through the graphene layer, much more than in devices that sandwich graphene between bulk materials. In principle, this could provide a way for information to be carried between chips using photons rather than electricity. That, says Polini, could mean faster and more efficient communications within the chips. \n               Practical predictions \n             The current buzz around 2D materials is reminiscent of the excitement about graphene in 2005, says physicist Jari Kinaret of Chalmers University of Technology in Gothenburg, Sweden, who heads the European Union's Graphene Flagship \u2014 a programme that also studies other 2D materials. But Kinaret cautions that it could take two decades to really assess the potential of these materials. \u201cThe initial studies on 2D materials are focusing a lot on their electronic properties, because these are close to physicists' hearts,\u201d says Kinaret. \u201cBut I think that the applications, if and when they come, are likely to be in a completely unpredicted area.\u201d (See \u2018A 2D information highway\u2019.) \n               boxed-text \n             Materials that look good in the lab are not always those that make it out into the real world. One major issue facing all 2D materials is how to produce uniform, defect-free layers cheaply. The sticky-tape method works well for TMDCs and phosphorene, but is too time-consuming to scale up. It is also expensive to make bulk black phosphorus, because it involves subjecting naturally occurring white phosphorus to extreme pressure. No one has yet perfected the process of growing single sheets of 2D materials from scratch, let alone the layered structures that physicists find so promising. \u201cIt took a long time to make our heterostructures,\u201d says Xiaodong Xu, a physicist at the University of Washington in Seattle. \u201cHow can we speed that up or make it automatic? There is a lot of work to do.\u201d Such practical considerations could prevent 2D materials from living up to their early promise. \u201cThere have been many rushes like this, and some have turned out to be fads,\u201d says Kis. \u201cBut I think the sheer number of materials and different properties should make sure something comes out of this.\u201d Meanwhile, the zoo is expanding, says Coleman. Arsenene, a heavier cousin to phosphorene, is already on researcher's minds. \u201cAs people start to branch out, they are discovering new materials that have these wonderful properties,\u201d says Coleman. \u201cThe most exciting 2D material is probably one that hasn't been made yet.\u201d \n                     Graphene booms in factories but lacks a killer app 2015-Jun-17 \n                   \n                     Batches of ultra-thin transistors made from 2D materials 2015-Apr-29 \n                   \n                     Graphene\u2019s cousin silicene makes transistor debut 2015-Feb-02 \n                   \n                     Graphene conducts electricity ten times better than expected 2014-Feb-06 \n                   \n                     Phosphorene excites materials scientists 2014-Feb-04 \n                   \n                     Graphene: The quest for supercarbon 2013-Nov-20 \n                   \n                     Electronics and optoelectronics of two-dimensional transition metal dichalcogenides 2012-Nov-06 \n                   \n                     EU Graphene Flagship \n                   \n                     Andras Kis \n                   \n                     The Home of Graphene (University of Manchester) \n                   Reprints and Permissions"},
{"file_id": "523398a", "url": "https://www.nature.com/articles/523398a", "year": 2015, "authors": [{"name": "Elie Dolgin"}], "parsed_as_year": "2006_or_before", "body": "Cells contain an ocean of twisting and turning RNA molecules. Now researchers are working out the structures \u2014 and how important they could be. When Philip Bevilacqua decided to work out the shapes of all the RNA molecules in a living plant cell, he faced two problems. First, he had not studied plant biology since high school. And second, biochemists had tended to examine single RNA molecules; tackling the multitudes that waft around in a cell was a much thornier challenge. Bevilacqua, an RNA chemist at Pennsylvania State University in University Park, was undeterred. He knew that RNA molecules were vital regulators of cell biology and that their structures might offer broad lessons about how they work. He brushed up on plant anatomy in an undergraduate course and worked with molecular plant biologist Sarah Assmann to develop a technique that could cope with RNAs at scale. In November 2013, they and their teams became the first to describe the shapes of thousands of RNAs in a living cell \u2014 revealing a veritable sculpture garden of different forms in the weedy thale cress,  Arabidopsis thaliana 1 . One month later, a group at the University of California, San Francisco, reported a comparable study of yeast and human cells 2 . The number of RNA structures they managed to resolve was \u201cunprecedented\u201d, says Alain Laederach, an RNA biologist at the University of North Carolina at Chapel Hill (UNC). Scientists' view of RNA has transformed over the past few decades. Once, most RNAs were thought to be relatively uninteresting pieces of limp spaghetti that ferried information between the molecules that mattered, DNA and protein. Now, biologists know that RNAs serve many other essential functions: they help with protein synthesis, control gene activity and modify other RNAs. At least 85% of the human genome is transcribed into RNA, and there is vigorous debate about what, if anything, it does. But a key mystery has remained: its convoluted structures. Unlike DNA, which forms a predictable double helix, RNA comprises a single strand that folds up into elaborate loops, bulges, pseudo-knots, hammerheads, hairpins and other 3D motifs. These structures flip and twist between different forms, and are thought to be central to the operation of RNA, albeit in ways that are not yet known. \u201cIt's a big missing piece of the puzzle of understanding how RNAs work,\u201d says Jonathan Weissman, a biophysicist and leader of the yeast and human RNA study. Reporter Elie Dolgin discusses the importance of RNA architecture. In the past few years, researchers have begun to get a toehold on the problem. Bevilacqua, Weissman and others have devised techniques that allow them to take snapshots of RNA configurations en masse inside cells \u2014 and found that the molecules often look nothing like what is seen when RNA folds under artificial conditions. The work is helping them to decipher some of the rules that govern RNA structure, which might be useful in understanding human variation and disease \u2014 and even in improving agricultural crops. \u201cIt gets at the very basic problem of how do living things evolve and how do these molecular rules affect what we look like and how we function,\u201d says Laederach. \u201cAnd that, fundamentally as a biologist, is really exciting.\u201d The best-described RNA structures are what Kevin Weeks, a chemical biologist at the UNC, calls \u201cRNA rocks\u201d: molecules that have changed little in their sequence or structure over evolutionary time. These include transfer RNAs and ribosomal RNAs (both involved in protein synthesis) as well as enzymatic RNAs known as ribozymes. \u201cBut in the world of RNAs,\u201d Weeks says, \u201cthese are probably huge outliers.\u201d The bulk of the RNA world is like unexplored, shifting sand. \u201cWe know next to nothing about the structure of most RNAs,\u201d says Robert Spitale, a chemist at the University of California, Irvine. RNA molecules typically exist as a linear string of nucleotides \u2014 or bases \u2014 for only an instant after they are produced from their template DNA. They quickly fold back on themselves, and complementary nucleotides pair up. They then contort further into complex 3D configurations, interact with proteins and other RNAs and change shape to carry out different jobs. Most techniques for probing RNA structure make use of the reactivity of the nucleotides, or their sensitivity to certain enzymes: regions that are paired up tend to respond differently from those that remain single-stranded. Computer algorithms then help to model the overall structure of the molecule. But these experiments are painstaking and laborious because researchers could interrogate only one part of one molecule at a time. That changed five years ago, with the arrival of a technique called PARS (parallel analysis of RNA structure), developed by genome scientist Howard Chang at Stanford University in California and computational biologist Eran Segal at the Weizmann Institute of Science in Rehovot, Israel. PARS uses one enzyme to cut RNA where it is single-stranded and another to cleave it at double-stranded sites. Researchers treat a sample of RNA with each enzyme independently to produce two libraries of chopped-up RNA; they then sequence and analyse both collections to work out which nucleotides are paired, and can do this for thousands of RNA types at once. \n               RNA rules \n             Chang and Segal first used PARS in the budding yeast  Saccharomyces cerevisiae  to reveal the structures of more than 3,000 messenger RNAs (mRNAs) 3 , which bear instructions for building proteins. As well as some weird and wonderful shapes, the scientists also found one of the first clues to the laws that dictate RNA structure: the regions that code for proteins generally contain more base-pairing and have more-elaborate structures than do flanking sequences known as untranslated regions. This pattern makes sense, Chang says, because untranslated regions often interact with regulatory proteins and so need to be in a more-open, accessible orientation. The pair followed this up last year with a study of human mRNA. Led by graduate student Yue Wan, the researchers looked at more than 20,000 mRNA structures from blood cell lines generated from two parents and their child, and discovered around 1,900 single-nucleotide variations in regions that do not code for protein that had altered RNA structure 4 . The question now is whether these affect what the RNAs do, or whether they are mostly background noise. At least some evidence suggests that they matter. In May, Laederach and his team reported on variants in the untranslated region of an mRNA that is linked to a rare form of eye cancer called retinoblastoma. In healthy individuals, this mRNA simultaneously adopts three structures, but in two people with the disease, nucleotide variants force the molecule to collapse into a single conformation 5 . Laederach thinks that such variations in mRNA folding could be a general mechanism of disease and a source of human variation in common traits such as height. A major limitation of the PARS method is that the enzymes used cannot easily penetrate the cell membrane, so scientists must extract the RNA from the cell and, in doing so, disrupt the native structure. In principle, base-pairing should ensure that RNAs spring back into roughly the same shape when they are allowed to refold in a test tube. But in fact, the technique strips away RNA-binding proteins, a process that can dramatically alter a molecule's structure. To get at RNA structures  in vivo , many scientists have turned to dimethylsulfate (DMS). This chemical penetrates cells, where it reacts with two of the four RNA nucleotides \u2014 adenine and cytosine \u2014 but only when they are in an unpaired state. Researchers then convert the RNA into DNA and sequence it. Any nucleotides that have been altered by DMS block the conversion into DNA, so scientists can use prematurely shortened bits of DNA to identify nucleotides that were unpaired. Weissman and his colleagues deployed this method to analyse the full complement of mRNAs in yeast and humans, both in living cells and after the molecules had been extracted and allowed to refold 2 . \u201cIt was very exciting at first because we really didn't know what the differences would be  in vivo  and  in vitro ,\u201d says Silvi Rouskin, a graduate student who worked on the project and is now at the Whitehead Institute in Cambridge, Massachusetts. Many scientists had expected to see more RNA folding inside a cell because they thought that interacting proteins would stabilize RNA structures there. But Weissman and his team saw the opposite. This, they now think, could be because mRNAs inside cells are actively generating proteins \u2014 and looser molecules are more available to the cell's protein-building machinery. Bevilacqua and Assmann saw something curious when they used the DMS approach in their study of mRNA in  A. thaliana 1 . mRNAs from genes that are involved in stress responses \u2014 ones activated during drought, say \u2014 tended to be folded more loosely inside a cell than predicted by computer modelling. By contrast, mRNAs of 'housekeeping' genes, which are involved in essential cell maintenance, mostly matched the predictions. The team proposes that stress-response RNAs are folded loosely so that they can shift shape easily inside a cell and thereby change the level of protein production in the face of changing conditions. By contrast, the housekeeping RNAs have to churn out relatively stable levels of protein. \u201cThat was just an amazing moment to see that dichotomy,\u201d Assmann says. The trouble with the DMS method is that it reveals the pairing of only two types of nucleotide, and computer modelling fills in the rest. To obtain pairing information for every letter of RNA inside the cell, Chang and Spitale adapted a structure-probing technique called SHAPE 6 . This allowed them to deduce the structures of more than 19,000 RNAs in mouse embryonic stem cells, an effort they published earlier this year 7 . The researchers showed that a common chemical modification to mRNA unfurls the molecule's structure, and they detected distinctive structural 'signatures' that predict where proteins will bind to control RNA shape. Some researchers are already mulling over ways to put these revelations to use. Assmann and Bevilacqua are probing the structures of RNAs in rice, one of the world's most important staple foods, and plan to do the same for other agriculturally important plants. They would like to find ways to manipulate RNA shapes to improve stress tolerance and ultimately crop yield. Rouskin, meanwhile, is looking at the RNAs of fruit flies to improve understanding of how these molecules' structures affect embryonic development. \u201cNow we finally have the tools,\u201d she says. \u201cAnd we can ask all these questions that we never even thought about asking.\u201d \n                     Insights into RNA structure and function from genome-wide studies 2014-May-13 \n                   \n                     Molecular biology: A second layer of information in RNA 2014-Jan-29 \n                   \n                     Life hackers seek new tools 2011-Jun-14 \n                   \n                     Evidence of altered RNA stirs debate 2011-May-25 \n                   \n                     The code within the code 2010-May-05 \n                   \n                     Genetics: The production line 2008-Aug-27 \n                   \n                     The RNA structurome: a review of structure-probing techniques \n                   \n                     Philip Bevilacqua's lab \n                   \n                     Jonathan Weissman's lab \n                   \n                     Howard Chang's lab \n                   Reprints and Permissions"},
{"file_id": "523516a", "url": "https://www.nature.com/articles/523516a", "year": 2015, "authors": [{"name": "Emily Anthes"}], "parsed_as_year": "2006_or_before", "body": "An easy method that promised to cut complications in surgery may not be so simple after all. Before making the first incision, confirm the patient's identity. Mark the surgical site. Ask about allergies. Discuss any anticipated blood loss. Introduce yourself by name. These are some of the 19 tasks on the World Health Organization (WHO) Surgical Safety Checklist, a simple list of actions to be completed before an operation in order to cut errors and save lives. In 2007 and 2008, surgical staff at eight hospitals around the world tested the checklist in a pilot study 1 . The results were remarkable. Complications such as infections after surgery fell by more than one-third, and death rates dropped by almost half. The WHO recommended that all hospitals adopt its checklist or something similar, and many did. The UK National Health Service (NHS) immediately required all of its treatment centres to put the checklist into daily practice; by 2012, nearly 2,000 institutions worldwide had tried it. The idea of checklists as a simple and cheap way to save lives has taken hold throughout the clinical community. It has some dynamic champions, including Atul Gawande, a surgeon at Brigham and Women's Hospital in Boston, Massachusetts, who led the pilot study and has spread the word through talks, magazine articles and a best-selling book,  The Checklist Manifesto  (Metropolitan, 2009). But this success story is beginning to look more complicated: some hospitals have been unable to replicate the impressive results of initial trials. An analysis of more than 200,000 procedures at 101 hospitals in Ontario, Canada, for example, found no significant reductions in complications or deaths after surgical-safety checklists were introduced 2 . \u201cWe see this all the time,\u201d says David Urbach, a surgeon at the University of Toronto who led the Ontario analysis. \u201cA lot of studies that should be a slam dunk don't seem to work in practice.\u201d The stakes are high, because poor use of checklists means that people may be dying unnecessarily. A cadre of researchers is working to make sense of the discrepancies. They are finding a variety of factors that can influence a checklist's success or failure, ranging from the attitudes of staff to the ways that administrators introduce the tool. The research is part of the growing field of implementation science, which examines why some innovations that work wonderfully in experimental trials tend to fall flat in the real world. The results could help to improve the introduction of other evidence-based programmes, in medicine and beyond. \u201cWe need to learn the lessons from programmes and interventions like the checklist so we don't make the same mistakes again,\u201d says Nick Sevdalis, an implementation scientist at King's College London. \n               Replication frustration \n             One of the first to demonstrate the potential of checklists in health care was Peter Pronovost, an anaesthesiologist and critical-care physician at Johns Hopkins University School of Medicine in Baltimore, Maryland. In 2001, Pronovost introduced a short checklist for health-care workers who insert central venous catheters, or central lines, which are often used in an intensive care unit (ICU) to test blood or administer drugs. The trial showed that asking practitioners to confirm that they had performed certain simple actions, such as washing their hands and sterilizing the insertion site, contributed to a dramatic reduction in the risk of life-threatening infections 3 . The list got a larger test in a now-famous trial 4  known as the Keystone ICU project, launched in Michigan in October 2003. Within 18 months, the rate of catheter-related bloodstream infections fell by 66%. Checklists were not completely new to medicine, but Pronovost's work attracted attention because it suggested that they could save lives. Gawande penned an inspiring feature in  The New Yorker 5 , asking: \u201cIf something so simple can transform intensive care, what else can it do?\u201d Checklists began to proliferate. Now there are checklists for procedures involving anaesthesia, mechanical ventilation, childbirth and swine flu. Many studies have generated promising results, showing that the lists improve patient outcomes in hospitals from Norway to Iran. But there have also been some failures. This January, less than a year after the report from Ontario, a different team of scientists reported 6  that a surgical checklist modelled on Pronovost's list did not improve outcomes at Michigan hospitals. And although the central-line checklist for ICUs has provided lasting benefits in Michigan, a British initiative called Matching Michigan, which aimed to replicate the Keystone programme, seemed to make no difference to infection rates 7 . Some experts suspect that the failure to replicate could be a matter of how the initial trials or the follow-up studies were designed. Gawande's pilot study of the WHO surgical checklist, for example, was not randomized and had no control group. Instead, it compared complication and death rates before and after the checklist was introduced. Critics say that this makes it difficult to determine what other factors might have influenced outcomes. Gawande acknowledges the limitation, which was due to cost restrictions, but he points out that many subsequent trials, including ones that were randomized, have also demonstrated large reductions in complications and mortality following the introduction of the checklist. The list works, he says \u2014 as long as it is implemented well. \u201cIt turns out to be much more complex that just having the checklist in hand.\u201d \n               Ticking boxes \n             Implementation scientists are trying to make sense of that complexity. After the NHS mandated the WHO checklist, researchers at Imperial College London launched a project to monitor the tool's use, and found that staff were often not using it as they should. In a review of nearly 7,000 surgical procedures performed at 5 NHS hospitals, they found that the checklist was used in 97% of cases, but was completed only 62% of the time 8 . When the researchers watched a smaller number of procedures in person, they found that practitioners often failed to give the checks their full attention, and read only two-thirds of the items out loud 9 . In slightly more than 40% of cases, at least one team member was absent during the checks; 10% of the time, the lead surgeon was missing. Going through all the steps in the list really mattered, the research showed. The more of the checklist that teams completed, the lower the complication rates. Several other studies have also revealed that higher compliance with the checklist is associated with better outcomes. \u201cIf it's used well, if it's used in the original spirit and intention with which it was designed, I think it has real potential,\u201d says Sevdalis, who was part of the Imperial College research team. \u201cIf it's used for people to tick the box and say, 'Oh yes, we've done it,' but without really thinking about the patient, without really informing their team members about aspects of the procedure that are relevant to them, I don't think the checklist will make any difference.\u201d To find out why checklists were not being used properly, Sevdalis and his colleagues interviewed more than 100 members of operating-theatre staff at 10 NHS hospitals 10 . Half of the respondents reported that senior surgeons and anaesthesiologists sometimes actively resisted the checklists, making it difficult for the rest of the team to complete the tasks. Staff also complained about the checklist itself: that it was poorly worded, time-consuming, inappropriate for certain procedures or redundant with other safety checks. Some also questioned whether there were enough data to support the checklist's use (see 'Why checklists fail'). About one-quarter of the respondents objected to how the checklist had been introduced. Although some hospitals provided training and solicited feedback from staff, at other institutions there was little involvement from those actually working in the operating theatre. That strategy might make it difficult for staff to feel invested in the checklist, and ultimately undermine its correct use. \u201cWhen it was introduced without any programme or support, it was just impossible, I think, for teams to buy into it,\u201d says psychologist Stephanie Russ, who was part of the research team and is now at the University of Aberdeen, UK. Mary Dixon-Woods, a medical sociologist at the University of Leicester, UK, interviewed staff members at 17 of the ICUs participating in Matching Michigan 11 . She found that by the time the programme began, British hospitals had already been involved in numerous government-led efforts to reduce infections. The checklist, she says, was viewed as \u201cyet another example of these top-down, intrusive, imposed initiatives\u201d. It became \u201csomething that had to be endured rather than enjoyed\u201d. In Michigan, by contrast, the tool was considered new and exciting. And it was not imposed by the government \u2014 it was organized by the well-regarded state hospital association, and participation was voluntary. Dixon-Woods did identify one exemplary ICU, in which a high infection rate fell to zero after Matching Michigan began. The unit was led by a charismatic physician who championed the checklist and rallied others around it. \u201cHe formed coalitions with his colleagues so everyone was singing the same tune, and they just committed as a whole unit to getting this problem under control,\u201d says Dixon-Woods. Other work has also found that it might be helpful to enlist local champions who can promote an intervention within a hospital, and some have hinted at how to get colleagues on board. In a 2011 study 12  of five hospitals in Washington state, Gawande and his colleagues found that it is crucial that leaders take the time to explain how to use the checklist and why it should be used. \u201cThat might have included pulling on somebody's heart strings, it might have included sharing as much evidence as possible, it might have included talking through the theoretical story or giving some important example,\u201d says Sara Singer, a health-policy researcher at the Harvard T. H. Chan School of Public Health in Boston, Massachusetts, who co-authored the study. \n               A local list \n             Experts also recommend that hospitals modify standard checklists to help the tool fit into the local workflow and to produce a feeling of investment and ownership. Pronovost encouraged the ICUs that participated in the Keystone project to make his checklist their own. \u201cThey were 95% the same, but that 5% made it work for them,\u201d he says. \u201cEvery one of these hospitals thought that theirs was the best.\u201d Pronovost and Dixon-Woods also think that several other factors contributed to the success in Michigan ICUs. Providing the hospitals with regular feedback on their infection rates created social pressure for improvement, they say, and regular in-person workshops allowed staff from different hospitals to share their experiences and created the sense of a shared mission. Beyond that, logistics are crucial. When Pronovost was first developing his checklist at Johns Hopkins, he noticed that ICU doctors had to go to eight different places to collect all the supplies they needed to perform a sterile central-line insertion. As part of the Keystone programme, hospitals assembled carts that contained all the necessary supplies. In a 2013 study 13 , Dixon-Woods found that an African hospital using the WHO surgical checklist had regular shortages of the basic tools \u2014 such as surgical markers, antibiotics and pulse oximeters \u2014 that are required to complete the list. But the staff often ticked those boxes anyway; as one anaesthetist pointed out, it was often better for a patient to undergo surgery without these supplies than not to have surgery at all. If the checklist is going to succeed in low-income settings, these problems have to be addressed. \u201cThere's no point in having an item that says, 'Have the antibiotics been given?' if there are no antibiotics in the hospital,\u201d says Dixon-Woods. The clear lesson for hospital leaders is that they cannot just dump a stack of checklists in an operating room \u2014 they must observe them being used. Are team members all present? Are they rushing, or skipping steps? If so, then the lapses should be discussed and addressed. Implementation researchers say that the checklist story may hold lessons for the introduction of other programmes in fields including medicine, education and social work. \u201cWe have this massive influx of money to develop innovations,\u201d says Dean Fixsen, who co-founded the US National Implementation Research Network at the University of North Carolina at Chapel Hill. \u201cBut the track record of getting that science into practice where it actually produces the kinds of outcomes that we want to see \u2014 that track record is abysmal.\u201d Over the past few decades, researchers have published countless papers on evidence-based literacy programmes and teaching strategies. And yet literacy rates for US nine-year-olds, for instance, have barely budged. Fortunately, Fixsen says, the lessons of implementation science are \u201ccompletely generalizable\u201d, and all programmes could benefit by noting the importance of engaged leadership, local adaptation and user buy-in. \u201cIt doesn't matter how good the innovation is, it doesn't matter how much has been invested,\u201d says Fixsen. \u201cIf we don't have the implementation savvy, we're going to get the crummy outcomes that we have seen decade after decade.\u201d \n                     How to beat HIV 2015-Jul-07 \n                   \n                     Evidence-based medicine: Save blood, save lives 2015-Mar-31 \n                   \n                     Announcement: Reducing our irreproducibility 2013-Apr-24 \n                   \n                     Safety in surgery: the checklist 2009-May-01 \n                   \n                     The WHO Safe Surgery Checklist \n                   Reprints and Permissions"},
{"file_id": "521412a", "url": "https://www.nature.com/articles/521412a", "year": 2015, "authors": [{"name": "Alison Abbott"}], "parsed_as_year": "2006_or_before", "body": "By revealing that fish cooperate, cheat and punish, Redouan Bshary has challenged ideas about brain evolution. Redouan Bshary well remembers the moment he realized that fish were smarter than they are given credit for. It was 1998, and Bshary was a young behavioural ecologist with a dream project: snorkelling in Egypt's Red Sea to observe the behaviour of coral-reef fish. That day, he was watching a grumpy-looking grouper fish as it approached a giant moray eel. As two of the region's top predators, groupers and morays might be expected to compete for their food and even avoid each other \u2014 but Bshary saw them team up to hunt. First, the grouper signalled to the eel with its head, and then the two swam side by side, with the eel dipping into crevices, flushing out fish beyond the grouper's reach and getting a chance to feed alongside. Bshary was astonished by the unexpected cooperation; if he hadn't had a snorkel in his mouth, he would have gasped. This underwater observation was the first in a series of surprising discoveries that Bshary has gone on to make about the social behaviour of fish. Not only can they signal to each other and cooperate across species, but they can also cheat, deceive, console or punish one another \u2014 even show concern about their personal reputations. \u201cI have always had a lot of respect for fish,\u201d says Bshary. \u201cBut one after the other, these behaviours took me by surprise.\u201d His investigations have led him to take a crash course in scuba diving, go beach camping in Egypt and build fake coral reefs in Australia. The work has also destroyed the stereotypical idea that fish are dumb creatures, capable of only the simplest behaviours \u2014 and it has presented a challenge to behavioural ecologists in a different field. Scientists who study primates have claimed that human-like behaviours such as cooperation are the sole privilege of animals such as monkeys and apes, and that they helped to drive the evolution of primates' large brains. Bshary \u2014 quiet, but afraid of neither adventure nor of contesting others' ideas \u2014 has given those scientists reason to think again. \u201cRedouan has thrown down the gauntlet to us primatologists,\u201d says Carel van Schaik, an expert in orang-utan culture at the University of Zurich in Switzerland. \u201cHe has made us realize that some of the explanations of primate intelligence that we have cherished don't hold water anymore.\u201d \n               Stream fishing \n             Bshary says that he was \u201cpre-imprinted to like fish\u201d. As a child in Starnberg, Germany, he played constantly in the stream at the edge of the family garden, building dams and pools and trapping fish. Passionate about animal behaviour, he studied evolutionary ecology at the University of Munich, and then did a PhD at the Max Planck Institute for Behavioural Physiology in Starnberg. But for his field work, he journeyed to the C\u00f4te d'Ivoire, where he followed tree-living monkeys and discovered that different species collaborate to reduce predator risk. His PhD supervisor, Ronald No\u00eb, thought it would be \u201cnear impossible\u201d to stalk monkeys that leap from tree-top to tree-top, but Bshary seemed to have a flair for it. On occasion, he even camouflaged himself under a leopard skin to imitate one of their predators. And he became fascinated by one question: what makes animals cooperate when standard natural selection would predict selfish behaviour to be the norm? No\u00eb, a primate behavioural ecologist now at the Hubert Curien Multidisciplinary Institute in Strasbourg, France, had come up with a biological market-based theory of cooperation. It proposed that animals cooperate to trade a specific commodity \u2014 such as food \u2014 for a service that would promote their survival, such as protection from a predator 1 . \u201cAn attractive theory \u2014 but, there were no strong data to support it,\u201d says Bshary. He looked around for a system where market forces might be operating. And he found one when Hans Fricke, a fish ecologist working at the Max Planck institute, told him the strange tale of barrier-reef fish that operate a remarkable system of cooperation. 'Cleaner' fish, such as the brightly striped wrasse, will nibble parasites off the skin of 'client' fish in small coral territories known as cleaning stations. Bshary realized that this provided a perfect situation in which to test the market theory because client fish seemed to be trading food \u2014 in the form of parasites \u2014 for a skin-cleaning service. He decided to follow his hunch and study the coral-reef fish. There was one small problem: Bshary had never been scuba diving. He took his first lessons during a snowy winter in Lake Starnberg, then set off for the Red Sea, setting up camp in Ras Mohammed National Park in Egypt. Together with a few students, Bshary spent two full months a year camped on a scorching beach, sleeping under the stars, eating a diet of fruit and vegetables and doing four exhausting, 75-minute dives a day. \u201cIn the mornings he would wake up and immediately put on his wetsuit and jump straight into the sea,\u201d recalls former student Erica van de Waal, now a research fellow at the University of Zurich. Armed with a plastic underwater writing slate, a pencil and a stopwatch, he shadowed client fish, observing their interactions with the wrasse cleaner fish \u2014 and soon collected evidence of a well-functioning market. \u201cFor me this system was a gold mine,\u201d Bshary says, and he mined a lot of gold. He discovered, for example, that fish did not just trade parasites for skin cleaning; the cleaner fish also cheated on the deal. Rather than eating parasites, they actually preferred the nutritious protective mucus that covers fish skin, and were constantly tempted to take a quick, illicit bite of it. Bshary could count how often this happened \u2014 and therefore whether the clients were getting a good or a bad cleaning service \u2014 because the clients gave a jolt when they were bitten. The market theory predicted that if there were lots of clients around, the cleaners would enjoy a seller's market and would risk taking more bites of mucus. This is just like a mechanic getting away with shoddy car services when there are no competing businesses in town. Bshary found this to be true, and he also found that the buyers could protest. Because some client fish roam large territories, they could choose to boycott any cleaning stations that deliver a bad service \u2014 just as someone who received a poor car service might travel farther to find a better garage 2 . While racking up evidence for the market theory, Bshary also observed a range of other social behaviours that had never been seen before in fish. He saw that unsatisfied clients sometimes punish cheating cleaners by chasing them around, and that this punishment makes these fish less likely to cheat 3 . He saw cleaners ingratiating themselves with certain clients: they gave preference to visiting fish such as groupers, rather than the smaller, local fish that did not have the option of going elsewhere. He found that the cleaners cheated less when they were being watched by other potential clients \u2014 a sign that they were buffing their reputations 4 . And he saw reconciliation: if cleaners behaved badly, they then massaged the backs of offended clients with their pelvic fins 5 . It was all adding up to a catalogue of behaviours worthy of Niccol\u00f2 Machiavelli's  The Prince  \u2014 but it was based on observation alone. Bshary needed to move to an experimental set-up where he could test how the fish behaved. And so in 2003, he began experiments at Lizard Island Research Station on Australia's Great Barrier Reef. He was employed, however, on the other side of the world: first at the University of Liverpool, UK, and now at the University of Neuch\u00e2tel, Switzerland. \u201cIt was not difficult to sign up to a lifetime of fieldwork at warm coral beach locations,\u201d he admits. Over the next few years Bshary would capture fish in the wild reefs, house them in tanks for the duration of his experiments, then release them. He simulated the choice that cleaners make between parasites and mucus by building moveable plastic plates smeared with prawn, which the fish love, and fish flakes, which they enjoy less. In this set-up, the plates may be snatched away if the cleaners go for the prawns \u2014 just like a client fish may swim away if its mucus gets bitten too often. So the cleaners learned to cooperate and eat fish flakes instead. Such experiments take patience: some fish take a month just to adjust to the tanks. But in this way, Bshary proved that all the behaviours he had observed in the wild could be repeated under experimental conditions. And he discovered even more bizarre facts about the social lives of fish. In one experiment, he showed that when cleaners work in male and female pairs, as frequently happens in the wild, they are much less likely to cheat than when they work alone 6 ; and that this is mostly because the female gets punished by being chased around by the male if she slacks off 7 . Perhaps the laboratory's most imaginative experiment involved the construction of an entire fake coral reef, complete with dummy eels. The job fell to doctoral student Alex Vail, who glued together bits of coral rubble and put them in a hot-tub-sized tank. Vail then made models of moray eels by printing, gluing together and laminating two life-sized photographs, and attaching nylon strings that allowed him to pull the fake eels out of the fake coral, like a puppet. (Vail subsequently went on to a successful career in underwater filming.) Using this set-up, the team explored the behaviour that so shocked Bshary when he observed it in 1998: a grouper and eel teaming up to flush out fish to eat. They showed that the grouper quickly learnt to signal \u2014 by turning and shaking its head \u2014 only to those moray eels that responded by moving towards, rather than away from, the fake reef. Bshary amassed ample evidence that fish engage in a range of social behaviours, and he assumed that all of them resulted from simple evolution at work. Natural selection favoured fish that could learn, by simple association, which choices allowed them to efficiently rid themselves of parasites or access food. By 2010, Bshary's thoughts were turning back to the world of primatology, in which he had been immersed during his PhD. He knew that he had observed in fish many of the behaviours that primatologists had shown in monkeys and apes. But primatologists had made grander claims for their observations. The 'social brain' theory argues that primates evolved brains that are large for their body size to manage their unusually complex social systems. Only primate brains, the theory says, have the depth of cognitive analysis necessary to cooperate, deceive and solve other problems in a social world. Bshary disagreed. Maybe, he thought, these particular social behaviours in primates were also learnt by simple association and did not require the extra computing power of their big brains. And his findings meshed with those emerging from studies on the social behaviours of other animals, ranging from elephants to birds. \u201cI think primatologists tend to make big claims because they look up the evolutionary chain and compare the primates' behaviours to humans, instead of looking down the evolutionary chain to see if the phenomena also existed in lower species,\u201d he says. At the time, primatologists were certainly not looking at fish. But that changed when Bshary teamed up with primatologist Sarah Brosnan at Georgia State University in Atlanta to directly pit the skills of cleaner fish against capuchin monkeys, chimpanzees and orang-utans in a foraging test. Each animal was presented with food on two differently coloured plates, one of which was a permanent fixture in their tanks or pens, whereas the other was temporary. The challenge was to learn to eat from the temporary plate first, before it disappeared \u2014 and the scientists counted how many trials it took for the animal to figure this out. The cleaner fish solved the problem first 8 ; they have evolved in their ecological niche to preferentially feast from visiting clients before they disappear. For fun, Bshary set up an equivalent 'foraging' test for his four-year-old daughter, complete with temporary and permanent plates, each bearing one chocolate M&M. In a series of 100 different trials, she never learnt to eat from the temporary plate. The fish, meanwhile, were already aceing a more advanced test. When Bshary and Brosnan switched the coloured plates so that the permanent one suddenly became temporary and vice versa, the fish again understood the switch faster than the apes did (and equally as fast as the capuchins) 8 . This is known as reversal learning \u2014 and when the primatologists read that result, they took note. \u201cReversal learning has often been touted as the gold standard of general cognitive abilities,\u201d says van Schaik \u2014 a sophisticated skill that correlates with brain size. \u201cSince small-brained fish do it quite well, maybe we'll have to abandon this idea.\u201d  He would wake up and immediately put on his wetsuit and jump straight into the sea.  \u201cThe ball is in our court,\u201d says evolutionary psychologist Robin Dunbar of the University of Oxford, UK, who developed the social brain theory. Dunbar now accepts that the evolution of large brains was not driven by the need to carry out single 'smart' behaviours such as cooperation or deception. But that doesn't mean the social brain theory has to be abandoned, he says \u2014 just refined. He and other primatologists now propose that primates evolved bigger brains because they needed an all-round high level of general intelligence to survive the pressures of living in tight social groups \u2014 for example, to recognize large numbers of individuals and remember their complicated genetic and hierarchical relationships. Fish, which tend to have one-on-one interactions and live in loose schools, do not need to multi-task in quite the same way, Dunbar says. \u201cIt may boil down to the speed of cognitive processing and accuracy of judgement,\u201d he suggests. \n               Intelligence tests \n             Michael Tomasello, an evolutionary psychologist at the Max Planck Institute for Evolutionary Anthropology in Leipzig, Germany, bounces the ball right back to Bshary, challenging him to show how smart fish really are. \u201cPerhaps the most pressing question is how flexible and general fish cognition is,\u201d he says \u2014 something Bshary is already testing by designing further fish intelligence tests. The mysteries of the fish brain deepened in 2009, when Bshary's team chanced across a habitat in the reefs around Lizard Island that had relatively few fish and therefore less competition and social complexity. To Bshary's surprise, the cleaner fish there turned out to be much less socially smart than cleaner fish just 20 metres away 9 . But their skill level may be optimal for their environment \u2014 another hypothesis that he now plans to explore. Whatever the next instalment brings, colleagues say that Bshary has already shifted a view of animal cognition in which humans and their primate cousins tower over everything else. \u201cPrimate chauvinism may now be poised to decline, thanks in large part to Bshary's fish work,\u201d says primatologist and ethologist Frans de Waal of Emory University in Atlanta, Georgia. \u201cThey now really do have to take on board that most species are going to have a type of smart intelligence.\u201d \n                     Neuroscience: The rat pack 2010-May-19 \n                   \n                     Science at the movies: The fabulous fish guy 2004-Feb-19 \n                   \n                     Lizard Island Research Station \n                   Reprints and Permissions"},
{"file_id": "523396a", "url": "https://www.nature.com/articles/523396a", "year": 2015, "authors": [{"name": "Quirin Schiermeier"}], "parsed_as_year": "2006_or_before", "body": "Climate change is a major threat to food production, so researchers are working with farmers to make agriculture more resilient. When Frank Untersmayr was growing up near Amstetten, Austria, he saw his father wait until the soil warmed up at the end of April to plant maize. \u201cBut the climate here has got a lot warmer since, so we can now often begin to sow before mid-April,\u201d says Untersmayr, now 44 and a farmer himself. \u201cThat's good because it means that maize, which in our climate doesn't fully ripen, has two weeks longer to grow.\u201d But more changes are coming, which is why Untersmayr and half a dozen other farmers from the region gathered at the local chamber of agriculture on a rainy day in May. They met to talk to scientists about how increasing temperatures and shifts in precipitation might affect agriculture in their area \u2014 and how farmers might need to adapt. Martin Sch\u00f6nhart, an agro-economist at the University of Natural Resources and Life Sciences in Vienna, presented preliminary forecasts for average agricultural yields in 2040. Some crops and fruit benefited from the amount of warming expected. But the yields of other crops \u2014 including maize \u2014 decreased by up to 20% because changes in precipitation and extreme weather events wiped out the benefits brought by warmer temperatures. Hearing such negative projections, some farmers shook their heads in disbelief. \u201cI would rather trust my own experience than any such forecast,\u201d said Untersmayr. His reaction reveals the communication gap that has long separated scientists from farmers in planning for climate change. \u201cThere is a deep divide between the science and its supposed end-users,\u201d says Nora Mitterb\u00f6ck, who oversees climate-change adaptation policies at the Federal Austrian Ministry for Agriculture and the Environment in Vienna. \u201cThere is no lack of climate-impact research, but very little of it arrives on the farm. It's a sad situation that must absolutely change.\u201d Around the world, scientists, farmers, agricultural companies and governments are struggling to make agricultural systems more 'climate smart', which will be necessary if they are to feed the ever-swelling global population. Some are working in the short term to make today's farms more resilient. Others are looking further ahead to provide the information required for making major changes, such as investing in large irrigation systems. Sch\u00f6nhart's work is part of a \u20ac14-million (US$15-million) programme called Modelling European Agriculture with Climate Change for Food Security (MACSUR), which aims to help European nations to prepare and adapt to climate change. Another international programme, the Agricultural Model Intercomparison and Improvement Project (AgMIP), is bringing together hundreds of researchers to inform policy-makers in developing countries, as well as agricultural extension agencies, which aid farmers. Meetings such as the one in Amstetten are a key part of this work. For climate-adaptation programmes to succeed, researchers need to learn from farmers and agricultural officials what kind of information will help them the most, says Anne-Maree Dowd, a social scientist with the Commonwealth Scientific and Industrial Organisation in Kenmore, Australia. \u201cScientists tend to think primarily in terms of publications as the main reward for their work,\u201d she says. \u201cWhen it comes to climate-change adaptation, they need to thoroughly switch their mindsets and first think about the overall practical goal of what they are doing.\u201d \n               Adapt to survive \n             Farmers worldwide produce more than 1 billion tonnes of maize annually, along with some 750 million tonnes of rice, more than 700 million tonnes of wheat and nearly 2 billion tonnes of sugar cane. Despite all this, more than 800 million people go hungry each year. Even without climate change, agriculture will face enormous pressure as the global population swells from 7 billion to perhaps 9 billion by 2050. A new rice strain could ease methane emissions \u2014 and help to feed more people. Adam Levy finds out more about this climate-friendly crop. Changing rainfall and temperature patterns will cause added stress for farmers, particularly in poorer countries, if heatwaves, droughts and extreme storms become more common, as is expected in many areas 1 . Agricultural forecasts are notoriously difficult because they face multiple tiers of uncertainty: in how climate will change regionally, in assumptions about what crops might be planted, in the availability of fertilizers and in economic projections. But last year, a comprehensive study 2  that used multiple climate and agriculture models forecast that problems from climate change would generally outweigh the benefits for wheat and maize production in low-latitude regions, where developing countries are concentrated. Another study 3  analysed 1,700 simulations and projected that without adaptation efforts, yields of maize, wheat and rice will decline in both temperate and tropical regions if temperatures rise by 2 \u00b0C. One of the first steps towards building the agricultural systems of the future is helping farmers to deal with today's weather extremes. Crop developers, for example, are breeding varieties that can tolerate floods, droughts or increased salinity caused by rising sea levels. Millions of farmers in low-lying parts of India, Nepal and Bangladesh are now growing a rice variety developed by the International Rice Research Institute (IRRI) in Los Ba\u00f1os, Philippines, that can survive floodwaters better than traditional types of rice. Flood-tolerant varieties have raised yields of temporarily submerged fields by up to 45% and have helped to avert food shortages after major floods in southeast Asia, according to the IRRI. Digital communication tools also provide opportunities to protect yields and safeguard farmers' incomes. An app developed by the IRRI allows regional agricultural offices to send farmers recommendations on when to apply fertilizers and when to harvest, based on weather and local soil conditions. In the first 6 months of 2015, the app sent 170,000 recommendations. Average yields for those who used the tool have increased by about half a metric tonne per hectare \u2014 almost 10%, says Matthew Morrell, head of research at the IRRI. Customized real-time advice is expected to become even more important as farmers try to keep up with new weather patterns. Successful adaptation will also require bigger steps over the next few decades. In some regions, farmers might need to switch from irrigating crops to using semi-arid techniques, or might even have to abandon some land. Governments might choose to invest in expensive irrigation systems; in May, for example, Australia decided to fund projects totalling AUS$65 million (US$48 million) to irrigate the drought-struck Murray\u2013Darling river basin, which produces one-third of the nation's food. Most developed nations have already started planning for the long term by developing comprehensive adaptation strategies. Austria's scheme lists more than 130 measures to make the country's economy climate-fit. In the agricultural sector, the proposed measures range from diversifying crops to letting fields go fallow and reducing tillage of soil to fight erosion. But it has been a struggle to get farmers to implement some of these recommendations, says Mitterb\u00f6ck. \u201cFarmers seek to be profitable in the very near-term. From their perspective, 2040 is light years away.\u201d Successful adaptation in agriculture, she says, requires all relevant stakeholders to be involved in the scientific process so that farmers can get the information and incentives that they need. Most climate impact and adaptation studies so far have failed to take into account the complexity of modern farming, says Holger Meinke, director of the Tasmanian Institute of Agriculture in Hobart, Australia. \u201cAdaptation research must be a cross-cutting affair because hard-nosed decisions are never solely based on climate-change considerations.\u201d In Amstetten, farmers could not agree more. \u201cWe practise adaption all the time, but we mainly adapt to food prices and subsidy programmes and to modern machinery,\u201d says Untersmayr. \u201cAnd of course we must constantly adapt to the weather, no matter if the climate is changing or not.\u201d Governments and researchers are starting to listen. In Australia, scientists involved in a national climate-adaptation initiative are regularly consulting farmers about their problems with, for example, weed management, and how science might be able to solve them. Developing nations have fewer resources to plan for the future, but AgMIP scientists are reaching out to farmers and stakeholders in 20 countries in Africa and South Asia. Launched in 2010, the \u20ac15-million programme is combining information drawn from climate projection and crop and economic models with empirical data collected in the field by 7 regional teams. To account for disagreements between models, AgMIP researchers aim to develop an optimistic and a pessimistic agricultural scenario for future conditions in each region. Over the next five years, they will advise local planners on how climate change may affect farmers in their region, and which social groups and farm types are most vulnerable. That will greatly help adaptation planning in poorer countries, says Dumisani Mbikwa Nyoni, an agricultural extension officer in Zimbabwe's Matabeleland North Province who took part in a meeting in June in Victoria Falls, Zimbabwe, with an AgMIP regional research team. \u201cClimate change is causing drought in our country,\u201d he says. \u201cSo we need to identify crop varieties that can stand dryness and inadequate soil moisture, and we need to know what other options exist that will sustain our farmers. I hope science will help us do all that.\u201d The information from AgMIP can also help officials in Zimbabwe decide where to put a planned 15,000 hectares of irrigation systems over the next 3\u20135 years, he says. AgMIP is determined to provide the kind of information that will make a difference, says Cynthia Rosenzweig, a climate-impact researcher at the NASA Goddard Institute for Space Studies in New York City and a principal investigator of the project. \u201cIt is utterly important that planners in each region and each locality will have all the knowledge in place that they need,\u201d she says. \u201cThere are no dumb farmers, but farmers focus on present realities. We must leave no stone unturned to help them plan for a hotter future.\u201d See Editorial  page 381. \n                     The political economy of climate adaptation 2015-Jun-24 \n                   \n                     Climate-adaptation effort cuts hunger in African villages 2015-Mar-13 \n                   \n                     Earth systems: Model human adaptation to climate change 2014-Aug-27 \n                   \n                     Farmers dig into soil quality 2013-Oct-29 \n                   \n                     Adapting to a changing climate 2013-Oct-29 \n                   \n                     Adapting to a warmer world: No going back 2012-Nov-28 \n                   \n                     MACSUR \n                   \n                     AgMIP \n                   Reprints and Permissions"},
{"file_id": "522406a", "url": "https://www.nature.com/articles/522406a", "year": 2015, "authors": [{"name": "Ewen Callaway"}], "parsed_as_year": "2006_or_before", "body": "When the Francis Crick Institute opens in London this year, it will be Europe\u2019s largest biomedical research centre. Can director Paul Nurse make this gamble pay off for UK science? Fifteen years ago, Paul Nurse had what he now calls \u201ca really stupid idea\u201d. The Nobel-prizewinning geneticist proposed bringing two of London's most esteemed biomedical-research centres under one big tent. Literally. He suggested building a megalab inside the Millennium Dome, a 1-kilometre-round, 52-metre-high open-plan exhibition centre with a plastic and fibreglass roof on the Greenwich Peninsula in southeast London. \u201cIt was crazy, because it was not built as a research institute,\u201d says Nurse. \u201cIt got nowhere.\u201d Actually, Nurse's idea did go somewhere: about 10 kilometres northwest, right in the heart of London. In November, he will open the doors to the Francis Crick Institute, a lab even more ambitious than he originally conceived. When the \u00a3700-million (US$1.1-billion), 93,000-square-metre institute reaches full capacity by 2021, it will house some 1,600 scientists and support staff, making it Europe's single largest biomedical lab. Vast expectations come with that size. Nurse and the UK government have advertised the Crick as a boon to British science \u2014 one that will generate groundbreaking discoveries, lure the world's most brilliant young researchers to the United Kingdom and boost the British life-sciences industry. And it will do so while eschewing departments, permanent positions and any sort of scientific focus. \u201cIt is quite a different sort of beast,\u201d says Nurse. \u201cI sometimes think not everybody realizes what's been put together here.\u201d If it works, this eccentric experiment in scientific infrastructure could guide how other countries form their labs of the future. But some question whether the Crick will have been worth the risk. They warn that the centre could become a black hole that devours public and private research funding, while widening the gap between the haves and have-nots of UK science. \u201cIt is a big, big, big, enormous whale of an investment,\u201d says James Wilsdon, a science-policy researcher at the University of Sussex in Brighton, UK. \u201cIt has to succeed. If it doesn't, what do we do?\u201d \n               Big ambitions \n             The Crick was born of necessity. By the late 1990s, two of the United Kingdom's most esteemed biomedical-research institutions needed to find new homes. The pre-Second World War building that housed the National Institute for Medical Research (NIMR), in Mill Hill, north London, was showing its age. So were some lab facilities supported by the Imperial Cancer Research Fund, a charity later renamed Cancer Research UK (CRUK). Nurse was head of that charity when he proposed the Millennium Dome megalab. But rather than pursue that dream, he left England in 2003 to lead the Rockefeller University in New York City. Nurse's crazy idea lingered nonetheless. An influential 2006 government review, the Cooksey report, concluded that the country was \u201cat risk of failing to reap the full economic, health and social benefits that the UK's public investment in health research should generate\u201d, and called for increased collaboration between funders and greater innovation in biomedical research. So when then-NIMR head Keith Peters revived Nurse's idea of merging the two institutions, it gained traction. Peters settled on a plot of land just north of central London, between the British Library and St Pancras railway station. To pay for the new centre, he brought in nearby University College London and the Wellcome Trust biomedical charity; these were later joined by King's College London and Imperial College London, which contributed \u00a340 million each to construction costs. The universities will also house researchers at the Crick,  especially physical scientists and engineers , and give the institute access to their associated hospitals. The Crick, which is named after the co-discoverer of the structure of DNA, is billed as a basic-research institute. However, its funders are eager to see its discoveries turn into treatments. Researchers studying everything from stem-cell development to influenza will have access to drug-screening robots, high-powered microscopes and up to 200,000 animals, mostly mice and zebrafish. The institute will evolve in its early years. The 90 or so group leaders currently at the NIMR and CRUK's London Research Institute (LRI) will be its first occupants when they start moving in early next year. The rest of the roughly 120 group-leader slots will go to young researchers recruited to work at the Crick for a maximum of 12 years \u2014 a 6-year initial stint and a 6-year extension, subject to a positive review. Researchers from the NIMR and the LRI who already have permanent posts will keep them, but as they retire, the proportion of tenured group leaders will shrink to around one-third of group leaders. The regular staff turnover, says Nurse, will free the Crick to follow the latest research trends, be they stem cells or genome editing. \u201cA constantly rejuvenating institute stops things from becoming too inward-looking and ossified.\u201d \n               Lab as experiment \n             \u201cGentle anarchy\u201d is how Nurse has described his vision for the Crick. Researchers there will not belong to departments or divisions, and their location in the super-lab will depend on the core facilities they tend to use, from drug-screening robots to cell sorters. In Nurse's vision, researchers will form grass-roots interest groups on topics such as the microbiome. \u201cMost people think it's crazy \u2014 you have no sorts of structures,\u201d says Nurse. \u201cI think that it's very exciting and invigorating because the structure comes bottom up.\u201d Tom Cech, a Nobel-prizewinning biochemist at the University of Colorado Boulder and former president of the Howard Hughes Medical Institute (HHMI) in Chevy Chase, Maryland, says that science funders in other nations will watch the Crick closely. He has some experience with unusual ventures. During Cech's tenure, the HHMI opened the Janelia Farm Research Campus, a neuroscience institute near Ashburn, Virginia, that kept group sizes small, jettisoned tenure and emphasized interdisciplinary research. \u201cIf nobody does these experiments, the question remains hypothetical. Experiments like Janelia Farm, like the Crick, are important for society,\u201d says Cech, because they can inspire new ways of doing science. The Crick's supporters are also counting on the institute to be an economic engine for Britain by bridging the gap between the lab and market. Nurse's number two, chief operating officer David Roblin, was head of European research and development at pharmaceutical giant Pfizer until 2011, and has since worked for smaller biotechnology companies. \u201cAlthough I'm an industrial scientist, my instructions are not to make it a pharmaceutical company,\u201d Roblin says. \u201cThe Crick is a discovery research institute that is very interested in translation.\u201d Roblin is trying to lure pharmaceutical companies to place researchers at the Crick in the hope of speeding the transition to the clinic. But unlike at a private company, any developments would be openly reported. And the intellectual property would belong to the Crick. Even in a city filled with glass and steel megastructures, the Francis Crick Institute is an imposing landmark. Nurse, who was knighted in 1999, says that his employees sometimes call it \u201cSir Paul's cathedral\u201d. Perched on the edge of the city's revitalized King's Cross area, the building dwarfs the neighbouring British Library \u2014 the biggest public structure erected in Britain in the twentieth century. The Crick's twin, curved roofs give the appearance of an overturned ship, its hull split in two. The institute feels even larger inside, where hard hats, safety glasses and steel-toed boots were still mandatory as of May. Hallways of glass-fronted open-plan labs stretch almost the length of two football fields on either side of an expansive atrium, over several lofty floors. To orient people, the walls are coloured by floor and a smartphone app is being developed (\u201cI think we're blue. I can't remember,\u201d says one scientist who will move her lab to the Crick next year). Its four basement levels will house sensitive equipment such as electron microscopes, shielded by concrete slabs  to block the ever-present vibrations from nearby train and tube lines . Construction has begun on a high-containment lab for working on influenza and other deadly pathogens, as well as facilities for a menagerie of lab animals, including the only collection of opossums used for research in the United Kingdom.  Like many new laboratories, the Crick is designing spaces to encourage interactions between researchers. Each above-ground floor centres around meeting rooms, tea and coffee stations and 'collaboration spaces', meant to increase the odds of fruitful encounters. \u201cThat's one of the things I'm most excited about,\u201d says James Briscoe, a developmental biologist who is moving to the Crick from the NIMR. He has already met the group leaders with whom he will soon share bench space and equipment. Caetano Reis e Sousa, an immunologist coming from the LRI, says that he, like most other principal investigators, already works with colleagues elsewhere. It is the graduate students, postdocs and technicians who will benefit most from the Crick's social engineering, he says. \u201cThey are the ones who will establish collaborations and connections that will allow new things to be done.\u201d However, not everyone buys the argument that a giant fish bowl is needed for immunologists to have tea with physicists, the kind of interaction embodied by an unofficial mantra in many Crick documents and videos: \u201cdiscovery without boundaries\u201d. The phrase grates on some researchers. \u201cThe assumption is, previously, discovery had terrible boundaries,\u201d says Kieron Flanagan, a science and technology policy researcher at the University of Manchester, UK. \u201cAnd now that they're all in one building, suddenly the discoveries will flow. It's just bollocks.\u201d The Crick's size is another sticking point. \u201cEverybody believes that science has to get bigger and more capital intensive,\u201d says Flanagan. \u201cThere's a good bit of evidence that says small groups are more productive than large ones.\u201d And its location presents challenges. Building the Crick in one of Europe's biggest cities will be crucial to convincing the best young researchers from around the world to move to the United Kingdom, says Nurse. The city's hospitals offer a population of potential clinical-trial participants not matched anywhere else in the United Kingdom. But the institute has little room to expand, and has built several facilities off site, including a data centre and animal-breeding facility. A planned trans-London underground railway line, proposed to pass near the site, could overwhelm the vibration-dampening protections and jiggle the institute's sensitive scientific instruments (see  Nature   518 , 464\u2013465; 2015 ). And then there is the cost of living in London, which has skyrocketed since plans for the Crick were unveiled. Nurse had hoped to buy a block of flats for postdocs and graduate students, but he could not find the money. He is now working on smaller measures to make London more affordable for Crick scientists. Gerald Rubin, executive director of the Janelia campus, sees the lack of subsidized housing as the institute's only weakness \u2014 but worries that it could hamper recruitment. \u201cIf the Crick had that, they would probably rapidly become the premier research institute in Europe. If they don't, I think they'll struggle,\u201d he says. \n               Ripple effects \n             Nurse says that the Crick will serve as a scientific incubator for the United Kingdom: he hopes that researchers leaving the Crick will take their talents to universities throughout the country. \u201cI say half joking that they should fall in love with somebody who lives here,\u201d he says. Critics, however, say that Nurse is being glib about the serious economic challenges faced by young Londoners (a postdoc or even principal investigator's salary will not buy a flat anywhere near the institute). And they question whether the Crick will be the incubator that Nurse and the government contend. \u201cI don't see it as being possible to define that as a strategy for the institute \u2014 to populate the UK,\u201d says David Stephens, a cell biologist at the University of Bristol. \u201cIt could, but there's no de facto reason why it will.\u201d Researchers could just as well move to world-class centres in the United States, Germany or Asia, say Stephens and others. Some worry that the Crick will become 'too big to fail' \u2014 that its public and private funders will put the centre ahead of other priorities. Beyond its \u00a3700-million construction costs, the institute will have an annual budget topping \u00a3150 million per year (see 'Vital signs'), which will be paid by the government's Medical Research Council (MRC), CRUK and the Wellcome Trust. A flat or even diminished government science budget \u2014 increasingly likely after the election this year of a Conservative government that promises cuts \u2014 has amplified such concerns. \u201cTaking a large scoop out of the pot for one institute makes it harder for the rest of us to attract funding,\u201d says Stephens, whose lab is supported by the MRC and the Wellcome Trust. Meanwhile, Nurse is currently leading a government review of how the UK research councils (which include the Crick's major funder, the MRC) divvy up money, raising conflict-of-interest concerns. Nurse says that such complaints are unfounded \u201cscare stories\u201d from scientists who are \u201cterrified about their own funding\u201d. He sees the scale of the Crick as a strength, which will help the institute in terms of both conducting world-leading research and securing financial support. \u201cHaving a single large institute, which has been invested in and where people have got a lot of skin in the game, is indeed a good encouragement to make the thing work,\u201d he says. \u201cBut it means we have a responsibility to deliver.\u201d But deliver what? Gauging the ultimate success or failure of the Crick will be difficult. The researchers who will soon move into the institute are already leading scientists, and will continue to produce high-level work. But that is not enough. Cech says that the Crick should be judged on whether it contributes to discoveries that could not have been made by university researchers with the same funds. Wilsdon agrees: \u201cIt's going to have a lot of resources in it. It needs to do more than marginally outperform its competitors.\u201d Nurse is another question mark. He plans to helm the Crick for another five or six years before stepping aside as director. Matthew Freeman, a molecular biologist at the University of Oxford, UK, wonders whether a successor will be able to carry his vision forward. \u201cMy sense is this is very much designed in a way Paul wanted it to be. He's put a lot of his own personality and weight and charisma behind establishing this rather unusual model,\u201d says Freeman. \u201cI see this as an exciting experiment, but experiments can fail as well as succeed.\u201d Nurse concedes that the Crick's loftier goals \u2014 creating a nimble, highly collaborative basic-research institute that powers Britain's knowledge economy \u2014 are less than certain. \u201cI think we've got a reasonable chance,\u201d he says. But he is open to change. \u201cWhat if my lack of departments is chaos? If it's chaos, I'll put the white flag up and say, right, we'll do something else,\u201d he says. \u201cI am not a zealot.\u201d \n                     Francis Crick Institute raises alarm about train line 2015-Feb-25 \n                   \n                     UK scientists fear further cuts 2013-Jun-11 \n                   \n                     London biomedical hub sets its research agenda 2013-Jun-06 \n                   \n                     London's 'somewhat unusual' new research centre 2010-Jun-18 \n                   \n                     London's biomedical research institute takes shape 2009-Dec-07 \n                   \n                     Francis Crick Institute \n                   Reprints and Permissions"},
{"file_id": "522410a", "url": "https://www.nature.com/articles/522410a", "year": 2015, "authors": [{"name": "Helen Shen"}], "parsed_as_year": "2006_or_before", "body": "As researchers work out how oxytocin affects the brain, the hormone is shedding its reputation as a simple cuddle chemical. In April 2011, Robert Froemke and his team were reprogramming the brains of virgin mice with a single hormone injection. Before the treatment, the female mice were largely indifferent to the cries of a distressed baby, and were even known to trample over them. But after an injection of oxytocin, the mice started to respond more like mothers, picking up the mewling pup in their mouths. Froemke, a neuroscientist at New York University's Langone Medical Center in New York City, was monitoring the animals' brains to find out why that happened. At first, the mice showed an irregular smattering of neural impulses when they heard the baby's cries. Then, as the oxytocin kicked in, the signal evolved into a more orderly pattern typical of a maternal brain. The study showed in unusual detail how the hormone changed the behaviour of neurons 1 . \u201cOxytocin is helping to transform the brain, to make it respond to those pup calls,\u201d Froemke says. Oxytocin has been of keen interest to neuroscientists since the 1970s, when studies started to show that it could drive maternal behaviour and social attachment in various species. Its involvement in a range of social behaviours 2 , including monogamy in voles, mother\u2013infant bonding in sheep, and even trust between humans, has earned it a reputation as the 'hug hormone'. \u201cPeople just concluded it was a bonding molecule, a cuddling hormone, and that's the pervasive view in the popular press,\u201d says Larry Young, a neuroscientist at Emory University in Atlanta, Georgia, who has been studying the molecule since the 1990s. That view has led some clinicians to try oxytocin as a treatment for psychiatric conditions such as autism spectrum disorder. But the early trials have had mixed results, and scientists are now seeking a deeper understanding of oxytocin and how it works in the brain. Researchers such as Froemke are showing that the hormone boosts neuronal signals in a way that could accentuate socially relevant input such as distress calls or possibly facial expressions. And clinical researchers are starting a wave of more ambitious trials to test whether oxytocin can help some types of autism. The work is leading to a more sophisticated view of the hormone and its complex effects on behaviour \u2014 one that will take many types of expertise to refine. \u201cThe oxytocin field has just matured and ripened enough to draw in researchers from traditionally separate fields, catapulting this forward,\u201d says Young. \n               Birth accelerator \n             Oxytocin's story starts back in the early 1900s, when biochemists discovered that a substance from the posterior pituitary gland could promote labour contractions and lactation. When scientists later discovered the hormone responsible, they named it oxytocin after the Greek phrase meaning 'rapid birth'. Oxytocin is produced mainly by the brain's hypothalamus; in the 1970s, studies revealed that oxytocin-producing neurons send signals throughout the brain, suggesting that the hormone had a role in regulating behaviour. In a landmark 1979 study 3 , Cort Pedersen and Arthur Prange at the University of North Carolina in Chapel Hill showed that giving oxytocin to virgin rats could trigger maternal behaviours: the animals would build nests, lick or crouch over unfamiliar pups and even return lost pups to the nest. Researchers went on to show that oxytocin signalling in the brains of prairie voles ( Microtus ochrogaster ) helps the animals to form lifelong pair bonds 4  \u2014 a rarity among mammals. In 2012, researchers even found a version of oxytocin in the tiny roundworm  Caenorhabditis elegans , where it helps the animals find and recognize mates 5 . \u201cThis is a very ancient molecule,\u201d says Sue Carter, a neuroscientist at Indiana University in Bloomington, whose lab pioneered many of the early studies of oxytocin in voles. \u201cIt has been used and reused for many purposes across the evolution of modern animals, and almost everybody who's tried to look at an effect of oxytocin on anything like social behaviour has found something.\u201d In mammals, many mysteries remain. Oxytocin is difficult to measure reliably in the brain, making it hard to know exactly where, when and how much is normally released; nor do scientists understand precisely how it works to alter behaviour. \u201cWhat we need to start thinking about is the more fundamental role that oxytocin plays in the brain,\u201d Young says. The determination to find out has been strengthened by a growing move in neuroscience to characterize circuits that are important in brain operations. \u201cThat's the level that's critical for understanding how the brain is regulating behaviour,\u201d says Thomas Insel, director of the US National Institute of Mental Health in Bethesda, Maryland, who has studied oxytocin in voles. At Langone, Froemke focused on the circuits underlying the maternal response to pup cries \u2014 a behaviour that helps females to retrieve helpless newborns that can get lost when a mother is moving her nest. He focused on the left auditory cortex, a brain area thought to be involved in detecting the pups' ultrasonic cries. Froemke's study 1 , published in April, showed that oxytocin temporarily suppresses inhibitory neurons \u2014 those that dampen neural activity \u2014 which allows excitatory cells to respond more strongly and reliably. \u201cOur hypothesis is that the virgin brain is a blanket of inhibition, and that pairing the pup calls with oxytocin allows the network to be reconfigured,\u201d says Froemke. The hormone may serve to amplify incoming signals and allow them to be recognized as behaviourally important. (It is at least possible, he says, that this same mechanism could explain why some human mothers feel they are uniquely tuned to a baby's cries.) \u201cThe study is kind of a high-water mark for the field, putting different levels all together: a robust behaviour, a brain region, and a cellular basis for it,\u201d says Richard Tsien, a neuroscientist also at Langone. Tsien has been studying the action of oxytocin on neuronal circuits in detail, by examining slices of the hippocampus, a region involved in learning and memory. In a 2013 study 6  of rats, Tsien's team found that oxytocin selectively acts on a type of cell called an inhibitory interneuron in a way that quiets background chatter within the neuronal circuit. \u201cOxytocin improved signal transmission, almost doubling the ability of information to flow through the system,\u201d Tsien says. In effect, it is producing more signal and less noise. Froemke's and Tsien's work fits into a broader theory: that one way oxytocin helps social interaction and recognition is by enhancing the brain's response to socially relevant sights, sounds or other stimuli. Young has shown that the hormone helps mice to recognize and pay attention to the smells of other mice 7 ; others found that it promotes people's ability to recognize faces 8 . The hormone does not act alone. In 2013, neuroscientist Robert Malenka at Stanford University in California and his colleagues showed that oxytocin works together with the neurotransmitter serotonin to reduce the excitability of neurons in the nucleus accumbens 9 , a brain region involved in reward. This process seems to support the preference of mice to return to environments where they had rewarding social interactions with other animals. \u201cOxytocin is part of a system,\u201d Carter says, \u201cand it's not the only molecule that matters, but it's one that in some way is regulatory over a large number of other systems.\u201d \n               Matter of trust \n             The rapid evolution in basic research has been accompanied by a boom in clinical interest. Oxytocin has been used since the 1950s to accelerate childbirth, so many researchers consider it relatively safe to use in experiments. About ten years ago, psychology studies started to show that single doses of oxytocin, delivered through an intranasal spray, could promote various aspects of social behaviour in healthy adults. People who inhaled oxytocin before playing an investment game were more willing to entrust their money to a stranger than were placebo-treated players 10 . A dose of the hormone also increased the amount of time that people spent gazing at the eye region of faces 11 , and improved their ability to infer the emotional state of others from subtle expressions 12 . The idea that oxytocin is central to social cognition made it an attractive candidate for treating psychiatric disorders, especially autism spectrum disorder. People with this condition, who often have problems with social interaction and communication, may not process social stimuli appropriately \u2014 and scientists theorized that oxytocin might reverse some of the symptoms. Beginning in 2010, results emerged that seemed to support this theory: researchers found that single puffs of oxytocin could temporarily improve measures of empathy and social cooperation in people with autism spectrum disorder. \u201cPeople got quite excited,\u201d recalls clinical neuroscientist Evdokia Anagnostou, who co-directs the Autism Research Centre at Holland Bloorview Kids Rehabilitation Hospital in Toronto, Canada. But Anagnostou says that some preliminary steps were skipped over as researchers rushed to test oxytocin as a psychiatric drug. \u201cTo be honest, if we had done it properly, we wouldn't have done it the way we did. It went a little bit too fast,\u201d she says. Because oxytocin had cleared the early, standard steps of drug development decades earlier, some researchers did not systematically test a range of doses to see whether they had differing psychological effects. Many early studies of oxytocin for autism were limited because they assessed only a single dose and had relatively few participants, and later experiments with more doses failed to show the same promise. In 2010, clinical psychologist Adam Guastella at the University of Sydney in Australia studied 16 male adolescents with autism spectrum disorder, and found that one dose of oxytocin could improve their ability to gauge the emotions of others by looking at their eyes 13 . But when he tried giving twice-daily doses of the hormone for two months, he found no significant improvements in social interaction or social cognition 14 . \u201cStudies to this point have really shown limited benefit of oxytocin in improving psychiatric illnesses over time,\u201d he says. Guastella says that getting to the bottom of oxytocin's complex neurological effects will take time. \u201cIf we want a simple answer, we're not going to get it.\u201d \n               All in the detail \n             So far, few studies have definitively linked autism to problems in oxytocin signalling. Some of the clearest evidence emerged in February, from a team led by neurogeneticist Daniel Geschwind of the University of California, Los Angeles. The group showed that mice that lacked a working copy of the  Cntnap2  gene \u2014 which has been implicated in a small subset of human autism cases \u2014 had fewer oxytocin-containing neurons in the hypothalamus and socialized less with other mice than did control mice 15 . After receiving doses of oxytocin every day for two weeks, the mice behaved normally again. \u201cUntil this, there was no evidence that there was a subtype of autism that had to do with oxytocin deficits,\u201d Geschwind says. His study points to a more targeted approach in the clinic. \u201cAutism is highly heterogeneous, but if you can find subsets of individuals \u2014 those who have oxytocin-signalling deficits \u2014 they may be the best candidates for oxytocin therapy,\u201d says Karen Parker, a behavioural neuroscientist at Stanford. A handful of large-scale clinical trials are now getting under way to test oxytocin and oxytocin-based therapies for autism spectrum disorder, and to work out who could benefit. Linmarie Sikich, a child psychiatrist at the University of North Carolina is heading the largest of these trials. Sikich plans to recruit 300 people with autism spectrum disorder, ranging in age from 3 to 17, and give them 6 months of either oxytocin or a placebo, followed by 6 months in which everyone will receive oxytocin. Unlike previous studies, the trial will include people with a wide range of symptoms \u2014 and one of its major aims is to uncover the set of factors that influence whether and how strongly people respond to oxytocin. Sikich will analyse many measures of cognition and social functioning, and collect blood samples to look for biomarkers \u2014 such as levels of oxytocin and the receptor it binds to \u2014 that are associated with a response. \u201cLin has really been trying to create conditions under which you could study the potential beneficial effects of oxytocin and really do this right,\u201d says Carter. But Carter and other scientists are concerned by reports from the physicians and parents of children with autism spectrum disorder who say that they are already using oxytocin off-label \u2014 before it has been thoroughly tested. \u201cWe do not understand how the hormone works yet, or have enough information about what happens when it's given repeatedly,\u201d Carter says. \u201cThis is not a molecule that people should be self-administering or playing with.\u201d Some work has pointed to a potential dark side to oxytocin. Carter's group found that a single low dose of the hormone given to baby prairie voles improved their pair bonding as adults, but that higher doses interfered with that behaviour \u2014 possibly because oxytocin started to activate other receptors 16 . And human studies have suggested that in certain contexts, a puff of oxytocin can cause people to be more aggressive in defending themselves against outsiders or competitors 17 . In patients with a psychiatric condition known as borderline personality disorder, a single dose of oxytocin has been found to hinder trust and cooperation 18 . Young says that the oxytocin field would benefit from closer collaboration between basic and clinical researchers. If basic scientists can work out how oxytocin helps the brain to process social stimuli, then that might help in the design of stimuli \u2014 in the form of behavioural therapies \u2014 that could be given alongside the hormone to change behaviour, just as oxytocin and pup calls together affect virgin mice. \u201cI think in the future these two branches need to have more communication,\u201d Young says. But long before that, say researchers, oxytocin could use a rebranding. \u201cIt doesn't induce love; it doesn't induce massive amounts of trust,\u201d Guastella says. \u201cThe problem we've got ourselves into is that we're trying to look for a simple answer: either oxytocin does or does not work in a patient population, or it does or does not enhance a certain social process.\u201d But the science of life is rarely as simple as that. \u201cOxytocin is known to affect circuits in different ways, and it's not going to affect everyone in the same way,\u201d Guastella says. \u201cThe sorts of biology we're studying here are incredibly complex.\u201d \n                     Researchers chafe at halt of psychiatric trials 2015-Mar-30 \n                   \n                     Gene switches make prairie voles fall in love 2013-Jun-02 \n                   \n                     Those resistant to 'love hormone' may also be easier to hypnotize 2013-Apr-19 \n                   \n                     Treatments: In the waiting room 2012-Oct-31 \n                   \n                     Hopes grow over potential autism treatment 2010-Feb-16 \n                   \n                     Trust in a bottle 2005-Jun-01 \n                   \n                     Of mice and memory 2000-Jun-27 \n                   \n                     Nature Special: The autism enigma \n                   \n                     Robert Froemke \n                   \n                     Larry Young \n                   \n                     Sue Carter \n                   \n                     Richard Tsien \n                   \n                     Evdokia Anagnostou \n                   \n                     Adam Guastella \n                   \n                     Linmarie Sikich \n                   \n                     SOARS-B Oxytocin Trial \n                   Reprints and Permissions"},
{"file_id": "522020a", "url": "https://www.nature.com/articles/522020a", "year": 2015, "authors": [{"name": "Heidi Ledford"}], "parsed_as_year": "2006_or_before", "body": "A powerful gene-editing technology is the biggest game changer to hit biology since PCR. But with its huge potential come pressing concerns. Three years ago, Bruce Conklin came across a method that made him change the course of his lab. Conklin, a geneticist at the Gladstone Institutes in San Francisco, California, had been trying to work out how variations in DNA affect various human diseases, but his tools were cumbersome. When he worked with cells from patients, it was hard to know which sequences were important for disease and which were just background noise. And engineering a mutation into cells was expensive and laborious work. \u201cIt was a student's entire thesis to change one gene,\u201d he says. Then, in 2012, he read about a newly published technique 1  called CRISPR that would allow researchers to quickly change the DNA of nearly any organism \u2014 including humans. Soon after, Conklin abandoned his previous approach to modelling disease and adopted this new one. His lab is now feverishly altering genes associated with various heart conditions. \u201cCRISPR is turning everything on its head,\u201d he says. The sentiment is widely shared: CRISPR is causing a major upheaval in biomedical research. Unlike other gene-editing methods, it is cheap, quick and easy to use, and it has swept through labs around the world as a result. Researchers hope to use it to adjust human genes to eliminate diseases, create hardier plants, wipe out pathogens and much more besides. \u201cI've seen two huge developments since I've been in science: CRISPR and PCR,\u201d says John Schimenti, a geneticist at Cornell University in Ithaca, New York. Like PCR, the gene-amplification method that revolutionized genetic engineering after its invention in 1985, \u201cCRISPR is impacting the life sciences in so many ways,\u201d he says. Reporter Kerri Smith investigates the meteoric rise of CRISPR But although CRISPR has much to offer, some scientists are worried that the field's breakneck pace leaves little time for addressing the ethical and safety concerns such experiments can raise. The problem was thrust into the spotlight in April, when news broke that scientists had used CRISPR to engineer human embryos (see  Nature 520, 593\u2013595; 2015 ). The embryos they used were unable to result in a live birth, but the report 2  has generated heated debate over whether and how CRISPR should be used to make heritable changes to the human genome. And there are other concerns. Some scientists want to see more studies that probe whether the technique generates stray and potentially risky genome edits; others worry that edited organisms could disrupt entire ecosystems. \u201cThis power is so easily accessible by labs \u2014 you don't need a very expensive piece of equipment and people don't need to get many years of training to do this,\u201d says Stanley Qi, a systems biologist at Stanford University in California. \u201cWe should think carefully about how we are going to use that power.\u201d \n               Research revolution \n             Biologists have long been able to edit genomes with molecular tools. About ten years ago, they became excited by enzymes called zinc finger nucleases that promised to do this accurately and efficiently. But zinc fingers, which cost US$5,000 or more to order, were not widely adopted because they are difficult to engineer and expensive, says James Haber, a molecular biologist at Brandeis University in Waltham, Massachusetts. CRISPR works differently: it relies on an enzyme called Cas9 that uses a guide RNA molecule to home in on its target DNA, then edits the DNA to disrupt genes or insert desired sequences. Researchers often need to order only the RNA fragment; the other components can be bought off the shelf. Total cost: as little as $30. \u201cThat effectively democratized the technology so that everyone is using it,\u201d says Haber. \u201cIt's a huge revolution.\u201d CRISPR methodology is quickly eclipsing zinc finger nucleases and other editing tools (see 'The rise of CRISPR'). For some, that means abandoning techniques they had taken years to perfect. \u201cI'm depressed,\u201d says Bill Skarnes, a geneticist at the Wellcome Trust Sanger Institute in Hinxton, UK, \u201cbut I'm also excited.\u201d Skarnes had spent much of his career using a technology introduced in the mid-1980s: inserting DNA into embryonic stem cells and then using those cells to generate genetically modified mice. The technique became a laboratory workhorse, but it was also time-consuming and costly. CRISPR takes a fraction of the time, and Skarnes adopted the technique two years ago. Researchers have traditionally relied heavily on model organisms such as mice and fruit flies, partly because they were the only species that came with a good tool kit for genetic manipulation. Now CRISPR is making it possible to edit genes in many more organisms. In April, for example, researchers at the Whitehead Institute for Biomedical Research in Cambridge, Massachusetts, reported using CRISPR to study  Candida albicans , a fungus that is particularly deadly in people with weakened immune systems, but had been difficult to genetically manipulate in the lab 3 . Jennifer Doudna, a CRISPR pioneer at the University of California, Berkeley, is keeping a list of CRISPR-altered creatures. So far, she has three dozen entries, including disease-causing parasites called trypanosomes and yeasts used to make biofuels. Yet the rapid progress has its drawbacks. \u201cPeople just don't have the time to characterize some of the very basic parameters of the system,\u201d says Bo Huang, a biophysicist at the University of California, San Francisco. \u201cThere is a mentality that as long as it works, we don't have to understand how or why it works.\u201d That means that researchers occasionally run up against glitches. Huang and his lab struggled for two months to adapt CRISPR for use in imaging studies. He suspects that the delay would have been shorter had more been known about how to optimize the design of guide RNAs, a basic but important nuance. By and large, researchers see these gaps as a minor price to pay for a powerful technique. But Doudna has begun to have more serious concerns about safety. Her worries began at a meeting in 2014, when she saw a postdoc present work in which a virus was engineered to carry the CRISPR components into mice. The mice breathed in the virus, allowing the CRISPR system to engineer mutations and create a model for human lung cancer 4 . Doudna got a chill; a minor mistake in the design of the guide RNA could result in a CRISPR that worked in human lungs as well. \u201cIt seemed incredibly scary that you might have students who were working with such a thing,\u201d she says. \u201cIt's important for people to appreciate what this technology can do.\u201d Andrea Ventura, a cancer researcher at Memorial Sloan Kettering Cancer Center in New York and a lead author of the work, says that his lab carefully considered the safety implications: the guide sequences were designed to target genome regions that were unique to mice, and the virus was disabled such that it could not replicate. He agrees that it is important to anticipate even remote risks. \u201cThe guides are not designed to cut the human genome, but you never know,\u201d he says. \u201cIt's not very likely, but it still needs to be considered.\u201d \n               Editing out disease \n             Last year, bioengineer Daniel Anderson of the Massachusetts Institute of Technology in Cambridge and his colleagues used CRISPR in mice to correct a mutation associated with a human metabolic disease called tyrosinaemia 5 . It was the first use of CRISPR to fix a disease-causing mutation in an adult animal \u2014 and an important step towards using the technology for gene therapy in humans (see \u2018A brief history of CRISPR\u2019). The idea that CRISPR could accelerate the gene-therapy field is a major source of excitement in scientific and biotechnology circles. But as well as highlighting the potential, Anderson's study showed how far there is to go. To deliver the Cas9 enzyme and its guide RNA into the target organ, the liver, the team had to pump large volumes of liquid into blood vessels \u2014 something that is not generally considered feasible in people. And the experiments corrected the disease-causing mutation in just 0.4% of the cells, which is not enough to have an impact on many diseases. Over the past two years, a handful of companies have sprung up to develop CRISPR-based gene therapy, and Anderson and others say that the first clinical trials of such a treatment could happen in the next one or two years. Those first trials will probably be scenarios in which the CRISPR components can be injected directly into tissues, such as those in the eye, or in which cells can be removed from the body, engineered in the lab and then put back. For example, blood-forming stem cells might be corrected to treat conditions such as sickle-cell disease or \u03b2-thalassaemia. It will be a bigger challenge to deliver the enzyme and guide RNA into many other tissues, but researchers hope that the technique could one day be used to tackle a wider range of genetic diseases. Yet many scientists caution that there is much to do before CRISPR can be deployed safely and efficiently. Scientists need to increase the efficiency of editing, but at the same time make sure that they do not introduce changes elsewhere in the genome that have consequences for health. \u201cThese enzymes will cut in places other than the places you have designed them to cut, and that has lots of implications,\u201d says Haber. \u201cIf you're going to replace somebody's sickle-cell gene in a stem cell, you're going to be asked, 'Well, what other damage might you have done at other sites in the genome?'\u201d Keith Joung, who studies gene editing at Massachusetts General Hospital in Boston, has been developing methods to hunt down Cas9's off-target cuts. He says that the frequency of such cuts varies widely from cell to cell and from one sequence to another: his lab and others have seen off-target sites with mutation frequencies ranging from 0.1% to more than 60%. Even low-frequency events could potentially be dangerous if they accelerate a cell's growth and lead to cancer, he says. With so many unanswered questions, it is important to keep expectations of CRISPR under control, says Katrine Bosley, chief executive of Editas, a company in Cambridge, Massachusetts, that is pursuing CRISPR-mediated gene therapy. Bosley is a veteran of commercializing new technologies, and says that usually the hard part is convincing others that an approach will work. \u201cWith CRISPR it's almost the opposite,\u201d she says. \u201cThere's so much excitement and support, but we have to be realistic about what it takes to get there.\u201d \n               CRISPR on the farm \n             While Anderson and others are aiming to modify DNA in human cells, others are targeting crops and livestock. Before the arrival of gene-editing techniques, this was generally done by inserting a gene into the genome at random positions, along with sequences from bacteria, viruses or other species that drive expression of the gene. But the process is inefficient, and it has always been fodder for critics who dislike the mixing of DNA from different species or worry that the insertion could interrupt other genes. What is more, getting genetically modified crops approved for use is so complex and expensive that most of those that have been modified are large commodity crops such as maize (corn) and soya beans. With CRISPR, the situation could change: the ease and low cost may make genome editing a viable option for smaller, speciality crops, as well as animals. In the past few years, researchers have used the method to engineer petite pigs and to make disease-resistant wheat and rice. They have also made progress towards engineering dehorned cattle, disease-resistant goats and vitamin-enriched sweet oranges. Doudna anticipates that her list of CRISPR-modified organisms will grow. \u201cThere's an interesting opportunity to consider doing experiments or engineering pathways in plants that are not as important commercially but are very interesting from a research perspective \u2014 or for home vegetable gardens,\u201d she says. CRISPR's ability to precisely edit existing DNA sequences makes for more-accurate modifications, but it also makes it more difficult for regulators and farmers to identify a modified organism once it has been released. \u201cWith gene editing, there's no longer the ability to really track engineered products,\u201d says Jennifer Kuzma, who studies science policy at North Carolina State University in Raleigh. \u201cIt will be hard to detect whether something has been mutated conventionally or genetically engineered.\u201d That rings alarm bells for opponents of genetically modified crops, and it poses difficult questions for countries trying to work out how to regulate gene-edited plants and animals. In the United States, the Food and Drug Administration has yet to approve any genetically modified animal for human consumption, and it has not yet announced how it will handle gene-edited animals. Under existing rules, not all crops made by genome editing would require regulation by the US Department of Agriculture (see  Nature   500 , 389\u2013390; 2013 ). But in May, the agriculture department began to seek input on how it can improve regulation of genetically modified crops \u2014 a move that many have taken as a sign that the agency is re-evaluating its rules in light of technologies such as CRISPR. \u201cThe window has been cracked,\u201d says Kuzma. \u201cWhat goes through the window remains to be seen. But the fact that it's even been cracked is pretty exciting.\u201d \n               Engineered ecosystems \n             Beyond the farm, researchers are considering how CRISPR could or should be deployed on organisms in the wild. Much of the attention has focused on a method called gene drive, which can quickly sweep an edited gene through a population. The work is at an early stage, but such a technique could be used to wipe out disease-carrying mosquitoes or ticks, eliminate invasive plants or eradicate herbicide resistance in pigweed, which plagues some US farmers. Usually, a genetic change in one organism takes a long time to spread through a population. That is because a mutation carried on one of a pair of chromosomes is inherited by only half the offspring. But a gene drive allows a mutation made by CRISPR on one chromosome to copy itself to its partner in every generation, so that nearly all offspring will inherit the change. This means that it will speed through a population exponentially faster than normal (see 'Gene drive') \u2014 a mutation engineered into a mosquito could spread through a large population within a season. If that mutation reduced the number of offspring a mosquito produced, then the population could be wiped out, along with any malaria parasites it is carrying. But many researchers are deeply worried that altering an entire population, or eliminating it altogether, could have drastic and unknown consequences for an ecosystem: it might mean that other pests emerge, for example, or it could affect predators higher up the food chain. And researchers are also mindful that a guide RNA could mutate over time such that it targets a different part of the genome. This mutation could then race through the population, with unpredictable effects. \u201cIt has to have a fairly high pay-off, because it has a risk of irreversibility \u2014 and unintended or hard-to-calculate consequences for other species,\u201d says George Church, a bioengineer at Harvard Medical School in Boston. In April 2014, Church and a team of scientists and policy experts wrote a commentary in  Science 6  warning researchers about the risks and proposing ways to guard against accidental release of experimental gene drives. At the time, gene drives seemed a distant prospect. But less than a year later, developmental biologist Ethan Bier of the University of California, San Diego, and his student Valentino Gantz reported that they had designed just such a system in fruit flies 7 . Bier and Gantz had used three layers of boxes to contain their flies and adopted lab safety measures usually used for malaria-carrying mosquitoes. But they did not follow all the guidelines urged by the authors of the commentary, such as devising a method to reverse the engineered change. Bier says that they were conducting their first proof-of-principle experiments, and wanted to know whether the system worked at all before they made it more complex. For Church and others, this was a clear warning that the democratization of genome editing through CRISPR could have unexpected and undesirable outcomes. \u201cIt is essential that national regulatory authorities and international organizations get on top of this \u2014 really get on top of it,\u201d says Kenneth Oye, a political scientist at the Massachusetts Institute of Technology and lead author of the  Science  commentary. \u201cWe need more action.\u201d The US National Research Council has formed a panel to discuss gene drives, and other high-level discussions are starting to take place. But Oye is concerned that the science is moving at lightning speed, and that regulatory changes may happen only after a high-profile gene-drive release. The issue is not black and white. Micky Eubanks, an insect ecologist at Texas A&M University in College Station, says that the idea of gene drives shocked him at first. \u201cMy initial gut reaction was 'Oh my god, this is terrible. It's so scary',\u201d he says. \u201cBut when you give it more thought and weigh it against the environmental changes that we have already made and continue to make, it would be a drop in the ocean.\u201d Some researchers see lessons for CRISPR in the arc of other new technologies that prompted great excitement, concern and then disappointment when teething troubles hit. Medical geneticist James Wilson of the University of Pennsylvania in Philadelphia was at the centre of booming enthusiasm over gene therapy in the 1990s \u2014 only to witness its downfall when a clinical trial went wrong and killed a young man. The field went into a tailspin and has only recently begun to recover. The CRISPR field is still young, Wilson says, and it could be years before its potential is realized. \u201cIt's in the exploration stage. These ideas need to ferment.\u201d Then again, Wilson has been bitten by the CRISPR bug. He says that he was sceptical of all the promises being made about it until his own lab began to play with the technique. \u201cIt's ultimately going to have a role in human therapeutics,\u201d he says. \u201cIt's just really spectacular.\u201d \n                 Tweet \n                 Follow @NatureNews \n               \n                     US science academies take on human-genome editing 2015-May-18 \n                   \n                     Regulate gene editing in wild animals 2015-May-12 \n                   \n                     Embryo editing sparks epic debate 2015-Apr-29 \n                   \n                     Chinese scientists genetically modify human embryos 2015-Apr-22 \n                   \n                     Mini enzyme moves gene editing closer to the clinic 2015-Apr-01 \n                   \n                     Nature  special: CRISPR \n                   \n                     Video: Genome Editing with CRISPR-Cas9 \n                   Reprints and Permissions"},
{"file_id": "522026a", "url": "https://www.nature.com/articles/522026a", "year": 2015, "authors": [{"name": "Elie Dolgin"}], "parsed_as_year": "2006_or_before", "body": "Moderna Therapeutics has big ambitions and a bankroll to match. How a fledgling start-up became one of the most highly valued private drug firms ever. At a breakfast meeting two-and-a-half years ago, Pascal Soriot, the newly minted chief executive of pharmaceutical giant AstraZeneca, shook hands on the first major drug-development deal of his tenure. It was a research partnership with little-known biotechnology company Moderna Therapeutics of Cambridge, Massachusetts. Worth up to US$420 million, the deal was unusually large for a start-up that offered only a fledgling drug technology, especially one that had not yet even been tested in humans. That was the first of many huge cheques for Moderna. This January alone, the company announced a record $500 million in financing from a handful of investors, pushing it over the $1-billion fund-raising mark and making it the most highly valued venture-backed private company in drug development today. \u201cEverybody is talking about this,\u201d says Johannes Fruehauf, who runs LabCentral, an incubator and shared laboratory facility in the bustling Cambridge biotechnology hub known as Kendall Square. \u201cIt's inevitable with these large, eye-popping numbers.\u201d Investors are clearly attracted to Moderna's technology, which aims to use chemically modified messenger RNA (mRNA) molecules to produce any protein that the body might need. Backers have also bought into the reputation of the company's high-profile co-founders and its charismatic chief executive, whose bold ambition is to move 100 drugs into clinical testing within the next decade, treating everything from cancer to rare genetic diseases. But Moderna is also something of a mystery. As a private firm, it has revealed very little of its research. Its academic founders have published only one study 1  using Moderna's mRNA therapeutics technology in rodents. And the company itself has disclosed scientific details (including some about early work in non-human primates) only through patent filings. Add in questions about the strength of Moderna's patent position and the troubled history of other RNA-based drugs, and some analysts are wondering whether the company will be able to deliver on its promises. \u201cI don't think they've really overcome the critical issues,\u201d says Dirk Haussecker, an RNA-therapeutics consultant in Rastatt, Germany. Based on the publicly available records, he says, \u201cI haven't seen anything from Moderna that makes me say, 'Oh, they really have a competitive edge or they're very different \u2014 in a league of their own.' From a science point of view, it doesn't seem to make sense.\u201d But as a business it is surging ahead. \n               A simple approach \n             On paper, the idea of mRNA therapy seems simple. If someone cannot produce enough of a certain protein, or produces a broken version, doctors could inject their cells with mRNA that codes for a replacement protein. This would avoid the risks of tinkering with the genome permanently, as is done in some forms of gene therapy. And whereas growth factors, antibodies and other complex 'biological' drugs can be produced in vats by bioengineered cells, these are mostly limited to secreted molecules. An mRNA-based therapy would be able to make proteins that operate inside the cell as well. \u201cmRNA delivery would reinvent how we as an industry tackle many diseases,\u201d says Peter Kolchinsky, managing partner of RA Capital Management in Boston, Massachusetts, which is one of the latest investors in Moderna. But delivery is tricky. In the early 1990s, scientists first demonstrated that injected mRNA could generate proteins in mice 2  and rats 3 . But protein production was low and transient, and the mRNA seemed too unstable to make a suitable drug. Years later, researchers also realized that lab-synthesized mRNA tends to spur an immune attack after it is injected, triggering potentially dangerous inflammatory responses. So a handful of researchers started working their way around the body's defences by modifying the RNA. Moderna traces its origins to one such effort, in the laboratory of Derrick Rossi. A stem-cell biologist at Boston Children's Hospital, Rossi and his postdoc Luigi Warren were trying to use mRNA to coax cells into a 'pluripotent' state, capable of giving rise to many cell types. To avoid triggering inflammation, the researchers replaced some of the RNA's molecular building blocks \u2014 the nucleosides uridine and cytidine \u2014 with pseudouridine and 5-methylcytidine. This makes the RNA look more like something that the cell would produce itself, because invaders such as bacteria cannot usually make these modifications to their own mRNA. It worked. In 2010, Rossi and Warren filed to patent their method for making stem cells and later published the results of their research 4 . The work caught the attention of Robert Langer, a respected bioengineer and serial entrepreneur from the Massachusetts Institute of Technology in Cambridge, and Noubar Afeyan, chief executive of Cambridge biotech investment firm Flagship Ventures. Both men immediately saw the sweeping potential of the modified mRNA. The idea of side-stepping the cell's defences \u201cwas intriguing instantaneously\u201d, says Afeyan, who now chairs Moderna's board of directors. Rossi and Langer brought in a third academic co-founder \u2014 cardiovascular biologist Kenneth Chien, formerly at Harvard Medical School in Boston and now at the Karolinska Institute in Stockholm \u2014 and together they launched Moderna in September 2010. The name was Rossi's invention, a portmanteau of modified and RNA. There was just one problem. \u201cOur paper really put the whole thing on the map but, ironically, our paper didn't have anything really to do with mRNA therapeutics,\u201d says Warren, who now runs Stemiotics, a company in San Diego, California, that makes custom-order stem cells using modified mRNA. The modified RNAs were not even their innovation. They got the idea from Katalin Karik\u00f3 and Drew Weissman at the University of Pennsylvania in Philadelphia (UPenn). In two papers that largely fell under the radar at the time, these scientists showed that using pseudouridine and 5-methylcytidine made mRNA nearly invisible to cellular defences, both  in vitro 5  and in mice 6 . In 2005, the pair started filing to patent the technology for therapeutic purposes. \n               Difficult dealings \n             Karik\u00f3 and Weissman created a company called RNARx, which received close to $900,000 in small-business grants from the US government. In mice and monkeys they showed 7  that regular mRNA injections could boost production of erythropoietin, a hormone that is prescribed to treat some forms of anaemia. The company's research efforts ended there, however, in part because of disagreements between the researchers and the University of Pennsylvania over the licensing of their intellectual property (IP). The university eventually sold the licence to Cellscript, a firm in Madison, Wisconsin, for an undisclosed sum. Cellscript has mostly used the rights to market kits for making mRNAs with modified nucleosides, but chief executive Gary Dahl says that the company also has \u201can interest in therapeutics\u201d. He declined to discuss specifics. Karik\u00f3 and Weissman's patent posed a challenge for Moderna. A 2010 internal report from Flagship Ventures, which was nurturing Moderna into existence at the time, states that if scientists could not identify alternatives to pseudouridine and 5-methylcytidine, \u201cour company technology may be limited to licensing IP from UPenn\u201d. Moderna needed to find a way around the patent, and the task fell to its first employee, Jason Schrum. A nucleic-acid biochemist by training, Schrum set to work testing different types of modified nucleoside. He bought RNA-expression kits from Cellscript and assembled an array of nucleoside analogues, some of which he designed. Most of the modified nucleosides were not up to the job. But Schrum found one, a variant of pseudouridine called 1-methylpseudouridine, that seemed to do the trick. According to Schrum, mRNA with this nucleoside produced even higher levels of protein expression with less inflammation than did the mRNA in Karik\u00f3 and Weissman's papers. Last year, the US Patent and Trademark Office granted Moderna patents covering the use of 1-methylpseudouridine, among other nucleosides \u2014 but the University of Pennsylvania also received a patent that covers many of the same nucleosides. Several other mRNA-therapeutics companies say that they have proprietary formulations of modified RNA molecules as well, although few are willing to discuss details. \u201cIn mRNAs, everything is deathly quiet,\u201d says Ali Mortazavi, chief executive of Silence Therapeutics, an RNA biotech in London. \u201cThere's really no understanding of who owns what, so nobody wants to disclose anything \u2014 and we're included in that.\u201d Karik\u00f3, who now works at the German mRNA-therapeutics firm BioNTech in Mainz, points to early \u201csigns that there will be a fierce battle for licensing\u201d \u2014 and not just in the United States. Last year, the European Patent Office received two anonymous letters challenging the validity of Karik\u00f3 and Weissman's patent application covering modified mRNA; US authorities granted the patent in 2012, but a decision is still pending in the European Union. The uncertainties over intellectual property have clearly not dissuaded Moderna's investors. Kolchinsky says that patent disputes may be painful and expensive, but they eventually resolve. \u201cCompanies that enable such breakthroughs typically have the resources to fend off baseless claims, and settle, on reasonable terms, the ones that turn out to be legitimate,\u201d he says. Moderna also has time on its side. Flush with cash \u2014 the company has an estimated $900 million in the bank \u2014 it can continue to sign on pharmaceutical partners and outspend its rivals on science. This year alone, Moderna plans to spend between $150 million and $180 million on research and development \u2014 more than any other mRNA drug-maker. \u201cThey've created this air of inevitability,\u201d says Fruehauf. \u201cIt's a good strategy.\u201d \n               Fund-raiser-in-chief \n             Much of that momentum boils down to one man: chief executive St\u00e9phane Bancel. \u201cHe's a damn good salesman,\u201d says Justin Quinn, a staff scientist who worked at Moderna until 2012. Bancel joined the company in July 2011 after leading the diagnostics firm bioM\u00e9rieux of Marcy-l'\u00c9toile, France, for five years. Afeyan had repeatedly tried to recruit Bancel to run Flagship-launched companies, but Bancel was not interested in most of the projects \u2014 start-ups that tended to focus on one lead product in one disease area. Moderna was different: it promised to reinvent the drug industry. And for Bancel, a smooth-talking businessman with a penchant for stylish, slim-fitting clothing, \u201cit was worth taking a career risk and a massive pay cut to go to a start-up if it had the potential to be something really big\u201d, he says. Bancel quickly set to work on raising capital \u2014 with great success \u2014 but some question his tactics. In the opinion of a former staff scientist (who requested anonymity) Bancel used his charisma and connections, as well as the clout of the company's co-founders, to convince investors and partners of the uniqueness of the Moderna platform, while glossing over any possible holes in its intellectual property. \u201cHe did a tremendous job of persuading people to give the company money for technology that was not 100% theirs,\u201d the ex-employee says. In response, Bancel says that of course investors in Moderna did their due diligence before writing cheques: \u201cCompanies are a bit more sophisticated than that.\u201d He and other Moderna executives also acknowledge the seminal contributions made by Karik\u00f3, Weissman and others. But Tony de Fougerolles, who was Moderna's first chief scientific officer and now leads research efforts at Ablynx in Ghent, Belgium, argues that such early work was largely academic, and that Moderna approached the research \u201cfrom a pharmaceutical perspective\u201d. Moreover, Bancel says that Moderna's technology has now advanced to the point that the company's initial patent filings are \u201cirrelevant\u201d. \u201cThis is Moderna generation 1.0, and we're at 6.0 now,\u201d he says. Moderna no longer relies on 1-methylpseudouridine in its mRNAs, for example. And modified nucleoside chemistry is just one part of what goes into building an mRNA drug. Another crucial aspect involves working out how to get the mRNA into specific cells and tissues in the body \u2014 a challenge that continues to vex the related field of RNA-interference therapeutics, which emerged more than a decade ago but has had few clinical successes. \u201cThe key for messenger RNA is going to be delivery,\u201d says Joseph Payne, president and chief executive of Arcturus Therapeutics in San Diego, one of many drug developers working on nanoparticle-based delivery of mRNA therapeutics. \u201cThat's really the rate-limiting step,\u201d adds Haussecker. Bancel says that Moderna is exploring several delivery technologies through its in-house team and partnerships with others \u2014 although he would not divulge details of the company's approach. \u201cPeople will figure out in 18 months where we are now when they see the patents,\u201d he says. Although at that point, he adds, even those methods will probably be out of date. \n               The beast \n             At its sleek Cambridge headquarters, Moderna is equipping itself with the best laboratories that money can buy. In the middle of a third-floor lab sits \u201cthe beast\u201d, as Bancel calls it: a suite of robots that can make up to 50 lots of therapeutic mRNA per day for testing in non-human primates. Moderna also plans to open a facility for making human-grade mRNA later this year. Its resources have allowed the company to launch more than 50 drug-development programmes, mostly through external pharmaceutical partners, but also at three wholly-owned spin-offs: Onkaido, Valera and Elpidera, which focus on oncology, infectious diseases and rare diseases, respectively. Bancel says that Valera will be first to the clinic, with an mRNA drug that targets an undisclosed infectious disease. \u201cBy the end of 2016, we will have trials for all the therapeutic areas we are in today,\u201d he says. But clinical success is by no means guaranteed. \u201cIt will probably be like the technologies before it,\u201d says James McSwiggen, an independent biotechnology consultant who has worked with Moderna in the past. Other RNA-based drugs, such as antisense therapies, RNA interference and, most recently, microRNA, have all gone through periods of industry exuberance. These are generally followed by years wrestling with scientific realities before the technologies begin to show their true clinical promise. \u201cI suspect that the same will happen\u201d with mRNA, says McSwiggen. \u201cIf any company can weather that boom\u2013bust bit, I would imagine that, given the amount of money that they've raised, Moderna should.\u201d Other mRNA-therapeutics companies are persevering, and are getting promising data from studies in large animals. CureVac, a German company that spun off from the University of T\u00fcbingen in 2000, has found that it can get injected mRNA past the immune defences of pigs and monkeys by picking molecules with optimal sequences rather than by modifying their nucleosides 8 . So far, CureVac has struck deals with several big pharmaceutical companies and raised around $220 million in equity, including $52 million secured from the Bill & Melinda Gates Foundation in March this year. Dublin-based rare-disease specialist Shire, in collaboration with Ethris of Planegg, Germany, has achieved targeted lung delivery of mRNA in a pig model for cystic fibrosis. \u201cFor a huge idea\u201d like mRNA, says Michael Heartlein, head of MRNA therapeutics at Shire, \u201cI think there's a lot of room for different technologies and different players\u201d. But Bancel's ambition is for Moderna to grow so fast and so big that the competition simply has no chance. \u201cWe want to be the company that, if you want to make an mRNA drug five years from now, you pick up the phone and you call Moderna,\u201d he says. \u201cThink about it: if you're going to put $50 or $100 million into mRNA, do you want to put it into your own team, starting four years behind, and with all the IP issues? Or do you want to pile it on $900 million of someone else's money?\u201d As for the naysayers and critics, Bancel says, \u201cI understand people are not happy. I understand people are jealous. I understand all that. It's life.\u201d \n                     Biotech boot camp 2015-Mar-25 \n                   \n                     mRNA-based therapeutics \u2014 developing a new class of drugs 2014-Sep-19 \n                   \n                     RNA interference rebooted 2014-Apr-22 \n                   \n                     Profile: Being Bob Langer 2009-Mar-04 \n                   \n                     Moderna Therapeutics \n                   Reprints and Permissions"},
{"file_id": "522270a", "url": "https://www.nature.com/articles/522270a", "year": 2015, "authors": [{"name": "Corie Lok"}], "parsed_as_year": "2006_or_before", "body": "Microbiologists are finding new ways to explore the vast universe of unknown microbes in the hunt for antibiotics. The first time Robert Heinzen tried to get  Coxiella burnetii  to grow by itself, he failed miserably. The bacterium, which causes an influenza-like illness called Q fever, normally divides only inside the cells it infects \u2014 forcing researchers to grow it in mammalian tissue and hampering their efforts to investigate the microbe. When Heinzen tried to find a different way to culture it during his time as a postdoc in the early 1990s, he emerged with only half a book of scribbled notes. But the problem kept nagging at him until 2003, when the  C. burnetii  genome was sequenced 1  and he was starting a lab at the US National Institutes of Health's Rocky Mountain Laboratories in Hamilton, Montana. Heinzen thought that the genome could offer important clues to the bacterium's metabolism and growth. Even so, it took his postdoc Anders Omsland almost four years of systematically testing hundreds of combinations of culture conditions to come up with the perfect recipe for cultivating the microbe outside cells 2 . \u201cWhen he showed me the cultures, I thought, it's got to be a contaminant,\u201d Heinzen recalls. But several more months of work confirmed their success. Coxiella burnetii  is still in the minority. An estimated 85\u201399% of bacteria and archaea cannot yet be grown in the lab, drastically limiting scientists' knowledge of microbial life and holding back the search for new antibiotics, which tend to be derived from bacteria. That search is becoming more urgent as resistance to existing drugs surges: last month, the World Health Organization approved a global plan to combat antibiotic resistance, and a review panel appointed by the UK government called for a \u00a31.3-billion (US$2-billion) investment from the global drug industry to revitalize antibiotic research. To find new drugs, researchers say that they need alternative ways to investigate the array of uncultured organisms \u2014 the mysterious dark matter of the microbial world. Scientists are already taking steps towards this goal. Advances in cultivation methods and other technologies have helped them to grow previously unculturable microbes, and improved DNA sequencing and bioinformatics are allowing them to examine some microbes without needing to grow them at all. The work has uncovered a breathtaking amount of microbial diversity in samples ranging from soil to permafrost, marine sponges, hydrothermal vents and the crevices of the human body. Some of the discoveries are already pointing to possible antibiotics \u2014 and scientists say that they are only just scratching the surface. \u201cThere is, for sure, high potential for more biodiversity to be discovered,\u201d says Ute Hentschel, a marine microbiologist at the University of W\u00fcrzburg in Germany. \u201cIf you look for more, you'll find more.\u201d \n               Culture cocktail \n             Conventionally, biologists have studied microbes by growing pure cultures of a species in fairly standard sets of nutrients. The trouble is that bacteria do not live like that in nature: they inhabit a huge range of environments, usually alongside other organisms, and scientists have struggled to recreate those conditions. But as Heinzen and Omsland showed with their studies on  C. burnetii , genetic sequences can throw open a door. Omsland used sequencing to compare the genes expressed when the bacteria were growing successfully inside host cells with those expressed when they were struggling to grow alone. He found a suite of genes involved in protein synthesis that were less active in the struggling microbes, a hint that adding amino acids and peptides to the growth medium might help the bacterium to thrive. But even when Omsland managed to increase the bacterium's protein synthesis 13-fold, it still would not divide 2 . The final clue came from genes suggesting that  C. burnetii  could survive in low-oxygen environments. When the team placed the microbe in 5% oxygen or less, they finally saw it grow. \u201cThat was the critical finding,\u201d says Heinzen. \u201cIt wasn't a nutrient, it was an environmental factor.\u201d Since adopting the new 'axenic' or host-cell-free culture technique, the  C. burnetii  field has expanded. By selectively turning genes on and off, researchers have learned about how the bacterium interacts with host cells to infect them and divide. \u201cThe ability to grow  Coxiella  axenically has, without any exaggeration, completely revolutionized this field of study,\u201d says Hayley Newton, a microbiologist and  Coxiella  researcher at the University of Melbourne in Australia. The bacterium is highly transmissible through air and is considered a possible biothreat. Heinzen's lab is now working on making strains in which key virulence genes have been inactivated, in the hope that they might be useful in developing vaccines. Researchers are now designing culture systems for other microbes that grow only inside cells. Omsland, now at Washington State University in Pullman, has developed a cell-free culture system for  Chlamydia trachomatis 3 , the pathogen behind one of the most common sexually transmitted diseases. He has not yet coaxed  Chlamydia  to divide in his medium, but \u201cI was born optimistic,\u201d he says \u2014 and his success with  C. burnetii  fuels his hope. \n               Miniaturized cultures \n             One way to speed up the process of finding a culture recipe is to use microfluidic chips \u2014 devices with thousands of tiny wells connected by channels that make it possible to run many experiments in parallel. After using this method to cultivate a new microbe 4 , Rustem Ismagilov at the California Institute of Technology in Pasadena and his collaborators even named the bacterium isolate microfluidicus 1. Ismagilov was already working on microfluidics when, in 2012, a group of microbiologists issued a list of 'most wanted' taxa \u2014 a call to the research community to grow and sequence microbes that were relatively common in the human body, were distantly related to already-sequenced organisms and had eluded all attempts at cultivation 5 . Ismagilov and his team answered the call with a device that holds 3,200 nanolitre-sized wells and that can fit in the palm of a hand. They scraped samples from the gut lining of a healthy volunteer, and then diluted them so that no more than one cell would end up in each well. By filling so many wells, the researchers increased the chances that their target organism \u2014 a human-gut microbe in the  Oscillibacter  genus \u2014 would find its way into at least a few of them. The team used about ten chips to test various conditions, and looked for growth of the microbe by checking its DNA for a key marker gene. They managed to find their bacterium, and then grow it to larger amounts in Petri dishes. It was one of the first members of the wanted list to be cultivated. Further genetic study revealed that isolate microfluidicus 1 was not actually part of the  Oscillibacter  genus; it had been classified incorrectly and was actually part of a new, related group that the team is now working to characterize. A key ingredient for growing this bacterium, the team found, was a dash of fluid that had been extracted from the volunteer's intestine. Being able to stretch the use of such a precious sample across thousands of experiments is an important advantage of the microfluidics approach, says Ismagilov. Another is that each starting cell does not have to compete with other species. \u201cMicrofluidics allows us to identify culture conditions efficiently and then increase our chances that our target will grow,\u201d he says. Xiaoxia Nina Lin, a chemical engineer at the University of Michigan in Ann Arbor, is using microfluidics to hunt down members of the most-wanted list in human faecal samples. Microbes normally live in complex communities and often rely on other species, so Lin is trying to dissect those relationships by putting two, three or four cells together in myriad combinations on a chip, and working out who is dependent on whom. \u201cIt's a good engineering approach,\u201d says Vincent Young, an infectious-disease researcher at the University of Michigan who is helping Lin to obtain clinical samples. \u201cYou can quickly reduce the complexity.\u201d \n               Nature's incubator \n             When Slava Epstein and Kim Lewis started to collaborate 15 years ago, they realized that they might not need to coax recalcitrant microbes into growing in the lab. If a bacterium already grows happily in its natural environment, they reasoned, then why not just cultivate it there? So the two microbiologists, from Northeastern University in Boston, Massachusetts, started working on a simple device that they could stick in the ground. They called it the iChip. The approach paid off earlier this year, when Lewis, Epstein and scientists from their start-up company NovoBiotic Pharmaceuticals in Cambridge, Massachusetts, reported that they had used the iChip to isolate a new bacterial species from soil 6 . The thumb-sized device is less sophisticated than a microfluidics chip: it consists of 384 tiny wells that were filled with samples of soil that have been mixed with agar and diluted to ensure that only one cell ends up in each chamber. The chip is sealed with a membrane that traps the bacteria but allows molecules to diffuse back and forth, and was planted in a grassy field in Maine \u2014 the same soil from which the sample had been taken. After a month, the researchers transferred colonies from the chip to Petri dishes in the lab, took extracts from them and screened those for antibiotic activity. They had grown 10,000 types of bacterium \u2014 many more than if they had just put the soil sample on an agar plate. They homed in on a new species that they called  Eleftheria terrae 6  and found that the bacterium produces an antibiotic, called teixobactin, that kills various human pathogens in the lab, including drug-resistant strains of  Staphylococcus aureus . \u201cTo me that's a phenomenal result, that they can find really neat new molecules from groups of organisms that pharma largely hasn't focused on,\u201d says Sean Brady, a chemical biologist at Rockefeller University in New York. But what generated headlines was the discovery that other bacteria did not develop resistance to teixobactin 6 , as they do to most other antibiotics. That is because teixobactin binds to molecules that have important roles in cell-wall synthesis; bacteria are not known to modify these molecules to evade the effects of antibiotics. The clincher, Lewis says, is that although  E. terrae  is inherently resistant to the teixobactin, it does not seem to have resistance genes that could be transferred easily to other bacteria. This does not mean that resistance will never emerge, but that it could take 20 or 30 years. The NovoBiotic team went on to grow larger quantities of the bacterium. It is now generating grams of the drug using a fermenter, doing extensive preclinical testing of this and other drug candidates, and seeking more leads from uncultivated microbes in soil and marine samples. Epstein is using the iChip to culture new microbes from soil and water in Greenland, and says that he has received more than 200 requests this year for the device and advice on how to use it. \n               Don't culture, sequence \n             Despite these successes, culturing microbes is still a complicated, hit-and-miss affair, so many researchers are bypassing it altogether and instead learning what they can from DNA. Advances in sequencing methods mean that scientists can now analyse genomes from individual uncultured microbial cells \u2014 rather than, as before, typically sequencing a community of many different types of microbe en masse and then trying to piece the sequences back together. Tanja Woyke at the US Department of Energy's Joint Genome Institute in Walnut Creek, California, first got interested in single-cell sequencing not long after the key discovery ten years ago that an enzyme from a bacteria-infecting virus could be used to make many copies of a bacterial cell's genome 7 . Woyke wanted to use the tools to fill out the microbial tree of life. She and her group collected samples from nine different habitats, including sediment from a Nevada hot spring and water near a Pacific hydrothermal vent. They isolated some 200 cells, sequenced the genome of each one and classified the cells into more than 20 new lineages that do not have any cultivated representatives 8 . \u201cThey were the first to really take single-cell genomics to the next level, in terms of the number of sequences and single cells analysed,\u201d says Hentschel. Last year, J\u00f6rn Piel of the Swiss Federal Institute of Technology in Zurich and his colleagues reported that they had used single-cell sequencing and other techniques to identify uncultured bacteria in marine sponges 9 . These filter-feeding creatures have long been of interest to scientists because they produce a rich set of chemicals with anticancer, antibiotic and other medicinal properties. They also harbour dense microbial communities that contribute up to 40% of the sponge's mass and were suspected to be the source of these chemicals. But the members of those communities had not been cultured. Piel and his group focused on the sponge  Theonella swinhoei , which harbours about 1,000 types of bacterium and generates dozens of known bioactive compounds. In 2011, they started to sequence DNA from individual bacterial cells isolated from sponge samples and looked for two gene clusters known to be involved in the production of biologically active molecules. They found these genes in a bacterium called  Entotheonella 9 . What was most surprising to Piel, however, was that this one organism was responsible for nearly all of the bioactive compounds linked to the sponge \u2014 something that became clear when sequence data showed that the bacterium harboured all the necessary genes. When Piel received the key data from his collaborators, \u201cI almost fell out of my chair,\u201d he says; it was the first evidence that an uncultivated microbe can be such a 'talented' producer of bioactive chemicals. \u201cThe ability to create many distinct compounds in a single strain, this is not that common,\u201d he says. Piel's lab is now trying to engineer gene clusters from  Entotheonella  into a culturable organism such as  Escherichia coli  so that the host can churn out the compounds, something that is not likely to be easy given that biosynthetic genes can be large. He is also mining the genomes of microbes in sponges from Japan, Papua New Guinea and Israel in search of other bacterial super-producers. \n               Gene prospecting \n             Michael Fischbach, a biochemist at the University of California, San Francisco, has developed a different way to analyse microbial sequences: rather than isolating single cells, he sifts through the growing banks of bacterial genomic data. Fischbach and his group developed a machine-learning algorithm that is trained to recognize key patterns associated with bacterial genes that synthesize interesting molecules such as antibiotics. Then they let it loose on a large set of bacterial genomes to look for new gene clusters with similar features. Their targets included bacteria from soils and oceans, which are known to harbour an incredible diversity of microbes. But the algorithm generated a surprising number of hits from microbes that live on or in the human body, known collectively as the human microbiota. Fischbach was both excited and intimidated when he got the first results. Excited, because the bioactive compounds made by the microbiota were largely uncharted territory, and could have important roles in human health and disease. Intimidated because Fischbach mostly worked on microorganisms in soil. Still, Fischbach decided to take the plunge, and has since switched his entire lab's focus to the human microbiome. Using an improved version of the algorithm, his team went prospecting in the genomes of almost 2,500 organisms in the human body 10 , and found more than 14,000 biosynthetic gene clusters. \u201cIt has been remarkably easy to find interesting molecules from the human microbiome,\u201d says Fischbach. \u201cIt's so easy that I can tell it has nothing to do with our ability to find them, it's more that there's just a lot to find.\u201d The group narrowed down the list to more than 3,000 common gene clusters, and found that one generated an antibiotic, lactocillin, made by a microbe commonly found in the vagina. It is one of only a handful of bioactive chemicals isolated from the human microbiota. Lactocillin blocks the growth of common vaginal pathogens such as  S. aureus , but not of other bacteria that normally inhabit the vagina. Fischbach is now generating the molecules from the gene clusters he is finding, solving their structures, and working with collaborators to learn more about their function. The natural-products field has tended to focus on soil and marine microorganisms rather than on human ones, says Gerry Wright, a chemical biologist at McMaster University in Hamilton, Canada. \u201cI think it's a great idea to look at those genes and clusters,\u201d he says. However, turning such compounds into usable drugs will require a lot of preclinical work, says Wright. \u201cJust by looking at a molecule, it's almost impossible to tell whether it's going to be suitable as a drug.\u201d And even if it seems promising, the barriers to commercializing a new antibiotic are high (see M. Woolhouse and J. Farrar  Nature   509 , 555\u2013557; 2014 ). But Lewis takes hope from the recent progress. With all the burgeoning efforts to culture and analyse uncultured microbes, he is already imagining what could be discovered if such efforts were scaled up. He hopes to see a level of drug discovery to match that in the Waksman era, the time in the 1940s and 1950s when Nobel-prizewinning microbiologist Selman Waksman discovered more than 20 antibiotics by systematically screening thousands of soil microbes for their ability to block the growth of other bacteria. \u201cThe fact that we're finding compounds with such remarkably interesting modes of action that we haven't seen before, that's the most interesting part,\u201d says Lewis. \u201cWe've scratched only a tiny part of Mother Earth.\u201d \n                     Antibiotic alternatives rev up bacterial arms race 2015-May-27 \n                   \n                     Promising antibiotic discovered in microbial \u2018dark matter\u2019 2015-Jan-07 \n                   \n                     Antibiotic resistance: The last resort 2013-Jul-24 \n                   \n                     Researchers glimpse microbial 'dark matter' 2013-Jul-14 \n                   \n                     Antibiotics: Recover the lost art of drug discovery 2012-May-23 \n                   \n                     World Health Organization tackles antibiotic resistance \n                   \n                     Novobiotic Pharmaceuticals \n                   \n                     Selman Waksman and Antibiotics \n                   Reprints and Permissions"},
{"file_id": "521408a", "url": "https://www.nature.com/articles/521408a", "year": 2015, "authors": [{"name": "Andy Extance"}], "parsed_as_year": "2006_or_before", "body": "Long a staple of science fiction, laser weapons are edging closer to the battlefield \u2014 thanks to optical fibres. Silently, the drone aircraft glides above the arid terrain of New Mexico \u2014 until it suddenly pivots out of control and plummets to the ground. Then a mortar round rises from its launcher, arcs high and begins to descend towards its target \u2014 only to flare and explode in mid-flight. On the desert floor, on top of a big, sand-coloured truck, a cubic mechanism pivots and fires an invisible infrared beam to zap one target after another. This High Energy Laser Mobile Demonstrator (HEL MD) is a prototype laser weapon developed for the US Army by aerospace giant Boeing of Chicago, Illinois. Inside the truck, Boeing electrophysics engineer Stephanie Blount stares at the targets on her laptop's screen and directs the laser using a handheld game controller. \u201cIt has a very game-like feel,\u201d she says. Andy Extance discusses the military reality of laser weapons That seems only natural: laser weapons are a staple of modern video games, and ray-guns of various sorts were common in science fiction for decades before the first real-life laser was demonstrated in 1960. But they are not a fantasy anymore. The Boeing prototype is just one of several such weapons developed in recent years in both the United States and Europe, largely thanks to the advent of relatively cheap, portable and robust lasers that generate their beams using optical fibres. The output of these fibre weapons is measured in kilowatts (kW), orders of magnitude less than the megawatt-class devices once envisioned for the US Strategic Defense Initiative \u2014 an ultimately unsuccessful cold-war plan that sought to use lasers to disable ballistic missiles carrying nuclear warheads. But the modern, less ambitious, weapons are on the brink of real-world deployment. Tests such as those of the Boeing system show that the lasers have enough power to overcome threats from terror groups \u2014 at a fraction of the price of conventional defences. \u201cIt's a very cost-effective solution to taking out cheaply made weapons like small mortars or rockets made out of sewer pipe,\u201d says Blount. In late 2014, for example, the US Navy showed that a ship-mounted laser-weapon system called LaWS could target small boats, such as those used by terrorists and pirates. That experimental weapon is currently installed on the USS  Ponce , an amphibious support ship in the Gulf. Many challenges to full-scale deployment remain, warn developers, from the need to boost the weapons' power to the difficulty of operating a laser in fog and clouds. But specialists in defence and security are starting to take lasers seriously. \u201cAfter a nearly half-century quest, the US military today is on the cusp of finally fielding operationally relevant directed-energy weapons,\u201d wrote Paul Scharre, an advanced-technology specialist at the Washington DC-based Center for a New American Security (CNAS), in a report on laser weapons released in April 1 . \n               The power predicament \n             Laser weapons have long fascinated weapons developers \u2014 most notably during the heyday of the Strategic Defense Initiative, nicknamed Star Wars, in the 1980s and 1990s. US spending on laser-weapons research peaked in 1989 when, according to the CNAS report 1 , the government spent the equivalent of US$2.4 billion in 2014 dollars. Funding has continued at lower levels ever since. Yet the original goal, of being able to shoot down incoming ballistic missiles, proved unattainable. The trick with any laser weapon is to focus its energy into a spot that is small enough to heat up and damage the target \u2014 and to do that with a machine that is compact and portable enough for the battlefield. This is easier said than done. In 1996, for example, the US Air Force initiated the Airborne Laser project as one of its contributions to defence against ballistic missiles. Because it was impossible at the time to generate the required megawatts of optical power electrically, the developers chose a chemical oxygen iodine laser (COIL) that could be fuelled by a chemical reaction. But the COIL was so bulky that it could only be carried on a Boeing 747, and left little space for laser fuel. \u201cIt needed remote mixing units and chemicals weighing tens of thousands of pounds,\u201d says Paul Shattuck, head of directed-energy systems for Lockheed Martin Space Systems, which provided the project's beam-control technology. Another major problem was the atmosphere, says Phillip Sprangle, senior scientist for directed-energy physics at the Naval Research Laboratory in Washington DC. Not only was the beam scattered by dust and natural turbulence, he says, but its passage caused 'thermal blooming'. When the beam propagated at very high powers, Sprangle explains, \u201cthe atmosphere absorbed laser light, heating the air and causing the laser beam to spread out\u201d. That spreading, in turn, dissipated the laser's energy. The good news for the Airborne Laser project was that this issue, at least, had a solution: adaptive optics technology similar to that used by astronomers to clarify their view of the stars (see  Nature   517 , 430\u2013432; 2015 ). The technology uses mirrors to automatically distort the laser beam in a way that cancels the effects of the turbulence, with the same result as a pair of glasses correcting for aberrations in the eye. \u201cAs the laser beam passes through the atmosphere,\u201d says Shattuck, \u201cit cleans up, and it's nice and tight when it gets to the target.\u201d By 2010, the adaptive optics was good enough for the Airborne Laser to destroy a ballistic missile in flight. By then, however, logistical issues such as the size problem had led the Department of Defense to lose its enthusiasm for energy weapons in general. It cancelled the Airborne Laser programme outright by early 2012. At the same time, the department's spending on high-energy lasers in general was falling; it dropped from $961 million in 2007 to $344 million in 2014. \n               Fibres in the spotlight \n             The money did not vanish entirely: attention was already shifting to fibre lasers as a way to deliver results more economically. Fibre lasers were invented in 1963, and since the 1990s they have been advanced almost entirely by IPG Photonics in Oxford, Massachusetts. Whereas other solid-state lasers use rigid rods, slabs or discs of crystal to generate the beam, and so have to be fairly large, fibre lasers use thin optical fibres that can be wrapped into compact coils (see 'Fibre power'). The fibres can collect their optical energy from brighter versions of the cheap laser diodes used in DVD players, and then amplify the light to higher power, with overall electrical-to-optical conversion efficiencies greater than 30%. This is at least double the efficiency typical of other solid-state lasers, and close to that of chemical lasers such as COIL. And, being intrinsically long and thin, the fibres have a high surface area to volume ratio and can radiate away waste heat very quickly \u2014 an ability that helps to give the lasers a long working life and low maintenance requirements. These advantages first attracted attention during the 1990s, when fibre lasers began to be used to beef up optical signals carrying Internet data through undersea cables. But since the early 2000s, IPG has focused on developing kilowatt-class industrial lasers for welding, drilling and cutting \u2014 devices that also attracted the attention of military researchers. Around 2010, recalls Shattuck, he and his colleagues at Lockheed Martin heard from Israeli civilians targeted by rockets launched from the Gaza Strip. \u201cThe mayor of a village stood up and said, 'Please, give me some kind of defence,'\u201d Shattuck says. This inspired Lockheed Martin to develop the Area Defense Anti-Munitions (ADAM) system, which uses an off-the-shelf 10-kW laser from IPG to keep costs down. Since 2012, the company has shown that ADAM can disable targets such as boats, drones and simulated small-calibre rockets from about 1.5 kilometres away. Although unwilling to disclose the price of ADAM \u2014 or whether anybody has bought one \u2014 Lockheed Martin says that it is now ready to provide the system to customers. Blount is less reticent about Boeing's HEL MD prototype, which also uses a commercial 10-kW fibre laser. With the system drawing its power from the vehicle engine or a separate generator, she says, \u201cit takes less than two cups of fuel to fire the laser for long enough to disable many targets.\u201d This makes it much cheaper to use for defence than conventional missiles. \u201cAn inexpensive missile is $100,000 and that's one shot,\u201d says David DeYoung, Boeing's director of directed-energy systems. \u201cTo shoot a laser-weapon system once is less than $10.\u201d Blount stresses that the resurgence of laser weapons owes at least as much to advanced image-recognition and targeting systems as to the laser itself. \u201cThe better the pointing and tracking system,\u201d she says, \u201cthe better able you are to put the beam on the most vulnerable point of a target.\u201d Thanks to computerized aiming, HEL MD can operate in wholly autonomous mode, which Boeing tested successfully in May 2014 \u2014 although the trials uncovered an unexpected challenge. The weapon's laser beam is silent and invisible, and not all targets explode as they are destroyed, so an automated battle can be over before operators have noticed anything. \u201cThe engagements happen quickly, and unless you're staring at a screen 24\u20137 you'll never see them,\u201d Blount says. \u201cSo we've built sound in for whenever we fire the laser. We plan on taking advantage of lots of  Star Trek  and  Star Wars  sound bites.\u201d \n               Strength in numbers \n             Aiming and targeting may be battle-ready, but power is still a problem. A commercial laser's 10-kW output is at the low end of what is useful for laser weapons. And using fibres puts limits on the beam's power and quality \u2014 not least because at high powers, the cascade of photons surging through the fibre can heat it up faster than it can radiate the energy, and can thus cause damage. To avoid this, researchers are working to combine the output from several lasers. The ideal way to do this would be 'coherent combining', in which the waves from each laser march together in tightly synchronized formation. This technique is widely used in radio and microwave applications, says Tso Yee Fan, a laser scientist at the Massachusetts Institute of Technology's defence-oriented Lincoln Laboratory in Lexington. But coherence is much tougher to achieve with visible and infrared light. The waves from each laser must have almost identical wavelengths, the planes of their oscillations must precisely align, and the peaks and troughs of each wave must coincide. \u201cIn radio-frequency or microwaves, the wavelength's a few centimetres,\u201d Fan says. \u201cIn optics, the wavelength's around a micrometre, so being able to do those kinds of controls has been really difficult.\u201d But that may not matter much, says Sprangle. In 2006, he and his team reported computer simulations suggesting that an 'incoherent combination' of several fibre-laser beams hitting a single spot would be almost as effective as a coherent combination 2 . With either approach, he says, \u201cwhen you're propagating over long ranges through atmospheric turbulence, you get approximately the same power on the target\u201d. In 2009, his group confirmed this theory by using mirrors to combine 4 fibre-laser beams into a 5-centimetre spot on a target more than 3 kilometres away 3 . Building on Sprangle's work, the US Office of Naval Research has developed the 30-kW LaWS, which incoherently combines six commercial fibre-lasers. LaWS has been installed on the USS  Ponce  since September 2014, and has been tested on objects such as small boats and drones. The missile-specialist MBDA Germany in Schrobenhausen has developed a similar approach 4 . In October 2012, the firm successfully used its 40-kW combined fibre-beam system to destroy model artillery shells towed through the air some 2 kilometres away. MBDA's tests have also helped to debunk the science-fiction idea that reflective armour would defend against laser weapons. They found that any dust on the mirrored surface would get burned in, and lead to the destruction of the target even faster than with a non-reflective surface. Markus Martinstetter from MBDA's Future Systems Directorate argues that high-precision targeting minimizes the chances of accidentally hurting bystanders while trying to shoot down targets, especially compared with conventional explosives. \u201cThere is no risk from fragmenting ammunition and we only start the irradiation when the aim point is exactly on target,\u201d he says. Lockheed Martin is also working on laser weapons that can take on targets that are more complex or farther away than can be tackled by its low-cost ADAM system. In March, for example, the company reported that its Advanced Test High Energy Asset (ATHENA) system could disable the running engine of a small truck mounted on a test platform. ATHENA uses a similar adaptive-optics system to the Airborne Laser, coupled with Lockheed's Accelerated Laser Demonstration Initiative (ALADIN) fibre-laser system. ALADIN combines the output of several fibre lasers, each with a slightly different wavelength, into a single 30-kW beam. This 'wavelength beam combining' approach originated at the Lincoln Laboratory 5  and is similar to methods that channel Internet traffic into fibre-optic cables. Fan notes that this method is easier than coherent combining, but gives better-quality beams than incoherent combining, so it can more easily hit smaller targets from longer distances. Jason Ellis, a visiting fellow at the CNAS and lead author of the think tank's laser-weapons report 1 , says that such developments convince him that fibre-laser weapons are coming of age \u2014 and that emerging advances could take them to hundreds of kilowatts and extend their range to hundreds of kilometres. Despite such advances, a February 2014 poll 6  of US national-security specialists found that just one-fifth believed that directed-energy weapon technologies would be mature within a decade. Michael Carter, a programme manager for photon science at Lawrence Livermore National Laboratory in Livermore, California, cautions that today's lasers are a very long way from their science-fiction counterparts. \u201cThey're not yet the  Star Trek  phaser,\u201d he says. \u201cPeople talk about speed-of-light engagement, but it still takes time to demolish targets. At the most basic level, if you can't see it \u2014 if there's too much rain or fog \u2014 your laser can't hit it.\u201d He suggests that the greatest value of the current generation of demonstration systems may be in working out how to handle such broader challenges before better lasers emerge. \u201cDon't mistake what they're doing on the USS  Ponce  for a new strategic superiority,\u201d Carter warns. \u201cIt may be the first step in that direction but it's not going to change the game by itself.\u201d Even the weapons companies are cautious not to overstate their case. For example, MBDA expects that it will take 3\u20135 years for truly operational systems to appear even in the tens of kilowatts range. And in some circumstances \u2014 such as a foggy day \u2014 conventional weapons will always be more effective. \u201cYou give the defender of the future both, and put the choice in their hands,\u201d DeYoung recommends. Despite their modest capabilities, Scharre claims that fibre-laser weapons could find a niche in US military defence in 5\u201310 years. \u201cThey may not be as grand and strategic as the Star Wars concept,\u201d he says, \u201cbut they could save lives, protect US bases, ships and service members.\u201d \n                 Tweet \n                 Facebook \n                 LinkedIn \n                 Weibo \n                 Wechat \n               \n                     Astronomy: Laser focus 2015-Jan-21 \n                   \n                     Secret weapons 2012-Sep-12 \n                   \n                     Microwave weapons: Wasted energy 2012-Sep-12 \n                   \n                     IPG Photonics \n                   \n                     Boeing's Directed Energy group \n                   \n                     Lockheed Martin's directed-energy research \n                   \n                     MBDA \n                   \n                     Naval Research Lab \n                   Reprints and Permissions"},
{"file_id": "522146a", "url": "https://www.nature.com/articles/522146a", "year": 2015, "authors": [{"name": "Rachel Cernansky"}], "parsed_as_year": "2006_or_before", "body": "Long overlooked in parts of Africa, indigenous greens are now capturing attention for their nutritional and environmental benefits. One lunchtime in early March, tables at Nairobi's K'Osewe restaurant are packed. The waiting staff run back and forth from the kitchen, bringing out steaming plates of deep-green African nightshade, vibrant amaranth stew and the saut\u00e9ed leaves of cowpeas. The restaurant is known as the best place to come for a helping of Kenya's traditional leafy green vegetables, which are increasingly showing up on menus across the city. Just a few years ago, many of those plates would have been filled with staples such as collard greens or kale \u2014 which were introduced to Africa from Europe a little over a century ago. In Nairobi, indigenous vegetables were once sold almost exclusively at hard-to-find specialized markets; and although these plants have been favoured by some rural populations in Africa, they were largely ignored by seed companies and researchers, so they lagged behind commercial crops in terms of productivity and sometimes quality. Now, indigenous vegetables are in vogue. They fill shelves at large supermarkets even in Nairobi, and seed companies are breeding more of the traditional varieties every year. Kenyan farmers increased the area planted with such greens by 25% between 2011 and 2013. As people throughout East Africa have recognized the vegetables' benefits, demand for the crops has boomed. This is welcome news for agricultural researchers and nutritional experts, who argue that indigenous vegetables have a host of desirable traits: many of them are richer in protein, vitamins, iron and other nutrients than popular non-native crops such as kale, and they are better able to endure droughts and pests. This makes the traditional varieties a potent weapon against dietary deficiencies. \u201cIn Africa, malnutrition is such a problem. We want to see indigenous vegetables play a role,\u201d says Mary Abukutsa-Onyango, a horticultural researcher at Jomo Kenyatta University of Agriculture and Technology in Juja, Kenya, who is a major proponent of the crops. Scientists in Africa and elsewhere are now ramping up studies of indigenous vegetables to tap their health benefits and improve them through breeding experiments. The hope is that such efforts can make traditional varieties even more popular with farmers and consumers. But that carries its own risk: as indigenous vegetables become more widespread, researchers seeking faster-growing crops may inadvertently breed out disease resistance or some of the other beneficial traits that made these plants so desirable in the first place. \u201cIt is important that when we promote a specific crop, that we try to come up with different varieties,\u201d says Andreas Ebert, gene-bank manager at the World Vegetable Center (AVRDC), an agricultural-research organization based in Shanhua, Taiwan. If the increasing popularity of these vegetables limits choices, he says, \u201cthe major benefits we are currently seeing will be lost\u201d. \n               Protein from plants \n             For Abukutsa, indigenous vegetables bring back memories of her childhood. Cow's milk, eggs and some fish made her ill, so doctors advised her to avoid all animal protein. Instead, the women in her family made tasty dishes out of the green vegetables that grew like weeds around her house. Her mother often cooked the teardrop-shaped leaves of African nightshade ( Solanum scabrum ), as well as dishes of slimy jute mallow ( Corchorus olitorius ) and the greens of cowpeas, known elsewhere as black-eyed peas ( Vigna unguiculata ). One grandmother always cooked pumpkin leaves ( Cucurbita moschata ) with peanut or sesame paste. Abukutsa relished them all and ate the greens with ugali, a polenta-like dish common in East Africa. She chose to pursue a career in agriculture because she wanted to \u201cunravel the potential hidden in African indigenous vegetables\u201d, she says. Now, she is considered a leader across Africa, and increasingly around the world, in a robust, rapidly growing field. \u201cShe's almost like the mother of indigenous vegetables in Kenya,\u201d says Jane Ambuko, head of horticulture at the University of Nairobi. Abukutsa started out in the early 1990s, surveying and collecting Kenya's indigenous plants to investigate the viability of the seeds that farmers were using. In the decades since, she has come to focus mainly on the vegetables' nutritional properties. Today, she is far from alone. The AVRDC has a dedicated research and breeding programme at its office in Arusha, Tanzania, and the Kenya Agricultural and Livestock Research Organization in Nairobi does similar work. Other health and agriculture organizations in both East and West Africa focus on boosting consumer use and improving the viability and yield of these crops. That fits into a global trend emphasizing bioregional foods \u2014 using crops that are well adapted for a given climate and environment, rather than foreign plants that tend to be less nutritious and require extra water or fertilizers. Most of the indigenous vegetables being studied in East Africa are leafy greens, almost all deep green in colour and often fairly bitter. Kenyans especially love African nightshade and amaranth leaves ( Amaranthus  sp.). Spider plant ( Cleome gynandra ), one of Abukutsa's favourites for its sour taste, grows wild in East Africa as well as South Asia. Jute mallow has a texture that people love or hate. It turns slimy when cooked \u2014 much like okra. Ebert says that moringa ( Moringa oleifera ) is not only one of the most healthful of the indigenous vegetables \u2014 both nutritionally and medicinally \u2014 but it is also common in many countries around the world. Research by Abukutsa and others shows that amaranth greens, spider plant and African nightshade pack substantial amounts of protein and iron \u2014 in many cases, more than kale and cabbage 1 . These vegetables are generally rich in calcium and folate as well as vitamins A, C and E (ref.  2 ). In recent years, Abukutsa has been studying how to maximize nutritional benefits using different cooking methods. Compared with raw vegetables, boiled and fried greens contain much more usable iron 3  and could help to combat the high rates of anaemia in parts of East Africa. They can also be important sources of protein, she says. \u201cSome people just live on vegetables, and they cannot maybe afford meat.\u201d Abukutsa is currently studying the antioxidant activity of indigenous vegetables, as well as how resilient they are to the effects of climate change. Most of the traditional varieties are ready for harvest much faster than non-native crops, so they could be promising options if the rainy seasons become more erratic \u2014 one of the predicted outcomes of global warming. Slenderleaf ( Crotolaria  sp.) is particularly hardy during drought because it quickly establishes its taproot. \u201cIf we have a short rain because of climate change, it can survive,\u201d she says. She is working with other research partners to select vegetables with increased tolerance for variations in rainfall and temperature. Early on, Abukutsa recognized that she needed to do more to convince people to add indigenous vegetables to their diets. Since around 2000, she has led public education campaigns and worked with restaurants and supermarkets around Kenya to find out what they would need to start selling these foods. A simple but significant problem was that people did not know how to cook the vegetables. Unlike larger leafy vegetables such as kale, many of the indigenous varieties have small leaves that must be separated from their stems individually before cooking \u2014 a laborious process. Recipes are often vegetable-specific; spider plant can be cooked with sour milk, for example, but cowpea leaves go better with soya bean or peanut paste. Although older generations and some rural populations know what to do with nearly any local vegetable, much of the region's traditional cooking knowledge has been lost. So Abukutsa got to work on collecting and testing recipes to maximize the amount of iron and other nutrients the dishes contain. K'Osewe was one of the first restaurants to take an active interest, and others soon followed. For Abukutsa, indigenous vegetables are not just a research subject \u2014 they remain a central part of her own diet. \u201cToday at lunchtime, I ate pumpkin leaves and nightshade,\u201d she says. The vegetables' new-found popularity is spreading throughout East Africa. At a bustling market in Arusha, a young woman wearing a light-blue headscarf shops for sweet-potato leaves ( Ipomoea batatas ), known locally as matembele, which have a reputation for improving the blood. She buys them from an elderly woman who sells almost exclusively indigenous vegetables under a large red umbrella that protects her stock from the afternoon sun. She says her sales of such plants have climbed substantially over the past five years. \n               Global appeal \n             Green vegetables are not the only indigenous crops attracting researchers' attention. In the 1990s, the US National Research Council (NRC) in Washington DC convened a panel to examine the potential of Africa's 'lost crops', including grains, fruits and vegetables. Chaired by renowned agricultural researcher Norman Borlaug, the panel concluded that native plants held tremendous potential for improving food security and nutritional intake across Africa, and should be a greater focus for researchers 4 . Today, the World Agroforestry Centre in Nairobi is studying a range of Africa's more than 3,000 indigenous fruit species, and finding that they are generally more nutritious, drought-tolerant and pest- and disease-resistant than their exotic counterparts. But vegetables have gained the most notice, both in the marketplace and among researchers. Raymond Vodouhe, a plant breeder and geneticist with Bioversity International in Cotonou, Benin, says that his team's work in West Africa has focused on domesticating wild vegetables. The hardy wild plants help African families to get through periods of drought or crop failure, but are threatened by deforestation and other types of land-clearing. By domesticating them, researchers can give farmers more-reliable access to indigenous vegetables so that they can better endure lean times. The AVRDC is doing active research on native species in Asia and Oceania, as well as Africa. \u201cA rich diversity of indigenous vegetable species exists throughout these areas,\u201d says Ebert, pointing to okra and African eggplant ( Solanum aethiopicum ) in Mali, bitter gourd ( Momordica charantia ) and Malabar spinach ( Basella alba ) in India, and slippery cabbage ( Abelmoschus manihot ) in the Pacific Islands. \u201cThe challenge we face is selecting which indigenous vegetable species to study \u2014 with more than 2,000 plants that can be considered and consumed as vegetables, and very limited research funds, it's a tough choice.\u201d Less than 10% of the AVRDC's roughly US$20-million annual budget goes to studying indigenous vegetables, he says. A main focus has been basic problems such as difficulties with germination and a lack of information about how best to store seeds. Indigenous vegetables are not up to modern farming standards for characteristics such as uniformity of seeds and yield, so there is a lot of catching up to do. But efforts to improve indigenous vegetables could come at a cost, say researchers. If breeders focus only on increasing yields, they could accidentally eliminate nutritional benefits. And if farmers seek to drive up production through monocrop agriculture \u2014 planting just one crop \u2014 they risk losing some of the qualities that make these vegetables such a draw. Plots with single crops, for example, face higher risks of being completely wiped out by insects or diseases. At the AVRDC's office in Arusha in late February, vegetable breeder Fekadu Dinssa walks through a screened enclosure filled with plants used for breeding. He surveys a table covered with starter trays of little amaranth plants from 57 breeding lines. In one tray, the plants are twice as tall as their neighbours, but their pale colour will not be popular in the market, he says. Dinssa wants to breed the fast-growing trait into other lines to develop a new type of commercially viable amaranth. It is a trial-and-error process that can take years. \n               Strength in diversity \n             As indigenous vegetables are planted in greater numbers, it will be a challenge to prevent less-common varieties from disappearing, say researchers. That could threaten the crops' resilience, because different varieties can carry separate genes for resistance to pathogens and pests. Loss of diversity could also limit the vegetables' appeal. In Kenya, for example, coastal communities tend to like giant African nightshade, whereas western communities prefer a variety with smaller leaves that has a much more bitter taste. Some narrowing of choices has already happened. Simlaw Seeds in Nairobi, a division of Kenya Seed Company, sells only a couple of varieties each of amaranth and African nightshade, chosen because they are the most popular at the national level. \u201cOf course it's a concern, because practically speaking, we can't promote them all,\u201d says Abukutsa. She and other researchers compromise by promoting certain types while trying to preserve the full diversity in gene banks in Kenya and at the AVRDC. The researchers also encourage communities to continue growing the varieties they have traditionally favoured. Calestous Juma, director of the Science, Technology, and Globalization Project at Harvard University in Cambridge, Massachusetts, sees these efforts as crucial. And with advances in genomics, he says, researchers should seek ways to improve indigenous crops \u2014 by lengthening their shelf life, for example \u2014 and to use them in breeding other plants. \u201cThey may have traits that may be useful for other crops.\u201d Juma, who served on the NRC's lost-crops panel, urges more agricultural research centres in Africa to study these vegetables. The work that Abukutsa and her colleagues are doing, he says, \u201cshould be done at every university\u201d. On a hot Wednesday morning in March, Abukutsa walks around the university campus to survey some of her students' work. One has spread amaranth leaves in a wooden box in the sunlight to test how drying will alter the plants' nutritional profile. Abukutsa stops to talk to another student standing amid dozens of rows of recently sprouted African nightshade plants, part of an experiment on their genetic diversity. \u201cWe've come so far,\u201d Abukutsa says, \u201cbut there's still so much to be done.\u201d \n                 Tweet \n                 Follow @NatureNews \n               \n                     Agriculture: State-of-the-art soil 2015-Jan-14 \n                   \n                     African agriculture: Dirt poor 2012-Mar-28 \n                   \n                     Africa's neglected bounty 2006-Oct-31 \n                   \n                     Nature  special: Africa \n                   \n                     The World Vegetable Center \n                   \n                     World Agroforestry Centre \n                   Reprints and Permissions"},
{"file_id": "522142a", "url": "https://www.nature.com/articles/522142a", "year": 2015, "authors": [{"name": "Sara Reardon"}], "parsed_as_year": "2006_or_before", "body": "DARPA is making a big push into biological research \u2014 but some scientists question whether its high-risk approach can work. When Geoffrey Ling talks about the future of technology, his ideas go flying around the room like a whirlwind. Ling eagerly describes a world in which people live far beyond their natural lifespans, minds can be downloaded into external 'hard drives' for enhancement by artificial intelligence and robots and aircraft are controlled by human thought. \u201cIt's abso-posi-frickin-lutely going to happen,\u201d he declares. \u201cThe next 20 years are going to make our heads spin, because we've already crossed over into that realm.\u201d Sara Reardon investigates the US military\u2019s new biology office Ling should know: he is doing as much as anyone to make these visions real. A neurologist by training, he is also a US Army colonel and director of the first biology funding office to operate within the Defense Advanced Research Projects Agency (DARPA), the Pentagon's avant-garde research arm. The Biological Technologies Office (BTO), which opened in April 2014, aims to support extremely ambitious \u2014 some say fantastical \u2014 technologies ranging from powered exoskeletons for soldiers to brain implants that can control mental disorders. DARPA's plan for tackling such projects is being carried out in the same frenetic style that has defined the agency's research in other fields. Ever since it was created in 1958, a year after the Soviet Union beat the United States into space by launching the world's first artificial satellite, Sputnik, the agency's mission has been to prevent any more such surprises by getting there first. So DARPA's programme managers at the BTO are free to pour tens of millions of dollars into ambitious projects without waiting around for niceties such as peer review. And by working closely with its contractors as they develop their technology, the agency aims to drive discoveries across the often-deadly gap between basic research and commercialization. That aggressive, high-risk strategy has had spectacular pay-offs \u2014 most famously with the agency's development of the Internet in the 1970s. And that has happened often enough to inspire imitators such as ARPA-E, a branch of the US Department of Energy that is devoted to high-risk research into alternative energy sources. But some wonder whether DARPA's full-speed-ahead model will work as well for biology as it has for the physical sciences and hardware. Living systems are much more complex, they argue, with a multitude of variables that are either unknown or difficult to engineer and control. And because so much of the agency's biological research is directly applicable to humans, the work is fraught with ethical concerns \u2014 not to mention the possibility that even the most benign-sounding developments could be co-opted for war. Synthetic organisms designed to produce greener biofuels could also make explosives, for example, and brain-stimulation technology intended to heal wounded soldiers could also enhance combat abilities. Edward Hammond, a biology-policy consultant in Austin, Texas, wonders whether the agency often has ulterior motives when it contracts researchers. \u201cYou don't ever really know what DARPA wants,\u201d he says. \u201cBut they're pretty good at finding people who are resolving questions they're interested in for other reasons.\u201d Still, many biologists are willing to accept money from the Department of Defense (DOD), on the grounds that innovations such as better prosthetics and improved mental-health treatments are needed no matter who is paying for them. And Ling insists that DARPA understands the concerns: every programme in the BTO has a bioethics advisory board. Besides, he says, if visionary biotechnologies are inevitable, then it is DARPA's duty to race ahead and invent them. \u201cSome people think it's scary,\u201d he says, contemplating that future. \u201cBut I think it's rather exhilarating.\u201d \n               Time to combine \n             DARPA's embrace of bioscience began in earnest in 2001, when anthrax spores posted to media offices and members of the US Congress brought concerns about bioterrorism to the fore. Then came the wars in Afghanistan and Iraq, which led the agency to invest in fields such as neuroscience, psychology and brain\u2013computer interfaces \u2014 all with the intention of helping injured veterans. By 2013, the number of biology-related programmes had grown such that DARPA decided to consolidate them under one roof. The natural choice to head the new office and its US$288-million annual budget was Ling, who was deputy director of DARPA's science division at the time. The office will certainly speed up research, says George Dyson, an independent science historian in Bellingham, Washington, and not least because of the military's culture of completing missions quickly, without lengthy reflection or debate. Looking at what DARPA has already done in fields such as computing, says Dyson, \u201cit's always the military who move quickly enough to fund the interesting things\u201d. A good example is DARPA's reaction to US President Barack Obama's 2013 announcement of the BRAIN initiative: a high-profile, multi-agency effort to understand the circuitry of the brain. The National Institutes of Health (NIH) spent months designing a ten-year strategic plan for the initiative before distributing its share of the money, and the National Science Foundation (NSF) opened a competition for its spending share to any research project related to brain networks. But DARPA quickly funnelled more than $50 million into just a few five-year programmes. These efforts now fall under the remit of the BTO. One, called Restoring Active Memory, is attempting to create a stimulation device that restores soldiers' ability to form memories after brain damage. Another, called SUBNETS (System-Based Neurotechnology for Emerging Therapies), is developing a brain implant that can treat seven mental and neurological disorders. As a first step, both projects are monitoring the brain activity of people with epilepsy who have had temporary electrodes implanted to locate the origin of their seizures. The investigators ask these patients to carry out memory exercises, or to perform tasks that involve neural pathways that might be impaired in addiction or depression, and record the electrical patterns that result. The pay-offs could still be some way off, however. \u201cThere's no question this is a very ambitious goal,\u201d says Edward Chang, a neurosurgeon at the University of California, San Francisco, who co-leads one of the SUBNETS teams. \u201cI don't think anyone is naive enough to think they'll be easily solved in the next five years.\u201d As ambitious as DARPA is, however, its funding process can be unsettling for researchers who are accustomed to elaborately peer-reviewed grants from civilian agencies. At DARPA, much of the authority is vested in the programme managers, who rotate in and out from academia, industry and the armed services. They alone design the initiatives, invite researchers to apply for contracts with specific goals and milestones and select the groups they think are most likely to achieve the goals. Then they work closely with the researchers to guide the project as it proceeds. DARPA calls its grant recipients 'performers' \u2014 and if they do not meet their milestones, the axe can fall quickly. In 2007, for example, DARPA started a programme called RealNose: an effort to develop a synthetic dog nose with real olfactory receptors for detecting odorants such as chemical weapons. But the agency killed the programme three years later, after it became clear that the receptor proteins were too unstable at room temperature. Researchers who follow DARPA's choreography are almost always free to publish their results, says BTO deputy director Alicia Jackson: very few of the agency's projects are classified as secret. But DARPA grant recipients do give up a certain amount of freedom: if they come across an interesting scientific question as they work, for example, they cannot use DARPA funding to pursue it. \u201cInitially it was a change in culture,\u201d says Emad Eskandar, a neurosurgeon at Massachusetts General Hospital in Boston and director of one of the SUBNETS programmes. But Eskandar and his partner, psychiatrist Darin Dougherty, maintain that DARPA's oversight has made the project better. \u201cIt's helped us to focus and move ahead,\u201d Dougherty says. Certainly, Ling is determined to prove that DARPA's model can work as well for biologists as for military contractors. One of his favourite successes is a prosthetic arm that DARPA developed in collaboration with the biotechnology firm DEKA in Manchester, New Hampshire. The device works by picking up the electrical signals that travel from the brain's motor cortex into nerves in the stump, then translating those signals into the appropriate motions of the attached prosthetic hand. This allows wearers to perform difficult tasks such as handling soft fruit and even rock climbing. The device won approval from the US Food and Drug Administration last year \u2014 the first nerve-controlled prosthetic to do so \u2014 and the company says that it is now working on commercialization. Similar arms are being developed for DARPA at the Johns Hopkins University in Baltimore, Maryland, and elsewhere; all of them are also being tested in people with paralysis in the hope that brain implants can translate their intentions into electrical signals that drive the hand. The BTO has also taken over DARPA's health programmes, including one that is seeking to turn bacteria that prey on other bacteria into therapeutic antimicrobial agents. Other programmes have more obvious military applications, such as an exoskeleton that boosts a soldier's strength and speed. A programme called Narrative Networks studies how the brain reacts to different stories and arguments, which could be helpful for planning how to convince a disaster-stricken village to accept US military aid, or to turn terrorists away from their agendas. And several synthetic-biology initiatives are making biological systems that can be programmed to produce any compound a user wants, including some that do not exist in nature. These could include materials for making lightweight body armour, coatings for strengthening equipment, tissues that can be used to repair wounds, and more-efficient biofuels. Ling and his DARPA colleagues revel in such ideas \u2014 the more far out, the better. \u201cWe look for ways to say yes, not no,\u201d he says. For all its breakthrough successes, however, there is little evidence that DARPA's fast-track approach is consistently any better than peer review at choosing winners. \u201cThey've been successful when they've been successful,\u201d says Jonathan Moreno, a bioethicist at the University of Pennsylvania in Philadelphia. A DARPA spokesperson says that the agency cannot determine how often goals are met or contracts are cancelled. One reason is that the goalposts keep moving: if a project starts to seem unfeasible, programme managers often change the criteria for success and salvage what they can rather than cancelling the contract. Another is that unlike civilian agencies such as the NSF and the NIH, DARPA does not make public the grants that it makes. Nor does it conduct internal analyses that could determine whether its programme managers are choosing the best teams and paying for the best science that they possibly could. \u201cTo me, that's a big problem,\u201d says Pierre Azoulay, an economist at Massachusetts Institute of Technology in Cambridge. \u201cPointing to great successes is not enough,\u201d he says. The agency's idea of programme evaluation is \u201cvery much in the mode of, 'Look, the Internet!'.\u201d But Jackson literally laughs at the idea that the BTO should be more introspective. The office's budget is 1% of the size of the NIH's, she says, with little margin for overhead costs. And besides, she says, \u201cwe go with whoever can get the job done\u201d \u2014 never mind factors such as experience or lab size. \u201cI think we have a really good track record in our 50-plus-year history,\u201d she says. Listing DARPA's successes, she starts with the Internet. But if DARPA is not slowing down to evaluate its successes, is it evaluating the effect they could have on society? Ling says yes, pointing to the ethicists who provide ongoing guidance on the implications of the BTO's work. That is far beyond the level of scrutiny given to most NIH- and NSF-funded projects, notes James Giordano, a neuroethicist at Georgetown University in Washington DC, and an adviser on SUBNETS. Usually, these undergo ethics evaluation only at their beginning or end. Moreno agrees. \u201cThe irony is that people think the national-security world is so far behind the civilian world on these things,\u201d he says. \u201cBut again and again, DOD has been ahead.\u201d Nevertheless, some researchers continue to be sceptical. At Oregon Health & Science University in Portland, emeritus neuroscientist Curtis Bell worries that technologies such as brain stimulation could be used to subdue people, in a similar way to the prefrontal lobotomies that were used in the mid-twentieth century to deal with some troublesome prisoners. \u201cYou could imagine such things being more sophisticated nowadays,\u201d he says. \u201cYou wouldn't need to damage all the frontal lobes if you could go to a specific nucleus and alter someone's personality.\u201d Dyson points out that there is no guarantee that the Pentagon will actually listen to ethicists' concerns \u2014 or to DARPA's. \u201cSome of these technologies are absolutely fascinating and intriguing and hold all this promise for good, but they're very close to being weaponized easily,\u201d he says. And, says Moreno, although many people in the military think deeply about the implications of new technologies, the worry is that the political authorities above them may not allow them much freedom to slow down or change direction. Of particular concern are the BTO's synthetic-biology programmes. The Pentagon has talked about engineering bacteria to clean up sites contaminated by radiation or chemical weapons, stoking fears that these organisms could get out of control when released into the environment. Although there is no reason to think that the United States is creating synthetic biological weapons, some fear even the intimation that microbes are strategically useful. \u201cIt's sending a signal that there's a role for synthetic-biology products for use in the field,\u201d says Hammond. \u201cI would be concerned about that, and I'm concerned that DARPA doesn't seem to be.\u201d But other researchers are more supportive of the BTO. Ultimately, says Giordano, it may not matter who funds the research and who accepts the funding, because anyone can use published research for their own ends. \u201cIndividuals who look at DOD funding as Darth Vader science don't recognize that any science can be channelled through Darth Vader channels.\u201d That is exactly why Ling feels that DARPA needs to jump into controversial science without hesitation: if the United States does not do it, someone else will. \u201cThe only thing we can do is do the work,\u201d he says, \u201cbut do it in a way where we're thinking about the untold consequences and how to mitigate them.\u201d Ling says that he plans to keep expanding his office over the next year \u2014 how far depends on funding \u2014 to anticipate surprises coming from any sector. The BTO currently has 11 programme managers specializing in fields from infectious disease to natural ecosystems, and is looking to expand its repertoire to even more-far-flung fields such as palaeontology and astronomy. An expert in exoplanets, Ling says, could develop projects in preparation for the possibility of threats from outer space as well as the more likely scenario that signs of life will be discovered on another planet. \u201cThat is without a doubt going to be the most exciting scientific news in the history of mankind,\u201d he says. \u201cAnd I'd love for it to be funded by DARPA.\u201d \n                     Biodefence researchers seek 'Homo chippiens' 2015-Feb-17 \n                   \n                     Memory-saving devices snag US research funds 2014-Jul-10 \n                   \n                     Bioengineers debate use of military money 2011-Nov-22 \n                   \n                     Pentagon turns to 'softer' sciences 2010-Apr-14 \n                   \n                     Defence research: Still in the lead? 2008-Jan-23 \n                   \n                     DARPA Biological Technologies Office \n                   Reprints and Permissions"},
{"file_id": "523020a", "url": "https://www.nature.com/articles/523020a", "year": 2015, "authors": [{"name": "Gabriel Popkin"}], "parsed_as_year": "2006_or_before", "body": "Researchers are racing to determine whether forests will continue to act as a brake on climate change by soaking up more carbon. In a forest just west of Chesapeake Bay, Geoffrey Parker wraps a tape measure around a young tulip tree. He jots the reading down in a field notebook, marks the tree with blue chalk and moves on to the next trunk. Parker spends about 10 seconds on each tree. Wrap, measure, record. Since 1987, he and others have logged more than 300,000 tree measurements at their plots in the Smithsonian Environmental Research Center (SERC) near Edgewater, Maryland. This 1,070-hectare site is filled with tulip trees, oaks, beeches and other mostly deciduous trees. Some stout specimens have stood here for centuries. Others are just a decade old, sprouting from land that was recently logged. To keep tabs on the growth, the researchers measure their trees every three to five years. All that patient record-keeping can help to answer two major questions about climate change: how much carbon dioxide pollution are forests mopping up, and will their capacity shrink over time? Studies from Parker's group and others reveal that trees around the globe are going through a growth spurt and are absorbing billions of tonnes of the greenhouse gas, meaning that forests are putting a brake on global warming. But there is no guarantee that forests will keep that up, Parker says. \u201cI think of it like these performance enhancers that some stellar athletes use: it bumps up performance, but not for ever.\u201d In fact, studies of some regions suggest that forest growth may already be slowing down. And humans are adding to the problem by cutting down trees, especially in tropical forests. Getting an accurate reading on the status of Earth's forests is hard because scientists cannot wrap measuring tapes around the roughly 400 billion trees scattered across the planet. So researchers are exploring ways to track forest growth more efficiently, using planes and satellites. And they are feeding all of their data into sophisticated computer models that are designed to forecast how trees will respond in the future. Such forest measurements are sorely needed as nations wrestle with how to slow climate change. Some plans call for wealthy governments or private companies to pay poorer nations in return for safeguarding the carbon in their forests. With a major international climate negotiation approaching later this year, and billions of dollars in forest payments potentially on the table, scientists are racing to advise countries and other stakeholders about just how much carbon trees are storing, and how long that carbon will stay locked up. \u201cThe critical thing that matters is to what extent the biosphere remains a brake on the rate of global climate change,\u201d says Yadvinder Malhi, a forest ecologist at the University of Oxford, UK. That brake will weaken or disappear if forests take up carbon more slowly. Worse, if forests start emitting more carbon than they absorb each year, they could become an accelerator. If that were to happen, says Malhi, \u201cit makes it all the more challenging for us to bring CO 2  down to avoid some threshold of dangerous climate change\u201d. \n               The missing sink \n             In the 1990s, researchers stumbled across a mystery when they tried to track down all of the carbon humans were emitting by burning fossil fuels. Measurements showed that roughly three-quarters of the CO 2  was accumulating in the atmosphere and oceans. The remainder was presumably captured on land, but no one knew where it was going. The problem became known as the 'missing sink'. The world's forests, which pull carbon out of the air through photosynthesis, were a possible hiding place. Today, they collectively hold around 650 billion tonnes of carbon, and it seemed plausible that they could be mopping up the missing carbon. But ecologists were slow to acknowledge that forests could be the missing sink. The community's reticence resulted largely from the work of pioneering ecologist Eugene Odum. He argued in the late 1960s that undisturbed ecosystems rapidly reach an equilibrium, after which they lose as much carbon through respiration, death and decay as they gain through photosynthesis 1 . Without much evidence to the contrary, Odum's paradigm held sway for several decades. \u201cMathematically, it's convenient if something is in equilibrium,\u201d says Sebastiaan Luyssaert of the Laboratory for Climate Sciences and the Environment in Gif-sur-Yvette, France. \u201cWe were happy with it, because it made life easier.\u201d That started to change as ecologists analysed long-term data from big networks of forest research plots. Many of the measurements came from a trio of projects: the Amazon Forest Inventory Network (RAINFOR), the African Tropical Rainforest Observation Network (AfriTRON) and the Smithsonian's Forest Global Earth Observatories (ForestGEO) network, which includes the SERC forest and 61 other plots around the world. Starting in the late 1990s, scientists with the RAINFOR and AfriTRON networks began reporting that intact tropical forests were adding biomass, contradicting Odum's hypothesis. At the Chesapeake site, Smithsonian ecologist Sean McMahon and his colleagues analysed 22 years' worth of data and found that tree stands of all ages were growing two to four times faster than expected 2 . The tree growth records are backed up by CO 2  measurements taken on tall towers at more than 20 sites in North America and Europe: these 'flux towers' have revealed that many forests are absorbing more CO 2  than they are giving off 3 . Researchers suspect several factors are at play. Because trees require CO 2  for photosynthesis, the atmospheric build-up of this gas can fertilize plants, allowing them to grow faster. Also, CO 2  warms the planet, which can lengthen the growing seasons of trees and speed up temperature-dependent processes involved in growth. Scientists are currently teasing out which factors have the largest roles. Whatever the cause, all that accelerated growth is having a major effect on the global carbon cycle. In 2011, an international team led by US Forest Service researchers Yude Pan and Richard Birdsey concluded that the world's trees had sequestered enough carbon during the period from 1990 to 2007 to account for the entire missing sink 4 . The hungriest carbon absorbers were the temperate forests, particularly areas where abandoned farmland had given way to young, fast-growing trees. High-latitude boreal forests ate up a smaller amount, and tropical forests, on balance, were not taking up carbon because tropical deforestation released about as much CO 2  as forests were soaking up. The team projected that if deforestation were halted, Earth's forests could take up around half of the carbon emitted by human activity, which would substantially slow down global warming. But the uncertainties in these estimates are large because forest data are sparse and vary widely in quality. Many countries have no systematic forest inventory system or do not share their data. In their analysis, Pan and Birdsey relied largely on RAINFOR and AfriTRON for assessing the globe's old-growth tropical forests. These networks collectively sample just a few square kilometres in the Amazon and Africa; they have no data from the large and diverse tropical forests of southeast Asia. Beyond determining the size and location of the forest sink, scientists are trying to assess whether it is changing. In March, the RAINFOR team analysed more than 850,000 measurements of approximately 189,000 individual trees and found that the large Amazon forest carbon sink seems to be shrinking 5 . Carbon uptake in their plots during the past decade was one-third smaller than during the 1990s. The researchers suspect multiple factors might be at play. Major droughts that hit the Amazon in 2005 and 2010 could have slowed tree growth during this period. Meanwhile, rising temperatures and CO 2  levels may be accelerating the life cycles of trees: if so, trees are now dying earlier than expected, says Roel Brienen, an ecologist at the University of Leeds, UK, and lead author of the study. Some other researchers are not convinced by the evidence. Helene Muller-Landau, an ecologist at the Smithsonian's Tropical Forest Research Institute on Barro Colorado Island in Panama, thinks that the RAINFOR group is finding an apparent decline now largely because it overestimated the Amazonian carbon sink during the 1990s. The group's plots, she says, sample too small an area \u2014 just three square kilometres out of the vast two-million-square-kilometre Amazon \u2014 to support its broad claims. \u201cIf you actually look at the area covered, it's just so pitifully small,\u201d she says. There can also be bias in how researchers have typically chosen plots and measured biomass, Muller-Landau says. Tropical forests can be hot, humid, buggy, dangerous and in some cases nearly impossible to reach. So rather than sample randomly, scientists often choose study sites based on ease of access. And biomass estimates vary depending on the choice of species-specific equations used to convert circumference and height measurements; for many tropical trees, reliable equations are still being worked out. Although no one doubts that forests are taking up some of the CO 2  emitted by human activity, scientists are still unsure which forests are sequestering the most carbon, and how much is stored in long-lasting wood versus in roots and soil. \n               Help from above \n             Researchers will only ever be able to measure a tiny fraction of the world's trees by wrapping tapes around them one at a time, so they are taking to the skies to get a broader perspective. Some planes and satellites are outfitted with laser-based lidar systems that measure the height of the tree tops. Scientists can then estimate an area's biomass by using the forest's average canopy height and tree type. Plane-mounted lidar can collect data for 35,000 hectares in one hour, says Gregory Asner, an ecologist with the Carnegie Institution for Science in Stanford, California. The uncertainties in his lidar-based forest biomass estimates are now down to around 10%, comparable to those from ground-based studies, he says, although others say the uncertainties in both types of estimates are larger. For a truly global view, scientists agree that nothing beats a satellite. Current Earth-observing satellites lack the resolution of plane- or ground-based measurements but can fill in areas where data are scarce or non-existent. NASA's Orbiting Carbon Observatory-2 (OCO-2), launched in July 2014, will soon provide fresh data to help locate the missing sink. The satellite uses spectrometers to measure concentrations of CO 2  to within a few parts per million, allowing scientists to pinpoint the locations where carbon is being emitted and sequestered (see 'Tree tales'). A separate measurement by the same instrument can determine how much photosynthesis is occurring at a specific location. Although OCO-2 does not measure tree biomass directly, it will provide enough data for scientists to determine how much carbon is entering and leaving different ecosystems. NASA expects to release preliminary results from the satellite by the end of the year, but it will be at least several years before the data can address whether forest sinks are changing. And even then, the OCO-2 measurements won't answer whether carbon is going into trees, soil or somewhere else, so ground-based observations will still be needed, says David Crisp, chief scientist for OCO-2. \n               Tomorrow's trees \n             Other scientists seeking to predict the carbon sink's future are turning the clocks forward \u2014 with experiments that expose today's forests to future conditions. One strategy involves piping CO 2  into a forest to raise concentrations from the present 400 parts per million to roughly 550 parts per million \u2014 a level expected before this century's end. In experiments in the United States and Europe, trees dosed with extra CO 2  grew faster, just as expected. But the effects often did not last. One explanation is that enhanced trees may quickly use up other vital nutrients, such as nitrogen, says ecologist Richard Norby at Oak Ridge National Laboratory in Tennessee, who led one of the experiments. Researchers from the United States, the United Kingdom and Brazil are now building a CO 2 -enrichment experiment near Manaus, Brazil (see   Nature   496 , 405\u2013406; 2013), which they hope to start next year. That experiment will provide valuable information about trees in the tropics, but it will not represent the future of all forests in that region, says Simon Lewis, an ecologist at the University of Leeds and University College London who is involved in the RAINFOR and AfriTRON networks. The region around Manaus has poorer soils than other parts of the Amazon so trees grow more slowly, says Lewis, and \u201cit will take longer for the impacts to be seen\u201d. In the meantime, researchers are trying other methods to peer into the future. Some 20 teams have built Earth-system models that seek to simulate the climate and vegetation on the planet, including how carbon moves between the oceans, atmosphere and continents. These models currently represent forests in a simplified manner, and they disagree about the future. Some predict that forests will continue to soak up massive amounts of carbon in coming decades, whereas others suggest that forests could become stressed by droughts and high temperatures and die back, releasing carbon into the atmosphere. The emerging insights about forests \u2014 from individual tree measurements to satellite data to computer simulations \u2014 will all play a part in how countries decide to manage their resources. And that has implications for global climate negotiations because some carbon-reduction schemes rely on rewarding nations for keeping carbon locked up in forests. For that to work, researchers will need to find reliable ways to track the changing amounts of forest carbon. The current level of uncertainty in forest biomass estimates \u201cdoes not exactly provide a lot of confidence\u201d, Muller-Landau says. \u201cHaving something verifiable would have to be fairly key\u201d for carbon accounting, she adds. To that end, scientists such as Parker are developing more precise ways to monitor trees growing in their experimental plots. On a cloudy spring day at the Smithsonian's Chesapeake site, Parker directs volunteers to install spring-tensioned steel bands called dendrometers in a 130-year-old stand. As the tree trunks expand over time, they will widen gaps in the bands, which can be measured using digital calipers. The technique can track changes down to a hundredth of a millimetre \u2014 thinner than a human hair \u2014 giving researchers an unprecedented ability to study growth patterns. The method can even detect how trees swell and contract over a few hours as they absorb or lose water. By the end of the day, Parker's team has finished attaching several more dendrometers. More than a thousand trees at the Smithsonian centre now sport the metal rings, and their number is increasing around the world. Parker puts his equipment in a truck and drives off towards home. But he and his crew will be back soon to check how their trees are responding as Earth's climate \u2014 and its forests \u2014 enter uncharted territory. \n                     Stopping deforestation: Battle for the Amazon 2015-Apr-01 \n                   \n                     Carbon sequestration: Managing forests in uncertain times 2014-Feb-12 \n                   \n                     Northern forests rev up carbon cycle 2013-Aug-08 \n                   \n                     Forest ecology: Splinters of the Amazon 2013-Apr-17 \n                   \n                     Climate: Counting carbon in the Amazon 2009-Oct-21 \n                   \n                     Nature  special: Planetary boundaries \n                   \n                     Nature  special: Outlook for Earth \n                   \n                     Smithsonian Environmental Research Center \n                   \n                     Amazon Forest Inventory Network \n                   \n                     African Tropical Rainforest Observation Network \n                   \n                     Carnegie Airborne Observatory \n                   \n                     Orbiting Carbon Observatory-2 \n                   Reprints and Permissions"},
{"file_id": "523024a", "url": "https://www.nature.com/articles/523024a", "year": 2015, "authors": [{"name": "Boer Deng"}], "parsed_as_year": "2006_or_before", "body": "Working out how to build ethical robots is one of the thorniest challenges in artificial intelligence. In his 1942 short story 'Runaround', science-fiction writer Isaac Asimov introduced the Three Laws of Robotics \u2014 engineering safeguards and built-in ethical principles that he would go on to use in dozens of stories and novels. They were: 1) A robot may not injure a human being or, through inaction, allow a human being to come to harm; 2) A robot must obey the orders given it by human beings, except where such orders would conflict with the First Law; and 3) A robot must protect its own existence as long as such protection does not conflict with the First or Second Laws. Fittingly, 'Runaround' is set in 2015. Real-life roboticists are citing Asimov's laws a lot these days: their creations are becoming autonomous enough to need that kind of guidance. In May, a panel talk on driverless cars at the Brookings Institution, a think tank in Washington DC, turned into a discussion about how autonomous vehicles would behave in a crisis. What if a vehicle's efforts to save its own passengers by, say, slamming on the brakes risked a pile-up with the vehicles behind it? Or what if an autonomous car swerved to avoid a child, but risked hitting someone else nearby? Geoff Marsh speaks to Boer Deng about designing robots to handle moral dilemmas \u201cWe see more and more autonomous or automated systems in our daily life,\u201d said panel participant Karl-Josef Kuhn, an engineer with Siemens in Munich, Germany. But, he asked, how can researchers equip a robot to react when it is \u201cmaking the decision between two bad choices\u201d? The pace of development is such that these difficulties will soon affect health-care robots, military drones and other autonomous devices capable of making decisions that could help or harm humans. Researchers are increasingly convinced that society's acceptance of such machines will depend on whether they can be programmed to act in ways that maximize safety, fit in with social norms and encourage trust. \u201cWe need some serious progress to figure out what's relevant for artificial intelligence to reason successfully in ethical situations,\u201d says Marcello Guarini, a philosopher at the University of Windsor in Canada. Several projects are tackling this challenge, including initiatives funded by the US Office of Naval Research and the UK government's engineering-funding council. They must address tough scientific questions, such as what kind of intelligence, and how much, is needed for ethical decision-making, and how that can be translated into instructions for a machine. Computer scientists, roboticists, ethicists and philosophers are all pitching in. \u201cIf you had asked me five years ago whether we could make ethical robots, I would have said no,\u201d says Alan Winfield, a roboticist at the Bristol Robotics Laboratory, UK. \u201cNow I don't think it's such a crazy idea.\u201d \n               Learning machines \n             In one frequently cited experiment, a commercial toy robot called Nao was programmed to remind people to take medicine. \u201cOn the face of it, this sounds simple,\u201d says Susan Leigh Anderson, a philosopher at the University of Connecticut in Stamford who did the work with her husband, computer scientist Michael Anderson of the University of Hartford in Connecticut. \u201cBut even in this kind of limited task, there are nontrivial ethics questions involved.\u201d For example, how should Nao proceed if a patient refuses her medication? Allowing her to skip a dose could cause harm. But insisting that she take it would impinge on her autonomy. To teach Nao to navigate such quandaries, the Andersons gave it examples of cases in which bioethicists had resolved conflicts involving autonomy, harm and benefit to a patient. Learning algorithms then sorted through the cases until they found patterns that could guide the robot in new situations 1 . With this kind of 'machine learning', a robot can extract useful knowledge even from ambiguous inputs (see  go.nature.com/2r7nav ). The approach would, in theory, help the robot to get better at ethical decision-making as it encounters more situations. But many fear that the advantages come at a price. The principles that emerge are not written into the computer code, so \u201cyou have no way of knowing why a program could come up with a particular rule telling it something is ethically 'correct' or not\u201d, says Jerry Kaplan, who teaches artificial intelligence and ethics at Stanford University in California. Getting around this problem calls for a different tactic, many engineers say; most are attempting it by creating programs with explicitly formulated rules, rather than asking a robot to derive its own. Last year, Winfield published the results 2  of an experiment that asked: what is the simplest set of rules that would allow a machine to rescue someone in danger of falling into a hole? Most obviously, Winfield realized, the robot needed the ability to sense its surroundings \u2014 to recognize the position of the hole and the person, as well as its own position relative to both. But the robot also needed rules allowing it to anticipate the possible effects of its own actions. Winfield's experiment used hockey-puck-sized robots moving on a surface. He designated some of them 'H-robots' to represent humans, and one \u2014 representing the ethical machine \u2014 the 'A-robot', named after Asimov. Winfield programmed the A-robot with a rule analogous to Asimov's first law: if it perceived an H-robot in danger of falling into a hole, it must move into the H-robot's path to save it. Winfield put the robots through dozens of test runs, and found that the A-robot saved its charge each time. But then, to see what the allow-no-harm rule could accomplish in the face of a moral dilemma, he presented the A-robot with two H-robots wandering into danger simultaneously. Now how would it behave? The results suggested that even a minimally ethical robot could be useful, says Winfield: the A-robot frequently managed to save one 'human', usually by moving first to the one that was slightly closer to it. Sometimes, by moving fast, it even managed to save both. But the experiment also showed the limits of minimalism. In almost half of the trials, the A-robot went into a helpless dither and let both 'humans' perish. To fix that would require extra rules about how to make such choices. If one H-robot were an adult and another were a child, for example, which should the A-robot save first? On matters of judgement like these, not even humans always agree. And often, as Kaplan points out, \u201cwe don't know how to codify what the explicit rules should be, and they are necessarily incomplete\u201d. Advocates argue that the rule-based approach has one major virtue: it is always clear why the machine makes the choice that it does, because its designers set the rules. That is a crucial concern for the US military, for which autonomous systems are a key strategic goal. Whether machines assist soldiers or carry out potentially lethal missions, \u201cthe last thing you want is to send an autonomous robot on a military mission and have it work out what ethical rules it should follow in the middle of things\u201d, says Ronald Arkin, who works on robot ethics software at Georgia Institute of Technology in Atlanta. If a robot had the choice of saving a soldier or going after an enemy combatant, it would be important to know in advance what it would do. With support from the US defence department, Arkin is designing a program to ensure that a military robot would operate according to international laws of engagement. A set of algorithms called an ethical governor computes whether an action such as shooting a missile is permissible, and allows it to proceed only if the answer is 'yes'. In a virtual test of the ethical governor, a simulation of an unmanned autonomous vehicle was given a mission to strike enemy targets \u2014 but was not allowed to do so if there were buildings with civilians nearby. Given scenarios that varied the location of the vehicle relative to an attack zone and civilian complexes such as hospitals and residential buildings, the algorithms decided when it would be permissible for the autonomous vehicle to accomplish its mission 3 . Autonomous, militarized robots strike many people as dangerous \u2014 and there have been innumerable debates about whether they should be allowed. But Arkin argues that such machines could be better than human soldiers in some situations, if they are programmed never to break rules of combat that humans might flout. Computer scientists working on rigorously programmed machine ethics today favour code that uses logical statements, such as 'If a statement is true, move forward; if it is false, do not move.' Logic is the ideal choice for encoding machine ethics, argues Lu\u00eds Moniz Pereira, a computer scientist at the Nova Laboratory for Computer Science and Informatics in Lisbon. \u201cLogic is how we reason and come up with our ethical choices,\u201d he says. Crafting instructions capable of the logical steps that go into making ethical decisions is a challenge. For example, Pereira notes, the logical languages used by computer programs have trouble coming to conclusions about hypothetical scenarios, but such counterfactuals are crucial in resolving certain ethical dilemmas. One of these is illustrated by the trolley problem, in which you imagine a runaway railway trolley is about to kill five innocent people who are on the tracks. You can save them only if you pull a lever that diverts the train onto another track, where it will hit and kill an innocent bystander. What do you do? In another set-up, the only way to stop the trolley is to push the bystander onto the tracks. People often answer that it is all right to stop the trolley by hitting the lever, but viscerally reject the idea of pushing the bystander. The basic intuition, known to philosophers as the doctrine of double effect, is that deliberately inflicting harm is wrong, even if it leads to good. However, inflicting harm might be acceptable if it is not deliberate, but simply a consequence of doing good \u2014 as when the bystander simply happens to be on the tracks. This is a very difficult line of analysis for a decision-making program. To begin with, the program must be able to see two different futures: one in which a trolley kills five people, and another in which it hits one. The program must then ask whether the action required to save the five is impermissible because it causes harm, or permissible because the harm is only a side effect of causing good. To find out, the program must be able to tell what would happen if it chose not to push the bystander or pull the lever \u2014 to account for counterfactuals. \u201cIt would be as if a program was constantly debugging itself,\u201d says Pereira \u2014 \u201cfinding where in a line of code something could be changed, and predicting what the outcome of the change would be.\u201d Pereira and Ari Saptawijaya, a computer scientist at the University of Indonesia in Depok, have written a logic program 4  that can successfully make a decision based on the doctrine of double effect, as well as the more sophisticated doctrine of triple effect, which takes into account whether the harm caused is the intended result of the action, or simply necessary to it. \n               Humans, morals, machines \n             How ethical robots are built could have major consequences for the future of robotics, researchers say. Michael Fisher, a computer scientist at the University of Liverpool, UK, thinks that rule-bound systems could be reassuring to the public. \u201cPeople are going to be scared of robots if they're not sure what it's doing,\u201d he says. \u201cBut if we can analyse and prove the reasons for their actions, we are more likely to surmount that trust issue.\u201d He is working with Winfield and others on a government-funded project to verify that the outcomes of ethical machine programs are always knowable. By contrast, the machine-learning approach promises robots that can learn from experience, which could ultimately make them more flexible and useful than their more rigidly programmed counterparts. Many roboticists say that the best way forward will be a combination of approaches. \u201cIt's a bit like psychotherapy,\u201d says Pereira. \u201cYou probably don't just use one theory.\u201d The challenge \u2014 still unresolved \u2014 is to combine the approaches in a workable way. These issues may very soon come up in the fast-moving field of autonomous transport. Already, Google's driverless cars are zipping across parts of California (see  Nature   518 , 20\u201323; 2015 ). In May, autonomous trucks from German car-maker Daimler began driving themselves across the Nevada desert. Engineers are thinking hard about how to program cars to both obey rules and adapt to situations on the road. \u201cUp until now we've been trying to do things with robots that humans are bad at,\u201d such as maintaining attention on long drives or being quick on the brakes when the unexpected occurs, says Bernhard Weidemann, a spokesperson for Daimler in Stuttgart. \u201cGoing forward, we will have to try to program things that come more naturally to humans, but not to machines.\u201d \n                     Robotics: Ethics of artificial intelligence 2015-May-27 \n                   \n                     Intelligent robots must uphold human rights 2015-Mar-24 \n                   \n                     Robo-rescuers battle it out in disaster challenge 2015-Mar-03 \n                   \n                     Computer science: The learning machines 2014-Jan-08 \n                   \n                     IEEE: What is robot ethics? \n                   \n                     Robot ethics: Morals and the machine \n                   \n                     Robotics research sponsored by the US Office of Naval Research \n                   Reprints and Permissions"},
{"file_id": "523276a", "url": "https://www.nature.com/articles/523276a", "year": 2015, "authors": [{"name": "Monya Baker"}], "parsed_as_year": "2006_or_before", "body": "A look at some of the most innovative science-education programmes from kindergarten to university. The five-year-olds are confident: trees, they agree, make the wind by shaking their branches. Their teacher does not correct them, but instead asks whether anyone has seen the wind in a place where there are no trees. One boy recalls a visit to the seashore, where the wind was whipping up water and sand with no trees in sight. Another child says that moving cars make fallen leaves twirl. Perhaps, they decide, trees are not the source of a breeze. So goes a typical day for participants of Germany's  Haus der kleinen Forscher  (Little Scientists' House), a programme that in less than a decade has grown to reach about half of that country's children between ages three and six. Launched in 2006 by a group of German business leaders who were dismayed by their country's lacklustre performance on international student exams, Little Scientists' House got support and funding from the federal government in 2008. Today, versions of the programme are also operating in Australia, Austria, the Netherlands, Brazil and Thailand \u2014 including more than 14,000 centres in Thailand alone. Little Scientists' House is just one of many programmes around the world that try to inspire young people's inner scientists through active engagement with the world around them. The effectiveness of this approach has been verified by hundreds of empirical studies. \u201cIt means learning content not as something you memorize and regurgitate, but as raw material for making connections, drawing inferences, creating new information \u2014 learning how to learn,\u201d says Jay Labov, a senior education adviser at the US National Academy of Sciences, one of many organizations to endorse this mode of learning. Here,  Nature  profiles innovative exemplars of such engagement, from preschool to university. If someone wanted to turn a toddler into a scientist for the twenty-first century, this is what the curriculum might look like. \n               Preschool experimenters \n             Little Scientists' House marks a departure from educators' traditional role, says Christina Jeuthe, a kindergarten teacher who participates in the programme. \u201cYou have to be willing to do something with the kids that might not lead to a result,\u201d she says. \u201cThey will not take something home that they can show their parents.\u201d Instead, teachers trained in the method try to get children to ask questions about natural phenomena and everyday objects. And when the children give naive answers (for example, that shaking leaves produce wind), the teachers help them to come up with activities to test those answers \u2014 in effect, emulating how grown-up researchers do science. But just as with scientific discovery, the end points are uncertain, says Jeuthe. \u201cI myself had to be strong enough to not put my expectations on a specific scientific question for the kids \u2014 but let them decide, ask and discover.\u201d Noah Baker speaks to teachers, researchers and students about new ways to teach science to children In a unit about water, for example, one five-year-old argued that more water drops could collect on a euro coin than on a slightly larger 50-cent piece because the former buys more. He and his classmates counted how many drops they could dribble onto the coins' surfaces. In the end, the children could not come to a definitive answer, but that is OK, says Jeuthe. The point is to spark questions, and a conviction that they can be explored rationally. Activities start with objects and experiences that children are familiar with \u2014 which can call for considerable creativity when adapting the programme to different places and cultures. The Australian version cannot draw on children's experience of wintry weather; instead, they focus on ice cubes. In Thailand, one activity relies on sky lanterns \u2014 miniature hot-air balloons that are common in holiday festivities. However it is done, the children say that they have fun carrying out their impromptu experiments \u2014 and in the process, say advocates of the programme, the children are learning invaluable lessons on how to plan and solve problems, not to mention gaining self-confidence. Unfortunately, pinning down the programme's effects on students will be hard, warns Mirjam Steffensky, a chemistry educator at the Leibniz Institute for Science and Mathematics Education in Kiel, Germany. If nothing else, she says, comparisons are difficult because educators in each location are free to implement the Little Scientists' House curriculum in different ways. Still, the German Academy of Science and Engineering and other education foundations have commissioned Steffensky and several other researchers to carry out independent assessments of the programme. The three-year studies, which include control groups, will cover hundreds of students from dozens of centres to see whether the programme boosts children's language and science skills. These assessments will not be completed until next year, but a 2013 questionnaire of more than 3,000 participating educators found that they felt more confidence and interest teaching science. \u201cJust give the children the room, the time and the possibility,\u201d says Jeuthe. \u201cBelieve that they will work it out, and they will.\u201d \n               High-school collaborators \n             The Hwa Chong Institute (HCI) is an elite high school in Singapore that enrols only the best-performing students and then gives them access to advanced equipment, including an atomic force microscope and cell-culture incubators. The tools would be the envy of many a university, but to director of studies Har Hui Peng, that is not enough. She has always wanted to give her students an extra challenge, and a flavour of doing science in an interconnected world. She got her chance a decade ago thanks to a lucky encounter with George Wolfe, a US educator who told her that he was setting up the Academy of Sciences (AoS): a selective, publicly funded high school in Sterling, Virginia, where students could design and conduct research. Both recognized a unique opportunity to teach their students a skill essential for twenty-first-century science: collaboration. Every October or November since 2006, a dozen or so 14- and 15-year-old HCI students have travelled to the AoS to start research projects that will last the academic year. They work in teams of four \u2014 two students from each country \u2014 on projects such as screening maggots for antimicrobial compounds. Nine months later, the AoS students join their HCI teammates back in Singapore to complete the final analysis and prepare presentations of the results. Particularly at the beginning, some of the cultural stereotypes applied, says Ashley Ferguson, who took part in the programme as an AoS student. The US students were \u201cmore creative and free-flowing\u201d, she says, whereas their HCI teammates were more focused and directed: they considered what instruments were available and what experiments could be designed around them. \u201cSome of that more-structured thinking was good for us to learn,\u201d says Ferguson, now a senior student at the University of Virginia in Charlottesville. Ernest Chen, an HCI graduate now studying at the University of Cambridge, UK, says that the project taught him the importance of communication. When he hit a snag with his project \u2014 chemically modifying a polymer to sop up dissolved metal ions \u2014 he and the other HCI student in his team wanted to change the methods. This annoyed their AoS teammates, who wanted to stick with the agreed protocol. The resulting e-mail exchanges taught everyone the skills of persistence and persuasion. \u201cInstead of just sending a first e-mail saying, 'I'm going to change this', I would say, 'we tried this, and it doesn't work, therefore we want to change it'.\u201d Several years later, the team still stays in touch over social media. Most important is learning to work effectively as a team, Har and Wolfe agree. The best part is when the students \u201cstart to care for each other\u201d, says Har. For example, students at one school will make sure their part of a project is completed well before another schools' exams to give their colleagues time to study, she says. Such consideration is exactly the point, says Wolfe, now director of the AoS. \u201cOur mission is to teach kids to do science. If you look at what scientists really do in the real world, people don't work in a vacuum.\u201d \n               Teenage researchers \n             Cal Hewitt does his physics calculations by accessing a grid of distributed computers set up in the United Kingdom by CERN, the European particle-physics lab near Geneva, Switzerland. Tapping into the equivalent of nearly 40,000 personal computers, Hewitt and his colleagues are calculating the types, energies and trajectories of particles detected by an experiment developed at his institution and launched into space last year. The group's findings could suggest ways to prevent damage to satellites, and perhaps firm up theories about the source of extragalactic cosmic rays. And with any luck, this will happen before Hewitt turns 18. Hewitt is a student at the Simon Langton school in Canterbury, UK, where students routinely design and perform real, ambitious experiments. Some of the students \u2014 Hewitt included \u2014 have presented their work at scientific conferences; a few have even published original research in the peer-reviewed literature. The school's philosophy is simple, says Becky Parker, who directs the Langton Star Centre, which hosts the school's research programmes: \u201cLet's give students a chance to do real science and get the thrill of discovery.\u201d Simon Langton is a state-funded, elite institution: students are accepted on the basis of an aptitude test at the age of 11. But the school's path to teen research began just over a decade ago, when Parker decided to sign up for a programme that gave secondary students remote access to telescopes in Australia and Hawaii. Rather than opting for the standard teacher-led demonstration, Parker handed the reins to her students \u2014 who used their freedom to confirm the presence of half-a-dozen known asteroids with orbits that bring them near Earth, and went on to discover two new ones. Around the same time, Langton students entered a competition run by the UK National Space Centre to design an experiment that would be conducted in space, basing their proposal on cosmic-ray-detection technology that they had encountered on a field trip to CERN. Contest organizers offered to launch the programme if students found the funding for it. They did. And high-calibre research projects have topped students' extracurricular activities ever since. Now the students are running calculations on data from CERN's MoEDAL (Monopole and Exotics Detector at the Large Hadron Collider) to look for some of physics' most exotic phenomena, such as microscopic black holes. Langton is the only secondary school participating as a full member in any major particle-physics collaboration, says James Pinfold, a particle physicist at the University of Alberta in Edmonton, Canada, and spokesperson for the MoEDAL collaboration. \u201cThis work in space convinced us they could handle the job,\u201d he says. Elsewhere in the school, one student team is using genetic analysis to breed and evaluate drought-resistant strains of wheat. Another is unravelling molecular mechanisms for multiple sclerosis \u2014 a project that required a licence for genetic modification of yeast so that the students could investigate the human gene for myelin basic protein. Langton is the first secondary school to get such a licence. Parker estimates that Langton supplies almost 1% of all students, and at least 2% of female students, who enter undergraduate physics programmes in the United Kingdom. Other secondary schools also promote student-led research. But the scale, scope and quality of the work at the Langton centre make it stand out. To support the work, Parker and her students have raised funds from bodies such as local government and national science organizations. Such awards even supported a particle physicist to work on Langton's campus full time to advise students and build research capacity at other secondary schools. Most credit Parker for the school's scientific success. (At one point, project teams were limited to the number of students she could fit in her car.) But Parker says that teachers are eager to put in the time for extracurricular research once they see what is possible. To help the Langton idea spread, Parker's next project will be the Institute for Research in Schools, which will support school science teachers who want to launch genuine research projects. And that is what education should be, says Caitlin Cooke, a Langton student who works on the MoEDAL team. \u201cBecause we've already experienced so much work at the frontier, it demonstrates to us the reality of what it is to do physics.\u201d Her colleague, Fleur Pomeroy, agrees. \u201cWhy do people question why we can be doing real science?\u201d \n               Interdisciplinary undergraduates \n             When Tyler Heist was considering his first year at university, he decided to throw himself into science with abandon. Most university science courses are run by individual departments and focus on a single discipline. But the Integrated Quantitative Science class at the University of Richmond in Virginia offered simultaneous introductions to five: biology, chemistry, physics, mathematics and computer science. Better still, the course would organize the lessons around interdisciplinary problems such as antibiotic resistance and cells' responses to heat. In 2010, Heist applied for one of the course's 20 available spots and was accepted. Inspired by that experience, he will head off later this year to do doctoral work in computational biology at Princeton University in New Jersey. The origins of the integrated course stem from a report issued more than a decade ago. The US National Research Council concluded that biological research had changed dramatically to incorporate physical and computational sciences, but biological education had not. April Hill, a biology professor at the University of Richmond, thought that the best way to fix that problem was to retool the introductory courses to view core concepts from many disciplines through the lens of real science questions, rather than taking students on the traditional march through the disciplines one by one. Hill and her colleagues ran their course for the first time in 2009. Although interdisciplinary courses are hardly new, Hill's approach stands out for combining five distinct disciplines, for targeting introductory classes, and for including a stint of paid laboratory research in the summer following the course. Ellen Goldey, who chairs the biology department at Wofford College in Spartanburg, South Carolina, says that the University of Richmond effort has inspired other undergraduate institutions to set up similar programmes. \u201cThere is an existing model now so they will not need to reinvent the whole wheel,\u201d she says. Hill says that the extra effort required to integrate multiple disciplines more than pays for itself; the course has prompted cross-disciplinary collaborations in her own work, on gene networks that govern the development of the most basic multicellular creatures. \u201cNow that I have six years of interdisciplinary teaching I can't imagine not doing it,\u201d says Hill. In 2012, the number of students taking interdisciplinary courses doubled at the university, as did efforts to recruit students from a minority background. A companion programme called SMART, now in its second year, serves students with less rigorous high-school preparation. A precollege summer programme full of mentoring and maths helps to prepare students for the interdisciplinary courses. More than 30% of the students who took the integrated class in 2009 and 2010 went on to PhD programmes. Those who take the integrated course are more likely to graduate with a STEM major \u2014 92% versus 60% or less of other undergraduates who start out in STEM. And they also take a greater variety of classes. Heist, for example, says that the programme helped him to get through upper-level classes that required him to read primary biology literature that incorporated concepts from physics or computer science, and credits the course with broadening his approach to scientific investigation. \u201cIt makes you rethink the boundaries you put on things,\u201d he says. \n                     Early child development: Body of knowledge 2015-Jul-15 \n                   \n                     University learning: Improve undergraduate science education 2015-Jul-15 \n                   \n                     The scientist of the future 2015-Jul-15 \n                   \n                     Lifelong learning: Science professors need leadership training 2015-Jul-15 \n                   \n                     Why we are teaching science wrong, and how to make it right 2015-Jul-15 \n                   \n                     Science education: Reading, writing and nanofabrication 2009-Jul-08 \n                   \n                     Learning science like a scientist 2006-Oct-01 \n                   \n                     Nature  special: Building the 21st century scientist \n                   \n                     Little Scientists\u2019 House \n                   \n                     Academy of Science, Loudoun County Public Schools \n                   \n                     Hwa Chong Institution \n                   \n                     Langton Star Centre \n                   \n                     URISE integrated science programme \n                   Reprints and Permissions"},
{"file_id": "523142a", "url": "https://www.nature.com/articles/523142a", "year": 2015, "authors": [{"name": "Alexandra Witze"}], "parsed_as_year": "2006_or_before", "body": "Meet the seismologists who work around the clock to pinpoint major earthquakes anywhere on Earth. At 17 minutes past midnight on Saturday 25 April, Rob Sanders's computer started chiming with alerts. On his screen, squiggly recordings poured in from seismometers in Tibet, Afghanistan and nearby areas that were feeling the first vibrations from  a tremendous earthquake . Sanders was part way through his shift as an on-duty seismologist at the US Geological Survey's National Earthquake Information Center (NEIC) in Golden, Colorado. It was his job to work out what was happening \u2014 and fast. Within 30 seconds, he began analysing the seismic data and realized it was time to call his boss. Find out how on-call geologists from the USGS spot a quake and raise the alarm When the phone rang, Paul Earle was dozing in the room of his four-year-old son, where he had nodded off earlier that evening. Earle rolled out of bed and logged onto his home computer. As chief of 24/7 operations at the NEIC, Earle knew that time was short. For any major earthquake in the world, the US Geological Survey (USGS) is committed to publishing the shock's magnitude and location online within 20 minutes. The team also puts out rapid estimates for how many people may have been hurt. Various nations issue alerts for quakes in their vicinity, but Earle's crew is the only one that analyses tremors around the globe. The NEIC information helps governments and humanitarian groups to decide how to respond in times of crisis. It determines whether search-and-rescue teams pack their bags, and whether financial markets begin responding to a catastrophic natural disaster. When minutes count, hundreds of key responders \u2014 from the White House to the United Nations \u2014 rely on the NEIC team to tell them exactly how bad an earthquake was. On 25 April, the work that began on Sanders's screen ended up with the US government dispatching a response team to the quake's epicentre in Nepal within hours. The NEIC seismologists do not always get it right. Sometimes, deceived by the rawness of the data, they put out an alert containing the wrong quake location or size, before quickly retracting the information. But they are continually refining their techniques to speed up response times while maintaining accuracy. \u201cBeing reliable is more important than pure speed,\u201d says Earle. \n               The night shift \n             The NEIC takes up the fifth floor of a blocky building on the campus of the Colorado School of Mines in Golden, not far from the original Coors brewery and bronze sculptures of the miners who shaped this region of Colorado. A decade ago, television satellite trucks regularly clogged the car park after any large earthquake. Now, most of the journalists stay at home \u2014 they can get information from the centre faster over the Internet. Computer monitors have replaced the slowly rotating paper drums that once displayed the vibrations measured at seismic stations around the world. But the centre has kept one relic on display: a large wooden globe that often appeared in television reports. Patches of its coloured surface are worn away from decades of seismologists jabbing their fingers at earthquake locations. Southern California has basically disappeared. So has Japan. Established in 1966, the NEIC originally operated during normal business hours, with seismologists on call at other times. But in 2004, a magnitude-9.1 earthquake hit Sumatra, triggering a ruinous tsunami that killed almost a quarter of a million people around the Indian Ocean. In an effort to improve its response times in major disasters, the earthquake centre moved to operating around the clock. Fourteen seismologists now cover three shifts, with at least two people on duty at any given time (coordinating their toilet and meal breaks). The NEIC analyses more than 20,000 earthquakes a year, everything from imperceptible ones in California to the monsters that occasionally shake the globe. It reports on any earthquake of magnitude 5 or greater worldwide, and down to magnitude 3 in parts of the United States. On 25 April, the only earthquake that mattered began beneath Nepal. The jolt started 15 kilometres underground, on the huge Himalayan fault where the tectonic plate carrying India rams into Asia. At 11:56 a.m. local time (11 minutes past midnight in Colorado), the stress of that geological collision ruptured a 120-kilometre-long segment of Earth's crust beneath the Nepalese district of Gorkha. Waves of seismic energy raced outwards in all directions. Within 16 seconds they reached Kathmandu, almost 80 kilometres to the southeast, and began toppling thousands of buildings. Just over a minute later they passed Lhasa, 600 kilometres northeast of the epicentre, and shook seismometers bolted into granite in a hillside tunnel. Those machines, part of the Global Seismographic Network, immediately relayed their data to the NEIC. At the Colorado centre, an alert dinged and a window popped up on Sanders's screen, which filled with information from stations around Asia. Sanders started sorting through the data, choosing the best seismic records to include in his analysis. A second seismologist on duty that night called and woke Earle, who began to work on the seismic data from home. As the minutes ticked away, the three of them faced a crucial task \u2014 deciding on the quake's magnitude. The USGS measures eight types of magnitude, each of which conveys different information about the strength of an earthquake's vibrations and the amount of energy it releases. Certain magnitude scales are most accurate for smaller quakes, whereas others are better at describing long-lasting, larger shocks. At 12:29:42 a.m. \u2014 18 minutes and 16 seconds after the earthquake began \u2014 the NEIC released its first answer. Location: 77 kilometres northwest of Kathmandu. Size: 7.5 on the moment magnitude scale. This particular scale relies on computer modelling of a certain type of seismic wave, and Earle chose it because of a gut feeling for what he thought would represent the most meaningful magnitude. But as is often the case with large quakes, the first official magnitude was not the last. The team had only just started its analyses. Earle called and woke up two more colleagues \u2014 Harley Benz and Gavin Hayes \u2014 then jogged the two blocks from his home into work. Even as news agencies began broadcasting alerts of a magnitude-7.5 earthquake in Nepal, the NEIC researchers were sifting through fresh data. From his home, Hayes ran a separate set of model calculations, which use data on longer-period seismic waves that arrive at stations later but are more appropriate for the world's largest quakes. At 1:04 a.m., on the basis of this 'W-phase' analysis, the NEIC updated the Nepal quake's magnitude to 7.9. \u201cNone of those numbers are wrong,\u201d says Earle. \u201cThey're all right for that particular magnitude scale.\u201d (Three hours later, the centre would announce a final magnitude of 7.8, also based on the W-phase approach but incorporating more-detailed modelling with newer data.) Even as Earle was wrestling with the quake's magnitude, he called NEIC seismologist David Wald, who happened to be awake. Wald runs a set of programs that take the initial magnitudes and estimate possible fatalities and economic losses. The system, called PAGER (Prompt Assessment of Global Earthquakes for Response), relies on databases of where people live, the types of building in the region of an earthquake and how many people had been killed in similar quakes in the area before. If a quake is big enough, PAGER sends out alerts automatically. At 12:34 a.m., the system used the initial magnitude of 7.5 to predict between 100 and 1,000 deaths, and damages between US$10 million and $100 million. That ranked it an 'orange', the second-highest alert on the PAGER colour-coded system. \u201cThat's when we knew it was going to be deadly,\u201d Wald says. As the minutes crept by, aftershocks kept pummelling Kathmandu. PAGER automatically updated three more times at the orange level, the last at 2:16 a.m.. Then Wald took some data on how much the ground had moved and how widespread the aftershocks were, and manually fed the fresh information into PAGER. The alert immediately escalated to red, estimating between 1,000 and 10,000 deaths. It was 4:14 a.m.. \n               Global response \n             In Washington DC, Gari Mayberry's mobile phone woke her up with the first NEIC alert. Mayberry, a USGS volcanologist, advises the US Agency for International Development on natural hazards. The agency funded PAGER's development, precisely to simplify split-second decisions after earthquakes. \u201cDo I need to call my boss at 3 a.m.?\u201d asks Mayberry. \u201cThat's what people want to know.\u201d For Nepal, the answer was yes. As the Colorado team released its analyses, Mayberry quickly fed information to her bosses, who help to coordinate search-and-rescue teams for international disasters. In such situations, she says, every minute counts. Within hours, the US government had a team on the way to Nepal. Other groups also rolled into action. Gisli Olafsson in Reykjavik, who directs emergency response for a consortium of 43 humanitarian groups called NetHope, says: \u201cI always look at PAGER once it becomes available.\u201d Studying the USGS information, he was relieved to see that the shock had originated relatively far from Kathmandu. But he also learned that the quake had struck in mountainous terrain on a fault close to Earth's surface, which meant that it had probably destroyed roads. NetHope immediately started preparing for the complicated logistics of getting in and out of rural areas with limited access, and Olafsson flew to Kathmandu to coordinate its response. Even the financial world got involved: the Inter-American Development Bank uses PAGER numbers to trigger payouts on catastrophe bonds, a type of insurance against natural disasters such as earthquakes. The most recent estimates suggest that the 25 April earthquake and its aftershocks, including a magnitude 7.3 on 12 May, killed roughly 8,700 people \u2014 close to the PAGER estimates of around 10,000 deaths. Other catastrophe experts had estimated 50,000 dead or more, using independent assessments of population exposure and building vulnerability. One factor that may have saved lives in Kathmandu was how buildings were constructed, says Kishor Jaiswal, a civil engineer at the NEIC. Many of the newer buildings in the city have concrete frames reinforced with steel bars, which kept a lot of them from collapsing. Jaiswal had previously analysed this construction, and his work was one reason that the PAGER fatality estimates were relatively low. Although the toll was great, he knew that much of the city would survive. \n               Need for speed \n             Most of the NEIC's work is much calmer than on the night of the Nepalese disaster. Of the thousands of earthquakes that the team tracks every month, the vast majority do not kill anyone. Earle, Benz and Hayes spend their time developing ways to analyse earthquake ruptures as quickly and accurately as possible. Hayes, for instance, specializes in 'moment tensor' and 'finite fault' calculations, both of which convey information about exactly how a fault has ruptured. One of Earle's top priorities for the earthquake centre is to avoid making major mistakes, although his team sometimes does err. Notable bloopers include issuing an alert on Christmas Day 2013 for a magnitude-22 earthquake. It was supposed to say magnitude 2.2; the typo caused the NEIC to remove all human typing from the real-time system. And in May this year, the USGS reported several phantom quakes in California \u2014 in reality, they were vibrations from more-distant shocks in Alaska and Japan. An on-duty seismologist had caught the problem, but the software that distributes the alerts had not responded to the correction. Cutting back on false alerts while making sure that the real ones get out in time takes a nuanced mix of skill and speed. The NEIC gets data from nearly 1,800 stations worldwide, but there are gaps that slow the seismic analyses. China's national seismological alerting network puts a 30-minute delay on much of the information, so Earle's team can rarely use it. And India does not release its seismic data. Nepal, where seismologists have long warned about the earthquake risk, did not have a single station feeding real-time data into the USGS system. Had the agency received more real-time data from locations closer to the epicentre, seismologists could have accurately located the Nepal quake faster than they did, says Thorne Lay, a seismologist at the University of California, Santa Cruz. Even with all its speed, the NEIC is not the fastest earthquake-alert system in the United States. That title goes to the National Oceanic and Atmospheric Administration's two tsunami-warning centres. Drawing on the same seismic network, they release rougher magnitudes and locations within 3 minutes of an earthquake striking, but they handle only shocks in oceans near US territory. The NEIC keeps pushing to shave as many seconds off its own notifications as possible. One ongoing project involves Twitter. Earle has set up an automated system that hunts for words such as 'earthquake' in various languages in tweets from around the world (P. Earle  Nature Geosci.   3 , 221\u2013222; 2010 ). He has to filter out unrelated instances, including references to the video game  Quake , but once that is done he can get a heads-up that something big is beginning. When someone in Indonesia tweets ' gempa ', or earthquake, \u201cit's on our server in five seconds,\u201d he says. Tweets can arrive at the NEIC faster than seismic waves can reach recording stations. In 2012, a magnitude-4.0 jolt in Maine set off a stream of tweets from the region around the epicentre. Earle got an automatic text notification before the shaking spread across New England. \u201cI was at Safeway buying groceries, and I knew about the quake, from nothing but Twitter data, before other people felt it,\u201d he says. The Twitter experiment is most useful in places where the USGS does not receive a lot of real-time data, such as parts of South America or Indonesia. Although it will never replace the NEIC's conventional methods, it can alert the seismologists there to keep a lookout for incoming data. The earthquakes never stop coming. Towards the end of a long Friday afternoon in May, Earle is at his standing desk when his iPhone buzzes with a report of a magnitude-6.9 quake in the Solomon Islands. \u201cThat one isn't going to be near a populated area, but it's a big quake,\u201d he says. \u201cI'm gonna get someone.\u201d He is heading out of the door nearly before he finishes the sentence. Earle speed-walks down the hallway, past the row of display monitors set up for television cameras, and pokes his head into the office of seismologist Jana Pursley. \u201cJana, have you got that?\u201d he asks. \u201cNo, Sean does,\u201d she says, waving her hand at the on-duty seismologist down the hall. \u201cOK,\u201d says Earle. \u201cSean will release it, and then I'll have Bruce review the moment tensors for it, and then we'll be done.\u201d With that earthquake sorted, Earle heads back to his office. He switches on the electric kettle that sits next to two containers of freeze-dried, generic-brand coffee. \u201cI get the cheapest possible coffee because I don't even taste it anymore,\u201d he says. \u201cI just drink it.\u201d And he turns back to his monitor, to wait for the next one. \n                     Global risks: Pool knowledge to stem losses from disasters 2015-Jun-17 \n                   \n                     Mappers rush to pinpoint landslide risk in Nepal 2015-May-11 \n                   \n                     How scientists are aiding quake recovery in Nepal 2015-May-01 \n                   \n                     Major earthquake hits Nepal 2015-Apr-26 \n                   \n                     Seismology: Quake catcher 2013-Jun-19 \n                   \n                     World view: Brick by brick 2010-May-05 \n                   \n                     USGS real-time earthquake map \n                   \n                     USGS National Earthquake Information Center \n                   \n                     European-Mediterranean Seismological Centre \n                   Reprints and Permissions"},
{"file_id": "521274a", "url": "https://www.nature.com/articles/521274a", "year": 2015, "authors": [{"name": "Monya Baker"}], "parsed_as_year": "2006_or_before", "body": "Antibodies are the workhorses of biological experiments, but they are littering the field with false findings. A few evangelists are pushing for change. In 2006, things were looking pretty good for David Rimm, a pathologist at Yale University in New Haven, Connecticut. He had developed a test to guide effective treatment of the skin cancer melanoma, and it promised to save lives. It relied on antibodies \u2014 large, Y-shaped proteins that bind to specified biomolecules and can be used to flag their presence in a sample. Rimm had found a combination of antibodies that, when used to 'stain' tumour biopsies, produced a pattern that indicated whether the patient would need to take certain harsh drugs to prevent a relapse after surgery. He had secured more than US$2 million in funding to move the test towards the clinic. But in 2009, everything started to fall apart. When Rimm ordered a fresh set of antibodies, his team could not reproduce the original results. The antibodies were sold by the same companies as the original batches, and were supposed to be identical \u2014 but they did not yield the same staining patterns, even on the same tumours. Rimm was forced to give up his work on the melanoma antibody set. \u201cWe learned our lesson: we shouldn't have been dependent on them,\u201d he says. \u201cThat was a very sad lab meeting.\u201d Antibodies are among the most commonly used tools in the biological sciences \u2014 put to work in many experiments to identify and isolate other molecules. But it is now clear that they are among the most common causes of problems, too. The batch-to-batch variability that Rimm experienced can produce dramatically differing results. Even more problematic is that antibodies often recognize extra proteins in addition to the ones they are sold to detect. This can cause projects to be abandoned, and waste time, money and samples. Many think that antibodies are a major driver of what has been deemed a 'reproducibility crisis', a growing realization that the results of many biomedical experiments cannot be reproduced and that the conclusions based on them may be unfounded. Poorly characterized antibodies probably contribute more to the problem than any other laboratory tool, says Glenn Begley, chief scientific officer at TetraLogic Pharmaceuticals in Malvern, Pennsylvania, and author of a controversial analysis 1  showing that results in 47 of 53 landmark cancer research papers could not be reproduced. A few scientists who have been burned by bad experiences with antibodies have begun to speak up. Rimm's disappointment set him on a crusade to educate others by writing reviews, hosting web seminars and raising the problem in countless conference talks. He and others are calling for the creation of standards by which antibodies should be made, used and described. And some half a dozen grass-roots efforts have sprung up to provide better ways of assessing antibody quality. But it is too soon to call the cause a movement. \u201cThere are all these resources out there, but nobody uses them and many people aren't even aware of them,\u201d says Len Freedman, who heads the Global Biological Standards Institute, a non-profit group in Washington DC committed to improving biomedical research. \u201cMost vendors have no incentive to change what's going on right now, even though a lot of the antibody reagents suck.\u201d \n               Buyer beware \n             Take the example of Ioannis Prassas, a proteomics researcher at Mount Sinai Hospital in Toronto, Canada. He and his colleagues had been chasing a protein called CUZD1, which they thought could be used to test whether someone has pancreatic cancer. They bought a protein-detection kit and wasted two years, $500,000 and thousands of patient samples before they realized that the antibody in the kit was recognizing a different cancer protein, CA125, and did not bind to CUZD1 at all 2 . In retrospect, Prassas says, a rush to get going on a promising hypothesis meant that he and his group had failed to do all the right tests. \u201cIf someone says, 'Here is an assay you can use,' you are so eager to test it you can forget that what has been promised is not the case.\u201d Most scientists who purchase antibodies believe the label printed on the vial, says Rimm. \u201cAs a pathologist, I wasn't trained that you had to validate antibodies; I was just trained that you ordered them.\u201d Antibodies are produced by the immune systems of most vertebrates to target an invader such as a bacterium. Since the 1970s, scientists have exploited antibodies for research. If a researcher injects a protein of interest into a rabbit, white blood cells known as B cells will start producing antibodies against the protein, which can be collected from the animal's blood. For a more consistent product, the B cells can be retrieved, fused with an 'immortalized' cell and cultured to provide a theoretically unlimited supply. Three decades ago, scientists who needed antibodies for their experiments had to make them themselves. But by the late 1990s, reagent companies had started to take over the chore. Today, more than 300 companies sell over 2 million antibodies for research. As of 2011, the market was worth $1.6 billion, according to global consultancy Frost & Sullivan. \n               Devastating effects \n             There are signs that problems with antibodies are having broad and potentially devastating effects on the research record. In 2009, one journal devoted an entire issue to assessing the antibodies that are used to study G-protein-coupled receptors (GPCRs) \u2014 cell-signalling proteins that are targeted by drugs to treat various disorders, from incontinence to schizophrenia. In an analysis 3  of 49 commercially available antibodies that targeted 19 signalling receptors, most bound to more than one protein, meaning that they could not be trusted to distinguish between the receptors. The field of epigenetics relies heavily on antibodies to identify how proteins that regulate gene expression have been modified. In 2011, an evaluation 4  of 246 antibodies used in epigenetic studies found that one-quarter failed tests for specificity, meaning that they often bound to more than one target. Four antibodies were perfectly specific \u2014 but to the wrong target. Scientists often know, anecdotally, that some antibodies in their field are problematic, but it has been difficult to gauge the size of the problem across biology as a whole. Perhaps the largest assessment comes from work published by the Human Protein Atlas, a Swedish consortium that aims to generate antibodies for every protein in the human genome. It has looked at some 20,000 commercial antibodies so far and found that less than 50% can be used effectively to look at protein distribution in preserved slices of tissue 5 . This has led some scientists to claim that up to half of all commercially available antibodies are unreliable. But reliability can depend on the experiment. \u201cOur experience with commercial antibodies is that they are usually okay in some applications, but they might be terrible in others,\u201d says Mathias Uhl\u00e9n at the Royal Institute of Technology in Stockholm, who coordinates the Human Protein Atlas. Researchers ideally should check that an antibody has been tested for use in particular applications and tissue types, but the quality of information supplied by vendors can vary tremendously. A common complaint from scientists is that companies do not provide the data required to evaluate a given antibody's specificity or its lot-to-lot variability. Companies might ship a batch of antibodies with characterization information derived from a previous batch. And the data are often derived under ideal conditions that do not reflect typical experiments. Antibody companies contacted for this article said that it is impossible to test their products across all experimental conditions, but they do provide reliable data and work with scientists to improve antibody quality and performance. Many academics use Google to find products, so optimizing search results can sometimes matter more to a company than optimizing the actual reagents, says Tim Bernard, head of the biotechnology consultancy Pivotal Scientific in Upper Heyford, UK. Christi Bird, a Frost & Sullivan analyst based in Washington DC, says that researchers are often more interested in how quickly reagents can be delivered than in searching for antibodies with appropriate validation data. \u201cIt's the Amazon effect: they want it in two or three days, with free shipping.\u201d Researchers who are aware of the antibody problem say that scientists need to be more vigilant. \u201cAntibodies are not magic reagents. You can't just throw them on your sample and expect the result you get is 100% reliable without putting some critical thinking into it,\u201d says James Trimmer, head of NeuroMab at the University of California, Davis, which makes antibodies for neuroscience. Like many suppliers, NeuroMab explicitly states the types of experiment that an antibody should be used for, but scientists do not always follow the instructions. Ideally, researchers would refuse to buy antibodies without extensive validation data or would perform the validation themselves (see 'Bad antibodies'). This is something that Rimm is passionate about: he has developed a multistep flowchart for effective validation 6 , which he shares with anyone who will listen. But the process is time consuming \u2014 Rimm recommends control experiments that involve engineering cell lines to both express and stop expressing the protein of interest, for example. Even he acknowledges that few labs will perform all the steps. Some scientists buy half a dozen antibodies from different vendors, and then run a few assays to see which performs best. But they may end up buying the same antibody from different places. The largest vendors compete on catalogue size, so they often buy antibodies from smaller suppliers, relabel them and offer them for sale. Bernard says that the 2 million antibodies on the market probably represent 250,000\u2013500,000 unique 'core' antibodies. By necessity, many researchers rely on word of mouth or the published literature for advice. But that creates a self-perpetuating problem, in which better-performing antibodies that become available later are rarely used, says Fridtjof Lund-Johansen, a proteomics researcher at the University of Oslo. \u201cWe have very good antibodies on the market,\u201d he says, \u201cbut we don't know what they are.\u201d Lund-Johansen is trying to change that by developing high-throughput assays that could compare thousands of antibodies at once. \n               Testing times \n             In the past decade, various projects have sprung up to try to make information about antibodies easier to find. The online reagents portal Antibodypedia ( antibodypedia.com ), which is maintained by the Human Protein Atlas, has catalogued more than 1.8 million antibodies and rated the validation data available for various experimental techniques. Antibodies-online ( antibodies-online.com ), another portal, set up a programme two years ago for independent labs to do validation studies, generally at the vendors' expense. But out of 275 studies, less than half of the products tested have made the cut and earned an 'independent validation' badge. The non-profit Antibody Registry ( antibodyregistry.org ) assigns unique identifiers to antibodies and links them to other resources. Another project, pAbmAbs ( pabmabs.com/wordpress ), operates in a similar way to the social-recommendation web service Yelp, by encouraging people to review antibodies. But none of these efforts has gained much of a foothold in the scientific community. Many of the scientists contacted for this article were unaware that such resources existed. The antibody market has grown so crowded that a reputation for quality is becoming part of some suppliers' business plans. \u201cNow there is so much competition that you have to differentiate yourself,\u201d says Bernard. Vendors such as Abcam in Cambridge, UK, are encouraging users to report their own data and rankings on the company's website. Abcam's analysis of purchasing behaviour shows that its customers look at data pages on average nine times before buying, suggesting that customers want more information. Abgent, an antibody company based in San Diego, California, and a subsidiary of WuXi AppTec in Shanghai, China, tested all of its antibodies about a year ago. After reviewing the results it discarded about one-third of its catalogue. Whether that was a good decision depends on whether customers will be willing to spend more for better reagents, says John Mountzouris, site leader at the company. Already, he says, customer complaints have plummeted. Some scientists are calling for much more radical change. In a Comment in  Nature  in February 7 , Andrew Bradbury of Los Alamos National Laboratory in New Mexico and more than 100 co-signatories proposed a massive shift in the way antibodies are produced and sold. They suggested using only antibodies that have been defined down to the level of the DNA sequence that produces them, and then manufactured in engineered 'recombinant' cells. This would circumvent much of the variability introduced by production in animals. But the proposal demands information about individual antibodies that many companies consider to be trade secrets \u2014 and the antibody marketplace and its millions of products would have to be essentially demolished and reconstructed. Uhl\u00e9n, a co-signatory on the Comment, regards the plan as a distant hope. He estimates that the 'recombinant antibodies' that Bradbury hopes for would each cost 10\u2013100 times more to generate than the conventional sort, and that they would not necessarily perform better. \u201cAt the end of the day, how the binder works in the application is more important,\u201d he says. \u201cHaving a sequence for sure doesn't tell you if it works.\u201d Other efforts are under way to find cheap, fast, reliable ways of making antibodies without immunizing animals, for example by expressing and optimizing them in viruses. The pressure to characterize currently available antibodies is surging. As part of efforts to improve reproducibility, some researchers have started to discuss enlisting an independent body to establish a certification programme for commercial antibodies. And several journals (including  Nature ) ask authors to make clear that antibodies used in their papers have been profiled for that particular application. The quality will creep, rather than leap, forward, says Trimmer, who hopes to see a positive-feedback loop: as scientists become aware of artefacts, they will be more likely to challenge results and uncover more artefacts. Already, he says, the widespread insouciance about antibody validation has started to fade. \u201cIt's turning around a little bit,\u201d he says. \u201cWe need to keep talking about it.\u201d \n                     'Young blood' anti-ageing mechanism called into question 2015-May-19 \n                   \n                     First results from psychology\u2019s largest reproducibility test 2015-Apr-30 \n                   \n                     Reproducibility: Standardize antibodies used in research 2015-Feb-04 \n                   \n                     Drug development: Raise standards for preclinical cancer research 2012-Mar-28 \n                   \n                     Nature  special: Challenges in irreproducible research \n                   Reprints and Permissions"},
{"file_id": "521141a", "url": "https://www.nature.com/articles/521141a", "year": 2015, "authors": [], "parsed_as_year": "2006_or_before", "body": "A special issue explores the enormous potential and major challenges for research in south Asia's superpower. India is racing forward. With nearly 1.3 billion people and a steady growth rate, it is expected to become the world's most populous nation within a generation. Its gross domestic product more than tripled between 2000 and 2013, and its economy ranks third in the world in terms of purchasing power, behind only China and the United States. India's scientific production has also surged, with the number of published papers quadrupling over the same period. But the country has far to go before it earns the status of a scientific superpower. By almost every metric \u2014 spending, number of researchers and quality of publications \u2014 India underperforms relative to developed nations and the ascendant economies to which it is most often compared, such as China and Brazil. This week,  Nature  takes an unvarnished look at the challenges and opportunities for scientists in India. An infographic ( page 142 ) assesses the country's strengths and weaknesses by comparing its research and development landscape with those of comparable countries. A News Feature ( page 144 ) probes beneath the numbers, examining Indian successes in space, biotechnology and energy, as well as exploring bureaucracy, underfunding and other obstacles to higher education and scientific research. Scientists have high hopes that Krishnaswamy VijayRaghavan, the new secretary of the Department of Biotechnology, can help to drive change in biomedical research. He is profiled on  page 148 . Ten Indian research leaders offer their suggestions for how to build their country's scientific capacity \u2014 from better funding, facilities, education and mentoring to fairer recruiting, more autonomy and a focus on local problems ( page 151 ). Cheap and clean power will be key, say energy specialists Arunabha Ghosh and Karthik Ganesan ( page 156 ). Only by tackling such basic issues can India hope to catch up with other rapidly advancing nations. \n                     India\u2019s budget disappoints scientists 2015-Mar-02 \n                   \n                     India joins elite Mars club 2014-Sep-24 \n                   \n                     Nature  special: Science in India \n                   Reprints and Permissions"},
{"file_id": "523146a", "url": "https://www.nature.com/articles/523146a", "year": 2015, "authors": [{"name": "Erika Check Hayden"}], "parsed_as_year": "2006_or_before", "body": "Scientists have the tools to end the epidemic. They just need better ways to use them. On the shores of Lake Victoria, Kenyan fishermen spread out their nets on the sand to dry their catch in the sun. At a clutch of tents next to the beach, health-care workers are casting a very different kind of net, one that could help to capture the best approach to eradicating HIV. The tents draw a steady stream of visitors because the fishermen and their families, as well as farmers, students and others from the surrounding communities, have heard that they can get vitamin A, condoms, and medicines for worms and malaria there. At the same time, they are offered various screening tests \u2014 including one for HIV. The hope is that, along with taking advantage of the other medical services, they will agree to be tested and, if necessary, treated for the sexually transmitted virus. Epidemiologist Farley Cleghorn points to some reasons why HIV treatments don\u2019t always work on the ground Here in Kenya's Nyanza Province, which has the country's highest rate of HIV infection, this community is part of a groundbreaking study designed to explain a troubling conundrum. Interventions to prevent HIV transmission that work in trial settings \u2014 such as rapid on-the-spot HIV tests coupled with effective treatments \u2014 often fail to make as much of a dent in the epidemic as they should. The current trial, known as Sustainable East Africa Research in Community Health (SEARCH), has enrolled more than 335,000 people in Kenya and Uganda and is at the forefront of a shift in thinking about how best to deal with HIV. In the past, there was a sense that stopping the HIV/AIDS epidemic would require some radically new biomedical intervention, such as a cure or a vaccine. The growing consensus, however, is that the tools needed to stamp out HIV already exist if they could just be used in the right way. In trials over the past decade, experimental interventions such as voluntary male circumcision or the use of prophylactic drugs produced head-turning results that earned them funding for broader implementation. But they have not succeeded when rolled out more generally: in some cases because the funding did not last, but in others because the conditions of a clinical trial are not the same as those in real life. SEARCH and efforts like it are intended to explain why. They fall within the domain of implementation science, an emerging multidisciplinary field that seeks to understand and overcome factors \u2014 such as human behaviour and economics \u2014 that can lessen the impact of interventions that have otherwise proved effective. Major aid programmes are taking an interest. The US President's Emergency Fund for AIDS Relief (PEPFAR), for example, launched a US$60-million programme in implementation science in 2012. Among other aims, this programme is testing whether integrating the prevention and treatment of HIV infection with other facets of countries' health and social systems \u2014 such as family planning, tuberculosis treatment and education \u2014 could help to get the HIV epidemic under control. \u201cA lot of my university colleagues are very good at doing the studies and coming up with a finding, but are clueless about how to get that finding into actual practice,\u201d says epidemiologist Farley Cleghorn of the Futures Group in Washington DC, which contracts with governments to conduct aid programmes. \u201cThe challenge for implementation science is to diminish that reduction in impact that happens when you move from a controlled environment to the general population.\u201d \n               Ambitious goals \n             SEARCH fits into a bold global strategy for eradicating HIV. In 2014, the Joint United Nations Programme on HIV/AIDS (UNAIDS), based in Geneva, Switzerland, laid out the '90-90-90' target: getting a diagnosis for 90% of people infected with HIV; putting 90% of those on antiretroviral therapy; and getting 90% of those virally suppressed, meaning that they have an undetectable level of HIV in their bodies. Achieving these goals by 2020 would herald an end to the epidemic as a global threat by 2030, with the number of new infections per year limited to about 200,000. That is easier said than done, however. \u201cTo say it's an ambitious target would be an understatement,\u201d says Mitchell Warren, director of AVAC, an AIDS prevention advocacy group. Less than half of the people with HIV in some areas of the world, such as southern Africa, have access to HIV tests. In most regions, less than 40% of people with HIV are being treated, and the percentage of people with HIV who are virally suppressed is quite low in many regions (only about 30% in the United States, for example). Worldwide, about 15 million people will have access to antiretroviral treatment in 2015 (see 'Signs of change'). The problem is that individuals drop out at each step of the path that leads to viral suppression. Most people with HIV have never been tested. Of those who have, many do not start treatment; and of those who do, many stop for a variety of reasons. Implementation science is finding that some of the best ways to plug the holes in this leaky cascade of care are to make it easier and more rewarding for patients to get the medical attention they need. The problem is acute in sub-Saharan Africa, which represents 70% of the global total of both new infections and people living with HIV. Eight years ago, doctors with the aid group M\u00e9decins sans Fronti\u00e8res (MSF) noticed that, as the number of people being treated with antiretrovirals increased, patients attending an HIV clinic in the township of Khayelitsha in Cape Town, South Africa, were finding it increasingly difficult to get their medication. To pick up their pills, they had to visit the clinic for frequent check-ups and tests of their viral load and T-cell count \u2014 indications of the progression of the infection. But at every appointment, they faced hours-long waits to see overburdened nurses. And people frequently left empty-handed owing to a shortage of the drugs. As many as one-quarter of patients who started HIV treatment stopped after one year. MSF decided to try something different: it set up clubs that met every two months at the clinic, led by trained counsellors, many of whom were patients themselves. The clubs met during slow times at the clinic, and counsellors brought each patient's supply of medicines to the meeting in a pre-packed bag and led a group discussion about the importance of staying on treatment. A nurse visited once a year to take blood samples and measure viral load and T-cell count. The clubs were a dramatic success: for the patients who received their care in this way, there was a 57% decrease in the number of people dropping out (through either death or giving up treatment) compared with the group that continued to receive care through the previous system at the clinic itself 1 . Such clubs are now seen as a model of how to keep patients in care and have been organized in less formal settings such as private homes after work hours. The SEARCH trial is building on the idea of adapting the care of people with HIV to their needs. It is taking a broader look at the problem by not only bringing care closer to patients and making it easier for them to get it, but also examining whether integrating HIV care into the overall health-care system can help stop the leaks at each step of the care cascade. The first step is diagnosis. As few as 40% of Kenyans infected with HIV know that they have it. One problem has been that people eschew targeted HIV-testing campaigns. Another is that those most likely to be infected, such as people who migrate to find work, are least likely to be reached by testing campaigns. So SEARCH is evaluating other ways to attract people \u2014 for example, by deploying community health campaigns such as the one in Nyanza Province, where people can access much-desired medical services as well as HIV tests. People who do not attend the community programmes are approached through door-to-door campaigns and are offered HIV tests that they can take in their own homes. This combination of mobile campaigns followed by home visits has boosted the proportion of adults who had taken at least one HIV test from 57% to 80% in the communities included in SEARCH, Gabriel Chamie, at the University of California, San Francisco (UCSF), reported on behalf of the study at a conference in February. As the SEARCH trial progresses, it will assess ways to get those who test positive more quickly into care and to keep them there. They are started on antiretroviral drugs rapidly \u2014 sometimes on the same day as their diagnosis. The project has enacted a triage system for speeding HIV patients who feel well into and out of the clinic when they attend appointments, and for reducing the number of visits. The project is also trialling appointment reminders and is setting up a telephone hotline to help keep patients engaged in their care. And the project will measure a person's viral load at the start of treatment, six months later, and each year subsequently, to check whether the treatment is working. \u201cWhat we've tried to go do is greatly simplify HIV care delivery,\u201d says Diane Havlir at UCSF, one of the directors of the SEARCH study. Researchers are also using implementation science to understand why prevention methods such as circumcision and prophylactic drug treatment have not been adopted as widely as they could have been. For instance, trials in the mid-2000s proved that voluntary circumcision for men cut the risk of their acquiring HIV from a female sexual partner by 60%. The World Health Organization recommended in 2007 that circumcision be used for prevention and, with UNAIDS and the Bill & Melinda Gates Foundation in Seattle, Washington, set a target to circumcise 80% of eligible men in Africa by 2016 to prevent up to 3.4 million new HIV infections (see  Nature   503 , 182\u2013185; 2013 ). PEPFAR and others provided funding, and 9 million circumcisions have been performed since 2007. But even this massive campaign has up to now reached only 28% of its target. One problem is that circumcision is a surgical procedure and so requires different expertise and resources from those in current HIV programmes. And setting up stand-alone circumcision programmes diverts resources from existing surgery, which is already under-resourced. \u201cThere's a whole lot of logistical and operational issues that are resulting in countries not meeting their targets,\u201d Cleghorn says. A different set of real-world issues has complicated what is known as pre-exposure prophylaxis (PReP) \u2014 the concept of taking a dose of antiretroviral medication regularly or around the time of sexual intercourse to prevent infection. In the PROUD study in the United Kingdom, which reported results in February, this has been shown to reduce the risk of infection by 86% in men who have sex with men, and studies of PReP in Africa showed a decrease of 73% in heterosexual couples 2 . But despite these results, PReP is not widely used. One reason is that some people at highest risk of becoming infected with HIV are also those most likely to be in denial about their risk, or unable to access services, and are therefore least likely to take a medicine to prevent infection. And developing countries have enough difficulty distributing antiretrovirals to people already infected to make a serious effort to give them to anyone else. In June, for instance, MSF reported that one in three health facilities in South Africa reported a shortage of medications for HIV or tuberculosis late last year. That makes it hard to face the additional challenge of getting drugs to those who are HIV negative. \u201cThey can't wrap their heads around it,\u201d Cleghorn says. In addition, PReP has consistently failed to protect those arguably most in need of new prevention options \u2014 young unmarried women. In most of the poor countries hit hard by HIV, 80% of new infections among adolescents are in girls. Yet PReP has failed in this demographic in trials that used many different delivery approaches, such as vaginal gels containing antiretroviral medication or oral pills taken daily or before and after sex. The main problem is that many women did not use the products they were given. In one study of 5,000 women in South Africa, Zimbabwe and Uganda, blood tests showed that only 25\u201330% of participants actually used the medications, even though 88% said that they had. Those questioned in small groups said that they did not use the products because of social factors, such as fear that they would be ostracized or perceived as having HIV already if they were known to possess HIV drugs. The problem is part of a broader social context that makes girls more vulnerable to HIV than boys of their age. Many date older men, who have a higher prevalence of HIV infection than adolescent boys; some engage in transactional sex to afford necessities; and some are abused. \n               Implementing solutions \n             Implementation science is trying to find ways to address these broader factors in an attempt to cut the HIV risk in girls. In a meta-analysis published in March, social scientist Nicole Haberland of the Population Council in New York City examined programmes designed to reduce pregnancy, HIV and sexually transmitted disease infection rates in girls 3 . She found that when these programmes included educational components that specifically addressed gender or power \u2014 for instance, by including discussions of how girls could negotiate condom use and how gender inequality influenced their own lives \u2014 they were more likely to reduce disease risk. Eight of 10 programmes that included such components worked, compared with 2 of 12 that did not address these issues. Responding to findings such as these, in December 2014, PEPFAR announced the DREAMS initiative which, in conjunction with the Bill & Melinda Gates Foundation and the Nike Foundation, will spend $210 million over two years to provide a combination of preventive interventions targeting young girls, such as HIV testing, counselling and care for rape survivors, and programmes aimed to boost the resilience of girls and their families, such as cash payments for girls who stay in school. But drawing a direct link between some of these interventions and lowering the risk of HIV infection in girls has been difficult. Two studies that are specifically testing whether cash transfers for children who meet certain academic goals can cut the risk of new HIV infections in South Africa are expected to report their results at the upcoming meeting of the International AIDS Society in Vancouver, Canada, on 19\u201322 July. Epidemiologist Audrey Pettifor, who leads one of the trials, says that although such interventions have worked in very poor countries \u2014 such as Malawi \u2014 they may not apply elsewhere. In her trial, girls and their families were paid the equivalent of $24 per month if the girls attended school, but the youngsters in South Africa have very different expectations from those in much poorer African nations. Along with high levels of poverty, unemployment and HIV prevalence is a desire for luxury goods \u2014 the girls in Pettifor's trial named items such as designer jeans, Italian shoes and Blackberry smartphones as necessities. \u201cIf we're trying to deter transactional sex, it's going to be a big ask,\u201d Pettifor says. It may not work. Implementation science is still relatively new to the HIV/AIDS field, and it is not yet clear if it will help researchers to hit all of the 90-90-90 goals. \u201cThe evidence base is still mixed on programmes or interventions to reach these goals,\u201d Pettifor says. Researchers hope that the field will mature and become more rigorous. The SEARCH trial, for example, is assessing whether streamlining HIV care has knock-on health and economic benefits for the community, such as elevated fishing or farming revenues, or enhanced education rates among children. The fish catch of a small community on the shores of Lake Victoria may seem far removed from the goal of stopping HIV \u2014 but implementation scientists see it as an essential part of the work. \u201cWe've set these very aspirational goals,\u201d says Havlir. But if they want to reach them, then scientists must get to grips with the complexities of the real world. See Editorial  page 127 \n                     The HIV epidemic can be stopped 2015-Jul-07 \n                   \n                     AIDS prevention: Africa's circumcision challenge 2013-Nov-13 \n                   \n                     Treatment is prevention 2011-Jun-07 \n                   \n                     Cash cure for the AIDS epidemic? 2011-Jun-06 \n                   \n                     SEARCH trial \n                   Reprints and Permissions"},
{"file_id": "520422a", "url": "https://www.nature.com/articles/520422a", "year": 2015, "authors": [{"name": "Chelsea Wald"}], "parsed_as_year": "2006_or_before", "body": "Forensic geologist Lorna Dawson has pioneered methods to help convict criminals using the dirt from their shoes. On a Saturday night in October 1977, Lorna Dawson was studying in her dormitory at the University of Edinburgh, UK, when two 17-year-old girls disappeared off a nearby street. The teenagers had been on a pub crawl with friends, stopping at an old Scottish tavern called The World's End before vanishing. Police officers remembered seeing two men with them. The next day, the girls turned up dead eight kilometres apart \u2014 one on a beach and another in a remote wheat field. They had both been raped, beaten and strangled. Despite a nationwide manhunt, police could not find the assailants. Dawson was a country girl, new to the city, and was working towards a geology degree at the time of the crime, later dubbed the World's End murders. \u201cIt was my first time away from home,\u201d she says, and the case left her \u201cterrified to go out\u201d. It also left her with a passion for justice. Now at the James Hutton Institute in Aberdeen, UK, Dawson runs one of the world's only labs dedicated to forensic soil science, where in the past decade she has worked on more than 70 cases from around the globe. At the time of the murders, soil was rarely used as evidence, and techniques were \u201celementary\u201d, she says. But today, soil evidence regularly leads to bodies, overturns alibis and reveals the origins of artefacts. That is in no small part due to Dawson, who has advanced methods in soil forensics and worked to disseminate the techniques to others. \u201cLorna has been instrumental in promoting the new renaissance in forensic geoscience throughout the world,\u201d says Marianne Stam, who recently retired from the Riverside Crime Laboratory of the California Department of Justice. But Dawson says that there is still more to be done. She is now part of an international collaboration developing a method to profile microbial communities using DNA. This could make soil more valuable for crime fighting, says Rob Fitzpatrick, who founded the Centre for Australian Forensic Soil Science in Adelaide, Australia. \u201cWhat Lorna is doing is pioneering new ground, developing methods that others could use and should try more.\u201d Dawson says that she does it for the victims, such as the girls in the World's End murders \u2014 a case she would return to several times in her career. \u201cGetting some sort of closure for the victims' families, it's a really rewarding thing to be able to do,\u201d she says. \u201cThat's what drives you on into the hard hours.\u201d \n               Beyond Sherlock Holmes \n             Forensic soil science was nearly 150 years old by the time Dawson took it up. Police in Germany used sandy soil to solve a crime in 1856 and Arthur Conan Doyle noted the forensic potential of dirt three decades later in his first Sherlock Holmes mystery,  A Study in Scarlet . In the story, Watson says that Sherlock \u201ctells at a glance different soils from each other. After walks, has shown me splashes upon his trousers and told me by their colour and consistence in what part of London he had received them.\u201d In the real world, forensic soil science advanced little beyond analyses of \u201ccolour and consistence\u201d for the next century. \u201cIt was really just about the larger components of the soil. You took a sample and you shook it up in a test tube and noted its colour, that sort of thing,\u201d says retired forensic scientist Dave Barclay. From 1997 to 2006, he was head of physical evidence at the National Crime Faculty, a forensics agency that helped UK police departments with the most serious crimes. When he looked at the state of soil forensics, he was dismayed. \u201cIt wasn't being used, or it was being done on a sort of cottage-industry basis,\u201d he says. \u201cIt wasn't being done necessarily to the standards of normal science.\u201d A few scientists were starting to rigorously apply new techniques, such as specialized scanning electron microscopy for mineralogy, but other practitioners continued to use unsophisticated methods to give investigators the answers they wanted, he says. In other countries, the situation was similar or worse. Barclay wanted to make sure that UK investigators drew on the firmest science, whether or not it supported the prosecution's case. \u201cWhat we needed was a sort of unifying group who would work to strict forensic and scientific practices, and I could then use them to coordinate the work of other people or to get them to peer review other people's work,\u201d he says. To develop that expertise, Barclay approached Dawson's institute (then called the Macaulay Land Use Research Institute) in 2003 because it was already doing soil analysis for government agencies. That led him to Dawson, who was eager to participate and was already well versed in analyses that could be adapted to forensics work, such as X-ray diffraction, scanning electron microscopy and Fourier transform infrared spectroscopy. Although the World's End murders had affected her deeply, Dawson had never thought of crime solving as a career. Dirt, on the other hand, came naturally. She grew up on a farm in Angus county, south of Aberdeen. A favourite time of year for her had been the \u201ctattie holidays\u201d, when kids could earn pocket money harvesting tatties, or potatoes. \u201cI just loved that, working outside. You used to make enough money that you could buy a new bike,\u201d she says. After completing her undergraduate degree at Edinburgh, she went on to do a PhD in soil science at the University of Aberdeen, and then worked her way up at the Macaulay, doing a mix of projects related to agriculture and environmental science. When she got the call from Barclay, the offer to work on forensics intrigued her. \u201cIt's just another, different sphere of life that the soil can contribute to, really,\u201d she says. Dawson and Barclay put together a team of scientists, investigators and lawyers, which got a grant from the UK Engineering and Physical Sciences Research Council to develop standards for using soil-science techniques in forensic investigations. It was important to get it right, Barclay says, because the high costs of lab work meant that soil science would typically be used only in the most serious cases. Dawson saw opportunities to develop new techniques, especially for soil's organic matter \u2014 the part made up of dead and decomposed plants and organisms. The advantage of studying organic characteristics is that they vary on the scale of centimetres or metres, whereas inorganic components may be broadly the same over kilometres. \u201cThe organic takes you to a much finer spatial scale of resolution,\u201d she says. When combined with soil-survey databases that document a variety of soil characteristics, that resolution could help investigators to use soil attached to a suspect's shoe or tyre to locate a burial site, for example. Soon after starting her work in soil forensics, Dawson had a chance to help on a familiar case \u2014 the World's End murders. Barclay had been looking into the cold case and he asked Dawson to analyse some dirt and plant material that had come from the bare feet of one of the murdered girls, Helen Scott. But Dawson could not learn much from the soil when she looked at it in 2003 because there was too little to be analysed by all but the most powerful microscopes.  Getting some sort of closure for the victims' families, it's a really rewarding thing to be able to do.  Frustrated but determined to do better, Dawson devoted herself to adapting chromatography and mass spectrometry \u2014 techniques she knew from her work in agriculture and environmental science \u2014 for use in forensic cases. She was able to substantially reduce the necessary sample size \u2014 from about a teaspoon down to about 20 milligrams, roughly the equivalent of a grain of rice. Then she put it into practice. In one case, she was able to use organic characteristics to match soil from getaway vehicles to a crime scene on a remote farm track. When the suspects learned of the soil evidence against them, they pleaded guilty. Dawson also spread the word about soil forensics by organizing conferences and training in Australia, the United Kingdom, the United States, Russia and elsewhere and by holding events for public audiences at home. She collaborated with crime writers such as Ann Cleeves and Stuart MacBride, as well as the BBC, as a way to show the public \u2014 and potential jurors \u2014 that real forensic evidence is not as clear cut as it is often portrayed on television shows such as  Silent Witness  and  CSI . At the same time, she demonstrated how useful soil could be in solving crimes. When MacBride fashioned a minor character after her, he gave her a fitting catchphrase: \u201cThe soil never lies.\u201d \n               Buried drugs \n             In her own research, Dawson has kept abreast of developments in soil science, hoping to adapt them to forensics. That is what brought her, clad in hiking boots and gaiters, to a farm southwest of Aberdeen last May. Investigators wanted to know whether a stash of illegal drugs buried at the top of a hill could be linked to dirt samples taken from a suspects' boots and spade. Dawson hoped to use microbial DNA to solve the case. Soils host vast communities of microbes (up to a couple of billion cells per gram), and those communities can vary on scales as small as millimetres. For several years, forensic scientists have argued that the DNA of those microbial communities could serve as another kind of soil fingerprint (L. M. Macdonald  et al .  J. Appl. Microbiol.   105 , 813\u2013821; 2008 ). Such soil fingerprinting has been tried before with mixed success by investigators in Italy, Spain and the Netherlands, which have legal systems that readily consider new forensic techniques. It is much more difficult to use new techniques in trials in common-law countries such as the United Kingdom, United States and Australia, where expert testimony must pass a test to establish the reliability of the methods used. Among other things, judges look for whether the methods have gone through peer review, are generally accepted in the scientific community and have appropriate standards in place. Dawson, working with a European Union-funded international collaboration called MiSAFE, is trying to meet this high standard for the microbial genetic-profiling technique. That is why the May farm investigation was not a real case, but a mock crime scene. The 'drugs'? White powder. The 'suspects'? Dawson herself and a Hutton staff member. Back at the Hutton institute, molecular microbiologist Thomas Freitag did the analysis for the mock crime scene: he amplified then chopped up a marker gene that codes for the 16S ribosomal RNA molecule from all the microbes in a clump of soil. Cataloguing these fragments by length leads to a \u201ccoarse portrait\u201d of the community, Freitag says. No two clumps' profiles will match exactly, but clumps that are close together should be roughly similar. Because the profiling method uses the same technology that forensic units are using today to analyse human DNA, forensic scientists worldwide should be able easily adopt it, says MiSAFE coordinator Edouard Jurkevitch of the Hebrew University of Jerusalem. In the case of the mock crime scene, the tests performed as hoped. Freitag's analysis revealed, correctly, that the spade belonging to the suspect had been used at the drug-burial site. Dawson presented early results from MiSAFE at a meeting on forensic geosciences in London last December. The collaboration is now fine-tuning the approach. But Dawson says that she is confident enough in the technique to add microbial DNA profiling to her arsenal as soon as the project wraps up in May. Dawson's forensic work eventually led her back to the World's End case. Last year prosecutors brought a suspect, Angus Sinclair, to trial. By then, Sinclair was considered the worst serial killer in Scotland's history. DNA from his semen had previously linked him to the World's End murders but it was not enough to convict him during a trial in 2007.  There's no cost that you should stop at to try and find justice.  Prosecutor Deborah Demick called on Dawson to reanalyse the soil and plant material from Scott's bare feet. Sinclair claimed that he and his brother-in-law, Gordon Hamilton (who has since died), had had consensual sex with the two girls in their van that night, which explained the presence of his semen on Scott's coat. But Sinclair said that the girls were \u201calive and unharmed\u201d when Hamilton had dropped him off at a fishing spot, and that Hamilton must have killed the girls later on his own. By 2014, forensic science had finally advanced enough to refute this alibi and construct an alternative timeline. Investigators reanalysed DNA from inside the knots binding the girls' limbs, showing that Sinclair had helped to tie them. They also concluded that Sinclair had deposited his semen on Scott's coat just minutes before he left her body in the field. The debris pressed into Scott's soles helped to fill in the details \u2014 and the brutality \u2014 of those final minutes, Dawson told the jury on 22 October. Some bits of soil contained traces of plant wax that matched the wheat field where Scott's body was found; other bits matched its grassy border. A similar conclusion emerged from studies of the husks and grains recovered from the dirt on her feet. \u201cThe pattern of the soil on her feet suggested that she had walked or stood in that particular field,\u201d Dawson says. The prosecutors used this and other evidence to argue that Sinclair had helped to tie up Scott and forced her to walk from the van into the field, where he beat and strangled her to death. On 14 November, just over 37 years after the attack, the jury convicted Sinclair of the double murder. His life sentence of 37 years was Scotland's longest ever, and the media hailed the historic case as a triumph of forensic science. Dawson's contribution was small but crucial, Demick says, because it \u201cenabled the Crown to have a clear narrative of events and emphasize to the jury the sheer horror of what had happened to Miss Scott \u2014 being walked into the field to her death\u201d. For Dawson, the case was poignant and powerful. Her career had come full circle and she had helped to ensure that Sinclair would never again terrify young women. Dawson thought of her own daughters, and how one had almost died several years earlier of leukaemia. She thought of Helen Scott. And she thought of herself at university, studying geology in her dorm room. \u201cLife's so precious, and if it's taken away for whatever reason, particularly if it's taken away by someone else's actions, then I think that there's no cost that you should stop at to try and find justice, to find that person who's done that.\u201d\n \n                     Forensics specialist discusses a discipline in crisis 2015-Feb-12 \n                   \n                     Faulty forensic science under fire 2014-Feb-04 \n                   \n                     Forensics fiasco 2013-Jul-31 \n                   \n                     Forensic investigation needs more science 2012-Aug-23 \n                   \n                     Science in court 2010-Mar-17 \n                   \n                     Nature  special: Science in court \n                   \n                     James Hutton Institute \n                   \n                     MiSAFE project \n                   \n                     International Union of Geological Sciences Initiative on Forensic Geology \n                   Reprints and Permissions"},
{"file_id": "521278a", "url": "https://www.nature.com/articles/521278a", "year": 2015, "authors": [{"name": "Zeeya Merali"}], "parsed_as_year": "2006_or_before", "body": "A wave of experiments is probing the root of quantum weirdness. Owen Maroney worries that physicists have spent the better part of a century engaging in fraud. Ever since they invented quantum theory in the early 1900s, explains Maroney, who is himself a physicist at the University of Oxford, UK, they have been talking about how strange it is \u2014 how it allows particles and atoms to move in many directions at once, for example, or to spin clockwise and anticlockwise simultaneously. But talk is not proof, says Maroney. \u201cIf we tell the public that quantum theory is weird, we better go out and test that's actually true,\u201d he says. \u201cOtherwise we're not doing science, we're just explaining some funny squiggles on a blackboard.\u201d It is this sentiment that has led Maroney and others to develop a new series of experiments to uncover the nature of the wavefunction \u2014 the mysterious entity that lies at the heart of quantum weirdness. On paper, the wavefunction is simply a mathematical object that physicists denote with the Greek letter psi ( \u03a8 ) \u2014 one of Maroney's funny squiggles \u2014 and use to describe a particle's quantum behaviour. Depending on the experiment, the wavefunction allows them to calculate the probability of observing an electron at any particular location, or the chances that its spin is oriented up or down. But the mathematics shed no light on what a wavefunction truly is. Is it a physical thing? Or just a calculating tool for handling an observer's ignorance about the world? The tests being used to work that out are extremely subtle, and have yet to produce a definitive answer. But researchers are optimistic that a resolution is close. If so, they will finally be able to answer questions that have lingered for decades. Can a particle really be in many places at the same time? Is the Universe continually dividing itself into parallel worlds, each with an alternative version of ourselves? Is there such a thing as an objective reality at all? \u201cThese are the kinds of questions that everybody has asked at some point,\u201d says Alessandro Fedrizzi, a physicist at the University of Queensland in Brisbane, Australia. \u201cWhat is it that is really real?\u201d Debates over the nature of reality go back to physicists' realization in the early days of quantum theory that particles and waves are two sides of the same coin. A classic example is the double-slit experiment, in which individual electrons are fired at a barrier with two openings: the electron seems to pass through both slits in exactly the same way that a light wave does, creating a banded interference pattern on the other side (see 'Wave\u2013particle weirdness'). In 1926, the Austrian physicist Erwin Schr\u00f6dinger invented the wavefunction to describe such behaviour, and devised an equation that allowed physicists to calculate it in any given situation 1 . But neither he nor anyone else could say anything about the wavefunction's nature. \n               Ignorance is bliss \n             From a practical perspective, its nature does not matter. The textbook Copenhagen interpretation of quantum theory, developed in the 1920s mainly by physicists Niels Bohr and Werner Heisenberg, treats the wavefunction as nothing more than a tool for predicting the results of observations, and cautions physicists not to concern themselves with what reality looks like underneath. \u201cYou can't blame most physicists for following this 'shut up and calculate' ethos because it has led to tremendous developments in nuclear physics, atomic physics, solid-state physics and particle physics,\u201d says Jean Bricmont, a statistical physicist at the Catholic University of Louvain in Belgium. \u201cSo people say, let's not worry about the big questions.\u201d But some physicists worried anyway. By the 1930s, Albert Einstein had rejected the Copenhagen interpretation \u2014 not least because it allowed two particles to entangle their wavefunctions, producing a situation in which measurements on one could instantaneously determine the state of the other even if the particles were separated by vast distances. Rather than accept such \u201cspooky action at a distance\u201d, Einstein preferred to believe that the particles' wavefunctions were incomplete. Perhaps, he suggested, the particles have some kind of 'hidden variables' that determine the outcome of the measurement, but that quantum theories do not capture. Experiments since then have shown that this spooky action at a distance is quite real, which rules out the particular version of hidden variables that Einstein advocated. But that has not stopped other physicists from coming up with interpretations of their own. These interpretations fall into two broad camps. There are those that agree with Einstein that the wavefunction represents our ignorance \u2014 what philosophers call psi-epistemic models. And there are those that view the wavefunction as a real entity \u2014 psi-ontic models. To appreciate the difference, consider a thought experiment that Schr\u00f6dinger described in a 1935 letter to Einstein. Imagine that a cat is enclosed in a steel box. And imagine that the box also contains a sample of radioactive material that has a 50% probability of emitting a decay product in one hour, along with an apparatus that will poison the cat if it detects such a decay. Because radioactive decay is a quantum event, wrote Schr\u00f6dinger, the rules of quantum theory state that, at the end of the hour, the wavefunction for the box's interior must be an equal mixture of live cat and dead cat. \u201cCrudely speaking,\u201d says Fedrizzi, \u201cin a psi-epistemic model the cat in the box is either alive or it's dead and we just don't know because the box is closed.\u201d But most psi-ontic models agree with the Copenhagen interpretation: until an observer opens the box and looks, the cat is both alive and dead. But this is where the debate gets stuck. Which of quantum theory's many interpretations \u2014 if any \u2014 is correct? That is a tough question to answer experimentally, because the differences between the models are subtle: to be viable, they have to predict essentially the same quantum phenomena as the very successful Copenhagen interpretation. Andrew White, a physicist at the University of Queensland, says that for most of his 20-year career in quantum technologies \u201cthe problem was like a giant smooth mountain with no footholds, no way to attack it\u201d. That changed in 2011, with the publication of a theorem about quantum measurements that seemed to rule out the wavefunction-as-ignorance models 2 . On closer inspection, however, the theorem turned out to leave enough wiggle room for them to survive. Nonetheless, it inspired physicists to think seriously about ways to settle the debate by actually testing the reality of the wavefunction. Maroney had already devised an experiment that should work in principle 3 , and he and others soon found ways to make it work in practice 4 , 5 , 6 . The experiment was carried out last year by Fedrizzi, White and others 7 . To illustrate the idea behind the test, imagine two stacks of playing cards. One contains only red cards; the other contains only aces. \u201cYou're given a card and asked to identify which deck it came from,\u201d says Martin Ringbauer, a physicist also at the University of Queensland. If it is a red ace, he says, \u201cthere's an overlap and you won't be able to say where it came from\u201d. But if you know how many of each type of card is in each deck, you can at least calculate how often such ambiguous situations will arise. \n               Out on a limb \n             A similar ambiguity occurs in quantum systems. It is not always possible for a single measurement in the lab to distinguish how a photon is polarized, for example. \u201cIn real life, it's pretty easy to tell west from slightly south of west, but in quantum systems, it's not that simple,\u201d says White. According to the standard Copenhagen interpretation, there is no point in asking what the polarization is because the question does not have an answer \u2014 or at least, not until another measurement can determine that answer precisely. But according to the wavefunction-as-ignorance models, the question is perfectly meaningful; it is just that the experimenters \u2014 like the card-game player \u2014 do not have enough information from that one measurement to answer. As with the cards, it is possible to estimate how much ambiguity can be explained by such ignorance, and compare it with the larger amount of ambiguity allowed by standard theory. That is essentially what Fedrizzi's team tested. The group measured polarization and other features in a beam of photons and found a level of overlap that could not be explained by the ignorance models. The results support the alternative view that, if objective reality exists, then the wavefunction is real. \u201cIt's really impressive that the team was able to address a profound issue, with what's actually a very simple experiment,\u201d says Andrea Alberti, a physicist at the University of Bonn in Germany. The conclusion is still not ironclad, however: because the detectors picked up only about one-fifth of the photons used in the test, the team had to assume that the lost photons were behaving in the same way 7 . That is a big assumption, and the group is currently working on closing the sampling gap to produce a definitive result. In the meantime, Maroney's team at Oxford is collaborating with a group at the University of New South Wales in Australia, to perform similar tests with ions, which are easier to track than photons. \u201cWithin the next six months we could have a watertight version of this experiment,\u201d says Maroney. But even if their efforts succeed and the wavefunction-as-reality models are favoured, those models come in a variety of flavours \u2014 and experimenters will still have to pick them apart. One of the earliest such interpretations was set out in the 1920s by French physicist Louis de Broglie 8 , and expanded in the 1950s by US physicist David Bohm 9 , 10 . According to de Broglie\u2013Bohm models, particles have definite locations and properties, but are guided by some kind of 'pilot wave' that is often identified with the wavefunction. This would explain the double-slit experiment because the pilot wave would be able to travel through both slits and produce an interference pattern on the far side, even though the electron it guided would have to pass through one slit or the other. In 2005, de Broglie\u2013Bohmian mechanics received an experimental boost from an unexpected source. Physicists Emmanuel Fort, now at the Langevin Institute in Paris, and Yves Couder at the University of Paris Diderot gave the students in an undergraduate laboratory class what they thought would be a fairly straightforward task: build an experiment to see how oil droplets falling into a tray filled with oil would coalesce as the tray was vibrated. Much to everyone's surprise, ripples began to form around the droplets when the tray hit a certain vibration frequency. \u201cThe drops were self-propelled \u2014 surfing or walking on their own waves,\u201d says Fort. \u201cThis was a dual object we were seeing \u2014 a particle driven by a wave.\u201d Since then, Fort and Couder have shown that such waves can guide these 'walkers' through the double-slit experiment as predicted by pilot-wave theory, and can mimic other quantum effects, too 11 . This does not prove that pilot waves exist in the quantum realm, cautions Fort. But it does show how an atomic-scale pilot wave might work. \u201cWe were told that such effects cannot happen classically,\u201d he says, \u201cand here we are, showing that they do.\u201d Another set of reality-based models, devised in the 1980s, tries to explain the strikingly different properties of small and large objects. \u201cWhy electrons and atoms can be in two different places at the same time, but tables, chairs, people and cats can't,\u201d says Angelo Bassi, a physicist at the University of Trieste, Italy. Known as 'collapse models', these theories postulate that the wavefunctions of individual particles are real, but can spontaneously lose their quantum properties and snap the particle into, say, a single location. The models are set up so that the odds of this happening are infinitesimal for a single particle, so that quantum effects dominate at the atomic scale. But the probability of collapse grows astronomically as particles clump together, so that macroscopic objects lose their quantum features and behave classically. One way to test this idea is to look for quantum behaviour in larger and larger objects. If standard quantum theory is correct, there is no limit. And physicists have already carried out double-slit interference experiments with large molecules 12 . But if collapse models are correct, then quantum effects will not be apparent above a certain mass. Various groups are planning to search for such a cut-off using cold atoms, molecules, metal clusters and nanoparticles. They hope to see results within a decade. \u201cWhat's great about all these kinds of experiments is that we'll be subjecting quantum theory to high-precision tests, where it's never been tested before,\u201d says Maroney. \n               Parallel worlds \n             One wavefunction-as-reality model is already famous and beloved by science-fiction writers: the many-worlds interpretation developed in the 1950s by Hugh Everett, who was then a graduate student at Princeton University in New Jersey. In the many-worlds picture, the wavefunction governs the evolution of reality so profoundly that whenever a quantum measurement is made, the Universe splits into parallel copies. Open the cat's box, in other words, and two parallel worlds will branch out \u2014 one with a living cat and another containing a corpse. Distinguishing Everett's many-worlds interpretation from standard quantum theory is tough because both make exactly the same predictions. But last year, Howard Wiseman at Griffith University in Brisbane and his colleagues proposed a testable multiverse model 13 . Their framework does not contain a wavefunction: particles obey classical rules such as Newton's laws of motion. The weird effects seen in quantum experiments arise because there is a repulsive force between particles and their clones in parallel universes. \u201cThe repulsive force between them sets up ripples that propagate through all of these parallel worlds,\u201d Wiseman says. Using computer simulations with as many as 41 interacting worlds, they have shown that this model roughly reproduces a number of quantum effects, including the trajectories of particles in the double-slit experiment 13 . The interference pattern becomes closer to that predicted by standard quantum theory as the number of worlds increases. Because the theory predicts different results depending on the number of universes, says Wiseman, it should be possible to devise ways to check whether his multiverse model is right \u2014 meaning that there is no wavefunction, and reality is entirely classical. Because Wiseman's model does not need a wavefunction, it will remain viable even if future experiments rule out the ignorance models. Also surviving would be models, such as the Copenhagen interpretation, that maintain there is no objective reality \u2014 just measurements. But then, says White, that is the ultimate challenge. Although no one knows how to do it yet, he says, \u201cwhat would be really exciting is to devise a test for whether there is in fact any objective reality out there at all.\u201d \n                 Tweet \n                 Follow @NatureNews \n               \n                     Physics: Quantum quest 2013-Sep-11 \n                   \n                     Experts still split about what quantum theory means 2013-Jan-11 \n                   \n                     Data teleportation: The quantum space race 2012-Dec-05 \n                   \n                     Quantum mechanics: Get real 2012-May-06 \n                   \n                     Quantum theorem shakes foundations 2011-Nov-17 \n                   \n                     Written in the skies: why quantum mechanics might be wrong 2008-May-15 \n                   \n                     Nature  special: The quantum atom \n                   \n                     Owen Maroney \n                   \n                     Quantum Technology Laboratory, University of Queensland \n                   \n                     Aephraim Steinberg \n                   \n                     Angelo Bassi \n                   \n                     Howard Wiseman \n                   Reprints and Permissions"},
{"file_id": "519276a", "url": "https://www.nature.com/articles/519276a", "year": 2015, "authors": [{"name": "Elie Dolgin"}], "parsed_as_year": "2006_or_before", "body": "Short-sightedness is reaching epidemic proportions. Some scientists think they have found a reason why. The southern city of Guangzhou has long held the largest eye hospital in China. But about five years ago, it became clear that the Zhongshan Ophthalmic Center needed to expand. More and more children were arriving with the blurry distance vision caused by myopia, and with so many needing eye tests and glasses, the hospital was bursting at the seams. So the centre began adding new testing rooms \u2014 and to make space, it relocated some of its doctors and researchers to a local shopping mall. Now during the summer and winter school holidays, when most diagnoses are made, \u201cthousands and thousands of children\u201d pour in every day, says ophthalmologist Nathan Congdon, who was one of those uprooted. \u201cYou literally can't walk through the halls because of all the children.\u201d Ian Morgan talks about about ways to prevent myopia East Asia has been gripped by an unprecedented rise in myopia, also known as short-sightedness. Sixty years ago, 10\u201320% of the Chinese population was short-sighted. Today, up to 90% of teenagers and young adults are. In Seoul, a whopping 96.5% of 19-year-old men are short-sighted. Other parts of the world have also seen a dramatic increase in the condition, which now affects around half of young adults in the United States and Europe \u2014 double the prevalence of half a century ago. By some estimates, one-third of the world's population \u2014 2.5 billion people \u2014 could be affected by short-sightedness by the end of this decade. \u201cWe are going down the path of having a myopia epidemic,\u201d says Padmaja Sankaridurg, head of the myopia programme at the Brien Holden Vision Institute in Sydney, Australia. The condition is more than an inconvenience. Glasses, contact lenses and surgery can help to correct it, but they do not address the underlying defect: a slightly elongated eyeball, which means that the lens focuses light from far objects slightly in front of the retina, rather than directly on it. In severe cases, the deformation stretches and thins the inner parts of the eye, which increases the risk of retinal detachment, cataracts, glaucoma and even blindness. Because the eye grows throughout childhood, myopia generally develops in school-age children and adolescents. About one-fifth of university-aged people in East Asia now have this extreme form of myopia, and half of them are expected to develop irreversible vision loss. This threat has prompted a rise in research to try to understand the causes of the disorder \u2014 and scientists are beginning to find answers. They are challenging old ideas that myopia is the domain of the bookish child and are instead coalescing around a new notion: that spending too long indoors is placing children at risk. \u201cWe're really trying to give this message now that children need to spend more time outside,\u201d says Kathryn Rose, head of orthoptics at the University of Technology, Sydney. \n               Vision quest \n             For many years, the scientific consensus held that myopia was largely down to genes. Studies in the 1960s showed that the condition was more common among genetically identical twins than non-identical ones, suggesting that susceptibility is strongly influenced by DNA 1 . Gene-finding efforts have now linked more than 100 regions of the genome to short-sightedness. But it was obvious that genes could not be the whole story. One of the clearest signs came from a 1969 study of Inuit people on the northern tip of Alaska whose lifestyle was changing 2 . Of adults who had grown up in isolated communities, only 2 of 131 had myopic eyes. But more than half of their children and grandchildren had the condition. Genetic changes happen too slowly to explain this rapid change \u2014 or the soaring rates in myopia that have since been documented all over the world (see 'The march of myopia'). \u201cThere must be an environmental effect that has caused the generational difference,\u201d says Seang Mei Saw, who studies the epidemiology and genetics of myopia at the National University of Singapore. There was one obvious culprit: book work. That idea had arisen more than 400 years ago, when the German astronomer and optics expert Johannes Kepler blamed his own short-sightedness on all his study. The idea took root; by the nineteenth century, some leading ophthalmologists were recommending that pupils use headrests to prevent them from poring too closely over their books. The modern rise in myopia mirrored a trend for children in many countries to spend more time engaged in reading, studying or \u2014 more recently \u2014 glued to computer and smartphone screens. This is particularly the case in East Asian countries, where the high value placed on educational performance is driving children to spend longer in school and on their studies. A report last year 3  from the Organisation for Economic Co-operation and Development showed that the average 15-year-old in Shanghai now spends 14 hours per week on homework, compared with 5 hours in the United Kingdom and 6 hours in the United States. Researchers have consistently documented a strong association between measures of education and the prevalence of myopia. In the 1990s, for example, they found that teenage boys in Israel who attended schools known as Yeshivas (where they spent their days studying religious texts) had much higher rates of myopia than did students who spent less time at their books 4 . On a biological level, it seemed plausible that sustained close work could alter growth of the eyeball as it tries to accommodate the incoming light and focus close-up images squarely on the retina. Attractive though the idea was, it did not hold up. In the early 2000s, when researchers started to look at specific behaviours, such as books read per week or hours spent reading or using a computer, none seemed to be a major contributor to myopia risk 5 . But another factor did. In 2007, Donald Mutti and his colleagues at the Ohio State University College of Optometry in Columbus reported the results of a study that tracked more than 500 eight- and nine-year-olds in California who started out with healthy vision 6 . The team examined how the children spent their days, and \u201csort of as an afterthought at the time, we asked about sports and outdoorsy stuff\u201d, says Mutti. It was a good thing they did. After five years, one in five of the children had developed myopia, and the only environmental factor that was strongly associated with risk was time spent outdoors 6 . \u201cWe thought it was an odd finding,\u201d recalls Mutti, \u201cbut it just kept coming up as we did the analyses.\u201d A year later, Rose and her colleagues arrived at much the same conclusion in Australia 7 . After studying more than 4,000 children at Sydney primary and secondary schools for three years, they found that children who spent less time outside were at greater risk of developing myopia. Rose's team tried to eliminate any other explanations for this link \u2014 for example, that children outdoors were engaged in more physical activity and that this was having the beneficial effect. But time engaged in indoor sports had no such protective association; and time outdoors did, whether children had played sports, attended picnics or simply read on the beach. And children who spent more time outside were not necessarily spending less time with books, screens and close work. \u201cWe had these children who were doing both activities at very high levels and they didn't become myopic,\u201d says Rose. Close work might still have some effect, but what seemed to matter most was the eye's exposure to bright light. \n               See the light \n             Some researchers think that the data to support the link need to be more robust. Most epidemiological studies have estimated children's time outdoors from questionnaires \u2014 but Christine Wildsoet, an optometrist at the University of California, Berkeley, says that such data should be treated with caution. In a small, pilot study of wearable light sensors 8 , she found that people's estimates often do not match up with their actual exposure. And Ian Flitcroft, a myopia specialist at Children's University Hospital in Dublin, questions whether light is the key protective factor of being outdoors. He says that the greater viewing distances outside could affect myopia progression, too. \u201cLight is not the only factor, and making it the explanation is a gross over-simplification of a complex process,\u201d he says. Yet animal experiments support the idea that light is protective. Researchers first demonstrated this in chicks, a common lab model for studying vision. By fitting chicks with goggles that alter the resolution and contrast of incoming images, it is possible to induce the development of myopia while raising the birds under controlled conditions in which only light intensity is changed. In 2009, Regan Ashby, Arne Ohlendorf and Frank Schaeffel from the University of T\u00fcbingen's Institute for Ophthalmic Research in Germany showed that high illumination levels \u2014 comparable to those encountered outside \u2014 slowed the development of experimentally induced myopia in chicks by about 60% compared with normal indoor lighting conditions 9 . Researchers elsewhere have found similar protective effects in tree shrews and rhesus monkeys 10 . But what scientists really needed was a mechanism: something to explain how bright light could prevent myopia. The leading hypothesis is that light stimulates the release of dopamine in the retina, and this neurotransmitter in turn blocks the elongation of the eye during development. The best evidence for the 'light\u2013dopamine' hypothesis comes \u2014 again \u2014 from chicks. In 2010, Ashby and Schaeffel showed that injecting a dopamine-inhibiting drug called spiperone into chicks' eyes could abolish the protective effect of bright light 11 . Retinal dopamine is normally produced on a diurnal cycle \u2014 ramping up during the day \u2014 and it tells the eye to switch from rod-based, nighttime vision to cone-based, daytime vision. Researchers now suspect that under dim (typically indoor) lighting, the cycle is disrupted, with consequences for eye growth. \u201cIf our system does not get a strong enough diurnal rhythm, things go out of control,\u201d says Ashby, who is now at the University of Canberra. \u201cThe system starts to get a bit noisy and noisy means that it just grows in its own irregular fashion.\u201d \n               Time out \n             Based on epidemiological studies, Ian Morgan, a myopia researcher at the Australian National University in Canberra, estimates that children need to spend around three hours per day under light levels of at least 10,000 lux to be protected against myopia. This is about the level experienced by someone under a shady tree, wearing sunglasses, on a bright summer day. (An overcast day can provide less than 10,000 lux and a well-lit office or classroom is usually no more than 500 lux.) Three or more hours of daily outdoor time is already the norm for children in Morgan's native Australia, where only around 30% of 17-year-olds are myopic. But in many parts of the world \u2014 including the United States, Europe and East Asia \u2014 children are often outside for only one or two hours. In 2009, Morgan set out to test whether boosting outdoor time would help to protect the eyesight of Chinese children. He and a team from the Zhongshan Ophthalmic Center (where Morgan also works) launched a three-year trial in which they added a 40-minute outdoor class to the end of the school day for a group of six- and seven-year-olds at six randomly selected schools in Guangzhou; children at six other schools had no change in schedule and served as controls. Of the 900-plus children who attended the outside class, 30% developed myopia by age nine or ten compared with 40% of those at the control schools. The study is being prepared for publication. A stronger effect was found at a school in southern Taiwan, where teachers were asked to send children outside for all 80 minutes of their daily break time instead of giving them the choice to stay inside. After one year, doctors had diagnosed myopia in 8% of the children, compared with 18% at a nearby school 12 . Morgan is buoyed by the preliminary findings, but thinks that he can do even better. \u201cWe've got proof of principle that increasing the amount of time children spend outside actually works,\u201d he says. \u201cThe question then is how do we make this work in practice at a level that would have a significant impact?\u201d He recognizes that many schools do not have the flexibility to add time outdoors. So last year, in collaboration with Congdon, he began piloting the idea of teaching kids in a classroom made of glass to let in more natural light. \u201cThis glass classroom idea is quite applicable for whole swathes of China,\u201d Congdon says. Rose points out that additional outdoor time \u201chas to be mandated through the schools, because getting parents to voluntarily do this is extremely difficult\u201d. Saw and her colleagues learned this when they trialled a 9-month programme to teach parents in Singapore about the importance of outdoor time in order to prevent myopia. They provided step-counters, organized outdoor weekend activities for families and even offered cash prizes for cooperation. But by the end of the trial, the time spent outdoors was not statistically different from that for a control group with no such campaign 13 . In some places, children cannot get any more outdoor light: there are too few hours of daylight, the sun is too fierce, or the cold too intense. Animal research 10  has suggested that powerful indoor lights could do the trick instead: light boxes currently sold to treat seasonal affective disorder, for example, can deliver up to 10,000 lux illumination, but their effects on myopia have not been tested extensively in humans. Meanwhile, researchers have been working on ways to prevent myopia from worsening. Sankaridurg and her colleagues have developed special glasses and contact lenses that can alter eye growth by focusing light from distant images across the entire field of view, rather than just at the centre, as standard lenses do. Other research groups have shown that nightly eye drops with a neurotransmitter-blocking drug called atropine can also help to control myopia progression 14 , although the mechanism remains unclear. \u201cWe want to take a holistic approach\u201d to tackling myopia, Sankaridurg says. But eye drops and light boxes do not have quite the appeal of sending children outside to play, which has plenty of other benefits besides those for the eyes. \u201cIt probably also increases physical activity, which decreases likelihood of obesity and enhances mood,\u201d Rose says. \u201cI can only see it as a win \u2014 and it's free.\u201d More than a century ago, Henry Edward Juler, a renowned British eye surgeon, offered similar advice. In 1904, he wrote in  A Handbook of Ophthalmic Science and Practice  that when \u201cthe myopia had become stationary, change of air \u2014 a sea voyage if possible \u2014 should be prescribed\u201d. As Wildsoet points out: \u201cWe've taken a hundred years to go back to what people were intuitively thinking was the case.\u201d \n                     Curing blindness: Vision quest 2014-Sep-10 \n                   \n                     Vision science: Seeing without seeing 2011-Jan-19 \n                   \n                     Neuroscience: Learning to see 2010-Oct-27 \n                   \n                     Facts about myopia \n                   \n                     Ian Morgan discusses the results of myopia prevention study \n                   Reprints and Permissions"},
{"file_id": "520020a", "url": "https://www.nature.com/articles/520020a", "year": 2015, "authors": [{"name": "Jeff Tollefson"}], "parsed_as_year": "2006_or_before", "body": "Brazil has waged a successful war on tropical deforestation, and other countries are trying to follow its lead. But victory remains fragile. Oziel Alves da Silva reins his horse to a stop near the edge of a pasture, and adjusts a baseball cap that has done little to protect his leathery skin from the tropical sun. Keeping an eye out for his herd, he surveys his 274-hectare ranch located in the eastern Amazonian state of Par\u00e1. Where he once dreamed of a vast open field covered with grasses and cattle, he sees nothing but palm trees that he cannot cut down. The 39-year-old rancher is one of thousands of Brazilian landowners stymied by a historic campaign to halt the destruction of the world's largest rainforest. He was fined 720,000 reals (US$230,000) and banned from selling cattle after trying to clear this field in 2009. Now Alves da Silva is once again operating legally, and he has little hope of expanding his pasture and increasing his herd. Along with many fellow ranchers in the county of Brasil Novo, he has stopped cutting down trees and is trying to make peace with the law. \u201cWe came together and decided we needed to change,\u201d he says. Over the past decade, while the world has been busy haggling over future commitments to reduce greenhouse-gas emissions, Brazil has lowered its carbon dioxide output more than any other country through a historic effort to slow forest loss. The deforestation rate here last year was roughly 75% below the average for 1996 to 2005 \u2014 just shy of Brazil's pledge to achieve an 80% reduction by 2020. The country has managed this feat while increasing the amount of food it produces, much of it for export to a growing and hungry world. Brazil's experience suggests that humanity has a chance to control agricultural expansion and preserve the planet's most diverse ecosystems. If other countries follow suit by protecting and expanding forests, which lock carbon up in trees and soils, they could slow the growth of global CO 2  emissions and buy the world some time to solve the thornier problem of curbing emissions from cars, power plants and industrial facilities. \u201cThere is no question that Brazil has made a fundamental departure from the past,\u201d says Achim Steiner, executive director of the United Nations Environment Programme. \u201cAnd it has given credence to the notion that forest conservation may be an important mechanism for international cooperation on climate.\u201d Although Brazil's downward trend in deforestation has been evident for nearly a decade, it is only in the past couple of years that researchers have pieced together how the country put the brakes on an epidemic of illegal development that has eliminated roughly 20% of the Brazilian Amazon over the past half century. Even today, the story varies depending on who is telling it. This is what drew me to the Brazilian Amazon for two months last year. I travelled throughout the region, talking to scientists, ranchers, politicians, loggers and members of indigenous tribes \u2014 all with the aim of understanding how Brazil altered its environmental trajectory and where it goes from here. Various factors conspired to curtail deforestation. The federal government designated areas in the Amazon basin for protection, cracked down on ranchers, farmers and land speculators, and put pressure on local governments, while environmentalists ramped up campaigns against companies that were exporting beef, leather and soya beans from illegally cleared land. States and communities recognized that their economies were at risk, which drove them to develop their own policies (see \u2018How fish and condoms can save the forest\u2019). Brazil's success thus far offers potential lessons for other tropical countries where deforestation is on the rise, but the situation in the Amazon remains precarious. Enforcement has increased, but the basic factors driving deforestation \u2014 including poverty and the profitability of agricultural land \u2014 have not changed. Although the rate of land clearing in Brazil last year fell to its second lowest level since 1988, it had spiked in 2013, and some scientists expect another increase in 2015. \u201cBrazilians do not want deforestation,\u201d climate scientist Carlos Nobre told me when I visited him in Brasilia, where he was finishing his term as secretary for research and development at the Ministry of Science. But clearing and planting new land remains the primary force for economic growth in the Amazon, he says. \u201cWe do not yet have an alternative model.\u201d \n               Incendiary measures \n             The battle against illegal deforestation in Brazil starts with satellite images of the land surface. Since 1988, researchers have been compiling high-resolution maps of the forest cover each year. They obtain low-resolution images more frequently to spot fresh openings in the forest. Over the past decade, scientists have begun providing real-time information to Brazil's environmental enforcement agency, the Brazilian Institute of Environment and Renewable Natural Resources (IBAMA). In June last year, I joined an IBAMA team and its heavily armed police escort as they launched raids in southwestern Par\u00e1, which remains a hotbed for deforestation. We spent hours barrelling down shoddy roads in search of fresh clearings seen on satellite imagery. One day, the team interrogated landowners, searched homes and confiscated guns and chainsaws, but did not find the suspicious spot. A second outing in a different area looked like it was going to end the same way, but towards evening the crew found a couple of trails off the road. We hiked 50 metres through the underbrush and the sky opened up over a field of felled trees. On the other side of the road was an encampment, complete with a large tarpaulin-covered A-frame, hammocks and a propane stove. The team promptly burned the camp to the ground, putting an end to that operation \u2014 at least for the moment. The culprits that IBAMA encounters on the ground are often bit players, but the government is also investigating criminals higher up the chain, who make money by speculating on illegally cleared land. After I left, last August, the agency cracked down on a crime syndicate in Par\u00e1, arresting 22 people. And in February, IBAMA announced the arrest of the \u201clargest deforester of the Amazon\u201d: Ezequiel Castanha, a businessman in Novo Progresso who allegedly headed the syndicate and had spent months on the run. Officials say that deforestation in the region has dropped by 65% since August. The basic outline of this enforcement strategy emerged in 2004 under former environment minister Marina Silva, a lifelong environmentalist and candidate in last year's presidential elections. As minister, Silva tackled deforestation by strengthening IBAMA and bringing other government agencies on board. One key change she made was instituting a sophisticated system to root out corruption within IBAMA. In parallel, the environmental group Greenpeace increased public pressure on companies by documenting the link between soya-bean farming and deforestation in media campaigns in Brazil and internationally, which pushed supermarket chains and food companies such as McDonald's to declare a boycott on the purchase of illegally farmed soya. All of these changes helped to push the country's major exporters to sign a moratorium in 2006, banning the purchase of soya beans from recently cleared land. Two years later, IBAMA published a blacklist of counties with the highest deforestation rates. Areas on the list faced increased enforcement by IBAMA, and landowners encountered tighter standards when they tried to take out agricultural loans. Brasil Novo was on the inaugural list, and IBAMA quickly descended on ranchers such as Alves da Silva. Brasil Novo has since reduced its deforestation rate and is one of the latest counties to make it off the blacklist, but it was a hard road, says Zelma Campos, the region's secretary of the environment. At a public meeting on land regulation in May last year, Campos told me that all ranchers \u2014 even law-abiding ones \u2014 had trouble marketing their beef when the blacklist came out. As a result, the local economy shrank and the tax base contracted, which undermined public services. Eventually, Brasil Novo's only slaughterhouse was shut down. \u201cNo one wants to invest in a municipality with environmental problems,\u201d explained Campos. But this was just the beginning. In 2009, a 27-year-old federal prosecutor named Daniel Azeredo filed a lawsuit against various ranchers and 11 of the largest slaughterhouse operators in Par\u00e1, the state with the most deforestation in the Amazon. He warned major purchasers of beef and leather \u2014 including the supermarket chain Walmart, McDonald's and the Adidas clothing company \u2014 that they could be held accountable for marketing illegal products. Greenpeace mounted another international public-relations campaign, and the cattle industry in Par\u00e1 briefly ground to a halt. For Azeredo, the fundamental problem was that nobody knew who owned what, which enabled outlaws to rule with violence. In a series of legal settlements, he pushed companies and local governments to support a rural land registry in Par\u00e1 that was designed to help resolve conflicts over land ownership and allow the government to formally license agricultural operations. Greenpeace followed up by pushing major slaughterhouses into signing a moratorium \u2014 like the soya-bean companies had three years earlier \u2014 on the purchase of beef from recently deforested lands. The upshot is that the land registry has expanded from around 500 properties in 2009 to more than 112,000 today, covering 62% of the private land in the state. Deforestation in Par\u00e1 has dropped by more than 57% over the same period (see 'Food and forests'). \u201cThis was huge,\u201d says Paul Barreto, a senior scientist with the Amazon Institute of People and the Environment, an environmental group based in Par\u00e1's capital, Bel\u00e9m. \u201cThe lawsuit was against the big companies but in the end it brought along everyone.\u201d In 2012, faced with rural protests over the new enforcement regime, the Brazilian Congress revised its forest code. The new law scaled back various forest protections and let some landowners off the hook for past deforestation, but it also created a national land registry that was designed to serve as the basis for federal land management. The move has triggered its own controversies. The soya-bean industry says that because the federal registry will enable the government to improve monitoring of landowners, the 2006 moratorium on sales is now unnecessary. But environmentalists argue that the registry is not ready. The debate has intensified questions about what caused the drop in deforestation, and what should come next. \n               boxed-text \n             \n               Forces in the forest \n             Scientists have been looking into these questions, trying to pick apart the factors that influence deforestation. In a study published last year, a research team confirmed suspicions that broader economic forces \u2014 which reduced agricultural profitability a decade ago \u2014 deserve partial credit for the initial drop in deforestation (D. Nepstad  et al .  Science   344 , 1118\u20131123; 2014 ). But deforestation rates remained low even when the economics improved; stricter enforcement and initiatives such as the moratoria seem to be why. \u201cIt's basically a diffusion of different instruments, some of which have gained traction,\u201d says lead author Daniel Nepstad, a tropical ecologist who heads the Earth Innovation Institute, an advocacy group based in San Francisco, California. \u201cIt's impossible to quantify any of these factors individually, but they are all pushing in the right direction.\u201d Holly Gibbs, a geographer at the University of Wisconsin\u2013Madison, says it is possible to identify some of the more successful policies. She and her colleagues found that deforestation was higher in areas not covered by the soya-bean moratorium, including on properties that are already on the federal land registry (H. K. Gibbs  et al .  Science   347 , 377\u2013378; 2015 ). Unpublished results suggest that the beef moratorium has had a similar effect on ranchers, who fear being banned from markets if they clear land. \u201cThese moratoria are really leading to huge changes on the ground in Brazil,\u201d says Gibbs, and that raises questions about what will happen if the soya-bean moratorium is lifted as scheduled next year. Brazilian officials nonetheless see the registry as the foundation for a new brand of land management. Government researchers are working on a monitoring system to classify and track different kinds of land use across the entire country as a complement to the national land registry. This could lead to an unprecedented capacity to track, study and promote better land use nationwide, they say. \u201cIf we are successful in implementing this, it's going to be a revolution,\u201d says Francisco Oliveira, who heads the forest enforcement programme at the Ministry of the Environment in Brasilia. Even if the registry is successful, a fundamental challenge remains. It is cheaper for landowners \u2014 and more profitable for rogue speculators \u2014 to slash and burn forest than to rejuvenate soils and replant fallowed fields. Brazil is looking for ways to tilt the balance by improving and expanding operations on tracts of land that have already been cleared, using an influx of money designated for forest protection. In 2008, Norway agreed to pay $1 billion if Brazil successfully reduced deforestation and thus CO 2  emissions. It was the world's first large-scale demonstration of a strategy called REDD (Reducing Emissions from Deforestation and Forest Degradation). And Norwegian officials visited Brazil last month to talk about a second investment.  If we are successful in implementing this, it's going to be a revolution.  Brazil has dispersed more than $150 million so far for projects on issues such as agricultural productivity, biodiversity research and land-use planning. But relatively little money has gone to landowners or programmes that noticeably benefit them. \u201cThe farmers are sort of sitting there bewildered, because they are not getting the incentives they were promised,\u201d Nepstad says. He is working with major soya-bean and beef companies, as well as government officials, on an approach that would help farmers by rewarding those who meet key standards instead of punishing them for poor performance. Landowners in counties that reduce deforestation could get easier access to low-interest loans, for instance. This approach could also involve direct payments to counties and landowners. Brazil's experience could inform the rollout of an international REDD programme created in 2013 under the United Nations Framework Convention on Climate Change. Although it is a shadow of the plan that many had imagined, the basic idea remains the same: industrialized nations pay for carbon to be maintained or increased in trees and soils through better forest management. This approach has received more than $7 billion from countries such as Norway, Germany, the United Kingdom and the United States. Much of that money has been invested in projects that are intended to demonstrate the idea and help governments to improve their forest-monitoring expertise. Last year, Brazil became the first country to submit its baseline forest assessment documenting deforestation to the United Nations. In December, five other countries announced their own submissions. Initial payments could begin as early as 2017. Although there are no current provisions for long-term funding, negotiators hope to secure money in a treaty that nations plan to sign in Paris this year. Brazil is hoping for some of that cash but is not counting on it; officials say that they will continue to focus on domestic efforts. International attention is shifting now to Indonesia, which is clearing more forest than any other country. Norway has committed $1 billion to the country if the government can demonstrate reductions in deforestation and emissions. Environmentalists are also transferring their experience in Brazil to Indonesia, and have extracted promises to tackle deforestation from various international corporations that are active in the palm-oil industry there. Scepticism remains about whether these strategies will succeed in Indonesia, which is building a monitoring and enforcement programme from scratch. But Nepstad points out that a decade ago, nobody would have believed Brazil was about to turn a corner. \u201cThere are seeds of what we saw in Brazil ten years ago in Indonesia today,\u201d Nepstad says. \n               Future of the forest \n             Despite a decade of progress, the future of the Amazon rainforest remains uncertain. Some lawmakers want to scale back protected areas, and President Dilma Rousseff is encouraging investments in ports and hydroelectric dams, which could trigger more deforestation. Added to that is concern over the impacts of climate change, which threatens both the rainforest and existing crops. Paulo Moutinho, former executive director of the Amazon Environmental Research Institute in Brasilia, fears that the government is overlooking more obvious solutions, such as designating more land for permanent protection.\u201cIt's stupid,\u201d he says, \u201cbut there's a sense in Brasilia that we have too much protected area.\u201d Others are more sanguine. Back in Par\u00e1, Azeredo told me that Brazil's march towards law and order on the frontier is slowly paying off. With a little persistence, he says, the beef industry could achieve a reasonable level of compliance in several years' time. \u201cWe are creating a system of governance,\u201d he says. \u201cBefore, we didn't even know where to start.\u201d This is a message that ranchers such as Alves da Silva seem to have taken to heart. \u201cEvery day that passes, government enforcement is going to increase,\u201d he says. \u201cIt's only going to get harder to break the law.\u201d With little hope of expanding his operation, Alves da Silva concentrates on the herd at hand. He ropes and vaccinates a pair of newborn calves and then finishes for the day. As the light fades, we mount our horses and set off through his pasture. Behind us, the silhouette of the forest looms large. See Editorial  page 5 \n                     Brazil unveils tool to track emissions 2012-Aug-29 \n                   \n                     Brazil set to cut forest protection 2012-May-01 \n                   \n                     The Amazon basin in transition 2012-Jan-18 \n                   \n                     Fighting for the forest: The roadless warrior 2011-Nov-30 \n                   \n                     A struggle for power 2011-Nov-09 \n                   \n                     Brazilian National Institute for Space Research \n                   \n                     UN REDD programme \n                   Reprints and Permissions"},
{"file_id": "520144a", "url": "https://www.nature.com/articles/520144a", "year": 2015, "authors": [{"name": "Kendall Powell"}], "parsed_as_year": "2006_or_before", "body": "There is a growing number of postdocs and few places in academia for them to go. But change could be on the way. By the time Sophie Thuault-Restituito reached her twelfth year as a postdoctoral fellow, she had finally had enough. She had completed her first postdoc in London, then moved to New York University (NYU) in 2004 to start a second. Eight years and two laboratories later, she was still there and still effectively a postdoc, precariously dependent on outside grants to secure and pay for her position. Her research on Alzheimer\u2019s disease was not making it into high-profile journals, so she was unable to compete for academic positions in the United States or Europe. She loved science and had immense experience, but with two young children at home, she knew she needed something more secure. \u201cMy motivation was gone. I was done with doing research,\u201d she says. So in 2013, Thuault-Restituito moved into a job as a research-laboratory operations manager at NYU, where she coordinates building renovations and fosters collaboration between labs. She enjoys the fact that her staff position has set hours, as well as better pay and benefits. But at the time of the move, she mourned the loss of a research career and she regrets the years wasted pursuing one. \u201cI stayed five years more than I should have,\u201d she says. Thuault-Restituito is the face of a postdoctoral system that is broken. These highly skilled scientists are a major engine driving scientific research, yet they are often poorly rewarded and have no way to progress in academia. The number of postdocs in science has ballooned: in the United States alone, it jumped by 150% between 2000 and 2012. But the number of tenured and other full-time faculty positions has plateaued and, in some places, it is even shrinking (see  Nature 472, 276\u2013279; 2011 ). Many postdocs move on to fulfilling careers elsewhere, but those who want to continue in research can find themselves thwarted. They end up trapped as \u2018permadocs\u2019: doing multiple postdoc terms, staying in these positions for many years and, in a small but significant proportion, never leaving them. Of the more than 40,000 US postdocs in 2013, almost 4,000 had been so for more than 6 years (see \u2018The postdoc pile-up\u2019). This problem is felt acutely in the large US biomedical-sciences workforce, but the trends are similar in many other countries and disciplines \u2014 and the economic drivers are too. Postdoc salaries have remained low\u00a0\u2014\u00a0often less than the stipend and tuition costs of a graduate student. \u201cWe had the incentives all wrong,\u201d says Paula Stephan, an economist at Georgia State University in Atlanta who studies research labour markets. \u201cWe made postdocs so cheap that principal investigators had lots of incentives to hire them.\u201d Discussion about the postdoc problem has grown increasingly loud. In December 2014, a committee convened by the US National Academies released a report aimed at highlighting and improving the postdoc\u2019s plight. The committee called for a hike in salaries, from the current recommended starting salary of US$42,840 to $50,000, and a 5-year limit on the length of postdocs. Senior scientists in the United States, who have been urging reforms for the scientific workforce as a whole, have identified the postdoc oversupply as one of the most urgent issues ( B. Alberts  et al. Proc. Natl Acad. Sci. USA    112,  1912\u20131913; 2015 ). Experts acknowledge that change will be hard; after all, the National Academies made similar recommendations 15 years ago with little effect. But some institutions and countries have started to address the issue. Several US universities have enforced 5-year term limits, New Zealand inadvertently narrowed the pipeline when it slashed the number of postdocs available, and some laboratories are moving permadocs into stable, better-paid positions. Other scientists who are keen to help postdocs are watching the results with interest. (Readers can vote on their preferred solution in our poll.) \u201cWe\u2019ve always been at risk of producing more scientists than we have places for, but the stresses and strains were not harmful in the way they are now,\u201d says Shirley Tilghman, president emerita of Princeton University in New Jersey, who has studied the workforce problem. \u201cSome changes will have to happen.\u201d \n               The fixed-term postdoc \n             In 2008, while Thuault-Restituito was there, NYU\u2019s School of Medicine decided to try a tough-love approach: it began enforcing a rule that researchers could hold a postdoc for a maximum of 5 years\u00a0\u2014\u00a0including time spent at other institutions. In 2014, 35 of the roughly 400 postdocs there left because their time was up. The time limit can be painful for people who feel forced out, says Keith Micoli, chairman of the board of the National Postdoctoral Association and director of the NYU School of Medicine postdoctoral programme. \u201cPeople coming up against it put me in an ethical quandary: what\u2019s best for that postdoc and what\u2019s best for postdocs as a whole?\u201d Micoli says that term limits combat two problematic phenomena. The first is the \u2018just one more year, experiment or paper\u2019 syndrome, in which postdocs feel that they must endlessly build their academic CV before moving on. The second is the permadoc who stays on indefinitely, eventually runs into his or her adviser\u2019s retirement and is stranded without a job, a situation that Micoli himself encountered. Having a hard deadline forces postdocs to make career decisions and \u201cpeople are better for it\u201d, he says. Of the postdocs who left NYU in 2014, Micoli says that roughly equal numbers got faculty positions and left academia. Other major research universities, such as the University of California system and the University of North Carolina, Chapel Hill (UNC), have also implemented 5-year term limits. But the limits are not always strictly enforced. Postdocs and their advisers can often request a sixth year and some postdocs are moved into positions that are postdocs in everything but name. When Thuault-Restituito bumped up against the 5-year rule in 2006\u00a0\u2014\u00a0before the university enforced it more strictly\u00a0\u2014\u00a0she was promoted to \u2018associate research scientist\u2019, a staff position that brought better benefits, but no extra pay or job security. Her position was still contingent on outside grant funding, which was far from guaranteed. \u201cAt the end of the day, my job and what I was doing in the lab didn\u2019t change at all,\u201d she says. Sibby Anderson Thompkins, who directs the postdoctoral affairs office at UNC, says that the most-recent postdocs there embrace the term limit. They enter with a plan to find a career path quickly and exit the postdoc early if an opportunity arises. Anderson Thompkins, who also sat on the 2014 National Academies report committee, says that this type of planning should begin in graduate school, alongside raised awareness of the academic bottleneck that trainees will face. Whereas about 65% of US PhD-holders continue into a postdoc, only 15\u201320% of those move into tenure-track academic posts. The European situation is even more competitive\u00a0\u2014\u00a0in the United Kingdom, for example, about 3.5% of science doctorates become permanent research staff at universities. Term limits have also been tested in the United Kingdom, France and Germany, where labour laws limit the number of years that academic researchers can remain on short-term contracts before they must be hired permanently. But it is unclear whether these laws help or hurt, because there are often ways around them. In Germany, for example, a law originally intended to curb postdoc contracts to about six years after completing a PhD was altered so that scientists can remain on short-term contracts as long as they are funded by an external grant and not paid directly by the university. The result is that scientists surf endlessly from one postdoc to another: \u201cThere are unlimited numbers of short-term contracts,\u201d says Sibylle Anderl, a German postdoc in astronomy at the Grenoble Institute for Planetary Sciences and Astrophysics in France. \u201cThe real problem for German postdocs is that we don\u2019t have enough permanent positions available.\u201d \n               The elite postdoc \n             Postdocs don\u2019t have to be forced out of the pipeline if, instead, they are never let in. That was the result when, in 2010, the New Zealand government decided to axe a scheme that had funded roughly 90 postdoc slots\u00a0\u2014\u00a0eliminating nearly one-third of its postdocs in one fell swoop. Before this, the government covered salaries for a huge chunk of the country\u2019s postdocs, who enjoy salaries and benefits nearly equivalent to those starting permanent academic positions. For most labs, postdocs are too expensive to fund from research grants. So when the government funding disappeared\u00a0\u2014\u00a0mainly a money-saving decision\u00a0\u2014\u00a0so too did many postdoc spots. Lara Shepherd got caught in the squeeze when fellowships vanished in her field of evolutionary biology and she reached the end of her first postdoc, at the Museum of New Zealand Te Papa Tongarewa in Wellington. She secured a second postdoc at Massey University in Palmerston North, using a grant to pay half of her salary and working part time to cover the rest. But she could not land a coveted academic position. \u201cNew Zealand is so small \u2014 there are very few jobs in your particular area of expertise,\u201d she says. Shepherd eventually found a temporary research position back at the Museum of New Zealand, and scored an early-career grant from the Royal Society of New Zealand, which she leveraged into a permanent position. She now oversees genetic analyses of plant, animal and fossil samples. Without the early-career fellowship, she says, \u201cI would have been looking outside of science.\u201d Many principal investigators (PIs) in New Zealand are unhappy with the situation. With no postdocs to help them, they struggle with lab management and mentoring, and they say that labs have become dependent on graduate students. \u201cAll we\u2019ve done is to outsource our postdocs,\u201d says Shaun Hendy, a physicist at the University of Auckland. \u201cWe\u2019ve removed a cohort of young researchers from our system and replaced them with even younger, less-experienced researchers.\u201d Once trained, the country\u2019s best PhD students tend to head out of science or to postdocs overseas. One lab head describes a top marine-biology graduate who\u00a0\u2014\u00a0with no prospect of a postdoc or academic job\u00a0\u2014\u00a0ended up driving a forklift before eventually landing a position in the country\u2019s statistics bureau. Hendy predicts that the postdoc void will result in lower-quality, less-complex research projects. \u201cI\u2019m sure there will be productivity hits down the line.\u201d Simon Davy, head of the school of biological sciences at the Victoria University of Wellington, says that the research culture of university departments loses vibrancy without any postdocs. His department of 35 research groups hosts fewer than 10 postdocs. His own lab has been lucky enough to have a couple of them in the past 5\u20136 years and he says that this has tripled his group\u2019s productivity. If Davy could wave a magic wand and bring back the government-funded postdoctoral positions, he would\u00a0\u2014\u00a0and so would 560 of the country\u2019s scientists, who, in 2011, collectively sent a letter of protest to the science minister, among other government leaders. \u201cI\u2019ve struggled to think of positives from our experience,\u201d Hendy says. Science-development manager Anne Berryman, from the New Zealand Ministry of Business, Innovation and Employment, says the decision to cut postdocs was designed to reprioritize government support towards later career stages, and contends that there is no evidence of harm to the country\u2019s scientific research. Most US researchers balk at the idea of restricting the number of postdocs entering the system. Jennifer Lippincott-Schwartz, a cell biologist at the US National Institute of Child Health and Human Development in Bethesda, Maryland, says that it is nearly impossible to determine who has the characteristics of a superstar researcher until mid-way through a postdoc term. \u201cI don\u2019t think it\u2019s bad when part of that workforce has to leave and move into other professions,\u201d she says. \u201cThey carry with them skills that are not wasted. They still have a knowledge base that is valuable to society.\u201d \n               The superdoc \n             If postdocs are so prized, then one obvious solution is to reward them. Both the 2014 National Academies report and earlier reports urged US lab heads to consider creating senior staff scientist, or \u2018superdoc\u2019, positions. These would be higher-paid, permanent jobs for talented postdocs who have no desire to start their own labs. Some funding agencies and institutions around the world already offer this option. Lippincott-Schwartz, for example, has two superdocs in her cell-biology laboratory at the National Institutes of Health (NIH). One serves as a software developer for the lab\u2019s super-resolution imaging of intracellular structures. The other is a microscopy specialist and lab manager. They both mentor trainees, help to write publications and keep up with the latest technological advances in the field. \u201cThese staff scientists offer so much to individual laboratories,\u201d she says. \u201cThey can do the science they love without dealing with all the bureaucratic stuff associated with being the PI.\u201d Each of her superdocs earns $20,000\u201330,000 more than postdocs typically earn\u00a0\u2014\u00a0a cost she was able to cover by requesting more funds for her lab\u2019s annual budget from the NIH. But other lab heads say that they struggle to find the resources to pay for superdocs, and without an increase in funding, the inevitable trade-off is fewer workers. That reality is hard to stomach for lab heads who are trying to balance the pressure to produce results and papers\u00a0\u2014\u00a0generally maximized by lots of staff on low salaries\u00a0\u2014\u00a0with the desire to keep and promote experienced employees. \u201cIt\u2019s economics, and we need to face up to that. There may not be as many people working in your lab. No one wants to talk about that,\u201d says Micoli. One scientist struggling with this dilemma is Leslie Leinwand, a molecular cell biologist at the University of Colorado Boulder\u2019s BioFrontiers Institute. She relies on two postdocs, Massimo Buvoli and Steve Langer, who have been in her lab for nearly two decades. But if she created staff-scientist positions for them\u00a0\u2014\u00a0as the National Academies report recommends\u00a0\u2014\u00a0the two increased salaries would equal nearly two-thirds of the annual budget for a typical NIH R01 grant, on which many biomedical labs rely. \u201cThere needs to be a place for such people who just want to stay at the bench, but I stay awake at night worrying about salaries for Massimo and Steve. Frankly, I can\u2019t afford to pay them what they deserve,\u201d Leinwand says. Anne Carpenter, a computational biologist at the Broad Institute in Cambridge, Massachusetts, requested extra grant funds to hire more permanent scientists rather than trainees, but found that her proposals were criticized by grant reviewers, who questioned why she was using such expensive staff to do the work. Some funding bodies do offer funds specifically for staff scientists, and others are introducing them. In March, the US National Cancer Institute proposed a grant programme designed for superdocs that would cover a salary in the range of $75,000\u2013100,000 for five years. It is planning to grant 50\u201360 such \u2018research specialist awards\u2019 throughout the next 18 months. \n               The reinvented lab \n             The real solution to the postdoc problem, Tilghman says, lies in dramatically changing the composition of labs to make them smaller, with a higher ratio of permanent staff scientists to trainees. This was also a key recommendation in the National Academies report. \u201cThe more I have thought about this question, the more I\u2019m convinced that at the heart of the problem is the structure of the lab,\u201d says Tilghman, who headed up a 2012 study of the NIH workforce (see  go.nature.com/wsqzgj ). The biggest challenge, she says, is persuading lab heads to embrace such a model when there is a tremendous bias in favour of the cheap labour that graduate students and postdocs represent. But that bias is short-sighted, she argues, when one staff scientist can do the work of three less-experienced researchers. \u201cWe\u2019ve got to persuade faculty that this is a true trade-off, and a positive trade-off for their research productivity.\u201d Labs stuffed full of trainees do not always translate to better results, says Gregory Petsko, chair of last year\u2019s National Academies committee and a neuroscientist at Weill Cornell Medical College in New York City. \u201cI don\u2019t think many of us need the labs to be the size we have them.\u201d Petsko proposes combining various strategies\u00a0\u2014\u00a0term limits, fewer postdoc positions and more staff scientists\u00a0\u2014\u00a0to deflate the swollen postdoc population. That would stop the postdoctoral fellowship from being the default step after earning a doctorate. \u201cI think the goal is to make the postdoc something special,\u201d he says. \u201cIt should be hard to get a postdoc \u2014 harder than getting into graduate school.\u201d The question is, can the scientific community be convinced? No one interviewed for this story \u2014 whether lab heads or postdocs themselves \u2014 wanted to give up these highly valued research positions. But few lab leaders, institutions or funders seem willing or able to spend what it takes to reward them appropriately. Petsko says that funding agencies could step in and enforce change, by demanding that universities direct a portion of their overhead payments\u00a0\u2014\u00a0money given to the university rather than the lab\u00a0\u2014\u00a0towards creating more staff-scientist positions. Davy points out that the solution needs to be global, or else postdocs denied jobs in one country will simply slide across country borders to find them elsewhere. In an ideal world, he says, postdocs would be able to take their funding wherever they like. \u201cPeople should be going to the best labs, the best places for them to work and be trained, which are dotted around the world.\u201d As for Thuault-Restituito, she does not regret her postdocs. But if she had to walk that path again, she would move into another career much earlier. She agrees that fewer PhDs should be flowing into postdocs, and is frank with graduate students who ask her for advice: \u201cIf you are not 150% sure you want to do it right now, don\u2019t do a postdoc.\u201d \n                     Bigger is not better when it comes to lab size 2015-Feb-05 \n                   \n                     Short-term contracts: Labs leak staff under French law 2015-Feb-04 \n                   \n                     Harsh reality 2014-Dec-03 \n                   \n                     Life outside the lab: The ones who got away 2014-Sep-03 \n                   \n                     Education: The PhD factory 2011-Apr-20 \n                   \n                     Give postdocs a career, not empty promises 2011-Mar-02 \n                   \n                     Nature  Careers \n                   \n                     National Academies report (2014):  The Postdoctoral Experience Revisited \n                   \n                     National Academies report (2000):  Enhancing the Postdoctoral Experience for Scientists and Engineers \n                   Reprints and Permissions"},
{"file_id": "519402a", "url": "https://www.nature.com/articles/519402a", "year": 2015, "authors": [{"name": "Heidi Ledford"}], "parsed_as_year": "2006_or_before", "body": "US funding agencies are turning to a Silicon Valley entrepreneur to focus fledgling biomedical companies on success \u2014 even when that means making a scientific course correction. David Johnson was just one minute into making his pitch when the interruptions started. \u201cWhy do I care?\u201d barked a bespectacled man at the back of the seminar hall. Johnson, chief executive of the California biotechnology start-up GigaGen, blinked. He had condensed his company's story into a neat ten-minute presentation for I-Corps, a nine-week course designed to teach business skills to entrepreneurial scientists like him. Now his talk was derailed. At first Johnson did not understand the question. He thought it was aimed at the therapy that GigaGen, based in San Francisco, plans to develop for people with weakened immune systems. \u201cNo. You. Why do I care about you?\u201d the man demanded. Johnson was not the only one getting gruff treatment at I-Corps' kick-off meeting in Chevy Chase, Maryland, last October. When another team squandered a few precious minutes elaborating on the need for new therapies to treat pain, I-Corps creator Steve Blank pounced. \u201cIf you spend the next ten weeks telling us about pain, you're going to be in pain,\u201d he said. Blank later let fly again. \u201cYou may have noticed that your presentation was different from the others \u2014 and not in a good way,\u201d he told the president of another firm who had not clearly elaborated his team's business strategy. Rough treatment like this is part of the pedagogy of I-Corps, a boot camp for technology-based start-ups that has now been rolled out for biomedical firms as part of an experiment by the US National Institutes of Health (NIH). Blank had given his fellow teachers explicit instructions to rattle the teams to make them more receptive to change. \u201cThe shock-and-awe part is not to embarrass people or make them feel bad,\u201d he says, \u201cbut to get them out of their default mode of 'I think it, therefore it must be right'.\u201d It will take years to find out whether the approach and theory behind I-Corps is adaptable to the unique challenges of drug development. But it was already clear by the conclusion of the inaugural class last December that many of the 19 teams had learned some unexpected lessons: several companies were told to drastically change course, and in some cases to abandon promising science for something more market-savvy. \u201cYou can be a great researcher and you can think you have great ideas,\u201d says Congressman Dan Lipinski (Democrat, Illinois), who had pushed to see Blank's approach implemented for government-funded research. \u201cBut until you're forced to talk to a potential customer, you never really know.\u201d \n               All about the science \n             At 61, Blank epitomizes the contradictions of California's Silicon Valley, with a sun-drenched conviviality that never completely conceals his no-nonsense efficiency. He will take the time to tell a funny anecdote \u2014 often using himself and his business mistakes as the punchline \u2014 but almost every session of the I-Corps meetings in Chevy Chase adjourned ahead of schedule. Blank is a college drop-out who wandered into Silicon Valley in 1978 after years of repairing fighter jets for the US Air Force. He arrived in California just before the technology boom, and his love of gadgets made him a perfect fit. He was involved in eight technology companies there, not all of them successful; he counts two \u201ccraters\u201d among them. Then, in 1999, he retired to a ranch in Pescadero, California. The sudden infusion of free time allowed him to examine his successes \u2014  and, more importantly, his failures . From that introspection he crafted a curriculum for tech entrepreneurs, to teach them to think beyond their own technology and to dive early and deep into the details of commercialization: who the customers are, what they need and how much they are willing to pay. The technique has swept through the tech industry, says Steven Phelan, who studies entrepreneurship at Fayetteville State University in North Carolina. It is bringing welcome changes to the way that businesses are developed, but some find flaws in the approach. Relying too heavily on customer input can lend itself to incremental \u2014 rather than revolutionary \u2014 improvements, Phelan cautions. \u201cIf you ask people what they want, they're just going to say something that they're familiar with,\u201d he says. (Blank counters that customer research can be tailored to avoid this problem.) A few years ago, Lipinski, a former engineer who serves on the US House Committee on Science, Space, and Technology, dropped in on a class Blank taught at Stanford University, and saw a new use for the programme. \u201cThis is something researchers don't have training in,\u201d he says. \u201cI was sold.\u201d Lipinski has long been concerned about the quality of research funded by the US Small Business Innovation Research (SBIR) programme. The funds are intended to stimulate translation of scientific discoveries into the marketplace, but critics have raised questions about how effective the programme is.  A 2013 analysis by  Nature  found that the top earners of such grants were rarely focused on commercialization. \u201cSometimes it seems like SBIR is being used in many cases not to further a business, but to continue research,\u201d says Lipinski. In Blank's class, Lipinski saw a way to beef up the business acumen of SBIR grant recipients. He urged the NIH and the National Science Foundation (NSF) to adopt the programme.  The NSF picked it up first , christened it I-Corps, for Innovation Corps, and offered it to scientists on the threshold of launching a company. Since 2011, about 500 teams have taken the course. If success can be measured by personal epiphanies, then it has been a qualified victory: almost every team changed its original business strategy, and more than half of them went on to found a company. It took until last year for the NIH to sign on, with the National Cancer Institute (NCI) serving as guinea pig. In the beginning, Blank had said that his method was applicable to all industries \u2014 except one. \u201cI said it wouldn't work for life sciences because it takes 10 to 15 years to get to a phase I clinical trial,\u201d he says. \u201cThere, it really was all about the science.\u201d But in 2013, the head of technology transfer at the University of California, San Francisco, convinced Blank to challenge those assumptions. Blank did what he tells his own students to do: he went out into the community to interview leaders in the biomedical industry. Those leaders told him that his conception of the pharmaceutical business was outdated. Blank was imagining the drug development of the 1990s, when in-house scientists carried out much of the research at big companies, and large firms forged few partnerships with smaller players. In that model, a small biotechnology firm had no customers until it had nearly brought its drug to market. Since then, the pharmaceutical industry has changed. Companies have cut back on in-house research in favour of early partnerships with smaller firms \u2014 effectively turning big pharma into early customers. Blank realized that the I-Corps approach might help biomedical researchers hoping to enter this world. But he recognized important differences between biology-based start-ups and the tech firms where he had cut his teeth. First, biomedical firms are much more heavily regulated, even before their product hits the market. Second, intellectual property is more important for health-care companies \u2014 the patents that a company can file and license wield enormous influence over the direction of its business. The third, and perhaps most overlooked, challenge: payment for services and therapies in the United States is often indirect and complex, involving a labyrinthine system of billing codes and intermediaries. Understanding those particulars \u2014 how procedures and therapies are billed, how insurance companies process the claims \u2014 is not sexy science, but it is crucial. \u201cGrandma is not the one paying for her new hip,\u201d says Blank. \u201cIf you don't understand reimbursement, you're dead.\u201d \n               The interview \n             Blank worked hard to get that message across during the three-day kick-off meeting in October as the teaching staff grilled the teams. Each morning was spent presenting \u2014 and then re-presenting \u2014 the ten-minute team pitches. Each afternoon, the teams raced to interview experts in their fields, then reported back for more workshops. Nights were filled with class readings, homework and preparations for the next day's presentations and interviews. The interviews are central to the process (see \u2018Start-up pitfalls\u2019). Teams needed to talk to scientists, pharma company reps, regulators, doctors, billing specialists and more \u2014 essentially, any person with expertise in what it takes for companies to get their products to patients and get paid. It is a time-consuming process, and Blank insists that the interviews be conducted face-to-face, to build rapport and allow interviewers to better gauge their subjects' emotions. If an expert cannot be met in person, the team must hold a video-conference. When one team let slip that some of its interviews were done over the phone, Blank's face grew red. \u201cThis is bullshit,\u201d he spat, and invited the team to leave the programme. (It stayed.) \n               boxed-text \n             Other teams embraced the I-Corps strategy with gusto. One morning, Eric Bressler, a research scientist at AsclepiX Pharmaceuticals in Baltimore, Maryland, told the group how his team had shown up at a hospital and bounced from one administrative assistant to another, asking questions about how treatments are billed and reimbursed. Eventually someone noticed that the team did not have clearance from security to wander around asking questions. It was escorted out. \u201cI predict great things,\u201d said Blank, admiring the team's audacity. \u201cAnd/or an arrest record.\u201d (His praise proved ephemeral, however: a few minutes later he scolded Bressler for wasting time with the story: \u201cYou were bullshitting for a large part of your presentation.\u201d) Some said that the interviews provided immediate insight. BCN Biosciences of Pasadena, California, had been in business for nearly a decade developing drugs to protect normal tissue from radiation during cancer treatments. One of the first things the team learned is that because new technologies can deliver radiation more precisely, doctors saw no great need to protect healthy tissue. It was an 'Aha' moment, says Andrew Norris, BCN's director of research. \u201cTrying to sell something that nobody wants is a stupid thing to do.\u201d Of all the learning that happened during those first few days in Chevy Chase, Blank seemed most proud of the progress of Abreos Biosciences, a company in San Diego, California, that is developing ways to detect counterfeit drugs. The team's presentation, 'Lateral flow immunoassay for therapeutic monoclonal antibody quality assurance', was ridiculed by one instructor. \u201cCould you make it more complicated?\u201d he ribbed. By day three, the team had swapped technical terms for more market-friendly lingo. The title became, 'Quick tests for point-of-care validation of biologic drugs'. Abreos co-founder Bradley Messmer also developed better ways to describe their product, likening it to \u201ca pregnancy test that tells you whether your drug is real or not\u201d. Blank was thrilled that the team had learned how to talk to investors better. \u201cFor commercialization, being able to explain it to your mother is what matters,\u201d he said. \u201cJust that one change might be worth $50 million.\u201d \n               Pivot points \n             When the kick-off meeting ended, the teams returned home to complete their coursework: more interviews, at least 100 over the next 9 weeks. Several I-Corps participants were already weighing up significant changes to their business strategy. AsclepiX, which was founded by bioengineers at Johns Hopkins University in Baltimore, Maryland, had been developing a cancer therapy based on the marriage of two unusual approaches \u2014 a novel drug made of a short string of amino acids, and a nanoparticle to target the drug to cancer cells. The company had exciting preclinical data on head and neck tumours and was eager to move the drug forward. But early interviewees advised the team to change focus. Head and neck cancers are too heterogeneous, they said, and there are already a number of available therapies to treat them. AsclepiX's drug targets blood vessels that feed tumours, and similar drugs had shown promise against glioblastoma, a rare and devastating brain tumour with few available treatments. Some interviewees suggested that a treatment for glioblastoma would be eligible for accelerated approval programmes in the United States and Europe, clinical trials would be smaller, and there would be less competition from other therapies. The AsclepiX crew began to get up to speed on glioblastoma, and planned interviews with neuro-oncologists. It soon identified another problem: the nanoparticle. This would provide better targeting and protect the drug in the bloodstream, but even during the I-Corps meeting, the team had noticed that nanotechnology got a chilly reception from those in the know. Karl Handelsman, a venture capitalist and course instructor, quipped at one point: \u201cThe smallest thing about nanotechnology is the market.\u201d From interviews the team learned that the word presents too many unknowns for consumers. \u201cIf you publish a paper with 'nano' in the title it gets a lot of buzz and attention,\u201d says Jordan Green, chief executive of AsclepiX. \u201cBut on the commercial side, they don't look at nano as a plus. It may even be a liability.\u201d There were concerns about potential toxicity, and worries over how the Food and Drug Administration would evaluate a nanoparticle delivery system. Many were also worried about batch-to-batch variability in manufacturing. AsclepiX is still young and flexible \u2014 the company has just three full-time employees and is still ordering basic equipment for its new laboratory in a renovated silverware factory. In response to the feedback, the company re-evaluated early animal tests of the peptide drug without a nanoparticle. The results were promising, says chief scientific officer Aleksander Popel. And Green says that the company is exploring ways of manufacturing the particle continuously, in the hope of alleviating concerns about batch-to-batch variability. By the end of the nine-week I-Corps programme, others had changed strategy too. When participants reassembled for their final meeting in December, Affinity Therapeutics in Cleveland, Ohio, reported that interview number 82 had uncovered a fatal flaw in the company's product, an implanted device to repair blood vessels in patients on dialysis. Affinity's device is coated with a drug-releasing polymer designed to prevent smooth muscle cells from growing into the device \u2014 a process that gradually reduces the diameter of currently used products. \u201cThis will never work,\u201d a doctor told them, noting that muscle cells also grow into the natural blood vessel downstream of the device junction. Then interviewee number 116, Timmy Lee at the University of Alabama at Birmingham, told them how to correct the problem by shifting the placement of the coating. In the meantime, the company learned more about regulation and billing practices. Before I-Corps, Affinity had been weighing up two possibilities: it could market the coated device or market the coating to be added to devices sold by other companies. The team learned that the first option could require approval from two different centres within the US Food and Drug Administration. Selling the coating alone was likely to involve only one centre, but would require the creation of a new hospital billing code, a process that can take years and cost millions of dollars. As a result, Affinity is mainly focusing on the coated device. BCN, the company developing drugs to protect normal tissue from radiation damage, had found a possible new application for its compounds. The doctors it interviewed kept returning to what had initially seemed an extraneous point: if the drug helped to prevent the formation of stiff tissue in the lung caused by radiation, perhaps it would also work against a spontaneous and devastating disease called idiopathic pulmonary fibrosis, which also causes lungs to stiffen. The company initially disregarded the comments, but they kept coming up. Similarly, dermatologists had wondered whether the drug might battle the stiffening of skin that comes with ageing. BCN will expand its focus to include these other areas. With the pressure of interview counts and coursework behind them, many of the teams expressed gratitude for the experience when they gathered for the final meeting. \u201cI wasted millions on projects that were technically sexy and ultimately not commercially viable,\u201d said Mark Bates, a cardiologist and serial entrepreneur who was working with one of the teams. He turned to Blank. \u201cI'm kind of pissed right now. Where were you 20 years ago?\u201d \u201cI was pissing other people off,\u201d answered Blank, referring to his own failed companies. \u201cI was losing $35 million.\u201d The NIH will be watching closely to see if its investment in I-Corps pays off. Michael Weingarten, head of the SBIR programme at the NCI, says that he will track the teams' success over the next five years \u2014 monitoring how many partnerships with major pharmaceutical or medical-device firms the companies form, and whether they receive funds from other investors. \u201cWe still have to show to NIH management that this is having a positive impact before we move to the next stage and get more companies involved,\u201d he said in October. For now, Weingarten says, the teams filled out surveys before and after the programme that indicate how much they think they have learned; 82% of participants said they would recommend the programme to others. He expects the NCI to decide whether or not to continue the programme within the next two months. Meanwhile, Blank's method continues to spread. At the end of October, the US Department of Energy announced a project similar to I-Corps. Lipinski says that the Department of Defense is considering one as well. And Blank has been contacted by several university technology-transfer offices, asking for consultations on how they can use his methods to aid academic entrepreneurs. Imperial College in London, for example, has adopted a similar programme for start-ups based on synthetic biology. Many of the NIH's inaugural teams say that they already have enough data to testify to the utility of I-Corps. Johnson's team from GigaGen has interviewed 256 people, 93 of them after I-Corps concluded. For the past few weeks, he has been pitching the company to venture capitalists, in the hope of raising more money for the firm. Venture capitalists can be a little snarky, he says, but he felt well prepared. \u201cNormally they tend to just ask you questions until you just can't answer,\u201d he says. \u201cBut nobody's been able to ask me a question I haven't been able to answer yet.\u201d \n                     US research firms put under pressure to sell 2013-Jul-09 \n                   \n                     Scientists, meet capitalists 2011-Nov-30 \n                   \n                     Embrace failure to start up success 2011-Sep-07 \n                   \n                     Start-ups: In search of venture capital 2011-Apr-20 \n                   \n                     \n                         Nature Biotechnology  \n                       \n                   \n                     NSF Innovation Corps \n                   \n                     I-Corps at NIH: The pilot cohort \n                   Reprints and Permissions"},
{"file_id": "520278a", "url": "https://www.nature.com/articles/520278a", "year": 2015, "authors": [{"name": "Heidi Ledford"}], "parsed_as_year": "2006_or_before", "body": "Thirty years of pursuit have failed to yield a drug to take on one of the deadliest families of cancer-causing proteins. Now some researchers are taking another shot. When Stephen Fesik left the pharmaceutical industry to launch an academic drug-discovery laboratory, he drew up a wanted list of five of the most important cancer-causing proteins known to science. These proteins drive tumour growth but have proved to be a nightmare for drug developers: they are too smooth, too floppy or otherwise too finicky for drugs to bind to and block. In the parlance of the field, they are 'undruggable'. One of the first culprits that Fesik added to his list was a protein family called Ras. For more than 30 years, it has been known that mutations in the genes that encode Ras proteins are among the most powerful cancer drivers. Ras mutations are found in some of the most aggressive and deadly cancers, including up to 25% of lung tumours and about 90% of pancreatic tumours. And for some advanced cancers, tumours with Ras mutations are associated with earlier deaths than tumours without them. Decades of research have yet to yield a drug that can safely curb Ras activity. Past failures have driven researchers from the field and forced pharmaceutical companies to abandon advanced projects. But Fesik's laboratory at Vanderbilt University in Nashville, Tennessee, and a handful of other teams have set their sights anew on the proteins. They are armed with improved technology and a better understanding of how Ras proteins work. Last year, the US National Cancer Institute launched the Ras Initiative, a US$10-million-a-year effort to find new ways to tackle Ras-driven cancers. And researchers are already uncovering compounds that, with tweaking, could eventually yield the first drugs to target Ras proteins. Researchers are mindful that they still have many hurdles to jump. \u201cYou have to have a lot of respect for Ras,\u201d says Troy Wilson, president of Wellspring Biosciences, a company in La Jolla, California, that launched in 2012 with its sights set on Ras. \u201cIt is not to be underestimated. But it's also one of the most important oncogenes in cancer.\u201d Advocates of this Ras renaissance say that any signs of success could provide lessons on how to target other important proteins that are deemed to be undruggable. Just because people assume Ras proteins are too difficult to target does not mean that scientists should give up, says Channing Der, a cancer researcher at the University of North Carolina at Chapel Hill. \u201cDogma is a moving target.\u201d \n               High-hanging fruit \n             In 1982, Der's team was one of the first to show that mutations in human genes encoding Ras proteins can cause cancer 1 . This finding marked the culmination of a hunt for oncogenes \u2014 genes that can drive cancer \u2014 in the human genome. They had previously only been described in viruses and animal models. The discovery laid the foundation for the modern cancer-research juggernaut, with its emphasis on tracking genetic mutations and mapping altered molecular pathways. It also prompted hopes of finding drugs that would target oncogenes and cure some cancers. The following years were filled with discovery. It became clear that humans produce three highly similar Ras proteins and that these are activated when cells need to proliferate (to replace damaged tissue, for example). Signals from outside the cell switch Ras to an 'on' state, in which it is bound to a molecule called GTP. Cancer-causing forms of Ras proteins have a disabled 'off' switch and cannot properly process the GTP. So it seemed logical to search for drugs that could interfere with GTP binding to stop mutant Ras. But as the understanding of Ras biochemistry grew, so too did a sense of pessimism. The family's affinity for GTP turned out to be extraordinarily high, and finding another compound that could block GTP's access seemed impossible. Ras proteins also work by interacting with other proteins, but small-molecule drugs that are able to get inside cells are often too small to cordon off the wide surface area usually involved in protein\u2013protein interactions. (Antibodies can make excellent drugs and can mask a large area on their targets, but most do not penetrate cell membranes.) Ras structures offered more reasons for concern. Drug developers look at a protein's shape to gauge the likelihood of finding a compound that will bind to a critical site. They like to see a protein with deep pockets that a drug can slip into and bind with multiple points of contact. However, Ras proteins are relatively smooth. Twenty years ago, researchers thought they had the problem solved. To function, Ras proteins need to latch on to the inside of the cell membrane through a fatty tail. That tail is added by farnesyl transferase \u2014 an enzyme that is more amenable to drug targeting than Ras proteins. So the idea was to hobble Ras activity by finding drugs that inhibit farnesyl transferase. At first, it looked like a winning strategy. Farnesyl transferase inhibitors damped down cell proliferation in mice and human cancer cells 2 . By the early 2000s, at least six pharmaceutical companies were racing to bring the drugs to market. Many abandoned other Ras-related projects because they thought the Ras problem was solved, says chemist Herbert Waldmann of the Max Planck Institute of Molecular Physiology in Dortmund, Germany. \u201cThe whole field took a deep breath and waited,\u201d he says. The wait ended with one of the biggest disappointments in pharmaceutical history. One by one, the drugs failed in human clinical trials. Der, who was still studying Ras at the time, says that the episode taught him, and everyone else, an important lesson about Ras biology. The three forms of human Ras are nearly identical in terms of structure and amino-acid sequence. Researchers assumed that their functions would be similar too. Most of the tools used to study Ras proteins \u2014 cell cultures, transgenic mice and antibodies \u2014 were developed using H-Ras, which was easier to work with than the other forms. \u201cAll of us, including myself, thought why bother studying the other ones when we can just learn all about H-Ras,\u201d says Der. \u201cUnfortunately, a lot of money was spent on that misconception.\u201d It turned out that the other two forms of Ras in humans \u2014 K-Ras and N-Ras \u2014 are much more important in cancer, and the cell has a contingency plan in place to keep them working. In the absence of a farnesyl tail, another enzyme is able to tack on a different fatty tail, rendering the experimental drugs useless. The Ras field was scarred by this episode, and it took some time before researchers were willing to give the proteins another look. But about a decade later, they started coming back. \u201cAll of a sudden people turned around and said, 'Hey, this is still one of the most important targets in oncology. Nobody has done anything in the field for ten years. Let's do something',\u201d says Waldmann. This time, researchers took a fresh approach by looking for weaknesses in Ras-driven tumours. One such weakness is 'synthetic lethality'. When Ras proteins are in overdrive, cancer cells often become dependent on other molecular pathways for survival. Blocking these other pathways might not affect normal cells, but it kills Ras-driven tumour cells. Laboratories set about screening for the synthetic-lethal partners of mutated genes encoding Ras, with the idea that targeting them would kill cancer cells but leave normal cells unaffected. The result was a wave of papers reporting possible new targets \u2014 followed closely by another wave of reports that the synthetic-lethal results were irreproducible 3 . Last October, William Sellers, Global Head of Oncology at the Swiss drug maker Novartis, reported at a conference that his team had tried and failed to reproduce the most prominent published Ras synthetic-lethal findings. Changes in context, such as the cell type used or specific screening conditions, could easily change the outcome of the experiment, says Julian Downward, a cancer researcher at the Francis Crick Institute in London. Researchers are still sifting through the results to find targets that hold up, but Downward is doubtful that the efforts will bear fruit. \u201cEveryone seems to get something different from those experiments,\u201d he says. \u201cI suspect these are not going to be the most robust targets.\u201d \n               Tailored to fit \n             With the disappointment of the synthetic-lethal approach fresh in their minds, several researchers have been looking to target Ras itself (see 'Ras attack'). \u201cWe decided you have to go to Ras directly,\u201d says Brent Stockwell, a chemical biologist at Columbia University in New York. Improvements made during the past five years in computer modelling and in ways of screening for drug compounds offer fresh hope for targeting the smooth, unpocketed terrain of Ras proteins, Stockwell says. Researchers are now better able to predict the affinity of small molecules for proteins, for example, and have a better understanding of protein dynamics. Stockwell's team is capitalizing on this to design small molecules that are tailored to the surface of Ras proteins \u2014 first in the computer, and then in the laboratory. \u201cMaybe for these proteins, you're just not going to find the right solution anywhere out there in the world,\u201d Stockwell says. \u201cYou've just got to make it.\u201d Fesik is also building new drugs, but starting from a library of existing compounds. In his former career at Abbott Laboratories in Abbott Park, Illinois, Fesik devised ways to disrupt interactions between proteins by piecing together fragments of compounds that bind, however weakly, to the target. The result is a large, novel compound that is unlikely to be found in the standard chemical libraries used to hunt for drugs. Fesik likens the technique, called fragment-based screening, to constructing a key to fit a lock by cutting one notch at a time. \u201cEventually you combine all the notches,\u201d he says. \u201cThe compound has never been made before and yet you find it because you're building it up slowly and tailoring it to your protein.\u201d Fesik's lab and his industry collaborators have found more than 130 molecules that bind weakly to K-Ras 4 . The compounds induce a change in the protein's structure, opening up a binding pocket in the process. The team is now trying to add on other fragments to improve the fit \u2014 in effect, the second notch in the key. Der notes that Fesik built a reputation for drugging the undruggable in industry before he left to pursue an academic career. \u201cIf anyone is going to do it, it is Fesik,\u201d he says. Others are looking more closely at exploiting specific mutations within K-Ras. Although there are many different cancer-associated mutations in the gene that encodes it, just three are responsible for the vast majority of Ras-driven cancers. Each of these yields an enzyme with slightly different behaviour, says Der. \u201cIf we begin to think about different mutations as having different personalities, those different personalities may open up unique vulnerabilities,\u201d he says. Kevan Shokat, a chemical biologist at the University of California, San Francisco, joined the Ras hunt six years ago. In 2013, he reported a compound that targets a K-Ras mutation known as G12C (ref.  5 ). The mutation, which is found in 20% of lung cancers, replaces the amino acid glycine with cysteine, which readily reacts with other molecules. Shokat's compound exploits the reactive cysteine and binds to it irreversibly. The inhibitor will require additional tinkering before it can be used in human patients but, as the first drug candidate that truly binds directly to Ras, it has generated a tremendous amount of excitement, says Downward. \u201cIt has re-energized the whole area,\u201d he says. Shokat says he has long thought that a mutation-specific approach might work, but he hesitated to pursue it in his laboratory until recently. Drug developers were afraid of drugs that seize upon their target and do not come off, he says, because they seemed more likely to have unanticipated reactions with other proteins in the body. But several successful drugs, such as the lymphoma and myeloma drug ibrutinib, have recently been found to bind irreversibly to their targets. Meanwhile, pharmaceutical companies are increasingly open to the idea of developing drugs that work in subsets of patients with cancer who carry specific mutations. \u201cThere won't be one drug that will work for every K-Ras patient,\u201d predicts Timothy Burns, a cancer researcher at the University of Pittsburgh in Pennsylvania. Fesik says that the solutions to Ras's puzzles, whatever they are, will probably emerge from academic institutions. He left pharma in part because he loved the pursuit of important targets, regardless of how easy or hard they are to hit. Chasing an undruggable protein can be difficult to justify in industry, where scientific interest must often take a backseat to the near-term potential for profit. \u201cMost pharma companies don't want to take the risk to go after these undruggable targets, and if they do, it's temporary,\u201d he says. Bridges are forming, however. Fesik's laboratory has partnered with the German pharmaceutical company Boehringer Ingelheim to evaluate its first-generation Ras-binding drug. And Shokat co-founded Wellspring Biosciences to bring his inhibitor to market. The work soon won support from Janssen Biotech of Horsham, Pennsylvania. The efforts are getting government attention as well. The multimillion-dollar Ras Initiative is supporting the development of tools and basic research on Ras protein structures to aid drug discovery, says Frank McCormick, a cancer researcher at the University of California in San Francisco and co-director of the project. \u201cWe are trying to de-risk Ras as a target so that others will jump back in the ring and have another shot,\u201d he says. For years, the pharmaceutical industry has pursued low-hanging fruit in a different category of proteins called kinases, McCormick says. Those were easier to target, and yielded many useful cancer drugs. But that wave is starting to subside, he argues, and it is time to focus on the higher-hanging fruit: tougher targets, such as Ras proteins, that are known to be crucially important. Stockwell says he hopes that the recent revival of research on Ras proteins could inspire scientists studying other intractable targets. \u201cIf there is some success there, maybe that excitement will extend to other targets,\u201d he says. \u201cIf we really want to impact disease, there's this vast space of additional targets that have never been mined.\u201d \n                     Health: Make precision medicine work for cancer care 2015-Apr-15 \n                   \n                     Drugging the undruggable RAS: Mission Possible? 2014-Oct-17 \n                   \n                     Old cancer drug gets fresh look 2014-May-27 \n                   \n                     Cancer treatment: The killer within 2014-Apr-02 \n                   \n                     HPV: Sex, cancer and a virus 2013-Nov-20 \n                   \n                     Cancer research: Open ambition 2012-Aug-08 \n                   \n                     Nature Milestones: Cancer \n                   Reprints and Permissions"},
{"file_id": "520024a", "url": "https://www.nature.com/articles/520024a", "year": 2015, "authors": [{"name": "Emily Anthes"}], "parsed_as_year": "2006_or_before", "body": "Transfusions are one of the most overused treatments in modern medicine, at a cost of billions of dollars. Researchers are working out how to cut back. In 2009, a major California hospital was looking for ways to cut costs. Stanford Hospital and Clinics was on track that year to purchase nearly US$6.8 million worth of blood for transfusions. But a growing body of evidence was suggesting that physicians could often forego the procedure. So, beginning in July 2010, whenever a clinician used the hospital's computerized ordering system to request blood, it would call up the patient's most recent lab results. If the numbers indicated that she or he should be healthy enough to get by without a transfusion, an alert would pop onto the screen gently reminding the doctor of the guidelines and requesting further justification for the order. The results, detailed in two papers published in the past 18 months 1 , 2 , were dramatic. The number of red-blood-cell transfusions dropped by 24% between 2009 and 2013, representing an annual savings of $1.6 million in purchasing costs alone. And as transfusion rates fell, so did mortality, average length of stay and the number patients who needed to be readmitted within 30 days of a transfusion. By simply asking doctors to think twice about transfusions, the hospital had not only reduced costs, but also improved patient outcomes. Transfusions are common procedures, at least in developed nations. In 2011, US doctors transfused 21 million units of blood and blood products; in the United Kingdom, the number was nearly 3 million. But although transfusions can be lifesaving, they are often unnecessary and are sometimes even harmful. \u201cI think we were kind of brainwashed into thinking that blood saves lives, and the more you give the better,\u201d says Steven Frank, an anaesthesiologist and director of the blood-management programme at the Johns Hopkins Health System in Baltimore, Maryland. \u201cWe've gone 180 degrees, and now we think that less is more.\u201d Scientists are now recommending a more conservative approach to transfusions. But changing decades of established medical practice is not easy. Even when guidelines are clear, evidence suggests that clinicians often fail to follow them. \u201cWeaning doctors off their love affair with blood is going to be harder than we think,\u201d says Ian Roberts, director of the Clinical Trials Unit at the London School of Hygiene & Tropical Medicine. \n               Transfusion triggers \n             Significant blood loss \u2014 as well as conditions ranging from leukaemia to vitamin deficiencies \u2014 can leave body tissues starved of oxygen. Transfusions of red blood cells collected from compatible donors are designed to reverse this state. (Some patients may receive transfusions of other blood components, such as platelets, which help with clotting, but red-cell transfusions are by far the most common.) Scientists and doctors have experimented with transfusion since at least the seventeenth century, but the procedure did not become routine until the early 1900s, after researchers found that there were different blood groups and learned how to store donated blood. Blood banking really took off during the Second World War. In Britain, collection teams travelled around the country, tapping citizens' arms to help soldiers on the front lines. \u201cWill you help by giving a little of your blood?\u201d a 1944 poster implored. \u201cThe lives of our wounded depend upon it.\u201d By the end of the war, more than 750,000 people had heeded the call, some donating seven or eight times. In the decades since, appeals for blood have become common, particularly in times of war or disaster. But transfusions were widely adopted without rigorous scientific scrutiny. At the time, randomized controlled trials were not standard, and the rationale for transfusion seemed obvious. \u201cI think people took blood for granted,\u201d Roberts says. \u201cThey thought 'Well, if people are losing blood then they must need blood'.\u201d In the 1980s and 1990s, a confluence of factors sparked interest in cutting back. The discovery of the blood-borne hepatitis C and HIV raised concerns about the safety of transfusions. The resulting expansion of testing for infectious diseases increased the cost of collection, and toughened screening standards contributed to a decline in donations. Some clinicians began to wonder if they could get by with less. In 1994, a team of Canadian researchers launched a study to evaluate how patients would respond to more sparing use of blood. Doctors typically decide whether to do a transfusion by measuring a patient's level of haemoglobin, the protein inside red blood cells that binds to oxygen. The World Health Organization defines a healthy haemoglobin level as 13 grams per decilitre (g dL \u22121 ) of blood or higher in men, and 12 g dL \u22121  in women. Historically, doctors would consider a transfusion when a patient's haemoglobin fell below 10 g dL \u22121 , a trigger that was proposed in a 1942 paper 3 . The Canadian team, led by epidemiologist and critical-care specialist Paul H\u00e9bert, put this widely used threshold to the test. The researchers randomly assigned 838 intensive-care patients to two groups: those in one group would receive a transfusion if their haemoglobin levels fell below 10 g dL \u22121 , and the other if their levels dropped under 7 g dL \u22121 . After 30 days, all the people in the first group had received a transfusion, each receiving an average of 5.6 units of red blood cells (a unit is the amount extracted from around 500 mL of donated blood). Patients in the more restrictive group got just 2.6 units, on average, and one-third of the group received no blood at all. Yet the probability of death remained the same in both groups. And when the researchers analysed two subgroups of patients \u2014 those under 55 years old and those with milder illnesses \u2014 they found that the restrictive approach had actually reduced mortality. \u201cWhen we saw the results, the first thing I asked the statistician was, 'Are you sure the group assignment was correct?'\u201d recalls H\u00e9bert, who is now at the University of Montreal in Canada. \u201cAnd then we proceeded to check all of our results because, frankly, we didn't believe it.\u201d The team published its results in the  New England Journal of Medicine 4  in 1999. It was just one trial, but it got people's attention, says Lawrence Tim Goodnough, director of the transfusion medicine programme and transfusion services at Stanford University Medical Center. \u201cEverybody saw that and said, 'We need to redo this in other clinical settings'.\u201d Between 2007 and 2014, at least six more large, randomized trials were published 5 , 6 , 7 , 8 , 9 , 10 , each comparing restrictive guidelines to liberal ones. These trials enrolled patients with a wide variety of conditions \u2014 septic shock, traumatic brain injuries, gastrointestinal bleeding \u2014 as well as children in intensive care, adults undergoing cardiac surgery and older adults having hip surgery. All six studies revealed that patients fare just as well, and sometimes better, when doctors use lower haemoglobin thresholds. \n               Risky medicine \n             Researchers are now trying to understand why transfusions do not always have their intended benefits. It could be that haemoglobin levels are not a good a proxy for what doctors really care about, which is whether enough oxygen is actually being delivered to tissues. Or it could be that the blood people are receiving is not doing its job properly. Fresh red cells are flexible, and flow easily through the body's tiniest capillaries. But after a few weeks in a blood bank, their membranes stiffen. The cells change shape, become stickier and cling more tightly to oxygen. These changes, known as the storage lesion, could make red blood cells less effective. \u201cThis may explain why the so-called 'gift of life' isn't translating into benefit for patients,\u201d Goodnough says. Research has yielded contradictory findings as to whether the storage lesion actually worsens patient outcomes, but the results of a large randomized trial are expected later this year.  Weaning doctors off their love affair with blood is going to be harder than we think.  Transfusions not only have uncertain benefits, they also have risks. They can transmit infectious diseases, overwhelm the heart and injure the lungs. They can also wreak havoc on the immune system. \u201cBlood is analogous to a liquid organ transplant,\u201d Frank says. \u201cIt's foreign tissue from another person.\u201d Doctors can prevent most catastrophic immune responses by ensuring that donor and recipient are compatible for the proteins or carbohydrates known as antigens that characterize the ABO and Rh blood types. But blood cells contain many other antigens, and incompatibilities can spark immune reactions that range from mild to fatal. Paradoxically, transfusions can dampen the immune response and leave patients more vulnerable to infection, although the mechanism behind this remains uncertain. These risks may have gone unnoticed because they are not easy to observe in the course of day-to-day practice. Many people who receive transfusions are already critically ill, and infections are not uncommon in hospitals. The elevated risk that accompanies transfusions becomes apparent only when scientists analyse large patient populations. For some patients, of course \u2014 especially those who are rapidly losing a lot of blood \u2014 transfusions are lifesaving. In a study published last year 11 , Roberts and his colleagues found that transfusions were beneficial only to those with the most severe injuries \u2014 they actually increased mortality in people with mild injuries. And where the line should be drawn is not completely settled: there have not yet been any large, randomized trials examining whether lower thresholds are appropriate for patients having heart attacks or strokes, for example. In January, scientists unexpectedly found that liberal transfusion strategies yield better outcomes in people having surgery for cancer 12 . The complexities of individual ailments and risk factors means that doctors still need to exercise their clinical judgement when deciding whether to prescribe a transfusion. Nevertheless, experts say, it is evident that many patients have been getting unnecessary transfusions. As Roberts puts it: \u201cThere are some patients who will die without transfusions and there are some that will die because of transfusion.\u201d \n               A clinical evolution \n             The conservative approach is starting to gain acceptance among clinicians. More and more medical associations and professional organizations now recommend haemoglobin thresholds of around 7 g dL \u22121  to 8 g dL \u22121  \u2014 and hospitals are implementing strategies to reduce the odds that a patient will need a transfusion in the first place. Doctors are administering iron supplements to people with anaemia who are scheduled for elective surgery, minimizing the amount of blood drawn for laboratory tests and using 'cell salvage' techniques that collect and then re-infuse the blood a patient loses during surgery. Many of these measures have long been used to treat Jehovah's Witnesses, who object to transfusions on religious grounds; now they are being applied to the broader population. \u201cWe're seeing more and more countries coming on board, asking for help in setting up patient blood-management programmes,\u201d says Aryeh Shander, executive medical director of the Institute for Patient Blood Management and Bloodless Medicine and Surgery at Englewood Hospital and Medical Center in New Jersey. The Netherlands has been at the cutting edge. In 2000, it adopted a transfusion threshold of 6.4 g dL \u22121  for otherwise healthy patients, and at least one blood bank reported a 12% decline in transfusions by 2009. And changes to blood-management programmes, new clinical guidelines and a shift towards less-invasive surgical techniques have led to declines in many other countries. In the United Kingdom, for example, the demand for red blood cells dropped by one-fifth between 1999 and 2012. And in the United States, the number of transfused units of whole blood and red blood cells fell by 8% between 2008 and 2011, the latest year for which data are available. The AABB, formerly known as the American Association of Blood Banks, predicts that statistics to be released later this year will show a further 10% drop. Since 2001, the proportion of US hospitals that have had to cancel elective surgery because of a blood shortage has also steadily fallen. Few believe that it is time for donors to stop rolling up their sleeves. There may still be shortages in some regions or in the aftermath of major disasters, and doctors anticipate an ongoing need for certain blood types and components, such as platelets, which do not last long in storage. But there is still plenty of room to reduce demand, says AABB chief executive Miriam Markowitz. A 2011 audit 13  of more than 9,000 UK transfusions, for example, found that more than half were potentially avoidable. Merely changing clinical recommendations may not be enough. \u201cMost people don't pay attention to guidelines,\u201d says Victor Ferraris, a cardiothoracic surgeon at the University of Kentucky in Lexington. And that may be particularly true when the guidelines seem to contradict first-hand observations. \u201cSurgeons are very, very experience-oriented,\u201d Ferraris says. \u201cEvery surgeon who's ever lived has seen someone's life saved by a blood transfusion.\u201d A study 14  published last October illustrates the challenge. When scientists surveyed doctors working at two intensive-care units at Johns Hopkins Hospital, the vast majority of the clinicians reported that the ideal transfusion threshold was 7 g dL \u22121 . But the hospital's electronic medical records revealed that 84% of patients in one unit and 92% in the other received transfusions before their haemoglobin levels fell that low. Some of the doctors deemed their patients too ill for the lower triggers and that the evidence did not apply to them, says David Murphy, the study's first author and a critical-care specialist at Emory University in Atlanta, Georgia. He and his colleagues also found that although doctors generally knew the recommended thresholds, many nurses did not. Nor did the units have a standardized approach, and caregivers rarely discussed the transfusion strategy for individual patients. \u201cIf you have ambiguity regarding what we should do for a patient, this greatly influences the likelihood of being able to deliver the right care,\u201d Murphy says. It is possible to overcome these problems, as the Stanford study showed (see 'Doctor's orders'). In the year before the computerized alerts began, just over half of transfusions were done on patients with haemoglobin levels of greater than 8 g dL \u22121 . By 2013, that proportion had fallen below 30%. \u201cThe fall-off was very immediate and it's been sustained,\u201d says Goodnough, who was the first author on the two papers 1 , 2  that reported the results. He thinks that the simple intervention succeeded for multiple reasons. For one thing, doctors may change their behaviour when they think that they are being watched. But the alerts also reminded clinicians about the guidelines, and provided links to the relevant literature. They also forced doctors to slow down and think, rather than defaulting to reflexive and long-ingrained standard procedure. Finally, they may have provided an opening for caregivers to discuss the needs of individual patients. \u201cMaybe the intern, who was ordering the blood because they were told to, goes back to the team and says, 'I have to give a reason', and then they discuss it,\u201d Goodnough says. The clinicians might decide to order the blood anyway, of course. Or they might stop, consider the evidence, and come to agree with what Goodnough believes is its clear message. \u201cThe safest blood transfusion,\u201d he says, \u201cis the one not given.\u201d \n                     Ageing research: Blood to blood 2015-Jan-21 \n                   \n                     First trials of blood-based Ebola therapy kick off 2014-Dec-15 \n                   \n                     Blood made suitable for all 2007-Apr-01 \n                   Reprints and Permissions"},
{"file_id": "520148a", "url": "https://www.nature.com/articles/520148a", "year": 2015, "authors": [{"name": "Mark Peplow"}], "parsed_as_year": "2006_or_before", "body": "Swiss-cheese-like materials called metal\u2013organic frameworks have long promised to improve gas storage, separation and catalysis. Now they are coming of age. Sprawled across a vast site on the river Rhine in Germany is a small city built from glittering steel: the headquarters of chemical giant BASF. Boasting a daytime population of about 50,000 people, it is criss-crossed by a grid of streets bearing names that commemorate the company's stock in trade: Methanolstrasse, Ammoniakstrasse, Gasstrasse. Over the past two years, a small fleet of delivery vans and cars has clocked up thousands of kilometres on these streets while carrying a big secret: fuel tanks packed with an unusual crystalline material that is riddled with pores roughly a nanometre wide. Inside these pores sit methane molecules arrayed in neat stacks, ready to fuel the vans' combustion engines. Mark Peplow discusses whether metal\u2013organic frameworks are delivering on their promise. The crystals are metal\u2013organic frameworks (MOFs), molecular scaffolds made up of metal-containing nodes linked by carbon-based struts (see 'An open box'). The resulting pores are ideal for trapping guest molecules and, in some cases, forcing them to participate in chemical reactions. And they can be tailored with exquisite precision: researchers have created more than 20,000 types of MOF, with potential applications that range from stripping carbon dioxide from power-plant exhausts to separating intractable industrial mixtures, catalysing chemical reactions and revealing molecular structures. \u201cMOFs are the fastest growing class of materials in chemistry today,\u201d says Omar Yaghi, a chemist at the University of California, Berkeley, and one of the pioneers of the field. MOFs were long thought to be too frail for use in the real world, often collapsing as soon as the guest molecules were removed. Many researchers were sceptical that the products could ever compete against the tough inorganic materials called zeolites, whose pores are exploited in a wide variety of industrial processes, including filtration and catalysis. But after more than a decade of intensive research in labs around the world, MOFs are poised to make their debut in commercial applications. Although unwilling to reveal the identity of the MOF in question, BASF has said that it is ready to market a methane-storage system this year that can cram in much more fuel than a conventional pressure vessel. MOF researchers say that this milestone would be a shot in the arm for their work, and potentially help to stimulate commercial interest in the many other applications that are close behind, often from other producers. \n               Storage wars \n             Much of the ferment in MOFs dates back to 1999 and the debut of two unusually robust varieties: HKUST-1, developed at the Hong Kong University of Science and Technology 1 ; and MOF-5, developed by Yaghi 2 . The latter has an internal surface area of at least 2,300 square metres per gram \u2014 enough to cover more than eight tennis courts. \u201cThat was the turning point, because it broke all surface-area records,\u201d says Yaghi. \u201cYears later, BASF told me that they thought it was a misprint.\u201d More internal surface area means more places to stack guest molecules, and Ulrich M\u00fcller, who leads BASF's research on porous materials, was quick to see an opportunity. \u201cWe started working on MOFs directly after Yaghi's paper,\u201d he says, and the pair quickly forged a collaboration that continues to this day. The key to making stable MOFs is to use clusters of metal atoms as the nodes, rather than individual ions. The geometry of the clusters determines the overall architecture of the crystal, which can be held together by a cornucopia of organic linkers. The growing set of interchangeable Tinkertoy components makes MOFs much more adaptable than zeolites and enables chemists to design products with pores that have just the right size and chemical properties for specific applications. Today, there are MOFs that can withstand temperatures of 500 \u00b0C, or easily endure a week in boiling methanol; others have internal surface areas that are triple that of MOF-5, or pores large enough to accommodate chunky proteins 3 . BASF currently dominates the nascent MOF market. It has targeted methane storage because shale gas is cheap and increasingly available, so could be used to power automobiles that incur lower running costs and generate less CO 2  than conventional vehicles. However, at present, the gas needs to be stored in bulky and expensive high-pressure tanks, which is a major disincentive. MOFs could overcome that by storing more methane at lower pressures. To make that application work, the size and chemical properties of the MOF's pores must be just right, because they determine how the methane molecules stack up inside. \u201cIf you just have methane floating inside the pore, you might as well use an empty canister,\u201d says Yaghi. To tie down the methane, researchers use MOFs with pores that boast exposed metal ions. The ions distort methane's electron cloud, polarizing it so that the gas molecules stick to the metal. But if the pores bind to methane too weakly, the gas will leak out; too strongly, and the vessel will be hard to empty. The best MOF crystals occupy a Goldilocks zone that gives them at least twice the capacity of an empty vessel at moderate pressure, yet still allows them to release almost all their methane as the pressure drops. \u201cMethane storage for automobiles is largely a solved problem,\u201d says Yaghi. Commercial success is far from guaranteed, however. Since the price of crude oil began to plummet last year, the economic incentive of gas has disappeared. \u201cThis gap is currently almost non-existent,\u201d says M\u00fcller. \u201cEverything is in a little bit of turmoil because of that.\u201d Market-watchers predict that the price of oil will rebound sooner or later. But in the meantime, Jeffrey Long at the University of California, Berkeley, says that there is ample scope to improve MOF methane-storage systems. In collaboration with Yaghi, BASF and the Ford Motor Company, he is aiming to reduce the pressure needed to fill a tank. \u201cIf you go down to 35 bar, people can potentially fill their cars at home,\u201d he says. Long and his colleagues say that they have created a MOF that stores more methane than the best current compounds at low pressure and are preparing to publish their results. \u201cWe can beat it by a reasonable margin,\u201d Long says. MOFs could make an even bigger impact on transportation by storing hydrogen for fuel-cell vehicles. Compressing chilled gas into high-pressure tanks is complex and expensive. But replacing those tanks with MOFs that can store useful amounts of hydrogen is a tough challenge. \u201cThere's no absorbent out there that has a high enough capacity to be used commercially,\u201d says Long. Long's team has developed 4  a record-breaking nickel-based MOF that binds to hydrogen strongly enough to carry 12.5 grams of the gas per litre of storage tank at room temperature and 100 bar. That is, however, well short of the US Department of Energy's hydrogen-storage target for 2020, which calls for a corresponding figure of 40 grams per litre. Using MOFs with metal ions in their pores that can each bind to several hydrogen molecules could bring researchers closer to that goal. In the meantime, others are looking to commercialize MOFs for niche gas-storage applications. Omar Farha at Northwestern University in Evanston, Illinois, co-founded the spin-out company NuMat Technologies in Skokie, Illinois, in 2012 to develop MOFs that can safely store some of the toxic gases used in the semiconductor industry, including boron trifluoride, phosphine and arsine. \u201cWe're doing something different from everybody else,\u201d he says. \u201cIt's a smaller market that we can capitalize on very quickly.\u201d Farha reckons that the company's first product will launch in the next two years, helped by a recent boom in the use of computer modelling to predict the properties of MOFs. In 2012, Farha and his colleagues showed that they could reliably screen almost 140,000 hypothetical MOFs for their methane-storing ability 5 , and now they are saving time and money by synthesizing only those MOFs that show promise in similar computational tests. \n               Trial separation \n             Researchers are also hoping that MOFs can pluck specific molecules out of the air \u2014 literally. \u201cGas separation, in particular, is where these materials could have a competitive advantage,\u201d says Long. They could be particularly attractive for industrial cracking plants, which heat crude oil to break down large molecules into light hydrocarbons. These gases can be extremely difficult to separate. Propene and propane differ by just two hydrogen atoms, for example, and their boiling points are only around 5 \u00b0C apart. At the moment, refiners isolate them by cooling the mixture until it liquefies, then warming it slowly so that first one gas and then the other boils off. But these temperature swings make it one of the most energy-intensive processes in the chemical industry. Long's group has shown that a crystal known as Fe-MOF-74 makes the job easier and potentially much less costly. The crystal's exposed metal cations can latch onto the electrons of a passing propene molecule, slowing down its passage. At a balmy 45 \u00b0C, propane emerges first; warming the MOF then frees a 99% pure stream of propene 6 . Another crystal, Fe 2 (BDP) 3 , can efficiently separate isomers of hexane 7 , which comes in linear and variously branched forms. The linear molecules get stuck in the corners of the MOF's triangular channels, an architecture that Long says would be impossible to achieve with zeolites. Perhaps the ultimate test of MOF-based separation would be to capture some of the 13.7 gigatonnes of CO 2  produced by fossil-fuel power plants each year. Conventional carbon-capture systems rely on solvents that react with the CO 2  in a power station's 40 \u00b0C exhaust stream. Removing and heating the solvent to 120 \u00b0C or more frees the gas for collection and storage. But swinging the temperature back and forth eats up 20\u201330% of the plant's power, and requires expensive infrastructure. Last month, Long's team showed 8  that magnesium- and manganese-based MOFs could absorb and release more than 10% of their weight in CO 2  from flue gas with a temperature swing of just 50 \u00b0C. Their pores are lined with amine molecules that are similar to the solvents already used for carbon capture, and react with CO 2  to produce chains of tightly packed ammonium carbamate molecules. A similar, as yet unpublished MOF can release its carbon cargo below 100 \u00b0C, and Long hopes to test it at the US National Carbon Capture Center in Wilsonville, Alabama. It has a higher working capacity and lower temperature swing than solvent systems, so Long expects to be able to reduce the size of the capture unit and cut infrastructure costs; he has already co-founded a start-up company, Mosaic Materials in Berkeley, to produce the MOFs. \n               Crystal sponges \n             Industrial-scale development of any new material is slow work. But applications can blossom remarkably quickly if only small quantities are required. Just two years ago, Makoto Fujita at the University of Tokyo developed a MOF that can help to determine the structure of pharmaceuticals and other organic molecules. Now he is deluged with commercial interest. Many organic molecules steadfastly refuse to form crystals, which normally rules out using the conventional technique of X-ray crystallography to determine the precise spatial arrangement of their atoms. But in 2013, Fujita's team showed that a zinc-based MOF could soak up molecules of miyakosyne A, a natural marine product, and hold them steady in its pores so that X-rays can reveal their structure 9 . \u201cI thought, 'Wow! This is a way to revolutionize the way organic chemistry can progress,'\u201d says Phil Baran, an organic chemist at the Scripps Research Institute in La Jolla, California. Others, however, were less impressed. Crystallographers found the MOF's performance hard to reproduce, and then Fujita's team found an error 10  in its structure of miyakosyne A, making others wary of the technique. Since then, however, Fujita and others have produced detailed instructions 11  that are winning over sceptics. The technique cannot handle all molecules, but Fujita thinks that 20\u201330% of the organic compounds they test can have their X-ray crystal structure determined in this way, using as little as 5 nanograms of the guest molecule. Last year, the Japan Science and Technology Agency awarded Fujita US$15 million over five years to help him to commercialize the method, and some pharmaceutical companies are now using the technique to assist drug development. And a Japanese reagent company plans to make Fujita's crystal sponge \u2014 and the successors being developed in his lab \u2014 available off-the-shelf within the next three years. \n               Go-faster MOFs \n             Catalysis has long been touted as one of the most promising applications for MOFs. Their tunable pores can hold reagents in place, cleave specific bonds and then forge new ones, just like the active site of an enzyme. But until a few years ago, progress on such catalysts was very slow, says chemist Joseph Hupp at Northwestern University, not least because very few MOFs were chemically stable enough to put through multiple rounds of reactions 12 . As a result, says Hupp, \u201cthere is not yet an example of a reaction for which MOFs are so superior that a typical organic chemist would choose a MOF-based catalyst over an existing catalyst\u201d. Now, however, researchers are making promising catalysts by taking stable MOFs and tweaking the chemical groups around their pores 3 . They have also gone one stage further, gradually swapping out entire linkers or metal nodes to transform the MOF's chemical and physical properties without collapsing the whole structure 13 . These advances have allowed chemists to design and make a much wider variety of rock-hard but chemically-active MOFs. \u201cThere's a lot of MOFs now that we just couldn't make five years ago,\u201d says Hupp. Indeed, one of the growing challenges for the field is the bewilderingly large number of MOFs. \u201cWe have too many,\u201d says Yaghi. Hupp agrees. Researchers need to dial back on synthesizing MOFs whose properties are never fully explored, he says, and instead focus on refining those that have proven stability or activity. Another challenge is that MOFs have to compete with incumbent technologies such as zeolites. This puts a premium on getting the cost down by making MOFs from abundant metals and cheap organic linkers that can be manufactured in safe and inexpensive processes. BASF, for example, is making MOFs at the tonne-scale in water, rather than in other solvents. Yet MOFs can compete through their originality. Yaghi is developing MOFs that contain several types of pore within the same crystal, so that molecules would undergo a predefined sequence of reactions as they pass from one region to the next 14 . These MOFs could behave like microscopic versions of a chemical plant, allowing scientists to synthesize molecules piece by piece in a continuous process. \u201cThat's our dream,\u201d says Yaghi, \u201cand it's only possible in MOFs.\u201d \n                     Materials chemistry: Cooperative carbon capture 2015-Mar-11 \n                   \n                     Materials chemistry: Selectivity from flexibility 2014-May-21 \n                   \n                     Revolutionary method for probing molecular structure unravels 2013-Sep-24 \n                   \n                     Taking the crystals out of X-ray crystallography 2013-Mar-28 \n                   \n                     Materials chemistry: Space invaders 2007-Aug-15 \n                   \n                     BASF \n                   \n                     Omar Yaghi\u2019s laboratory \n                   \n                     Jeffrey Long\u2019s laboratory \n                   \n                     NuMat Technologies \n                   Reprints and Permissions"},
{"file_id": "520282a", "url": "https://www.nature.com/articles/520282a", "year": 2015, "authors": [{"name": "Alexandra Witze"}], "parsed_as_year": "2006_or_before", "body": "Key voices from the project reveal the twists and turns of the space telescope\u2019s 25-year career. When the Hubble Space Telescope blasted into space on 24 April 1990, it promised astronomers an unprecedented view of the Universe, free from the blurring effects of Earth's atmosphere. But Hubble's quarter-century in orbit has never gone according to plan. The telescope \u2014 a joint venture between NASA and the European Space Agency (ESA) \u2014 faced a crippling flaw after launch that required astronauts to fly up and fix it. Later, problems with Hubble and NASA's shuttle programme left the telescope's future in jeopardy. Through it all, Hubble emerged as the world's foremost astronomical observatory. Conceived by astronomer Lyman Spitzer in the 1940s, the telescope has led to fundamental discoveries, revealing for instance that the furthest reaches of the Universe are full of galaxies and that dark energy is pushing the cosmos apart at an ever faster rate. Its stunning images have transformed scientific understanding of the Universe and become wildly popular. Here,  Nature  tells the story of Hubble through the words of some of its key players, beginning in 1972. At that time, the space telescope was little more than a set of engineering drawings. Robert O'Dell, former Hubble project scientist:  I was told it would not take very long to build it. But I went in with my eyes wide open. I could see that building Hubble was going to be the future. It was a chance to lead and influence the development of what I thought, even then, would be the most important telescope of my generation. Jean Olivier, former Hubble chief engineer:  Hubble was a proving ground for many technologies. Things you would think would be low-tech, like designing latches, evolved into a major problem. We kept uncovering more and more challenges. It got to be such a long programme that I began to think it's not real life, it's a game \u2014 and one day they're going to say: \u201cWe're just kidding, we wanted to see how much you could take.\u201d O'Dell:  The lowest period was when it was becoming clear that we couldn't afford to do everything that we wanted to. This was right in the early hardware phase. I proposed that we would initially launch Hubble without all the instruments that were being developed. I proposed that out of desperation because people were actually saying we were going to cancel the programme unless you significantly reduce the costs. The lowest day for me was being chewed out in NASA headquarters for not standing up for the science of the project. \n               Hubble finally soared into orbit in 1990 aboard the space shuttle  \n               Discovery \n               . But when the first image came back, it was blurry owing to a flaw known as spherical aberration. \n             Sandra Faber, astronomer, University of California, Santa Cruz:  The picture was taken with our camera [the Wide Field and Planetary Camera], and it looked weird. It was a star, but it had a bright point at the centre. One of the astronomers on our team looked at the image and said, \u201cThis telescope has spherical aberration.\u201d That immediate diagnosis was extremely severe, with huge consequences. Olivier:  The months immediately after launch were just a nightmare. Faber:  Our team wanted to know whether that was really true. We moved the secondary mirror in and out of focus in order to sample the spherical aberration at different levels. In June, at a project meeting, we showed our results and there could be no doubt. It was a catastrophe. Olivier:  I got a phone call to come into NASA headquarters. We explained what the problem was. The deputy administrator, J. R. Thompson, kept telling me, \u201cOlivier, you've got to turn another knob on the spacecraft to fix this!\u201d I said, \u201cJ. R., I don't have a knob to turn.\u201d It took a few days for the top men to realize, deep down in their hearts, that they had a real problem. We put a telescope in space and it could hardly see. I felt terrible. I felt like a dog wouldn't take a bone from me. \n               The problem turned out to originate from a spacing error in the device used to shape the primary mirror. The error had been made by the mirror contractor, Perkin-Elmer Corporation, and had been missed repeatedly by NASA. It affected all five of Hubble's initial instruments, and could not be fixed from the ground. \n             Edward Weiler, former Hubble chief scientist:  I had the unique honour of being the one to explain what the impacts on the scientific programme of Hubble would be. That was the day of infamy. But luckily, about two hours before the press conference, [Hubble imaging expert] John Trauger pulled me aside and said: \u201cEd, I think we've got something you should know about. We think we can fix this. We have these four relay mirrors that are flat, but if we put a small curve on them, a curve that is the opposite of the bad curve on the mirror, it will cancel out.\u201d I reported this to the press conference. I promised we had this fix in hand, and of course nobody believed anything we said. It was not a friendly situation. I had neighbours come up to me and say how much sympathy they had for me working on a national disaster. Faber:  Our big fear that was Hubble would not be fixed. How would we keep the public's and NASA's interest alive in Hubble while a repair plan could be invented? \n               It took three years to make that plan. NASA engineers had to develop ways to fix each instrument, with all the work done by astronauts in bulky spacesuits working in zero gravity. In December 1993, seven astronauts launched aboard the space shuttle  \n               Endeavour \n                to save Hubble. \n             Weiler:  If you had asked me for the odds ahead of time, I'd have said 50% success. This was the first time we ever tried to repair a satellite. Five [spacewalks] all had to go perfectly. But things kept going right. It was like a dream sequence. You were afraid you were going to wake up and there was going to be a problem. We went home at the end of the mission like a surgeon goes home after an eye operation: they've done everything they can, but until the bandages come off you won't know for sure. Antonella Nota, ESA Hubble Project scientist, Space Telescope Science Institute (STScI), Baltimore, Maryland:  When we saw the first images, it was like history had erased those three years of pain. Weiler:  We were all huddled around a little screen, waiting for the first image to come down. It probably only took five seconds but it seemed like six hours. First we saw a little dot in the centre, but it was a really well-focused dot. And then we saw the faint stars. You just knew, right then, that we had nailed it. That night, I slept like a baby. The trouble with Hubble was over. \n               With its corrected vision, the telescope could start doing the science astronomers had always hoped for \u2014 including responding to fast-moving celestial events, such as the death of comet Shoemaker\u2013Levy 9, which plunged into Jupiter just months after the repair mission. But that first big test for Hubble was almost a failure. \n             David Leckrone, former senior project scientist:  That was the most exciting week I had on Hubble. Many people don't realize that less than two weeks before the first impact, Hubble went into safe mode. Two days before a critical observation, a software engineer at Goddard [Space Flight Center] figured it out and fixed it. It was a brilliant success, to watch a comet tear apart into fragments and crash into the planet a few months after Hubble had been repaired. Imagine if that had happened in 1993 instead of 1994. Zoltan Levay, image scientist, STScI:  The first test. That was a huge deal. Weiler:  It's a classic great American comeback story. \n               One after another, Hubble's discoveries began landing on the front pages of newspapers and in top scientific journals. \n             Weiler:  Hubble has been the greatest scientific success in NASA's history. With just one picture it could show how the Universe didn't read our textbooks. Nota:  Hubble can look in wavelength regimes that are not accessible from the ground, like ultraviolet, because ultraviolet radiation gets absorbed by the atmosphere. Jennifer Wiseman, senior project scientist, Goddard Space Flight Center, Greenbelt, Maryland:  There was a burst of new science from Hubble right after 1993. One of these iconic images is the Eagle Nebula, where you see columns of gas where stars have recently formed and are still forming. The informal name is the 'Pillars of Creation', a grandiose title. This gave us a visual clue as to the interaction of young stars. Leckrone:  Bob O'Dell got pictures of the Orion Nebula. They showed these funny little cocoons all over the place. As you looked more closely, you saw examples of stars surrounded by dark disks. My god, these are places where planets must be forming! O'Dell:  It was the only truly eureka moment I've had as a scientist. Wiseman:  Hubble homed in on the core of the galaxy M87 to monitor the motion of gas there. The astronomers used a spectrograph to find the gas was moving about a million miles per hour in one direction on one side of the core, and a million miles per hour in the other direction on the opposite side. The only way something could be orbiting this fast would be if there were something very massive in the core in a very small volume. This was the first definitive observation of a supermassive black hole in the core of another galaxy. Leckrone:  Hubble continues to defy all expectations in creative new ways in which it can be used. Look at dark energy. Kenneth Sembach, head of the Hubble Mission office, STScI:  We know dark energy pervades the Universe because we've been able to measure the expansion rate of the Universe at different times. The key to doing that has been looking at distant supernovae [with Hubble]. The more distant supernovae are dimmer than you would have expected. The teams that won the Nobel Prize in Physics in 2011 realized that the Universe was expanding at an accelerating rate. This is the equivalent of throwing a ball up in the air and it just decides to speed up and keep going up. That would be a repulsive force rather than an attractive force. It works against gravity. Wiseman:  The repaired Hubble had exquisite angular resolution that allowed us to look for individual stars, to separate them in crowded regions. In this way you could actually study populations of stars and map out their properties. \n               The public responded to the flood of gorgeous imagery. Hubble became NASA's first Internet sensation. \n             Leckrone:  We've developed a following of people who are not astronomers but have learned to love astronomy. Levay:  I'm honoured that people admire these results. It has just kind of snowballed. People have done songs and stuff inspired by Hubble. There's poetry, artwork. We've been batting around ideas of why Hubble is so much in the public consciousness. One is because we came along right when the Internet was really starting to take off. A lot of people had easy instant access to the results from Hubble. Nota:  We call it the people's telescope. We have really brought the Universe to people's homes. Some 15 years ago I was in this remote area of Papua New Guinea, living on a ship that would dock in places where there wasn't even a harbour. One time, we couldn't believe it, there was a kid wearing a Hubble T-shirt. The child was delighted when we gave him a set of Hubble cards to play with, to go with his T-shirt. Weiler:  After I retired and moved to Florida, I negotiated with my wife. Half the pictures in the house are Hubble, and half are other things. \n               Astronauts continued to visit the telescope, upgrading and replacing its instruments regularly to extend its life. Sometimes, Hubble's future looked dim. In 1999, astronauts launched an emergency repair mission after three of the telescope's six gyroscopes failed. \n             John Grunsfeld, NASA astronomer and astronaut who has performed eight spacewalks to service Hubble : Hubble had gone dark, and it was a real question as to whether the science was over. For an astronomer and an astronaut, this was a holy grail of repair missions. Up we went, and soon enough we saw this bright star on the horizon. It was Hubble. It was surreal. There was one moment when I was out at the end of the robotic arm, and the operator drove me towards Hubble, slowly turning me over. I put out my index finger and just kind of tapped the telescope, to prove to myself it was all real. We deployed it on Christmas Day. I remember thinking, what better present could there be for planet Earth than a repaired Hubble? \n               Four years later, in the wake of the  \n               Columbia \n                shuttle disaster, NASA administrator Sean O'Keefe cancelled a final planned servicing mission, citing safety concerns. \n             Matt Mountain, former director, STScI:  What made it worse was the instruments started failing. It was actually pretty bleak. It was clear Hubble was not doing as well as it should be. Weiler:  Luckily administrators changed, and we got Mike Griffin in there. He supported looking at the alternatives, and at the end of the day we got our servicing mission. Mountain:  Griffin announced he would allocate two shuttles to this. That's an incredible commitment by a space agency to a science mission. Suddenly the attitude changed, and there was a future for the whole team at Hubble. Grunsfeld:  When we saw it on approach [on the final servicing mission, in 2009], it was as if we were seeing an old friend. Very few people have hugged Hubble the way I have. I knew all the handrails practically by name. When we let it go, it was in the best shape of its life. We had accomplished our job, and its science heritage would continue. \n               The telescope remains a premier tool, particularly for time-consuming, data-rich surveys that are meant to benefit the astronomical community for years to come. Hubble set the standard for uploading data to a communal archive available to all astronomers. \n             Jennifer Lotz, astronomer, STScI:  I feel incredibly lucky to have started my career in the golden age of astronomy and the golden age of Hubble. The idea of saving all the data and making it available to people after a certain amount of time, that was pretty radical. Now it is accepted practice. You don't have to be the student of the most famous professor in the world to have access to the best data in the world. Jason Kalirai, astronomer, STScI:  People have the misconception that its best days are behind it. More than two research papers every day come out of Hubble. What it's doing today is different from what it's done in the past. Nota:  Look at one example of a topic that didn't even exist when Hubble was launched: exoplanets. When Hubble launched we didn't even know about the existence of planets outside our Solar System. In 25 years that field has completely revolutionized. Hubble was not designed to study exoplanets but now is characterizing their atmospheres. Hubble always surprises us. \n               NASA is currently testing Hubble's successor, the James Webb Space Telescope, which is scheduled to launch in 2018. But researchers are still planning for Hubble's final years. \n             Wiseman:  Hubble right now is as scientifically powerful as ever, perhaps more scientifically powerful than ever. Sembach:  In the time we have left, we want to push the envelope. We want to do different things that we haven't done before. We've put out a call to the community asking for creative ideas. Should we be devoting more time to specific types of observations? Should we be devoted to helping students do research with the observatory? We expect to operate through at least 2020. Right now things look pretty good. That gives us a chance to overlap for a year or two with the James Webb Space Telescope. Paul Hertz, director, astrophysics division, NASA:  We will operate Hubble as long as it stays scientifically productive. My guess is that something's going to break someday. Leckrone:  It will be a gradual, graceful failure. With creative engineering you can keep doing good science. As long as we have at least two good instruments, I think we can keep going even when the spacecraft itself has suffered multiple failures. That might take us to 2025. But it's not going to be with us forever, and we're really going to miss it when it's gone. \n                 See Comment \n                 page 287 \n               \n                     Astronomy: Hubble's legacy 2015-Apr-15 \n                   \n                     Hubble successor will struggle to hunt alien life 2015-Feb-09 \n                   \n                     Hubble telescope time preferentially goes to men 2014-Sep-19 \n                   \n                     Hubble telescope reveals deepest view of the Universe yet 2014-Jan-08 \n                   \n                     Hubble spots water spurting from Europa 2013-Dec-12 \n                   \n                     18 years of science with the Hubble Space Telescope 2009-Jan-01 \n                   \n                     Nature  special: Hubble\u2019s 25th anniversary \n                   \n                     Hubble Space Telescope \n                   \n                     European Space Agency's Hubble page \n                   Reprints and Permissions"},
{"file_id": "520426a", "url": "https://www.nature.com/articles/520426a", "year": 2015, "authors": [{"name": "XiaoZhi Lim"}], "parsed_as_year": "2006_or_before", "body": "Chemists hope to break China's monopoly on rare-earth elements by finding cheap, efficient ways to extract them from ore. In July 2010, the Chinese government sent a chill through the world's high-technology industries when it announced a 37% cut in export quotas for rare-earth elements \u2014 a group of 17 metallic elements that are essential ingredients in display screens, low-energy lighting, high-powered lasers and a host of other twenty-first-century products. China has a near monopoly on production of these elements, generating 97% of the world's supply in 2010. So although Beijing said that it was just trying to clean up a particularly dirty sector of its mining industry, the cutback sent rare-earth prices soaring and raised the spectre of major economic disruptions. The reality turned out to be less dire: several Western mining companies have now started producing rare earths, and China, responding to demands from the World Trade Organization, has pledged to end the cutbacks by 2 May this year. Nevertheless, the incident prompted the United States and Europe to launch major research initiatives aimed at securing non-Chinese sources of rare earths. Those programmes are now beginning to achieve results. Central to the effort is a challenge for chemists. The rare earths are chemically almost identical, generally found together in ore deposits and extremely difficult to separate: the standard method involves some 300 steps and the copious use of hazardous chemicals. China has a network of extraction plants that can undercut any other producer in the world, thanks to the country's historically lax attitude towards costly environmental safeguards, along with factories that incorporate the elements into devices. But if chemists can come up with easier, faster, greener and, above all, cheaper extraction methods, then the balance could shift. Other countries could afford to exploit their own rare-earth deposits, and to recover rare-earth elements from their electronic waste. \u201cFor any Western company ever to be competitive,\u201d says Jack Lifton, a consultant for the rare-earth mining industry who works in Farmington Hills, Michigan, \u201cit has to find ways to produce the individual rare earths at lower cost than China.\u201d \n               Mash-up of metals \n             Disentangling the rare earths is so hard that it took chemists more than a century to identify and name them all: the first, yttrium, was discovered in 1794, yet the last two, lutetium and ytterbium, were not separated until 1907 (see 'Birds of a feather'). Early manufacturers did not bother purifying the metals. The first applications, early in the twentieth century, used a blended 'mischmetal' in cigarette lighter flints and tracer bullets. Individual rare-earth elements did not begin to find uses until after the Second World War, when the problem of how to separate them caught the attention of Frank Spedding, a chemist at what is now Iowa State University in Ames who had pioneered uranium purification for the Manhattan Project. Spedding perfected a method called ion-exchange chromatography, in which a mixture of rare earths was washed through a vertical glass column packed with polymer beads. The rare earths stuck to the beads, and were then washed off with dilute citric acid. Spedding adjusted the acid's pH so that each element would dissolve from the polymer at a slightly different rate; as a result, the ions travelled down the column at different speeds before emerging at the bottom in bands of slightly different composition. With enough repetitions, researchers could separate high-purity elements in sufficient quantities to study their properties. This led to an explosion in rare-earth applications, starting in the early 1960s with the discovery that a pinch of europium oxide mixed with other materials made a bright red colour for television screens. By 1965, soaring demand had led Western mining companies to expand europium production and construct the first separation plants. Because ion exchange was unsuited to large-scale production, the plants used a method called solvent extraction. A mixture of rare earths is dissolved in water, then shaken with organic solvents containing extractants \u2014 compounds that bind to some rare-earths more strongly than others. The rare earths move into the organic solvent, and after the mixture has settled, they can be chemically removed from the extractants and redissolved in water. It takes hundreds of repetitions to isolate the elements. Over the next few decades, Chinese chemical engineers began to up the solvent-extraction technology for use at their own facilities, which were soon able to sell individual rare earths of higher purity and at lower prices than Western producers could. By 1999, China had near-total control of the global rare-earth supply. \n               Cheaper, faster, cleaner \n             One way to change that situation is to step up production at the few solvent-extraction plants outside China, while finding ways to make the method cheaper and more efficient. \u201cChanging the chemistry is a lot easier than changing the infrastructure,\u201d says Alex King, director of the Critical Materials Institute (CMI): a US$120-million research hub headquartered in Ames, Iowa, that was set up by the US Department of Energy in 2013 to address supply problems in rare earths and other materials. Some researchers are trying to change the chemistry by finding extractants that can do a better job of differentiating between the rare earths. It is not easy, says Scott Herbst, a chemical engineer at the CMI who is doing just that at Idaho National Laboratory outside Idaho Falls. \u201cYou're separating almost apples from apples.\u201d He and his colleagues are tackling the problem both theoretically, by trying to design better molecules with computational modelling, and empirically, seeing whether they can adapt extractants developed for other industries. Other researchers are looking for better solvents. At the Catholic University of Leuven (KU Leuven) in Belgium, for example, chemist Koen Binnemans is making and testing a variety of ionic liquids: salts that are molten at room temperature. These typically consist of a large organic molecule that carries an electric charge, coupled with a small inorganic ion of the opposite charge. Binnemans says that such ionic liquids are safer, less volatile and more recyclable than the organic solvents most commonly used in industry \u2014 not to mention that they can hold around six times the number of dissolved rare-earth ions. Binnemans is trying to develop ionic solvents that can also function as extractants. Some companies are adapting separation methods from other industries. \u201cIf they can cut the cost, they will be very competitive,\u201d says Lifton. At one deposit in Alaska, for example, Ucore Rare Metals of Bedford, Canada, has turned ore into gram-scale quantities of 99%-pure individual rare-earth elements, using molecular recognition technology. Developed by the IBC Advanced Technologies in American Fork, Utah, this technique has been used industrially to remove bismuth impurities from copper, and to recover platinum-group metals from scrap catalytic converters. In the Ucore system, a solution of mixed rare earths passes in sequence through 17 different columns, each loaded with a compound tailored to bind to a specific element. That element can then be extracted in 99%-pure form by rinsing the column with dilute acid. Ucore says that the process requires no more than a few repetitions, depending on the desired purity, and so has the potential to be both efficient and environmentally friendly. The company is working with IBC scientists to prove the technology at a pilot plant over the next few months. \n               Begin again \n             Digging for rare earths is not always an option \u2014 particularly in Europe, where deposits are few and opposition to mining is widespread. But developed nations all over the world have large potential sources of rare earths in the form of used electrical devices. Most fluorescent lights contain europium, yttrium and terbium, for example, and strong permanent magnets typically contain neodymium and dysprosium. \u201cWhen these reach end of life, we still send them to China\u201d for recycling, says Tom Van Gerven, a chemical engineer at KU Leuven. \u201cWe would like to stop, and we can do that if we perform the recycling in Europe.\u201d In recycling, the challenge lies in extracting dilute resources: electronic waste typically has a lower concentration of rare earths than ore does. But any one component also tends to have fewer elements to separate. Usually, magnets must be dissolved in strong acids to extract the rare earths. Van Gerven is looking for an alternative method: he blasts the solid magnets with ultrasound, commonly used in chemistry laboratories to clean apparatus by dislodging particles from its surfaces. Although experiments are still ongoing, he hopes that the ultrasound will erode the magnet surfaces, allowing extractants to pull out the rare-earth elements without the need to dissolve the entire object. The CMI also has a team focused on recycling. In one project, extractants are bound to a membrane, where they can catch the rare earths as a solution flows past. This technique can separate elements from even very dilute solutions, says project leader Eric Peterson, who is based at Idaho National Laboratory. He expects full commercialization in the next year or two. The real-world effect of all of this research is difficult to predict \u2014 not least because China is a moving target. Beijing is working hard to reform and strengthen control over its rare-earth industry by consolidating 140 rare-earth companies into 6, cracking down on illegal mining, eliminating unneeded separation plants to reduce supply and raise prices, and conducting environmental remediation to make the industry profitable and socially acceptable. \u201cIt was a cowboy industry,\u201d says Lifton. \u201cThey're reining it in.\u201d These actions could add to the operating cost of China's separation industry, and make it easier for Western plants to compete. But separation is not the whole story, says King. Western manufacturers still have nothing to rival China's network of factories that turn separated rare earths into components such as monitors, magnets and lights. \u201cAll of the links in the chain have to be there,\u201d he says. Until such plants are built in the West, rare-earth producers such as Molycorp of Greenwood Village, Colorado, and Lynas of Perth, Australia, will have few places to sell their output \u2014 except China. And with global demand threatening to outstrip that country's mining capacity, Chinese producers are already looking to feed their factories by mining deposits of rare-earth elements overseas. So even if Western producers manage to develop separation processes that can produce the individual elements cheaply, economics might eventually force an ironic turnaround \u2014 with China importing rare earths from the West. \n                     Coming clean 2015-Jan-28 \n                   \n                     Resources: Track flows to manage technology-metal supply 2013-Dec-31 \n                   \n                     Japan and Vietnam join forces to exploit rare-earth elements 2012-Jul-13 \n                   \n                     Sea holds treasure trove of rare-earth elements 2011-Jul-03 \n                   \n                     Critical Materials Institute \n                   \n                     Koen Binnemans \n                   \n                     Tom Van Gerven \n                   \n                     Ucore \n                   \n                     Documentary on China\u2019s rare-earth industry \n                   Reprints and Permissions"},
{"file_id": "520600a", "url": "https://www.nature.com/articles/520600a", "year": 2015, "authors": [{"name": "David Cyranoski"}], "parsed_as_year": "2006_or_before", "body": "The fall out from the STAP case is still being felt across Japan. Hironobu Fujiwara was already troubled as he made his way to work on 5\u00a0August 2014. As a laboratory head at the RIKEN Center for Developmental Biology (CDB) in Kobe, Japan, Fujiwara had been enduring criticism ever since a case of misconduct had been exposed at the centre earlier that year. A media furore had escalated week by week, as newspapers, social media and television programmes all demanded an explanation for how scientists at a prestigious institute could have conducted shoddy work. Although he was not involved in the suspect studies, Fujiwara and many other employees at the centre felt under attack. And then, when he arrived that day, he heard terrible news. Yoshiki Sasai \u2014 a founding member of the CDB who had been implicated in the misconduct case \u2014 had hanged himself in an adjoining building that morning. \u201cI was just in shock,\u201d Fujiwara says. \u201cI didn\u2019t know what to think, or whether to believe it was true.\u201d Revelations of scientific misconduct always cause collateral damage: they taint colleagues and co-authors of the person responsible, and can close down labs. But the case at the CDB triggered unusually strong and far-reaching aftershocks. The case involved two high-profile papers in  Nature 1 , 2  that described a surprisingly simple method to make pluripotent stem cells \u2014 cells with the prized ability to develop into any of the body\u2019s cell types. After the misconduct came to light, a crucial report blamed not just the individuals involved, but also the entire centre \u2014 and recommended that it be dismantled. Since then, the CDB\u2019s funding has been slashed, half of its labs have been closed, merged or assigned to other organizations, and its leadership has been replaced. The upheaval reaches far beyond the centre. A government science administrative reform has been put on hold and the scientific community across Japan is now bracing for the impact of anti-misconduct policies introduced in the wake of the affair. To many scientists and journalists in Japan, this was an appropriate response. They thought that the stem-cell results had been sensationalized, and that it was fitting to take drastic action when those results were shown to be false. But other scientists in Japan and many abroad call it an overreaction and say that events over the past year reveal how responses to a crisis can create their own problems. The way that scientists and the media reacted exposed long-standing jealousies in Japan towards the 15 well-funded RIKEN centres and institutes, which had already made the CDB a target for critics. The frenetic media and social-media response created a storm that bureaucrats and science-policy administrators were desperate to quell. And reporting by  Nature  calls into question whether the CDB was given enough of a chance to defend itself: a committee behind the key report on the future of the CDB did not interview most of those involved to find the causes of the misconduct. Teruo Kishi, who led the committee, defends the process. \u201cYou seem to think that by talking to people involved we would find something out,\u201d he says. \u201cBut we wouldn\u2019t have learned anything from asking them.\u201d Some scientists worry that the ferocity of the response will hamper research for current or former CDB scientists and even dampen Japan\u2019s ability to support innovative science in the long run. \u201cIt was only two papers, but from the beginning to the end the media kept blowing it up larger and larger,\u201d says former cancer geneticist Yuko Ito, now a science-policy expert at the Japan Science and Technology Agency in Tokyo. \u201cIn the end, a lot of scientists became victims.\u201d \n               Scientist\u2019s paradise \n             RIKEN was founded in 1917 as the Institute of Physical and Chemical Research in Tokyo, and it expanded heavily into biological sciences with a series of institutes that opened from the late 1990s onwards. The centres became known in Japan as a scientist\u2019s paradise, because researchers there had no undergraduate teaching responsibilities and enjoyed generous salaries and research funds that meant that they did not depend on grants. Even before the stem-cell crisis, \u201cthe relationship between RIKEN and universities was not really good\u201d, says Hiroshi Hamada, a developmental biologist formerly at Osaka University, who took over as CDB director this year. Even among RIKEN centres, the CDB stood out. Launched in 2000, when Japan was trying to rejuvenate its research infrastructure, the institute abandoned the crusty hierarchical structure that encumbered university laboratories. Scientists and staff address each other with the common  san rather than the  sensei  typically used for superiors. In a 2002 interview with  Nature , Sasai vowed to \u201cgive young researchers a degree of independence\u201d that was previously unknown in Japan (see  Nature 415, 952\u2013953; 2002 ) \u2014 and soon young principal investigators, some still in their 20s, were mapping out their research programmes. A handful of established researchers, known as group directors and including Sasai and founding director Masatoshi Takeichi, oversaw operations. The approach quickly paid off, and high-profile results emerged. Lab leader Mitinori Saitou won acclaim for engineering germ cells in a dish (see  Nature 500, 392\u2013394; 2013 ); Sasai built a reputation for his skill growing eye and brain structures in culture (see  Nature 488, 444\u2013446; 2012 ). In a country where research has often struggled to make an international impact, the CDB developed a global reputation for exceptional work in developmental biology. The institute was proud to state that of 163 papers that its researchers published in 2013, one-third were in leading international journals including  Nature ,  Science  and  Cell . The centre seemed to score another big coup in January 2014, when some of its scientists published the two papers on pluripotent stem cells. Developing the technology, called stimulus-triggered acquisition of pluripotency, or STAP, was the project of biochemist Haruko Obokata, who had started the work at Harvard University in Cambridge, Massachusetts, and brought it to the CDB. She had worked with three highly respected scientists at the centre: mouse-cloning pioneer Teruhiko Wakayama, stem-cell biologist Hitoshi Niwa and Sasai. When the technique was published, it caused excitement in Japan. Pluripotency had become almost a household word since Shinya Yamanaka at Kyoto University had won a Nobel prize in 2012 for his work on a related technique to make induced pluripotent stem (iPS) cells. And the media made much of Obokata, a young, quirky woman who wore a  kappogi  \u2014 a type of traditional apron that she had received from her grandmother \u2014 instead of a lab coat, and whose laboratory walls were painted pink and yellow. She was everything that the traditional Japanese scientist was not. Within weeks of the papers\u2019 publication, however, the work began to unravel. Science blogs pointed to manipulated figures and other scientists could not repeat the results (see  go.nature.com/h9tr5w ). A committee at RIKEN investigated and found signs of misconduct. RIKEN officials held marathon four-to-five-hour press conferences, formally accused Obokata of misconduct on 1\u00a0April, and recommended that the papers be retracted, which they later were. The committee\u2019s report said that Sasai and Wakayama had not been involved in the misconduct, but shared \u201cgrave responsibility\u201d for not catching the problematic data. Niwa was cleared of wrongdoing. At this point, the typical university fraud case in Japan \u2014 and there have been several high-profile examples over the past 15 years \u2014 winds down. The researcher implicated in the misconduct usually resigns and the tainted papers are retracted. Witness, for example, the resignation of University of Tokyo molecular biologist Shigeaki Kato and the retraction of dozens of his papers between 2012 and 2014. In that case, the damage was limited to members of Kato\u2019s group. But with STAP, the media was hungry for more. News teams camped in the lobby of the CDB and filled the corridors of the science ministry, looking for twists to the story. They trailed Obokata, Sasai and many other CDB researchers and administrators. (Japan\u2019s national broadcaster, NHK, apologized after Obokata was injured in one scrape with reporters.) News articles, tweets and blogs started spreading the blame to the CDB and RIKEN as a whole. Critics said that the misconduct investigation had been hastily prepared, and had failed to get to the bottom of the problem. They also focused on a patent that RIKEN and the scientists involved in the work had filed on the STAP technique \u2014 standard practice in academia, but taken as evidence that the centre and its researchers were driven by economic incentives. Noriko Osumi, a developmental biologist at Tohoku University and one of the most vocal critics of RIKEN during the STAP crisis, faulted the organization for \u201cinviting excessive attention from the media\u201d, which she blamed on \u201cthe influence of commercialization and industrialization of research in the life sciences\u201d. The desire for money and media attention was seen by others as a reason that senior scientists at the CDB had thrown their weight behind the STAP project and let their guard down, allowing poor science to take place. Young CDB investigators who had nothing to do with Obokata\u2019s work were worried that their reputations were being tarnished. \u201cAt first the line was blurred, then we saw that there was no line at all,\u201d says Yu-Chiun Wang, who had arrived at the CDB in October 2013 from Princeton University in New Jersey. \u201cWe were all seen as part of the same criminal complex.\u201d CDB scientists, technical staff and secretaries were being slandered online, says Takeshi Imai, a neurobiologist at CDB. \u201cMyself, my lab members and colleagues were also there,\u201d he says. Even researchers at other RIKEN centres felt under attack. \n               Quieting the storm \n             The media hullaballoo was a headache for the science ministry, which was making plans to give certain research organizations unprecedented autonomy. RIKEN was to be the first. The ministry \u201cwanted to finalize the misconduct matter quickly\u201d, says Maki Kawai, a former RIKEN executive director. It thought an aggressive action \u201cwould seem a more reasonable response to the taxpayers\u2019 voice, which is the media\u201d. On 9 April 2014, shortly after Obokata was charged with misconduct, RIKEN established an independent \u2018reform committee\u2019 chaired by Kishi, a lean 75-year-old materials scientist with a long list of top administrative positions. On 12 June, the Kishi committee released eight recommendations, including the promotion of research integrity, new fraud-prevention measures and a more thorough investigation of the STAP papers. But among these, one recommendation leapt out: to \u201cdismantle\u201d the CDB. \u201cThe Kishi report threw me out of my chair,\u201d says Ichiro Hiratani, a developmental biologist who had joined the CDB the previous year. The Kishi committee\u2019s main task was to suggest measures to prevent misconduct, but half of its report analysed what had happened in the STAP misconduct episode, and why. The centre should be dismantled, the report said, because \u201cthe CDB, as an organization, had structural flaws, that induced or could not deter fraud\u201d. One of the main problems was an ossification of leadership: since its founding, the centre had had basically the same group directors. These, the report said, had come unconsciously to rely on each other in a cosy relationship that prevented critical scrutiny of each other\u2019s decisions. Takeichi says that he and the CDB had been aware that a change was overdue well before the crisis. Over the preceding three years, the centre had searched for a foreign director to breathe new life into the CDB and increase its global status, but found it difficult to recruit a scientist of suitable calibre. However, Takeichi\u2019s leadership had not been in question before the STAP crisis. In 2011, RIKEN\u2019s advisory committee opposed his retirement. The Kishi report said that poor governance at the CDB had led to mistakes. It accused the centre of circumventing normal procedures to help hire Obokata, and \u201csurmises\u201d a reason: \u201cthe CDB was strongly motivated by a desire to acquire a groundbreaking result\u201d that would outdo Yamanaka\u2019s discovery of iPS cells. The report \u201csurmises\u201d that Sasai \u201cnaturally was involved in STAP in anticipation of the huge budget\u201d that it would bring to the CDB. The report also said that Obokata\u2019s decision to wear a  kappogi  was part of a \u201cshowy PR strategy\u201d orchestrated by Sasai. In response to the report, RIKEN quickly convened a committee to start working out how to implement the recommendations. \u201cGiven how harshly the media was treating RIKEN, it had no choice but just accept it,\u201d a science-ministry official who does not want to be named told  Nature . But Takeichi and Sasai contested many of the assertions in the report. Takeichi said that the hiring process for Obokata was normal; he and Sasai denied that there was a desire to outdo the iPS discovery, and other researchers at the CDB say that there was no pressure to do so. Part of Sasai\u2019s job as CDB deputy director was to organize and seek funding \u2014 a job at which he was very successful \u2014 but in an e-mail to  Nature \u2019s news team in June 2014, he denied that he had become involved in STAP because it would attract money to the institute. Sasai, Takeichi and RIKEN\u2019s public-relations office have all denied being involved in Obokata\u2019s choice of clothes. In the interview with  Nature  in February, Kishi stood by his committee\u2019s report and acknowledged that it went beyond fact. \u201cThere was a lot of speculation, guesses, in the report,\u201d he said. \u201cBut they were guesses made with confidence,\u201d on the basis of what he had seen in long, televised press conferences. Kishi said that in preparing its report, the committee did not talk to Obokata, Sasai or anyone at the CDB aside from Takeichi \u2014 a fact that other members of the committee confirmed. Kishi says that RIKEN would not let the committee meet with Sasai. But, Kishi said, \u201chis televised conference made me feel that I didn\u2019t need to hear any more than what he said. I felt he said enough, he cannot be changed.\u201d Then Kishi passed a surprisingly harsh judgement on Sasai. \u201cEven if we had asked him, he would not have told us the truth,\u201d he said. This differs from the opinion of those who knew Sasai well, who say he was an honest, broad-minded person, devoted to scientific research. Committee member Masaki Nakamura, a research-integrity specialist and historian of science at Osaka University, also defended the report. He says that speculation was used in it the same way that a prosecutor will fill in the motivations of a defendant in a court of law. Four other members of the Kishi committee did not respond to requests for comment, or refused to comment. Nicholas Steneck, an expert in research integrity at the University of Michigan in Ann Arbor, says that in a misconduct case, he personally \u201cwould not be comfortable speculating about anything that was not based on some facts and direct information\u201d. He says, however, that such speculation is surprisingly common among scientists. \u201cHistory and reporting on current events should follow basically the same methods as science,\u201d he says. When the Kishi report came out, many foreign scientists found its conclusions excessive and arbitrary, and more than 150 letters of support were sent to the CDB. But in Japan, it was mostly received uncritically. Scientific organizations such as the Science Council of Japan threw their weight behind the judgement. The most dispiriting thing for many CDB researchers was the indifference or criticism from their scientific colleagues \u2014 some of it a reflection of the resentment of the CDB and RIKEN that had built up over the years. \u201cNewspaper and media, fine, they are trying to sell newspapers,\u201d says Hiratani. \u201cBut getting almost no support from scientists was shocking and depressing.\u201d The fallout was especially hard on Sasai. Until June he had been \u201csomehow getting by\u201d, says Keiko Muguruma, a stem-cell biologist at the CDB and a frequent collaborator with Sasai \u2014 they had just submitted a paper for publication. \u201cIn science, he felt he could recover from the harm to his reputation. But for things that he had no control, like the dismantling and budget decrease, which would affect all the young researchers, he felt guilt and responsibility,\u201d says Muguruma. According to the lawyer working with Sasai\u2019s family, the Kishi report and media attacks were factors in explaining Sasai\u2019s suicide. Kishi and Nakamura respond that it is difficult to know why Sasai took his own life. In August, the CDB began making changes in line with the Kishi report. RIKEN announced a plan to introduce new fraud-prevention measures and strengthen governance. Months later, in November, Takeichi stepped down and an interim director was put in place. The CDB saw 9 of its 40 laboratories shifted to other RIKEN centres, and another 11 were merged or closed. The centre also changed its Japanese name to \u201ccentre for research into the formation of multicellular systems\u201d, although it kept its original English name. In December, after announcing that she had failed to repeat her experiments, Obokata resigned. The changes are still going on. On 1 April this year, Hamada took over as director and a 40% cut to the centre\u2019s budget kicked in. Researchers have been scrambling to supplement their finances with grants. \n               Assessing impact \n             In interviews in the past two months, Kishi and Nakamura expressed tempered views of the CDB. Nakamura says that Takeichi\u2019s leadership had been \u201cextremely good in comparison to other universities and research institutes\u201d. Although \u201cwe wrote about problems in their research ethics and education\u201d, Nakamura says, the CDB \u201cwas relatively speaking really quite advanced\u201d. The word dismantle ( kaitai ) was a strategic choice meant to please an angry press rather than to put an end to the CDB, say both Kishi and Nakamura. It was \u201can appeal to society, trying to show the idea that RIKEN was taking this problem seriously\u201d, Nakamura says. Both say that they wanted to see the centre rebooted under new leadership and with a new name. Kishi says he wanted a \u201creborn CDB\u201d \u2014 and in his view, the restructuring has met that aim. \u201cNothing\u2019s really changed,\u201d says Kishi. But that is not how it looks to those on the ground. Hamada and other CDB researchers worry about whether they will be able to attract new principal investigators and postdoctoral researchers to the now-tarnished centre. It has lost two of its most prominent researchers in Sasai and Niwa, who is moving to Kumamoto University this year. Fujiwara has stayed at the CDB, but says that his research has suffered. When he was supported by secure funding from the centre, he had planned a five-year project to map proteins in the extracellular matrix of hair follicles. Now that he must secure year-to-year grants, that project looks harder to realize. The impact of the STAP episode has also been felt further afield. Since August, policies for preventing and dealing with science misconduct have been released by the science ministry, the health ministry and other bodies. As part of this, data-management rules are being tightened to ensure that data are checked more often and are made available for verification. Such rules are frustrating scientists, and there are widespread concerns that research is becoming overly bureaucratic. \u201cWe had a pretty good system, but now we have to check this and that,\u201d says Kawai. \u201cI don\u2019t want to grab time from scientists.\u201d The new science-ministry guidelines make an institution responsible for fraud, and say that its funding can be cut if fraud-prevention measures are deemed inadequate. Imai says that this could be counterproductive, because it could push institutes and scientists to cover up suspected misconduct. STAP could have a more subtle but pervasive impact on science, says Ito. Historically, young researchers in Japan have been able to take an experiment in a different direction from their supervisor\u2019s, and to follow their hunches down often productive paths. But the fear of fraud is likely to lead to stricter internal auditing, including the checking of lab notebooks, so \u201cnow they\u2019ll be afraid to. That will affect their ability to be innovative and they will be less motivated,\u201d says Ito. \u201cThat will be the biggest impact going forward.\u201d It is ironic, she says, that the experience of the CDB, which was born out of the desire to encourage young scientists, could end up stifling them. Steneck says that it is useful to assess whether the STAP case was handled in the best way. He says that RIKEN responded well when the issue came to light: \u201cThey faced up to the problem quickly and brought in other opinions.\u201d But he questions the make-up of the Kishi committee, which was dominated by scientists. He thinks that a misconduct investigation should be led by experts in studying behaviour in scientific and research institutions. \u201cHaving a committee of non-experts gather their own evidence does not work.\u201d Paul Taylor, director of the Office for Research Ethics and Integrity at the University of Melbourne in Australia, says that a Japanese investigatory organization akin to the US Office of Research Integrity might have helped. \u201cIt provides an independent source of expertise in conducting investigations, and perhaps ensures that trust in research is maintained,\u201d he says. He adds that Japan\u2019s tightened requirements on data management and research-integrity training might help to prevent scientific misconduct in the future \u2014 but there are limits. \u201cIf someone is able to justify to themselves why it is OK to fabricate an image, or steal someone\u2019s work, then they will do that even in the presence of the best research-integrity environment.\u201d Hamada is optimistic that the CDB can move on. He hopes to negotiate with the science ministry to restore researchers\u2019 budgets within three years, and plans to continue the CDB\u2019s mission of gambling on promising young scientists. \u201cI have to reorganize, refresh,\u201d he says. \u201cMy job is to forget about what happened.\u201d That will not be easy for Fujiwara. He was just starting to get good data \u2014 but is worried that he will have trouble getting funding for the mouse experiments that he needs to even publish his work so far. \u201cIt was going to be an important year for us,\u201d he says. Now, he just hopes that his science can survive. \n                     President of Japan's RIKEN research labs resigns 2015-Mar-24 \n                   \n                     Stem-cell pioneer blamed media 'bashing' in suicide note 2014-Aug-13 \n                   \n                     Stem-cell scientists mourn loss of brain engineer 2014-Aug-05 \n                   \n                     Research integrity: Cell-induced stress 2014-Jul-03 \n                   \n                     Papers on \u2018stress-induced\u2019 stem cells are retracted 2014-Jul-02 \n                   \n                     Scientists rally around beleaguered Japanese research centre 2014-Jul-01 \n                   \n                     Nature  special: The rise and fall of STAP \n                   \n                     RIKEN Center for Developmental Biology \n                   \n                     RIKEN \n                   Reprints and Permissions"},
{"file_id": "520604a", "url": "https://www.nature.com/articles/520604a", "year": 2015, "authors": [{"name": "Mohammed Yahia"}, {"name": "Declan Butler"}], "parsed_as_year": "2006_or_before", "body": "Four years after revolutions shook governments in North Africa and the Middle East, scientists face an uncertain future. As a young physicist in Tunisia, Imen Sfar gave little thought to politics until March 2010, when a street vendor set himself on fire to protest about corruption in the city of Monastir, where Sfar worked. Two months later, an influential blogger and close friend of Sfar's was arrested and interrogated for helping to organize a demonstration opposing Internet censorship in Tunisia. His detention gave Sfar the \u201ccourage factor\u201d, she says, to join the revolution that erupted in December 2010, after another street vendor set himself aflame in Sidi Bouzid to protest against harassment by local officials. That act unleashed years of pent-up frustrations against the repressive government of President Zine al-Abidine Ben Ali, who had ruled the country for almost a quarter of a century (see  Nature   469 , 453\u2013454; 2011 ). \u201cEvents unfolded very quickly, with people taking to social media and the streets,\u201d recalls Sfar. \u201cIt was what I call the abolition of fear; we no longer were scared of anything.\u201d The uprising triggered a wave of unrest that swept across North Africa and the Arabian Peninsula. Demonstrations and revolts destabilized long-standing regimes, ultimately toppling rulers in Tunisia, Libya, Egypt and Yemen. Nearly five years after the Arab Spring, the political situation in most of those countries remains volatile and many scientists are struggling more than they did before. This week,  Nature  profiles how researchers' situations have changed in Tunisia, Egypt and Syria, which have followed starkly different political trajectories. In Tunisia, scientists are celebrating the country's successful transition to democracy, although the hard-won political freedoms have not yet translated into changes in the research system. In nearby Egypt, many of the freedoms won through the revolution have been reversed and the status of scientists has improved little. Syria has deteriorated the most, spiralling into an all-out civil war that has driven most scientists to flee the country. Yet even amid the chaos, some researchers have remained in Syria, where they struggle to teach students and carry on their research in any way possible. \u201cI do not fear death, it will happen anywhere you go \u2014 but many students in Syria need our help,\u201d says Ahmad Almansour, a materials engineer at the University of Aleppo. \n               Tunisia \n             Before the Arab Spring, Tunisia had a long tradition of support for education and research, unlike many of its neighbours. It produced more papers, relative to the size of its population, than any other Arab nation apart from Saudi Arabia, and it was the only one to invest more than 1% of its gross domestic product (GDP) in research and development. But the repressive government kept tight control of university policies and stifled academic freedom, especially in disciplines that delve into potentially controversial territory, such as the social sciences. Faouzia Charfi, a retired physicist and veteran opponent of the regime's human-rights restrictions, tells how researchers needed permission for almost everything, even organizing academic conferences and collaborating with foreign peers. The regime feared people acting on their own initiative, says Charfi, who became junior minister for higher education in the first post-revolution transition government. Universities and researchers had little freedom to develop their own policies or strategies. And regime bureaucrats blocked attempts to build links between universities and industry, stifling the kind of innovation that builds economies and creates jobs, she says. The police state established by Ben Ali affected most aspects of everyday life. Few dared to criticize the government or even to discuss politics, and people could never be sure that neighbours or friends were not part of the regime's web of informers, says Sfar. Instead, they took refuge in the safe subject of sport. \u201cBefore the revolution, Tunisians discussed nothing but football,\u201d she says. After Ben Ali fled Tunisia in January 2011, freedom of expression exploded overnight, says Sfar. But the euphoria soon dimmed as the country went through four years of successive governments and political turmoil, including the assassination of leading opposition politicians. At times, many feared that the democratic gains of the revolution would be reversed. But Tunisia's robust civil society kept the revolution on track, with huge street demonstrations and strikes. After the moderate Islamist party Ennahda reached a compromise with secular parties, Tunisia last year became the first and only Arab Spring country to successfully transition to democracy. In a historic moment, parliament overwhelmingly passed a constitution that guaranteed free speech, freedom of expression, religious freedom and equality between the sexes. The constitution also explicitly protects scientific and academic freedom, and mandates that the state \u201csupply the means necessary for the development of scientific research and technology\u201d. In October, a secularist party won the parliamentary elections, and in December its candidate Beji Caid Essebsi was elected as the nation's president. The biggest change since the revolution, says Sfar, \u201cis this kind of feeling of freedom to say what we think about politics and the administration without fearing serious repercussions\u201d. At the professional level, she says that the revolution has had little impact on her field of condensed-matter physics and she has been preoccupied with starting her career and a family. She has taken a position at the University of Tunis El Manar, and is contemplating switching to theoretical work on materials with unusual electrical and magnetic properties because she lacks access to the kind of equipment she used while completing her PhD in France. More broadly, the dramatic changes in Tunisian society have yet to permeate the research and higher-education systems. The past quarter of a century of authoritarian rule has left a system that is far from realizing its potential, says Hechmi Louzir, director of the Pasteur Institute in Tunis. The regime's tight grip on free thinking stifled creativity, he says. \u201cPeople didn't stray from the beaten path,\u201d says Louzir. Sfar says that even now, many of her students do not seem keen to think for themselves, and they expect lecturers \u201cto show them one, and only one, road to the truth\u201d. And Charfi is disappointed that researchers have not yet taken advantage of their new freedoms to bring in sweeping reforms, attributing the reticence to the highly conservative culture of the country's scientific community. Educational reform and innovation in science are important, say researchers, because they will help Tunisia to create wealth and jobs \u2014 a priority given the country's dismal economic state and high unemployment. And Tunisians are ever mindful of local threats; just last month radical Islamist militants killed 22 people in a terrorist attack on the Bardo National Museum in Tunis. Still, Tunisian researchers point to some positive changes. In the past, political nepotism and influence often determined who held key university posts, but democratic elections now decide, they say. As Sfar looks back over the changes, she sees grounds for optimism. \u201cThere is now freedom of expression and thought, and an extraordinarily active civil society,\u201d she says. \n               Egypt \n             After a citizen uprising toppled the regime of President Hosni Mubarak in February 2011, microbiologist Ramy Aziz returned to Egypt full of hope for change. \u201cI had wanted to come back to Egypt to bring the scientific experience I gained in bioinformatics back to Cairo University,\u201d says Aziz, who was at San Diego State University in California at the time. \u201cThe revolution was the spark that changed everything overnight.\u201d Students, researchers and professors took part in the uprising and their expectations soared in the months that followed. The government promised to increase science funding and many expatriate researchers vowed to return home. Researchers called for reforming outdated policies \u2014 such as requirements that slowed down the transfer of samples across borders or limited interactions with industry. Such changes, they said, would help Egypt to target its most pressing problems: water and energy insecurity, poverty and mounting unemployment. But the high hopes of Aziz and others have yet to be fulfilled. The government approved a doubling in science funding in 2012, but there was no clear agenda on how to use those funds and the research ministry failed to spend more than 80% of its budget. \u201cOn the ground we did not feel any change in the budget really,\u201d says Aziz, who is now at Cairo University. Then, on 3 July 2013, a popularly backed coup removed the first democratically elected president, Mohamed Morsi, who represented the Muslim Brotherhood. Several of the gains that researchers won after the uprising have since been reversed. In June last year, for example, newly elected president Abdel Fattah al-Sisi scrapped the law introduced after the uprising that allowed professors to choose their university's leaders. The positions are now appointed by the president (see  Nature  511, 5; 2014 ). \u201cFrom a scientific point of view, I can't see any change in Egypt,\u201d says Aziz. He does feel that university students and postgraduates have become bolder and more willing to challenge their professors and ask for their rights \u2014 a positive outcome. But he does not expect that to make a big difference in Egypt's science. The biggest win for scientists is constitutional, says Alaa-Eldin Adris, who chairs the department of petroleum and energy engineering at the American University in Cairo. The new constitution, passed in January 2014, states that 1% of GDP should be spent on scientific research, and that this can be increased up to international standards. \u201cOn paper this looks good,\u201d Adris says. But he cautions that the constitutional change is only the first step. \u201cIt is worthless unless it creates laws that are enforced.\u201d According to the World Bank, science spending in Egypt rose from 0.24% of GDP in 2009 before the uprising to 0.43% in 2011. But most of the increase goes to salaries rather than to funding research. In the  Global Competitiveness Report 2014\u20132015 , produced by the World Economic Forum, Egypt ranks among the worst ten countries in the world in the quality of its scientific research institutes. \u201cWe have many researchers, which is good, but when it comes to using science to solve problems or drive economic growth, we are almost at zero,\u201d says Adris. Aziz says that working life in Egypt today is a continuous battle in a stilted system, where he lacks access to core facilities and an environment that supports research. In Egypt, he focuses on teaching and mentoring postgraduates, but he returns to the United States for a few months a year to conduct most of his experiments. When he works there, primers for DNA synthesis arrive a day after he orders them. \u201cIn Egypt, you get primers in three weeks. If they don't work well, you will need another three weeks,\u201d he says. Aziz thinks that the current political leadership is sincere in its goals to support science but has done little to actually help researchers. Others, however, accuse the regime of taking a strong stand against freedom of expression within universities. Students have been arrested during protests on the campuses of Cairo's Al-Azhar University and at Cairo University and security forces have frequently stormed the university gates, leading to violent confrontations with protesters. Many young people who might otherwise have helped to build up Egyptian science are being driven out. Islam Kotb left after he lost many friends in a violent dispersal of protesters in August 2013. \u201cLeaving the country became my ultimate aim at that time and I accepted the first job offer I got,\u201d he says. Now working as a critical-care specialist in Jeddah, Saudi Arabia, he hopes to return to Egypt one day, but stresses \u201cthat scientific research can never succeed and flourish in such an environment\u201d. For Aziz and Adris, the real threats to research in Egypt are the bureaucracy (buying chemicals can take several months, for example) and the government's failure to engage in serious reform, such as overhauling education in ailing public schools and universities. None of these problems has been addressed by any of the successive ruling regimes, they say. \u201cI personally expect that by 2020 there will be little change, if any, in scientific research and output,\u201d says Aziz. Yet even with all these problems, he still finds hope. \u201cI see excellent students trying to learn against insurmountable odds,\u201d he says. \u201cYou need to create a new generation with a different mindset and proper education that will eventually replace the current people in control, with their outdated vision.\u201d \n               Syria \n             In January 2013, students were gathering for their mid-year examinations at the University of Aleppo when a series of rockets hit residence halls and other buildings. Smoke billowed across the campus and people ran in panic to find shelter. The attack killed at least 82 people, including students and refugees who had sought protection at the university. The rebels and the government blamed each other for the bombing. Materials engineer Almansour was lucky that day. The blast blew out the windows of the building where he was working, 300 metres from the centre of the explosion, but he and his family were unharmed. Although most Syrian scientists have fled the country to escape the ongoing civil war, Almansour feels an obligation to his students and is staying put. \u201cIf we all leave the country, who is going to teach them?\u201d he asks. Whereas Tunisia and Egypt have started to stabilize politically, the situation in Syria has grown steadily worse. What started as an uprising against lack of freedom and an oppressive autocratic regime has turned into a civil war with many factions and little hope of resolution. Throughout the four years of turmoil, however, some education and research have continued. When Michel Rahal graduated in 2013 from Damascus University with a bachelor's in applied chemistry, he had hoped to pursue a postgraduate degree in Europe. But the government eliminated almost all scholarships for overseas education, and in any case, Rahal felt a responsibility to stay in his country during these challenging times. He is now studying for a master's degree at the Higher Institute of Applied Science and Technology in Damascus, although he feels that his options are limited while in Syria. Some of those who have left say that it was impossible for them to remain in the country. Ahmad Salman, who asked to have his real name changed for fear of retribution against his family still in Syria, is one of those who tried to stay as long as possible. \u201cDespite the difficulties of life in Syria during the war with the lack of security, access to water or fuel and the huge inflation, I lived with the hope that the revolution would end in favour of the people,\u201d he says. But when the regime passed a law drafting all men into the army, he left his position at Al-Baath University and fled to Turkey, where he is now jobless. \u201cI only had two choices, either join the military and be on the front lines to kill or be killed by my countrymen, or get out of Syria.\u201d Most international researchers in Syria have also fled the violence, and that has impaired research in the country. The International Center for Agricultural Research in the Dry Areas (ICARDA), one of the biggest research centres in Syria, struggled for almost two years to keep its doors open before leaving the country for neighbouring Lebanon in late 2012. Mahmoud Solh, a geneticist and director-general of ICARDA, says that looters had repeatedly attacked the facilities and stolen vehicles, computers and other equipment. But the centre managed to save its agricultural gene bank, one of the most important such collections, before it relocated. Researchers who remain in Syria say that the working conditions there are extremely difficult. \u201cStudents in the medical and engineering school need equipment and materials that they cannot get,\u201d says Zeina Al-Ahmad, a finance specialist at Tishreen University in Latakia. \u201cWe cannot participate in conferences as we used to either, which has also affected the quality of research in Syria.\u201d The high inflation rate, which peaked at 121% in August 2013, has hit everyone hard, making it nearly impossible for scientists to fund their research and publishing activities. And security remains a major concern for students and faculty members, who must often pass through dangerous war zones to reach their universities. Almansour lives close to his university's campus, but many students have had to drop out for fear of their safety. \u201cThe number of students attending classes is about 35\u201340% of the registered number,\u201d he says. \u201cI am most worried about the possible destruction of the infrastructure. It took us a long time to build campuses, but if the fighting spares universities and schools then we will be able to go on in spite of the terrible circumstances.\u201d As bad as conditions are in Aleppo and other areas controlled by the government, the situation can be even worse for those elsewhere. The extremist group ISIS has taken control of large areas of eastern Syria, where it has closed schools and universities and is forcing teachers to work in its newly founded schools or lose their lives, impelling many professors and teachers to flee. There are few signs that things are likely to improve, with ISIS and other groups now involved in the civil war. Given all the problems, Rahal plans to move to Europe as soon as he can. Many other researchers are also leaving, says al-Ahmad. \u201cThe young generation is trying to finish university and find a job abroad.\u201d \n                     Science reborn in Tunisia 2012-Jan-27 \n                   \n                     The Arab Spring offers hope but no quick fix 2011-Aug-31 \n                   \n                     Security focus hinders progress in Arab world 2011-Feb-10 \n                   \n                     Revolution may end repression of academic freedom in Tunisia 2011-Jan-26 \n                   \n                     Tunisian minister outlines hopes for the future 2011-Jan-26 \n                   \n                     Tunisian scientists rejoice at freedom 2011-Jan-25 \n                   \n                     Nature  Special: The Arab Awakening \n                   \n                     Nature  Middle East \n                   \n                     World Economic Forum competitiveness report \n                   \n                     UNESCO science report 2010 \n                   Reprints and Permissions"},
{"file_id": "521024a", "url": "https://www.nature.com/articles/521024a", "year": 2015, "authors": [{"name": "Linda Nordling"}], "parsed_as_year": "2006_or_before", "body": "After years of second-class status in research partnerships, African scientists are calling for change. The gavel fell at precisely midday on 18 July 2014. Ruling on a lawsuit lodged against the Kenya Medical Research Institute (KEMRI), the Nairobi Industrial Court agreed that six Kenyan doctors in an international research partnership had been systematically passed over for promotion and training, whereas their European colleagues had flourished. It was a landmark case \u2014 and not just because the judge ordered KEMRI to pay each of the doctors 5 million Kenyan shillings (US$56,000) to compensate for their stymied careers. It was also perhaps the first time that African researchers had so strongly \u2014 and so publicly \u2014 voiced resentment of their perceived second-class status in partnerships with foreign colleagues. Collaborations have proliferated in recent decades as international agencies have stepped up funding for health research in Africa. Yet African scientists say that they often feel stuck in positions such as data-collectors and laboratory technicians, with no realistic path to develop into leaders. \u201cI think it's a big problem,\u201d says Marcel Tanner, director of the Swiss Tropical and Public Health Institute in Basel. And when partnerships fail, he says, it is often the people from developing countries who have the most to lose. \u201cThey don't have the support to see them through changes of funder or projects.\u201d This happened, for example, in 2010, when French backers had to withdraw from a malaria-control project in C\u00f4te d'Ivoire because of civil war. As a result, African staff members were left with a half-built laboratory, no way to pay the people they had employed and no funds to continue the project. To try to give African scientists more independence, several global funders are testing ways to build research leadership in Africa, and are transferring some ownership of their own projects there to local scientists. In the works for later this year is a 'fairness index', in which institutions and funders will earn certificates for engaging in equitable partnerships. But there is still a long way to go, says Glenda Gray, president of the South African Medical Research Council (SAMRC) in Cape Town. No matter how hard African scientists struggle for control of the research agenda, she says, they will not fully succeed until their own governments start to pay their share. International funders do sometimes still act as \u201cneighbourhood bullies\u201d in Africa, she says \u2014 \u201cbut it's easy to be strong when you have money\u201d. \n               Swim against the tide \n             The roots of these unequal partnerships lie in how modern research began in Africa \u2014 generally with European nations that opened research stations in their colonies to study economically important issues, such as tropical diseases, crops or agricultural practices. As the centres grew, Africans were recruited first as assistants and then as scientists. But even after countries such as Tanzania and Kenya became independent in the 1960s, the most influential research centres there were often funded, led or at least heavily influenced by partners in wealthier countries. By the mid-1990s, however, researchers both in Africa and in the developed world were voicing concerns about 'helicopter science' or 'sample safaris', in which foreign scientists were coming to Africa and collecting samples \u2014 often with local scientists' help \u2014 before going home to analyse them and write up their findings, giving little or no credit to their African counterparts. In 2001, for example, Kenyan researcher Moses Otsyula claimed that scientists from the University of Oxford, UK, stole blood samples that he had collected from unusually HIV-resistant children at a Nairobi orphanage. The Oxford scientists called it an inadvertent error of miscommunication, saying that they thought they had permission at the time. These misunderstandings are becoming rare as foreign scientists grow more sensitized to ethical issues, and as African scientists realize the need for clearer rules governing sample collection and export. Yet inequitable partnerships remain a problem. In a survey published last year, the European Association of Development Research and Training Institutes in Bonn, Germany, found that the role of partners in Africa and other developing regions was \u201coften still primarily limited to collecting data\u201d, whereas partners from the developed world played a leading role in analysing and publishing the findings (see  go.nature.com/42xjns ). Some of the people interviewed for the study said that, even for developed-world scientists who want to build more-equitable partnerships, the mounting pressure to publish articles to advance their careers often leaves them no time to do so. Many African scientists have had personal experience with problematic partnerships. Some describe being shut out of collaborations that they helped to set up, whereas others talk about running projects on the ground as their developed-nation colleagues fly in and out \u2014 often combining a little work with a lot of tourism. But few are prepared to make their stories public because that might mark them as troublemakers and cut them off from future funding. \u201cThere are good and bad relationships, and all of us have had them,\u201d says Gray. Although her council spends roughly US$9 million a year on health research \u2014 one of the biggest such budgets on the continent \u2014 that figure is only about one-fifth of what the US National Institutes of Health spends in South Africa. Still, she says, overseas funders are increasingly willing for African agencies such as hers to influence the agenda. The SAMRC has made steps towards that. In 2013, for example, it began to combine its funding with that of international donors to create joint-funding pots in which both groups have a say in what they support. There has definitely been progress in recent years, agrees Kevin Marsh, who until last year headed a partnership between KEMRI and the UK Wellcome Trust, known as KWTRP. During his tenure, he battled fiercely with funders to improve the conditions for Africans who are working for the collaborative programme. At one point, Marsh says, he nearly resigned when the funders did not want to provide equal salaries to UK and African staff. \n               A long road \n             But even though there are many more prominent African researchers today than there were 20 years ago, he says, it is important not to overstate the gains. That is why he is helping to set up the Alliance for Accelerating Excellence in Science in Africa (AESA), a research-management hub in Nairobi, that African presidents will launch in June. Designed to manage outside funding for programmes in Africa, AESA has start-up support from several major organizations, ranging from the KWTRP to the policy-making body New Partnership for Africa's Development, headquartered in Pretoria. Another project is the COHRED Fairness Index, which takes its name from its leading sponsor, the Council on Health Research for Development (COHRED) in Geneva, Switzerland. The index will permit institutions, funders or other groups involved in research to receive certification if they engage in good partnerships. Organizations can use this certification as a label to signify their status, in the same way that manufacturers put symbols on their goods to mark them as eco-friendly. A funder might get high marks if its programmes target a developing-country's national research priorities or its burden of disease, says Najia Musolino, a senior specialist with COHRED who is working on the index. Conversely, a health or science ministry in a developing country might get a bad rating if it does not regularly set or update national research priorities. The index is expected to enter a trial period towards the end of this year and, if successful, could be extended to research areas other than health. Musolino says that funders and academic institutions in Africa and abroad have offered cautious expressions of support, and think that the index could help African researchers to choose good partners. Gray says that the index will succeed only if developing-country researchers get a voice in how it works \u2014 and if African countries start to put more money into health research, so that their scientists have a stronger base from which to negotiate partnerships. That argument resonates with other African scientists, who want nothing more than to be accepted as equals. \u201cNobody wants to be a token scientist,\u201d says Kenyan immunologist Faith Osier, who works with the KWTRP. \u201cIt's true that African scientists face disadvantages,\u201d she says. But at the end of the day, she wants her contributions to be judged on merit \u2014 not on anything else. Some African scientists think that it is wrong to place all the blame for inequalities in Africa's research landscape on international funders. \u201cIt is pointless to say to the United States that 'you should fund our scientists, but we should tell you what you should do'. That, to me, is hypocritical and disrespectful, of not just the funders, but also of their own local scientists,\u201d says Salim Abdool Karim, director of the Centre for the AIDS Programme of Research in South Africa in Durban. \u201cWhat we need is for African scientists to understand, appreciate and promote excellence as a mechanism by which they can deal with international researchers on a collegial basis, not on a neocolonial basis.\u201d Many of the affected researchers agree with Gray that a key missing factor is national government commitments to science. Despite promises of increased funding by many African governments over the past decade \u2014 in Kenya, Nigeria, Tanzania and South Africa among others \u2014 few have fulfilled their stated ambitions. \u201cWe should look critically at ourselves first and foremost before we start blaming funders or European universities,\u201d says Kelly Chibale, a Zambian biochemist who heads a drug-discovery unit at the University of Cape Town, South Africa. \u201cUnless \u2014 and until \u2014 we create good local infrastructure and a supportive local environment, we will continue to struggle.\u201d \n                     African hub set up to boost research autonomy 2015-Apr-08 \n                   \n                     Developing world: Discuss inequality 2014-Sep-16 \n                   \n                     Kenyan doctors win landmark discrimination case 2014-Jul-22 \n                   \n                     Africa science plan attacked 2014-Jun-25 \n                   \n                     Foundation opens TB lab in Africa 2012-Oct-02 \n                   \n                     Nature  special: Africa \n                   \n                     COHRED Fairness Index \n                   \n                     South African Medical Research Council \n                   Reprints and Permissions"},
{"file_id": "521144a", "url": "https://www.nature.com/articles/521144a", "year": 2015, "authors": [{"name": "T. V. Padma"}], "parsed_as_year": "2006_or_before", "body": "Despite great strides in some areas of research and development, the nation still has a long way to go. With her jeans, T-shirt and spirited attitude, Tapasya Srivastava could pass for a student as she works in her brightly lit cancer-biology lab on the University of Delhi South Campus. Srivastava, who oversees a team of eight researchers, is thrilled that she earned \u201ca small research space of my own\u201d in 2010, while still in her thirties. \u201cWith a decent list of publications under my belt, I am one of the few who have studied and undergone training entirely in India,\u201d she says. Eight kilometres away, in the chemical-engineering department of the Indian Institute of Technology Delhi, Shalini Gupta's team is developing sensors to detect early-stage sepsis and typhoid. Gupta did her doctorate in the United States but returned to India to focus on its needs: \u201cI am more connected to society and its challenges,\u201d she says. T V Padma discusses the biggest wins in Indian science \u2014 and its major challenges Srivastava and Gupta are part of a wave of young Indian scientists convinced that they can do high-quality research at home rather than having to move abroad. Such optimism reaches all the way to the top: in January, Indian Prime Minister Narendra Modi told an assembly of scientists to \u201cdream, imagine and explore. You will have no better supporter than me.\u201d India has much to be proud of. Last year, it became the first to reach Mars on its initial attempt. It boasts a thriving pharmaceutical industry that produces low-cost medications that are desperately needed by the developing world. And in his first year in office, Modi launched an ambitious plan to make India a leader in solar power. Such successes cannot hide the huge challenges facing this country of 1.3 billion people, which leads the world in tuberculosis incidence and maternal deaths, and lacks electricity for one-quarter of its citizens. India is expected to become the world's most populous nation within a generation, and it will require a robust science and technology sector to supply the needed energy, food, health care, jobs and growth. Yet researchers in India and abroad say that the country has a relatively weak foundation in science and engineering. Indian research is hampered by stifling bureaucracy, poor-quality education at most universities and insufficient funding. Successive governments have pledged to increase support for research and development to 2% of India's gross domestic product (GDP), but it has remained static at less than 0.9% of GDP since 2005. Despite its huge size, India has a relatively tiny number of researchers, and many of its budding scientists leave for other countries, never to return. Only by tackling its systemic problems can India compete with other emerging powerhouses such as Brazil and China. \u201cThe density of scientists and engineers in India is one of the lowest in the world,\u201d says Sunil Mani, an economist at the Centre for Development Studies in Trivandrum, who is assessing Indian science and engineering for an upcoming report by the United Nations Educational, Scientific and Cultural Organization. \u201cThere are very many important areas where we are not able to do research.\u201d \n               Space to grow \n             In one of the cleanest rooms in India, Mylswamy Annadurai is busy conducting fitness tests on a 750-kilogram patient \u2014 a gleaming satellite called ASTROSAT. The probe is strapped to a table, where it is being shaken at six times the strength of gravity to simulate the intense forces of lift-off. ASTROSAT must also pass tests in extreme high and low temperatures and vacuum conditions, followed by checks on its solar arrays and antennas. If all goes well, the satellite will blast into orbit by September, armed with two telescopes and four other instruments to study both nearby and distant stars. Annadurai, who is head of the satellite centre of the Indian Space Research Organisation (ISRO) in Bangalore, says that ASTROSAT will be India's \u201cfirst full-fledged science mission\u201d in space. It will carry instruments ten times heavier than those on India's first mission to the Moon, 2008's Chandrayaan-1, and its 2014 Mars Orbiter Mission, nicknamed Mangalyaan. With its run of recent accomplishments, India has earned international acclaim for its ambitious space programme, which includes launch vehicles, communication satellites and one of the world's largest constellation of remote sensing satellites, as well as its science missions. Since ISRO was founded in 1969, the government has invested heavily in it, and even established a dedicated university in 2007 to train personnel. \u201cThe ISRO technical test, assembly and launch facilities are first class,\u201d says Paul Spudis, senior staff scientist at the Lunar and Planetary Institute in Houston, Texas, who was the principal investigator for one of Chandrayaan-1's experiments. Chandrayaan-1 carried an orbiter and a 35-kilogram probe that took images as it smashed into the Moon at high speed. ISRO plans to follow it in 2017 with Chandrayaan-2, which will gently set down a lander and a six-wheeled rover; together with an orbiter, they will study the composition of the Moon's surface. Up next after that is the Aditya mission to study the Sun's corona, in 2018. Spudis is critical of last year's Mars mission, calling it \u201clargely irrelevant\u201d and saying that it would have been better to return quickly to the moon. ISRO, he says, \u201cseems to lack a strategic vision of what it wants to accomplish in space\u201d. But the agency counters that it is pursuing several missions in parallel; the Mars mission just proceeded faster than Chandrayaan-2. And the success in reaching Mars has convinced others at ISRO that they can carry out world-class space-science missions, says Annadurai. \u201cThe Mars mission experience has once again strengthened our belief that we can.\u201d \n               Biotech bonanza \n             In Genome Valley, a biotechnology park in Hyderabad, entrepreneur Krishna Ella is confounding expectations. Ella returned home from the United States in 1996 with a 12-metre shipping container filled with vaccine-making equipment to support his grand plan of producing a US$1 vaccine for hepatitis B. That goal, which would make his vaccine at least an order of magnitude cheaper than the available one, struck investors as crazy, he says. But within three years, Ella's company Bharat Biotech International Limited (BBIL) succeeded in producing the Revac-B+ hepatitis vaccine at $3 a dose, which has since dropped to 30 cents per dose. It went on to develop vaccines against Japanese encephalitis, rabies, haemophilus influenza virus B and, most recently, rotavirus. Each costs barely a dollar per dose. Affordable medicines are the cornerstone of India's health-care sector, where publicly funded hospitals struggle to provide treatment. The country has long battled infectious diseases such as tuberculosis, malaria and dengue, but is now facing rising numbers of non-communicable illnesses, including diabetes and coronary heart disease. A 2014 report from the World Economic Forum and Harvard School of Public Health estimates that non-communicable diseases and mental illness could cost India $4.58 trillion by 2030. Low-price vaccines and generic drugs have helped India to carve out a niche in the international pharmaceutical industry. The global medical charity M\u00e9decins Sans Fronti\u00e8res (also known as Doctors Without Borders), which relies on Indian generics for 80% of its anti-HIV drugs, hails the country as the \u201cpharmacy of the developing world\u201d. Other international organizations, including the UN children's charity UNICEF and the Global Fund to Fight AIDs, Tuberculosis and Malaria, routinely use Indian vaccines and generic drugs to treat infectious diseases (see  Nature   468 , 143; 2010 ). But India is battling criticism over the quality of some of its pharmaceuticals. In 2012, for example, the World Health Organization took BBIL's hepatitis B vaccine and oral polio vaccine off the list of drugs preapproved for use by the UN. Ella says that the issues related to documentation submission and that they have since been sorted out. The vaccines are now back on the list. In 2014, the US Food and Drug Administration (FDA) sent warning letters to seven Indian firms over various concerns relating to pharmaceutical production there. An FDA spokesperson told  Nature : \u201cWhile some Indian companies meet US product quality standards, others have been found to lack sufficient controls and systems to assure drug quality, both of finished product and active ingredients.\u201d The FDA has an India office to work closely with Indian drug regulators to solve those problems. And some in the biotech sector warn that India has a long way to go to create a thriving enterprise in developing new drugs. The country's success in the generics industry relies on a different set of skills: reverse-engineering pharmaceuticals created elsewhere by breaking them into their components and remaking them through cheaper routes. \u201cThe challenge for the sector will be to graduate from reverse engineering to new-drug discovery,\u201d says Pallu Reddanna, a biotechnologist at the University of Hyderabad. \u201cThere is need for incentives and promotion of academy\u2013industry interactions.\u201d The government and private sector are trying to jump-start such efforts by setting up incubators that help transfer university and lab know-how to industry, and provide infrastructure and financial support to start-ups. Such incubators are the \u201cgreatest changer in the drug-discovery sector in India\u201d, says P. Yogeeswari, a chemist at the Hyderabad campus of Birla Institute of Technology and Science. Krishnaswamy VijayRaghavan, secretary of the government's Department of Biotechnology, commends \u201cincredible growth\u201d in India's biotech entrepreneurship \u2014 despite the lack of big drug companies and the relatively low domestic investment in drug discovery. International and industry collaborations with academia are helping to advance the sector, he says (see page 148). In 2013, the department started two major projects seeking drugs for drug-resistant tuberculosis and chronic disorders such as heart disease. In early leads, scientists have zeroed in on some human proteins that are crucial for the survival of multidrug-resistant tuberculosis strains. Proof-of-concept studies in mice have demonstrated that targeting such host proteins could help to kill the drug-resistant strains, says VijayRaghavan (S. Jayaswal  et al .  PLoS Pathog.   6 , e1000839; 2010 ). \u201cWe are at an exciting early applied stage,\u201d he says. \n               Power hungry \n             Nearly 2,000 kilometres north of Genome Valley, 9.7 hectares of solar panels cover a building in Punjab state, generating 7.5 megawatts of electricity. This project is India's largest roof-top solar installation that is connected to an electrical power grid, and it signals India's outsize ambitions in renewable energy. Coal supplies two-thirds of the electricity in India and will remain king for some time. But the government has set aggressive goals for installing solar-energy capacity. In 2014, Modi's government announced that it would develop 100 gigawatts of solar-energy capacity by 2022. This is a huge leap from the existing 3.7 gigawatts of solar capacity \u2014 just 1.4% of India's total electricity generation today. \u201cIndia is one of the most attractive markets in the world,\u201d says Pashupathy Gopalan, Asia Pacific head of SunEdison, a global solar-energy company based in Maryland Heights, Missouri, which is joining Adani Enterprises of Ahmedabad to build India's largest solar-panel-manufacturing facility. \u201cWe are entering a new era where solar electricity is competitive and has achieved 'socket parity' with other sources of energy in India.\u201d There are other big international collaborations. The Solar Energy Research Institute for India and the United States was established in 2012 to target emerging research areas. In one project, researchers are trying to generate solar thermal power by using sunlight to heat up a highly compressed fluid form of carbon dioxide so that it turns electricity-generating turbines. This could be used in much smaller plants than conventional steam-driven turbines. But some analysts say that India suffers from \u201cgigawatt obsession\u201d. The focus on giant solar plants comes at the expense of smaller facilities that do not require large parcels of land, but could provide electricity to isolated towns, even without being connected to the grid. \u201cThe gigawatt rush must pay attention to the pace with which the capacity is to be built in India,\u201d says Satish Agnihotri, former secretary of India's Ministry of New and Renewable Energy. Plans to build large plants could run into opposition in densely populated or heavily farmed areas, and in remote areas it can be difficult to hook gigawatt projects up to the electrical system. News and debates about the government's current focus on solar power have overshadowed past successes in wind energy. India has more than 23 gigawatts of installed wind-power capacity, which puts it roughly even with Spain as the world's fourth biggest producer. And Mumbai-based Suzlon is the world's seventh-largest turbine manufacturer. India has been able to develop its wind power in part because of long-term government policies and financial incentives, as well as a growing interest from independent power producers and financiers, says Shantanu Jaiswal, lead analyst at Bloomberg New Energy Finance in New Delhi. But some of the concerns about solar power also hamper wind projects, which face difficulty acquiring land, encounter lengthy permitting processes and often have trouble connecting to the electrical power grid. \n               Education outlook \n             Back on her leafy campus in Delhi, Srivastava and her fellow young faculty members are less concerned about big national projects than about producing their own high-quality research. They are lucky, they acknowledge, to work in one of India's top federally funded universities, which has superior faculty members and equipment. Others are not so fortunate. India has some 700 universities of varying quality, from the elite institutions funded by the central government to more than 300 state universities and about 200 private ones. \u201cThe landscape of science education is uneven,\u201d says Sri Krishna Joshi, former director-general of India's Council of Scientific and Industrial Research (CSIR) and former chair of the advisory committee of the University Grants Commission, which funds and oversees university education in India. In the top institutions, he says, \u201cscience students are doing world-class research, publishing in leading journals and boosting the global reputation of our country\u201d. National scientific research institutes and leading universities have all contributed to India's growing strengths in research: the country's output of scientific publications quadrupled between 2000 and 2013. Even so, India is not keeping pace with some other emerging nations, which have increased their scientific output more quickly (see page 142). And the advances in India's global science metrics mask some signs of declining quality in university science education, especially at the cash-starved universities funded by state governments that account for the majority of India's science undergraduates, says Joshi. Publicly supported universities depend on the Ministry of Human Resource Development for funds, and the higher-education budget was hit by a 3% cut in the 2014\u201315 budget cycle. \u201cLack of even bare, minimal and sustainable funds for teaching, let alone research, has seriously plagued the quality and standards of science education,\u201d says Krishna Ganesh, a chemist and director of the Indian Institute of Science Education and Research in Pune, one of five top universities set up in India since 2006. Many students at state universities are receiving a substandard education, says Joshi. \u201cHere, there are no good science teachers, no good Indian textbooks, and most of the science laboratories are poorly equipped,\u201d he says. \u201cWe are caught in a vicious circle of mediocrity,\u201d says geneticist Deepak Pental, former vice-chancellor of the University of Delhi. Most analysts are concerned over the plight of science departments in state universities. At the University of Calcutta, for example, even procuring a laptop involves endless red tape, says physicist Amitava Raychaudhuri. At some other institutions, support from funding agencies helps to purchase equipment, but there is a shortage of qualified faculty members to train the students. Beyond questions of quality, the quantity of available university spots is a persistent problem. India has gone through a university building boom, but there still is a huge shortage of slots for students (see  Nature   472 , 24\u201326; 2011 ). \u201cThere is a rise in the number of students going for higher education in India, which reflects the rising aspirations of its society. But this rise should be matched by better infrastructure and financial support,\u201d says Joshi. \n               Research investments \n             Investments in science have also dragged. India's research intensity \u2014 the share of its gross domestic product devoted to research and development (GERD) \u2014 remains lower than those of many other nations, including Brazil and Russia. Twenty years ago, India's GERD exceeded China's. Now, it is less than half. But those numbers do not tell the whole story, says Ashutosh Sharma, secretary of the government's Department of Science and Technology \u2014 one of India's largest research-funding agencies. \u201cThe total funding is, perhaps, not as poor as it seems in terms of absolute numbers, because the number of full-time scientists doing research is also low.\u201d India averages about 4 full-time researchers per 10,000 people in the labour force, whereas China boasts 18 and nations with advanced science and technology sectors have around 80. \u201cIndia spends about $150,000 per scientist per year, which is probably not too far from the optimal levels,\u201d says Sharma. India's notorious bureaucracy deserves part of the blame for the problems afflicting science education and research. The administrators of several state universities are political appointees rather than leading academics. \u201cOften the appointed person has never been exposed to a good university in India or abroad,\u201d says Kizhakeyil Sebastian, chair of the science-education panel of the Indian Academy of Sciences in Bangalore. \u201cThere is over-bureaucratization within the universities and their controlling bodies,\u201d says Pental. It often takes two years to recruit an academic after announcing an open post, which means that the best applicants can slip away, says Raychaudhuri. The governmental quagmire has begun to affect some elite national research institutes, too. Of the 38 national laboratories that are part of the CSIR, only 25 have full-time directors. The rest are making do with acting directors, or temporary arrangements. Even the CSIR headquarters in New Delhi has been without a full-time leader since January 2014. Interim director-general Madhukar Garg says that \u201cthe current situation is indeed challenging. CSIR is the backbone of scientific and technological research in the country. In case the prevailing scenario continues, it will affect the national innovation system as a whole.\u201d Sharma acknowledges that red tape is \u201call-pervasive\u201d, but he says that the challenges are not bogging down Indian science. \u201cIn terms of output indicators such as the number of papers per dollar spent, Indian science is among the very top performers in the world,\u201d he says. And there are some signs that India might be slowing its debilitating brain drain. Although the vast majority of Indians who obtain science doctorates in the United States remain there for at least 5 years after graduation, the proportion has declined: from 89% in 2001 to 82% in 2011, the most recent year for which data are available. Kaustuv Datta, a geneticist at Delhi University South Campus, is one of those who returned. Datta may \u201chate the red-tapism\u201d at universities in India, but he still prefers doing research back home. \u201cMy parents are here, in India. And academics have a strong, positive influence on the next generation of students,\u201d says Datta. \u201cI wanted to make that contribution in India\u201d. \n                     Indian bioscience: The anti-bureaucrat 2015-May-13 \n                   \n                     Science in India 2015-May-13 \n                   \n                     India by the numbers 2015-May-13 \n                   \n                     Research management: Priorities for science in India 2015-May-13 \n                   \n                     India\u2019s budget disappoints scientists 2015-Mar-02 \n                   \n                     India's PhD students on hunger strike over delayed pay rise 2015-Feb-24 \n                   \n                     India joins elite Mars club 2014-Sep-24 \n                   \n                     First Modi budget spells austerity for Indian science 2014-Jul-11 \n                   \n                     Nature  special: Science in India \n                   \n                     Department of Science & Technology \n                   \n                     Department of Biotechnology \n                   \n                     Indian Space Research Organisation \n                   \n                     Ministry of New and Renewable Energy \n                   Reprints and Permissions"},
{"file_id": "521148a", "url": "https://www.nature.com/articles/521148a", "year": 2015, "authors": [{"name": "Apoorva Mandavilli"}], "parsed_as_year": "2006_or_before", "body": "K. VijayRaghavan is determined to cut through red tape and build up biological science in India. On 12 April, Krishnaswamy VijayRaghavan posted an update to his more than 2,500 Facebook friends. It announced a bold plan from India's Department of Biotechnology (DBT) \u2014 the agency that VijayRaghavan leads, and the country's largest funder of biomedical research \u2014 to establish a new marine-biology institute and research stations along India's vast coastline. Within hours, 500 people had 'liked' the post and more than 60 had left comments of congratulations. Only one offered a critical note. A graduate student said that starting programmes is all well and good, but the DBT must hold the researchers whom it already funds accountable for the quality of their science. Shortly after, VijayRaghavan replied: \u201cYour words are very wise and correct! Thank you. We must keep your points in mind if we are to get maximum for our Rupee and have quality science.\u201d It is rare for a public official to be so responsive and open to criticism, especially in a country as steeped in bureaucratic hierarchy as India, says biologist Inder Verma at the Salk Institute for Biological Sciences in La Jolla, California, who has served as a scientific adviser to the Indian government since the 1980s. Yet almost anyone who contacts VijayRaghavan by Facebook, Twitter or e-mail gets a personal response in minutes. \u201cVijay is a breath of fresh air,\u201d Verma says. VijayRaghavan is more than that. He is a respected fly geneticist and administrator who helped to build the National Centre for Biological Sciences (NCBS) in Bangalore, one of India's most prestigious institutions, from the ground up. In January 2013, he left his job as NCBS director and moved to New Delhi to lead the DBT. He says that he wants to inject rigour into Indian science and train scientists to work together on tractable problems. As grand visions go, his can seem muted, almost modest. \u201cI'm not going to be stupid and try something completely nutty; I'm going to try something within my grasp,\u201d he says. Researchers are optimistic about what he might be able to achieve. \u201cIt's very rare to have a scientist of Vijay's calibre heading a government department,\u201d says Jyotsna Dhawan, a stem-cell biologist who worked with VijayRaghavan for seven years. \u201cSo I think all of us in the scientific community have very high hopes.\u201d But they also recognize the challenges, which include wrangling with New Delhi's murky politics \u2014 known for ensnaring plans in red tape \u2014 and the DBT's long, painful grant-review process. In the past couple of years, the Ministry of Finance has made it difficult for the agency to honour even approved grants. And although the DBT is a major funder of extramural research, the money that it actually gets each year \u2014 a little more than 14 billion rupees (about US$225 million) \u2014 is a fraction of that commanded by analogous agencies elsewhere, such as the US National Institutes of Health. Given the challenges, even the most ardent well-wishers are holding their applause. \u201cIt's not entirely apparent to me what an individual, even one so dynamic and forward-looking as VijayRaghavan, can do to cut through the red tape,\u201d says Dhawan. \n               A passion to learn \n             A self-described \u201cair-force brat\u201d, VijayRaghavan grew up all over India, moving every few years. He was hungry for knowledge, and, as a teenager, used to cycle to his local branch of the British Council or the US Information Service \u2014 the main sources of foreign publications in those days \u2014 and read every book and magazine that he could find. \u201cIn the pre-Internet days, that was my food,\u201d he says. After studying chemical engineering at the Indian Institute of Technology Kanpur, VijayRaghavan was preparing to leave for a bioengineering PhD programme in Switzerland when he chanced on an article by renowned molecular biologist Obaid Siddiqi on using genetics to understand the nervous system. It was a departure from the work that VijayRaghavan had originally planned to do, but, he says, \u201cI found the formalism of genetics easy to grasp, and that excited me very much.\u201d He sought out Siddiqi at the Tata Institute of Fundamental Research (TIFR) in Mumbai, where he began a PhD programme. It was, at the time, a place that afforded considerable freedom to its students. \u201cYou did what you pleased and you joined whomever you wanted to for your research,\u201d VijayRaghavan says. \u201cIt was an exhilarating experience.\u201d But there was a growing air of complacency and nepotism at the TIFR that frustrated Siddiqi. For years, he had been planning to build a new institute, and he saw a natural ally in VijayRaghavan. The pair began to hatch plans, even as VijayRaghavan embarked on further training at the Medical Research Council Laboratory of Molecular Biology in Cambridge, UK, and later, undertook a postdoctoral fellowship at the California Institute of Technology (Caltech) in Pasadena. Elliot Meyerowitz, VijayRaghavan's adviser at Caltech, says that lab members routinely tried to flummox foreign postdocs with US slang and customs, but they could never rattle VijayRaghavan. \u201cI don't know whether he understood, or if he was just so cool, we didn't know he didn't understand,\u201d Meyerowitz says. VijayRaghavan says that he did understand, thanks to his time devouring British and US magazines. \n               From the ground up \n             In 1988, VijayRaghavan left Caltech and returned to India to head a lab at the TIFR, and he, Siddiqi and a handful of other scientists laid groundwork for the research centre in Bangalore. They wanted the institute, which would be named the NCBS, to be different from any in India before it. Siddiqi became founding director, but VijayRaghavan and a few others were closely involved in its development. \u201cWe were in the trenches together \u2014 young, some very talented, all very driven,\u201d VijayRaghavan says. \u201cWe had a sense of rebellion.\u201d From the start, VijayRagahvan wanted to recruit people trained in multiple disciplines who were focused on cutting-edge techniques, such as single-cell analysis, says statistician Partha Majumder. \u201cThis trait of being able to look way into the future is what sets him apart.\u201d In 1991, VijayRaghavan moved to Bangalore to launch the NCBS's first lab. Over the next year, two more faculty members joined him. The entire centre was a \u201cshack\u201d, he recalls, situated on one floor of the radio-astronomy building at the Indian Institute of Science. VijayRaghavan had to cycle 1.5 kilometres to the nearest biology lab to photograph his DNA gels. \u201cWe had an absolute ball of a time,\u201d he says. Along with building the institute, VijayRaghavan was strengthening his scientific reputation. He borrowed equipment to set up a series of elegant genetic experiments that would enable him to write several high-profile papers defining specific events in  Drosophila  muscle development. Other faculty members, such as Gaiti Hasan, M. K. Mathew and Jayant Udgaonkar, were publishing groundbreaking papers in cell signalling and protein folding, which in turn helped to entice other scientists to join the NCBS. \u201cWe made extraordinarily rash promises that we would do everything for them, which we did,\u201d VijayRaghavan says. In 1993, for example, VijayRaghavan learned that Satyajit Mayor, a cell biologist in New York City whom he was trying to recruit, needed a pricey Zeiss inverted microscope. VijayRaghavan had been promised some equipment for his own lab through a grant from the Rockefeller Foundation in New York City, but stringent rules from the TIFR and the Indian government had held up the procurement for years. He changed his request to get the microscope instead. All that Mayor knows of the negotiation is that he sent an e-mail to VijayRaghavan one night telling him that he would not be able to join the institute without that type of microscope. He woke up the next morning to VijayRaghavan's reply: \u201cIt'll be here when you arrive.\u201d Mayor joined the NCBS about 18 months later. \n               United by science \n             VijayRaghavan took over from Siddiqi as director of the NCBS in 1996. As the institute grew, he strove to build a democratic system, in which even graduate students had a say, and criticism was not just accepted, but expected. Before moving ahead with any new plans, he always made sure that he \u201cbrought people along with him\u201d, says Mayor, now the institute's director. In 1999, for example, the leaders of the institute were considering adding a master's programme in wildlife ecology and conservation. At first, only 3 or 4 of its then 22 faculty members were in favour of the idea. At a meeting, VijayRaghavan carefully listened to the pros and cons, and was ultimately able to convince everyone, recalls Mayor. \u201cEverybody left that meeting feeling like we'd done the right thing,\u201d he says. \u201cThe way the discussion and the dialogue and the arguments were put across, quite masterfully, was so Vijay.\u201d The programme has become one of the institute's most successful, with eight field stations across the country and faculty members drawn from the United States and Germany. By the time VijayRaghavan left, the NCBS was widely regarded as one of India's leading research organizations. But the NCBS, and a few other select institutes, are exceptions in India. Much of the country's science is beset with the same problems it has had for decades \u2014 interminable waits for reagents, or a granting scheme that places a 3-year limit on funding, forcing researchers to write new applications in unrealistic cycles. VijayRaghavan's predecessor at the DBT, Maharaj Kishan Bhan, had done much to modernize Indian research and make it more independent, including helping to develop a low-cost rotavirus vaccine and setting up an organization to support entrepreneurs. \u201cIf Bhan was not able to succeed in some places,\u201d says Verma, it was because of limited resources and not having enough people to support his vision. \u201cPerhaps he bit off more than he could chew \u2014 the same could happen to Vijay.\u201d \n               Back to basics \n             Since his arrival at the DBT, VijayRaghavan has unveiled a few plans. Financially, Indian science is no match for that in the United States, Europe or China, something that he freely acknowledges. But he says that India can make big gains by capitalizing on its advantages and by collaborating with others. His main priorities are to invest in basic research areas \u2014 such as computational biology, in which India is already strong \u2014 to break down the barriers between disciplines and to improve training for all scientists. \u201cI think he's got a very, very strong vision about the importance of fundamental and basic science,\u201d says Eve Marder, a neuroscientist at Brandeis University in Waltham, Massachusetts, who serves alongside VijayRaghavan on the scientific advisory board for the Janelia Research Campus in Ashburn, Virginia. The DBT's marine-biology initiative exemplifies this vision. The effort, intended to chart biodiversity and identify compounds for biotechnology development, is the DBT's \u2014 and VijayRaghavan's \u2014 brainchild. But it involves the Indian Space Research Organisation in Bangalore and the Ministry of Earth Sciences in New Delhi, both first-time partners for the DBT. In addition, the French National Centre for Scientific Research and the Pierre and Marie Curie University in Paris will help to train Indian researchers. The project is part of VijayRaghavan's strategy to compensate for the DBT's limited budget by partnering with every ministry that has funds allotted for science and technology, including the sanitation, maternal-health and nutrition ministries. In the next year, he plans to roll out an Indian body modelled after the European Molecular Biology Organization \u2014 a scientific society that would promote India as a hub for international collaborations and offer online training for scientists at all levels. He is also encouraging local collaboration on training and research. In the Delhi area, for example, he plans to persuade the leaders of well-established immunology, mathematics, engineering and medical institutes to work together. \u201cBoom!\u201d he says. \u201cWithin a few years, you're going to have extraordinary-quality people being trained, both engineers and clinicians\u201d. In the short term, he wants to play to India's strengths. Getting India's thriving community of mathematicians and computer scientists to work on problems in biology, for example, could help the country to gain an edge in bioinformatics and quantitative biology \u2014 fields that do not typically require as much funding as bench biology. This is all easier said than done,VijayRaghavan admits, but he intends to use financial incentives and disincentives \u2014 what he calls \u201cfire in the belly\u201d and \u201cfire in the rear\u201d \u2014 to make it happen. In the past, the DBT has set out strict budgetary allocations at the beginning of each year, and had little flexibility in later months. But last year, VijayRaghavan set aside a pot of about $33 million from the DBT's annual budget. The money can be used to respond to innovative ideas, reviewed by international experts. This is sure to aggravate some institutes that are used to being well-funded, but VijayRaghavan is unmoved. \u201cIt's about time that we recognize excellence and recognize shoddiness,\u201d he says. VijayRaghavan is also tackling the grant-review process. He is streamlining the DBT's online application system, he has created timelines for submitting grants and has introduced the DBT's first open-access and conflict-of-interest policies. \u201cWhen people complain about problems in India, it's rather astounding how few of us are actually doing something about them,\u201d he says. \u201cIf you actually start doing something about anything, the situation changes.\u201d But some colleagues have concerns. Mayor says that VijayRaghavan's desire for consensus and harmony could prove a weakness. \u201cHe is so keen to be extremely positive about everyone,\u201d Mayor says. \u201cWhen you're operating in the real world where you have to get things done, that, I would say, is a bit of a problem.\u201d And in a government department, VijayRaghavan will not be able to hand-pick people who share his vision, as he did at the NCBS. \u201cI'm a little concerned that if he doesn't have that, he will burn out, because he will try to do it all himself,\u201d Mayor says. VijayRaghavan shows no sign of burnout yet. He still maintains a lab, albeit a lean one, conferring with his team in the evenings through Skype and returning to Bangalore every weekend. Besides the tweets and Facebook posts, he blogs and makes time to run several times a week \u2014 and, he says, he is having fun doing it all. \u201cI have to tell you one simple rule in any job. If you wake up for several days in a row and say, 'Why am I doing this?' then you're better off quitting,\u201d he says. \u201cNot only has that not happened, I'm actually quite excited when I wake up every day. I just look at the day and hit it hard.\u201d \n                     India\u2019s budget disappoints scientists 2015-Mar-02 \n                   \n                     India's PhD students on hunger strike over delayed pay rise 2015-Feb-24 \n                   \n                     Policy: Free Indian science 2014-Apr-02 \n                   \n                     Indian science in need of overhaul 2012-Jan-06 \n                   \n                     Nature  special: Science in India \n                   \n                     \n                         Nature India \n                       \n                   \n                     Department of Biotechnology \n                   \n                     National Centre for Biological Sciences \n                   Reprints and Permissions"},
{"file_id": "521142a", "url": "https://www.nature.com/articles/521142a", "year": 2015, "authors": [{"name": "Richard Van Noorden"}], "parsed_as_year": "2006_or_before", "body": "Highs and lows in the country\u2019s research landscape. Indian science is a study in contrasts. With its vast population and rapidly expanding economy, the country has ramped up scientific production at an impressive rate. India started the twenty-first century well behind Russia, France, Italy and Canada in terms of yearly publications and it now leads them all by healthy margins. It is quickly closing in on Japan. Despite those gains, India is not yet a major player in world science. Its publications generate fewer citations on average than do those of other science-focused nations, including other emerging countries such as Brazil and China. Relative to its size, India has very few scientists; many Indian-born researchers leave for positions abroad and very few foreign scientists settle in India. The country invests a scant portion of its economy in research and development (R&D), and it produces relatively few patents per capita compared with other nations. But there are bright spots. India boasts several world-class centres for science education, particularly the highly regarded Indian Institutes of Technology. Businesses in the country are investing more in R&D, which bodes well for future innovation. And more women are participating in science, although their numbers still fall far below those of men. \n               Top ten institutions \n               \n                     Indian bioscience: The anti-bureaucrat 2015-May-13 \n                   \n                     Science in India 2015-May-13 \n                   \n                     Policy: Rethink India's energy strategy 2015-May-13 \n                   \n                     India: The fight to become a science superpower 2015-May-13 \n                   \n                     Research management: Priorities for science in India 2015-May-13 \n                   \n                     A nation with ambition 2015-May-13 \n                   \n                     India\u2019s budget disappoints scientists 2015-Mar-02 \n                   \n                     Policy: Free Indian science 2014-Apr-02 \n                   \n                     Nature special: Science in India \n                   \n                     Department of Science & Technology \n                   Reprints and Permissions"},
{"file_id": "521020a", "url": "https://www.nature.com/articles/521020a", "year": 2015, "authors": [{"name": "Megan Scudellari"}], "parsed_as_year": "2006_or_before", "body": "When and how to exit research has become a charged issue in science. Hans-Hilger Ropers vividly recalls the moment when he realized that he was running out of time to fulfil his scientific ambitions. It was at a lecture series in 2002, and the first three talks, delivered by distinguished colleagues from Max Planck Institutes (MPIs) across Germany, each began with a variation of, \u201cLadies and gentlemen, what I'm going to talk about today has kept me busy for 20 years.\u201d Ropers did not have 20 years. He had become director of the MPI Department of Human Molecular Genetics in Berlin when he was 51. He was fast approaching 60, and under German law he would be required to retire in 5 years. Ropers knew that he would not have enough time to complete his work on the genetic underpinnings of early-onset cognitive disorders. He also knew that when he retired, the institute would dissolve his 70-member department, so his employees would be out of jobs. Peter Lawrence and Tom Sanders discuss scientific life after 65 Ropers is hardly the only person to worry about retirement, which has become a charged issue in the sciences. The proportion of US National Institutes of Health (NIH) grants awarded to people over the age of 65 more than doubled between 1998 and 2014, going from 4.8% of the grant budget to 12%. Out of concern that this greying scientific workforce is limiting the availability of grants and jobs for young scientists, the NIH this year proposed a solution: an 'emeritus' award to encourage senior scientists to wind down their research and hand projects over to junior faculty members. It was met with overwhelming disapproval. Scientists argued that the proposal placed too much focus on age, and risked awarding grants for factors other than the merits of the research. \u201cWhile I like the idea of transferring knowledge and resources, if you can't do so efficiently inside a typical 40-year career span, then why should you be given longer?\u201d says Paul Brookes, 42, a biologist at the University of Rochester Medical Center in New York. \u201cNobody can argue that emeritus professors were somehow caught unaware that they'd have to retire some day.\u201d To explore the rules, culture and expectations of retirement,  Nature  contacted academics in the middle of making their exit. The issues faced by older scientists can vary dramatically by country, and there is no right way to retire, says Tania Saba, associate dean of graduate studies at the University of Montreal in Canada, who has studied retirement trends since 1993. \u201cSome want to keep working; others would retire the next morning if they could.\u201d \n               Fighting the clock \n             After Ropers had swallowed his panic, he crafted a plan. He rushed to obtain a 3-year extension so he could work until he was 68 (a provision now available to most academics in Germany), and later a second, unusual extension to work to 71. But even so, uncertainty about his future affected his department. \u201cAll your co-workers realize that this guy is turning 65 in a couple years, so they start to look for alternatives.\u201d Over his final 6 years, Ropers watched his department shrink from 70 people to 25, then to 10. If he had known from age 60 that he would be able to work for 11 more years, Ropers would have sought appropriate grants, and would have been able to retain faculty members and plan his retirement better. \u201cIt would have made a world of difference,\u201d he says. Mandatory retirement for public employees used to be the norm across Europe, and despite some changes, it maintains a foothold in countries including Germany, Sweden and Spain. Legal challenges calling the practice discriminatory have mostly been unsuccessful. In 2007, the top European court ruled that European Union (EU) countries can force workers to retire as a way to free up job places and reduce unemployment for younger workers. But there is no empirical evidence that such an approach works. In fact, studies suggest that higher employment for older people is often correlated with higher employment and higher wages for younger workers. Some people facing mandatory retirement choose to leave the country. That was the reason immunologist Klaus Rajewsky left the University of Cologne in Germany in 2001 to set up a lab in the United States. \u201cIt's not so easy, particularly for experimental scientists to start a new lab at that stage,\u201d he says. \u201cSo most people retire, or try to get some prolongation where they are.\u201d Ropers' department officially closed in November 2014. His last EU grant will expire shortly, and the remaining lab space is now being refurbished for the director of a new department. \u201cAfter that, I will still have a chair and a room,\u201d says Ropers. \u201cThat will be it, though.\u201d He still plans to publish what he calls the biggest paper of his career, with four years' worth of data on the genetics of mental disabilities. He has also accepted a part-time, paid job as a clinical geneticist at the Institute of Human Genetics at the University of Mainz. Although still concerned about the Max Planck Society's decision to close its only department for human genetics, he says that it is time to move on to other things, including an around-the-world trip with his wife. \u201cThe sun is shining,\u201d Ropers says. \u201cLife could be worse.\u201d \n               Changing course \n             Japanese stem-cell biologist Norio Nakatsuji turned 65 this year, and just like in Germany, that means mandatory retirement. As his final grant runs out over the next 2 years, his 2 labs at Kyoto University will close and most of his 16 laboratory members must seek employment elsewhere. Although he is leaving academia, Nakatsuji has big plans in business. As former director of the Institute for Frontier Medical Sciences at Kyoto University, he has spent a career deriving and distributing stem cells. During that time, he co-founded a biotechnology company, ReproCELL in Yokohama, which made a splashy initial public offering in Japan in 2013. Using US$2 million of his earnings, Nakatsuji founded two stem-cell-related companies: a consulting firm that bridges academia and industry, and a biotech company that is developing drug-testing devices based on heart muscle derived from stem cells. \u201cThey were unexpected funds, so I can put my money towards being an entrepreneur,\u201d says Nakatsuji. \u201cThat would be impossible for an ordinary professor.\u201d His move into business is unusual in Japan. It is much more common for a retired academic to take a position at a private university. But this did not appeal to Nakatsuji. Such jobs, he says, \u201cshould be made open to young people\u201d. Japanese government policies have increased the number of PhD graduates by an estimated 6.2% per year since the 1990s. But the number of academic positions available to those graduates has remained stagnant. The pipeline narrows as soon as they graduate: in 2014 the Japan Society for the Promotion of Science, the country's largest scientific funding agency, granted only 362 out of 3,222 applications for postdoctoral fellowships across all fields. Most PhD graduates who do not get fellowships take on short-term jobs at universities or public research institutions. Making space for young scientists should be a priority in Japan, but it is a great challenge, says Osamu Terasaki, a physicist who worked in academia there for 36 years before taking on a professorship at Stockholm University, a position he retired from in 2010, at age 67. \u201cIn Japan, senior investigators keep so much power, and for a long time,\u201d says Terasaki. \u201cYoung people should take over. It would make the situation more healthy.\u201d \n               Keeping busy \n             The top task on Uta Frith's post-retirement to-do list is to publish a graphic novel. Frith, a developmental psychologist, retired from University College London in 2006, at the mandated age of 65. Although she could have fought to get another grant and keep working, she opted instead to leave the lab. \u201cI was so happy that I wouldn't have to apply for any grants ever again.\u201d Frith gave up hands-on research but has taken advantage of the time to write up some of her previous work: she has published 33 papers since 2007. Now 73, she calls her retirement the \u201cbest years of my life\u201d. They are also some of the busiest. She has founded two networks for women in science and technology, called Science&Shopping and UCL Women; she collaborated with the BBC on a television documentary about autism and is working on another about obsessive\u2013compulsive disorder; she chairs the diversity programme of the Royal Society; and she tweets regularly, to more than 15,000 followers. Frith also travels with her husband, psychologist Chris Frith, to Aarhus University in Denmark for a month once a year, to advise students and teach. \u201cI only do things I really feel passionate about. That's the beautiful luxury of retirement,\u201d she says. Frith's current passion is her graphic novel. She and her husband won the French National Centre for Scientific Research's 2014 Jean Nicod Prize for cognitive science and philosophy, and promptly spent the \u20ac25,000 (US$28,000) award on paying an editor and artists to help them develop the novel, which is about social cognition: the study of how the human brain processes and uses information about other people. The Friths hope that their book \u2014 which they will release in instalments starting as early as this year \u2014 will help them to share their research with the public. \u201cWe could never do this if we had to pursue paid work,\u201d says Frith. \u201cRetirement is a time you can really have complete liberation from responsibilities and duties, and devote time to things you might otherwise never have done.\u201d Although mandatory retirement was phased out in the United Kingdom in 2011, its effects can still be felt, says Peter Lawrence, 73, a developmental biologist at the University of Cambridge and an outspoken opponent of age discrimination. \u201cMany regard people above retirement age as odd to still be working. You feel as if you're not entitled to work.\u201d A 2010 study from the UK Department for Work and Pensions found that, despite the lack of evidence, employers still believe that productivity declines with old age 1 . Many still hew to the traditional retirement age; the average age of retirement in the United Kingdom today is 64.7 for men and 63.1 for women, although it has been increasing. \n               Following passions \n             In the United States, working past traditional retirement age seems to be becoming the rule rather than the exception. Mandatory retirement was phased out of US law starting in the late 1970s, and was ultimately abolished in 1986, although academic institutions had until 1994 to comply. This has resulted in ample opportunity to observe what happens in the absence of a forceful shove out of the door: people work for longer (see 'Retired yet?'). According to the National Science Foundation, the proportion of working scientists and engineers in the United States over the age of 50 has increased from 1 in 5 in 1993 to 1 in 3 in 2010 (ref.  2 ). \u201cAs long as somebody is contributing substantially \u2014 teaching, administratively, research-wise \u2014 why force them to retire?\u201d asks John Dowling, a neuroscientist at Harvard University in Cambridge, Massachusetts, who will formally retire this June, at the age of 79. Although financial security may play a part, some certainly stay on for the love of science: 54% of employed people aged 65 or over say that they are working because they want to, not because they have to, according to the Pew Research Center 3 . But what if they no longer want to? Physicist Mark Adams voluntarily retired from the University of Illinois at Chicago at the age of 59, to take advantage of state pension rules. He feared that changes to the poorly funded system, which had been attacked by politicians, would leave him worse off. If it had not been for that, he would probably have worked for another four years, he says. Adams also knew that his work would not languish when he retired. \u201cI have thousands of colleagues,\u201d he says, including international collaborators working on results from the now-decomissioned Tevatron proton\u2013antiproton collider at Fermilab in Batavia, Illinois, and the Large Hadron Collider (LHC) at CERN, Europe's particle-physics lab near Geneva, Switzerland. The field, he says \u201chas almost a corporate structure to keep experiments going\u201d. Two years after his official retirement, Adams still uses his office at the university, where he is completing the analysis of data from one of the particle detectors at the LHC. But he no longer receives a pay cheque, and he recently transferred his last student to a colleague. He plans to ease out entirely soon, to devote himself full-time to QuarkNet, an educational programme that he joined 14 years ago to bring high-energy physics experiments to schools in the Chicago area. \u201cI feel like that's a real volunteer thing I can do with my skill set, something that makes a difference. That's where I want to put my time.\u201d \n               Letting go \n             H\u00e9l\u00e8ne Delisle also wanted to accomplish things outside academia. She had been a professor of nutrition at the University of Montreal for 29 years. But in January 2014, when she turned 70, she informed her department director that she would be retiring towards the end of the year. Like the United States, Canada no longer enforces retirement: mandatory retirement was abolished province by province between 1973 and 2009. So Delisle was free to work for as long as she desired, and eventually that desire waned. Her husband had fully retired from medical practice by 2013, and it became apparent that they would have a better lifestyle together if they were both retired, pursuing new endeavours together rather than trying to plan around her career. Delisle remained fully active right up to her official retirement in September, mainly finishing reports for a six-year nutrition project in West Africa. Since then, she has been transferring the leadership of a World Health Organization Collaborating Centre that she headed to a colleague, has served on several scientific committees, and has joined the editorial board of a nutrition journal. She visits her office at the university weekly to see students who are wrapping up their graduate work, and those regular visits have made the transition smoother. \u201cIn a way, retirement is a separation, a severance, because you used to go to work every day and have plenty of activity and hectic business travel,\u201d she says. \u201cYou've got to let that go, and that can be difficult.\u201d Most of all, Delisle is finally making space for other pursuits, foremost among them social activism, and spending more time with her husband and her piano. Delisle has played in a chamber-music trio for years, and looks forward to devoting more time to concerts. Although there are few universal lessons to be drawn about how to wind down one's career, most researchers abhor the mandatory system. Delisle and Adams say that it was important to make their own decisions about retirement on the basis of their desire and productivity, and not have the decisions made for them because of their age. \u201cThe ideal would be that no one checks how old you are but just looks at what you're doing and what you are able to do,\u201d says Ropers. \u201cBut that idea hasn't pervaded into public routine.\u201d In fact, pushing senior researchers out may be doing more harm than good. \u201cThere is no evidence that shows that early retirement can reduce unemployment, particularly youth employment,\u201d says Saba. Most studies show the opposite: an analysis from the nonprofit US National Bureau of Economic Research in Cambridge, Massachusetts, for example, found that paying senior individuals to retire actually increases the unemployment rate for younger workers, because older people remain productive and spur the creation of new jobs 4 . Research suggests that there is not a fixed number of jobs in the economy, so the idea of one-to-one replacement is false. For example, women did not replace men in the workforce in the late twentieth century; on the contrary, two-earner families increased disposable income and prompted job creation. But how this dynamic might differ in a academia, which is dominated by tenure and the pressures of limited funding, has not been fully analysed. Still, most senior scientists interviewed for this article emphasized concern for those in the next generation and felt they should be making room for them. \u201cIt's important to give young people the opportunity to do research,\u201d says Dowling. But no matter how or why you go about it, do not think of retirement as an end, says Frith. \u201cStopping doesn't mean stopping. It means do what you've always wanted to do.\u201d \n                     The idea factory 2015-Feb-18 \n                   \n                     Young scientists lead the way on fresh ideas 2015-Feb-18 \n                   \n                     NIH plan to give ageing scientists cash draws scepticism 2015-Feb-11 \n                   \n                     German science benefits from pensioner power 2010-Dec-23 \n                   \n                     Retiring retirement 2008-May-28 \n                   Reprints and Permissions"},
{"file_id": "517136a", "url": "https://www.nature.com/articles/517136a", "year": 2015, "authors": [{"name": "Kat Austen"}], "parsed_as_year": "2006_or_before", "body": "Step aside, fitness trackers. The next wave of personal sensors is giving people the ability to monitor the air they breathe. When winter descends on Ulaanbaatar, Mongolia, the air turns foul. Here in the world's coldest capital city, residents light open fires of coal or wood to heat their uninsulated houses. Soot fills the skies, and people don face masks to ward off smog so thick it can hide buildings a few hundred metres away. \u201cWhite clothing becomes grey after a few hours,\u201d says Munkhmandakh Myagmar, executive director of the Press Institute of Mongolia. The city is one of the most polluted in the world, according to the World Health Organization (WHO) \u2014 and estimates suggest that particulate air pollution causes one-tenth of the city's deaths. But information about the extent of that pollution is limited and hard to find. The WHO's online database for pollution has readings from just one year for Ulaanbaatar, showing concentrations of harmful particles called PM 10 , which have a diameter of 10 micrometres or less, and PM 2.5 , with a diameter of 2.5 micrometres or less. Eager to fill the knowledge gap, journalists from the Press Institute are taking matters into their own hands. In collaboration with the Earth Journalism Network \u2014 an international group of environmental journalists \u2014 Myagmar and her colleagues distributed five devices, each about the size of a child's lunch box, around the city in July. Called DustDuinos, the devices measure particulate-matter concentrations and quickly upload the data to a public website. Despite some initial problems with charging and connectivity, preliminary results from a sensor in the city's centre showed that concentrations of PM 10  often surged to at least twice the WHO's recommended limit. The DustDuino and other pollution sensors, some of which can be built for as little as US$50, and instructions for which are available online, are part of the next wave in the environmental movement (see 'Sensors for the people'). Across the globe, journalists, advocacy groups, hackers and others are starting to use low-cost monitoring devices to vastly expand the amount of data that are publicly available on forms of air pollution such as particulate matter and toxic gases. The devices are easy to deploy and can complement data from official networks, which rely on sophisticated but sparsely distributed sensors. The 'citizen-science' approach aims to provide high-resolution measurements of air pollution where people actually live. Work is also under way to develop wearable sensors to monitor personal exposure levels. Built on the principle of openness, such do-it-yourself (DIY) efforts are part of a push to democratize air-quality monitoring so that it no longer remains solely in the domain of governments and academic researchers. But advocates of the approach still have to convince conventional pollution researchers, who worry about the quality and usability of data from cheap sensors operated by relatively untrained people. Still, everyone agrees that more resources need to go into monitoring air pollution, which kills around 7 million people a year. \u201cIt's the largest, single most important, environmental health risk in the world,\u201d says Joshua Apte, an environmental researcher at the University of Texas at Austin, who sees an emerging role for cheap, plentiful pollution gauges. \u201cThe fact that you can buy 50 low-cost sensors for the cost of one regulatory sensor is a tremendously powerful thing.\u201d \n               Pollution pigeons \n             The roots of the movement go back to 2006, when Beatriz da Costa, an artist at the University of California, Irvine, strapped a small bundle of sensors onto homing pigeons. Da Costa had worked with engineers to develop the instrument package, which measured carbon monoxide and nitrogen oxides and tracked the pigeons' movements using a Global Positioning System (GPS) receiver. She published the data from her project, called PigeonBlog, with the aim of disrupting the status quo and giving the public a role in gathering data on pollution. Around the same time, sensors for other uses were starting to emerge. More than two-thirds of US adults now say they use technology to track their heart rate and other health data; gadgets for the home sense water and electricity usage by the minute, and cities are employing sensors to track everything from pedestrian traffic to leaky pipes. Options for grass-roots pollution monitoring were scarce until a few years ago: conventional air-quality sensors are expensive or require training, and the data they provide are often inaccessible or hard to work with and share. That began to change when 'makers' \u2014 tinkerers who work in backyard sheds and collective hackspaces \u2014 started soldering together circuit boards and sensors to take on the challenge. In 2011, a group of hackers, makers and artists who called themselves the Sensemakers gathered at meet-ups in Amsterdam and New York. Governments were not monitoring pollution at the local level, where it affects people, they wrote on their blog. The Sensemakers launched an online call for a community-developed sensor that would measure air pollution. With more than $144,000 raised through the crowdfunding platform Kickstarter, the Sensemakers developed the Air Quality Egg, a device to measure temperature, humidity, carbon dioxide and nitrogen dioxide. The sensor costs $185 \u2014 less than one-tenth the price of a mid-range device. Similar efforts were emerging in Spain at around the same time. Tomas D\u00edez Ladera, director of Fab Lab Barcelona at the Institute for Advanced Architecture of Catalonia, had dreams of citizens being able to monitor the air they breathe in real time and stream the data so that others in their community could benefit. After spending a year looking at what technology was available, D\u00edez Ladera decided that he and his team would have to realize this dream themselves. Their early prototypes grew into a package of sensors called the Smart Citizen Kit (SCK) \u2014 which measures everything the Air Quality Egg does, as well as light intensity and noise. The group launched a website to encourage DIY-ers to build or buy the device and monitor local pollution. Because open-source sensors such as these can be made anywhere, it is difficult to track how many have been produced. But according to their inventors, at least 35 DustDuinos, some 2,500 Air Quality Eggs and 1,000 SCKs have been deployed. Many of the data produced by these devices are openly available through online platforms such as Xively. These efforts have already captured the attention of city officials and citizen groups. In early 2014, officials in Amsterdam provided 100 citizens with SCKs and instructions on how to use them in their neighbourhoods. In May, a community effort coordinated by FutureEverything, an innovation lab in Manchester, UK, created a network of sensors around that city's centre. The aim is to test how such data can enhance urban living by showing, for example, how efforts to encourage bicycling affect air quality. \n               Data divide \n             All this enthusiasm by proponents has not yet won over environmental researchers. The sensors currently used in the SCK and the Air Quality Egg are not up to the task of providing robust pollution data, says Ben Barratt, an air-quality scientist at King's College London, who helps to run the London Air Quality Network, an online resource for pollution data. \u201cMonitoring air-pollution levels is far more involved than the manufacturers and suppliers of cheap sensors suggest,\u201d Barratt says. The problem is that temperature, humidity and some gases skew the results from sensors such as those used in the Air Quality Egg and SCK, making it difficult to compare results between devices, he says. DIY endeavours also lack input from pollution experts, says Tim Chatterton, a pollution-policy researcher at the University of the West of England in Bristol, UK, who has worked with the UK government to monitor air quality. Professional technicians and scientists pay close attention to siting and maintaining sensors, he says. \u201cWithout due attention to these things, the data is essentially meaningless because it's not comparable.\u201d These problems reduce the usefulness of such data in environmental advocacy, says Kirk Smith, an environmental health researcher at the University of California, Berkeley. The US Environmental Protection Agency (EPA) has strict rules about what holds up in court, he says, \u201cand these monitors don't meet their criteria \u2014 yet\u201d. The data are also often streamed and stored in ways that make them difficult to analyse and visualize to tell a coherent story, adds Smith, whose group has created particulate-matter sensors for use indoors. \u201cOne thing we learned in developing our smart, cheap monitors is that they are producing a lot of data and it's not clean. To get something useful out of the end takes a lot of work,\u201d he says. Given such concerns, the keepers of scientific data sets have not yet embraced the information produced by most DIY sensors. Barratt, for example, says he will not include data from citizen-sensing projects in the London Air Quality Network until the quality improves. Matthew Schroyer, who developed the DustDuino, says that its data are comparable to those generated by more expensive sensors when averaged over a sufficient time span. An independent team of researchers reported that the sensor used in the DustDuino performs as well as a higher-cost sensor when sampling air quality over windows of 1 hour (D. M. Holstius  et al .  Atmos. Meas. Tech. Discuss.   7 , 605\u2013632, 2014 ). Schroyer, a technology developer and communications specialist in Champaign, Illinois, acknowledges that the DustDuino is not accurate enough for instantaneous data gathering, because its measurements contain too much noise. The developers of the Air Quality Egg and the SCK also recognize that the sensors face teething problems, but they say that they are addressing them. D\u00edez Ladera says that the next release of the SCK, expected in early 2015, will be precalibrated and have better sensors, which should make its data reliable enough for comparison with official air-quality standards. Apte's previous work has shown just how important it is to understand how pollution levels vary at the human scale. Last year he spent four months driving through New Delhi's frenetic roads on a rickshaw kitted out with a mid-range sensor called a DustTrak, which can deliver real-time feedback about pollution. Apte wanted to understand how pollution levels change as people go about their daily business \u2014 and he discovered large variations. In traffic, for example, a passing truck spewing smoke can cause concentrations of PM 2.5  to jump by as much as 50%. During those rides, Apte found even greater fluctuation in the levels of harmful ultrafine particles, which are smaller than PM 2.5 , and of pure carbon specks. \u201cWe're interested in deploying networks of air-pollution monitors all around cities,\u201d says Apte. \u201cYou couldn't do this if you were paying $10,000 per sensor.\u201d Other researchers are engaging with the citizen-sensing movement. Despite his concerns over the data quality, Barratt is advising projects such as the London Sustainability Exchange, which is working with communities to measure air pollution in the city. The EPA is also starting to embrace the citizen-monitoring concept. Tim Watkins, acting deputy director of the EPA's National Exposure Research Laboratory, wants to explore how cheaper, less accurate sensors can provide data that will complement the sparsely spaced, top-of-the-range kits. \u201cThis new tech is potentially very valuable. And it's coming, whether or not we are investing or using it,\u201d he says. In 2013 the EPA announced the winners of a competition for developers to create low-cost wearable sensors that integrate air-quality measurements with health data. And in the next few months the agency will announce the winners of a $4.5-million competition to fund research on community use of low-cost sensors to measure air quality. As budget cuts cause governments to trim expensive sensor networks, citizen sensing will develop to fill the gap, says Schroyer. He foresees a day when people's clothes will measure their exposure to carcinogens, their phones will sniff for polluting particles and drones will hover over cities, searching for natural-gas leaks. \u201cMobile, fabrics, health monitors \u2014 these are all quite possible,\u201d he says. \u201cThere are hackers the world over working on these technologies right now.\u201d \n                     Air pollution: Clean up our skies 2014-Nov-19 \n                   \n                     Global health: Deadly dinners 2014-May-28 \n                   \n                     Development: Mobilize citizens to track sustainability 2014-Mar-30 \n                   \n                     Scientists ask public to hunt for power plants 2013-May-13 \n                   \n                     Citizen scientists' climate-impact survey wraps up 2011-Dec-29 \n                   \n                     Personal technology: Phoning in data 2009-Apr-22 \n                   \n                     Smart Citizen \n                   \n                     Air Quality Egg \n                   \n                     DustDuino \n                   Reprints and Permissions"},
{"file_id": "517262a", "url": "https://www.nature.com/articles/517262a", "year": 2015, "authors": [{"name": "Ed Yong"}], "parsed_as_year": "2006_or_before", "body": "Margaret McFall-Ngai has dissected the relationship between a beautiful squid and its live-in bacteria \u2014 and found lessons for microbiome research on the way. The aquarium looks empty, but there is something in it. A pair of eyes stick out from the sandy floor, and their owner is easily scooped up into a glass bowl. At first, the creature looks like a hazelnut truffle \u2014 small, round and covered in tiny flecks. But with a gentle shake, the flecks of sand fall off to reveal a female Hawaiian bobtail squid ( Euprymna scolopes ), about the size of a thumb. As she jets furiously around the bowl, discs of pigment bloom and fade over her skin like a living pointillist painting. There are no other animals in the bowl, but the squid is not alone. Its undersides contain a two-chambered light organ that is full of glowing bacteria called  Vibrio fischeri . In the wild, their luminescence is thought to match the moonlight welling down from above and cancel out the squid's shadow, hiding the animal from predators. From below, the squid is invisible. From above, it is adorable. \u201cThey're just so beautiful,\u201d says Margaret McFall-Ngai, a zoologist at the University of Wisconsin\u2013Madison. \u201cThey're phenomenal lab animals.\u201d Few things excite McFall-Ngai more than the partnership between the bobtail squid and  V. fischeri  \u2014 and that is after studying it for more than 26 years. Over that time, she has shown that this symbiotic relationship is more intimate than anyone had imagined. She has found that the bacterium out-competes other microbes to establish an entirely faithful relationship with one host. It interacts with the squid's immune system, guides its body clock and shapes its early development by transforming its body. Some of these discoveries have helped to shape her field. When McFall-Ngai started her career in 1978, microbiologists were focused almost entirely on pathogens and disease. But in the past decade, advances in genetic sequencing have allowed scientists to identify the trillions of microbes in the bodies of humans and other animals, and to show how they support development, digestion and even behaviour. The study of these communities, collectively known as the microbiome, is now one of the hottest areas in biology, and some of the discoveries made by McFall-Ngai have paved the way. \u201cShe pioneered work on animal\u2013microbe interactions well before everyone caught up and the microbiome became such a sexy topic,\u201d says Dianne Newman, a geobiologist at the California Institute of Technology in Pasadena. The microbiome boom is both a blessing and a curse. Attention and funding has focused heavily on projects to sequence microbes en masse, particularly in the human body, and on efforts to understand how they affect health. The squid and its luminous partner risk being eclipsed, at a time when funding is increasingly tight. But even the most prominent microbiome researchers say that they have time for McFall-Ngai and her squid\u2013bacteria symbiosis, because understanding this simple relationship could help to make sense of more complex microbial communities, which are, by their nature, harder to study. \u201cI'd argue that it's important to take advantage of the lessons emerging from such systems,\u201d says Jeff Gordon at Washington University in St. Louis, one of the leading figures in human microbiome research. \u201cTheir importance isn't diminished.\u201d The squid may represent the road less travelled \u2014 but McFall-Ngai has always been drawn by such paths. \u201cWhen I first met her, we were both in LA, driving a lot,\u201d recalls her partner, Ned Ruby. \u201cIf she was driving from A to B, even if there was one obvious way, she'd try all these routes. Most would be longer. I'd say, 'Why are we doing this?' She'd say, 'You never know when the freeway's going to be blocked. I want to scout out the ways of going round.' That's how she does science. She doesn't go down the main road and get blocked. She goes down the side roads.\u201d \n               Light the way \n             McFall-Ngai started down her scientific road as a graduate student, when she became fascinated by bioluminescence and started studying ponyfish, which carry a glowing bacterium. She wanted to understand how these partnerships began, but was frustrated because the fish proved impossible to raise in a lab. Then, a colleague said to her, \u201cHey, have you heard about this squid?\u201d A few embryologists had been studying the creature, which swims in shallow reef flats around Hawaii and emerges at night to forage. But no one had paid attention to the relationship with its bacteria \u2014 until 1988, when McFall-Ngai flew out to Hawaii to take a look. First, she had to learn how to catch the animals; in knee-deep water, she could snag dozens with just torches and nets. She began breeding them in 1989, when she started her own lab at the University of Southern California in Los Angeles. She found that just 8\u201310 pairs could produce 60,000 juveniles a year. And unlike animals whose symbionts provide essential nutrients, the squid can survive without  V. fischeri . This meant that McFall-Ngai could raise the partners separately, introduce them, and watch their first dates. But first, she needed a collaborator \u2014 someone who understood the bacterium. \u201cI think I was the third microbiologist she came to and the first who said yes,\u201d says Ruby. The two had met when they were taking courses in Los Angeles. They have been professional partners ever since she started working with the squid, and romantic ones for most of that time. \u201cI think it's a real symbiosis the two of them have,\u201d says Nicole Dubilier from the Max Planck Institute for Marine Microbiology in Bremen, Germany. McFall-Ngai and Ruby embarked on a journey to unpick every aspect of the squid\u2013bacterium symbiosis, at first in separate institutions, then on adjacent floors at the University of Hawaii at Manoa in Honolulu, and finally in adjoining rooms at the University of Wisconsin\u2013Madison. They knew that the squid are colonized by  V. fischeri  within hours of hatching. But how does the bacterium infiltrate the light organ? And why is it the only species to do so, when other ocean bacteria collectively outnumber it 1,000-fold? To find out, McFall-Ngai carefully dissected the light organ, and Ruby loaded the bacteria with fluorescent proteins to track the microbes' movements. Some details of the symbiosis are still falling into place. But the pair now know that the relationship begins on the underside of the newborn squid, when mucus-lined fields of beating hairs called cilia create a current that draws bacteria close. Physics then gives way to chemistry. When  V. fischeri  first touches the squid, it changes the expression of scores of squid genes \u2014 a finding 1  made in 2013 by former postdoc Natacha Kremer. Some of these genes produce a cocktail of antimicrobial chemicals that create an inhospitable environment for most microbes while leaving  V. fischeri  unharmed. Others release an enzyme that breaks down the squid's mucus to produce chitobiose, a substance that attracts more of the bacterium. It takes just five  V. fischeri  cells to trigger these changes, and the microbe soon dominates the fields of cilia (see 'What the squid hid'). Chitobiose also stimulates the bacteria to start migrating into three blind-ended crypts in the squid's light organ. Once they reach their destination, they cause the pillar-like cells that line the crypts to become bigger and denser, enveloping the microbes in a tight embrace 2 . The crypts close off, sealing  V. fischeri  inside for the rest of the squid's 3\u201310-month life 3 . In 2004, McFall-Ngai's team showed that two molecules carried by the bacteria \u2014 peptidoglycan and lipopolysaccharide \u2014 are responsible for these changes 4 . That was a surprise. At the time, these chemicals were known only in the context of disease \u2014 they were described as pathogen-associated molecular patterns, or PAMPs, tell-tale substances that alert animal immune systems to burgeoning infections. McFall-Ngai took the acronym, swapped the pathogenic P for a microbial M, and rebranded them as MAMPs. These molecules, she proposed, can trigger debilitating inflammation but they can also start a friendship: without them, the squid's light organ never reaches its mature form. To McFall-Ngai, these results hinted at a broader theme in biology: animals grow up under the influence of their microbes, not just the blueprints encoded in their genomes. \u201cMost of us would say: Isn't that interesting? Margaret said: That's interesting \u2026 and microbes play a role in development,\u201d says Angela Douglas, an entomologist and microbiologist at Cornell University in Ithaca, New York. \u201cShe doesn't deal in little ideas.\u201d McFall-Ngai proposed 5  the concept in 1991, and other scientists have confirmed it, finding that the bodies and immune systems of animals ranging from tsetse flies to mammals mature properly only after exposure to bacteria \u2014 sometimes in response to the same MAMPs. Michael Hadfield, a marine biologist at the University of Hawaii, for example, has shown that the larvae of some marine worms metamorphose into adults only when they encounter bacterial molecules 6 . This made sense when he considered that the earliest animals originated in oceans that were swarming with bacteria. \u201cThey very likely evolved to 'use' those bacteria as a source of cues for developmental change,\u201d he says. McFall-Ngai has championed other ideas, too. One of them emerged when she started thinking about the adaptive immune system, a trademark of vertebrates that targets incoming microbial threats with bespoke antibodies and retains a memory of past encounters. Invertebrates, including squid, rely on innate immunity \u2014 a simpler, short-lived and ever-present battalion of defensive cells. Many immunologists had assumed that vertebrates evolved adaptive immunity because they live longer than invertebrates, and a more complex immune system affords them better protection against pathogens across an extended lifespan. In 2007 \u2014 just as interest in microbiomes was taking off \u2014 McFall-Ngai offered an alternative explanation. In a commentary for  Nature  called Care for the Community 7 , she argued that adaptive immunity evolved because vertebrates need to control a more complex microbiome than invertebrates do. They use it to support beneficial microbes and to block those that pose a threat. Not everyone buys into the hypothesis. Forest Rohwer, an immunologist at San Diego State University in California, points out that corals lack adaptive immunity but host some of the most complicated microbiomes around. Still, he agrees that adaptive immunity might allow vertebrates to fine-tune their large microbiomes, and other scientists concur. \u201cIt's a different way of thinking about the immune system,\u201d says Douglas. \u201cPeople can agree or disagree with it, but it is a touchstone. If someone says, 'Remember Care for the Community?', everyone knows what they mean. It's short for a suite of ideas that challenge traditional notions in a really informed way.\u201d \n               Talk about a revolution \n             McFall-Ngai exudes a stateswoman's confidence as well as a scientist's exuberance; friends describe her as regal. And so convinced is she by the importance of animal\u2013microbe interactions that her message can verge on evangelism. \u201cWe now know that microbes make up the vast diversity of the biosphere, and that animal biology was shaped by interacting with microbes,\u201d she says. \u201cIn my mind, this is the most significant revolution in biology since Darwin.\u201d McFall-Ngai has broadcast that message widely. In 2005, when the American Society for Microbiology was dominated by infectious-disease researchers, she persuaded the organization to run its first meeting on beneficial microbes \u2014 a meeting that continues to be popular today. She served on a National Academy of Sciences committee convened by President Barack Obama to outline where biology in the United States will go in the 21st century. In 2012, she helped Newman to create a course that would teach undergraduates the principles of biology using microbes as the starting point of every topic \u2014 and she regularly flew from Madison to Pasadena during her holidays to teach the class. Her passion for the squid has also spawned an academic dynasty. Ruby and McFall-Ngai have now trained dozens of scientists, around 16 of whom are still studying the same symbiosis, now as heads of their own labs. But the duo discourages rivalries. \u201cI grew up watching fields that eat their young, and I didn't want that,\u201d says McFall-Ngai. The pair invites postdocs who set up their own lab to claim an aspect of the symbiosis for themselves \u2014 and every year they host a symposium-cum-party affectionately called the Pow-Wow, at which everyone gets together to share their results and plans. \u201cIf someone else says, 'I was going to do that too', they sit in a corner and talk about it,\u201d says Ruby. Despite the conviviality, the group knows that it must compete for a limited pot of funds. \u201cI've been told, 'We've already funded Margaret or Ned; how many more can we fund?',\u201d says Spencer Nyholm, an early student of McFall-Ngai who now works at the University of Connecticut in Storrs. \u201cI can't imagine they would ask this if someone proposed to work with  Drosophila  or  C. elegans  or mice.\u201d McFall-Ngai says that she and her prot\u00e9g\u00e9s are just getting started. In one project, she is examining an evolutionary theory predicting that every microbiome should be plagued by cheats \u2014 microbes that reap the benefits of life in their hosts but do not provide anything in return. Sure enough, the squid is sometimes colonized by strains of  V. fischeri  that do not make any light. McFall-Ngai's team has found that the squid can use light-sensitive proteins in its light organ to detect a few dark bacteria among a million brightly glowing ones, and selectively evict them 8 . The team now wants to find out more about how it does this \u2014 and the answers might help to explain how humans and other vertebrates manage more complicated microbiomes. The team has also shown that the squid's relationship with  V. fischeri  varies over the course of a day, controlling the microbes so that they produce light only at night 9 . And in 2013, former student Elizabeth Heath-Heckman showed that  V. fischeri , in turn, influences the squid's daily rhythms through a gene that makes a cryptochrome \u2014 a type of protein that affects circadian rhythms in many animals 10 . Cryptochromes are usually activated by environmental light, but Heath-Heckman showed that one of the squid's cryptochrome genes responds only to the blue light that  V. fischeri  emit, ramping up production of the protein. On the basis of this work, the team predicted that interactions between people and their resident microbes might also change from day to night \u2014 and soon, the evidence was pointing that way. Last year, a group in Israel showed that a significant proportion of microbes in the human gut rise and fall in abundance in a 24-hour cycle 11 , and regular jetlag, for instance, can promote weight gain by disrupting these rhythms. \u201cOne of the things we pound into people who come to the labs is that nobody really gives a damn about the squid,\u201d says Ruby. \u201cThey care about the big questions that the squid will help to answer.\u201d To tackle more of those questions, in a few months McFall-Ngai and Ruby will move to share the squid's home. They will return to Hawaii, where McFall-Ngai will head the Pacific Biosciences Research Center in Honolulu. It is a dream job, and a chance to indulge more in her favourite pastimes \u2014 skateboarding and bodysurfing \u2014 as well as watch the squid on moonlit nights. \u201cThis was completely backwater science,\u201d she says. \u201cNow it's front-seat science. It's been fun to watch people realizing that microbes are the centre of the Universe, and to see the field blossom.\u201d \n                     Microbiology: Ditch the term pathogen 2014-Dec-10 \n                   \n                     Microbiology: Microbiome science needs a healthy dose of scepticism 2014-Aug-20 \n                   \n                     Microbiology: Tinker, bacteria, eukaryote, spy 2009-May-13 \n                   \n                     Adaptive Immunity: Care for the community 2007-Jan-10 \n                   \n                     The winnowing: establishing the squid\u2013vibrio symbiosis 2004-Aug-01 \n                   \n                     Margaret McFall-Ngai's lab \n                   \n                     Ned Ruby's lab \n                   Reprints and Permissions"},
{"file_id": "517258a", "url": "https://www.nature.com/articles/517258a", "year": 2015, "authors": [{"name": "Rachel Cernansky"}], "parsed_as_year": "2006_or_before", "body": "A charcoal-rich product called biochar could boost agricultural yields and control pollution. Scientists are putting the trendy substance to the test. For more than 150 years, the Brooklyn Navy Yard constructed vessels that helped to stop the slave trade from Africa, lay the first undersea telegraph cable and end the Second World War. Now, this sprawling industrial facility in New York City is filled with artists, architects, producers of artisanal moonshine and people growing organic vegetables. On a drizzly day in autumn, Ben Flanner tends a sea of red and green lettuce on a 6,000-square-metre rooftop farm. The soil beneath the plants looks ordinary, but Flanner grabs a handful and holds it up for inspection. Amid the brown clods of dirt are small black particles \u2014 remnants of charcoal fragments that were mixed into the soil two years ago. Flanner thinks that this carbon-rich material, known as biochar, has helped the crops to thrive, possibly even increasing their yield, and he hopes for more impressive results over the next few years. Across the United States, sales of this long-lasting soil additive have surged over the past few years, tripling annually since 2008, according to some estimates. The Biochar Company in Berwyn, Pennsylvania \u2014 which supplied Flanner's Brooklyn farm \u2014 sells it both wholesale and direct to consumers, through outlets including Amazon and some Whole Foods stores. And countries ranging from China to Sweden are using biochar on agricultural fields and city lawns. Proponents see big potential for the soil enhancer, which is produced by heating biological material \u2014 such as husks and other agricultural waste \u2014 in a low-oxygen chamber. Biochar can be made as a by-product of biofuel generation, so some companies are hoping to cash in on both products as demand grows for greener forms of energy. Interest in biochar is also growing among scientists, who are quickly ramping up studies to test its potential. They are particularly interested in how the chemical and physical properties of biochar particles affect water moving through soil, remove pollutants, alter microbial communities and reduce emissions of greenhouse gases. The hope is that biochar can help farmers around the world, particularly those in Africa and other developing regions, who often struggle with poor soils. Johannes Lehmann, a crop and soil scientist at Cornell University in Ithaca, New York, says that different types of biochar \u201chave unique potential to mitigate some of the greatest soil-health constraints to crop productivity \u2014 for example, in highly weathered and sandy soils\u201d. But there are still many questions about biochar, particularly in terms of making sure that it is affordable and has positive effects. In some studies, the material has actually reduced yields. Part of the difficulty is that biochar can be produced from all kinds of biomass and at different temperatures and speeds, which leads to huge variation in the substance \u2014 and in results. \u201cI always say we should not even use the singular for biochar,\u201d says Lehmann. \u201cThere are only biochars.\u201d \n               Amazonian roots \n             Although it is just starting to catch on with farmers today, biochar has ancient roots. Hundreds to thousands of years ago, residents of the Amazon produced it by heating up organic matter to create rich, fertile soils called  terra preta . But the practice was abandoned around the time that European nations invaded South America, and relatively few farmers elsewhere have routinely used biochar. Scientists first took a big interest in the material about a decade ago, when growing concerns over global warming led some to tout biochar as a way to store huge amounts of carbon underground. Hope for that application has faded somewhat, but soil scientists are now exploring its use in agriculture and remediating pollution. A particular focus has been explaining how biochar affects water movement through soils. Rebecca Barnes, a biogeochemist at Colorado College in Colorado Springs, and some of her colleagues tested that by adding biochar to different materials 1 . In sand, through which water typically drains very quickly, biochar slowed the movement of moisture by an average of 92%. In clay-rich soil, which usually retains water, biochar sped up movement by more than 300%. The researchers suggest that the biochar alters how water moves through the interstitial space \u2014 the gaps between grains in the soil. \u201cClays tend to be flat grains and sand tends to be circular grains, but biochar is very amorphous \u2014 and so it's not only creating these crazy pathways through the biochar, but it's also creating crazy pathways in that interstitial space,\u201d says Barnes. She and her colleagues suggest that these convoluted pathways help to slow down drainage in sand and speed it up in clays. That is significant, Barnes says, because even though clays can hold large amounts of water, that moisture has a hard time moving through the grains and reaching plant roots. Some studies have shown that plants grow better in soil with added biochar than in plain soils or those treated just with compost 2 . Researchers are also teasing apart how biochars influence microbial activity in soil. Microbes typically act as a community; for example, many pathogenic bacteria attack a plant's roots only when they have sufficient numbers to overwhelm the host's immune response. Caroline Masiello, a biogeochemist at Rice University in Houston, Texas, and her co-workers have found 3  that biochar can inhibit this by binding to the signalling molecules that bacterial cells secrete to coordinate their activity. \u201cThey all think they're alone, because the telephone wires have been cut,\u201d says Masiello. With further research, she says, it might be possible to fine-tune this function of biochar to reduce plant infections. Other researchers are exploring how biochars can cut emissions of nitrous oxide, a greenhouse gas, from agricultural fields. Last year, Xiaoyu Liu, a soil scientist at Nanjing Agricultural University in China, and his colleagues reported 4  that after biochar had been applied to maize (corn) and wheat fields once, nitrous oxide emissions declined over the following five crop seasons, a period of three years. Other studies have shown reductions as well, but researchers have not yet been able to determine what exactly causes this effect. Applying biochar \u201ccan also improve some soil properties, like it can increase the potassium availability, and the soil organic-matter content\u201d, says Liu, who has obtained some funding from biochar producers. But not all studies show biochar to be a wonder material. In some cases it has reduced crop yields 5 , and one study 6  suggests that it lowers the activity of plant genes that help to defend against insect and pathogen attacks. Lehmann says that this may come down to improper applications of biochar. In some of the studies that showed decreases in yields, he says, the soils were perfectly fine to start with. Other work suggests 7  that using the wrong type of biochar can negatively impact the soil's microbiota or, potentially, its carbon-storage capacity. A biochar made from rice straw, for example, will function differently in a certain soil than will biochar made from wood or manure. Overall, however, the positive impacts of biochar seem to outweigh the negative ones. A 2011 meta-analysis 8  found an overall average yield increase of 10%, rising to 14% in acidic soils. Biochar's greatest potential might be in places where soils are degraded and fertilizer scarce, in part because it helps the soil to better retain any nutrients that it does have. Andrew Crane-Droesch at the University of California, Berkeley, has been studying the impacts of biochar in such degraded soils in western Kenya. His preliminary data suggest that farms using biochar averaged 32% higher yields than controls. In June, a World Bank report 9  said that biochar probably holds the most potential for small farmers in developing countries, not just because they are working with the soils most likely to benefit, but because biochar may be a key element of 'climate-smart' agriculture \u2014 practices that both help to mitigate climate change and reduce vulnerability to its effects. \n               Pollution wrangler \n             Biochar's start may have been in agriculture, but researchers are now looking at other applications. Biochar can bind to heavy metals in soil, which helps to keep them from reaching plants or entering water supplies. That has attracted the notice of the US Environmental Protection Agency, other agencies, and companies seeking to reclaim land formerly used in mining. At the Hope Mine near Aspen, Colorado, biochar added in 2010 helped to neutralize the impacts of decades-old mine refuse by immobilizing the metals and increasing the amount of water held on the slope \u2014 thereby reducing the opportunity for contaminated water to become run-off. It also helped to spur plant growth on the formerly barren hillside, according to the Aspen Center for Environmental Studies. Biochar is also showing promise in cleaning up polluted water, perhaps as a much cheaper replacement for activated charcoal, which is used at sites ranging from treatment plants to areas that are heavily contaminated with toxic chemicals. Biochar particles have a relatively large surface area, which expands even further in water, providing a vast number of sites for contaminants to bind to, says Charles Pittman, a retired chemist at Mississippi State University in Starkville. He says that this type of pollution remediation may be particularly beneficial in countries that lack full water-treatment systems. It could also help to remove antibiotics or chemical wastes, which are difficult to strip out with conventional water treatments. Scientists have even explored biochar's potential for treating fluids used in oil and gas drilling, and as a component of print toners and paint products. \u201cThere's a lot of other markets that haven't fully been explored yet,\u201d says Kurt Spokas, a soil scientist with the US Department of Agriculture's Agricultural Research Service in St Paul, Minnesota.  The hope is that biochar could help farmers, particularly in Africa and other developing regions.  Experts caution, however, that it is not clear when or whether remediation \u2014 or other applications \u2014 will be economical, particularly in agriculture. Poor soils and poverty often go together. After demonstrating yield increases in Kenya, Crane-Droesch looked at the economic viability of biochar in the same communities. \u201cWhat we found was almost nobody was willing to pay for biochar when offered at roughly the price it took to make it,\u201d he says. Biochar prices vary widely, but in the United States some products cost US$3 per kilogram, comparable to certain fertilizers and more than many composts. On a large scale, biochar production may make economic sense only when biofuel production does \u2014 for example if it is subsidized or because policies to reduce carbon emissions drive fossil-fuel prices up. And if demand ever does surge, there will be questions about the environmental impact of producing biochar. One key concern is the choice of feedstock. China is eager to use agricultural waste, such as rice and wheat straw, and some researchers in the United States are even pushing animal manure, but neither may be the most efficient way to produce it on a massive scale. And using wood could spur deforestation or harmful land-use practices. \u201cIt's an incredibly important question to ask: what is the sustainability of the feedstock?\u201d says Alfred Gathorne-Hardy, research director of the India Centre for Sustainable Development at the University of Oxford, UK. \u201cThis is the kind of debate I don't think we're seeing enough about within the biochar world.\u201d \n               Growth industry \n             That debate may grow as consumer interest does \u2014 something that is slowly happening around the world. Bj\u00f6rn Embr\u00e9n, who is responsible for tree planning and protection in Stockholm, says that the city has been using biochar to boost local vegetation since 2009; he credits it with the city's healthiest tree growth in recent years. In September, the New York-based charity Bloomberg Philanthropies awarded Stockholm \u20ac1 million (US$1.2 million) to launch a city-wide programme that will turn residential garden waste, and eventually food waste and even sewage, into biochar. Back in Brooklyn, Flanner continues to monitor his crops. The lettuces and carrot tops glisten under the rain as he steps carefully between rows in his bright yellow rain jacket. He thinks that the biochar will be good for his soils over the long term because it helps them to retain nutrients and water. \u201cThose are both very important, especially in such a well-drained soil as on a green roof,\u201d he says. \u201cWe tend to lose both of those quickly.\u201d But before he adds more of the black grains to other parts of his farm, he will wait to see how the crops respond over the next few years. Like the scientists studying biochar, he is eager to see whether it will live up to its bright promise or fade like so many other would-be wonder materials. \n                     Biochar by design 2014-Apr-29 \n                   \n                     Enabling food security by verifying agricultural carbon 2014-Apr-25 \n                   \n                     Amazon ecology: Footprints in the forest 2013-Oct-09 \n                   \n                     Charcoal's green image blackened 2008-May-01 \n                   \n                     Putting the carbon back: Black is the new green 2006-Aug-09 \n                   \n                     Blog post: Scientists tell governments to commit to agriculture funding at Rio+20 \n                   \n                     Blog post: Scientists launch African agricultural monitoring system; global network to follow \n                   \n                     Blog post: Picture post: the story is in the soil \n                   \n                     International Biochar Initiative \n                   \n                     US Agricultural Research Service \n                   \n                     Food and Agriculture Organization of the United Nations: Climate-smart agriculture \n                   Reprints and Permissions"},
{"file_id": "517542a", "url": "https://www.nature.com/articles/517542a", "year": 2015, "authors": [], "parsed_as_year": "2006_or_before", "body": "Research relies on unsung heroes working behind the scenes \u2014 and some of them have rather unusual jobs. \n               THE GLASS-BLOWER IN THE BUSH \n             \n               Sarah Davis creates bespoke scientific glassware for chemists from her shed in rural Australia. \n               By Michael Hopkin \n             The West Australian town of Jarrahdale (population 1,082) seems an unlikely place to go if you need to get your hands on some highly technical glassware in a hurry. Turn off the main street with its tavern, general store and logging museum, and the road quickly becomes dirt punctuated by sun-faded letter boxes, wonky fences and dusty driveways. But it is down one of these driveways that you'll find Sarah Davis, who has been running a scientific glass-blowing business since 2010. Working from her garage, she provides local researchers \u2014 mostly university chemists in nearby Perth \u2014 with handmade flasks, tubes, condensers and bespoke items that don't even have a name. \u201cIf they want a simple condenser, I can whip that up in half an hour,\u201d says Davis, referring to the glass tube used to cool hot vapours. \u201cI get people ringing up saying, 'I've broken this,' and generally I get it out for them the next day.\u201d For scientists who live in one of the world's remotest cities, this makes Davis an extremely useful person to have around. The alternative is to wait at least six weeks for orders to be made and shipped from Sydney. \u201cSometimes she comes in after a couple of days and says, 'I've finished,' and I say, 'Already?'\u201d says Grant Cope, who orders from Davis as part of his job as stores officer for the chemistry department at Curtin University in Perth. Many big research institutions have their own scientific glass-blowers \u2014 and that is what Davis was doing until five years ago, working as the in-house glass-blower on the University of Western Australia (UWA) campus in Perth. But in 2010, when she was laid off in a round of university cutbacks, she decided to go it alone, putting her outbuildings into service as her workshops. \u201cWhat could be more Australian than working in your garage and seeing a kangaroo come hopping down the drive?\u201d she says. This is, in fact, routine. She also shares the garage with two possums that like to take naps in the rafters when the temperature creeps past 40 \u00b0C, as it tends to do in summer. But in the worst of the weather, Davis is less likely to be toiling over hot glass: she is a volunteer firefighter and is regularly called up to deal with bush fires. The rustic setting belies the fact that Davis's craft is a highly technical practice, bearing very little resemblance to traditional glass-blowing. For a start, there is not much blowing involved. She works with borosilicate glass, which unlike standard glass, can withstand temperatures of 300 \u00b0C, as well as corrosive chemicals and high pressures. She heats and softens the glass over a gas flame, then uses a variety of tools to work it into shape. Perhaps most important is the glass-blowing lathe, with two spindles facing one another, both turning at precisely the same speed. On a day in December, with blowtorch in one hand and safety glasses firmly on (hot borosilicate glass gives off a dangerously intense orange glare, not to mention lots of ultraviolet radiation), she carefully attaches a section of glass to the end of a long tube mounted on the lathe, rounding it off to create a test tube the size of her arm. She uses a similar process to make her flasks and other more specialized glassware. To finish off, Davis bakes her wares at 560 \u00b0C in an annealing oven, smoothing out stress points that could otherwise break the glass. With diamond saws, tube cutters, lathe and oven, Davis estimates that her set-up is probably worth around half a million Australian dollars (US$400,000), and as a result she prefers to keep a low profile \u2014 even in a quiet town. \u201cI don't tend to have clients come and see me, I don't have a web page; it's all word of mouth and previous clients.\u201d She has plenty of those, garnered over a 20-year career that started when, as a newly qualified lab technician in Perth, she landed a job that included a glass-blowing traineeship. Davis admits that she had never heard of scientific glass-blowing before that. George Koutsantonis, a chemist at the UWA, describes her components as \u201cvital\u201d for his research on pyrophoric chemicals, which ignite spontaneously if exposed to air. \u201cIt's not the sort of thing you can buy off the shelf,\u201d he says. Davis's strangest commission so far has been from some intrepid zoologists who asked her to make a glass funnel to hold over a dolphin's blowhole in the hope of catching a sample for analysis. \u201cI never got to see it in action,\u201d she says. These kinds of weird and wonderful commissions are a lot rarer now. Thanks to financial pressures, only a handful of Australian universities still have an on-campus glass-blower \u2014 researchers have to order off-the-shelf glassware, and are less likely to request customized parts if they have to pay freelance glass-blowers out of tight budgets. Even counting those still plying their trade off-campus, there are only 25 scientific glass-blowers left in Australia and New Zealand, says Davis. \u201cThere are just two of us in Western Australia that do it \u2014 the other guy is getting to retirement age. Hopefully I've got another 25 years left in me, but the chance of training someone is probably not there. It's a dying art.\u201d \n               THE SNAKE MILKER \n             \n               Jim Harrison extracts venom from some 600 snakes a week \u2014 and has the scars to show for it. \n               By Kelly Rae Chi \n             In nearly four decades collecting deadly snake venom, Jim Harrison says, he has been bitten \u201conly eight times\u201d. And although he remembers each one vividly, tallying them up on his fingers can be tricky. An Indian cobra ( Naja naja ) mangled his right little finger 12 years ago, leaving it curled and increasingly sore until he had surgery to repair it . A bite from a desert horned viper ( Cerastes cerastes ) dissolved part of the bone in his left middle finger. Two other fingers, although functional, bear the scars of his profession. All this is par for the course when you nurture lethal snakes for science. Harrison and his wife, Kristen Wiley, run the Kentucky Reptile Zoo (KRZ) in Slade, which Harrison opened in 1990 as a research and education centre. It houses 1,600 snakes from more than 100 species, and it is one of just a handful of places around the world producing snake venom for biomedical research. Snake venoms contain a complex cocktail of enzymes and other substances that help to immobilize or digest prey, and which are of great interest to scientists. Drugs used to treat hypertension have been modelled on substances in venom that drastically lower the blood pressure of prey, for example. Other proteins in venom have been used to identify and study specific signalling molecules in the nervous system. And venoms are needed to develop antivenoms. The KRZ sells about 1,400 grams of venom per year. Wiley and Harrison \u201cprovide a tremendous service, because most of us don't have time to be zookeepers\u201d, says Steven Aird at the Okinawa Institute of Science and Technology Graduate University in Japan, who has studied venom. \u201cThey really become not just suppliers, but almost collaborators in a sense.\u201d Harrison's fascination with snakes and other reptiles took hold when he caught a garter snake at the age of six. Throughout childhood, he read voraciously on reptiles and amphibians; at 16, he worked on an alligator farm. Harrison started keeping venomous snakes as a hobby. He learned about venoms and extraction from books, including those written by Sherman Minton, a prominent herpetologist in Indianapolis with whom Harrison eventually became friends. Minton connected Harrison with others interested in venoms, and soon Harrison began to milk king cobras ( Ophiophagus hannah ) for university researchers. Harrison never believed that he could have a career involving snakes, so he became a police officer instead. But he continued extracting venom in a home laboratory equipped with a centrifuge to purify venom and a lyophilizer for freeze-drying it. At 26, after getting mown down by a stolen car while trying to make an arrest, Harrison's heart stopped. He decided that policing was too dangerous, so he retired early and dedicated his career to snakes. Since then, snake bites have stopped Harrison's heart three more times. These days, Harrison and Wiley divide the work of running the reptile zoo. Wiley, who did an internship at the KRZ in 1998, manages the zoo's educational programmes, reads the scientific literature and attends conferences to stay current on venoms and work out whether to breed a particular species that year. The actual milking falls to Harrison, who for liability reasons is the only staff member at the KRZ who does it. In front of a group of goggle-eyed schoolchildren, he demonstrates his technique on a monocled cobra ( Naja kaouthia ), a species that put him in hospital on life support after a bite in 2012. He pulls the 1.2-metre-long, dishwater-grey specimen onto a padded mat and pins its head down with the flat part of a long metal hook. Harrison grabs the cobra behind its head. As it reveals its fangs \u2014 a natural response to threat \u2014 Harrison plants them through a sheet of plastic film stretched across a funnel. He uses his thumb and a partially missing forefinger to massage the muscle supporting its venom glands. He will do this on between 600 and 1,000 snakes per week. If everything goes as it should, he says, then milking snakes is methodical \u2014 \u201cboring\u201d, even. In fact, according to data that a physician friend gathered on him, Harrison's heart beats faster when he is driving to the supermarket than when he is milking. Stephen Mackessy at the University of Northern Colorado in Greeley says that the KRZ's reputation and knowledge of venoms sets it apart. Some companies provide repackaged venoms, but the provenance of these products, which can matter greatly in research, is uncertain at best, he says. Wiley says that much of this comes down to understanding the animals, which she and Harrison breed themselves, but also obtain from zoos and universities. \u201cWe attempt, as much as we can, to provide the locale and the origin information to the researcher,\u201d says Wiley. Harrison says that the benefits for medical researchers \u2014 and for society \u2014 make him willing to take his daily calculated risks. \u201cI don't plan on slowing down,\u201d he says. \u201cI will keep extracting until I die.\u201d \n               THE SQUID COLLECTOR \n             \n               Bill Klimm gave up commercial fishing to catch sea creatures for scientists, off the Massachusetts coast. \n               By Elie Dolgin \n             On a blustery morning in late October, the wind is blowing up quite a swell \u2014 enough to make this reporter heave his breakfast into the briny deep \u2014 but Bill Klimm is unperturbed. The 78-year-old fisherman sits calmly in his captain's seat, arms folded, staring straight ahead at the choppy waters off the coast of Martha's Vineyard, Massachusetts, as his boat, the  Gemma , travels southwest. Klimm and his co-captain, Dan Sullivan, are heading to Menemsha Bight in search of longfin inshore squid ( Doryteuthis pealeii ). These squid are prized for their giant nerve fibres, which allow biologists to study neurotransmission in exquisite detail. For the past 18 years, Klimm has been collecting these and other saltwater specimens for scientists at the Marine Biological Laboratory (MBL) in Woods Hole, Massachusetts, and elsewhere around the world. From invertebrates such as sponges, worms, sea stars, urchins and anemones to several fish species and some plants, the creatures have a wide range of habits and dwelling places, but Klimm knows where to find them. And if he does not, he has a network of local fishermen that he can tap for advice. David Remsen, who manages the Marine Resources Department at the MBL, tells Klimm what to catch on the basis of the orders he receives from scientists. He says that a good specimen collector needs intuition for the local seas and the skills to maintain the boats that navigate them. Klimm has it all. \u201cHe knows the waters, he knows the equipment, and he takes ownership of both,\u201d says Remsen. Klimm's knowledge of marine biology runs deep, too. \u201cIf you want to understand something about the squid life cycle, you will learn more in ten minutes talking to Bill than you will spending a week talking to so-called experts,\u201d says Joseph DeGiorgis, a squid neurobiologist at Providence College in Rhode Island, and an adjunct faculty member at the MBL. H. William Klimm III was born with Cape Cod fishing in his blood. His grandfather was a fisherman and lobsterman who owned a boatyard in Hyannis Harbor, Massachusetts. His father was a commercial fisherman operating out of Falmouth, who collected squid for the MBL as a sideline for 45 years \u2014 until the age of 88. Klimm himself started fishing commercially when he was 23 years old. He caught cod, flounder, swordfish and lobsters for 30 years, until a boat fire cast him ashore in 1990. He fixed boats in Boston for five years before landing the MBL position. At the age of 60, after decades of long trips at sea, Klimm was finally home every night after work. \u201cMy wife calls it a toy job,\u201d he says. David Bodznick, who studies the neurobiology of behaviour at Wesleyan University in Middletown, Connecticut, has spent summers at the MBL for more than 30 years, researching electrosensing in skate. \u201cYou could really tell when [Klimm] came on board that changes had been made,\u201d he says. For example, Klimm installed new reels and altered the nets to minimize damage to the squid and other animals. \u201cThe whole operation became more efficient,\u201d says Bodznick. When Klimm and Sullivan reach their destination at Menemsha Bight, they drop a large net into the water, tow it along for 25 minutes, pull in the line and sort through the catch \u2014 all without exchanging a word. \u201cWe do it so many times that we don't have to talk about it,\u201d Klimm remarks afterwards. Large squid (those 25 centimetres long or more) go in one bucket; medium in another. Any small or damaged animals get tossed to the squawking seagulls overhead. A majority of the hundred or so squid collected today will be used to train neurosurgeons attending a week-long teaching course at the MBL. Some will go to the nearby Woods Hole Oceanographic Institution, where scientists are investigating the effects of ocean acidification on squid physiology, and 10\u201320 go to a visiting researcher at the MBL, Yuyu Song, who is studying how misfolded proteins affect neurotransmission in the squid's giant synapse. \u201cThe  Gemma , her captain and the MBL collecting expeditions are all very dear to me since a big part of my research would have been impossible without them,\u201d says Song, a neuroscientist normally based at the Yale School of Medicine in New Haven, Connecticut. Back in port, Klimm talks about what he does with his free time, gesturing across the dock to his \u201cplay boat\u201d, the  Sea Dog IV . That is where he and his wife can be found most weekends in the summer, tooling around Martha's Vineyard and the Nantucket Sound. \u201cFor years and years and years I've done that \u2014 stepped from one boat to the other,\u201d he says. \u201cIt's kind of stupid, I suppose, but that's what I do. That's what I do.\u201d \n               THE DATA MECHANIC \n             \n               Dawn Johnson keeps the wheels turning beneath some of the world\u2019s biggest bioinformatics databases. \n               By Ewen Callaway \n             When Dawn Johnson opens the doors into her workspace, the first thing you notice is the roar. The noise comes from whirring fans, which are required to cool the towering stacks of 16 computer servers that form walls of black and silver. Bundles of multicoloured cables, as thick as small trees, trail upwards like an electrical rainbow. \u201cIf anything goes wrong, I'll be the first port of call,\u201d Johnson says, standing beside a toolbox the size of a shopping trolley. \u201cI'll rip 'em to bits and find out.\u201d Computational biologists the world over rely on Johnson to do that, even though most do not know her. That's because Johnson is a computer-hardware engineer at the European Bioinformatics Institute (EBI) in Hinxton, UK. The servers that she keeps running hold one of the world's most extensive collections of molecular databases \u2014 from an archive of DNA-sequencing data to the leading repository of protein structures. The machines that she and her colleagues maintain hold a whopping 60,000 terabytes of data, and people at around half a million unique Internet addresses use these data each month. A blip in availability is not an option. \u201cIt's imperative that it's there 24/7,\u201d says Johnson. For Johnson, bearing the weight of the bioinformatics world on her shoulders was particularly burdensome late last year. Besides the centre in Hinxton, the EBI data had been spread across another two locations in London, but a contractor change meant that they had to move to a single location in a nearby town \u2014 and Johnson had to coordinate it. She and a small team of fellow engineers had to ensure that there was adequate space, power and cabling for the move, which involved roughly 9,500 computers connected by 850 power cables and 3,400 network cables. \u201cThe complexity of that is hair-raising,\u201d she says, with a relaxed shrug. Still, the move went \u201cincredibly well\u201d, says Steven Newhouse, head of technical services at the EBI \u2014 with Johnson playing a crucial part in coordinating the logistics. The success meant, of course, that the researchers who rely on the EBI never so much as noticed. \u201cVery few scientists appreciate the size of the computing infrastructure that they depend on nowadays,\u201d says Newhouse. When she is not at her desk dealing with the ins and outs of such projects, Johnson spends her time in the Hinxton data centre, which the EBI shares with the neighbouring Wellcome Trust Sanger Institute. Johnson and her colleagues install, maintain and repair the machines that feed the centres' seemingly insatiable hunger for data storage \u2014 which is projected to reach 2 exabytes (2 \u00d7 10 18  bytes, or 2 million terabytes) by 2016. There are occasional emergencies. Several years ago, a cooling-system failure forced Johnson to rush into work on a Saturday to keep the servers from overheating. She spent a stressful weekend getting the centre back online, so as to minimize the disruption to researchers. Computers were not the first machines that Johnson learned to rip apart. \u201cMy father's a mechanic and an engineer, and so I was always in the garage with him fixing and tinkering with cars, and that was really what I wanted to do,\u201d Johnson says. \u201cBut it was 1979 when I left school. They just didn't hire lady mechanics.\u201d She went into secretarial work at a firm in Cambridge, UK, that sold and serviced computers for businesses. After a few years, her boss asked her what she wanted to try next, and she opted to work as a computer engineer. She was the only woman on the team. \u201cWhen I was in the field, I was a novelty, I guess. But it was quite good. All the guys wanted to help me get on and succeed, and all the women saw me as sort of a stand for women's lib and rights and stuff and were really on my side as well,\u201d she says. Even now, \u201cI don't meet many other women in my career, which is a shame\u201d. Johnson's move into the bioinformatics world happened by chance. In the 1990s, she was doing contract work on mainframe computers at the Sanger Institute, which had a leading role in the Human Genome Project. She remembers a celebration to mark the completion of a draft human-genome sequence. \u201cI saw that happening and thought I would like to be a part of that,\u201d says Johnson. A hardware-engineer job opened up five years ago, and she jumped at the opportunity. \u201cIt's great when I drive into work and hear people on the radio talking about the latest studies,\u201d she says. \u201cI'm very proud and lucky to be part of it.\u201d \n                 See Editorial \n                 page 527 \n               \n                     Technical support 2015-Jan-28 \n                   \n                     There is life after academia 2014-Sep-03 \n                   \n                     Life outside the lab: The ones who got away 2014-Sep-03 \n                   \n                     Education: The PhD factory 2011-Apr-20 \n                   \n                     Nature Careers \n                   \n                     Woods Hole Oceanographic Institution \n                   \n                     Kentucky Reptile Zoo \n                   \n                     Scientific Glassblowers Society of Australia and New Zealand \n                   \n                     European Bioinformatics Institute \n                   Reprints and Permissions"},
{"file_id": "518024a", "url": "https://www.nature.com/articles/518024a", "year": 2015, "authors": [{"name": "Alison Abbott"}], "parsed_as_year": "2006_or_before", "body": "Babies are increasingly surviving premature birth \u2014 but researchers are only beginning to understand the lasting consequences for their mental development. Fabienne never found out why she went into labour three months too early. But on a quiet afternoon in June 2007, she was hit by accelerating contractions and was rushed to the nearest hospital in rural Switzerland, near Lausanne. When her son, Hugo, was born at 26 weeks of gestation rather than the typical 40, he weighed just 950 grams and was immediately placed in intensive care. Three days later, doctors told Fabienne that ultrasound pictures of Hugo's brain indicated that he had had a severe haemorrhage from his immature blood vessels. \u201cI just exploded into tears,\u201d she says. Both she and her husband understood that the prognosis for Hugo was grim: he had a very high risk of cerebral palsy, a neurological condition that can lead to a life of severe disability. The couple agreed that they did not want to subject their child to that. \u201cWe immediately told the doctors that we did not want fierce medical intervention to keep him alive \u2014 and saw the relief on the doctors' faces,\u201d recalls Fabienne, who requested that her surname not be used. That night was the most tortured of her life. The next day, however, before any change had been made to Hugo's treatment, his doctors proposed a new option to confirm the diagnosis: a brain scan using magnetic resonance imaging (MRI). This technique, which had been newly adapted for premature babies, would allow the doctors to predict the risk of cerebral palsy more accurately than with ultrasound alone, which has a high false-positive rate. Hugo's MRI scan showed that the damage caused by the brain haemorrhage was limited, and his risk of severe cerebral palsy was likely to be relatively low. So just 24 hours after their decision to let his life end, Hugo's parents did an about-turn. They agreed that the doctors should try to save him. Thanks to medical advances since the 1970s, premature infants \u2014 those born before 37 weeks of gestation \u2014 are increasingly able to survive. Some hospitals now try to save babies born as early as 22 weeks. But those developments are forcing doctors and parents to grapple with difficult decisions, because the chances of severe disability increase with the extent of prematurity. Cerebral palsy, for example, affects 1\u20132% of babies born at term, 9% of those born earlier than 32 weeks and 18% of those born at 26 weeks. That is just half the story. Neuroscientists are developing an increasingly sophisticated picture of premature infants' brains that could help to inform medical decisions and treatments. From some long-term studies, they are learning that premature children face a higher risk than was previously thought of developing cognitive or behavioural problems \u2014 according to some studies, as many as half of them will. Researchers are starting to ask why this should be, whether it could be avoided and what is the best way to provide educational support for the affected children. \u201cWe need to gather a lot more data to understand what the best strategies are,\u201d says Petra H\u00fcppi, a neonatologist and developmental paediatrician at the University of Geneva in Switzerland, who is following the brain development of children who were born prematurely. \n               Early birthday \n             Prematurity \u2014 also called pre-term birth \u2014 is extremely common. According to World Health Organization statistics from 2012, more than one in 10 babies \u2014 around 15 million in total \u2014 are born prematurely each year. The great majority are born between 32 and 37 weeks of gestation, but 1.6 million are born between 28 and 32 weeks and 780,000 are born 'extremely pre-term', before 28 weeks (see 'Born too soon'). In low-income countries, more than 90% of extremely pre-term babies born alive soon die, which helps to explain why prematurity is now the second biggest cause of death in children under five, after pneumonia. But in richer countries, with sophisticated neonatal intensive-care facilities, more than 90% of these extremely pre-term babies survive, and doctors are continuing to push the age of survival even earlier in development. Doctors in the United States are debating a controversial recommendation to lower the gestational age at which a baby should be considered potentially viable from 24 weeks to 23 weeks. In Japan, babies born at 22 weeks have been considered viable since 1991. Parents of premature children face agonizing waits as their children fight for their lives. Hugo's parents endured tense weeks during which their son had a series of operations to fix damaged organs, and to create essential connections between major blood vessels that had not had time to develop before birth. They knew he could still die at any time. \u201cBut I felt like we were back on the TGV,\u201d says Fabienne, referring to the French high-speed trains. \u201cThe train goes fast and it rocks frighteningly \u2014 but we were on it again.\u201d But what happens after the immediate danger has passed? Just a few studies have so far followed up the long-term fate of premature babies, because it is time-consuming and expensive to track them with sophisticated cognitive and behavioural tests over many years. One of the first studies to show the extent of developmental problems was EPIPAGE, which looked at a cohort of all live births between 22 and 32 weeks of gestation from 9 regions of France in 1997, and a reference group of 664 full-term babies 1 . Up to half of the premature babies who survived to five years of age had some sort of neurodevelopmental problem by then, and the impairments in cognitive development grew more pronounced for each extra week of prematurity. On a score of cognitive ability, the team observed impairment in 44% of those born between 24 and 25 weeks of gestation and 26% of those born at 32 weeks, compared with 12% of full-term controls. \u201cWe were shocked to see just how many children had problems,\u201d says H\u00fcppi. Moderately premature babies may be at lower risk than extremely premature babies, she notes, but there are many more of them. The effects seem to continue into adulthood. Developmental psychologist Dieter Wolke led an unusual study of hundreds of children born between 26 and 31 weeks of gestation in Bavaria in the mid-1980s. He assessed them at six years old 2 , and again at 26 years 3 . Last year, he reported 3  that most of those who had cognitive problems as children still had them as adults: one-quarter of them had moderate to severe cognitive deficits, and half had mild cognitive deficits. Most of those who experienced problems had short attention spans, and as a group they tended to underachieve academically and career-wise. Wolke, who is currently at the University of Warwick, UK, observed subtler lifestyle differences, too. \u201cThey are less likely to take risks, smoke, drink or have early sexual relationships,\u201d he says. Scientists are still struggling to understand the physical changes in the brain that underlie all these differences. The brain is made up of grey matter, which comprises densely packed cell bodies, and white matter, the long-reach axons of cells that connect different brain regions. These axons are covered in a protective coating called myelin during development, in a precise sequence that begins in the womb and continues for the first decade or so after birth. In the premature brain, immature, fragile blood vessels struggle to provide tissue with enough oxygen for normal development. When a vessel ruptures, crucial areas of white matter are destroyed and cerebral palsy can result. But very little is known about what causes the more subtle brain problems that cohort studies of premature infants are revealing. \n               Too much too soon \n             Scientists suspect that when the brain is forced to carry out a crucial part of its development while the child is in the outside world instead of a warm, watery womb, it receives inappropriate signals from the environment that affect how its neurons are linked into networks. \u201cThe premature brain gets subjected to quite different sensory inputs \u2014 like visual stimulation and gravity effects \u2014 which it is not supposed to be subject to,\u201d says Ghislaine Dehaene-Lambertz of the INSERM-CEA Cognitive Neuroimaging Unit in Paris, who studies language development in infants. \u201cThey can be sudden, intense but also unpredictable.\u201d Some of these unnatural sensory signals are inevitably provided by the intensive medical procedures that keep premature babies alive. Pioneering brain-scanning studies support the idea that altered networks play a part in cognitive problems. H\u00fcppi's Swiss collaboration looked at 52 six-year-olds who had been born prematurely, using MRI scans optimized to reveal tracts of neurons connecting brain regions 4 . Compared with children born at term, the premature children's neuronal tracts were organized less efficiently, often taking a more meandering path. These changes in organization were correlated with reduced social and cognitive skills. In another study, neonatologist Jeffrey Neil, then at St. Louis Children's Hospital in Missouri, and his team used functional MRI to study the premature brain at rest. The low-level, idling activity of a resting brain gives a read-out of its working connections, whose general topology is laid out before birth (see  Nature   489 , 356\u2013358; 2012 ). The team showed 5  that in babies born between 23 and 29 weeks of gestation, this 'resting-state connectivity' tends on average to be less complex and active at term-equivalent age than it is in full-term babies at birth. Another study \u2014 on the 26-year-old Bavarians \u2014 showed 6  that this reduced complexity of resting-state connectivity stretches into adulthood. Researchers agree that the most revealing studies would monitor the brains of premature babies and full-term comparison babies from as early as possible after birth, with follow-up scans and assessments throughout life. But such studies are difficult, and not only because it is hard to keep tabs on families who may move house, lose interest or lose touch over the years. Parents are rarely keen for their newborns \u2014 whether premature or full-term \u2014 to be whisked away into the loud and lonely chamber of a distant MRI machine without a burning medical reason. (In some countries, such as the Netherlands, it is illegal to do so.) And not all obstetricians are comfortable with subjecting delicate premature babies to brain scans at a medically and emotionally fraught time. Fabienne was happy for Hugo to be scanned, but recalls how painfully long it took to get from the paediatric ward to the scanning suite in her hospital. \u201cI felt half-dazed walking alongside Hugo in his incubator through a long underground tunnel to get there,\u201d she says. \u201cIt looked like the tunnel you are supposed to see when you are dying.\u201d A small vanguard of scientists and clinicians is pushing ahead, and several large long-term studies are under way around the world, collecting neurological, cognitive, behavioural and genetic data from birth, along with brain scans. In France, EPIPAGE 2 is now running, and has recruited more than 4,200 premature babies from all over the country 7 . In the United Kingdom, a team led by neonatologist David Edwards of King's College London has launched a study that will track children from their time  in utero  until they are two years old, collecting brain scans and blood samples along the way. Some of these children will inevitably be born prematurely, and the plan is to identify molecular signatures that might predict which of those infants are particularly vulnerable, or resistant, to altered neurodevelopment. Edwards' preliminary studies 8  on premature babies suggest that some genes \u2014 including several associated with lipid metabolism, which is crucial for white-matter development \u2014 may modify the risk of altered brain development. \u201cHaving a particular genetic profile might make certain babies less vulnerable,\u201d he says. \n               Brain protection \n             With scientists still working to identify the molecular, cellular and network differences in the premature brain, finding treatments seems a fond hope. But H\u00fcppi is attempting to do it. She is conducting a clinical study of erythropoietin, or EPO, a drug that stimulates the production of red blood cells. It is already a standard treatment to aid oxygenation of internal organs \u2014 not to mention being a favourite among endurance-sport cheats \u2014 and it is also thought to protect and support neurons. Anecdotal reports had suggested that erythropoietin might help long-term neurodevelopment, and H\u00fcppi's team is assessing this in a prospective, randomized and controlled study in nearly 500 very premature babies born in Switzerland, who are being MRI scanned at term-equivalent age. The first results, published in 2014, showed 9  that treated babies had fewer signs of neurological problems than did children in a control group. But the acid test, says H\u00fcppi, will come when they are assessed at two years old, when neurodevelopment has proceeded further. Where does all this leave parents, who still have to make decisions about their children's treatment with only limited information about the long-term prognosis? Some, such as Fabienne, can be helped by MRI scans that can detect damage in white and grey matter, and make it possible to predict the risk of severe brain damage more precisely than in the past. H\u00fcppi says that the technology helps doctors to advise parents, \u201cand it is a terrible responsibility if we are wrong\u201d. But this does little to identify which children will have milder developmental problems, or what those might be. Edwards and others think that brain imaging alone can never provide that type of information \u2014 but that combining scans with genetic and other molecular and clinical data may eventually lead to much greater precision. Should this become possible, it will throw open a whole new debate about how best to ameliorate any future problems for premature children, through very specific social and educational support \u2014 something that neuroscientists and education experts are only beginning to grapple with. Fabienne, like many parents of premature children, would like to have that information. Hugo, who is now seven, occupies most of her time. He has difficulties with fine movements, and some visual problems; he also needs a lot of extra assistance at school. Fabienne is deeply engaged with educational training programmes, which she hopes will be helpful, although she cannot know for sure. But Hugo is an unadulterated joy to her, and she is endlessly grateful for the MRI scans that were so crucial in the decision to save him. \u201cNeuroscience was able to say that Hugo would be able to have a reasonable quality of life,\u201d she says. And she monitors from a distance the new wave of scientific interest in the brains of premature babies. \u201cNeuroscience is coming up with a lot of good information \u2014 I really hope that they will soon translate what they are discovering into concrete actions that parents can usefully undertake.\u201d \n                     Cooling protects oxygen-deprived infants 2014-Jul-09 \n                   \n                     Neuroscience: Idle minds 2012-Sep-19 \n                   \n                     Tissue-bank shortage: Brain child 2011-Oct-26 \n                   \n                     Neuroscience: The most vulnerable brains 2010-Jan-13 \n                   \n                     WHO  Born Too Soon: The Global Action Report on Preterm Birth \n                   Reprints and Permissions"},
{"file_id": "518020a", "url": "https://www.nature.com/articles/518020a", "year": 2015, "authors": [{"name": "M. Mitchell Waldrop"}], "parsed_as_year": "2006_or_before", "body": "Automation is one of the hottest topics in transportation research and could yield completely driverless cars in less than a decade. This summer, people will cruise through the streets of Greenwich, UK, in electric shuttles with no one's hands on the steering wheel \u2014 or any steering wheel at all. The \u00a38-million (US$12-million) project, part of a larger study of driverless cars funded by the UK government, is just one of many efforts that seek to revolutionize transportation. Spurred in part by a desire to end the carnage from road accidents \u2014 about 90% of which are caused by driver error \u2014 the race is on to transfer control from people to computers that never doze at the wheel, get distracted by text messages or down too many pints at the pub. Almost every major car maker is working on some form of automation, as are many electronics companies. But looming over everyone is the Internet giant Google: the company has been widely acknowledged as the world leader in driverless-car research since October 2010, when it announced that it had entered the field a year earlier \u2014 and that its driverless test vehicles had already logged more than 200,000 kilometres on roads near its headquarters in Mountain View, California, and elsewhere in the state. The public's enthusiastic response to that revelation galvanized car makers and government research-funding agencies around the world to accelerate their efforts in this arena. \u201cI've never seen anything move so quickly from concept into products,\u201d says Richard Bishop, an automotive consultant who headed a US Department of Transportation research programme on automated motorways in the 1990s. Although many technical challenges remain, developers say they can see clear paths for solving most or all of them. At Google, for example, most of the driverless-car work so far has been carried out using standard passenger vehicles fitted with Global Positioning System receivers and mapping technology, along with radar to detect obstacles, a laser ranging system to scan the surroundings in three dimensions, and video cameras to identify objects such as traffic lights, construction signs, pedestrians and other vehicles (see 'A world of driverless cars'). The on-board computer \u2014 with processing power equivalent to several desktop units \u2014 integrates all the information and decides how the car should behave in any given situation. To lessen the load on the driving algorithms, Google equips the car with ultra-detailed maps that tell it exactly what to expect, down to the height of every curb. Sceptics point out that this mapping requirement restricts the car to places that have already been surveyed to that level of precision, such as Mountain View. But it would be relatively easy to expand those maps as part of the company's ongoing efforts to photograph the world's roadways for Google Maps, says Sebastian Thrun, an engineer who founded the Google car project and ran it until 2013. A tougher problem, says Thrun, is teaching the car how to respond to what he calls \u201cthe long tail of unlikely events\u201d. Early on, he says, the Google team developed algorithms for handling frequent, obvious challenges such as intersections or rain-slicked roadways. But as the cars drove for thousands of kilometres, they recorded oddball events such as a plastic bag blowing across the motorway or a couch sitting in the middle of the road. \u201cThere were many more of those than we believed in the beginning,\u201d says Thrun. The only way to handle such rare events has been to record them as they arise, devise responses with the help of high-powered machine-learning algorithms \u2014 and then test those solutions with simulations and yet more driving. \u201cIf we do it long enough,\u201d says Thrun, \u201cthe hope is that the software will be as safe as a human driver\u201d \u2014 and eventually much safer. How long that will take remains an open question. Google has publicly estimated about five years \u2014 but the company is currently not granting interviews about its project. Additional safety could come from equipping cars and trucks with Wi-Fi-like vehicle-to-vehicle (V2V) radios, which would allow them to warn each other of dangerous situations such as a car running through a red light. That would give both driverless and human-operated vehicles time to steer clear. Although V2V technology will probably be incorporated into driverless vehicles, it has been developed largely through separate efforts. The concept has already been road-tested as part of the European Union's Safe Road Trains for the Environment project, in which lines of cars followed bumper to bumper behind a truck, like ducklings tailing their mother. These road trains, or platoons, avoid catastrophic pile-ups because the V2V signals cues every car to hit the brakes at the same instant as the truck. And because of aerodynamics, the road trains saved at least 10% in fuel consumption. Such experiments have piqued the interest of the car maker General Motors, which last September announced it will support V2V technology in future models. At first, such cars will have few other vehicles to talk to. But the US National Highway Traffic Safety Administration is looking to issue regulations requiring V2V radios in all new US cars later this decade. \u201cThe vehicles would just be broadcasting a basic safety message: position, speed, direction of travel,\u201d says Josh Switkes, chief executive of Peloton Technology, a start-up firm in Menlo Park, California, that is seeking to commercialize V2V technology for heavy trucks. But that will be enough to eliminate many accidents, he says. Then there are the energy efficiencies \u2014 not just from platooning, he says, but from cars working together to minimize stop-and-go traffic, where fuel efficiency is dismal. If the cars can also ask smart traffic lights to adjust themselves to prevailing traffic density \u2014 a practice sometime known as Vehicle to Infrastructure (V2I) \u2014 then the system might be able to minimize the need to stop at all. Ultimately, the timescale for deploying these technologies will depend on the answers to much broader questions. How much will they cost? Who will own them \u2014 individuals, or service companies that provide transportation on demand? Who will face legal liability when a driverless car gets into an accident? And will people accept and trust them? Such questions can only be answered through experience. And given the pace of innovation today, experience is accumulating fast. See Editorial  page 6 \n                     Google car takes the test 2014-Oct-22 \n                   \n                     Street-Legal Robots 2014-Jul-15 \n                   \n                     Computer science: The learning machines 2014-Jan-08 \n                   \n                     Sustainable mobility: A vision of our transport future 2013-May-08 \n                   \n                     Google's Driverless Car project \n                   \n                     Sebastian Thrun talks about Driverless Cars \n                   \n                     The Greenwich GATEway project \n                   \n                     The EU SARTRE Project \n                   \n                     Peloton Technology \n                   Reprints and Permissions"},
{"file_id": "517426a", "url": "https://www.nature.com/articles/517426a", "year": 2015, "authors": [{"name": "Megan Scudellari"}], "parsed_as_year": "2006_or_before", "body": "By splicing animals together, scientists have shown that young blood rejuvenates old tissues. Now, they are testing whether it works for humans. Two mice perch side by side, nibbling a food pellet. As one turns to the left, it becomes clear that food is not all that they share \u2014 their front and back legs have been cinched together, and a neat row of sutures runs the length of their bodies, connecting their skin. Under the skin, however, the animals are joined in another, more profound way: they are pumping each other's blood. Parabiosis is a 150-year-old surgical technique that unites the vasculature of two living animals. (The word comes from the Greek  para , meaning 'alongside', and  bios , meaning 'life'.) It mimics natural instances of shared blood supply, such as in conjoined twins or animals that share a placenta in the womb. In the lab, parabiosis presents a rare opportunity to test what circulating factors in the blood of one animal do when they enter another animal. Experiments with parabiotic rodent pairs have led to breakthroughs in endocrinology, tumour biology and immunology, but most of those discoveries occurred more than 35 years ago. For reasons that are not entirely clear, the technique fell out of favour after the 1970s. In the past few years, however, a small number of labs have revived parabiosis, especially in the field of ageing research. By joining the circulatory system of an old mouse to that of a young mouse, scientists have produced some remarkable results. In the heart, brain, muscles and almost every other tissue examined, the blood of young mice seems to bring new life to ageing organs, making old mice stronger, smarter and healthier. It even makes their fur shinier. Now these labs have begun to identify the components of young blood that are responsible for these changes. And last September, a clinical trial in California became the first to start testing the benefits of young blood in older people with Alzheimer's disease. Science writer Megan Scudellari discusses the rejuvenating effects of young blood \u201cI think it is rejuvenation,\u201d says Tony Wyss-Coray, a neurologist at Stanford University in California who founded a company that is running the trial. \u201cWe are restarting the ageing clock.\u201d Many of his colleagues are more cautious about making such claims. \u201cWe're not de-ageing animals,\u201d says Amy Wagers, a stem-cell researcher at Harvard University in Cambridge, Massachusetts, who has identified a muscle-rejuvenating factor in young mouse blood. Wagers argues that such factors are not turning old tissues into young ones, but are instead helping them to repair damage. \u201cWe're restoring function to tissues.\u201d She emphasizes that no one has convincingly shown that young blood lengthens lives, and there is no promise that it will. Still, she says that young blood, or factors from it, may hold promise for helping elderly people to heal after surgery, or treating diseases of ageing. \u201cIt's very provocative,\u201d says Mark Mattson, chief of the Laboratory of Neurosciences at the US National Institute on Aging in Bethesda, Maryland, who has not been involved in the parabiosis work. \u201cIt makes you think. Maybe I should bank some blood of my daughter's son, so if I start to have any cognitive problems, I'll have some help,\u201d he says, only half-joking. \n               The power of two \n             Physiologist Paul Bert performed the earliest recorded parabiosis experiment in 1864, when he removed a strip of skin from the flanks of two albino rats, then stitched the animals together in hopes of creating a shared circulatory system 1 . Biology did the rest: natural wound-healing processes joined the animals' circulatory systems as capillaries regrew at the intersection. Bert found that fluid injected into a vein of one rat passed easily into the other, work that won him an award from the French Academy of Sciences in 1866. Since Bert's initial experiments, the procedure has not changed much. It has been performed on hydra \u2014 small freshwater invertebrates related to jellyfish \u2014 frogs and insects, but it works best on rodents, which recover well from the surgery. Up to the mid-twentieth century, scientists used parabiotic pairs of mice or rats to study a variety of phenomena. For example, one team ruled out the idea that dental cavities are the result of sugar in the blood by using a pair of parabiosed rats, of which only one was fed a daily diet of glucose. The rats had similar blood glucose levels owing to their shared circulation, yet only the rat that actually ate the sugar developed cavities 2 . Clive McCay, a biochemist and gerontologist at Cornell University in Ithaca, New York, was the first to apply parabiosis to the study of ageing. In 1956, his team joined 69 pairs of rats, almost all of differing ages 3 . The linked rats included a 1.5-month-old paired with a 16-month-old \u2014 the equivalent of pairing a 5-year-old human with a 47-year-old. It was not a pretty experiment. \u201cIf two rats are not adjusted to each other, one will chew the head of the other until it is destroyed,\u201d the authors wrote in one description of their work 4 . And of the 69 pairs, 11 died from a mysterious condition termed parabiotic disease, which occurs approximately one to two weeks after partners are joined, and may be a form of tissue rejection. Today, parabiosis is performed carefully to reduce animal discomfort and mortality. \u201cWe observe the mice at length and have long discussions with our animal-care committee,\u201d says Thomas Rando, a Stanford neurologist who has used the procedure. \u201cWe don't take this lightly.\u201d Mice of the same sex and size are socialized with each other for two weeks before attachment, and the surgery itself is done in a sterile setting with anaesthesia, heating pads and antibiotics to prevent infection. Using inbred lab mice, genetically matched to one another, seems to reduce the risk of parabiotic disease. Joined mice eat, drink and behave normally \u2014 and they can be separated successfully. In McCay's first parabiotic ageing experiment, after old and young rats were joined for 9\u201318 months, the older animals' bones became similar in weight and density to the bones of their younger counterparts 5 . More than 15 years later, in 1972, two researchers at the University of California studied the lifespans of old\u2013young rat pairs. Older partners lived for four to five months longer than controls, suggesting for the first time that circulation of young blood might affect longevity 6 . Despite these intriguing findings, parabiosis fell out of use. Those who have studied the technique's history speculate that researchers thought they had learned all they could from it, or that the bar for getting institutional approval for parabiosis studies had become too high. Whatever the reason, the experiments stopped. That is, until a stem-cell biologist named Irving Weissman brought parabiosis back to life. \n               Back to the source \n             Weissman learned to join mice together at the age of 16, under the supervision of a hospital pathologist in the small town of Great Falls, Montana, in 1955. His supervisor was studying transplantation antigens, proteins on the surface of transplanted cells or tissues that determine whether they are accepted or rejected by the host. Weissman remembers adding a fluorescent tracer to the blood of one mouse in a pair and watching it go back and forth between the animals. \u201cIt was really amazing,\u201d he says. He went on to spend three decades studying stem cells and regeneration in natural parabionts, sea squirts of the species  Botryllus schlosseri . In 1999, Wagers, then a new postdoctoral fellow in Weissman's Stanford lab, wanted to study the movement and fate of blood stem cells, so Weissman recommended that she use parabiotic mice and fluorescently label the cells she wanted to track in one animal of a pair. Wagers' experiments led to two rapid-fire discoveries on the nature and migration of blood stem cells 7 , 8 . It also inspired her Stanford neighbours. In 2002, Irina Conboy, a postdoctoral fellow in Rando's lab, presented one of Wagers' papers at a journal-club meeting. Michael Conboy, Irina's husband and a postdoc in the same lab, was dozing in the back of the meeting room. The mention of stitching mice together jolted him awake. \u201cWe had been in discussion for years that ageing seems to be all cells in the body, that all tissues seem to go to hell in a handbasket together,\u201d says Michael. Yet they had been unable to think of a realistic experiment with which to investigate what coordinates ageing throughout the body. \u201cI thought, 'Hey wait, they're sharing blood,'\u201d says Michael. \u201c'This could answer that question we've been asking for years.'\u201d At the end of the presentation, he ran up to Irina and Rando. He had not even finished his pitch before Rando said: \u201cLet's do it.\u201d The researchers teamed up with Wagers, who performed the old\u2013young pairings for the experiment and taught Michael the technique (see 'Share and share alike'). Rando says that he did not expect the experiment to work, but it did. Within five weeks, the young blood restored muscle and liver cells in the older mice, notably by causing aged stem cells to start dividing again 9 . The team also found that young blood resulted in enhanced growth of brain cells in old mice, although the work was left out of their 2005 paper describing the results. All in all, the results suggested that blood contains the elusive factor or factors that coordinate ageing in different tissues. After the team published its results, Rando's phone started ringing incessantly. Some of the calls were from men's health magazines looking for ways to build muscle; others were from people fascinated by the prospect of forestalling death. They wanted to know whether young blood extended lifespan. But despite the hints that this was true from the 1970s, no one has yet properly tested the idea. It would be an expensive, labour-intensive experiment. Instead, members of the original research team branched out into separate efforts to determine what exactly in the blood is responsible for the rejuvenating effects. In 2008, Irina and Michael Conboy, by then at the University of California, Berkeley, linked 10  muscle rejuvenation to the activation of Notch signalling \u2014 which promotes cell division \u2014 or to the deactivation of the transforming growth factor (TGF)-\u03b2 pathway, which blocks cell division. Then, in 2014, they identified 11  one of the age-defying factors circulating in the blood: oxytocin, a hormone best known for its involvement in childbirth and bonding, and already a drug approved by the US Food and Drug Administration for inducing labour in pregnant women. Oxytocin levels decline with age in both men and women, and when injected systemically into older mice, the hormone quickly \u2014 within a couple of weeks \u2014 regenerates muscles by activating muscle stem cells. \n               All the organs \n             Wagers was following up on the anti-ageing work at Harvard, where she had started her own lab in 2004. She recruited the help of experts in various organ systems to help her to evaluate the impact of young blood on their respective tissues. With neuroscientist Robin Franklin at the University of Cambridge, UK, her team showed 12  that young blood promotes repair of damaged spinal cords in older mice. With Harvard neuroscientist Lee Rubin, she found 13  that young blood sparks the formation of new neurons in the brain and olfactory system. And with cardiologist Richard Lee at Brigham and Women's Hospital in Boston, Massachusetts, she found 14  that it reverses age-related thickening of the walls of the heart. With Lee, Wagers began screening for proteins that were particularly abundant in young blood but not old blood. One leapt out at them: growth differentiation factor 11, or GDF11. Wagers and Lee showed 14  that direct infusions of GDF11 alone were sufficient to physically increase the strength and stamina of muscles, as well as to reverse DNA damage inside muscle stem cells. No mouse studies outside of Wagers lab have yet replicated the finding, but a similar protein in fruit flies extends lifespan and prevents muscular degeneration 15 . It is perhaps fitting that parabiosis' newfound popularity has spread among labs with close ties. Wyss-Coray, who worked in the room next to Rando's lab, had previously discovered prominent changes in levels of proteins and growth factors in the blood of ageing humans and people with Alzheimer's disease. Following up on Rando's unpublished brain results, he used old\u2013young mouse pairs to show 16  that old mice exposed to young blood did indeed have increased neuron growth, and that young mice exposed to old blood had reduced growth. Plasma alone had the same effects. \u201cWe didn't have to exchange the whole blood,\u201d says Wyss-Coray. \u201cIt acts like a drug.\u201d Next, the team looked at overall changes in the brain, and found that young plasma activates brain plasticity and memory formation in older mice, and increases learning and memory. \u201cWe could not believe that this worked,\u201d says Wyss-Coray. Neither could the reviewers. The first time Wyss-Coray submitted the work to a journal, it was rejected, he says, responding that it was too good to be true. So his team spent a year repeating the experiments at the University of California, San Francisco \u2014 a different facility with different staff, instruments and tools. The researchers got the same results. \u201cAfter that, I was really reassured,\u201d says Wyss-Coray. \u201cI'm convinced it works.\u201d His research, published last May 17 , caught the attention of a company in Hong Kong owned by a family with a history of Alzheimer's disease, which is characterized by neuron loss. One family member's condition had reportedly temporarily improved after they received a plasma transfusion. So the company put forward the initial funding to translate Wyss-Coray's approach to human clinical trials. Wyss-Coray formed a start-up company, Alkahest in Menlo Park, California, and in September 2014 it began a randomized, placebo-controlled, double-blind trial at Stanford, testing the safety and efficacy of using young plasma to treat Alzheimer's disease. Six out of a planned 18 people with Alzheimer's, all aged 50 or above, have already begun to receive plasma harvested from men aged 30 or younger. In addition to monitoring disease symptoms, the researchers are looking for changes in brain scans and blood biomarkers of the disease. \n               Bad blood? \n             Wagers is eager to see the results, but she worries that a failure would be difficult to interpret and so could set the whole field back. Plasma from a 30-year-old donor may not contain factors beneficial to patients with Alzheimer's, for example. She, Rando and others would prefer to see testing for a specific blood factor or combination of known factors synthesized in the lab, for which the mechanism of action is fully understood. There are also lingering concerns as to whether activating stem cells \u2014 which is what the young blood most often seems to do \u2014 over a long period of time would result in too much cell division. \u201cMy suspicion is that chronic treatments with anything \u2014 plasma, drugs \u2014 that rejuvenate cells in old animals is going to lead to an increase in cancer,\u201d says Rando. \u201cEven if we learn how to make cells young, it's something we'll want to do judiciously.\u201d Michael Conboy is concerned for another reason: he has seen enough paired mice die of parabiotic disease to be cautious about trying it in humans. \u201cI would be leery\u201d of any trial in which significant amounts of blood or plasma were transfused into an older person regularly, he says. Alkahest's chief executive, Karoly Nikolich, says that he understands the safety concerns, but he emphasizes that millions of blood and plasma transfusions have been carried out safely in humans. The initial Alkahest study is expected to conclude by the end of this year, and the company plans to initiate further studies testing young plasma in the treatment of different types of dementia and age-related conditions. All the caution over young blood is justified, given the history of dashed hopes in the anti-ageing field. In the past two decades, researchers have identified the anti-ageing properties of numerous treatments, including calorie-restricted diets; resveratrol, a chemical found in the skin of grapes; telomerase, an enzyme that protects the integrity of chromosomes (see Books & Arts, page 436); rapamycin, an immune-suppressing drug that extends lifespan in mice; and stem cells, which decline in function and number as people age. Only two of these \u2014 caloric restriction and rapamycin \u2014 have been shown to reliably slow or reverse the effects of ageing across many mammalian tissue types, but neither has turned into an anti-ageing treatment. The former has produced conflicting results in primates; the latter has toxic side effects. Young blood, by contrast, seems to turn back the effects of ageing, potentially with few known safety concerns in humans and, so far, with corroborated results from parabiotic ageing studies in multiple labs. But scientists and ethicists still worry about the treatment being tried in people outside approved clinical trials before evidence on its safety and effectiveness is in. Unlicensed stem-cell transplants are already a booming industry, warns Mattson, and unlicensed transfusion of young blood would be even easier. \u201cYou often have these lucrative markets emerge on a slender foundation of credible work,\u201d says Leigh Turner, a bioethicist at the University of Minnesota in Minneapolis who has studied the anti-ageing field. For now, any claims that young blood or plasma will extend lifespan are false: the data are just not there. An experiment to test such claims would take upwards of six years \u2014 first waiting for the mice to age, then for them to die naturally, then analysing the data. \u201cIf we had funding to do this, I'd do it. But we don't,\u201d says Michael Conboy. Still, he adds, \u201cI hope that someone, somewhere is.\u201d \n                     Pet dogs set to test anti-ageing drug 2014-Oct-29 \n                   \n                     Medical research: Treat ageing 2014-Jul-23 \n                   \n                     Biomarkers and ageing: The clock-watcher 2014-Apr-08 \n                   \n                     Blood hormone restores youthful hearts to old mice 2013-May-10 \n                   \n                     Ageing: Much ado about ageing 2010-Mar-24 \n                   \n                     Blog post: ORI: former Harvard postdoc guilty of misconduct \n                   Reprints and Permissions"},
{"file_id": "518154a", "url": "https://www.nature.com/articles/518154a", "year": 2015, "authors": [{"name": "Elizabeth Gibney"}], "parsed_as_year": "2006_or_before", "body": "Shape it, squeeze it, energize it or tie it into knots. Scientists are taking light to new extremes. \n               SHAPING LIGHT \n             \n               Miles Padgett twists light in unexpected ways. \n             Physicist Miles Padgett starts to describe the concept of twisted light by taking down a rainbow-coloured spiral that hangs from the ceiling of his office at the University of Glasgow, UK. Then he stops and scours the room for more props: dinner plates, paper, pencils and even leftover Christmas chocolates. Light is made of oscillating electric and magnetic fields, he explains. In a conventional laser beam, the oscillations are always in step, with the peaks and troughs lined up from one side of the beam to the other. (Padgett illustrates the flat, planar waves with a stack of dinner plates that he moves face-forward.) But things get more interesting when parts of the beam fall out of step. This is where Padgett points to the spiral: the peaks of the wavefront can be manipulated to the point at which they curl around the beam's direction of motion in a corkscrew. This is twisted light, says Padgett, who has spent two decades learning to exploit its unique properties. He has pioneered applications that range from moving cells without physically touching them to packing lots of information into an optical signal \u2014 and even tying light in knots. In the process, he has developed a rare instinct for the subject, say collaborators and colleagues. \u201cMany other scientists might need to do a calculation, run a model or do an experiment before they can get an idea about how light should behave,\u201d says Mark Dennis, a theoretical physicist at the University of Bristol, UK. \u201cOne of Miles's great talents is having this knack at being able to anticipate what the results should be.\u201d Props are not the only thing in Padgett's office. It houses the lab's coffee machine, and doubles as its kitchen and common room \u2014 complete with sink. Padgett is a fan of productive chance encounters, and likes to keep the place buzzing with people picking each other's brains. It was a chance encounter that led him to twisted light in the first place. In 1994, as a research fellow at the University of St Andrews, UK, he had dinner with physicist Les Allen intending to discuss laser technology. But the conversation turned to Allen's experiments with twisted light 1 . Allen, then at the University of Essex in Colchester, UK, baited Padgett by saying that he knew how to give the light its twist using the stem of his wine glass as a lens. This strange idea had Padgett hooked. By 1997, he and his colleagues had not only learned how to make twisted light for themselves, but had also devised a way for it to function as an 'optical spanner' to trap cells and other microscopic particles, then rotate them into any position 2 . Turning light into a spanner is really about shaping it, Padgett says. A very simple example of shaping is a digital projector, which creates a changing image by altering a beam's intensity pixel by pixel. A more sophisticated example is a liquid-crystal device that does nothing to the intensity of the light passing through each pixel, but instead shifts its 'phase' \u2014 the relative position of the wave's peaks and troughs. In the stacked-dinner-plate analogy, the plates collectively warp and bend. Getting to twisted light is a matter of taking that warping to extremes \u2014 so that the wavefronts form a spiral. That twist means that the beam not only exerts radiation pressure on the objects it encounters, nudging them forward, but also tries to rotate them. \u201cIt's just like turning and pushing a door knob to open a door,\u201d says Padgett. The optical spanner passes this momentum to microscopic objects to trap, rotate and move them. Using such devices, biologists can bump beads into cells to measure the cells' stiffness, and engineers can create unique nanoscale materials. Twisted light also provides a new way to encode information. The conventional approach to doing this with light is to encode each bit as a single photon spinning either clockwise or anticlockwise around its direction of motion. Quantum mechanics allows only those two possibilities, so this gives a natural way to represent the 1s and 0s of binary code. But twisted light has an extra rotational quantity known as orbital angular momentum. This differs from intrinsic spin in the same way as Earth's yearly motion around the Sun differs from its daily rotation on its axis. And it is much less constrained by quantum mechanics. In theory, says Padgett, twisted light can have an infinite number of orbital angular momentum patterns, or modes, each twisted tighter than the last. \u201cThis is like having a whole alphabet with which to communicate,\u201d he says. A decade ago, Padgett was among the first to show that each mode can be used to encode different information 3  \u2014 such as shades of grey or numbers \u2014 which allows much more data to be carried by the same optical signal than is possible with just spin encoding. Last year, a team at the University of Vienna encoded grey-scale images of Wolfgang Amadeus Mozart and other famous Austrians using 16 twisted modes, and successfully sent the images through 3 kilometres of air 4  (see  Nature   http://doi.org/ztt ; 2014 ). By using extra channels of information, such techniques could increase the data-carrying capacity of fibre-optic cables and radio waves.  This is like having a whole alphabet with which to communicate.  Padgett has found even more imaginative ways to play with twisted light. When a beam of it illuminates a wall, for example, the spot will have a dark centre. That is because a spinning beam of light has a vortex in the middle where intensity is zero. Look closely at a spot of laser light, says Padgett, and it seems to be riddled with such dark spots, known as speckles. If you could trace these spots back through the laser beam, they would form continuous lines of zero intensity twining in three dimensions 5 . \u201cThese can be like cooked spaghetti, or you can form them into spaghetti hoops or even chain mail,\u201d says Padgett. (He points to a poster on his wall showing what that looks like: its title is 'Speckleghetti'.) In 2010, he and his collaborators showed how to form the lines into knots 6 . It took theorist Mark Dennis at the University of Bristol, UK, a decade to create the complex mathematical recipe of overlapping beams needed to make an isolated, pretzel-like knot. Only with the recipe in hand could Padgett's team use its light-shaping skills to make the abstract mathematics a physical reality. Padgett believes that the best way for a person to succeed is for them to find something they are good at, and then to apply it everywhere. \u201cOur team can shape light beams,\u201d he says. \u201cSo we use shaped light in communications, microscopy, in imaging, in sensors. We always ask, how can we apply what we know to areas that others are interested in?\u201d He is using that philosophy in his latest project: leading the Quantum Imaging Hub, a collaboration between 6 universities and 30 companies, which is one of the 4 Quantum Technology Hubs launched last November by the UK government. His group is creating infrared cameras that use a single-pixel detector rather than the millions of expensive pixels in a conventional camera. By projecting masks of black and white squares onto an object, flickering 20,000 times a second, the team can measure how incoming intensity varies, and reconstruct a picture 7 . \u201cIt's a convoluted, but much cheaper, way of doing the job,\u201d says Matthew Edgar, a physicist in Padgett's lab. With image-compression techniques and boosted computer power, the team hopes to extend the technique to video, allowing infrared cameras to spot gas leaks or see through smoke. Back in his office and packing up to head into the Glasgow rain, Padgett reflects on what he loves about light. It is not its endless uses. Instead, he says, the beauty of light is that the more deeply you understand it, the more straightforward it gets. \u201cIf light ever surprises me, it's not in its complexity, but that it is so simple,\u201d he says. \n               SQUEEZING LIGHT \n             \n               Pierre Berini harnesses light at the nanoscale using \u2018plasmons\u2019. \n             Pierre Berini knows a bargain when he sees one; the evidence is in his lab, which is cluttered with lasers, oscillators and other components that he bought at auctions after local companies folded. The University of Ottawa physicist often buys in batches, after spotting essential items in a job lot that otherwise looks like junk. \u201cThere are lots of surprises,\u201d he says. Berini has a certain sympathy for the failed companies. He is a leader in plasmonics, a way of manipulating electrons with light that could be used to transmit information in super-fast computers. Months after launching a venture-backed firm, Spectalis, to market plasmonics circuitry to the communications industry in the early 2000s, he began to feel the effects of the dot-com bubble bursting. He ended up hosting an auction of his own and closing shop. Unperturbed, he plans to try again this year, launching a company to apply his technology to tiny sensors in handheld devices that detect diseases rapidly and with extreme sensitivity. The devices use a peculiar kind of light that emerges from waves of electrons propagating across a metal surface in contact with an insulator, such as air or glass. When excited with a laser, these charges, or plasmons, generate fluctuating electric and magnetic fields that flow just above the metal surface. Trapped at the interface, the waves can be funnelled into structures that confine their wavelengths to a few tens of nanometres \u2014 as little as one-tenth of the laser's wavelength. The squeezed waves travel more slowly than laser light, so can retain the same frequency. Berini backed into studying plasmonics while looking for ways to improve normal electrical components and photo detectors in the late 1990s. Light travels much faster than electrical signals, so using it to connect silicon chips would massively speed up calculations. But light is limited by its wavelength: although electronic devices can be shrunk to a few tens of nanometres, the infrared light used in telecommunications cannot focus to spots much smaller than a micrometre. \u201cIt's a fundamental incompatibility,\u201d says Berini. The smaller wavelengths available with plasmons looked promising, but plasmonic light does not always behave. The waves, created by the movement of electrons, decay quickly as a result of resistance in the metal, and they travel only micrometres. Berini used tools that can craft nanoscale structures, which were becoming cheaper and more readily available, to create the first plasmonic waves that could travel for centimetres (ref.  8 ). His lab made whole circuits, guiding plasmons down metal strips less than 30 nanometres thick. But allowing the waves to travel farther increases the light's wavelength. Although plasmonic waves are still smaller than conventional light waves, the compromise lessened their advantage and Berini found it tough to crack the telecommunications industry, where each component in use had been honed over decades. So he and others have been busy developing other techniques to deal with the short range of plasmonic light, either branching out into applications that turn the loss into an advantage, such as photodetectors, or by using nanostructures to amplify the waves. Physicists are now developing an assortment of nanoshapes \u2014 stars, rods and crescents \u2014 in a range of materials that could harness these waves for applications such as capturing solar energy, killing cancer cells and creating chip-integrated lasers, known as spasers. Henry Schriemer, a physicist at the University of Ottawa, calls Berini a \u201cquintessential experimentalist with a deep appreciation for the theory\u201d. But Berini says that it is applications that turn his lab on; he attributes this entrepreneurial bent to his parents, who ran their own businesses in Timmins, the Ontario mining and logging community where he grew up. Today, Berini is recycling the efforts made in long-range circuits to make a detector for dengue fever. The device, a handheld biosensor developed last year with researchers at the University of Malaya in Kuala Lumpur, sends plasmon waves down a chip scattered with dengue virus particles. A blood sample is placed on the chip; if the donor has the infection, the sample will contain antibodies that bind to the virus, disrupting the wave and producing a signal 9 . Berini says that the sensors could speed up diagnosis, which normally involves sending samples away to a lab. A new company is now in the works to commercialize a range of similar biosensors. But Berini believes that the application is just one of many that squeezed light will have in the future. \u201cWith plasmonics, there is a lot of new physics to be uncovered,\u201d he says. All of which means that some of the random equipment that litters the lab might find a new use. \n               FAST LIGHT \n             \n               Margaret Murnane creates ultrashort laser pulses on a tabletop. \n             When Margaret Murnane was growing up in rural County Limerick, Ireland, in the 1960s, she had no talent for activities considered suitable for girls, such as sewing or art, and never thought of herself as being good with her hands. What she did enjoy was going on long walks with her father, and gazing at rain-drenched Ireland's multitude of rainbows \u2014 an activity that led to a lifelong fascination with light. In following that passion, she says, \u201cit turned out I have a talent I never knew for aligning lasers. But in normal life, how would you ever know?\u201d Murnane's life is now that of a physicist at JILA in Boulder, Colorado, a joint institute between the University of Colorado and the US National Institute of Standards and Technology. There, with husband Henry Kapteyn, she runs a lab that is leading development of an X-ray laser that strobes in attosecond pulses, each blast lasting just one-billionth of one-billionth of a second \u2014 almost the same proportion of a second as that second is of the entire age of the Universe. Such ultrafast X-rays, which have tiny wavelengths and high energies, are often used to penetrate deep into atoms and image them at the nanometre scale. Usually, this happens at billion-dollar facilities that generate X-rays by accelerating electrons to near light speeds, such as the SLAC Linac Coherent Light Source in Menlo Park, California. By contrast, Murnane's set-up fits on a dining-room table. It allows scientists to watch the movement of electrons around atoms, probing chemical bonds or studying spins in a magnetic hard drive. Murnane's background \u2014 a childhood spent without central heating or indoor plumbing, but with a love of knowledge and learning \u2014 lies behind much of her drive, says Kapteyn. \u201cShe worked her way up,\u201d he says. Murnane met Kapteyn as a graduate student at the University of California, Berkeley, and the two have worked together ever since \u2014 forming a stable partnership that Murnane believes underlies their scientific success. \u201cIt helps to have someone who will challenge you hard. Those relationships are good for science, but difficult for individuals to learn,\u201d she says.  She is really able to push the envelope of what is possible, year after year.  Together they tackled a problem that they first attempted in graduate school \u2014 how to generate laser-like light beams at high energies. Rather than accelerating electrons, as huge facilities do, their strategy was to combine many visible-light photons into a handful of higher-energy X-ray photons. The process has an analogy with sound. In stringed instruments, plucking a string gently generates a single tone. \u201cIf you pluck it harder and harder, higher harmonics emerge,\u201d says Murnane, each at larger integer multiples of the original frequency. When ultrashort-pulse lasers were developed in the 1990s, Murnane and Kapteyn realized that they might be able to use them to 'pluck' an electron violently \u2014 accelerating it away from and back towards an atom of helium \u2014 and thereby generate harmonics in the form of higher-energy photons. The team succeeded in making bright ultraviolet beams 10 , but it was more difficult to increase the energy while keeping the beam laser-like, with the waves emerging in synchrony. Murnane often says that she picked physics \u201cbecause it was the hardest subject\u201d at university \u2014 an attitude that stood her in good stead with this challenge, which took 15 years to solve. The solution was to engage in what she calls \u201ca very different way of thinking\u201d, and start not with visible-light lasers, but with longer-wavelength infrared lasers. The photons had much less energy than before. But they resonated much more strongly with the electrons in the helium atoms \u2014 in effect, giving the string a much stronger pluck \u2014 which allowed the team to combine more than 5,000 laser photons into a single X-ray photon. Theorists believed that the technique would be too inefficient to make usable beams. But by carefully tuning the helium gas so that the laser and X-rays travelled at the same speed, Murnane's team predicted, then proved, that the X-rays would emerge in step, as a bright beam 11 . \u201cWhat was amazing was not just that they got the X-rays, but that they got plenty of them,\u201d says Mikhail Ivanov, a physicist at the Max Born Institute for Nonlinear Optics and Short Pulse Spectroscopy in Berlin. Murnane and Kapteyn have now made ultrafast lasers that produce X-rays of up to 1,000 electronvolts in energy, and in attosecond pulses. Although these devices do not reach the energies or brightness attained at the big free-electron laser facilities, they come close. And, at US$1 million, they are around one-thousandth of the price. The lab at JILA has eight such lasers, and discoveries in the nano-world are starting to trickle in. Murnane both builds and uses the lasers \u2014 processing the X-ray scatter patterns to capture images of charge and spin flows within materials. One counter-intuitive finding is that nanometre-sized heat sources cool quicker when packed closer together 12 . Murnane, together with collaborators including Kapteyn and Tenio Popmintchev at JILA and Andrius Baltuska at the Vienna University of Technology, is still working on refining the desktop set-up to make it even faster, more energetic and smaller. That would allow them to probe even quicker processes, deeper within materials and with higher resolution. \u201cWe're pretty optimistic we can do it,\u201d says Murnane. After visible lasers were invented in 1960, they underwent rapid development; the same revolution is now happening for tabletop X-ray sources. Other labs around the world have developed similar approaches, says Olga Smirnova, a theorist at the Max Born Institute. But what makes the JILA technique stand out is the ability to produce such high-frequency light, with such efficiency. And then there is Murnane herself, says Smirnova: \u201cShe is really able to push the envelope of what is possible, year after year.\u201d Murnane insists that they have not reached the limit yet \u2014 that higher-energy X-rays and even faster, zeptosecond (10 \u221221  s) pulses may be possible. \u201cA misconception in science sometimes is that lasers are now an old technology, and there's nothing new to learn,\u201d she says. \u201cThat's so far from the truth. \n                     Twisted light sends Mozart image over record distance 2014-Nov-12 \n                   \n                     Entangled photons make a picture from a paradox 2014-Aug-27 \n                   \n                     Ultrafast phenomena: Attosecond beacons 2012-Dec-21 \n                   \n                     Tabletop X-rays light up 2012-Jun-08 \n                   \n                     View from ... Advanced Photonics Congress 2011: Harnessing plasmonics on a chip 2011-Aug-30 \n                   \n                     Engineers devise invisibility shield 2005-Feb-28 \n                   \n                     Nature  special: Light \n                   \n                     Nature  collection: year lof Light \n                   \n                     International Year of Light \n                   \n                     University of Glasgow optics group \n                   \n                     Kapteyn-Murnane group \n                   \n                     Berini group \n                   Reprints and Permissions"},
{"file_id": "517430a", "url": "https://www.nature.com/articles/517430a", "year": 2015, "authors": [{"name": "Ann Finkbeiner"}], "parsed_as_year": "2006_or_before", "body": "By firing lasers into the sky, Claire Max has transformed the capabilities of current \u2014 and future \u2014 telescopes. On clear, moonless evenings, most of the biggest optical telescopes around the world begin the night's observations by firing a golden laser beam at the sky. Claire Max does not like to take credit for this astronomical light show, even though the lasers' widespread use is a tribute to her three-decade campaign to perfect and promote them \u2014 an effort that was recognized on 16 January when the American Astronomical Society awarded her its 2015 instrumentation prize. For Max, an astronomer at the University of California, Santa Cruz, self-aggrandizement would be \u2014 unbusinesslike. And she is all business; even her way of speaking is careful, like someone who feels obliged to stand behind every word she says. Her passion is reserved for the technology itself. \u201cI still get gripped by it,\u201d she says, showing off photograph after photograph of telescopes, lasers and thin beams of light shining upwards as straight as a ruler. The lasers, Max explains, are a crucial element of the telescopes' adaptive optics, which correct for turbulence in the atmosphere. Without adaptive optics, stars and galaxies viewed at high magnification will dance, distort and blur like stones seen at the bottom of a stream. With adaptive optics, they will remain steady and sharp, allowing telescopes on the ground to routinely equal or exceed the clarity obtained by NASA's Hubble Space Telescope. This capability has allowed current-generation telescopes to carry out high-resolution studies of objects ranging from moons in the outer Solar System to stars at the centre of the Milky Way. And now it is enabling the construction of telescopes measuring 20\u201340 metres across, as much as four times the diameter and 16 times the light-gathering power of any now in existence. Max has been involved in this development from its early days: from the first demonstration of laser-assisted adaptive optics to building the prototype and then establishing a centre that spread the technology to telescopes around the world. Yet Max's greatest triumph has also become her greatest challenge. Last October, at an age when other astronomers might be looking forward to retirement, the 68-year-old Max agreed to serve as interim director of the University of California Observatories (UCO) \u2014 the organization responsible for all the astronomical hardware owned by one of the biggest state university systems in the United States. And in that role, 'interim' or not, Max finds herself navigating the professional and cultural chaos in astronomy being triggered by the cost of these next-generation behemoths. There are three of these telescopes in various stages of planning and construction, each with a price tag in the order of US$1 billion. That cost, says Max, poses a quandary for their owners and funders \u2014 among them the UCO, a key partner in the Thirty Meter Telescope (TMT) that started construction last year atop Mauna Kea in Hawaii. How do they pay for all their older, smaller telescopes? Should the owners give in to financial pressure and close the facilities \u2014 even though the telescopes are still essential workhorses for individual researchers and training grounds for young astronomers? Or should they fight to find creative ways to keep all the doors open? Max's instinct is to fight \u2014 using her unique combination of warmth, empathy and determination. So far, she is winning. After three decades of persuasion and consensus-building in pursuit of adaptive optics, says Andrea Ghez, an astronomer at the University of California, Los Angeles, Max has developed a sure instinct for making connections among engineers, academics, funding officers, university administrators and all the others who have a say in telescope decisions. These are powerful players, says Ghez \u2014 \u201cgorillas at the table who'd like you for lunch\u201d. And to deal with them, she says, you need someone like Max: \u201ca gorilla with finesse\u201d. \n               First light \n             Max took her first look through a telescope in the early 1950s, when the Manhattan native was just 8 years old. \u201cAnd that was it,\u201d she says. \u201cI can still close my eyes and see the mountains of the Moon. So I became an astronomy nut.\u201d That passion led her first to Princeton University in New Jersey, where she earned a PhD in astronomy; then to a postdoctoral appointment at the University of California, Berkeley; to a staff position at the Lawrence Livermore National Laboratory in California; and in 1983, to membership in the Jasons: a group of scientists who meet from time to time to give technical advice on national security, often for the US Department of Defense. The first Jason study she joined was motivated by the US Air Force's desire to identify potentially hostile satellites \u2014 a task for which the atmosphere was just as big a barrier as it was for astronomers. Even with clear skies and a good telescope, turbulence smears out details smaller than about 1 arcsecond in angular diameter \u2014 good enough to look up at the Hubble telescope, which is similar in size and altitude to spy satellites, and tell that it is a cylinder, but not much else. Astronomers had already come up with a potential solution: a flexible mirror that could reflect the light coming into the telescope and deform under computer control. In principle, the distortions introduced by the mirror would exactly cancel out those produced by the atmosphere, restoring the image to near-perfection. But first, the distortion had to be measured, preferably by looking at what the atmosphere did to a bright 'guide' star near the target. And bright stars were not always available near the fast-moving targets of interest to the Pentagon. That is why the Air Force had asked the Jasons for help \u2014 which Max and her colleagues provided in a classified report. Just shine a laser upwards along the telescope's axis, they said. If the laser were tuned to the correct wavelength, the beam would then encounter a naturally occurring layer of sodium atoms floating in the atmosphere about 90 kilometres up and cause the sodium to fluoresce, producing a bright-yellow spot visible from the ground \u2014 in effect, a guide star available anywhere in the sky (see 'Untwinkle the stars'). But Max went further. Knowing that the sodium laser guide star would also be invaluable for astronomers, she came up with an additional design that the Air Force had not asked for, but was better adapted to research needs. \u201cI thought it was so cool,\u201d she says. Unfortunately, that design was classified along with all the other sodium laser guide-star technology. So Max, together with sympathetic Air Force scientists who knew the proper channels to follow, spent the next seven years lobbying the military to take the wraps off. And they won: in May 1991, Air Force scientist Robert Fugate was allowed to describe the artificial guide star at an open meeting of the American Astronomical Society in Seattle, Washington. Reactions were mixed: astronomers who had been working on similar technology but did not have the money to get very far were annoyed to learn that the military had already built it in secret. But most were just excited at the prospect of getting blur-free images anywhere they wanted. \u201cIt was just pandemonium,\u201d recalls Fugate. They asked \u201ca million questions\u201d. For all the astronomers' enthusiasm, however, the system was still technically demanding, expensive and in need of development that universities could not afford. \u201cPeople were writing about it,\u201d Max says, \u201cbut they weren't putting it on telescopes.\u201d So shortly after the declassification, Max decided that astronomers needed a proof-of-principle. The idea for how to do that hit during lunch with a colleague, Herb Friedman. \u201cWe looked at each other,\u201d she says, \u201cand we said, 'Well crap, we're from Livermore, we do lasers'.\u201d Indeed, the laboratory had an enormous underground laser that was normally used to separate isotopes, but could be tuned to sodium wavelengths. Max and Friedman therefore arranged to have an access cover removed from the roof of the laser tunnel, and a mirror installed to bounce the horizontally pointing laser light through the hole into the sky. Then they set up a small telescope beside the beam to look for the artificial guide star and measure the atmospheric distortions. The set-up must have looked eerie in the darkness, says Max \u2014 the tent that covered the telescope glowed yellow with stray laser light; the beam itself could be seen for ten kilometres or more \u2014 and one local woman kept calling the police to report that a UFO was sucking up all Livermore's secrets. But their demonstration did prove that the design would work the way that Max expected. The next step was to master the complex optics and engineering required for the laser system to function on a real telescope. A prototype that Max and her colleagues deployed at the UCO's Lick Observatory in San Jose in the mid-1990s eventually showed that, at least at longer wavelengths, the system allowed the observatory's 3-metre telescope to reach the finest-possible resolution permitted by the wave nature of light (D. T. Gavel  et al .  Proc. SPIE   4007 , 63\u201370; 2000 ). But even that failed to persuade astronomers to embrace the technology. At the time, people thought the laser guide star was \u201cso complicated it would never run in harness\u201d with other astronomical instruments, says Connie Rockosi, one of Max's colleagues. So Max decided that the technology needed \u201ca community of practice\u201d \u2014 a central home in which users could learn how to build laser guide stars for themselves. That idea became the Center for Adaptive Optics, which was funded by a $40-million, 10-year grant from the US National Science Foundation (NSF), and which opened on the Santa Cruz campus in 1999, with Max eventually as its head. By 2010, says Max, when the grant ended and the centre had to close its doors, it had grown from a handful of people to nearly 100. That did the trick. The laser-assisted adaptive-optics systems have to be custom-built for each telescope and are still pricey, running to several million dollars apiece. But astronomers, many of whom were trained at Max's centre, have now retrofitted the technology onto every optical telescope for which it makes sense. That includes almost all the flagship telescopes that currently rank as the largest in the world, from the twin Keck 10-metre telescopes atop Mauna Kea to the four identical 8.2-metre instruments comprising the European Very Large Telescope in Chile. \n               Good resolution \n             The bigger the telescope, the more advantage it can get from adaptive optics. At 1 micrometre, a wavelength in the infrared part of the spectrum, which is particularly useful for astronomy, the Hubble Space Telescope's 2.4-metre mirror can produce images with a resolution 0.11 arcseconds. At that wavelength, with the help of the laser guide star, the Lick 3-metre can do somewhat better: 0.08 arcseconds. But 8-metre instruments like those comprising the European quartet in Chile can get all the way down to 0.03 arcseconds \u2014 almost four times better than Hubble. That kind of visual acuity has allowed astronomers to track the stars orbiting the Milky Way's central black hole, image exoplanets around other suns, observe the common, low-mass stars known as brown dwarfs and pursue many other once-impossible studies. In these areas, says David Silva, director of the US National Optical Astronomy Observatory in Tucson, Arizona, \u201cwe couldn't have made any advances from the ground\u201d without adaptive optics. The systems' largest impact, however, will be on the 20\u201340-metre telescopes that are now under development: the European Extremely Large Telescope and the Giant Magellan Telescope, both in Chile, and the TMT on Mauna Kea. Telescopes this big can collect enough light to study faint, far-off objects such as the first galaxies to form after the Big Bang \u2014 but would hardly be worth their billion-dollar cost if they had a resolution of just 1 arcsecond. Unfortunately, that price tag is also why there are only three of these megatelescopes \u2014 which means, in turn, that only a small fraction of astronomers will ever get to use them. At the same time, 'have-not' astronomers at institutions not affiliated with one of the large projects are losing access to the 3- and 4-metre telescopes \u2014 even though these smaller instruments are often ideal for large-scale surveys of the sky, or targeted observations of relatively nearby objects. Citing a flat budget and its investment in large projects such as the ALMA radio telescope in Chile, the NSF has withdrawn support from optical telescopes in this class. \u201cFor the general astronomer, it's harder and harder to get time,\u201d said Richard Barvainis, programme officer for astronomy at the NSF. \u201cIt's becoming a major issue.\u201d At the UCO, which is facing its own cash crunch, Max is now in charge of a perfect microcosm of the situation she helped to create. In September 2013, provost Aim\u00e9e Dorr declared that the University of California's support for the Lick Observatory would end within five years, thanks to construction costs on the TMT; maintenance expenses on the Keck telescopes; and the UCO's declining budget, which has halved over the past decade. The astronomical community responded with a firestorm of protest. It insisted that Lick, in operation since 1888, is still valuable as a test bed for new instruments and as a training ground for graduate students. So last autumn, Max began her term as interim director of the UCO by walking into Dorr's office and saying \u201cWhat can we do to make this a win\u2013win?\u201d. Dorr was willing to try: Max was not only \u201ccredible, straightforward, honest, and moderate\u201d, she says, but she could create budgets that at least one previous director had said were impossible. Over the next several months, Max sorted through the UCO's convoluted partnerships and budgets, found ways to move around various pots of money, calmed the firestorm and got the university's funding for Lick reinstated. Max's next priority is to devise a comprehensive strategic plan for Lick, as well as for the two Keck telescopes. Because Lick has adaptive optics and it is relatively available to astronomers, it could be used for high-precision surveys of a few hundred to a few thousand objects such as quasars \u2014 studies, says Max, that \u201cyou can peck away one by one and add up to a survey with good statistics\u201d. Max is determined to have this plan in place when the university announces its choice for the permanent director of the UCO. That new director might be her \u2014 she has applied for the position \u2014 but if not, she says, she will simply go back to being the astronomer-engineer-community builder she has been all along. Along those lines, she is working on multi-laser adaptive-optics systems that can both correct for the atmosphere and widen the field of view. She is also training a particular brand of graduate student \u2014 the kind, she says, who \u201chas one hand in instruments and one in astronomy\u201d. That breadth of skills has allowed Max to do what she has done, says Ghez. \u201cA typical astronomer could never do it.\u201d \n                     California observatory wins back funding 2014-Nov-06 \n                   \n                     Astronomical instrumentation: Atmospheric blurring has a new enemy 2014-Aug-13 \n                   \n                     Megatelescopes look for support 2011-Nov-02 \n                   \n                     US survey sets cosmic priorities 2010-Aug-18 \n                   \n                     Measurement of atmospheric wavefront distortion using scattered light from a laser guide-star 1991-Sep-12 \n                   \n                     Experiments on laser guide stars at Mauna Kea Observatory for adaptive imaging in astronomy 1987-Jul-16 \n                   \n                     Video of laser guide star technology \n                   \n                     Time-lapse of laser guide star in action \n                   \n                     Jason's report on sodium laser guide-star \n                   Reprints and Permissions"},
{"file_id": "518158a", "url": "https://www.nature.com/articles/518158a", "year": 2015, "authors": [{"name": "Zeeya Merali"}], "parsed_as_year": "2006_or_before", "body": "Using techniques adapted from astronomy, physicists are finding ways to see through opaque materials such as living tissue. It seemed too good to be true, says Allard Mosk. It was 2007, and he was working with Ivo Vellekoop, a student in his group at the University of Twente in Enschede, the Netherlands, to shine a beam of visible light through a 'solid wall' \u2014 a glass slide covered with white paint \u2014 and then focus it on the other side. They did not have a particular application in mind. \u201cI really just wanted to try this because it had never been done before,\u201d Mosk says. And in truth, the two researchers did not expect to pick up much more than a faint blur. But as it turned out, their very first attempt 1  produced a sharp pinprick of light a hundred times brighter than they had hoped for. \u201cThis just doesn't happen on the first day of your experiment,\u201d exclaims Mosk. \u201cWe thought we'd made a mistake and there must be a hole in our slide letting the light through!\u201d But there was no hole. Instead, their experiment became the first of two independent studies 1 , 2  that were carried out that year pioneering ways to see through opaque barriers. So far it is still a laboratory exercise. But progress has been rapid. Researchers have now managed to obtain good-quality images through thin tissues such as mouse ears 3 , and are working on ways to go deeper. And if they can meet the still-daunting challenges, such as dealing with tissues that move or stretch, potential applications abound. Visible-light images obtained from deep within the body might eliminate the need for intrusive biopsies, for example. Or laser light could be focused to treat aneurysms in the brain or target inoperable tumours without the need for surgery. \u201cJust ten years ago, we couldn't imagine high-resolution imaging down to even 1 centimetre in the body with optical light, but now that has now become a reality,\u201d says Lihong Wang, a biomedical engineer at Washington University in St. Louis, Missouri. \u201cCall me crazy, but I believe that we will eventually be doing whole-body imaging with optical light.\u201d \n               Rich source \n             It is already possible to peer inside the body with X-rays and ultrasound. But the images produced by such tools are crude compared with those that should be possible with visible light. Partly this is because visible-light images tend to have higher resolution, says Wang. But it is also because optical wavelengths interact strongly with organic molecules, so the reflected light is packed with information about biochemical changes, cellular anomalies and glucose and oxygen levels in the blood. However, those interactions also make visible light prone to scattering and absorption. Absorption will scupper any imaging attempt: the information the photons pick up is lost as they are absorbed into the material. Scattering, however, preserves a ray of hope. Many materials, such as skin, white paint or fog, are 'opaque' only because photons passing through them ricochet until they are thoroughly scrambled. But they are not lost \u2014 so in principle, the scrambling can be reversed. Astronomers have already solved a version of this scattering problem using a technology called adaptive optics, which allows them to undo the distortions imposed on images of stars, planets and galaxies by the scattering of light in the atmosphere (see   Nature   517 , 430\u2013432; 2015). The basic idea is to collect light from a bright reference star and use an algorithm to calculate how the atmosphere has smeared and blurred its point-like image. The algorithm then controls a special 'deformable' mirror that cancels out the atmospheric distortions, turns the guide-star image into a point, and at the same time brings other distant objects into sharp focus. Unfortunately, this technique is tough to use in the body. Targets deep inside biological tissues do not shine the way that stars do \u2014 they have to be illuminated from the outside \u2014 and the scatterers are much more densely packed than those that scatter light in the atmosphere. \u201cYou'd need the equivalent of a deformable mirror with billions of moving parts to compensate for the scattering caused by an egg shell,\u201d says Ori Katz, an optical physicist at the Langevin Institute in Paris. That is why Mosk and Vellekoop were not too hopeful of success when they started. Still, the pair took heart from the advance of technology. \u201cUntil recently it had been preposterous to think you could control a million pixels, but, by 2007, every smartphone could do it,\u201d says Mosk. They therefore made use of a 'spatial light modulator': a device similar to an LCD smartphone display that can control the transmission of different parts of a laser beam by delaying one part relative to another. They fired their laser through the modulator towards the painted glass slide, placed a detector beyond the slide and used a computer to monitor how much light the detector picked up. The computer then added and subtracted delays at each pixel of the modulator, going through a process of trial and error to see what changes minimized the scattering of the laser light as it passed through the slide. In effect, it was trying to give the incoming light a distortion that the opaque barrier would exactly cancel out. Mosk and Vellekoop ran the algorithm for more than an hour, and when it was done they had a result that beat all their expectations: a focus that was a thousand times more intense than the background signal 1 . \u201cThe Mosk experiment was an eye-opener,\u201d says Katz. \u201cIt changed the paradigm of what could be done with optical light.\u201d Soon after his succcess, Mosk learned of similar work being done by bioengineer Changhuei Yang and his team at the California Institute of Technology in Pasadena. These researchers had used a different technique to focus scattered optical light, and a different opaque substance: a thin slice of chicken breast 2 . But they, too, were surprised by how easy it was to do. \u201cI had thought 'we'll spend six months on this, and when it doesn't work, we'll chalk it up as a learning experience',\u201d says Yang. \u201cBut actually it wasn't that hard.\u201d Soon after the two papers were published, the field exploded as other physicists rushed to join in. One of them was optical physicist Jacopo Bertolotti, who came to work with Mosk in 2010. Bertolotti, now at the University of Exeter, UK, says that he was drawn both by the \u201cbeauty of the experiment\u201d and by the potential it offered for medical imaging. But he could see that that goal was still a long way off.  Call me crazy, but I believe that we will eventually be doing whole-body imaging with optical light.  The first issue that Bertolotti faced was that Mosk's original set-up required a camera to be placed behind the opaque surface. That is a problem for medical applications because placing a camera under the skin would involve surgery, which would be invasive, dangerous and rarely worth the risk. In 2012, however, Bertolotti, Mosk and their colleagues devised a way to put both the laser light source and the detector in front of the surface 4 . Their target was a fluorescent Greek letter \u03c0 just 50 micrometres across hidden behind a thin opaque screen. As such, the target was roughly the same size as a cell and analogous with medical techniques that involved injecting fluorescent dyes into living tissue to aid in imaging. When the laser was switched on, the photons would bounce their way through the screen and produce a diffuse illumination of the fluorescent \u03c0. The light reflected from the letter would then make its way back through the screen and produce a blurry speckled pattern on the other side. It was like trying to see the symbol through a shower curtain. Yet the shape of the letter was still encoded in the scattered light. To retrieve that shape, the team recorded the speckle pattern, moved the laser to shine at a different angle, then recorded the new speckle pattern 4 . By repeating this many times and comparing the patterns point by point, a computer could work out how the patterns were correlated \u2014 and from that, work backwards to reconstruct the hidden letter \u03c0. That was progress, says Bertolotti, but it still was not good enough. \u201cIt only works if the object to be imaged is on the other side of the scattering medium,\u201d he says. For many medical applications, such as seeing inside the brain, or within a blood vessel, the target is buried within tissue. \n               Inside out \n             The challenge of imaging inside the scattering medium has been taken up by a number of groups, including Yang's and Wang's. In 2013, for instance, Yang's team achieved this feat with unprecedented resolution by picking out a fluorescent bead just one micrometre across sandwiched between two artificial opaque layers 5 . Yang, together with biologist Benjamin Judkewitz and the rest of his team did this by illuminating the medium and letting the light bounce its way through to the other side, then reflecting it back with a 'time-reversing' mirror, which effectively forces every light ray to exactly retrace its steps. Time-reversing all the rays would simply undo all the scattering, however. So instead, the team focused an ultrasound beam \u2014 which is not easily scattered \u2014 at one point in the medium, knowing that any optical light that happened to pass through that point would undergo a slight shift in frequency. Then on the far side, the researchers set up the time-reversing mirror tuned so that it would send back only the light that had experienced that frequency shift. The result was a thin, time-reversed beam that would automatically pass back through the focus and add its energy to the light from the first pass. This turned the ultrasound focus into a spot of comparatively high radiation intensity \u2014 \u201ca torch inside the wall\u201d, says Judkewitz, who is now at the Charit\u00e9 University Hospital in Berlin. Better still, the ultrasound focus could be moved around within the medium. And when it passed over the bead, the bead fluoresced (see 'Light and sound'). However, the technique was still a long way from seeing into deep layers of tissue, which pose another, much tougher challenge: they tend to move constantly as a result of blood flow and breathing. \u201cWe are still not so close to medical applications because these techniques tend to work only if the scattering medium is perfectly frozen in time,\u201d says Mathias Fink, a physicist at Langevin who pioneered a version of the time-reversal technique in the 1990s that used ultrasound alone 6 . Most groups have reduced the timing from Mosk's original hour or so to just tens of seconds, says Katz, and that is fine for imaging a bead or a letter \u03c0, but not for imaging a tumour in the body. But last year, a team led by Sylvain Gigan, a physicist at the Kastler Brossel Laboratory in Paris, and including Katz and Fink, demonstrated a way to reconstruct the image of the hidden object in just one camera shot 7 . \u201cIt's a bit like magic when you see the algorithm converge on the final image,\u201d Gigan says. Wang agrees that speed is of the essence. \u201cEverything is in motion and we only have a millisecond-scale window to make an image,\u201d he says. In a paper published in January 3 , Wang and his team managed to get the speed down to 5.6 milliseconds, \u201cwhich is fast enough for selected  in vivo  imaging\u201d, he says. Furthermore, their target was made from ink-stained gelatin and sandwiched between the ear of an anaesthetized mouse and a ground-glass diffuser. Getting success with a live mouse is impressive, says Bertolotti \u2014 although he points out that \u201cmoving from a mouse ear, which is relatively thin, to imaging human skin and flesh will still take a lot of extra work\u201d. As of today, Bertolotti adds, there is still no imaging approach that stands out above the rest. Each has its advantages and disadvantages. \u201cRather than developing one technique that's good for everything, I think we'll develop a suite of techniques that could one day all be combined into the same piece of apparatus,\u201d he says. \u201cI don't know how quickly that might happen, but this is a young and fast-moving community, so it could be within a few years.\u201d The techniques now being pioneered by bioengineers and physicists for medicine could also be put to a range of other purposes. Mosk, for example, believes that these methods could be a tool for art restoration. \u201cMost painters build up works in several layers, and the layers below can influence the chemical and physical ageing of the painting, so it's of some significance that you know what is in there if you want to preserve it,\u201d he says. Methods that in effect unscatter light could also help the telecommunications industry to unscramble the noise in optical fibres that is caused by scattered light. Another obvious customer is the military, says Fink, who thinks that the technology could be used to allow soldiers to see through a portable shield \u2014 either a physical screen or a fogging spray \u2014 that obscures them from their enemy's view. \u201cIt's not the same as being invisible, but it would allow you to see others while not being seen,\u201d he says. Almost all the scientists in this young field get excited when they start dreaming of applications. But Gigan, for one, is keen to keep the applications above board. \u201cWhen we tell people what we do, someone always asks if we'll create a phone app to let people look through shower curtains,\u201d he says. \u201cThis is something that could be done with our technique \u2014 but we don't intend to do it.\u201d \n                     Cosmology: The oldest cosmic light 2015-Feb-11 \n                   \n                     Physics in finance: Trading at the speed of light 2015-Feb-11 \n                   \n                     In retrospect: Book of Optics 2015-Feb-11 \n                   \n                     Light fantastic 2015-Feb-11 \n                   \n                     Optics: Leading lights 2015-Feb-11 \n                   \n                     Astronomy: Laser focus 2015-Jan-21 \n                   \n                     Blown-up brains reveal nanoscale details 2015-Jan-09 \n                   \n                     Technology: Ultrafast imaging takes on a new design 2014-Dec-03 \n                   \n                     Nobel for microscopy that reveals inner world of cells 2014-Oct-08 \n                   \n                     Neuroscience: Shedding light on a change of mind 2014-Aug-27 \n                   \n                     Exotic optics: Metamaterial world 2013-Aug-07 \n                   \n                     Nature  special: Light \n                   \n                     Nature  collection: Year of light \n                   \n                     Nature  special: Microscopy \n                   \n                     Allard Mosk \n                   \n                     Changhuei Yang \n                   \n                     Jacopo Bertolotti \n                   \n                     Benjamin Judkewitz's lab \n                   \n                     Lihong Wang \n                   \n                     Sylvain Gigan \n                   Reprints and Permissions"},
{"file_id": "518153a", "url": "https://www.nature.com/articles/518153a", "year": 2015, "authors": [], "parsed_as_year": "2006_or_before", "body": "Scientists are pushing the properties of light to new extremes. A special issue explores these frontiers. From glorious rainbows to the intricate mechanics of the human eye, light lies at the heart of phenomena that have fascinated scientists for millennia. Today, the latest optical technologies \u2014 from lasers to solar cells \u2014 harness light to advance physics and to serve society's needs. To put light itself in the spotlight, the United Nations designated 2015 the International Year of Light and Light-based Technologies. The celebration is also pegged to a string of anniversaries: Augustin-Jean Fresnel's proposal in 1815 that light is a wave; James Clerk Maxwell's 1865 electromagnetic theory; Albert Einstein's 1915 general theory of relativity; and in 1965, discovery of the cosmic microwave background (CMB) radiation and the development of optical fibres for communication. Nature  is paying its own tribute to light in this special issue. Contorting light is the goal of three physicists profiled in a News Feature on  page 154 : Miles Padgett twists laser beams to encode binary information; Pierre Berini reshapes light waves to speed up digital communications; and Margaret Murnane dissects X-rays into ultrafast attosecond pulses, one billionth of a billionth of a second long, to probe materials in exquisite details. Some advances in the physics of light are of great benefit to biology and medicine. Borrowing from astronomers, biophysicists are developing techniques for seeing through opaque layers, by detecting the minute glow of visible light scattered through body tissues. Such methods are likely to lead to more-powerful medical imaging, as explained on  page 158 . In another sphere entirely, near-speed-of-light communications are set to transform financial trading as laser links between banking centres come online. But there are major risks, Mark Buchanan explains on  page 161 . Trading stocks in milliseconds pushes algorithms to their limits, exposing flaws that can escalate in seconds to cause hundred-million-dollar losses. In a News & Views Forum on  page 170 , two cosmologists reflect on the clues to the origin of the Universe hidden in its oldest light, the CMB. And on  page 164 , physicist Jim Al-Khalili is dazzled by the afterglow of a 1,000-year-old treatise on the nature of light: Ibn al-Haytham's  Book of Optics . An online collection will highlight key papers on light from journals across Nature Publishing Group throughout the year (see  nature.com/yearoflight ). With so many facets, scientists' fascination with light looks unlikely to fade. \n                     Astronomy: Laser focus 2015-Jan-21 \n                   \n                     Solar energy: Springtime for the artificial leaf 2014-Jun-04 \n                   \n                     Exotic optics: Metamaterial world 2013-Aug-07 \n                   \n                     Nature  special: Light \n                   \n                     Nature  collection: Year of light \n                   \n                     International Year of Light \n                   Reprints and Permissions"},
{"file_id": "518288a", "url": "https://www.nature.com/articles/518288a", "year": 2015, "authors": [{"name": "Claire Ainsworth"}], "parsed_as_year": "2006_or_before", "body": "The idea of two sexes is simplistic. Biologists now think there is a wider spectrum than that. As a clinical geneticist, Paul James is accustomed to discussing some of the most delicate issues with his patients. But in early 2010, he found himself having a particularly awkward conversation about sex. A 46-year-old pregnant woman had visited his clinic at the Royal Melbourne Hospital in Australia to hear the results of an amniocentesis test to screen her baby's chromosomes for abnormalities. The baby was fine \u2014 but follow-up tests had revealed something astonishing about the mother. Her body was built of cells from two individuals, probably from twin embryos that had merged in her own mother's womb. And there was more. One set of cells carried two X chromosomes, the complement that typically makes a person female; the other had an X and a Y. Halfway through her fifth decade and pregnant with her third child, the woman learned for the first time that a large part of her body was chromosomally male 1 . \u201cThat's kind of science-fiction material for someone who just came in for an amniocentesis,\u201d says James. Claire Ainsworth discusses the spectrum between male and female. Sex can be much more complicated than it at first seems. According to the simple scenario, the presence or absence of a Y chromosome is what counts: with it, you are male, and without it, you are female. But doctors have long known that some people straddle the boundary \u2014 their sex chromosomes say one thing, but their gonads (ovaries or testes) or sexual anatomy say another. Parents of children with these kinds of conditions \u2014 known as intersex conditions, or differences or disorders of sex development (DSDs) \u2014 often face difficult decisions about whether to bring up their child as a boy or a girl. Some researchers now say that as many as 1 person in 100 has some form of DSD 2 . When genetics is taken into consideration, the boundary between the sexes becomes even blurrier. Scientists have identified many of the genes involved in the main forms of DSD, and have uncovered variations in these genes that have subtle effects on a person's anatomical or physiological sex. What's more, new technologies in DNA sequencing and cell biology are revealing that almost everyone is, to varying degrees, a patchwork of genetically distinct cells, some with a sex that might not match that of the rest of their body. Some studies even suggest that the sex of each cell drives its behaviour, through a complicated network of molecular interactions. \u201cI think there's much greater diversity within male or female, and there is certainly an area of overlap where some people can't easily define themselves within the binary structure,\u201d says John Achermann, who studies sex development and endocrinology at University College London's Institute of Child Health. These discoveries do not sit well in a world in which sex is still defined in binary terms. Few legal systems allow for any ambiguity in biological sex, and a person's legal rights and social status can be heavily influenced by whether their birth certificate says male or female. \u201cThe main problem with a strong dichotomy is that there are intermediate cases that push the limits and ask us to figure out exactly where the dividing line is between males and females,\u201d says Arthur Arnold at the University of California, Los Angeles, who studies biological sex differences. \u201cAnd that's often a very difficult problem, because sex can be defined a number of ways.\u201d \n               The start of sex \n             That the two sexes are physically different is obvious, but at the start of life, it is not. Five weeks into development, a human embryo has the potential to form both male and female anatomy. Next to the developing kidneys, two bulges known as the gonadal ridges emerge alongside two pairs of ducts, one of which can form the uterus and Fallopian tubes, and the other the male internal genital plumbing: the epididymes, vas deferentia and seminal vesicles. At six weeks, the gonad switches on the developmental pathway to become an ovary or a testis. If a testis develops, it secretes testosterone, which supports the development of the male ducts. It also makes other hormones that force the presumptive uterus and Fallopian tubes to shrink away. If the gonad becomes an ovary, it makes oestrogen, and the lack of testosterone causes the male plumbing to wither. The sex hormones also dictate the development of the external genitalia, and they come into play once more at puberty, triggering the development of secondary sexual characteristics such as breasts or facial hair. Changes to any of these processes can have dramatic effects on an individual's sex. Gene mutations affecting gonad development can result in a person with XY chromosomes developing typically female characteristics, whereas alterations in hormone signalling can cause XX individuals to develop along male lines. For many years, scientists believed that female development was the default programme, and that male development was actively switched on by the presence of a particular gene on the Y chromosome. In 1990, researchers made headlines when they uncovered the identity of this gene 3 , 4 , which they called  SRY . Just by itself, this gene can switch the gonad from ovarian to testicular development. For example, XX individuals who carry a fragment of the Y chromosome that contains  SRY  develop as males. By the turn of the millennium, however, the idea of femaleness being a passive default option had been toppled by the discovery of genes that actively promote ovarian development and suppress the testicular programme \u2014 such as one called  WNT4 . XY individuals with extra copies of this gene can develop atypical genitals and gonads, and a rudimentary uterus and Fallopian tubes 5 . In 2011, researchers showed 6  that if another key ovarian gene,  RSPO1 , is not working normally, it causes XX people to develop an ovotestis \u2014 a gonad with areas of both ovarian and testicular development. These discoveries have pointed to a complex process of sex determination, in which the identity of the gonad emerges from a contest between two opposing networks of gene activity. Changes in the activity or amounts of molecules (such as WNT4) in the networks can tip the balance towards or away from the sex seemingly spelled out by the chromosomes. \u201cIt has been, in a sense, a philosophical change in our way of looking at sex; that it's a balance,\u201d says Eric Vilain, a clinician and the director of the Center for Gender-Based Biology at the University of California, Los Angeles. \u201cIt's more of a systems-biology view of the world of sex.\u201d \n               Battle of the sexes \n             According to some scientists, that balance can shift long after development is over. Studies in mice suggest that the gonad teeters between being male and female throughout life, its identity requiring constant maintenance. In 2009, researchers reported 7  deactivating an ovarian gene called  Foxl2  in adult female mice; they found that the granulosa cells that support the development of eggs transformed into Sertoli cells, which support sperm development. Two years later, a separate team showed 8  the opposite: that inactivating a gene called  Dmrt1  could turn adult testicular cells into ovarian ones. \u201cThat was the big shock, the fact that it was going on post-natally,\u201d says Vincent Harley, a geneticist who studies gonad development at the MIMR-PHI Institute for Medical Research in Melbourne. The gonad is not the only source of diversity in sex. A number of DSDs are caused by changes in the machinery that responds to hormonal signals from the gonads and other glands. Complete androgen insensitivity syndrome, or CAIS, for example, arises when a person's cells are deaf to male sex hormones, usually because the receptors that respond to the hormones are not working. People with CAIS have Y chromosomes and internal testes, but their external genitalia are female, and they develop as females at puberty. Conditions such as these meet the medical definition of DSDs, in which an individual's anatomical sex seems to be at odds with their chromosomal or gonadal sex. But they are rare \u2014 affecting about 1 in 4,500 people 9 . Some researchers now say that the definition should be widened to include subtle variations of anatomy such as mild hypospadias, in which a man's urethral opening is on the underside of his penis rather than at the tip. The most inclusive definitions point to the figure of 1 in 100 people having some form of DSD, says Vilain (see 'The sex spectrum'). \n               The sex spectrum \n               But beyond this, there could be even more variation. Since the 1990s, researchers have identified more than 25 genes involved in DSDs, and next-generation DNA sequencing in the past few years has uncovered a wide range of variations in these genes that have mild effects on individuals, rather than causing DSDs. \u201cBiologically, it's a spectrum,\u201d says Vilain. A DSD called congenital adrenal hyperplasia (CAH), for example, causes the body to produce excessive amounts of male sex hormones; XX individuals with this condition are born with ambiguous genitalia (an enlarged clitoris and fused labia that resemble a scrotum). It is usually caused by a severe deficiency in an enzyme called 21-hydroxylase. But women carrying mutations that result in a milder deficiency develop a 'non-classical' form of CAH, which affects about 1 in 1,000 individuals; they may have male-like facial and body hair, irregular periods or fertility problems \u2014 or they might have no obvious symptoms at all. Another gene,  NR5A1 , is currently fascinating researchers because variations in it cause a wide range of effects 10 , from underdeveloped gonads to mild hypospadias in men, and premature menopause in women. Many people never discover their condition unless they seek help for infertility, or discover it through some other brush with medicine. Last year, for example, surgeons reported that they had been operating on a hernia in a man, when they discovered that he had a womb 11 . The man was 70, and had fathered four children. \n               Cellular sex \n             Studies of DSDs have shown that sex is no simple dichotomy. But things become even more complex when scientists zoom in to look at individual cells. The common assumption that every cell contains the same set of genes is untrue. Some people have mosaicism: they develop from a single fertilized egg but become a patchwork of cells with different genetic make-ups. This can happen when sex chromosomes are doled out unevenly between dividing cells during early embryonic development. For example, an embryo that starts off as XY can lose a Y chromosome from a subset of its cells. If most cells end up as XY, the result is a physically typical male, but if most cells are X, the result is a female with a condition called Turner's syndrome, which tends to result in restricted height and underdeveloped ovaries. This kind of mosaicism is rare, affecting about 1 in 15,000 people. The effects of sex-chromosome mosaicism range from the prosaic to the extraordinary. A few cases have been documented in which a mosaic XXY embryo became a mix of two cell types \u2014 some with two X chromosomes and some with two Xs and a Y \u2014 and then split early in development 12 . This results in 'identical' twins of different sexes. There is a second way in which a person can end up with cells of different chromosomal sexes. James's patient was a chimaera: a person who develops from a mixture of two fertilized eggs, usually owing to a merger between embryonic twins in the womb. This kind of chimaerism resulting in a DSD is extremely rare, representing about 1% of all DSD cases. Another form of chimaerism, however, is now known to be widespread. Termed microchimaerism, it happens when stem cells from a fetus cross the placenta into the mother's body, and vice versa. It was first identified in the early 1970s \u2014 but the big surprise came more than two decades later, when researchers discovered how long these crossover cells survive, even though they are foreign tissue that the body should, in theory, reject. A study in 1996 recorded women with fetal cells in their blood as many as 27 years after giving birth 13 ; another found that maternal cells remain in children up to adulthood 14 . This type of work has further blurred the sex divide, because it means that men often carry cells from their mothers, and women who have been pregnant with a male fetus can carry a smattering of its discarded cells. Microchimaeric cells have been found in many tissues. In 2012, for example, immunologist Lee Nelson and her team at the University of Washington in Seattle found XY cells in post-mortem samples of women's brains 15 . The oldest woman carrying male DNA was 94 years old. Other studies have shown that these immigrant cells are not idle; they integrate into their new environment and acquire specialized functions, including (in mice at least) forming neurons in the brain 16 . But what is not known is how a peppering of male cells in a female, or vice versa, affects the health or characteristics of a tissue \u2014 for example, whether it makes the tissue more susceptible to diseases more common in the opposite sex. \u201cI think that's a great question,\u201d says Nelson, \u201cand it is essentially entirely unaddressed.\u201d In terms of human behaviour, the consensus is that a few male microchimaeric cells in the brain seem unlikely to have a major effect on a woman. Scientists are now finding that XX and XY cells behave in different ways, and that this can be independent of the action of sex hormones. \u201cTo tell you the truth, it's actually kind of surprising how big an effect of sex chromosomes we've been able to see,\u201d says Arnold. He and his colleagues have shown 17  that the dose of X chromosomes in a mouse's body can affect its metabolism, and studies in a lab dish suggest 18  that XX and XY cells behave differently on a molecular level, for example with different metabolic responses to stress. The next challenge, says Arnold, is to uncover the mechanisms. His team is studying the handful of X-chromosome genes now known to be more active in females than in males. \u201cI actually think that there are more sex differences than we know of,\u201d says Arnold. \n               Beyond the binary \n             Biologists may have been building a more nuanced view of sex, but society has yet to catch up. True, more than half a century of activism from members of the lesbian, gay, bisexual and transgender community has softened social attitudes to sexual orientation and gender. Many societies are now comfortable with men and women crossing conventional societal boundaries in their choice of appearance, career and sexual partner. But when it comes to sex, there is still intense social pressure to conform to the binary model. This pressure has meant that people born with clear DSDs often undergo surgery to 'normalize' their genitals. Such surgery is controversial because it is usually performed on babies, who are too young to consent, and risks assigning a sex at odds with the child's ultimate gender identity \u2014 their sense of their own gender. Intersex advocacy groups have therefore argued that doctors and parents should at least wait until a child is old enough to communicate their gender identity, which typically manifests around the age of three, or old enough to decide whether they want surgery at all. This issue was brought into focus by a lawsuit filed in South Carolina in May 2013 by the adoptive parents of a child known as MC, who was born with ovotesticular DSD, a condition that produces ambiguous genitalia and gonads with both ovarian and testicular tissue. When MC was 16 months old, doctors performed surgery to assign the child as female \u2014 but MC, who is now eight years old, went on to develop a male gender identity. Because he was in state care at the time of his treatment, the lawsuit alleged not only that the surgery constituted medical malpractice, but also that the state denied him his constitutional right to bodily integrity and his right to reproduce. Last month, a court decision prevented the federal case from going to trial, but a state case is ongoing. \u201cThis is potentially a critically important decision for children born with intersex traits,\u201d says Julie Greenberg, a specialist in legal issues relating to gender and sex at Thomas Jefferson School of Law in San Diego, California. The suit will hopefully encourage doctors in the United States to refrain from performing operations on infants with DSDs when there are questions about their medical necessity, she says. It could raise awareness about \u201cthe emotional and physical struggles intersex people are forced to endure because doctors wanted to 'help' us fit in,\u201d says Georgiann Davis, a sociologist who studies issues surrounding intersex traits and gender at the University of Nevada, Las Vegas, who was born with CAIS. Doctors and scientists are sympathetic to these concerns, but the MC case also makes some uneasy \u2014 because they know how much is still to be learned about the biology of sex 19 . They think that changing medical practice by legal ruling is not ideal, and would like to see more data collected on outcomes such as quality of life and sexual function to help decide the best course of action for people with DSDs \u2014 something that researchers are starting to do. Diagnoses of DSDs once relied on hormone tests, anatomical inspections and imaging, followed by painstaking tests of one gene at a time. Now, advances in genetic techniques mean that teams can analyse multiple genes at once, aiming straight for a genetic diagnosis and making the process less stressful for families. Vilain, for example, is using whole-exome sequencing \u2014 which sequences the protein-coding regions of a person's entire genome \u2014 on XY people with DSDs. Last year, his team showed 20  that exome sequencing could offer a probable diagnosis in 35% of the study participants whose genetic cause had been unknown. Vilain, Harley and Achermann say that doctors are taking an increasingly circumspect attitude to genital surgery. Children with DSDs are treated by multidisciplinary teams that aim to tailor management and support to each individual and their family, but this usually involves raising a child as male or female even if no surgery is done. Scientists and advocacy groups mostly agree on this, says Vilain: \u201cIt might be difficult for children to be raised in a gender that just does not exist out there.\u201d In most countries, it is legally impossible to be anything but male or female. Yet if biologists continue to show that sex is a spectrum, then society and state will have to grapple with the consequences, and work out where and how to draw the line. Many transgender and intersex activists dream of a world where a person's sex or gender is irrelevant. Although some governments are moving in this direction, Greenberg is pessimistic about the prospects of realizing this dream \u2014 in the United States, at least. \u201cI think to get rid of gender markers altogether or to allow a third, indeterminate marker, is going to be difficult.\u201d So if the law requires that a person is male or female, should that sex be assigned by anatomy, hormones, cells or chromosomes, and what should be done if they clash? \u201cMy feeling is that since there is not one biological parameter that takes over every other parameter, at the end of the day, gender identity seems to be the most reasonable parameter,\u201d says Vilain. In other words, if you want to know whether someone is male or female, it may be best just to ask. \n                     Diversity: Pride in science 2014-Sep-16 \n                   \n                     Policy: NIH to balance sex in cell and animal studies 2014-May-14 \n                   \n                     Ethics: Taboo genetics 2013-Oct-02 \n                   \n                     Intersex surgery disregards children's human rights 2004-Apr-15 \n                   \n                     Brain development: The most important sexual organ 2004-Jan-29 \n                   Reprints and Permissions"},
{"file_id": "518292a", "url": "https://www.nature.com/articles/518292a", "year": 2015, "authors": [{"name": "Christopher Kemp"}], "parsed_as_year": "2006_or_before", "body": "The billions of specimens in natural-history museums are becoming more useful for tracking Earth's shrinking biodiversity. But the collections also face grave threats. Ricardo Moratelli surveys several hundred dead bats \u2014 their wings neatly folded \u2014 in a room deep inside the Smithsonian Institution in Washington DC. He moves methodically among specimens arranged in ranks like a squadron of bombers on a mission. Attached to each animal's right ankle is a tag that tells Moratelli where and when the creature was collected, and by whom. Some of the tags have yellowed with age \u2014 they mark bats that were collected more than a century ago. Moratelli selects a small, compact individual with dark wings and a luxurious golden pelage. It fits easily in his cupped palm. To the untrained eye, this specimen looks identical to the rest. But Moratelli, a postdoctoral fellow at the Smithsonian's National Museum of Natural History, has discovered that the bat in his hands is a new species. It was collected in February 1979 in an Ecuadorian forest on the western slopes of the Andes. A subadult male, it has been waiting for decades for someone such as Moratelli to recognize its uniqueness. He named it  Myotis diminutus 1 . Before Moratelli could take that step, however, he had to collect morphometric data \u2014 precise measurements of the skull and post-cranial skeleton \u2014 from other specimens. In all, he studied 3,000 other bats from 18 collections around the world. Kerri Smith visits the London\u2019s Natural History Museum to search the vaults for a new species. Myotis diminutus  is not alone. And neither is Ricardo Moratelli. Across the world, natural-history collections hold thousands of species awaiting identification. In fact, researchers today find many more novel animals and plants by sifting through decades-old specimens than they do by surveying tropical forests and remote landscapes. An estimated three-quarters of newly named mammal species are already part of a natural-history collection at the time they are identified. They sometimes sit unrecognized for a century or longer, hidden in drawers, half-forgotten in jars, misidentified, unlabelled. \u201cIt's certainly the case that collections right now have vast resources of undescribed material,\u201d says Robert Voss, curator of mammals at the American Museum of Natural History (AMNH) in New York. These collections are becoming increasingly valuable thanks to newly developed techniques and databases. Through DNA sequencing, digital registries and other advances, existing collections can be interrogated in new ways, revealing more about Earth's biodiversity, and how quickly it is disappearing. But just as the collections are growing more valuable, they are falling into decline. With many institutions struggling to cope with significant budget cuts, some collections are being neglected, damaged or lost altogether. And the scientists who study them are also threatened as their positions disappear. \n               Cuts to collections \n             \u201cThis is the repository of all life that we know has existed,\u201d says Michael Mares, director of the Sam Noble Museum at the University of Oklahoma in Norman, and past president of the American Society of Mammalogists. \u201cIf you want to go back and do a survey of the mammals of Kuala Lumpur or something 30 years or 40 years ago, you can't go back,\u201d he says. \u201cYou have to go to the collections to do it.\u201d In 1758, with the publication of the encyclopaedic  Systema Naturae , Carl Linnaeus attempted to classify nature \u2014 an effort that continues today at almost 8,000 natural-history collections around the world. The United States alone holds an estimated 1 billion specimens, and the global figure may reach 3 billion. The average institution displays only about 1% or less of its store. The rest \u2014 often hundreds of thousands of specimens \u2014 is catalogued and stored away, inaccessible to the public. The collections are overseen by a dwindling corps of managers and curators \u2014 mainly taxonomists who describe species, and systematists who study the relationship between organisms. The Field Museum in Chicago, Illinois, had 39 curators in 2001. Today, there are just 21. At present, there is no curator of fishes \u2014 an enormously diverse class of animal. Neither The Field Museum nor the AMNH \u2014 which hold two of the largest collections in the world \u2014 has a lepidopterist on staff, even though both collections contain hundreds of thousands of butterfly and moth specimens. Similarly, the National Museum of Natural History has seen a steady drop in the number of curators \u2014 from a high of 122 in 1993 to a low of 81 last year. The decline is not limited to the United States. \u201cThe situation in the United Kingdom is the same, or worse,\u201d says Paolo Viscardi, chair of the UK-based Natural Sciences Collections Association and a curator at the Horniman Museum in London. Commonly, a museum will restructure its staff, replacing three or four curatorial positions with a single collections manager, and sometimes an assistant. That manager might cover every discipline, from contemporary art to the natural sciences. Since the economic crisis of 2008, many institutions are operating with smaller budgets. The few museums that get significant numbers of research grants have shifted their science focus to molecular techniques, which are better funded than more traditional taxonomic approaches. Many museums are emphasizing education and entertainment as they cut back on curatorial staff, says Scott Schaefer, associate dean of science for collections at the AMNH. Schaefer says that he has seen significant changes in many natural-history museums since 2008. \u201cThey tend to shift away from the conduct of research to simply the telling of the story of the sciences, in the same way that Walt Disney Company may represent science as entertainment,\u201d he says. According to Mares, most of the estimated 1,800 collections in the United States are small. \u201cThe great majority of these are hanging by a thread,\u201d he says. \u201cThey have nobody to care for them.\u201d Even well-funded institutions are facing difficulties. At the University of Michigan in Ann Arbor, for example, one of the country's largest biodiversity collections has been warehoused in new state-of-the-art facilities, carefully maintained but difficult for researchers to access, says Voss, who did his graduate work at the university. \u201cIt's as if we decided that we didn't want anybody doing research in our libraries anymore,\u201d he says, \u201cbut we're going to keep the books.\u201d As curators are lost, actual specimens sometimes disappear through neglect or accidents. In 2010, a fire consumed 85,000 snake specimens and an estimated 450,000 scorpion and spider specimens at the Butantan Institute in S\u00e3o Paulo, Brazil. \u201cWe see a decline in many collections in many countries,\u201d says Mares. \u201cIf a collection is sinking, no one will say it is.\u201d The concern is that administrators will get rid of collections if museum personnel point out problems, he says. \u201cIt's too dangerous. They survive by hiding.\u201d \n               Decades of waiting \n             Museum staff and researchers have a name for the barriers that slow down species discovery: the taxonomic impediment. And one measure of the taxonomic impediment is the lag time \u2014 the gap between when a new species is first collected and when it is identified. Currently, the average lag time is 21 years 2 . It is not clear whether that lag is increasing, but it often stretches much longer than the average. In April 1856, Henry Clay Caldwell of the United States Navy found a large, fruit-eating bat on the Samoan island of Upolu. The specimen currently resides at the Academy of Natural Sciences of Drexel University in Philadelphia, Pennsylvania, and details of the find are now scarce: a few faded, hand-written descriptors on a box, a skull and a fragment of discoloured skin. In 2009, Kristofer Helgen, a mammal curator at the Smithsonian Institution, held the skull up to the light and realized it was an unknown species. More than 150 years after it was first collected, he named the species  Pteropus allenorum  \u2014 the small Samoan flying fox 3 . The species is already extinct on the island. Like Helgen, Moratelli is fascinated by natural-history collections. His interest in zoology began as a child, watching the  Wild America  documentary series on television with his father. Moratelli has described six species of bat and is preparing descriptions of eight more, all of which he found in collections. The shortest lag time was 29 years; the longest was 111. Researchers say that such work is crucial for understanding biodiversity and how it is being threatened. \u201cWe are in the middle of a biodiversity crisis, and collections-based institutions have a unique role in society to document that biodiversity,\u201d says Quentin Wheeler, a taxonomist and president of the College of Environmental Science and Forestry at the State University of New York in Syracuse. \u201cWhen we only know 10\u201320% of the species, we're at a huge disadvantage to detect changes in the environment, whether it's species extinctions or introductions or whatever.\u201d The threats to museum staff and collections reflect changes that have been reshaping research for decades. With the rise of molecular biology, funding agencies and universities are providing less support for ornithologists, herpetologists, botanists and other specialized researchers who practise taxonomy. New species are still being described. But by whom? \u201cThere are increasing numbers of non-taxonomists describing species because there are no taxonomists doing it,\u201d says Wheeler. Instead, it has fallen to geneticists, behavioural zoologists and others not trained in taxonomy to name species. \u201cIncreasing numbers of biologists have to resort to naming them themselves or it simply won't get done.\u201d Such careful taxonomic work is required for cataloguing biodiversity and protecting endangered species, says Helgen, who has named more than 30 species from specimens found in collections. \u201cEvery time I name one of these species,\u201d he says, \u201cpeople start to think more about it; try to learn more about it; it gets on endangered-species lists.\u201d Even with the problems facing museum collections and those who study them, there are some bright spots. The California Academy of Sciences in San Francisco is recruiting curators and growing its collection. This year, it will acquire a collection of 1.5 million weevils \u2014 a gift from a pair of scientists who wish to remain anonymous. Museums are also trying to reach wider audiences by digitizing their collections and making them more accessible. \u201cThat's a major thrust to the Smithsonian right now,\u201d says John Kress, the institution's interim under-secretary for science. By the time the process is complete, he says, around 5 million botanical specimens \u2014 the oldest dating back to 1504 \u2014 will have been scanned. The California Academy of Sciences has partnered with Google to put images of its specimens online, along with other identifying information. The push towards digitization will make collections more available for researchers as well as amateur taxonomists, who have described a growing number of species in recent years. But digitization cannot fill the role of physical collections, because not all databases include key data such as the three-dimensional scans of specimens that would allow researchers to remotely measure body parts. Other technological advances, such as advanced DNA sequencing, are boosting the value of collections, allowing researchers to identify species that were previously indistinguishable from their closest relatives. James Hanken, a herpetologist and director of the Harvard University Museum of Comparative Zoology in Cambridge, Massachusetts, has used DNA sequencing to study  Thorius , a genus of pygmy salamander that is endemic to Mexico. For more than 100 years, no one was able to distinguish most  Thorius  species from each other. \u201cThey're very tiny animals,\u201d says Hanken. \u201cThey're hard to tell apart just by looking at them.\u201d But DNA sequencing helped Hanken to describe and name 14 species, all of which have been declared endangered by the International Union for Conservation of Nature. Usually, says Hanken, once genetic data have identified a species, he can find subtle features \u2014 in the skeleton, coloration or body size \u2014 that allow him to tell the animals apart. In biodiversity work, researchers are increasingly using DNA barcoding, a molecular technique that relies on characteristic genetic sequences to identify a species. But a DNA barcode cannot tell a researcher anything about how a particular species of bat flies, for example. Collections are often the best, or only, option in those cases \u2014 and that message has not been heard, either by the public or by funding agencies, say some researchers. \u201cWe're not very good at quantifying the benefits of collections,\u201d says Christopher Norris, senior collections manager at the Yale Peabody Museum of Natural History in New Haven, Connecticut. \u201cWe've not been very good historically at explaining to people in nuts-and-bolts terms why it matters that we understand biodiversity.\u201d \n               Stored viruses \n             Some scientists see applications for collections beyond documenting new species and studying biodiversity. The Bernice Pauahi Bishop Museum collection in Honolulu, for example, contains millions of mosquito specimens, which might tell virologists about the dynamics of mosquito-borne pathogens. Ten years ago, says Norris, researchers assumed that preservatives would have degraded the DNA of any pathogens in a specimen. But studies are showing that it is possible to recover and analyse viral DNA from museum specimens. In 2012, researchers were able to study the evolution of a retrovirus by extracting viral DNA from 120-year-old koala skins and comparing it with DNA found in skins from the 1980s 4 . Norris says that the same could be done with bats to help track diseases such as Ebola. (Researchers strongly suspect that bats triggered the recent outbreak in West Africa.) \u201cYou could go into museum collections and you could prospect for viral DNA,\u201d says Norris. The AMNH alone has more than 125,000 bat specimens from around the world. \u201cI guarantee there is something out there that is probably more scary than Ebola that we haven't encountered yet.\u201d But thoughts of deadly diseases are far from the mind of Moratelli as he bends to his work at the Smithsonian, calipers in hand. He carefully measures another bat, enters the data into his spreadsheet and places the animal onto a tray. Measure and repeat. In cabinets within reach, he has yet more specimens on loan from museums in Pennsylvania, Louisiana and California. Last year, while at Texas Tech University in Lubbock, Moratelli discovered what appeared to be a specimen of an unknown species of Guyanese bat. He will know for certain later this year when he travels to Canada to compare the specimen to a large collection of several hundred bats from Guyana. A few years ago, he travelled to the French National Museum of Natural History in Paris to inspect just two specimens. In the months ahead, Moratelli will repeat the measurement process thousands of times, and he knows he will discover new species. For some of these \u2014 critically endangered bats with dwindling habitats \u2014 his findings might help to avert extinction. For others, it is already too late. \n                     Biodiversity: Life \u00ad\u2013 a status report 2014-Dec-10 \n                   \n                     Taxonomy: The spy who loved frogs 2013-Sep-11 \n                   \n                     Chicago's Field Museum cuts back on science 2012-Dec-20 \n                   \n                     Superstars of botany: Rare specimens 2012-Apr-25 \n                   \n                     Biodiversity: On the origin of bar codes 2009-Nov-18 \n                   \n                     Blog post: Herbaria: botany's final frontier \n                   \n                     Nature  special: Biodiversity \n                   \n                     Nature  web focus: Linnaeus at 300 \n                   \n                     IUCN Red List \n                   \n                     Integrated Digitized Biocollections \n                   \n                     Society for the Preservation of Natural History Collections \n                   \n                     The Global Registry of Biorepositories \n                   Reprints and Permissions"},
{"file_id": "517132a", "url": "https://www.nature.com/articles/517132a", "year": 2015, "authors": [{"name": "Sara Reardon"}], "parsed_as_year": "2006_or_before", "body": "Twenty-four years after the conflict ended, scientists and veterans are still fighting for recognition of Gulf War illness. The meeting last April was supposed to be a scientific review, but the scene looked more like boxers lining up for their turn at a punchbag. The target was Robert Jesse, who at the time was deputy undersecretary for health at the US Department of Veterans Affairs (VA). Veterans, scientists and VA administrators were meeting in Washington DC to discuss Gulf War illness, a complex disorder that affects some 250,000 veterans of the 1990\u201391 military operations in the Gulf. After 24 years, the condition is still the subject of intense controversy in the United States and the United Kingdom. \u201cFrom the beginning, the VA has refused to honestly face the problems that face veterans,\u201d said Joel Graves, a Gulf War veteran who until last year had served on a committee advising the VA on research priorities related to the illness. Graves and others contend that the agency has refused to recognize Gulf War illness as a unique physiological condition, maintaining instead that it is psychosomatic or the result of stress. The VA, they claim, has obstructed research into Gulf War illness, stacked scientific review panels with members who would favour a psychological explanation and defanged the research advisory committee (RAC) that Graves served on. As a result, critics contend, thousands of soldiers have found it difficult to get a diagnosis or related health benefits. At the meeting, James Binns, an attorney and chair of the RAC, called the VA's actions \u201cmorally and intellectually bankrupt\u201d. When Jesse finally had a chance to speak, he flatly denied that the VA has clung to a psychological origin for the illness: \u201cWe have said unequivocally we do not believe that.\u201d Rather, he and other VA officials believe that the veterans' health problems are too complex to classify. Jesse then pointed to the VA's multimillion-dollar research programme as proof that the agency takes the illness seriously. The rift between scientists, veterans and the VA is deep, but upheavals at the agency in the past year may create an opportunity for reconciliation. \u201cI don't think it's a hopeless cause,\u201d says Victor Kalasinsky, an environmental toxicologist and the VA's programme manager for Gulf War illnesses, \u201cbut people need to listen to one another.\u201d In 1990 and 1991, the United States deployed some 700,000 military personnel to the Gulf to form a coalition with the United Kingdom, Saudi Arabia and several other countries to expel Iraqi forces from Kuwait. The seven-month campaign resulted in few coalition casualties. But soon after returning home, about 30% of US veterans began to get sick. Their illnesses were difficult for doctors to understand. They shared a cluster of symptoms \u2014 including severe fatigue, chronic pain, gastrointestinal disorders and cognitive problems. But few individuals had all of the symptoms, and there were many proposed causes. The destruction of chemical-weapons repositories was a leading suspect. Troops also marched past burning oil wells, slept in tents doused with pesticides and received new vaccines and pills to protect them from diseases and biological and nerve agents. \n               Brain and body \n             Government agencies such as the VA, the US Department of Defense and the UK Ministry of Defence were initially reluctant to conclude that something unique was happening \u2014 veterans experience health problems after every war \u2014 and many doctors dismissed it as a form of post-traumatic stress disorder (PTSD) or even a psychosomatic condition. But it was clear early on that something else was at play, says Roberta White, a neuroscientist at Boston University in Massachusetts who would eventually become scientific director of the RAC. Several early studies showed the same constellation of symptoms in veterans who had been deployed to the Gulf \u2014 particularly those exposed to chemical agents and pesticides \u2014 but not in veterans of the same era who had served elsewhere 1 . Evidence emerged that exposure to organophosphate chemicals in pesticides and the nerve agent sarin, and to nerve-gas prophylactics, were the probable cause. For example, a set of genetic analyses 2  by epidemiologist Robert Haley of the University of Texas Southwestern in Dallas found that symptoms were more severe in veterans who had a less active version of an enzyme that breaks down organophosphates. Veterans with fatigue and pain were more likely than controls to have brain-scan signals suggestive of nerve-fibre damage in certain areas of the brain 3 , 4 . And veterans experienced cognitive and movement problems similar to those in farmers exposed to high levels of pesticides 5 . But as research progressed, experts began to suspect that US and UK government agencies were favouring a psychological explanation. This is hardly unprecedented: people with complex conditions such as chronic fatigue syndrome and the pain disorder fibromyalgia, which have some overlap in symptoms with Gulf War illness, have fought for years for acceptance by the scientific and medical community with limited success. Veterans who felt that they had been ignored or marginalized after serving their country found allies among elected officials. In 1997, a congressional report 6  determined that the VA and Department of Defense were concentrating too much on psychological causes, and called their research programme \u201cirreparably flawed\u201d. Among other findings, it charged that the agencies had lost or hidden chemical-exposure data, biased research funding towards psychiatric conditions, and made research impossible by automatically diagnosing veterans with PTSD and ignoring chemical exposures. The report prompted Congress to create the RAC to evaluate the VA's Gulf War research programme and advise on how to improve it. Its members would be appointed by the VA but would perform their analyses independently. Binns, a former defence-department official, was appointed as its first director in 2002, accompanied by 11 scientists and veterans. In 2008, the RAC produced a 454-page report 7  establishing that Gulf War illness was a distinct disorder tied to chemical exposures, with little to no role for stress and psychological factors. \u201cI think that public opinion changed about Gulf War illness being a physical disease versus a psychiatric one\u201d after this report, says White. \u201cThat was a big sea change for the field.\u201d The VA says that the report changed its thinking, too. But it continued to call the condition \u201cchronic multisymptom illness\u201d or \u201cundiagnosed illness\u201d, incensing veterans and researchers. The VA has resisted giving it an official name because it is difficult to establish diagnostic criteria for a unique syndrome. Epidemiologists have proposed several definitions, but they tend be either too narrow to account for the diverse symptoms or so broad as to be meaningless. Although anyone who served in the Gulf War is eligible for health care paid for by the agency, it can be difficult for veterans trying to claim disability benefits to prove that their disability is connected to the war. Of the 54,000 claims for Gulf War-related illnesses filed as of last March, nearly 80% were denied, although more than half of those denied received compensation for other service-connected conditions. In Britain, which sent more than 53,000 personnel to the Gulf War, the Ministry of Defence agreed in 2005 to use 'Gulf War syndrome' as an umbrella term \u201cto provide an element of closure for those who have sought some acknowledgment that their ill-health is connected to their Gulf service\u201d, says a government website. Still, advocacy groups such as the National Gulf Veterans and Families Association in Hull have found that many Gulf War veterans have difficulties claiming disability benefits. In the United States, failing to adopt a standard definition for Gulf War illness has also complicated research. It has made it difficult to compare veterans who have the condition with those who do not, says RAC member Beatrice Golomb, a neurobiologist at the University of California, San Diego. What is more, the VA's database defines Gulf War veterans as anyone who was deployed to the Gulf after 1990, and most in that category have not had the same chemical exposures as those in the 1990\u201391 conflict. That dilutes the sample, says Golomb. Critics say that the VA has also hobbled an important research tool. Roughly every ten years, the agency conducts a survey of 30,000 veterans from the Gulf War era, asking about their health, symptoms and treatments. But the questions have changed every decade, making it hard to accurately track changes in health over time. The most recent version, sent out in 2012, included many questions about psychological stress, but not key questions that critics say are necessary to diagnose Gulf War illness. The RAC did not see the questionnaire before it went out, and when the committee pointed out the survey's flaws, the VA responded that it would be too expensive to change it. \n               Institutional support \n             As well as doing its own research and funding other researchers, the VA has commissioned a series of reports on Gulf War veterans' health from the US Institute of Medicine (IOM). But critics from the RAC and elsewhere have claimed that the VA stacked the IOM committees with members who favoured psychological explanations for the disorder, and that it tailored the questions that the committees were to study or the pool of studies they were to review. For the first time in 2010, an IOM committee specifically looked for a link between Gulf War service and chronic multisymptom illness. Although the report 8  determined that a link exists, it said that there was not enough evidence from the human studies it had assessed to finger specific toxic exposures as a cause. A 2014 IOM report 9  urged the VA to start calling the disorder Gulf War illness. Although still hazy on the disease's origins, it recommended that the VA adopt two sets of diagnostic criteria that it could choose between, depending on its research needs. One, from the US Centers for Disease Control and Prevention, is broad; the other, developed by former RAC member Lea Steele, an epidemiologist at Baylor University in Waco, Texas, has stricter criteria and excludes people with known psychiatric disorders. Although Binns sees the report as a great foundation for defining the disorder, some argue that the approach is a sign of equivocation. Bradley Flohr, senior adviser for compensation service at the VA, says it shows that even experts at the IOM are unable to agree. The major limitation that the IOM has dealt with is the quality of the studies available for review, most of which are not comprehensive or have weaknesses in their methodology, says Kenneth Shine, former IOM president and head of several of its committees. He is not confident that any biomarker or discovery will make a more precise definition possible. \u201cWe have to say that the longer it goes from the deployment, the less likely it is that there will be a firm definition,\u201d he says, because the veterans are ageing and acquiring more illnesses that muddy the picture. Kalasinsky says the agency does plan to adopt a definition for research purposes, but not necessarily for medical claims. As  Nature  went to press, the VA was still considering how to respond to the IOM's recommendations, nearly nine months after the report's release. \n               All change \n             Interactions between the VA and its RAC had long been adversarial (see 'A tense relationship'). But things came to a head in 2013 when, at a congressional hearing, members of the RAC testified that they had \u201cno confidence\u201d in the VA's research programme. Although the VA inspector-general found no evidence of wrongdoing, an investigation by the congressional committee on veterans' affairs has backed the RAC's claims that the agency was misappropriating money for Gulf War research and stacking IOM committees in its favour. \n               boxed-text \n             In June that year, the VA announced that it would be replacing most of the RAC. Members are supposed to serve for 2 years, but many had served for 12, so the VA said it was simply upholding the rules. It also rewrote the RAC's charter, stripping the mandate to evaluate the effectiveness of the VA's research programme and limiting the committee to reviewing current research and providing advice. Existing members of the RAC see the move as retaliation. They say that they welcome new expertise on the committee, but they worry that the VA will try to appoint people who push a psychosomatic explanation. E-mails obtained by  Nature  show Jesse proposing candidates for three scientist positions; two have expertise in psychosomatic illness and stress. Binns objected vigorously, and Jesse withdrew the nominations. The VA would not discuss the search for candidates, but Kalasinsky denies that the decision to replace members was retaliatory and says that the changes to the charter were \u201cnot as draconian\u201d as the RAC members believe. But things are changing at the VA, which has suffered high-profile problems in the past year. The agency was thrown into chaos after an unrelated scandal in which VA hospitals falsified records to hide how long veterans were waiting for care. VA secretary Eric Shinseki resigned in May, and many officials, including Jesse, have moved positions. As a result, goals such as adopting a case definition for Gulf War illness have been delayed, says Kalasinsky. At the RAC's most recent meeting, in September, several of the soon-to-be-dismissed members eagerly anticipated an appearance by the new VA secretary, Robert McDonald \u2014 a visit that Shinseki had never made. When he arrived, McDonald assured researchers and veterans that the VA believed that the veterans' suffering was real. In an interview with  Nature , McDonald said that he was busily educating himself on the illness. \u201cVeterans believe they're not getting the care they need,\u201d he says. \u201cOur job is to get to the bottom of this.\u201d Although he stopped short of saying that the disorder was something distinct, he said that the IOM's publications \u201cseem to say it is very real\u201d. Binns and Kalasinsky take McDonald's visit as a hopeful sign for reconciliation. But both acknowledge that it will be difficult. \u201cThe RAC makes recommendations. If they expect us to implement them all, that's not being realistic,\u201d says Kalasinsky. Nor is it accurate or helpful, he says, to continue suggesting that the VA supports the idea that Gulf War illness is psychosomatic. He says he does believe that veterans sometimes hear this from VA doctors, but that can be corrected. \u201cWe simply have to do a better job getting the word out and improve on our education programmes. I think the secretary is very committed to that.\u201d \n                     Science and war 2014-Jul-02 \n                   \n                     Some Gulf War veterans have different brains 2007-May-01 \n                   \n                     Does Gulf War syndrome exist? 2004-Nov-18 \n                   \n                     Sick veterans pin hopes on Gulf War inquiry 2004-Aug-18 \n                   \n                     UK government accused of hindering Gulf inquiry 2004-Jul-21 \n                   \n                     Combat raises risk of rare disorder 2004-Apr-30 \n                   \n                     Enzyme key to chemical weapon weakness 2003-Mar-17 \n                   \n                     Research Advisory Committee on Gulf War Veterans' Illnesses \n                   Reprints and Permissions"},
{"file_id": "518470a", "url": "https://www.nature.com/articles/518470a", "year": 2015, "authors": [{"name": "Alexandra Witze"}], "parsed_as_year": "2006_or_before", "body": "Leslie and Eliot Young have spent their lives studying Pluto. Now they are gearing up for the biggest event of their careers. In a spare conference room in Boulder, Colorado, planetary scientists Leslie and Eliot Young quiz a graduate student to prepare him for his upcoming exams. They take their task seriously, interrupting often as he answers questions about Pluto and Neptune's moon Triton. Leslie makes a technical comment about the light reflecting off those distant worlds. Then, Eliot notes that Pluto and Triton may have started out very similar to one another in the early Solar System before evolving down different paths. \u201cIt's a classic case of nature versus nurture,\u201d he says. \u201cThey are siblings.\u201d So, too, are the Youngs. Eliot and Leslie grew up as the oldest children of an astronautics researcher, and their mutual interests converged on one dwarf planet. \u201cThey are the only brother\u2013sister Pluto team in the Solar System,\u201d says Alan Stern, a planetary scientist and principal investigator of NASA's New Horizons mission, which has been hurtling towards Pluto for the past nine years. Kerri Smith discusses this year\u2019s missions to Pluto and Ceres with reporter Alexandra Witze. The Youngs and other Pluto researchers will be gearing up over the next few months as New Horizons finally nears its quarry, 4.8 billion kilometres from Earth. A telescope on the spacecraft has already begun capturing fuzzy pictures of Pluto, which will grow sharper as the probe closes in. And when New Horizons passes within 12,500 kilometres of Pluto on 14 July, it will provide the first close-up look at the world's icy surface, and the best chance yet to answer major questions about the evolution of the outer Solar System (see  page 468 ). The fly-by will mark a major milestone in both the personal and the professional lives of the Youngs, who occupy adjoining offices at the Southwest Research Institute in Boulder. Over the past quarter of a century, their careers have intersected with Pluto science at key points, from helping to discover the dwarf planet's atmosphere to making some of the first detailed maps of its enigmatic surface. Whatever New Horizons finds this year will build in large part on work done by the siblings. \u201cWe've had some ideas about how Pluto works for decades now,\u201d says Eliot. \u201cWe'll finally find out if they are right.\u201d \n               Family orbit \n             When Eliot and Leslie were growing up in Newton, Massachusetts, family life revolved around their father, Larry Young, a legendary researcher at the Massachusetts Institute of Technology (MIT) in Cambridge. Young specializes in the biological effects of weightlessness, and he trained to fly on the space shuttle although he never went into orbit. Eliot, Leslie and their younger brother, Robert, sometimes played poker with visiting astronauts. Larry Young was also a passionate skier who studied skiing injuries. On most winter weekends, his family took a long car trip to New Hampshire that was filled with brain games and chatter about mathematics. Larry is not surprised that his two oldest children pursued science, but he never imagined them both studying the same dwarf planet in the distant reaches of the Solar System. \u201cI think it's the closest they could get to doing science fiction and still earn a living,\u201d he says. (Robert ended up in software development.) Eliot felt the pull of Pluto first. As a graduate student at MIT in the late 1980s, he worked with Jim Elliot and Ted Dunham, who were building instruments for airborne astronomy missions, including ones to study Pluto. But his sister, three years his junior, quickly followed. One day, she stopped in at his lab to show him a piece of computer coding she had done. Even though she was still an undergraduate at nearby Harvard University, Jim Elliot was impressed enough to offer her a job working on software. The MIT team specialized in studying distant worlds using stellar occultations \u2014 when an object of interest moves between Earth and a background star. By measuring how much the light dims, planetary scientists can determine the size of the blocking object. And by noting whether the light dims abruptly or gradually, they can deduce whether that object has an atmosphere. Pluto is so small (about two-thirds the size of Earth's Moon) and far away (between about 30 and 50 times farther from the Sun than Earth is) that astronomers need to use every creative technique they can think of to tease out information. \n               Skywatch \n             One night in June 1988, several members of the MIT group took off from Honolulu, Hawaii, in the Kuiper Airborne Observatory, a telescope-carrying plane that flew above the obscuring effects of Earth's atmosphere. Astronomers at the time suspected that Pluto had an atmosphere, but no one had ever spotted it. Leslie Young was not yet a graduate student, but she was on the plane to help with the measurements. She distinctly remembers the excitement as the star's light dimmed gradually, and Pluto's long-sought atmosphere was revealed 1 . The discovery, supported by ground-based measurements of the same occultation, made front-page headlines. When her brother Robert asked how it felt to have rewritten the textbooks, \u201cI told him it felt pretty good,\u201d she says. Pluto remained a fuzzy dot on a map of the outer Solar System, even as other distant planets were coming into focus. NASA's Voyager 2 spacecraft had visited Uranus in 1986, and three years later it swept past Neptune but did not go near Pluto. Researchers knew little about that world, other than that its surface seemed to be mostly ice, rather than rock, and it was accompanied by a moon, Charon, which was half the size of the dwarf planet itself. (Pluto was demoted from planet status in 2006.) Helped by her keen coding skills, Leslie went on to make a series of major discoveries as part of the MIT team, including spotting methane in Pluto's atmosphere 2  and nitrogen ice on its surface 3 . \u201cIf somebody asks me what's my favourite colour,\u201d she says, \u201cI say 2.15 microns\u201d \u2014 the wavelength of the light absorbed by the frozen nitrogen on Pluto's surface. In the years that followed, Leslie developed computer models to describe how the surface and atmosphere of Pluto interact. Because the orbit of the dwarf planet is extremely stretched out in an elongated ellipse, the amount of sunlight reaching its surface changes markedly throughout the Pluto year, which lasts 248 Earth years. When Pluto's orbit carries it closer to the Sun, methane, nitrogen and other ices on the surface sublimate and form a tenuous atmosphere, roughly one millionth the thickness of Earth's. Some researchers argue that as Pluto gets farther away from the Sun in the coming years, the gases in the atmosphere will refreeze and drop to the surface, although Leslie's latest models suggest that the atmosphere never completely disappears 4 . Occultation studies 5 , 6  indicate that the density of Pluto's atmosphere doubled between 1988 and 2002 and has stayed pretty much constant since then. So one of New Horizons' major goals at Pluto is to unravel the icy interplay between the surface and the atmosphere. \n               Face of Pluto \n             The big brother who got Leslie into Pluto has made his own mark. During and after his graduate studies, Eliot worked to map the face of Pluto by taking advantage of a geometric coincidence. Between 1985 and 1990, the orbital planes of Pluto and Charon tilted such that the two worlds regularly passed in front of one another as seen from Earth, allowing astronomers to watch a series of mutual eclipses. By measuring how Pluto's face dimmed as sections of it disappeared from view, researchers could work out which areas were dark and which were light \u2014 a property called albedo. Eliot was one of several scientists piecing together these mosaics to produce maps of the dwarf planet's surface 7 . The maps were far from perfect: \u201cThe resolution is like somebody with a strong glasses prescription getting drunk and going to look at the Moon,\u201d says Eliot. But they provided some of the first real knowledge about what Pluto might look like. \u201cIt was foundational information that got people excited about Pluto,\u201d says Marc Buie, a Pluto astronomer now at the Southwest Research Institute. He competed with Eliot to generate the Pluto maps, and gives him credit for inventiveness. \u201cHe came up with some ways of tackling the data that I never would have thought of in a million years,\u201d says Buie. Since then, the Hubble Space Telescope has managed to make sharper images of Pluto's surface. By around May this year, New Horizons will be close enough to Pluto to capture images better than Hubble's, and the dwarf planet will finally begin to come into focus. In the highest-resolution pictures, scientists should be able to pick out details as small as the lakes in New York City's Central Park. Eliot is perhaps best known for his mapping work, but says that his most useful contribution to Pluto science is a method for modelling occultation light curves. He is happiest at the interface between technology and space science, especially if a healthy dose of hardware is involved. \u201cI may still be an engineer at heart,\u201d he says. Dunham, now at Lowell Observatory in Flagstaff, Arizona, remembers Eliot building his own computers and housing them in cardboard boxes while in graduate school. In recent years, Eliot has spent less time on Pluto and more on pushing technical boundaries in another frontier of Solar System science \u2014 sending balloons above Earth's atmosphere to make planetary observations. His propensity for technical tinkering has served him in his favourite hobby, too. He developed a new timing system for races at a ski resort near Boulder, where he coaches a team. Eliot and Leslie's mother, Jody Williams, is not surprised that the siblings have ended up working closely together. During their childhood trips to New Hampshire, where they had no television or other distractions, Eliot and Leslie would play for hours, building small towns and fantasy worlds with shared rules. Now, Williams says, \u201cevery time Eliot runs into a problem, he calls Leslie, and she never lets him down\u201d. Today, Eliot retains a measure of his big-brother status: he is the gregarious one who often speaks for both of them and is known more widely among planetary astronomers. Leslie, more reserved, sometimes gets noticed initially because she is his sibling. \u201cThey learn I'm his sister and they figure I'm worth listening to,\u201d she says. Yet the siblings clearly revel in their close relationship. Both usually work some mention of each other into professional discussions within minutes, and they constantly ricochet ideas between each other's offices. \u201cThey know each other better than any of us know either of them,\u201d says Stern. \n               Fly-by frenzy \n             As Pluto scientists approach peak excitement this year, Eliot will be helping to coordinate a cadre of amateur astronomers across the Southern Hemisphere who will try to capture a major occultation as Pluto passes in front of a particularly bright star on 29 June. It is the last of these events before the New Horizons fly-by, and a crucial data point in the series of occultation studies dating back to the mid-1980s \u2014 the only long-term measurements of how Pluto's atmosphere has changed. Leslie will move to mission control at the Johns Hopkins University Applied Physics Laboratory in Laurel, Maryland, to prepare for intense data gathering. She is a deputy project scientist for New Horizons and the chief architect of the details of the fly-by, a job that involves a steady diet of teleconferences and spreadsheets to coordinate what observation will be made by what instrument at what time. The mission team has carried out several dry runs of the entire encounter, trying to anticipate every possibility in what will be an action-packed few days leading up to and after the closest approach. \u201cI've been working for the future for 15 years, and the pay-off is coming this summer,\u201d says Leslie. Among other things, she has developed alternative trajectories for the spacecraft to divert to if it seems to be heading for a particularly dusty patch of space. With the spacecraft moving at nearly 50,000 kilometres per hour, collisions with dust particles could endanger it or damage instruments. Cathy Olkin at the Southwest Research Institute, who was part of Jim Elliot's MIT group a few years after Leslie and is another New Horizon's deputy project scientist, says that observing occultations was good training for making high-stress, time-crucial measurements. She and Leslie have chased Pluto's shadow across islands in the Pacific and during snowstorms in New Zealand. \u201cWe know the value of telescope time and being prepared and having thought out what we're going to do at each step in time,\u201d says Olkin. \u201cWe know we have to get the data.\u201d Back in her office, Leslie Young takes down her 1978 edition of David Halliday and Robert Resnick's  Fundamentals of Physics  textbook. She opens it to the back, to the reference table listing characteristics of Solar System objects. For Mercury, Venus, Earth and the rest of the planets, the table looks reassuringly full. For Pluto, the data column is incomplete. One-third of the entries are question marks. Many of the rest are flat-out wrong. \u201cMoons: none,\u201d it reads. (Charon had yet to be discovered.) \u201cAtmosphere: none.\u201d (Ditto.) Leslie runs her finger down the column, ticking off the Pluto discoveries that she and her big brother have been involved in. Atmosphere, radius, albedo, surface temperature \u2014 all key to understanding this curious little world. In July, she and Eliot hope to help fill in many of the remaining question marks. And maybe even add some new ones. \n                     Pluto-bound probe faces crisis 2014-May-20 \n                   \n                     Hubble telescope spots a 5th Plutonian satellite 2012-Jul-12 \n                   \n                     Pluto voyage: A man with a mission 2005-Aug-03 \n                   \n                     Nature  Special: Pluto \n                   \n                     New Horizons mission \n                   Reprints and Permissions"},
{"file_id": "518474a", "url": "https://www.nature.com/articles/518474a", "year": 2015, "authors": [{"name": "Sara Reardon"}], "parsed_as_year": "2006_or_before", "body": "Brain-scanning techniques promise to give an objective measure of whether someone is in pain, but researchers question whether they are reliable enough for the courtroom. Annie is lying down when she answers the phone; she is trying to recover from a rare trip out of the house. Moving around for an extended period leaves the 56-year-old exhausted and with excruciating pain shooting up her back to her shoulders. \u201cIt's really awful,\u201d she says. \u201cYou never get comfortable.\u201d In 2011, Annie, whose name has been changed at the request of her lawyer, slipped and fell on a wet floor in a restaurant, injuring her back and head. The pain has never eased, and forced her to leave her job in retail. Annie sued the restaurant, which has denied liability, for several hundred thousand dollars to cover medical bills and lost income. To bolster her case that she is in pain and not just malingering, Annie's lawyer suggested that she enlist the services of Millennium Magnetic Technologies (MMT), a Connecticut-based neuroimaging company that has a centre in Birmingham, Alabama, where Annie lives. MMT says that it can detect pain's signature using functional magnetic resonance imaging (fMRI), which measures and maps blood flow in the brain as a proxy for neural activity. The scan is not cheap \u2014 about US$4,500 \u2014 but Steven Levy, MMT's chief executive, says that it is a worthwhile investment: the company has had ten or so customers since it began offering the service in 2013, and all have settled out of court, he says. If the scans are admitted to Annie's trial, which is expected to take place early this year, it could establish a legal precedent in Alabama. Most personal-injury cases settle out of court, so it is impossible to document how often brain scans for pain are being used in civil law. But the practice seems to be getting more common, at least in the United States, where health care is not covered by the government and personal-injury cases are frequent. Several companies have cropped up, and at least one university has offered the service. Reporter Sara Reardon explores whether brain scans for pain are ready for the courtroom, with Geoff Marsh. The approach is based on burgeoning research that uses fMRI to understand the nature of pain \u2014 a very subjective experience. Scientists hope that the scans can provide an objective measure of that experience, and they see potential applications, such as in testing painkillers. But many neuroscientists say that the techniques are still far from being accurate enough for the courtroom. Critics say that the companies using them have not validated their tests or proved that they are impervious to deception or bias. And whereas some think the technologies will have a place in legal settings, others worry that the practice will lead to misuse of the scans. \u201cThere's a real desire to come up with some more-objective proxy for pain,\u201d says Karen Davis, a neuroscientist at the University of Toronto in Canada. But such measures must be extremely accurate, she says. \u201cThe outcome of having a wrong answer can be quite catastrophic.\u201d \n               Neural origins \n             The methods that doctors commonly use to assess pain can seem crude. People are asked to rate their pain on a scale from one to ten, or choose from a row of cartoon faces that go from happy to anguished. These measures can help to chart changes in pain, as someone recovers from surgery, for example. But each person will experience and rate their pain differently, so one person's five could be worse than another's seven, and a nine might or might not be bad enough to keep someone from working. An objective answer should lie in the brain, where the experience of pain is ultimately constructed. And although every experience is different, pain should share some common elements. Neuroscientist Tor Wager at the University of Colorado Boulder has been trying to decipher pain's signature in the brain by placing people in an fMRI scanner while they touch a hot plate. As the researchers turn the plate's temperature up and down, they record the activity across different parts of the brain, including the sensory regions associated with the hand. From these patterns, Wager says, they can predict with better than 90% accuracy whether the plate is just warm or painfully hot 1 .  There's a real desire to come up with some more-objective proxy for pain.  But this measures acute pain \u2014 the immediate response to an obvious stimulus. Chronic pain, like Annie's, affects hundreds of millions of people worldwide. And although its cause can be obvious, that is not always the case. Vania Apkarian of Northwestern University in Chicago, Illinois has scanned dozens of individuals soon after a back injury and then again over the course of a year or more. The pain went on to become chronic in roughly half of those people, and even though they described the pain the same way throughout, Apkarian could detect a shift in the pain signature in their brains 2 . It changed from a signal of activity in the insula, which is associated with acute pain, to one of activity in the medial prefrontal cortex, which processes cognitive behaviour, and the amygdala, which controls emotion. \u201cOur interpretation is that the pain is becoming more internalized,\u201d Apkarian says. This and other work suggests that there is an emotional component to chronic pain that is not necessarily involved in acute pain. Chronic pain and depression often coexist and reinforce one another. And some chronic pain can be eased with antidepressant drugs. But Wager cautions that focusing on these links can be treacherous. Suggesting that pain is all in the head \u2014 even if that is technically the case \u2014 does not mean that it is imagined or faked. \u201cPeople will always go to that black and white line,\u201d he says. That line is a particular challenge in legal settings. \u201cA person cannot be found disabled based on pain unless they can point to a specific cause,\u201d says Amanda Pustilnik, a legal expert at Harvard Law School in Cambridge, Massachusetts. \n               Isolated instances \n             The United States sees tens of thousands of injury lawsuits every year, most of which involve claims of unresolved pain. But that might be unusually high \u2014 countries with national health systems, such as Canada, see fewer lawsuits, says Davis. So far, the only pain case involving brain-imaging techniques known to have progressed to trial involved a truck driver named Carl Koch, whose wrist was burned by a glob of molten asphalt in 2005. A year later, he said he was still in pain and sued his former employer, Western Emulsions in Tucson, Arizona, for damages. Koch had had his brain scanned by Joy Hirsch, a neuroscientist who was running the fMRI Research Center at Columbia University in New York City. Hirsch had developed a method that she says can \u201ctap into\u201d chronic pain. Lightly touching the affected wrist provoked a signal in sensory regions and other brain areas associated with pain; touching the other wrist did not. The test, she says, is a well-characterized way to distinguish allodynia \u2014 a pain response to a stimulus that does not normally cause pain \u2014 from imagined pain. At the trial, Western Emulsions called Sean Mackey, a neurologist at Stanford University in Redwood City, California, as an expert witness. Mackey maintained that pain is too subjective to measure in this way and that the signature Hirsch was detecting could have been produced if Koch had expected to feel pain in the affected wrist or was unduly concentrating on it \u2014 deliberately or not. Hirsch argued that there are known signals for imagined pain that were not apparent in the scans. Ultimately, the judge admitted the scan, and the case settled for $800,000 \u2014 more than ten times the company's initial offering, according to Koch's lawyer, Roger Strassburg. Another issue, Mackey says, is that it might be possible for people to cheat the test. In a 2005 study, he instructed volunteers to lie in an fMRI scanner and touch a hot plate while he showed them a video of flames that became more or less intense on the basis of their brain activity. Given this visual feedback, volunteers were able to control the intensity of the flames by imagining the pain as being more or less severe than it actually was 3 . Mackey is looking into the technique as a way to control chronic pain, but he is also studying whether people can trick the scanner. After the Koch case, the use of such techniques began to pick up. Hirsch, who is now at Yale University in New Haven, Connecticut, says that while she was at Columbia, she had been doing two to three pain-related scans per month, many of which were to support lawsuits. She is hoping to offer the service at Yale. A main criticism of the various techniques being used in civil suits is the paucity of publications to validate them. Hirsch has not published anything on her method, but says that she does not think it is necessary. The way in which different body parts are represented in the brain has been well mapped, she says, and the scans she has done provide no further insight than answering whether or not the person was in pain. MMT takes a somewhat different approach: it compares scans before and after an individual engages in a painful activity. For example, Annie was scanned before and after walking around, and the company claimed that it could detect a clear pain signal in the second scan. But the company's only publication, led by co-founder and chief science officer Donald Marks, has been a single case study. After the person did something painful, a brain scan revealed particularly strong activity in the insula, which is involved in consciousness and self-regulation, and the somatosensory cortex, which processes sensations from the various parts of the body 4 . These regions are involved in pain, but they are also involved in many other things. \u201cIf you went to a Society for Neuroscience meeting and walked into any non-pain-related slide session, you'd see the same regions being talked about,\u201d Davis says. Getting a patient such as Annie to walk around between scans would not only cause her pain, but also increase her awareness of her back, which would activate the insula. Davis, who does not think that pain imaging should be used in court for this purpose, says that she finds it disturbing that Marks's study cites her work, which measured a different kind of brain activity. \u201cIt's quite shocking for them to be quoting studies that don't back up their technology at all,\u201d she says. Moreover, the test cannot be validated in a single person, Wager says. Any number of confounding factors \u2014 emotion, expectation, or head movement in the scanner, for instance \u2014 could account for the signals the company sees. To prove that the method is valid, the researchers would have to show that the signals differ between people in pain and controls, he says, and that there is a biological mechanism that accounts for the signal. Without that, \u201cit's like reading tea leaves\u201d. Marks disputes this, saying that numerous studies, including Wager's, have shown that fMRI can reliably distinguish between pain states. \u201cMy work is an application on an individual basis of all the data to date which validates this approach,\u201d Marks says. He also argues that the approach is not meant to determine whether or not someone who says they are in pain actually is, \u201cI'm taking individuals that everyone agrees have pain and providing a visual graphic representation of that pain.\u201d \n               Close to market \n             Using different techniques, Chronic Pain Diagnostics (CPD) of Roseville, California, is planning to offer commercial scans for litigants. CPD compares scans taken of a person's brain after they received an electric shock to a database of images from 30 individuals with and without chronic pain. People with chronic pain respond to a stimulus differently from healthy controls, and the company has developed an algorithm that allows it to distinguish between the two with 92% accuracy 5 . CPD president and co-founder Shaun England says that he expects a scan to cost between $5,000 and $6,000. Mackey says that the application is interesting and potentially useful if the technique is replicated in larger groups. But Apkarian says the sample size is too small to determine meaningful differences at this point. Just as in MMT's technique, background signals such as head movement could confound the interpretation. \u201cIf you simply blindly use it, there is a very good chance you will always find a difference\u201d between groups, he says. CPD's executive research director, Daniel Callan, says that the company has ways to control for outside factors that could affect its database, such as randomizing the order in which the patients are scanned and using people of different ages and genders. But he agrees that further experiments are needed to determine how well the algorithm works for individual patients. England says that the company hopes to start another study soon. Scientists' concerns about the validity of pain scans might not matter much to legal professionals and the courts, says Michael Flomenhaft, an attorney in New York City who specializes in chronic pain and neuroimaging. \u201cThere's a lot of scientific information that can't be stated with the level of certainty you'd need to present it at a scientific conference, but is confident and valuable in a legal setting.\u201d There is, however, evidence that brain scans could be overly persuasive to jurors. Research has suggested that the general public is more likely to accept poor arguments if they are accompanied by neuroscientific evidence 6 . In the Koch case, Mackey says, \u201cpretty brain pictures ended up being very compelling\u201d.  If we accept the logic that the brain imager knows, then we have to accept that it's going to win even in cases when we don't want it to.  The efforts to introduce pain imaging are similar in some ways to attempts over the past decade to use fMRI as a lie detector. Most researchers question the reliability of this technique. It is difficult to validate because study volunteers tend not to have the same motivations to lie as criminal defendants. But that has not stopped several companies from trying \u2014 thus far unsuccessfully \u2014 to have the evidence introduced in US courts. Pain imaging has been more successful owing to richer research on the topic. And the stakes are much lower for a civil case than in a criminal trial, so the bar for what constitutes evidence is lower, according to an analysis in the  Journal of Law and the Biosciences 7 . But some scientists and ethicists are concerned about where the increasing acceptance of pain imaging might lead. Pustilnik worries that it could become a sort of pass\u2013fail test, not just forcing litigants to provide proof of their pain, but potentially making it a requirement to get prescription medications or insurance coverage. She is heading a working group at Harvard that is developing a list of ethical and scientific standards for the technologies before they become widespread. Levy and Marks insist that their technology is not capable of that. \u201cFundamentally, we can't prove that a patient does not have pain,\u201d Levy says, because an individual might still be experiencing pain even if the scanner does not show it. But that situation may be inevitable, says Stuart Derbyshire, a neuroscientist at the National University of Singapore. \u201cIf we accept the logic that the brain imager knows, then we have to accept that it's going to win even in cases when we don't want it to.\u201d Even so, many say that the research should continue to strive for application, including inside the courtroom. \u201cWe already make many wrong treatment and legal decisions about who is and is not in pain and who shouldn't be believed,\u201d Wager says. \u201cIf we had new information, that could help us do a better job.\u201d \n                 See Editorial \n                 p.456 \n               \n                     Tackling the US pain epidemic 2012-Jun-01 \n                   \n                     Science in court: Arrested development 2012-Apr-18 \n                   \n                     Clear up this fuzzy thinking on brain scans 2012-Feb-29 \n                   \n                     Clear up this fuzzy thinking on brain scans 2012-Feb-29 \n                   \n                     Science in court: Head case 2010-Mar-17 \n                   \n                     Neuroscience: Shooting pain 2009-Oct-28 \n                   \n                     Thought control brings pain into line 2005-Dec-12 \n                   \n                     Nature Podcast: David Eagleman discusses how we think about free will and the law \n                   \n                     Nature  special: Science in court \n                   \n                     Harvard Pain Symposium \n                   Reprints and Permissions"},
{"file_id": "519020a", "url": "https://www.nature.com/articles/519020a", "year": 2015, "authors": [{"name": "Katia Moskvitch"}], "parsed_as_year": "2006_or_before", "body": "Women are under-represented in physical sciences and in science in the developing world. Meet three who beat both sets of odds. Patchanita Thamyongkit was waiting patiently near the stage at a conference on the importance of science for Thailand, when the organizer rushed up to her and asked whether she had seen the next presenter, Professor Thamyongkit. \u201cThat's me,\u201d she replied. An awkward pause followed. \u201cOh, I thought you were his secretary,\u201d came his reply. The presenter was probably more embarrassed by the 2008 incident than Thamyongkit, who is used to being taken for a secretary. She is a physical organic chemist at Thailand's biggest scientific establishment, Chulalongkorn University in Bangkok, where she has won several awards for her work. But senior female scientists are a rarity in Thailand: top science positions are scarce, and many women are forced out of research because of cultural expectations that they will take care of their households, raise children and help ageing parents. Women around the world continue to face major challenges in pursuing a research career \u2014 particularly in the physical sciences. In the United States, women make up nearly half of college-educated workers and working life scientists, but only 30% of physical scientists. The gender gap widens higher up the professional scale. There is much debate about why: discrimination, conscious and unconscious bias in hiring and promotion, lack of role models and the demands of bearing and caring for children are all thought to play a part. In developing countries, female scientists can face even higher cultural and societal barriers, such as overt sexism, and a lack of contraception, reproductive choice or access to education. Many live in regions with desperate poverty, few high-quality schools, political instability and sometimes civil conflict. \u201cThere is a great, growing interest in science and engineering among women in developing countries,\u201d says Romain Murenzi, a physicist and former Rwandan science minister who is now executive director of TWAS, the World Academy of Sciences, which promotes the advancement of science in developing countries. Yet, he says, \u201cwomen face obstacles from their earliest years, and across the landscape of their lives\u201d. The situation varies enormously from one place to the next: figures from the United Nations Educational, Scientific and Cultural Organization (UNESCO) show that women in Thailand make up just over half of science doctoral students and researchers, whereas in Nigeria, those figures are 24% and 23%. Nature  talked to three women who are in this minority of minorities \u2014 working in physical sciences in developing countries \u2014 to find out what challenges they have faced and how they overcame them. Their stories highlight two of the ingredients for success: a supportive family and a huge dose of determination. Becoming a leading scientist involves \u201cconvincing your family first\u201d, says Thamyongkit, \u201cand then, for the rest of your life, going out of your way to prove to everyone that you're actually really good\u201d. \n               A CHEMIST IN THAILAND \n             \n               Patchanita Thamyongkit found that the academic pipeline for women is leaky. \n             Thamyongkit lives with her elderly parents in a narrow, three-storey house in blaring central Bangkok. The neighbourhood is a sea of concrete, with wall-to-wall houses. When she was young, her parents ran an automobile-parts shop and a book store. They were ambitious for their daughter, and wanted her to become a doctor. \u201cI am lucky in the sense that my parents have always supported me,\u201d says Thamyongkit \u2014 even when she chose to study physical sciences instead of medicine at Chulalongkorn University. Thamyongkit embarked on an academic world tour: she did a PhD in Germany and took a postdoctoral post in the United States before returning, speaking two foreign languages, to join the chemistry department at Chulalongkorn in 2005. She researches novel organic materials for optoelectronic applications, such as energy-efficient solar cells. Last November, she won a L'Or\u00e9al\u2013UNESCO prize for women in science. Equal proportions of men and women participate in Thai science education and scientific research as a whole. At Chulalongkorn, roughly 25% of faculty members are women in physics and 35% in chemistry, proportions roughly similar to those in the United States. Many universities have nurseries and schools on campus, and some researchers say that they do not perceive a gender gap at all. The country elected its first female prime minister, Yingluck Shinawatra, in 2011. But as in other nations, the academic pipeline for women is leaky. \u201cReally, really few actually reach the top,\u201d says Supot Hannongbua, dean of the faculty of science at Chulalongkorn. \u201cResearchers maybe publish one paper a year \u2014 it's very difficult for them.\u201d Out of 80 faculty members in the department of chemistry, he says, there may be between 5 and 7 people doing outstanding research or in senior positions, and rarely is one a woman. Part of the problem is that top research jobs are rare. Just 0.2% of the country's gross domestic product goes on research, and only around 20 of Thailand's 160 or so universities have proper research facilities. With so few opportunities, women often lose out. Most of the 12 female graduate students in Thamyongkit's lab late last year said that they wanted to work in industry, care for their families, or both.  Thailand has no particular and serious campaign to encourage women in science.  That efflux is also driven by Thai society's traditional expectations. Many women work, but it is assumed that they will get married and shoulder caring and domestic duties. Although overt sexism is frowned on, discrimination persists. \u201cIf a man and a woman candidate with exactly the same characteristics apply for a top scientific position, the man is more likely to get hired,\u201d says Hannongbua. Some advertisements for industrial jobs state that the applicant must be a man. Women can also struggle to make the right connections, says Tyrell Haberkorn, who studies Thai politics and history at the Australian National University in Canberra. \u201cWomen in Thailand face everyday sexism of the boy's-club variety, in which important decisions and relationships are forged outside the workplace and instead in the male-only spaces in which men go out and eat and drink together.\u201d One female chemist at Chulalongkorn, Parichatr Vanalabhpatana, says that she had to learn to drink beer to be accepted by her male colleagues. The Thai government has tried to institute change. Shinawatra promoted gender equality through a National Development Fund for Women, which offers low-interest loans to further women's welfare and occupations; and in 2007, Thailand criminalized domestic violence. But Thamyongkit, Hannongbua and others say that the government should do more. \u201cI still insist that Thailand has no particular and serious campaign to encourage women in science,\u201d says Thamyongkit. Overcoming entrenched societal attitudes \u201cwould take what would amount to a social revolution in how gendered roles are conceived\u201d, says Haberkorn. \u201cBut in truth, I think that continuing to open up education, particularly tertiary education, to a broader and broader group of people is the most concrete strategy.\u201d Thamyongkit has found her own way through. In her twenties, she decided to not have a family. \u201cI did want to have kids before, but if ten years ago I had had a family, I would have never managed to get to the same level in my career where I am now,\u201d she says. Now 38, she says that it is probably too late. As she walks through the campus, Thamyongkit passes a temple, where she takes a few seconds to stop, kneel and pray. \u201cI'm not religious,\u201d she says. \u201cBut I meditate, I say a few words to reinforce all the positive things in my life, to keep the positive energy flowing.\u201d At the end of the day, as Thamyongkit sits down with her family for dinner, her parents say that they would love to have a grandchild. But they don't pressure her, she says, and she is grateful for that. \u201cMy relatives want to know why I don't have a family. They say, 'You don't look that bad, so why? Why?' But my parents know I am happy, and for them it's the main thing.\u201d \n               A PHYSICIST IN NIGERIA \n             \n               Rabia Salihu Sa\u2019id negotiated societal and personal challenges to pursue research. \n             On a Sunday morning in April 2012, Rabia Salihu Sa'id, a 52-year-old professor of atmospheric and space-weather physics at Bayero University in Kano, northern Nigeria, was at home with her six children when she heard some shocking news. Fighters from the militant Islamist group Boko Haram had stormed a packed church on the university campus, killing 16 people and wounding many others, mostly students and academics. Sa'id lost two of her colleagues and friends in the attack, which was part of Boko Haram's campaign to build an Islamic state and stamp out Western education. Sa'id knew that she could have been one of those killed \u2014 but it did not stop her from teaching or doing research, and neither have further Boko Haram attacks. \u201cThere's a lot of security at the university now, you can't come in without being searched,\u201d she says. \u201cBut we are not afraid.\u201d Even before these threats, Sa'id's career was riddled with challenges. A growing number of girls in Nigeria receive an education, but in the north of the country \u2014 where Sa'id grew up \u2014 they often receive little schooling and get married in their teens, says Sa'id's colleague Babatunde Rabiu, director of the Centre for Atmospheric Research at Nigeria's National Space Research and Development Agency in Anyigba. The country is rated poorly in a 2014 index compiled by the Organisation for Economic Co-operation and Development to measure discrimination against women in areas such as law and social norms. Within science, physics is still very much considered a 'male' subject, says Rabiu. A report 1  that Sa'id co-authored in 2013 found that 5\u201320% of students enrolled in physics in Nigerian universities were women, and UNESCO figures show that women make up 24% of doctoral students in all areas of science. \u201cIt is common for single women who go for further degrees to be mocked by men and even by some females,\u201d says Rabiu; traditionally, women are expected to marry and stay at home.  I wanted to be in a position where my opinion will matter, where I will be respected.  Like Thamyongkit, Sa'id overcame these hurdles thanks to the encouragement of her family. Her father, an officer in the Nigerian Army, encouraged all of his children to pursue education and hoped that his daughter would become a doctor. Instead, she married straight out of school, at the age of 18. When her first son was born with a club foot, she shelved her higher-education plans. Soon, two more children followed. Aged 29, she decided to go to university at last. She did not have a high enough grade in chemistry to enter medicine, so she opted for physics. But first, there was the pressing question of how to pay for university. She sold a gold necklace from her bridal dowry to pay for the registration, and she ran a nursery school to cover the rest. \u201cMany women would not be able make it like I did, and would have to give up the dream of a university degree if they have no financial support,\u201d she says. Months after Sa'id began her studies, her fourth child was born with sickle-cell anaemia \u2014 an incurable genetic disorder. With two children needing medical care, \u201cIt was incredibly challenging for me\u201d, recalls Sa'id. \u201cI was really in and out of hospitals.\u201d Her extended family and friends helped out, staying with one boy or the other at hospital while she went to lectures. Sa'id was one of 4 women in a class of 21 who graduated in 1996. She says that a streak of ambition kept her going. \u201cI wanted to be in a position where my opinion will matter, where I will be respected, where my children could have the good things in life,\u201d she says. She went on to get a PhD, then a job as a graduate assistant and eventually a permanent teaching and research post. She now studies the effect of dust aerosols on climate \u2014 she has shown, for example, how the constituents of Saharan dust aerosols alter the aerosols' cooling or warming effects 2 . The challenges continue in her research: she has electricity for only four hours each day. Sa'id and other female scientists in Nigeria have found forms of support. Some research-funding agencies give female applicants particular encouragement or priority, and the Nigerian government has taken steps to reduce discrimination and increase opportunities for women overall. Four years ago, Sa'id and her colleagues formed Nigeria's Association of Women Physicists, to encourage more women in the country into the field. At their first conference, it became clear that all the women had had similar experiences. The group agreed to try to improve representation of women through mentoring, prizes for female students, efforts to improve school physics teaching and a meeting twice a year. Last month, at the annual meeting of the American Association for the Advancement of Science in San Jose, California, Sa'id received an Elsevier Foundation Award for Women in Science in the Developing World \u2014 she was one of five women honoured for work in physics and mathematics. \u201cI have learnt that with high motivation and hard work, one can succeed,\u201d she says. But women cannot progress without improvements in the country's research infrastructure as a whole, she says. \u201cThis is very important.\u201d \n               A PARTICLE PHYSICIST IN INDIA \n             \n               Rohini Godbole is one of her country\u2019s few leading physicists. \n             \u201cI have read your husband's papers \u2014 great work!\u201d exclaimed a German physicist as he shook hands with Rohini Godbole. The seminal papers on particle physics, he assumed, had been written by a man. But the author was Godbole herself \u2014 and the remark was one of many that she has deflected throughout her career. Based at the Indian Institute of Science in Bangalore, Godbole is one of her country's few leading physicists. In 1991, she discovered a way to describe the complex interplay of high-energy particles in linear colliders 3  with Manuel Drees, now at the University of Bonn in Germany. Godbole is now one of the 16 members of the International Detector Advisory Group for the proposed International Linear Collider. She has received numerous awards and honours, and is an elected fellow of all three academies of science in India, a nation where about 37% of science PhD-holders, 20% of working scientists and less than 10% of professors are women. Even today, says Anuradha Misra, head of physics at the University of Mumbai, \u201cmost promising women scientists in India, in spite of having immense potential, are not able to achieve as much due to societal and cultural constraints\u201d. Godbole was born in Pune, the first scientist in a family rich in doctors and engineers. Her mother was a teacher, and her father said she could be whatever she wanted. She won a coveted government scholarship to study physics, then did a master's at the Indian Institute of Technology (IIT) in Mumbai and a PhD at Stony Brook University in New York. \u201cUntil I finished my PhD in physics, I never really thought that being a woman and doing science was somewhat of a special combination,\u201d she says. That soon changed, as Godbole encountered discouragements on every score. When she joined the physics department at the Tata Institute of Fundamental Research (TIFR) in Mumbai for a postdoc in 1979, she found that there was a men's toilet on every floor, but a women's only on every alternate one \u2014 because there were so few women working there. In some other physics departments, there were no female toilets at all. At the end of her postdoc, she was told by a senior professor that she should take up teaching in a women's college. \u201cThere's no expectation that you could be at the top, leading something, having a lab, directing PhD students,\u201d she says. It was advice she chose to ignore, and soon she had a job as a lecturer at the University of Mumbai. The subtle \u2014 and not so subtle \u2014 messages have kept on coming, says Godbole. Five years ago, she was elected a distinguished alumna of IIT, but women were still such a rarity in the group that the letter addressed her as 'Dear Sir'. \u201cThese things could have put me down,\u201d she says, but her energy carried her through. At Mumbai, she would usually lecture and work at the university until noon, then go to the TIFR some 30 kilometres away to continue her collaborations there. She would pause for dinner, work until 11 p.m., and then be ready to teach again the next day. The excitement of discovery was a driver too. \u201cOnce in a while you figure out something that, until then, others have missed, and you feel suddenly like, 'Hey, I know something that others did not know!'\u201d she says. By 1994, she was a professor at the Indian Institute of Science. Meanwhile, Godbole's family had been fretting that she was getting old and would never find a husband. Indian society expects women to put marriage and children first, and to weave their job around the family, says Vinita Sharma, former head of Science for Equity, Empowerment and Development, a Ministry of Science and Technology initiative that promotes science projects to help disadvantaged sections of society. Perceptions and attitudes are changing. In big cities, it is now considered acceptable for women to study physics. The proportion of women starting a bachelor's degree in science rose from 20% in 1970 to 40% in 2005, the most recent figures available. Half of the students doing a physics master's at the University of Mumbai are now women. In 2003, the Ministry of Science and Technology started a programme to encourage female scientists to return to research after a career break. And in 2005, the government set up a National Task Force for Women in Science, which made recommendations, but has had limited impact. Shobhana Narasimhan, a physicist at the Jawaharlal Nehru Centre for Advanced Scientific Research in Bangalore, was on the task force. She says that there are two issues. First, the need to raise women's aspirations \u2014 for this, leadership programmes can help, she says. (Godbole chairs a panel to study the issue of women in science, and has undertaken several initiatives, such as mentoring aspiring female scientists.) The second is the need to change the attitudes of men, so that they consider women for leadership positions. \u201cThis is hard!\u201d says Narasimhan. \u201cI think it helps for women to just 'hang out' with the guys and get accepted as one of them, but in conservative societies, this can often cause a lot of problems.\u201d Godbole did get married, but never had children. \u201cI didn't see how I could juggle all the balls,\u201d she says. \u201cIf there had been child care at institutions, it would have tipped the balance in terms of my decision not to have children. That's the only regret I have.\u201d She thinks future scientists should not be forced to make that choice, and she would like to see better child-care provision and extensions to postdoctoral contracts for women who have children during their positions, as is commonly done in Europe. Murenzi points to the significant cultural evolution that has opened up opportunities for women in places such as the United States and Europe: there is much more awareness of the obstacles than there once was, and greater efforts to minimize them. \u201cFor the most part, developing nations have not come so far, so quickly,\u201d he says. \u201cMake no mistake, though \u2014 that evolution definitely is under way in many developing countries.\u201d A few years ago, Godbole co-edited a book of biographies of almost 90 female scientists in India 4 , in which she wrote that a female scientist needs a large dose of luck to have a successful career, a happy family and a happy marriage. Today, she says: \u201cWhatever can be done to take the requirement of luck from this equation will be good.\u201d \n                 Tweet \n                 Facebook \n                 LinkedIn \n                 Weibo \n               \n                     Developing world: Far-flung physics 2014-Nov-19 \n                   \n                     Inequality quantified: Mind the gender gap 2013-Mar-06 \n                   \n                     Laboratory life: Scientists of the world speak up for equality 2013-Mar-06 \n                   \n                     Research policy: Only wholesale reform will bring equality 2013-Mar-06 \n                   \n                     Nature  special: Women in science \n                   \n                     UNESCO report on women in science \n                   Reprints and Permissions"},
{"file_id": "519024a", "url": "https://www.nature.com/articles/519024a", "year": 2015, "authors": [{"name": "Erika Check Hayden"}], "parsed_as_year": "2006_or_before", "body": "One of the most devastating consequences of the Ebola outbreak will be its impact on maternal health. The woman was lying still on the floor of the ambulance when Emma \u00c5kerlund opened the doors. Between her legs was a tangle of fabric with something inside\u00a0\u2014\u00a0the body of a newborn boy with the placenta and umbilical cord twisted up in his mother\u2019s skirt. If he survived the delivery, he did not last long after. The woman had gone into labour earlier that day in November last year. But health workers feared that she had Ebola, and refused to treat her. She had spent hours alone in the back of the ambulance as it made its way to ELWA3, an Ebola management centre in Monrovia, Liberia, run by the medical aid group M\u00e9decins Sans Fronti\u00e8res (MSF). \u00c5kerlund, a gynaecologist and obstetrician, was able to examine the woman while wearing full protective gear: a waterproof body suit, gloves, boots and goggles. She asked the woman if she wanted to see her son; the mother declined. So \u00c5kerlund zipped the baby into a body bag and sterilized it with bleach, just in case the mother and child were infected. Erika Check Hayden talks about Ebola\u2019s toll on mothers-to-be Tests performed later that day proved that they were not. There had been no medical reason for doctors and nurses to turn the woman away. And although there is no guarantee that the child would have survived with proper medical care, there was no reason for him to come into the world\u00a0\u2014\u00a0and then exit it\u00a0\u2014\u00a0on the floor of an ambulance. \u201cI\u2019m not blaming anyone for not taking them in,\u201d \u00c5kerlund says. \u201cIt\u2019s a huge problem to try to take care of pregnant women in an Ebola epidemic.\u201d Ebola is having tremendous knock-on effects for maternal health in Liberia, Guinea and Sierra Leone. Pregnancy seems to make women uniquely vulnerable to the effects of the disease, and babies born to infected women have not been known to survive. Compounding these individual tragedies, the blood and abundant bodily fluid that accompanies delivery or miscarriage pose enormous risk of infection to health workers. As a result, many refuse to treat patients who are pregnant for fear that they will become infected. And throughout the region, fears about Ebola and stories about women being turned away have convinced many pregnant women to stop showing up for routine prenatal visits or for assistance with delivery. The epidemic has so far infected more than 23,900 people, killing 9,700 of those, and although it is on the wane it is unclear when it will end. For maternal health, the effects are devastating. The United Nations Population Fund (UNFPA) estimates that, either directly or indirectly, the epidemic will result in as many 120,000 maternal deaths by the end of October. Researchers, public-health experts and activists worry that this trend could undermine advances made in the region\u2019s health, education and more. For instance, children whose mothers die may end up orphans and be forced to choose work over school. Teenagers have found themselves becoming family breadwinners. \u201cThe impact on maternal and child health will be one of the most important and long lasting from this outbreak,\u201d says Alexandre Delamou, chief of research at Guinea\u2019s National Centre for Training and Research in Maferinyah. \n               Fearsome odds \n             Doctors are still trying to work out why Ebola is more deadly for some people than others. For pregnant women, the consequences of infection seem dire. In the first Ebola outbreak ever identified, in Zaire in 1976, 73 of 82 infected pregnant women died. In a 1995 outbreak in Kikwit, Democratic Republic of the Congo, researchers observed that all but one of 15\u00a0pregnant women treated at one hospital died. Such samples are too small to say anything with certainty, but the idea that pregnant women would be more at risk of dying makes sense. Ebola increases the risk of haemorrhage, which is already elevated in childbirth. And pregnant women are more likely than other adults to die from several other infectious diseases because of a natural suppression of immunity that helps to protect developing fetuses. The fetuses never seem to make it. In every case reported in the literature of a woman with Ebola giving birth, the baby has either been stillborn or died shortly thereafter. Why this happens is unclear; it could be because of the high viral load that crosses the placenta, because the virus is transmitted through breast milk or because mothers who are sick or recovering from Ebola are in no shape to care for newborn babies. Pregnant women with Ebola also pose serious risks to caregivers. The symptoms of Ebola for a pregnant woman\u00a0\u2014\u00a0abdominal pain, vaginal bleeding, premature labour, nausea, diarrhoea and vomiting\u00a0\u2014\u00a0can be very difficult to differentiate from standard pregnancy complications. Given the high exposure to bodily fluids during delivery, those who guess wrong often do so at their peril. About one-third of the medical staff who died from Ebola in Sierra Leone between April and September last year, for instance, were mother-and-child health aides. This group had a higher death rate than other types of health worker, such as doctors or nurses. Amadu Jawara, a nurse and community health worker in Freetown, Sierra Leone, is one of the lucky ones. In November, he and Samuel Batty treated a pregnant woman with abdominal pain and a fever. Because the fever responded to medication, they assumed she had malaria and did not use full protective gear while examining her. But she soon worsened and later died of Ebola. Jawara and Batty were placed in isolation for 21 days. Batty died from the disease on 2\u00a0December; Jawara did not get infected.  The Ebola epidemic may cause as many as 120,000 maternal deaths by the end of October.  On the day he reported back to work, six women arrived at the hospital in need of Caesarean sections. Many doctors and nurses had abandoned their posts during the epidemic, and there were no surgeons available. Jawara, who had been trained in basic surgery, was the only person there to perform the deliveries. \u201cThey told me I should intervene,\u201d Jawara says. \u201cI had no choice.\u201d He has since performed 100 such procedures. The staff try to screen out patients who have a fever or demand that they take an Ebola test. But this leaves some women without care during labour, and some deliver on the floor of the waiting room with no one to help them. And screening is not foolproof. On 13 January, Jawara saw a patient who had been four months pregnant but whose fetus had stopped moving. She had a low-grade fever, and Jawara admitted her, believing that she had an infection. Two days later, she expelled her fetus and Jawara helped to deliver her placenta. The woman soon developed diarrhoea and a high fever. She tested positive for Ebola. Jawara\u2019s lucky streak continued; he did not become infected. But every episode like this deepens the fear that health-care workers have of treating pregnant women. Some worry that the rift between the two will outlast the epidemic. \u201cEbola will have an everlasting impact on the health system in Sierra Leone,\u201d Jawara says. \u201cThe fear is still there.\u201d Even as the epidemic winds down, many health workers remain absent from their posts. \n               Turned away \n             Researchers who study maternal mortality talk about the \u201cthree delays\u201d that increase the proportion of women who die in childbirth: delays in deciding to go to a hospital, delays in getting there and delays after arriving caused by factors such as electricity cuts or the lack of a trained doctor. Ebola has exacerbated all three. In September, a team of researchers led by the US Centers for Disease Control and Prevention (CDC), for example, interviewed pregnant women and health-care workers in Kenema, a town in southeastern Sierra Leone that was at the epicentre of the epidemic when it started in the country. They found that pregnant women stayed away from hospitals for fear of contracting Ebola, or because of rumours that health-care staff were injecting patients with Ebola, harvesting their blood for sale or misdiagnosing them with Ebola because they were paid extra for referring them to treatment units. Even after outreach workers conducted education campaigns in local communities, all of the 27 pregnant or breastfeeding women who participated in the study said that they knew of at least one person who was refusing to go to health-care facilities because of such rumours. Many women who wanted to go to clinics were prevented from reaching them by quarantines, roadblocks or a lack of transportation. And those who made it were often turned away because the staff were afraid of getting infected. Many hospitals were formally or effectively closed because staff stopped going to work. In Freetown, the capital and largest city of Sierra Leone, the biggest public maternity hospital has been running on skeleton staff for much of the outbreak. In the second largest city, Bo, MSF closed one of the few programmes in the country that offered advanced surgical care for pregnancy complications last August. The decision came after several instances in which staff had difficulty diagnosing Ebola cases. It has still not reopened. CDC researchers found from their surveys that education campaigns are helping to ease fears in regions of the epidemic that are no longer seeing many new cases. The immediate damage is nevertheless erasing fragile gains that these countries had made in maternal health. \n               Delivering change \n             Before the outbreak began, Liberia, Guinea and Sierra Leone had some of the world\u2019s highest maternal death rates: in Sierra Leone, more than 1,000 of every 100,000 child births results in the death of the mother. That is hundreds of times the ratio for developed countries in Europe. But it actually represented a vast improvement from where Sierra Leone was near the end of its civil war in 2000, when the death rate was twice as high. Rates of death during childbirth had also dropped in Liberia and Guinea thanks to a combination of national governments declaring maternal mortality an important health priority, an influx of foreign aid and major public-health campaigns. The gains have been made despite a lack of trained medical professionals and the fact that these countries are some of the poorest on Earth. \u201cIt\u2019s like we had taken one step forward before Ebola and now we\u2019ve taken three steps back.\u201d says Shumon Sengupta, country director in Sierra Leone for Marie Stopes, a non-governmental organization that provides family-planning services. A study conducted on behalf of the UNFPA, for instance, modelled the impact of the reductions on routine care provided to pregnant women in Sierra Leone in the first six months of the epidemic. The preliminary work found that the cuts may have caused 20% more maternal deaths. Now, the maternal death rate is projected to double to more than 1,000 per 100,000 live births in Guinea and Liberia and to more than 2,000 per 100,000 live births in Sierra Leone, returning to wartime levels. \u201cWe think the collateral damage of the epidemic is higher than the damage caused by the epidemic itself,\u201d says gynaecologist S\u00e9verine Caluwaerts, who works at the Institute of Tropical Medicine in Antwerp, Belgium, and with MSF. The UNFPA is looking to convene a meeting to analyse the state of the science around reproductive health and Ebola and how to prevent such impacts during future outbreaks. A crucial issue is how to boost the survival of pregnant women who become infected.  An estimated 1.2 million women may lack access to family-planning services, which will increase the number of unexpected pregnancies.  MSF obstetrician Benjamin Black and his colleagues have designed a \u2018maternity box\u2019\u00a0containing all the supplies that caregivers would need to help prevent haemorrhage in cases in which the uterus fails to contract after delivery,\u00a0a major complication that threatens pregnant women with Ebola. These supplies include drugs such as oxytocin and misoprostol, which encourage contraction, and a catheter to empty the bladder, which can speed up delivery. The box spares caregivers from having to exit the patient area, take off their protective equipment and re-gown if they need something vital. Simple adjustments such as this have helped MSF to save the lives of 23 pregnant women with Ebola. In January, MSF opened a clinic in the outskirts of Freetown that specializes in care for pregnant women with Ebola. The clinic has adapted its procedures to meet the additional risks in looking after these women\u00a0\u2014\u00a0for instance, adding a third pair of gloves that reach the elbow to the standard two pairs. And staff try to avoid surgical procedures, such as episiotomies and Caesarean sections, that are too dangerous for workers and patients. \u00c5kerlund says that she is constantly thinking about how to protect herself and other staff so that they feel able to care for pregnant women rather than turning them away. One case from her time in ELWA3 haunts her: an apparently full-term pregnant woman who arrived at the treatment centre without a single symptom of Ebola. She had thought that her waters had broken and she headed for the hospital but was turned away from multiple clinics by reticent staff. \u00c5kerlund thought that the woman was another perfectly healthy mother-to-be who might lose her baby to nothing more than fear. She and her staff admitted the woman, brought her to a place where she could be kept separate from other patients and ordered an Ebola test. Hours later, \u00c5kerlund received the results: the test was positive. A second test was also positive. The woman was moved into the ward for confirmed cases on 26\u00a0November. Within days, her condition deteriorated; her blood stopped clotting, and her baby stopped moving. The following day, the woman died. The case sticks in \u00c5kerlund\u2019s mind as an example of the dilemmas that confront both caregivers and pregnant women in an Ebola epidemic, when diagnostic tests can take hours or days and a woman in labour may need help immediately. But \u00c5kerlund knows that giving up is not an option. \u201cI don\u2019t think the solution is to say you can\u2019t do maternity care in an outbreak and leave these women without anything.\u201d See Editorial  page 5 The Pulitzer Center on Crisis Reporting provided support for this coverage. \n                     Fatal fallout 2015-Mar-04 \n                   \n                     The Ebola questions 2014-Oct-29 \n                   \n                     Ebola outbreak shuts down malaria-control efforts 2014-Oct-01 \n                   \n                     Infectious disease: Ebola\u2019s lost ward 2014-Sep-24 \n                   \n                     Nature  special: Ebola outbreak \n                   \n                     CapaCare \n                   \n                     MSF on Ebola \n                   \n                     Marie Stopes International \n                   \n                     MamaYe \n                   \n                     African Health Stats \n                   Reprints and Permissions"},
{"file_id": "519280a", "url": "https://www.nature.com/articles/519280a", "year": 2015, "authors": [{"name": "Daniel Cressey"}], "parsed_as_year": "2006_or_before", "body": "Daniel Pauly is sounding the alarm over global fish harvests, but others think he is making too much noise. Off the coast of Morocco, small wooden fishing boats bob in the sea, collecting squid. On the other side of the Atlantic Ocean, powerful cabin cruisers speed out of Nassau, carrying tourists in pursuit of huge mahimahi, wahoo and marlin. None of these vessels exist \u2014 at least in the official tally of marine harvests kept by the Food and Agriculture Organization of the United Nations (FAO). That global database, known as the State of World Fisheries and Aquaculture, or SOFIA, is generally regarded as the bible of information about marine food resources. But SOFIA includes only information that nations give to the FAO, which is of varying quality and often ignores subsistence fishing, illegal harvesting and the sport trade. Daniel Pauly is obsessed with those missing fishing vessels and the untold amounts of food that they pull from the oceans. The soft-spoken but intense marine biologist at the University of British Columbia in Vancouver, Canada, seeks to bring them out of the shadows. Over the course of a long career that has been equal parts celebrated and controversial, he has set himself the huge task of working out what he calls the \u201ctrue catch\u201d of the world's fisheries. He hopes that such figures will help countries \u2014 especially in the developing world \u2014 to take control of their resources and feed their people. Now he is reaching the culmination of the project. Towards the end of this year, his team will publish a global atlas of all its data, revealing for the first time its estimate of the actual annual harvest from the world's fisheries. Preliminary publications indicate that the figures will be at least 50% above the FAO numbers for developed countries, and much higher for the developing world. The data, Pauly says, paint a bleak picture of the state of the oceans. Although official FAO numbers suggest that catches have been roughly stable in recent years, Pauly's work indicates that the global catch is falling. The debate is not just academic. Declining catches could mean that fisheries in many countries are being over-exploited, says Pauly. \u201cThat is the scary part.\u201d \n               True-catch detective \n             Growing up in landlocked Switzerland, Pauly did not seem destined to spend his life thinking about oceans. But at university, he started studying fisheries science with the aim of working in developing countries \u2014 and escaping from Europe, where he felt he was always reminded of his biracial background (his father is African American). \u201cI wanted to get to a place where they wouldn't question so much,\u201d he says. After earning a PhD in Germany, he worked in Asia on fisheries-management issues. In 1999, he ended up on the western seaboard of North America, as head of the Sea Around Us Project, which is dedicated to studying fisheries and their influence on ecosystems. The effort is a collaboration of the University of British Columbia and the Pew Charitable Trusts. Created, led and shaped by Pauly, this programme crystallized around a series of questions. How many fish are caught around the world? What species are they? And how can this catch be changed to ensure that fisheries are sustainable and to prevent damage to the oceans? \u201cThis always was on the horizon, this basic question: how much is really caught in the world?\u201d says Pauly. When he started, Pauly relied heavily on the FAO numbers. \u201cI for years have done analysis of the FAO data. And I thought they are like all numbers that you can get \u2014 they have their up and downs, and overall this balances out,\u201d he says. But over time, as he pored through the data and obsessed over their nuances, he realized how many important factors were missing. \u201cThat got me scared, because I realized that it was not an exception but the rule.\u201d He describes his growing disenchantment with the FAO statistics as a \u201cgradual falling out of love\u201d. Pauly says that the problem boils down to the fact that \u201cpeople, when they don't know, they put zero\u201d. Fishery officials report that they do not catch any of a particular species, or they do not have any small-scale subsistence fishing \u2014 people in small boats providing for their families. \u201cBut these zeroes that are soft zeroes \u2014 reflecting uncertainty \u2014 become hard zeroes.\u201d This led him to his next realization \u2014 that someone had to undertake the mammoth task of fixing this problem, and \u201crecreating the statistics of the world from the bottom up\u201d. In 1998, Pauly wrote an article 1  suggesting a way to do that. He proposed that researchers could reconstruct catch data by hunting through old government files, harbour masters' records, aerial photographs and interviews with fishers, along with published scientific reports. This approach relies on the understanding that fishing is a social activity, says Pauly. \u201cIt throws shadows on the society in which it operates\u201d. Fishing influences the restaurants and markets at which fishers sell their products, and the boat-builders and fuel suppliers who allow them to work. \u201cIt is not possible to have a fishery and have no data about that fishery,\u201d he says. The Sea Around Us group uses these shadows to find the fisheries hidden from the FAO. It has worked country by country to piece together a comprehensive database of what it thinks is the true catch. Pauly's group published its first taste of this approach in 2006, in a report on the United States' islands in the western Pacific 2 . The Sea Around Us had been contracted by the fisheries-management council in the region to analyse the local catch. Pauly's colleague Dirk Zeller, a fisheries researcher at the University of British Columbia, and his team used a six-part approach that later became standard for the global effort. First, the group gathered existing catch reports dating back to 1950. Then it identified the gaps in these data \u2014 time periods with missing information, species that should have been in the statistics but were not, and types of fishing that were likely to have occurred but were absent from the FAO data, such as local people fishing to feed themselves rather than to sell. To fill in the missing data, the team combed through the scientific literature and interviewed local experts, ranging from scientists to fishermen and fisherwomen. The researchers built anchor points using all the hard data, and then interpolated between these anchor points to estimate the missing figures. This helped them to tally up a total catch for the whole time period. For the island of American Samoa, their hard data points included a local statistical digest and official statistics published by the Western Pacific Fisheries Information Network. The team also used published estimates of how much fish was caught per capita each year, and combined them with census data to yield a total catch estimate. When Zeller added up the numbers, the total estimated catch between 1950 and 2002 was 17 times the reported statistics. \u201cThat opened up a Pandora's box,\u201d he says. \u201cIf entities associated with the US have trouble with that, what about the rest of the world?\u201d Since then, Zeller and the Sea Around Us team have been applying versions of the same methodology elsewhere. In 2014, for example, they calculated that the true catch in Portugal was 36% higher than the officially reported fish landings 3 , and the difference in Panama reached 40% (ref.  4 ). One thing is constant, says Zeller: \u201cWherever I turn, data are missing.\u201d \n               The gap narrows \n             The biggest differences between FAO and Sea Around Us numbers are in the past. Pauly says that for recent years, the two sets of figures are edging closer \u2014 a trend that can be seen in Senegal. The reconstruction there was headed by Dyhia Belhabib with the Sea Around Us, who has led work all along the West African coast. Belhabib estimates that at times in the past 40 years, the true catch for Senegal was 4 times higher than the official data show, whereas in the past decade, the gap has shrunk (see 'Something fishy'). Still, her analysis 5  suggests that illegal, unregulated and unreported fishing took US$300 million of fish from Senegal's waters in recent years.\u201cWe did not expect it to be this much. I thought in Senegal there was better monitoring.\u201d There are signs that the Sea Around Us work is having an impact. In 2014, Senegal's fishing ministry seized a Russian vessel and accused it of fishing in Senegalese waters illegally. The government successfully extracted a fine of more than $1 million from the ship's owners. And in press conferences afterwards, the ministry cited the $300 million unregulated-fishing figure from the Sea Around Us. Two years ago, says Belhabib, \u201cI would say we're the outsiders, we're the black sheep.\u201d But after the group worked with governments in West Africa, she says, \u201cI feel it's changing.\u201d The Sea Around Us Project now has an agreement in place with the Ministerial Conference on Fisheries Cooperation Among African States Bordering the Atlantic Ocean (ATLAFCO). One aim of the collaboration is to help African nations to develop their own capacity to reconstruct catches. It is also encouraging member nations to publish their data in peer-reviewed journals. Not everyone has been so positive about this work. A group of researchers in France and Senegal has questioned a number of the assumptions used in Belhabib's analysis. The team argued 6  that the Sea Around Us work ignores crucial differences between types of fishing, in one case taking data on fish discarded during shrimp harvesting and inappropriately applying the same discard rates to fin-fishing operations. The reconstruction produced huge overestimates and \u201cmay lead to inadequate management advice\u201d, the authors conclude. Belhabib, Pauly and their colleagues rebut these criticisms 7 , maintaining that their estimates are sound. In the case of the discard rates, they say that they did not base their estimates on data from shrimp harvests \u2014 and that the rates for the two are roughly similar anyway. But criticisms go beyond Senegal: China has been another flash point. Pauly's group published estimates 8  that the annual catch by the Chinese fishing fleet around the globe averaged 4.6 million tonnes per year in 2000\u201311 \u2014 more than 10 times the FAO number (see  Nature   496 , 18; 2013 ). The FAO said that these numbers are much too high. Sachiko Tsuji, a senior fisheries statistician at the FAO in Rome, says that the problem comes down to the methodology used by Pauly's group. \u201cIn many cases their reconstructions are based on extremely small samples,\u201d says Tsuji. Others echo that concern, saying that the Sea Around Us extrapolates from limited samples and sometimes unreliable numbers, which can produce huge overestimates. Pauly's group triggers strong feelings among some at the FAO, who believe that their work is being unfairly targeted, says Tsuji. \u201cI personally do not think so, but that is the current relationship between FAO and the Sea Around Us.\u201d Tsuji also points out that the FAO can only report official statistics that are supplied by UN member states. It addresses aspects such as illegal fishing in special reports. For example, it collaborated with the World Bank to produce the 2008 report  The Sunken Billions , which acknowledged both the problems of illegal fishing and the fact that some fishing was unreported in the agency's official statistics. But the FAO is not allowed to modify the statistics reported by member nations to include such details in the official catch statistics. \n               Catch conflict \n             Pauly has received a plethora of awards for his work, but he has also made enemies. Never afraid to ruffle feathers, he is outspoken about ocean conservation and willing to point fingers at the huge multinational companies that control much of the world's fishing industry. Part of his motivation for seeking the true catch of developing nations is to allow such countries to take control of their own resources. He says that much of the criticism comes from people who think that he has climbed into bed with environmental non-governmental organizations, which fisheries scientists have tended to shy away from. Pauly's strident warnings over declining fish populations have landed him in fierce fights 9 , particularly with Ray Hilborn of the University of Washington in Seattle, one of the world's leading fisheries researchers. Hilborn has little faith in the numbers produced by the Sea Around Us. \u201cThey end up just using someone's opinion,\u201d he says. \u201cI think they're just pissing in the wind.\u201d Trevor Branch, another fisheries scientist at the University of Washington, is more accommodating. Catch data, he says, are \u201cessential\u201d, and the reconstructions by Zeller and Pauly are \u201cincredibly valuable\u201d. But Branch argues that it is not possible to use catch data alone to assess the status of a fishery, in the absence of information about the actual abundance of a given species in the ocean. For example, catches in a given area may have declined because people simply stopped fishing there, rather than because the number of fish has fallen. Pauly argues that catch figures are the only source of information on many species of fish. Data on actual abundance are expensive and time-consuming to collect, so catch figures have to be used to assess the status of species. The dispute has divided much of the fisheries community. But no one denies Pauly's influence. Boris Worm, a fisheries researcher at Dalhousie University in Halifax, Canada, is one of the few to have authored papers with both Hilborn and members of the Sea Around Us. He has estimated global mortality for sharks 10 , using data from the Sea Around Us Project and other sources; his work suggests that the global shark catch is three or four times what FAO records suggest. Asked which numbers he uses to estimate how many fish are actually being caught, Worm answers almost before the question is finished: \u201cDaniel Pauly.\u201d Despite all the controversies, Pauly seems unfazed. In fact, his manner gives the impression that nothing much fazes him. His life has taken him around the world, and left him with a certainty about what he is doing and why. He sees much of the dispute about the health of the oceans as boiling down to the fact that the traditional heavyweights in fisheries science are from the developed world, which has enjoyed relatively effective management and assiduous record keeping. The picture is different in the developing world, where Pauly's attention has long been focused. Getting a handle on fishing in these regions requires going beyond the official data, he says. Yet even as it focuses on developing nations, the Sea Around Us Project has tried to take a broad perspective. Fisheries science is often, by necessity, highly localized. Researchers can spend years unpicking the factors that control the numbers of Maine lobster, or sprat in the Baltic Sea. Many researchers still insist that this is how fisheries science has to work. But Pauly's research has forced people to take a global view \u2014 to paint a full picture of the health of the oceans. \u201cYou need to take yourself out of that local perspective,\u201d says Belhabib. \u201cIt requires a lot of guts to do that. It took Daniel Pauly to do that.\u201d \n                     Detective work uncovers under-reported overfishing 2013-Apr-02 \n                   \n                     Seafood labelling under fire 2012-May-11 \n                   \n                     Battling scientists reach consensus on health of global fish stocks 2009-Jul-30 \n                   \n                     The Sea Around Us Project \n                   Reprints and Permissions"},
{"file_id": "519148a", "url": "https://www.nature.com/articles/519148a", "year": 2015, "authors": [{"name": "Dan Jones"}], "parsed_as_year": "2006_or_before", "body": "The world is full of bloody conflicts that can drag on for decades. Some researchers are trying to find resolutions through complexity science. In the seven decades that Colombia has been riven by civil war, the country has seen kidnappings, rapes, terrorist attacks and pitched battles that have cost more than 220,000 lives and displaced millions of people. Negotiations, peace accords and ceasefires have come and gone to little lasting effect. The latest round of this seemingly unending cycle began in August 2012, when the Marxist rebels of the Revolutionary Armed Forces of Colombia (FARC) agreed to meet with the central government in yet another round of peace talks. But the negotiations collapsed in November after the rebels kidnapped a Colombian army general. The talks have since resumed, but even if they one day yield a peace accord, there is no guarantee it will hold. More than one-third of the world's peace agreements and ceasefires since the 1950s have relapsed into violence within five years. Colombia's long history of strife is a classic example of 'intractable' conflict \u2014 a self-perpetuating cycle of hostility that can grind on for decades. Such conflicts are relatively scarce \u2014 only about 5% of the world's myriad wars qualify \u2014 but their longevity means that they exert a huge toll on societies. Their tragic poster child is the 68-year-long Israeli\u2013Palestinian conflict. But the list also includes India and Pakistan's equally long battle over Kashmir, and Sri Lanka's 26-year civil war. The Democratic Republic of the Congo (DRC) has been riven by conflict since 1996, as has South Sudan since its inception in 2011. Any number of intractable conflicts may now be emerging in the Middle East as Libya, Syria and Iraq are ripped apart by sectarian violence and with the rise of the Islamist group ISIS (see 'Intractable conflicts'). The intensifying civil war in eastern Ukraine may eventually join the list as well. By definition, these are the conflicts that are resistant to all the mainstream techniques of dispute resolution, says Robert Ricigliano, a mediation expert at the University of Wisconsin Milwaukee. Typically they are plagued by a history of \u201cfixes that fail\u201d, he says \u2014 peace agreements that collapse within days or weeks. \u201cWe mediate agreements, change leaders, arbitrate boundaries,\u201d he says. \u201cBut those things don't necessarily get at the underlying dynamics fuelling conflict.\u201d He and a growing chorus of other conflict researchers have therefore been pushing for a fresh approach \u2014 one that views intractable conflicts as dynamic, complex systems similar to cells, ant colonies or cities, and analyses them with the mathematical and computational tools developed over the past 30 years in complexity science. Mainstream practitioners tend to be dubious, says Dan Smith, head of the London-based peace-building organization International Alert. \u201cWe know that conflicts are complex,\u201d he says. \u201cWhat would be useful would be a clearer idea of what to do about it.\u201d But Ricigliano and others have begun to answer that criticism by using complexity-inspired techniques to help resolve conflicts in places such as the DRC. They say that the approach can be a much-needed corrective to business as usual in the conflict-resolution world, where governments and international organizations too often tackle conflicts piecemeal. These bodies tend to \u201clook at the economy, or governance, or gender relations or education as if each existed in isolation\u201d, says Smith. \u201cIt's a convenient way to handle the issues, but it means you don't really address the complex reality.\u201d \n               Hard problems \n             It was just this kind of blinkered thinking that led psychologist Peter Coleman to rebel. It was 2000, recalls Coleman, head of the Morton Deutsch International Center for Cooperation and Conflict Resolution at Columbia University in New York City. He had broken his foot and decided to spend his convalescence at home delving into the research literature on intractable conflict. But what he found left him deeply frustrated. \u201cPeople had their simple, sovereign theories about why conflicts become intractable,\u201d he says. \u201cIt's because of trauma, or social identity or a history of humiliation. We understood pieces of the problem, but not how they interact.\u201d Coleman discovered an alternative approach just a few years later, when he came across the work of social psychologists Robin Vallacher and Andrzej Nowak, both now at Florida Atlantic University in Boca Raton. Their work was not directly related to conflict \u2014 they were studying things such as how the human sense of self emerges, and how feelings about others can switch from positive to negative. But Coleman was impressed with Vallacher and Nowak's use of a mathematical tool known as dynamical systems theory to analyse their results. Made famous by James Gleick's 1987 book  Chaos , this theory provides a framework for understanding a remarkably broad range of complex systems, from weather patterns to neural activity in the brain. One way to visualize the mathematics is to imagine a landscape of hills and valleys. The behaviour of the complex system corresponds to the path of a ball rolling across this landscape. The trajectory becomes very complicated as the ball is deflected by the hills. But eventually, the ball will get trapped in one of the valleys, where it will either cycle endlessly around the walls or sink to the middle and lie still. The ball's final trajectory or resting place is called an attractor. To Coleman, this kind of entrapment was the perfect metaphor for the stable, if destructive, patterns of social behaviour seen in intractable conflicts. The landscapes in this case are mainly psychological and social, comprising innumerable strata of history, identity and collective memories of harms suffered at the hands of the 'other'. Yet the resulting conflict attractors are terribly real, he says, with psychological forces conspiring to \u201ccreate simplistic narratives about conflicts that are devoid of nuance and keep us locked in\u201d. To make this mathematical view of intractable conflicts into something more than a metaphor \u2014 and hopefully to turn it into a set of tools that could make a difference in the real world \u2014 Coleman, Vallacher and Nowak in 2004 formed the Dynamics of Conflict working group, which has since attracted four more members. As a result of this collaboration, Nowak has started to create computational models that capture the dynamics of conflicts. These include 'agent-based' simulations that contain thousands of digital robots \u2014 the agents \u2014 each of which embodies some of the simple behaviours that social psychologists believe have a role in conflict. One such model, developed with researchers outside the working group, features agents that vary in how competitive or cooperative they are, and adjust those proclivities according to how much hostility or aggression they experience from the other agents. In this simulation 1 , small conflagrations flare up and die down much as they do in real communities. Occasionally, however, the conflicts expand until they lock the whole virtual community into a cycle of recrimination \u2014 the classic sign of intractability. Working with Dean Pruitt of the School for Conflict Analysis and Resolution at George Mason University in Arlington, Virginia, Nowak has also developed mathematical models showing how attractors can explain the escalation of conflicts that tips them into an intractable state 2 . Now he and his colleagues are working on the next step: comparing the evolution of communities in these simple models with data from real-world conflicts such as the Israeli\u2013Palestinian stand-off. \u201cThis is the first time we've added empirical data to a dynamical model, and we're getting promising results,\u201d says Nowak. \n               Making sense of the system \n             Another line of research is to move from generalities to specifics, and develop visualization tools that can help mediators to untangle the complexities of real-world conflicts. The hope is that such 'conflict maps' will help researchers to keep track of the interconnections between players and events, and make clear the feedback loops and key networks that can escalate or inhibit conflict. Conflict maps can take many forms, from hand-drawn sketches on a whiteboard to computer-generated networks based on real data. But whatever their form, they get strong endorsement from Ricigliano, who has worked on peace-building interventions in areas ranging from Colombia and South Africa to Iraq and Cambodia. In 2000, for example, Ricigliano went to the DRC to try to find some resolution to the Second Congo War: a blood-drenched conflict between various rebel groups and Mai-Mai militias fighting for the government. Behind the scenes, he and his colleagues watched the unravelling of one hard-won peace agreement after another. \u201cAt best we were having a neutral impact,\u201d he says, \u201cand maybe even a negative one.\u201d But then in 2002, he and his colleagues began to map all the connections between warring parties and competing interests in the conflict. The maps made it clear that local groups were being manipulated by national rebel organizations, who wanted the conflict to continue because it allowed them to access valuable minerals. \u201cSo we shifted tactics, and began trying to break the links between the national-level actors who were manipulating local actors, and to facilitate local-level ceasefires of significance, says Ricigliano. By 2003, these dialogues had helped the United Nations to negotiate a transition government that included the major rebel groups, and violence declined. \u201cIt wasn't perfect,\u201d says Steve Smith, an independent conflict-resolution consultant who was in the DRC at the time. \u201cNot everyone was in agreement, and little conflicts continued, but we had a structure in place and a direction to go.\u201d \n               State of mind \n             Beyond the models and the maps, advocates of systems thinking are hoping to spread a shift in perspective on intractable conflicts. One convert is Andrea Bartoli, dean of the School of Diplomacy and International Relations at Seton Hall University in South Orange, New Jersey, and a mediator who has worked in countries such as Mozambique and Kosovo. When he first learned about the dynamical systems perspective in discussions with Coleman a little over a decade ago, he says, \u201cit provided a new language for talking about conflict, and opened up new ways to think about old problems\u201d. He has since joined the Dynamics of Conflict working group, and in 2009 joined with Coleman and Beth Yoshida-Fisher, director of the Negotiation and Conflict Resolution program at Columbia University, to set up the Advanced Consortium on Cooperation, Conflict, and Complexity (AC4) there. That new language can be a revelation even to professionals, says Naira Musallam, a conflict researcher at New York University's Center for Global Affairs and a member of both the Dynamics of Conflict group and AC4. She tells the story of a course she teaches at the US Military Academy West Point in New York, in which she starts by running through a list of common mental shortcomings in how people think about conflict, poverty and other social problems. \u201cWe compare fluid situations to fixed things,\u201d she says, \u201cwe think in straight lines rather than loops, we focus on understanding problems and assume that this will lead to solutions, and often miss the unintended consequences of well-intentioned interventions.\u201d After one class, says Musallam, an officer who had served in Iraq and Afghanistan wrote to her. \u201cI know many good people who have died because of errors [highlighted] on this list,\u201d he wrote. \u201cI also see several errors that I have made before \u2026 It's frustrating that this is the first time that I've seen this list in a way that challenges my world view around conflict.\u201d These same straight-line assumptions are also built into the way in which many institutions operate, says Musallam \u2014 and not just those devoted to peace-building. \u201cThey want nice, tidy plans for interventions, and clear deliverables over the short term\u201d, she says. This often leads to plans to 'solve' complex problems through a series of discrete steps that are defined in advance by experts. One of the key lessons of the systems mindset is to stop approaching conflicts as problems that need to be fixed, says Ricigliano, and instead think of them as systems with underlying dynamics that need to shift. \u201cSuccess doesn't mean that we've ended the conflict,\u201d he says. \u201cIt means we've engaged a system so that violence declines over time.\u201d This view is finding increasing support from outside allies. The non-profit Berlin-based Berghof Foundation, for example, has used systems thinking in its efforts to resolve political and ethnic violence in countries such as Sri Lanka, which has been torn by civil war since 1983. But there is plenty of room for scepticism. Dan Smith, for one, is sympathetic to the complex systems view of conflict, but is wary of its sweeping generalizations. \u201cAny analysis employing these principles is only going to be as good as the analyst doing it,\u201d he says. \u201cYou can have the best methodology, but if you have an uninformed or incurious analyst, you won't get good results.\u201d Even advocates admit that specific recommendations are a work in progress. That is why in 2013, Coleman and Ricigliano joined with others to set up an annual five-day workshop known as the Dynamic Systems Theory Innovation Lab, which brings together biologists, economists, physicists, political scientists and other scholars and practitioners to talk about real-world applications. \u201cWe hope that five years out, we'll have a better idea of what matters most,\u201d says Coleman. There is already a growing body of experiments they can draw on. In Israel, for example, a series of anti-conflict interventions being developed under the leadership of psychologist Eran Halperin at the Interdisciplinary Center Herzliya in Israel have proved effective in making people more open to seeing things from the other side's point of view 3 , 4 , 5 . Although the label 'intractable conflict' implies unending strife, no struggle lasts forever. As the 1980s drew to a close, South Africa had been locked in racial conflict for decades and was on the brink of a civil war between increasingly militant members of the African National Congress (ANC) and the government of President Frederik Willem De Klerk. Amid international condemnation of the apartheid system, and fearing that the country could become engulfed in a bloody street war, De Klerk began releasing imprisoned ANC members in late 1989. Finally, in February 1990, he freed ANC leader Nelson Mandela after 27 years in prison. That conciliatory move was the tipping point for the emergence of multiracial democracy within three years. South Africa's long transition was a difficult journey, with many losses and setbacks along the way \u2014 par for the course for any intractable conflict. Yet as Mandela once famously said: \u201cIt always seems impossible until it's done.\u201d \n                     Climate cycles drive civil war 2011-Aug-24 \n                   \n                     Common ecology quantifies human insurgency 2009-Dec-17 \n                   \n                     Comparing the horror of wars 2008-Dec-15 \n                   \n                     Advanced Consortium on Cooperation, Conflict, and Complexity \n                   \n                     Beyond Intractability Project \n                   \n                     Dynamical Systems Theory Innovation Lab \n                   \n                     Dynamics of Conflict \n                   \n                     International Center for Cooperation and Conflict Resolution \n                   Reprints and Permissions"},
{"file_id": "519144a", "url": "https://www.nature.com/articles/519144a", "year": 2015, "authors": [{"name": "Richard Monastersky"}], "parsed_as_year": "2006_or_before", "body": "Momentum is building to establish a new geological epoch that recognizes humanity's impact on the planet. But there is fierce debate behind the scenes. Almost all the dinosaurs have vanished from the National Museum of Natural History in Washington DC. The fossil hall is now mostly empty and painted in deep shadows as palaeobiologist Scott Wing wanders through the cavernous room. Wing is part of a team carrying out a radical, US$45-million redesign of the exhibition space, which is part of the Smithsonian Institution. And when it opens again in 2019, the hall will do more than revisit Earth's distant past. Alongside the typical displays of  Tyrannosaurus rex  and  Triceratops , there will be a new section that forces visitors to consider the species that is currently dominating the planet. \u201cWe want to help people imagine their role in the world, which is maybe more important than many of them realize,\u201d says Wing. Simon Lewis discusses the best candidate dates to define the beginning of the Anthropocene This provocative exhibit will focus on the Anthropocene \u2014 the slice of Earth's history during which people have become a major geological force. Through mining activities alone, humans move more sediment than all the world's rivers combined.  Homo sapiens  has also warmed the planet, raised sea levels, eroded the ozone layer and acidified the oceans. Given the magnitude of these changes, many researchers propose that the Anthropocene represents a new division of geological time. The concept has gained traction, especially in the past few years \u2014 and not just among geoscientists. The word has been invoked by archaeologists, historians and even gender-studies researchers; several museums around the world have exhibited art inspired by the Anthropocene; and the media have heartily adopted the idea. \u201cWelcome to the Anthropocene,\u201d  The Economist  announced in 2011. The greeting was a tad premature. Although the term is trending, the Anthropocene is still an amorphous notion \u2014 an unofficial name that has yet to be accepted as part of the geological timescale. That may change soon. A committee of researchers is currently hashing out whether to codify the Anthropocene as a formal geological unit, and when to define its starting point. But critics worry that important arguments against the proposal have been drowned out by popular enthusiasm, driven in part by environmentally minded researchers who want to highlight how destructive humans have become. Some supporters of the Anthropocene idea have even been likened to zealots. \u201cThere's a similarity to certain religious groups who are extremely keen on their religion \u2014 to the extent that they think everybody who doesn't practise their religion is some kind of barbarian,\u201d says one geologist who asked not to be named. The debate has shone a spotlight on the typically unnoticed process by which geologists carve up Earth's 4.5 billion years of history. Normally, decisions about the geological timescale are made solely on the basis of stratigraphy \u2014 the evidence contained in layers of rock, ocean sediments, ice cores and other geological deposits. But the issue of the Anthropocene \u201cis an order of magnitude more complicated than the stratigraphy\u201d, says Jan Zalasiewicz, a geologist at the University of Leicester, UK, and the chair of the Anthropocene Working Group that is evaluating the issue for the International Commission on Stratigraphy (ICS). \n               Written in stone \n             For geoscientists, the timescale of Earth's history rivals the periodic table in terms of scientific importance. It has taken centuries of painstaking stratigraphic work \u2014 matching up major rock units around the world and placing them in order of formation \u2014 to provide an organizing scaffold that supports all studies of the planet's past. \u201cThe geologic timescale, in my view, is one of the great achievements of humanity,\u201d says Michael Walker, a Quaternary scientist at the University of Wales Trinity St David in Lampeter, UK. Walker's work sits at the top of the timescale. He led a group that helped to define the most recent unit of geological time, the Holocene epoch, which began about 11,700 years ago. The decision to formalize the Holocene in 2008 was one of the most recent major actions by the ICS, which oversees the timescale. The commission has segmented Earth's history into a series of nested blocks, much like the years, months and days of a calendar. In geological time, the 66 million years since the death of the dinosaurs is known as the Cenozoic era. Within that, the Quaternary period occupies the past 2.58 million years \u2014 during which Earth has cycled in and out of a few dozen ice ages. The vast bulk of the Quaternary consists of the Pleistocene epoch, with the Holocene occupying the thin sliver of time since the end of the last ice age. When Walker and his group defined the beginning of the Holocene, they had to pick a spot on the planet that had a signal to mark that boundary. Most geological units are identified by a specific change recorded in rocks \u2014 often the first appearance of a ubiquitous fossil. But the Holocene is so young, geologically speaking, that it permits an unusual level of precision. Walker and his colleagues selected a climatic change \u2014 the end of the last ice age's final cold snap \u2014 and identified a chemical signature of that warming at a depth of 1,492.45 metres in a core of ice drilled near the centre of Greenland 1 . A similar fingerprint of warming can be seen in lake and marine sediments around the world, allowing geologists to precisely identify the start of the Holocene elsewhere. Even as the ICS was finalizing its decision on the start of the Holocene, discussion was already building about whether it was time to end that epoch and replace it with the Anthropocene. This idea has a long history. In the mid-nineteenth century, several geologists sought to recognize the growing power of humankind by referring to the present as the 'anthropozoic era', and others have since made similar proposals, sometimes with different names. The idea has gained traction only in the past few years, however, in part because of rapid changes in the environment, as well as the influence of Paul Crutzen, a chemist at the Max Plank Institute for Chemistry in Mainz, Germany. Crutzen has first-hand experience of how human actions are altering the planet. In the 1970s and 1980s, he made major discoveries about the ozone layer and how pollution from humans could damage it \u2014 work that eventually earned him a share of a Nobel prize. In 2000, he and Eugene Stoermer of the University of Michigan in Ann Arbor argued that the global population has gained so much influence over planetary processes that the current geological epoch should be called the Anthropocene 2 . As an atmospheric chemist, Crutzen was not part of the community that adjudicates changes to the geological timescale. But the idea inspired many geologists, particularly Zalasiewicz and other members of the Geological Society of London. In 2008, they wrote a position paper urging their community to consider the idea 3 . Those authors had the power to make things happen. Zalasiewicz happened to be a member of the Quaternary subcommission of the ICS, the body that would be responsible for officially considering the suggestion. One of his co-authors, geologist Phil Gibbard of the University of Cambridge, UK, chaired the subcommission at the time. Although sceptical of the idea, Gibbard says, \u201cI could see it was important, something we should not be turning our backs on.\u201d The next year, he tasked Zalasiewicz with forming the Anthropocene Working Group to look into the matter. \n               A new beginning \n             Since then, the working group has been busy. It has published two large reports (\u201cThey would each hurt you if they dropped on your toe,\u201d says Zalasiewicz) and dozens of other papers. The group has several issues to tackle: whether it makes sense to establish the Anthropocene as a formal part of the geological timescale; when to start it; and what status it should have in the hierarchy of the geological time \u2014 if it is adopted. When Crutzen proposed the term Anthropocene, he gave it the suffix appropriate for an epoch and argued for a starting date in the late eighteenth century, at the beginning of the Industrial Revolution. Between then and the start of the new millennium, he noted, humans had chewed a hole in the ozone layer over Antarctica, doubled the amount of methane in the atmosphere and driven up carbon dioxide concentrations by 30%, to a level not seen in 400,000 years. When the Anthropocene Working Group started investigating, it compiled a much longer long list of the changes wrought by humans. Agriculture, construction and the damming of rivers is stripping away sediment at least ten times as fast as the natural forces of erosion. Along some coastlines, the flood of nutrients from fertilizers has created oxygen-poor 'dead zones', and the extra CO 2  from fossil-fuel burning has acidified the surface waters of the ocean by 0.1 pH units. The fingerprint of humans is clear in global temperatures, the rate of species extinctions and the loss of Arctic ice. The group, which includes Crutzen, initially leaned towards his idea of choosing the Industrial Revolution as the beginning of the Anthropocene. But other options were on the table. Some researchers have argued for a starting time that coincides with an expansion of agriculture and livestock cultivation more than 5,000 years ago 4 , or a surge in mining more than 3,000 years ago (see 'Humans at the helm'). But neither the Industrial Revolution nor those earlier changes have left unambiguous geological signals of human activity that are synchronous around the globe (see \u2018Landscape architecture\u2019). This week in  Nature , two researchers propose that a potential marker for the start of the Anthropocene could be a noticeable drop in atmospheric CO 2  concentrations between 1570 and 1620, which is recorded in ice cores (see  page 171 ). They link this change to the deaths of some 50 million indigenous people in the Americas, triggered by the arrival of Europeans. In the aftermath, forests took over 65 million hectares of abandoned agricultural fields \u2014 a surge of regrowth that reduced global CO 2 . In the working group, Zalasiewicz and others have been talking increasingly about another option \u2014 using the geological marks left by the atomic age. Between 1945 and 1963, when the Limited Nuclear Test Ban Treaty took effect, nations conducted some 500 above-ground nuclear blasts. Debris from those explosions circled the globe and created an identifiable layer of radioactive elements in sediments. At the same time, humans were making geological impressions in a number of other ways \u2014 all part of what has been called the Great Acceleration of the modern world. Plastics started flooding the environment, along with aluminium, artificial fertilizers, concrete and leaded petrol, all of which have left signals in the sedimentary record. In January, the majority of the 37-person working group offered its first tentative conclusion. Zalasiewicz and 25 other members reported 5  that the geological markers available from the mid-twentieth century make this time \u201cstratigraphically optimal\u201d for picking the start of the Anthropocene, whether or not it is formally defined. Zalasiewicz calls it \u201ca candidate for the least-worst boundary\u201d. The group even proposed a precise date: 16 July 1945, the day of the first atomic-bomb blast. Geologists thousands of years in the future would be able to identify the boundary by looking in the sediments for the signature of long-lived plutonium from mid-century bomb blasts or many of the other global markers from that time. \n               A many-layered debate \n             The push to formalize the Anthropocene upsets some stratigraphers. In 2012, a commentary published by the Geological Society of America 6  asked: \u201cIs the Anthropocene an issue of stratigraphy or pop culture?\u201d Some complain that the working group has generated a stream of publicity in support of the concept. \u201cI'm frustrated because any time they do anything, there are newspaper articles,\u201d says Stan Finney, a stratigraphic palaeontologist at California State University in Long Beach and the chair of the ICS, which would eventually vote on any proposal put forward by the working group. \u201cWhat you see here is, it's become a political statement. That's what so many people want.\u201d Finney laid out some of his concerns in a paper 7  published in 2013. One major question is whether there really are significant records of the Anthropocene in global stratigraphy. In the deep sea, he notes, the layer of sediments representing the past 70 years would be thinner than 1 millimetre. An even larger issue, he says, is whether it is appropriate to name something that exists mainly in the present and the future as part of the geological timescale. Some researchers argue that it is too soon to make a decision \u2014 it will take centuries or longer to know what lasting impact humans are having on the planet. One member of the working group, Erle Ellis, a geographer at the University of Maryland, Baltimore County, says that he raised the idea of holding off with fellow members of the group. \u201cWe should set a time, perhaps 1,000 years from now, in which we would officially investigate this,\u201d he says. \u201cMaking a decision before that would be premature.\u201d That does not seem likely, given that the working group plans to present initial recommendations by 2016. Some members with different views from the majority have dropped out of the discussion. Walker and others contend that human activities have already been recognized in the geological timescale: the only difference between the current warm period, the Holocene, and all the interglacial times during the Pleistocene is the presence of human societies in the modern one. \u201cYou've played the human card in defining the Holocene. It's very difficult to play the human card again,\u201d he says. Walker resigned from the group a year ago, when it became clear that he had little to add. He has nothing but respect for its members, he says, but he has heard concern that the Anthropocene movement is picking up speed. \u201cThere's a sense in some quarters that this is something of a juggernaut,\u201d he says. \u201cWithin the geologic community, particularly within the stratigraphic community, there is a sense of disquiet.\u201d Zalasiewicz takes pains to make it clear that the working group has not yet reached any firm conclusions.\u201cWe need to discuss the utility of the Anthropocene. If one is to formalize it, who would that help, and to whom it might be a nuisance?\u201d he says. \u201cThere is lots of work still to do.\u201d Any proposal that the group did make would still need to pass a series of hurdles. First, it would need to receive a supermajority \u2014 60% support \u2014 in a vote by members of the Quaternary subcommission. Then it would need to reach the same margin in a second vote by the leadership of the full ICS, which includes chairs from groups that study the major time blocks. Finally, the executive committee of the International Union of Geological Sciences must approve the request. At each step, proposals are often sent back for revision, and they sometimes die altogether. It is an inherently conservative process, says Martin Head, a marine stratigrapher at Brock University in St Catharines, Canada, and the current head of the Quaternary subcommission. \u201cYou are messing around with a timescale that is used by millions of people around the world. So if you're making changes, they have to be made on the basis of something for which there is overwhelming support.\u201d Some voting members of the Quaternary subcommission have told  Nature  that they have not been persuaded by the arguments raised so far in favour of the Anthropocene. Gibbard, a friend of Zalasiewicz's, says that defining this new epoch will not help most Quaternary geologists, especially those working in the Holocene, because they tend not to study material from the past few decades or centuries. But, he adds: \u201cI don't want to be the person who ruins the party, because a lot of useful stuff is coming out as a consequence of people thinking about this in a systematic way.\u201d If a proposal does not pass, researchers could continue to use the name Anthropocene on an informal basis, in much the same way as archaeological terms such as the Neolithic era and the Bronze Age are used today. Regardless of the outcome, the Anthropocene has already taken on a life of its own. Three Anthropocene journals have started up in the past two years, and the number of papers on the topic is rising sharply, with more than 200 published in 2014. By 2019, when the new fossil hall opens at the Smithsonian's natural history museum, it will probably be clear whether the Anthropocene exhibition depicts an official time unit or not. Wing, a member of the working group, says that he does not want the stratigraphic debate to overshadow the bigger issues. \u201cThere is certainly a broader point about human effects on Earth systems, which is way more important and also more scientifically interesting.\u201d As he walks through the closed palaeontology hall, he points out how much work has yet to be done to refashion the exhibits and modernize the museum, which opened more than a century ago. A hundred years is a heartbeat to a geologist. But in that span, the human population has more than tripled. Wing wants museum visitors to think, however briefly, about the planetary power that people now wield, and how that fits into the context of Earth's history. \u201cIf you look back from 10 million years in the future,\u201d he says, \u201cyou'll be able to see what we were doing today.\u201d See Editorial  page 129 \n                     All in good time 2015-Mar-11 \n                   \n                     Defining the Anthropocene 2015-Mar-11 \n                   \n                     First atomic blast proposed as start of Anthropocene 2015-Jan-16 \n                   \n                     Biodiversity: Life \u00ad\u2013 a status report 2014-Dec-10 \n                   \n                     The invisible extinction 2014-Oct-21 \n                   \n                     Storm warning 2014-Jun-25 \n                   \n                     The Anthropocene could raise biological diversity 2013-Oct-02 \n                   \n                     Human influence comes of age 2011-May-11 \n                   \n                     Blog post: Panel wades into science, politics and the perils of the Anthropocene \n                   \n                     Anthropocene Working Group \n                   \n                     International Commission on Stratigraphy \n                   Reprints and Permissions"}
]