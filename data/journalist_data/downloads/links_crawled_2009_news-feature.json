[
{"file_id": "462978a", "url": "https://www.nature.com/articles/462978a", "year": 2009, "authors": [{"name": "Eric Hand"}], "parsed_as_year": "2006_or_before", "body": "As a physicist, he found a way to capture atoms and won a Nobel prize. Now he is marshalling scientists and engineers to transform the world's biggest energy economy. Eric Hand profiles the US energy secretary,  Nature 's Newsmaker of the Year.   STEVEN CHU   is heading home on a bright day in October. His motorcade of government cars powers up the slope of Cyclotron Road, past the fragrant stands of eucalyptus and through the guard station at the entrance of Lawrence Berkeley National Laboratory. The vehicles continue along Chu Road and come to a stop near the top of the hill. The man after whom the road is named heads into Building 50, which housed his office for the five years that he ran this laboratory overlooking the University of California, Berkeley. Inside an auditorium, 225 former colleagues await his arrival. Some wear suits; others slouch in hooded sweatshirts and sandals. There is an eager anticipation in the air, and moments before Chu arrives, the crowd grows quiet. Orange-vested security guards, armed with walkie-talkies, open the doors, and Chu walks down to the podium, his entourage trailing. \"It's very good to be back here,\" he says, flipping open his computer. \"You people know I do my own PowerPoints. That has not changed.\" He launches headlong into a fast-paced and scattered talk that leaps across dozens of topics, all under the banner of climate change. He clicks ahead to the crucial slide \u2014 the one that shows actual measurements of rising global temperatures outpacing what would be expected without all the carbon dioxide that humans have spewed into the atmosphere. \"Here's the evidence,\" he says. \"I have to play this over and over again.\" Such is his task back in Washington DC, where Chu now works as Secretary of the Department of Energy (DOE) and a member of President Barack Obama's cabinet \u2014 the first Nobel-prizewinning scientist to hold such a high office in the US government. He is charged with transforming the world's biggest energy economy, and he has assumed the role of persuader-in-chief, trotting before Congress to explain the science of climate change and his plans for combating it. Meeting regularly with representatives and senators, he targets sceptics and walks them through the data. \"I say, 'Come to my office and we'll talk about it',\" he explains. \"At the very least you can put a little doubt in their minds. If they're so sure it's natural causes, they may be less sure.\" It helps to have a Nobel prize, he adds. In confronting what he sees as the most pressing problem facing the world today, Chu looks back in time to chart a way forwards. The Berkeley lab he once ran is the descendant of the Radiation Laboratory, where the physicist Ernest Lawrence helped find ways to enrich uranium for the Manhattan Project. Chemist Glenn Seaborg's team discovered plutonium there, and theoretical physicist Robert Oppenheimer worked just down the hill before heading into the New Mexico mountains to build the first nuclear bombs. Chu plans to tackle climate change by reviving the scientific and technological urgency of the Manhattan Project \u2014 enlisting some of the nation's best minds to find a way to power the world without ruining it. His plans start at home, where he is trying to push the ponderous DOE to support riskier research that could yield huge dividends. With a budget of US$27 billion, the department runs 17 national laboratories, oversees America's nuclear stockpile and manages the environmental clean-up after the early nuclear age. It is the largest source of funds for physical-science research in the United States, and this year Chu had a much bigger pot to dole out. Just one month into his tenure, Congress gave the agency $37 billion in economic stimulus money \u2014 funds that Chu is steering towards renewable energy, nuclear power, carbon-sequestration pilot plants and projects to modernize the electric grid, all of which should help to solve the climate problem. \"They say that necessity is the mother of invention and this is the mother of all necessities,\" he says. \"So we're going to get the mother of all inventions. And it's not going to be just one, it has to be many.\"  \n                Hands-on manager \n              In the 1980s, Chu made his name scientifically by trapping atoms using lasers tuned with the utmost precision. Now he is applying that same mastery of detail to a vastly more complex system: an agency of 100,000 people working on all aspects of energy and nuclear issues. Some Washington veterans have questioned whether Chu's research talent and hands-on style of management will serve him well, both at the DOE and amid the harsh political environment of the nation's capital. He has made some mistakes, notably in his dealings with Congress. But nearly a year into his tenure, Chu has proved that he is a quick learner. He has established himself as a voice that can be trusted by politicians of various stripes. He has helped to bridge international divides, particularly between the United States and China. And he has lured some top scientists from industry and universities to join him at the DOE in his quest. Carol Browner, Obama's climate tsar, works often with Chu as part of the president's 'green cabinet', a group of senior officials who oversee environmental matters. \"I think he's going to turn out to be the best energy secretary ever,\" she says. Praise also flows from some Republican politicians. Samuel Bodman, who led the DOE for former president George W. Bush, says that Chu has \"shown skills as a manager. I think it was an inspired choice by the president to pick him.\" Growing up in a New York suburb during the 1950s, Chu and his two brothers learned quickly that academic excellence \u2014 and competition \u2014 were family traditions. The boys would watch  College Bowl , a 1960s television quiz show, and \"the three of us would shout out answers and try to beat the contestants\", recalls Morgan Chu, the youngest brother and a high-profile lawyer in California. Chu's father and mother fled China during the Second World War and both did graduate work at the Massachusetts Institute of Technology (MIT) in Cambridge. The eldest son, Gilbert, followed the path of academic prestige \u2014 accumulating science degrees from Princeton University in New Jersey and MIT before gaining an MD from Harvard University in Cambridge, Massachusetts. Morgan did a PhD in social science before heading to Harvard Law School. Steven, on the other hand, was the A-minus student who favoured tinkering over schoolwork. In a family of Ivy Leaguers, he says he was the \"academic black sheep\", who settled for the University of Rochester in New York, where he studied mathematics and physics. Family pressures, he says, drove him \u2014 and frustrated him \u2014 early on, but once at Rochester, his facility for science flourished. \"All of a sudden, the things they wanted me to do were very natural,\" he says. On entering graduate school at Berkeley in 1970, Chu began a love affair with lasers. The work that was once a chore became the focus of an obsessive energy. \"I've never been that good at apportioning time,\" he says. \"When I got really excited about something, I would dig into it. It turns out that is a quality that the best researchers have.\" Another Berkeley graduate student, Phil Bucksbaum, recalled nearly getting into a fist fight with Chu because he was being \"bossy about the lasers\", until a third student, who had studied with Chu at Rochester, explained to Bucksbaum: \"It's the way he always has been. Focused and brusque,\" says Bucksbaum. Chu's graduate work using polarized light to probe atomic transitions was good enough for him to get a job at Bell Labs in New Jersey, then a utopia for basic research. Chu thrived there, but he also made sacrifices. As his work progressed, he spent more time away from home, says his ex-wife, Lisa Chu-Thielbar. Sometimes, she would smuggle his first son, Geoffrey, under her overcoat onto the laboratory campus to catch some time with his father. \"He was always a scientist first and a father second,\" says Chu's second son, Michael, who doesn't fault his father for the singular focus that allowed him to achieve so much. \"The ambition was all intellectual and scientific. Steve never cared about money. He didn't even care about advancement,\" says Chu-Thielbar. After seven years at Bell Labs, Chu had a key insight in 1985 into how to trap atoms. He crossed six lasers to form what he called \"optical molasses\", a goo of photons. It slowed atoms nearly to a standstill, making them sluggish enough to be held by the electromagnetic forces of an additional laser. A year later, in the winter of 1986, Chu glimpsed the foundation of his Nobel prize through the windows of a vacuum chamber. Sodium atoms, cooled in optical molasses to 240 millionths of a degree above absolute zero, grew bright orange as they fell, one by one, into a trap the size of a sand grain. A colour photo, the first ever published in  Physical Review Letters , provided the proof of his success ( S. Chu, J. E. Bjorkholm, A. Ashkin and A. Cable  Phys. Rev. Lett.    57,   314\u2013317; 1986 ). The work would spawn applications across several disciplines. It provided biologists with 'optical tweezers' \u2014 ways to manipulate individual biomolecules, such as DNA. And it gave other atomic scientists the tools to create Bose\u2013Einstein condensates, the super-cooled states of matter that can trap light and bring photons to a standstill, in a reversal of Chu's original technique. By 1987, Chu was ready to move back to academia. He had offers at Harvard and Berkeley, but was intrigued at the idea of helping to build up a less-celebrated physics department at Stanford University in Palo Alto, California. It was a good plan. Stanford soon became a powerhouse; beginning in 1995, physicists there would win four Nobel prizes in a row, including Chu in 1997. While at Stanford, Chu started to push off in new directions, personally and professionally. He divorced Chu-Thielbar and married Jean Fetter, a physicist and former dean of admissions at Stanford. He took on graduate students with interests in biology and helped to convince the Stanford administration to build a $150-million biophysics centre. But in 2004, just after that centre was completed, the Lawrence Berkeley National Laboratory (LBNL) came calling. Chu, who had never managed anything bigger than a physics department, was ready to make the leap to running the laboratory, which now has 4,000 employees and a $650-million budget. He showed his mettle early on, pushing the University of California system, which manages the LBNL, to use its debt service in an unprecedented way to finance new buildings for the lab, and fighting to save employee pension plans. Chu personally argued on behalf of his employees with the president of the University of California system until he relented, says Graham Fleming, a chemist at Berkeley and Chu's deputy at the time. \"If one argument didn't work, he'd try another,\" he adds.  \n                The climate crusader \n              Chu says that there was no one moment when he decided to devote himself full time to climate and energy puzzles. He had been digesting the science for years, reading reports of the Intergovernmental Panel on Climate Change. And he had pursued energy efficiency in his own life with his customary precision, complaining when workers skimped on insulation in his Stanford home. But soon after arriving at the LBNL, he decided that the time was ripe to resurrect an energy-research programme that had lain largely dormant since the fuel crisis of the 1970s. The lab was ready to revive those efforts, but it needed Chu's energy and vision, says Paul Alivisatos, who succeeded Chu as the head of the LBNL. \"It's a bit like a supersaturated solution that you drop a seed crystal into,\" he says. \"Steve was the seed crystal.\" Chu gave the sprawling lab a purpose and convinced many scientists to make the switch, as he had, into energy research. He attracted large infusions of funding from the DOE and from the energy company BP. The lab launched major initiatives in biofuels and photovoltaics, but Chu also got involved in the little stuff. Alivisatos recalls Chu's interest in revamping a system of old, lumbering shuttle buses that circle the heights of the Berkeley Hills. Chu would stand on the balcony of the director's office and keep tallies of riders at a bus stop. \"He thinks at an incredibly high level, but he also delves down into the finest detail,\" says Alivisatos. \"And one of his abilities is to find the salient detail that matters enormously to the big picture and to show you how those connect.\" But some thought that Chu went too far by micromanaging lab operations. \"He doesn't see the necessity to get other people involved,\" says one scientist who knows him well but did not want to be identified as criticizing an official who controls so much research funding. \"His whole career has been founded on his fantastic ability to worry about all the details himself. And that makes it hard for him to empower an effective staff.\" Obama's election, and his campaign pledges to revamp the US energy system, created new opportunities for Chu. A few weeks after the election, Chu flew to Chicago to meet with the president-elect. \"A lot of people are telling me you're the person for the DOE,\" said Obama, according to Chu. Rarely at a loss for words, Chu could only think to quip, \"Who are these former friends of mine?\" Inspired by a sense of service, Chu planned to accept the job, but he did have a demand. He had seen the energy department hamstrung in the past by ineffectual people placed in posts to satisfy political obligations, so he wanted control over senior appointments. \"There was a reasonable shot I could attract the right people. There are a whole bunch of people that have to lift this load,\" Chu says. Obama agreed, and Chu has recruited top talent such as Steven Koonin, former chief scientist of BP and provost of the California Institute of Technology in Pasadena, who is now undersecretary for science. On the top floor of the energy department, portraits of secretaries past preside over the long, carpeted hallway leading from the elevators to the secretary's office. Most are of career politicians, with a few exceptions: Charles Duncan, who ran his family's coffee company, Donald Hodel, who would go on to lead two Christian evangelical groups, and James Edwards, a dentist. Bodman, Chu's predecessor, has an engineering degree from MIT. But Chu is the first scientist to lead an agency that has such an important role in physical-science research. On an end table in his waiting room lie some recent biophysics papers on which Chu is a co-author. Chu puts the papers out to make a point \u2014 to visitors and himself \u2014 that he is still a working scientist. During his time at the LBNL, he kept a small research group of Stanford and Berkeley students, holding group meetings on Friday nights and on weekends. Even now, he says, he finds a little time for research during plane flights. Although Chu's October visit to the LBNL was his first lab-wide talk, he had visited before to check in with postdocs and meet new, young scientists \u2014 trips that Alivisatos calls Chu's \"science vacations\". During an interview at his office, Chu settles into the centre of a couch, his back to an expansive view of Washington DC's mall and the Smithsonian Castle. At 61, Chu is resolutely trim. Although he no longer commutes to work by bike, as he often did at Berkeley, he manages long weekend rides and regularly climbs the seven flights of stairs to his office. Chu leans back when he listens, which is often, and leans forward when making a point. He is quick to crack a joke and eager to please. At least, he's that way with politicians (and reporters). With scientists, he can be impatient. \"He does not suffer fools,\" says Michael Levi, an astrophysicist at the LBNL. Chu retains a scientist's candour \u2014 and that can sometimes get him into trouble. At his confirmation hearing, some senators jumped on Chu for calling coal \"his worst nightmare\" in a 2007 talk. (Chu says that the United States, China and India are unlikely to turn their backs on their huge coal reserves and that underscores the need to find clean ways to use the fuel.) A month after taking office, Chu slipped when he told reporters that it was \"not in his domain\" whether the Organization of the Petroleum Exporting Countries (OPEC) should cut oil production, an impolitic statement. He acknowledges that he was surprised at how his words have been magnified by the press. Yet his inability to mince words is also an asset, especially in the floors below him in the DOE's headquarters, a fortress on concrete stilts. The DOE national labs have been characterized as inefficient, but that is in part because past safety and security lapses have led to a culture that stresses caution over aggressive research. When, during his talk at the LBNL, Chu mentions his desire to return to the original spirit of the labs, GOCO \u2014 government owned, but contractor operated \u2014 he gets a hearty round of applause. Chu says that the risk-averse culture, at both headquarters and the labs, must be changed. \"The best way to protect yourself from something bad happening is to not do much.\" Chu has already made headway. When he found out that billions of dollars in loans for energy projects that had been authorized in 2005 had not progressed, he insisted that they be pushed out in months, with the first one going to a solar-power company. It has helped to get involved personally, he says. Before closing on a $5.9-billion loan with Ford Motors, Chu says he was talking to the firm's chief executive every third day \u2014 an example that sent a clear message to his subordinates to act. \"In certain areas, I'm not going away,\" he says. \"The pressure is not going to let up.\"[ image 4 right]To encourage more adventurous research, he has pushed to develop the Advanced Research Projects Agency-Energy, known as ARPA-E, which draws its inspiration from DARPA, the celebrated research programme run by the Department of Defense that had an important role in creating the Internet. ARPA-E is designed to pursue high-risk, high-reward research on new forms of energy and conservation (see  'Blue sky, green tech' ). The programme preceded Chu, but it's a pet of his, not least because he recommended it as a co-author on an influential study by the National Academies entitled  Rising Above the Gathering Storm , which in 2005 warned of declining American competitiveness. The ARPA-E concept will work, says Chu, only if the smartest reviewers are enlisted to pick out the most innovative ideas \u2014 otherwise incremental research is rewarded. \"I unfortunately can't review all of the proposals myself,\" he told a group of clean-energy businesspeople in October, only half-jokingly. So Chu wrote a letter to the presidents of top research universities asking them to nominate their best researchers as ARPA-E reviewers. Five hundred responded to the call for duty. Chu himself spent about two hours with the final set of proposals. Fleming, Chu's former LBNL deputy, says this sort of task suits his old boss. \"I've never known anyone able to go away and come back 10 minutes later knowing so much about a new topic.\" And on the day that he visited the LBNL, Chu announced the 37 winning proposals, which would use $151 million of an initial $400 million given to the programme. Chu's most ambitious idea has been to create eight focused and independent energy labs, modelled after the Manhattan Project, to develop technologies such as next-generation batteries and advanced nuclear power (see  'Chu's innovation factories' ). But this is where he has run into the most trouble, and it exposes the limitations of the do-it-all-yourself approach. As Congress debated whether to fund Chu's new labs in fiscal year 2010, staffers found that they couldn't get the details on what, exactly, the DOE wanted. Would they be virtual labs, or permanent facilities? How many years would they be funded for? What mix of basic and applied science would be supported? \"The hubs were just dropped on Congress,\" says one congressional staffer who adds that Chu's office did not provide consistent or timely information. The communication problems with the Hill were on display at a hearing of a congressional appropriations committee in May. Senator Diane Feinstein (Democrat, California), a friend of Chu's, had a complaint. She had wanted to talk privately with him about some solar projects, but she had not been able to make an appointment to see Chu via his staff. \"I'm a little bit surprised if you asked to see me and my staff said no,\" Chu replied. \"We just haven't gotten a response, that's sort of the way it's done,\" said Feinstein in an apparent attempt to educate the secretary on Washington customs. But Chu, who likes to deal with issues himself, did not seem to understand. \"I'm still surprised,\" he said. \"You actually have my private number.\" In the end, when Congress doled out money to the DOE, Chu lost some battles. Money that he had proposed cutting from hydrogen research was reinstated. A $115-million education programme he had championed received nothing. Worst of all, for Chu, only three of his eight energy hubs were funded. Chu's critics say that more attention to Congress could have alleviated the problems, but nearly a year into his tenure, he has not appointed an assistant secretary to head up his legislative-affairs office. Chu says the vacant position was not the problem. The issue was that he hadn't followed through himself. \"The failure was on my part,\" he says, \"because I wasn't communicating what the real vision was.\"  \n                Energy ambassador \n              On a cold day in early December, Chu was preparing to travel to the United Nations' climate-change conference in Copenhagen. Before the trip, one of the last public events on his schedule was to appear with Secretary of Commerce, Gary Locke, to talk about speeding up the process for granting patents on green technologies. In July, the two secretaries went to Beijing together to meet with Chinese energy ministers. Locke, a prominent politician of Chinese descent, was greeted warmly. But Chu, with his Nobel-prize pedigree, was a rock star in a culture that reveres education. \"He was like a Michael Jordan,\" says an administration official. \"Everybody knew this guy.\" Chu has taken a particular interest in China not because of his ancestry, he says, but because it emits more carbon dioxide than any other nation and it is also spending billions of dollars on clean-energy research. During the trip, Chu and Locke announced that the United States and China would jointly pursue research in areas such as energy efficiency and capturing carbon dioxide from coal-plant exhaust. In his trip to Denmark, Chu reprised his role as energy ambassador. He announced plans to hold a conference next year with foreign energy ministers and pledged $85 million in US aid for renewable-energy projects in the developing world. For Chu, the summit served as a prelude to the fight next year, when he will use his main weapons \u2014 knowledge and powers of persuasion \u2014 to try to convince members of Congress to vote for a climate bill that would for the first time cap US emissions of greenhouse gases. Chu says that when he ends his time as energy secretary, he will measure his success by two criteria: whether he aided adoption of a climate bill, and how much he changed the way that the DOE supports science. Those metrics would have seemed odd to a young scientist at Bell Labs in the 1980s who spent his days fretting over the precision of laser beams. Chu didn't plan on working his way to the upper echelons of the US government, where he is the first scientist since the cold war to play such an active part. \"It just sort of happened,\" he says. \"I followed the path first from going and doing the science, to getting very concerned about some issues that affect us all as a society, to finally saying, I can't sit idly by and occasionally give a talk on this. I really have to get proactive and put my money where my mouth is and do a career shift because it is that important.\" But looking back, it's possible that the call to public service may have been whispering to Chu even during his graduate-school days at Berkeley, where the memories of the war effort remained fresh in the physics department. When Chu briefly took up sculpting at Berkeley, he chose to make a bust of Oppenheimer, the physicist-turned-manager who oversaw all details of the Manhattan Project. Chu is now looking to another Berkeley star for inspiration. Lately, he has been reading the journals of Seaborg, who led the war-time team racing to extract plutonium for a bomb. Seaborg recounts how his group required fast-working Geiger counters that were not available at the time. So he pushed his crew to invent the needed detectors. For Chu, that sense of urgency in the face of a great threat stands out in Seaborg's work: \"He kept saying: 'This isn't university research. We've got to move much faster'.\"   See Editorial,  \n                     page 957 \n                   . \n                     Steven Chu's Nobel Prize autobiography \n                   Reprints and Permissions"},
{"file_id": "4611042a", "url": "https://www.nature.com/articles/4611042a", "year": 2009, "authors": [{"name": "Anjali Nayar"}], "parsed_as_year": "2006_or_before", "body": "Deep in the Himalayas, the disappearance of glaciers is threatening the kingdom of Bhutan. Anjali Nayar trekked through the mountains to see how the country is adapting to a warming world. Kaka Tshering loops a piece of frayed jute rope around a 150-kilogram boulder. A handful of his fellow workers line up on either end and pull the rope taught. \"Shochi, Shoni,\" the workers call in unison, as they heave. Their voices are raspy from the 4,400-metre altitude and moist, cold air. \"Put your strength together.\" After rocking a couple of times, the boulder rolls over and the labourers tumble backwards. Their cries are drowned out by the furious work around them as more than 300 men and women scrape away rocks with spades and shovels to reach their daily quota. The work force is a cross-section of life in Bhutan. There are a number of young dropouts from the capital Thimphu, with greasy, shoulder-length hair and tattoos running up their forearms. Retired soldiers from the Royal Bhutan Army labour alongside former students of Buddhism. There are a handful of women in their traditional tartan-style robes, beaded necklaces and antique silver and turquoise brooches. Together in matching hard hats and leaky rubber boots, they make up Bhutan's army against the effects of climate change. Their task is to deepen and widen the outlet channel from lakes formed by the rapidly melting Thorthormi glacier. By helping the water to drain faster, Bhutanese officials hope to prevent a catastrophic flood. Glaciers in the Himalayas are retreating faster than in any other part of the world and they could disappear completely by 2035 (ref.  1 ). This puts the mountainous nation of Bhutan at a special risk. In an area smaller than Switzerland, it has 983 glaciers and 2,794 glacial lakes, some of which have burst to produce deadly glacial lake floods. As a poor nation without even its own helicopter, Bhutan lacks the resources to combat global warming. It is carrying out the work at Thorthormi glacier with the help of money from various international donors, including US$3.5 million from the Least Developed Countries Fund, created under the United Nations Framework Convention on Climate Change. The global cost of adaptation could total hundreds of billions of dollars a year \u2014 orders of magnitude more than what is available to poor countries at the moment. During December's UN talks in Copenhagen, developing countries will be pushing for more generous \u2014 and reliable \u2014 funding to help them mitigate the impacts of climate change. As the first nation to get adaptation money from the Least Developed Countries Fund (see  'The long wait for adaptation money' ), Bhutan is something of a pioneer among developing nations in their quest to adapt to a warmer future. And the struggles at Thorthormi glacier illustrate the enormous obstacles that adaptation efforts still face.  \n                Dangerous dam \n              The sounds of global warming are deafening at Thorthormi glacier. Every few minutes, a block of ice rips off the glacier and crashes into the lake in a trail of dust and ice. These are some of the tallest mountains in the world and form Bhutan's northern boundary with Tibet. To get a look at the hazard posed by the melting glacier, Karma Toeb, the project's glaciologist and team leader, scrambles to the top of a moraine \u2014 a steep ridge of loose, angular boulders built up by the glacier as it pushes debris along its edges. This moraine is a dam between two bodies of water (see  map ). To the east, Toeb points out how several slushy grey-brown ponds have formed on top of the Thorthormi glacier. To the west lies a much larger lake called Rapstreng \u2014 vast and milky green \u2014 about 80 metres lower in elevation. Toeb first came to this site in 1997, during a project to shrink Rapstreng lake. Over three years, a thousand labourers widened and deepened Rapstreng's natural outlet to lower the lake by 4 metres. At the time, nobody was worried about the Thorthormi glacier next door. \"Thorthormi was almost pure ice,\" says Toeb. \"We could cross over and walk on the glacier to do our research.\" The glacial lake didn't even appear in an exhaustive study of the region's dangerous lakes in 2001 by the International Center for Integrated Mountain Development (ICIMOD), based in Kathmandu, Nepal 2 ,   3 . It is only within the past decade that researchers realized that Thorthormi could pose a threat. Thorthormi's ponds were expanding and merging to form larger bodies of water. The changes have been dramatic even in the past few months. \"Just before we started our work here in July this year, that part of the lake was water,\" says Toeb, pointing down to a number of icebergs. \"The ice blocks have been breaking off the mother glacier upstream.\" But the rate at which Thorthormi glacier is melting is not the only concern, says Toeb, motioning at the boulders underfoot. The moraine separating the Thorthormi and Rapstreng lakes is at places only around 30 metres wide and is prone to landslides. Geophysical testing in 2008 showed that the moraine contains a substantial amount of ice. As that ice melts, along with the rest of the glacier, the ridge could collapse, releasing the water from Thorthormi into Rapstreng below. Then there would be a combined outburst flood, Toeb says.  \n                Building pressure \n              A four-year study led by Hermann H\u00e4usler from the University of Vienna in collaboration with Bhutan's geology and mines department predicted the moraine between Thorthormi and Rapstreng could give way as early as 2010, because of the hydrostatic pressure of the growing Thorthormi glacial lake. A combined flood would unleash around 53 million cubic metres of water down the Pho river 4 . Every country within the Himalayan region has suffered a glacial outburst flood at some point 3 , and Bhutan is no exception. A number of glaciers in that country are missing sections of their moraines, suggesting that lakes have burst there in the past. Half a century ago, floods originated from the Lunana region in northern Bhutan, which is home to Thorthormi and many other glaciers. The floods may have come from glacial lakes, but there were no studies of them at the time. The only scientifically documented glacial flood in Bhutan's history came from a lake at the bottom of Luggye glacier, on Thorthormi's eastern margin, 15 years ago. Ponds first appeared on the Luggye glacier in the late 1960s, and by the early 1990s the glacier was retreating by up to 160 metres a year 2 . On 7 October 1994, the glacier's moraine broke and released an estimated 18 million cubic metres of water and debris down the Pho river, killing 21 people and razing fields and settlements downstream with a jumble of uprooted trees, boulders and mud. Memories of that flood still haunt Dawa Gyeltshen, a herder who lives in the village of Taksho, beside the Pho river around 20 kilometres downstream of Luggye. He remembers being woken by a thunderous noise early that morning. For hours he raced around outside his house in the dark, terrified as the river rose. It wasn't until dawn that Gyeltshen saw the extent of the damage. The forested river valley and a large portion of the fields in front of his house had been washed away and in their place were mounds of boulders and glacial silt. Today, the landscape is recovering. Pockets of shrubs and wildflowers have settled in along the river and Gyeltshen's remaining land is golden with this year's crop of buckwheat. White prayer flags line the rugged edge of the property, marking the course of the water during the flood. But even now, Gyeltshen says he still has trouble sleeping through the night, worried about another flood. A combined flood from Thorthormi and Rapstreng lakes could cause at least ten times the damage and fatalities of the 1994 event 4 . In the past 15 years, hospitals and schools have popped up along the river, along with a partially completed $760-million, 1,200-megawatt hydropower dam project. It was in part because of the 1994 disaster that Bhutan became a leader in adapting to climate change. Following the flood, scientists from Bhutan, Japan and Austria, among other countries, launched research programmes into the glaciers in the Lunana region. When the Least Developed Countries Fund was created in 2001, \"we had a lot of good information on glacial floods\", says Thinley Namgyel, the deputy chief environment officer at the National Environment Commission in Thimphu.  \n                Gross national happiness \n              Bhutan's work on adaptation is backed by a strong record of environmental protection. As a follower of the Buddhist tenet of non-extremism \u2014 the Middle Path \u2014 Bhutan's monarchy has stressed that modernization and economic development are important, but not at the expense of the country's natural environment and cultural traditions. In 1972, Bhutan's fourth King, Jigme Singye Wangchuck turned away from the classic yardsticks of growth, such as gross domestic product, to focus on a more holistic approach to his people's well-being. The measurement, which he called gross national happiness, is based on several measures of environmental and social contentment. The country has some of the most progressive \u2014 and controversial \u2014 environmental regulations in the world, including bans on plastic bags, timber exports, hunting and even tobacco sales. They've also minimized tourist traffic by charging visitors to the country around $200 a day. The nation's proven environmental record, coupled with the immediacy of the risks of a glacial flood, made Bhutan's adaptation needs especially attractive to donors. \"From the start, Bhutan was in the driving seat,\" says Bonizella Biagini, a senior programme manager at the Global Environmental Facility, which manages the Least Developed Countries Fund. Other developing nations have been less successful in getting adaptation money through that fund. \"If everyone behaved like Bhutan, every project would be in the process of being implemented,\" says Biagini. Bhutan has also received $1.3 million in adaptation support from other sources, including the Austrian Development Cooperation, the conservation group WWF Bhutan and the United Nations Development Programme. Yet even with money and political will-power, the task is daunting. The main goal at Thorthormi glacier is to lower the lake's water level by 5 metres, which would reduce the hydrostatic pressure pushing on Thorthormi's unstable natural dam. Under normal conditions, the lowering would be an easy task, but the remote location of the lake, the extreme altitude, the boulder-strewn terrain and the unpredictable weather have made the project logistically difficult. The project's engineer, Karma Tenzin, had hoped originally to fly a couple of excavators to the site and do the digging in a few weeks. But soon after visiting Lunana this year, he realized an industrial solution would be difficult. The nearest potential helicopter landing was an hour and a half away by foot. Even if an excavator was brought in, it could topple on the uneven terrain, says Tenzin. \"Everywhere you look are boulders,\" he says. \"Bringing in an excavator is useless.\" The price was also a factor: without its own helicopter, Bhutan would have had to hire one from Nepal for several thousand dollars per trip. Because unemployment is rising in Bhutan, the project board reasoned that it would be better to pump the money into the country's economy \u2014 by paying local horsemen to transport the project's goods and by providing excavation jobs. So in July, in the middle of the monsoon season, hundreds of workers set off on the 9-day journey to Lunana from the capital. At the site, using shovels and spades, they load cobbles into strips of burlap and carry them to the sides of the channels. Boulders are broken crudely with blunt hammers and hauled off-site with weathered rope. At the end of September, a few jackhammers and chisels arrive to help split boulders. A shipment of rope also arrives, although Tenzin admits that it is unlikely to last long. \"At least for the next two weeks we will be able to pull rocks,\" he says. \"After that, the rope is likely to be all broken again.\" The working season for the project is less than four months a year, because snow blocks the path to the site for all but July to October. But heavy rainfall this year washed away several key bridges, delaying work for an additional month. \"We almost cancelled the project for this year,\" says Dowchu Dukpa, the project's manager. \"We thought: by the time we get there, we will have to go back.\" But the project went ahead. When the last workers put down their tools for the year on 15 October, they had lowered the lake by around 90 centimetres. This was considerably less progress than the original goal of 1.67 metres for this first year, but the group thinks it can make up the difference during the next two years of the project. \"I am sure the goal will be achieved,\" says Tenzin.  \n                Practical challenges \n              Even if the final goal of 5 metres is achieved by 2011, glacial experts cannot say how much that will reduce the risk of a moraine burst. Part of the problem is the lack of basic information about the Thorthormi glacier and its lake. The glacier is covered in boulders and silt, which makes it difficult to measure how quickly the ice is retreating, says Koji Fujita, a glaciologist from the University of Nagoya in Japan. The lake's drifting icebergs also make a comprehensive bathymetric survey extremely dangerous, says Fujita. Bhutanese officials only took a few depth measurements in 2008 with a simple string and weight, because strong winds made it difficult to handle the boat. \"We don't know how much water will come out of the lake or how fast it will flow in an outburst,\" says Fujita. What's more, developing countries such as Bhutan don't have the technical and financial resources to study the changes that are happening in their countries. High-resolution satellite imagery can be expensive and the country can afford it only infrequently. Difficult and remote terrain also hampers their monitoring work. Every scientist and bit of instrumentation has to make the journey to Lunana on foot. To top it off, they are contending with a landscape that is rapidly changing because of global warming. Project leaders reason that their strategy at Thorthormi must reduce the hazard there to some degree. \"If the lake is lowered by some metres, that much pressure is released from the moraine wall,\" says Toeb. \"So the chance of the moraine wall failure is less.\" But Toeb admits that the work will not eliminate the possibility of a flood. \"A piece of ice could detach from the mother glacier, fall into the lake and generate a surge wave,\" says Toeb. \"Or if there is a big seismic event in this area, the whole moraine wall surrounding the glacial lake may collapse.\" Because artificially lowering Thorthormi isn't enough to prevent a flood, the government intends to install an automatic flood early warning system in the Pho river valley, which will also be paid for with money from the development fund. The Bhutanese Department of Energy currently runs a manual warning system in the Lunana region. A couple of attendants equipped with a VHF radio and satellite phone are supposed to monitor the lakes and river three times a week. But the system isn't working. \"We never see them at the site,\" says Toeb, who has reported the problem to the department. The new automatic system will record the water levels using sensors. In the case of an outburst, the system would send out a signal to towers downstream to warn communities of an impending flood, potentially providing hours of advance notice. Because Bhutan does not have the capacity or expertise to do the work on its own, its Department of Energy has put the project out to tender, in hopes of an international bid within its budget. Construction is expected to start next year, says Karma Chhophel, the head of the department's hydrological- and meteorological-services division. The entire Thorthormi mitigation project will last at least four years and cost $7.4 million. But Thorthormi is only one of thousands of glacial lakes in Bhutan. ICIMOD has identified 25 potentially dangerous lakes in Bhutan, but the list was based on crude satellite data 2 . The centre will release an updated inventory of dangerous lakes in Bhutan next year based on satellite imagery with \"better spectral, spatial and temporal resolution\", says Pradeep Mool, a remote sensing specialist with ICIMOD, who was involved in the study. The inventory will also rank lakes based on socioeconomic information, including the potential loss of life and damage to infrastructure. \"All the lakes need attention,\" he says. \"But we have to prioritize where we do the big-scale mitigation work, the early-warning system, and the hazard zonation maps.\"  \n                Spy data \n              But qualitative analysis of satellite images isn't enough to determine which lakes are dangerous, according to Fujita. He is trying to develop quantifiable criteria, such as the angle between the level of the water in the glacial lake and the slope of the moraine, that dictate a moraine's stability and the likelihood of a glacial outburst. His team is testing that hypothesis by comparing new satellite images with data from old American spy satellites to create digital elevation models of lakes that burst in the past. The researchers will also develop their own list of the region's dangerous lakes next year, but the results are likely to be substantially different from ICIMODs, says Fujita. \"Some of its 25 dangerous lakes are not a hazard, but we found more that ICIMOD did not point out,\" he says. Regardless of which lakes are a risk now, the number will rise in the future, as glaciers continue to melt. \"Thorthormi is a very good lesson,\" says Toeb. \"Nobody knows what will happen, taking into account all the changes in climate; the same situation may happen to any of the glaciers in Bhutan.\" Although glacial lake bursts cause considerable damage in the Himalayas, an even bigger catastrophe would come from the disappearance of those very same glaciers, and the water they produce, which is predicted within the next few decades. That loss could significantly harm the 69% of the Bhutanese population that relies on farming, mostly subsistence. A decrease in water flow could also seriously affect the country's plan to boost its hydropower production by 10 gigawatts by 2020. \"We don't know the medium- or long-term costs of climate change in Bhutan,\" says Namgyel. \"A few decades down the line, the glaciers will retreat and we are not sure what impact it will have on the economy.\" The impacts will reach far beyond Bhutan's borders. The glacier-fed rivers that flow south from the Himalayas are the arteries of south Asia. It is estimated that the retreat of glaciers will affect the water supply of roughly 750 million people across South Asia and China, says Rajendra Pachauri, the chairman of the Intergovernmental Panel on Climate Change. Across Asia, there are countless cases like Thorthormi, where the needs are great and the resources scarce. Regarding the effects of climate change and their costs, \"every single estimate that people have come up with has been exceeded by reality\", says Pachauri. \"The impacts of climate change are clearly turning out to be much worse than what we had anticipated earlier.\" Standing on Thorthormi's natural dam, the scale of the problems ahead strain the imagination. From here, the sounds of the labourers struggling 100 metres below fade to whispers on the wind. At this distance, it is impossible to see their frost-bitten cheeks, or the cuts and bruises earned during their months of labour. In a line of 30 people, they look like a caterpillar pulling pebbles across a path. They are dwarfed by the size of the lake, the glaciers above, and the mountain of work confronting Bhutan and the rest of the world, as it tries to keep pace with the changing climate. See Editorial,  \n                     page 1027 \n                   , and online at  \n                     http://www.nature.com/roadtocopenhagen \n                    a video at  \n                     http://go.nature.com/VGGayN \n                   . Anjali Nayar is an International Development Research Centre fellow at  Nature . \n                     Nature Asia-Pacific \n                   \n                     Nature Reports Climate Change \n                   \n                     National Portal of Bhutan \n                   \n                     The Global Environment Facility \n                   \n                     The UNFCCC adaptation page \n                   \n                     The International Centre for Integrated Mountain Development \n                   \n                     International Institute for Environment and Development webpage \n                   Reprints and Permissions"},
{"file_id": "461870a", "url": "https://www.nature.com/articles/461870a", "year": 2009, "authors": [{"name": "Glennda Chui"}], "parsed_as_year": "2006_or_before", "body": "Geological faults are not behaving as scientists once expected. Glennda Chui reports on efforts to forge a new understanding of quake behaviour. That low rumbling emanating from California is no earthquake. It is the sound of the state's carefully honed earthquake-forecasting process being shaken hard and put back together. \"We're talking about a radical overhaul,\" says Ned Field, a seismologist with the US Geological Survey (USGS) in Denver, Colorado, and chairman of the Working Group on California Earthquake Probabilities. The group is charged with forecasting the probability of damaging earthquakes throughout the state over the next 30 years. The group's report is due out in 2011 and, like its predecessors (see 'Earthquake probabilities in the California area'), it is expected to have enormous influence \u2014 not only on research but also on public policy in California, the most populous state in America and home to three-quarters of the nation's seismic risk. California has the most comprehensive earthquake-forecasting effort in the world and some of the best-studied faults, so lessons learned there will have ripple effects on other quake-plagued nations. From the standpoint of seismology, the report could push aside an influential family of ideas that have long dominated earthquake research and forecasting. One hypothesis views faults as creatures of habit that tend to rupture in large, 'characteristic' earthquakes of about the same magnitude again and again. Another model asserts that big quakes are most likely to strike in 'seismic gaps' that haven't suffered major jolts in a long time. Both attempt to explain quake behaviour in relatively simple physical terms, and make intuitive sense. But recent research has caused confidence in them to waver. The world of characteristic earthquakes is \"a world we all wish we lived in\", says Thomas Jordan, director of the Southern California Earthquake Center in Los Angeles. \"It hasn't worked out that way because earthquakes don't occur on simple fault structures, but on fault systems that are rather complicated.\" Jordan contends that earthquakes involve complex interactions among faults, leading to chaotic behaviour that is very difficult to predict. The physics of individual faults and the way in which they influence each other may, in fact, be so complex that no simple deterministic rules can explain their behaviour. This new understanding challenges not only the idea of characteristic earthquakes but also an older and more fundamental notion that faults obey regular patterns of behaviour. Called the elastic rebound theory, that concept sprang from the earthquake that levelled large parts of San Francisco in 1906 (pictured, right). It was developed by geologist Henry Fielding Reid of Johns Hopkins University in Baltimore, Maryland, who led the official investigation into the geology and physics of the disaster. After studying the rupture along the San Andreas fault and the aftermath of the ground movement, Reid came to view earthquakes as a repeating pattern of pressure accumulation and release. Strain would build up in the ground until it reached a critical level. At that point, the rocks on one side of a fault would jerk forward relative to their neighbours on the other side. The sudden movement would release almost all the pent-up energy in the ground, and it would take some time before enough strain would accrue to make another quake possible. Reid's was a revolutionary idea, coming as it did half a century before plate tectonics explained where all that stress on the San Andreas fault was coming from. And it was wonderfully appealing, because it implied that the cycles might be regular enough to predict earthquakes. In his 1910 report detailing the results of his investigation 1 , Reid proposed a way to foretell when the next earthquake was due: plant a line of piers at a right angle across the fault and measure their relative angles, distances and heights from time to time. When the measurements show that the strain has built to a critical level, he wrote, \"we should expect a strong shock\".  \n                Stretched too far \n              For a century, the elastic rebound theory has been a foundation on which later seismic hypotheses were built: after all, the strain that builds up in the ground as Earth's crustal plates jostle past each other must be released, eventually, in the form of earthquakes or through a slow, quiet alternative called aseismic slip. But the idea that quakes come tumbling out as steadily as stress goes in is increasingly coming under attack as earthquakes prove to be much more complicated \u2014 and more staunchly individual \u2014 than Reid ever imagined. That individuality has made itself known in numerous ways. In some cases, quakes have come in clusters, such as the giant magnitude-9.1 shock that hit off the northwest coast of Sumatra in 2004 and was followed by a series of nearby quakes, including one last month. In other instances, they rupture large sections of faults, combining patches that had not been known to move together in previous seismic events. Earthquakes even hop from fault to fault, as seen in the magnitude-7.3 earthquake that struck the town of Landers in California's Mojave Desert and the magnitude-7.9 shock that hit central Alaska in 2002. Both of these quakes also had other claims to fame: they triggered tremors, geyser eruptions and other seismic activity thousands of kilometres away. The evidence against seismic regularity has emerged even along the San Andreas fault, where earlier research had provided the appearance of a repeating pattern of large earthquakes. That picture had come from research done in the late 1970s and early 1980s on the south-central section of the San Andreas fault, north of Los Angeles. Geologists working there had determined how much streams and other features had been offset by the great Fort Tejon earthquake of 1857, which measured magnitude 7.9. Trenches dug along the fault had revealed a series of earlier, similar quakes spread about 200\u2013400 years apart. In each spot, the fault appeared to have slipped about the same amount during each quake 2 . But recent excavations have dramatically changed that picture. In 2004, Lisa Grant Ludwig of the University of California at Irvine, began digging trenches in a place called the Bidart Fan, where a series of small streams have cut channels across the south-central San Andreas fault through a thick deposit of sediment. Along the trench walls, past earthquakes show up as disruptions in the sedimentary layers, which can be dated using carbon-14 analysis. In January, she and two other geologists published a paper 3  that lowered the recurrence interval for large earthquakes on this part of the fault to 137 years. Since then they have dropped their estimate to less than 100 years, with some large quakes striking as few as 50 years apart \u2014 implying that earthquakes are not as regular as the previous studies suggested. \"Elastic rebound, I think, is still kind of like the foundation,\" Grant Ludwig says, \"but maybe the building that's going to be built on that foundation is going to look a little different.\" Thomas Fumal, a palaeoseismologist with the USGS in Menlo Park, California, who has been investigating the San Andreas Fault for 25 years, says that the picture along most other parts of the fault also does not conform with past expectations: scientists are finding large quakes striking sections of the fault much more often than previous studies suggested \u2014 and with a wide range of magnitudes.  \n                Tardy quake \n              The idea of characteristic earthquakes suffered another blow in 2004, when the San Andreas fault let loose a magnitude-6 shock near the central Californian town of Parkfield. Researchers had set up a dense network of instruments there in the 1980s, anticipating a moderate quake by 1993 as part of a roughly 22-year cycle that had been witnessed in quakes going back to the 1800s. When it finally arrived more than a decade late, its timing and other idiosyncracies \"confounded nearly all simple earthquake models\" according to a 2005 report 4 . David Jackson has long expected these kinds of developments because they match what he has been seeing in earthquake patterns around the world. In 1991, Jackson and Yan Kagan, both at the University of California, Los Angeles, analysed a global catalogue of seismic events and concluded that large earthquakes bunch together in space and time 5 . This wasn't supposed to happen. If earthquakes obeyed the elastic rebound theory, large quakes should reset the seismic clock and there should be a gap before another big shock hit the same region. Kagan and Jackson argued that regions hit by a big quake were more likely, rather than less likely, to be struck by another 6 . Other geophysicists largely dismissed their claims, but then a stunning sequence of earthquakes hit California's Mojave Desert. A magnitude-6.2 earthquake struck Joshua Tree in April 1992, followed two months later by the Landers quake and the magnitude-6.5 Big Bear quake, and then seven years later by the magnitude-7.1 Hector Mine earthquake. Scientists had known for some time that smaller earthquakes can come in clusters; that's what happens when the ground rattles with aftershocks following a major tremor. But the idea that one large quake could follow on the heels of another was startling. Researchers now routinely talk about large quakes triggering others, both on nearby and distant faults. In fact, the giant 2004 Sumatran quake was followed by an unusual pattern of small tremors near Parkfield \u2014 suggesting that the Indonesian event had weakened the San Andreas fault some 8,000 kilometres away, according to an analysis published this month 7 . In that study, Taka'aki Taira, a seismologist at the University of California, Berkeley, and his colleagues suggest that the Indonesian quake also could have triggered a spate of magnitude-8 shocks around the world.  \n                Trigger happy \n              Karen Felzer of the USGS in Pasadena, California, says that she has come to believe that many quakes of all sizes are triggered by other shocks, rather than simply responding to local forces. \"Personally I agree with Jackson and Kagan that elastic rebound doesn't really happen \u2014 that what is controlling the timing of all earthquakes is the triggering process.\" The upheaval in the world of seismology will play out most strikingly in the work leading up to the 2011 report on California earthquake probabilities. When Earth scientists drew up the first incarnation of this report in 1988, the picture was much simpler. An expert group of geophysicists and geologists divided the faults of the San Andreas system into well-delineated segments. They assigned probabilities of future seismic shocks to each segment on the basis of how much time had passed since that particular locale had generated a major quake. But over the years, successive versions of the report have addressed complexities that have emerged from studies of past earthquakes, laboratory experiments and theory. Models were introduced that allowed fault segments to combine in various ways to create bigger quakes. And researchers recognized the hazard of as-yet unknown faults and assigned probabilities to them. The latest version of the report, published this year 8 , identified several issues that needed to be addressed urgently. \"Does the interactive complexity of a fault system effectively erase or at least significantly reduce any predictability implied by elastic rebound theory?\" asked the report. To this and other troubling questions, the working group acknowledged that the answer may be 'yes'. Field says that the layers of complexity in the most recent forecasts have accumulated to the point that \"in some ways we've built a house of cards. We've identified several things we don't like about the model, things that need to be improved.\" But every time scientists tweak one part of the structure, another part starts to wobble. Now, Field says, \"we're in a situation where we can tear down the whole thing and start from scratch. I think we'll end up with a much cleaner, simpler model rather than a patchwork of modifications\". The 2011 report, known as Uniform California Earthquake Rupture Forecast 3, will put less emphasis on fault segmentation or maybe even eliminate it, acknowledging that an earthquake may start or end anywhere. And although it won't throw elastic rebound out the window, neither will it assume that this model is the dominant factor in shaping patterns of seismic activity over the relatively short timescales \u2014 decades to centuries \u2014 that people care about most. \"We want to include elastic rebound, because there's still a lot of the community that thinks it is very important at the larger magnitudes, at least,\" Field says. However, no one yet knows how the model will be incorporated into a world of constant fault interaction and fewer constraints on fault ruptures, he says. In fact, some say earthquakes are so complex that it may be impossible \u2014 as well as impractical \u2014 to pin them down based on an understanding of their physics alone. That has led researchers to propose purely statistical models to forecast upcoming quake behaviour in much the same way that they forecast aftershocks now. \"There's a lot of push now toward the statistical side,\" says Andrew Michael of the USGS. Geophysicists are putting some of their forecasting theories to the test through an Internet-based project called the Collaboratory for the Study of Earthquake Predictability. \"The idea is to provide an environment where people can do scientific earthquake-prediction experiments that will be evaluated objectively,\" says Jordan, who started the centre three years ago with funding from the W. M. Keck Foundation. The project has testing centres in Southern California, New Zealand, Japan and Switzerland. Thirty-nine forecasting models are being tested at the Southern California Earthquake Center, and more than 40 in other countries. Each will run for five years, with no tweaking allowed along the way. Information from these simulations, along with new data from the San Andreas fault and elsewhere, will be folded into the deliberations of the working group as it seeks to reconcile the competing views of how earthquakes work. Whether this overhaul of the forecasting method will be as radical as Field anticipates remains to be seen. In the end, the working group will use complex logic trees based on the full range of expert opinion to reach its conclusions. David Schwartz, a geologist with the USGS and one of the founders of the characteristic quake model, says that it would be a mistake to toss out the basic elements of elastic rebound or the idea that earthquakes on at least some fault systems come in regular cycles. He does not mind researchers exploring new models outside of the public-forecasting exercise, which has real consequences for state residents. \"That's fine, to see where it leads,\" he says, \"but not to set my earthquake insurance rates.\" Glennda Chui is a science writer based in California. \n                     Nature Geoscience \n                   \n                     Collaboratory for the Study of Earthquake Predictability \n                   \n                     Southern California Earthquake Center \n                   \n                     Uniform California Earthquake Rupture Forecast \n                   Reprints and Permissions"},
{"file_id": "4611189a", "url": "https://www.nature.com/articles/4611189a", "year": 2009, "authors": [{"name": "Michael Bond"}], "parsed_as_year": "2006_or_before", "body": "Can the general public learn to evaluate risks accurately, or do authorities need to steer it towards correct decisions? Michael Bond talks to the two opposing camps. A group of eight-year-olds sits around a classroom table, playing with coloured, plastic boxes called tinker-cubes and linking them into chains. It could be playtime at almost any primary school in the world. But in this classroom, located in Stuttgart, Germany, the 'toys' are actually giving the children their first lesson in probabilistic reasoning. The cubes represent the children's attributes \u2014 red cubes for girls, blue for boys; a yellow cube attached to a red cube for a girl with glasses, a green cube attached to a blue for a boy without glasses. The students end up with a symbolic representation of their classmates as a group. And by collecting the cubes in various bins \u2014 boys versus girls, glasses versus non-glasses and so on \u2014 they begin to get a feel for the probability that, say, a boy will wear glasses or that a girl will not. It is play that is not quite play \u2014 yet the children seem hooked. Eight might seem a little young to be learning a branch of mathematics that many students struggle to master in high school. But the idea behind the exercise \u2014 an experiment devised in 2005 by Elke Kurz-Milcke at the Institute of Mathematics and Computing in Ludwigsburg, Germany, and tested in a number of German schools \u2014 is that earlier is better. Teaching schoolchildren how to deal with frequencies and probabilities helps to prepare them for the complexities and uncertainties of the modern world, and will help them make sound decisions throughout their lives. That's a view strongly endorsed by Gerd Gigerenzer, a psychologist at the Max Planck Institute for Human Development in Berlin and a frequent collaborator with Kurz-Milcke. \"At the beginning of the twenty-first century, nearly everyone living in an industrial society had been taught reading and writing but not how to understand information about risks and uncertainties in our technological world,\" he says. Earlier this year, Gigerenzer set up the Harding Center for Risk Literacy at the Max Planck Institute to try to remedy this situation. Funded for an initial five or six years by a \u20ac1.5-million (US$2.2-million) grant from David Harding, managing director of the London-based investment-banking firm Winton Capital and a teacher of risk communication at the University of Cambridge, UK, Gigerenzer and his team of five scientists have a twofold aim. First is to do basic research on how people perceive risk and second is to improve people's statistical and decision-making skills through education programmes. Indeed, Gigerenzer is an outspoken advocate for the idea that people can be taught to improve their decision-making skills and has taken it upon himself to organize other researchers and set up projects. But this idea is considerably more controversial than it might seem. \"There is a serious division in the research community,\" says Dan Kahan, who studies risk perception at Yale Law School in New Haven, Connecticut. He points out that many specialists in the field conclude from existing research that the public will never really be capable of making the best decision on the basis of the available scientific information. Therefore, he says, \"risk decision-making should be concentrated to an even greater extent in politically insulated expert agencies\". Those agencies, in turn, should guide or 'nudge' people into better decisions by presenting information more appropriately. One thing both sides agree on is that poor decision-making is ubiquitous and has a serious effect on people's well-being. Faced with an unfamiliar or emotion-fraught situation, most people suspend their powers of reasoning and go with an instinctive reaction that will often lead them astray. Witness the widespread fears in the United Kingdom and the United States over the past 10 years over links between autism and the measles-mumps-rubella vaccine. Despite the lack of convincing evidence for such an association, many parents have chosen not to have their children vaccinated, leading to a rise in cases of potentially lethal measles. Likewise, a warning by the UK Committee on Safety of Medicines in 1995 that the third-generation contraceptive pill increased the risk of dangerous blood clots by 100% was followed by an additional 13,000 abortions the next year, many of them in teenage girls. The fact that the increased risk amounted to just an extra 1 in 7,000 was lost on most people \u2014 and, crucially, was not passed on by the media. Exaggerated risk judgements also make themselves felt on environmental issues. Examples include persistent fears over the dangers of genetically modified crops in Europe, despite studies showing that the risks are considerably lower than the scare stories allege, and the hysteria triggered in the United States during the late 1980s by reports \u2014 arguably overblown and still controversial \u2014 that the plant growth regulator daminozide (Alar), used on apples and other fruit, was a potent human carcinogen. \"Exaggerated risk judgements can lead to anxiety that degrades quality of life and causes excessive vigilance and self-protective behaviours,\" warns Ellen Peters of Decision Research, a non-profit group in Eugene, Oregon, that investigates human judgement and decision-making.  \n                Top down \n              Even those who might be expected to know better \u2014 doctors, medical journalists or financial speculators, for example \u2014 often fall into the same traps as everyone else. In one experiment, Gigerenzer asked 160 gynaecologists to interpret some basic statistics about a woman's chances of having breast cancer, given that her mammography screening had come back positive. Just 21% gave the right answer 1 . \"Our ability to de-bias people is quite limited,\" says Richard Thaler, director of the Center for Decision Research at the University of Chicago in Illinois. Thaler teaches a course in decision-making to MBA students in their final quarter at the university's business school. Even though the students should have picked up a lot about statistics and decision-making by this time, when tested at the start of his course they exhibit all the same biases found in other groups, says Thaler. \"After ten weeks of my course they do learn a bit,\" he says, \"but I hardly turn them into rational economic decision-makers.\" The problem, as many researchers in cognitive neuroscience and psychology have concluded, is that people use two main brain systems to make decisions. One is instinctive \u2014 it operates below the level of conscious control and is often driven by emotions. The other is conscious and rational. The first system is automatic, quick and highly effective in situations such as walking along a crowded pavement, which requires the near-instantaneous integration of complex information and the carrying out of well-practised action. The second system is more useful in novel situations such as deciding on a savings plan, which calls for deliberative analysis. Unfortunately, the first system has a way of kicking in even when deliberation would serve best. Consider a well-known example: a bat and a ball cost $1.10 in total, the bat costs a dollar more than the ball, so how much does the ball cost? When Shane Frederick at the Massachusetts Institute of Technology in Cambridge analysed the responses to this question by nearly 3,500 individuals at eight American universities, less than half gave the right answer (5 cents) 2 . Intuition suggests that the answer is 10 cents (it seems to fit and it feels right), and the rational system does little to correct this unless a conscious effort is made to intervene. Such findings are why many researchers think that attempts to improve decision-making through education, which tries to put the rational system in charge of the instinctive one, lie somewhere between over-optimistic and hopeless. Two of the most prominent sceptics are Thaler and Cass Sunstein, a professor at Harvard Law School who heads the White House's Office of Information and Regulatory Affairs. Thaler and Sunstein's 2008 book  Nudge   (Yale University Press) urges governments and institutions to steer people's choices in ways that should improve their lives \u2014 an approach Thaler and Sunstein call \"libertarian paternalism\". Examples include automatically enrolling people into organ-donation schemes and pension plans unless they specifically choose to opt out (rather than the default being that they are not enrolled, then asking them to opt in); dollar-a-day programmes to reduce teenage pregnancies (girls receive a dollar for each day they are not pregnant); and the use of software recognition to delay the transmission of angry e-mails, giving people the option to delete before sending. In general, the idea behind the 'nudge' approach is to shape incentives and present information in a way that increases the chances that people will exercise good judgement. Gigerenzer has no problem with improving the way that the information is presented. He points out that health statistics are often framed in ways that confuse not only patients but doctors, too. His Harding Center is collaborating with health insurers in Germany to persuade authorities to present health information more transparently, and he has convinced a German medical association to rewrite one of its brochures to achieve the same kind of clarification. But Gigerenzer is critical of those who push the nudge approach exclusively and essentially give up on people's ability to learn and reason for themselves. Some people, he says, like to attribute every poor decision to hard-wired mental processes that humans cannot control. He maintains that there is plenty of evidence that people can learn to rewire their minds \u2014 or at least, that they can learn cognitive tricks that help them to recognize and compensate for their biases. Back in the 1980s, for example, Richard Nisbett at the University of Michigan in Ann Arbor and his colleagues found that half an hour's training in statistical reasoning significantly improved people's ability to rationalize everyday problems 3 . That included problems not generally thought of in terms of probabilities, such as whether a group's performance can be predicted from the performance of one or two of its members, or how to infer someone's personality from first impressions. Gigerenzer's optimism about education finds cautious support from Daniel Kahneman, a senior scholar at the Woodrow Wilson School of Public and International Affairs at Princeton University in New Jersey, and a winner of the Nobel prize in economics for his pioneering work in the psychology of decision-making. \"It takes an enormous amount of practice to change your intuition,\" says Kahneman. \"Intuition rules decision-making, that is human nature and that is how it is going to be.\" Nonetheless, he says, people can improve their critical thinking so that they become better at detecting when they might make a mistake. They are then in a better position to prevent or correct it.  \n                Instinctive bias \n              Researchers have found that some of the most effective cognitive tricks include looking at a problem from an outsider's perspective; considering the opposite of whatever decision you are about to make; and weighing up multiple options simultaneously rather than accepting or rejecting each one in turn 4 . Such tricks add up to what Jonathan Baron at the University of Pennsylvania in Philadelphia calls \"actively open-minded thinking\" \u2014 an approach in which people intentionally look beyond the first conclusions that come to mind. He and other researchers have found that some people are much better at this than others. \"It isn't completely clear where these differences come from, but I think this kind of result is optimistic as it suggests these biases \u2014 unlike, say, [interpretation of] visual illusions \u2014 are not an unalterable part of the human condition.\" One clue to the origin of the differences comes from mathematics. Peters has found that when people with low numeracy skills are asked to assess the risks of a potential terrorist action, they are more likely than high-numeracy individuals to over-estimate the likelihood of an attack 5 . In addition, she found that numerate people are better at interpreting data about real-word scenarios, such as the performance and quality of hospitals and health insurance plans 6 . Peters argues that people who use numbers more effectively in decision-making do so because they are better at giving numbers emotional significance and seeing them as representing reality in some way \u2014 what is known as 'affective meaning'. She suggests that it may be no coincidence that people with low numeracy skills tend to have a high body-mass index and tend to be poor at managing their own health. The challenge, says Peters, is to find a way to structure mathematics education so that students grasp the meaning in numbers faster. This is what her colleague Paul Slovic at Decision Research calls \"learning to feel the numbers\". He favours teaching children to deal with numbers in a contextual way as soon as they start to learn to count. For example, teachers should describe the number 10 in terms of something tangible \u2014 say, 10 ice-cream cones \u2014 so that children can remember the number in a way that relates to the real world. Or they could ask children to consider how it makes them feel if someone gives them a penny. What about two pence, three pence? \"Get them to think about their feelings in relation to numbers and whether their feelings are logical or not,\" says Slovic.  \n                Statistical shortfall \n              Gigerenzer's goal is to make such ideas an integral part of education at every level. Much of his educational work is aimed at adults who deal with risk in their professional lives. The Harding Center offers training seminars in decision-making and understanding uncertainties to doctors, journalists and other specialist groups, an activity that has taken Gigerenzer around the world. His past clients include about 1,000 German gynaecologists \u2014 one-tenth of all those practising in the country \u2014 and 40 US federal judges. Of some 200 accredited law schools in the United States, he points out, only one \u2014 George Mason University School of Law in Arlington, Virginia \u2014 regularly teaches statistical thinking. \"So you have an entire society, including judges and doctors, who are not being prepared for a modern technological world containing many kinds of risks,\" he says. Gigerenzer is also trying to persuade education authorities to integrate the latest findings on risk perception into school curricula, starting when children first start school and continuing right through until they leave. He is in regular contact with German education authorities, and is also working with the largest German health insurance company, AOK, to find a way to implement a programme on statistical thinking in schools in the state of Baden-W\u00fcrttemberg. Health insurers are interested, he says, because they realize that the health system does not run effectively, \"partly because patients don't understand the evidence\". The idea is to prepare the next generation so they know what questions to ask. The key, he says, is for schools to teach real-world statistical problems \u2014 for example, calculating the chance that someone who tested positive for HIV actually has the virus, or comparing the dangers of riding a motorcycle in different countries. Primary schools should help pupils get used to probabilistic thinking with programmes such as Kurz-Milcke's tinker-cube exercise. \"Our goal is for statistics to be taught not as a mathematical discipline, but as a problem-solving discipline,\" Gigerenzer says. Gigerenzer has had some success: several German statistics textbooks now use examples from his 2002 book  Reckoning With Risk   (Allen Lane). Furthermore, in many German states it is now compulsory to start teaching data analysis and probabilities from the first year of school. The idea is also catching on in the United States, where the National Council of Teachers of Mathematics has declared its commitment to teaching probabilities up to year 12. Still, says Gigerenzer, there is no nationwide programme in any country that systematically teaches examples in statistics that students can usefully apply to real-life situations. And even in schools that have accepted the need for a comprehensive education in probabilities and risks, there is often resistance from teachers who are wedded to the old system of teaching it. \"In most parts of the world, children are taught the mathematics of certainty, not the mathematics of uncertainty,\" he says. \"Although geometry and trigonometry are beautiful systems, they are of little use in life after school compared with statistical thinking. The twenty-first century is at least as risky and uncertain as those before, and we need to prepare the next generation.\" In the end, both the education approach and the nudge approach are likely to have a role. When it comes to making better judgements, whether it's dealing with complex data or with conflicting emotional states, people \u2014 and societies \u2014 need all the help they can get. \"Societally, we can do more with nudging people along, but individuals and organizations still want to think more clearly,\" says Max Bazerman, who studies decision-making at Harvard Business School. With Sunstein now working within the administration of US President Barack Obama, the nudge approach seems to be gaining political capital; reforming education is proving more of a struggle. The problem, says Gigerenzer, is as much ignorance as resistance to change among educators and policy-makers. \"Often those who don't understand, don't understand that they don't understand.\" But he is convinced it is worth the fight to get the message across. He receives \"a stream of letters\" from mathematics teachers who have used his real-life statistical examples in their lessons and found that their students become much more interested in the subject because it applies to the world they see around them. The long-term benefits for children could be spectacular: a statistical education that they will be able to draw on throughout their lives. The eight-year-olds puzzling over their coloured tinker-cubes in that classroom in Stuttgart should leave school well equipped to deal with the uncertainties of the modern world. Michael Bond is a freelance writer based in London. \n                     Harding Center for Risk Literacy \n                   \n                     Decision Research \n                   \n                     Elke Kurz-Milcke \n                   \n                     Nudge website \n                   \n                     Shane Frederick \n                   \n                     Richard Nisbett \n                   Reprints and Permissions"},
{"file_id": "4611048a", "url": "https://www.nature.com/articles/4611048a", "year": 2009, "authors": [{"name": "Jeff Tollefson"}], "parsed_as_year": "2006_or_before", "body": "If the next climate treaty tackles deforestation, tropical nations will need to monitor the biomass of their forests. One ecologist has worked out a way to do that from the sky, finds Jeff Tollefson. Greg Asner peers out an open window, taking stock of the jungle as the single-engine prop plane chugs over a pair of scarlet macaws gliding among the treetops 120 metres below. The Peruvian Amazon stretches in all directions, painted in countless shades of green, accented here and there by patches of purple, pink and yellow. Occasionally, naked white trunks rise amid the leaves, a reminder that even the rainforest has deciduous tendencies. Forty-five minutes into the flight, Asner spots his quarry: narrow red trails, barely visible, then a fallen tree in the middle of an otherwise intact canopy. The cause isn't immediately clear to the untrained eye, but Asner knows all too well. \"When trees die in the tropics, they don't just fall over,\" he says as the plane passes over more downed trees, a road, then a small clearing that contains stacked logs and a bulldozer. It is a legal concession, authorized by the Peruvian government to extract just three species of hardwood trees. As the plane veers away from the clearing, Asner gives his verdict. \"The biomass levels are going to be a lot lower here,\" he says, \"but it really is low-impact logging compared with the mayhem of Brazil.\" As a tropical ecologist with the Carnegie Institution for Science's global ecology department in Stanford, California, Asner has developed a keen ability to interpret the rainforest from great heights. Frequently operating with oxygen masks at high altitude, his team uses a powerful laser system to map trees and calculate the biomass of the forest. Satellites extend his view across the tropics, and he has developed automated software that can track annual changes in forest cover and calculate the biomass of the vegetation. The system can even spot small logging operations like the one he just passed, which escape detection in most satellite studies. The fully integrated system is designed to measure the amount of carbon locked up in forests and to track changes over time \u2014 an exercise that may become a crucial foundation of the new climate treaty that global leaders are hoping to sign at the United Nations Climate Change Conference in Copenhagen this December. Tropical deforestation accounts for up to 20% of the carbon dioxide emitted by humanity each year and there is broad agreement on the need to include a forest-protection element in the new treaty. This component \u2014 known as REDD, for Reducing Emissions from Deforestation and Forest Degradation \u2014 would allow developed nations to meet their required emissions-reduction targets in part by paying tropical countries to preserve their forests, which keeps carbon in trees and out of the atmosphere. But first, tropical nations must determine how much carbon is in the forest, a notoriously difficult task. A leading researcher on remote sensing in the tropics, Asner is out to prove that developing countries can quickly and cheaply perform their own analyses, then move on to long-term carbon monitoring. He has come to Peru to demonstrate the technology and he will present the results of this proof-of-concept test at a REDD meeting he is planning to coincide with the negotiations in Copenhagen. Success in Peru, he hopes, will bolster efforts to include a strong forest carbon component in the agreement. The European Union last year called for a halving of deforestation by 2020, a goal that has since picked up political momentum. Estimates range widely, but reaching that target could mean pumping some US$20 billion into tropical countries each year, according to the Union of Concerned Scientists in Cambridge, Massachusetts. Venture capitalists see profits in forest carbon and have approached Asner with business offers. Instead he has licensed his technology, dubbed CLASLite, for Carnegie Landsat Analysis System Lite, and is providing it for free to governments and others. Starting in Latin America, his team is training scientists, officials and advocacy groups on how to use the software. He likes to say that he is putting himself and other scientists out of business by injecting a decade's worth of work into the public sphere. All this effort in testing and training has meant devoting less time to academic research, but Asner says that his work on REDD has been nothing short of rejuvenating. \"This is more fun than anything I've ever done as a scientist,\" he says. \"These forests are really special. We need to get these people some cash and protect them.\"  \n                Sweeping the forest \n              For Asner's crew in Peru, work starts early each morning. Today, a three-man team heads to a small airport in the southeastern part of the country before sunrise in an effort to get as much mapping done as possible before the jungle pumps enough water into the air to form midday clouds. Ty Kennedy-Bowdoin runs the Light Detection and Ranging system, or LIDAR, which sweeps a laser back and forth, blasting the forest with 70,000 laser pulses per second. A sensor continuously records the signals as they bounce off leaves, branches and other objects. These data enable the researchers to calculate the height, structure and density of the forest, and they use this information to determine how much biomass it holds. Beside Kennedy-Bowdoin in the plane is James Jacobson, who monitors a hyperspectral imager that takes pictures using frequencies of light that range from the visible into the infrared. The spectral data are used mostly for research on forest biodiversity, although they can also be combined with the LIDAR data to create full-colour three-dimensional images. On this particular flight, Kennedy-Bowdoin eyes a computer monitor to confirm that the LIDAR readings are coming in properly as the plane slowly flies back and forth. But after several passes small clouds move in and block the laser. The plane returns to the airport, allowing the crew to break for lunch before another run in the evening, once the clouds have dissipated. Back on the ground, Asner and a few colleagues head off into the jungle. They drive west along the InterOceanic Highway that crosses South America, then take a small logging road into the rainforest. It's dark by the time they finish a final hike to the Tambopata River and hop on a motor boat for a 45-minute ride to a research station upstream; a spotter scans the river with a flashlight, occasionally directing the boat away from logs. There they meet up with a second team that is organizing ground plots. After waking up to the sound of monkeys in the morning, the team divides into several groups and heads out into the field; they have to finish around 30 plots before moving on to other locations throughout a study area larger than Denmark. At one of the field plots, strings stretch 30 metres out from a pole in multiple directions. In the middle, the researchers are busy running a tape measure around any tree bigger than 10 centimetres in diameter; in some places, they sample down to 5 centimetres. A laser finder gauges the height of each tree. With those data, Asner's team can calculate biomass for every tree in the plot, and those numbers are then plugged into equations to calculate the biomass for the patch of forest. With a smile, Asner explains that this cumbersome process is currently the gold standard for biomass assessment. \"People running tape measures around trees. This is what we've got to get away from,\" he says. Asner does this by taking to the skies. The laser pulses of the airborne LIDAR provide the main biomass estimate for large swathes of forest. The ground plots help his team to interpret and verify the laser readings for different types of vegetation (see  'How to measure a forest' ). Old-growth forest filled with hardwood, for example, contains much more biomass per hectare than do regions dominated by bamboo. The CLASLite system then adds the numbers up according to vegetation type and extent to produce an estimate for the entire forest. Initial results suggest that these remote-sensing techniques are just as accurate as plots, Asner says, but they allow a small team to cover vast territories in a short time. \"Greg has already demonstrated that a tiny group of people can deploy this system over the scale of an entire country; the preliminary activities in Peru make that clear,\" says Chris Field, Asner's boss at Carnegie who is co-chair of the impacts, adaptation and vulnerability working group for the Intergovernmental Panel on Climate Change (IPCC). \"I think it's an incredibly important development in science, and I'm a tremendous fan of his ability to make these things happen.\" Sandra Brown, a leading biomass expert at the non-profit organization Winrock International in Arlington, Virginia, acknowledges that plot-based assessments, although reliable, simply cannot cover as much territory as remote-sensing methods. She also says that published biomass estimates vary wildly for a given region, highlighting the need to establish baselines across the tropics that everybody can agree on. Stopping short of a blanket endorsement, she says that she is encouraged by Asner's initial results and is anxious to see the process in action. \"I think it's got a lot of promise,\" she says. The current project in Peru costs around $430,000, half of which was funded by the Norwegian government through a grant to the environmental group WWF. That equates to roughly 10 cents per hectare, with about half of the money spent on fuel and the plane, but Asner expects the costs to fall over time. Countries could elect to develop their own LIDAR capacity, he says, or hire one of the more than 100 commercial operators around the world. For perspective, Asner estimates that he could map all of the world's tropical forests with his system for roughly $15 million\u2013$20 million.  \n                Map quest \n              Asner owes his interest in forests to a hurricane named Iniki, which swept the Hawaiian islands in 1992. Asner was stationed there with the US Navy at the time, and he wound up surveying forest damage on Kaua'i after taking a job with the Nature Conservancy the following year. Asner recalls being \"enthralled with the beauty, the wildness, the enormous hurricane damage and the onslaught of invasive species\". His first scientific paper focused on the carnage caused by Iniki, but it was his work with invasive species that made him realize he was missing something as he struggled to assess the forest: maps. His quest for spatial data took him to the University of Colorado at Boulder for graduate work in ecology, biogeochemistry and remote sensing, and then all over the world. For Asner, now 41, it is hard to separate research from the rest of his life. His wife, Robin Martin, is a postdoc in his lab and they spend most of their time together in the field. And for the first test of the new biomass system earlier this year, he chose to measure Hawai'i, where he lives part time. In that analysis, Asner's group identified roughly 48 million tonnes of above-ground biomass across the island. That is about 40% lower than the results the researchers obtained from published data and a simple protocol established by the IPCC, which would be the probable starting point for any country that is setting up a carbon-assessment programme. Asner's method has cut the calculated uncertainty in half and he says his system is accurate enough to meet criteria set out by the IPCC for advanced biomass monitoring ( G. P.  Asner Environ. Res. Lett.    4,   034009; 2009 ). Satisfied with his results in Hawaii, Asner set his sights on Peru, where he has worked for several years on estimating biomass and other research. The Carnegie crew began its project in Peru with a CLASLite analysis of NASA Landsat images around Puerto Maldonado, which is tucked into a corner of the southeastern Peruvian Amazon near Bolivia and Brazil (see map). Puerto is a poor frontier town of nearly 30,000 inhabitants, with more descending from the Andes along the newly paved InterOceanic Highway every day in pursuit of riches provided by logging, agriculture and, most recently, gold mining. Rainforest is nowhere to be seen in this dusty Amazonian town, but thousands of motorcycles buzz about, many powering a local variation on the rickshaw taxi. The region around Puerto Maldonado, however, is one of the most biodiverse locations on the planet and a hub of deforestation. This makes it a prime target for Asner, who says that the government has shown it is ready to protect its forests. In 2008, Peru created its first environment ministry. Vanessa Vereau, former vice-minister, says that Peru is working to build up regional governance even as it increases federal enforcement capacity by putting more police on the ground. It is also installing federal prosecutors in rural areas to ensure that everybody \u2014 government officials, companies and individuals \u2014 face real consequences when they break environmental laws. Success is by no means assured, but the government has not even tried to impose such order until now, says Vereau. Peru has also established its own version of a REDD programme by creating monetary incentives for indigenous communities that agree to protect their forests instead of cutting them down. Unlike Brazil, where squatters take advantage of undesignated territories, Peru has designated its land in the Amazon as parks and conservation areas as well as concessions for timber, Brazil nuts and other resources. Along the roads outside Puerto, however, are mostly fallowed fields in place of former rainforest, illustrating how poverty and poor soils combine to create endless pressure to clear land. Although the new federal actions are encouraging, governance remains weak and the fundamental frontier economics have not changed, says George Powell, a wildlife biologist with the WWF who is based in Peru and has become one of Asner's partners on the project. \"The forces against deforestation are getting stronger, but the forces driving deforestation are getting stronger too,\" Powell says one evening after the team returns from the airport to a small hotel that serves as base camp for Asner's team and for local bosses who are directing work on the InterOceanic Highway. Even as the Peruvian government paves the way for bigger trucks to haul more resources out of the jungle, he says, Asner is blazing a trail that could make carbon payments a reality and help tilt the balance in favour of standing forests. \"Most people are still talking about what remote sensing can and can't do,\" says Powell. \"Greg is already down the road putting it in the hands of users. He's three steps ahead.\" Seen from the vantage of a satellite, the deforestation around Puerto extends like fish bones, following roads in long, crossing lines. These are the obvious clear cuts, where forest has been chopped down for farming. Asner's CLASLite analysis also picks up countless blips of small-scale logging far out into the forest. The best Landsat images for analysing vegetation have a 30-metre resolution, which is roughly the size of a large tree crown. This misses small roads and hardwood logging operations that substantially reduce biomass levels deep within the forest and precede widespread deforestation. In 2005, Asner published a paper on the Brazilian Amazon in  Science   illustrating how probabilistic algorithms could be used to sniff out the hidden spectral signals of selective \u2014 and often illegal \u2014 logging ( G. P. Asner  Science    310,   480\u2013482; 2005 ). That study also roughly doubled previous estimates of the amount of forest affected by human activities, while increasing the estimated greenhouse-gas emissions from the Amazon by up to 25% compared with deforestation alone. It also made a name for Asner and helped him to attract financial backing from the MacArthur Foundation, the Gordon and Betty Moore Foundation and other organizations. In total, they have given him more than $11 million to support his biomass and deforestation work, as well as other biodiversity research. Some of that funding has gone into producing and testing a new version of CLASLite, which is designed to make it easy to conduct a biomass assessment and to set up a monitoring programme. Import a publicly available satellite image and CLASLite will correct for atmospheric conditions at the time the image was taken, then analyse the spectrum of each pixel. Vegetation that photosynthesizes has a different spectral signal from dead trees, rocks or soil. A 'Monte Carlo' analysis then produces a range of possible combinations that converge on the most likely explanation for the data. For instance, bare soil is rarely exposed to the sky in a fast-growing rainforest; if CLASLite picks up even a tiny red signal associated with the region's iron-rich soils, and that red colour extends in a line through multiple pixels, the most likely explanation would be a road. In addition to analysing the satellite data, the software automatically pulls in existing vegetation maps and suggests locations for both aerial measurements and ground plots. In other words, the program helps to plan an integrated biomass analysis similar to the one that Asner is conducting around Puerto. \"We were trying to figure out what users want, and then we finally realized they want everything, and they want to be able to hit a button,\" Asner says. \"There's 10 years wrapped up in this little widget. That's super-secret sauce.\" As of this week, Asner has trained more than 240 people in six countries on the software, including several Peruvian government officials. For Peru, the project represents an opportunity to build up its scientific capacity and perhaps even leapfrog Brazil, which currently has the world's most advanced forest-monitoring programme. Deforestation is responsible for a large \u2014 and unknown \u2014 fraction of emissions in Peru, and the country is banking on REDD as a new development aid to buttress conservation efforts while reducing carbon emissions. \"It's a new issue for us, but there is a real political will to protect the forests,\" says Vereau. She sees the current partnership with Asner as a pilot project that could go national \"if it has strong results\". Asner acknowledges the tremendous challenges ahead. He must convince not only the Peruvian government that the system works but also the broader scientific community and ultimately, perhaps, policy-makers who are debating how to restructure the global economy around carbon. But if he has any lingering doubts, they are smothered by his boundless enthusiasm. Flying over the Amazon, Asner can't help but marvel at the sights below. \"Wow, there's another super-giant! That's a 300-year-old tree,\" he says. \"These are just amazing organisms!\" Asner's work could soon have a much broader impact thanks to an agreement he recently reached with  Google.org  on a forest-monitoring application that would be freely available on the web. That partnership, which could be announced as early as next month, could further reduce start-up costs for tropical countries by providing them with processing power and easy access to freely available satellite data from agencies such as NASA and the Brazilian Space Agency. Asner isn't allowed to talk about it, and Google officials won't go into detail except to say that they are building a rainforest-monitoring platform and that Asner's CLASLite software will be part of the package. Dan Nepstad, a tropical ecologist at the Woods Hole Research Center in Falmouth, Massachusetts, predicts that the remote-sensing community is on the verge of a major transformation that will open the doors to high-quality forest monitoring on a global scale. He cites progress on many fronts but says that Asner has shown a particular knack for making his science relevant and useful to policy-makers, particularly those in tropical countries. \"There has to be trust in the forest-monitoring data, and these nations have to see them as their own,\" he says. \"There's this face-to-face collaboration that is really critical.\" Having spent most of his time over the past two years working to deploy his vision of REDD, Asner is now planning to shift into other projects. He is already working on an instrument for his biodiversity and biomass research that combines a more powerful LIDAR with sensors that capture reflected light in 440 frequencies. He is also taking another look at the issue of selective logging. Asner recently completed his first, as yet unpublished, analysis of logging across the tropics and found that small-scale logging operations have a footprint that is 20 times larger than the more obvious wholesale deforestation. Right now everybody is focused on deforestation, but degradation raises a new set of scientific questions, including how much carbon is being lost and how quickly the forests will recover. Asner calls this a \"future frontier science activity\", which means he'll tackle the issue next year. First, he has to finish his work in Peru. Walking along the logging road back from the Tambopata River, Asner comes across a series of trees that have recently been cut down and cast aside. They were old, hollow and unusable, but the loggers only discovered that fact after felling the trees. They could have checked the wood without hurting the trees, says Asner, by inserting a chainsaw directly into their trunks and then pulling it back out. Now the wood is simply rotting away, its carbon needlessly committed to the atmosphere. \"There are millions of trees just like this,\" Asner says, kicking one of the stumps. \"Can you hear them falling? I can. I can hear them in my sleep.\" See Editorial,  \n                     page 1027 \n                   , and online at  \n                     www.nature.com/roadtocopenhagen \n                   . \n                     Road to Copenhagen \n                   \n                     Greg Asner's webpage \n                   \n                     United Nations Climate Change Conference in Copenhagen \n                   \n                     Carnegie Institution for Science's Department of Plant Biology \n                   Reprints and Permissions"},
{"file_id": "462026a", "url": "https://www.nature.com/articles/462026a", "year": 2009, "authors": [{"name": "Anjali Nayar"}], "parsed_as_year": "2006_or_before", "body": "Projects in Madagascar could provide a model for stemming deforestation. But first these efforts must deal with the poverty and political upheaval that threaten forests, reports Anjali Nayar. F\u00e9lix Ratelolahy and his team of field researchers are hiking through the dense growth of the Makira forest in northeastern Madagascar. Uapaca trees tower over them, with their spider-leg roots tall enough to walk under. Brilliant white orchids pour out of their perches in the trees. And every so often the leaves above rustle, as googly-eyed lemurs dance among the branches. Ratelolahy, an ecologist with the Wildlife Conservation Society (WCS), and his team have spent most of the past year in this 5,200-square-kilometre forest to determine how much carbon is stored there. And today, after a three-hour hike up a precipitous slope to their first survey point, the team methodically gets to work setting up circular plots and measuring the diameter of trees. \"We record all the numbers \u2014 the trees, the dead wood and the leaf litter,\" says Ratelolahy, looking up from his clipboard. \"And then back in the capital, poof, the computer calculates the amount of carbon in the forest.\" On these multiple-week traverses through the forest, Ratelolahy has glimpsed much of the region's endemic beauty, such as the leaf-tailed gecko and the all-white silky sifaka \u2014 a type of lemur that is one of the rarest animals in the world. But his missions have also been disturbing, he says. Over the years, Ratelolahy has watched subsistence farmers slash and burn away the margins of the forest to grow rice. And he has come across gangs pillaging the forest for rosewood, ebony and quartz. \"It looks as though bombs have fallen on the place,\" says Ratelolahy about the ransacked areas. Makira is on the front line of the war being waged to slow global warming. As one of Madagascar's largest forests, it stores millions of tonnes of carbon. But as in most forests in the country, that carbon is being rapidly released to the atmosphere as trees are cut down for agriculture, timber, mining and firewood. The WCS, in collaboration with the government and other organizations, is hoping to protect Makira and, at the same time, generate money to support local communities by 'renting' the forest to rich countries. The idea is that wealthy nations could meet their greenhouse-gas emissions targets in part by buying carbon credits from developing countries such as Madagascar. The poorer nations could earn money by keeping their forests standing, rather than cutting them down (see  'A growing market' ). This strategy, known as reducing emissions from deforestation and forest degradation (REDD), is one of the topics up for discussion at the UN climate-change summit in Copenhagen this December. Countries will negotiate whether REDD should be included in the climate deal that takes over from the Kyoto Protocol when it expires in 2012. Proponents for REDD say that this mechanism is key to cutting deforestation, which accounts for around 20% of greenhouse-gas emissions. It is also estimated that REDD could generate billions of dollars each year for forest conservation, far more than is currently spent. Hoping to cash in on the future market, projects have burgeoned around the developing world, with those in Madagascar being some of the earliest to take shape. The projects are also helping to establish technical standards and methodologies for carbon accounting. REDD projects must keep the promised forests standing. To succeed, this means addressing the poverty and political instability in developing countries that often lead to deforestation. These problems are particularly acute in Madagascar, where a coup earlier this year disrupted conservation efforts and raised questions about the future of REDD there. Non-governmental organizations such as the WCS and Conservation International are working through the turmoil. But even they are worried. \"We could have a very difficult time selling carbon if this political situation becomes the norm,\" says Lisa Gaylord, head of the WCS in Antananarivo, the country's capital. \"Why would an investor want to come here?\"  \n                Hungry for land \n              Madagascar is one of the wealthiest countries in terms of biodiversity, but its people are among the world's poorest. Around 85% of the population live below the World Bank's $2-a-day poverty line and most rely heavily on the country's natural resources. The hilly countryside is scarred by slash-and-burn agriculture, locally known as  tavy . Once people fully exploit the fertile river valleys, they head uphill, clearing the forests to cultivate rice, the country's staple food. These rain-fed fields are harvestable for only a few seasons before productivity drops and villagers clear new land for their crops. Estimates of Madagascar's original forested areas vary widely, but some studies suggest that trees once blanketed 90% or more of the island. Since aerial photographs of the country were taken in the 1950s, forests have decreased by more than 40% and by about 2005, they covered only around 15% of the country. Over the past two decades, deforestation has been decreasing slowly with the creation of protected areas. Grants from the World Bank and USAID helped Madagascar to become one of the first countries in Africa to develop and implement a national environmental action plan. By 2000, the nation had protected 17,000 square kilometres of forest, mainly as national parks. But further expansion of the park system stalled because the funding stream from donors dried up. So, in 2001, the government teamed up with non-governmental organizations and started exploring the idea of selling carbon credits from their forests. \"It was clear that there was a carbon market emerging and that avoided deforestation could be a very powerful way of protecting forests in Madagascar,\" says Frank Hawkins, vice-president for Africa and Madagascar of Conservation International in Washington DC. Hawkins was part of the team that popularized REDD on the island. The efficacy of a REDD project depends on how much carbon the project will prevent from being released in the absence of protective measures. Calculating that number requires first measuring the current carbon content of a forest \u2014 as Ratelolahy and his crew are currently doing \u2014 and then projecting future deforestation rates with and without the project in place. The WCS is doing that by using past satellite imagery and making forecasts that account for factors such as the proximity of the forest to roads and villages. A study in 2004, conducted in collaboration with the non-profit organization Winrock International in Little Rock, Arkansas, estimated that the annual rate of deforestation was 0.15% in Makira. The analysis projected that the rate would rise to 0.2% a year by 2034 without any intervention. But with the REDD project in place, the deforestation rate would slow to about 0.07%. These preliminary estimates indicated that the 30-year project would avert the release of more than 9 million tonnes of CO 2  equivalent, similar to taking 2 million cars off the road in the United States for a year. \n               boxed-text \n             Madagascar's other two active REDD projects, in the Ankeniheny-Zahamena and the Fandriana-Vondrozo forest corridors, run by Conservation International, are each projected to prevent emissions of 9 million to 10 million tonnes of CO 2  during the same time period. Together, at a conservative price of US$5 per tonne, the REDD schemes in Madagascar could yield up to $5 million a year for conservation and community development in the country, about the same as the budget for the national park system. Christopher Holmes, the technical director of the WCS's Madagascar programme, says 50% of the money from future carbon sales will go to the communities. The rest will be used to cover the costs of running, monitoring and marketing the project. In the future, the WCS intends to pay the affected communities directly using the carbon money from Makira, but currently there is no distribution mechanism in place. In the meantime, the money will go to health and development projects aimed at reducing poverty. \"People are not walking five kilometres to find forest to cut down to plant rice on a 30\u00b0 slope because that's the best thing to do,\" says Holmes. \"They are doing it because it's the only thing to do.\" But setting up a REDD project in this remote part of the country is challenging, he says. Without much existing infrastructure there, the WCS has had to establish community organizations and legislative bodies that allow each community to manage their forests. In Madagascar, the government owns the forest, so the conservation organizations are helping local communities to gain rights over the natural resources through management contracts. But progress is slow. \"Some of the communities require two days of driving and three days of hiking on foot to reach,\" says Holmes. Since the project started in 2003, only one-quarter of Makira's 83 community organizations have signed the contracts with the government. The contracts give communities legal access to a buffer zone surrounding a core protected area of the forest. When someone in the community wants to build a new house or dugout canoe, that person applies to the community's organization for a permit to use the buffer zone. Because such agreements effectively limit how much local residents can take from the forest, the WCS is trying to help communities in other ways, through projects to increase rice production and by expanding the country's ecotourism industry into the Makira region. But for now, communities such as the remote village of Andaparaty on the eastern cusp of Makira do not see the potential benefits of carbon sales.  \n                Pressures on the forest \n              At sunrise, the riverside village is already bustling. Women are busy laying out a patchwork of woven mats topped with drying rice, beans and vanilla. Later, their meagre produce will be taken downstream to be sold. Barnety, a man nearing 80, watches the ebb and flow of his village. He is a  tangalamena   or traditional leader in the community. When he was a boy, he says, there were only a few families in Andaparaty. Now there are hundreds of people competing for the same land. Bit by bit, the forested slopes around the village have been stripped away to plant rice and cassava. When the Makira project reached Andaparaty in 2004, Barnety supported it. \"Our lives depend on the forests,\" he says. \"If there aren't any forests, if there isn't any land, we can't live.\" But without a sustainable source of income and food, some villagers are finding it difficult to accept restrictions on their access to the forest. \"People are frustrated because before the project, they were completely free to hunt, fish and cut down the forests,\" says Cressant Rakotomanga, president of the local community organization. It could be years before carbon payouts come through a UN-regulated REDD system. The WCS and its donors have already spent $1.9 million to establish the Makira REDD project, and the support going to local communities will increase when the carbon funds arrive. But for the people of Andaparaty, the support can't come soon enough. \"People are wondering where the money from REDD is, or if it will ever come,\" says Ratelolahy. Political unrest in Madagascar makes the future of REDD projects there even less certain. On 17 March this year, following two months of protest, the 35-year-old mayor of Antananarivo, Andry Rajoelina, took the presidency in a military-backed coup. The ensuing political instability caused a surge in illegal wildlife trafficking, mining and logging activities, threatening the country's forests. \"Because of the political instability, people feel liberated, which translates to more exploiting,\" says Haja Salava, the director of Masoala National Park, adjacent to Makira. Armed gangs are ravaging the northeastern region for its valuable rosewood and ebony. The park rangers and community members who have tried to impede the illegal trade have been threatened with death, says Salava. In the past few months, thousands of illegal loggers have been raiding his national park, Salava estimates. In his dilapidated office in the regional capital Maroantsetra, he scrolls through his monthly reports to the government. The pages are a collage of photos of men posing with freshly cut wood. The loggers aren't afraid of being caught because, despite Salava's repeated calls for assistance, the government police do not stop the trade, he says. \"It's a free-for-all.\" Mariot Rakotovao, who led the country's Ministry of the Environment and Forests at the time, said his department had ramped up police patrols in the region and had fined illegal loggers. But the Missouri Botanical Garden, which has been following the illegal timber trade, estimates that more than 850 shipping containers of rosewood have left the country since January, when the political problems began. Almost all of the exported wood was from illegal logging, says Porter Lowry of the Missouri Botanical Garden, which has conservation programmes in Madagascar. The environment ministry did not respond to requests from  Nature   for comment. A government official, who asked not to be named, challenged the claim about such shipments. \"That's a lot of wood. I don't believe it,\" he says.  \n                Wood for sale \n              Other details of the timber trade are more certain. On 21 September, the government issued an inter-ministerial order allowing another wave of timber exports. The new shipments would have a combined value of tens of millions of dollars. According to the ministerial order, the shipments are designed to empty the stockpiles of timber in the country's ports and are limited in number to discourage operators from returning to the forest to cut more precious wood. The order also stipulates that taxes on the exports will raise money for the central treasury and for forest conservation through a new fund called Action against the Degradation of the Environment and Forests (ADEF). Jean Roger Rakotoarijaona, chair of the REDD technical committee that is putting together the country's national strategy, says legitimizing the exports of illegally felled wood will only propagate more cutting in Madagascar. He also questions the purpose of the new forest fund. \"We already have a national forest fund that needs replenishment,\" he says. Regarding the new fund, he says, \"I'm a little worried about what this is going to be used for\". The ongoing illegal felling of trees could lead to a lot of extra carbon emissions. But even worse for Madagascar's REDD projects, the country's political instability has caused international donors to cut funding for forest and development programmes. In March, the World Bank's Forest Carbon Partnership Facility froze a $200,000 grant supporting Madagascar's work preparing a national REDD strategy. That delay imperils Madagascar's chances of winning a follow-up $3.6-million grant to implement the plan, says Rakotoarijaona. The environment and forests ministry also lost about 95% of its funding, which had come from international donors, says Rakotovao. The cuts have severely impeded the ministry's ability to manage and patrol the country's forests, he says. Investments in the country's carbon-credit projects have also stalled. The World Bank's BioCarbon Fund put a hold on its initial payment to buy offsets for 1.5 million tonnes of CO 2  emissions from one of Conservation International's REDD and reforestation projects in eastern Madagascar. The loss of funding for development projects that provide rural communities with alternative livelihoods also means more people are returning to slash-and-burn agriculture. \"The short-term pillaging of forests is a problem, but slash-and-burn agriculture is the worst poverty trap,\" says Hawkins. \"In Madagascar, it leads to the permanent loss of forests \u2014 you can't dig yourself out of that.\" In the past few months, pressure from the international community forced Rajoelina into power-sharing talks with the country's three former governments. The aim is to bring about elections before the end of next year. But even if the political situation stabilizes and investors come back, there is a long road ahead to get REDD working in the towns bordering Makira. The WCS is running workshops to teach villagers how to increase their crop production. But few people have adopted the improvements, says Jean Jaonary, the local community organization president in the village of Ambodivoahangy, on the northeast side of Makira. \"Using the new method, my rice production has doubled,\" he says, while wading through the neat rows of his lime-green fields. \"I don't know why other people haven't caught on.\" As Jaonary walks through Ambodivoahangy's rice fields with Ratelolahy, women and men are busy with work, their straw hats popping in and out of the greenery. Ratelolahy says the land is all the rural populations have, and they are weary of new ways of farming and efforts to keep them out of the forest. He has faith that REDD can help these communities, but it will take time to convince each of the villagers to change. Looking towards the future, Ratelolahy summons up an old Malagasy saying: \"Cows don't all wake up at the same time.\"   Anjali Nayar is an International Development Research Centre fellow at    Nature . \n                 See Editorial,  \n                 \n                     page 11 \n                   \n                 . Watch an audio slideshow at  \n                 \n                     go.nature.com/VGGayN \n                   \n               \n                     Conservation International's Madagascar Programme \n                   \n                     Wildlife Conservation Society's Madagascar Programme \n                   Reprints and Permissions"},
{"file_id": "4611194a", "url": "https://www.nature.com/articles/4611194a", "year": 2009, "authors": [{"name": "Erik Vance"}], "parsed_as_year": "2006_or_before", "body": "Sean Mackey inflicts pain on people in the hope of learning how to relieve it. Erik Vance gets on the receiving end. Outside neurology and his family, Sean Mackey doesn't have many hobbies. The one exception is his monstrous flat-screen television and large film collection. Driving to Stanford, California, on the day I am to visit Mackey's lab for testing, I am reminded of a scene from his favourite movie,  The Princess Bride . In the film, the villain, Count Rugen, straps the hero Westley into a sinister apparatus and confesses a \"deep and abiding interest in pain\". Then he tortures the hero in the name of science. It turns out that this is not far from what is in store for me. Mackey heads the Pain Management Center at Stanford School of Medicine where, as part of his research on ways to relieve pain, he routinely inflicts it. Widely seen as one of the field's rising stars, Mackey is part of a movement to upend the way scientists look at pain, drawing the focus away from the nerves that sense it, towards the brain that processes it. His primary tool is functional magnetic resonance imaging (fMRI), which can create images of the brain responding as the body is hurting. The trick now \u2014 and one focus of Mackey's work \u2014 is to understand whether a person can consciously change the way the brain processes and perceives pain. That's where I come in. The plan is to put me inside the fMRI scanner, apply burning heat, and see whether I can train myself to regulate my pain. As part of his studies, Mackey has found himself struggling with a question facing the fMRI field as a whole: when is the technique ready for use outside the lab? One of his former colleagues has started a company that plans to offer patients the fMRI 'feedback' pain-control technique that Mackey was involved in developing. But Mackey has distanced himself from the company in these early stages, based on what he has observed elsewhere. \"I've seen too many treatments that are the next latest and greatest thing out there that people get really excited about. Everybody gets on board and initially the results are fantastic. And then as time goes by we start to see that the results are not as good as initially proposed,\" he says. \"And then you find out that it doesn't work at all.\" Walking into his office near the Stanford Hospital, Mackey is more reminiscent of a corporate executive than a brain researcher. He is a cheery, focused ball of energy, with a quick smile and a firm, reassuring handshake. He regularly wears a suit in the lab. He says it fights the stereotype that all anaesthesiologists \u2014 he trained as one \u2014 wear \"pyjamas\" to work.  \n                Destructive force \n              According to Mackey, the suit also tells patients that he takes his work seriously. They may be recovering from knee surgery, they may be wounded veterans or they may have a rare neurological condition that can cause excruciating full-body pain. According to the International Association for the Study of Pain, one in five people endures moderate to severe chronic pain. \"I have seen it take people who are otherwise normal and turn their lives upside down and absolutely destroy them,\" says Mackey. Or as his colleague Ian Carroll, another Stanford anaesthesiologist, puts it: \"It's like the black hole of the brain. It dominates it and forces everything to spin around it.\" The experience of pain typically starts in receptors near the skin called nociceptors that transmit information through axon fibres to neurons in the spine, then to the brain. Until the 1990s, pain research focused mostly on nociceptors as well as neurons near the spinal cord. Pain experts would treat a backache, say, directly on the back. If they addressed the brain, it might have been with opioids, whose mechanisms were somewhat mysterious. The arrival of the fMRI scanner changed that. The technique is an indirect measure of neural activity in the brain: as a region activates, it consumes oxygen, and neurologists use fMRI to track fresh oxygenated blood surging in to replace the old. \"Imaging is now a huge part of the field,\" says Allan Basbaum, editor of the journal  Pain . Along with behavioural studies, imaging has helped to build the view that pain involves many brain areas and that chronic pain may cause long-term changes to the morphology or function of some of these regions. As a technique, fMRI has its share of detractors. Interpreting the image relies on complicated analyses that link blood movement to a given task performed by the subject, and many neuroscience studies have come under fire for poor data analyses or interpretation. Although Mackey is a devoted user of fMRI, he is also an occasional critic. He has testified against the use of the technique in several court cases where defendants wanted to use fMRI images to prove they were telling the truth. \"This is an interesting time in the use of this tool,\" says Mackey. \"You are now seeing that the application has gotten easier and easier. It's still not quite like ordering a McDonald's Happy Meal, where you put somebody into the scanner, press a button, and the brain pictures come out, but we are probably going to get there.\" It may seem odd to base so much of one's scientific life on a single tool and then lobby against its application. But Mackey has a different background from many pain researchers. In addition to the MD in anaesthesiology, he has a PhD in electrical and computer engineering from the University of Arizona. He chose to work in neurology \u2014 he calls it an uncharted \"Wild West\" \u2014 because he could treat patients and flex his engineering muscles. This training, colleagues say, gives him a valuable understanding of the biology of pain, the complex instrumentation for measuring it and the limitations of that technology.  \n                Personal limits \n              Mackey thinks that pain could be a useful proving ground for fMRI. Unlike hard-to-define cognitive or emotional states \u2014 say, deception, jealousy or anger \u2014 pain can be elicited in a controlled way at specific levels, is highly repeatable and leads to a common response: it hurts. The intensity of the pain experienced varies from person to person, but can be ranked on a scale. The previous day, I had gone to have my own pain threshold tested. A friendly doctor took me into a small room and strapped to my arm a 'noxious thermal stimulus', more accurately a 'hot metal pad that hurts'. She added a pepper-based cream that made my skin sensitive to heat. Then she slowly worked me up to a pain level that I ranked as seven out of ten. A seven is considered the worst pain a person can tolerate without moving, and everyone will reach their seven at a different level of heat. In the fMRI machine, Mackey planned to use the same burning metal plate to take me directly to seven and leave me there. I would be practising a technique that grew out of a collaboration between researchers at Stanford and Christopher deCharms, a visiting researcher from the Massachusetts Institute of Technology (MIT) in Cambridge. The team showed people a changing image \u2014 a line graph or a picture of a fire \u2014 representing the real-time fMRI signal in their anterior cingulate cortex, a region commonly studied in relation to pain. They showed that people could learn to manipulate the fMRI signal and their perception of pain intensity through visualization exercises, such as 'turning down' pain like a radio dial. The team reported in  Proceedings of the National Academy of Sciences   ( PNAS ) that patients with chronic pain reported on average a 64% reduction in pain on one scale 1 . The study showed that fMRI could be used not only as a diagnostic, but as the means to therapy itself, and that people can exert conscious control over specific brain regions much as it is known that some people can consciously alter their heart rate. In some of his other work, Mackey's laboratory has used fMRI to explore these connections between pain processing and cognitive processes. Fear of pain, for example, can increase the pain itself, and Mackey's group studied some of the brain regions involved in this anticipation 2 . In another study 3  he showed that watching someone else in pain activates brain areas that are fairly distinct from those active during one's own pain. And in unpublished work he has found that romantic love can lessen the experience of pain. Mackey says these connections demonstrate how strong an influence conscious thought may have over pain processing. But can this conscious control be put to use? Inside the coffin-sized tube of the fMRI machine, spasms in my back from its powerful magnet distract from the burning plate strapped again to my arm. On a screen above, I can see a squiggly line that represents the activity in a part of my anterior cingulate cortex. Mackey asks that I envision the heat as alternately searing and soothing. The aim is to master control of the line so that it (and thus my pain) goes up and down. As I switch between these visions the line on the screen twitches up and down. It is surprisingly difficult. Willpower and meditation have little effect, and after two hours it is increasingly hard to make the stubborn little line move at all. The  PNAS   study suggests that it would probably take several sessions before I could be deft enough to reduce other pains in my body.  \n                Commercial approach \n              DeCharms, first author of the  PNAS   paper, started a company called Omneuron in Menlo Park, California, to offer real-time fMRI sessions as a form of therapy. The company has already garnered a fair amount of attention and is funded by the US National Institutes of Health (NIH) in Bethesda, Maryland, to conduct a phase II clinical trial in people with pain conditions such as neuralgia, fibromyalgia or migraines. As a control, some participants will see feedback from a previous participant's brain. Omneuron is also investigating feedback fMRI to help addicts combat their cravings. DeCharms said in a short interview that feedback fMRI may someday be a valuable tool to ease chronic pain. Mackey has little to say directly about Omneuron, beyond cautiously wishing them good fortune. He adds that Stanford currently has no connections with the company's trials. \"There is something different about the balance of goals that a company has versus a nominally disinterested science project,\" says John Gabrieli, deCharms' former supervisor at Stanford and now at MIT. \"I think that is partially why Mackey and deCharms parted ways. I think they couldn't find that balance between them.\" Gary Glover, who directs Stanford's radiological-sciences lab, was a co-author on the  PNAS   paper and is more openly sceptical of attempts to commercialize feedback fMRI. He points out that the  PNAS   study determined that the participants benefited on average. This does not necessarily mean that the technique would work for any particular individual though. And customizing the feedback fMRI is generally more difficult than customizing the dose of a drug, because each person might be helped best by targeting a different brain region with a different mental exercise. Glover also worries that with such a new technique, no one has any idea if it carries side effects: perhaps after a while the pain will worsen, or perhaps the therapy could stop patients feeling useful pain in other situations. \"There are ethical questions when you are in the middle of someone's cognition,\" says Glover. Linda Porter, who oversees the NIH programme funding Omneuron's trial, says the obstacles are surmountable and that, compared with drug trials, the minimal risk of side effects is one of the appealing aspects of feedback fMRI. Mackey is split on the issue of fMRI therapy. As a clinician he thinks that research should be guided by patients' needs and that new technology should be released as soon as possible. As a researcher, he stresses that it is too early for clinical use. Pain therapies are highly subject to placebo effects, which often boost initial results and then whither away during later trials. The therapy is dependent on clinicians' ability to locate the correct part of the brain to show a patient. Much of the research so far has targeted single regions of the brain, yet it is possible that multiple regions are involved. Mackey admits that he doesn't know what is required to convince him fMRI therapy is ready for patients. But he is working to convince himself. His latest study is a feedback-fMRI experiment aimed not at specific brain regions, but at the connections between them. He is also experimenting with new ways to visualize the pain and, of course, ways to apply it. It is easy to see why some are so excited about the possibilities of fMRI therapy. It could allow the vast body of research into brain imaging to have a direct outlet that benefits patients. If it works, it would be the first non-invasive, drug-free method of directly treating specific regions of the brain, with potential applications beyond pain relief, such as addiction and depression. But it is not without its shortcomings. After two hours in the fMRI machine, I am stiff and woozy. It is likely I have more pain than when I went in. Mackey assures me that regulating pain is in no way attached to intelligence, which seems like a polite way of saying I did not do very well. Afterwards, we sit on a shady bench and talk about the potential of the mind to regulate itself. Still a bundle of energy, Mackey enthuses about the future of pain treatment in general, calling current technology \"the dark ages\". He has no doubt that self-directed treatment such as feedback fMRI is the future, perhaps done alongside drug prescriptions and physical therapy. He adds that he is looking for volunteers for his latest round of experiments. I stay quiet, and rub my arm a little. \n                 Erik Vance is a freelancer writer based in Berkeley, California.  \n               \n                     Omneuron \n                   \n                     Sean Mackey \n                   \n                     International Association for the Study of Pain \n                   Reprints and Permissions"},
{"file_id": "462030a", "url": "https://www.nature.com/articles/462030a", "year": 2009, "authors": [{"name": "Emma Marris"}], "parsed_as_year": "2006_or_before", "body": "Unsatisfied with merely halting environmental destruction, some conservationists are trying to reconstruct ecosystems of the past. Emma Marris travels back in time with the rewilders. Fenced off from the modern Dutch countryside is a scene that looks more like a diorama in a natural-history museum than a typical central-European park: a plain is dotted with wild herds of large mammals. Empty of humans and haunted by eagles, it is a vision of a distant past on some of the newest land on Earth \u2014 the reserve was only reclaimed from the sea in 1968. It's called the Oostvaardersplassen, and the visionary behind it is Frans Vera, a tall, greying government scientist from Staatsbosbeheer, the organization responsible for overseeing Dutch nature reserves. Vera designed the 6,000-hectare reserve to replicate Europe's prehistoric past. That has meant 'rewilding' the area, populating it with the kinds of creature that lived there many thousands of years ago. That some of these creatures \u2014 including wild horses called tarpans, and aurochs, wild ancestors of modern cattle \u2014 are extinct, was not enough to stop Vera. In the 1980s, he went shopping for substitutes. He acquired Konik horses, believed to be descendants of wild Tarpans, from Poland. For the aurochs, he substituted Heck cattle, a line developed by two German brothers in the early twentieth century from a number of cattle breeds with the intention of mimicking auroch features. The herds number in the hundreds and graze alongside a red-deer population of about 2,000. Next, Vera would love to get his hands on some European bison and boar. Vera's reasons for developing this eclectic menagerie might not be immediately clear. It isn't a tourist attraction; few people are granted entry, although viewing points for bird watchers dot the periphery. It has, however, succeeded as a conservation area; several bird species rare to Western Europe, such as the white-tailed eagle, have moved in. Vera says that it is also a large science experiment, designed to test his theories about how European landscapes used to look. But he has struggled to keep the reserve open and lacks the funding for graduate students. The project generates very little systematic data or scientific papers, adding to its mystique.  \n                Pleistocene parks \n              Vera's isn't the only rewilding project. Schemes in locales as diverse as New Zealand, Saudi Arabia and the Russian Far East aim to do more than hold the line against further environmental destruction (see  'Lost landscapes' ). They are attempting to recreate the ecological workings of previous eras, often those of the Pleistocene from 2 million to 10,000 years ago. Advocates for Pleistocene rewilding would restore, if they could, the age when nature lived wild and large, when mastodons, sloths and three-tonne wombats heaved their bulk around Earth, and predators were big, fast and ubiquitous. Because many of the fauna in these systems are now extinct, the schemes often turn to proxies to fill abandoned roles: grazing, browsing, defecating and culling the herd. Critics, however, say that the projects are more sentiment than science. Dustin Rubenstein, an ecologist at Columbia University in New York, argues that placing proxy animals in a modern landscape is not the same as turning back the clock, and could spell trouble 1 . These ecosystems have changed and existing species have evolved in the thousands of years since megafauna extinctions. Attempting to fill gaps that closed long ago with proxy animals can generate unpredictable results. \"It is the difference between the known and the unknown,\" he says. Proxy animals, he suggests, could become invasive pests, or escape their parks and cause trouble with local landowners, who would then turn against the conservationists. Moreover, Rubenstein worries about how the projects in Europe and elsewhere are being run and says that information about them is not being disseminated. \"We're not seeing the results in the peer-reviewed literature.\" Without anything being published, he says, rewilders can't dispel worries about the consequences of such proxy introductions. Vera says that the complaint has merit, but that the project doesn't have enough money or personnel to monitor and publish on the wildlife in the reserve. William Sutherland, an ecologist at the University of Cambridge, UK, with a special interest in documenting conservation experiments, says that it's a common, frustrating occurrence. \"It is as if you can only find out about details of a cancer treatment by going round the wards with the doctor,\" he says. Sutherland says that Vera and other rewilders \u2014 along with many other conservation biologists \u2014 are \"too busy conserving to have the time to monitor and publish\". The only way to learn about the Oostvaardersplassen is to go there. Nestled between windmills, small dairy farms, a dyke and the city of Almere, it is, above all, totally unexpected: a Serengeti of sorts with vast, grassy plains where wild horses, cattle and red deer move in massive herds. Two white-tailed eagles perch near their nest, an enormous tree house of bone-white branches. Spoonbills, greylag geese, ibises and many other European bird species converge on the marshes. From the taller grass, a white and orange mist of butterflies explodes in front of Vera's van. And on one edge of the park, an abandoned calf \u2014 its eyes glazed, its knees wobbly \u2014 faces death. Vera watches the calf for a few moments, then reaches for his radio and calls one of the reserve's staff members, who will come and kill it. Although the park is supposed to support nature in its raw form, the Dutch public considers letting animals starve to death too cruel to stomach, so Vera has worked out an agreement with the government that provides swift dispatch to sick and starving animals. Every year, 10\u201320% of the large herbivores in the park die from natural causes or are killed by humans. But their corpses in turn support other animals. Vera was perhaps the only person not surprised to see a Eurasian black vulture, long absent from the area, appear in the park \u2014 although its tenure was short. It strayed outside the fence and was run over by a train. The Heck cattle at the Oostvaardersplassen could, at a pinch, be mistaken for the aurochs painted in famous Palaeolithic caves at Lascaux in southwestern France. Large-bodied and wide-horned, they come in several colours, from black to brown to beige. \"For me, the point was that they not look like dairy cattle,\" says Vera. He wants people to see them as wild animals, and \u2014 apart from the compromise reached on mercy killings \u2014 they live a wild life, without veterinary care, supplemental feeding or even ear tags. They do what comes naturally, and in so doing they are keeping the reserve grazed down and open, turning the earth with their massive hooves and, in their wallows, creating depressions that fill with water and become mini-ecosystems.  \n                Reshaping landscapes \n              Vera hopes that the Oostvaardersplassen will help to answer a couple of scientific questions: can grazers drive ecosystem processes, and will the resultant landscapes be patchy or densely forested? Vera's vision of Europe's past is of a patchy landscape, what he calls a \"park-like landscape\" in which any given area cycles between open grassland maintained by herbivores, thorny shrubs that sneak in where herbivores are caught napping and tree-dominated groves that grow in the shelter of the herbivore-proof shrubs. The sequence of events could explain why oak seedlings don't grow up to be adult oaks in the shade of other trees, even though giant oaks are found in all of the continent's ancient forests. Vera believes that most old oaks grew up in open areas and then saw the forest fill in around them. His is the minority view. Most scientists think a closed forest covered the continent. So far, the Oostvaardersplassen has shown that a high density of grazers can certainly affect the landscape: they have largely mowed it clean. Vera suspects that thorny shrubs will establish themselves and act as nurseries for tree seedlings, even if it doesn't happen in his lifetime.  \n                Rough approximations \n              Vera's experimental set-up does have major limitations. As he and other designers readily admit, every rewilding project will be only an approximation of a past ecosystem. With taxon substitutions and incomplete sets of plant and animal functional groups, many of these systems perhaps evoke the past more than they replicate it. The Oostvaardersplassen, for example, contains none of its lost predators, such as bears or wolves, yet other reintroduction experiments have shown that they can alter the entire ecosystem. When wolves were reintroduced to Yellowstone National Park in Wyoming, for example, the elk soon learned to feed only in areas where they could see wolves if they approached. As a result, areas with bad sight lines were soon thick with willow and cottonwood seedlings 2 . Perhaps this 'ecology of fear' would promote the growth of the thorny shrubs that Vera eagerly awaits. However, weary from battles over the management of the Oostvaardersplassen, he says he is in no hurry to step into the political minefield of wolf reintroduction in a country where livestock owners are a powerful political force. Instead, he says, he will wait for wolves from increasingly popular reintroduction programmes elsewhere in Europe to converge on the Netherlands. Wolves are notoriously good at covering ground, even in population-dense areas such as northern Europe. \"They will come, whether we like it or not,\" he says. Josh Donlan, director of the conservation consultancy Advanced Conservation Strategies in Midway, Utah, and a champion of Pleistocene rewilding projects, says that for some projects the most important motivations are not science- but conservation-oriented. Others, such as a tortoise proxy programme in the Mascarene Islands near Madagascar are \"completely hypothesis driven\", says Donlan, and being carefully documented. These reintroductions offer a huge opportunity to do science, he says, not only to learn about the ecology of the past, but also to feed back into the projects, \"which are going to need all the information they can to be successful\". Donlan and others have proposed that Pleistocene reserves be created in North America, where African and Asian animals could fulfil the roles of the large mammals that went extinct on the continent about 13, 000 years ago. The proposal would have two conservation goals. Lineages of species that have been driven to extinction would have a chance to return to North American ecosystems and start to evolve along their own unique path, and species that would themselves be saved from extinction in their home countries would resuscitate extinct processes in North America. Elephants, for example might eat large fruit such as the osage orange, dispersing their seeds naturally as mastodons and other elephant relatives were thought to do thousands of years ago. This vision of vast areas populated with camels, wild asses and lions has caused a major stir. It has generated strong responses from the public, both for and against, with many dismissing the whole idea as insane. But Donlan says that many people are coming around to the concept. \"I think that it is certainly gaining traction in the public view,\" he says. \"Conservationists are starting to rally around this idea, almost certainly partly because it is proactive.\" That is, it is something that conservationists can do beyond trying to prevent development and negative change \u2014 a strategy that Donlan considers pessimistic and mostly ineffective. Rubenstein has problems with Pleistocene rewilding as it stands, but gives credit to Donlan's desire for change. For Stuart Pimm, a conservation biologist at Duke University in Durham, North Carolina, and one of the few scientists to have set foot in the Oostvaardersplassen, the re-appearance of white-tailed eagles, breeding barnacle geese and at least one black vulture is proof that the reserve is achieving its conservation goals. \"They've got it right and a lot of things have come back as a consequence,\" he says. As for the scientific goals, he urges a broader view. Sure, there are no control areas or replications. But at this scale, how could there be? \"The idea that we have to do neatly replicated experimental design with little squares doesn't capture the whole range of what science does,\" he says. Vera is approaching retirement age now, and looking for a successor to take over the job he's been doing, under various titles, for 30 years. For his retirement, who knows. Perhaps he'll consult on rewilding projects elsewhere. \"I would love to find out what an elephant eats in the temperate regions,\" he says. Click  here  for a slideshow of scenes from the Oostvaardersplassen. Click  here  to hear Emma Marris talk about this story on the this week's  Nature   Podcast. \n                     Josh Donlan \n                   \n                     The IUCN/SSC Re-Introduction Specialist Group (RSG) \n                   \n                     Pleistocene Park \n                   \n                     Mahazat as-Sayd Protected Area \n                   Reprints and Permissions"},
{"file_id": "462154a", "url": "https://www.nature.com/articles/462154a", "year": 2009, "authors": [{"name": "Brendan Maher"}, {"name": "Declan Butler"}], "parsed_as_year": "2006_or_before", "body": "Nature   reports from three laboratories scrutinizing the pandemic flu virus. As the world mobilizes against the H1N1 flu pandemic, researchers are working to answer pressing questions about the virus. Brendan Maher visited pathologists at the US Centers for Disease Control and Prevention who are looking at how the virus kills, and a New York laboratory that is testing how it spreads. Declan Butler spent time at a French biosafety level-4 facility where researchers are working out the chances that the pandemic virus will reassort with the H5N1 avian flu virus. \n               1. How does it kill?  \n             There are five sets of eyepieces on the microscope, and Sherif Zaki is looking down one of them. Looking down the other four are members of the team he leads at the infectious-disease pathology branch at the Centers for Disease Control and Prevention (CDC) in Atlanta, Georgia. Other researchers trained in epidemiology, microbiology and electron microscopy watch a large, flat-screen monitor at the end of the narrow conference room that shows an image projected from the microscope. Dianna Blau, an epidemiologist, reveals the source of the tissue: an 11-year-old girl who died in September, probably from H1N1 influenza. The team had already detected viral RNA in the girl's samples. But observing the presence of the H1N1 virus, especially in this tissue from deep down in the lungs, provides a more accurate and detailed diagnosis. The tissue seems messy, and is flooded with blue-stained  Staphylococcus aureus . Zaki scans back and forth across the slide, seeking the hint of red staining that would indicate the presence of antibodies bound to the H1N1 virus. He finds it on one slide, where a red blob indicates a profusion of virus being released from a rupturing cell. They mark her as positive. Zaki's lab holds these 'sign out' meetings every afternoon, examining autopsy and other tissues that have been sent to the CDC from doctors and medical examiners around the world to find, confirm or rule out any number of diseases. Since April, their case load has doubled thanks to H1N1. So far, the group has received samples from more than 300 suspect cases, and it has confirmed swine flu in more than a third of them. In addition to providing a diagnosis, the group's analyses are helping to build up a picture of how the virus kills. Pathological studies can show what tissues are affected and to what extent, adding detail to the worldwide monitoring and surveillance efforts. The profile emerging is of a distinctive virus. Although seasonal flu tends to infect just the cells high in the upper airway, H1N1 penetrates down into the terminal air sacs called alveoli. \"This is not an area of the lung where you would usually see seasonal flu,\" Zaki says. He has seen such behaviour before, though \u2014 in the few samples of lung tissue he has examined from humans killed by the H5N1 avian flu virus. But the virus is much more prevalent in the tissues from the severe H1N1 cases he has examined \u2014 \"like avian flu on steroids\" as Zaki puts it. Zaki says that his observations fit well with recent research looking at the mechanism of infection. A group led by Mikhail Matrosovich at Philipps University Marburg in Germany and Ten Feizi at Imperial College London studied sialyl glycans, glycoproteins that the flu virus binds to in order to gain entry to human cells 1 . Although seasonal strains of H1N1 bind mostly to versions of the glycoproteins known as \u03b12-6, the researchers found that the new pandemic H1N1 can also bind to a version called \u03b12-3, which is found in greater proportion in the lower respiratory tract. Co-infection is common with pandemic H1N1, at least in those who have died. Zaki's group has observed infection with bacteria such as  S. aureus   or  Streptococcus pneumoniae   in about a third of the fatal swine-flu cases it has examined. In the rest, the virus seems to be lethal on its own. Zaki slots onto the microscope stage a slide from a 38-year-old male who died without bacterial co-infection. This one is filled with the red staining; also the walls of the alveoli are ruptured and blood cells and fluid fill the spaces normally reserved for gas exchange. Scar-like pink ribbons, called hyaline membranes, arc through the tissue. Pathologists call this state 'diffuse alveolar damage', and it tells them that the man had respiratory distress syndrome. \"It's very difficult to treat a patient once they get to this state,\" Zaki says. He points out that the man was obese and had a history of hypertension and heart problems. Some 90% of the cases his team has reviewed have had some underlying medical condition. Zaki says that they plan to publish some of their observations soon, and he has been sharing them with others at the CDC and with the public-health community at large. Although pathology can't predict what the virus will do in the future, it can help to identify those most at risk of severe disease. \"We've really learned a lot with the lab pathology,\" says Anne Schuchat, director of the CDC's National Center for Immunization and Respiratory Diseases. \"Both the prominence of the pneumonia in some infections and bacterial co-infection have important clinical implications,\" she says, explaining that pathology results have led the CDC to recommend pneumococcal vaccination for people in at-risk groups. At the end of the sign-out meeting, Zaki walks back to his office through a quiet lab; the technicians have gone home for the day and the machines used to prepare and stain slides are silent. Six fresh case studies in purple folders sit in a neat row along one of the lab benches ready for processing and examination. They contain slides or tissue embedded in paraffin: one still has lumps of formalin-soaked flesh sitting in a plastic specimen jar. As the flu season ramps up, Zaki says, he suspects the number of flu cases will increase dramatically. Still, his lab sees only a small slice of what's happening worldwide. \"We're looking at the tip of the iceberg in terms of these cases,\" he says. It's a grim job, reading the tales of the dead, but he adds \"that information is important for the living\". \n               Brendan Maher  \n             \n               2. How does it spread?  \n             In a cramped locker room high above the streets of upper Manhattan, laboratory personnel take a visiting journalist into the emerging pathogens facility, an enhanced biosafety level 3 (BSL-3) laboratory on the New York campus of Mount Sinai School of Medicine. The suiting-up procedure is complicated. Rubber-soled booties must be tucked under a jumpsuit to ensure that spilled fluids don't drip into them. Purple latex gloves are layered underneath green ones; the different colours are used to ensure that rips in the outer layer are visible. Black N95 face masks pinch off the nostrils, forcing the wearer to breathe noisily through his or her mouth. The staff call them Darth Vader masks. They also wear papery mesh scrubs and white 'bunny suit' coveralls made from the material used to insulate houses. Biosafety officer Philip Hauck quotes Dante as he prepares to go in: \"Abandon all hope ye who enter here.\" In another preparatory room, they add heavy, belted, battery-powered air filters with a hose that hooks into the back of a white head-covering. Once it's adjusted, air begins rushing up over the face, puffing up the suit. The positive pressure in the suit, combined with the negative air pressure in the lab, ensures that the flow of air will carry any airborne pathogens away from the body. This is the procedure that researchers at Mount Sinai go through every day to carry out their animal studies on deadly pathogens. Since May, pandemic H1N1 has been one of those pathogens. As the pandemic was emerging, John Steel and Anice Lowen secured strains of novel H1N1 from California and the Netherlands, and made plans to include them in their work modelling the transmission of influenza in guinea pigs. BSL-3 facilities are not required to work with H1N1, but Steel and Lowen's institutional review boards wanted the work to be carried out under the most stringent conditions on site. Many labs use ferrets to model the transmission and pathology of human viruses, but the animals can be difficult to work with. Peter Palese, the head of microbiology at Mount Sinai, came up with the idea to work with guinea pigs a few years ago after reading an article published after the 1918 influenza pandemic, in which researchers in New Mexico noted that the infection had killed off a number of laboratory guinea pigs 2 . He convinced Lowen and Steel, assistant professors in his lab, to test whether the creatures would transmit human influenza. They did. In 2006, the group showed that the guinea pigs pass the virus between them with about the same efficiency as humans 3 , and the researchers still use them as a model of transmission even though their animals, purchased from a lab supplier, do not show symptoms or die from the disease. The first thing the researchers did with the samples of pandemic H1N1 was to compare its transmissibility with that of seasonal flu. They squirted about 10,000 infectious H1N1 particles up the noses of four guinea pigs and, a day later, placed these animals in cages next to uninfected animals, separated by wire-mesh walls that allow respiratory droplets to pass through. Within five days all the exposed animals tested positive for the virus. The team's paper, published last month in the  Journal of Virology , suggests that the H1N1 swine flu virus transmits just as efficiently as seasonal flu 4 . This is contrary to some work on transmission done at the start of the pandemic 5  but mirrors real-world data showing that H1N1 spreads rapidly. \"The pandemic sort of scooped us,\" says Lowen. Palese's group also found that previous exposure to the H1N1 and H3N2 subtypes of seasonal influenza limits the ability of exposed animals to become infected with the pandemic H1N1, which supports the idea that a seasonal flu infection or vaccine might offer some cross-protection against swine flu. Inside the facility are the huge steel ferret cages that Steel and Lowen have adapted for their guinea-pig work so that two animals can fit inside each. They are now planning new experiments, hoping to find the sequences within the genome that enable the virus to transmit so well between humans compared with other swine-adapted influenza strains. They may create reassortments of the virus and then test how well they are transmitted by the guinea pigs. The results might aid in surveillance efforts aimed at identifying other strains from animals that could make the leap to humans. Everything that goes into the BSL-3 lab must be decontaminated before coming out. This involves spraying down the bodysuit with ethanol, carefully peeling it off and then leaving the facility by way of a shower room. A pen and notepad won't survive the caustic chemical dunk by the exit, so the pages are scanned and put onto a CD that can be sprayed down. Outside again on the Manhattan streets, H1N1 could be encountered in any cough or sneeze. Without all the protective gear, one feels rather vulnerable. \n               Brendan Maher  \n             \n               3. What could it turn into?  \n             A deadly line-up of viruses is locked up in the computer-controlled safes at the Jean M\u00e9rieux/INSERM biosecurity level four (BSL-4) facility in Lyon, France, including Ebola, Nipah, Lassa, Hendra and Marburg. And in the next few weeks, scientists working there are planning to manufacture a new resident. They hope to test whether the highly transmissible pandemic H1N1 virus could reassort with its deadlier cousin, the H5N1 avian flu, to make a virus with the worst properties of both. Classed as a national high-security facility, the laboratory is a three-storey shoebox made from armoured glass, perched above a biology research centre of INSERM, the national biomedical agency. The edifice is built to withstand earthquakes, bullets and explosives. It is also smack in the city centre, its entrance just metres from mothers pushing prams along the pavement on avenue Tony Garnier. The lab itself is surrounded by empty corridors and a spartan decor of steel staircases, trusses, raised walkways and ventilation pipes. The air hums from the air-ultrafiltration and other support machinery that take up entire floors above and below. Under negative air pressure to stop viruses escaping in the event of a breach, the lab is split into three zones, with the airstream flowing towards the 'hottest' exposure risk zone, the animal house at the back. When the current H1N1 pandemic began in April, priority lab time here was allocated to Bruno Lina, a virologist and flu researcher at the CNRS, France's basic-research agency, who works at the University of Claude Bernard Lyon-1. Pandemic H1N1 influenza is not itself a BSL-4 agent \u2014 a BSL-2 facility is adequate. But in France, the health ministry classes viral reassortment experiments of the sort Lina is performing as requiring BSL-4 precautions. The ability to predict which reassortments might take place, and what type of flu viruses might result, could be key to predicting the behaviour of this and future pandemics. The eight genes in the influenza virus's segmented genome are easily swapped between strains. But to be viable, new gene combinations must also be able to work together to package themselves into a virus particle. Viruses within a subtype tend to reassort with one another more easily and generate more viable reassortants than do those from different subtypes. Over the summer, Lina's team has been using the BSL-4 facility to investigate the likelihood that pandemic H1N1 will acquire resistance to the front-line antiviral drug oseltamivir (Tamiflu) through reassortment, and how easily these reassortants might spread. Resistance can emerge by spontaneous mutation, but given that seasonal H1N1 is already resistant to the drug and spreads easily, reassortment is perhaps the most likely way that pandemic H1N1 will acquire resistance \u2014 especially as seasonal H1NI and pandemic H1N1 are the same subtype. Since the start of the pandemic, Tamiflu-resistant strains have sporadically appeared in several countries but none has yet gained a foothold. That they haven't arisen more often or spread more easily may be because there is little seasonal H1N1 circulating, as pandemic H1N1 is outcompeting it \u2014 a large number of co-infections are needed for transmissible reassortants to arise. In his work, Lina co-cultured the two H1N1 viruses in a cell line. He is now testing how pathogenic the reassortants are in mice, before using ferrets to test their capacity to spread. The biggest hazard that the scientists face in the lab is a bite or a scratch from an infected animal, so all manipulations are done on animals that have been anaesthetized remotely. Lina is also preparing to submit a protocol to the facility's scientific board seeking the green light to try to reassort pandemic H1N1 and the H5N1 avian flu virus. \"It's controversial research, but it is basic science that needs to be done,\" says Lina. H5N1 has killed more than half of the people it has infected since it resurged in 2003, but has rarely spread from one person to another. H1N1, on the other hand, seems to be as transmissible as seasonal flu, but mild in most people, if severe in some. The aim of Lina's proposal is to find out the probability of a reassortant arising that combines the lethality of H5N1 with the transmissibility of H1N1. In particular, Lina is searching for putative molecular controls of the virus's segmented genome that may determine why some reassortants can package the virus and others can't. \"We don't know which regions in H5N1 are responsible for that control,\" says Lina. That could have benefits for disease surveillance, he says. If researchers know the key genetic regions that facilitate reassortment, surveillance efforts could watch out for H5N1 or H1N1 viruses with changes in those regions, ones that might be on the verge of dangerous reassortments. Lina will use reverse genetics to generate a soup of reassortants, test whether any are viable, and if they are, assess their virulence and transmissibility. Because H1N1 and H5N1 are different subtypes, Lina does not expect them to swap genes easily. In 2005, he tried to reassort H5N1 with seasonal H1N1 and H3N2 viruses, without success. \"After a year we only had three reassortants, and none was fit,\" recalls Lina, \"they just don't reassort well.\" The experiments that Lina's team is carrying out in the BSL-4 lab aren't technically novel or difficult in themselves. But the encumbrance of the safety procedures for getting in and out of the high-security lab make even the most straightforward procedures complicated, says Lina. Looking through the bulletproof window, one can see why. As the researchers move around the lab, they connect and disconnect their blindingly white spacesuits from some 60 yellow air hoses dangling from the ceiling. The hands of the researchers snipping away at tissue samples are wrapped in multiple layers of gloves to avoid accidental pricks. \"It's like trying to do surgery wearing boxing gloves,\" says Herv\u00e9 Raoul, director of the facility. Before researchers can even begin to work in the lab, they must pass a three-week training course. It's only after a further 200 hours of practice that they are given any real autonomy in the lab, and they are never allowed inside alone. Users are limited to working a maximum of one four-hour shift a day, so that they are less likely to make mistakes. They are also encouraged to share any concerns about the behaviour of colleagues, or whether they themselves are going through a difficult patch, which might cause distraction and accidents. Before Lina's experiments get approved, the facility's external scientific board will need to be convinced of the public-health justification, that the science is top-notch and that the experiments can be done safely, says Raoul. It will also need to be approved by government regulatory agencies. If all goes well, he could have his authorization in weeks, Lina says. \"Compared to the big guns in flu research, I'm a little guy,\" says Lina. \"But having access on my doorstep to one of the rare BSL-4 facilities in the world with an animal house is a big advantage.\" \n               Declan Butler  \n               \n                     Swine flu special \n                   \n                     Nature Education: Genetics of the Influenza Virus \n                   \n                     Working with dangerous bugs \n                   \n                     Lyon BSL-4 lab \n                   \n                     CDC Swine Flu Update \n                   \n                     Mount Sinai School of Medicine Biosafety Documentation \n                   Reprints and Permissions"},
{"file_id": "462266a", "url": "https://www.nature.com/articles/462266a", "year": 2009, "authors": [{"name": "Gene Russo"}], "parsed_as_year": "2006_or_before", "body": "While species losses mount worldwide, conservationists in Brazil have made great strides towards saving the golden lion tamarin and its forest habitat from destruction. Gene Russo reports. At a farm less than two hours from the sprawl of Rio de Janeiro, a small group of ecotourists strolls to a patch of forest to see one of the rare victories in the fight to preserve Earth's dwindling biodiversity. Walking among trees near the town of Silva Jardim, the visitors are greeted with high-pitched squeals emanating from the canopy. A few seconds later, they spot tufts of brilliant orange fur flying from branch to branch as three families of golden lion tamarin ( Leontopithecus rosalia ) compete for territory in an area that should really accommodate only one. The overcrowding of these small, twitchy primates in this forest fragment demonstrates both the successes and remaining challenges surrounding efforts to preserve the golden lion tamarin and its native habitat, the Atlantic forest of Brazil. Known as Mata Atl\u00e2ntica, these scattered fragments of forest dot the eastern coast of Brazil and contain one of the richest assortments of endemic species in the world, many of them endangered. And like the golden lion tamarin, the Atlantic forest almost disappeared because of development and an exploding human population. But both the flagship primate and its habitat have been saved, at least temporarily. \"When I come to the Atlantic forest, I think 'Thank God',\" says Russ Mittermeier, president of Conservation International. Mittermeier, who travels the world visiting places in various states of ecological distress, lauds the area's conservationists, researchers and a frequently supportive public. \"It's as good as it gets,\" he says. By many measures, that's still not very good. The scraps of Atlantic forest occupy only about 10% of the area covered when Europeans arrived in 1500 (see  map ). And just 9% of those remaining bits are protected. But the rate of deforestation has slowed in the past decade and in some places conservation efforts have even helped the forest to rebound. Beyond the hectares saved, the resurgence of the Atlantic forest shows how successful conservation can emerge through a combination of forces. The lessons learned here may resonate beyond Brazil, as nations seek to reverse the rapid loss of the planet's species. \"The Atlantic forest shows that it's not a hopeless cause in these high-priority areas,\" says Mittermeier. For this biome, it was the plight of the golden lion tamarin that helped motivate a conservation movement. As its habitat vanished, the population of these primates dropped to roughly 150 individuals in the 1970s, which caused conservationists and researchers to take aggressive action to save the species (see ' Bred to survive '). But it soon became clear that the golden lion tamarin could not be preserved on its own. Jim Dietz, a conservation biologist at the University of Maryland in College Park, remembers flying over the Atlantic forest in a helicopter in 1985 while there to investigate the tamarins. He was stunned by what he saw \u2014 a speckled landscape of forest fragments interspersed among pastures. The sight turned Dietz from a golden lion tamarin researcher to a forest conservationist. \"I was down there to study mating systems,\" he says. \"But I hadn't understood the grave nature of the threat.\" Stemming the deforestation required a broad set of measures: new laws and governmental incentives, the commitment of researchers and conservationists, increased funding from international donors and the Brazilian government, and a growing community awareness. Lately, a boost has come from efforts to emphasize the forest's value as a source of water, a draw for ecotourism and a generator of other ecosystem services. International pressure has also helped. Through the Convention on Biological Diversity, countries have committed to slow the rate of biodiversity loss and to protect 10% of their ecoregions by 2010. Although few nations will meet these goals, Brazil has set aside 16% of its land. Most of this is in the Amazon, but the biodiversity treaty has put pressure on Brazilian authorities to establish state parks in the Atlantic forest southwest of S\u00e3o Paulo, says Oliver Hillel, an officer in the convention's secretariat in Montreal, Canada.  \n                In the balance \n              Preservation efforts have to fight against a long history of forest destruction in the Mata Atl\u00e2ntica. Shortly after landing in the region in 1500, Portuguese settlers began cutting down trees. By 1797, they had cleared so much land that Queen Maria the Pious of Portugal called for measures to stop the forest's destruction. In recent decades, sugarcane cultivation, logging and ranching have shattered the forest into fragments. Oil exploration on the coast and massive development \u2014 70% of Brazil's 200 million people live within the Atlantic forest biome \u2014 have shrunk it further. \n               Click here for larger image \n               Estimates vary of how much forest survives. Measurements that count only fragments larger than 100 hectares indicate that just 7\u20138% of the forest is left. Calculations that include smaller fragments and more mountainous areas suggest that 11\u201316% remains. In either case, the forest is badly broken up. Roughly 83% of the fragments are less than 50 hectares in size ( M. C. Ribeiro  et al .  Biol. Conserv.    142,   1141\u20131153; 2009  ). The smaller the fragment, the harder it is to sustain existing ecosystems, especially those that include animals with large ranges, such as tamarin, jaguar, puma and many birds. And yet, despite the devastation, these splintered ecosystems remain a hotbed of life. The forest, which stretches from the eastern tip of Uruguay northeast through Brazil's population centres to the tiny state of Rio Grande Do Norte, contains a far more varied range of elevations and climates than the vast Amazon basin. Its diverse environments support an unusually high number of species, many of them existing only in this biome. The Atlantic forest has an estimated 20,000 plant species, 8,000 of which are believed to be endemic. And 940 of its 2,155 vertebrate species are thought to be endemic. Roughly 190 animal species and 300 plants in the forest are considered threatened. Based on this density of life alone, there is a lot at stake. The Brazilian government took a significant step to preserve its forests in 1965, when it revised the country's forest code. The law required landowners to preserve areas around rivers and forests on steep slopes. For the Atlantic forest, it dictated that 20% of any rural property must be maintained as a reserve. More legislation in the 1980s and 1990s sought to conserve forest resources and ecosystems. But it was not until 2006 that Brazil passed a law specifically to protect the Atlantic forest, by demarcating its extent and its ecosystems and by requiring special permission for activities that damage them. The law prohibits the removal of vegetation in areas housing endangered species. It also provides for watershed protection, erosion control and the formation of corridors between forest remnants. The long struggle to win even the promise of such safeguards was led in part by Marina Silva, a senator who had previously served as environment minister. She and others had battled an array of forces including the timber industry and landowners. Despite the environmental victories in the legislature, it has been difficult to win protection on the ground. \"If the forestry code had been enforced and complied with, we would probably have 30% of the Atlantic forest left,\" says Lucio Bede, manager of Conservation International's Atlantic forest programme. Although the code specified punishment, it provided no real incentives for compliance. And enforcement has been lax, says Bede. As for the Atlantic Forest Law, Silva says that too few municipalities are engaged in monitoring and recovery efforts. \"The application of the law is not uniform throughout all Atlantic forest states,\" she says. The state of Santa Catarina, for example, has attempted to loosen the federal law with its own environmental code. But even with such problems, the recent law has helped raise awareness and slow deforestation, says Silva. Notions of pristine environments, peaceful forest canopies and noble animals hold scant interest for poor people seeking jobs and income. So conservationists have recently tried a different tactic, stressing what intact forests can do for people living everywhere from rural towns to the megacities of S\u00e3o Paulo and Rio de Janeiro. These population centres rely on the forests to provide clean water, and deforestation threatens the watershed serving millions. This approach relies on advertising the ecosystem services provided by the forest, a tactic adopted by conservation proponents and researchers in many parts of the world over the past decade. In Brazil, it is starting to yield results. \"Municipalities are just beginning to understand conservation and environmental services,\" says Denise Mar\u00e7al Rambaldi, secretary-general of the Golden Lion Tamarin Association and a key figure in the project to save the species. \"Decisions made in a municipality are much more important for land use and planning than those in the federal government.\"  \n                A river runs through it \n              In the city of Petr\u00f3polis, 70 kilometres outside of Rio de Janeiro, a small river runs near the summer residence of Pedro II, emperor of Brazil in the nineteenth century. The Piabanha river has become a focus of efforts to preserve the forest in and around the city. In 1992, Petr\u00f3polis and its surrounds became the first federally decreed environmental protection area. Now, engineers and local government officials are trying to conserve and enhance the vegetation along the river, in the hope that the Piabanha can serve as a thread that sews together forest fragments on the edges of the city. The challenges are many. In spots, particularly at high altitudes, forest fires have eliminated native plants and allowed invasive grasses to take hold. Roads, condominium developments and shanty towns known as favelas impinge on the watershed, threatening water quality and encouraging erosion. According to Yara Valverde, a biologist and former head of the Petr\u00f3polis protection area, irresponsible development could lead to floods that would taint water sources. But the city imposes fines for environmental crimes. And guided by citizens and federal and local governments, the preservation efforts have allowed landowners to protect former grazing lands, allowing for some natural regeneration of the forest. The progress, albeit incremental, in Petr\u00f3polis demonstrates the potential of emphasizing what the forest provides to the environment and community. \"We hope to incorporate conservation planning into watershed planning and create an economy based on ecosystem services,\" says Jos\u00e9 Maria Cardoso da Silva, Conservation International's vice-president for South America. Currently, there are legal mechanisms to pay landowners to protect forests along a watershed; the crucial next step is to develop overall governance structures for each watershed across the Atlantic forest, says Cardoso da Silva. Government-sanctioned watershed committees have started to form that include local government, businesses and conservation organizations. By charging for water use, some committees have generated money for conservation, reforestation and sanitation management. The focus on ecosystem services, says Bede, makes clear how protecting the forest helps preserve the water supply. Valuing ecosystem services, however, does not necessarily protect the forest's flora and fauna, because it shifts the emphasis away from saving species. Mittermeier endorses continued ecosystem-services projects but he warns: \"If we focus on the ecosystem-service argument only, and don't focus on critical endangered species, then we could end up with good forest and good ecosystem services but no species.\" Although different approaches could sometimes come into conflict, an emphasis on ecosystem services may actually dovetail well with protecting species, according to an unpublished study. Frank Wugt Larsen, a postdoc at Conservation International's Center for Applied Biodiversity Science in Arlington, Virginia, assessed 524 sites around the world in terms of the species they harbour and the services they provide, such as storing carbon in vegetation and providing clean water. He found that areas identified as the last-known habitat for a given species provide more ecosystem services than nearby 'control' areas. In Brazil, the case of the golden lion tamarin shows how multiple conservation interests can converge to raise public awareness and move landowners to action. For more than 40 years, Marcos da Silva Freire and his family have owned 350 hectares, outside Rio near Po\u00e7o das Antas, the first federal biological reserve in Brazil. Freire, an immunologist, works in Rio like many middle-class landowners, but he maintains the land and visits when he can. In the late 1980s, researchers contacted him and his father about introducing two groups of tamarins, bred in zoos, onto their land. They agreed, and theirs was the first of several farms to offer the animals indefinite accommodation. But interspecies altruism is not the only factor that motivates Freire. He enjoys the tamarins \u2014 and he has considered establishing his own ecotourism venture to bring in additional revenue. Other conservation efforts could help sustain poor farmers and, at the same time, enlist them to protect the forest. On a hot, clear day in August, Adeildo Ataliba proudly shows visitors his plot of land near the Po\u00e7o das Antas reserve, one of the main homes of the golden lion tamarins. Walking through tall trees and shrubs, he points to plants he has recently sown, including seedlings of coffee, yucca, guava and jatoba. Nearby is a large banana tree and a single stalk of sugarcane. Just a few years ago, this land was a field of ankle-high scrub. As part of an agrarian resettlement effort, Ataliba and his family were one of several landless families who received a plot from the government. But many of these families struggled because they lacked agricultural training and an environment conducive for traditional cash crops. A few of the farmers even resorted to hunting in the nearby reserve. In an effort to aid the farmers and protect the reserve, the Golden Lion Tamarin Association \u2014 whose mission has expanded beyond tamarins since it was founded in 1992 \u2014 established 'agroforestry systems' of native tree species planted together with vegetables and fruits. This helps restore forest corridors for wild animals and yields produce for the families. The association helps out farmers by providing seedlings and technical assistance. Conservation groups have even more ambitious aims for the future. A coalition of non-governmental organizations, research institutions and private companies hopes to double the amount of Atlantic forest by 2050. Some of the money for this effort, expected to cost tens of billions of dollars, could come from payments to store forest carbon, an ecosystem service that could become much more valuable under a new climate treaty. As the various stakeholders in Brazil explore different approaches to preserving the Atlantic forest, the golden lion tamarins continue to multiply in their newfound territory. Currently, their population in the wild has swelled to around 1,500 on more than 10,000 hectares of protected land. But in many places, pastures and roads prevent the growing families from expanding into new territory. Dietz is seeking funding to join up enough isolated tamarin populations via forest corridors to enable gene flow throughout the entire population. \"It would be self-sustaining in perpetuity,\" he says. Rambaldi warns that the tamarins and their habitat still face significant threats. \"We have ten years to ensure the legal protection and the sustainability of these fragments,\" she says. Otherwise, urban pressures and oil exploration threaten to claim land. And the fragmented forest, says Rambaldi, will become just an empty collection of trees. See Editorial,  \n                     page 251 \n                   , and  \n                     http://www.nature.com/darwin \n                   . Gene Russo is editor of  Naturejobs . \n                     Road to Copenhagen \n                   \n                     Darwin 200 \n                   \n                     Conservation International, Atlantic forest hotspot \n                   \n                     Golden Lion Tamarin Association \n                   \n                     Pact for the restoration of the Atlantic forest \n                   \n                     Smithsonian National Zoo, Golden Lion Tamarin Association \n                   \n                     SOS Mata Atlantica \n                   Reprints and Permissions"},
{"file_id": "462272a", "url": "https://www.nature.com/articles/462272a", "year": 2009, "authors": [{"name": "Nick Lane"}], "parsed_as_year": "2006_or_before", "body": "Genetic sequences in a cell's mitochondria can be used to accurately determine species. Could this be because they are responsible for creating what they identify? Nick Lane investigates. Mitochondria, the cell's energy producers, keep a low profile in terms of their genome. Descended from free-living bacteria that took up residence within other cells some 2 billion years ago, they've maintained a modest genetic repertoire \u2014 a mere 37 genes in vertebrates, compared with more than 20,000 in a nucleus. Yet within this little genome, researchers have pinpointed a 648-nucleotide stretch as the ultimate identifier of species, dubbed the DNA bar code. The sequence can distinguish between closely related species such as humans and chimps and even classify new species from identical-looking ones, such as the blue-flasher butterfly ( Astraptes fulgerator ), which has since been divided into ten separate species, verified by the habitats, lifestyles and diets of their caterpillars. The DNA bar code has been both praised and attacked for its simplicity. Many assume that it misses taxonomic subtleties that can be revealed only through traditional systematics or more extensive sequencing. However, proponents take the criticisms in their stride. \"The fact is these short sequences yield surprisingly accurate information about the composition of the entire genome,\" says Donal Hickey, an evolutionary biologist at Concordia University in Montreal, Canada. A part of a mitochondrial gene was chosen simply because it worked better than other sequences, and researchers assumed that the sequences became unique after two species split from their common ancestor. But what if DNA bar codes work for a deeper reason? Molecular biologist Dan Mishmar and his colleagues at the Ben-Gurion University of the Negev in Beer-Sheva, Israel, have been collecting evidence to support a new hypothesis: that mitochondrial sequences \u2014 such as those that give rise to each species's unique bar code \u2014 might actually be powerful drivers in the process of speciation 1 . Rather than simply going along for the ride, they say, they could be responsible for undermining the reproductive compatibility within a species when they conflict with sequences in the nucleus. Although it is largely \u2014 some would say almost entirely \u2014 speculative, some evidence supports the idea, including data from a project called the Barcode of Life Initiative, says founder Paul Hebert from the University of Guelph in Canada. The initiative is a collection of research projects, organizations and individuals devoted to developing DNA bar-coding as a global standard for identifying species. So far, it has described the bar codes of almost 65,000 species. If true, Mishmar's hypothesis could connect the origins of species with biodiversity in a satisfying way. \"Reproductive isolation may arise through a complex genomic ballet \u2014 a  pas de deux   between the mitochondrial and nuclear genomes,\" proposes Hebert.  \n                Mitochondria matter \n              Cell survival depends on respiration, which takes place in the mitochondria through a series of large protein complexes, each built from as many as 43 subunits. These subunits are encoded by genes that reside in both the mitochondrial and the nuclear genome, and they must interact intimately with each other or respiration doesn't work. Take the enzyme cytochrome oxidase, for example, which handles the final step of cell respiration. In mammals, the complex is composed of 13 subunits, three of which \u2014 including subunit 1, the bar-code gene \u2014 are encoded by mitochondrial DNA, and ten by nuclear genes. If the subunits of cytochrome oxidase don't work together properly, electrons are not passed to oxygen and respiration fails, triggering the death of the cell. Making this cooperation even trickier is that fact that the two genomes evolve in different ways. Nuclear genes are mixed by sexual reproduction, every generation different alleles are introduced, whereas the mitochondria divide in a simple asexual fashion. Moreover, the mitochondrial gene sequences generally change much faster from generation to generation than the nuclear ones \u2014 typically 10\u201330 times faster. Given the penalty for failure, it is hardly surprising that the two genomes have adapted to work together. Every generation the mitochondrial genes are tested against the new nuclear background of the offspring. If they don't work, there can be a developmental failure or a serious reduction in fitness after birth, referred to as hybrid breakdown. The outcome is that selection acts to ensure that the two genomes function properly together; and there is plenty of evidence showing that, despite their different modes of evolution, changes in one genome bring about a strong selection for compensatory changes in the other 2 . In 2006, Mishmar found evidence for this co-adaptation while working with Doug Wallace, a geneticist at the University of California, Irvine. They found that the primate nuclear genes that encoded mitochondrial proteins had a similar number of changes to the mitochondrial genes and that these genes evolved ten times faster than other genes in the nucleus 3 . In other words, the mitochondrial and nuclear genes adapt to each other within a population, and the process must happen quickly because the mutation rate is so high in mitochondrial DNA. Mishmar started thinking about the implications of this. If one set of genes with a critical effect on survival evolves ten times faster than other genes, two populations of the same species could quickly diverge, possibly even driving a wedge between them reproductively.  \n                Where's the evidence? \n              Perhaps the best evidence for reproductive incompatibility comes from marine biologist Ron Burton and his colleagues at the Scripps Institution of Oceanography in La Jolla, California. They have shown that incompatibilities between mitochondrial and nuclear genes can seriously undermine the fitness and fertility of the intertidal copepod  Tigriopus californicus 4  \u2014 a crustacean whose small size and abundance often earns it the name 'insect of the sea'. Burton cross-bred individuals from nearby populations of  T. californicus   that don't usually interbreed so that the mitochondria from one population ended up paired with the nuclear genes of the other \u2014 a process known as introgression. The mismatch suppressed reproduction and cellular respiration of offspring by as much as 40%. Low respiration was also linked to slow juvenile development and poor survival \u2014 altogether, a serious reduction in fitness. If, in effect, all these hybrid offspring are runts with low fertility, then they are less likely to survive and reproduce than the offspring of pairings within the same population. Over time, what started as partial reproductive incompatibility ends up as a total failure for the two populations to produce viable offspring when crossbreeding \u2014 which is to say, speciation. Burton, however, is careful not to over-claim. And although others praise his work, they too caution against over-interpretation. \"Just because a genetic interaction causes hybrid problems doesn't mean it was responsible for speciation in the first place,\" says Jerry Coyne, an evolutionary biologist at the University of Chicago in Illinois. At least 200 genetic interactions are known to cause inviability when  Drosophila simulans   is crossed with  Drosophila melanogaster . But when the common ancestor originally split into these two species, only one of them was probably the driver, because any one alone causes inviability. Other differences would only have emerged after the species had diverged. Pinning down exactly which of these 200 interactions is responsible for speciation is an arduous task, and Coyne has seen no hint that mitochondrial interactions have a role. Far from it \u2014 he believes that the existing evidence refutes the postulated role of mitochondria in speciation. \"Closely related species living together often have identical or very similar mitochondrial DNA, even when their nuclear DNA is more diverged,\" Coyne says. Mitochondrial genes seem to flow between closely related species more easily than nuclear genes do, for unknown reasons \u2014 although this does not happen frequently enough to totally derail bar codes. David Rand, a molecular evolutionist at Brown University in Providence, Rhode Island, has come across similar problems. With Brown's Colin Meiklejohn and Kristi Montooth, now at Indiana University in Bloomington, Rand has transplanted mitochondrial DNA from different species of fruitfly into the nuclear background of  D. melanogaster , and found little evidence of the hybrid breakdown that would support the idea of mitochondria controlling speciation. \"I'm sure there are cases in which mitochondrial\u2013nuclear interactions have had roles in speciation\" says Rand, \"but how common is it? Few people have dissected this carefully enough yet.\" As one of the few who has approached the topic, Burton is not surprised that examples are scarce as yet. The degree of hybrid breakdown, he says, depends on how fast the mitochondrial DNA mutates \u2014 and that is under tight genetic control, and varies from species to species. Fruitflies and other heavily studied model systems don't have particularly high rates of mutation, he says. \"That makes them the last place to look for evidence.\" The best places to look, according to Burton, are taxa that have high mitochondrial DNA mutation rates, which range widely from rodents to Galapagos tortoises, from snails to copepods \u2014 even yeast. Populations from these taxa are much more likely to break down if crossed with other populations of the same species, as a first step towards reproductive isolation and speciation \u2014 as happens in Burton's copepods, and in yeast 5 . But why do some organisms have a much faster mutation rate than others? The mitochondrial DNA of birds, for example, changes at a quarter of the speed it does in mammals 6 . So what is the benefit of a fast mutation rate; and what, if anything, does it say about speciation? Wallace says the answer is simple: rapid mitochondrial mutation is adaptive. \"Reproductive success requires adapting to different food sources \u2014 carbohydrates, proteins or fats \u2014 and climates from icy cold to intense heat or humidity. A lot of this adaptation goes on in the mitochondria,\" he says. A fast mutation rate can quickly produce variants that are suited to the changing environmental conditions. The only drawback is that a high rate of change should also lead to lots of negative mutations, and ultimately a meltdown for some populations. It's hard, on the face of it, to see how that could be adaptive. Last year, however, Wallace's group came up with an answer to this conundrum 7 . In mice, severe mitochondrial mutations are eliminated in the germ line \u2014 eggs with mitochondrial mutations fail to develop, meaning that the eggs that do develop are more likely to be healthy. The fast mutation rate generates variation coupled to a developmental filter that gets rid of the most detrimental mutations before they have the chance to undermine the health of an animal's offspring. This means that a high mutation rate in mitochondrial DNA can be adaptive. It is beneficial and can be selected for. But equally, it affects speciation. A fast mutation rate means that the genes controlling respiration change quickly over generations. And that, in turn, increases the chance of mismatch between mitochondrial and nuclear gene sequences if and when individuals outbreed with other populations. This scenario paints a radically new picture of mitochondrial genes as being tightly regulated by selection. Until recently, most had thought of them as little more than 'neutral markers'. Evolutionary biologist Nicolas Galtier and his colleagues at the University of Montpellier 2 in France have been chipping away at this neutral view, and have found that in most species, the variation in mitochondrial DNA is surprisingly restricted 8 . If mitochondrial DNA really is a neutral marker, mutations should build up quickly over time, giving plenty of variation. But if mitochondrial DNA is subject to periodic bouts of selection then much of this variation would get purged. That's what Galtier sees in the data. Each such 'selective sweep' wipes out the common ground between species, and leaves little variation within a species \u2014 a nifty trick if you're looking for a bar code.  \n                From bar codes to species \n              By definition, a DNA bar code is a unique identifier. Between any two humans, it varies at no more than two positions 2 . By comparison, humans differ from chimpanzees at approximately 60 sites, and from gorillas at about 70. Much the same is true of the other 65,000 or so species that have had their bar codes sequenced. Just as Galtier has found in mitochondrial DNA in general, DNA bar codes vary remarkably little within a species, but have little or no overlap between species. Given the characteristic mitochondrial combination of rapid mutation and limited variation, only two processes can generate such a pattern: natural selection, or genetic drift after a population 'bottleneck'. The human genome seems to be a result of the latter. As mitochondria are inherited only through egg cells, human mitochondrial DNA can be traced back to a shared common female ancestor, thought to have lived in Africa 170,000 years ago, and named Mitochondrial Eve. But Mitochondrial Eve did not live alone, so why has all humanity inherited her mitochondrial DNA? The standard answer invokes a bottleneck that reduced the number of humans down to a few thousand individuals, whose descendants took over the world. From a limited repertoire of mitochondrial DNA, one type happened to become fixed through genetic drift \u2014 no selection necessary. Like Galtier's findings, data from the Barcode of Life Initiative raise doubts about this interpretation. All species show the same lack of bar-code diversity. Although it is easy to imagine that humans passed through a bottleneck 170,000 years ago, it's hard to believe that exactly the same thing happened in all species. \"Did herrings really pass through an equally recent population bottleneck? Anchovies too?\" asks Hebert. In his view, the only explanation is heavy selection across whole populations. That makes a lot of sense in the context of Wallace's ideas on mitochondrial adaptation to climate or food. Mitochondrial genes mutate rapidly, generating variation that is subject to selection whenever environmental conditions change. That's suggestive of a selective sweep. And as Mishmar has been noticing, mitochondrial sequences place a premium on compatible nuclear genes, forcing fast changes on them. If individuals from nearby populations are then mated, the outcome is hybrid breakdown \u2014 the serious loss of fitness chronicled in Burton's crossed copepods. The rate at which any of this happens depends on the mitochondrial mutation rate \u2014 fast rates leads to fast divergence and a greater likelihood that mitochondria will have a role in speciation. In this view, the DNA bar code does not merely track species \u2014 it could very well create them. Is this really a powerful driver of speciation? It's still too early to say. Mishmar is about to embark on a series of studies on cellular respiratory function in mismatched populations, and he challenges others to follow. If bar codes are fundamental, then the best place to look will be in closely related species with distinct bar codes. Mismatches in nuclear and mitochondrial genes between such populations should lead to hybrid breakdown that can be rescued simply by backcrossing. There is already a database of 65,000 species out there \u2014 and everything to play for. See Editorial,  \n                     page 251, \n                    and the whole biodiversity special at  \n                     www.nature.com/darwin \n                   . Nick Lane is the first Provost's Venture Research Fellow at University College London and author of  Life Ascending: The Ten Great Inventions of Evolution . \n                     Darwin 200 \n                   \n                     More Darwin Resources from NPG \n                   \n                     Barcode of Life Initiative \n                   \n                     Dan Mishmar \n                   \n                     Jerry Coyne \n                   Reprints and Permissions"},
{"file_id": "462270a", "url": "https://www.nature.com/articles/462270a", "year": 2009, "authors": [{"name": "Emma Marris"}], "parsed_as_year": "2006_or_before", "body": "Gretchen Daily knows the value of ecosystems \u2014 but can ascribing financial worth to them help to maintain biodiversity? Emma Marris meets an ecosystem-services evangelist. When Gretchen Daily was young, she watched acid rain slowly killing the forests around her in Germany's Taunus mountains. As a researcher, she cut her teeth studying extinctions under Paul Ehrlich, an ecologist famed for his predictions of mass starvation. Last month, Daily travelled to Japan in advance of next year's meeting of the Convention on Biodiversity in Nagoya, where the world will hear how spectacularly that treaty has failed to protect the planet's species. All this makes it remarkable that Daily is, eternally, sunny. Daily's enthusiasm bubbles forth in meeting rooms around the globe as she promotes the 'ecosystem-services' approach to conservation, of which she has become the world's most passionate proponent. Her argument \u2014 and the argument of a group of like-minded researchers \u2014 is that undeveloped nature provides services to human society such as clean water and flood protection that can be valued in financial terms that are large enough to justify protecting it (see ' Ready to serve '). She also believes that this protection can be achieved by installing sufficient financial incentives to make owners want to preserve this bounty. In 1997, Daily edited an influential book that made a first coherent case for saving the planet with cash ( Nature's Services: Societal Dependence on Natural Ecosystems ). That same year, a much-discussed paper estimated the total worth of 17 of Earth's major ecosystem services at US$33 trillion a year ( R. Costanza  et al. Nature    387,   253\u2013260; 1997 ). The resultant buzz propelled the idea into the Millennium Ecosystem Assessment report of 2005, which used ecosystem services as a framework to discuss the state of the planet and how to preserve it. Daily, now working at Stanford University in Palo Alto, California, is gearing up for a major publicity push for the concept in 2010, the International Year of Biodiversity. But this increased attention could also highlight the flaws of the ecosystems-services approach, one of which is its uncertain ability to protect biodiversity: in some cases a biodiverse ecosystem does not necessarily provide services that are more financially valuable. Not that these arguments will stop Daily. \"Gretchen is going a zillion miles an hour and she's got this crusade, if you will,\" says Steve Polasky, an environmental economist at the University of Minnesota, St Paul. \"You often get these crusaders and it is all about them \u2014 but with Gretchen it is really about getting ecosystem services on the agenda.\" Economists have been working on attaching monetary value to components of natural systems since at least the 1960s, evaluating the cost of damage caused by oil spills, for example. But environmental activists and conservationists didn't pay this work much attention. Many felt that nature should be saved not for it price, but for its own sake. Daily's conversion happened gradually. Born in the United States, she spent her adolescence in Germany in the midst of early 1980s environmental protests. \"It was amazing to see the demonstrations out in the street protesting acid rain and everything connected to it,\" she says. The experience convinced her of the value of using science and activism to tackle environmental problems. She did both, working at the Worldwatch Institute, an environmental think tank in Washington DC, as an undergraduate in the mid 1980s and then applying for her graduate studies to work with Ehrlich at Stanford. At a field station in Gothic, Colorado, in the early 1990s, Daily mixed with Ehrlich's influential friends. These included Peter Bing, a rich businessman and then chairman of the board at Stanford, who \"knocked some sense into me\" on day-long hikes, says Daily, encouraging her to talk with business people in their language \u2014 economics \u2014 rather than see them as the enemy. She also met Tim Wirth, who was then one of Colorado's senators and an early advocate of cap-and-trade approaches to combating pollution. Daily became convinced that such incentive schemes were the way to save the environment. She won financial support from various foundations to prepare, edit and publish  Nature's Services   and she has hardly looked back since. \"There has been tremendous behind-the-scenes progress,\" says Daily. The concept has been widely embraced by policy-makers. Ecosystem-services projects are now so thick on the ground that one needs a dictionary to keep track of all the acronyms. Among those that Daily salts her conversation with are TEEB (the Economics of Ecosystems and Biodiversity), a European study on how much money the continent might be losing through ecosystem loss, and IPBES, the Intergovernmental Platform on Biodiversity and Ecosystem Services, a proposed scientific advice generator modelled on the Intergovernmental Panel on Climate Change (IPCC).  \n                Seeing solutions \n              When Daily is not spreading the word on ecosystem services to scientists, policy-makers and broader audiences, she is pursuing her own studies in places such as Costa Rica and Hawaii. In Hawaii, Daily brought together parties who had long fought over land use \u2014 ranchers, native Hawaiians and water and power companies \u2014 and persuaded them to write a report, now under review, to the state legislature that recommended reforesting areas of ranchland. Daily got them all to sign up to the same recommendations in part by focusing on their common concerns about land being converted to be used for high-end homes. Under the new proposal, the ranchers would be paid for reforesting, the native Hawaiians would have access to the forest and the trees would retain rainwater and keep salination of the drinking water supply at bay. \"It was really stunningly easy to get people together in dialogue,\" she says. Long-time collaborator Peter Kareiva, chief scientist of the Nature Conservancy in Seattle, Washington, says of Daily that \"people around her are energized. She's built relationships. She just has that personality. She sees a solution.\" Daily focuses much of her energy on the Natural Capital Project, a joint effort she brokered between Stanford, the conservation group WWF and the Nature Conservancy. The project, which she co-directs, is developing a software system to help people weigh up the value of land in terms of ecosystem services, alongside its value for building houses or other development. Maps of an area are layered with information \u2014 much of which can be displayed in dollars \u2014 such as which parts of the landscape are best for filtering water, where the real-estate is most valuable, where the most carbon can be stored and where the biodiversity is highest. Such maps could help governmental organizations evaluate, for example, the cost of building on land that provides free water filtration if the development would then require the construction of a costly water plant. \"We can bring about this transformation if we supply tools that make it easy for decision-makers to compare alternative scenarios,\" she says. Over the past few years, the maps have been used by officials in China's upper Yangtze River basin to help plan urban and agricultural expansion and dam construction. \"I don't earn that much money, but I am laying down my life for all this,\" Daily says. \"I work day and night. I am basically a fanatic.\" The idea of ecosystem services has its critics. John Echeverria, an environmental lawyer at Vermont Law School in South Royalton, says that paying landowners not to damage the environment sets up an expectation of reward for refraining from bad behaviour, and a financial obligation for future taxpayers. \"The implicit message of agreeing to pay is that they should be entitled to proceed to destroy nature,\" says Echeverria. Instead, he suggests, landowners should in general be expected to do the right thing and be punished when they don't \u2014 the model enforced by the US Endangered Species Act and equivalent legislation in other countries. Daily contends that the Endangered Species Act and similar laws are failures because their restrictions and penalties have angered many landowners. They also create an incentive for landowners to remove any endangered species from their land before the authorities find out about them. This approach has \"led to these past decades of lose\u2013lose battles on the environment\", she says. Richard Carson, an economist at the University of California, San Diego, is a fan of Daily's work, but he says that her pitch tends to focus on the easy cases. \"If there is a problem, it is that she has created the impression in people that if you just think about these things in the right way, everyone is going to come out ahead.\" Daily agrees that she is going after the win\u2013win situations, and says it is because there are still so many easy gains to be made. But eventually, she knows, there will be some tough decisions. If a fish species is close to extinction, it may be necessary to completely close the fishery for some years to ensure that the service (provision of fish) is maintained in the future; and it is difficult to make that decision a 'win' for fishermen.  \n                Rationale for destruction? \n              There is another fundamental limitation to the ecosystem-services framework: some services provided by an ecosystem are simply not considered valuable enough to warrant protecting. Biodiversity is particularly problematic. In some cases, a monotonous plain of non-native grass delivers better and cheaper ecosystem services, measured in water filtration, carbon sequestration and flood protection, than a diverse marsh. Attaching explicit values to things can provide a rational basis for ignoring them. Nevertheless, Daily says, the ecosystem-services approach can save many places with high biodiversity \u2014 and at the very least it will give certain ecosystems time until society shows more willingness to protect them for other reasons. \"I think it is going to be a long haul for biodiversity for its own sake. For me, ecosystem services is a strategy to buy time as well as getting buy-in.\" Such sentiment reveals that the ecosystem-services approach is not necessarily that different from conventional environmentalism. Advocates of both viewpoints believe that nature is intrinsically valuable, and they hope to preserve nature by appealing to this belief in others or, where it is absent, by creating it. The difference is that Daily works to convince others by showing them the profitable side of nature first. Peering through the blur of her hectic work life \u2014 the conferences, authoring, media interviews and research \u2014 it is clear that Daily isn't just a sunny personality. She is a true optimist. She believes that people can and will save the planet's biodiversity \u2014 not just because there is something in it for them, but because, eventually, they will care. See Editorial,  \n                     page 251 \n                   , Opinion,  \n                     page 277 \n                   , and the biodiversity special at  \n                     http://www.nature.com/darwin \n                   . \n                     Gretchen Daily \n                   \n                     Natural Capital Project \n                   Reprints and Permissions"},
{"file_id": "462408a", "url": "https://www.nature.com/articles/462408a", "year": 2009, "authors": [{"name": "Lucas Laursen"}], "parsed_as_year": "2006_or_before", "body": "An intuitive approach to computer modelling could reveal paths to discovery, finds Lucas Laursen. Grabbing one of the three laptops in her office at Microsoft Research in Cambridge, UK, Jasmin Fisher flips open the lid and starts to describe how she and her collaborators used an approach from computer science to make a discovery in molecular biology. Fisher glances across her desk to where her collaborator, Nir Piterman of Imperial College London, is watching restlessly. \"I know you could do this faster,\" she says to Piterman, who is also her husband. \"But you are a computer scientist and I am a biologist and we must be patient.\" After a few moments, patience is rewarded: Fisher pulls up a screen of what looks like programming code. Pointing to a sequence of lines highlighted in red, she explains that it is a warning generated by software originally developed for finding flaws in microchip circuitry. In 2007, she, Piterman and their colleagues found a similar alert in a simulation they had devised for signalling pathways in the nematode worm  Caenorhabditis elegans . Using that as a clue, they predicted and then experimentally verified the existence of a mutation that disrupts normal cell growth 1 . 'Executable biology', as Fisher calls what she's demonstrating, is an emerging approach to biological modelling that, its proponents say, could make simulations of cells and their components easier for researchers to build, understand and verify experimentally. The screen full of code doesn't look especially intuitive to a non-programmer. But Fisher toggles to another window that shows the same  C. elegans   simulation expressed graphically. It now looks much more like the schematic diagrams of cell\u2013cell interactions and cellular pathways that biologists often sketch on white boards, in notebooks or even on cocktail napkins. One big goal of executable biology is to make model-building as easy as sketching. Fisher explains that each piece of biological knowledge pictured on the screen, such as the fact that the binding of one protein complex to another is necessary to activate a certain signal, corresponds to a programming statement on the first screen. Likewise, the diagram as a whole \u2014 illustrating, say, a regulatory pathway \u2014 corresponds to a sequence of statements that collectively function as a computer simulation. Ultimately, she says, this kind of software should develop to a point at which researchers can draw a hypothetical pathway or interaction on the screen in exactly the way they're already used to doing, and have the computer automatically convert their drawing into a working simulation. The results of that simulation would then show the researchers whether or not their hypothesis corresponds to actual cell behaviour, and perhaps \u2014 as happened in the 2007 work \u2014 make predictions that suggest fruitful new experiments. In the meantime, however, Fisher and her fellow executable-biology enthusiasts have a lot of convincing to do, says Stephen Oliver, a biologist at the University of Cambridge, UK. \"Modelling in general is regarded sceptically by many biologists,\" he points out.  \n                Born-again modeller \n              Fisher's fascination with this type of modelling started in about 2000. She was studying for her PhD in neuroimmunology at the Weizmann Institute of Science in Rehovot, Israel, when she encountered David Harel, a computer scientist who was applying computational ideas to biology. Harel wanted to get around the problems encountered in conventional simulations, which use reaction-rate equations and other tools of theoretical chemistry to describe, step by step, how reaction networks and cell interactions change over time. Such simulations can provide biologists with a gratifying level of detail for testing against reality. But the number of differential equations in these models escalates rapidly as more reactions are included, until they become a strain on even the most powerful computers. In one recent model of the networks involving epidermal growth factor, for example, 499 equations were required to describe 828 possible reactions 2 . Even if the computers can handle such a load, the output is often difficult to interpret. Such models quickly become \"an impossibly unwieldy black box\", says Vincent Danos, a computational biologist at the University of Edinburgh, UK. And if the models have such a hard time simulating the behaviour of a single set of signalling pathways, he adds, then it's hard to imagine they will ever be of much use in systems biology, which might, for example, seek to understand all the pathways in a cell as an integrated whole. Harel's approach was to represent networks of biological events by a considerably smaller set of logical statements. For example, instead of specifying the number of signal molecules involved in a particular cell\u2013cell interaction, or the sensitivity of the various receptors, a statement might simply say 'when cell X is near cell Y for long enough, cell Y switches from one type of behaviour to another'. And, unlike the conventional equations, the rules tend to be independent of one another \u2014 an important part of why the simulations are so much easier to build. An additional advantage of the logic-based approach was that standard model-checking algorithms \u2014 widely used by industry for testing computer hardware \u2014 could check whether the statements were logically consistent, and capable of producing the behaviour seen in cells. This analysis would highlight points in the model at which the behaviour was going awry, which in turn might suggest experiments to look for previously unsuspected reactions and molecular species at that point (see graphic). Fisher became so caught up in the idea that in 2003 she joined Harel's lab as a postdoc. She continued to work in the field during a three-year postdoc appointment under Thomas Henzinger at the computer-science department of the Swiss Federal Institute in Lausanne (EPFL). Piterman, whom she had married in 1998, came to the EPFL as well, and the three of them collaborated with their colleague Alex Hajnal to build the  C. elegans   model. They started by recording all the rules they could find in the literature pertaining to the maturation of a simple, well-studied system of six vulval precursor cells. \"I wrote it all down first in a diagram,\" says Fisher, pointing to a figure in a research article on her desk, \"then we formalized all the arrows and feedback loops into the computer program.\" Because the model needed only rules, not numbers, most of the information was qualitative (for example, this cell is closest to the cell sending the signal so the messenger molecules reach it first).  \n                Lab confirmation \n              The team knew that genetic mutations could nudge the cells into different roles during maturation, but they wanted to know more about the cascade of signals that dictate the fate of each cell. The model-checker explored the set of 48 mutations known to affect vulval development, which could have up to 92,000 possible outcomes. All but four of the perturbations predicted normal cell fates, so the team concentrated on simulating different timings of those four cases. They found two previously unknown effects. First, a set of inhibitory genes collectively known as  lst   genes have to be activated for vulval cells to convert to their 'primary' fate, meaning that their daughter cells will make up the vulval opening. Second, if another gene was disrupted and signals between the cells weren't timed just in just the right sequence, the cell would adopt a different fate. A laboratory experiment confirmed both predictions. \"We used this qualitative model because we simply didn't have the quantitative knowledge,\" says Fisher. But now that the approach and its predictions have been verified in the lab, she says, \"you can't argue with it\". Since then, Fisher has become one of the world's most energetic proponents of executable biology 3 , but she is far from being the only enthusiast. In 2007, for example, biologist John Heath of the University of Birmingham, UK, was trying to model signal transduction pathways and protein\u2013protein interactions. \"The processes are just really just too complicated to understand using intuition,\" he says. He discussed his problem with University of Oxford computer scientist Marta Kwiatkowska, who was then working in the adjacent building at Birmingham, and she gave him a paper on model-checking. \"I was reading the opening paragraph on the train and I thought, 'This is exactly what I want',\" says Heath. In collaboration with Corrado Priami, who leads the Centre for Computational and Systems Biology at the University of Trento in Italy, Heath was soon modelling the gp130/JAK/STAT signalling pathway 4 , a well-studied system involved in human fertility, neuronal repair and embryonic stem-cell renewal. Their model reproduced the dynamic behaviour of the pathway as observed in the laboratory, and has allowed them to make testable predictions about which parts of the pathway are most sensitive to mutation or other perturbation. Heath, like Fisher, is now actively promoting executable biology, and has joined with Kwiatowska to publish a review paper on the approach 5 .  \n                Another level \n              Executable biology does have limitations, Fisher acknowledges. At present, for example, such models can handle only one level of narrowly defined biological activity at a time \u2014 the level of protein\u2013protein interaction, say, or the level of cell\u2013cell interaction. \"We know there is feedback between the levels,\" Fisher says, \"but we don't know enough about it\" to get a computer to simulate that feedback. An additional complication is that the different levels are best handled by different computer languages. To model the molecules that travel between cells, for instance, the most natural languages are those known in computer science as 'process calculi', which were devised to model information flow through communication webs. But to model the behaviour of an individual cell and its components, as in the various signalling and regulatory pathways, the most natural languages are those based on the theory of interacting 'state machines', which was developed to describe how objects transition from one state to another. The long-term goal, says Fisher, is to develop more sophisticated and complete simulations that would help researchers explore a wider range of biological phenomena, both by integrating behaviour at the genetic, molecular and cellular levels, and by integrating executable models with more mathematical models. Indeed, as a group of bioengineers led by C. Anthony Hunt of the University of California, San Francisco, pointed out in a response 6  to Fisher and Henzinger's 2007 review, it's not an either\u2013or choice between the executable biology and conventional mathematical modelling: both have their uses and limitations, depending on the level of biological activity being simulated. Fully integrated modelling is still a long way off, admits Fisher. But now that executable-biology predictions have been verified in the lab, the field has begun to attract more attention. Labs worldwide are starting to use executable biology to study systems, and Fisher herself is giving invited lectures on the subject 15\u201318 times per year around the world. Meanwhile, she and Piterman are trying to make the software more accessible to biologists, so that researchers can make executable-biology simulations a routine part of their work. Other research groups are working towards the same end. Priami's group is trying to write interfaces so simple that biologists can fill in tables with their data, specify the rules they want to use in spatially organized diagrams and sit back while the program translates the data into a computer-readable language that can execute a simulation 7 . \"We develop languages that allow people to program without knowing they are programming,\" says Priami.  \n                Commercial efforts \n             In another effort to make the executable-biology approach more intuitive, Walter Fontana of the Harvard Medical School in Boston, Massachusetts, has joined with colleagues at the start-up firm Plectix to launch Cellucidate, an online visual interface for biological-pathway modelling that generates statements in an executable computer language called Kappa, which Fontana developed explicitly to model molecular interactions. Cellucidate \u2014 available for free during its trial period \u2014 allows collaborators to add information to a shared online model and revise it Wikipedia-style, something Fontana says is increasingly important because the empirical facts on which models are based are continually being revised. Fisher hopes that the excitement will catch on in more groups and suggests that some of the computer-inspired ideas she is testing in her group's latest  in vivo   experiments, which now extend to fruitflies and yeast cells, should entice more interest in executable biology among lab-based biologists. But in the end, Fisher emphasizes, the fact that using executable rules could make the models easier to visualize is only an added bonus. Executable biology's real pay-off is that it can help biologists to understand the complexity of living things, whether at the level of groups of molecules, such as Kappa describes, or at that of signals sent between cells, as in the nematodes Fisher herself studies. And that enhanced understanding, in turn, helps biologists ask new questions, design new experiments and make new discoveries. \"But however good the models are, \"you still need a good scientist to implement them\", says Kwiatkowska. \"The model is not an oracle,\" Heath agrees, \"It's an automation of your understanding.\" Lucas Laursen is a freelance journalist in Cambridge, UK. \n                     Review in Nature Biotechnology \n                   \n                     Letter in Nature Biotechnology \n                   \n                     EGF model in Molecular Systems Biology \n                   \n                     Jasmin Fisher's web site \n                   \n                     John Heath's web site \n                   \n                     Corrado Priami's web site \n                   \n                     Cellucidate web site \n                   Reprints and Permissions"},
{"file_id": "462560a", "url": "https://www.nature.com/articles/462560a", "year": 2009, "authors": [{"name": "Nicola Nosengo"}], "parsed_as_year": "2006_or_before", "body": "An underwater effort to detect subatomic particles has ended up detecting sperm whales instead. Nicola Nosengo reports on a partnership between marine biologists and particle physicists. To the dock workers and sailors at the port of Catania, in Eastern Sicily, it all looked very suspicious. About once a month during 2005 and 2006, two strangers would walk out to a large wooden cabin at the end of a pier, unlock the door, and remove a small box. Then they would lock up again and disappear until the next month. The locals had to question what the two men were up to. But when asked, the strangers reassured them that there was nothing to worry about. They were scientists. And the boxes they were retrieving were computer hard drives containing hours of sound data relayed by an underwater cable from microphones \u2014 or, more accurately, hydrophones \u2014 placed on the Mediterranean sea floor 28 kilometres offshore. Giorgio Riccobene, a particle physicist at the Southern Laboratories of the Italian National Institute for Nuclear Physics (INFN) in Catania, was hoping to show that the hydrophones could be used to detect subatomic particles called neutrinos that had come from deep space. Giovanni Pavan, a marine biologist from the University of Pavia in Northern Italy, was there to help Riccobene deal with background noise in the recordings. But what Riccobene and Pavan discovered as they listened to their data will bring them back to the port next year with their roles reversed. Then, the physicist will be helping the biologist, and their quarry will not be neutrinos, but sperm whales. The road to this unexpected destination began nearly a decade ago with Riccobene's involvement in the Neutrino Mediterranean Observatory (NEMO), a collaboration of around 100 researchers from the INFN and other Italian institutes who are hoping to study neutrinos in the ocean. Cosmological neutrinos are constantly streaming through Earth, carrying invaluable information about distant sources such as supernovae. But these fundamental particles have no electric charge and have masses close to zero; they interact with matter so rarely that studying them requires gigantic detectors \u2014 the bigger, the better. Hence the NEMO design calls for thousands of optical detectors distributed over 2 cubic kilometres of water, 3,500 metres under the sea at a site off Capo Passero in southern Sicily. The idea is that an incoming neutrino will very occasionally interact with a water molecule, producing a pulse of light that the detectors will capture. Riccobene was working on a way to enhance the detection. \"Theoretically, higher-energy neutrinos should also produce detectable sound waves,\" he says. \"As sound travels better than light in water, an acoustic detector could multiply chances to capture neutrino events.\" No one knew if this would work. But as the NEMO design includes hydrophones anyway \u2014 they are needed to position the optical detectors\u2014 Riccobene was asked in 2002 to supervise a feasibility study called the Ocean Noise Detection Experiment (ONDE), which would be located at the project's 2,000-metre-deep test site east of Catania.  \n                Noise control \n              To educate himself, Riccobene went to Paris for a workshop about acoustic neutrino detection, and immediately noticed something missing from the talks. \"Background noise was not even mentioned,\" he recalls. \"Everyone was taking for granted that at great depths it would be very low, but there were no published data.\" Riccobene went back to Catania, just in time to discover that a local environmental group was hosting a talk by Pavan, who had pioneered the digital recording of sea-mammal sounds in the early 1980s, and who was acknowledged as one of the world's leading experts in the field. He was obviously the right man to answer Riccobene's question: how high would background noise be at a depth of 2,000 metres? With little data to rely on, Pavan had no simple answer. \"Systems to record at great depths were simply not available until a few years ago,\" he says. About all he could say for sure was that deep waters were not nearly as silent as the neutrino physicists were assuming. \"At first I was appalled,\" Riccobene says. The noise levels Pavan estimated were well above the expected level of a neutrino event. That did not necessarily make neutrino detection unfeasible, he says. But it did mean that the NEMO team couldn't hope to isolate the neutrino signals until it had an accurate survey of the background noise it would have to filter out. Riccobene invited Pavan to join the ONDE team on a long-term monitoring project of the Sicilian seabed soundscape \u2014 the first ever attempted at such depths. Pavan had no funds to support his participation, but accepted anyway. Riccobene would give him access to depths he could never reach otherwise, allowing him to study the largely unknown acoustic environment of the deep sea. Pavan particularly hoped to measure the level of sound pollution there, as it is a potential cause of stranding for many deep-diving whales \u2014 whose vocalizations he also expected to hear in the recordings. By January 2005, Riccobene and his team had positioned four high-sensitivity hydrophones at the NEMO test site and had laid an optical data cable back to that cabin on the pier in Catania. Soon after that Riccobene and Pavan were obtaining data. And in April 2005, Pavan began listening to the first recordings. As he predicted, Pavan could hear low, uniform background noise, mostly caused by natural water movement and ship traffic, plus an occasional burst of identifiable sounds: the propeller of a large ship, a sonar impulse, even some explosions. But what captured his attention were short, regularly repeating sequences of 'clicks' \u2014 the signature sounds made by sperm whales compressing air through their respiratory system. \"They probably use them to estimate depth and to locate prey, measuring their echoes more or less like bats do,\" Pavan says. Hearing clicks every now and then was not surprising: they are among the loudest sounds produced by any animal, and can travel up to 20 kilometres in water. What was surprising was that the clicks kept appearing in the recordings month after month. \"Sperm whales are considered very rare in this area,\" Pavan says. \"Published data hint at a very sparse population.\" But these studies are usually based on sightings or sound recordings taken near the sea surface. The ONDE recordings came from the deep waters where sperm whales dive for food and spend most of their time, suggesting that the creatures could be much more abundant in this area than previously thought. Riccobene and Pavan kept recording until November 2006, when the hydrophones were removed and replaced by a prototype of the optical detection system, which was connected to the same cable. By that point the two scientists had collected more than 600 hours of recordings, and although they hadn't detected any neutrinos, they were satisfied. \"My interest at the time was in proving that deep underwater recording was feasible, and could produce data of good quality,\" Riccobene says. \"I did not expect to detect neutrinos at this stage.\" At the same time, Pavan now realized he could get much more from the data than a simple list of background noise sources. Yet hearing sperm whales is one thing. Counting them and compiling reliable statistics is another. In principle, the acoustic properties of a click can give an idea of the animal's size and sex. And if the click's reflection from the sea surface also shows up in the recording, it is possible to estimate the animal's position and its ascending or descending trajectory. \"But either you develop a software algorithm to do this, or you do a long and awful hand count,\" Pavan says \u2014 and the latter is exactly what he and his colleagues have been doing since 2006, as they had no funding to pay for such algorithms. Still, Pavan says, a statistical picture has slowly begun to emerge. Sperm whales appear in half of the recorded days, something existing population estimates cannot account for. Pavan and his colleagues have also been able to detect seasonal patterns and hints of social behaviour in the recordings \u2014 data they presented at a conference in Pavia in September. \"Whales are more frequent in spring and autumn, with more animals moving at the same time, probably belonging to the same school,\" Pavan says. Sometimes the recordings contain 'codas', brief sequences of clicks with a peculiar pattern. These sounds are thought to have a social function, as they are emitted only when males, normally solitary, gather around female groups. Codas seem to work as local dialects, with different populations using different patterns. \"The most frequent pattern in the Mediterranean is the 3 + 1 type,\" says Pavan, \"a rapid succession of three clicks and then an additional one.\" But the recordings show that a 2 + 1 type is more frequent than expected, which could hint at whales in transit from outside the Mediterranean basin.  \n                Deep-down disturbances \n              Meanwhile, Riccobene, by now an enthusiast of bioacoustics almost as much as of neutrino physics, has managed to set up a continuation of the study by involving the European Seas Observatory Network (ESONET), a European collaboration that is developing a network of deep-sea monitoring stations in the Mediterranean. Although ESONET's focus is on geophysics and climatology, it has agreed to finance a new incarnation of the ONDE platform called LIDO (Listening Into the Deep Ocean). LIDO will last for three years, starting next March when an array of four hydrophones will once again be deployed off Catania to listen for whales. This time, continuous recording sessions will last for up to one year, and algorithms will be used to choose which data to keep and which to disregard. Detailed comparisons of recordings from the four hydrophones, placed on the vertices of a square, will allow researchers to determine each detected animal's size, speed and direction, and give a more precise estimate of sperm-whale populations and seasonal habits. In the future, the project envisages the deployment of similar stations at other locations, including the Gulf of C\u00e1diz, close to the Strait of Gibraltar. \"One of our key problems is to understand whether Mediterranean sperm whales are a closed population, or whether there is an exchange with the oceanic population,\" Pavan says. Riccobene and his colleagues are also continuing their work on the final, large-scale NEMO observatory. If all goes as the developers hope, it will be deployed before the end of the next decade off Capo Passero. The larger project will include an acoustic system, at least for positioning purposes if not for detection. Riccobene now knows there is plenty of background noise in deep waters, but he has decided that he needs more data to work out whether he can detect neutrinos acoustically, and that depends on winning more funds. And NEMO's data will be shared with biologists, seismologists and any other interested researchers. Pavan will surely be among them, eager to study not just sperm whales but also fin and beaked whales, whose low frequencies are best heard at greater depths. \"Sea floors at such depths are among the least-known regions of the planet, and there is a lot down there for many researchers,\" Riccobene says. \"Once we put a broadband connection 3,500 metres under the sea, I have a feeling people will queue up for the data.\" \n                 Nicola Nosengo is a freelance science writer based in Rome.  \n                 Click  \n                 \n                     here \n                   \n                  to hear a group of whales share a social moment, and click  \n                 \n                     here \n                   \n                  to hear a series of clicks from a sperm whale passing by.  \n               \n                     A series of clicks from a sperm whale passing by \n                   \n                     A group of whales share a social moment \n                   \n                     Nature Physics \n                   \n                     NEMO project \n                   \n                     Interdisciplinary Center for Bioacoustics and Environmental Research \n                   \n                     ESONET \n                   Reprints and Permissions"},
{"file_id": "462406a", "url": "https://www.nature.com/articles/462406a", "year": 2009, "authors": [{"name": "Meredith Wadman"}], "parsed_as_year": "2006_or_before", "body": "The new head of the US Food and Drug Administration has inherited an agency battered by crises. Meredith Wadman asks whether Peggy Hamburg can concoct a cure. When Margaret 'Peggy' Hamburg was appointed health commissioner of New York City in 1992, the 36-year-old physician and former researcher took a job that few people wanted. A tuberculosis epidemic was roaring out of control in the city, facilitated by rising rates of HIV/AIDS. Rodents plagued city streets and a demoralized health department was facing draconian cuts in its budget as the city government fought a ballooning deficit. Hamburg's predecessor had abruptly resigned early in his tenure, after antagonizing the city's vocal HIV/AIDS community. Within three years Hamburg had turned the department around. She rescued it from proposed cuts that would have crippled its public-health lab and its immunization and school-health programmes. She won extra money for rodent control. She launched a needle-exchange programme to combat AIDS and she reversed the tuberculosis epidemic, with a 21% drop in new cases 1  over the course of a programme the United Nations later cited as exemplary. Hamburg, who became commissioner of the US Food and Drug Administration (FDA) in May, is now hoping to repeat that success at a beleaguered federal agency that has endured a string of recent crises (see  'Peggy Hamburg's FDA fixes' ). Looking back on her experiences in New York, she says: \"We took what was a faltering agency and restored it to its former position as the premier health department in the country. I do feel that that has a lot of relevance to the FDA now.\" The parallels are striking. The FDA, once globally revered as the gold standard in regulation of food and medical-product safety, has lapsed repeatedly in recent years under a string of different leaders and a long stretch without any permanent chief. In 2004, the anti-inflammatory drug Vioxx (rofecoxib) was taken off the market after five years of sales because of cardiovascular side effects that may have caused tens of thousands of deaths in the United States alone 2 . The agency's scientific capabilities eroded to the point that its own science board declared two years ago that \"not only can the agency not lead, it cannot even keep up with the advances in science\" 3 . The FDA's centre in charge of medical-device approvals had also been rocked by charges that top regulators there overrode centre scientists and approved devices that put the public at risk \u2014 charges first aired by the centre's own scientists and given further credence in January in a report from the Government Accountability Office. And a series of food-poisoning outbreaks \u2014 including  Salmonella -tainted peanut butter \u2014 have led to persistent questions about the agency's ability to ensure the safety of the country's food 4 . Although its $2.7-billion budget for 2010 lags far behind those of its sister agencies in the Department of Health and Human Services, the FDA is charged with policing the safety of goods that account for about one-quarter of the US consumer economy, from pacemakers to toothpaste. And its mandates keep growing. In 2007, the agency received new powers to police the safety of drugs already on the market, and in June, a new law brought tobacco products under its jurisdiction. Increasing the budget of the 11,000-person agency \"is critical\", Hamburg says. \"Not only are we trying to rebuild important core functions, we are also adding on some new responsibilities and gaining new authorities.\" Hamburg has made it clear that she plans to step up enforcement to catch both fraudulent operations that prey on consumers and above-board companies that fail to comply with required manufacturing standards. The FDA set a tough tone starting in May by clamping down on websites that were marketing products fraudulently claiming to diagnose, prevent or treat pandemic H1N1 influenza.  \n                Persuasive policy-maker \n              The qualities that Hamburg brings to the job, say her supporters, include an ability to analyse complex issues and to persuade \u2014 rather than browbeat \u2014 others to accept her point of view. \"Pounding on the desk is not her style,\" says David Dinkins who, as mayor of New York in the early 1990s, was persuaded by Hamburg to back a controversial needle-exchange programme to combat the spread of HIV/AIDS. \"She would reason with you. She would set forth a rationale. It just made sense. She seemed to me to always make sense.\" Perhaps most importantly for scientists at the agency, Hamburg, although steeped in public health, has an innate familiarity with scientific culture. She grew up on the Stanford University campus in Palo Alto, California, where her parents were both on the medical-school faculty. As a 15-year-old, she spent a semester in Africa with primatologist Jane Goodall, and before entering medical school she worked in a neuropharmacology lab at the National Institute of Mental Health in Bethesda, Maryland. Later, as a resident in internal medicine, she worked in the lab of neuroscientist Paul Greengard at the Rockefeller University in New York. In 1989 and 1990, she was deputy director of the National Institute of Allergy and Infectious Diseases in Bethesda, where she focused on infectious-disease research and policy. Hamburg is proud of her time in the research world. She says she was \"irritated\" by media coverage opining that her public-health background \u2014 including four years as a top health adviser during the Clinton administration \u2014 made her a more natural fit to lead the Centers for Disease Control and Prevention. \"As though that was my only background,\" she says. \"People should recognize that I actually began my career in medicine doing some bench research.\" She says that the results of research inform her policy decisions, noting that she pushed for the needle-exchange programme in New York because of evidence that needle exchange reduced the rate of HIV transmission. At the FDA, however, that kind of science-driven stance has not yet become evident in one contentious area: an emergency contraceptive called Plan B remains unavailable as over-the-counter medication to girls younger than 17 years old, even though FDA scientists in 2004 called it safe and effective for women of all ages and a federal judge urged the agency to revisit the issue in March. In June, dozens of groups, including the American Academy of Pediatrics and the American College of Obstetricians and Gynecologists, wrote to Hamburg asking her to lift existing restrictions and make Plan B available without a prescription to younger girls. They have received no response. \"We are disappointed at the lack of action,\" says Kirsten Moore, president and chief executive of the Reproductive Health Technology Project, a non-profit organization based in Washington DC that organized the letter. Judy Leon, an agency spokeswoman, says that because there are ongoing legal challenges to the FDA concerning Plan B, the agency could not comment. Despite the expectation that Hamburg will favour tighter regulation, industry reviews of her performance have been positive. \"It is obvious that she brings clear vision and dedication to the job,\" says Ken Johnson, senior vice-president at the Pharmaceutical Research and Manufacturers of America in Washington DC, a lobby group for the country's pharmaceutical manufacturers. Tevi Troy, a visiting senior fellow at the right-leaning Hudson Institute in Washington DC who was deputy secretary at the Department of Health and Human Services in the George W. Bush administration, calls Hamburg \"the right kind of person\" for the job. Troy is especially pleased with her background in biological security: in New York City, she instituted the first health-system-based anti-bioterror programme in the nation, and from 2001 until her appointment as FDA commissioner she was vice-president for biological programmes at the Nuclear Threat Initiative. Still, says Troy, he has been made \"nervous\" by the abrupt departure in August of Dan Schultz from his position as chief of the FDA's device centre. He sees it as \"a signal that anyone who even gets a reputation as being willing to listen to industry concerns may have trouble going forward\" in a Hamburg-run FDA. Yet some inside the agency assert that Hamburg has done nothing to change what they call a long-standing bias at the FDA in favour of the drug and device industries. \"We have had multiple examples since the change in administration where the work of [FDA scientists charged with monitoring the safety of marketed drugs] was basically filed in the wastebasket,\" says one scientist who did not wish to be named because of job-security fears. Hamburg has spent much of her short tenure dealing with the H1N1 pandemic; her agency is responsible for licensing vaccines and overseeing their quality after production. The demands of responding to H1N1 make clear just how much Hamburg's success at the agency will hinge on how well she balances unforeseen crises with her overall goal of reinvigorating the agency. She will be helped, say her supporters, by an almost preternatural ability to remain calm under pressure. Two years ago, Hamburg was in New York to give a speech on biological security when her husband called to say that their house in Washington DC was on fire. \"Everyone is okay, but three fire trucks just pulled up,\" he said. She went ahead and gave the speech, without mentioning that her house was burning. \n                     Nature Medicine \n                   \n                     FDA \n                   \n                     Margaret Hamburg \n                   Reprints and Permissions"},
{"file_id": "462562a", "url": "https://www.nature.com/articles/462562a", "year": 2009, "authors": [], "parsed_as_year": "2006_or_before", "body": "A unique collaboration is bringing automated screening to the study of fly behaviour and could change the way that machines see humans. Lizzie Buchen reports. At full speed, the altercation would have looked like nothing \u2014 a brief contact, fractions of a second long, between two flies. But slowed to 1/20th of normal speed it has all the flash and dazzle of an elaborate professional wrestling move. Biologist David Anderson calls the grainy, black and white video on his computer the \"fly lucha libre\". One fly, a male, rears up and clamps down on his neighbour. Flipping backwards he whips his opponent into the air, executing two somersaults in the process. The victim, helpless, flaps its wings as it crashes to the floor, then rights itself and flies away. Take-downs like these are just one of dozens of behaviours that Anderson studies at California Institute of Technology (Caltech) in Pasadena (see the video from Martin Heisenberg at the University of W\u00fcrzburg in Germany at  http://go.nature.com/o8sRLs ). \"It's this whole world you'd have no clue existed,\" says Anderson's collaborator, Pietro Perona, whose forays into this world have been relatively recent. For 20 years the computational-vision scientist, also at Caltech, has been trying to develop machines capable of detecting and interpreting complex behaviours in humans. It's an ambitious goal, but he has found unlikely allies in Anderson and behavioural neuroscientist Michael Dickinson. Since about 2005, the three have collaborated to create tools that combine the ease of manipulating the fly nervous system with ways to automatically track complex social behaviours such as aggression and courtship. The hope is to understand their neural underpinnings, which may inform studies in humans. This year, the group demonstrated two systems that can do this 1 ,   2 . Anderson is now using them to screen thousands of lines of mutant flies to understand the genes and neural circuits that control behaviours, and Dickinson is exploring the social dynamics of flies in large groups \u2014 something that has been nearly impossible to approach in the lab. \"This is a very tough problem,\" says Joel Levine, a neurogeneticist at the University of Toronto in Mississauga, Ontario, who has been using one of the software programs, called Ctrax, since November 2008. \"I've been trying for a very long time to get people in my lab to write this kind of software, and no one could do it.\" Because the programs are freely available, fly researchers from around the world are using the new technology and taking off in their own directions. And through the flies, Perona is inching closer to his goal of building machines that can interpret human behaviour. The tools promise to be a boon for the field of fly behaviour, and especially its students, technicians and postdocs who perform a thankless job: watching videos frame by frame, tallying every lunge, timing every chase, classifying every wing threat and repeating. \"It's mind-numbing to sit in front of videos and score them by hand,\" Anderson says over lunch with Perona and Dickinson at the Caltech faculty restaurant. It's not just a matter of saving time, Perona adds. Accuracy counts, yet observing flies can be a subjective exercise. \"Suppose we use the clipboard and stopwatch method. We have a postdoc at Caltech who does it, and another postdoc in Norway who does it.\" If results don't agree, getting the answer is next to impossible: \"Those postdocs are gone, they've got jobs somewhere else, so we're left in this situation of irreconcilably different opinions \u2014 opinions that never become fact because there is nothing to ground them on.\"  \n                Big brother \n              Perona had been looking to develop machines that can understand peoples' actions and intentions by watching their movements. Machines with such capabilities would have an array of applications. Imagine workplace safety cameras that can warn when a machinist is dozing or a security camera at an airport that flags suspicious behaviour. To build these systems, machine-vision scientists train computers using a common language: labelled data. Annotate thousands of examples of pedestrians stepping on to a crossing, for example, and one can produce an algorithm that could identify that behaviour in novel situations. But simple, specific behaviours are of limited use when the goal is to recognize behaviours that would qualify as 'suspicious' or 'dangerous'. Perona is approaching this problem by trying to break down behaviour into its component parts. Pouring a glass of wine, for example, involves gripping the bottle, lifting it, moving it towards a glass and finally tipping it down and up. Perona has dubbed such elementary motions 'movemes' 3 , and they form the basis of his behavioural hierarchy. Movemes combine into an action, such as pouring wine. An action that is prolonged in time is an activity, such as having dinner. But testing such a theory, he says, requires enormous sets of labelled videos \u2014 videos that are hard to come by. \"You just can't get enough data,\" Perona says as he counts off the reasons on his fingers. It's illegal to tape without permission, difficult to get approval and then harder, with permission granted, to get natural, candid activity. \"Also humans are boring,\" Perona says. \"How much time do you have to spend in pubs before you finally see a fight?\" Perona realized that he could learn a lesson from the life sciences. \"When biologists encounter complicated issues such as schizophrenia, what do they do?\" he asks. \"They use a model organism. Something that lets them do very intrusive, invasive experiments and get answers much more quickly.\" A few years ago, he started asking his colleagues at Caltech about their model organisms, searching for one with behaviours that were interesting, but not too complex. Meanwhile, Anderson had just come back from a sabbatical where a lab was using an off-the-shelf tool to measure how much flies were moving around. \"It worked, but it was clunky,\" Anderson remembers. Dickinson suggested that he should build something himself. So Anderson approached Perona with descriptions of the flies' intricate behaviours. Perona was intrigued. \"I didn't know flies do more than just go buzz and push themselves through the air,\" he says.  \n                High ambitions \n              Anderson wanted a program that could automatically detect specific behaviours during aggression and courtship \u2014 counting and timing wing threats, chases, wrestling bouts and copulations. He wanted to be able to run many pairs of flies at the same time and have the machine spit out when, where and in what order every action occurred. Dickinson, who has devoted most of his career to the aerodynamics and biomechanics of fly flight, wanted to understand how flies behave in groups of dozens to hundreds. To do this, he would need to keep track of each fly's movement and maintain their identity over long periods of time \u2014 something no one had done before. There were a couple of programs that could follow flies, but they had an 'occlusion problem'. Whenever two insects touched or entered into the same pixel, an experimenter would have to manually tell the machine how to separate the flies out. Both had tall orders for Perona. \"But that's the great thing about collaborations,\" Anderson says. \"Something that to biologists looks nearly impossible may be in the realm of achievability for someone like Pietro.\" Anderson's specialized behaviour detector, and Dickinson's exploratory tracker required very different software, but the tasks seemed simple enough. Perona shared one postdoc with Dickinson \u2014 Kristin Branson \u2014 and another with Anderson \u2014 Heiko Dankert. Perona estimated that each program would take a matter of months. \"Of course it ended up taking three years, but that's fine,\" Perona says. Much of the time was spent making it understandable to biologists and also robust enough to work reliably in the hands of others, what Dickinson calls the \"fender and body work\". Meanwhile, Dickinson was tinkering away, trying to create an environment for Branson's software \u2014 Ctrax \u2014 to analyse. The product was a circular arena just large enough to hold a football, surrounded by eight halogen lights. Inside, up to 50 flies walk around; their wings clipped to prevent them from taking off and confusing the tracking system. A camera hangs directly over the centre of the arena, capturing the movements and relaying them to a nearby computer. On the monitor, each fly leaves a thin, brightly coloured trail as it explores the arena. After a few minutes the screen is so clogged with lines that it looks like a ball of multicoloured yarn. In essence the program doesn't do much more than follow flies around. The idea was to create an all-purpose tracker and quantifier that would let the experimenter decide which behaviours were interesting \u2014 and to tell the machine not only what to look for but how to learn what to look for. \"This is one of the most exciting aspects of it, this discovery phase of research,\" says Dickinson. Pietro describes one intriguing behaviour they identified. \"So there was one fly going slowly, and another fly was overtaking it,\" Pietro says, demonstrating the motions with his fingers. \"As the faster fly was overtaking the slow fly, the slow fly stops. Now it could have been a fluke that the fly decided to stop, but so then Michael goes forward and he finds another example of the same thing happening. And so, now we can very quickly program the software to pull out every encounter of flies going like this.\" All they have are hunches as to why the flies do it, says Perona. But once an experimenter recognizes an interesting sequence of movements or interactions, he or she can propose hypotheses and answer them with a new analysis \u2014 no new data required. The program will then generate quantitative descriptions of each behaviour for each individual fly. These 'ethograms' allow the user to discover and quantify subtle behavioural differences between populations of flies, such as males versus females or flies with genetic mutations, and even between individuals within a population \u2014 fly personalities. Dickinson's group has already made some novel observations using the ethograms. Male flies, for example, are not shy \u2014 they walk close to one another, inspecting, even nudging each other from time to time. But females need their personal space. They obey certain rules of etiquette when they meet each other and when they walk around. His group is also starting to explore how social interactions change in more complex environments, placing cones and obstacles on the landscape. He has found that flies tend to climb to the top of the cones, and chase each other off in 'king of the mountain' fashion. With these new observations come new hypotheses. Dickinson says the behaviour might have something to do with maintaining a good vantage point for identifying food sources. In a lab a few hundred metres away, Anderson is using his program, called the Caltech Automated Drosophila Aggression-Courtship Behavioral Repertoire Analysis (CADABRA), to take a quantitative look at behaviours he already knows are there \u2014 wrestling, tackling and other manoeuvres that flies attempt during aggressive bouts, and the serenades and dances that lead to mating. He is now developing a screen in which he has genetically activated and silenced different populations of neurons. These selective disruptions will allow him to dissect the neural circuitry of each behaviour \u2014 not just aggression as a whole, but the frequency of each component of the behaviour, and even the frequency at which one behaviour leads to another. In his set-up, a pair of flies sits in a shallow well about as wide as a golf ball. A plate about a centimetre high holds 12 such wells. Hanging above, a camera films all 12 interactions in parallel. The video can then be run, frame by frame, through CADABRA, which then produces statistics on every behaviour. CADABRA's ethograms, for example, illustrate the frequency of each lunge and chase, as well as the frequency of transitions \u2014 such as how often a chase is followed by a lunge versus a flirtatious wing extension. \"To do this by hand would take 270 person hours,\" Anderson says. \"We basically did it in 20 minutes.\"  \n                Flight club \n              Levine learned about Ctrax when he and Dickinson were lecturing at the Marine Biological Laboratory in Woods Hole, Massachusetts, in the summer of 2009. He has now configured the system so that it can follow individual flies in a group over a period of days, without any interference. \"So little is known about group dynamics,\" he says. \"Who interacts with whom? Why do some flies mate more than others? How do social interactions affect the circadian rhythms? This has been very difficult to study. And now along comes Michael with this software, and, finally, we can ask these questions.\" Leslie Vosshall, a neuroscientist at Rockefeller University in New York, is using Ctrax to study courtship. She is using a modified set-up, designed in Dickinson's lab, in which the flies wander around in a dish-like arena covered by a plastic lid. This allows them to move about with their wings intact \u2014 a crucial modification, because of the involvement of wing signals and wing songs in courtship. And Michael Reiser \u2014 Dickinson's former graduate student, now at the Howard Hughes Medical Institute's Janelia Farm Research Campus in Virginia \u2014 is taking advantage of Ctrax's flexibility for his 'Fly Olympiad'. He is putting mutant flies through a barrage of behavioural tests \u2014 from visual reflexes to walking behaviour to odour sensation. Perona says he's pleased that his work has started to bear fruit for biologists. And Anderson's and Dickinson's projects are producing the videos of labelled behaviours of which he had been starved. The translation of fly behaviours into ethograms is also giving Perona his first opportunity to test his hypothesis of behavioural hierarchies \u2014 in which a continuous, extended activity such as courtship can be broken down into actions, which can further be decomposed into elementary but meaningful motions. He hopes the concept will form the basis of an overall computational theory of behaviour. But although the simplicity of fly behaviour allows Perona to make conceptual progress, it can only take him so far. \"Am I closer to building a machine that can figure out what people are doing? Yes and no. Flies have a much simpler repertoire of behaviour than humans and they don't waste time.\" Watching fly behaviours should allow him to go back to human data with more solid ways of testing his hypotheses. But Perona's goals have also been evolving. He says he has become increasingly interested in fly behaviour for its own rewards. \"If it all ended up with me having contributed something to biology, it would still be fantastic,\" he says. Lizzie Buchen is an intern with  Nature   based in Washington DC. \n                     Neuroscience Nature News Special \n                   \n                     Michael Dickinson \n                   \n                     Pietro Perona \n                   \n                     David Anderson \n                   \n                     Ctrax \n                   Reprints and Permissions"},
{"file_id": "462714a", "url": "https://www.nature.com/articles/462714a", "year": 2009, "authors": [{"name": "Jeff Tollefson"}], "parsed_as_year": "2006_or_before", "body": "The United Nations Climate Change Conference is mainly a political affair but it has drawn hundreds of scientists to the Danish capital. Jeff Tollefson finds out what they hope to gain. As the United Nations summit on global warming kicks into gear in Copenhagen this week, upwards of 15,000 people are converging on the city. The official negotiators from 193 countries will spend much of their time behind closed doors at the Bella conference centre, but they will be a minority of the visitors. Orbiting around the negotiators will be representatives of almost every segment of society, including hundreds of scientists. The researchers will attend scheduled science sessions and gather for countless impromptu discussions in corridors and cafeterias. Many are presenting their latest work \u2014 on a vast array of topics including forest carbon, emissions scenarios and green technologies. Some hope to influence policy-makers and provide technical advice on issues that emerge during the negotiations. Others are coming to educate themselves about the treaty process and to network. A climate summit is a flurry of activity, with the central negotiations surrounded by side shows that last from early in the morning until late at night. When the formal sessions finally wind down (if, in fact, they do, as negotiations have been known to go all night), discussions often continue over dinner and drinks. The Copenhagen meeting, which runs from 7 to 18 December, is officially the 15th Conference of the Parties (COP) to the United Nations Framework Convention on Climate Change, which was signed at the Earth Summit in Rio de Janeiro in 1992. Negotiators have been meeting each year for a COP since 1995, but the expectations and the stakes for this summit are orders of magnitude higher than for any previous one. Twelve years after taking their first tentative steps with the Kyoto Protocol, countries are now aiming to restructure the global economy and to lock in deep cuts in greenhouse-gas emissions for decades to come. In advance of the summit,  Nature   talked to researchers from around the world about how they plan to take part. \n               Ottmar Edenhofer, co-chair, Working Group III, IPCC; deputy director of the Potsdam Institute for Climate Impact Research, Germany.  \n             \n               \"I wouldn't say that I am depressed, but I feel very sad about the negotiation process as it stands now. But I don't see that this can be changed substantially by scientists.\"  \n             Edenhofer is wearing two hats in Copenhagen. As co-chair of the Intergovernmental Panel on Climate Change (IPCC) working group on mitigation, he is presenting results from the group's 2007 assessment at several side events and briefings for policy-makers. But those events frequently lead to additional contacts, requests and conversations in which, as an independent scientist, he can offer his own thoughts on the latest research and what it means for policy-makers. Edenhofer says that the negotiations are falling short of what is needed to address global warming and that scientists are unlikely to change that now. However, he argues it would be wrong to downplay the role of science in the process. Scientists were the first to raise concerns about climate change, and the IPCC's fourth assessment has served as the foundation for the negotiations. He sums up the IPCC's findings this way: humans cause climate change; climate change has severe impacts; and it is not too costly to reduce emissions. \"These three messages have already changed the mindset of the negotiators,\" he says. \n               Beth Sawin, biologist and programme director, the Sustainability Institute, Hartland, Vermont.  \n             \n               \"We have this philosophy that if science is going to be helpful, it has to show up, wanting to serve. What can we do to our model to make it more useful to somebody who is incredibly busy, overwhelmed, with not enough time and a huge responsibility?\"  \n             The Sustainability Institute has developed user-friendly climate-modelling software that can be run on a laptop computer to help negotiators assess the ultimate impact of any given emissions scenario. Negotiators can manually adjust the emissions and other parameters to analyse their own proposals as well as those of other countries; the model spits out forecasts for variables such as future temperatures and sea-level rise. In Copenhagen, Sawin says, the team is providing a \"widget\" that can be installed on computers to get the latest climate readings whenever Sawin's group updates its model with any new commitments announced by countries. The application has generally received positive feedback from negotiators, but Sawin acknowledges the sobering reality that some delegates are less interested in detailed climate projections than in the next election in their home country. Nonetheless, she finds the whole affair touching. \"I see that there are warts, and there is unfairness, and there are flaws in this process, but at least it's happening,\" she says. \"So when I come home and talk to my kids, that's what I emphasize: that we happen to be alive at a time when people are trying to make common decisions about how to protect our common planet.\" \n               Albert Binger, science adviser to Grenada and the Alliance of Small Island States.  \n             \n               \"I never had the slightest notion in my mind that one day I would be the guy telling everybody that the [target of] 2 \u00b0C the majority of the world wants is absolutely crazy. 2 \u00b0C is too much for too many people.\"  \n             Raised in the mountains of Jamaica, Binger did a brief stint as a chemical engineer in the petroleum industry before earning a doctorate in agronomy at the University of Georgia. Today he is an official delegate advising island nations that are seeking to limit average global warming to 1.5 \u00b0C \u2014 or preferably less. Regularly oscillating between anger and a healthy island humour, he says. \"Everybody needs to clean up their own goddamn mess.\" Although Binger has full access to the talks, he leaves negotiating to the negotiators. His job is to harness scientific evidence in the push for more stringent greenhouse-gas targets. In practice, this means helping to answer questions that arise during the talks and providing scientific evidence for use in speeches and debates. As an islander who stands to lose everything to ocean acidification and sea-level rise, Binger takes the issue personally. \"We want 1.5 \u00b0C or less, and we don't really ask it selfishly. Every person on this planet is better off at 1.5 \u00b0C than they are at 2 \u00b0C. I can sleep very easily with that.\" \n               Lawrence Buja, climate modeller, National Center for Atmospheric Research (NCAR), Boulder, Colorado.  \n             \n               \"To a certain degree, the physical modellers have a much easier job than these politicians. Our molecules don't think for themselves and start doing different things midstream.\"  \n             At COP 14 in Poland in 2008, Buja gave a briefing on NCAR's climate-modelling results for the fourth assessment of the IPCC, issued in 2007. He headed the modelling team at the time but is now directing a new group that is developing integrated climate models that include social and economic forces. His career change reflects a larger shift \u2014 Buja goes so far as to call it a \"sea change\" \u2014 for NCAR as an institution. Physical modelling will remain a core activity as scientists seek to clarify and provide more detail about the potential impacts of greenhouse gases, he says, but NCAR recognizes that it needs to provide policy-makers with more information about potential solutions. In Copenhagen, one of his colleagues is presenting modelling results analysing the level and timing of emission-reduction targets, focusing on the 2050\u20132100 time frame. Buja is on hand to talk about these issues as well as to answer questions about the physical modelling, which is now being ramped up for the IPCC's fifth assessment, due out in 2014. But information flows both ways at these meetings, he says. \"What this exposes the scientists to is how these negotiations and agreements are developed and what our role in informing them might be.\" \n               Martin Parry, climate scientist, Imperial College London.  \n             \n               \"For individual scientists like me, frankly, many would say there's not much point in going. But I think it's a chance to meet those at the fringes of the political system who potentially do have quite a lot of leverage.\"  \n             Were you to bump into him in Copenhagen and ask how the negotiations are going, Parry says he wouldn't have a clue. He has minimal or no contact with negotiators but says he finds value in exchanging ideas with scientists and activists. Those discussions can be particularly important, Parry says, because advocacy groups such as the WWF can then inject the latest scientific thinking into the political process as they lobby negotiators and government officials. In Copenhagen, he is expecting to participate in two side events, one on development issues and a second on agriculture. Parry is also thinking about how to assess a major hole in how the world intends to respond to climate change. Some impacts can be avoided by reducing greenhouse-gas emissions. Others can be managed with enough money; in the vulnerable developing world, that means financial aid from wealthier nations. But the current proposals for emissions cuts and monetary support are not enough to avoid major impacts. \"We're trying to close a gap here, coming at it from both ends,\" he says. Parry hopes that framing the issue this way \u2014 and quantifying the impacts \u2014 in Copenhagen will clarify where the policy-makers are coming up short, both in terms of emissions reductions and money for adaptation. \n               Paulo Moutinho, research coordinator, Amazon Environmental Research Institute, Brasilia, Brazil.  \n             \n               \"I believe that [the forest-protection strategy called] REDD could make a difference in COP 15, not just as a way to address emissions from tropical deforestation, but also to create a new kind of synergy among nations. I believe that. That's exactly why I am going.\"  \n             Moutinho started his career studying ants but has spent most of his time in recent years looking at ways to use carbon markets to stem emissions from deforestation while protecting biodiversity and the rights of indigenous people. Hopes have faded for a complete treaty in Copenhagen, but he is holding out for a significant decision on the forest-carbon component known as Reducing Emissions from Deforestation and Forest Degradation (REDD). Through REDD, wealthier nations seeking to 'reduce' their emissions would provide money for developing nations to protect their forests. In Copenhagen, Moutinho is presenting his organization's latest work on REDD in the Amazon and discussing Brazil's national greenhouse-gas commitments. For him, Copenhagen is a perfect fit. Spending time in the field and publishing papers in  Nature   or  Science   is one thing, he says, but the goal must be to translate results into a digestible form for policy-makers. \"Science is a tool to reach sustainable development. That's my view about science, and that's exactly what I'm doing.\" And when it comes to REDD, Moutinho says, the science is evolving rapidly and still plays an important part in the negotiations.   For more on Copenhagen, see  \n                     www.nature.com/roadtocopenhagen \n                   . \n                     Climate-change special \n                   \n                     UNFCCC \n                   \n                     Danish conference website \n                   Reprints and Permissions"},
{"file_id": "462717a", "url": "https://www.nature.com/articles/462717a", "year": 2009, "authors": [{"name": "Anjali Nayar"}], "parsed_as_year": "2006_or_before", "body": "How do you persuade philanthropists to pay $1 million for every pathogenic human virus you discover? Anjali Nayar talks to 'virus hunter' Nathan Wolfe in Cameroon to find out. Every day, more than 100 patients line up for treatment outside the bare cement walls of a rural health clinic in the Niete forest of southern Cameroon. Most of them suffer from what virologist Nathan Wolfe calls \"the usual suspects\": malaria and typhoid. But every once in a while there is something a little different: a case that is tough to diagnose, or that doesn't respond as expected to medication. For the patients and the medics here, this presents a problem. For Wolfe, it is also an opportunity. \"I get all excited,\" he says. \"These areas are choc-a-block full of interesting, unusual viruses.\" In 2005, Wolfe and his colleagues identified two unusual viruses that had jumped from non-human primates into Cameroonian hunters 1 , and that are closely related to pathogenic viruses that have spread through the world's population. Since then, Wolfe has made a name and a livelihood for himself in search of more. In 2007, he founded the Global Viral Forecasting Initiative (GVFI), an effort to monitor for new viruses and other microbes in communities around the world that are in intimate contact with animals. By identifying these agents as soon as they leap into humans, and before they start passing easily between them, Wolfe believes that researchers can predict and prevent human pandemics before they start, averting the next HIV, SARS, Ebola or pandemic influenza, diseases that have all been traced to animals.  \n                Brand Wolfe \n              Other researchers share that belief and are involved in this type of infectious disease surveillance. But what sets Wolfe apart is his swashbuckling style \u2014 he chooses to do most of his work in the field \u2014 combined with a flair for communication and negotiation. \"He's in the minority of scientists; he's good at science, politics, media and PR,\" says Jeremy Alberga, the chief operating officer at the GVFI. \"We have a brand and it's Nathan Wolfe and the GVFI,\" he says, \"not the GVFI and Nathan Wolfe.\" That brand has sold well. In 2008, Google.org, the philanthropic arm of Google, and the Skoll Foundation, an organization in Palo Alto, California, that supports social entrepreneurs, announced they would invest up to US$11 million to help expand the GVFI's work in Africa and southeast Asia. In October 2009, the United States Agency for International Development (USAID) named the GVFI as a main partner in its $330-million Emerging Pandemic Threats programme, a project involving various experts in wildlife surveillance, and for which the GVFI will receive tens of millions of dollars over the next five years. Last month, Wolfe unveiled a new $600,000 update to his lab in Yaound\u00e9, Cameroon's capital, marking ten years of research in the country. Wolfe and his work have also found a wider audience, featuring in news outlets ranging from  Wired   to  Men's Journal . Earlier this year,  Rolling Stone   magazine ranked Wolfe at number 53 of \"people who are changing America\", and an article in  Popular Science   magazine was simply titled: \"Nathan Wolfe: did we mention this guy was brilliant?\" \"We joke about it sometimes,\" says Frank Rijsberman, a programme director at Google.org, about the unrelenting press Wolfe receives. \"He's definitely audacious \u2014 he has big visions \u2014 but he has enough of a track record in the field to do it realistically.\"  \n                Lasting mark \n              Other 'virus hunters' say that they are less interested by Wolfe's semi-celebrity status, and more interested in what he is going to find in the future. \"Nathan's a bright, energetic young guy and I anticipate he will be successful,\" says Ian Lipkin, a specialist in pathogen surveillance and discovery at Columbia University in New York, who recently started collaborating with Wolfe. \"For the sake of the field it's important that he is. A lot of resources are allocated to Nathan. He's going to have to deliver.\" \"The real breakthroughs are published in the literature, and not on CNN,\" adds Peter Daszak, a disease ecologist and president of the Wildlife Trust in New York, who is working with Wolfe on the USAID initiative (and says he has been on CNN, a US cable news network, himself). \"If you look back on this field in 10 years, you will say who actually did the discovery, was there a paper that came out of this group and did they prevent the next disease.\" Wolfe's interest in virus emergence dates back to his master's degree in biological anthropology in the 1990s. Working under Marc Hauser and Richard Wrangham at Harvard University in Cambridge, Massachusetts, Wolfe was studying the behaviour of wild chimpanzees to see if they were consuming certain plants medicinally. He never answered the question \u2014 but while tracking chimpanzees in the forests of western Uganda, Wolfe witnessed several dramatic ambush hunts. \"The chimps were up to their eyeballs in the blood and body fluids of all these animals,\" he says. \"I just thought to myself what a perfect scenario for cross-species transmission of microbes.\" He postulated that the rural human populations of Central Africa, with a diet heavily dependent on bush meat, could be affected the same way. \"I just thought it would be this wonderful grab-bag of new things jumping into humans and bouncing around,\" he says. Wolfe started testing his ideas, looking at cross-species viral transmission first during doctoral work in Malaysian Borneo, and then back in Central Africa after being recruited by Donald Burke, an infectious-disease expert at Johns Hopkins University in Baltimore, Maryland. Wolfe started to follow subsistence hunters into the forest as they caught and butchered monkeys and other wild animals. As the hunters walked back with the catch over their shoulders, blood would run down their backs and into the cuts on their bare legs and feet. Wolfe thought that there was a high chance that viruses were passing between them, and he collected blood from the hunters and screened for retroviruses related to those found in non-human primates. There was precedent: researchers think that the retrovirus HIV was originally transmitted to humans from non-human primates in Central Africa, perhaps during hunting or butchering. In 2004, Wolfe showed that around 1% of those he tested had been exposed to a retrovirus called simian foamy virus, and sequence analysis showed that the virus had been passed into humans in the past from a mandrill, a gorilla and a type of monkey known as De Brazza's guenon 2 . The next year he reported the discovery of two new human retroviruses \u2014 called human T-lymphotropic viruses (HTLVs) \u2014 one of which is related to a group of simian viruses and both of which are likely to have crossed over from non-human primates 1 . Another member of this viral group, HTLV1, is also thought to have spread into humans and is associated with leukaemia and other conditions. Wolfe's studies suggested that certain viruses were passing from non-human primates quite freely and frequently into humans. \"Retroviruses are jumping over all the time,\" says Wolfe, even though \"only a few of them end up being significant\". Wolfe's work convinced him that the significant viruses could be identified and eliminated with proper surveillance of areas where cross-species transmission was a particularly high risk. Much of the challenge though, as other researchers have found, lies in getting funding to do the kind of large-scale field studies and genetic sequencing that this surveillance requires. Microbe discovery \"doesn't rate real well\" in funding applications that tend to focus on known threats, says Gregory Gray, who studies zoonotic infections at the University of Florida, Gainesville. It's hard to justify science \"when you don't even know exactly what you are going to encounter\", he says. Wolfe was helped by his timing, his scientific findings and his eloquence. In 2005, he secured one of the National Institutes of Health five-year, $2.5-million Pioneer Awards, and the next year a tenured position at the University of California, Los Angeles (UCLA). Soon he was building up a public profile, with speaking arrangements around the world. Interest in wildlife surveillance had been rising on the back of scares over West Nile virus, SARS and the H5N1 avian influenza. \"Nathan has been riding the wave of interest in the field,\" says William Karesh, director of the Wildlife Conservation Society's Global Health Program. \"It's like surfing, and the wave started getting really big,\" he says. \"And, you know, if you are on a big wave, you look better if you know how to surf well.\" Wolfe caught the eye of Google.org, which was looking to invest money and information-technology expertise in the field of emerging infectious diseases. The fact that Wolfe is \"media-genic\", was key, says Rijsberman. The parties discussed the collaboration for nearly a year before the $11-million funding deal was signed in October 2008. Wolfe had founded the GVFI the previous year, but the agreement \"made it real\" says Wolfe, and he left his position at UCLA to head the institute full time.  \n                Viral marketing \n              Part of Wolfe's magnetism for money and media seems to lie in the niche he has carved out for himself as an adventuring field virologist \u2014 going into the field to collect samples himself, rather than sending students or postdocs to do it. This also helps the science. Lipkin says that tissue or blood samples can easily become contaminated \u2014 plus samples are best frozen quickly. \"It has been difficult to collect high-quality materials and move them from remote sites in the developing world to laboratories,\" says Lipkin. \"Nathan is doing us an enormous service with these collections.\" Wolfe thrives on the difficulties involved in securing the right samples from people or animals. \"There is a lot of logistics in what I do, there's a political angle to what I do, and there's a tremendous amount of negotiation,\" he says. \"I love when people say: you simply can't collect specimens like that in China, or it is impossible to have a long-term program in the Democratic Republic of the Congo. I sort of enjoy that challenge.\" The GVFI now runs human and animal collection sites in the Democratic Republic of the Congo, Malaysia and China \u2014 some of these from the types of markets where humans and animals are cheek by jowl, sites that helped SARS and H5N1 emerge and spread. Wolfe narrows down his hunt by focusing on these 'hot spots' for emerging disease, along with pathogens (such as retroviruses) that have a history of making the jump. \"It's not like you are picking randomly from space what's going to be your next pandemic,\" says Wolfe, who maintains academic affiliations with Stanford University in Palo Alto, California, and with Johns Hopkins University. The Cameroon team is more than 30 strong, and has collected blood and tissue samples from more than 27,000 humans and 26,000 animals. With the re-vamped laboratory there, equipped with PCR machines to amplify DNA, and separate facilities for animal and human work to avoid contamination, Wolfe plans to boost the group's capacity to do molecular analyses on-site. But for the most part, he collaborates with world-class laboratories in Europe and the United States for his gene-sequencing work. Identification of a new virus is only the first step though: to spawn a pandemic, a virus has to adapt to humans and spread easily between them, something that can be examined by surveillance and modelling. Wolfe is monitoring the novel viruses he identified in bush-meat hunters to see if they are spreading. How to stop a potential pandemic, if Wolfe finds one, will depend on the case in hand, but might involve some combination of diagnostics, behaviour change and vaccines or treatments, if they could be found. Karesh points to the Republic of the Congo, where scientists found out that Ebola outbreaks were spread into humans through handling dead gorillas and chimpanzees. Communities were trained to avoid handling sick animals and there has barely been another outbreak since. Wolfe's group is already working on prevention programmes that might reduce transmission \u2014 such as encouraging hunters to wrap slaughtered animals in leaves when they are being transported. Wolfe has not yet identified a nascent pandemic, but he is finding out something about existing ones. Earlier this year, his team used samples from chimpanzees in Cameroon and the C\u00f4te d'Ivoire to shed some light on the origin of the human malarial parasite  Plasmodium falciparum.   Genetic analyses showed that all existing populations of  P. falciparum   originated from the chimp version,  Plasmodium reichenowi , and that the chimp parasite probably transferred across species between 10,000 years and 2\u20133 million years ago 3 . There is pressure on Wolfe to deliver much more. Only $5 million of the $11 million from Google.org and the Skoll Foundation came up front and the group is eligible for another $1 million every six months \u2014 if they earn it. They earned the first $1 million by collecting nearly 10,000 new samples and enrolling around 900 people in their studies in Asia. Rijsberman, who now sits on the GVFI board, helped the team to establish goals they call \"scientific home runs\": the origin of a major infectious disease or the identification of a new pathogenic virus circulating in human populations. With each home run, the group earns another $1 million. The group has submitted the malaria work to the Google.org board and hopes to hear in the next few weeks if it qualifies as their first home run. Wolfe, aware of the scientific competition, is careful not to say much about what his next million will be for. But he surely has a strategy to earn it. During a retreat to the coastal Cameroonian resort town of Kribi last month, Wolfe and his team were plotting their next scientific articles, making notes on papers and laptops. After a day of hard work, they sit down for drinks and take in the setting sun. Wolfe takes on his laboratory director in a game of backgammon, starting the game with an aggressive move. \"Nathan always has to do things differently,\" says Alberga, looking on. After a few more dice rolls, it's neck and neck. But, despite his nonchalance, Wolfe is constantly calculating probabilities \u2014 and in the end, he comes out on top. Anjali Nayar is an International Development Research Centre fellow at  Nature . Click  here  for a  Nature   video on Nathan Wolfe and virus hunting in Cameroon. \n                     Origins of major human infectious diseases \n                   \n                     Nature Video: Virus hunting in Cameroon \n                   \n                     Global Viral Forecasting Initiative \n                   Reprints and Permissions"},
{"file_id": "462840a", "url": "https://www.nature.com/articles/462840a", "year": 2009, "authors": [{"name": "Colin Macilwain"}], "parsed_as_year": "2006_or_before", "body": "A network of social scientists in the United Kingdom is seeking better ways to study the work of biologists. But, asks Colin Macilwain, can it earn its subjects' trust? There was something of a chill in the air at Cardiff's City Hall in October, and not just because autumn was arriving. Social scientists, and some life scientists, were gathering there for the annual meeting of the Genomics Network, a programme run by Britain's Economic and Social Research Council (ESRC) to stimulate dialogue between the disciplines. The plenary talks got under way, led by Christine Hauskeller, a philosopher with the network from the University of Exeter, and Martin Evans, winner of the 2007 Nobel Prize in Physiology or Medicine for his development of gene knockout technology. But when the initial call for questions sparked little real discussion, it was clear that dialogue was going to take some stimulating. Then Evans was asked what he thought of his hosts. \"They like to say, why are we doing things?\" he growled. \"We should be asking, why are they doing things?\" The Genomics Network started its work back in 2002. The previous year, when the British government had said it would earmark an additional \u00a3200 million (US$290 million in 2001) to genomics research, the ESRC successfully argued that about \u00a39 million of it should go to the investigation, by social scientists, of genomics issues and the scientists who study them. Seven years after its creation, the network, which supports about 100 researchers at five universities, is one of the largest projects of its kind in the world, and has broadened its interests beyond genomics to embrace synthetic biology and other areas. Two years ago, the ESRC announced a further \u00a318 million in funding for three of its centres after peer review. The money was for a second, and final, five-year term: the ESRC doesn't support permanent centres of excellence, on the grounds that societal challenges are always changing. The centres have a very broad scope. Cesagen, the Centre for Economic and Social Aspects of Genomics, is the largest of the groups, and is co-hosted by Cardiff University and Lancaster University. The University of Exeter is home to Egenis, which uses philosophy-based approaches to study genomics questions. Innogen, which studies innovation in genomics and the life sciences, is co-located at the University of Edinburgh and the Open University in Milton Keynes. And the Genomics Forum, also at Edinburgh, was set up in 2004 to help coordinate the Genomics Network and push its findings into wider political and public arenas.  \n                Embedded in the community \n              Unlike some previous attempts by sociologists to 'study' scientists at work, in these centres the social scientists are organized into multidisciplinary teams \u2014 often including lapsed natural scientists, as well as sociologists and philosophers \u2014 with funding to do empirical research. And the researchers embed themselves deeply in the community of natural scientists that they are seeking to study. Three-quarters of the way though the centres' ten-year lives, their track record is mixed. They have provided a stable and conducive environment for social-sciences research, much of their work is undoubtedly original and some of it has made its impact felt in policy circles, influencing debates on the legislation of animal\u2013human hybrid embryos, for example, and on innovation at the Organisation for Economic Co-operation and Development (OECD) in Paris. \"I think that we've really added value by having these centres,\" says Ruth Chadwick, the director of Cesagen and chair of the ethics committee at the international Human Genome Organisation. \"The ESRC made it clear from the outset that it wanted to see interactions with hard scientists. And we've seen increasing openness to such collaboration: in part because the funding agencies demand it.\" Well-trained social scientists can play a much bigger part in helping scientists to build bridges with the outside world, says Grahame Bulfield, former head of science and engineering at the University of Edinburgh and an early champion of the two centres there. \"We need organizations that will study the interactions between science and society in a scholarly way, and interpret their findings for the public,\" he says. But some critics fault the ESRC for failing to provide sufficient direction for the network since its foundation. Its staff case officer, Liz Grassby, is the fourth or fifth official to hold that position since the network was conceived. \"The quality of the research output has not been as good as you might have expected,\" said one senior social scientist from outside the network. \"Much of the problem can be laid at the door of the ESRC; for whatever reason, it has high staff turnover and no collective memory. The initiative could have been better managed.\"  \n                Chinks in the wall \n              The relationship between natural and social scientists has, historically, been more fraught than fruitful. Scientists are often prickly about being studied by outsiders such as sociologists or historians of science. Lately, however, some chinks have appeared in the wall that separates the two realms. Brian Wynne, a sociologist, former physicist and associate director of Cesagen, based at Lancaster, says that he has noticed profound changes over the past decade in the natural sciences' receptiveness to social science. \"Social scientists have become welcome, and indeed essential, partners with the natural sciences,\" he says. \"We've become embedded \u2014 like the media folks in the Iraq war,\" he says. Wynne, for example, has been involved in planning a citizens' science programme at the Natural History Museum in London, which seeks, among other things, to integrate huge banks of data collected by amateur naturalist groups into the museum's study of biodiversity. Scientists such as Johannes Vogel, keeper of botany at the museum, have built on Cesagen's work on public engagement to help draw on this amateur expertise. For example, the detailed observation of river conditions by fly fishermen led UK regulators to revise their criteria for measuring water quality. In a separate but related project, Vogel, Wynne and two Cesagen researchers, sociologist Claire Waterton and anthropologist Rebecca Ellis, have been contributing to the Barcode of Life initiative, which seeks to build tools that will enable biologists to identify species from short stretches of DNA. Here, the social scientists have been mediating between people who have their own genetic methods for species identification (such as public-health officials checking different strains of mosquito) and the larger international project, which needs global standards for genetic bar-coding. As a result, Wynne says, the whole project is embracing standards, such as on what gene segments to use, that better incorporate existing approaches. It was James Watson who pioneered the large-scale, systemic involvement of non-scientists in the life sciences when, as associate director for human genome research at the US National Institutes of Health in 1988, he casually suggested that about 3% (later 5%) of all Human Genome Project funds \u2014 about $10 million per year since then \u2014 should go to the investigation of the Ethical, Legal, and Social Issues (ELSI) involved in the Human Genome Project. The ELSI programme now serves as the dominant global model for this sort of social-science effort. But it also became synonymous with poor relations between the observers and the observed. From the scientists' point of view \"there were two main frustrations with ELSI\", says Robert Cook-Deegan, director of the Center for Genome Ethics, Law and Policy at Duke University in Durham, North Carolina. \"One was its association with what I call 'finger-wagging ethics',\" \u2014 telling researchers how they ought to conduct their business. \"The other was the way that it created a constituency that wanted grant money more than it wanted to go out and help solve real problems.\" Ros Rouse, the ESRC programme officer who built the Genomics Network and is now head of policy at Research Councils UK, the umbrella group for the seven UK research councils, says the ESRC wanted the Genomics Network centres to address \"a totally different terrain\", from the ELSI programme, including original research into the sociology of biology and medicine, and the nurturing of better links between science, the public and policy-makers. \"This is more of a serious attempt to engage with the life sciences than the original ELSI, which was seen as an 'add-on' to the genome project,\" says John Dupr\u00e9, a philosopher of biology who runs Egenis. Dupr\u00e9 is particularly interested in the way that scientists continue to work with a 'tree of life', the representation of species' relationships to each other in a branching tree, even though genomic data challenge it. Genome sequencing has shown that bacteria and other prokaryotes have swapped genes so extensively that their evolutionary histories cannot be represented on a conventional phylogenetic tree 1 . Dupr\u00e9 and his colleagues are now working with philosophers, evolutionary biologists and others to develop other means, such as webs or grids, to represent organisms' genetic relationships. Ford Doolittle of Dalhousie University in Halifax, Canada, is one of the biologists most closely involved. \"I think that it's been very useful because biologists don't think very much about philosophy,\" he says. \"We impose patterns on nature for philosophical reasons, and then deny that philosophy is important. Philosophers care about the structure of arguments and it is a very good exercise for scientists to start looking at this too.\" Researchers at Innogen have been examining what Joyce Tait, a former chemist and the first director of Innogen, calls the \"innovation triangle\" of the pharmaceutical industry, the regulators and the consumer. Tait and her colleagues say that the speed and nature of innovation in some sectors, such as pharmaceuticals and agricultural biotechnology, are determined largely by the regulatory apparatus. They argue, in particular, that it is not the severity of a regulatory system, but rather its ability to discriminate in how it treats large and small companies, that has the greatest bearing on rates of innovation. These findings are being absorbed in high places. The OECD, for example, used an Innogen study 2  about the future of the pharmaceutical industry as the basis for the health component of a report 3 ,  The Bioeconomy to 2030 , that it published in June. Given the OECD's considerable prestige, the report is likely to exert a strong influence on government approaches to drug regulation around the world.  \n                Broad reach \n              The ESRC always wanted the Genomics Network to reach out beyond academia to public and policy circles, and the Genomics Forum was added with this in mind. \"If you have a social-science centre, its prime emphasis is going to be on getting its best work published,\" says Steven Yearley, a sociologist and director of the forum. \"What the forum does is to accept this and make sure that we take that work to a broader audience.\" Research teams at the network centres, including Jane Calvert at Innogen and Emma Frow at the Genomics Forum, are involved in helping to shape UK participation in the field of synthetic biology \u2014 the search for approaches to build novel biological systems and even entire organisms. The concept of artificial life is expected to stir strong public passions, and those studying it have turned to social scientists from the outset. Some argue that if synthetic biology is to get off the ground, the working dynamic between biologists and engineers needs to be examined and improved. Calvert and Frow have been studying some of these questions, and helping scientists and organizations such as the Royal Academy of Engineering in early public consultations on synthetic biology. Alistair Elfick, a medical engineer at the University of Edinburgh, is joint leader of a UK-wide synthetic-biology network that is looking at the technical standards that may be needed if approaches to synthetic biology \u2014 such as the design of DNA 'biobricks' that can be pieced together like Lego \u2014 are to be successfully pursued. He says he appreciates the social scientists' perspective. \"Having their insight will be hugely valuable to us,\" he says, adding that the nascent discipline is serious about working with social scientists on public engagement. \"It's a matter of entering into a dialogue with the public about what it wants us to do. We need to have the authority of society, in order to proceed.\" But does the social study of science need to have this kind of practical utility? Some social scientists caution that producing work that is useful to policy-makers, scientists or the public \u2014 as the Genomics Network has set out to do \u2014 is not always consistent with their core scholarly activity, of seeking to better understand how science works. Paul Martin, a medical sociologist at the University of Nottingham who is not part of the network, warns that the desire to make work fit the needs of policy-makers can create conflicts of interest, in which those who are trying to objectively study science and innovation end up being part of the scientific and innovative process. \"We want to be engaged, but not reduced to a handmaiden's role for new technologies,\" he says. Nik Brown, a sociologist at the University of York who is also outside the network, worries that policy-makers and scientists can expect the wrong things from social scientists. \"There's a misperception that our main role is to ease the interaction between scientists and the public,\" he says. \"What we want to do is understand the science, and how it is constructed \u2014 which is not a public-understanding-of-science question.\" The biggest practical challenge for the network, though, is the interactions between the scientists and social scientists themselves. One senior researcher, who knows the Genomics Network well, said privately that social scientists still have difficulty speaking a language that scientists can relate to. \"There's still a huge credibility gap,\" according to this observer. \"Their methodologies just don't align well.\" The issues that interest social scientists most are not always the top concern of many research scientists, who are busy with funding, publications and the science itself. \"Scientists don't especially want to know whether their work produces an ethical dilemma,\" says Tony Woods, head of medicine, society and history grants at the London-based Wellcome Trust, the medical-research charity. \"Questions about societies' concerns don't always weigh heavily on their minds.\" Asked whether the network has succeeded as whole, Jim Stevenson of the University of Southampton, a psychologist and member of the ESRC's strategic research board, equivocates somewhat: \"I think now it is working well, but it has taken a while.\" He says that the centres need to persevere. \"We've got to get to a situation where science, and industry, takes this work seriously,\" he says. \"But it's a slow process.\"   See Editorial,  \n                     page 825 \n                   . \n                     Nature Human Genome collection \n                   \n                     ESRC Genomics Network \n                   Reprints and Permissions"},
{"file_id": "461866a", "url": "https://www.nature.com/articles/461866a", "year": 2009, "authors": [{"name": "Alison Abbott"}], "parsed_as_year": "2006_or_before", "body": "Neurosurgeons have unparalleled access to the human brain. Now they are teaming up with basic researchers to work out what makes it unique, finds Alison Abbott. Two years ago, a top salesman in a major Italian engineering company drove himself to the emergency department of the San Matteo Hospital in Pavia. He was frightened. He could think clearly and he could move his hand normally, but for the past few days he hadn't been able to write. The business e-mails he had confidently typed out on his computer made no sense when he read them back. And he found he was unable to write a simple word by hand. Tests at the hospital showed him to be apparently fit. He could express himself without problem, read, spell words out orally letter by letter \u2014 and he could even draw simple objects. The doctors started to wonder whether they should call in a psychiatrist. But when a brain scan showed what seemed to be a tiny tumour, they called neurosurgeon Lorenzo Magrassi instead. Magrassi immediately prescribed drugs to treat the swelling, which was pressing on surrounding brain areas, and within a couple of days the patient was able to write again. Magrassi was intrigued. He had not heard of a similar case of agraphia \u2014 the inability to write. So when he operated the following week to remove the tumour, he also undertook a little research. The patient, who was in full agreement, was asked to speak while Magrassi stimulated his brain with an electrode. He was also provided with paper and a pen to jot down dictated sentences until the surgeon found just the spot that controlled writing. \n               Click here for larger image \n               Magrassi's techniques were not new: brain surgeons frequently stimulate around a tumour or other diseased tissue that they plan to remove while a patient is awake and able to answer questions. This 'functional brain mapping' allows them to identify the areas involved in speech and other functions that they would prefer not to cut into (see 'Open brain biography'). What is new is the way that neurosurgeons are using these techniques for bolder exploratory brain mapping, often in collaboration with basic researchers. In short, the surgeons have a unique opportunity to access and stimulate the human brain, and, as technologies improve, more of them are starting to use it. \"Neurosurgery can contribute to neuroscience by giving a glimpse into the mind, a rare window into the brain,\" says surgeon Itzhak Fried from the University of California, Los Angeles. \"We can start to look at uniquely human abilities.\" Some of the collaborations springing up are tackling aspects of humanness \u2014 such as consciousness or language \u2014 that are of long-standing interest to neuroscientists and of practical relevance to neurosurgeons. Magrassi emphasizes that his probing was not purely experimental, but has application for future surgery. \"It is often as clinically important to preserve writing after surgery as it is to preserve speech, and we need to understand which areas to avoid,\" he says. Nonetheless, brain science has benefited. Over the next few months, and again with informed consent, Magrassi repeated his mapping activity in two other patients with brain tumours even though they had not shown writing difficulties. He found that the area required for writing was always in the same spot in a structure called the superior parietal gyrus on the left side of the cortex. His results \u2014 yet to be published \u2014 extend theories about the neural circuitry of writing, a quintessentially human activity. Writing seems to co-opt brain structures in areas involved in language, visual processing and facilitating movement, and lesions in some of these areas can drastically interfere with writing ability. Normally these deficits are accompanied by others, such as an inability to speak, read or move normally. Since stimulation of the 'writing spot' did not interfere with these, Magrassi's studies suggest that the linguistic and motor circuits involved in writing are more integrated with each other than previously thought. Exploratory brain studies are not yet commonplace. It can be difficult to experiment in the operating theatre with the active participation of patients, who are stressed and distracted. But in some circumstances patients' brains can be available for days after the surgery. When neurosurgeons implant electrodes into the subcortical part of the brain to treat Parkinson's disease with deep-brain stimulation (DBS), they sometimes leave the leads outside the skull for a day or two to make sure that the procedure was successful before connecting them to a stimulator and battery and sewing everything under the skin. In clinical trials of patients with psychiatric disorders, neurosurgeon Volker Sturm, at the University of Cologne, Germany, leaves the electrode leads hanging out even longer \u2014 up to four days \u2014 to provide time to experiment. Sturm is extending DBS treatment to disorders such as major depression, obsessive-compulsive disorder and alcoholism. The operation involves placing electrodes in a small area within the nucleus accumbens, a tiny structure that has a fundamental role in reward and is easily hijacked by addictive substances. Many of the tests that Sturm's neuroscience collaborators perform while the electrode leads are exposed have direct clinical relevance. Some, for example, are designed to investigate the effect of stimulation in the nucleus accumbens on neurons' electrophysiological properties and whether these properties can help to predict any psychiatric benefits that might later emerge. Others look like experiments in basic brain science. In a study published last June 1 , Sturm and his collaborators investigated what happens in the brain when patients change strategies to fit new circumstances. A major function of the nucleus accumbens is to assess the value of a reward and adapt behaviour in response. The group analysed data from six patients fresh from surgery. A day after the electrodes were implanted in their left and right nuclei accumbens, the researchers recorded local field potentials around the electrodes. They also used electrodes placed on the scalp to record neurophysiological oscillations in the medial frontal cortex, which has direct connections to the nucleus accumbens. The patients then did a simple gambling task in which they had to guess which side of a coin would pay out a cash prize. When the experimenters changed the odds of winning, the patients changed their gambling strategies \u2014 and the synchrony of electrical oscillations in the various brain areas also changed. The team concluded that a neural network between the nuclei accumbens and medial frontal cortex activates rapidly when the patients adjusted their behaviour to fit the new facts. \"Yes, it is basic science,\" says Michael Cohen, a member of the collaboration and a psychologist at the University of Amsterdam, \"but it may eventually have clinical relevance.\" For instance, he says, surgery might be avoided if the nucleus accumbens can be modulated indirectly via the medial frontal cortex, which can be stimulated through the skull. New surgical procedures for epilepsy offer a particularly tantalizing opportunity for neuroscientists, because they allow access to the cortex \u2014 the region where thinking goes on \u2014 and the opportunity for very precise recording. Some particularly brutal forms of epilepsy that don't respond to drug treatment originate in the medial temporal cortex and can be cured by surgical removal of the region responsible. Neurosurgeons identify the exact focus by implanting a handful of electrodes around the medial temporal cortex, waiting for the patients to have a spontaneous seizure and then determining the origin of the epileptic activity. They typically use 1-millimetre diameter electrodes that pick up the average signal from some one thousand to one million neurons. But more teams are starting to use 'microelectrodes' too. This technique involves fixing eight very fine recording wires, with diameters of less than 50 micrometres, to the end of the larger electrodes like wiry spiders. Whether the microelectrodes have further clinical benefit remains unknown, but it is not thought to do any harm, and for research it allows many levels of electrical signals to be recorded, from the oscillations in large populations of cells down to firing in single cells. Fried has been using the electrode systems since the 1970s and is the acknowledged pioneer of this technique in research. In the past half-dozen years, strides in data analysis have allowed faint and infrequent signals from single cells to be extracted from the noise. These are the sort of signals that excite those interested in human consciousness because they reflect rare or subtle events \u2014 such as recognizing the face of a celebrity. Rodrigo Quian Quiroga, a physicist-turned-neuroscientist and one of Fried's collaborators, hit headlines around the world in 2005 when he and his colleagues unveiled their concept of the 'Jennifer Anniston neuron' 2 . Quian Quiroga came to the California Institute of Technology (Caltech) in Pasadena as a postdoc in 2001 mainly for the opportunity of recording from single neurons in patients with epilepsy who were being operated on by Fried. He was particularly interested in how the human hippocampus, part of the medial temporal lobe, is involved in recognition of people or objects. Recording from an array of 64 individual electrodes in a brain can generate hundreds of gigabytes of information a day. So Quian Quiroga made it his first task to find a better way to sort through the vast dumps of data. Having a background in physics helped, and by 2003 he had devised a neat algorithm that could sort through the tangled electrical signals and unambiguously identify spikes from single neurons. In one step he raised the entire game. He tackled the recognition question by following the activity of single neurons while patients looked at hundreds of pictures on his laptop \u2014 from the actress Jennifer Aniston to the Sydney Opera House. Each neuron typically fired to one concept, but it did so rather flexibly: the Jennifer Aniston neuron would fire to different pictures of the actress but not to other celebrities. In some patients, Jennifer Aniston neurons would also fire to other actresses in  Friends , the popular television series in which she starred. But they would never fire to other similar-looking, but otherwise unconnected, actresses. Quian Quiroga, now at the University of Leicester, UK, recalls his amazement when he found a single neuron in a patient firing in reaction to himself, even though they had met just a couple of days before. He went on to show that these neurons fire only if the patients consciously recognize the pictures 3  and that they would also fire if a person was told the name of a person or object 4 . \"These neurons in the hippocampus encode information in a very abstract way, whatever type of sensory information leads to it,\" says Quian Quiroga. \"This makes sense given that long-term memories are stored as abstractions \u2014 'Jennifer Aniston as a concept' \u2014 and we tend not to remember details like what her hair looked like.\" With these types of revelation emerging, it is perhaps not surprising that other epilepsy neurosurgeons are getting in on the act, starting to use microelectrodes and forge imaginative collaborations with neuroscientists, particularly now that the sophisticated components can be bought commercially. It is a slow business though. One reason is the scarcity of patients who undergo this sort of neurosurgery. Even Fried's large centre operates on barely a dozen such people per year. Another is that it takes time to develop the expertise and build up a team. Neurologist Mark Richardson from King's College London, acknowledges that two years after implanting microelectrodes into his first patients, he is still very much at the beginning when it comes to research. \"It takes a very long time for things to become routine,\" he says. Neurosurgeon Adam Mamelak of the Cedars-Sinai Medical Center in Los Angeles, California, has built up collaborations with neuroscientists at Caltech. One such effort explored people's ability to detect 'novelty', something fundamental to many types of learning. The researchers identified some neurons in the hippocampus and amygdala that increase their rate of firing when the brain is confronted with new images, and some that do so when confronted with familiar images 5 . In another Caltech collaboration, he is looking at conscious decision-making in gambling tasks with neuroeconomist Antonio Rangel. \"Animals can't perform these sorts of tasks,\" he says. A big advantage of animals, though, is that the experimenter can decide where in the brain to place the electrodes. In patients, electrodes must be positioned strictly according to clinical need. \"The task of experimenters is to work out what we can do, given a particular distribution,\" says Quian Quiroga. \"Patients always come first and we don't do anything with them if they are tired or have visitors.\" All experiments are carefully regulated by ethics boards. Neurosurgeons say that patients nearly always agree to participate \u2014 and enjoy it. \"There is absolutely no risk for them, and they are just sitting around getting bored,\" says Mamelak. \"They take pride in contributing to the field of brain science,\" adds Fried. Fried says that his collaborations with Qian Quiroga and others have allowed him to follow a line of research on memory. \"Everything that you will consciously remember will have to be processed in the hippocampus,\" he says, adding that such studies could eventually help guide epilepsy surgery in the medial temporal lobe, \"where memory networks sometimes overlap with epilepsy networks\". Fried says that some neurosurgeons wonder whether it would actually be unethical not to experiment, given how little is known about the brain entrusted to their hands, and given that the opportunity is there. The potential to learn about the neural basis of humanness \u2014 so that they can try to preserve it \u2014 is an opportunity that few want to pass up. \n                     University of Leicester NeuroEngineering lab \n                   Reprints and Permissions"},
{"file_id": "462843a", "url": "https://www.nature.com/articles/462843a", "year": 2009, "authors": [{"name": "Elie Dolgin"}], "parsed_as_year": "2006_or_before", "body": "Dedicated scientists are working hard to close the gaps, fix the errors and finally complete the human genome sequence. Elie Dolgin looks at how close they are. From her windowless fifth-floor office at the US National Institutes of Health in Bethesda, Maryland, Deanna Church has few distractions from the job that lies before her. On her computer sit 888 open 'tickets', or outstanding problems with the human genome sequence. Although that number fluctuates, it's a not-so-subtle reminder that she and her team at the National Center for Biotechnology Information (NCBI) have a long way to go to finish the job started nearly two decades ago by the Human Genome Project. This is the same project that an international team of scientists spent close to US$3 billion on to complete. In 2000, the scientists announced, to much fanfare at a White House ceremony, that they had finished the draft sequence of the human genome. They waxed poetic about opening 'evolution's lab notebook' when they published the draft the next year 1 . And they uncorked champagne bottles again in 2003 when the sequence was officially deemed finished 2 . By then, media outlets were reporting the developments with a twinge of fatigue. \"This time it is the real thing, scientists promise,\"  New Scientist   reported. Another year passed before the final analyses were published 3 , and two more went by before the paper detailing the last, fully polished chromosome came out in 2006 (ref.  4 ). Still, three years later, Church is hunched over her computer, clicking away at her mouse, quietly clearing up the lingering troubles with the iconic sequence. Some of her tickets, submitted by her collaborators and users from around the world, are reports of missing bits. Others describe stretches in which someone thinks the sequence is mistaken. Still others are unique and unexpected challenges, such as complex DNA rearrangements, that could take years to sort out. \"It's a frustration,\" says Richard Gibbs, director of the Human Genome Sequencing Center at Baylor College of Medicine in Houston, Texas. \"It's an extremely high-quality genome. It's the best there is, period. The problem is that a very small percentage of uncertainties still translates into a significant number of problems.\" Church and her colleagues are working to build a solid, accurate reference, but their efforts have revealed how slippery that concept can be. The sequence, for instance, does not represent any one person's genome. It is an amalgam of DNA from different people, both male and female. It was put together this way to maintain anonymity for those who contributed the DNA and to ensure that the sequence represented all humanity \u2014 \"our shared inheritance\", as then-head of the project, Francis Collins, said. But that shared inheritance is hard to capture. The genomes of two individuals look less alike than many had originally assumed. Rather than following a linear path of 3 billion base pairs with a letter changed now and then along the way, human genomes detour into hundreds of vastly different stretches in which, depending on the individual, millions of base pairs can be deleted, inserted, repeated or inverted. A finished reference genome \u2014 if attainable \u2014 will therefore look very different from the project's first renditions. That's where Church and her team of finishers come in. They are striving to smooth out the differences and to develop a more dynamic platform that can capture much of humanity's commonalities and uniqueness. Some say it's a wasted effort now that individual human genomes can be sequenced at a fraction of what it cost ten years ago, but most say the reference is invaluable as a bedrock to support the sequencing of future human genomes. Resolving the problems in the sequence will not win Church many accolades. She won't meet the president or land any papers in high-impact journals as those who \"finished\" the genome before her did. And once she puts a ticket to rest, there's always another one waiting. \"It's not sexy,\" she says. \"But it's important.\"  \n                A coalition of the responsible \n              By April 2003, the sequencing had surpassed the international project's technical definition of completion \u2014 the sequence contained fewer than 1 error per 10,000 nucleotides and covered 95% of the gene-containing parts of the genome. But there were still errors \u2014 around 350 gaps in the sequence \u2014 and much of the structural variation was not included. In 2004, Church and a few dozen researchers met to discuss genomics and structural variation at the Wellcome Trust Sanger Institute in Hinxton, UK. One complaint was echoed repeatedly: there was no easy way to fix or update the genome with new data. In the 1990s, when sequencing was in full swing, researchers could contact the specific chromosome curators at each of the major sequencing centres involved with the project to report any sequencing slip-ups. But by 2004, few of the centres involved were actively monitoring their slices of the genome and there was little scientific impetus to revisit old work. This posed a problem. \"Someone needs to have responsibility for the genome so that if errors are found, improvements can be made,\" says Adam Felsenfeld, a programme director at the National Human Genome Research Institute (NHGRI) in Bethesda, Maryland. \n               boxed-text \n             Together with Ewan Birney of the European Bioinformatics Institute (EBI) in Hinxton, Church appealed to the NHGRI and the Wellcome Trust for funding. It took more than two years of meetings and deliberations, but eventually the NHGRI agreed to set aside up to US$1 million in operating funds from an annual large-scale sequencing award (more than $30 million per year) to Washington University in St Louis, Missouri. Sanger and the Wellcome Trust agreed to a similar amount, and the EBI and NCBI handle the informatics as part of their normal operations. The collaboration, known as the Genome Reference Consortium (GRC), is now the epicentre for genome improvement. To improve the reference, the GRC is concentrating on three main goals: to correct assembly errors; to fill in the genome's remaining gaps; and to produce alternative sequences for regions of the genome with extensive variability. Researchers have had the first two objectives on their agendas since the Human Genome Project was concluded, and they have been chipping away at them ever since. Some of the regions have been particularly difficult to polish off. For some repetitive stretches, for example, researchers struggled to make multiple copies in bacteria \u2014 a necessary part of the sequencing process. But newer methods are now allowing them to fill in these pieces of the genome. Earlier this year, a team led by Chad Nusbaum, co-director of the Genome Sequencing and Analysis programme at the Broad Institute in Cambridge, Massachusetts, used a next-generation sequencing technology that does not require bacteria to amplify the DNA 5 . Nusbaum's team then handed the sequences over to the GRC, which incorporated them into the reference assembly. The third goal reflects something that has only recently come to light. At first, researchers assumed that genetic variation between people largely consisted of differences in single DNA letters. Now, however, they better understand the extent of structural variations \u2014 including deletions, insertions, duplications and inversions. Although some of these variants are involved in heritable disease, they are much more difficult to keep track of than single-letter differences because they often don't map easily to the reference. So instead of representing the genome as a single path of three billion letters, the GRC is introducing alternative paths to reflect its diversity.  \n                Twists and turns \n              One such region is the major histocompatibility complex (MHC) \u2014 a 4-million-base-pair stretch of chromosome 6 that contains many immunity-related genes and is recognized as one of the most variable slices of the human genome. The original reference sequence was a hotchpotch of multiple blocks of DNA, called haplotypes, taken from several donors, so the sequence that resulted didn't actually exist in any real person. To create a reference with clearer origins, a team led by Stephan Beck of the University College London Cancer Institute sequenced a single MHC haplotype. They then compared it against seven other common European haplotypes and discovered more than 37,000 single DNA letter differences and around 7,000 structural variations \u2014 a level of genetic diversity about an order of magnitude greater than the genomic average 6 . Beck's team's reference has now been swapped into the GRC's default sequence and the other seven haplotypes are included as alternative pathways. Two other regions also have substitute haplotypes. One sits on chromosome 4 around the gene encoding the UGT2B17 enzyme, which metabolizes steroid hormones and many drugs. The 'finished' reference had misassembled two haplotypes, introducing a false gap. A corrected assembly found that the 'gap' was actually a deletion found only in some people, flanked by large duplications. That section is now included as an alternative pathway in the GRC reference. The other region, on chromosome 17, encompasses the  MAPT   gene, and provides a case study of the limits of the original reference sequence, which consisted of only one haplotype. An alternative haplotype, a complex inversion of the first that is found in 20% of Europeans, was shown in 2005 to correlate with larger family sizes, suggesting that this second haplotype was under some form of positive selection 7 . But in 2006, Evan Eichler 8 , a geneticist at the University of Washington in Seattle, and two other groups 9 ,   10  showed that the inverted region was also prone to frequent spontaneous deletions that led to mental retardation. The inverted haplotype seemed to be both adaptive and the source of debilitating deletions. \"You have a yin for the yang here determined by genomic structure,\" Eichler says. \"The question was, 'What's going on?'\" To answer that question, Eichler needed the sequences. He teamed up with Michael Zody, chief technologist of the Genome Biology Program at the Broad Institute, to resequence the whole region and showed that the architecture of the inverted haplotype predisposed the sequence to undergo deletions associated with mental retardation 11 . By the time Eichler and Zody published their results, in 2008, the GRC was already in full swing and gearing up to release the next build of the genome. The researchers handed over their sequences to the consortium, and both haplotypes were included in the reference sequence. \"The GRC provided a central clearing house for us to go through,\" Zody says. Given the clinical relevance of these and other complex regions, providing multiple references is essential to detect the mutations underlying many diseases, says Eichler. \"Once we get the alternative structures worked out, I believe we'll be able to make disease associations that were previously impossible,\" he says. Eichler estimates that around 5% of the genome \u2014 corresponding to around 400 specific locations \u2014 will need alternative sequences to provide a platform that adequately captures the spectrum of human diversity. These regions include more than 1,000 genes that affect a wide range of physiological processes, including immune responses, drug detoxification and reproductive ability, he says.  \n                A common task \n              The GRC's first public offering \u2014 a more accurate version of the human genome \u2014 rolled out online in March 2009, updated the three parts of the genome with the alternative assemblies, corrected more than 150 alignment problems and closed 25 sequencing gaps. But that still leaves more than 300 gaps. In September, 20 of the GRC's core members gathered in Hinxton for the group's twice-yearly meeting to discuss what steps to take next. While lab workers were clacking away on their keyboards, bioinformaticians were working through one of the consortium's most contentious issues \u2014 how to change the reference genome to display only the 'common' gene variants. The GRC's nine-member scientific advisory board, which includes Eichler and Gibbs, has recommended that, wherever possible, the genome should include the common versions of the DNA sequence. But it hasn't defined what 'common' means. Should it be the highest-frequency variant or something that is shared by a reasonable proportion of the population? Should it be calculated across the world's six billion-plus residents or just in a particular ethnic or geographic group? Results emerging from the 1000 Genomes Project, a major international sequencing effort to catalogue human genetic variation in around 2,000 people from across four continents, should inform their decisions. Some of the GRC members disagree with making such fundamental changes to the reference sequence. \"I don't think we should be going through and flipping single bases throughout the genome,\" says Paul Flicek, who leads the vertebrate genomics group at the EBI. \"Informatically, it just doesn't matter. As long as it works, I think it's okay.\" Others outside the GRC question whether the entire project is justified. Why bother tinkering with a decade-old reference, asks Lincoln Stein, a bioinformatician at the Ontario Institute for Cancer Research in Toronto, Canada. He calls the effort \"more of an abstract exercise than one that's going to have a practical impact\". Church, for her part, waves off such criticisms as being from those preoccupied with large-scale genomics. As a detail-oriented person, she knows that the little things count. Individual investigators love their pet genes. That's one reason her queue of tickets is always full. And as genomics increasingly moves to the forefront of personalized medicine, many regions of clinical utility might slip through the cracks. For researchers interested in a particular disease-relevant locus \"it doesn't matter that the genome may be 99% complete\", she says. \"If they're [working with] a region that's incomplete and wrong, they're screwed.\" And so the GRC continues with its quiet quest, crossing Ts and sometimes changing them into As, Cs or Gs. Until a reference is no longer needed to assemble the DNA coming from current sequencing technologies, it will continue to document the evolving understanding of human variability in the reference genome. The GRC has also taken on the mouse sequence and will take responsibility for the zebrafish sequence in 2010. Although it may not capture headlines, most in the research community recognize its worth. \"The GRC has exactly the right idea,\" says Jonathan Sebat, a geneticist who studies structural variation at Cold Spring Harbor Laboratory in New York. \"It's a no-brainer that someone would be needed to clean up the mess.\"   See Editorial,  \n                     page 825 \n                   . Elie Dolgin is assistant news editor for  Nature Medicine   in New York. \n                     Nature's Human Genome Collection \n                   \n                     Personal Genomes Web Focus \n                   \n                     The Genome Resource Consortium \n                   \n                     Eichler Lab \n                   Reprints and Permissions"},
{"file_id": "4601071a", "url": "https://www.nature.com/articles/4601071a", "year": 2009, "authors": [{"name": "Brendan Maher"}], "parsed_as_year": "2006_or_before", "body": "Some diseases defy diagnosis. Brendan Maher meets two people who hope that the US National Institutes of Health can help. Dunham Aurelius is eager to take his shirt off and show his scars. One, a centimetre wide and roughly 20 long runs up his lower back and is from the placement of a steel rod to straighten his spine at the age of 14. Two others, looking like bullet wounds, are above his left buttock. But it's not his scars, nor his barrel-chested physique that have earned him the nickname 'ultimate fighting champion'. His urologist bestowed that title because of the fact that since the age of 22, Aurelius has passed a dozen and a half kidney stones \u2014 many, he's proud to say, without assistance. Aurelius is 39, a sculptor and a former triathlete with curly blond locks and a surfer's drawl. His wife, Michelle Barry Aurelius, jokes that he's like a human oyster. But the stones he grows are no smoothened pearls. At the cinema in 2008, Aurelius stepped out to use the bathroom. When he returned, he handed her the four-millimetre wide 'barnacle' of calcium phosphate his body had just expelled. She had noticed he was quiet that evening. On a February morning this year, Aurelius and Barry are waiting in a hospital room in the sprawling Clinical Center on the campus of the US National Institutes of Health (NIH) in Bethesda, Maryland. They have travelled here from their home in Santa Fe, New Mexico, so that a small team of clinicians and research scientists can try to diagnose the mysterious disease that has dealt Aurelius more urological pain than most should have to bear. When William Gahl, the team's lead investigator and clinical director at the NIH's National Human Genome Research Institute (NHGRI) enters Aurelius' room at 9:50 a.m., he has a gaggle of clinical-genetics fellows in tow. He rattles through an introduction to Aurelius, and then stops himself. \"Why don't you tell us,\" Gahl says, picking his words carefully, \"why are you different from the average person?\" Elsewhere in the Clinical Center, another far-from-average person is awaiting time with Gahl. Sally Massagee, a 54-year-old certified public accountant from Hendersonville, North Carolina, is watching a neuromuscular specialist remove three deep-red slivers of muscle from her bicep. Although she was nervous going into the biopsy, Massagee jokes that she can spare the tissue. A little more than a decade ago, she started putting on weight. By the spring of 2007 she had gained nearly five kilograms on a compact 1.68-metre frame \u2014 all of it muscle. People around her thought she was training for competitive bodybuilding, but pain stopped her doing any exercise but tennis. Eventually she outgrew her clothes. Massagee and Aurelius have few, if any, symptoms in common. What they do both have is a spot in the NIH's Undiagnosed Diseases Program, an effort to identify and characterize previously unknown diseases by drawing on the institution's 6,000 clinical and biomedical experts and the medical technologies at their fingertips. Gahl, a medical geneticist specializing in metabolic disorders, started the programme in May 2008 with $280,000 in pilot funding from the NIH's Office of Rare Diseases. It received $1.9 million more in its first year, and has been approved as a fully fledged NIH programme at $3.5 million per year for the next five. Patients such as Aurelius and Massagee hope that this financial and academic wealth can finally provide the diagnosis that has eluded all the other specialists they have consulted over the years. \"There's nothing so complicated for a patient as not being able to put a name to their disease,\" says Carl May, a professor of medical sociology at Newcastle University, UK, who has studied doctor\u2013patient relationships in chronic disease. \"If we can't put a name to it, it's hard for others to see or understand, and most importantly to believe, that something is legitimate or warranted.\" The researchers want a diagnosis, too \u2014 but their motives are somewhat different. For them, Aurelius, Massagee and the other individuals are also a research opportunity, the chance to discover a new disease and potentially one that can be characterized at a genetic level. This could provide a new foothold in understanding human biology and perhaps the origins of other, more common diseases. Such diagnoses can result in high-profile publications and spur the development of new fields. As Clemens Bergwitz at the Massachusetts General Hospital in Cambridge puts it, the programme allows scientists to \"make use of the human mutation pool\". That approach is not new: throughout much of medical history clinicians with just the right background have stumbled on just the right patients to come up with a new diagnosis. Gahl says that one way of thinking about the Undiagnosed Diseases Program \"is to reduce that need for serendipity\" by setting out to find the unusual cases and throw at them everything research has to offer, including individualized sequencing of candidate genes and a genome-wide scan of genetic variations. \"The sort of modern twist to this classic approach is the molecular-biology techniques available,\" says NIH endocrinologist Michael Collins, who has been working on Aurelius's case. Aurelius and Massagee spotlight the relationship between subject and scientist at its most focused, modern and expensive. The question is: what, if anything, will each side gain?  \n                Stony symptoms \n              Aurelius reclines in a hospital bed while Gahl runs through his medical history. The stones are generally calcium phosphate. The largest two-to-three-centimetre stone was removed by surgery, which resulted in a perforated colon and left Aurelius with the bullet-hole scars in his back. He has regular gastrointestinal discomfort, and Aurelius says that he has very high calcium and vitamin D levels in his blood. Gahl asks how calcium in food affects him. \"I avoid it in most cases. It makes me feel distended. If I had a bowl of ice cream I'd be miserable.\" \"Ever take a vitamin D pill?\" Gahl asks. \"No, but we could try it!\" says Aurelius. Like others involved in the programme, Aurelius is happy to be a part of it even though he knows the chances of a diagnosis, let alone a treatment, are low. It means he hasn't been given up on. \"Most doctors would throw their hands up,\" Aurelius says, describing his quest over the years. Gahl, no enemy to truth in levity, replies: \"We may too, but we'll do it behind closed doors, after you're gone.\" After Aurelius is sent off for a bone-density scan and an ultrasound on his kidneys, the closed-door discussion begins. Gahl meets with a group of experts working on the case: Collins, nurse practitioner Colleen Wahl, pathologist Panagiota Andreopoulou, and attending genetics fellow Galina Nesterova. The meeting moves quickly as they bandy about the names of genes that might be responsible for Aurelius's unusual blood test results. At first the talk centres on a protein called fibroblast growth factor 23 (FGF23), which lowers phosphorus absorption and, through a series of regulatory loops, helps to cap the production of active vitamin D. Too much vitamin D can result in high calcium levels in the blood and urine \u2014 hypercalciuria \u2014 which leads to kidney stones. Nesterova, who worked as a nephrologist in Russia before coming to the United States, has another idea. She suggests looking at two genes,  CYP27B1   and  CYP24A1 , that activate and deactivate vitamin D, respectively. Her hunch is based in part on some ongoing work with two sisters with elevated vitamin D levels and highly calcified kidneys, who she also suspects of having mutations in these or related genes. Collins is sceptical, not wanting to close the door on alternative explanations. Aurelius has low blood phosphorus levels, and Collins wonders whether the underlying problem could lie in the kidneys: if they are excreting too much phosphorus, this would feed back to the body, instructing it to manufacture more active vitamin D. He posits a mutation in one of the genes for a sodium/phosphate transporter, called  SLC34A3 , in the kidney tubule wall that may be causing Aurelius to dump out phosphorous. \"It's neat that in-house, we've got Mike,\" says Gahl later, referring to Collins. \"He knows 100 times as much about this as me.\" Collins was first introduced to Aurelius's case through a monthly meeting for the Undiagnosed Diseases Program in which upwards of 50 basic- and clinical-research scientists sit and listen to presentations on potential patients to decide who should be invited into the programme. During the screening process they try to pull out diseases with simple genetic roots, ones probably caused by a single mutated gene, or a deletion or duplication of a large chunk of DNA. Maybe there is a mention of a family history in a chart, or signs that the symptoms are related to an organ system in a way that suggests a single unifying cause. Complex disorders with roots in multiple genes have to take a back seat, says Gahl: \"This is triage, and triage means you go after what you can do.\" Since its inception, the programme has received more than 2,100 inquiries from doctors around the world, reviewed more than 900 full applications and, so far, seen little more than half of the 160 patients that, like Massagee and Aurelius, have been invited to come for an intensive week of tests and consultations. As Nesterova and Collins spar collegially, Gahl finds a possible way to settle the debate. \"Let's get the FGF23 back and have a plan then,\" he says, referring to a test already requested that will measure the amount of the hormone in Aurelius's blood. A high concentration might change the types of genes that they would test, probably striking  SLC34A3   from the list. Less than an hour after talking vitamin D metabolism, Gahl is discussing muscle physiology and genetic tests he hopes to run on Massagee's blood. Like Aurelius, Massagee has already been through three and a half days of testing and has just a few more appointments to go. One of these is a wrap-up interview with Gahl and a different team of specialists, including Justin Kwan, the doctor who did the muscle biopsy, and clinician Irini Manoli from the NHGRI. Accompanied by her husband, Massagee is dressed in velour tracksuit pants and an oversized man's dress shirt. She looks fatigued, but alert, her brown eyes peeking out over her puffy, muscle-tightened cheeks. When in 2008 doctors at Duke University Medical Center in Durham, North Carolina, did a magnetic resonance imaging test to try to diagnose her condition, they were shocked to see that even the orbital muscles that control her eye movements seemed to have doubled in size. Massagee says that when she had been exercising, the muscles had tone, and she believes many of the specialists she saw suspected she was taking steroids. Now the muscles are rock-hard, painful and toneless, weighing her down and leaving her exhausted. Massagee's case has also drawn in experts from across the NIH, including Alexandra McPherron of the National Institute of Diabetes and Digestive and Kidney Diseases. For her PhD thesis, McPherron characterized a protein that doubled the skeletal muscle of mice when it was mutated ( A. C. McPherron, A. M. Lawler and S.-J. Lee Nature  387, 83\u201390; 1997 ) and she's worked on it ever since. Later named myostatin, the protein was found to be the molecular culprit behind heavily muscled cattle, sheep, dogs and in one reported case of a human \u2014 a baby boy born in Germany with massive muscles in his thighs and upper arms ( M. Schuelke  et al. N. Engl. J. Med.    350,   2682\u20132688; 2004 ). So, when a muscle-laden woman was accepted to the Undiagnosed Diseases Program, myostatin \u2014 and McPherron \u2014 were on Gahl's mind. He sent over photos of Massagee and eventually went to visit McPherron in her office. \"I've never met a patient before,\" McPherron says. But she agreed to get involved with this one.  \n                A question of need \n              Humans present experimental challenges that McPherron's mice do not. \"The big question,\" she says, \"was, do I want a muscle biopsy?\" McPherron would need some tissue to do a thorough analysis of Massagee's myostatin expression levels. She knew the discomfort involved, but the only material already available was old and not prepared in a way that would allow protein extraction and RNA analysis. \"I was hemming and hawing,\" she says, but when the rest of the team decided to go for it anyway, she decided she should be there to see Massagee and to carry the tissue back to the lab herself. The main results Gahl's team wanted would be coming from the biopsy tissue, both from McPherron's lab and from the laboratory at the Armed Forces Institute of Pathology (AFIP) in Washington DC that would be doing a full histopathological work-up. There are other tests to run. Gahl explains to Massagee the purpose of a 'million SNP array', an assay that is used commonly in research genetics but rarely in clinical diagnostics. The array looks for single nucleotide polymorphisms (SNPs), spots in the genome that differ between individuals with relatively well known frequency. The SNPs serve as landmarks and if one that is expected to neighbour another is missing or doubled up, it can show where DNA has been deleted or duplicated, perhaps pointing to the genetic root of a disease. Gahl's team does an assay for everyone who enters the Undiagnosed Diseases Program, and often for their family members too. Gahl appears to take a certain pleasure in explaining the method, but he seems to lose Massagee while trying to explain such concepts as 'loss of heterozygosity'. Still, he is patient with her, mirroring her wonder over how some tests work and assuring her that it is pointless for the time being to worry about what the results might mean for her children. \"We're operating on best guesses and a lot of ignorance,\" he says. But of course the patients do worry. Just days before Aurelius and his wife arrived in Bethesda, they found out that she was pregnant. Although extremely early in the pregnancy, they told the doctors in case there was anything they should know. The results of genetic tests take on greater significance when another generation could inherit the result. Aurelius explained the frustration of not knowing what to expect from his body or for his children. \"Everything's good with me, but I have this alien disease. I keep wondering what's going to happen.\" At the end of a week of tests, Massagee and Aurelius are discharged and return to their homes to await their results. \"It's going to be nice to be able to walk out of here, if not with a diagnosis, with at least the next step,\" says Aurelius. Massagee effuses gratitude and says how wonderful everyone at the NIH has been. \"At its best it is a wonderful place,\" Gahl says, adding dryly, \"at its worst it's a government organization.\" Over the next few months, work starts on the data from Massagee and Aurelius. The test for the FGF23 levels in Aurelius's blood comes back normal. Nesterova had already begun to sequence  CYP27B1   and  CYP24A1 , and in May she finds that one copy of Aurelius's  CYP24A1   is missing three base pairs. This 'microdeletion' is not currently listed in any databases of known human variation, suggesting that it may be a novel change. It could be disabling or at least limiting Aurelius's ability to deactivate vitamin D, explaining his high levels of the vitamin. Then again, it could be harmless. \"It's hard to put the weight of significance on these findings, for now,\" Nesterova says. Collins still favours his hypothesis about the sodium/phosphate transporter, and at his urging Aurelius consents to send DNA samples to a programme, run by Bergwitz, that is sequencing genes that code for various versions of the transporter. Others with mutations in these genes have low phosphate levels and bone disorders such as rickets. \"Our interest is to find more mutations,\" Bergwitz says, ones that create different symptoms. These could reveal what parts of the genes and their corresponding protein actually do, be it ion-pumping mechanics or insertion into cell membranes. The million-SNP arrays come back with reams of data. Thomas Markello, who runs the studies for the programme, says that they spit out many hits but little in the way of answers. Raw data suggest that each person has between 3 and 10 positions in the genome in which both copies of a given genetic region are deleted, plus 50\u2013200 instances each of single-copy gene deletions and duplications, any of which \u2014 or none of which \u2014 could be involved. \"This is not what most physicians are used to seeing,\" Markello says. So far, the SNP arrays have helped with just one diagnosis. But this is a research programme, and part of the research is to determine how useful these techniques can be. Moreover, the data and samples, stored at the NHGRI, may still prove informative for future studies. Markello calls the programme \"training wheels\" for using whole-genome sequencing in the clinic, in which the number of genetic differences found in a given individual will go up many orders of magnitude but their clinical significance will be even harder to tease out. In the near future, he says, those in the programme will have their entire genome sequenced.  \n                Hints of progress \n              On 8 April, Massagee e-mailed Manoli, her main contact point at the programme. Massagee was more fatigued than ever, having to stop what she was doing every 20 minutes and take a break. Work was becoming difficult, and she couldn't walk more than a block without losing her breath. Manoli called her back that day to ask if she could come back to Bethesda for more testing. Although the full report from the AFIP had not yet come back, the pathologists had found hints that proteins were building up in the walls of the blood vessels that feed Massagee's muscles. On her second visit to the NIH, Massagee got her diagnosis. She had amyloid light-chain, or AL, amyloidosis, a rare disorder that is tied to the bone marrow's abnormal production of immune cells that make immunoglobulin proteins. Excess immunoglobulin accumulates into the proteinaceous build-ups that were lining some of her blood vessels. Around 1,200 to 3,200 cases of AL amyloidosis are reported each year in the United States. Amyloid can build up in pretty much any tissue or organ, but Massagee's presentation in skeletal muscle is especially rare. The researchers do not know why the immunoglobulin caused her muscles to bulk up. But happily for Massagee, her heart muscles seemed to be unaffected and there was no serious damage to her kidneys, which can lead to death. Manoli contacted Morie Gertz, who studies the disease at the Mayo Clinic in Rochester, Minnesota, and pushed for a swift appointment. Gertz's team saw Massagee the following week to determine whether she would be a good candidate for a clinical trial to treat the disease with chemotherapy and autologous bone-marrow transplantation. She was. On 19 June, Massagee underwent the bone-marrow transplantation procedure. Afterwards she developed a condition called peri-engraftment syndrome, a poorly understood complication of autologous transplants that made her very sick. But now she says she's feeling stronger every day. Her doctors are uncertain whether treatment of the haematological condition will reverse the build up of muscle. But Massagee says that her muscles feel softer to the touch already, and that she considers the NIH programme to have saved her life. Gahl's team was also pleased to get a diagnosis, even if it was one that is not new to medicine. None of their hunches about the involvement of myostatin came true. But McPherron is keeping Massagee's muscle sample in the freezer. She hopes to use it to investigate how the accumulation of immunoglobulin led to such an overgrowth of muscle. One idea is that the build up, or an inflammatory response to it, activated the satellite stem cells that normally divide to create new muscle tissue. But anything McPherron could do with it would be extremely preliminary. \"Our time will come with respect to new diseases,\" says Gahl. \"We're very pleased to find different presentations of known disease, and I wouldn't discount the learning process.\" The team has made other such diagnoses. Gahl says that they have recognized a handful of cases of multiple sclerosis for patients enrolled in the programme and they were able to diagnose an atypical case of lymphoma simply from a chart review. But these are the happy endings. There are still upwards of 50 open cases in Gahl's files. May, the sociologist at Newcastle University, says that the doctors involved in the Undiagnosed Diseases Program are unusual in this sense because they know that the vast majority of their cases will never be solved. \"There is a conflict there between having someone who is an interesting case and somebody who is going to be evidence of one's failure.\" Gahl brings up one other statistic, and it's clear that it weighs heavily on him. Twelve of the patients who applied to the programme have died so far. One of these, a young woman called Summer Stiers, had serious symptoms affecting many of her organ systems and was the subject of several news stories earlier this year. The tests that Gahl's team ran when she was at the NIH generated few concrete leads. Stiers decided, with her local doctors' acquiescence, to discontinue the regular dialysis and other treatments that had been keeping her alive. She died within three days. Stiers had called Gahl a few days before this, in part to make final arrangements for her body to be shipped to the NIH for further study. Even at the end of her quest for a diagnosis, she wanted to help. Aurelius, in Santa Fe, is still anxiously awaiting news. Nesterova contacted him in July to tell him that she wished to publish an abstract about the  CYP24A1   microdeletion for a meeting on metabolic disorders in San Diego, California, this month. She suspects that it is responsible for his symptoms; if it is, it could mean the identification of an entirely new genetic disease. Although excited by this possibility, Nesterova is reluctant to become too confident until she can do more follow-up work to show definitively that the mutation affects the function or levels of the protein it encodes. Her colleagues have been sequencing the gene in 100 healthy controls to see whether the deletion is simply a harmless variant. \"She's cautious and thorough,\" Aurelius says. Despite being happy that someone is still working on his case, Aurelius says the pace still feels slow. \"At the end of the day what I want to stop is the kidney stones because I don't want to have renal failure,\" he says. Another stone is currently growing in his left kidney. Aurelius and his wife are preparing for the birth of their child in late October, and he continues with his sculptures \u2014 large, craggy, organic-looking pieces in bronze and wood that unmistakably evoke the calcium phosphate stones that have caused him so much pain. He is even planning a show for the Clinical Center starting in November. He's promised that a portion of the proceeds from any sales will go to a patients' fund at the NIH. \"It's rare in life when you feel like someone gives so much to you \u2014 taking me out there for a week, getting all these doctors together. It's important to give back to that.\" And Gahl, like the other doctors in the programme, continues to wrestle with his dual motivations for it. \"The physician in me is interested in helping people,\" he says. \"The scientific part of us goes after the new disease areas. That's very stimulating \u2014 to be the first to discover something. I think all of us feel that way\". Even if they don't find either diagnosis or new disease, the Undiagnosed Diseases Program offers at least an extension of hope for those who enter it. \"I'm astounded at how appreciative they are of our failed efforts,\" says Gahl. Brendan Maher is  Nature 's biology features editor. \n                     Nature Insight Human Genomics and Medicine \n                   \n                     Web Focus Personal Genomes \n                   \n                     NIH Undiagnosed Diseases Program \n                   \n                     Dunham Aurelius \n                   \n                     Summer Stiers \n                   \n                     In Need of Diagnosis \n                   \n                     Syndromes Without a Name \n                   Reprints and Permissions"},
{"file_id": "461027a", "url": "https://www.nature.com/articles/461027a", "year": 2009, "authors": [{"name": "Emily Waltz"}], "parsed_as_year": "2006_or_before", "body": "Papers suggesting that biotech crops might harm the environment attract a hail of abuse from other scientists. Emily Waltz asks if the critics fight fair. Emma Rosi-Marshall's trouble started on 9 October 2007, the day her paper was published in  Proceedings of the National Academy of Sciences   ( PNAS ). Rosi-Marshall, a stream ecologist at Loyola University Chicago in Illinois, had spent much of the previous two years studying 12 streams in northern Indiana, where rows of maize (corn), most of it genetically engineered to express insecticidal toxins from the bacterium  Bacillus thuringiensis   ( Bt ), stretch to the horizon in every direction. Working with colleagues including her former adviser Jennifer Tank at the University of Notre Dame, Indiana, Rosi-Marshall had found that the streams also contain  Bt   maize, in the form of leaves, stalks, cobs and pollen. In laboratory studies, the researchers saw that caddis-fly larvae \u2014 herbivorous stream insects in the order trichoptera \u2014 fed only on  Bt   maize debris grew half as fast as those that ate debris from conventional maize. And caddis flies fed high concentrations of  Bt   maize pollen died at more than twice the rate of caddis flies fed non-_Bt   pollen. The transgenic maize \"may have negative effects on the biota of streams in agricultural areas\" the group wrote in its paper, stating in the abstract that \"widespread planting of  Bt _ crops has unexpected ecosystem-scale consequences\" 1 . The backlash started almost immediately. Within two weeks, researchers with vehement objections to the experimental design and conclusions had written to the authors,  PNAS   and the US National Science Foundation (NSF), Rosi-Marshall's funder. By the end of the month, complaints about the paper had rippled through the research community. By the time Rosi-Marshall attended a National Academy of Sciences (NAS) meeting on genetically modified organisms (GMOs) and wildlife on 5 November 2007, \"She looked hammered\", says Brian Federici, an insect pathologist at the University of California, Riverside, one of those who commented on her work. \"I felt really sorry for her. I don't think she realized what she was getting into.\" The response we got \u2014 it went through the jugular. Emma Rosi-Marshall ,  No one gets into research on genetically modified (GM) crops looking for a quiet life. Those who develop such crops face the wrath of anti-biotech activists who vandalize field trials and send hate mail. But those who, like Rosi-Marshall and her colleagues, suggest that biotech crops might have harmful environmental effects are learning to expect attacks of a different kind. These strikes are launched from within the scientific community and can sometimes be emotional and personal; heated rhetoric that dismisses papers and can even, as in Rosi-Marshall's case, accuse scientists of misconduct. \"The response we got \u2014 it went through your jugular,\" says Rosi-Marshall.  \n                Problem papers \n              Behind the attacks are scientists who are determined to prevent papers they deem to have scientific flaws from influencing policy-makers. When a paper comes out in which they see problems, they react quickly, criticize the work in public forums, write rebuttal letters, and send them to policy-makers, funding agencies and journal editors. When it comes to topical science that can have an impact on public opinion, \"bad science deserves more criticism that your typical peer-reviewed paper\", Federici says. But some scientists say that this activity may be going beyond what is acceptable in scientific discussions, trampling important research questions and stifling debate. \"It makes public discussion very difficult,\" says David Schubert, a cell biologist at the Salk Institute in La Jolla, California, who found himself at the sharp end of an attack after publishing a commentary on GM food 2  (see ' Seeds of discontent '). \"People who look into safety issues and pollination and contamination issues get seriously harassed.\" To see the effect that biotech crop research can have on policy \u2014 and why some researchers feel that they need to weigh in against such studies as quickly and forcefully as possible \u2014 it is instructive to look back to a study 3  published in  Nature   in 1999. In it, John Losey, an entomologist at Cornell University in Ithaca, New York, and his colleagues reported that nearly half of the monarch butterfly caterpillars eating leaves dusted with  Bt   maize pollen died after four days, compared with none exposed to untransformed pollen. The media and the anti-GMO community erupted. \"Gene Spliced Corn Imperils Butterflies\" headlined the 20 May 1999  San Francisco Chronicle . Greenpeace activists demonstrated in front of the US Capitol dressed as monarch butterflies, collapsing from 'killer' GM maize. In response, the US Environmental Protection Agency (EPA) told seed companies to submit data about the toxicity of  Bt   maize pollen in monarch butterflies or lose the right to sell the maize. Scientists dived into the research, using industry and government funding. The effort produced six  PNAS   papers in 2001 that concluded that the most common types of  Bt   maize pollen are not toxic to monarch larvae in concentrations the insects would encounter in the fields 4 . (Losey had used higher concentrations in his lab studies.) \"The Losey paper resulted in a lot of good work and brought to a close that particular question,\" says Alison Power, who studies ecology and evolutionary biology at Cornell University. Yet some scientists were dismayed that a single paper with preliminary data gave so much ammunition to anti-GMO activists and caused an expensive diversion of resources to calm the scare. They did not want it to happen again. The caddis-fly study was Tank and Rosi-Marshall's debut in GM research. The idea stemmed from a 2002 talk that Tank gave at Michigan State University in East Lansing about nitrogen dynamics in streams. A researcher in the audience asked whether organic debris from fields of transgenic maize drains into streams, and whether it has any effect on stream life. \"We've never thought about that,\" Tank told the questioner. And once the paper was complete, Tank, Rosi-Marshall and their collaborators had little idea of the storm it was about to kick up. \"I thought the response would be 'So what? We're going to lose a few trichopterans',\" says co-author Todd Royer, an assistant professor at Indiana University in Bloomington. On a Friday after the paper was published, Federici and plant biotechnologist Alan McHughen, also at the University of California, Riverside, met at a campus bar for a beer after work. \"[McHughen] was really annoyed,\" says Federici. \"I don't think there's been another case where I've seen him so really ticked off.\" Federici says he too was annoyed \u2014 Rosi-Marshall's study was \"bad science\", he says, and they feared that activists would use it to forward an anti-GMO agenda. McHughen and Federici wanted to neutralize any effects that Rosi-Marshall's paper might have on policy. The two discussed the key points of a rebuttal letter. McHughen wrote the critique and \"circulated it around to people who might be sympathetic\", says Federici. The letter listed six grievances with the \"sloppy experimental design\", and said the publication of the paper had \"seriously jeopardized the credibility of  PNAS \". \"How many busy scientists and how much scarce money will we need to divert to calm this new scare?\" the researchers wrote. McHughen got ten other scientists' signatures, including Federici's. On 22 October, they sent the letter to the journal and to the NSF. Days later, Klaus Ammann, a retired botanist and professor emeritus at the University of Bern in Switzerland who had signed the McHughen letter, posted it on an online discussion forum 5 .  \n                Critical mass \n              Wayne Parrott, a crop geneticist at the University of Georgia in Athens, also began working on a rebuttal to Rosi-Marshall's paper as soon as he saw it. He said recently that in his opinion: \"The work is so bad that an undergrad would have done a better job. I'm convinced the authors knew it had flaws.\" He e-mailed the authors, the NSF and  PNAS   two bulleted lists of flaws that he said invalidated the paper. He wrote: \"It is risky to extrapolate from lab results to field results, particularly when key factors were not monitored, measured or controlled appropriately.\" In January 2008,  PNAS   published a slimmed-down version of this letter 6  and the one from McHughen 7 . Tank and Rosi-Marshall were dismayed by Parrott's e-mail. A few days after receiving it, Tank called James Raich, her contact at the NSF, to talk it over. \"I told her to ignore it,\" says Raich, an ecosystem ecologist at Iowa State University in Ames who worked for the NSF for two years reviewing grant proposals. He told her that letters like these were unusual. But the critiques kept on coming. On 30 November, Monsanto, a maker of  Bt   maize based in St Louis, Missouri, sent the EPA a six-page critical response 8  to the paper, and posted it online. Eric Sachs, director of global scientific affairs at Monsanto, says that regulators ask seed companies to notify them of papers that relate to crop safety, so Monsanto often includes with its notification evaluations of these papers. Four other signatories of the McHughen letter went on to publish scathing opinion articles over the next few months. In a March 2008 article 9  criticizing four papers on biotech crops, Ammann joined forces with Henry Miller, a research fellow at the Hoover Institution in Stanford, California, to ask \"Is biotechnology a victim of anti-science bias in scientific journals?\". They called Rosi-Marshall's conclusions \"dubious\", and said their use of evidence \"arguably amounts to investigator misconduct\". And in a July 2008 commentary in  Current Science 10 , Shanthu Shantharam, a visiting research scholar at Princeton University in New Jersey said Rosi-Marshall's \"offending\" paper \"carried a wrong message to farmers and environmentalists\", and that anti-biotech crop activists would use the paper to \"hamper the progress of science\". Rosi-Marshall took the hits hard. \"I experienced it in person and in writing,\" she says. \"These are not the kind of tactics we're used to in science.\" She was a few years out from her PhD, she did not have tenure at Loyola and her first paper in a prominent journal was getting trashed, along with her reputation. \"She's young and was getting picked on,\" says Michelle Marvier, a biologist at Santa Clara University in California who attended the NAS November 2007 meeting. It was at least some comfort to Rosi-Marshall and Tank that e-mails and phone calls of encouragement came pouring in from other scientists. Some of their supporters had observed similar attacks on other biotech crop papers. \"The most reassuring thing we learned was that it had happened before and by the exact same people,\" says Tank. What was it about Rosi-Marshall's paper that prompted such a strong reaction? The wording of the abstract \u2014 \"widespread planting of  Bt   crops has unexpected ecosystem-scale consequences\" \u2014 was a particular point of contention. Her critics say that the data do not support such a definitive conclusion. \"They absolutely went too far,\" says Randy Schekman, editor-in-chief of  PNAS . Of the half-a-dozen letters received by the journal, most of them protested at this wording, he says. \"Why this would have escaped the attention of the referees beats me.\" The authors agree that the wording was unfortunate and in retrospect say that the sentence should have articulated the potential for ecosystem-scale consequences within streams, rather than suggesting that such consequences were observed. \"This was an oversight,\" says Rosi-Marshall. \"But we did not expect that this sentence would, in light of all of the other statements in our paper, elicit the response it did. We thought the paper would be taken as a whole.\" The study's methods also came under fire. It is unclear, for example, whether it was the  Bt   toxin itself affecting the caddis flies, or some other difference between  Bt   and non-  Bt   plants. To test this possibility, critics say the caddis flies should have been fed isogenic lines: strains of maize that are genetically identical except for  Bt   genes. The authors say they chose not to use such lines because their nutritional quality would have differed \u2014  Bt   maize has higher concentrations of lignin than non-  Bt   maize, and so is less nutritious. So the authors matched the  Bt   samples with non-  Bt   samples that had similar levels of lignin and other nutrients. \"To do otherwise would have resulted in a confounded experiment. Pairing the treatment on the basis of isolines might be standard for agronomic studies, but was inappropriate for an ecological feeding study,\" the authors told  Nature   in an e-mail. Rosi-Marshall and her colleagues made this point and other responses to their critics in a correspondence 11  published online in  PNAS   the week after McHughen's and Parrott's critiques. It is also unclear how much  Bt   toxin the caddis flies ate. The authors let the insects eat as much as they wanted, as they would in the wild. Critics argue that the authors should have fed the insects known amounts of the toxin in a method called a dose-response study that is routine in toxicity assessments. \"The Rosi-Marshall  et al . paper would have benefited from additional toxicological data,\" says Doug Gurian-Sherman, a senior scientist at the Union of Concerned Scientists in Cambridge, Massachusetts, and a former reviewer for the EPA. But the method the authors used \"is a widely accepted method, and is generally adequate for a preliminary study of possible toxicity\", he says.  \n                Omitted study \n              The paper was also accused of omitting contrary findings. In June 2007, four months before Rosi-Marshall's  PNAS   paper was published, Jillian Pokelsek, a master's student at Loyola University Chicago working with Rosi-Marshall, presented results from a preliminary field experiment at the annual meeting of the North American Benthological Society in Columbia, South Carolina. The work showed that  Bt   maize pollen did not influence the growth or mortality of filter-feeding caddis flies. The society posted an abstract 12  of the presentation on its website attributing the work to Pokelsek, Rosi-Marshall, Tank, Royer and four other scientists who also authored the  PNAS   paper. It was not mentioning this study that prompted Miller and Ammann's accusation of misconduct 9 . The authors defend the omission on the grounds that the data in the meeting presentation were not published or peer-reviewed, and were less reliable than those in the  PNAS   paper. \"Field experiments are inherently difficult to control and have lower statistical power to detect significant differences compared with controlled laboratory experiments, thus we included the more controlled and statistically rigorous lab experiments in our paper,\" Tank and Rosi-Marshall told  Nature . Also, the caddis flies in the student presentation belonged to a different family, with different feeding mechanisms to those in the  PNAS   study. Miller's response: \"I don't want to split hairs,\" he says. \"If you don't do appropriate controls or if you draw conclusions that are erroneous, I think that's misconduct.\" But Ammann says he has a \"bad feeling\" about the accusation. \"Maybe we should have been more careful with the wording.\" Scientists who were not involved in the debate over Rosi-Marshall's paper say the results were preliminary and left some questions unanswered, but that overall the data are valuable. \"The science is fine as far as I'm concerned,\" says Arthur Benke, an aquatic ecologist at the University of Alabama in Tuscaloosa, who called the strong language in some of the criticisms \"inappropriate\". When bad science is used to justify bad policies, we all lose. Alan McHughen ,  What drives the critics? Financial or professional ties to the biotech industry don't seem to be the impetus. Such ties do exist \u2014 like many people researching biotech crops, some have received research grants from industry or have other interactions with it \u2014 but in interviews they say that these are not the major driving force. Rather, many of them feel strongly that transgenic crops are safe and beneficial to the environment and society, and that the image and regulation of these crops has been too harsh. Many of the critics have been studying biotech crops since they were developed commercially in the late 1980s, and some were involved with the first regulatory approvals. They have specific ideas about how the risks of these crops should be scientifically assessed. And they worry that papers that fall short of high standards will give anti-GMO activists ammunition to influence policy, just as the monarch-butterfly study did. \"When bad science is used to justify bad public policies, we all lose,\" says McHughen, who says he is on a \"campaign to make academic scientists a little less politically naive and a bit more careful in their scientific work\". Miller adds that \"agricultural biotech has been so horrendously, unscientifically regulated and so over-regulated and so inhibited over the past 30 years that to have these pseudo-controversies stirred up unnecessarily does a disservice to everyone and everything\". Ammann points to the example of golden rice, a variety engineered in the late 1990s to contain more vitamin A. Regulations have delayed the rice's development, he says, although more than 250,000 children a year go blind from vitamin-A deficiency. \"We have to get emotional,\" says Ammann. \"I can't agree with the cool scientists' perspective \u2014 only dealing with the facts. We live in the real world.\" In 2006, Ammann formed a rebuttal team called ASK-FORCE to challenge reports about biosafety of GM crops. On one online site, Ammann criticizes 20 reports \u2014 none of them positive toward biotech crops \u2014 that he considers biased or bad science. In July, he was revising a critique of a paper that appeared in  The Lancet   ten years ago. \"I'm working nearly day and night on these things,\" says Ammann. The emotional and sometimes harsh quality of some of the attacks strikes some scientists as strange and unlike the constructive criticism to which they are accustomed. Benke points out that none of the criticisms on the caddis-fly paper, for example, called for further study on the insects. \"What papers like this do is alert us to possible reasons to look into this more carefully,\" he says. \"No one mentioned this.\" To try to dismiss the research out of hand ignores how science is supposed to work, adds Power \u2014 you make a hypothesis, test it, refine it, test it and refine it again. \"You keep doing that until you have an answer that is as close as you're going to get,\" she says. \"I don't understand the resistance to that notion.\"  \n                Arbiters of the truth \n              Some scientists say they are galled by the certainty with which some of the critics state their opinion. \"Part of what exasperates me is that they have declared themselves to be the experts in this field, and forcefully present themselves as the ultimate arbiters of truth,\" says an editor for the Entomological Society of America who asked to remain anonymous. \"I personally am in favour of GMOs in general, and think that they are very beneficial for the environment. But I do have problems with the tactics of the large block of scientists who denigrate research by other legitimate scientists in a knee-jerk, partisan, emotional way that is not helpful in advancing knowledge and is outside the ideals of scientific inquiry.\" It is critical to assert the right of scientists to question each other's work. Wayne Parrott ,  The critics respond that they are simply pointing out flaws in research, and that this is an important part of the scientific process. \"It is neither fair nor accurate to equate pointing out serious deficiencies with experimental design and data interpretation as 'denigration',\" Parrott says. \"For science to maintain its integrity and move forward, it is critical to assert the right of scientists to question each other's work.\" McHughen says that he doesn't condone  ad hominem   attacks. \"They are invariably unproductive,\" he says, and points out these tactics are often used against scientists who don't oppose GM crops. Federici says he finds it inappropriate to call the reactions 'knee-jerk' ones. \"Losey and colleagues, and Rosi-Marshall and colleagues at the time of their studies were newcomers to the field. Most of the people who found their studies flawed and protested had extensive experience with  Bacillus thuringiensis .\" He also points out that the critics varied in how strongly they responded to the Rosi-Marshall paper, saying \"I don't consider writing a letter to the editor a harsh response.\" Young people are not going into this field because they are discouraged by what they see. Ignacio Chapela ,  Ignacio Chapela, a microbial ecologist at the University of California, Berkeley, says that the attacks may be deterring young scientists from pursuing careers in biotech crop research. \"I have a very long experience now with young people coming to me to say that they are not going into this field precisely because they are discouraged by what they see,\" he says. Chapela faced criticism from pro-GMO scientists after publishing a 2001 paper in  Nature , in which he reported that native maize varieties in Mexico had been contaminated with transgenic genes 13 . Following the criticism,  Nature   decided that \"the evidence available is not sufficient to justify the publication of the original paper\". At its worst, the behaviour could make for a downward spiral of GM research as a whole, says Don Huber, a emeritus professor of plant pathology at Purdue University in West Lafayette, Indiana. \"When scientists become afraid to even ask the questions \u2026 that's a serious impediment to our progress,\" he says. Miller says: \"I don't see how criticism of flawed science that verges on misconduct should discourage anybody.\" Researchers could be invigorated by entering a field with such lively debate. \"For some people it might be exciting because you're doing science that is relevant to society,\" says Power.  \n                Pervasive spread \n              Rosi-Marshall's caddis-fly paper did find its way into the anti-GMO rhetoric, although on nowhere near the scale that the monarch butterfly paper did. For example, the London-based Institute of Science in Society, a not-for-profit organization involved in the GM debate, on 30 October 2007 posted its summary of the paper, saying that: \"calling a halt to planting  Bt   corn next to streams \u2026 would be in keeping with the evidence [the authors] have provided\". Greenpeace included the paper in an April 2008 briefing on  Bt   maize, citing it as evidence of environmental risk. The impact went further than that. On 9 January 2008, three months after Rosi-Marshall's paper was published, France's watchdog on GM foods ruled that one of Monsanto's types of  Bt   maize, known as MON810, may have an impact on wildlife. The evidence it cited included Rosi-Marshall's paper. Two days later, the French government announced a ban on cultivating the maize. \"[The paper] got to every agency and non-governmental organization that doesn't like the technology and gave them a flag to wave,\" says Parrott. Not that he considers the effort wasted: \"I have no doubt the impact on policy-makers would have been much worse had it not been countered.\" Nearly two years since the paper was published, the critics' comments are still pointed. \"It was just an idiotic experiment,\" Miller said this July. But Rosi-Marshall and her co-authors stand behind their paper. \"We believe our study was scientifically sound,\" they wrote in an e-mail, \"although many questions on the topic remain to be answered. The repeated, and apparently orchestrated,  ad hominem   and unfounded attacks by a group of genetic engineering proponents has done little to advance our understanding of the potential ecological impacts of transgenic corn.\" And Rosi-Marshall's career seems to have survived the furore. In May 2009 she secured tenure at Loyola University Chicago, and in August she moved to the Cary Institute of Ecosystem Studies in Millbrook, New York. There she will study human-dominated ecosystems and will continue to investigate the influence of maize varieties on stream ecosystems. Since the caddis-fly paper, she has co-authored another study on transgenic crops showing that  Bt   maize debris decomposes in streams at a faster rate than conventional maize 14 . She says more data produced with the NSF grant are on the way and that the attacks won't deter her from her studies. \"It toughened me up a lot,\" she says. \"I'm not going to be intimidated.\"   Emily Waltz is a freelance writer based in New York City.    See also Correspondence    Battlefield: hitting the supporters of biotechnology   and    Battlefield: useful debate needs caution and civility . \n                     Nature Biotechnology: Report claims no yield advantage for Bt crops \n                   \n                     Nature Biotechnology: Up in arms \n                   \n                     Emma Rosi-Marshall \n                   \n                     PNAS \n                   \n                     ASK-FORCE \n                   Reprints and Permissions"},
{"file_id": "461034a", "url": "https://www.nature.com/articles/461034a", "year": 2009, "authors": [{"name": "Jane Qiu"}], "parsed_as_year": "2006_or_before", "body": "Lightning and fires on the Arctic tundra seem to be on the rise. Jane Qiu meets the researchers learning from the scorched earth in Alaska. More than 20,000 lightning strikes were recorded on the North Slope of Alaska in 2007. Some struck the vast stretches of lakes; some hit the treeless tundra. And one of them torched into life the largest and longest-lasting tundra fire recorded in the state's history. The blaze, which started near the Anaktuvuk River on 16 July, burned 7,000 hectares a day at its peak, and eventually consumed 100,000 hectares, an area larger than that of New York City. It finally stopped burning in early October, smothered by thick snow. Two years later, the scars left by the blaze are all too apparent from a helicopter circling over the region. So too is the area's quick recovery. Tussock grass, the predominant vegetation in northern Alaska, sends up vibrant green shoots from scorched meristems. Its white flowers bloom over the deeply blackened soil like a dust of snow, stretching to a hazy horizon. It is surprisingly beautiful. This is more than a view of nature's swift destruction and renewal; it is also a site of intense research. \"The Anaktuvuk River fire is a large natural experiment,\" says Gaius Shaver, an ecologist at the Marine Biological Laboratory in Woods Hole, Massachusetts, who leads an effort funded by the US National Science Foundation to study the fire's environmental impact. \"It provides an unprecedented opportunity to study how the entire ecosystem responds to major disturbances.\" Scientists from ten research groups have been flying into the burned area from the nearby Toolik research station to assess how the fire has shifted the carbon balance and affected the hill slopes, valleys, streams and lakes in the region. Understanding the effect of fires on the Arctic tundra may become more important as the climate gets warmer. Tundra fires used to be rare events, but higher temperatures and a more arid climate seem to be changing that. According to the US Bureau of Land Management in Washington DC, the frequency of lightning on the North Slope has increased tenfold in the past decade. And many researchers fear that the increased lightning may increase the fire risk. Of the 26 recorded fires on the North Slope since 1950, close to one-third has taken place in the past three years \u2014 and the region burned in the Anaktuvuk River fire alone constitutes more than half of the total burned area. Just as climate change may fuel fires, fires may accelerate climate change. These vast areas of tundra store about 14% of the world's soil carbon at the surface alone. Fires could release a large amount of that, either directly through combustion or indirectly by modifying the tundra ecosystem. Many of the studies in Alaska are designed to look at how this could play out. \"Tundra fires could significantly impact the carbon balance of the Arctic and exacerbate global warming,\" Shaver says.  \n                The first pulse \n              One of the first researchers to arrive in the area after the fire was Cody Johnson, an aquatic ecologist at Utah State University in Logan, along with a small team. The razed landscape had been buried under snow all winter. And it still wasn't exactly camping season when, in May 2008, they flew out to Dimple Lake in northern Alaska, at 69\u00b0 latitude and 200 miles north of the Arctic circle. The crew landed in skis, dragged their heavy equipment to the lake shore, set up camp and waited in sub-zero temperatures for the first snow melt. After two weeks, Johnson could see the land for the first time. \"The surface was covered in a thick layer of ash,\" he says. \"There was a pungent smell of the burn in the air.\" Underneath the ash they found a blackened land with islands of yellow tussock trunks sticking out. These many-layered skirts of grass are so thick and packed so tightly that the core never dried out, saving them from the fire. The main purpose of Johnson's camping trip was to study the movement of burned materials, especially ash, nutrients and sediments, into streams and lakes, and its effect on the ecosystem's carbon cycle. After the plants that hold the soil together have burned, these nutrients and sediments can be readily flushed out. A boost in nutrient levels could stimulate the rate of primary production \u2014 the production of organic compounds from the fixing of carbon dioxide during photosynthesis. By contrast, additional sediments could partly offset this effect by smothering algae, the key player in carbon fixation. Working out the net outcome of these and other interconnecting processes on carbon is no mean feat. George Kling, a lake ecologist at the University of Michigan in Ann Arbor, says that the lakes in the Arctic, already supersaturated with carbon, are a net source of carbon dioxide and methane \u2014 they release more of these gases than they can store. He and others have shown that when carbon moves from land into lakes, some 20% of the Arctic carbon sink is re-released 1 . \"You can think of the aquatic system as a release valve, which leaks carbon from land to water and then back to the atmosphere,\" says Kling, who is collaborating with Johnson. \"Major disturbances such as the Anaktuvuk River fire could turn that trickle into a large flood and cause much greater movement of carbon.\" The researchers predicted that the biggest inflow of nutrients after the fire would be from the snow run-off. \"We wanted to capture that first pulse,\" says Johnson. Every day they sampled what went into the streams and how much of those materials ended up in rivers and lakes. Their initial results, which have not yet been published, show that streams in the burned areas dumped up to five times more sediment into Dimple Lake than those in unburned ones, but carried only twice as much carbon. Puzzled by the discrepancy, Kling and his colleagues looked at the quality of the water that leached out from the soil and found that microbes performed decomposition seven times faster in water from the burned area than in unaffected sites. \"It seems that carbon is released into the atmosphere through microbial decomposition before it reaches the streams,\" says Kling. Johnson and his team weren't camping alone for long. In June 2008, Michelle Mack, an ecologist at the University of Florida in Gainesville, flew to the burned area with a handful of colleagues to reconstruct how much carbon and nitrogen were there before the fire, and to calculate how much carbon was released into the atmosphere during and after it. By using tussock meristems as measuring sticks, they estimated the depth of the soil before the fire. They also gauged the carbon concentration in the soil from unburned sites at the same climate and elevation as the burned area. The blaze generated a big carbon cloud. Mack found that the fire emitted 1.8 million tonnes of carbon dioxide into the atmosphere, equivalent to about 0.03% of the annual global carbon emission due to human activity. \"That amount was certainly large enough to change the carbon balance of the entire North Slope for 2007,\" Shaver says.  \n                Burning up \n              The burned area continues to pump carbon into the atmosphere, says Adrian Rocha, an ecologist working in Shaver's group. To measure the net carbon flux, Rocha and his colleagues set up a meteorological tower in a severely burned area, a moderately burned one and an unburned control site. They found that, within a radius of one kilometre around the tower, the severely burned site released 80 grams of carbon per square metre during the summer months, whereas the unburned tundra was absorbing 50 grams of carbon per square metre. They suspect that, in the burned area, the rate of carbon fixation by photosynthesis dropped because so many plants were destroyed, and that was outweighed by the rate of carbon being released by microbial decomposition in warmer soil. Now Rocha is extrapolating the results to a larger area. He is using a satellite instrument called the Moderate Resolution Imaging Spectroradiometer (MODIS) to measure how green the land surface is, and is building a model to correlate the greenness to the type of tundra vegetation 2 . Once he has a good model, he will be able to estimate the carbon flux of the entire burned area for different periods after the fire. Mack found that 60% of the initial carbon emission was from the burning of organic plant and animal material in the soil \u2014 which is usually pretty wet on the tundra and doesn't make an efficient fire fuel. The rest came from mosses, lichens and other plant materials on the tundra surface. The organic matter in soil insulates the permafrost \u2014 the permanently frozen ground \u2014 underneath, and any reduction in the soil depth could mean that the underlying permafrost warms and thaws, with big repercussions for the ecosystem. At the end of June 2009, the permafrost had thawed to a depth of 40 centimetres in severely burned areas, compared with 18 centimetres in unburned sites. Permafrost thaw can be exacerbated by the fire-blackened surface, something else that Rocha and his colleagues are measuring. Researchers have been trying to understand the impact of permafrost thaw on carbon exchange for some time. A study by Edward Schuur, an ecologist at the University of Florida, and his colleagues, showed that there was net carbon loss from a tundra landscape that had undergone permafrost thaw for more than several decades 3 . The world's permafrost contains twice as much carbon as its atmosphere; it would be bad news if even a fraction of that were released 4 . The accelerated melting of permafrost caused by the Anaktuvuk River fire has carved more visible changes out of the landscape. The newly softened soil can collapse or slide away, creating a landscape that is pock-marked with hollows known as thermokarsts. Johnson noticed thermokarsts forming in autumn 2008, and they had grown much bigger when he went back this summer. He watched one, on the shore of Horn Lake, double in width to 200 metres in two days. In one valley, which has now been dubbed 'the valley of thermokarsts', there are about a dozen on each slope. \"It's a muddy holocaust,\" says Kling. Many fear that major disturbances to the Arctic, such as fires and the resulting thermokarsts, could change the vegetation on the tundra, so they are monitoring it closely as it rebounds. Tussocks are amazingly resilient and even the most severely burned ones sprouted as soon as the spring kicked in last year. Sphagnum moss and fruticose lichens, the 'down jacket' of the tundra that insulates the soil from incoming heat, have shown no sign of recovery. In their place are patches of copper-wire moss, with its red stems and golden, hanging capsules, and liverwort, holding up its peculiar umbrella-shaped reproductive organs.  \n                Spread of the shrubbery \n              The researchers worry that exposed mineral soil and more nutrient availability after fires will favour the expansion of species such as shrubs. This, too, could alter the net carbon exchange: studies of experimental plots show that more nutrients are associated with more shrubs and a net loss of soil carbon. But there are issues beyond that. Shrubs are drier and woodier so burn better than most other tundra vegetation \u2014 more shrubs growing after this fire could therefore push up the risk of future ones. \"Shrub expansion could be a positive feedback to climate change and fire frequency and severity,\" says Charles Racine, an ecologist who has been studying fires in Alaska since 1977, and who has retired from the Cold Regions Research and Engineering Laboratory in Hanover, New Hampshire. In collaboration with Randi Jandt of the Alaska Fire Service at Fort Wainwright, Racine and his colleagues have documented the vegetation recovery after the 1977 tundra fire on the Seward Peninsular in northwestern Alaska, which left less severe damage than the Anaktuvuk River fire even though a similarly large area was burned 5 . Thirty-two years after the fire, there is still no sight of the sphagnum moss or fruticose lichens, which before the fire covered 20% and 7% of the surface, respectively. By contrast, the area covered by willow, a deciduous shrub, rose from 5% in 1973 to 25% in 2001 and to about 40% this year. Philip Higuera, a palaeoecologist at the University of Idaho in Moscow, Idaho, and his colleagues looked further back in history. By analysing the pollen and charcoal contents in sediments from lakes in northern\u2013central Alaska that are 15,000 years old, they found that an increase in fire frequency coincided with the vegetation transition from herbs to shrubs around 14,000 years ago 6 . Whatever fuelled the blazes of the past, researchers are keen to find out what kept the Anaktuvuk River fire burning, and how others could be predicted in the future. The summer of 2007 saw the record high temperature and record low precipitation north of the Brooks mountain range, resulting in extremely dry soil 7 . Yet there were two other fires near the Anaktuvuk River that went out rapidly. Nancy French, an expert on remote sensing at the Michigan Tech Research Institute in Ann Arbor, and her collaborators, including Jandt and Racine, are hoping to get a grant from NASA to map past tundra fires across Alaska and Canada using satellite data. \"There is an urgent need to have a comprehensive record of tundra fires, so we could be in a better position to predict tundra fire regimes in the future,\" she says. The researchers plan to correlate tundra fires with weather conditions, and to construct computer models to calculate carbon emission during a fire, predict vegetation recovery and test how climate change could affect fires in the tundra. For the researchers at Anaktuvuk River, meanwhile, the tundra is changing almost daily. On the northeastern bank of Dimple Lake, a massive patch of soil has slumped into the water, muddying the turquoise with grey. Farther up the shore, the cotton grass ripples across the hill in a breeze. This is the scene after just one lightning strike sparked a fire. And researchers can be confident that lightning will strike again. Jane Qiu, a recipient of the 2009 Marine Biological Laboratory Logan Science Journalism Fellowship, writes for  Nature   from Beijing. \n                     Nature Reports Climate Change \n                   \n                     Road to Copenhagen \n                   \n                     National Science Foundation \n                   \n                     Arctic Long Term Ecological Research Site \n                   Reprints and Permissions"},
{"file_id": "461164a", "url": "https://www.nature.com/articles/461164a", "year": 2009, "authors": [{"name": "Henry Nicholls"}], "parsed_as_year": "2006_or_before", "body": "Hagfish and lampreys are the only surviving fish without jaws. And they could solve an evolutionary mystery, finds Henry Nicholls. In the basement of the National Museum of Natural History in Paris, two men come to a standstill in the long, gloomy corridor nicknamed 'the submarine'. Philippe Janvier, a senior palaeontologist at the museum, unlocks a door, flicks on the light and leads the way into the  'salle poissons' , the room that houses the museum's impressive collection of fossil fish. His visitor, Shigeru Kuratani, is a developmental biologist at Okayama University, Japan, who usually studies the lamprey \u2014 one of only two groups of jawless fish with living members. But today, he has come to see some of its long-extinct cousins. As Kuratani peers at the vivid impression of a jawless fish etched into rock around 400 million years ago, the two get talking. Janvier suggests that Kuratani try to get his hands on an embryo from a hagfish, the only other group of jawless fish that still survives. Few researchers have been able to do it; if Kuratani could, it might resolve a taxonomic dispute that has troubled scientists for more than a century. For several years after that encounter in 2000, Kuratani mulled it over. Then, in 2004, he took on Kinya Ota as a postdoc at his lab at the RIKEN Center for Developmental Biology in Kobe, and set him the task of succeeding where dozens had failed. \"If you get embryos,\" Kuratani assured him, \"just one or two, it will make a very important paper.\" Kuratani and Janvier are not alone in their obsession with hagfish and lampreys. To a dedicated group of biologists these 'living fossils' are highly prized for what they promise to reveal about some of the earliest events in vertebrate evolution. And advances in developmental biology and molecular genetics are starting to fulfil that promise. Hagfish and lampreys take researchers back around 500 million years to a time when the first jawed vertebrates, or gnathostomes, evolved along with a truly 'vertebrate' body plan. The gnathostomes eventually dominated; apart from the hagfish and lampreys, the jawless 'agnathans' went extinct. The question is how exactly the split occurred between the hagfish, lampreys and gnathostomes (pictured, left to right), and the conflict between researchers' answers has been described as \"one of the most vexing problems in vertebrate phylogenetics\" 1 . \"We are struggling with this discrepancy at the very base of the vertebrate tree and we can't get out of it right now,\" says Janvier. \"We have to find more and different kinds of data.\" It is a problem with a history. In 1806, French zoologist Andr\u00e9 Dum\u00e9ril decided that the striking but similar mouthparts of hagfish and lampreys meant that they should be grouped together (see graphic) and called cyclostomi, or 'round mouths'. But from the 1970s onwards, morphologists began to have their doubts. Looking beyond the mouth, they found that adult lampreys boast a suite of characteristics that hagfish don't have, including elements of a vertebral column, an ability to control water content by osmoregulation, and the presence of true lymphocytes, a type of white blood cell. This suggested a tree in which lampreys were more closely related to gnathostomes than to the more primitive hagfish lineage. \n               boxed-text \n             That might have been the end of it, were it not for molecular biology. From the first trickle of sequence data to today's bioinformatics deluge, just about every molecular analysis suggests that Dum\u00e9ril was right after all: hagfish and lampreys are more closely related to each other than either is to gnathostomes. In this case, the last common ancestor of the two had a vertebral column and other characteristics, and these were secondarily lost by hagfish. Only one of these trees can be right. It is rather important which one, as the precise route that these branches took has a profound effect on what can be inferred about the evolution of early vertebrates. For many researchers, the morphologists' tree is rather more alluring, as it would allow them to map out the events on the evolutionary path from headless invertebrates through hagfish with heads but no vertebrae, to lampreys with vertebrae but no jaws, to jawed gnathostomes (see 'Fossil finds' boxed-text ). But morphologists and molecular biologists \u2014 each of whom are staking out their own arrangements \u2014 seem unlikely to come to any kind of consensus. To Janvier, the idea of plugging these different types of data into a combined analysis doesn't make much sense. A study earlier this year did combine them, and in doing so it illustrated the depth of the divide. Thomas Near, a molecular systematist at Yale University, was the first person to force morphological and molecular data sets into a single analysis 1 . With molecular data pulled together from 4,638 ribosomal RNA sites and more than 10,000 amino acids, hagfish and lampreys emerge as undisputed sister groups. But the addition of just 115 morphological characteristics (from the skeleton and from the sensory, nervous and circulatory systems, for example) re-roots the tree, suggesting instead that lampreys are more closely related to gnathostomes. Near says that it is probably the molecular data that are giving the misleading result, because of difficulties in using DNA and protein sequences to shed light on events that occurred over a very short timescale \u2014 hagfish, lampreys and ghathostomes all diverged within a few million years \u2014 relative to the hundreds of millions of years that have passed since then. The findings give reason, the paper concludes, \"to view the strong support for cyclostome monophyly inferred from molecular data sets with a measured degree of skepticism\" 1 . So how to resolve the problem?  \n                Start at the beginning \n              That's where Kuratani's embryos come in. One way of working out evolutionary relationships is to look for a common developmental trajectory in the shape and growth of embryos \u2014 a field called 'evo-devo'. \"As a general rule there is a danger of looking at an adult and assuming homology between different structures,\" Kuratani says. \"Embryology cuts through that problem.\" What researchers want to do is line up the embryos of hagfish, lampreys and a descendant of an early jawed vertebrate \u2014 such as the tropical brown-banded bamboo shark ( Chiloscyllium punctatum ) \u2014 and compare not only their morphological development but also their patterns of gene expression. But getting hold of embryos from hagfish, lampreys or a species representative of early gnathostomes has proven extremely tricky. For many years, lampreys have been the only cyclostome that evo-devo biologists have had to work with. These slender animals spend most of their lives as mud-dwelling, filter-feeding larvae before metamorphosing into toothy adults that often latch onto fish, rasping them with their tongue until they make enough of a wound to suck blood. The embryos are available for only a few weeks a year, so are difficult to obtain. For several years, members of Marianne Bronner-Fraser's lab at the California Institute of Technology in Pasadena, for example, collected adults in the field, massaged the gametes from them, then performed  in vitro   fertilization and rudimentary investigations of lamprey development on the spot. Then, Bronner-Fraser says, \"we realized the adults could be FedExed\", and have since worked out how to extend their reproductive period in the lab. Hagfish embryos have been even more challenging. The natural habitat of the few dozen described species is in the sludge at the bottom of the ocean. So elusive are hagfish that in the 1860s, the Danish Royal Academy of Sciences and Letters in Copenhagen offered a reward for the first person to work out the reproductive and developmental secrets of the Atlantic hagfish ( Myxine glutinosa ). Almost a century and a half later, the prize is still unclaimed. After Ota accepted Kuratani's challenge, his first stop was the local fishermen. One of them agreed to supply some adult Japanese inshore hagfish ( Eptatretus burgeri ). Ota put them in a large tank back at Kuratani's laboratory, placed oyster shells and plastic drainpipes in the bottom to give the hagfish somewhere to hide, then regularly hauled the hideaway out on a rope to check for eggs. Finally, Ota found what he was looking for: a cluster of eggs deposited on the fine-grained sand 2 . A year later, the embryos became visible; a  Nature   paper followed soon after 3 . The researchers did not resolve the phylogenetic debate, though. The paper showed that in hagfish, development of the embryonic structure called the neural crest and expression of the genes there are very similar to what is seen in both lampreys and jawed vertebrates. Since then, further embryos have been forthcoming. \"We are trying to identify the basic design of vertebrates,\" says Kuratani. \"If we can resolve this phylogenetic relationship between lamprey, hagfish and shark, then we can nail what kind of shape would have been there in the latest common ancestor of vertebrates,\" he says.  \n                Head to head \n              For now, he and Ota are concentrating on comparing the heads of lampreys and hagfish. The head is a highly specialized structure that \"defines the vertebrates\", Kuratani says, because building features such as nostrils and a mouth opening required specific and \"elaborate\" developmental changes during evolutionary history. The researchers are comparing the first pharyngeal arch, for example \u2014 a nub of tissue that appears early in the life of vertebrate embryos and gives rise to the jaw and other head structures. This could show whether, as they suspect, the patterns of gene expression seen in the developing lamprey more closely resemble those observed in gnathostomes. While some researchers focus on embryos, others are concentrating on genetic sequences. With genome sequencing for the hagfish pencilled in by the US National Human Genome Research Institute in Bethesda, Maryland, the sea lamprey already sequenced to 6\u00d7 coverage and a draft genome assembled for the elephant shark (a jawed reference point), there is already a mass of genetic evidence to bring to the problem. But as Near found in his analysis, standard sequence data may not be enough. So some researchers are now looking to other molecular data, in particular micro RNAs (miRNAs) \u2014 the snippets of RNA that are not translated into proteins but perform important regulatory functions. miRNAs are continually added to the genomes of complex eukaryotes such as vertebrates and, once they find a use in a genetic network, they are highly conserved by evolution and rarely lost. This means that if researchers can identify which miRNAs are present \u2014 much as a morphologist would score the presence or absence of a physical characteristic \u2014 they can potentially reveal more about when the two lineages split than they can by comparing in detail other genetic sequences, which requires complex statistics. \"There's no other set of molecular data like it,\" says Kevin Peterson, a palaeobiologist at Dartmouth College in Hanover, New Hampshire. \"Unlike other molecular data, it's treated as a set of binary characters,\" he says. \"The morphologists can deal with these data.\" A couple of years ago, Peterson compared the miRNA sequences of numerous organisms, including invertebrates such as sea urchins, and vertebrates such as sharks. He unearthed an extraordinary pulse of miRNA acquisition somewhere between 550 million and 505 million years ago \u2014 at around the same time that complex vertebrate features such as the head, gills, kidneys and thymus evolved 4 . \"Something really amazing was happening to the vertebrate genome at that time,\" says Peterson. He says that acquisition of these miRNAs could have allowed cells to adopt more complex regulatory systems and to develop new and diverse cell functions. \"It's those miRNAs that I would argue allow you to get novel cell types,\" he says. But can this help solve the hagfish\u2013lamprey problem? Peterson has been working with palaeobiologist Philip Donoghue of Bristol University, UK, to produce a library of the miRNAs present in hagfish, lampreys and some living gnathostomes \u2014 elephant shark, zebrafish and human. \"We can use their presence or absence to finally resolve after 150 years or so the relationships between hagfish, lampreys and gnathostomes to work out the pattern of assembly of the body plan of jawed vertebrates,\" says Donoghue. The libraries have been sequenced and analysed, although neither Peterson nor Donoghue is giving away the result \u2014 yet. On that cliffhanger, the story now rests. Whichever phylogenetic tree Peterson's results favour, he is hoping that it will be something that morphologists and molecular biologists can mull over together. \"Our data clearly indicate that one answer is right,\" teases Peterson. \"They unequivocally resolve the debate.\" Henry Nicholls is a freelance writer based in London. \n                     Kuratani Lab \n                   Reprints and Permissions"},
{"file_id": "461330a", "url": "https://www.nature.com/articles/461330a", "year": 2009, "authors": [{"name": "Meredith Wadman"}], "parsed_as_year": "2006_or_before", "body": "Paul Thacker, a reporter-turned-Congressional-investigator, has disrupted the careers of several top researchers with lucrative industry ties. Meredith Wadman tracks his effect on US science. In February 1991, a US Army specialist, one month past his 21st birthday, was busy driving huge supply trucks bearing tank ammunition through the Saudi Arabian desert as part of the US First Infantry Division in the Gulf War. At night, he would watch the vast desert sky, lit by the bombs of US warplanes pounding Saddam Hussein's Iraqi troops. \"You felt sorry for them,\" he recalls, \"but better them than us.\" At the same time, half a world away, a 41-year-old MD\u2013PhD at Duke University in Durham, North Carolina, was preparing to move to Emory University in Atlanta, Georgia. There he would take over an academically obscure psychiatry department and turn it into a Mecca of psychiatric research \u2014 in the process making himself a magnet for megabucks from the pharmaceutical industry. Both men were bright, driven, ambitious and not easily deterred from their goals. But in an unlikely collision, 17 years later, one would dramatically change the life of the other. The army specialist was Paul Thacker \u2014 now an investigator working for Senator Charles Grassley (Republican, Iowa). Thacker is the point person behind a far-reaching probe into the financial reports required from biomedical researchers who receive federal funds. The rising academic star was Charles Nemeroff, the chair of psychiatry at Emory until he was forced out of that position last December as a result of Grassley's investigation. Thacker had requisitioned documents from the pharmaceutical industry showing that Nemeroff had violated National Institutes of Health (NIH) rules by failing to declare to the university at least $1.2 million in income from drugmakers. Nemeroff is not the only academic target that Thacker has hounded in the past 18 months. He has identified seven other physician\u2013researchers who broke NIH rules by not disclosing large payments from drug and device companies. In several cases, the researchers were involved in human trials of the products made by some of the very companies with which they had financial relationships. The revelations and the ensuing negative publicity have had profound effects on US universities and biomedical scientists. So far, several dozen leading research institutions in the United States have tightened their financial oversight or are in the process of doing so. And the NIH is planning to strengthen the conflict-reporting requirements for universities and researchers who receive its funds. The outcry has also created support for a federal law proposed by Grassley, the Physician Payments Sunshine Act, which would require drug and device companies to register on a public website virtually all payments made to physicians. By the unwritten rules of most congressional offices, employees such as Thacker work behind the scenes and rarely get mentioned by the press. But the path that Thacker took to becoming the ethics cop of US biomedicine shows how one Senate employee can affect an entire field of research. Thacker has become worrisome enough to university officials that this March, administrators at Tufts University in Medford, Massachusetts, where Thacker was making preliminary inquiries about a researcher, refused to participate in a public ethics discussion unless Thacker was taken off the list of planned speakers. The Grassley investigation \"has changed the practice of medicine\", says Steven Nissen, a cardiologist at the Cleveland Clinic in Ohio, which last December began requiring its physicians to publicly disclose industry payments of more than $5,000 a year. \"And Paul Thacker is the bulldog that's leading the charge.\" That canine breed comes up frequently in descriptions of Thacker, a former investigative reporter. His tenaciousness, however, hasn't always been well received. Three years ago, Thacker was fired from  Environmental Science & Technology , a publication of the American Chemical Society (ACS), based in Washington DC, which represents academic and industrial chemists. He had written a series of expos\u00e9s that a senior ACS official claimed showed an anti-industry bias. Many, both inside and outside the biomedical research community, have praised the Grassley team's investigation for helping to clean up the field. But others contend that Thacker's zeal, given the power of Senate backing, has crossed the line from vigilant to harmful; that, however egregious his findings about researchers such as Nemeroff may seem, he is using a damningly broad brush that tars an entire community, the great majority of whom are playing by the rules. What's more, institutions and the federal government, in the interest of fuelling drug discovery, encourage researchers to foster industry collaborations, an art that Nemeroff had perfected. A case such as Nemeroff's \"has been portrayed as this black and white thing and it's not. It's nuanced and it's subtle and it's difficult. And there are a lot of aspects to this whole issue where there are no right answers,\" says Joseph Cubells, a neurogeneticist at Emory and a colleague of Nemeroff.  \n                From Iraq to Congress \n              Thacker is a compact man with short-cropped blond hair, intense blue eyes and an open, direct manner. He makes $63,000 a year working in a small, windowless room deep in Grassley's warren of offices in the Hart Senate Office Building on Capitol Hill. He is assisted piecemeal by other employees, including interns, bringing to roughly two the number of full-time people that Grassley has devoted to the NIH investigation. Thacker grew up in California and Texas, the eldest child of a construction supervisor and a childcare worker. He joined the army after high school and served in Saudi Arabia and Iraq during the Gulf War. After obtaining a biology degree at the University of California, Davis, in 1997, he moved to Atlanta and went to work at Emory as a biomedical lab technician. He was thinking of going into science but dreaded the prospect of more classwork. Soon, he was writing pieces on faculty doings for  The Emory Report . In 2000, he left for a journalism internship at  Audubon Magazine   and later joined the staff of  Environmental Science & Technology , in an upscale office building two blocks from the White House. Thacker became known there for both his charm and intensity. He was often seen pacing circuits of the office's square hallway, deep in thought, his hands jammed in his pockets. Alan Newman, Thacker's former editor, recalls Thacker's delight when he was closing in on a story. \"He had this laugh \u2014 this sort of 'ah-hah' laugh [that said]: 'I got it. I'm seeing the pieces fall together.'\" In May 2006, Thacker published the article that would begin his undoing at the ACS. Called 'The Weinberg Proposal', it focused on a consulting firm based in Washington DC called the Weinberg Group, which offers corporate clients advice on how to 'shape the debate' about potentially toxic compounds. Acting on a tip, Thacker had dug up an unsolicited written sales pitch in which the Weinberg Group proposed using scientists and publications to help the materials firm DuPont dispel concerns about a chemical \u2014 perfluorooctanoic acid \u2014 that Dupont was being sued over in a West Virginia class-action lawsuit alleging health effects. Dupont did not hire Weinberg for the proposed work but did engage the company in another capacity. Dupont says it settled the claims in 2005 for $107 million. Thacker's article would later prompt a congressional investigation by John Dingell (Democrat, Michigan) about Weinberg's involvement defending another controversial chemical. But it also hit a nerve with the ACS president, who communicated his unhappiness to Rudy Baum, the society's top news editor. In a meeting with Baum, Thacker was told that he was too immature for investigative reporting. Baum later described the Weinberg piece in the Society of Environmental Journalists' newsletter as a \"hatchet job\". He said he was \"uniformly unimpressed with Paul's journalistic skills\". Others disagreed. The article generated media attention for the Weinberg group and it was part of a package that was honoured by the Society of Environmental Journalists. Baum told  Nature   that Thacker's article was factually correct.  \n                Fired for insubordination \n              After the Weinberg piece ran, Thacker went on to pursue another investigative story \u2014 this time about White House interference in climate science, but he was not allowed to publish the piece. So he found a new job and gave his notice, then published the climate story on Salon.com. Two days before he was scheduled to leave the ACS, he was fired for insubordination. Despite that setback, Thacker left with a set of new skills that soon helped him make the switch from journalism to congressional investigator, a position that tends to draw tenacious individuals who often view the world in stark ethical terms. Some journalists say that they look at Thacker with both respect and wariness. \"When he started working for Grassley, my feeling was: 'My God, I hope I'm never on the other end of his gun,\" says a science journalist who did not want to be identified for commenting on a fellow reporter. When Thacker arrived on Grassley's staff in April 2007, the senator was already well known for his investigations, including a probe of how drug-industry payments might be influencing advisers on government drug approvals. Thacker himself had spent little time thinking about the finances of biomedical researchers. But within weeks of his hiring, he was inspired to pursue a new investigation on that topic. The impetus was an article in  The New York Times   describing payments from drug companies to psychiatrists in Minnesota, the only state that at the time required companies to make public their payments to physicians.  The New York Times   had found that the psychiatrists taking the most from companies were more likely to prescribe a controversial class of anti-psychotic drugs to children. Melissa DelBello, a psychiatrist from the University of Cincinnati in Ohio, was quoted in the story, refusing to disclose how much she made from the drugmaker AstraZeneca. DelBello had published a company-funded study in 2002 that reported positive results for AstraZeneca's anti-psychotic drug Seroquel (quetiapine) in adolescents with bipolar disorder. Sales of the drug jumped 27% the year after publication. \"Trust me, I don't make much,\" she told the newspaper. (DelBello later said she was misquoted, that she had been referring only to her fee for giving a single lecture.) The \"trust me\" quote caught Thacker's eye. Still holding the newspaper, he walked into the office of Emilia DeSanto, Grassley's chief of investigations. \"This reads like nonsense,\" Thacker told her. From working for the  Emory Report , he added, he knew that researchers had to file financial-interest disclosure forms with their universities. Thacker drafted a letter to the general counsel of the University of Cincinnati, requesting copies of DelBello's disclosure forms. Days later, it went out with Grassley's signature.  \n                Missing money \n              Three months later, Grassley made a nine-minute speech on the Senate floor launching his investigation by announcing that AstraZeneca had said it had paid DelBello more than $180,000 in 2003 and 2004, the first two years after the Seroquel study was published. The money was for lectures, consulting fees, travel expenses and other services. Within days, DelBello and the University of Cincinnati challenged Grassley in comments that appeared in the online publication  Inside Higher Ed.   University spokesman Richard Puff said that Grassley had falsely implied that she was disingenuous. \"She has been completely open in disclosing her payments. She's made complete disclosures to the university and its [institutional review board]. Furthermore, she's made full disclosure to the Senate Finance Committee,\" he said. Thacker read the article and wasn't happy. Several days later, on a \"lark\", he says, he attached a question to a letter going out the door to AstraZeneca from another Grassley staffer on another matter. He asked the company for records of how much it had paid DelBello in recent years. Even as he was digging into the DelBello case, he was having trouble rationalizing Grassley's involvement in the issue. It had to be linked to the senator's work as an overseer of federal funds: he is the top Republican on the finance committee. Yet the arrangements between DelBello and AstraZeneca were in the private sector. Thacker eventually found the connection: NIH grant recipients are compelled, under federal rules in place since 1995, to disclose financial interests of more than $10,000 in cash or 5% equity in a company to their universities. \"That was the whole hook,\" Thacker recalls. \"You tie in NIH money, that's when a senator can come forward and say: 'Hey look, you're taking taxpayer dollars'.\" After that, \"it became a process\", says Thacker, of identifying NIH-funded researchers who were probably earning top dollar from drug or device firms, often by following leads in the media or from whistleblowers. Thacker would solicit researchers' conflict-of-interest reports from their universities and simultaneously ask relevant companies to provide their own records of what they had paid those individuals. So far, Thacker and his colleagues have sent letters to more than 30 universities requesting the financial-disclosure forms filed by some 50 researchers. Among the first letters was one that went to Emory University on 25 October 2007, asking for the financial disclosure reports filed between January 2000 and January 2007 by Charles Nemeroff, a leader in the field of psychopharmacology (see  graphic ). boxed-text Over the next few months, Thacker collected records methodically. He was in his element. \"Good investigation to me is like doing science,\" he says. \"You know something before anybody else knows it.\" By spring 2008, he had his ducks in a row. He was also getting ready to work the media as perhaps only a former reporter can, tipping journalists to findings published by Grassley in the obscure pages of the  Congressional Record . That generated major stories in  The New York Times   and the  Wall Street Journal   along with  Nature   and  Science , among other science publications. \"He really knows how to work the phones and maintain contact with folks in a way I've never seen with anybody else,\" says Newman. In April, Grassley took to the Senate floor to announce that AstraZeneca had paid DelBello $238,000 between 2005 and 2007, during which she had reported receiving $100,000 from all sources. \"Obviously, the university is engaged in the practice of 'trust but did NOT verify',\" Grassley concluded. (When contacted last week by  Nature , DelBello referred questions to the university, which said that she is closely following a conflict-management plan. The university also said it had strengthened its conflict-reporting policy for researchers.) Relying on Thacker's harvest, Grassley was just getting warmed up. In June 2008, he reported that three psychiatrists at Harvard University had each under-reported hundreds of thousands of dollars in payments.  \n                Storm at Stanford \n              The same month, he reported that Alan Schatzberg, the chair of psychiatry at Stanford University in Palo Alto, California, owned equity then worth $6 million in the drug firm Corcept Therapeutics, in nearby Menlo Park. Schatzberg had co-founded the company to develop the drug mifepristone for treatment of psychotic depression. At the same time, he was principal investigator of an NIH-funded study, which included a component that tested the drug. Grassley's report triggered a media storm and Stanford removed Schatzberg as principal investigator, pending an investigation. The university said that although it believed that Schatzberg and Stanford had not broken any rules, \"we can see how having Dr. Schatzberg as the principal investigator on this grant can create the appearance of a conflict of interest\". (Schatzberg says that Stanford and the NIH had preapproved his service as the study's principal investigator and that \"I did not run the [mifepristone] trial at all\". Stanford reinstated him as principal investigator in late July this year after the NIH wrote to the university saying that no rules had been broken; by that time the mifepristone part of the study had concluded.) Back in 2008, with his investigation making news, Grassley turned up the heat on then-NIH director Elias Zerhouni to better police grant recipients. \"NIH oversight of the extramural program is lax and leaves people with nothing more than questions,\" said Grassley in a letter to Zerhouni that he inserted into the  Congressional Record   on 4 June.  \n                Years of trouble \n              When Emory received Grassley's letter about Nemeroff, the questions about his finances did not come as a surprise to research administrators there. Over the course of 16 years, Nemeroff had built the psychiatry department into one of the field's leading centres. But at the same time, he had developed substantial financial ties to many companies and a history of failing to disclose them to the university. Seen from one perspective, the connections to industry were just what universities and the federal government wanted. In fact, Nemeroff had won a $3.95-million grant from the National Institute of Mental Health (NIMH) in 2003 that would test at least five new antidepressants developed by the British drug giant GlaxoSmithKline, whose advisory board Nemeroff chaired. In soliciting grant proposals, the NIMH had asked researchers to involve industry as a way to accelerate the development of new drugs. But Nemeroff had made mistakes in managing these relationships, in the eyes of Emory administrators. By late 2003, officials there had grown concerned enough about his network of industry ties that the university's conflict-of-interest committee launched an investigation. In a 14-page report issued in May 2004, it concluded that there were \"serious lapses\" in Nemeroff's reporting of his relationships with 19 companies. Two years later, Nemeroff's name landed in the  Wall Street Journal . He had favourably reviewed an implantable device as a treatment for depression, without disclosing that he had financial ties to the company that made the device (C. B. Nemeroff  Neuropsychopharmacology   31 , 1345-1355; 2006). He was also editor of the journal  Neuropsychopharmacology , where the review was published. A representative of that journal told the  Wall Street Journal   that Nemeroff had filled out author-disclosure forms but did not report his financial ties in the manuscript, as the journal required. Stories such as this one soon caught the attention of Thacker when he was looking for investigational targets. Privately, Emory tried to rein in its star psychiatrist. \"I can't remember when I have gotten so many complaints about the action of one or more of our faculty from inside and outside the institution,\" one administrator wrote in an e-mail to Nemeroff in the wake of the  Neuropsychopharmacology   article controversy. But the university was singing another tune publicly, especially after Nemeroff landed a $9.3-million grant from the NIMH for a trial on depression. The three-armed trial aimed to compare the effectiveness of two drugs against cognitive behavioural therapy \u2014 'talk therapy' \u2014 in people with major depression. Its investigators planned to use genetic scans and brain imaging to see whether there are factors that predispose some people to respond well to different treatments \u2014 creating a rational alternative to the 'try-this-pill-and-see-if-it-works' approach that is common practice in psychiatry today. As is typical of large clinical trials \u2014 this one originally aimed to enrol 400 people \u2014 it took months to gear up. Participants weren't easy to find because they had to agree to an arduous 14-week course of investigations and treatment and they couldn't participate if they had ever been treated for major depression. By August 2008, two years into their funding, the researchers had enrolled just a handful of patients. Still, there was a sense that enrolment momentum was growing. The NIH, however, had become aware that Grassley's investigation had found large discrepancies between what Nemeroff had disclosed to Emory and what he had actually earned. Zerhouni quietly approved a suspension of the study in mid-August, forcing the university to stop enrolling patients. In mid-September, Emory learned in a letter from Grassley about the discrepancies. Of $2 million he had collected from drug companies between 2000 and 2007, Nemeroff had failed to report at least $1.2 million. Any hope of keeping those disclosure lapses private disappeared in October when the story broke in the media. At the same time, reporters learned that the NIH had suspended the study. \"Frankly, the integrity of the research was at stake,\" Zerhouni says, explaining why the NIH had stopped supporting the study. \"We said: This can't continue [under this principal investigator].\" Grassley had wanted this outcome all along. \"All NIH should have to do is pull back one grant or refuse to give a grant to a university that's not [policing conflicts properly] and they all get in line,\" Grassley told Nature Medicine last September (M. Wadman  Nature Med.    14,    1006\u20131007;  2008). Nemeroff's study is the only one to have been suspended because of Grassley's investigations. And ironically, although his financial connections with industry were the source of the problem, the Emory trial is not one that would necessarily be beloved by drugmakers. By including a cognitive-behavioural therapy group, in which patients take no drugs at all, it opens the distinct possibility of identifying patient subgroups for whom psychotherapy should be the first-line, or even the only, treatment. The trial's structure, says Cubells, \"is a really important example of [Nemeroff's] intellectual honesty. There is a huge prejudice against non-medication treatment approaches in all of psychiatry, due in no small part to the fact that when well-executed psychosocial interventions are provided, no corporate interests profit. Charlie has bucked this trend actively.\" What annoys Cubells, he says, is \"the unproven insinuation of the Grassley witch hunt that [Nemeroff] is some kind of advocate for pharmaceutical companies because of his ties with the industry\". But others say that the investigation is on the mark and that anger at Grassley is misdirected. \"People flouted the rules, didn't disclose, and did it for years on end, repeatedly,\" says Zerhouni. \"That tells you the problem is not Grassley. The problem is our current system of managing conflicts,\" he says. Prompted by Thacker's findings, Emory conducted its own investigation into Nemeroff. In December 2008, it announced that Nemeroff had received \u2014 and not declared \u2014 $800,000 in payments from GlaxoSmithKline for giving more than 250 talks to psychiatrists between January 2000 and January 2006. Nemeroff told the university that, because the talks dealt with general educational topics, such as the treatment of depression, he didn't consider them promotional and thought they were off-the-books under Emory's rules for declaring outside income. The evidence that Emory examined supported Nemeroff's description of his talks, but the university concluded that he should have declared the income. He was removed from the position of chairman and the university forbade him to apply for, or be involved with, any NIH grants for at least two years or to receive any outside compensation without prior review and approval by the dean's office. It also issued a statement that the investigation \"found no evidence that Dr. Nemeroff's outside speaking activities affected clinical care for patients or persons enrolled in clinical trials, and no evidence that his activities biased scientific research in which he was engaged\". On 4 March this year, the depression study was restarted, this time with a different principal investigator. Nemeroff, who has not talked with the press since last October when news broke about his disclosure lapses, told  Nature   last week that \"I made mistakes in the area of conflict of interest for which I am sorry and remorseful. However the mistakes I made were honest mistakes.\" He explained that \"my actions were, in my view at the time, in keeping with my understanding of the current Emory policies\". Nemeroff adds that he continues to see patients and teach and \"I also plan to use my recent experience to help others avoid problems with conflict of interest from the lessons I have so painfully learned.\"  \n                Open payments \n              He and others will have to keep up with a changing world. In the coming months, any health reform that becomes law may well include the Physician Payments Sunshine Act, which is part of legislation that is being developed in both the House and the Senate, with the House version lowering the threshold for reportable payments to $5. The prospect of a public website that documents every payment to physicians isn't being received with enthusiasm by some researchers. \"By dint of [Grassley] posting every relationship every physician has, it's guilt by association,\" says Jeffrey Garber, chief of endocrinology at Harvard Vanguard Medical Associates in Boston and an active member of the New York-based Association of Clinical Researchers and Educators, a group of physicians that sprang up in July to resist what it calls the overreach of the conflict-disclosure movement. A new sunshine law in Massachusetts, for example, requires companies to disclose online any payments to doctors of more than $50. Some researchers complain that the disclosures are an invasion of privacy; others argue that lowering the reporting threshold, as the NIH is expected to do, will take a toll. \"The thing that I am concerned about is the regulatory burden. When it comes to practising scientists, we want them to have time to do experiments,\" says Paul Kincade, an immunobiologist at the Oklahoma Medical Research Foundation in Oklahoma City. It remains unclear whether a national disclosure law would deter the industry\u2013academic collaborations that are touted as the best way to get cures into clinics, especially in this tight funding environment. In fact, many university administrators welcome the prospect. \"It becomes a tool that complements our own public disclosures,\" says Steven Fluharty, vice-provost for research at the University of Pennsylvania in Philadelphia. On 1 July, the university's School of Medicine began posting on a public website all relationships between faculty members and any industry from which they receive any amount of money. Grassley's revelations, adds Fluharty, \"have been an eye-opening experience for us all\". Thacker, in the meantime, has set his sights on some new targets: military doctors who fail to report financial disclosures as they are required to do; and spinal surgeons with lucrative ties to medical device-makers such as Medtronic in Minneapolis, Minnesota. In an interview with  Nature   in June, Grassley said that he is pleased with his lead biomedical investigator and what he has been able to achieve in two short years. \"Paul's good,\" said the senator, sitting across from Thacker. \"If you're going to be successful in these investigations, you gotta have people like Paul.\" Thacker, for his part, says he likes his job because \"you can have an impact. I wanted to do something that, when it hits, it reverberates for a while.\"   See Editorial,  \n                     page 315 \n                   . \n                     Senate Finance Committee compendium of hyperlinks to Nemeroff-related documents \n                   \n                     Senate Finance Committee archive of news stories by targeted investigator \n                   \n                     Emory University index of hyperlinks on conflict and Nemeroff-related announcements \n                   \n                     National Institutes of Health proposed rules changes on conflict disclosure for extramural investigators \n                   \n                     SE Journal, summer 2007: Thacker's account of his ouster from Environmental Science and Technology, and Rudy Baum's written response \n                   \n                     Association of Clinical Researchers and Educators \n                   Reprints and Permissions"},
{"file_id": "4601076a", "url": "https://www.nature.com/articles/4601076a", "year": 2009, "authors": [{"name": "Nicola Nosengo"}], "parsed_as_year": "2006_or_before", "body": "He looks like a child and plays like a child. But can the iCub robot reveal how a child learns and thinks? Nicola Nosengo reports. Giulio Sandini cannot help smiling as his child reaches out a hand and tries to grasp the red ball that Sandini keeps waving before his eyes. \"He is getting really good at it,\" he says, with the proud tone of any father. True, most fathers would expect more from their three-year-old than the ability to grasp a ball. But Sandini is indulgent: although the object of his affection has the wide eyes and rounded cheeks of a little boy, he is, in fact, a robot. His name is iCub or, as the team calls him, iCub Number 1. Together with his brothers now in laboratories around the world, this little robot may help researchers to understand how humans learn and think. Grasping a ball is only a first step, says Sandini, director of the robotics and cognitive-sciences department at the Italian Institute of Technology (IIT) in Genova, and head of the child-robot project since it started in 2004. Sandini is confident that iCub will learn more and more tricks \u2014 until, in the end, he is even able to communicate with humans. \"We wanted to create a robot with sufficient movement capabilities to replicate the learning process a real child goes through\" as it develops from a dependent, speechless newborn into a walking, talking being, Sandini says. So he and his colleagues have not only given iCub the hands, limbs and height of a toddler, they have also tried to give him the brain of one \u2014 a computer that runs algorithms allowing iCub to learn and develop as he interacts with his surroundings. In a child, says Luciano Fadiga, a neurophysiologist at Italy's University of Ferrara who is part of the team that developed iCub, those interactions are essential for shaping the rapidly growing brain. Before children can grasp a moving ball, for example, they must learn to coordinate head and eye movements to keep the ball in their visual field; use visual clues to predict the ball's trajectory and guide their hand; and close their fingers on the ball with the right angle and strength. None of these abilities is there at birth, and children cannot grasp appropriately until they reach around one year of age. \"Many theories try to explain what happens in the brain as it learns all this stuff,\" says Fadiga, \"and the only way to test them is to see what works best in an artificial system.\" Such testing is certainly not new. Cognitive scientists have been using computer models to simulate mental processes since the 1950s, including algorithms that mimic learning. But many of these simulations have focused on the high-level, conscious reasoning used to solve logical puzzles, play chess or make medical diagnoses. And many others \u2014 notably 'neural network' models \u2014 have simulated neurons. But Sandini and Fadiga are among the many researchers who have come to think that both types of simulations leave out something essential: the body. \"There is ever-growing evidence from neuroscience that visuo\u2013motor processing, and manipulation in particular, are crucial for higher cognitive development, including social behaviour and language,\" Sandini says. It was this line of thinking that led Sandini and his co-workers to their central hypothesis \u2014 that the best way to model the human mind would be to create a humanoid robot that is controlled by realistic learning algorithms, then let it explore the world as a child would. They gathered together scientists from 11 European universities and research institutions to form the RobotCub project, and began work with \u20ac8.5 million (US$12 million) in funding from the European Union. The IIT is the project's leading partner, and it is here that iCubs are born.  \n                Form and function \n              Researchers can already choose from a list of robots that includes Khepera, a simple and affordable wheeled robot built by a Swiss consortium and used to study locomotion, and humanoid robots such as HRP-2, PINO and ASIMO, all built in Japan. But Sandini's ambition was to create a humanoid robot that combined unprecedented mechanical versatility with open-source software, so that researchers could change both the hardware and the algorithms as needed. \"We started from the hand, and built the rest of the robot around it,\" Sandini says. With seven degrees of freedom in the arm and nine in the hand, and its mechanical shoulders, elbows, wrists and fingers controlled by electric motors, iCub's arm is by itself a robotic marvel that took years to perfect. Next, iCub needed human-like senses. Project engineers gave him stereoscopic vision through two cameras mounted on moving ocular bulbs, complete with eyelids that close every now and then. And they gave him touch through sensors on his arms that can detect pressure applied from outside. They are also developing an artificial skin that will allow the robot to detect an object's shape and surface properties. Other team members tried to figure out what should happen inside iCub's brain. They decided to give him a few innate abilities similar to those seen in newborns, such as recognizing a face as being human and detecting objects against a background. Everything else would have to be learned. After reviewing evidence from neuroscience, psychology and animal studies, they came up with a three-level software architecture, mostly designed by Giorgio Metta of the IIT and by David Vernon of the Khalifa University of Science, Technology and Research in Sharjah, United Arab Emirates. The first level gathers information on what iCub sees and feels. It collects raw signals from the cameras and other sensory systems, and channels them through a set of filters to determine which signals are most salient \u2014 a process similar to the human attention system. The second level is a kind of traffic director called the 'modulation' system. Loosely based on the functions of the hippocampus, basal ganglia and amygdala, this system takes in data from the lower level as the robot tries to grasp a ball, say, compares those data to combinations of action and sensory information iCub has encountered before, then decides what the robot should do next. In doing so, the modulation system is driven by some basic motivations, corresponding to a child's curiosity for new stimuli and tendency to engage in social interactions. The third level uses prior experience to play 'what if?' with the current situation. What will happen if I move towards the ball with this force and this angle? What if the ball moves in the meantime? This information goes back down to the middle level to help determine the robot's next action. It was only after almost three years of effort that the team finally activated its first complete robot. Their artificial child could now move, see and touch, which was all the researchers technically needed. But, in a symbolic gesture of whimsy, they decided they should also make it look like a child. The undeniably cute result was a semi-transparent mask with luminous colour light-emitting diodes under the surface to outline eyebrows and a mouth, allowing the robot to smile and frown. This engaging face may have more uses than just making the robot look good in promotional pictures, says Fadiga. \"In the future, some groups plan to try iCub with children who are autistic, testing their reactions to his expressions and movements\". iCub Number 1 was never meant to be an only son. After the first robot became operational, the consortium issued an open call for proposals to conduct experiments. The six winners, chosen by an independent panel appointed by the consortium and the European Union, have received their own iCub for free. And anyone else can order one for the cost of producing it, some \u20ac180,000\u2013200,000. \"It was part of the deal with the European Union that we should provide a number of robots to interested groups,\" Sandini says. This way, the team hopes to create a de facto standard in robotics, facilitating data exchange. \"There is a desperate need for standardization in our field,\" says Paul Verschure, a technology professor at the Catalan Institute of Advanced Research in Barcelona, Spain, and one of those selected to receive an iCub. \"People use a variety of platforms, they rarely publish every detail of their algorithms, and replication of experiments is minimal.\"  \n                Small has its place \n              However, not even the most ardent enthusiasts believe that iCub will send all other robots into retirement. \"Simpler and more affordable robots will remain important,\" says Chad Jenkins, a computer scientist at Brown University in Providence, Rhode Island. Jenkins chaired the robotics workshop at the International Joint Conference on Artificial Intelligence in Pasadena, California, at which iCub made his American debut in July. \"They may be more limited in the long run, but they are easier to use in small-scale experiments.\" Indeed, most of the researchers who have got their hands on an iCub find that programming him is a hard job, and it is likely to take some time before meaningful results can be seen. \"Nobody expected anything different,\" says Verschure, \"this is not a car you just buy and start to drive around; we're in totally new ground.\" Sandini agrees that the robot is still a work in progress, and predicts that it will take two or three years to see an impact in terms of publications in major journals. But, he adds, a better measure of success will be if neuroscientists and psychologists start to see the robots as useful experimental tools. \"Engineers creating an intelligent robot and neuroscientists studying the brain are asking the same questions, only with different words,\" Sandini says. \"How can this particular task be performed with limited computational power?\" Convincing neuroscientists may be the hardest part, though, particularly those who question iCub's theoretical background. Although he sees many interesting ideas in iCub, Alfonso Caramazza, director of the Cognitive Neuropsychology Laboratory at Harvard University, says that \"the claims being made about shared intentions and language in robots still seem light years away from being realized\". In particular, he says, \"to account for cooperation and communication you also need symbolic thought, and I do not see how such mechanisms can emerge from mere sensory-motor processes in a robot\". Sandini and Fadiga reply that a complete explanation of higher cognitive functions is a problem for any area of neuroscience, not just robotics. Furthermore, iCub's emphasis on perception and manipulation may one day lead to a better understanding of what 'symbolic thought' really is. The final word, of course, will come from the robots themselves. Eight iCubs have left Genova since late last year for laboratories in Europe and Turkey, and ten more are being built. Add the two kept by Sandini's team, and the family will comprise 20 brothers by the end of the year.  \n                Great expectations \n              Most of the researchers will first try iCub in experiments they were already performing with simpler robots. In Barcelona, for example, Verschure's group plans to see how a computer model of a cerebellum it has been working on for years performs in an iCub. \"The cerebellum is a crucial organ for motion, it sets the timing and pace of our movements,\" Verschure says. \"But the timing of actions depends on the body's shape. For example, the speed at which I can move my arm is limited by its length and weight. So it makes no sense to study the cerebellum outside the body.\" At Imperial College London, Murray Shanahan, professor of cognitive robotics, is teaching his iCub to do very basic motor tasks, such as making circular hand movements, using a 'spiking' neural network. These artificial neurons not only 'fire', but the intensity of their firing changes over time. \"It is a more biologically plausible model than typical networks,\" Shanahan says, \"one in which the temporal dynamics of neurons is also modelled.\" Once validated, the spiking-network concept will be used to simulate more complex tasks. The IIT's two robots will be used to study the development of goal-oriented behaviour. Researchers will start by teaching them to recognize a specific object such as a hammer among many different objects, then to grasp the hammer by the handle and not by the head, and finally to swing it appropriately while hitting a nail, something it may learn by imitation. But to prove that it has really developed an internal representation of what a hammer is for, the robot will have to be able to use it in cooperation with humans. Fadiga says that in two or three years he hopes to get to the point at which \"I pick a nail, I put it on the wall, there is a hammer on a table nearby. The robot sees all this and, without any further input, grasps the hammer and hands it to me\". As happens in every family, the brothers face different expectations. At Britain's University of Plymouth and at the Institute for Cognitive Sciences and Technologies in Rome, researchers led respectively by Angelo Cangelosi and Stefano Nolfi are undertaking what is probably the most ambitious of the iCub projects: studying how children learn language. \"We believe that manipulation and communication co-evolve during the first three years\" says Cangelosi. \"Children learn relations among objects by touching them, and learn to express the same relations with language.\" To test this idea, he and his colleagues will treat the robot pretty much like a real child. While showing it a blue cup, for example, they will say 'blue cup', and so on, so that in time iCub can make associations between the sounds he hears and the data coming in through the visuo\u2013motor system. \"By the third year of the project, he should be able to use transitive and intransitive verbs in simple sentences, such as 'put red cup on yellow cup',\" says Cangelosi. In the very long term, the project aims to give the robot the ability to communicate with humans using natural language, with basic but appropriate grammar \u2014 but with no language rules coded anywhere in its software. Language, the researchers hope, will gradually emerge as an extension of visual and motor abilities, providing a strong proof of principle that the same may have happened in humans during evolution. But some question whether iCub can live up to such expectations. Oliviero Stock, a senior researcher at the Bruno Kessler Foundation in Trento, Italy, and a leading expert in the application of artificial intelligence to linguistics, says that the 'bottom up' approach that iCub adopts can go only so far when it comes to language. \"People in the field are coming to terms with the fact that to explain language, you have to presume some innate abilities for it, for grammar, and syntax in particular,\" he says. \"I doubt that such a system can do more than utter single words without some kind of a priori linguistic skills, call it a language instinct if you want. That's what probably happens in humans too.\" The idea of an innate predisposition for language in humans was famously introduced in the 1950s by Noam Chomsky, who wrote of a \"universal grammar\" that children seem to be primed for. Although hotly debated, this has become the dominant view in linguistics. iCub might now provide a way to put it to a rigorous test. While grown-ups have been arguing passionately about their robots' talents, the iCub brothers have been having a good time, as would be expected from children during the summertime. In late July the Italian brothers spent two weeks on Italy's Liguria coast, where 37 roboticists from many parts of Europe and the United States had convened for a summer school, arranged by Sandini and his colleagues to give them a chance to try their algorithms on an iCub. In pictures posted on the Internet, the robots can be seen wearing hats, grasping the usual ball, and even assembling LEGO bricks. Their actions are not exactly masterful, Sandini admits. But, ever the encouraging father, he says it is only a matter of time. Nicola Nosengo is a freelance science writer based in Rome. \n                     RobotCub \n                   Reprints and Permissions"},
{"file_id": "461336a", "url": "https://www.nature.com/articles/461336a", "year": 2009, "authors": [{"name": "Daniel Cressey"}], "parsed_as_year": "2006_or_before", "body": "How do researchers and policy-makers decide on the value of health? Daniel Cressey looks at Britain's National Institute for Health and Clinical Excellence. In May 2008, a research report concluded that it would be too expensive to keep some British people with kidney cancer alive. The 290-page review and economic evaluation had been put together to assess the effectiveness of four new treatments \u2014 bevacizumab, sorafenib, sunitinib and temsirolimus \u2014 for renal-cell carcinoma. The research team concluded that there was some evidence that patients could benefit from these treatments, in some cases even doubling the time that they lived without their disease worsening from five to ten months. However, \"the probability that any of these interventions would be considered cost effective\" in Britain's nationalized health system, it said, \"is zero\" 1 . A few months later, an expert panel from Britain's National Institute for Health and Clinical Excellence (NICE) confirmed that prediction, stating in an August decision based heavily on the report that the four drugs \"are not recommended as treatment options for advanced and/or metastatic renal cell carcinoma\". The critics erupted. The drug manufacturers and patients were furious. \"They are saying to me, as a cancer patient, that I don't deserve the right to live, whether it's for one month, two months, six months or even years,\" a person with kidney cancer told the BBC. Britain's leading cancer charity Cancer Research UK said that it was \"very disappointed\" with the decision, and that \"NICE needs to consider how it can reconcile making recommendations so clearly at odds with current clinical opinion\". \"We were not aware at the start that this was going to be controversial,\" says Jo Thompson Coon, a researcher at the Peninsula Medical School in Exeter, UK, and the corresponding author of the research report. This is the brutal reality of NICE, the body that decides which medical treatments the nation can afford to buy. To some, NICE is a world-leading body \u2014 one at the forefront of 'comparative effectiveness' research, which compares one treatment with another, and exemplary for the tough decisions it makes on cost effectiveness. To others it is a loathed body ready to deny people life-giving treatment with little justification. Which of these portraits is more accurate has implications for people far beyond the United Kingdom. NICE's decisions and its decision-making methods have already been adopted by several other countries (see ' NICE abroad '). Some health economists say that the United States \u2014 where the debate over health-care reform has reached fever pitch \u2014 will have to incorporate techniques similar to those that NICE uses if the country is serious about reining in its more than $2-trillion health-care bill. But to many others in the United States and elsewhere the idea of a NICE-like body is unfathomable. Former Republican vice-presidential candidate Sarah Palin summed up the feelings of many when she warned in a statement in August this year that health-care rationing would lead to a \"death panel\" for patients in which \"bureaucrats can decide \u2026 whether they are worthy of health care\". In the United Kingdom, bureaucrats and health-care rationing are a way of life. What is sometimes lost in the midst of the debate is the research itself, which in the case of NICE involves synthesizing evidence and building models that attempt to assign hard values to the quality and cost of each additional month by which a treatment extends life. The researchers who do this work, however, are well aware that their results can have profound effects for the population. \"You are obviously always very aware that this is a very important piece of work,\" says Thompson Coon. \"It is not going to sit on a dusty shelf for no one to read. On the other hand, you have to keep it evidence-based.\"  \n                Health arbiter \n              NICE was born in 1999 amid concerns that patients in Britain's National Health Service (NHS), which provides the vast majority of the nation's health care, were not being given the most effective treatments. Established by the Labour government, its purpose is to make sure that the NHS spends its budget \u2014 raised by taxation \u2014 wisely, using a transparent decision-making process that is based on the best evidence available. For many conditions, there is scant evidence to show doctors and patients which of two or more medical alternatives \u2014 be it treatments, diagnostic techniques or prevention methods \u2014 is likely to be more successful. Comparative-effectiveness research aims to find out which one is best, either by pulling together existing research or by commissioning new studies. On its own the research is relatively uncontroversial. Where NICE gets into hot water is when money is added to the mix. Even if a treatment is more effective clinically than a rival, NICE committee members can decide that the gain in health that it bestows is not worth the additional cost. NICE issues guidance on issues ranging from medical procedures to public health, and the ones that tend to be most controversial are its 'technology appraisals', which tell the NHS under what conditions treatments \u2014 particularly new ones \u2014 must be offered. The decision over bevacizumab, sorafenib, sunitinib and temsirolimus was one of the most divisive so far. In 2007, the Department of Health asked NICE to assess the drugs, made by Roche, Bayer, Pfizer and Wyeth, respectively. NICE turned, as it routinely does, to an external assessment group, in this case the group of seven health economists led by Thompson Coon. The researchers considered the clinical effectiveness question first, namely, what is the effect of the four drugs compared to current standard treatment for renal-cell carcinoma, a particularly nasty type of kidney cancer that is diagnosed in around 6,000 people a year in Britain. One existing treatment option was to remove the tumour entirely by taking out the kidney. If the cancer had spread, or metastasized, then immunotherapy with interferon-\u03b1 was commonly used. But even with treatment, only 10% of people whose cancer has metastasized survive for at least five years. The team initially identified 888 potentially relevant studies through electronic searches of various databases. Many of these were excluded for being reviews, reanalyses of the same data or studies that did not meet the researchers' experimental standards, such as being placebo controlled. This left them with 13 relevant papers from 8 clinical studies. For this study \"there was actually quite a reasonable amount of evidence\", says Thompson Coon. \"Because they're new drugs there isn't always that much data available. If you were doing a systematic review of a more established technology you'd probably expect to find more.\" In a number of patient groups and disease scenarios, the research team found that the new drugs were better than existing treatments. For example, one of the studies 2  they included showed that people with untreated metastatic renal-cell carcinoma who were given bevacizumab and interferon-\u03b1 lived without their cancer progressing for just 10.2 months, compared with 5.4 months for those given a placebo and interferon-\u03b1. But a drug that extends life is not much good if the individuals who take it spend that time in crippling pain. So the trick for the assessment team is to assign a somewhat abstract quality of life value \u2014 called a 'utility value' \u2014 to these additional months or years. Typically, perfect health scores a one. Dead is zero. Putting values in between is the complicated bit. One way to measure quality of life is using a 'time trade-off' method. Individuals are asked: if you were going to be in a particular health state for ten years, how many years of life would you be prepared to forfeit to be in perfect health? To avoid constant pain, for example, someone might be prepared to give up eight years of life for two in perfect health: this state would be rated 0.2. To avoid a more minor condition a person might only be prepared to forfeit six months, rating 0.95 on the scale. \"What's very important is the measure of quality must be based on actual choices that people are willing to make,\" says Karl Claxton, a health economist at the University of York, UK, who has served on various NICE appraisal committees. Ideally, the quality of life of a person taking, say, cancer drugs, is measured from patients in a clinical trial. These can be translated into standard utility values by, for example, attaching to them 'reference' measures of time trade-off from the general UK population. For NICE assessments, research teams then combine the quantity of life that a treatment buys, with the quality of that time into the Quality Adjusted Life Year (QALY), a measure widely used by health economists. This is calculated by multiplying the utility value of a health state by the length of time spent in that state. One year spent in perfect health, for example, gives a QALY of one. Three years spent in a health state with a utility value of 0.5 equals 1.5 QALYs, equivalent to 1.5 years of perfect health.  \n                Complex calculations \n              For most conditions, working out the QALY is more than a back-of-the-envelope calculation. Clinical trial results cover only the relatively short span of the trial, but researchers are trying to extrapolate from this a QALY for the rest of a patient's life, during which their health may change. For this they turn to mathematical models into which they put all the information on the effectiveness of the drugs from the literature search, the changes the treatments are likely to make to patients' quality of life, how the disease progresses and the timescales involved. And the answer? The model developed by Thomson Coon's assessment team showed that treating renal-cell carcinoma with interferon-\u03b1 gives 1.19 QALYs to the average patient. Adding one of the new drugs, bevacizumab, to interferon-\u03b1 increased this to 1.45 QALYs. Sunitinib was even better, producing 1.62 QALYs. If money were no object, then sunitinib would be the obvious choice. But for the NHS, money is crucial \u2014 and when cost was added the outcome was much less clear-cut. A course of treatment with interferon-\u03b1 alone cost \u00a38,438 (US$13,786) at the time of the assessment, whereas one with sunitinib was \u00a339,623. This means the additional 0.44 QALYs \u2014 effectively just over five extra months of healthy life \u2014 costs an extra \u00a331,185. NICE typically calculates the difference as the cost per QALY, which was \u00a371,462 for sunitinib and \u00a3171,301 for interferon with bevacizumab. How well the QALY system actually reflects patient preferences is still debated. Stephen Birch, a health economist at McMaster University in Ontario, Canada, says that it fails at this most fundamental of levels. A key problem, as Birch sees it, is the assumption built into calculation of QALYs that it is possible to separate time and health. \"These two things are not separable,\" he says. A person who has been sick for a long time might rate their quality of life differently to someone who has only just become sick. \"The QALY doesn't have that,\" Birch says. Despite these criticisms and others, many health economists consider the QALY to be the best option available in comparative-effectiveness research. \"The argument at a theoretical level is whether the QALY is a good measure of utility or whether it's a crude measure of length and quality of life that doesn't bear close relationships to people's satisfaction,\" says Alan Maynard, a health economist at the University of York. \"It's not strongly theoretically based. It's pragmatic best practice. That's where we're at.\" Once calculated, the QALYs and their costs are returned by the assessment team to NICE, where a separate appraisal committee composed of researchers, medical practitioners and laypeople decide what to do with it. The committee produces a draft decision \u2014 called an appraisal consultation document \u2014 before accepting comments from interested parties and then releasing a final decision. Generally anything coming in with a cost per QALY gained of under \u00a330,000 is approved for use \u2014 and none of the four renal cancer drugs came even close. More than 80% of drugs are approved for use in some form, however (see  table 1 ). The \u00a330,000 threshold has been criticized for being somewhat arbitrary, but health economists say that it is broadly in line with other spending decisions taken in the NHS, such as those made by health authorities in the absence of NICE guidance. \"You can argue that NICE is too generous,\" says Maynard. \"It probably puts too many things on to the approved list for the NHS, but it's a system that is explicit.\" An immediate solution for the cancer drugs would be to bring down the cost of the drug. But NICE is not allowed to engage in direct debates with drug companies about the cost of their products, although there are various schemes that allow manufacturers \u2014 who are keen to have their drug approved by NICE \u2014 to offer 'discounts' to the NHS. Pfizer, for example, offered a deal in which the first cycle of sunitinib would be free to the NHS. NICE's chairman, Michael Rawlins, says it \"gets up my nose a bit\" when the agency is criticized for rationing when pharmaceutical companies escape criticism for their pricing. In October, after two of the cancer-drug manufacturers submitted new clinical data or comments, NICE commissioned another review of the evidence, eventually concluding in its final appraisal that the cost of sunitinib \"could be less than \u00a350,000 per QALY gained\" 3 .  \n                Citizens advice \n              But in the end, redoing the calculations was not what made the difference. The outcry over NICE's decision was such that at the start of January, NICE made a very significant move, effectively rewriting its procedures for drugs such as those for renal cancer. Based in part on the recommendations of its Citizens Council, NICE issued additional guidance for appraising treatments that \"may be life-extending for patients with short life expectancy, and which are licensed for indications affecting small numbers of patients with incurable illnesses\". For these drugs, says NICE, appraisal committees can consider \"giving greater weight to QALYs achieved in the later stages of terminal diseases\". It was effectively telling its committees to be lenient about approving certain treatments that fall above its \u00a330,000 threshold. In its final appraisal a month later, the committee decided that sunitinib should be used in some circumstances despite its cost. The committee concluded that \"in this case there was a significant step-change in treating a disease for which there is currently so little to offer patients\" 3 . This August, it confirmed it was rejecting use of the other three drugs. \"The difficulty with these drugs was they were in a disease area where there wasn't much else,\" says Peter Littlejohns, NICE's clinical and public-health director. \"Probably that was an issue that we weren't sensitive enough to at that time. That was why there was a huge backlash.\" Littlejohns says that the additional end-of-life guidance and the approval of sunitinib \"represented NICE responding to scientific and public concern\". Rawlins agrees, saying that the guidance change reflects the beliefs and desires of the British public. The new procedure does not sit well with everyone. \"I'm very worried about the notion of weighting different people's QALYs differently,\" says Claxton. He worries that health gained by those on sunitinib will be health lost by others in the NHS when treatments they will need in future can no longer be afforded. \"Of course the people who stand to benefit from a technology might have characteristics that we believe ought to give them a particular social weight, but what about the people whose health care gets displaced because of the cost?\" he asks. As the renal-cancer story illustrates, whatever hard numbers the models spit out, the decisions about what to do with these figures \u2014 and hence the outcome of the appraisal process \u2014 are influenced by many other social, economic and political factors. Claxton is not the only one who thinks that two groups could well come up with different \u2014 and wholly legitimate \u2014 answers to the same cost-effectiveness question. Rawlins likens each decision made by an appraisal committee to being tried in court by a jury. \"You might get off with one and not with the other,\" he says. \"We try to make sure they're as consistent as possible but at the end of the day there are [scientific] judgements and then of course on top of that these social judgements too.\" \"Ideologically this isn't very pleasant,\" sums up Maynard, \"but as I tell my medical students, there are two certainties in life, one is death and the other one is scarcity of resources.\"   See Editorial, see  \n                     page 315 \n                   . \n                     NICE \n                   \n                     PenTAG \n                   \n                     Centre for Health Economics at York \n                   Reprints and Permissions"},
{"file_id": "461160a", "url": "https://www.nature.com/articles/461160a", "year": 2009, "authors": [{"name": "Bryn Nelson"}], "parsed_as_year": "2006_or_before", "body": "Most researchers agree that open access to data is the scientific ideal, so what is stopping it happening? Bryn Nelson investigates why many researchers choose not to share. In 2003, the University of Rochester in New York launched a digital archive designed to preserve and share dissertations, preprints, working papers, photographs, music scores \u2014 just about any kind of digital data the university's investigators could produce. Six months of research and marketing had convinced the university that a publicly accessible online archive would be well received. At the time of the launch, the university librarians were worried that a flood of uploaded data might swamp the available storage space. Six years later, the US$200,000 repository lies mostly empty. Researchers had been very supportive of the archive idea, recalls Susan Gibbons, vice-provost and dean of the university's River Campus Libraries \u2014 especially as the alternative was to keep on scattering their data and dissertations across an ever-proliferating array of unintegrated computers and websites. \"So we spent all this money, we spent all this time, we got the software up and running, and then we said, 'OK, here it is. We're ready. Give us your stuff',\" she says. \"And that's where we hit the wall.\" When the time came, scientists couldn't find their data, or didn't understand how to use the archive, or lamented that they just didn't have any more hours left in the day to spend on this business. As Gibbons and anthropologist Nancy Fried Foster observed in their 2005 postmortem 1 , \"The phrase 'if you build it, they will come' does not yet apply to IRs [institutional repositories].\" A similar reality check has greeted other data-sharing efforts. Most researchers happily embrace the idea of sharing. It opens up observations to independent scrutiny, fosters new collaborations and encourages further discoveries in old data sets (see  pages 168  and  171 ). But in practice those advantages often fail to outweigh researchers' concerns. What will keep work from being scooped, poached or misused? What rights will the scientists have to relinquish? Where will they get the hours and money to find and format everything? Some communities have been quite open to sharing, and their repositories are bulging with data. Physicists, mathematicians and computer scientists use  http://arXiv.org , operated by Cornell University in Ithaca, New York; the International Council for Science's World Data System holds data for fields such as geophysics and biodiversity; and molecular biologists use the Protein Data Bank, GenBank and dozens of other sites. The astronomy community has the International Virtual Observatory Alliance, geoscientists and environmental researchers have Germany's Publishing Network for Geoscientific & Environmental Data (PANGAEA), and the Dryad repository recently launched in North Carolina for ecology and evolution research. But those discipline-specific successes are the exception rather than the rule in science. All too many observations lie isolated and forgotten on personal hard drives and CDs, trapped by technical, legal and cultural barriers \u2014 a problem that open-data advocates are only just beginning to solve. One of those advocates is Mark Parsons at the National Snow and Ice Data Center at the University of Colorado in Boulder. Parsons manages a global programme to preserve and organize the data produced by the International Polar Year (IPY) that ran from March 2007 to March 2009 and included an estimated 50,000 collaborators from more than 60 countries. The IPY policy calls for data to be made available fully, freely, openly and on the shortest feasible timescale. \"Part of what is driving that is the rapidness of change in the poles,\" says Parsons. \"If we're going to wait five years for data to be released, the Arctic is going to be a completely different place.\"  \n                Reality bites \n              But reality is forcing a longer timescale. As soon as they began implementing the data policy, Parsons and his team encountered a staggering diversity of incoming information, as well as wide variations in the culture of data sharing. Fields such as atmospheric science and oceanography, Parsons says, have well-developed traditions of free and open access, and robust databases. But fields such as wildlife ecology and many of the social sciences do not. \"What we discovered was that this infrastructure to share the data doesn't really exist, so we need to start creating that,\" Parsons says. But his programme lacks the resources required to create that infrastructure on a large scale. So the team has resorted to preserving as much data as it can. It has delegated much of that job to national coordinators, or \"data wranglers\", as Parsons calls them, who contact investigators and, \"get the data branded and put in the IPY corral\". One of the most successful data-wrangling countries has been Sweden, which formed a subcommittee to correct its early lag in collecting and then received national funding for its own IPY data archive. National coordinator H\u00e5kan Olsson, a specialist in remote sensing at the Swedish University of Agricultural Sciences in Ume\u00e5, says that the country's archive is helping to house data from smaller, independent projects that would never reach large international databanks. Nevertheless, he says, many Swedish researchers still don't archive their data, or don't put data in formats that make them easily searchable and retrievable. He faults the funding agencies too. \"Unlike some other countries,\" he says, \"the research councils in Sweden do not yet have a practice to grant funds with the condition that data from the project is sent to a data centre.\" Even when wranglers can identify the data, it is not always obvious where the data should go. For example, says Parsons, \"you would think that any snow and ice data would go into the National Snow and Ice Data Centre\". But the centre's funding is generally tied to specific data streams, he says, which means it can find itself in the position of accepting glacial data from a programme it has money for, while being forced to turn away similar glacial data from programmes where it does not. Despite the launch earlier this year of the Paris-based Polar Information Commons to make polar data more accessible, Parsons says, that with all the \"naive assumptions\", the lack of planning and other unanticipated obstacles, properly managing the IPY data will require another decade of work. In other fields, however, the main barriers to data sharing are concerns about quantity and quality. The US National Science Foundation's (NSF's) Laser Interferometer Gravitational-Wave Observatory (LIGO), for example, uses giant detectors in Louisiana and Washington to search for gravitational waves that might indicate the presence of rare phenomena such as colliding black holes or merging stars. LIGO is also working with the Virgo consortium, which operates a similar detector near Pisa, Italy. Neither team has detected the signal they are looking for yet \u2014 but that's not surprising: gravitational waves are expected to be extraordinarily faint. The key to detecting them is to eliminate every possible source of spurious vibration in the detectors, whether from seismic events, electrical storms, road traffic or even from the surf on distant beaches. It requires what Szabolcs M\u00e1rka, a physicist at Columbia University in New York and the university's lead scientist for LIGO, calls \"a really paranoid monitoring of the environment\". The question of what data should be shared has provoked strong debate within the LIGO and Virgo teams. Should they open up all their terabytes of data to outside scientists, including the torrents of environmental data? Or should they release just the cleaned-up data stream most likely to reveal a gravity wave? Would naive outsiders fail to process the raw data adequately, leading to premature announcement of gravitational wave 'discoveries' that would hurt everyone's credibility? Or would the extra eyes bring fresh perspective to the search? \"I'm torn,\" says M\u00e1rka, who says that the precise terms of data sharing are being negotiated with the project's funders. \"We don't just have to analyse the data, we need to make sure the data are right.\" How data should be shared is also a substantial problem. A prime example is the issue of data standards: the conventions that spell out exactly how the digital information is formatted, and exactly how the contextual information (metadata) is listed. In some disciplines it is comparatively easy to agree on standards, says Clifford Lynch, executive director of the Coalition for Networked Information based in Washington DC, which represents academia on data and networking issues. \"If you look at something like the sequencing of a genome, there's a whole lot of tacit stuff that's already settled,\" he says. \"Sequencing one genome is very similar to sequencing another.\" But for other groups \u2014 say, environmental scientists trying to understand the spread of a pollutant \u2014 the choice of common standards is far less obvious. The all-too-frequent result is fragmented and often mutually incomprehensible scientific information. And that, in turn, stifles innovation, says James Boyle, a law professor at Duke University in Durham, North Carolina, and a founding board member of Creative Commons, a non-profit organization that supports creative content sharing.  \n                Always somebody smarter \n              \"Researchers generally create their own formats because they believe that they know how their users want to use the data,\" says Boyle. But there are roughly a billion people with Internet access, he says \"and at least one of them has a smarter idea about what to do with your content than you do\". For example, web users are using applications such as Google Earth to plot the spread of pandemics 2  or to collect information on the effects of climate change. All that is needed, says Boyle, are common languages and formats for data. Perhaps not surprisingly, data-sharing advocates say, the power to prod researchers towards openness and consistency rests largely with those who have always had the most clout in science: the funding agencies, which can demand data sharing in return for support; the scientific societies, which can establish it as a precedent; and the journals, which can make sharing a condition of publication. The trick is to wield that power effectively. The NSF, for example, has funded ground-breaking research into digital archiving, search and networking technologies. But its data-sharing policies for standard research grants, for example, have come under fire for being scattered and ad hoc; they are often stipulated on a per-project basis. Gibbons says she is especially disappointed with a 2003 mandate by the US National Institutes of Health (NIH), which could have dramatically changed the culture of data sharing. The mandate does require a data-sharing plan for any grant worth $500,000 or more in direct annual costs or an explanation of why sharing isn't possible. But details about how to make the data available were so vague, says Gibbons, that researchers soon stopped paying attention, content to sit back until someone got in trouble for not playing by the rules. Officials at the NIH Office of Extramural Research reply that the data-sharing policy's 'vagueness' is, in fact, flexibility, an attempt to avoid forcing every research programme into a one-size-fits-all straightjacket. They note that the policy also recognizes that there may be valid reasons for not sharing, including concerns about patient privacy and informed consent.  \n                The chicken or the egg? \n              Nonetheless, until data sharing becomes a requirement for every grant, says Daniel Gardner, a physiologist and biophysicist at the Weill Medical College of Cornell University, \"people aren't going to do it in as widespread of a way as we would like\". Right now, he says, \"you can't ask large numbers of people to do it, because it's a lot of work and because in many cases the databases don't exist for it. So there is kind of a chicken and egg problem here.\" One solution would be for agencies to invest in the infrastructure necessary to meet their archiving requirements. That can be difficult to arrange, says Boyle. \"Infrastructure is the thing that we always fail to fund because it's kind of everybody's problem, and therefore it's nobody's problem.\" Yet some agencies have been pioneers in this area. One often-cited example is the Wellcome Trust, the largest non-governmental UK funder of biomedical research. Since 1992, its Sanger Institute near Cambridge has been developing and housing some of the world's leading databases in genomics, proteomics and other areas. Another prominent example is the NIH's National Library of Medicine, which in 1988 established the National Center for Biotechnology Information (NCBI) to manage its own collection of molecular biology databases, including the GenBank repository. James Ostell, chief of the NCBI's Information Engineering Branch, likes to show a colour-coded timeline of contributions to GenBank since its founding in 1982 \u2014 a progression that dramatizes the fast-evolving history of genetic sequencing. Ostell points out thick waves of colours flowing from the left side of the chart. Representing traditional sequence divisions such as viruses, rodents, primates, plants and bacteria, they dominated GenBank's contents for years. Other sequences, produced by faster techniques, began to put in appearances in the mid 1990s. Then in late 2001 a sudden surge of green, representing DNA snippets derived from whole-genome shotgun sequencing, quickly took over. By 2006, the green accounted for more than half of the database's contents. Keeping up with ever-shifting technology has created its own set of challenges, says Ostell. \"Nobody has infinite resources. And storing electronic information over time is a dynamic process. If you try to look at a file that you wrote with a word processor 20 years ago, good luck.\" In the same way, if a data set isn't readable by the latest version of a database, it isn't usable. So an archive may well have to choose between tossing old data out, and paying to preserve the out-of-date software required to make sense of them. Even more challenging are the legal minefields surrounding personal data and privacy. The need to protect human subjects has led to starkly different approaches. Some projects openly share data, whereas others require researchers to navigate a labyrinthine approval process before granting access. The NCBI has tried to build such requirements into its newer databases. A case in point is its database of Genotype and Phenotype (dbGaP), which archives and distributes the results of genome-wide association studies, medical DNA sequencing, molecular diagnostic assays and almost anything else that relates people's traits and behaviours to their genetic makeup. The dbGaP allows open access to summaries and other forms of information that have been stripped of personal identifiers. But it grants controlled access to personal health information only after a researcher has been approved by a formal review committee.  \n                Novel meaning \n              Such measures can be cumbersome, says Ostell. Yet the benefits of sharing far outweigh the costs. Some of GenBank's early sequences, for example, included genes from yeast and  Escherichia coli   labelled as DNA repair enzymes. Years later, researchers studying human colon cancer made a link between mutations in patients and those same enzymes 3 . \"If you just did a literature search, you would never make that connection,\" Ostell says. \"But when you search on the basis of their genes, suddenly you connect meaning in a way that's novel, which is the basis of discovery.\" Sharing is obviously easier when the expectations are clear, and many scientists point to a 1996 meeting in Bermuda as a defining moment for genomics. At the meeting, leaders working on the Human Genome Project hammered out a set of agreements known as the Bermuda principles. Chief among them was the stipulation that sequences longer than 1,000 base pairs be made publicly available, preferably within 24 hours. The Bermuda principles, in turn, built on the foundations laid a decade earlier by the editors of journals such as  Nucleic Acids Research , who spurred the early development of GenBank and other genomic repositories by requiring researchers to deposit their data there as a precondition for publishing. Newer journals, such as the open-access Public Library of Science journals, have made publication contingent on making the data \"freely available without restriction, provided that appropriate attribution is given and that suitable mechanisms exist for sharing the data used in a manuscript\". The journal  Neuroinformatics   devoted its September 2008 issue to data sharing through the NIH Neuroscience Information Framework.  Ecological Archives   publishes appendices, supplements and data \u2014 related to studies appearing in other ecology journals \u2014 which include the metadata needed to interpret them. ( Nature   journals require authors \"to make materials, data and associated protocols promptly available to readers without preconditions\".) Yet the journals' power to compel data sharing and scientific culture change is not absolute. In March 2009, for example, the journal  Epidemiology   felt able to call only for a \"small step\" towards more openness. \"We invite our authors to share their data and computer code when the burden is minimal,\" said an editorial 4  in that issue. \"We believe that data sharing is a matter of time,\" says Miguel Hern\u00e1n, an epidemiologist at Harvard University and a co-author of the editorial. But prematurely forcing a sharing requirement on authors \"would be suicidal\", he warns, especially with unresolved concerns over patient confidentiality. They would simply submit their papers somewhere else. Another issue facing journals and data banks is how to ensure proper citations for data sets. \"The one thing that people clearly care about in the sciences is attribution,\" says Boyle. Without an agreed-on way of assigning credit for original data falling beyond the parameters of a publication, however, it's no wonder that scientists are reluctant to share: their hard work may never be recognized by their employers or by granting agencies. Worse yet, it could be poached or scooped. This is one place that technology might help, says Boyle. He points to a music site associated with Creative Commons known as ccMixter, in which users can upload an a capella chorus, a bass line, a trumpet solo or other musical samples. Users are free to remix the samples into new tracks. But when they do, the program automatically keeps a continuous credit record. So why not implement a similar system that would add a link back to a database every time a researcher repurposed some data? It wouldn't necessarily solve the problem of scooping, Boyle says, \"but it aligns the social incentives with the individual incentives\". It could also provide a feasible way for universities or funding agencies to track the value of a researcher's data.  \n                International agreement \n              Other Creative Commons tools are already making their way into international scientific agreements. In May, for example, Creative Commons' CC0 licence was endorsed by participants at a meeting in Rome on resource and data sharing within the mouse functional genomics community. The licence, which allows its users to \"waive all copyrights and related or neighbouring rights\" and thereby share more of their work, has been translated into dozens of languages. As welcome as such developments are, however, Boyle points out that the creation of the legal and technical infrastructure to accommodate researchers' data-sharing concerns is a huge task, and should not be left solely to non-profit organizations and individual universities. Nor should it be left to the funding agencies' grant-by-grant allocations for data sharing. It will require major government investments, starting with demonstration projects to explore how sharing can best be done. \"What we need is a working example that you can point to,\" he says. If William Michener has his way, a virtual data centre funded by the NSF and hosted by his university will be one of those examples. DataONE (Data Observation Network for Earth) exists only on paper, but a five-year, $20-million grant through the NSF's DataNet programme will help to turn it into an open-access database focusing on biology, ecology and environmental science data. Four other $20-million archives are planned under DataNet's first phase. Michener, director of e-science initiatives for University Libraries at the University of New Mexico, Albuquerque, and a leader of DataONE, says that the archive is designed to accommodate many of the orphan data sets that have yet to find a home, and will target resource-strapped colleges, field stations, and individual or small teams of scientists. In the longer term, the DataONE consortium, which encompasses two dozen partner institutions in the United States, the United Kingdom, South Africa, Australia and Taiwan, will explore business models that could sustain the archive well beyond its initial grant and potential five-year renewal. Among the plans under consideration are a fee-for-service set up, a membership requirement for participating entities and the solicitation of external grants for education and outreach. DataONE's success, however, may depend on overcoming the same ambivalence among researchers that has bedevilled the University of Rochester and other builders of public databases. Although a strategy is still being worked out, Michener envisions a combination of workshops, seminars, websites and other educational tools to help clarify the how and why of sharing. But one archive can only do so much. Larger efforts will be required to tackle what Michener sees as the overriding challenge: \"Changing the culture of science from one where publications were viewed as the primary product of the scientific enterprise to one that also equally values data.\" Without that cultural shift, says Gibbons, many digital archives are likely to remain little more than stacks of empty shelves.   Bryn Nelson is a freelance science and medical writer based in Seattle, Washington.    See Opinion,    pages 168  and  171   and    online special \n                     Nature Neuroscience \n                   \n                     Nature Reviews Genetics \n                   \n                     Nature Reviews Drug Discovery \n                   \n                     Molecular Systems Biology \n                   \n                     Science Commons \n                   \n                     Joint Information Systems Committee \n                   \n                     NIH Data Sharing Policy \n                   \n                     Wellcome Trust Data Sharing Information \n                   \n                     DSpace Open-Source Digital Repository \n                   Reprints and Permissions"},
{"file_id": "461466a", "url": "https://www.nature.com/articles/461466a", "year": 2009, "authors": [{"name": "Jane Qiu"}], "parsed_as_year": "2006_or_before", "body": "When American and Chinese scientists agreed to measure pollution and dust over China, nobody foresaw how difficult it would be. Jane Qiu reports. The meteorological bureau in the sleepy town of Shouxian in eastern China was buzzing with excitement. It was May 2008, and the spacious courtyard was littered with sophisticated remote-sensing instruments that had just arrived on loan from the United States Department of Energy (DoE). The bureau had been expecting the equipment earlier, but it had been held up by Chinese customs officials for more than two months. A group of climate researchers and government officials from China and the United States eagerly inspected the new arrivals, which included a cloud radar, a tailor-made lidar (a radar-like instrument that sends out laser beams rather than microwaves) and sensors for studying various features in the atmosphere and the radiation from the Sun. \"We can do great things with these here,\" said Zhanqing Li, an atmospheric scientist at the University of Maryland at College Park, who was leading the Sino-American collaboration. Over the next few months, these instruments would be pointed up into the Chinese sky to monitor and study aerosols \u2014 tiny airborne particles such as dust and soot. The researchers were particularly interested in tracing how aerosols alter the personality of clouds by influencing whether clouds produce rain, how high they extend, how much sunlight they reflect and how long they persist. At present, atmospheric researchers have only a rudimentary understanding of how aerosols affect clouds and that ignorance is one of the major sources of uncertainty in forecasts of future climate. For aerosol experts, China's sky is close to heaven. The country has high concentrations of particles arising from pollution as well as natural dust blowing in from surrounding deserts. Researchers expected that data from such a particle-rich atmosphere would help to resolve major questions about aerosols and climate. At the same time, it was hoped that the project, staged at four sites across China (see map) would reap political rewards. The joint collaboration, conducted under the umbrella of the DoE's Atmospheric Radiation Measurements (ARM) programme, was viewed as a sign of China's movement towards openness. \"This kind of collaboration would have been inconceivable ten years ago,\" says Thomas Ackerman, an atmospheric scientist at the University of Washington in Seattle. The political winds did not, however, always blow favourably. With much frustration, the DoE had to alter its usual mode of operation and settle for lower-quality data and a smaller range of measurements than expected. \"Between the heights of hope and the depths of despair, it was the most up-and-down deployment we have ever had,\" says Warren Wiscombe, ARM's chief scientist and an atmospheric scientist at NASA's Goddard Space Flight Center in Greenbelt, Maryland. The stakes are high because the data collected by the ARM programme will be used to improve the way that climate models simulate clouds and aerosols. When the programme was created in 1989, it collected measurements only at fixed sites within the United States. \"But it soon dawned on us that we need as much information as possible from diverse climate systems around the world to build up a complete picture of global climate change,\" says Wiscombe. This resulted in a mobile facility, built in 2004, that contains most of the remote-sensing instruments present at the fixed sites. Each year, it visits a different region around the world, where it is run by an on-site technician. A team of scientists in the United States monitors each instrument remotely; they review the data for quality and then upload them in real time into the ARM data archive for use by the worldwide scientific community. In 2007, Li and Chen Hongbin, deputy director of the Beijing-based Institute of Atmospheric Physics of the Chinese Academy of Sciences, won the bid to bring the mobile facility to China the following year. The project came under the umbrella of a climate-science agreement signed in 1987 between the DoE and China's science ministry. Under that compact, the two countries are committed to sharing data and collaborating on joint field campaigns, climate modelling and strategies for adapting to climate change.  \n                Cloud puzzle \n              One aim of deploying the ARM mobile facility in China was to investigate an observation that had puzzled atmospheric scientists for some time. Microwave-sensing instruments on the joint US\u2013Japanese Tropical Rainfall Measuring Mission satellite detect large amounts of liquid water in clouds over the coastal region of eastern China, yet the satellite's radar shows that there is very little rainfall. \"The two satellite instruments disagree with one another, which is very unusual,\" says Chris Kummerow, an atmospheric scientist at Colorado State University in Fort Collins, who discovered the discrepancy with his colleagues ( W. Berg  et al. J. Appl. Meteorol. Climatol.    45,   434\u2013454; 2006 ). Some researchers suspect that the high level of aerosols in the Chinese atmosphere might be the culprit. The standard thinking about aerosols is that the particles often suppress rainfall by providing a nucleation site on which water can condense; they increase the number of cloud droplets and reduce their average size, thus making it harder for small droplets to grow big enough to drop out of the cloud as rain. And China's air is chock-full of aerosols, at concentrations three times the global average. The density of aerosols, particularly in the populous eastern part of the country, has been rising rapidly. In eastern central China, Li and his colleagues have found that the number of clean air days \u2014 defined as having at least 75% of the maximum visibility \u2014 has declined continuously over the past three decades, from 26% in 1976 to 14% in 2007. The full effect of all those aerosols is hard to discern. Many other variables, such as the types of aerosol, the nature of the land surface and atmospheric circulation patterns, could influence how aerosols alter clouds. \"We need a large data set from as many climate regions as possible,\" says Mark Miller, an atmospheric scientist at Rutgers University in New Brunswick, New Jersey. \"The ARM deployment in China is an important part of that effort.\" The plan was to take the monitoring facility to four locations in China with different climates and aerosol types. The two eastern sites were rural Shouxian, 500 kilometres northwest of Shanghai, and Taihu (Lake Tai), in the industrial heartland of the Yangtze River delta region, about 100 kilometres west of Shanghai. These two locations have a similar climate but very different atmospheric aerosols: those over Taihu come mainly from industrial pollution, whereas those in Shouxian come from windblown soil and smoke from burning crop residues. Taihu is also the site of the largest aerosol paradox: nowhere else is there a greater discrepancy between the amount of liquid water in clouds and the amount of rainfall. \"The Taihu site is ideally situated for studying the effects of aerosols on precipitation,\" says Li. Another set of instruments was deployed at Zhangye in northwestern China and then the equipment was moved to Xianghe near Beijing. These two sites are much drier and less cloudy than the southeastern locations, allowing researchers to compare aerosol data taken under different climate and environmental conditions. At Zhangye, which is downwind of the Taklimakan and Gobi deserts, the researchers set out to measure dust aerosols and their effect on the amount of solar radiation that reaches the land surface. At the end of the dust-storm season in July 2008, the instruments were moved to Xianghe to study how government actions to reduce pollution during the Beijing Olympics affected solar radiation reaching the surface. \"You rarely have the opportunity to do something like that in a major metropolis,\" says Miller.  \n                Stops and starts \n              Despite the high hopes, however, the work in China was an uphill struggle at each step, says operational manager Kim Nitschke. \"Originally, our expectations were very high as we thought it would be an exciting time to get into China during the Olympics,\" says Wiscombe. Then, the spate of problems crushed their hopes. \"At some points we thought we wouldn't be able to get anything out of it,\" he says. The difficulties began right at the start, when the instruments became stuck in Chinese customs because of tightened regulations due to the Olympics. \"We were not aware of some of the changes in customs regulations and did not have all the paperwork necessary for getting the instruments through,\" says Chen. On top of that, some of the sophisticated, upward-pointing instruments, such as the cloud radar, raised much suspicion among customs officials and this helped hold up their release. The delay severely curtailed the field campaign, especially at Zhangye where many of the spring dust storms were missed. When the instruments were finally up and running in China, the support team in the United States found that the Chinese government would not allow them to access the instruments through the Internet. They could not check the quality of the measurements or fix problems remotely, nor could they upload data to the public database. The on-site technician was able to solve most of the technical issues that emerged, but some subtle problems remained for much of the deployment in China. To the DoE's further dismay, officials from the Chinese Meteorological Administration (CMA) shut down photometers at the Shouxian and Xianghe sites that were measuring particles of black carbon, a major component of soot. Interestingly, the CMA did not interfere with measurements at other observing sites, possibly because these were being jointly operated with Chinese universities. At one point in July last year, the DoE threatened to terminate the ARM project in China, but it decided to stay on, in consideration of the effort already invested. Eventually, after lengthy negotiations, the American researchers were granted a two-week window in October 2008, during which they could connect with the instruments remotely from the United States for about four hours each day. Only then were they able to fix the broken cloud radar, enabling it to collect data during the last two months of the project. Political problems caused headaches up to the final days of the year-long stay in China. \"Even at the end of the deployment, we were not sure whether we would be able to get anything out of it,\" says Nitschke. \"I had all the data with me on a portable hard drive, but wasn't sure whether I would manage to get them out of China or if the Chinese government would let us make them publicly available.\" The CMA did give the green light, and the original data from Shouxian and Zhangye are now available from ARM's archive. Many researchers are philosophical about the difficulties encountered by ARM's China deployment. \"China is going through a transitional phase,\" says Daniel Rosenfeld, an atmospheric scientist at the Hebrew University in Jerusalem, Israel. Although the country is still not as open as people would like it to be, the collaboration clearly signals a move away from the old ways, he says. \"The process is not complete yet, but I hope the issue of openness will be a thing of the past soon.\" Wei-Chyung Wang, a climate scientist at the State University of New York at Albany, and the United States' chief scientist managing the climate-science agreement, says that the project is \"one of the most successful collaborations under the agreement\". He adds that \"this unique experience has really opened up the dialogue and will stimulate more interest in similar collaborations\".  \n                Data feast \n              Participants in the project say they are happy with what they were able to collect. \"We now have cloud data from China nobody has ever had before,\" says Miller. Since the data collected at Shouxian and Zhangye were made publicly available in March and April 2009, respectively, researchers have been busy analysing them. Connor Flynn, an atmospheric scientist at the DoE's Pacific Northwest National Laboratory in Richland, Washington, is excited by the data. \"Some of the lidar images are just tragically breathtaking,\" he says of the pollution measurements. He has plotted the lidar data to reveal the concentration and composition of aerosols at various altitudes. \"You can see atmospheric layers swirling together, ice crystals falling from a high cloud and their properties changing as they go through other layers,\" says Flynn. The data also show air masses with distinct aerosol compositions coming together from dust, urban, industrial and agricultural regions and mixing at different altitudes. \"You don't always have those diverse sources of aerosols at high concentrations in other parts of the world,\" he says. The information will be enormously valuable for climate modellers trying to simulate those processes, Flynn adds. Using data from radiometers, lidar, cloud radar and weather balloon, Li and his colleagues have made some headway in understanding the water\u2013rainfall paradox of the clouds over eastern China. At a meeting jointly convened in Beijing last month by the DoE and China's science ministry, Li showed that the effect of aerosols on rainfall depends on the amount of liquid water in the clouds. When clouds are relatively dry, adding aerosols can suppress precipitation. In wetter clouds, however, aerosols make rain more likely. This is consistent with the observation that the number of days of light rainfall has decreased by 23% in the past 50 years in eastern China; cloud modelling studies show that this can be explained by the increased aerosol concentrations in the region ( Y. Qian  et al. J. Geophys. Res.    114,   D00K02; 2009 ). What's more, data collected at Shouxian and Taihu show that aerosols apparently affect the thickness of clouds and the altitudes at which they form. \"If this proves to be the case, the implications of aerosols for climate change will be tremendous,\" he says. But the Chinese data have not solved all the cloud conundrums. Using a computer simulation, Kummerow and his colleagues have pinpointed \"a very complicated pattern\" in how aerosols affect clouds. \"They seem to increase precipitation in some places and decrease rainfall in others,\" he says. For example, the cloud radar and radiometer data from Shouxian indicate that, on days with comparable aerosol amounts and cloud liquid water, sometimes it rained and sometimes it didn't. These preliminary passes through the ARM data collected in China show how much remains to be learned there. And the scientific results are not the only dividends the project has yielded. To Chinese researchers such as Huang Jianping, an atmospheric scientist at Lanzhou University in Gansu province, the ARM collaboration helped strengthen China's capability to run long-term field campaigns. Since 2005, Huang has been building up an observational site 40 kilometres east of Zhangye, focusing on climate research of semi-arid regions. \"The ARM deployment in Zhangye has allowed us to work alongside the best people in the field,\" says Huang. And that experience has helped spur Chinese scientists to set their sights far higher. Jane Qiu writes for  Nature   from Beijing. \n                     Nature Reports Climate Change \n                   \n                     The Atmospheric Radiation Measurement (ARM) Program \n                   \n                     ARM mobile facility deployment in China \n                   \n                     Zhanqing Li \n                   Reprints and Permissions"},
{"file_id": "461586a", "url": "https://www.nature.com/articles/461586a", "year": 2009, "authors": [{"name": "Quirin Schiermeier"}], "parsed_as_year": "2006_or_before", "body": "The collapse of communism opened up the world to scientists from eastern Europe. Quirin Schiermeier talks to researchers about what changed. On the 9 November 1989, the lights were out early for Pavel Jungwirth. Jungwirth had recently graduated from Charles University in Prague, Czechoslovakia, and he was hoping eventually to pursue a PhD in physics. But his plans had been interrupted when he was drafted into the army, and for now he was quartered in barracks outside the city. With a 10 p.m. curfew, Jungwirth had little opportunity to watch the historic spectacle that was captivating audiences around the world, as an ecstatic crowd breached the Berlin wall. That year, Soviet-controlled communist governments throughout central and eastern Europe had been stumbling in a historic chain reaction. In May, Hungary had begun to dismantle the Iron Curtain. In June, the Polish anti-communist Solidarity party led by shipyard worker Lech Wa\u0142\u0119sa had won by a landslide in free elections. But for many, the most powerful symbol of communism's collapse was on the November day when East Germany's rulers surrendered to weeks of peaceful rebellion by its citizens and announced that people from East Berlin could pass through the formidable barrier that had divided the city since 1961. One of those watching the events on her family television was Alicja J\u00f3zkowicz, a 22-year old then studying zoology at the Jagiellonian University in Krak\u00f3w, Poland. \"It was a very emotional moment,\" she says. Like Jungwirth, J\u00f3zkowicz was set on becoming a scientist. The drama unfolding on the screen made her realize, she says, \"it would change my professional opportunities\". Neither Jungwirth nor J\u00f3zkowicz could anticipate how dramatically their professions would change. The region's political transformation meant that universities and research institutes that had been isolated from international science were now expected to take part in it. Some researchers found this a formidable challenge, grappling for the first time with having to publish their work and compete for funding. But many others, such as Jungwirth, J\u00f3zkowicz and a generation of eastern European students who in 1989 were embarking on a career in science, saw it as an opportunity. These researchers soon became a common sight at labs in western Europe and North America, and many of them have gone on to establish competitive laboratories. \"I consider it a small miracle that within the past 20 years sprouts of excellence have grown in my country,\" says Jungwirth, who now runs a chemistry laboratory in Prague. \"Compared with 1989, our academic community is profoundly different \u2014 inspiring, self-confident and international.\" J\u00f3zkowicz, now a group leader in the Department of Medical Biotechnology at Jagiellonian University, agrees: \"Personally I have a feeling I am a rightful member of an international society of scientists.\"  \n                Lost tradition \n              Yet something could have been lost in the internationalization of eastern Europe, says Wolf Lepenies, a sociologist and former director of the Institute of Advanced Study in Berlin. When he and others used to visit the East, \"institutions were poor, instruments were lacking, but intellectual traditions had prevailed that we had barely taken notice of\", he says. \"Debates were conducted with an earnestness and a responsibility that were largely unfamiliar to us.\" Lepenies says that the 'we will help you' approach that western Europe has often used in its interactions with the East carries a whiff of condescension. \"Eastern Europe is a good place to shake the complacency of the West,\" he says. \"'We need each other!' would have been a more appropriate guideline.\" Jungwirth's family expected him to study science. His father, a physicist himself, went frequently to Novosibirsk, a strong centre of Soviet physics and chemistry, and he would years later become a vice-president of the Academy of Sciences of the Czech Republic in Prague. But in 1970, Jungwirth's father was kicked out of the Communist Party because of his open disagreement with the 1968 Soviet occupation. This stance affected his son's plans. Jungwirth had hoped to study medicine, but according to an unwritten rule, children of politically unreliable citizens should not enrol in subjects through which they might exert an undue influence on people. Medicine was one such discipline. Jungwirth instead opted for physics, which was considered socially neutral. As a student, he followed the 1989 political events with eager anticipation. When writing his diploma thesis he had access to a computer and a printer \u2014 a rarity in a country where the few functioning photocopiers were rigidly controlled for fear of dissident activity. Some like-minded friends asked him to print copies of a letter they had written in support of V\u00e1clav Havel, the playwright and essayist who had become the political figurehead of the anti-communist movement and had been taken into custody. Jungwirth helped them out, but a copy of the letter fell into the wrong hands and he was summoned to the dean. \"I thought 'that's it, I'm out',\" he says. He faced the dean over a massive rosewood desk surrounded by insignia of communism and the portrait of the Czech party leader Gust\u00e1v Hus\u00e1k. Then something unexpected happened. After lecturing him for a minute or two, the dean crumpled up the ill-fated letter and flung it into a corner of his study. Perestroika, Jungwirth thought, had definitely arrived in Prague. By November, huge momentum was building for Havel's 'Velvet Revolution' and a student rebellion had expanded into a nationwide general strike. On 25 November, Jungwirth slipped out of the barracks to join the millions shouting for freedom on the streets of Prague. Hus\u00e1k resigned on 10 December, paving the way for democracy, and on 29 December Havel was installed as the new President of Czechoslovakia. Jungwirth switched from military to civil duty as soon as this option was made possible in 1990 and resumed his studies a year later. He was interested in atmospheric chemistry and, at the J. Heyrovsk\u00fd Institute of Physical Chemistry in Prague, started putting together molecular simulations of hydrophobic molecules such as methane. Soon he was embarking on his long-awaited PhD.  \n                Shades of grey \n              During J\u00f3zkowicz's school years in Krak\u00f3w in the 1980s, the Western world and Western science seemed unimaginably far away. \"I never even dreamed of going there,\" she says. \"I remember my childhood and youth as a time when everything was grey, and each year things just seemed to get worse,\" she says. \"The stores were empty all the time.\" J\u00f3zkowicz was curious about the natural world and she remembers her science teachers \u2014 and textbooks \u2014 as being excellent. \"Much better actually than the science education my son is getting now,\" she says. The high standard of science education was true of many eastern European countries. J\u00f3zkowicz was aware early on of the Solidarity movement that began in the early 1980s and that was to become the nucleus for political changes in Poland. Solidarity, led by Wa\u0142\u0119sa and the first independent trade union in the Soviet-controlled Eastern bloc, quickly turned into an openly anti-communist political and social movement supported by millions of Poles. For J\u00f3zkowicz, the most significant time was when Solidarity was elected and Tadeusz Mazowiecki became prime minister. \"We really felt that a new era was about to begin,\" she says. But when capitalism eventually arrived in Poland it also brought problems, as it did in other countries. The political and intellectual freedom was invigorating but, at least in the early days, disillusionment was never far behind. \"The stores filled up,\" she says. \"But salaries dropped just as quickly.\" Scientists found the transition particularly harsh. Until that time their research money had been guaranteed by the government. With the collapse of communism, many countries were close to bankrupt and science funding dropped abruptly to almost zero. It was impossible in the early years for most scientists to make a living without getting second jobs, and many gave up altogether. Those who managed to hold onto their labs could travel freely to other countries for the first time, meet their counterparts there and begin to integrate with the international scientific community. But this came with tough international competition and new codes of conduct. Conferences were rare, peer review was unknown, and few researchers spoke English. Many developed something of an inferiority complex. The national science academies, which operate hundreds of basic-research institutes throughout central and eastern Europe, today have a disproportionate number of members close to or above the age of retirement who have continued to pursue their research and have domestic influence but have never really entered the international science arena. Jungwirth and other young scientists, though, had little to lose. Pitifully low salaries, poorly equipped labs and a lack of grant money quickly led to thousands of mainly young scientists leaving for the West. Jungwirth was lucky: his PhD supervisor, chemist Rudolf Zahradn\u00edk, had good contacts and used them to send Jungwirth to Switzerland in 1992. He spent a year there working with Thomas Bally at the University of Fribourg, where his 3,000 Swiss-franc (US$2,290) monthly salary made him feel \"rich beyond measure\". (During his PhD Jungwirth also became casual acquaintances with another former PhD student of Zahradn\u00edk's, Angela Merkel). Later, Jungwirth worked as a postdoc for a few months at the Hebrew University of Jerusalem in Israel and at the University of California, Irvine, with the same supervisor, Benny Gerber. He became interested in computational modelling of atmospheric chemistry. \"In Switzerland, and later in the United States, I saw that scientists acquired positions because they knew something, not because they knew somebody,\" says Jungwirth. He was impressed by how his supervisor \"developed new methods in amazing outbursts of creativity. I felt on top of the world.\" J\u00f3zkowicz was also learning about science abroad. Her first international meeting \u2014 the 1991 world immunology congress in Budapest \u2014 was a shock. Not only did she realize that she could hardly understand the English-language talks, she also found that her group's science was hopelessly behind. She had recently started a PhD comparing the amphibian immune system to those of mammalian species in the department of evolutionary immunology at Jagiellonian University. \"Our research was completely outside the mainstream,\" she says. \"Worse, the methods we used were just totally outdated.\" Her lab had no cell sorters and only the most basic molecular-biology apparatus and microscopes. J\u00f3zkowicz realized that she needed to change direction. She started to learn English intensively and decided that as soon as she had completed her PhD she would switch from zoology to medicine. She did this in 1996, moving to the Department of Clinical Biochemistry in the medical school at Jagiellonian University. She studied the control of blood-vessel growth \u2014 angiogenesis \u2014 by the gas nitric oxide. But she was still not happy with the pace at which things were improving. In Poland, as in other countries, grant systems for science were being established and foreign aid and philanthropic aid were providing some relief. In 1992, for example, Hungarian-born billionaire George Soros founded the International Science Foundation, which provided emergency grants to more than 20,000 researchers across eastern Europe. In Poland, grants were provided by the Warsaw-based Ministry of Science and Higher Education and Foundation for Polish Science. But individual grants were still small, modern equipment remained mostly unaffordable and J\u00f3zkowicz felt that Polish science generally lacked international flair and recognition. An opportunity presented itself in 1997 when a poster of hers on  in vitro   gene therapy was shown at a congress in Germany. Diabetes researcher Lawrence Chan of the Baylor College of Medicine in Houston, Texas, noticed it and invited her to join his group for a year. Four weeks later she landed in Houston, leaving behind her husband and six-year-old son. She threw herself into her work. \"I felt painfully lonely,\" she says.  \n                Culture shock \n              Texas was quite an experience for the then 30-year-old, now officially a postdoc (a job term that at the time hadn't been much used in eastern Europe). J\u00f3zkowicz found the style with which Chan ran his 40-strong research team inspiring. \"Each postdoc got a gene, human, mouse or monkey, to work on, and within one week of their arrival everybody was busy doing experiments and producing results,\" she says. J\u00f3zkowicz got mice, and her experiments with them identified a way to perform gene therapy that protected against atherosclerosis for the animal's lifetime. It was to become her most cited paper 1 . \"The year with Chan taught me what real teamwork can achieve \u2014 it was really the first crucial step in my development.\" She later signed up for three years of postdoctoral research at the Medical University of Vienna. Just as J\u00f3zkowicz was leaving home, Jungwirth was returning. Like many young scientists who had taken the opportunity to leave eastern Europe, he decided to go back there (see  'Heading home' ). In 1995 he took up a group-leader position in his native country \u2014 by this time the Czech Republic after its split with Slovakia \u2014 at the Institute of Organic Chemistry and Biochemistry of the Academy of Sciences of the Czech Republic in Prague. With the help of a 200,000-Deutschmark (US$143,000) grant from the German Volkswagen Foundation, Jungwirth was able to quickly establish a reasonably competitive group. He also secured grants from the US National Science Foundation in Arlington, Virginia, and the North Atlantic Treaty Organization (NATO) Science Programme in Brussels. \"Buying serious equipment was still out of the question,\" Jungwirth says. \"But the students were excellent and the computers were just fine.\" As a theorist, computers were the main thing he needed and Jungwirth started to establish a reputation with his molecular models 2 . Over time Jungwirth started to feel that things were getting better. The Czech government invested relatively generously in science, and the Academy of Sciences was less resistant to reform than its counterparts in some other countries. Jungwirth expanded his research interests. By the late 1990s he was exploring atmospheric chemistry, simulating ions such as chloride at aqueous surfaces to try to work out how they could react with ozone and other pollutants 3 . He also ventured into biology, studying how salt ions influence the properties of proteins 4 . \"I had a lot of luck,\" he says. \"The timing was just right. I see many people 5\u201310 years older than me who are now in a much worse situation.\" In 2003, J\u00f3zkowicz returned home too \u2014 to Krak\u00f3w, where her son had just celebrated his twelfth birthday. She had maintained a formal affiliation with Jagiellonian University, and was eager to teach a group of her own the new methods and styles she had learned abroad. But although the world had been changing, Polish academic circles had not. For 18 months all her applications for independent funding came to nothing. The academic establishment was still very hierarchical, with advancement based on favouritism rather than merit. Changing labs was \u2014 and sometimes still is \u2014 considered an act of disloyalty. J\u00f3zkowicz found that the years spent abroad pretty much debarred her from the national funding system. Her break came in 2004 when she won a prestigious Central European Senior Research Fellowship worth some \u00a3350,000 (US$570,000) awarded by the British Wellcome Trust, which is open to researchers in Estonia, Poland, the Czech Republic and Hungary. The money supported projects aimed, among other things, at studying blood vessels in tumours and during cardiovascular disease.  \n                Home for good \n              The grant seemed to make it easier for her to convince Polish grant-givers of the quality of her research and since 2007 she has won funding totalling \u20ac330,000 (US$488,000) from the Polish Science Ministry, the Wellcome Trust and from Adamed, a Warsaw-based pharmaceutical company. Her small group \u2014 now with access to modern fluorescent microscopes and other equipment \u2014 has become the nucleus for Jagiellonian's Department of Medical Biotechnology, created in 2005, where she hopes to receive tenure soon. \"My plans?\" she says. \"Make the most of the new money, produce papers in leading journals \u2014 and live in Krak\u00f3w with my family for the rest of my life.\" Some of the region's catch-up has been aided by the expansion of the European Union (EU), which was joined in May 2004 by ten countries, including Poland and the Czech Republic, followed by Bulgaria and Romania in 2007. J\u00f3zkowicz's department this year received around \u20ac6,000,000 from EU structural funding, which is aimed at generating equal living conditions across the EU and can be used to fund research infrastructure and equipment. The university received an extra 45 million zloty (US$16 million) to establish a new Jagiellonian Center for Experimental Therapeutics, due to open later this year. The European Research Council has also begun to make generous grants available to young investigators across Europe. Both Jungwirth and J\u00f3zkowicz believe that the East\u2013West gap will narrow further as old cultural habits die away and scientific borders dissolve. \"Don't forget we started from empty walls,\" says J\u00f3zkowicz. \"But we now have a young and energetic environment for science, with excellent students who are eager to work. I do believe that at least some labs in Poland and elsewhere will very soon become attractive to foreign scientists.\" Some of that change is palpable in the attitudes of the scientists that Jungwirth now meets. \"Until the turn of the decade or so I could still play the game of coming from a poor country,\" says Jungwirth. \"People had been eager to see us, and keen to help. The feeling of being welcomed with open arms made things a lot easier for us when times were not so good.\" Not so now, he says, as the history starts to fade from memory: scientists who are starting their PhDs today were born after the Berlin Wall fell. \"We cannot say now that we do not publish well,\" says J\u00f3zkowicz, \"or we do not make valuable research because of 'them' or 'circumstances beyond our control'. I remember that, when I was a teenager, the often repeated sentence was 'in the West it would be obvious or easy but here \u2014 forget about it. Never-ever would it be possible in Poland'. And it is possible.\" If any further evidence was needed of what is possible for scientists in the Eastern bloc, it was apparent at a party last year in Prague. Jungwirth was attending the 80th birthday of his former supervisor, Zahradn\u00edk, who served as president of the Czech Academy of Sciences between 1993 and 2001. Late in the evening a black limo with a German licence plate pulled up. Merkel, now the German Chancellor, had spontaneously decided to come down from Berlin to congratulate her aged PhD supervisor. Free from rules of protocol, she chatted the hours away, in fluent Russian, with her science colleagues of old. The word is that it was a good party.   For more on eastern Europe, see  \n                     http://tinyurl.com/eeurope \n                   . \n                     Eastern Europe special \n                   \n                     Institute of Organic Chemistry and Biochemistry \n                   \n                     Jagiellonian University \n                   \n                     Charles University \n                   \n                     Wellcome Trust Central European Senior Research Fellows \n                   Reprints and Permissions"},
{"file_id": "460792a", "url": "https://www.nature.com/articles/460792a", "year": 2009, "authors": [{"name": "Quirin Schiermeier"}], "parsed_as_year": "2006_or_before", "body": "When nations made plans to save the ozone layer, they didn't factor in global warming. Quirin Schiermeier reports on how two environmental problems complicate each other. Later this month, something sinister will start to take shape above Antarctica. As sunlight reappears in the polar skies after the long winter, chlorine and bromine compounds in the stratosphere will begin destroying part of the ozone layer that shelters Earth's surface from harmful ultraviolet radiation. Over the next few months, these pollutants will eliminate enough ozone to create a hole in the protective veil over the Antarctic continent. That same phenomenon has occurred every spring since the late 1970s, although it took several years before scientists recognized and documented the emerging pattern. When, in 1985, a team of researchers from the British Antarctic Survey published a paper in  Nature   describing the ozone hole 1 , the world was put on high alert. At about the same time, it was becoming clear that the ozone shield over much of the planet was vulnerable to pollutants emitted into the atmosphere. That concern propelled countries in 1987 to agree to phase out the production of ozone-destroying compounds \u2014 which stands out even today as the most ambitious action ever taken to tackle a global environmental problem. Thanks to that agreement, the Montreal Protocol, nations have made great strides towards repairing the planet's sunshield. Although the ozone hole that appears over Antarctica each year remains as bad as ever, the amount of chlorine in the atmosphere has started to decline and the rest of the ozone layer has started to show signs of recovery from a less dramatic, but still dangerous, thinning. Amid the good news, however, lurk big questions about how long it will take to fix the sky. A decade ago, researchers projected that the ozone layer would fully recover by 2050, but now there is far more uncertainty in their estimates. One of the complicating factors is that greenhouse gases have altered atmospheric conditions in many ways since the Montreal Protocol was signed, some of which speed up ozone recovery and some of which delay it. Scientists are only now gaining the necessary computing power to run long-term simulations that allow them to test which effects of climate warming might win out. These types of studies suggest that part of the global ozone layer will recover decades earlier than previously thought, whereas the Antarctic ozone hole may linger decades longer than was once hoped.  \n                Complex interplay \n              Just as the climate influences ozone, changes to the ozone layer will, in turn, alter the climate. Ozone loss over Antarctica has already affected the climate there, for example, by helping to warm the Antarctic Peninsula and thereby contributing to the destruction of several ice shelves. And it may have tilted the odds towards more frequent droughts and fires in Australia. \"Stratospheric ozone and surface climate are coupled in many ways,\" says Susan Solomon, an atmospheric chemist at the National Oceanic and Atmospheric Administration in Boulder, Colorado. \"It's a fascinating interplay of which we may not yet know the whole story.\" Well before the ozone hole was recognized, scientists had started to worry about the effects that humans were having on the ozone layer. In 1974, Sherwood Rowland and Mario Molina, chemists at the University of California, Irvine, warned that chlorofluorocarbons (CFCs) could break down in the stratosphere and the chlorine released could destroy atmospheric ozone 2 . They later shared the 1995 Nobel Prize in Chemistry with Paul Crutzen of the Max Planck Institute for Chemistry in Mainz, Germany, for their pioneering work in understanding ozone chemistry. Concern over that global thinning and the ozone hole led to the 1987 treaty known as the Montreal Protocol on Substances that Deplete the Ozone Layer. The treaty came into effect 20 years ago and was followed up by amendments that banned the worst ozone-destroying chemicals, such as CFCs (used in refrigeration, air-conditioning and foam production) and bromine-containing halons (used in fire extinguishers). The agreements yielded quick dividends: the effective concentration of ozone-destroying compounds in the stratosphere peaked in the late-1990s and has declined since then 3 . The Montreal provisions also came with a free gift for the climate because CFCs and their kin are much stronger greenhouse gases than carbon dioxide. By getting rid of those compounds, the ozone agreement has achieved five to six times greater reductions in warming effect than the Kyoto Protocol 4 . \"The Montreal Protocol is just the most successful piece of international environmental legislation ever,\" says Solomon. \"It also contains the unwritten memo to climate negotiators that we need to, and can, do much better in controlling greenhouse warming than we have been doing so far.\"  \n                The 'world avoided' \n             A glance at a hypothetical future with no ozone protections illustrates what could have been. Paul Newman of NASA's Goddard Space Flight Center in Greenbelt, Maryland, and his colleagues, used a model that simulates chemical reactions, atmospheric circulation and solar radiation to forecast a future in which production of ozone-depleting compounds is never regulated and grows at an annual rate of 3%. By 2065, two-thirds of the ozone would be destroyed \u2014 not just over the poles but everywhere 5 . CFCs would virtually eliminate the global ozone layer by the end of the century. People in New York, Buenos Aires and Tokyo, like everyone else living in mid-latitudes (the temperate regions between about 30\u00b0 and 60\u00b0 from the Equator), would be exposed to ultraviolet radiation so extreme that they would develop dangerous sunburn within 5 minutes, on average, one-third of the time it takes today. The level of DNA-mutating ultraviolet radiation would rise about six-fold, dramatically boosting skin cancer cases in humans (see  'Ozone and cancer' ). But the ozone treaties will spare humans from living in a world unshielded from the might of the Sun. \"The worst has been avoided,\" says Martin Dameris, an atmospheric and climate scientist at the German Aerospace Centre in Oberpfaffenhofen near Munich. \"But it is also a reminder that we must not even think about watering down the protocol.\"  \n                Continued pollutant use \n             Environmental groups worry in particular about methyl bromide, which is used in agriculture to control pests. Under the provisions of the ozone-protection treaties, developed nations were supposed to stop using this compound by 2005, with developing nations following 10 years later. But lobbying by farming groups led to a stay of execution and the chemical is still in use in many developed countries. Another complicating factor is the large volume of chlorine and bromine stored in older air-conditioning and fire-fighting systems, much of which will eventually make its way into the atmosphere. Once the chemicals get there, they linger for decades. So even though CFCs and several other ozone destroyers have been phased out, they will continue to eliminate ozone for many years. At present, stratospheric ozone concentrations around the globe are about 4% below the 1964\u201380 average, but the depletions differ substantially between hemispheres and latitudes. In tropical regions, there is relatively little ozone loss. Over the mid-latitudes, where the air mixes more readily with ozone-depleted air parcels from the poles, total ozone loss has reached 3% in the Northern Hemisphere, and around 6% in the Southern Hemisphere, since 1980 (ref.  3 ). Although relatively minor compared with the extreme ozone loss over Antarctica each spring, the mid-latitude changes have a notable effect because so much of the population lives there. The additional ultraviolet light may cause hundreds of thousands of extra cases of skin cancer each year worldwide, with the full effects still decades away 3 . But there is good news for the mid-latitudes, because ozone concentrations there have started to show signs of rebounding. \"It seems as if mid-latitude ozone is going up,\" says Richard Stolarsky, an atmospheric chemist at Goddard. The upward trend is still not clear enough to be able to firmly establish its cause, he says, \"but we're pretty confident that reduced chlorine load [in the atmosphere] is contributing to the rebound\". Although researchers once estimated that it would take until 2050 for the mid-latitude ozone layer to recover fully, model simulations now suggest that point will come perhaps 20 years earlier, especially in the Northern Hemisphere, which is not greatly affected by the Antarctic ozone hole 6 . The faster recovery can be considered a benefit of global warming. As greenhouse gases trap heat in the lower atmosphere, they cool the stratosphere above, which slows down the ozone-destroying chemical reactions. Moreover, models that simulate climate and atmospheric chemistry suggest that global warming will speed up the circulation pattern that carries ozone-rich air from the tropics to the mid-latitudes, thereby boosting ozone amounts there.  \n                Quick reactions \n              Closer to the poles, however, the influence of global warming becomes less clear, especially for the Antarctic ozone hole. Cooling in the stratosphere there would, by itself, lead to more ozone destruction by stimulating the growth of polar stratospheric clouds. In the extremely dry stratosphere above Antarctica, such clouds form when temperatures drop below \u221278 \u00b0C, and they provide a surface for the chemical reactions that rapidly strip an oxygen atom from the three-oxygen ozone molecule. The ice particles play a powerful part. Outside the polar regions, a chlorine atom in the stratosphere can eliminate only a few hundred ozone molecules before it reacts with other gases such as nitrous oxide, breaking the cycle. But on the surface of polar ice particles, it can catalytically convert tens of thousands of ozone molecules. And it happens quickly. In the heart of the Antarctic ozone hole, between 14 and 21 kilometres above the surface, the loss rate can reach up to 3% per day 3 . By the beginning of October, the cloud-mediated reactions destroy nearly every ozone molecule in that altitude band. In 1992, researchers predicted that greenhouse warming would speed up the destruction so strongly that it would cause ozone holes to open above the Arctic as well 7 . But that analysis left out an important effect, says John Austin, an author of the study and a modeller at the Geophysical Fluid Dynamics Laboratory in Princeton, New Jersey. Normal atmospheric flow, called the Brewer\u2013Dobson circulation, causes air to rise into the stratosphere over the tropics, and then travel towards the higher latitudes, where it sinks back into the lower atmosphere (and heats up as it gets compressed). If climate change accelerates that cycle, it will speed up the downward flow above the polar regions, which would enhance the compression of the sinking air and raise atmospheric temperatures there. Especially in the Arctic, that heating effect in the polar stratosphere will impede ozone loss, says Austin. With climate change acting both to warm and to cool the polar stratosphere, researchers do not yet know which effect will win out. The results are not consistent from one model to the next and there is still considerable uncertainty on this topic, says Darryn Waugh, an atmospheric scientist at Johns Hopkins University in Baltimore, Maryland. One factor holding back progress in this area is the time it takes to conduct the research. There are still relatively few groups with the computing power to run long simulations with sufficiently complex models that mimic both atmospheric chemistry and climate. In the case of the Princeton lab, it takes 3 months of continuous computing with a 100-processor system to conduct a 100-year simulation, says Austin. \"I have to watch it every day,\" he says, to make sure that the experiment does not get stuck. \"Otherwise three months turns into four or five.\" For now, the ozone hole above Antarctica shows no sign of getting any better. In 2002 and 2004 the ozone loss was less severe, but in 2006 ozone levels fell to a new record low and have since remained depressed. Most scientists don't expect the recovery to start until at least 20 years from now \u2014 a decade or so later than was projected just five years ago. That is because models that simulate the movement of chemicals suggest that the stratosphere over Antarctica will remain saturated with ozone-destroying substances for another 10\u201320 years. And according to many models, it will take until 2060 or 2065 for nearly complete recovery, when polar concentrations of ozone-destroying compounds drop below their 1980s values. The latest modelling results have raised an additional complicating factor: the influence of bromine compounds. Concentrations of bromine in the stratosphere are higher than models suggest they should be, perhaps because more bromine is making its way into the stratosphere than had been predicted. If that trend continues, a small ozone hole \u2014 about one-tenth its current size \u2014 would continue to form for decades beyond 2065, according to work done by the Princeton lab. \"Even by the end of the century, you don't see complete recovery,\" says Austin, who cautions that this result is still preliminary. For humans, and for most animals and plants, the extra ultraviolet light let in by the ozone hole may not present a significant problem. The peak ozone loss occurs early in the spring, when the Sun is still low over the horizon, which limits the amount of ultraviolet radiation reaching the surface. \"If you have to have an ozone hole, the best place is Antarctica,\" says Solomon. \"If it occurred in the tropics it would be very damaging to life.\" But aside from letting in slightly more radiation, the ozone hole has the indirect effect of altering the local climate, which causes atmospheric changes that can be felt throughout much of the Southern Hemisphere. Ozone is normally an enormous source of heat for the stratosphere because it absorbs solar ultraviolet radiation. The absence of ozone throughout much of the polar stratosphere leads to a 6 \u00b0C or so cooling there. Solomon has found that this cooling effect has helped to seal off the Antarctic continent by strengthening the vortex of westerly winds that flow around the polar cap 8 . This discovery serves as an explanation for several puzzling features of Antarctica's climate. The strong vortex locks very cold air on the high plateaus in the continent's interior and thus shelters the coldest regions on Earth from the effects of greenhouse warming. Large parts of Antarctica have experienced a cooling trend over the past 30 years \u2014 a fact often raised by those who deny any reality to global warming. But Solomon's work shows that ozone loss helps to explain the lack of warming in the interior parts of Antarctica.  \n                Message in a bottle \n              With the colder air bottled up over Antarctica, it does not flow out as frequently over the edges of the continent. That helps to explain why the Antarctic Peninsula, exposed to relatively mild oceanic air, has become one of the fastest-warming places on Earth. In the past couple of years, two large ice shelves have disintegrated along the peninsula, whereas six others show signs of retreat, sending gargantuan icebergs floating off into the Southern Ocean. So as the ozone hole shrinks during this century, it will uncork the air bottled up over the continent. \"The polar vortex won't be as tight anymore, and cold interior air could make it to the peninsula more frequently,\" says Solomon. \"That would help.\" \n               boxed-text \n             Elsewhere around Antarctica, for instance in the Ross Sea, sea-ice coverage is growing. Solomon was among the first to suggest that stronger atmospheric circulation around Antarctica may be behind that seemingly puzzling trend as well. Indeed, model experiments have recently confirmed that the ozone hole alters circulation patterns in just such a way to explain the sea-ice expansion and the overall climate changes in the Antarctic 9 . Looking beyond Antarctica, James Risbey of the Center for Dynamical Meteorology and Oceanography at Monash University in Clayton, Australia, suggests that ozone loss is responsible for reduced rainfall and more frequent droughts over southern Australia. This happens, he says, because the cooling caused by ozone depletion over Antarctica has lowered atmospheric pressures there, speeding up the vortex of winds around the continent (see  graphic ). That pulls the rain-bearing westerly winds southwards, keeping them away from Australia. Risbey speculates that the ozone loss over Antarctica has caused a 20% reduction in rainfall in the dry season over Australia. \"Ozone chemistry and atmospheric dynamics influence each other. That's why it's difficult to keep apart what's pushing from what's pulling,\" says Dameris. \"Much is speculation still, and there may be surprises.\" His experiments with a model that simulates climate and atmospheric chemistry suggest that rising sea surface temperatures in the tropical oceans will strengthen the Brewer\u2013Dobson circulation 10 . That could result in less ozone inside the tropics and more ozone outside, leading to abnormally high ozone concentrations in the mid-latitudes. That extra ozone would help protect people but it could have unwanted effects, perhaps by altering the chemistry of the stratosphere or by hindering the growth of plants, says Dameris. Over the long term, as chlorine and bromine pollution are cleaned from the skies, climate change will take over and be the dominant human influence on ozone concentrations, he says. \"Not many people have started thinking about what that might mean. It's getting about time.\" \n                 See Editorial,  \n                 page 781 \n               Additional reporting by Richard Monastersky. \n                     The Road to Copenhagen \n                   \n                     United Nations Ozone Secretariat \n                   \n                     NASA Ozone Hole Watch \n                   Reprints and Permissions"},
{"file_id": "460944a", "url": "https://www.nature.com/articles/460944a", "year": 2009, "authors": [{"name": "Virginia Gewin"}], "parsed_as_year": "2006_or_before", "body": "Ecologists have struggled to reconcile what they see in the lab and in the wild. But both views are needed to understand the effects of extinction, finds Virginia Gewin. More than 100 streams run through Bradley Cardinale's laboratory. Some trickle, others gush. Some are home to one species of freshwater algae, others to eight. And they were all created to answer a simple ecological question. Cardinale wants to know how the number of species living in a stream affects the quality of water that flows through it. The problem calls for controlled, replicable experiments, hence the artificial streams in his lab at the University of California, Santa Barbara. But he must also show that the results he sees in the lab apply to the water found in natural streams. So he is running a parallel experiment in the Sierra Nevada mountain range to compare how communities with naturally low and high species diversity clean up the streams that flow there. Tackling this question has taken Cardinale five years, in part because he has gone to great pains to include an aspect of reality in his lab experiment \u2014 water flow, which influences diversity. In the next stage he will have to quadruple the number of streams to analyse the effects of adding one algae-eater and its predator into the system. \"The complexity of this work is invigorating and, at times, frustrating,\" Cardinale says. \"But in an era when up to half the world's species hang in the balance, we must confront the complexity head on.\" Within that complexity, hope Cardinale and others, lie answers to the question of whether the current wave of extinctions will just result in fewer pretty flowers and birds to look at, or whether it will mean poorer soils, more carbon in the atmosphere and the loss of billions of dollars' worth of 'ecosystem services' \u2014 natural processes that benefit humans, such as water purification, pollination and pest control. How to measure the impact of species loss on ecosystems and their services has been the subject of lengthy, inconclusive and sometimes bitter debate among ecologists. It has taken a long time to work out the best way to ask the question, with lab and field experiments often pitted against one another, and researchers agonizing over whether they should sacrifice biological reality for experimental purity. In recent years, ecologists such as Cardinale have realized that 'field with lab' is a more productive path than 'field versus lab'. Findings from such combined studies suggest that researchers have underestimated how important biodiversity is to ecosystem functioning, which is both vindicating and sobering to those who stress nature's practical value. The next step, they say, is to try to use this hard-won knowledge to guide efforts to save what's left, and to help restore natural places to make them useful to human needs. \"We need to amass the science necessary to make predictions now, before more species go extinct,\" says Cardinale, \"so that we can understand how many species must be restored to degraded ecosystems in order to regain functions like pest or disease control.\"  \n                Variety: the spice of life \n              By the mid-1990s, most of the world seemed to believe that conserving the species in an ecosystem was important to maintaining its natural functions. In December 1993, countries that are party to the United Nations Convention on Biological Diversity committed to developing strategies to conserve and sustainably use biological diversity because, the convention argued, conservation would provide environmental and economic returns. Yet up until then, there had been little direct evidence of diversity's importance for environmental and human welfare. The debate swung backwards and forwards for decades, between ecologists' intuition that diverse ecosystems must be healthier than impoverished ones, and theoretical studies that suggested that as the number of species in a system grew, more extinctions would be expected 1 . With the convention, governments, along with the researchers they look to for guidance, had an urgent need to clarify the relationship between diversity and function, so that they would know what was important to conserve. In 1994, two papers brought new experimental approaches to the question. Using a long-term study of grassland plots in Minnesota 2 , ecologists David Tilman of the University of Minnesota in St Paul and John Downing, who was then at the University of Montreal, Canada, found that plots with more species were less affected by droughts, showing smaller reductions in biomass, and recovering to their pre-drought state more quickly. In the same year, Shahid Naeem and his colleagues at Imperial College London used the 'Ecotron' \u2014 a system of walk-in cells containing square-metre plots, each stocked with differing numbers of plant, herbivore, decomposer and predator species \u2014 to show that more diverse microcosms produced more biomass and consumed more carbon dioxide. A doubling or tripling of the number of species resulted in a roughly equivalent rise in productivity 3 . Tilman and Downing saw their results as support for the idea that the more species you have, the more likely a system will stay productive in the face of a drought or other crisis, because some species will be tolerant to the stress. Naeem's team suggested that the key lay in division of labour: a more diverse community is more fully able to exploit the spectrum of resources, such as light, soil nutrients or water availability. Either way, the implications for conservation were enormous. \"If the number of species was important, then we had a responsibility to conserve every species,\" says Cardinale. But many ecologists challenged the papers' conclusions. Until then, most scientists had believed that an ecosystem's function depended on whether it contained key species, with the total number of species having at most a minor effect. One of the fiercest critics, Michael Huston of Texas State University in San Marcos, thinks that the 1994 studies failed to exclude this possibility. \"These experiments were designed, intentionally or not, to always show that more species have a higher level of ecosystem function,\" he says. Huston argues that any relationship between diversity and function in those studies was probably due to a 'sampling effect' 4 ; that is, higher productivity in diverse ecosystems could be explained by the greater probability that they would contain one or two highly productive species doing all the ecological heavy lifting. \"No one disagrees that biodiversity influences ecosystem function,\" Huston says, \"but is it the number of species or the properties of specific species?\" Conservation efforts, he argues, shouldn't be based on the understanding that more species make for better ecosystem functions. Some charged the researchers with blatant bias, accusing them of interpreting their studies so as to advance a political agenda for conservation by showing that biodiversity was economically valuable and important to human well-being. \"Those comments were said in private conversations, and I personally found them astounding,\" says Tilman. He knew from the start that the study's conclusions would cause controversy, so he sat on his data for years, testing alternative hypotheses. \"I didn't quite believe the results myself at first,\" he says. But Naeem, a postdoc at the time, and now at Columbia University in New York, was surprised that what he calls his \"innocent experiment\" caused such a stir. He says that his adviser, John Lawton, a community ecologist at Imperial College London, was concerned that Naeem's experimental approach would jeopardize his nascent career. In ecology, says Naeem now, any experimental approach is liable to draw criticism. \"The only thing that is real in science is what can be seen through a pair of binoculars. As soon as we start to mess with it, it becomes suspect,\" he says. Even so, many ecologists spent the next decade messing with it. They built model ecosystems in an effort to control the myriad variables at work and to isolate the effects of species number. But in the quest for rigour, the experiments and the way they were manipulated became so unlike natural ecosystems that the goal \u2014 to uncover what happens when species go extinct \u2014 was sometimes missed. For example, it became customary for controlled experiments to randomize the addition or removal of species and environmental conditions, but such processes are not random in natural systems. Critics charged that ignoring the factors controlling biodiversity while studying the ecological effects of it rendered such studies irrelevant.  \n                Heated debate \n              Temperatures rose as researchers took sides, some advocating the sampling effect, others arguing that efficient partitioning of resources among species was the key to increased productivity. \"The debate had to happen, and it helped the community make progress to reconcile seemingly contradictory hypotheses, but it got a bit nasty,\" says Michel Loreau, an ecologist at McGill University in Quebec, Canada. In 2000, Loreau and Naeem organized a meeting in Paris to encourage everyone to look at the same data and test hypotheses together. \"We established a behaviour of confronting differing opinions,\" says Loreau. A consensus paper resulted 5 , which proposed a quantitative method to parse out the effects of relationships between species versus Huston's sampling effect. It also prompted additional work to reconcile findings from theory, observation and experiment. Most ecologists are now convinced that more-diverse ecosystems are generally more productive. To get the big picture, Cardinale led two meta-analyses, one of more than 100 experiments 6 , the other of 44 (ref.  7 ), in which researchers had experimentally manipulated species in both artificial and natural aquatic and terrestrial systems. The meta-analyses showed that diversity actually has a larger effect on productivity than anyone had previously documented. Mixtures of two or more species were more productive than the average monoculture 79% of the time, and mixed plots were 1.7 times more productive as those with single species 7 . The longer an experiment ran, the more pronounced the difference became, probably because ecosystem processes change as species settle into stable, complementary relationships. \"We were stunned by the magnitude of diversity effects,\" Cardinale says. \"If anything, we have underestimated the impacts of species extinction on ecosystem productivity.\" The studies also revealed that although Huston's sampling effect is real, it accounts for just one-third of the overall increase in productivity. The other two-thirds is due to interactions between species and a better division of labour. A suite of different species seems to be able to partition resources in such a way that their own properties or specialties are performed more efficiently. Like the sampling effect, however, this insight throws the spotlight back on the attributes of species. Some think that understanding how these properties, known as functional traits, contribute to ecosystem function is more useful in predicting the effects of extinctions than looking at species number. \"We were so focused on the diversity of life,\" says Naeem, \"we didn't look at the diversity of function.\"  \n                Bridging the gap \n              Many ecologists, including Tilman and David Wardle, an ecologist at the Swedish University of Agricultural Sciences in Ume\u00e5, have been pursuing this approach all along. In 2005, for example, Wardle removed specific shrub species as well as specific functional traits, such as large roots, from 30 small islands in Swedish lakes. Some islands were affected very little and some a lot, showing how site-specific a trait's effects can be 8 . \"Rather than simplify the system, we started with real ecosystems and measured what happens when different species are removed,\" says Wardle. In recent years, ecologists have found other ways to bridge the gap between experimental clarity and biological reality. Experimenters have covered larger scales in space and time, and have expanded analyses across multiple levels of the food web, to include plants and animals, predators and prey. \"It is possible to be rigorous and relevant at the same time,\" says John Stachowicz, a marine ecologist at University of California, Davis. Like Cardinale, Stachowicz and his colleague Matthew Bracken, a marine biologist at Northeastern University's Marine Science Center in Nahant, Massachusetts, run parallel lab and field experiments. In 2008, they compared the effect of seaweed species richness on biomass in field plots on Pacific shores and in experimental tidal beds 9 . They found that more species meant a higher rate of biomass accumulation in the field, but not in the experimental chamber. They put this difference down to the fact that the chambers lacked the variability needed to pick up the effects of the division of labour, or the arrival or departure of species. They also found that the effects of biodiversity become stronger over time in the field, and suggested that many experiments did not run for long enough to see such effects. Bracken is opening up a third front of investigation \u2014  in silico . His team has combined experiments with computer simulations of 3,500 combinations of seaweed species to model a larger number of scenarios than can be covered experimentally. The researchers found that with a realistic mix of species \u2014 which in nature, is controlled in part by wave exposure \u2014 more diversity meant more nitrogen uptake, whereas randomized species mixtures showed no such effect 10 . Most recently, he modelled a more realistic food web by adding snails to the mix, and found that their preference for sea lettuce ( Ulva lactuca ), the region's dominant seaweed species, increased overall plant productivity by allowing other species to grow 11 . Every small narrowing of the gap between experiment and reality reveals new things about ecosystem dynamics. But the gap may not be shrinking fast enough to reach firm conclusions \u2014 which may present a problem for conservationists who want answers now. \"We're ten years away from telling policy managers which fraction of species to conserve,\" Cardinale says. Cardinale is still optimistic \u2014 in a pessimistic sort of way \u2014 about ecologists' ability to provide practical advice. When he's not tending his 100 streams in the lab, he spends much of his time working in the field, wading in stream ecosystems that are losing species owing to residential development or nutrient pollution. \"For many species and ecosystems, conservation efforts may have come too late,\" says Cardinale. But, he says, it may be possible to restore them, or to find another way to save or replace the ecological services they provide. \"If we can nail down this field, we might be able to turn back the hands of time.\"\n \n                 Virginia Gewin is a freelance writer based in Portland, Oregon.  \n               \n                     Brad Cardinale's lab \n                   \n                     Convention on Biological Diversity \n                   \n                     The Ecotron \n                   \n                     Biodiversity and Ecosystem Function Online \n                   Reprints and Permissions"},
{"file_id": "460947a", "url": "https://www.nature.com/articles/460947a", "year": 2009, "authors": [{"name": "Alison Abbott"}], "parsed_as_year": "2006_or_before", "body": "Patients checking in to the German Mouse Clinic will undergo the most sophisticated medical testing in the world. But, finds Alison Abbott, the waiting list is becoming a problem. Each and every Thursday it is the same. At 10 a.m., the conference room at the Helmholtz Centre Munich starts to fill with top consultants and clinical researchers from the university hospitals in Munich, Bonn and Heidelberg. On the table is a stack \u2014 thicker than the city's telephone directory \u2014 of clinical data on a couple of hundred patients. The data are comprehensive \u2014 from blood analyses, to lung function, to metabolism. Some weeks there are brain and body scans in there too. After two or three hours of presentations and cross-speciality discussions, the group comes up with diagnoses they consider most likely. It's a scene that could be imagined in any world-class hospital. But the patients involved are not human. This is the German Mouse Clinic, and the patients are genetically modified mice. The batteries of tests have been designed to identify every possible consequence of the animals' altered genes. And the results, the clinical scientists hope, will help them to work out what part each gene plays in cellular processes and in human disease. Perhaps the most famous of the clinic's patients is the Foxp2 mouse, engineered to bear a 'humanized' version of the  Foxp2   gene, implicated in the evolutionary development of human speech and language. The gene, modified in the mice so that it contains two amino-acid substitutions that are normally present only in humans, changed the animals' ultrasonic calls and reduced their propensity to explore, but had no effect on organs outside the brain ( W. Enard  et al .  Cell    137,   961\u2013971; 2009 ). \"The negative results are as exciting as positive results,\" says Wolfgang Enard, a computational biologist at the Max-Planck Institute for Evolutionary Anthropology in Leipzig, Germany, and creator of the  Foxp2   mouse. He was asked by the clinic to send a second batch of mice because staff couldn't believe the brain-specificity that they were seeing. The discovery that the humanized gene affects specifically the animal's brain and vocalization supports the idea that the amino-acid changes were important in human speech evolution. Everyone in the world is welcome to send their mutant mouse to the clinic, which opened in 2001. And many are starting to do so. \"We are getting more and more requests, and a waiting list is starting to build up,\" says geneticist Martin Hrab\u00e9 de Angelis, who conceived and heads the clinic. Geneticists estimate that around half of the 20,000 or so mouse genes have been altered by various means to make mutant strains. Some of these have been created expressly as animal models of disease or to get at a particular biological mechanism; others as part of systematic gene knock-out programmes such as the Knockout Mouse Project centred at the US National Institutes of Health (NIH) in Bethesda, Maryland. Many of these animals go through check-ups in the labs of the researchers that made them. But some have no obvious defects \u2014 and, for almost all of them, the rigorous 'phenotypic' analysis that the mouse clinic provides could reveal more useful data about their condition.  \n                Unknown associations \n              From the point of view of its patients, the mouse clinic offers two months of comfortable living interrupted by 320 individual tests in 14 clinical areas ranging from neurology, to vision, to immunology. They have roomy accommodation, good food and a warm, pathogen-free environment. Owners of the mutant mice complete a collaboration agreement with the clinic, and funding from the German research ministry allows the service to be offered free. The collaborators then provide the clinic with 80 mice, and testing begins. Some of the tests involve just being watched. The mice are placed in an 'open field' \u2014 a flat board with a camera suspended above it \u2014 while staff observe their breathing, how much they explore a new territory and with what enthusiasm. Other tests can be more challenging for the mice. To have their blood pressure measured, they are placed in a restraining box with their tails hanging out. An air cuff, similar to the arm air cuff in a doctor's surgery, is wrapped around the tail. The procedure is done several times a day to follow the circadian changes in blood pressure. The mouse clinic has an array of sophisticated apparatus \u2014 for scanning the body in different ways, for example, or for assessing the structure and function of the heart using ultrasound \u2014 all scaled down to mouse size. At some point in their stay, the mice make an excursion to the neurology clinic. There they get to playfully hang on a rod while a scientist, by lightly pulling on their tails, gauges how strongly their paws can grip. The mice also balance on the 'Rotarod', which turns at increasing speed until they fall off, to gauge coordination. In the sealed perspex cube of the metabolic chamber researchers measure the amount of oxygen the mice breathe, the carbon dioxide they exhale and the heat they generate, to calculate basal metabolism. Everything the animals eat is weighed and every bit of faeces combed through, to determine how much energy they have absorbed from food. Then, for those unlucky mice that are selected, comes pathology, histology and, as the web page euphemistically declares, 'chemical carcass analysis'. All these tests and more are the fodder for the clinicians' Thursday discussions, to which the owners of the mice are invited. On the basis of the diagnoses, the clinical team may advise the owners to dig more deeply into particular phenotypes in secondary and tertiary screens that the clinic can perform. Something abnormal in antibody measurements, for example, might point to more complex allergy tests. Hesitancy in the open field, or a wobble on the Rotarod, might prompt advice to run brain tests, such as electroencephalograms, as well as sophisticated tests for memory, depression or anxiety, or sensory or motor function. Such screens are inevitably more time-consuming, more expensive and more complex than the primary screen.  \n                Famous patients \n              Few other mouse facilities in the world perform such a breadth of phenotyping tests; hence few other mice on the planet are as closely scrutinized as the ones that enter this clinic. The level of data analysis and the generation of testable hypotheses also differentiates the German Mouse Clinic from screening offered elsewhere. Karen Svenson, who runs a mouse clinic at the Jackson Laboratories in Bar Harbor, Maine, says she hopes that \"in the near future there will be an international effort to support worldwide clinic approaches\", like that used by the Munich clinic. More than 200 mutant mouse lines have been put through their paces at the German clinic since it opened. The Helmholtz Centre Munich supplies many of them from its programmes for creating mutant mice. One of these, the Aga2 mouse, is among those patients of the clinic with the most potential to influence medical practice. As a model for brittle bone disease, or osteogenesis imperfecta, it carries a mutation in the  Col1a1   gene that is crucial in collagen production and mutated in the majority of people with this condition. It had been assumed that early death was usually a consequence of a stiffened ribcage. Unable to breathe deeply, patients succumb to multiple pneumonias. Hrab\u00e9 de Angelis was surprised when the clinical parameters thrown up in the primary mouse screen pointed in a different direction. \"The blood pressure measurements weren't normal and there was something not quite right about the breathing,\" he says. The next round of screening revealed that the mutated  Aga2   gene actually causes direct, and fatal, damage to the lungs and heart. Joan Marini, an expert in osteogenesis imperfecta at the NIH, says that the insights offered have been \"tremendous\". \"It will make clinicians more attentive to early preventative measures such as pulmonary exercise or bronchodilators, which could help keep the lungs elastic for longer,\" she says. The clinic's waiting list is only likely to grow. Most biomedical researchers would like their mice to have more than one mutation per gene, to explore their different biological effects. On top of this, they are starting to recreate in mice the more complex types of genetic alterations that have been associated, however loosely, with human disease. Dusan Bartsch, a molecular biologist at the Central Institute of Mental Health in Mannheim, Germany, for example, engineers in mice specific genetic architectures that are frequently found in people with mental disorders such as schizophrenia. These include absent regions called 'microdeletions' and copy number variations, genetic regions that are repeated different numbers of times. \"We are sending these mice to the clinic because there is just no way of predicting what other parameters could be affected by these complex gene changes,\" Bartsch says. Hrab\u00e9 de Angelis still wants to improve the range of testing. The phenotype expressed by any organism is dependent on the environment in which the organism finds itself. So the mouse clinic is piloting a new phase in which mice are tested in five different types of environment \u2014 what Hrab\u00e9 de Angelis refers to as envirotypes. These include exposing the mice to stress, exercise or infection, giving them high-calorie 'cafeteria' diets and placing them in 'city' air polluted with diesel fumes. But testing each mouse in the five envirotypes takes time, which will only make the waiting list even longer. To help keep pace with the number of mutant lines being created, Hrab\u00e9 de Angelis is pinning hopes on winning long-term funding for the 'Infrafrontier' consortium, a European Union project to develop mouse functional genomics and, in particular, to upgrade other, smaller, mouse clinics around Europe so that they can perform at least the first round of screens. These are the Mouse Clinical Institute in Strasbourg, France, the UK Medical Research Council's Harwell unit and the Wellcome Trust Sanger Institute in Hinxton, UK, but others are now planned in Spain, Italy and the Czech Republic. There are already mouse clinics either open or planned in China and Japan. Circadian biologist John Hogenesch from the University of Pennsylvania, Philadelphia, muses on how much faster his own field might have developed with the systematic phenotyping these clinics offer. The first mouse mutant with a disturbed circadian clock was made in 1997 ( D. P. King  et al .  Cell    89,   641\u2013653; 1997 ) and since then others have been found in individual laboratories. \"If we had had mouse hospitals earlier,\" says Hogenesch, \"we could have advanced circadian biology by a decade.\" \n                     Nature Reviews Genetics \n                   \n                     German Mouse Clinic \n                   Reprints and Permissions"},
{"file_id": "461462a", "url": "https://www.nature.com/articles/461462a", "year": 2009, "authors": [{"name": "Eric Hand"}], "parsed_as_year": "2006_or_before", "body": "Canada's Perimeter Institute of Theoretical Physics was intended to become a world leader in the field. Eric Hand finds out if it has lived up to its ambitions. Working at the Perimeter Institute for Theoretical Physics comes with certain perquisites. Whenever recruits arrive at the Toronto airport, for example, they are met by a limousine and driven west along Canada's Route 401 into the rich farmlands of Ontario. Eighty-five kilometres later, the limousine works its way through the streets of the town of Waterloo, and lets them out in front of a sleek building of black, green and glass squares that stands next to a pond in Waterloo Park. Stepping inside, the recruits find wall-to-wall blackboards, working fireplaces, a sauna, multiple dispensers of free coffee and the Black Hole Bistro, which serves free lunches on Wednesdays. And if they say yes to the recruiting pitch, they get a free BlackBerry smart phone \u2014 plus the power, even as a postdoc, to invite collaborators for visits of up to 18 weeks in the year. The building itself is a gleaming architectural marvel rising from the site of a former municipal ice-hockey rink, which had to be demolished to make way. The significance is not lost on Perimeter's director, Neil Turok, a South African-born physicist (see  'A theoretical firebomber' ) who understands ice hockey's place in the Canadian psyche. The puck has been passed, says Turok. Waterloo will soon be known the world over for theoretical physics. \"Part of what Perimeter represents in Canada is this search for self-confidence \u2014 the idea that, yes, we can do something better than anyone else,\" he says. Besides, says Turok, \"Mike thinks hockey is a complete waste of time\". Mike Lazaridis, a Turkish-born engineer, is both the co-founder of Research in Motion \u2014 the Waterloo-based company that makes the BlackBerry \u2014 and a romantic about the transformative power of basic research. A decade ago he gave Can$100 million (US$95 million) to start Perimeter, and in June 2008, gave Can$50 million more. Lazaridis is hardly the first philanthropist to give money to science. For example, entrepreneur Fred Kavli stumped up US$7.5 million to help found each of 15 research institutes \u2014 in astrophysics, theoretical physics, neuroscience and nanotechnology \u2014 that have been established in his name. Such gifts are generally tied to universities with existing facilities, but Lazaridis conjured up something where there was nothing. Independent institutes aren't new either. In 1930, for example, siblings Louis Bamberger and Caroline Bamberger Fuld contributed US$5 million from the sale of their department store business \u2014 about US$65 million today \u2014 to launch the Institute for Advanced Study (IAS) in Princeton, New Jersey. And ever since, academics have been trying to create similar sanctuaries of pure research, free from the constrictions of teaching, grant writing and university management. But where institutes such as the IAS have a reputation for being \"stuffy\", as Turok puts it, Perimeter is intended to be \"more energetic, more free-thinking and less project focused\". Turok, who became Perimeter's second permanent director in October 2008, has been making energetic use of the riches at his command. In July, the institute began driving piles into the ground for a Can$30 million expansion that will double the institute's square footage. And, in a time of recession, Turok is hiring. He already has the largest population of theoretical physics postdocs in the world, with 44. And he is planning to more than double his full-time faculty from 12 to 25. Eventually, the building expansion will allow the current research staff of 85 to triple to 250, including visitors. Stephen Hawking, a former colleague of Turok at the University of Cambridge, UK, is scheduled to preside over a decadal celebration in mid-October, which will provide an excuse to kick-start a campaign aimed at doubling the endowment from its present size of Can$200 million.  \n                Grand ambitions \n              In the institute's new five-year plan, which starts in 2010, Turok writes that the overarching goal is to be the world's leading centre for theoretical physics. But the pursuit of that goal actually started in 1999, when an intense, verbal and philosophical dreamer named Howard Burton was still trying to figure out what to do with a freshly minted physics PhD from the University of Waterloo. The 34-year-old Burton didn't seem headed for an academic career. And he was unexcited by the prospect of donning a tie and cranking out financial algorithms in New York City, as so many physicists were doing at the time. On a whim \u2014 and perhaps with a whiff of desperation \u2014 Burton sent a cover letter to the chief executive of Research in Motion with the tagline: \"Please help save me from a lucrative career on Wall Street.\" Lazaridis e-mailed back, telling Burton he was welcome to a regular job at Research in Motion. But might he be interested in this other idea he was pondering? Intrigued, Burton agreed to lunch at an Italian restaurant in a suburban Toronto strip mall. \"I'm talking about doing something big,\" Lazaridis told him, according to Burton's account in his new book,  First Principles   (see  page 477 ). Lazaridis passed Burton a number written on a napkin \u2014 the beginning and end of the salary negotiation phase, apparently. \"On a tactical level, I think he was feeling me out,\" Burton says in an interview from Lyons, France, where he moved with his family after leaving Perimeter in 2007. But Burton was also feeling out Lazaridis, making sure the chief executive wanted some sort of scientific think tank \u2014 not another research lab for Research in Motion. Burton took the job. On his first day of work, he came up with the name 'Perimeter', inspired by a walk around the curving edge of Lake Ontario. Next, he started visiting other physics institutes, partly to begin recruitment and partly to absorb the lessons of each. He decided that he wanted Perimeter to have a resident faculty like the IAS. But he also wanted to emulate the nimbleness of the Kavli Institute for Theoretical Physics (KITP) at the University of California, Santa Barbara. So he followed the KITP strategy of staying  au courant   with the hottest research via visitors that arrive for short-term programmes and workshops (see  Nature  doi:). Burton also wanted Perimeter's researchers to tackle overlooked niches of science in the manner of the Santa Fe Institute in New Mexico, which was founded by a group of Los Alamos physicists in 1984 to explore then-obscure notions of complex adaptive systems. So he looked for researchers who were interested in neglected areas of physics such as the root problems of quantum mechanics, a research area he called quantum foundations; and non-string theory approaches to quantum gravity, which seeks a unification of gravity and quantum mechanics. By 2001, Burton's first recruits were at work in Waterloo's historic post office building. In late 2004, his growing crew moved into their new home by the park. And by 2007, he felt he had achieved most of his goals. Burton stepped down as director in June of that year. He had never planned to stay forever, he says. But he could also feel a distinct chill in the atmosphere: he says that Lazaridis, the chairman of Perimeter's board, had stopped trusting and liking him \u2014 presumably because Lazaridis had learned that Burton was writing a book about his experiences. A Perimeter spokesman says that Burton's departure was a private matter and unconnected with a book.  \n                Unique set-up \n              Whatever the reason, Burton still bristles over what he perceives as Lazaridis' excessive control over the institute. He concedes that Lazaridis has given more to Perimeter than has any other source. But he points out the inventor's largesse is nearly matched by more than Can$100 million in federal and provincial funding. Even the city of Waterloo chipped in, giving Perimeter its park site. Why don't any of these institutions \u2014 or any physicists for that matter \u2014 have a seat on the board, asks Burton. \"Is this a situation where, effectively, it is a rich man's toy?\" Lazaridis has had an unusually strong hand in the management of Perimeter, agrees IAS astrophysicist Scott Tremaine, a Canadian who has served on Perimeter's scientific advisory committee. \"The usual tradition is that you leave your hands off,\" he says. Turok acknowledges that, when he was first offered the job, he thought it a bit strange that the board consisted of lawyers, businessmen and engineers. \"Why isn't this governed by a group of scientists?\" he asked. But he maintains that his relationship with Lazaridis, who declined to be interviewed by  Nature , is fine \u2014 not least because Lazaridis has kept his promise never to interfere with Turok's authority on the strategy and scientific direction of the institute. By now, says Turok, he actually prefers Perimeter's structure. The make up of the board helps give the place a risk-taking spirit that is more in keeping with a Silicon Valley start-up than an academic venture. And that means Turok can act faster. Since his arrival in October 2008, for example, he has doubled the areas on which researchers focus from four to eight, including a new emphasis on cosmology (see  'On the perimeter' ). He has added a 9-month graduate programme to a place that had no teaching. And to remind staff that the coffers aren't bottomless, he has switched to a cheaper brand of the coffee that Perimeter provides for free. This ability of Perimeter to move quickly \u2014 in matters both big and small \u2014 is what amazes Raymond LaFlamme, a quantum-information scientist who was plucked from Los Alamos National Laboratory in New Mexico as one of Perimeter's first hires. In 2002, he became director of a Perimeter spin-off: the Institute for Quantum Computing (IQC) also in Waterloo, which has received a separate injection of nearly Can$50 million from Lazaridis. The IQC has closer ties to the University of Waterloo, and therefore less independence than Perimeter, and LaFlamme has a telling example of the difference. He still keeps an office at Perimeter, and on a stand inside his office window is the receiver for a quantum-cryptography experiment that detects entangled photons from a transmitter on the University of Waterloo campus a few kilometres away. To allow the photons to reach the detector without destroying the quantum effects, LaFlamme needed to cut out a small piece of the tinted glass in his office window and replace it with clear glass. At a federally funded lab such as Los Alamos, he says, such a request would have been a nightmare. And at the nearby IQC building, where he keeps another receiver, the job took many months. At Perimeter, it took a day. \"I went down the hall and talked to the guy in charge of the building. He said, 'Okay'. And \u2014 bang \u2014 it was done.\" And yet \u2014 has this entrepreneurial energy led to new physics? Another bold goal of Turok's five-year plan is the expectation of \"major scientific breakthroughs\". \"Everybody wants a breakthrough,\" says KITP director and Nobel laureate David Gross, who is both a friend and mentor to Turok. But \"you don't order results like that\". That's true, says Turok. But you can increase the odds by packing as much talent as possible into a room, and fuelling everyone with free coffee.  \n                Different approach \n              The coffee is sometimes needed. Consider an evening in June, for example. The sun is sinking, but Nima Arkani-Hamed and Freddy Cachazo are just getting going. Several empty coffee mugs are scattered on Cachazo's desk. Arkani-Hamed has just a day before he jets to Rome to present their work at a major string-theory conference, and they are feeling the pressure. Their hope is that, by dusting off a long forgotten corner of particle physics \u2014 S-Matrix theory, a model of particle interactions that began to be superseded by more fruitful accounts in the 1960s \u2014 they can clarify a very modern notion known as holography, which holds that information about the Universe can be encoded in fewer space-time dimensions than are apparent to us. \"Freddy and I are solving the mysteries of the Universe,\" Arkani-Hamed announces, only half-jokingly. The exuberant Arkani-Hamed sits at Cachazo's desk, while the more softly spoken Cachazo lounges on a sofa thumbing his standard-issue BlackBerry. One of Perimeter's young stars, Cachazo says he came to Canada from Venezuela partly to escape his country's heat. Arkani-Hamed, a 'distinguished research chair' at Perimeter, is a frequent visitor from the IAS. Keeping themselves going with a stash of cereal kept behind a stack of books, they work through the night, trying to determine the signs of 12 terms in an equation. They finally crack it at 4 a.m., and leave an hour later for some sleep. It is a small step along what may end up being a blind alley. But it is the kind of effort that Turok wants: undirected, unconventional, ambitious. Only time will tell whether these two physicists can reach their goal \u2014 or whether Perimeter itself will. \"Early on,\" says Tremaine, \"I was asked about Perimeter, and I said it might be the most important new institute since the IAS was founded,\" he says. \"I still say that is true. And I still would use the word 'might'.\" A few hours after Cachazo wakes up from his all-nighter, a delivery truck accidentally backs into the sprinkler system below Perimeter's car park. A fire alarm goes off. Dozens of physicists emerge, blinking in the hot, midsummer sun, to discover a dislodged sprinkler pipe spewing a jet of water. Half of its spray hits Perimeter's entrance wall, and the young physicists \u2014 some in shorts, some in khakis \u2014 happily soak themselves in the mist. The other half of the jet soaks an Aston Martin belonging to Lazaridis, who is at Perimeter for one of his frequent visits. \"Mike's getting a free car wash,\" says Turok, as Lazaridis ducks into his sports car. The money man leaves, but the problems of physics remain. After the alarm is silenced, the physicists file back into the building's sun-soaked atrium, passing under a Greek inscription \u2014 the same phrase that supposedly hung from Plato's academy: \"Let no one untrained in geometry enter here.\" Cachazo returns to his top-floor office, where the sun is beating in, and resumes work. But the air conditioning isn't working \u2014 perhaps because of the fire alarm. The temperature in his office climbs to 30\u00b0 Celsius. He feels sluggish. His brain is useless. The Venezuelan declares he is ready again for Waterloo's winter: \"That's why I moved to Canada.\" \n                     Perimeter Institute website \n                   \n                     Next Einstein website \n                   Reprints and Permissions"},
{"file_id": "461590a", "url": "https://www.nature.com/articles/461590a", "year": 2009, "authors": [{"name": "Quirin Schiermeier"}], "parsed_as_year": "2006_or_before", "body": "From the 1950s, science has been a priority in the Soviet-controlled Eastern bloc. Space science and nuclear physics, in particular, received generous support, and achievements such as the Soviet space missions served as proof of the alleged superiority of the communist system. Then in the late 1980s, collapse of communist regimes and their replacement by democracy and market economies led to a dramatic drop in science expenditure across the region. Reliable data for the early 1990s \u2014 the period of worst hardship \u2014 are unavailable because national statistical services were not yet up and running or had no reported science figures. Between 2003 and 2004 the European Commission included the ten countries in central and eastern Europe that have since joined the European Union (see  map ) in its regular reports on science and technology indicators. Figures from the 2008\u201309 report show that research intensity \u2014 the percentage of gross domestic product spent on research and development (R&D) \u2014 is below the European average of around 1.8% in all ten countries (see  graphs ). \n               Click here for larger image \n               But there are strong differences within the region. Although the Czech government has in recent years invested substantially in science, the research bases of countries such as Bulgaria and Romania are still underdeveloped. Notable improvements, mainly driven by the business sector, have been made in Estonia and Hungary \u2014 although the recent economic crisis has hit Hungary harder than it has hit others, threatening the upward trend. Poland, the largest country in the region, has in recent years fallen further behind in reaching EU and national goals for R&D intensity, as have Slovakia and Bulgaria. There is a general lack of highly qualified scientists and technical workers, whose share of the overall workforce ranges between 9.8% in Romania and 16.8% in Estonia. The region also still fails to attract much foreign scientific talent. Only 1% or so of participants in the EU's Marie Curie Actions programme \u2014 which promotes and facilitates the mobility of young scientists in Europe \u2014 choose a university, research institute or industry lab in the new member states as their host institution, possibly due to the resources and reputations of other member states. \n                     European Union \n                   \n                     European Commission Research Directorate-General \n                   \n                     International Monetary Fund \n                   Reprints and Permissions"},
{"file_id": "461720a", "url": "https://www.nature.com/articles/461720a", "year": 2009, "authors": [{"name": "Joerg Heber"}], "parsed_as_year": "2006_or_before", "body": "Small oscillations of surface electrons that manipulate light on the nanoscale could be the route to applications as disparate as faster computer chips and cures for cancer. Joerg Heber reports. Toss a rock into a quiet pond, and watch the ripples spread out across its surface. This is pretty much what happens when a photon hits the surface of a metal \u2014 except that in this case, the 'ripples' consist of electrons oscillating en masse and have wavelengths measured in nanometres. Once they are set in motion, these 'surface plasmons', as the oscillations are known, can pick up more light and carry it along the metal surface for comparatively vast distances. \"A river of light\" is how Satoshi Kawata, a physicist at Osaka University in Japan, describes the phenomenon to his students. Plasmons can also focus light into the tiniest of spots, direct it along complex circuits or manipulate it many other ways. And they can do all of this at the nanoscale \u2014 several orders of magnitude smaller than the light's own wavelength, and therefore far below the resolution limits of conventional optics. The result is that plasmonics has become one of the hottest fields in photonics today, with researchers exploring potential applications in solar cells, biochemical sensing, optical computing and even cancer treatments (see  'Plasmons at work' ). Their efforts, in turn, have benefited greatly from the flowering of nanotechnology in general over the past decade, which brought with it a proliferation of techniques for fabricating structures at the nanoscale \u2014 exactly what plasmonics needed to progress from laboratory curiosity to practical applications. \"The late 1990s was kind of the turning point\" for plasmonics, says Harry Atwater, a physicist at the California Institute of Technology in Pasadena. One suprising example of the light-carrying phenomenon was witnessed in 1989 by Norwegian-born physical chemist Thomas Ebbesen, now at the Louis Pasteur University in Strasbourg, France. As he held to the light a thin film of metal containing millions of nanometre-sized holes, he found that it was more transparent than he expected. The holes were much smaller than the wavelength of visible light, which should have made it almost impossible for the light to get through at all. \"I first thought, 'Here was some kind of mistake',\" says Ebbesen. But it wasn't a mistake, although it took Ebbesen and his colleagues the better part of a decade to work out what was happening. When the incoming photons struck the metal film, they excited surface plasmons, which picked up the photons' electromagnetic energy and carried it through the holes, re-radiating it on the other side and giving the film its transparency 1 . Hole arrays are increasingly finding their way into applications, for example as selective filters for colour sensors. It turns out that the increased transmission through the sheet works only for light around the plasmons' natural oscillation frequency. But this frequency, which is typically in the visible or near-infrared part of the spectrum, can be adjusted by changing the geometry of the holes and their spacing. So hole arrays can be made into highly selective filters for sensors that depend on detecting specific colours, or for efficiently extracting monochromatic light from light-emitting diodes (LEDs) and lasers. Indeed, a number of commercial research labs, such as the Panasonic laboratory in Kyoto, Japan, and NEC in Tsukuba, Japan are working on prototypes of plasmon-enhanced devices for displays and telecommunications. Hole arrays can also be used to channel light into optical devices. In imaging chips for digital cameras, for example, researchers are studying how hole arrays placed on top of individual pixels might help capture incoming light more efficiently, and thus reduce pixel noise and improve camera sensitivity. Another plasmonic technique for channelling light into a device is to sprinkle its surface with nanoscale particles made of a metal such as gold. These nanoparticles function like an array of tiny antennas: incoming light is taken up by plasmons and then redirected into the device's interior.  \n                Slimming down \n              From a commercial perspective, perhaps the most promising application of such nanoantennas \u2014 or indeed, of hole arrays \u2014 is in the improvement of solar cells. Present-day solar cells are made from semiconductors such as silicon. But to catch as much light as possible from the broadest range of wavelengths, particularly in the red and infrared part of the spectrum, the semiconductor layer has to be relatively thick. \"Right now a silicon solar cell is up to 300 micrometres thick,\" says Albert Polman, a photonics researcher who directs the AMOLF institute in Amsterdam, where he works on improving solar-cell designs. And when cells are being deployed in arrays that cover a rooftop or more, he says, that adds up to a lot of expensive silicon. The price would come down a long way if the silicon was only 1 micrometre thick. \"But then you don't catch the red light because it goes straight through the chip,\" he says, thus wasting much of the sunlight's available energy. Other solar-cell materials have the same problem. With plasmonics, however, the problem goes away. In one approach that researchers are exploring, gold nanoparticles on the surface would act as reflectors that focus light into the semiconductor, where absorption efficiency increases with the light concentration. In another scheme, tiny gold nanoantennas could redirect sunlight by 90\u00b0, so that it propagates along the semiconductor rather than passing straight through. Either way, the cell could get by with a much thinner semiconductor layer. Even as plasmonic techniques are decreasing the cost of the cells, they could also greatly improve the cells' efficiency at extracting the available energy from sunlight \u2014 in a field in which even a few percentage points in efficiency improvement are celebrated. Overall, the use of plasmonics could increase the absorption two to five times, says Atwater, who has co-founded Alta Devices in Santa Clara, California, to commercialize such solar cells. For cells made from amorphous silicon, which today have efficiencies of around 10\u201312%, the predicted enhancements could translate into efficiencies of about 17%. For crystalline silicon cells, which currently have efficiencies around 20%, the new figure could approach the theoretical maximum of 29%. For commercial applications, the remaining challenges include developing workable device designs and fabrication techniques for mass production.  \n                Guiding light \n              Plasmonics researchers are also grappling with a longer-term challenge: the integration of optics and electronics on a single microchip. The decades-old idea is that, just as a fibre-optic cable can carry much more information than a copper wire, a light beam could, in principle, relay information through the chip on more channels and at a higher speed than conventional integrated circuitry can handle. But the experimental optical devices produced to date have been too large, and have showed rather high losses in the optical signal strength. \"You want to bring the optics closer in size to the transistor,\" says Polman. And that's the beauty of plasmonics, which can offer optical pathways on virtually the same scale as the silicon structures found in advanced microchips. \"Metals can be well integrated with the chip design,\" says Polman, \"so you may be able to distribute light over an integrated circuit by plasmons.\" Indeed, structures such as silver nanowires 2  or grooves etched into metal surfaces 3  can provide pathways that guide light across a chip in whatever direction the designers might need. But there is a trade-off as the structures get smaller. If the plasmons are forced to travel through a channel that's too narrow, they start to leak out from the sides and get lost, says Sergey Bozhevolnyi from the University of Southern Denmark in Odense, who is leading a European research project into integrated plasmonic circuits. Nevertheless, researchers can guide surface plasmons over distances of more than 100 \u03bcm, which is roughly a thousand times bigger than the features on a current-generation microchip. This is enough to open rich possibilities for plasmonic nanocircuits, in which light would carry information along complex paths and through many processing steps. Plasmonic waveguides are particularly promising if the light source \u2014 typically a laser \u2014 can be incorporated on the chip as well. This has been done with comparatively large lasers, on the order of the wavelength of the laser light. But plasmonics now offers the possibility of doing so at the nanoscale, at lengths much shorter than the wavelength. Rather than amplifying light in a conventional laser cavity, a plasmonic 'spaser' would amplify it with the help of plasmons \u2014 the first experimental evidence for such plasmon-based lasing was published in August 4 ,   5 . To fully integrate these plasmon lasers into standard microcircuitry, however, researchers will need to find a way to trigger the spasers using standard electrical currents. In addition to creating light and guiding it across a chip, optical computing will require a way to turn the flow of plasmons on and off at high speeds, so that the flow becomes a series of bits in a digital data stream. Many people have been working on such devices, and a plasmonic modulator based on silicon technology has been realized by Atwater's group. Like a conventional transistor, in which an electric voltage controls a tiny electrical current, the group's device is based on the use of an electric field to control the propagation of surface plasmons through the device 6 . Apart from their small size, compared with conventional optical counterparts, the operation frequency of plasmonic modulators can easily reach tens of terahertz, well above the gigahertz regime of modern computers. Many roadblocks still remain to the commercialization of such technologies \u2014 ranging from the integration with silicon to device issues. \"The key thing that keeps coming back are losses in the metals,\" says Mark Brongersma, a materials scientist at Stanford University in California. However, he adds, smart design of the plasmonic structures could, in principle, reduce losses to acceptable levels. Plasmonics research has made remarkable progress in the past decade, and researchers are working on pushing our knowledge of plasmons even further, for example to understand the physics very close to the metal surface. Nonetheless, says Atwater, \"what has happened in the past seven or eight years is that plasmonics has given to photonics the ability to go to the nanoscale and properly take its place among the nanosciences.\" Joerg Heber is a senior editor at  Nature Materials . \n                     Nature Insight on Photonic Technologies \n                   \n                     Nature Materials \n                   \n                     Nature Nanotechnology \n                   \n                     Nature Photonics \n                   \n                     Nature Physics \n                   \n                     Harry Atwater's group \n                   \n                     Mark Brongersma's lab \n                   \n                     Thomas Ebbesen's lab \n                   \n                     Naomi Halas's group \n                   \n                     Stefan Maier's group \n                   \n                     Albert Polman's lab \n                   \n                     Richard Van Duyne's group \n                   Reprints and Permissions"},
{"file_id": "461716a", "url": "https://www.nature.com/articles/461716a", "year": 2009, "authors": [{"name": "Natasha Gilbert"}], "parsed_as_year": "2006_or_before", "body": "Phosphate-based fertilizers have helped spur agricultural gains in the past century, but the world may soon run out of them. Natasha Gilbert investigates the potential phosphate crisis. Ten years ago, Don Mavinic was working on a way to get rid of a pesky precipitate that plugs up the works of waste-water treatment plants. Known as struvite, the solid crud forms in pipes and pumps when bacteria are used to clean up sewerage sludge. Mavinic, a civil engineer at the University of British Columbia in Vancouver, Canada, realized that struvite was more than just rubbish. A combination of phosphate, magnesium and ammonium, struvite contains many of the essential nutrients that plants need. Mavinic has developed a way to remove the precipitate during the water-treatment process and he is now selling it as a 'green' fertilizer. His technology was first used commercially in 2007 in a treatment plant in Edmonton, Alberta, Canada. It has since been exported to a plant in Portland, Oregon, which began using it this year. A sewage works in Derby, UK, successfully tested the technology in September. Aside from finding a use for a troublesome by-product, the recycling of struvite could also help solve a much bigger problem: the dwindling supply of phosphate rock. All life forms require phosphorus in the form of phosphate, which has an essential role in RNA and DNA and in cellular metabolism. Every year, China, the United States, Morocco and other countries mine millions of tonnes of phosphate from the ground, the bulk of which is turned into fertilizer for food crops. But such deposits are a finite resource and could disappear within the century. Experts disagree on how much phosphate is left and how quickly it will be exhausted. But many argue that a shortage is coming and that it will leave the world's future food supply hanging in the balance. \"I am starting to think phosphate rock is becoming a strategic material for many countries. In the future it's going to become more and more valuable,\" says Steven Van Kauwenbergh of the IFDC, an International Center for Soil Fertility & Agricultural Development based in Muscle Shoals, Alabama. Indeed, as political and social tensions build over the reserves of phosphate rock, the world could move from an oil-based to a phosphate-based economy, say some scientists and industry representatives. \"It is a very curious thing that something so important is so poorly understood and so little talked about in the larger political arena,\" says Arno Rosemarin, a water-resources specialist at the Stockholm Environment Institute who has researched global phosphate use. Although international leaders have not tended to focus on the potential for phosphate shortages, the issue has been proposed for discussion next month at a United Nations meeting on global food security \u2014 an indication that it is starting to attract the attention of the international community.  \n                Just decades left? \n              In many countries, phosphorus is a limiting plant nutrient in short supply in the soil. So farmers add phosphate-based fertilizers to increase agricultural yields. That has spawned a global phosphate-mining industry with sales totalling in the tens of billions of dollars. The US Geological Survey (USGS) in Reston, Virginia, estimates that around 62 billion tonnes of phosphate remain in the ground (see graphic). This includes 15 billion tonnes of deposits that are mineable at present and others that are not being exploited. The latter are left in the ground mainly because they contain too many impurities \u2014 such as cadmium and other toxic metals \u2014 or because they are offshore in difficult-to-reach places. In 2008, 161 million tonnes of phosphate was mined around the world, according to the latest, as yet unpublished, figures from the US Geological Survey. Stephen Jasinski, phosphate-rock commodities expert at the survey, says that demand for fertilizers is predicted to grow by 2.5-3% per year for the next 5 years. If that rate continues, the world's reserves should last for around 125 years. That is a relatively optimistic timescale, but it is echoed by the International Fertilizer Industry Association in Paris, whose members include 90% of the world's fertilizer producers. Michel Prud'homme, executive secretary of the association's Production and International Trade Committee, says that the industry anticipates that demand for fertilizers will grow at a \"fairly moderate rate\", slowing by the middle of the century. That would enable reserves to last for at least another 100 years. But others predict a faster growth in demand for fertilizers, which would deplete phosphate reserves even quicker. The increased use will be driven in part by the rising global population, which will require food production to at least double by 2050, according to the Food and Agricultural Organization of the United Nations (FAO). Rosemarin and others say that nations should not rely on the reserves laden with impurities or located offshore because of the costs \u2014 both environmental and economic \u2014 of extracting usable phosphate. The remaining accessible reserves of clean phosphate rock would run out in 50 years, if growth stays at 3% per year, says Rosemarin. But the estimates all suffer from a lack of reliable data. Most of the world's phosphate-mining companies are integrated with fertilizer firms and the mines are either owned by the companies or are under state control, says Prud'homme. As a result, it is difficult to get accurate, independent information on phosphate reserves. Eric Kueneman, deputy director of the FAO's plant production and protection division says, \"the reality is we as a public institution don't really know what the industry knows and nor do they know among themselves. To give a reliable answer to the question, 'will phosphates run out?', we need a crystal ball.\" The International Fertilizer Industry Association collects data from its members on their existing reserves and on potential upcoming capacity. But some experts question the accuracy of these data because they are supplied by producers who might be disinclined to provide proprietary information that could harm their commercial positions.  \n                No agreement \n              There is also a lot of uncertainty over the data supplied by governments, which is the case with China and Morocco, says Dana Cordell, who has just completed her doctoral thesis on the effect of phosphate reserves on food security at the University of Technology Sydney in Australia. For example, when China joined the World Trade Organization in 2001, its reported reserves of phosphate rock instantly jumped from just over 2 billion tonnes to nearly 8 billion tonnes 1 . Cordell and Kueneman call for independent data collection on phosphate rock reserves. \"Unlike for energy, water or nitrogen, there is no single international organization responsible for phosphate resources. That is very concerning,\" Cordell says. The IFDC hopes to generate more solid data about the extent of the world's phosphate resources and reserves. It will soon launch a project that will query phosphate producers, academics and other minerals specialists to collect extensive data on how much phosphate there is, how pure it is, what might be available in the future and the useful life of existing mines. Van Kauwenbergh, who is leading the project, expects to publish the first round of data in May next year. If the centre secures more funding, he hopes to continue the research for another 5 years. The USGS figures on phosphate reserves are the most-quoted publicly available information. But there are problems with them because the agency gets its information from foreign governments, not directly from producers, and it is not independently verified. \"We just don't know how good the USGS data are because they are based on second and third-hand information. The figures change all the time,\" says Van Kauwenbergh. Some people who track the phosphate industry say that there is no cause for concern about phosphate running out. \"I don't think this is an immediate crisis, but it is something we should be paying attention to,\" says Jasinski. Prud'homme is sanguine about prospects for the future. If demand rises, then so will prices, he says, allowing companies to explore for new reserves and mine those that are harder to reach or from a lower grade of rock. \"We feel there are enough reserves to meet food and material needs,\" he says. For example, companies have recently begun to investigate deposits in Peru, Australia and off the coast of Namibia that were not previously considered financially viable, says Prud'homme. These resources are not fully taken into account in the most recent USGS figures on world phosphate reserves, he says. And as some existing mines are tapped out, others are opening up in places such as Saudi Arabia. \"I am convinced there are other sources we have not yet found, but it is difficult to say how much impact these will have,\" he says. Others are sceptical that further exploration will uncover large new deposits or that they will solve the longer-term problem. \"We are not going to find another Morocco,\" says Jasinski, referring to the country with the biggest remaining reserves. In the meantime, companies have started to invest in new technologies to exploit the lower-grade and offshore deposits. The impetus for this move into more costly production was the hike in phosphate rock prices in 2008, when the value temporarily spiked at US$500 per tonne, more than five times the average price in 2007 2 . Prices had remained comparatively flat for the previous five years. The price hike was due to tight supplies of the rock caused by increased demand for phosphate-based fertilizers in India and China as well as record energy prices. Phosphate prices have since dropped back to their pre-spike levels.  \n                Few alternatives \n              Despite the investments in unconventional reserves, those deposits may not be viable in the long term. Jan-Olof Drangert, an expert in water and land resources at Link\u00f6ping University in Sweden, says that lower-grade reserves are \"not a solution\" if the world wants a sustainable system. Not only will extracting lower-grade phosphates be very expensive, it will also pollute the soils with cadmium, which is highly toxic to plant and animal life even in low doses, he says. \"And then there is still the problem of exhausting these lower-grade reserves,\" he adds. The increase in demand for fertilizer in 2008 may have been a taste of things to come, especially if demand for food rises as fast as some estimates suggest. The price hike last year \"was a huge shock to farmers\", says Cordell. Fertilizers had to be rationed in some cases. \"The bottom line is that it will just cost more to eat,\" says Rosemarin. \"There will be no cheap lunches any more.\" The uncertainty over the world's phosphate reserves is compounded by the fact that supply is concentrated is just a few hands. China, Morocco, the United States and Russia together hold more than 70% of the global phosphate deposits 3 , presenting the possibility of \"market manipulation\", says Amit Roy, president of the IFDC. Evidence of strategic manoeuvring can already be seen. In March 2004, the United States and Morocco signed a free-trade agreement that covered phosphate rock, among other commodities. In 2008, Morocco exported $65-million worth of fertilizer to the United States 4 . Although the United States has one of the world's largest phosphate rock reserves, the nation will see a significant drop in production in 25 years when it is estimated that production will peak at its key mines in Florida. The deal with Morocco, says Rosemarin, is aimed at securing the United State's future fertilizer and food supply. In the case of some finite resources, such as oil, alternatives can be found. But there are currently no substitutes for phosphates. Cutting usage will help to make reserves last longer (see  'Making fertilizers go further' ). But most agree that some of the biggest gains will probably be made from the recovery and recycling of phosphates, such as Mavinic's work mining the phosphate deposits inside water-treatment plants. In a back-of-the-envelope calculation, he estimates that if all domestic wastewater facilities in Canada were converted into biological treatment systems using his technology, the country could produce enough fertilizer to meet about 30% of its current needs. That pales, however, when compared with a much richer \u2014 and more pungent \u2014 source of phosphate: the manure generated by dairy and pig farming. Livestock waste contains around five times more phosphate than human waste. And the global livestock population is around 65 billion, more than ten times the human population. There is \"enormous potential\" for recovering phosphates from livestock waste, says Mavinic, who has turned his attention to doing just that. The problem his research team is trying to solve is that phosphates in livestock waste are not in a dissolved form, which is necessary to make struvite. If programmes to recover phosphates from livestock waste succeed, \"the sky is the limit\", says Mavinic. \"We would probably not have to import any fertilizer into this country.\" But all this takes time. Decades may pass before recycling technologies gear up and new supplies of phosphate come on line. At present, nations have expressed little concern over the finite phosphate resource and are eagerly consuming reserves. When solutions do eventually emerge, the world could already be in the grip of a fertilizer and food shortage. Natasha Gilbert is a reporter based in  Nature 's London office. \n                     Nature Geoscience \n                   \n                     USGS Minerals report for phosphate with preliminary numbers for 2008 (pdf) \n                   \n                     International Center for Soil Fertility & Agricultural Development \n                   \n                     International Fertilizer Industry Association \n                   Reprints and Permissions"},
{"file_id": "461862a", "url": "https://www.nature.com/articles/461862a", "year": 2009, "authors": [{"name": "Jonah Lehrer"}], "parsed_as_year": "2006_or_before", "body": "Researchers have engineered more than 30 strains of 'smart mice', revealing possible ways to boost human brains. But, as Jonah Lehrer finds, cognitive enhancement may come at a cost. Ten years ago, Joe Tsien eased a brown mouse, tail first, into a pool of opaque water. The animal squirmed at first; mice don't generally like getting wet. But once released, it paddled in a wide circle, orienting itself by the array of coloured shapes hung above the pool. Within seconds, the mouse headed straight for the safety of a small platform hidden just beneath the water's surface. Most mice require at least six sessions before they can remember the location of the platform in a Morris water maze. But this animal needed just three. Tsien, based at Princeton University in New Jersey at the time, named his creation Doogie after the teenage genius in the television programme  Doogie Howser, MD . The work was one of the earliest examples of neuroscientists using genetic engineering to generate cognitively enhanced animals in a bid to understand memory and learning. \"There's something magical about taking a mind and making it work better,\" says Alcino Silva, a professor of neuroscience at the University of California, Los Angeles, and one of the pioneers in the field of enhanced cognition. Researchers have now created or identified at least 33 mutant mouse strains that,  like Doogie , have enhanced cognitive abilities. The animals tend to learn faster, remember events longer and solve complex mazes better than ordinary mice. And because the molecular pathways used in the brain to form long-term memories are almost identical in humans and rodents, the hope is that the work will inform research into treatments for a wide variety of learning and memory problems, from dyslexia to dementia. Much of the work involves making an adult brain behave more like a younger, more flexible version of itself by increasing the organ's plasticity. This, in turn, means that some problems, long believed to have been made permanent during development, might actually be reversed. Moreover, the mice raise a tantalizing possibility that normally functioning human brains could be improved. Already, drugs designed to help with attention deficit and sleep disorders are infiltrating college campuses and workplaces around the world, where they are being used without prescription to enhance cognition. Within the next decade, it might be possible to take a pill that will not only help alleviate the symptoms of learning disorders but also act as an intellectual steroid, pumping up the brain's potential. What the mice have clearly shown, in ways that pill-popping humans have not, is that enhancement could have unexpected trade-offs.  \n                Improving on evolution \n              It was while Silva was studying mouse models of neurofibromatosis, a genetic disorder characterized by learning disabilities and benign tumours in nerve tissue, that he inadvertently created his first smart mouse. The disorder is caused by a mutation in a single gene, and Silva thought that models of the disease might allow him to investigate the molecular mechanisms underlying learning and memory. In one model, Silva and his colleagues found that Ras, a family of growth-promoting signalling proteins, had enhanced activity in a subset of neurons that inhibit the firing rate of connected neurons. Steven Kushner, a postdoc in Silva's lab, engineered a mouse that had a constantly active form of one of the Ras proteins, Hras, but only in excitatory neurons, which increase the firing rate of connected neurons. The team was surprised to find that these animals learned and remembered things much faster than normal mice in certain memory tests 1 . After just a single trial, the engineered mice learned to link a minor electrical shock with specific surroundings, causing the animals to freeze in fear when placed back in the cage where it first received the shock. Normal animals don't learn this association with such a mild shock. The team was able to identify how this enhanced learning came about at the molecular level. Long-term memory is believed to be based on the strength of the link between two nerve cells. What Silva's team saw was an increase in the amount of the neurotransmitter glutamine being released at the synapse \u2014 the junction between two neurons \u2014 in the Hras mutants, which strengthened the connection at that junction through a process called long-term potentiation (LTP). \"The thrilling part is being able to connect these seemingly slight differences at the molecular level to dramatic differences in observed behaviour,\" Silva says. \"That's a sign that we're really starting to understand the core processes of learning and memory in the brain.\" Unlike Silva, Tsien had set out to create a smart mouse when he developed Doogie. He focused on brain-cell receptors for the chemical NMDA ( N -methyl-D-aspartate). First linked to long-term memory in the late 1980s, the NMDA receptor is often referred to as the brain's 'coincidence detector' as it is activated only when two connected cells fire simultaneously. The receptor enhances LTP, with the end result that the brain can detect the connections between seemingly separate events, such as seeing fire and feeling pain. Tsien created Doogie by overexpressing a subunit of the NMDA receptor called NR2B. This kept the receptors open for longer, strengthening the synaptic link and making it easier for disparate events to be linked together. \"They all thought I was crazy,\" recalls Tsien. \"They said the brain has been optimized by evolution. You won't be able to improve it.\" When Tsien published his results 2  in 1999, the media reacted with excitement and hyperbole.  Time   magazine put the research on its cover, asking whether researchers had finally found the \"IQ gene\". Doogie and the enhanced mutants that have followed in its wake share more than just the accolade of being smart. \"What's most striking about these different animals is the convergence,\" Silva says. Nearly all of the mouse strains show enhanced LTP. \"There are so many different ways to tinker with learning and memory, and yet almost all of these improvements work through the same mechanism,\" he says. According to Silva and others, this is evidence that LTP is a fundamental feature of learning and memory, and that by increasing plasticity it is possible to increase cognitive capacity.  \n                The damage undone \n              Being able to genetically engineer an animal with enhanced brain power is exciting, but can a brain that has already developed abnormally be fixed? In some instances, the answer may be yes. One promising example involves work on a protein called CREB, which is implicated in memory formation. In 1995, Tim Tully, a neuroscientist then at Cold Spring Harbor Laboratory in New York, managed to improve memory and learning in a mutant fruitfly 3  by overexpressing a form of CREB. He and others built on this work in mice to try to tackle a rare genetic condition called Rubinstein\u2013Taybi syndrome. Characterized in humans by severe learning difficulties, as well as short stature and an increased risk of developing tumours, Rubinstein\u2013Taybi syndrome is caused by mutations in the gene for the CREB-binding protein. Neuroscientists had assumed that the cognitive defects caused by the syndrome were irreversible \u2014 especially as the condition can be diagnosed before birth. But in 2003, Tully and several others showed that administering drugs that increase CREB activity in mouse models of the disease dramatically improves the animals' ability to learn 4 ,   5 ,   6 . \"We get complete recovery in adult mice,\" says Mark Mayford, a neuroscientist at the Scripps Research Institute in La Jolla, California, who was an author on one of the studies. \"I was pretty amazed.\" The success stretches beyond Rubinstein\u2013Taybi syndrome \u2014 the cognitive defects in other developmental diseases such as neurofibromatosis, Down's syndrome and fragile X, a genetic disorder that causes a wide range of behavioural and intellectual deficits, have all proved to be reversible in mice 7 . Although it remains unclear if the same approach can be applied to humans, Tully and others are bullish. \"This work is a shot across the bow of the future,\" says Tully, now chief science officer at Dart NeuroScience in San Diego, California, which has been investigating compounds that manipulate the CREB pathway. \"It shows us just how important increasing plasticity can be, and how we can put plasticity to work.\" James Bibb at the University of Texas Southwestern Medical Center in Dallas says that drugs developed as a result of this work could be used to treat conditions such as post-traumatic stress disorder and drug addiction, which require people to unlearn negative associations. \"The purpose of the brain is to help us learn useful information,\" Bibb says. \"By increasing plasticity, you can push that process along.\" Silva imagines a near future in which people with learning and memory disorders will be slotted into a number of categories, based on the molecular specifics of their disorder. \"We could then target the therapy,\" he says. \"We could use what we've learned from these enhanced mice to selectively fix what isn't working.\" The concern for some is that otherwise healthy humans would want to take such drugs in a bid to make themselves smarter or stave off age-related cognitive decline. \"I think these drugs are going to lead to some real slippery-slope issues,\" says Martha Farah, who heads the Center for Neuroscience and Society at the University of Pennsylvania in Philadelphia. \"There is no clear or objective line between a normal brain and one that needs treatment. For instance, we can say that we're only going to use these memory drugs for people with demonstrated memory decline. But your memory starts to diminish in your thirties. Does that mean every 40-year-old is going to be taking these pills?\" Farah notes that this is already starting to happen with drugs used to treat attention deficit or sleep disorders, as these act as mental stimulants. For instance, one in five respondents to a web poll run by  Nature   in 2008 admitted to using some of these drugs, such as Ritalin (methylphenidate) or Provigil (modafinil), to enhance their focus and productivity 8 . \"You get more and more people taking them for less and less severe conditions,\" she says.  \n                Risky business \n              Little is known about the side effects and trade-offs of both the current usage or the drugs in development, but initial clues offered by smart mice raise concerns. The Hras strain developed in Silva's lab might be good at learning, but its fear response for a relatively benign stimulus would be counterproductive for a wild mouse. Its enhanced memory is both a blessing and a burden. Silva cites other strains of smart mice that excel at solving complex exercises, such as the Morris water maze, but that struggle with simpler mazes. \"It's as if they remember too much,\" he says \u2014 possibly taking in irrelevant information such as the position of windows or lights but missing the big clues. Farah sees a parallel between these mice and one of the few case studies of an individual with profoundly enhanced memory. In the early 1920s, the Russian neurologist Alexander Luria began studying the learning skills of a newspaper reporter called Solomon Shereshevsky, who had been referred to the doctor by his editor. Shereshevsky had such a perfect memory that he often struggled to forget irrelevant details. He was able to recite in Italian several stanzas of Dante's  Divine Comedy   after one learning session, even though he was unfamiliar with the language. Although this flawless memory occasionally helped Shereshevsky at work \u2014 he never needed to take notes \u2014 Luria also documented the profound disadvantages of such a capacious memory. Shereshevsky, for instance, was almost entirely unable to grasp metaphors, as his mind was so fixated on particulars. When he tried to read poetry, for example, \"the obstacles to his understanding were overwhelming\", Luria wrote in his book  The Mind of a Mnemonist . \"Each expression gave rise to a remembered image; this, in turn, would conflict with another image that had been evoked.\" For Luria, Shereshevsky's struggles were a powerful reminder that the ability to forget is as important as the ability to remember. Enhancing human memory in individuals without severe cognitive defects might prove counterproductive. Many scientists are concerned that the animal models of enhanced cognition might obscure subtle side effects, which can't be studied in rodents or primates. Farah is currently looking at the trade-off between enhanced attention \u2014 she gives human subjects a mild amphetamine \u2014 and performance on creative tasks. Other researchers have used computer models to show that memory is actually optimized by slight imperfections, as they allow one to see connections between different but related events 9 . \"The brain seems to have made a compromise in that having a more accurate memory interferes with the ability to generalize,\" Farah says. \"You need a little noise in order to be able to think abstractly, to get beyond the concrete and literal.\" And then there's the problem of non-cognitive side effects. Because many of these learning and memory enhancements involve molecules that regulate a wide variety of fundamental cellular pathways, such as CREB, it might be impossible to restrict their action to the brain. The Doogie mice, for example, seem to have an increased sensitivity to pain. And the  Hras   gene mutated in Silva's mice is commonly mutated in cancer. \"There's no such thing as an enhancement without side effects,\" says Nobel laureate Eric Kandel, a neuroscientist at Columbia University in New York and co-founder of Memory Pharmaceuticals, a biotechnology company in Montvale, New Jersey, that is trying to turn his research on LTP into novel drug therapies for memory disorders. \"It often takes years to fully understand all the side effects. The mice will help us work out some of the bugs, but these will still be very risky treatments.\" Although Silva recognizes the risks of enhancement, he remains hopeful that the performance of the normal human brain can be improved by neuroscience. \"We're getting to a point where we almost need these enhancements,\" he says. \"We don't have enough attention, we don't have enough memory, we don't have enough awake hours. There's clearly a demand to optimize the human brain given what it needs to do in the information age.\" Like Kandel, Tully has spent much of the past decade trying to translate the biochemistry of memory into useful medical therapies. He remains enthusiastic, although he is also aware that the road ahead is littered with false leads, mistaken hypotheses and treatments that work in mice but fail in clinical trials. It's been ten years since Tsien, now at the Medical College of Georgia in Augusta, created Doogie, and although that's a short time in research years, Tully, for one, is getting impatient. \"When I began working on these learning and memory drugs, I had no grey hair and I thought I'd find a drug that might be able to help my parents,\" Tully says. \"Now my hair is mostly white and my parents are dead. I'm just hoping that we can find a drug in time for me.\" Jonah Lehrer is a freelance writer based in Los Angeles, California. \n                     Would you boost your brain power? \n                   \n                     Center for Neuroscience and Society at the University of Pennsylvania \n                   \n                     Alcino Silva's Lab \n                   Reprints and Permissions"},
{"file_id": "461712a", "url": "https://www.nature.com/articles/461712a", "year": 2009, "authors": [{"name": "Kelly Rae Chi"}], "parsed_as_year": "2006_or_before", "body": "Genome-wide association studies have identified hundreds of genetic clues to disease. Kelly Rae Chi looks at three to see just how on-target the approach seems to be. Five years ago human geneticists rallied around an emerging concept. Technology had granted the ability to compare the genomes of individuals by looking at tens of thousands of known single-letter differences scattered across them. These differences, called single nucleotide polymorphisms or SNPs, served as reference points or signposts of common variation between individuals. The idea was that common variants in the genome might contribute to the genetics of common diseases. Genome-wide association studies (GWAS) could scan SNPs in thousands of people, with and without a disease. When a DNA variant can be associated with the risk of developing a disease, it signals that something in that area of the genome might be partly responsible. With such 'hits' would presumably flow a better mechanistic understanding of disease, genetic-testing abilities and even treatment. \"Many researchers really grabbed on to the common variant hypothesis, and in some cases it worked,\" says Jonathan Haines, director of the Vanderbilt University Medical Center's Center for Human Genetics Research in Nashville, Tennessee. But, he adds, \"it hasn't panned out to be as pervasive an explanation as we thought\". Here are the stories of three hits. One provides a near perfect example of the positive outcome that this sort of unbiased approach can have. One reveals that without biological context the findings can be hard to interpret. And the third demonstrates that GWAS in their current form can't cope well with some common traits.  \n                A direct hit in haemoglobin \n              In 2007, researchers reported on genome-wide scans of healthy adults looking for SNPs associated with very high or very low levels of fetal haemoglobin. Among several hits were variants of a gene on chromosome 2 called  BCL11A   (ref.  1 ). This finding, quickly replicated in multiple populations, generated a lot of excitement. Fetal haemoglobin is a remnant of embryonic development. For most people, the fetal version of this crucial oxygen-carrying protein drops off after birth as the adult version kicks in. Some people retain relatively high expression, which seems to have no effect in healthy adults. But for patients with blood disorders such as sickle-cell disease and \u03b2-thalassaemia, those expressing high fetal-haemoglobin levels can be protected from some of the nastier ravages of the disease, such as leg ulcers, severe pain and even death. GWAS findings often just provide the signpost, a rough coordinate for a causal gene. The SNP signal is often outside gene sequences. The variants in  BCL11A   were a direct hit in a gene, however, and a surprising gene at that. The protein it codes for, which controls the expression of other genes, had been associated with cancer progression, but never with haemoglobin production. A mouse model had even been made in which the gene had been knocked out, but until the GWAS no one had looked at its regulation of blood. \"Nobody would have ever dreamed that a gene like this would have any regulatory role in fetal haemoglobin,\" says Martin Steinberg, a haematologist at the Boston University School of Medicine in Massachusetts. Last year, he and his colleagues replicated the  BCL11A   finding in three different populations with haemoglobin disorders 2 . Of course, there was functional work to be done. Stuart Orkin's lab at Harvard Medical School in Boston reduced expression of  BCL11A   in cultured blood progenitor cells from humans. Fetal haemoglobin expression went up, suggesting that the gene normally acts as a repressor 3 . In a follow-up study 4 , the group showed that the gene controls the silencing of fetal haemoglobin during development in mice. But the exact switch mechanism has not been solved, says Orkin. His group has gone on to look for proteins and other genes that the product of  BCL11A   binds to or influences. Results from these studies will inform research that looks for molecules that would interfere with the gene's expression or function and thus serve as potential therapies to activate synthesis of fetal haemoglobin in people with blood disorders. Steinberg, for his part, hopes to use these and other GWAS findings to refine a computational tool that predicts disease severity or death in people with sickle-cell disease. The hope is to intervene earlier and with more specific treatments.   The verdict:   even those who have been generally pessimistic about the outcomes of GWAS consider the  BCL11A   find a win for science. \"It's a tour de force illustration of the value of GWAS,\" says David Goldstein, a geneticist at the Duke Institute for Genome Sciences & Policy in Durham, North Carolina. \"You learn something new, you understand the mechanism, and it's biologically and clinically important.\" As winners go, however, Goldstein says it is on a short list. In some ways, says Steinberg, the GWAS provided a lucky hit. The researchers built on years of evidence that fetal haemoglobin has a powerful effect on the severity of sickle-cell disease and \u03b2-thalassaemia. It was the clear physiological signal \u2014 quantity of fetal haemoglobin \u2014 that helped researchers to design the GWAS. Other genes and pathways will be found to affect the severity of the disorders, but probably none with the same force as fetal haemoglobin. \"We're not going to find another fetal haemoglobin,\" Steinberg says.  \n                Scoping schizophrenia \n              Schizophrenia genetics has been a mire of false starts. Scores of candidate gene association studies had identified promising targets, but few held up to further scrutiny. So the excitement around approaching the disease in an unbiased genome-wide study was high. But the first four schizophrenia GWAS reported no statistically significant associations. Then, in research published last year, researchers performed scans in roughly 500 people with the disorder and 3,000 healthy controls. When 12 of the hits that turned up were examined in 16,000 more individuals, a signal started to emerge. Three variants were significant, but only one of them was in a gene,  ZNF804A,   which encodes a protein with unknown function 5 . Having a potential candidate gave researchers something concrete to work with. One group took 115 healthy people, a little less than half of whom had two copies of the high-risk  ZNF804A   variant, and compared their brain activity using functional magnetic resonance imaging (fMRI), a method that reveals local blood oxygenation and presumably electrical activity in the brain in real time. Those with the variant, they found, had abnormal connectivity between certain brain areas, impairing \"the degree in which they talk to one another\", says Andreas Meyer-Lindenberg, the director of the Central Institute of Mental Health in Mannheim, Germany, who led the study 6 . Healthy adults with the variant were showing schizophrenia-like brain activity even though they showed no outward signs of disease. Combining genetic and brain-imaging data to study psychiatric disease is not new. Since 2001, researchers have used the strategy to link imaging data to candidate gene findings in schizophrenia, depression and autism. Meyer-Lindenberg's study is the first to use a genetic loci identified through GWAS for follow-up with fMRI. \"We've now applied [the technique] to a variant that has definitive support as being a schizophrenia risk gene. That wasn't available before,\" says Meyer-Lindenberg. Part of the problem when seeking schizophrenia-related genes is that, unlike fetal haemoglobin levels for example, the definition of the trait both within and between studies can differ. Also the spectrum and severity of schizophrenia symptoms varies between individuals and are sometimes subjective from a clinical perspective. That's why fMRI is attractive. The researchers hope to get closer to quantitative measures of psychiatric disorders. \"It makes sense to have a biological level of analysis on which these genetic associations can be studied,\" says Daniel Weinberger, the director of the genes, cognition and psychosis programme at the National Institute of Mental Health in Bethesda, Maryland, who pioneered the method in the 1990s. The  ZNF804A   association from GWAS has been replicated in some studies but not others. And there are few clues to the mechanism by which this gene might contribute to brain connectivity. It was the group of Michael O'Donovan, a professor of psychological medicine at Cardiff University, UK, that made the initial discovery using GWAS. The team is now carrying out a series of experiments to determine which DNA sequences and other proteins it binds, and how variants might alter gene expression.   The verdict:   some have doubts about the combined assault of GWAS and fMRI on psychiatric illness. Imaging data itself isn't the best quantitative trait, says Goldstein, because one three-dimensional fMRI image can contain more than 50,000 picture elements of data, a single trait can be defined in multiple ways. \"There hasn't been a sufficient consistency in how those phenotypes are defined,\" he says. Weinberger and others contend that the imaging paradigms are well established before they are used in imaging genetics research. \"I think it's very important that the phenotypes are well validated \u2014 that they are themselves heritable, and that they're related in some way to the underlying neural circuitry,\" says Weinberger. Reviews from others studying schizophrenia are somewhat lukewarm. K\u00e1ri Stef\u00e1nsson, chief executive of the Icelandic biopharmaceutical company deCODE Genetics in Reykjavik, says he's not completely convinced. \"In schizophrenia, the imaging differences are subtle,\" he says. Nevertheless, he plans to study differences in brain morphology using imaging and GWAS, in people with and without the disorder. Given the shortage of standout GWAS hits, should researchers continue to use the candidate-gene approach to form the basis for hypothesis-driven imaging genetics work? \"It is still a point of debate,\" Meyer-Lindenberg says.  \n                Sight set on height \n              Height has produced clearer hits than schizophrenia, but with a less than satisfying punch. In 2007, by analysing the genomes of nearly 5,000 people, researchers were able to see that a variant in a gene called  HMGA2   explains some of height's variability \u2014 about 0.3% (ref.  7 ). Since then, additional GWAS have revealed more than 40 loci involved in height. Added together, these variants account for 5% of the trait's variation. Even a clear quantitative trait doesn't necessarily provide simple answers. Genes are thought to contribute to roughly 60\u201380% of the variation in stature, leaving much of the heritability of height unaccounted for by GWAS findings. This 'missing heritability' has been a thorn in the side of the common-disease-common-variant hypothesis (see  page 747 ). \"In the field of height,\" says Haines, \"obviously that hypothesis is not completely correct.\" But the news isn't all bad. \"Optimistic people like me say we didn't know anything about the genetics of height before 2007,\" says Guillaume Lettre, a geneticist at the Montreal Heart Institute in Quebec, Canada. \"Now we have more than 40 loci.\" Researchers may have loci, but they have little idea how these contribute to height. As with other traits, many of the associated SNPs fall within the vast regions between genes or within genes whose function is unknown. And with little funding for understanding height variation and scant biological footholds, the field sees very little follow-up of its GWAS leads. Lettre is collaborating with others with the hope of tying mystery SNPs to genes through animal models. \"Basically what we are doing is taking the genes near these markers and looking at the expression of these genes in tissues that are relevant to height,\" he says. \"There are not so many: bone, cartilage and pituitary gland.\" He and others are also trying to coax existing height data into revealing stronger associations by grouping hits based on a single molecular pathway. Hong-Wen Deng, at the University of Missouri in Kansas City, is planning to analyse pathways involved in either bone health or stature. \"Many genes which may have small effects for height may not be detected if you analyse them individually,\" he says. But jointly their effects may be may be detected. Others are looking at height at various points with the hope that differences in growth curves will reveal larger genetic associations. Researchers have already examined height and growth rates in about 3,500 people from Northern Finland. Of 48 height-associated variants that they tested, 12 were linked with the rate of growth during infancy or puberty 8 .   The verdict:   some of the loci implicate molecular pathways already known to be involved in growth and development. A 1995 study had shown that a gene related to  HMGA2   could influence height: mice lacking the gene were shorter, whereas mice with a truncated version developed gigantism 9 . The  HMGA2   association has been further confirmed by most, but not all, GWAS. Predicting how hits outside genes will fare is more difficult, and depends to some extent on how close the hit is to the nearest gene. \"If you look at the height loci, they are much more likely to be near a gene that causes abnormal skeletal growth, than a similarly sized random set of loci,\" says Joel Hirschhorn, a geneticist at the Broad Institute in Cambridge, Massachusetts. But the nearest gene is a poor marker for what is likely to be causal says Goldstein. \"Depending on the genetic model for what is causing the association, it could be nearby or not nearby,\" he says. In some instances changes in DNA act on genes a million bases away. \"It really is remarkable that there are hundreds of reported associations, and the number that you can actually track down to an actual cause of the association is probably countable on one hand.\" Researchers point to height as a 'model trait' because it is simple to measure and relatively constant compared with phenotypes such as blood pressure or glucose level. Then again, in GWAS of height, tens of thousands of people have been necessary to see the slightest associations. As a model trait, that could be problematic. \"In some ways, it is showing us the future for other traits,\" says Karen Mohlke, a geneticist at the University of North Carolina at Chapel Hill who was involved in some of the initial height GWAS work. \"What it means for many other complex traits is that there will be as many loci found, or more.\"   See Editorial  \n                     page 697 \n                   , and the human genetics online special at  \n                     http://go.nature.com/VqPUE2 \n                   . Kelly Rae Chi is a freelance writer in Cary, North Carolina. \n                     Web Focus: Personal Genomes \n                   \n                     Nature Insight: Quantitative Genetics \n                   \n                     A catalogue of published genome-wide association studies by the NHGRI \n                   Reprints and Permissions"},
{"file_id": "460680a", "url": "https://www.nature.com/articles/460680a", "year": 2009, "authors": [{"name": "Mark Buchanan"}], "parsed_as_year": "2006_or_before", "body": "Could agent-based computer models prevent another financial crisis? Mark Buchanan reports. It's 2016, and experts at a US government facility have detected a threat to national security. A screen on the wall maps the world's largest financial players \u2014 banks, governments and hedge funds \u2014 as well as the web of loans, ownership stakes and other legal claims that link them. High-powered computers have been using these enormous volumes of data to run through scenarios that flush out unexpected risks. And this morning they have triggered an alarm. Flashing orange alerts on the screen show that a cluster of US-based hedge funds has unknowingly taken large ownership positions in similar assets. If one of the funds should have to sell assets to raise cash, the computers warn, its action could drive down the assets' value and force others to start selling their own holdings in a self-amplifying downward spiral. Many of the funds could be bankrupt within 30 minutes, creating a threat to the entire financial system. Armed with this information, financial authorities step in to orchestrate a controlled elimination of the dangerous tangle. Alas, this story is likely to remain fiction. No government was able to carry out any such 'war room' analyses as the current financial crisis emerged, nor does the capability exist today. Yet a growing number of scientists insist that something like it is needed if society is to avoid similar crises in future. Financial regulators do not have the tools they need to predict and prevent meltdowns, says physicist-turned-sociologist Dirk Helbing of the Swiss Federal Institute of Technology Zurich, who has spent the past two decades modelling large-scale human systems such as urban traffic or pedestrian flows. They can do a good job of tracking an economy using the statistical measures of standard econometrics, as long as the influences on the economy are independent of each other, and the past remains a reliable guide to the future. But the recent financial collapse was a 'systemic' meltdown, in which intertwined breakdowns in housing, banking and many other sectors conspired to destabilize the system as a whole. And the past has been anything but a reliable guide of late: witness how US analysts were led astray by decades of data suggesting that housing values would never simultaneously fall across the nation. Likewise, economists can get reasonably good insights by assuming that human behaviour leads to stable, self-regulating markets, with the prices of stocks, houses and other things never departing too far from equilibrium. But 'stability' is a word few would use to describe the chaotic markets of the past few years, when complex, nonlinear feedbacks fuelled the boom and bust of the dot-com and housing bubbles, and when banks took extreme risks in pursuit of ever higher profits. In an effort to deal with such messy realities, a few economists \u2014 often working with physicists and others outside the economic mainstream \u2014 have spent the past decade or so exploring 'agent-based' models that make only minimal assumptions about human behaviour or inherent market stability (see  page 685 ). The idea is to build a virtual market in a computer and populate it with artificially intelligent bits of software \u2014 'agents' \u2014 that interact with one another much as people do in a real market. The computer then lets the overall behaviour of the market emerge from the actions of the individual agents, without presupposing the result. Agent-based models have roots dating back to the 1940s and the first 'cellular automata', which were essentially just simulated grids of on\u2013off switches that interacted with their nearest neighbours. But they didn't spark much interest beyond the physical-science community until the 1990s, when advances in computer power began to make realistic social simulations more feasible. Since then they have found increasing use in problems such as traffic flow and the spread of infectious diseases (see  page 687 ). Indeed, points out Helbing, agent-based models are the social-science analogue of the computational simulations now routinely used elsewhere in science to explore complex nonlinear processes such as the global climate. That is why he is eager to bring social and physical scientists together to develop computational 'wind tunnels' that would allow regulators to test policies before putting them into practice. \"The idea is to invest a lot in science,\" he says, \"and thereby save hundreds of times as much by avoiding or mitigating future crises.\"  \n                Just more theory? \n              That notion is a tough sell among mainstream economists, many of whom are less than thrilled by offers of outside help. \"After any crisis,\" says Paul Romer of Stanford University, California, a leading researcher in the economics of innovation, \"you hear recommendations to recruit scientists from other fields who can purge economics and finance of ideology and failed assumptions. But we should ask if there is any evidence that more theory, developed by people who don't have domain experience, is the key to scientific progress in this area.\" Others think some fresh thinking is long overdue. \"We have had a massive failure of the dominant economic model,\" says Eric Weinstein, a physicist working in mathematical finance for the Natron Group, a hedge fund in New York, \"and we're trying to find the right people to deal with this failure. At least some of those people are likely to be unfamiliar voices and come from other parts of science.\" At least some economists agree. The meltdown has shown that regulatory policies have to cope with far-from-equilibrium situations, says economist Blake LeBaron of Brandeis University in Waltham, Massachusetts. \"Even fairly simple agent-based models can be used as thought experiments to see if there is something that hasn't been considered by the policy-makers.\" LeBaron has spent the past decade and a half working with colleagues, including a number of physicists, to develop an agent-based model of the stock market. In this model, several hundred agents attempt to profit by buying and selling stock, basing their decisions on patterns they perceive in past stock movements. Because the agents can learn from and respond to emerging market behaviour, they often shift their strategies, leading other agents to change their behaviour in turn. As a result, prices don't settle down into a stable equilibrium, as standard economic theory predicts. Much as in the real stock market, the prices keep bouncing up and down erratically, driven by an ever-shifting ecology of strategies and behaviours. Nor is the resemblance just qualitative, says LeBaron. Detailed analyses of the agent-based model show that it reproduces the statistical features of real markets, especially their susceptibility to sudden, large price movements. \"Traditional models do not go very far in explaining these features,\" LeBaron says. Another often-cited agent-based model got its start in the late 1990s, as the NASDAQ stock exchange in New York was planning to stop listing its stock prices as fractions such as 12\u00bc and instead list them as decimals. The goal was to improve the accuracy of stock prices, but the change would also allow prices to move by smaller increments, which could affect the strategies followed by brokers with unknown consequences for the market as a whole. So before making this risky change, NASDAQ chief Mike Brown hired BiosGroup, a company based in Santa Fe, New Mexico, to develop an agent-based model of the market to test the idea. \"Over ten years on the NASDAQ Board,\" says Brown, \"I grew increasingly disappointed in our approach to studying the consequences of proposed market regulations, and wanted to try something different.\" Once the model could reproduce price fluctuations in a mathematically accurate way, NASDAQ used it as a market wind tunnel. The tests revealed that if the stock exchange reduced its price increment too much, traders would be able to exploit strategies that would make them quick profits at the expense of overall market efficiency. Thus, when the exchange went ahead with the changeover in 2001, it was able to take steps to counter this vulnerability. Agent-based models are also being used elsewhere in the private sector. For example, the consumer-products giant Proctor & Gamble of Cincinnati, Ohio, has used agent-based models to optimize the flow of goods through its network of suppliers, warehouses and stores. And Southwest Airlines of Dallas, Texas, has used agent-based models for routing cargo. Despite such successes, however, financial regulators such as the US Securities and Exchange Commission (SEC) still don't use agent-based models as practical tools. \"When the SEC changes trading rules, it typically has either flimsy or modest support from econometric evidence for the action, or else no empirical evidence and the change is driven by ideology,\" claims computational social scientist Rob Axtell of George Mason University in Fairfax, Virginia. \"You have to wonder why Mike Brown is doing this, while the SEC isn't.\"  \n                Risk of the new \n              A big part of the answer is that agent-based models remain at the fringe of mainstream economics, and most economists continue to prefer conventional mathematical models. Many of them argue that agent-based models haven't had the same level of testing. Another problem is that an agent-based model of a market with many diverse players and a rich structure may contain many variable parameters. So even if its output matches reality, it's not always clear if this is because of careful tuning of those parameters, or because the model succeeds in capturing realistic system dynamics. That leads many economists and social scientists to wonder whether any such model can be trusted. But agent-based enthusiasts counter that conventional economic models also contain many tunable parameters and are therefore subject to the same criticism. Familiarity wins out, notes Chester Spatt, former chief economist at the SEC. Regulators feel duty-bound to adhere to generally accepted and well-vetted techniques, he says. \"It would be problematic for the rule-making process to use methods whose foundation or applicability were not established.\" Still, agent-based techniques are beginning to enter the regulatory process. For example, decision-makers in Illinois and several other US states use computational models of complex electricity markets. They want to avoid a repeat of the disaster in California in 2000, when Enron and other companies, following market deregulation, were able to manipulate energy supplies and prices for enormous profit. Rich computational models have made it possible to test later market designs before putting them in place. \"We've had a lot of success in developing these models,\" says economist Leigh Tesfatsion of Iowa State University in Ames, who has led the development of an open-source agent-based model known as the AMES Wholesale Power Market Test Bed. \"It has worked because we've focused on all the details of the real situation and can address questions that policy-makers really care about,\" she says. Other models have successfully simulated financial markets. At Yale University, for example, economist John Geanakoplos, working with physicists Doyne Farmer of the Santa Fe Institute and Stefan Thurner of the Medical University of Vienna, has constructed an agent-based model exploring the systemic consequences of massive borrowing by hedge funds to finance their investments. In their simulations, the funds frequently get locked into a self-amplifying spiral of losses (see  page 685 ) \u2014 much as real-world hedge funds did after August 2007. At the University of Genoa in Italy, meanwhile, Silvano Cincotti and his colleagues are creating an agent-based model of the entire European Union economy. Their model includes markets for consumer goods and financial assets, firms that interact with banks to obtain loans, and banks that compete with one another by offering different interest rates. Based on real economic data, the model currently represents some 10 million households, 100,000 firms and about 100 banks, all of which can learn and change their strategies if they find more profitable ways of doing business. \"We hope that these simulations will have an outstanding impact on the economic-policy capabilities of the European Union,\" says Cincotti, \"and help design the best policies on an empirical basis.\" This is the kind of ambition that has inspired Helbing. He doesn't pretend to be an economic modeller himself: since the early 1990s his own work has focused on simulations of human behaviour in relatively small groups \u2014 how traffic ebbs and flows through a road network, for example, or how crowds crush towards a door in a panic situation \u2014 as well as on experiments to test his predictions with real data. But that work has given Helbing a keen appreciation for the way complex collective phenomena can emerge from even the simplest individual interactions. If pedestrians can organize themselves into smoothly flowing streams just by trying to walk through a crowded shopping centre \u2014 as he has shown they do \u2014 just imagine how much richer the emergent phenomena must be in a group the size of a national economy.  \n                Crisis logic \n              That observation acquired fresh force for Helbing after last year's global financial meltdown made it clear that a regulatory system based on conventional economic theory had failed. \"It's remarkable,\" he says, \"that while any new technical device or medical drug has extensive testing for efficiency, reliability and safety before it ever hits the market, we still implement new economic measures without any prior testing.\" To get around this impasse, he says, researchers need to reimagine the social and economic sciences on a larger scale. \"I imagine experts from different fields meeting in one place for extended periods of time,\" he says, \"so that their complementary knowledge could 'collide', creating new ideas, much as particle colliders create new kinds of particles.\" Ultimately, such an effort would bring together social scientists, economists, physicists, ecologists, computer scientists and engineers in a network of large centres for socioeconomic data mining and crisis forecasting, as well as in supercomputer centres for social simulation and wind-tunnel-like testing of policy. That is a large ambition, Helbing admits \u2014 especially as he has only recently got tentative approval for a one-year grant from the European Commission to develop the idea. But now, in the aftermath of the meltdown, may be the time to start. Axtell endorses that view. \"Left to their own devices,\" he says, \"academic macroeconomists will take a generation to make this transition. But if policy-makers demand better models, it can be accomplished much more quickly.\" \"The revolution has to begin here,\" agrees Weinstein, who helped organize a meeting in May at the Perimeter Institute for Theoretical Physics in Waterloo, Canada, that assembled the kind of interdisciplinary mix of experts that Helbing envisions. \"And I think ideas from physics and other parts of science really have a chance to catalyse something remarkable.\"   See Editorial,  \n                     page 667 \n                   , and Opinion,  \n                     pages 685 \n                    and  \n                     687 \n                   . \n                 Mark Buchanan is a science writer based in Cambridge, UK. After writing this story, he was involved in reviewing grant proposals on the topic of agent-based modelling.  \n               \n                     Dirk Helbing's website \n                   \n                     Blake LeBaron's web page \n                   \n                     A NASDAQ Market Simulation \n                   \n                     AMES Wholesale Power Market Test Bed \n                   \n                     Silvano Cincotti's web page \n                   Reprints and Permissions"},
{"file_id": "459765a", "url": "https://www.nature.com/articles/459765a", "year": 2009, "authors": [{"name": "Jim Schnabel"}], "parsed_as_year": "2006_or_before", "body": "Assessing the effects of television on young children is far from easy. But, as researchers tell Jim Schnabel, that is no reason not to try. In 1998, Dimitri Christakis took time off from work to care for his two-month-old son. At home he found himself watching television to pass the time \u2014 \"more TV than I had ever watched in my life\", he remembers. Soon he noticed that his infant son was watching too. Even CNN kept the boy glued to the screen. \"Obviously he wasn't following the news,\" says Christakis, a professor of paediatrics at the University of Washington in Seattle. Christakis realized that the jumpy images on the screen were engaging the child's 'orienting response', a basic attentional reflex that directs the senses towards a sudden change in the environment. He wondered about the long-term effect of this on a brain that was at such a sensitive developmental stage. Could it alter the brain to 'expect' overstimulation, so that ordinary reality would thereafter seem dull by comparison? And could such a mechanism help to explain the ongoing tsunami of attention deficit hyperactivity disorder (ADHD) diagnoses, whose rise had roughly coincided with the dramatic increase in media consumption in Western societies? Christakis decided to try to address these questions with research. Together with several colleagues, he examined a database called the National Longitudinal Survey of Youth. After analysing some 1,300 children for whom the appropriate data were available, they found that on average, a child who had watched two hours of television per day before the age of three was 20% more likely to have attentional problems at the age of seven, compared with a child who had watched none. Christakis and his colleagues published their results in 2004 (ref.  1 ). Then, working with public-health expert Fred Zimmerman, who is now at the University of California, Los Angeles, Christakis did a follow-up study 2  with a different longitudinal sample, showing that the link to later attentional problems was particularly strong for cartoons and other entertainment programmes watched before the age of three. For educational programmes, such as the gently paced US series  Mister Rogers' Neighborhood , they found no such link. These studies were among the largest and most persuasive ever to have linked TV to reduced attention and, as such, they made splashes in the media and the research community. But as observational studies, they had their limitations. An association between TV watching and later attention problems did not prove that one had caused the other. A host of other factors, such as the socio-economic status of a household, could also have contributed to the association, and although researchers typically try to take those other factors into account, they can't always do so accurately. So, like other scientists who had addressed this issue, Zimmerman and Christakis concluded that more research was needed. In particular, Christakis believed that what was needed was a large-scale intervention study, a clinical-trial-type experiment in which one randomly selected group of children would be assigned to watch only a small amount of educational TV, whereas the other group would watch whatever their parents normally allowed. However, Christakis's 2006 proposal \u2014 which would have enrolled 900 children, reduced TV exposure in half of them in the first two years of life and assessed attention and related cognitive functions until the age of four \u2014 was turned down by reviewers for the US National Institutes of Health (NIH). Even now, the NIH does not have an interventional study under way or planned in this area \u2014 nor does Christakis know of one under way or planned anywhere else in the world. This apparent lack of follow-through seems to be part of a broader phenomenon. On the one hand, there is fairly convincing evidence that some TV content, such as the widely aired  Sesame Street , can benefit children in a certain age range. On the other, a great deal of research suggests that some TV content can be harmful \u2014 yet despite the ominous public-health implications, little seems to be done about it. \"Media are arguably the most ubiquitous environmental influence on kids' health and development,\" says Michael Rich, a paediatrician and media-effects researcher at Harvard Medical School in Boston, Massachusetts. \"The problem is that as a society we have not seen this as enough of a health concern that we've decided to invest in it.\" Because work on the potential effects of media is seldom distinguished from other kinds of developmental and public-health research, it is difficult to know how much is spent on it. But Christakis estimates that the worldwide total is \"below US$10 million per year\".  \n                Tuned out \n              Some major funding agencies just \"don't think that media are as important as other factors in children's lives,\" says Ellen Wartella, a developmental psychologist at the University of California, Riverside. She notes that at a recent meeting of the Society for Research in Child Development, part of the US National Academy of Sciences, \"out of hundreds of sessions on children, only three sessions were devoted to media effects\". Even when media-effects research is done, and its conclusions seem compelling, it appears to have little influence. For example, in at least two recent studies 3 ,   4 , researchers have tried but failed to find evidence that popular DVDs targeting infants have cognitive benefits, and one of these studies, co-authored by Christakis 3 , hinted that they might even impair children's language development. Because of research like this, the American Academy of Pediatrics (AAP) discourages parents from letting infants watch TV at all. But apart from France, which last year banned infant-targeted programmes from its broadcast channels, few, if any, countries have policies that restrict infant TV. And Wartella notes that according to surveys, mothers frequently ignore the AAP's advice and don't tell their paediatricians. \"They don't want to hear that they shouldn't put their child in front of the screen,\" she says. Science's slight impact on media consumption extends beyond issues relating to infants. In the past two decades, researchers have found strong and consistent links between older children's exposure to certain media content and, for example, obesity, eating disorders, aggression, desensitization to violence, sexual promiscuity and the use of alcohol and cigarettes. Brian Primack, a paediatrician at the University of Pittsburgh School of Medicine in Pennsylvania who researches media effects on adolescents, says that for some of these outcomes, \"I think we do have enough data [to justify] warning labels\". Studies of adolescents and younger children, meanwhile, show that their media exposure continues to expand rapidly, creating what media-effects researchers have begun to call a 'digital childhood'. Is this happening because the evidence for media's harmful effects remains sparse? Or have researchers such as Christakis found themselves up against cultural forces that they cannot defeat with evidence alone? Researchers argue that a host of factors, both scientific and societal, are to blame. For one thing, researchers such as Christakis, Rich and Primack are paediatricians, yet their proposals for epidemiological-type research on the effects of childhood media are usually reviewed by developmental psychologists. Developmental research traditionally hasn't placed great emphasis on media effects amid the complexities of childhood development, says Kevin Durkin, a developmental psychologist at the University of Strathclyde, UK. Among his colleagues, he adds, \"there's still a bit of a reluctance to engage with the real world and with things that [children] are actually doing on a daily basis\" \u2014 and a corresponding preference for theory-driven, laboratory-based research. Media-effects research, by contrast, has often been considered methodologically messy. Virtually all studies of potential health effects, for example, have been observational studies of real-world populations \u2014 populations before and after media exposure, or populations in which different people have different exposures. Because the groups being compared in these cases are not randomly assigned to 'watch' or 'not watch', it is at least conceivable that their differing outcomes are the result of other, perhaps hidden, factors not directly related to media exposure. \"There's always going to be that question, because it's not like a randomized clinical trial,\" says Primack. As he and other researchers note, the same problem once plagued researchers who wanted to show that cigarette smoking harmed people's lungs. A 'clinical trial' of smoking's health effects, in which groups of people were randomized to smoke or not smoke, would have been grossly unethical. Over time, however, researchers were able to build a case against tobacco with well-designed observational studies, for example showing a 'dose\u2013response effect' in which heavier smoking was linked to higher cancer risk. But although cigarette smoke is a relatively simple kind of exposure, media exposure is much harder to track. Typically, researchers have lacked the means to monitor precisely what their subjects are watching from day to day for study periods that may last months or even years. They have often relied on self-reporting or parental reports, both of which are considered unreliable. \"If you ask the parents about how much TV their kid watches, they tend to overestimate, whereas the kid tends to underestimate,\" says Rich. \"One way or another the report tends to be biased.\" In the past few years, Rich, Primack and others have been experimenting with other monitoring methods such as ecological momentary assessment, in which subjects or their parents report real-time data of their media exposures to researchers using cellphones, personal organizers and even camcorders.  \n                Information overload \n             On the whole, though, technological evolution may be making it harder to study media's effects. \"It's no longer the box in the middle of the living room that everybody gathers around,\" notes Rich. \"We have screens everywhere \u2014 in our pockets, you know? I was in a hotel recently and I realized that from where I sat I could see seven screens with seven different things on them. And I wasn't even actively watching media.\" Similarly, notes Susan Newcomer, a programme officer who oversees some media-effects research at the National Institute of Child Health and Human Development (NICHD) in Bethesda, Maryland, \"How do you measure screen exposure if the kid is clicking through 72 web pages a minute? And not just clicking through the web but also texting a friend, listening to music, all at the same time?\" Compounding this problem is the fact that even for a single, uncomplicated medium \u2014 such as one television channel \u2014 a viewer faces multiple potential influences that may be difficult, if not impossible, to tease apart. There is the stream of content within a television programme, for example, and then there are the contents of interleaved advertisements. Viewers may be affected, too, by the 'formal', non-conceptual properties of TV watching, such as the rapid image-shifts meant to trigger viewers' orienting responses. There is also the likelihood that in watching TV, a viewer will be inert on a sofa instead of exercising, sitting alone instead of socializing, and staying awake instead of sleeping \u2014 and all of these behavioural displacements have their own potential health effects. \"You're about to go to bed and you turn on the television to wind down, but then three hours have passed and you've seen  Dracula   again,\" says Primack, pointing out that a lack of sleep is now believed by some to contribute to obesity as well as mental-health problems. According to James Griffin, a programme officer whose NICHD branch oversees research on infants and young children, the institute now generally prefers studies that can identify specific cognitive mechanisms of a child's interaction with media. \"We are focusing more on looking at what infants and toddlers are capable of learning [from media],\" he says. In theory, interventional studies would get around many of these problems because the source and content of media could be controlled and monitored. So why not do them? \"Good luck getting that past either an NIH review panel or an institutional review board,\" says Newcomer. Over the time needed for any effect to appear, she explains, it would be \"both ethically and pragmatically a real challenge to constrain people not to look at television, for example, or to look at only certain kinds of television\". Yet Christakis argues that such intervention studies are still feasible, particularly when it comes to the effects of TV on very young children. In principle, for such children, TV watching can be more easily controlled by parents, and more easily monitored by researchers. The early developmental stage of these children also means that TV's effects may show up more quickly, thus requiring shorter studies. Without such investigations, Christakis says, \"it's going to be very difficult for us to satisfactorily prove to the critics and the cynics that a particular content is harmful\". Media-effects researchers are not just fighting resistance in the scientific community. They say they face a broad cultural resistance too, one rooted in core Western beliefs \u2014 that humans have 'free will'; that they choose autonomously and rationally from the information put before them; that a free market in information is better than a restricted one. \"In the United States, questioning any aspect of television has been viewed as a direct assault on the First Amendment,\" says Christakis. \"Tobacco researchers didn't have to deal with that issue.\" And the issue isn't confined to the United States. In 2005, India's government banned film images of actors using cigarettes \u2014 a known motivator for adolescents to start smoking \u2014 yet the country's High Court reversed the ban earlier this year, on free-speech grounds. Researchers say that they confront a related problem called the 'third-person effect'. \"Most people will acknowledge that TV is on balance a bad thing; they just don't think it's bad for  their   kids,\" explains Christakis. \"Somehow they think that their kids are immune, or that the way they use TV is different.\" But the conceit isn't justified, Rich says, because \"epidemiologic data [showing a harmful influence] are generated on our kids, not on somebody else's kids\". Media-effects researchers also say that their arguments are apt to be treated, by the public and even by other scientists, as old-fashioned arguments about the immorality of popular-media content, now dressed up as modern health issues. Yet Rich insists that the difference between moral issues and health issues is real and may be crucial in convincing people to change their media-consumption habits. \"The moral issue has helped to stalemate this,\" he says. \"People have different value systems. But if you present them clear data that show what the effect of this media content is on that health outcome, you can get them to agree.\" Even if people can agree such things on a rational level, in practice their media-consumption behaviours are deeply entrenched, as even Rich admits. \"Parents do tend to say I'm sorry I need to put the kids in front of the tube so I can get the house vacuumed. Sometimes, governments step in to break cultural stalemates, and in the United States \u2014 where the majority of media-effects research takes place \u2014 that almost happened. In 2004, Senator Joseph Lieberman (Democrat, Connecticut) introduced a bill called the Children and Media Research Advancement Act (CAMRA). Designed to remedy \"the paucity of research\" in this area, and enthusiastically backed by researchers including Rich and Wartella, it would have dedicated an NIH budget for media-effects studies during 2005\u201309, starting at $10 million per year and ending at $25 million per year \u2014 figures that would have dramatically increased the activity in this field. The bill never made it past the committee stage. Indeed, a group called Citizens Against Government Waste criticized Lieberman, stating in a press release that his proposed effort \"belittles the ability of parents to use common sense in deciding what entertainment is appropriate for their own child's consumption\". CAMRA has been reintroduced several times since then, with funding levels left unspecified. But it still has not made it into law. According to a Senate staffer who didn't want her name used, some senators have objected to the legislative determination of where NIH funds should be applied \u2014 which they say the agency has had enough of already. (A 2006 version would have placed the programme with the Centers for Disease Control and Prevention in Atlanta, Georgia.) Other legislators simply have not wanted to spend the money, although on an annual basis that money would have amounted only to the cost of a few episodes of a hit TV show. \"That in a nutshell is our societal ambivalence towards this subject,\" says Rich.  \n                Accentuate the positive \n              Last year, Christakis was able to get a media-effects study funded by the NICHD. Just starting now, the project will look at the effects on pre-school children of changing TV content rather than eliminating it. \"We're trying to get them to watch less aggression and more prosocial programming,\" says Christakis. \"But we're not telling them to watch less; we just want them to watch better TV.\" The pro-TV message, he adds, is \"something that everybody gets behind\". Rich, too, notes that \"the problem with coming at it from a totally negative standpoint is that people just check out\". He urges a greatly expanded research base \"that tells us how to live with media, because we're going to have to one way or another\". He also foresees that media use will change, albeit 'generationally', as it did with cigarettes. As evidence accumulates that TV can cause health or behavioural problems, he predicts, certain kinds of media use may start to be seen by youngsters and their parents not as forbidden pleasures but as \"just dumb\". Christakis is less sanguine. He stopped his own kids from watching TV until the age of two and now places modest restrictions on their Internet use. He worries that, in addition to its functional entrenchment in people's daily lives, a lot of media content might be physiologically addictive \u2014 perhaps less so than nicotine-laden cigarettes, but with a much larger susceptible population. \"I think there's a biochemical response to that kind of stimulation, that instant gratification, that rush, which isn't very well understood,\" he says. \"And I think there's a sense in which some people are very reluctant to acknowledge that there's a problem, because they also suffer from it.\" Although the concept of behavioural as opposed to drug-induced addiction is still somewhat controversial in Western countries, it is less so in South Korea and Japan. The exposure to electronic media, especially the Internet, seems to be greater there, and the reported prevalence of related addiction behaviours is remarkably high 5 . Christakis suspects that those societies may be \"the canary in the coal mine\" when it comes to media addiction. One advantage researchers had in studying the effects of cigarette smoking was that, even if they couldn't experiment directly on humans, they could do so on animals. Christakis now hopes to remove that advantage. This summer he will begin tests on a rodent model of media exposure, comparing rats raised in a 'hyperstimulating' environment of fast-changing sounds and lights, with those raised in a quiet setting, and looking at outcomes relating to attention and addiction. \"Animal studies have their limitations,\" he says. \"But they have their strengths too, because you can totally control the animal. You can drill down, so to speak, to a level you could never do in humans.\" \n                 Jim Schnabel is a freelance writer based in Maryland.  \n               \n                     Dimitri Christakis \n                   Reprints and Permissions"},
{"file_id": "459630a", "url": "https://www.nature.com/articles/459630a", "year": 2009, "authors": [{"name": "Alison Abbott"}], "parsed_as_year": "2006_or_before", "body": "\"We don't pretend to have invented new physics,\" says Ernst Stelzer modestly, standing next to an equally modest layout of lasers, mirrors and lenses. \"It has all just been plain common sense.\" Stelzer has been applying his common sense to microscopes ever since he arrived at the European Molecular Biology Laboratory in Heidelberg, Germany, in 1983 as a fresh-faced physics PhD student, and stayed on to head a team developing three-dimensional light microscopy. A quarter of a century in the field has now led to single plane illumination microscopy (SPIM), the technique with a modest face but an extravagant view: beautiful and unprecedented moving images of whole organisms as they grow one cell division at a time. When Stelzer started out in the 1980s, a method called confocal fluorescence microscopy was beginning to show huge potential. The technique exploited the labelling of molecules in cells with a fluorescent tag to build up a three-dimensional image. But it still relied on traditional light microscopy procedures by which the biological sample is attached to a two-dimensional slide and a beam of light passes through the lens and through the entire sample, stimulating fluorescent emission that is detected by a camera. Two problems with this approach bothered Stelzer. First, the squashed, two-dimensions of the slide were an unnatural environment for cells that live in three-dimensional tissue. And second, fluorescent microscopy slowly destroys the very thing that researchers are trying to make visible. The undiscriminating beam illuminates and triggers fluorescence from the entire sample, not just the focal plane. Yet photons are damaging to cells, which ? apart from those in skin, eyes or other body surfaces ? receive little daylight. Light also 'bleaches' the fluorescent tags, which limits the number of times that the sample can be observed. Stelzer determined to develop a microscope that could visualize living biological samples, for long time periods, in conditions that approximate normal physiology. To optimize the use of light, he turned a laser-light source at 90\u00b0 to the camera and lens. In 2002, he added the element that is central to SPIM ? mechanics that create a micrometre-thin sheet of light in a single plane that sweeps gradually through the specimen (see graphic). \"This idea had been around for a century,\" he says, \"but biologists didn't realize its potential for high-resolution.\" Only the fluorescent tags in this plane of light are excited, and the camera efficiently captures the photons emitted. The specimen is then rotated and the procedure repeated along half a dozen or so additional axes. Eventually all the planar images are merged together computationally into a three-dimensional whole. By eliminating excess illumination and dye photobleaching, the whole process can be repeated every two minutes or less for more than 24 hours. The specimen to be viewed ? intact, in three dimensions ? passes the time in a tiny, transparent cylinder filled with agarose gel to dampen any movement and perfused with physiological levels of gases such as oxygen and carbon dioxide. Stelzer, working with colleague Joachim Wittbrodt, has shot spectacular movies of the first day in the life of a zebrafish embryo, a species loved by microscopists for its transparent skin ( P. J. Keller  et al .  Science    322,   1065?1069; 2009 ). As development is triggered, a single oval cell undulates, seeming to squeeze out additional complexity. More and more new cells shoot out along purposeful paths in a coordinated and symmetrical way. Emerging shadowy shapes soon become recognizable as organs. It all happens at a dazzlingly swift and confident pace until the one cell has multiplied to some 20,000 cells. \"It gives you a good feeling to see what is happening ? you understand things more intuitively,\" says Carl-Philipp Heisenberg, from the Max Planck Institute of Molecular Cell Biology and Genetics in Dresden, Germany. But Heisenberg says that the real wonder is in the data rather than in the movies. \"That is where the real information is hidden ? it is immensely helpful that the technique allows you to track all of the cells all of the time.\" For developmental biologists interested in tracking the destiny of each and every cell, these global, dynamic data are a boon, he says. Stelzer's dream would take this further ? into the realm of systems biology. He sees a future in which movies of thousands of developing embryos, each with one gene mutated, are recorded and compared. \"Now we can think about systematizing the process to see the consequences of every possible gene mutation on development of the embryo, or of organs ? or the effect of small changes in conditions such as temperature or acidity,\" says Stelzer. \"It's a systems-biology microscope,\" agrees Wittbrodt, who talks about mapping the expression of genes on to the system too. The process is computationally very demanding. Twenty-four hours of data for the zebrafish embryo took around 1,000 hours to process using their system. Stelzer is working on robust hardware and software so the microscope can be used by non-specialists. He and others are also refining the ability of SPIM to record movies of biological processes in real time. \"Already you can see how an organ such as the beating heart develops, or how blood vessels develop, in the embryo\", says developmental geneticist Didier Stainier, who is working with Stelzer's former PhD student Jan Huisken at the University of California, San Francisco. Back in Heidelberg, Keller puts an adult fly into the container and flicks a switch. A fuzzy image of its eye, from a single-plane shot, appears on the computer screen. A few seconds later he flicks another switch to line up the images from hundreds of planes, and an intensely detailed view of the eye materializes like a rabbit from a magician's hat. But \"it is not magic\", says Stelzer. \"It's simple technology.\"   See also  \n                     page 629 \n                    and online at  \n                     http://tinyurl.com/microspecial \n                   . \n                     Microscopy special \n                   \n                     Cell imaging: New ways to see a smaller world \n                   \n                     Stelzer group \n                   Reprints and Permissions"},
{"file_id": "459636a", "url": "https://www.nature.com/articles/459636a", "year": 2009, "authors": [{"name": "Heidi Ledford"}], "parsed_as_year": "2006_or_before", "body": "Sunney Xie's newest microscopes don't look like the latest in sophistication. Tucked away in his biochemistry lab at Harvard University, they seem to be ad hoc assemblies of lasers, objectives and electronics, surrounded by a thicket of optical equipment. \"Don't worry about most of these,\" says graduate student Christian Freudiger, gesturing to the latest addition to Xie's microscope family. \"You only need a few optics to use the microscope. The rest are just for us to play with.\" As he leans briefly on the table, which is designed to eliminate vibrations, it counterbalances his weight with a reproachful \"shhhh\". Freudiger and the other researchers 'playing' in Xie's lab have pioneered techniques to see biological molecules in their natural state. Based on Raman spectroscopy, the methods detect compounds from the characteristic vibrations of their chemical bonds, and they free the user from having to label molecules by attaching tags such as gold particles, antibodies or fluorescent proteins. Many researchers are unwilling to abandon labels because they offer an unparalleled ability to distinguish target molecules in a cell. But for others, who want to observe a molecule 'naked', without the interference of a molecular tag, it is proving to be hugely liberating. In traditional Raman microscopy, laser beams illuminate a sample and the characteristic shift in wavelength caused by chemical bonds helps researchers pinpoint the identity and location of certain molecules. The method is therefore best suited for imaging molecules with distinct spectroscopic properties, such as lipids. But in early Raman microscopes the signal was weak and the technique required long exposure times (sometimes more than a day), precluding the possibility of monitoring biological processes that happen in minutes, seconds or less. The microscopes also relied on powerful lasers, which fry delicate biological samples. About a decade ago, Xie's lab developed a method called coherent anti-Stokes Raman scattering (CARS) microscopy, which uses two laser beams to excite molecular vibrations and generates a stronger signal. This technique cut down on exposure time and laser power, but was plagued by a high background signal. Then in December last year, Freudiger together with postdoc Wei Min reported a further improvement 1 . Called stimulated Raman scattering microscopy (SRS), the method excites molecules with two laser beams that have been calibrated so that the difference between the frequencies of the beams matches the vibrational frequency of the molecule to be imaged. The result: only the target molecules are excited and the troublesome background is squelched. By eliminating the high background signal, the SRS technique promises to extend label-free microscopy to molecules that couldn't previously be detected. Biologists are already lining up to give Xie's microscopes a try, along with similar instruments being developed by other groups 2 ,   3 . Biofuels researcher Shi-You Ding of the National Renewable Energy Laboratory in Golden, Colorado, wants to use SRS in his pursuit of techniques to degrade cellulose, a major component of plant cell walls, into smaller sugars that can be used for fuel. His research has been frustrated by the lack of a method to distinguish cellulose from lignin, another molecule found in plant cell walls, without having to stain cells with techniques that affect the distribution of the two compounds. \"Even if you want to tear down a building, you need to know what the structure is,\" says Ding, \"but in this case there was just no way to see it.\" Ding teamed up with Xie's graduate student Brian Saar to create movies of cellulose and lignin dynamics, allowing him to find chemical conditions that will break down lignin and leave the cellulose intact. \"It's a very powerful tool,\" Ding says. \"Biologists have never been able to get that kind of information before.\" Xie's group is also collaborating with researchers at the pharmaceutical company Pfizer in New York, who have already used the technique to track the acne-treatment retinoic acid as it is absorbed by skin. Scientists at Unilever based in the Netherlands and the United Kingdom want to use SRS to study the effects of cosmetics on skin and the distribution of fats and proteins in foods without having to attach bulky labels. At the moment, an SRS microscope costs in the order of half a million dollars, but Xie hopes that technological improvements will reduce the cost and complexity of the set-up. Zeiss and Leica, two major microscope manufacturers, have just licensed the technology, and aficionados who have set up their own CARS systems can tweak their microscopes to accommodate SRS, he says. SRS still has a few kinks to iron out, says Jing Kang, a professor of medicine at Harvard Medical School. It should enable researchers to distinguish between different types of unsaturated fatty acids at biological concentrations within the cell \u2014 discerning heart-friendly omega-3 fatty acids from the less-healthy omega-6 fatty acids, for example. But so far, Kang and his collaborators in the Xie lab have not been able to do so. \"In theory it should be okay,\" Kang says, \"but whether it's really going to make it \u2014 that's another story.\"   See also  \n                     page 629 \n                    and online at  \n                     http://tinyurl.com/microspecial \n                   . \n                     Imaging in Cell Biology \n                   \n                     Method of the Year 2008 \n                   \n                     Microscopy special \n                   \n                     Cell imaging: New ways to see a smaller world \n                   \n                     Sunney Xie \n                   \n                     Shi-You Ding \n                   Reprints and Permissions"},
{"file_id": "4591047a", "url": "https://www.nature.com/articles/4591047a", "year": 2009, "authors": [{"name": "Heidi Ledford"}], "parsed_as_year": "2006_or_before", "body": "The iPlant programme was designed to give plant scientists a new information infrastructure. But first they had to decide what they wanted, finds Heidi Ledford. In April 2008, Richard Jorgensen found himself in front of a group of expectant researchers gathered in Cold Spring Harbor Laboratory, New York. The pressure was on: Jorgensen had recently been placed in charge of iPlant, a US$50-million, five-year programme funded by the National Science Foundation (NSF). The project was supposed to tackle the biggest computation questions in plant biology \u2014 and his job was to unite the community behind the effort. \"The plant sciences are being given the opportunity to lead,\" he told the assembled crowd. But after two days of the meeting, the researchers were not clear where that leadership would be taking them. Brainstorming sessions had repeatedly slipped into guessing games as participants tried to infer what Jorgensen wanted as the project's 'grand challenges'. \"I'm still not sure I understand, in all honesty, what this is and what this is supposed to do,\" said June Medford, a plant synthetic biologist at Colorado State University in Fort Collins, during one session. Jorgensen, who is based at the University of Arizona in Tucson, was refusing to offer concrete suggestions for what the grand challenges should be for fear of unduly influencing the participants. \"I'm officially agnostic,\" he said, squinting in the midday sun on the last day of the meeting. \"My role is more like being a therapist, in a way.\" One year and several workshops later, Jorgensen's therapy seems to be paying off. Groups in the plant community now have half a dozen grand-challenge projects that tackle everything from evolutionary genetics to the mathematical modelling of plant development \u2014 and earlier this year the iPlant organizers announced which two they will pursue first. If the initial projects work out, the whole effort could be extended for another five years with an additional $50 million. iPlant will not just be a test of data-management. It will also be a test of an unusual organizational structure. The NSF decided to fund the project before knowing precisely what computing tools it would be paying for, leaving the scientists to decide. \"People would ask, 'Why do we have to go to all of this trouble? You know what we need \u2014 just go build it',\" Jorgensen says. \"But the NSF decided that you have to have a buy in from the users first, or you're going to build something they don't really want. And I think they're right.\"  \n                Model infrastructure \n              The outcome of iPlant could have repercussions for the broader biological research community, as it is also struggling to integrate and process a torrent of computational data. The iPlant model is one that the NSF may want to use for constructing 'cyberinfrastructure' in other fields, says Peter McCartney, the NSF's programme officer for iPlant. \"It's a grand experiment,\" he says. \"We don't really know how it works and we're sure that a lot of the things they try won't work. But we are confident that some will work and that will also provide us with a direction for the future.\" The NSF has invested heavily in plant biology over the past decade, particularly in high-throughput 'omics' projects. The agency has funnelled about $200 million into a project to determine the function of all 25,000 or so genes in the model plant  Arabidopsis thaliana , and has also contributed to a large, interagency programme for genomics projects in other plants. These and other efforts have generated rich databases and computational tools that are open for the community at large to use, but programme managers at the NSF realized that a problem loomed ahead. \"Here was a community in which there had been a substantial investment in tools, but there was some concern about how these tools were going to work together, and how they were going to persist long-term,\" says McCartney. This problem is not unique to the plant sciences. Researchers typically build databases the quickest way they know how, without necessarily considering whether they will work with other databases. And once a database is in place, it is very difficult to alter it, says Graham McLaren, programme leader in bioinformatics data management for the Generation Challenge Program of the Consultatative Group on International Agricultural Research in Texcoco, Mexico: \"I would say it's easier to get someone to change their spouse than their database.\" Some research communities in the physical sciences, such as astronomy and particle physics, tackled these issues long ago by agreeing on a unified cyberinfrastructure. But the problem is relatively new in the biological sciences. When the NSF looked to help by building a cyberinfrastructure project in the biological fields it funds, it decided that plants were an ideal place to start, McCartney says. Plant biology covers a very broad and disparate community that studies many model organisms, making data incompatibility a particularly acute problem. Integrating these data could have societal benefits in terms of agriculture and conservation \u2014 and, says Jorgensen, the field already has a long history of collaborative projects.  \n                Humble beginnings \n              Jorgensen was drawn into the field in 2006 as he was preparing for the end of a five-year tenure as editor-in-chief of the  Plant Cell   journal, and making plans for a sabbatical in Mexico. He started having second thoughts when he saw a call from the NSF for proposals in plant-science cyberinfrastructure. \"It was a new way to contribute,\" he says. \"It just seemed like one of the most challenging things that I'd encountered and a unique idea.\" He decided to find out whether his colleagues at the University of Arizona would be interested in taking on the challenge. When his team was awarded the grant, Jorgensen put the sabbatical on hold to coordinate the project. At his suggestion, it became known as iPlant. Jorgensen sees iPlant as an opportunity to unify a plant-biology community that has long been split along disciplinary lines \u2014 and he knew from the start that recruitment would be key to the project's success. He had to convince ecologists and evolutionary biologists that iPlant was not just about molecular biology and 'omics. He also had to sign up molecular biologists, who quickly assumed the collaboration was just another bioinformatics project. \"'Sure, I'll send my bioinformatician to the meeting', was their response,\" Jorgensen says, \"but it's not just the bioinformaticists that we need.\" He then enticed in dedicated computational biologists and software engineers. \"What the NSF has done is forced a kind of shotgun marriage between biologists and computer scientists,\" he says. If anyone can unite the community, many say that Jorgensen is the researcher to do it. Well-known but unassuming, he already has the respect of many scientists for his academic achievements and diplomacy. In the late 1980s, for example, Jorgensen and his colleagues at the biotechnology firm DNA Plant Technology in Oakland, California, decided to develop petunias with richer colours by boosting expression of a pigment gene called chalcone synthase. To their surprise, many of the resultant flowers were white: rather than enhancing expression of chalcone synthase, they seemed to have shut it down entirely. Jorgensen left the company to continue investigating the phenomenon, which he named 'cosuppression', and even conducted experiments at his own home for a while before he was given a lab at the University of California, Davis.  \n                Nobel thoughts \n              Some years later, researchers would realize that some cases of cosuppression, which had turned up time and again when researchers tried to make transgenic plants, were due to a process called RNA interference (RNAi). When Andrew Fire and Craig Mello were awarded the 2006 Nobel Prize in Physiology or Medicine for their work on RNAi in the nematode  Caenorhabditis elegans , some researchers complained that early contributions made by plant biologists, including Jorgensen, had been overlooked. Jorgensen demurred, pointing to the contributions that Fire and Mello had made to working out the mechanism behind RNAi. \"The Nobel prize is not really about making scientists famous \u2014 it is about making science interesting and accessible to the public,\" he wrote in a letter to the journal  Science   at the time ( R. Jorgensen  Science    314,   1242; 2006 ). Richard Jefferson, a plant molecular biologist and founder of CAMBIA, a non-profit research institute based in Canberra, Australia, said of Jorgensen: \"I think he's the smartest man in plant science \u2014 and the most intellectually generous.\" Jorgensen needed all those qualities to negotiate his way through the first year of iPlant and to overcome researchers' initial uncertainty. After the Cold Spring Harbor meeting, the NSF solicited proposals for grand-challenge workshops, and selected five that were held over the course of the next year. From those workshops, and a sixth held by the National Center for Ecological Analysis and Synthesis in Santa Barbara, California, emerged six grand-challenge teams, some of which united dozens of researchers. In April this year, the iPlant board of directors \u2014 comprised, at Jorgensen's request, of plant biologists and computer scientists rather than iPlant leaders \u2014 recommended two projects to focus on for the next two years. The board gave highest priority to a project already familiar to many plant biologists: developing a plant 'tree of life' to determine the evolutionary relationship between taxa. The NSF has long supported such efforts, including the 'Deep Green' plant phylogeny project of the late 1990s, and the broader Tree of Life project, which included all taxa and will reach the end of its funding in the next year. For Rob Last, a plant biologist at Michigan State University in East Lansing and associate chairman of the iPlant board of directors, prioritizing the tree-of-life project was a practical decision. \"This is a community that has worked together a lot. It has strong leadership,\" he says. \"And the tree of life is a really nice coordinate system that ultimately we should be hanging our data on.\" The iPlant proposal differs from previous tree-of-life projects in that it does not focus on data collection \u2014 iPlant is not allowed to distribute funds for this. Instead, it will concentrate on infrastructure and technology development, says project leader Mike Sanderson, a plant systematicist at the University of Arizona. These computing tools should allow researchers to extract gene sequences and morphological traits from a wide variety of databases, and compile that information into comprehensive evolutionary family trees. The aim is to build trees with the data available for about 50,000 plant taxa, even though the long-term goal of plant phylogeny projects is to generate a tree of the more than 500,000 taxa that are known. Lack of support for data collection was a common complaint in the early days of iPlant. At the initial Cold Spring Harbor meeting, the question came up repeatedly: why develop tools to unite incomplete databases of varying quality, when what the community needs is more complete data of high quality? With time, and with the knowledge that funding for iPlant does not eat into the NSF's plant-research budget, the community has come to accept the idea. \"What we consider high-quality data today may not be considered high quality five to ten years from now, so where do you start?\" says Steve Goff, iPlant's director of community interactions. \"You have to work with what you have at the time.\" The second prioritized project \u2014 called 'genotype-to-phenotype' \u2014 will explore how variations in genetic sequence relate to the appearance and behaviour of plants and was built by cherry-picking parts of various proposals. One part will make a stab at using new computational tools to study genetic and environmental influences on when a plant commits to flowering \u2014 a topic that has long interested farmers. Another aspect will build models of photosynthesis with the ultimate aim of learning how to convert 'C 3  photosynthesis', the kind present in many crop species, into the more efficient form called 'C 4  photosynthesis' that is present in corn and some other plants. A third focus will be on the effects of genotype on responses to climate change. \"This is really a unifying grand challenge in biology,\" says Last. The selection process has inevitably left some researchers disappointed. One grand challenge proposal aimed to tap into about 500 million digital records from herbaria and ecological study plots around the world, showing the occurrence of plants in different climates and environmental conditions. Linking these data could allow ecologists to monitor how species distributions and habitats have changed over time, with the long-term goal of understanding the impact of climate change. \"The data explosion in ecology is enormous,\" says University of Arizona plant ecologist Brian Enquist. Although iPlant directors have said they hope to tackle some aspects of the proposal, Enquist worries that a piecemeal approach will not suffice. He agrees with the decision to prioritize the tree of life project, but points out that it already has a long history of steady funding. \"We have just a kazillion ecological data points, but we have nowhere to go to combine them.\"  \n                Spreading out \n              Enquist and his colleagues may have another place to go if iPlant is successful. \"[The ecological community] is another that we'd probably be very interested in seeing if this kind of approach would help,\" McCartney says. But he acknowledges that it's still too early to judge whether iPlant will be a success. That will start to become possible when the first few computing tools are built, probably late this year, and researchers are testing them out. At the moment, those involved are determining where to start, breaking down the broad-sweeping challenge proposals into tasks that can be completed in the next two years. As for Jorgensen, he's finally taking that sabbatical. With the stress of the project launch behind him, he is hoping to have a little more time for his own research, before a new round of grand-challenge solicitations starts. He is also involved in planning an iPlant meeting for next year, \"to give the community a chance to look at what we've started\", he says, and to talk about what else the project should do as it matures. Clearly growth lies ahead. The question for researchers is whether they can grow iPlant into the framework they need: one that is big, strong and fast enough to support the data that they are also busy cultivating. \n                     Big Data special \n                   \n                     Nature Reviews Genetics Cyberinfrastructure Wiki \n                   \n                     iPlant Collaborative \n                   \n                     Richard Jorgensen \n                   Reprints and Permissions"},
{"file_id": "459902a", "url": "https://www.nature.com/articles/459902a", "year": 2009, "authors": [{"name": "Nicola Jones"}], "parsed_as_year": "2006_or_before", "body": "Metrologists are on a path to redefine the unit of temperature. The freezing point of water will never be the same again, finds Nicola Jones. A little over 20 years ago, Mike Moldover played a tune through a sphere of argon gas and became something of a rock star in the exacting field of metrology. The way those notes reverberated enabled him and his colleagues to determine the value of the Boltzmann constant \u2014 the relationship between the average kinetic energy of molecules and temperature \u2014 better than anyone ever before. Moldover was sufficiently confident of the result to put a bold promise into his paper: \"if by any chance our value is shown to be in error by more than 10 parts in 10 6 , we are prepared to eat the apparatus\" ( M. R. Moldover  et al. J. Res. Natl Bur. Stand.    93,   85\u2013144; 1988 ). No one expects Moldover to have to swallow his argon anytime soon. Moldover, who worked at the US National Institute of Standards and Technology in Gaithersburg, Maryland, and his colleagues determined the value of the Boltzmann constant ( k B ) as being 1.3806513 \u00d7 10 \u221223  joules per kelvin with an uncertainty of just 1.8 parts per million (p.p.m.). A generation later, that result continues to inspire others. \"Scientists are sort of in awe of Mike,\" says Michael de Podesta of the National Physical Laboratory in Teddington, UK. Today, de Podesta is racing others around the world to whittle down the error in Moldover's experiment to just 1 p.p.m. and at the same time confirm his result. Once the constant has been measured to that level of accuracy, an international committee plans to set it in stone, and use it to fundamentally shift the definition of the unit of temperature. \"It's the hope that once this is settled, the world will stop worrying about thermometry,\" says Moldover. The worry does not, admittedly, keep many people awake at night, nor is it of immediate practical importance. But it does have a certain philosophical heft. Today, the SI unit of thermodynamic temperature, the kelvin, is defined in terms of absolute zero and the triple point of water (the temperature and pressure at which water exists as a solid, liquid and gas in equilibrium), which is fixed at 273.16 K. This was done for historical reasons that were logical at the time. But to metrologists, the definition is inelegant, illogical and downright irritating. The kelvin is ripe for redefining. Richard Davis  \"It's a slightly bonkers way to do it,\" says de Podesta. According to the metrological code of ethics, it is bad form to grant special status to any single physical object, such as water. Worse, 'water' needs to be qualified further: at present, it is defined as 'Vienna standard mean ocean water', a recipe that prescribes the fractions of hydrogen and oxygen isotopes to at least seven decimal places. Finally, it makes no sense for researchers studying the chilly climes of low-temperature physics or the blazing heat of stars to have their measurements of temperature rely, even theoretically, on comparison to a random piece of ice. \"The kelvin is ripe for redefining,\" says Richard Davis, one of Moldover's former colleagues, who works at the International Bureau of Weights and Measures (BIPM) in S\u00e8vres, France. The BIPM, which is in charge of such things as units, aims to fix this unconscionable situation by 2011. The Boltzmann constant, which is currently a measured value with some uncertainty, will be declared a set number (letting it live up to its name of 'constant') and the kelvin will be redefined as something like the change in thermodynamic temperature that results in a change of mean translational kinetic energy of 1.38065 XX  \u00d7 10 \u221223  joules \u2014 the current work will fill in the  X s.  \n                Sound science \n              The change represents a metrological sleight of hand. Whatever uncertainty existed in the measure of the Boltzmann constant will be wiped clean by definition, and transferred to uncertainty in the measure of temperature. These new temperature uncertainties are so tiny that they certainly won't cause panic in any lab \u2014 most researchers won't even notice them. Instead the redefinition will open the door for improved accuracy at the far ends of the temperature spectrum. More than that, it frees the kelvin from an unsteady connection to water and rests it on the intellectually firmer foundations of a physical constant. As such, the move mirrors what metrologists are doing with all the units of measurement. In the case of the kelvin, the technique of choice for determining  k B  is called acoustic thermometry, and it relies on being able to precisely determine the speed of sound in a gas-filled sphere at a fixed temperature. From this the Boltzmann constant falls out of the equations relating the kinetic energy ( E ) to the thermal energy of the gas ( E   = 1/2 mv 2  = 3/2 k B T , in which  m   is the mass of one atom,  v   is the average speed of the atoms \u2014 which is proportional to the speed of sound in the gas \u2014 and  T   is the temperature). The speed of sound can be measured by analysing the frequency of the sound waves that resonate within the sphere. The good news is that experimentalists can measure the frequency with great precision; de Podesta has a rubidium clock in his lab that's accurate to 1 part in 10 13 . The bad news is that to analyse the resonances accurately, the volume of the container must be known with great precision. Moldover did this in his groundbreaking 1988 experiment by first filling his 3-litre sphere with mercury and weighing it. (He promised in his paper to eat this, too, should his result be proved wrong). The volume issue remains the hardest problem to crack. Filling the spheres with a liquid and weighing them is still a possible solution. But there is a more elegant method, the details of which have just been worked out by one of Moldover's original colleagues, James Mehl, professor emeritus at the University of Delaware in Newark. The idea is to fit the sphere with tiny antennas to measure the resonance of microwaves in the cavity, then use this measurement to determine the sphere's volume. The researchers will deliberately use a lopsided sphere, such as one with three axes of slightly different lengths, to avoid the complicated pattern of overlapping resonances that results from using a sphere approaching, but not reaching, perfection. \"It needs to be very exactly not a sphere,\" says de Podesta. To reach that kind of precision, he uses the same manufacturers that are involved in producing the mirrors for the future James Webb Space Telescope. The effort to best Moldover has inspired a friendly race between three main laboratories: the National Physical Laboratory, the National Institute of Metrological Research in Turin, Italy, and France's equivalent national lab outside Paris. All three are using acoustic thermometry \u2014 although with slightly different apparatus \u2014 to determine  k B . Every aspect of their work requires extreme care. \"You buy a very pure gas, which isn't pure enough. So you purify it more,\" says Roberto Gavioso from the institute in Italy, explaining just one technical detail from a long, long list. One or all of the labs should hit 1 p.p.m. by the end of 2009 or 2010. \"I'm cheering for these guys,\" says Moldover who, now 69, has passed on the Boltzmann torch. At the same time, the German National Metrology Institute in Berlin, heads an alternative approach. Bernd Fellmuth's kit consists of a pressure chamber surrounded by a cubic metre of water to keep the temperature constant. A set of capacitors inside the chamber measures the capacitance before and after helium is inserted, which allows the team to calculate  k B  using a different set of equations from those used by the acoustic thermometry group. A difficulty here, however, is how to measure the pressure to the required accuracy; currently that's only possible to about 4 p.p.m., and an entire team is required to reach that point. The group hopes to begin experiments in January 2010 that could get the accuracy down to 1 or 2 p.p.m. by the end of that year. Moldover calls that an optimistic target, but it would be good to have a check for the acoustic work in time for 2011, he says. Once the results are in, the decision goes to an international vote. Each of the 53 nations that are members of the BIPM supplies delegates to the General Conference on Weights and Measures, which will make the final decision. This high-level conference meets once every four years \u2014 the next being in 2011. Why the rush to this finish line for the kelvin? It is all the kilogram's fault. The kilogram is the last remaining quantity that is defined by a single, physical object \u2014 a lump of platinum and iridium, dubbed Le Grand K, that is held in a BIPM vault on the outskirts of Paris. This has the curious effect that the value of one kilogram, although meant to be constant, almost certainly changes as atoms are added to or brushed off the surface of Le Grand K. No one knows exactly how unstable this value is, of course, as there is no fundamental standard with which to compare it. This is hardly firm bedrock on which to rest all measurements of mass. To solve this problem, various research teams have been seeking to redefine the kilogram on the basis of a universal quality. The most intuitive technique involves making a perfect sphere of silicon-28, such that the number of atoms within the sphere can be precisely determined. Thus a kilogram could be redefined as the mass of a set number of silicon atoms, although in practice this would be almost impossible to achieve because making a perfect enough sphere is tremendously difficult to do. A more practical method involves a 'watt balance', which measures the electromagnetic force required to counterbalance a kilogram under Earth's gravity. The equations describing that electrical force involve Planck's constant, a fundamental parameter of quantum physics. Right now, like the Boltzmann constant, Planck's constant must be measured, but if it were given a set value, the kilogram could be redefined on the basis of that number. To accomplish this feat, metrologists are trying to determine Planck's constant with an uncertainty of less than the instability thought to be associated with Le Grand K. That outdated object, having been used as a standard to set the value of the Planck constant, could then be retired. Both methods are being actively pursued, as checks to one another, and researchers are keen to change the definition of a kilogram as soon as possible as it will stop the 'drift' in its value. As work on the kilogram progresses, metrologists have resolved to tie up all their other loose ends. In 2005, they decided to reconsider the kelvin, the kilogram, the ampere and the mole \u2014 four of the seven 'base quantities' \u2014 the other three being the metre, the second and the candela. These four all have definitional deficiencies and are ripe for updating. The redefinition of the ampere and the mole is linked to the work on the kilogram. But the kelvin stands alone. So thermometry specialists press on with their efforts to tidy up their corner of the metrological universe, even though this work will make little immediate difference to the world. To keep science and industry on an even keel, nothing will change initially except the definition (see  'A temperature for the masses' ). That lack of impact might provoke existential stress for some scientists. \"Sometimes I feel a bit guilty because maybe I should be helping starving children or something,\" says de Podesta. But he says he feels deeply that it really matters that there are people worrying about tiny fractions of a kelvin and about laying a foundation of truth in thermometry. One day, someone will need accurate and precise thermodynamic temperature readings, and having a fixed value for  k B  will help them. \"I'm doing a small thing,\" says de Podesta. \"But I do think it's a good thing for the world.\" \n                 See Editorial,  \n                 page 890 \n               Nicola Jones is a commissioning editor for  Nature 's Opinion section. \n                     BIPM thermometry committee \n                   \n                     Constant dictionary \n                   \n                     Beyond the kilogram \u2014 NIST news report \n                   \n                     Defining units in the quantum-based SI \n                   Reprints and Permissions"},
{"file_id": "459906a", "url": "https://www.nature.com/articles/459906a", "year": 2009, "authors": [{"name": "Emma Marris"}], "parsed_as_year": "2006_or_before", "body": "While conservation biologists debate whether to move organisms threatened by the warming climate, one forester in British Columbia is already doing it. Emma Marris reports. At a research station in the Okanagan valley in British Columbia, a few kilometres outside the town of Vernon, orderly rows of trees run alongside the road. Many of the conifers stand tall and full, producing seeds destined for plantations around the province. But one dusty brown field is filled with lines of seedlings just ankle high. Greg O'Neill, 45, who planted the trees in April, walks among them with a slightly paternal air. They are part of a very slow experiment that will yield the first results that are useful to policy-makers just about in time for him to retire. In a world in which many scientists fret about the toll that global warming is exacting on nature, O'Neill is actually doing something about it. A research scientist for the British Columbia Ministry of Forests, he is moving seedlings to areas that are outside their current comfort zone to test how they might handle the warmer conditions of the future. And he is behind a government push to move tree populations into new areas to prepare them for the warming climate. Some hail the policy as pragmatic and forward-thinking, but others label it as dangerous and premature. Plants moved by humans may become invasive in their new haunts or just fail to thrive. Yet O'Neill thinks the potential gains in terms of timber production and forest health are worth the risk. The Canadian province is a pioneer in the field of adaptation, according to Gerald Rehfeldt, a retired United States Forest Service geneticist based in Moscow, Idaho, who has long championed moving trees. \"British Columbia is ahead of everybody in the western hemisphere,\" he says. O'Neill's experiment is called the Assisted Migration Adaptation Trial (AMAT) and it is what foresters call a provenance trial. The AMAT is taking seedlings from 40 spots in British Columbia, Washington state, Oregon and Idaho and planting them in 48 sites all over those same areas. All told, 16 species are involved in the project (see  Testing assisted migration ). One element of the project tests whether moving trees north will enable them to fare better as the climate changes around them. Another part of the experiment turns the clock forwards on the forests by taking commercially important trees and moving them south, forcing them to endure a warmer climate, quickly simulating years of climate change. O'Neill expects 50% of his seedlings to die, mainly because trees are highly diverse genetically and are often adapted to specific local climates. Take a Douglas fir from the coast and bring it inland, and it may well die within a few years. If it doesn't, the chances are that it will be small, pest ridden or crooked. \"Douglas fir grows from Mexico City to central British Columbia, but move it 700-metres elevation downhill at any location, and you will be growing toothpicks,\" says O'Neill.  \n                Warmer woods \n              A prototype spruce-only provenance trial on an adjacent patch gives a preview of how some of these seedlings might look in five years. A few are about chest height, others are half or a quarter as tall. Some are dead or half dead, their tops dry and bent into a 'shepherd's crook', invaded by white pine weevil. In general, ones from farther away fare worse. Douglas fir grows from Mexico to British Columbia, but move it 700-metres elevation downhill and you will be growing toothpicks. Greg O'Neill  O'Neill sees a similar pattern at older provenance plantings, some now 50 years old. These trials were planted before climate change was an acknowledged problem. The idea was to see if new combinations of populations and areas could lead to better yields \u2014 this works because trees are not always particularly well adapted to their locations. Throughout the province, O'Neill thinks that many trees are already feeling the effects of climate change. \"We see some really ugly trees out there,\" says O'Neill. \"Maybe the pest and pathogen outbreaks we are seeing are compounded by that maladaptation.\" The mountain pine beetle has spread through the province in recent years, turning healthy lodgepole pines a desiccated red colour as it kills them. So far, 14.5 million hectares are affected 1  \u2014 an area larger than Greece. The warming of the climate is widely thought to be a factor in the outbreak \u2014 usually low temperatures in the winter would kill off much of the beetle population. For the Northern Hemisphere, climate models agree that regions in the north will warm more and faster than areas nearer the equator. British Columbia had already warmed 0.7 \u00b0C in the decade to 2006 (ref.  2 ) \u2014 almost as much as the globe has warmed over the past century \u2014 and a middle-of-the-road climate simulation for British Columbia projects another half a degree increase every decade. The province is not alone in facing changes to its forests. One analysis suggests that 88% of the western United States will see a turnover in the kinds of tree communities it hosts by the end of the century 3 . On the other hand, the Intergovernmental Panel on Climate Change predicts that rising carbon dioxide levels will lead to more growth, creating bigger trees and boosting the forestry industry in some regions 4 . If foresters stopped replanting trees, the wind and birds would carry pollen and seeds in all directions, and trees would thrive wherever seeds landed in a spot with a suitable climate. But judging from the results of past provenance tests, says O'Neill, this would be no slow and stately march. It would be more like a scrappy scramble, with few trees doing very well, and individual types moving in different directions at different speeds. For a long time to come, forests would probably look a bit more raggedy, stunted and jumbled. Not ideal for producing timber. The possible changes were brought home to foresters in British Columbia by a 2006 study 2  containing maps of what the climate in British Columbia might look like in 2025, 2055 and 2085. It shows that forestry zones \u2014 which include trees and their environment \u2014 will shift substantially. The 'climate envelope' of trees whose ranges now end in the province will move at least 100 kilometres north a decade. For example, a ponderosa-pine ecosystem, now found only in sunny spots around the Vancouver area, may well spread into the boreal forest in the northeast of the province. But dozens of factors, including the trees' genetic variability, the direct effect of increased carbon dioxide on growth and the complexities of migration will affect whether the trees make it all 100 kilometres. For British Columbia, forecasts of natural tree movement tell only part of the story because most new trees there are planted by people. Of the province's 60 million hectares of forest, 25 million hectares are commercial forests, and when one area is logged, trees must be replanted. To foresters, the projections of future change looked a bit like an instruction manual of what to plant where. \"When they showed that the climates represented by the forestry zones would shift really huge distances, it really hit home much more than saying the climate would warm 4 \u00b0C,\" says O'Neill.  \n                Layers of fear \n              The forest forecasts were just one of the things that scared foresters and policy-makers in British Columbia into action, says O'Neill. Another was the mountain pine beetles. \"People see red trees as far as the eye can see and it becomes easier to implement some rather aggressive policies,\" he says. The final things that crystallized opinion were the forest fires caused by drought in 1998 and 2003. As a result, in April, the province started systematically moving seeds uphill in what ecologists call the first widespread instance of assisted migration. The action came about through a change in the rules governing how timber companies replant. British Columbia, like most other wood-producing areas, has planting policies designed to take advantage of the local adaptation of trees. After companies log an area, they must chose seeds from nearby for replanting. Rules vary by species but, before this year, generally, a seed could be moved only within about 200 kilometres north, south, east or west and 200 metres downhill or up to 300 metres uphill from its origin. The new rule stretches the elevation guidelines so that seeds can be moved up to 500 metres uphill, in accordance with the idea that moving trees substantially higher in elevation will help them thrive in conditions to come. \"These changes encourage planting seed that is adapted to climates about 2 \u00b0C warmer than the planting site, to account for climate change in the past century and changes anticipated in the first portion of the planted tree's lifetime,\" says O'Neill. According to Jim Snetsinger, British Columbia's chief forester, foresters are fans of the change and eager for even more flexibility in policies so that they can chase the climate further. \"They're looking for something bigger,\" he says. Another potential action would be moving species outside their historical ranges. Neither O'Neill in his AMAT nor forestry companies have gone that far; they are moving populations of trees around within their historical range. But O'Neill says the more substantial movement is a natural extension of what he is doing.  \n                Upping the stakes \n              So far, the tree transplanting has happened only with commercial timber. But O'Neill is talking about \"taking it to a new level\" by putting in the odd hectare of southern seed in places such as parks and extremely remote forests in the province, places not likely to be logged and replanted for a long time, if ever. He's calling the idea \"directed diaspora\". When the temperature has risen, O'Neill hopes that the genes to adapt trees to the heat will already be circulating. \"Why not give them a boost?\" he says. But many conservation biologists think that moving species around is a bad idea. Humans have carted plants and animals all over the globe, intentionally and unintentionally, for millennia. Some of these movements have gone spectacularly badly when the 'invasive' species have been successful enough to drive native species to extinction, especially on islands and in lakes. Invasive pine species have swamped the diverse fynbos shrublands of South Africa, sucking up the area's water in the process, for example. The Monterey pine has been on both sides. Its native range includes a tiny sliver of the California coast and a couple of islands off Baja California in Mexico, where it is considered to be threatened by development and by an invasive tree disease. But the pine has itself turned into an invader in Australia, South Africa and several other Southern Hemisphere countries, where it was planted for timber. Daniel Simberloff, an ecologist at the University of Tennessee in Knoxville specializing in invasion biology, calls British Columbia's programme \"a waste of time\". He sees the whole notion of assisted migration as a bandwagon that has lately become chic. He is less worried about moving populations within the historical range of a species than with moving a species beyond its range. But O'Neill's AMAT trial won't be able to inform policy for a long time, says Simberloff. \"He'll be dead before there are any real data from this.\" Meanwhile, Simberloff says, there is just too great a chance that the translocated trees, or the diseases they host, will become invasive. \"I would want to know a lot more about pathogens and insects before I moved things,\" he says. \"There is very little evidence that it is going to help, and in addition there are potential downsides.\" I would want to know a lot more about pathogens and insects before I moved things. Daniel Simberloff  So far, critics have either not noticed policy changes in British Columbia or felt that they have yet to cross a line. \"We haven't heard anything negative from anyone,\" says Snetsinger. Moving trees is also being mooted in Europe. But, Csaba M\u00e1ty\u00e1s, a forestry researcher at the University of West Hungary in Sopron, says he hasn't seen scientific concern spill over into action yet. Nevertheless, he and his European colleagues are pressing for change. And he suggests that the European public won't be very worried about moving populations or species around. \"This is not an issue in Europe,\" he says, \"as this has been done by foresters for centuries.\" Although conservationists are often opposed to moving species, some have started to reconsider assisted migration (also called managed relocation and assisted colonization). Some conservationists see it as the only way to save some slow-moving species from climate change. The idea has been proposed for the California quino checkerspot butterfly and the whitebark pine, whose seeds are an important food for some bear species. But among even the most vocal proponents of this tactic, some think that O'Neill is going too fast. \"This experiment is wonderful, I am glad he is doing it,\" says Dov Sax, an ecologist at Brown University in Providence, Rhode Island. But, he says, the idea of moving seeds into natural forests gives him the shivers. \"The thought of planting a couple of hectares of trees far to the north of where people currently harvest \u2014 that sounds a little scary to me.\" Sax calls for more study, and no precipitous moves. But he admits that his call for caution is as predictable as O'Neill's eagerness to move ahead. \"You can see why people with a commercial interest are going to want to find a solution as quickly as possible. Conservationists, who have seen experiments go awry, are going to be more cautious.\" The nervousness of conservation biologists and ecologists can also be partly explained by the fact that their subjects and, often, their passion, are ecosystems and all the co-evolved interactions that compose them. Although they want to save species from extinction, ideally they want to save species by saving ecosystems. When trees from farther south suddenly appear, the ecosystem has changed, even if no species are harmed. In the end, if O'Neill decides to move trees as he would like, it won't matter much what conservationists decide. Even if his directed diasporas are banned from some tracts of land, those protected areas will still be assailed by the spread of seeds and a haze of lime green pollen drifting from commercial plantings. Emma Marris writes for  Nature   from Columbia, Missouri. \n                     Nature Reports Climate Change \n                   \n                     Climate Feedback \n                   \n                     Evolution & Ecology \n                   \n                     Assisted Migration Adaptation Trial \n                   \n                     Greg O'Neill at Nature Network \n                   Reprints and Permissions"},
{"file_id": "4591050a", "url": "https://www.nature.com/articles/4591050a", "year": 2009, "authors": [{"name": "Geoff Brumfiel"}], "parsed_as_year": "2006_or_before", "body": "Blogs and Twitter are opening up meetings to those not actually there. Does that mean too much access to science in the raw, asks Geoff Brumfiel. Last July, Lars Jensen carried a small shoulder bag of equipment into the atrium of the glass-and-steel conference centre in Toronto, Ontario. Jensen, a bioinformatician at the University of Copenhagen, was one of about 1,400 researchers at the annual Intelligent Systems for Molecular Biology meeting. With him he had the tools of any modern conference attendee: a laptop, a handheld PDA and a digital camera to snap a few photos of his trip. Jensen immediately did what most researchers do: he logged on to the wireless network. He used his PDA to check FriendFeed \u2014 an online social network similar to Facebook that is popular among biologists. \"Anyone else signed up for the 'orienteering' event today?\" Shirley Wu, a graduate student at Stanford University, California, had written on a page that members had already devoted to the conference. \"Nah,\" Jensen typed, \"bloggers don't need icebreaker events ;-)\" Jensen's joke about face-to-face contact proved prescient. Over the next few days, he and nearly 30 other researchers met mostly via their screens in the FriendFeed conference group. During sessions, many group members posted brief comments sent from their laptops or mobile phones to the popular website Twitter, and automatically cross-posted to FriendFeed. Some of these communiqu\u00e9s described a comment from a talk or the flavour of a session. Other posts were links to relevant papers, or photos from around the conference centre. At one point the group even served as a public address system: \"HL33: Session Chair MIA \u2014 if anyone sees Yanay Ofran they may want to point him to the session,\" wrote Shannon McWeeney, a bioinformatics researcher at the Oregon Health & Science University in Portland. The virtual coverage \"was not something that we had arranged beforehand\", Jensen says. But by the time he and others were finished, hundreds of comments had been posted to the group. The information was so complete that Jensen, Wu and the other ring leaders were able to use it to write an authoritative conference summary that was later published (N. Saunders  et al .  PLoS Comp. Biol.    5,    e1000263;  2009). For technophiles and advocates of scientific openness, this is the way of the future. Online groups allow meeting attendees to post and discuss research as it is presented, and follow parallel sessions. They also provide an opportunity for researchers not at the meeting, as well as a far wider community, to actively participate in it. \"I think it is extremely efficient,\" says Jean-Claude Bradley, a chemist at Drexel University in Philadelphia, Pennsylvania. Jonathan Eisen, an evolutionary biologist at the University of California, Davis, adds that 'twittering' about a presentation he is listening to helps him to focus. \"I don't want to screw up and say something that's technically wrong or conceptually wrong,\" he says. But some worry that these tools will undermine meetings. By disseminating scientific results far beyond the lecture hall, blogging and social networking blurs the line between journalists and researchers. Scientists in competitive fields may be more reluctant to discuss new findings if they can be posted on the Internet within seconds. And at a time when many conference attendees are already surfing the web rather than paying attention to the presenter, messaging is yet another annoyance. \"Frankly, it can be a distraction if people are typing on their keyboards in the meeting,\" says David Stewart, the director of meetings and courses at Cold Spring Harbor Laboratory in New York. \n                Blogging without leave \n              Last month, Stewart unwittingly found himself at the centre of the debate about such technologies at Cold Spring Harbor's annual Biology of Genomes meeting. The meeting was oversubscribed, and many researchers were interested in following the proceedings on the web. Cold Spring Harbor streamed video of the talks online for those who had registered and paid a fee, but an informal group of bloggers also began following events. Among them was Daniel MacArthur, a geneticist and author of the blog Genetic Future. MacArthur says that he decided to blog from the conference at the request of a number of people who couldn't attend. \"They asked whether it would be possible to communicate interesting things during the sessions.\" MacArthur's comprehensive postings were read by many scientists but they irked journalists attending the meeting. The meeting rules stated that reporters had to seek permission from speakers before publishing material on their work, rules that Cold Spring Harbor instituted in part because some journals, such as  Nature , discourage scientists from talking to the press before their work is published. But those rules didn't apply to scientist-bloggers like MacArthur and, after he posted details from a few talks, reporters contacted Stewart for clarification on the policies. The complaint was a wake-up call: \"For the first time, I became aware that people were blogging about the data at the meeting,\" Stewart says. Blogging can create much thornier issues for researchers. Many presenters are already cautious about revealing unpublished results at meetings for fear that rivals in the audience might note them down. Now that the note-taking is taking place live and on the web, the speed and distance that information spreads has jumped to a new level. \"With the set-up I have now, I would be able to sit in a conference, take pictures of every slide that is being shown, and it would be on the Internet within seconds, while the talk is still going on,\" says Jensen. This kind of direct-to-web exposure creates problems for many industrial and applied researchers. In the United States, patent applications must be filed within a year of any information becoming available to the public. The exact date of that 'public disclosure' used to be difficult to nail down, but no more, says Michael Natan, chief executive officer of Oxonica Materials, a nanotechnology company in Mountain View, California. In the Internet age, time-stamped photographs of a talk can let competitors know the exact minute a researcher presented a patentable result. Consequently, \"people in industry will be much more circumspect about what they present in public\", he says. Even basic researchers have reason to fret. Last year, a group of theoretical physicists photographed slides from a meeting presentation, extracted the data, and used them in their own analysis, which they published online (see  Nature    455,    7;  2008). In that case, the theorists were given permission by a presenter, and the photographs were properly cited, but the situation illustrates how easily data can find their way into the public sphere. In many fields, competition is so intense that you must conceal to survive, says Natan. For denizens of the blogosphere, these sorts of concerns seem a little out of date. \"I think scientific conferences are about your sharing with the world what you're doing,\" says Francis Ouellette, a researcher at the Ontario Institute for Cancer Research in Toronto, who twittered at the Cold Spring Harbor Meeting. \"Whether or not the participant you're sharing with is in the room is somewhat inconsequential.\" Ouellette and many other active bloggers are also members of the 'open science' movement, which encourages researchers to make their data public as quickly as possible. Bradley sees this openness as a powerful deterrent to anyone hoping to scoop him at a conference because anything cribbed from his talk is already out on the Internet for everyone else to view. \"If someone actually does copy something, I think it would be pretty embarrassing,\" he says, \"it's already there, and it's indexed to Google.\"  \n                The rights of scientists \n              Prompted by his recent experience, Stewart has come up with a pragmatic solution. At future meetings, anyone communicating information to third parties, whether by news story, blog or 'tweet', will now be required to ask presenters beforehand. \"What we really want to do is to protect the rights of individual scientists to present their data in a pre-published form,\" he says. \"I'm not saying don't blog, I'm saying blog by all means, but get permission.\" MacArthur says that the new policies seem like a fairly good compromise, but, he adds: \"I'm hopeful that other conferences will tend to adopt more open policies.\" Conference organizers contacted by  Nature   had a wide range of policies on social networking. Many societies have banned digital photography in talks and poster sessions and some consider bloggers to be members of the media and subject them to certain reporting restrictions. However, almost nobody has developed a policy on when twittering is fair play. \"This has not come up in the past but it may be something we consider in the future,\" says Kevin Wilson, a spokesman for the American Society for Cell Biology in Bethesda, Maryland. Journals are also pondering how best to handle social networking at meetings.  Nature   generally supports social media tools, says Philip Campbell,  Nature 's editor-in-chief. And as long as it's not a deliberate attempt to hype a new finding, he says that researchers should feel free to talk to colleagues who blog or twitter. Ginger Pinholster, the director of public programmes for the American Association for the Advancement of Science, the publisher of  Science , agrees. As long as the scientist wasn't trying to promote his or her work to the public, it wouldn't be a problem. She adds that blogging unpublished results is a problem that \"we just haven't run into yet\". Although Cold Spring Harbor has opted for control, the organizers of the Intelligent Systems for Molecular Biology meeting are choosing total openness. When this year's meeting opens in Stockholm later this week, they are planning to fully embrace social networking tools. FriendFeed entries will be created for each talk at the meeting, and the entries for the keynote sessions will be posted directly to the meeting's main website. That means some people could get the buzz of the meeting without travelling to Stockholm at all. But that's not why Lars Jensen is staying at home this year. The main papers being presented have already been published, he says, so there won't be much new. He might follow the sessions online, but then again, he might catch up the old-fashioned way. \"I have colleagues who are going,\" he says. \"I can always ask them afterwards whether anything interesting happened.\"   See Editorial, page  \n                     1033 \n                   , Essays, pages  \n                     1054 \n                   ,  \n                     1055 \n                    and  \n                     1057, \n                    and online at  \n                     http://tinyurl.com/sciencejournalism \n                   .   Join the online discussion on  \n                     Nature Network \n                   . \n                     Discuss this topic on Nature Network \n                   \n                     Discuss this story on FriendFeed \n                   \n                     Cold Spring Harbor Laboratory \n                   \n                     ISMB 2009 \n                   \n                     ISMB 2008 FriedFeed site \n                   Reprints and Permissions"},
{"file_id": "460164a", "url": "https://www.nature.com/articles/460164a", "year": 2009, "authors": [{"name": "Helen Pearson"}], "parsed_as_year": "2006_or_before", "body": "When the cystic fibrosis gene was found in 1989, therapy seemed around the corner. Two decades on, biologists still have a long way to go, finds Helen Pearson. During the day, Lap-Chee Tsui and Francis Collins were attending a gene-mapping workshop. At night they were scrutinizing the pages churning out of a fax machine they had set up in a dorm room. Their hunt for the cause of cystic fibrosis had reached a gene that looked from its sequence like it might have a role in transporting ions through cell membranes, a process that goes awry in those with the disease. The fax they received that night from Tsui's lab showed that many people who have cystic fibrosis lack three base pairs from both copies of this gene, whereas those without the disease always have at least one copy intact. With that fax, on a rainy night in May 1989, \"I was convinced \u2014 that was the moment,\" Collins says. Four months later a four-year-old boy with cystic fibrosis, Danny Bessette, was shown sitting cross-legged on the cover of  Science , framed by a rainbow of chromosomes. Inside the magazine, three papers 1 ,   2 ,   3  laid out the details of the discovery of the gene responsible for Bessette's condition \u2014 the first gene for a human disease discovered without the help of an already-known protein sequence or any clue to its whereabouts. \"In this issue \u2026 there is a story that does not begin at the beginning or end at the end, but has a very happy middle,\" wrote  Science  's editor Daniel Koshland 4 . \"One in 2000 children born each year with a fatal defect now has a greater chance for a happy future.\" By that stage, news of the finding had already leaked to the media, been the subject of two hastily assembled press conferences and been trumpeted in newspapers worldwide. \"It would be difficult to overstate the importance of the cloning of the cystic fibrosis gene,\" wrote geneticist Peter Goodfellow in  Nature   that month 5 . \"The implications of this research are profound: there will be large spin offs in basic biology, especially in cell physiology, but the largest impact will be medical.\" So far, Goodfellow's prediction has proved wrong, at least as far as medical impact is concerned. As Jack Riordan, who collaborated with Tsui and Collins on the original discovery, puts it: \"The disease has contributed much more to science than science has contributed to the disease.\" This is not to deny that medical progress has been impressive. An American born with cystic fibrosis today has a life expectancy at least ten years longer than one born in 1989 did. Such advancements help explain why Bessette \u2014 now 24, and pictured opposite \u2014 has a future at all. But many researchers concede that relatively little of that improvement can be laid at the door of the cystic-fibrosis transmembrane regulator gene, or  CFTR . Gene therapy \u2014 the source of so much of the hope in 1989 \u2014 has so far bought no one with this condition a single additional year of life; no therapies targeted at the CFTR protein have yet been approved. Researchers have not even fully agreed on a hypothesis to explain how mutations in the gene cause the condition. But the gene itself \"found its way into all departments\", says Riordan, leading to progress in fields as diverse as protein trafficking and membrane transport. And the gene-hunting techniques that Tsui, Collins, Riordan and their colleagues pioneered have laid the foundation for a genetic understanding of all human disease. Twenty years, although a long time in the life of a young man such as Bessette, is not the whole story. Several hundred million dollars have been spent trying to find a therapy that directly tackles the molecular defects that underlie cystic fibrosis; Collins, for one, thinks that this means the hopes on which gene therapy never delivered are about to be fulfilled. Like many researchers, he is excited by clinical results coming through on a pair of small molecules that could get mutant versions of the CFTR protein to work properly. Should the molecules be approved, \"it will be a pair of home runs, a milestone for all genetic disease\", Collins says. And those home runs would never have been hit without the gene and the opportunity to study the protein that needs fixing. \"You can paint a direct pathway from the gene discovery [to those drugs],\" he says. To call the path direct might be overstating it. Researchers have taken many paths from  CFTR , and their travels have shown that behind this gene and every one found since lie dauntingly complex biological stories. \"I think one of the lessons of cystic fibrosis is the recognition of the enormous challenge that faces us in human biology,\" says Riordan, now at the University of North Carolina, Chapel Hill. \"It's not like going to the Moon \u2014 it's going to Mars.\" The size of the challenge can sap enthusiasm. \"Looking back, it was an important contribution,\" says Tsui, \"but I'm disappointed because at this time, from my own research, I was not able to help very much.\" Riordan says that he now views \"the latest hot gene\" with a \"jaundiced eye\". But one thing that shines through when speaking to these three and other researchers is their continued optimism, their passion and their sense of urgency. \"Perhaps,\" says Collins, \"we've taken our blinkers off. Perhaps we couldn't deal with it before, and now we have a lot more tools to dissect the complexity.\" \"It's not that it hasn't worked,\" says Riordan.\"It's only been 20 years.\"  \n                Blind beginnings \n              Geneticists have been interested in cystic fibrosis since the disease was first identified in the 1930s. The disease is common in Caucasian populations \u2014 about 1 in every 25 people carries a mutated copy \u2014 and its pattern of inheritance is straightforwardly Mendelian: those with one mutated gene are healthy carriers; those who inherit two will have the condition. Doctors knew that although the pancreas often fails and the gut is unable to absorb nutrients, the lung is the organ that is crippled with recurrent and persistent infections, \"and that's unfortunately the one that kills them\", says Richard Boucher, a pulmonary physician and cystic fibrosis researcher at the University of North Carolina. But for decades no one knew exactly what was wrong with the cells, so no one knew what type of gene to look for. Paul Quinton helped change that. As a kid, Quinton had always coughed a lot, and his sweat was so salty that his clothes corroded the wire hangers they dried on. When, in 1965, as a 19-year-old at the University of Austin, Texas, he met a girl and his thoughts turned to marriage, he decided to find out what was wrong with him. The description of cystic fibrosis he found in the medical library fit his symptoms perfectly and he diagnosed himself with a disease that should already have killed him, but that he would spend the rest of his life studying. Quinton collected fresh sweat glands from visitors, from colleagues (Riordan, who visited Quinton's ranch, says he still bears the scars of Quinton's biopsies with a cork borer) and from other people with cystic fibrosis to explore why his sweat, and that of others with the disease, was so salty. In 1982, while working at the University of California, Riverside, an experiment measuring the ability of sodium and chloride to pass through the glands led him to finger a channel that was unable to conduct chloride ions across the epithelium of the skin, and that might also underlie problems in the lungs and the other affected organs 6 . \"I feel silly saying it but I literally jumped up and ran up and down the hall shouting 'Eureka',\" says Quinton, who now also works at the University of California, San Diego. \"I still get chills; it was one of those moments you get once in a lifetime.\" The disease is evident when he speaks: he still clears his throat and coughs a lot. Quinton's discovery and others like it told geneticists what they should be looking for: a gene that is involved in the movement of chloride, and perhaps other ions, across the epithelium. By now an intense and competitive hunt was under way. It was the 1980s, when the human genetic sequence was largely uncharted territory, and the human genome project was still a twinkle in various eyes, including Collins's. Finding the gene would be a technical and intellectual challenge as well as a medical breakthrough. Until that point, almost all of the genes that had been associated with human diseases had been identified by first isolating the protein responsible. A protein's amino-acid sequence reveals much of the gene's probable nucleotide sequence, and that made pinpointing the gene easier. The few exceptions, such as those found for Duchenne muscular dystrophy and retinoblastoma, were helped by a few patients with chromosomal abnormalities that pointed to the gene's position. For cystic fibrosis, researchers were working blind: they had no protein and no location. This was to be a big test of new 'reverse genetics' techniques, in which a gene is found by searching for markers in the genome that are consistently inherited with the disease in affected families and using them as signposts to the gene itself. Tsui, then at the Hospital for Sick Children in Toronto, Canada, and now at the University of Hong Kong, was a key player in the hunt; so were Robert Williamson at St Mary's Hospital Medical School in London, and a handful of other researchers. By 1985, several groups 7 ,   8 ,   9  had shown that the gene mapped to a region of chromosome seven, but it was still a vast genetic wilderness somewhere between one and two million base pairs wide. In 1987, Williamson announced that he had landed on the gene, but soon after had to admit he had got it wrong. Nevertheless, many groups assumed that Williamson was close and dropped out of the race at that point. Says Collins: \"Lap-Chee and I were more stubborn\". Collins, then at the University of Michigan in Ann Arbor and until last year the head of the National Human Genome Research Institute in Bethesda, Maryland, met Tsui at that year's meeeting of the American Society of Human Genetics. A few years previously, Collins had described the technique of 'chromosome jumping', a way of leaping across the vast genetic distances from one marker sequence in a region to another that was much faster than the conventional way of chromosome 'walking' 10 ,   11 . They agreed to collaborate: Collins's lab would bound to new positions, and Tsui's would walk forwards and backwards from the landing points looking for the gene. Two years later, on that rainy night in the dorm room, their fax machine told them they had found it. The  Science   papers showed that the gene looked like others encoding membrane proteins that transport ions. The three base pairs missing in the vast majority of people with cystic fibrosis eliminated an amino acid at position 508 of the protein's amino-acid sequence, a mutation called \u2206F508. \"It was exciting times,\" says Robert Beall, then executive vice-president for medical affairs at the Cystic Fibrosis Foundation in Bethesda, Maryland, and now its director. \"We had been at a bottleneck. We didn't know why chloride wasn't getting out of cells, and that gene solved it.\" The scramble of competition continued as researchers rushed to work with the gene. John Hanrahan at McGill University in Montreal, Quebec, recalls the time he was collaborating with Riordan on a paper for  Cell 12 . Riordan called him to ask him to fax through a figure for the manuscript as he was worried about a scoop from a competing paper at  Science . \"I raced to the airport in a snowstorm to send the originals by same-day courier, but it was the faxed version that went to press,\" Hanrahan says. \"When people look at the traces they must wonder, 'Why are they so pixelated?'\" But Hanrahan, like most researchers, says that the competition was a healthy one, even if it deprived them of a little sleep. \"I think a lot of data were published and some mistakes were made, but there was tremendous excitement and the field moved ahead rapidly.\" From the beginning, the goal was gene therapy. Get a good gene into the patients and they would make the proper protein; with the proper protein they'd be cured. But the path from gene to therapy wasn't smooth. It took more than a year just to get bacteria to produce the protein from the cloned gene, because of 'cryptic' sequences within the gene that prevented the bacteria from expressing it. But by 1993 the first clinical trials were under way. \"The expectation was that all you needed to do was get a little bit of stuff to act in the lungs and 'hey presto' you'd have a Nobel prize,\" says Steven Hyde, who works on cystic fibrosis gene therapy at the University of Oxford, UK. Among other things, the lung, researchers now realize, is just about the worst possible target for such an approach. Its sophisticated defences against infection have evolved precisely to prevent the sort of uptake and expression of foreign material the gene therapists were after. Mike Welsh at the University of Iowa and his colleagues, who in one of the first trials pushed the gene into cells in the nasal passage as a surrogate for those in the lung, later realized that the cells that had taken up the gene were probably damaged during the procedure. \"A whole slew of people did similar trials and everyone got a little disillusioned.\" Hyde says. Disillusionment isn't enough to kill off an idea \u2014 but death is. In 1999, a severe immunological reaction killed Jesse Gelsinger in a gene-therapy trial for an inherited liver disease, casting a pall over the entire field. In the United States, the field has never fully recovered. In other countries \u2014 the United Kingdom and France, for example \u2014 researchers have been much more active in pursuing the technique. Around the same time, Beall decided to turn the gene into a way to find a therapy, rather than being the therapy itself. He wanted to take advantage of new tools coming online for high-throughput drug screening. Researchers inserted the gene into cells, expressed the mutated protein, then screened for drugs that could correct the way the protein is made or the way it works. \"People thought we were crazy,\" Beall says. What started as US$2-million grant in 1999 has turned into an $76-million programme, and Beall proudly points to a chart showing the drugs working their way through the pipeline as a result. By far the most common mutation is \u2206F508. It causes the protein to fold up poorly, and a drug known as a corrector is needed to help it fold correctly and get to the membrane it needs to sit in. Other mutations \u2014 there are now more than 1,500 known in the gene \u2014 require different approaches. Versions of the gene in which protein translation stops short need drugs to override the stop signal. Then there are proteins that get made, fold up and reach the membrane but just don't work properly. They need what are called potentiators. In March 2008, investigators presented results from a phase II trial of the potentiator VX-770 to a room of several hundred researchers at a meeting of the Cystic Fibrosis Foundation. Just two weeks of treatment in 20 people with a rare mutation called G551D had dramatically lowered some people's sweat chloride and produced some improvement in lung function \u2014 something that clinicians found particularly remarkable given the battered state of their airways. \"When they showed those data and I saw the emotions from those physicians, it was unbelievable,\" Beall says. \"It was the most emotional time since the discovery of that gene. It's telling you we can change the course of this disease.\" Collins agrees. \"It was wildly better than even the most optimistic perspective for a small-molecule trial,\" he says. Phase III trials of VX-770, developed by Vertex Pharmaceuticals of Cambridge, Massachusetts, are now recruiting patients. Beall and others say that the drug might find a much wider market if it is also used in people with other mutations, including \u2206F508, in conjunction with a corrector. That corrector could be another Vertex drug called VX-809, which is just starting phase II trials.  \n                A special case \n              Must it take 20 years to get from gene to drug? No. Various things have made cystic fibrosis peculiarly difficult. One has been a lack of a complete understanding of how the CFTR protein leads to the disease. Many think that the defective channel causes the lungs to absorb too much water; others have argued that the primary problem is an incorrect ion composition that disables the lungs' normal defences against infection. This debate became so fierce it was described as the 'salt wars'. At least part of the problem seems to lie in another ion channel that CFTR interacts with. \"If you ask 20 people you'll get 20 different hypotheses,\" says Welsh. \"Everybody's got their favourite \u2014 I think we don't know.\" Then there are some purely technical problems. Mice with mutated versions of  CFTR   have few obvious lung problems and thus make poor models of the disease. (The models have, however, revealed something about why the mutated gene is so common \u2014 see  'A killer advantage' .) The CFTR protein is huge and is embedded in a membrane, making its structure difficult to determine with X-ray crystallography; plus the fact that airway cells tend to contain only a hundred or so copies of the protein, so there is very little of the stuff to play with. Together, these mean that no one has been able to resolve a complete high-resolution structure for the protein, which has hampered understanding of how it works and the design of drugs. Other genes have had it easier. Collins points to the gene for Hutchinson\u2013Gilford Progeria Syndrome (HGPS), an extremely rare single-gene disease that causes young children to shows signs of old age. The gene was discovered by Collins's team at the National Human Genome Research Institute in 2003 (ref.  13 ) and by another group in France 14 , and a treatment based on it went into a phase II clinical trial in 2007 \u2014 a notably fast pace of translation. Collins puts much of the speed down to serendipity. The mutated protein was an extremely well-studied one called lamin A, and a cancer drug that had already reached late-stage clinical trials was found to work against the mutated protein, saving some laborious drug screening and safety testing. What's more, the task required of the drug is simpler. Drugs for cystic fibrosis have to compensate for or restore the function of a mutated protein, whereas those for HGPS simply have to block the action of one that has turned toxic. The discovery of  CFTR   deserves at least some credit in the HGPS story, though, as it does for accelerating the pace of translation after almost every gene discovery since 1989. That's because hard work and mistakes made in this field have saved effort in every other. \"If you found a new gene tomorrow you could compress those 20 years hugely because of what's been done with cystic fibrosis,\" says Hyde. And gene therapy may yet prove possible for cystic fibrosis. In 2001, the Cystic Fibrosis Trust in Bromley, UK, asked Hyde's group and two others in Britain that were still working in the field to stop competing and start working together. They complied and have spent several years and around \u00a330 million (US$49 million) working methodically through some of the problems \u2014 such as devising better ways to measure changes in lung function. Earlier this year, researchers at Imperial College London treated the first of 27 people with cystic fibrosis in what is expected to become the largest gene-therapy trial ever undertaken for the disease. The aim is to test whether the gene can be delivered safely, in a fatty particle called a liposome. If it is, the researchers will scale up to a 100-person randomized controlled trial to see whether it is effective. \"I think we have now tempered the optimism of the early 90s with a heavy dose of realism,\" says Eric Alton, who directs the trial.  \n                Clinical changes \n              Throughout this time there have been dramatic changes in the way that cystic fibrosis is treated in the clinic. In 1994, Genentech introduced Pulmozyme (dornase alfa), an enzyme that breaks up some of the lung-clogging mucus that encourages infections. A few years later, aerosolized antibiotics were introduced to fight these infections more aggressively. Earlier this decade doctors in Australia started noticing that their patients who surfed felt better during the surf season \u2014 leading researchers to test the idea that the daily inhalation of super-salty water, called hypertonic saline, could help lubricate the lungs. It did 15 ,   16 , and this is now standard therapy for many patients. Not all of them benefit from this approach, though: Bessette stopped taking hypertonic saline after a few years because it made him cough blood from a burst vessel in his lung. Pulmozyme does feature in his 40\u201350 pill-per-day regime, and he anticipates more advances that might improve his lung function. \"Yes, we all hope for a cure, but if they can just help us stay healthy that in itself is quite an accomplishment,\" Bessette says. Quinton, too, follows a rigorous regimen, inhaling hypertonic saline every day and taking intravenous antibiotics every few months. He rides his bike to work, but he can't run far or play basketball. Both upper lobes of his lung have been removed because of chronic inflammation. For someone born when he was, though, things could have been much worse \u2014 and thanks to research into the  CFTR   gene, Quinton knows why they're not. Although he has one copy of \u2206F508, the mutation in his other gene, R17H, has relatively mild effects. He found this out when, several years after the gene was found, Garry Cutting at Johns Hopkins University School of Medicine in Baltimore, Maryland, analysed his genes as part of work on genetic testing for the disease. These tests are \"probably the most common form of genetic testing in the world today\", says Cutting, who now drafts clinical-testing guidelines for cystic fibrosis. In the United States and some European countries many pregnant women and their partners are offered testing for mutations in  CFTR , forcing clinical geneticists to confront issues about genetic counselling and genetic risk that are likely to escalate as more and more genes become as well studied. Working with a gene that has so many mutations, most of which are still little understood, underlines the futility of testing for something with no known clinical severity and therefore no rational basis on which to make decisions about ending a pregnancy. \"The agony I've seen for some couples where one is a carrier and one has a mutation of unknown significance,\" Cutting says, \"it is just immense.\" Newborn screening, which is also commonplace in some countries and typically involves a biochemical test followed by a genetic one, throws up similar issues for clinicians who may be unable to advise parents how severely their child is likely to be affected. New studies are making the molecular landscape look even more complicated. Two years ago, the Cystic Fibrosis Foundation helped to launch a North American consortium to search for 'modifier genes' at work in the disease that might explain why some people with two copies of the \u2206F508 mutation die at 16 whereas others have pretty healthy lungs into their 20s. The consortium members recently screened more than 4,500 people to look for genetic variations that are strongly linked with severity of the disease, says consortium member Michael Knowles from the University of North Carolina. One of the strongest variants to have emerged from previous studies of modifier genes, called TCF7L2, is also thought to strongly predispose carriers to type 2 diabetes 17 . The link may lie in the failure of the pancreas and consequent diabetes that cystic fibrosis frequently causes. Results such as these suggest that once the  CFTR   gene and its protein are viewed in context, cystic fibrosis will spiral into a new realm of dizzying complexity. If studies of one gene have expanded to fill 20 years, how many years can be filled once the tens or even hundreds of modifier genes are factored in, let alone whatever other influences there may be outside the genetic code? For Knowles, though, the results present an exciting opportunity rather than a daunting complexity. He sees cystic fibrosis as \"leading the way\" for researchers investigating more genetically complex diseases. If he and others can get to grips with the numerous mutations in  CFTR   and its modifiers, they say that cystic fibrosis could serve as a case study for personalized medicine. Newborns identified with the disease could have their  CFTR   gene and other major modifier genes analysed to choose the most appropriate therapies \u2014 assuming, that is, that such a range exists by that point. Although he has discovered molecular truths about himself that he might never have expected, \"it would be hard for me to say I have benefited from the work I've done\", Quinton says. Nonetheless, he, Riordan and others whose careers in this field stretch back farther than 1989 are still hopeful. Like most researchers and clinicians, they are focused on what they can achieve in the next 2\u20135 years, not what they should have achieved already. \"I'd say don't give up,\" Quinton says. \"This really is the only solution. As we succeed on one platform, it will make it much easier to succeed on another.\" \"It's a helluva lot more complicated than we realized,\" he says. \"We went to the Moon in '69 and the conceit was we could do anything \u2014 we corrected polio, we wiped out smallpox. But when you start taking the system apart we've been really naive.\" \"But that's biology \u2014 it's not fair.\" \n                     Insight: Human Genomics and Medicine \n                   \n                     Cystic Fibrosis Foundation \n                   \n                     Cystic Fibrosis Trust \n                   \n                     National Human Genome Research Institute \n                   Reprints and Permissions"},
{"file_id": "460029a", "url": "https://www.nature.com/articles/460029a", "year": 2009, "authors": [{"name": "Jeff Tollefson"}], "parsed_as_year": "2006_or_before", "body": "With their focus on greenhouse gases, atmospheric scientists have largely overlooked lowly soot particles. But black carbon is now a hot topic among researchers and politicians. Jeff Tollefson investigates. Steve Warren spent his spring break island-hopping with a couple of friends, but they didn't go to bask in the sun. Instead, his team from the University of Washington in Seattle toured the Canadian Arctic, digging pits in the snow and collecting hundreds of samples to take back to the lab. The targets of their expedition, hidden in all the whiteness, were specks of something called black carbon. These dark particles, the major constituents of soot, are the legacy of incomplete combustion in diesel engines, coal power plants, agricultural burning and wildfires far to the south. Prevailing winds sweep black carbon and other pollutants into the Arctic, where they circulate in a dirty yellow haze until storms wash them out of the air. Warren's team was collecting those that fell among the snow flakes. The aerosol haze has long plagued the Arctic, but scientists are only now taking stock of a different and potentially uglier dimension of soot. As its name would suggest, black carbon absorbs sunlight. These particles heat the atmosphere while aloft; when they settle on the snow, they hasten its melting. This exposes the dark land and water, which absorb more of the sun's energy and thereby drive up the region's temperature. Recent research 1  suggests that black carbon could be responsible for a large fraction of the Arctic warming. Soot also takes a toll elsewhere. In southeast Asia, studies suggest 2  that it is choking the moisture supply for the Indian monsoons and contributing to the retreat of mountain glaciers that provide fresh water for more than a billion people. At this point, scientists lack enough data to definitively conclude how strongly black carbon is affecting the climate. But some studies suggest that it could be second behind carbon dioxide in terms of its contribution to global warming. There is a crucial difference between the two pollutants, however: soot particles hang in the atmosphere for just a few weeks, whereas CO 2  molecules can remain in the air for centuries. This means that efforts to curb soot emissions could reap immediate climatic benefits. That possibility has recently pulled soot, which has conventionally been seen as a public-health issue, into the climate-policy arena. \"There's an urgency about this: we still don't have a viable way of cutting down CO 2 ,\" says Veerabhadran Ramanathan, an atmospheric scientist at Scripps Institution of Oceanography in La Jolla, California. By comparison, reducing soot emissions seems remarkably simple and cheap. \"It's not going to take 30 or 100 years to do it. If you halt the black carbon now, it will be gone in two weeks.\"  \n                Hazy data \n              Long before the current interest in black carbon, an accidental observation by Warren led him to do some pioneering work on the pollutant. In 1980, he and Warren Wiscombe of the National Center for Atmospheric Research in Boulder, Colorado, were having trouble developing a mathematical model of snow reflectance, or albedo. The two couldn't make their calculations align with the latest albedo measurements in the Arctic because the snow in their study was reflecting less light than expected. \"It turned out the snow was being collected downwind from a diesel generator,\" Warren says. Warren went on to collaborate with Antony Clarke at the University of Washington, who organized the first survey of black-carbon deposition in the Arctic, largely using samples collected by researchers who were going there for other reasons. On the basis of those data, Clarke concluded in 1985 that soot could have a measurable effect on the Arctic climate 3 . But his paper had little influence until Jim Hansen, the man known for alerting the world to the threat of CO 2  pollution, pressed the issue years later. In 2000, Hansen, director of New York's Goddard Institute for Space Studies, proposed 4  that the quickest way to combat global warming was to reduce black carbon, methane and other powerful warming pollutants that could be controlled more easily, and to greater effect, than CO 2 . Other studies soon followed. Building on earlier research on Californian smog, Mark Jacobson of Stanford University, California, reached a similar conclusion the following year 5 . He has since become a leading advocate for curbing black-carbon emissions and was invited to make his case before a congressional committee chaired by California Democrat Henry Waxman in 2007. Waxman, who is now chairman of the House energy and commerce committee, took the message on board. In a comprehensive climate bill that was passed by the House of Representatives last week, he calls on the US Environmental Protection Agency to analyse potential black-carbon strategies. Jacobson calculates that humans have pumped enough pollutants into the air to warm the planet by about 2\u00b0C. More than half that effect, however, is masked by other aerosol pollutants \u2014 including sulphates and light-coloured soot particles \u2014 which cool the planet by reflecting sunlight and seeding clouds. Jacobson estimates that altogether, the climate has warmed 0.75\u20130.85\u00b0C and black carbon is responsible for 0.25\u00b0C of that. \"In other words, you could control up to 30% of global warming if you could control soot,\" he says. Given that black carbon has its strongest effect in the Arctic, he suggests that such a strategy could slow sea-ice retreat until international controls on greenhouse gases kick in. \"It's the only mechanism that you have,\" he says, \"in the absence of putting in a bunch of refrigerators\" (see 'Stopping the soot'). A recent study of the Arctic supports Jacobson's assessment. Drew Shindell, a modeller at the Goddard Institute, recently used a coupled ocean\u2013atmosphere climate model to reconstruct twentieth-century influences on climate, or forcings, with and without black carbon. His results suggest that increases in black carbon from Asia and reductions in sulphate pollution have caused about 45% of the observed warming in the Arctic 1 . In Ramanathan's global assessment 2 , the forcing from black carbon equals 0.9 watts per square metre, which is more than the forcing from methane and 55% of that from CO 2 . These numbers are much higher than the estimates produced by the Intergovernmental Panel on Climate Change (IPCC) in its 2007 report. The IPCC pegged the direct influence of black carbon \u2014 not including its ability to speed the melting of ice \u2014 at 0.05 to 0.35 watts per square metre (ref.  6 ). Overall, the effect of all aerosols \u2014 both the heat-absorbing black carbon and the reflective light-coloured ones \u2014 produces a net cooling of 0.5 watts per square metre, according to the panel. That's enough to offset roughly one-third of the warming driven by CO 2 , but the estimate comes with a huge uncertainty range of 0.1\u20130.9 watts per square metre. It also does not include the indirect effect that aerosols have on cloud particles. Last month, Gunnar Myhre, a climate modeller at the Center for International Climate and Environmental Research in Oslo, attempted to lessen the uncertainty. He compared global aerosol models with observations derived from NASA's ground-based AERONET system, a network of passive aerosol sensors, as well as those from the MODIS (Moderate Resolution Imaging Spectroradiometer) instruments on NASA satellites. After accounting for historical emissions and tweaking the way his model handled clouds, Myhre produced a new estimate 7  of 0.3 watts per square metre of cooling and halved the IPCC's uncertainty range. In explaining his lower estimate for net cooling, Myhre notes that black-carbon emissions have increased roughly sixfold in the industrial era, whereas reflective aerosols have only increased three- or fourfold.  \n                Questions in the clouds \n              Although few doubt that black carbon has a warming effect, particularly in icy regions, some scientists are concerned that the policy debate is getting in front of the science. Uncertainties abound about everything from current and historical emissions to the actual chemical and physical processes that are driving black carbon's influence on snow and in the atmosphere. For his part, Hansen says that there have been some \"excessive claims\" about black carbon. He calls greenhouse gases \"the predominant cause\" of global and Arctic warming and believes methane remains the number two forcing agent. At the same time, he says soot's effect is amplified over snow, which means the numbers put forth by Shindell cannot be ruled out. \"Assuming greenhouse gases are reined in,\" he says, \"other things that we can do such as reducing black-carbon emissions will be useful.\" \n               boxed-text \n             All this uncertainty has put a premium on solid information, which is why Warren and his colleagues set out in 2006 to conduct the first survey of black-carbon deposition in the Arctic. Since then, they have collected samples in Siberia, Greenland and Alaska, as well as at the North Pole. In their most recent trip in April, they hired a ski plane in Inuvik in Canada's Northwest Territories and flew east, setting down on lakes, tundra and sea ice as well as on fields of permanent ice. Each of the team's sample pits captures an entire season of snow, providing clues about when different kinds of black-carbon pollution were deposited. They plan to feed this vast new resource to climate modellers, who are still trying to sort out ways that black carbon could affect temperatures. The samples that Warren's team have collected tell only part of the story. Researchers have also used instruments on the ground, on planes and on satellites to measure soot in the air. But the task is particularly difficult because soot's darkness makes it hard to detect. NASA hopes to get a better fix on black carbon around the globe when it launches the Glory satellite next year. A passive aerosol sensor will scan the atmosphere using seven wavelengths from visible light to short-wavelength infrared. It won't be able to pinpoint the amount of black carbon at specific altitudes, but it should be able to measure the total concentration from the ground up with an accuracy of 3%, says the Goddard Institute's Brian Cairns, who designed the instrument. Other clues to black carbon's influence are coming from deep below the surface of Greenland, where ice layers record how much soot wafted around the Arctic each year since before the industrial revolution. In 2007, Joe McConnell, a hydrologist at the Desert Research Institute in Reno, Nevada, measured black-carbon concentrations within an ice core from central Greenland dating back to 1788. He found a sevenfold increase between 1850 and about 1910, particularly in winter. Soot levels fell after that until about 1950 and then largely remained close to pre-industrial levels 8 . These data, and findings from a second Greenland core, seem to align fairly well with estimates of historical emissions in North America, a likely source of the soot. Taken at face value, it is difficult to match up the Greenland records with the contention that black carbon is now a major player in the Arctic. Arctic temperatures have surged in recent decades, when black-carbon levels have all but returned to historic levels. McConnell cautions against reaching too strong a conclusion about black carbon on the basis of his two published cores. And he has already started analysing another pair of ice cores from Alaska. The amount of soot in snow depends both on how much is emitted and on how it gets transported. There is no way to differentiate changes in one from the other on the basis of just a few sites.  \n                Primed to melt \n              Arctic circulations might have shifted at some point, or Greenland may simply not be representative of the entire Arctic. Variations on both arguments have been used to explain the apparent discrepancy. Another possibility is that the Arctic is more sensitive today than it used to be: decades of warming may have primed the climate system for early spring thaws. \"If you melt the snow just a few days earlier, that's quite a dramatic effect already,\" says Andreas Stohl, a researcher at the Norwegian Institute for Air Research in Kjeller who has been studying how black carbon gets into the Arctic. \"The same is true for sea ice. It may be melting anyway, but black carbon can cause more melting and earlier melting. That has quite a big effect.\" Last year Stohl led a project called POLARCAT, which involved using aircraft and satellites as part of an intensive mission that sampled Arctic aerosols. The project documented a number of smoke plumes from agricultural activities and open burning in Siberia. Such findings support data collected by Warren and his colleagues, suggesting that black carbon from fires \u2014 rather than industrial sources \u2014 has a dominant role during the melting season in the Arctic 9 . Those results, which surprised some researchers, could help explain how reduced levels of black carbon today could nevertheless have a strong influence on temperatures. Agricultural fires rarely happen during the dark Arctic winter, when black carbon has little effect; they tend to occur during the spring and summer, when the dark soot particles are particularly effective at melting snow. The story is different in southeast Asia, where China and India are in the midst of rapid development that combines industrial-scale fossil-fuel burning with extensive use of coal, wood and crop residues for home heating and cooking. The lowly cooking stove may contribute about 40% of the black-carbon pollution in China and about two-thirds of the total in places such as India, Pakistan and Bangladesh, says Ramanathan. Surabi Menon, a researcher at Lawrence Berkeley National Laboratory in California, was among the first, with Hansen, to dig into the effects of black carbon in southeast Asia. Using general circulation climate models to test the effects of aerosols in 2002, they found that black carbon and other particles could help explain droughts in northern China and flooding in southern China 10 . Menon has recently finished some modelling indicating that black carbon could be responsible for roughly one-third of the glacial retreat in the past two decades. Ramanathan took things a step further in March 2006, leading a team that sent a trio of unmanned aerial vehicles to sample black-carbon levels above the Indian Ocean 11 . Building on these results, he suggests that the Himalayas are hit twice: in addition to the direct effect on glaciers, black carbon and other aerosols also reduce snowfall by inhibiting the Indian monsoon 2 . One effect is that black carbon absorbs sunlight high in the atmosphere over the Indian Ocean, which shades the ocean surface and cuts down on evaporation. At the same time, it cools the continent enough to weaken the monsoonal winds from the Indian Ocean. Overall, those changes contribute to glacial retreat by denying precipitation that would otherwise replenish the snowpack each year. Ramanathan now heads a consortium involving scientists from countries such as India, China, Japan and South Korea that is trying to document effects in the Himalayas, the source of the region's fresh water. The team is even detecting strong signs of soot around Mount Everest. \"We're finding as much black carbon at altitudes of 3\u20136 kilometres as you are finding in downtown Los Angeles,\" he says, suggesting that the problem seems to grow the more he looks at it. Ramanathan is already convinced of the need to act on black carbon, although he realizes that effecting change in villages and cities across southeast Asia won't be easy. \"Politically, of course, it's a tough issue,\" he says. \"Thank goodness I'm not a politician.\" \n                 See Editorial,  \n                 page 12 \n               \n                     Nature Reports Climate Change \n                   \n                     Nature Geoscience \n                   \n                     Steve Warren \n                   \n                     V. Ramanathan \n                   \n                     Joe McConnell \n                   \n                     Mark Jacobson \n                   \n                     Jim Hansen \n                   \n                     Surabi Menon \n                   \n                     Glory mission \n                   \n                     POLARCAT mission \n                   Reprints and Permissions"},
{"file_id": "460025a", "url": "https://www.nature.com/articles/460025a", "year": 2009, "authors": [{"name": "Ed Gerstner"}], "parsed_as_year": "2006_or_before", "body": "Slotting a fusion reactor into the heart of a nuclear fission plant could accelerate the development of waste-free nuclear energy. So why are all the designs still on paper, asks Ed Gerstner. It seems like such a natural fit. Nuclear fission has proved that it can produce greenhouse-gas-free energy: the roughly 440 nuclear plants operating in 31 countries around the world collectively have the capacity to generate some 370 gigawatts of electrical power, or about 15% of the global total. But fission power also produces a stream of radioactive nuclear waste, laced with potentially bomb-grade plutonium \u2014 some 12,000 tonnes of waste per year, worldwide. That's quite a disposal problem. Thermonuclear fusion, meanwhile, promises to generate an even greater supply of clean energy. But so far no human-induced fusion reaction has produced more energy than was used to fire it up. What fusion does generate, however, is neutrons. And therein lies the fit: fusion's neutrons could burn up fission's waste almost completely, leaving a residue greatly reduced in both volume and radioactivity. So why not combine the two into a fusion\u2013fission hybrid reactor, and let each technology solve the problems of the other? There are several reasons why not. One is that nobody knows how. Building a fusion reactor is complicated enough without trying to build one inside a nuclear fission reactor \u2014 which is why hybrids currently exist only as designs on paper. Another is the unquantified risk factor: a hybrid reactor would almost inevitably put a potentially unstable thermonuclear plasma next to radioactive fission products. Still, a growing number of researchers around the world are convinced that it is worth tackling those challenges \u2014 because pure fusion power isn't getting much closer, and the piles of fission wastes are mounting ever faster. After decades of stagnation because of concerns about safety and waste, the fission-power industry seems on the verge of a renaissance. The need to cut greenhouse-gas emissions and to reduce the reliance on fossil fuels has prompted staunch opponents of nuclear power, such as Italy and Sweden, to reverse long-standing embargoes against the construction of new nuclear plants. Finland, France and the United Kingdom are preparing for new reactor building programmes. China is planning to build 40\u201350 fission plants within the next two decades. The US Nuclear Regulatory Commission has received applications for licenses to construct another 26 reactors in the United States, which has 104 already. And almost 45 fission plants are under construction around the globe. Clearly, a plan is needed for dealing with the resulting increase in nuclear waste. Such plans generally centre on burying it deep underground. The question, is where? Disposal can be a very politically sensitive issue. For example, a site at Yucca Mountain in Nevada was considered for more than two decades for use as a US national repository for spent nuclear fuel. But the plan sparked so much resistance from local residents and politicians that the US energy secretary, Steven Chu, announced in March that the underground facility was no longer an option. For the foreseeable future, the spent fuel from each US nuclear plant will continue to be stored on site. Many countries reduce the volume of waste by a factor of two or three by reprocessing the spent fuel \u2014 extracting the still-fissionable isotopes of uranium and plutonium, and fashioning them into new fuel elements. But reprocessing is expensive, inefficient and poses a nuclear proliferation risk: the extracted plutonium is usable in nuclear weapons. And it still leaves tonnes of highly radioactive residue to be disposed of. All of which is why hybrid reactors are suddenly starting to look good again.  \n                Nuclear roots \n              The hybrid idea dates back to the 1950s, when nuclear engineers were first struggling to harness nuclear reactions for generating electrical power. boxed-text They were already aware of the waste problem that arises from the nature of the fission reaction itself. The reaction starts when a neutron strikes the nucleus of a fissile isotope such as uranium-235 or plutonium-239, causing it to split apart. The result is a pair of lighter nuclei, a burst of energy and a number of new neutrons. The energy can be extracted as heat and used to drive an electricity turbine. But to keep the fission reaction going, some of the newly created neutrons must collide with other fissile nuclei, causing them to split and to release still more neutrons in a chain reaction. This happens easily as long as the fuel is fresh and there is nothing else for the neutrons to hit. As the reactions continue, however, the fuel accumulates more fission-product nuclei, most of which absorb the neutrons without doing anything else. Eventually, so many neutrons are absorbed that the chain reaction can no longer sustain itself, at which point the fuel becomes waste \u2014 even though most of the original fissile material is still there. Engineers realized that they could get around this problem by supplementing the chain reaction with an independent source of neutrons. If there were enough neutrons, they would use up much more of the uranium and plutonium in the fuel, and would also burn through most of the long-lived radioactive fission products, greatly reducing waste. And so the idea of the fusion\u2013fission hybrid was born, because fusion reactions \u2014 in which various isotopes of hydrogen fuse to become helium nuclei \u2014 produce a torrent of neutrons (one per fusion event). In principle, building a hybrid was simply a matter of wrapping a blanket of fissile material around a fusion reactor, and letting the energy flow (see graphic). In practice, though, the technology of the day was not up to the task. Harnessing fusion, whether for neutrons or energy, amounts to recreating the conditions that drive the reaction in the Sun. Somehow, a gas of hydrogen isotopes \u2014 usually deuterium and tritium \u2014 has to be compressed and heated until it becomes plasma at more than 150 million \u00b0C. And somehow, that plasma has to be confined in its dense, superheated state for long enough for the reactions to proceed. Faced with what seemed like insurmountable difficulties, the hybrid idea was shelved and physicists focused their efforts on the separate development of pure-fission and pure-fusion reactors. And except for a brief flurry of interest following the energy crisis of the late 1970s, when the Nobel laureate physicist Hans Bethe tried to drum up support for hybrids 1 , the idea has stayed on the shelf \u2014 until now.  \n                The advance of fusion \n              One reason for the renewed optimism about hybrids is half a century of progress in plasma containment. The most common approach today is to trap the plasma inside a doughnut-shaped device known as a tokamak, which holds it in place with an intense magnetic field. It is a measure of the difficulty of the task that the longest-lived fusion reaction demonstrated so far, achieved in 1997 in the Joint European Torus at the UK Atomic Energy Authority's (UKAEA) Culham Science Centre near Oxford, lasted no more than a few seconds. It generated fusion energy equivalent to 70% of the energy that was used to produce it. But many scientists in the tokamak-fusion community think that the next big machine \u2014 the International Thermonuclear Experimental Reactor (ITER), being built at Cadarache in the south of France \u2014 will have an energy output up to ten times its input. At about 19 metres wide and 11 metres tall, ITER's toroidal containment vessel will be twice the size of that of the Joint European Torus and, at an estimated construction cost of some \u20ac10 billion (US$14 billion), it will be one of the most expensive experiments ever undertaken 2 . However, the project has come under criticism for its cost overruns and delays and it will not start operation until 2018. Experiments on whether fusion would be viable for power aren't planned to begin until the end of 2026. Nonetheless, says Weston Stacey, a professor of nuclear engineering at the Georgia Institute of Technology in Atlanta, \"the physics and technology that are being developed at ITER are more than enough to build a good neutron source for a hybrid\". That's why Stacey, a leading proponent of the hybrid idea, has used the ITER design as the basis for the concept of a subcritical advance burner reactor 3 , a fusion\u2013fission hybrid design that he and his team at Georgia Tech have been working on for more than 10 years. Once we have a fusion reactor, says Stacey, \"the problem will be to fit all this together\". He laughs at himself for implying that putting a fusion reactor the size of ITER into the core of a fission reactor will merely be 'an engineering challenge'. But it is his engineering approach to design that has prompted people to take notice. \"One of the problems with designs in the past was that nobody who was interested in the hybrid had done very much serious engineering,\" says Jeff Freidberg, a nuclear scientist and engineer at the Massachusetts Institute of Technology in Cambridge. \"But the one that has perhaps done the best job so far is Stacey, because he's spent a lot of time to put some real engineering in there.\" China is also pursuing the hybrid idea: it is one of the goals of the country's High-Tech Research and Development Programme. \"At the moment it's mainly calculations to assess the various conceptual designs for a hybrid,\" says Jiangang Li, director of the Chinese Academy of Sciences' Institute of Plasma Physics in Hefei. But the country's big push into nuclear fission power gives the effort urgency, he says: \"China will be faced by both a need to be able to reprocess a lot of radioactive waste and a shortage of nuclear fuel because the country has relatively limited fission fuel resources.\" Freidberg is cautious about how realistic many of the ideas that are emerging in the field are in the short term, but is optimistic about their contribution in the longer term. In particular, he sees the hybrid as potentially easing some of the technical challenges posed by a pure fusion reactor. Because the fusion core of a hybrid would only have to generate neutrons, for example, it could be operated well below the power levels that a pure fusion reactor would need to work at to produce electricity. As a result, the plasma in a hybrid device would probably be less susceptible to instabilities and other disruptions.  \n                A real beast \n              Lower power levels should also ease the 'first wall' problem \u2014 the difficulty of finding materials for a fusion reactor's inside wall, which will be exposed to such an intense flux of high-energy neutrons that it will probably need to be replaced after just a year or two of operation. \"The problem of that first wall is a real beast,\" says Freidberg. That will still leave Stacey's engineering challenge to solve: building a fusion reactor inside an equally complex fission device. But Freidberg remains optimistic. \"None of these problems are insurmountable, but you can't solve them with paper studies, you ultimately need to build some devices. And maybe you can start to build things sooner than you could do for pure fusion electricity, because the end goal is a little bit nearer.\" Not everyone is convinced, however. Steven Cowley, director of the Culham Science Centre, speaks for many in the fusion community who are sceptical of the need to revisit the hybrid concept, and who are concerned that it is a distraction from the goal of sustainable, clean, pure fusion power. \"I'm not saying a hybrid can't be done,\" says Cowley. \"But if what you need is post-ITER in its abilities, then you should focus your attention at getting all the way to pure fusion. Because even though in principle it looks easier, you have to solve all these other problems as well.\"  \n                Beyond ITER \n              Hybrid designs don't have to be based on ITER. For example, a group of researchers at the Institute for Fusion Studies at the University of Texas in Austin argues that a practical hybrid will require a fusion core that is far smaller than that of ITER. \"A typical light-water fission reactor runs at about a gigawatt of electric power,\" explains Swadesh Mahajan, a senior research scientist with the institute. That gigawatt is generated within a certain volume, which means that the reactor has a power density of so many gigawatts per unit volume. And as it turns out, says Mahajan, that number is about five times higher than the power density of ITER. This creates a problem for the design of a hybrid. For the fission blanket to capture the fusion-generated neutrons efficiently, the fusion core and the blanket have to have roughly the same power density. So the fusion system has to have a power density about five times that of ITER, says Mahajan. Or to put it another way, the fusion core has to put out the same amount of energy (and neutrons) in a volume five times as small. The problem is that ITER's planned power density is already at the limits of current technology. These limits are set by the materials used to construct the exhaust of the reactor \u2014 the diverter. The materials have to withstand the high volume of superheated gas that must be continually vented from its plasma. To overcome this problem, Mahajan's group has come up with the Super-X Divertor 4 . The concept involves engineering the magnetic fields around the exhaust of a tokamak to increase the distance hot gases have to travel, giving them a chance to cool down before they come into contact with the solid walls of the exhaust. This enables the diverter to handle the exhaust from much higher power-density tokomaks. \"I think the Super-X Divertor is a big step forwards. A huge step forwards,\" says Cowley. He is excited about the prospect of a compact neutron source based on Mahajan's ideas, but nevertheless says that the UKAEA's fusion efforts are focused on ITER, and that it has no interests in developing a hybrid. \"The Super-X Divertor could certainly enable the high-power devices that the Texas group want for their hybrid. But I think it'll do more than that. It will be the model of the kind of diverter that we will have on any demo reactor.\" As well as testing the Super-X concept itself, a compact neutron source built with its help could be used to develop and test the materials needed for a commercial fusion reactor. However, adding the untested diverter to ITER would be too risky. Mahajan, for his part, is hoping that Cowley's work will further another of his group ideas, that of a disposable fusion 'battery' 4 , 5  \u2014 a much smaller fusion device than ITER that can be replaced in its entirety at the end of its operational life. \"The biggest engineering nightmare if you ask any fusion engineer is the problem of the first wall,\" says Mahajan. \"So we decided that if we had a fusion neutron source that was sufficiently small, not in the sense that you could put it in your handbag, but small enough that a crane could easily lift it, we could design a neutron source that is replaceable.\" With a reactor that can be lifted in an out of the middle of a hybrid's fission blanket, Mahajan argues, you no longer have to worry as much about the stability of the materials you use to make it. He envisages that the fusion part of such a device could be removed and replaced at the same time as the fission blanket's waste-fuel rods, about every 2 years. What's more, he says, in the test phase of such a device, this 2-year lifespan should enable the technology in the battery to be developed at a much faster pace than a conventional reactor. \"If we needed five iterations to improve the stability of our device, with a turn-around time of just 2 years, that would take us only 10 years. For other designs the same process would take 50 years.\" And it's not just the materials problems that the Texas approach would address, but the plasma physics as well. For ITER to produce ten times more energy from a fusion plasma than is put in, it will require the generation of unprecedented plasma temperatures, densities and confinement times. But because the fusion component of a hybrid need not produce energy, merely neutrons, the plasma conditions needed will be more modest. \"Because the Super-X Divertor takes care of the heat-flux issue, we can operate our device in the sorts of conventional plasma modes that every Tom, Dick and Harry in the plasma community are already producing in their laboratories,\" says Mahajan. And it could even deal with potential concerns about generating a hot thermonuclear plasma in the middle of a blanket of concentrated, highly radioactive, nuclear waste. \"Even if something did go wrong with the fusion module of our hybrid, the toroidal field coils of the module, which are made of strong metal, will shield the fission blanket from any stupid thing that the plasma is capable of doing,\" says Mahajan. This would not be the case in a conventional hybrid design, in which the components are more intimately connected. The Texas group is in talks with groups at Princeton University in New Jersey and Oak Ridge National Laboratory in Tennessee to investigate these ideas. Mahajan says he sees their work as essential to rebuilding the world's nuclear power capacity: \"One of the things I've been arguing for \u2014 because in 20 years we will be ready to destroy the waste \u2014 is to start building nuclear reactors now. Don't wait. The rate at which nuclear capacity has been destroyed is an act of monumental stupidity.\" If scientist don't start aggressively pursuing this form of clean energy, he says, \"We can forget about solving global warming. We will have screwed the planet royally if we don't take action fast.\"   Ed Gerstner is a senior editor of    Nature Physics .   See also Correspondence    'Where will we find the tritium to fuel hybrid reactors?' \n                     Nature Physics \n                   \n                     Fusion-Fission hybrids revisited \n                   \n                     ITER \n                   \n                     Culham Science Centre \n                   Reprints and Permissions"},
{"file_id": "459638a", "url": "https://www.nature.com/articles/459638a", "year": 2009, "authors": [{"name": "Alison Abbott"}], "parsed_as_year": "2006_or_before", "body": "\"It's childish, but it still gives me great pleasure to see high-res pictures everyone told me would be impossible,\" says Stefan Hell of the razor-sharp silhouettes of mouse neurons on his screen. Hell can't resist a small gloat. He frequently refers to his bitter struggle for recognition \u2014 the many years in the 1990s trying to reinvent microscopy as nanoscopy, able to reveal structures an order of magnitude smaller than light microscopy. 'You can't argue with the law of physics', was the no-can-do attitude of most microscopists at the time. They were referring back to 1873, when German physicist Ernst Abbe declared that diffraction inevitably limits the resolution of microscopy to around half the wavelength of light. Hell didn't argue with the laws of physics \u2014 he found a way around them. Hell's innovation was to think about how he might exploit the properties of the fluorescent dyes, or fluorophores, that are widely used to label molecules for microscopy in biological samples. Fluorophores can exist in three states: ground, excited and dark. The confocal microscope, a standard machine in cell biology, works by rapidly scanning a sample with a focused beam of light and capturing the photons emitted when fluorophores are activated by the beam from ground to excited state. In practice, the 'diffraction barrier' means that two fluorescent spots less than around 200\u2013300 nanometres apart appear as one blurred blob. It dawned on Hell that he could use another beam of light, of slightly longer wavelength, to switch the fluorophores on the edge of the blurry spot back to their ground state. So he added a second beam to his confocal microscope. This beam is fashioned like a tube of light with a hollow centre (see graphic). It hugs the excitation beam tightly, depleting all of the fluorescence in a sample except that at the very centre of each spot. This is the ingenious principle of stimulated emission depletion (STED) microscopy \u2014 and it throws objects measuring 20 nanometres into fine relief. Hell dispelled much scepticism about his technique with a 2000 paper 1  in  Proceedings of the National Academy of Sciences   \u2014 \"previously rejected by both  Nature   and  Science ,\" he observes ruefully \u2014 which showed that his STED method could generate nanoscale fluorescent images. The objects themselves (beads, yeast and bacteria) were not exciting, but the proof of principle was. Two years later he was appointed a director at the Max Planck Institute for Biophysical Chemistry in G\u00f6ttingen, Germany, with all the research money and facilities he could handle. \"There is still refinement needed \u2014 we are working on being able to do the same things at lower light levels and with a larger field of view,\" says Hell. \"But the basic principle has been cracked.\" Hell now finds himself with stiff competition as other labs race to develop, improve and commercialize techniques for 'super-resolution' microscopy. US scientists have developed methods known as stochastic optical reconstruction microscopy (STORM) and photoactivated localization microscopy (PALM), which use computation to work out the location of individual fluorophores and compile them into high-resolution images. And in structured illumination microscopy (SIM), biological samples are illuminated with stripes of light rather than a focused beam, and the interference patterns of the stripes are analysed and used to reconstruct images on nanometre scales. Biologists can't team up with the new nanoscopy labs fast enough. \"It is coming at a really exciting moment in biology,\" says Jeffrey Lichtman, a cell biologist at Harvard University, \"and particularly in neuroscience, because neural connections are \u2014 tantalizingly \u2014 just below the diffraction limit.\" In his energetic bid to map all the neural connections in the brain, Lichtman says he is collaborating with all the main super-resolution microscopy labs. STED microscopy has already revealed cell biological details that had been lost in the fluorescent fog. A video accompanying Hell's 2008  Science   paper 2  shows 40-nanometre-diameter vesicles moving within a nerve cell. Nothing like this had ever been seen before. The speed with which the vesicles zap around, like fleas on a blanket, is almost unseemly. It is one thing to appreciate the theory and mathematics of cell dynamics, but it is quite another to finally be able to see it in action. Kai Simons, a cell biologist at the Max Planck Institute of Molecular Cell Biology and Genetics in Dresden, Germany, has seen his own 'lipid raft' theory gain a slug of support from the Hell lab 3 . Simons's originally controversial theory, proposed in 1988, held that certain lipids and proteins in a cell membrane form temporary clumps to, for example, facilitate the transmission of signals across the membrane 4 . But the proposed rafts would have been as small as 30 nanometres in diameter \u2014 invisible until STED microscopy came along. Hell's group showed certain proteins and lipids being transiently trapped in the complexes, exactly according to the theory. \"It has given us real biological insight \u2014 you can't achieve a dynamic view at this scale any other way,\" says Simons. Although impressed by the technical advance, some researchers say that super-resolution microscopy has yet to prove itself. \"I think these techniques owe us a real scientific breakthrough in solving a question that couldn't be solved any other way,\" says Winfried Denk, a microscopy pioneer based in at the Max Planck Institute for Medical Research in Heidelberg, Germany. Others have no doubt that it will. For Tobias Bonhoeffer, a director of the Max Planck Institute of Neurobiology in Martinsried, Germany, STED microscopy is \"a transforming technology which will be crucial for a large number of biological questions\". In collaboration with Hell, he has published pictures of the tiny dendritic spines on neurons, which are central to processes such as information storage in the brain, learning and memory 5 . \"Spines are around a micrometre across \u2014 you can see them with light microscopy, but not the details,\" he says. \"With this increase in resolution, we can even see the fine filopodia reaching out to make connections.\"   See also  \n                     page 629 \n                    and online at  \n                     http://tinyurl.com/microspecial \n                   . \n                     Microscopy special \n                   \n                     Cell imaging: New ways to see a smaller world \n                   \n                     Imaging in Cell Biology \n                   \n                     Stefan Hell\u2019s lab \n                   Reprints and Permissions"},
{"file_id": "460171a", "url": "https://www.nature.com/articles/460171a", "year": 2009, "authors": [{"name": "David Cyranoski"}], "parsed_as_year": "2006_or_before", "body": "With its electron microscope, genetic sequencing machines and observatory, the Yokohama Science Frontier High School is equipped like no other. Will future scientists be inspired there, asks David Cyranoski. The timetable for 15-year-old students at Yokohama Science Frontier High School (YSFH) can be busy. Before break, they might grow single-layer carbon nanotubes in argon gas and evaluate them with micro-Raman spectroscopy. After break, there are polymerase chain reactions (PCR) to be done. Things don't slow down after class, when students stick around in the observatory to glimpse star clusters or Saturn's rings from the school observatory. The equipment list at Japan's first dedicated science high school, which began classes in April, could rival a small research institute. The school is also a sophisticated experiment. Its main champion, biophysicist and genomics pioneer Akiyoshi Wada, hopes a \"flood\" of such institutes will open up throughout the country, inspiring students and Japan's future leaders. Wada thinks that the school is key to reversing children's waning interest in mathematics and science, a phenomenon that has attracted political hand-wringing and has even been given a name \u2014  rika banare . As one of five 'super advisers' to the school, and the only one with a permanent position there, Wada has been instrumental in its creation. He spent most of his four-decade career initiating, managing and administrating ambitious science projects aimed at keeping Japan at the forefront of international scientific trends. A decade ago, when the city of Yokohama asked Wada to be on the planning committee for a new high school, he brought the same bold, uncompromising vision. \"It was precisely because of Dr Wada that the school was able to establish its educational principles and goals,\" says the school's principal, Haruo Sato. There are wrinkles to be ironed out. The school will be open to accusations of elitism and, with a price tag of \u00a59.5 billion (US$100 million) \u2014 not including the land, which was donated by the city \u2014 some people ask whether the model really has a chance of spreading. Wada answers yes to the question before it is even finished. But he acknowledges that the Yokohama experiment has much to prove. \"The future of science education in Japan will depend greatly on the success of the YSFH,\" he says, \"and I am aware of that massive responsibility.\"  \n                Ancient roots \n              Wada is softly-spoken. On a tour of the five-floor, 25,000-square-metre buildings that overlook the Sumida River, he trails behind letting an administrator, Yukimasa Uekusa, lead. Wada seems proudest of the finer details, such as the two famous trees outside \u2014 a descendant of the apple tree in Isaac Newton's garden and an offshoot of the grape vine used in some of Gregor Mendel's experiments \u2014 and the larger-than-life images of famous scientists that cover many of the walls. \"The captions are all in English,\" he says. \"They need to learn English.\" Besides spending 2\u20133 days a week at the school, Wada also runs the 'Wada Salon' where he discusses, over tea and scones, recent scientific articles and ethical issues. Visiting scientists might be more interested in the instruments, such as the 30-centimetre automated telescope with a retractable dome. For this and other expensive equipment, the school is devising a system by which students who have shown themselves to be capable of handling a machine will receive a licence to do so without supervision. Few schools in the world can match this level of instrumentation. Jim Jarvis, division manager for science and technology at the Thomas Jefferson High School for Science and Technology in Alexandria, Virginia, says that his school, like the YSFH, has a telescope, a scanning electron microscope and PCR machines. But his wish list would include some things that Yokohama has but he does not, such as multiple fume cupboards and gene sequencers. Judy Scheppler, who directs the Grainger Center for Imagination and Inquiry at the Illinois Mathematics and Science Academy (IMSA) in Aurora, says the nanofabrication and nano-observation facilities at Yokohama are what sets it apart. \"Different schools around the country and the world may have some of this. But one school having everything is extraordinary,\" she says. Sophisticated instruments don't mean much without sophisticated instructors, and the YSFH is providing those too. Before taking up his teaching post, for example, Yutaka Mizogami spent a year training in a Tokyo University laboratory. There he got his name \u2014 and his affiliation with the YSFH \u2014 in the scientific literature as a co-author 1 . He says he decided to come to the YSFH because he was tired of only being able to give 10% of his time to experiments at his previous school. He is happier now with 30%. In its first year, the school had more than 5 applicants for every one of its 240 spots for 15\u201318-year-old students \u2014 compared to 3 applicants for a place at the next most popular school in the Kanagawa prefecture. The main reason to come, according to a survey, was the chance to do experiments. In person, some students told  Nature   it was because the teachers are fun. One said he wants to make artificial muscle. Another expressed an interest in methane hydrates. When talking to students like this, it is hard to believe that there is much to  rika banare . But politicians have been worrying about science education since 2004, when Japanese 15 year olds dropped from first to sixth in standardized mathematics tests taken the previous year and other science scores started falling. Japan had always prided itself that on such tests it was at or very near the top. By 2006, the last year for which these figures are available, the country ranked tenth in maths and sixth in science.  \n                The counter-attack \n               Rika banare   has inspired the government to designate more than 100 'super science high schools' that receive \u00a550 million per year for three years to enhance science education. Wada describes this as a \"broad but shallow\" first step. Noting the decline in standardized test scores, Wada boasts: \"The students at the YSFH will be at the vanguard of a 180\u00b0 reversal of this trend.\" Wada has long been persuading policy-makers that high-impact science sometimes requires a concentration of resources \u2014 although he hasn't always got his way. As a biophysicist in the 1970s, Wada ran into many sceptical biologists when he was one of the first to envision large-scale automated genomic sequencing. But even as these technologies were ramping up elsewhere, Japan's bureaucrats stalled, its genomics fell behind, and when the human genome sequence was finished, Japan accounted for only 6% compared to the 59% and 31% that the United States and United Kingdom produced respectively. The unfolding of Wada's failed efforts are described in a book aptly titled  A Defeat in the Genome Project 2 . Later, Wada had better luck in turning his vision into reality. He was a key player in creating and maintaining the international Human Frontier Science Program, initiated in Japan in 1989, that has funded 3,000 scientists involved in collaborative projects, including 13 who went on to win Nobel prizes 3 . In 1998, Wada became the founding director of the RIKEN Genomic Sciences Center (GSC) in Yokahama, Japan's first large-scale effort at comprehensive genomics. The generously funded centre led Japan's human- and primate-sequencing efforts and rose to international acclaim with its project to catalogue the active, 'transcribed' parts of the mouse genome 4 ,   5 . \"He is a straight-talker, sometimes harsh, but never goes wobbly in his vision and decision,\" says Yoshihide Hayashizaki, who led the RIKEN mouse project. \"I guess that this is why he motivates others to follow him.\" Will others follow Wada and his vision for the Yokohama high school? The biggest sticking point in negotiations was the initial cost, which was paid by Yokohama city. Even so, Wada says there have already been half a dozen other regional governments calling to enquire about the school. Scheppler says that select high schools tend to come under pressure because only a few students benefit from their extraordinary facilities. \"Shouldn't every student get an excellent science education?\" she asks. Wada is sensitive on this point. \"Children with outstanding ability, even those from poor households, will be able to take advantage of the low-cost tuition and receive a great education,\" he says. As it is a public school, students must pay only \u00a59,000 a month. Nobel laureate chemist Ryoji Noyori, president of RIKEN in Wako, says that the school will help to undo the \"totally egalitarian public education system in our country\". Japan has been chipping away at this way of thinking, and university funding is increasingly focused on competitive funding and centres of excellence. But there is no guarantee that the privileged science track that Yokohama students start on will continue after they graduate. \"I fear some students will be disappointed when they later enter the national universities,\" says Noyori. Wada is intent on exposing the students to the more luxurious end of Japanese research. The school sits next door to the RIKEN GSC and has collaborative agreements whereby they can use some of the centre's facilities. Like Wada's other big ventures, this one will eventually be judged on its results: whether the originality being cultured there and at its potential spin-offs translate into improved standardized test scores and a new generation of inspired scientists. Jarvis says that two-thirds of the Jefferson school's 450 annual graduates work in science-related fields. If nanofabrication and PCR can incite a similar passion for science in Yokohama's teenagers, Japan's great experiment in science high schools will have paid off. \n                 David Cyranoski is  \n                 Nature  \n                 's Asia-Pacific correspondent.  \n                 See Editorial,  \n                     page 151 \n                   . \n                     Learn science at Nature \n                   \n                     RIKEN Yokohama Institute \n                   \n                     OECD Programme for International Student Assessment \n                   \n                     Trends in International Mathematics and Science Study \n                   \n                     Illinois Mathematics and Science Academy \n                   \n                     Thomas Jefferson High School for Science and Technology \n                   Reprints and Permissions"},
{"file_id": "460318a", "url": "https://www.nature.com/articles/460318a", "year": 2009, "authors": [{"name": "Kendall Powell"}], "parsed_as_year": "2006_or_before", "body": "Cellular life is all slopes, arcs and circles \u2014 but there is much debate about how these curves are built. Kendall Powell reports. To view the innards of a cell is to view architecture reminiscent of Antoni Gaudi: the gentle arc of the cell membrane, the contortions of internal tubing, the tight bubbles of vesicles. But for biologists, this architecture is an intellectual puzzle as well as a beautiful structure. Membranes generally prefer to be flat \u2014 so what, exactly, is generating all the curves? Pietro De Camilli thought he had found one answer when, just over a decade ago, he saw a tangle of tubules in an electron micrograph. The pictures came from an experiment carried out by Kohji Takei, De Camilli's postdoc at Yale University in New Haven, Connecticut. Takei had mixed a protein called amphiphysin, which is thickly clustered at the tips of neurons, with large bubbles of artificial membranes to try to replay the process by which neurotransmitter chemicals are packaged up. It came as a surprise when the membranes snapped into masses of jumbled, twisted tubes 1 . \"It looked like a plate of spaghetti, it was absolutely spectacular,\" says De Camilli. \"It was the first example of a protein that, by itself, had dramatic membrane-deforming properties,\" he says. The first, maybe \u2014 but not the last. Since this discovery, De Camilli and others have identified whole families of protein that have equally dramatic abilities to bend membranes, and a tightly knit community of researchers has built up to study them. Much debate has centred on whether the proteins create curves by wedging themselves into membranes, or by moulding them into shape using curved protein scaffolds. As it turns out, both may be at play \u2014 and more. The issue is not just one of beautiful architecture: the ability to turn membranes into circles and tubes is central to almost every cellular process. Spherical vesicles are essential for carrying construction materials and communication signals around the cell. And the undulating network of membranes that makes up the mitochondria, endoplasmic reticulum (ER) and Golgi complex are vital to these organelles' roles in energy production, protein synthesis and protein processing, respectively. Some researchers are now finding that, once built, the cell uses its curves to position cellular processes. \"The fact that the shape can also be information is very exciting,\" says Bruno Antonny, a biochemist at the CNRS Institute of Molecular and Cellular Pharmacology in Valbonne, France. \"That you can sense curvature means that you can organize reactions in time and space.\" Walling off spaces by wrapping membranes around them was also necessary for the evolution of cellular and organismal complexity, as it allowed cells to adopt specialized functions and trafficking to occur between compartments \u2014 the hallmarks of eukaryotic cells, says Harvey McMahon of the Medical Research Council Laboratory of Molecular Biology in Cambridge, UK. \"Evolution from the primordial single-celled organisms to multicellular organisms was made possible by the appearance of small membrane-bound spaces in the cell,\" he says.  \n                A matter of scale \n              The closest that most cells come to having straight lines are the surfaces of the encircling plasma membrane. On tightly packed and column-shaped cells, these are virtually flat; and even on curved cells, a particular spot on the circumference still seems level to the molecules there, much like the surface of Earth does to a person standing on it. Inside the cell, however, microscopy has revealed a landscape with much sharper contours. Yet for many years researchers gave little thought to how such curves were generated: cell membranes were thought of as fluid, pliable structures that could easily be pushed or pulled into shape. That view changed in the early 1970s when physicist Wolfgang Helfrich, one of those behind the invention of the liquid-crystal display, proposed a model showing that membranes are relatively rigid structures that require substantial energy to contort 2 , just as a liquid crystal does. Cell membranes are made up of lipid bilayers: two tightly packed rows of lipid molecules lined up with their hydrophilic lipid head groups facing outwards, and water-repellent fatty-acid chains sandwiched on the inside. Work by Helfrich and other biophysicists has since shown that lipids prefer to be lined up in planes, and it takes a lot of energy to create the kind of disturbance that comes with bending a plane into a cylinder or sphere. The situation is similar to blowing a soap bubble from a circular wand \u2014 it takes continuous blowing to deform the flat soap film into a sphere. Researchers spent many years looking for scaffolding proteins that could sustain a membrane curve, but none emerged that could, according to biophysicists' calculations, provide sufficient energy. De Camilli says that it wasn't until the 1999 amphiphysin experiment that biologists began to appreciate that proteins could be 'massaging' membranes into shape rather than forcing them. What made that discovery all the more intriguing to De Camilli was that amphiphysin is normally found floating in the cell cytosol, not integrated into membranes. He wondered whether there were more proteins that could distort membranes so strongly, and how they did it. By 2002, De Camilli's group and others had identified a few more proteins that had membrane-flexing abilities 3 ,   4 ,   5  \u2014 and saw that they shared an intriguing spiral structure called an amphipathic helix. Looking down the barrel of the helix, one half is charged and the other half uncharged. Put the protein near a bilayer and it will immediately insert itself lengthwise with its charged half among the lipid head groups and the uncharged half nestled within the fatty-acid chains. This suggested that insertion into the outer half of the bilayer would create a wedge, and if enough wedges were inserted then the membrane would bend (see  graphic ). It was an instantly appealing idea and one that De Camilli promoted \u2014 but it was about to be joined by an equally appealing rival. In 2004, a group led by McMahon and his colleague Phil Evans solved the three-dimensional structure of the 'BAR' domain of amphiphysin, which includes the amphipathic helix 6 . The helix didn't crystallize well, but the rest of the BAR domain did \u2014 and their work showed that it was crescent shaped. \"We could immediately see this banana shape, that, to me, is one of the most beautiful structures we've solved,\" McMahon says. Beauty aside, the banana suggested an alternative way for the BAR domain to bend membranes. The concave face of the banana is positively charged, which would make it stick to the negatively charged outer surface of membranes and mould them to their own curved shape. The bananas did not go down well at meetings, and McMahon recalls more than a few scoffs and scowls. The idea that they were acting as scaffolding seemed too similar to previous scaffolding models, which the physicists had already proved wrong. Clearly BAR domains were important: evidence was piling up that they were a huge family and the various relatives could subject membranes to all sorts of bending. But were the banana-shaped molecules generating the curvature or simply stabilizing or sensing the curvature after it had been produced by insertion of the amphipathic helix? The question has inspired vigorous debate and some flip-flopping between ideas. Although De Camilli says he was originally sceptical of the scaffolding concept, his work now promotes it. Last year, his group, in collaboration with Yale colleague Vinzenz Unger, published stunning cryo-electron microscopy images of BAR domains without amphipathic helices. The domains stacked up end to end to form a spiral around membrane cylinders much like a barber's pole 7 , suggesting that scaffolding alone can generate curvature \u2014 at least at the artificially high concentrations present in the test tube.  \n                Rods and curves \n              McMahon now raises an eyebrow at De Camilli's latest model for how thousands of banana domains could generate curvature alone. Work from his lab has shown that this domain is much less efficient at forming membrane tubes in the lab dish if the amphipathic helix has been lopped off. \"This was the first demonstration that the two functions might be joined in one protein,\" says McMahon. Recently, McMahon has started to promote the wedging idea. He teamed up with biophysicist Michael Kozlov at Tel Aviv University in Israel, who calculated that insertion of an amphipathic helix alone is energetically sufficient to generate curves, whereas scaffolding alone is not. The study showed that the most efficient way to bend a membrane is to insert a short rod shape, similar to the amphipathic helix, into the outer layer of the membrane at a depth \u2014 and this is key \u2014 of around one-third of the membrane's thickness 8 . Their calculations predicted that the bending ability of BAR domains can be credited solely to the insertion of the amphipathic helix. \"Now the feeling is that this banana-like shape stabilizes the curvature that is generated by some sort of insertion into the membrane,\" says Kozlov. Speaking of McMahon, De Camilli says, \"Both of us got it partially right and partially wrong, so it has been a humbling experience.\" \n               boxed-text \n             Just as researchers were finding something to agree on about BAR domains, a whole new hypothesis for generating curves burst onto the scene. This one came from the lab of Tom Rapoport at Harvard Medical School in Boston, Massachusetts. In the late 1990s, his graduate student Lars Dreier was mixing nuclear-envelope membranes with DNA, attempting to imitate in a test tube the way that the envelope reforms after cell division. In the background of the microscope slide, Dreier noticed a network of membrane tubules forming that closely resembled the ER, the main site of protein synthesis in the cell. When Rapoport saw it, he says, \"I ran around the lab asking everyone, 'Did you see this? It's fantastic!' and getting everyone to look through the microscope\". The experiment seemed to have triggered membranes into self-assembling an ER, which is normally continuous with the nuclear envelope membrane 9 . The excitement only intensified when Gia Voeltz, then one of Rapoport's postdocs, isolated a class of proteins responsible for curving the tubules in the ER 10 . Called reticulons, these proteins have neither a BAR domain nor an amphipathic helix and there is much discussion as to how they work. They do form a double hairpin-loop structure that inserts partway into the membrane, and Rapoport and Voeltz, along with De Camilli and Kozlov, think that these hairpins may act as a wedge. (Kozlov has calculated that the hairpins probably insert to the magical one-third depth.)  \n                The barber pole effect \n              Because reticulons are sunk permanently into the membrane, Voeltz, who is now at the University of Colorado, Boulder, says that they and proteins like them might be better suited for the long-term maintenance of an organelle's tubular shape than a protein such as amphiphysin, which inserts temporarily as a budding vesicle forms. She originally thought that the reticulons might stack up to make a \"nice little barber pole\" scaffold around the ER tubes. But now, she's pretty sure it's not that simple \u2014 and when she pulls up her lab's latest cryo-electron microscopy images of the ER tubes, it's easy to see why. In these images, which show the ER's three-dimensional structure at nanometre-scale, the tubules look nothing like neat cylinders and more like gnarly tree roots with wide and narrow stretches. Voeltz now thinks that a cluster of several reticulon molecules may help to stabilize the tubes by forming a half-ring at the narrowest points of constriction. This fits with the idea that cells have evolved multiple wedges, with different shapes and bending abilities, to fit different purposes. McMahon now says it was clear early on, \"that we were going to have a whole repertoire of proteins \u2014 some driving curvature, some limiting or stabilizing curvature, and some sensing curvature\". And Antonny's group has been focused on the sensing part of the repertoire. He is trying to understand how the bend of a membrane can act as a signal, such that its position, camber or direction can recruit additional proteins to a particular point on an organelle. Antonny's recent work has focused on one enzyme, called ArfGAP1, that detects the tight curve on transport vesicles and directs the removal of their protein coat before they can fuse with their destination membrane. His team showed that ArfGAP1 has greater activity when bound to highly curved membrane spheres \u2014 with the same diameter as transport vesicles \u2014 than to ones with a broader diameter and a gentler curve 11 . The degree of curvature tells the enzyme that there's a vesicle that needs uncoating, Antonny says, so that it binds only to vesicles and not to other curved surfaces. By chopping ArfGAP1 into smaller bits, the group found that its key curve-sensing stretch was an amphipathic helix. \"But the chemistry of the helix was almost the opposite of what you would expect,\" he says. Unlike that in the BAR domain, the helix in ArfGAP1 has almost no positive charge on one side and lacks the strong attraction to the membrane 12 . Antonny compares the protein's behaviour to that of a nervous swimmer about to take the plunge. \"It's as if the molecule is shy and the water is not warm enough when the membrane is flat. But when you bend the membrane, the molecule can sense the lipid packing defect and inserts.\" The molecule's shyness is what makes it a good sensor, he explains. It dives in only when the amount of curvature is right and gets out when that curvature changes. Last year, Antonny's lab found a similar curvature-sensing helix that may help to maintain the shape of the Golgi complex 13 , an organelle responsible for processing and trafficking proteins and other large molecules in the cell. The Golgi is a series of flattened membrane sacs stacked up like pancakes, with transport vesicles constantly budding off from its fringes. How it maintains its architecture in the midst of all this trafficking has been a puzzle. The new helix \"is a start to explaining why the Golgi has this beautiful architecture\", says Antonny. Explaining beautiful \u2014 but functional \u2014 architecture is after all what the whole field is about. \"They strike us, they surprise us, but we find a remarkably appealing harmony in such buildings,\" says De Camilli of Gaudi's constructions. And cells, he says, are the same. \"I think the beauty of great architecture, like the beauty of cellular structures, resonates in us precisely because they build on natural physical principles.\" Kendall Powell is a freelance science writer based in Broomfield, Colorado. \n                     Molecular Cell Biology \n                   \n                     De Camilli Lab \n                   \n                     McMahon Lab \n                   \n                     Rapoport Lab \n                   Reprints and Permissions"},
{"file_id": "460321a", "url": "https://www.nature.com/articles/460321a", "year": 2009, "authors": [{"name": "Anjali Nayar"}], "parsed_as_year": "2006_or_before", "body": "Africa's Lake Kivu contains vast quantities of gas, which makes it both dangerous and valuable. Anjali Nayar asks whether it is possible to tap the gas without causing a disaster. In late 2001, Mount Nyiragongo in the Democratic Republic of the Congo (DRC) was growing restless. Plumes of smoke issued from the central crater, alarming volcanologists in the nearby city of Goma. Then, on 17 January 2002, lava fountained from a fracture on Nyiragongo's southern flank. The molten rock snaked down the sides of the volcano and razed the centre of Goma, engulfing houses and setting off a string of explosions at fuel stations and power plants. That evening, the lava streamed into nearby Lake Kivu, generating a plume of water vapour that clouded the area for days 1 . More than 100 people were killed and nearly 300,000 people fled their homes. The only obvious refuge for the displaced people was along the shores of the lake. But Kivu poses its own threats. Beneath its placid surface, the lake contains 300 cubic kilometres of carbon dioxide and 60 cubic kilometres of methane. A disruption to the lake ? such as a bigger, closer eruption ? could cause a gas burst, with potentially deadly consequences for the roughly 2 million people who live along Kivu's shores. The risks are hard to quantify, however. Although scientists have studied the lake for decades, basic details about Kivu and its gases are still relatively scarce, and there is now debate about how hazardous the situation is. The issue is complicated by the lake's economic potential. The valuable methane dissolved in the water has started a feeding frenzy among energy companies working with the DRC and Rwanda, the other nation bordering the lake. In deals worth hundreds of millions of dollars, companies have started to siphon off the methane, in some cases working with the very scientists who have been assessing the lake's hazards. Proponents say that those degassing efforts will reduce the risk of gas eruption, but some researchers are worried that schemes to extract methane could make the situation more dangerous if they upset the lake's equilibrium. \"It could be one of the great remediation projects of all time: mitigating a lethal natural hazard and at the same time bringing power to people who desperately need it,\" says George Kling, a biogeochemist from the University of Michigan in Ann Arbor. \"If it is done right.\"  \n                An expanding problem \n              Lake Kivu lies in the Great Rift Valley, where tectonic forces are slowly ripping Africa apart. That movement brings up molten rock, which releases carbon dioxide that seeps into the bottom of Lake Kivu. Bacteria convert some of the carbon dioxide into methane, and other bacteria produce methane by breaking down organic matter in the deep waters (see graphic). Kivu is permanently stratified, with layers of dense salt-rich water below fresh water at the surface. Deeper than about 50?80 metres, the lake is anoxic and the concentrations of dissolved carbon dioxide and methane increase with depth 2 . The differences in density prevent the layers of water from vertically mixing, and so trap the gases at the bottom of the lake. Residents around the lake have known about the dissolved gases for many decades, but it wasn't generally thought to be a hazard. Then, in 1984, carbon dioxide erupted from Lake Monoun in Cameroon, killing 37 people. Two years later, another Cameroonian lake, Lake Nyos, spat up 0.3?1 cubic kilometres of carbon dioxide, asphyxiating more than 1,700 people. Kling was part of a team that visited Lake Nyos in the weeks following the eruption. \"The animals were all dead, thousands of cattle just lying about,\" he says. Kling had been to Nyos the year before, but had only sampled surface waters. \"We knew nothing about the gas bomb in the bottom of the lake,\" he says. It turned out that the deep waters of Lake Nyos were nearly saturated with carbon dioxide and, like in Lake Kivu, the gases were kept in solution by the pressure of the overlying water. Kling postulates that a landslide disturbed the lake's stratification, forcing gas-rich waters to move upwards 3 . That started a chain reaction. The reduction in pressure caused carbon dioxide to come out of solution and form bubbles, much like what happens when a bottle of champagne is uncorked. The rising bubbles dragged up the surrounding water, which also degassed, leading to a violent gas burst ? a limnic eruption. Carbon dioxide is denser than air so, when it emerged, it hugged the ground, smothering everything as it spread up to 26 kilometres from the lake. The scale of the disaster compelled scientists to assess the risk at Kivu, the other lake known for its dissolved gases. Latent threat There are no historic records of limnic eruptions in Lake Kivu. But gaps in layers of plankton fossils at the bottom of the lake suggest that such paroxysms have struck several times in the past 5,000 years 4 . If Kivu were to undergo a limnic eruption soon, it would dwarf the Nyos disaster. Kivu is more than 3,000 times larger and contains more than 350 times as much gas as was released by Lake Nyos. Kivu's shores are also densely populated. \"Kivu is basically the nasty big brother of Nyos,\" says Kling. But there is no scientific consensus on the current risks of that kind of a disaster. Gas concentrations measured in 1974 and a limited study conducted in 2004 (ref.  5 ) show that there has been a 15?20% increase in methane and a 10% increase in carbon dioxide levels in the lake in the past 30 years, says Martin Schmid, a researcher at the Swiss Federal Institute of Aquatic Science and Technology in Kastanienbaum. If this trend continues, the lake will be saturated within the century and, like Lake Nyos, it could erupt with even the slightest disturbance. At the moment, however, the closest Kivu comes to gas overload is at a depth of 330 metres, where the water is 55% saturated ? 10% with carbon dioxide and 45% with methane ? says Schmid, so an eruption is less likely. (Methane contributes most of the gas pressure and the risk of eruption because it is less soluble than carbon dioxide.) According to modelling work, only an intense eruption in the gas-rich depths would be powerful enough to overturn the lake 6 . The 2002 eruption barely affected its stability because the magma did not reach those depths 7 . \"The probability is low,\" says Schmid. \"At least I am not afraid of swimming in Lake Kivu.\" But Dario Tedesco, a volcanologist from the Second University of Naples in Caserta, Italy, who is writing the DRC's Mount Nyiragongo eruption contingency plan for the United Nations, says an eruption at the bottom of Lake Kivu is a possibility. Tedesco has studied bathymetric surveys of the lake and found cone-like structures that are probably volcanic in origin, he says. That evidence matches some other signs. During the 2002 Nyiragongo eruption, new fractures opened on the south side of the volcano, just a few kilometres from the lake. The lava from the fractures was also compositionally different from the volcano's crater lake 1 , suggesting that there are separate reservoirs of magma in the region, some of which could extend under the lake. \"Nyiragongo is going to erupt again,\" says Tedesco. \"The only real question is where.\" Tedesco's research at Kivu highlights its complexity. The lake contains at least five basins, with different characteristics and different probabilities of turnover 2 . The Kabuno basin, in the lake's northwest corner, has high gas concentrations only 12 metres below the surface. An eruption in Kabuno could release at least three times more gas than Lake Nyos did. Researchers agree that it is important to relieve gas pressure at Lake Kivu to avoid a natural disaster ? and the economic incentives are pushing that work forwards. The 60 cubic kilometres of methane equals roughly ten times the combined annual commercial energy needs of both the DRC and Rwanda. Tapping that reserve is particularly attractive to energy-starved Rwanda, and Lake Kivu has become the centrepiece of the country's plans to expand electricity production. Extracting methane from Kivu is not a new idea. A brewery in Rwanda burned the lake's methane to heat its boilers for 40 years before it shifted to electricity. But commercial interest in using methane to generate electricity only burgeoned in recent years, in part because of growing political stability in Rwanda. Around 60 companies, most of them from foreign countries, have approached the government for access to the lake since 2005, says Albert Butare, Rwanda's minister responsible for energy, water and sanitation. Rwanda has already handed out methane concessions totalling hundreds of megawatts to five consortia, including a US$325 million, 100-megawatt deal with ContourGlobal, an energy company based in New York. And in June, Rwanda and the DRC announced a new joint plan to develop an additional 200 megawatts from the lake. The growing interest in Kivu has kept scientists busy discussing best practices for methane removal. Most of the proposed models use a floating platform to suspend a vertical pipe down into the gas-rich layers. A small pump initially pulls up some of the bottom water to lower pressures, until it becomes saturated with gas and starts forming bubbles. After this priming, the bubbling drives water up the pipe without additional pumping. The extraction works like a controlled limnic eruption. The methane, being less soluble than the carbon dioxide, comes out of solution first. It is then piped onshore where it is burned to generate electricity. The problem is what to do with the carbon-dioxide-laden water. From the standpoint of safety, it would be ideal to extract the carbon dioxide and reinsert the degassed water into the deep parts of the lake, where it wouldn't disturb the equilibrium, says Kling. But removing the carbon dioxide makes the water lighter, hence less stable at depth. \"We don't want to generate any sinking or rising of water masses that will cause mixing,\" he says. Only water from the very bottom of the lake would still be dense enough to be reinserted in the deepest layers once it was degassed, but that would be prohibitively expensive. So Kling suggests keeping the carbon dioxide in the water. \"Every solution is a compromise,\" he says. \n                Conflict of interests? \n              Klaus Tietze, a geophysicist and the director of Physik-Design-Technology, a consulting company based in Celle, Germany, argues that with carbon dioxide concentrations in the lake increasing at a rate of more than 3% a decade, \"leaving the carbon dioxide in the lake is a very bad solution\". He is pushing for removing both methane and carbon dioxide as quickly as possible and reinserting the water above the main gas accumulations in the lake, so that it doesn't dilute the methane resource. Schmid discounts that plan, however, because the nutrients in the degassed deep water would overload the lake's upper layers. With so much disagreement among scientists, the best way forwards for developers is unclear, especially because the extraction technology has been problematic. So far only one 4-megawatt platform, called KP, is sporadically generating electricity for Rwanda's national grid. Another, a 3.6-megawatt project funded by the Rwandan Investment Group, sank last year a week before it was scheduled to begin production. Some have attributed the loss to sabotage but others have blamed bad engineering. Complicating the situation is the potential for conflicts of interest; some of the scientists who studied the lake and identified its hazard are now involved in the methane extraction projects. For example, Michel Halbwachs, a recently retired physicist from University of Savoie in France, has spent two decades studying Nyos and Kivu and now spearheads the Rwandan Investment Group's project. He says that the engineering of the sunken platform was sound and the company is now rebuilding it. Halbwachs says that his previous work on the lake is not a conflict of interest, but rather makes him uniquely knowledgeable about how to extract gases safely. Tietze agrees that only the few scientists who \"know the whole lake\" through decades of experience are qualified to run the methane projects. He is also trying to get into the extraction game and is now looking for investors. But other scientists worry that the safety of the lake could be compromised if the researchers who are assessing the gas hazard are also working on commercial methane extraction. The best interests of the lake's two million inhabitants could get lost along the way, says Tedesco, who calls the plans for tapping methane \"pure business\". \"This has nothing to do with the hazard of the situation,\" he adds. \"It's a little bit like someone has dropped free money on the street and everyone's running around trying to gather it up,\" says Kling. He is part of an international team that is working with the World Bank and the involved nations to create rules for methane extraction in the lake. The team wants every project to be monitored through a local institute, which then reports to an international group of experts. \"There has to be a separation between who is doing the work and who is checking the work,\" he says. In mid-June, the team came out with its first draft of legislation, which will now have to get parliamentary backing both in Rwanda and in the DRC. Although, in theory, removing the methane should make the lake safer, it remains unclear whether the hazard will be reduced. The plans for degassing are preliminary, and no one knows how they will affect the lake's stability. The chances of avoiding a disaster depend on many factors, some well beyond the control of the scientists or even the governments in that region. Armed militias in the DRC recently took over four of the area's seven seismic monitoring stations, hampering the ability of volcanologists to predict when Nyiragongo will erupt. And in the past few months, renewed fighting in rural areas has displaced hundreds of thousands of people. Goma's population has almost tripled since the 2002 Nyiragongo eruption, to an estimated 1.2 million people. There are shanties crammed into every usable piece of land, many constructed from lava blocks from the volcano's last eruption. So as scientists and developers fight over Lake Kivu's methane resource, the displaced people remain pinned between a volcano, the militias and an explosive lake. Anjali Nayar is an International Development Research Centre fellow at  Nature . \n                     Nature Geoscience \n                   \n                     Contour Global's website \n                   \n                     Rwanda Energy Company \n                   \n                     George Kling's homepage \n                   \n                     Martin Schmid's homepage \n                   Reprints and Permissions"},
{"file_id": "460454a", "url": "https://www.nature.com/articles/460454a", "year": 2009, "authors": [{"name": "Ehsan Masood"}], "parsed_as_year": "2006_or_before", "body": "Maurice Strong has shaped how nations respond to planetary crises. Ehsan Masood meets the man whose successes \u2014 and failures \u2014 laid the groundwork for the current climate talks. In 1971, Maurice Strong did something most people would find unthinkable: he showed up at the office of India's Prime Minister Indira Gandhi to tell her that she was wrong. The formidable Gandhi, like the leaders of other developing nations, wanted to boycott the United Nations' first major international environmental conference, planned for the following year in Stockholm. The developing nations feared that environmental agreements would undermine their ability to prosper \u2014 a concern that still resonates today. As the leader of the upcoming conference, Strong regarded the developing nations as essential players. So, against the advice of Indian diplomats, he flew to New Delhi and urged Gandhi to drop the boycott and attend the conference as an advocate for the developing world. \"She had a habit of falling silent during conversations and said absolutely nothing for 10 or 12 minutes,\" Strong recalls. \"Now, I've lived among the Inuit, and they, too, fall silent during conversations, although for much longer periods, so this didn't bother me at all. I waited for her reply and when she finally spoke, she said 'yes'.\" Strong has spent much of his life trying to make the world see matters his way, sometimes by remaining silent but more often through a mixture of negotiation and persuasion. He helped to found the United Nations Environment Programme (UNEP) in 1972. He chaired the 1992 Earth Summit in Rio de Janeiro, Brazil, that produced the first climate treaty. And he mentored a generation of environmental, political and business leaders around the world. Many of them gathered in Gland, Switzerland, this month at the headquarters of the International Union for the Conservation of Nature (IUCN) to celebrate Strong's 80th birthday. It was also an occasion to take stock of the planet and discuss the round of climate negotiations that will conclude in December in Copenhagen. The chances for success in Denmark depend to a large degree on what Strong has accomplished over the past 40 years, and on what he was unable to achieve. From the beginning, Strong had outsized ambition. Born into poverty in the Canadian town of Oak Lake, Manitoba, at the start of the Great Depression, he writes in his autobiography  Where on Earth are We Going?   that his childhood dream was to devote his life to the protection of nature and to work for world peace, having lived through the Second World War and seen its effects on humans and on the environment. When the war ended and countries were discussing the establishment of the UN, Strong says, \"I wrote in my diary: 'this is where I want to be'.\"  \n                Ascent of nations \n              Strong started at the bottom, working in the office that issues security passes at the UN building in New York. Later he would rise, via posts in the Canadian government and in business, to chair the Stockholm and Rio conferences on the environment. He helped to set up Canada's national oil company Petro Canada as well as its International Development Research Centre, and he led the team that proposed a plan to reform the UN. Strong's childhood instilled a determination never again to be poor, and he grew rich leading various companies and through property investment. In many ways, Strong's greatest legacy is the creation of UNEP, which emerged out of the 1972 Stockholm conference and has since played a catalytic part in international environmental issues. The organization oversaw the process that created the Montreal Protocol of 1987, a treaty to phase-out ozone-depleting compounds. In 1988, UNEP joined with the World Meteorological Organization to create the Intergovernmental Panel on Climate Change. Four years later in Rio, UNEP and the environmental movement shaped new international agreements on climate change and biodiversity, and laid the groundwork for a treaty to combat desertification. The climate convention provided the road map for the Kyoto Protocol \u2014 in which nations pledged to cut greenhouse-gas emissions \u2014 and established the process for the climate talks.  \n                Bridge-building \n              For the Stockholm and Rio conferences to succeed, Strong knew he needed to earn the trust of developing countries. But he also built bridges with the scientific and business communities and he opened up both events to participation from non-governmental organizations \u2014 often against the wishes of governments. \"Maurice really pushed the door for public engagement,\" says James Gustave Speth, a former administrator of the UN Development Programme and founder of the World Resources Institute, an environmental think-tank in Washington DC. \"You have to remember that this had never happened before and it had ramifications across all other UN processes.\" Yet despite Strong's successes in bringing people and nations together, the policy architecture he created has done little so far to make the world safer. Aside from the successful global effort to protect the ozone layer, international environmental agreements have faltered. In particular, they lacked mechanisms to verify whether countries met their commitments or to penalize those that fell short. This led to the anticipated failure of the Kyoto treaty, and will also doom the parallel commitment to slow down the rate of loss of biodiversity by 2010. The consensus at the meeting in Switzerland was that the fault lies with the current environmental policy structure. Strong acknowledges that the treaties have suffered because they allowed \"governments to say one thing, but do the opposite\". The fault may lie with the expectations that Strong and others had. The entire environmental movement had a misplaced confidence in the ability of governments to deliver on their promises, says Speth. That realization has tempered hopes for the climate negotiations this year, and Strong doubts whether December's Copenhagen climate summit will succeed. \"Radical thinking is needed,\" he says, \"but there is no evidence that governments are ready for that.\" There is no point to an agreement, he says, unless it has \"binding real penalties, fines and trade bans that are designed to make agreements enforceable\", rather like what happens in the World Trade Organization or the International Atomic Energy Agency. Another problem with past environmental agreements is the relationship between the richer and poorer countries. Developing nations have long insisted that if they were going to be forced to grow sustainably \u2014 unlike the richer nations \u2014 then substantial financial support should be made available. But the wealthy nations baulked at that request, in part because they didn't want to be seen as accepting responsibility for global environmental problems. It has taken nearly four decades for a potential solution to emerge. UK Prime Minister Gordon Brown proposed last month to start a climate fund that would give US$100-billion to developing countries each year, paid for by the world's wealthiest nations. A third issue that has dogged Strong is his poor relations with US administrations \u2014 and his belief that global agreements can work without the United States. \"I did spend a lot of time in America,\" he says. \"But America already had many people who could speak on its behalf. Yes, I could have put more time in, but even the Americans were telling me 'you've got to get the developing countries on side'. I used to tell the Americans that if I am seen to be in your pocket, then I am of no use to anyone.\" To this day, Strong gets bad press in the United States, where reporters often focus on potential conflicts of interest that affected him while he was working in the upper tiers of the UN.  \n                Little black book \n              Yet Strong has no shortage of supporters, especially among the world leaders he helped to groom. He has a legendary record in spotting promising individuals, which is evident in the roll-call of people whom he hired early in their careers. Canada's former prime minister Paul Martin, James Wolfensohn, who led the World Bank, Yolanda Kakabadse, former environment minister of Ecuador and now president of the environmental group WWF, and many serving heads of UN agencies are Strong prot\u00e9g\u00e9s and cite him as a major influence in their lives. When the IUCN was having financial problems in the 1980s, \"Maurice stepped in and came to our rescue\", says Julia Marton-Lef\u00e8vre, IUCN director-general. \"He would make the phone calls or get us a seat at the right table,\" she says. \"Maurice had this ability to reach out and grab world leaders by the arm, whether they liked it or not,\" says Jonathan Lash, president of the World Resources Institute. \"He used his access and networks shamelessly for all of us. Long before iPhones, the net and Twitter, we had Maurice.\" At the start of the Rio conference, Strong had insisted (and as usual got his way) on a private meeting between himself, the UN secretary-general and more than 100 heads of government, without their minders and attendants. In his address to them, Strong said that the planet had only one real chance, and that they stood between success and failure. Strong was wrong about the one chance. The Rio climate treaty, and its progeny, did not stop global warming, and there is debate about whether they have reduced the problem in any substantial way. But politics and the climate have evolved since Strong urged leaders towards action. The world has yet another chance in Copenhagen and the outcome depends largely on whether countries have learned from their past mistakes. Ehsan Masood teaches international science policy at Imperial College London. \n                     Nature Reports Climate Change \n                   \n                     Climate Feedback Blog \n                   \n                     Maurice Strong \n                   \n                     United Nations Framework Convention on Climate Change \n                   \n                     United Nations Environment Programme \n                   Reprints and Permissions"},
{"file_id": "460450a", "url": "https://www.nature.com/articles/460450a", "year": 2009, "authors": [{"name": "Emma Marris"}], "parsed_as_year": "2006_or_before", "body": "A small group of ecologists is looking beyond the pristine to study the scrubby, feral and untended. Emma Marris learns to appreciate 'novel ecosystems'. Joe Mascaro, a PhD student in a T-shirt and floral print shorts, is soaking in the diversity of the Hawaiian jungle. Above, a green canopy blocks out most of the sky. Aerial roots wend their way down past tropical trunks, tree ferns and moss-covered prop roots to an understorey of ferns and seedlings. The jungle is lush, humid and thick with mosquitoes. It is also as cosmopolitan as London's Heathrow airport. This forest on Big Island features mango trees from India ( Mangifera indica );  Cecropia obtusifolia , a tree with huge star-shaped leaves from Mexico, Central America and Colombia; rose apples ( Syzygium jambos ) from southeast Asia; tasty strawberry guava ( Psidium cattleianum ) from the threatened Atlantic coast of Brazil; and a smattering of Queensland maples ( Flindersia brayleyana ) from Australia. It also has candlenuts ( Aleurites moluccana ), a species that humans have moved around so much that its origins have become obscure. There is at least some native Hawaiian representation in the form of hala, or screwpine ( Pandanus tectorius ), which is pictured on the crest of Punahou School, where US President Barack Obama studied. There are no Hawaiian birds here though. Mascaro sees plenty of feral pigs, descendants of those brought by settlers from other parts of Polynesia or from farther afield. The soil is black and rich. Mascaro likes it here. Most ecologists and conservationists would describe this forest in scientific jargon as 'degraded', 'heavily invaded' or perhaps 'anthropogenic'. Less formally, they might term it a 'trash ecosystem'. After all, what is it but a bunch of weeds, dominated by aggressive invaders, and almost all introduced by humans? It might as well be a city dump. A few ecologists, however, are taking a second look at such places, trying to see them without the common assumption that pristine ecosystems are 'good' and anything else is 'bad'. The non-judgemental term is 'novel ecosystem'. A novel ecosystem is one that has been heavily influenced by humans but is not under human management. A working tree plantation doesn't qualify; one abandoned decades ago would. A forest dominated by non-native species counts, like Mascaro's mango forest, even if humans never cut it down, burned it or even visited it. No one is sure how much of Earth is covered by novel ecosystems. To help with this article,  Nature   asked Erle Ellis at the University of Maryland, Baltimore County, who produces maps of ways that humans use Earth, to take a stab at quantifying it. Defining novel ecosystems as \"lands without agricultural or urban use embedded within agricultural and urban regions\", Ellis estimates that at least 35% of the globe is covered with them (see  map ). Their share of the planet will probably expand, and many ecologists think that these novel ecosystems are worthy of study and, in some cases, protection. For one thing, some novel ecosystems seem to provide a habitat for native species \u2014 sometimes crucial habitat, if all that the species originally had is gone. They also often do a good job of providing 'ecosystem services', those things that nature does that benefit humanity, such as filtering water in wetlands, controlling erosion on hillsides, sequestering carbon from the atmosphere and building soil. Provision of ecosystem services is a popular argument for preserving intact ecosystems, but many of its advocates blanch a little when it comes to making the same case for these 'weedy' areas. Mascaro actually prefers novel ecosystems to some native ones that are so vulnerable to damage by humans that they require intense management to maintain in their 'pristine' state. He sees the latter as museum-piece parks. \"Do we value the fact that nature contains a list of things that were there 1,000 years ago, or do we value it because it has its own processes that are not under human control?\" Mascaro asks. For him, the value is in the processes.  Watching such processes unfold has scientific merit to many researchers. Novel ecosystems are often ideal natural experiments for studying things such as community assembly \u2014 how species find their way to a place and which species become permanent residents \u2014 and evolution of species in response to one another. In essence, it takes a dynamic ecosystem to study ecosystem dynamics, and these novel ecosystems are the planet's fastest movers. Mascaro bets that all the rules of thumb and general relationships developed over the years by ecologists working in 'intact' or 'historical' ecosystems will probably also apply in these new assemblages, but no one knows for sure, because no one has studied them much. There are some questions about the ways in which things might be different in novel ecosystems. Will landscape types remain the same, with forests replacing forests and grasslands replacing grasslands? Will novel ecosystems evolve faster? Will they be dominated by one species, as many who study invasive species fear? Will species composition oscillate wildly for decades or even longer? \"We can't know except to observe it,\" says Mascaro.  \n                Havens of biodiversity? \n              One of the first researchers to see the importance of the scrubby parts of Earth was Ariel Lugo, a forest-service ecologist in Puerto Rico. In 1979, Lugo was managing researchers who were measuring the ground covered by trees within pine plantations that were not being actively managed. His technicians came back to headquarters sweaty and discouraged. \"They said that they couldn't measure the trees without clearing all the new undergrowth,\" says Lugo. \"They said it was impenetrable. I thought they were wimps.\" The idea that ecosystems dominated by pine, an invasive species, were so thick that his workers couldn't even walk through them went against a central assumption of ecology: that native forests will be the lushest. Millennia of co-evolution should have created an ecosystem in which almost every niche is filled, converting the available energy into trees and other species in the most efficient way. Conservationists also generally assume that native ecosystems contribute best to ecosystem services. Lugo went to see for himself. Sure enough, the pine plantations were bursting with vigour, far more so than nearby native-only forests of the same age. Lugo did a systematic study of the pine plantations and some mahogany ones, and found that the plantation understoreys were nearly as species rich, had greater above-ground biomass (the sheer weight of all the living things) and used nutrients more efficiently than the native forest understoreys. He submitted his results to the journal  Ecological Monographs 1 . Reviewers were horrified. In the end, it took almost a decade to get the paper past peer review. Since then, Lugo has found many novel ecosystems in Puerto Rico and elsewhere that are much more diverse than native forests, but that are largely ignored by ecologists. \"That diversity doesn't count because they are the wrong species,\" says Lugo, shaking his head. He's found alien trees that, by creating a shaded canopy on parched, degraded pastureland, make possible the establishment of native trees that could never cope with such an environment on their own. As a result he now finds it difficult to despise invasive trees as he thinks his colleagues do, and even embraces the change. \"My parents and their parents saw one Puerto Rico,\" he says, \"and I am going to see another Puerto Rico, and my children will see another.\" Lugo wasn't the only researcher thinking along these lines, but it was not until 2006 that the new approach gained a manifesto \u2014 and a name. Lugo and 17 other researchers published a paper called \"Novel ecosystems: theoretical and management aspects of the new ecological world order\" 2  suggesting that such systems were worth scientific attention. To demonstrate the depth of resistance to the idea, the published paper quoted referees' comments on the submitted manuscript: \"One reviewer commented that the examples are ecological disasters, where biodiversity has been decimated and ecosystem functions are in tatters, and that 'it is hard to make lemonade out of these lemons'.\" But Lugo and his colleagues saw it in a different light: \"We are heading towards a situation where there are more lemons than lemonade,\" they wrote, \"and we need to recognize this and determine what to do with the lemons.\" Lemons can have their own value, says restoration ecologist Richard Hobbs, lead author of the paper and now at the University of Western Australia in Crawley. Some novel ecosystems, he says, are \"alternative stable states\", relatively entrenched ecosystems that would be very difficult to drag back to historical conditions. Around the time the paper came out, Mascaro became interested in Lugo's work and set out to see if his results could be replicated on the windward side of Hawaii's Big Island. Were the many novel ecosystems on the islands nurturing any native species? Were they providing ecosystem services? He studied 46 forests growing on lava flows of varying ages at various altitudes and dominated by a variety of species, including albizia ( Falcataria moluccana ), a fast-growing tree from southeast Asia, and Australian ironwood ( Casuarina equisetifolia ). He found that, on average, the forests had as many species as native forests. But by and large they weren't incubating natives as they seemed to in Puerto Rico 3 . Part of the reason for the difference may lie in the uniqueness of Hawaiian flora, which evolved in isolation for up to 30 million years 4 . Not many plants got to Hawaii in the first place, so competition and predation pressures weren't very fierce. Without having to worry about being eaten by anything larger than an insect, raspberries and roses lost their thorns and mints lost their minty defence chemicals. When people introduced plants from other parts of the world, along with their attendant herbivores, Hawaiian plants couldn't compete.  \n                Futuristic perspective \n              But Mascaro's results didn't put him off the novel-ecosystem concept. For one, he found that in many measures of forest productivity, such as nutrient cycling and biomass, novel forests matched or out-produced the native forests. They might not be 'natural' in the eyes of purists, but they are behaving exactly as they should. \"These ecosystems, like it or not, are going to be driving most of the natural processes on Earth,\" he said at the 2008 Ecological Society of America meeting in Milwaukee, Wisconsin. It's a message that Peter Kareiva, chief scientist at the Nature Conservancy in Seattle, Washington, wants to see move from the academic world to the world of conservation management. \"You hear conservationists talk about what they want to save, what they want to stop,\" he says. \"They should talk about what they want the world to look like in 50 years.\" Studies of novel ecosystems could help conservationists to \"face the facts and be strategic\", Kareiva says, rather than trying to beat back the unceasing tide of change. Kareiva is a great fan of the ecosystem-services argument for preserving nature. But he admits that the problem of what to do when novel ecosystems provide better services than the native ones is \"a question we don't talk about that much\". Nevertheless, he is willing to imagine a world in which, for example, exotic strains of the reed  Phragmites   are allowed to thrive in US wetlands because they provide a great habitat for birds, rather than be torn out in an expensive and potentially fruitless attempt to return native vegetation to dominance. Ecosystem-service arguments are powerful enough to get some ecologists to abandon, or at least put to one side, their deep distrust of novel ecosystems. Like many of his peers, Shahid Naeem, an ecologist at Columbia University in New York, says he \"would love to get rid of every invasive species on the planet and put all the native species back in their place\". Yet he's willing to see what can be made of novel ecosystems as he feels an imperative to improve conditions for the billions of humans on Earth. The idea that novel ecosystems provide welcome diversity has also gained traction. Thinking on 'invasive species' has mellowed significantly since the field was first established in the 1950s. Newer work by the likes of Mark Davis at Macalester College in Saint Paul, Minnesota, and Dov Sax at Brown University in Providence, Rhode Island, has shown that the vast majority of species that humans move around can slot into new ecosystems without driving anything else extinct, and that the common vision of invasive plants forming dense monocultural stands that take over everything else in their path is actually the exception. Yet the newcomers in novel systems can still be a genuine worry. Peter Vitousek, an expert on Hawaiian biodiversity at Stanford University in California, would put albizia forests in the category of dangerous invaders, because they wipe out stands of the native '\u014dhi'a tree ( Metrosideros polymorpha ). He acknowledges the services that novel ecosystems provide and that \"they may even support native biological diversity in some important circumstances\". But, he adds, \"as with many good ideas, [tolerance of novel ecosystems] can be taken to an extreme at which it is no longer useful. I think most of the albizia-dominated stands of Hawaii represent that extreme.\" His point is well illustrated where one of Mascaro's albizia forests abuts a native '\u014dhi'a forest. The albizia trees on the boundary actually lean out towards the '\u014dhi'a \u2014 growing sideways to escape the shade of the next row in, encroaching on the natives' sunlight and looking poised to usurp them. It is a menacing spectacle, and an apt symbol for their tireless expansion. Mascaro grants the point. \"I can understand where a manager wants to bulldoze an albizia forest if they are worried that it is going to exterminate an ecosystem type that is the last on Earth,\" he says. \"If we want to debate whether to use or conserve novel ecosystems, we will always have to deal with the risk they pose to other systems. But at the moment, we're scarcely debating it at all.\" Novel ecosystems are likely to cause at least some extinctions. For example, species that have evolved dependent relationships with other species are less likely to do well in a world in which the pot is stirred and everything is redistributed. Hawaiian honeycreepers, beautiful birds that often feed only on one type of flower, are not doing well; several are already extinct. So for those who care about slowing or stopping the rate of such extinctions, novel ecosystems are a net negative. James Gibbs, an ecologist at the State University of New York in Syracuse, subscribes to this view. \"I think celebrating [novel ecosystems] as equivalent or improved is not appropriate.\" As an example, he points to Clear Lake in Northern California, where the number of fish species has risen from 12 to 25 since 1800. Sounds like a success story. But, says Gibbs, species that were found only in that lake were replaced with fish that are common elsewhere \u2014 so there was a net loss in biodiversity. A similar caveat may hold for the genetic diversity hidden within a species. Forests dominated by the offspring of a handful of exotic colonizers could be less genetically diverse than forests that have sat there for thousands of years.  \n                A question of values \n              In the end, the question of novel ecosystems, like so many questions in ecology and conservation, boils down to what should be valued most in nature. For people who value processes, such as Mascaro, novel ecosystems are great hubs of active evolution. For those who value ecosystem services, any novel ecosystem could be better or worse than what came before depending on how it operates. For those who care about global extinctions or about preserving historical ecosystems, they are bad news. Gibbs says he values the exquisite complexity of ecosystems that have evolved together over thousands or millions of years. \"Why are we worried about the extinction of languages, the roots of music, all these weird cuisines?\" he asks. \"There is something about diversity and our need to steward it. It is the subtlety and the nuance and complexity that makes life interesting.\" Novel ecosystems seem, to him, to lack this value, to be samey and artificial, \"sort of like eating at McDonalds\". To Kareiva, though, that attitude is \"one of the reasons the conservation movement is failing. To think there is some kind of garden of Eden pristine ecosystem. There is none! That view is just going to get us nowhere.\" Indeed, the Garden of Eden view, in which ecosystems are static, is no longer widely held. This means that novel ecosystems, far from being a new phenomenon, simply represent the latest changes on a dynamic Earth. Gradual climatic changes and sheer randomness mean that some species wander around continents over vast timescales, fleeing glaciers, splitting up and reforming. This is why Davis and some others do not like the 'novel' label. \"Ecosystems are always new, from one year to the next,\" says Davis. \"Ecosystems are always encountering new species \u2014 it might be not from another country but from 100 metres upstream. Much more accurate would be to refer to these as 'rapidly changing' ecosystems \u2014 but I guess that is not catchy enough.\" Standing in his Hawaiian forest, Mascaro is all too aware of change \u2014 and it is something he values, even if humans did have a hand in the process. He never swore allegiance to preserving ecosystems as they were before humans arrived, as many conservationists of an older generation did. \"People come up to me and say 'it sounds like you've given up,'\" says Mascaro. \"I want to say 'I never took up arms, my man'. This isn't about conceding defeat; it is about a new approach.\"\n \n                 See Editorial,  \n                 page 435 \n               \n                     Nature Reports Climate Change \n                   \n                     Climate Feedback \n                   \n                     Evolution & Ecology \n                   Reprints and Permissions"},
{"file_id": "460568a", "url": "https://www.nature.com/articles/460568a", "year": 2009, "authors": [{"name": "Jeanne Erdmann"}], "parsed_as_year": "2006_or_before", "body": "Multiphoton microscopy is allowing immunologists to watch infections as they happen. Jeanne Erdmann pulls up a seat. Behind the heavy black curtains of his microscopy room, Mark Miller is shooting an action movie. He gives the settings on the multiphoton microscope a once-over while his senior scientist Vjollca Konjufca checks the sedated mouse on the warmed stage. Then Miller flicks a switch on the scanner and red, blue and green images flicker on the computer monitor. He points to what he's looking for. \"Right there,\" he says. Blue-labelled  Salmonella   bacteria gather near the top of a red villus, a finger-like projection from the wall of the mouse's small intestine. The bacteria look like helicopters buzzing around a mountain. It's early afternoon, and we are settling in to watch these bacteria for the next hour. Miller's experiments, and others like it, are not just gripping the audience behind the curtain \u2014 they are gripping a much broader audience of immunologists. Miller, at Washington University School of Medicine in St Louis, Missouri, runs one of the leading labs using multiphoton microscopy to watch infections in living animals in real time. Less than a decade ago, Miller and other immunologists mostly studied the process of infection  in vitro , mixing pathogens and the cells they interact with in a culture dish. These simulations had their limits because the cells, much like animals in a zoo, were removed from the environment that influences their behaviour. The advent of multiphoton microscopy allowed researchers to view deep inside living tissues and watch cell biology live and 'in the wild'. This is particularly valuable for the immune system, in which the component cells roam throughout the body's landscapes interacting with pathogens and surrounding cells. Miller likens multiphoton imaging to a \"naturalist studying a herd of gazelles. You infer their function from the behaviour you observe,\" he says. \"The beauty of multiphoton is that you don't have to know what it is you're looking for ahead of time. It just shows itself. We often find things we didn't expect.\" One thing that researchers didn't expect was the dramatic changes that take place in the first few minutes or hours of an infection, long before symptoms occur. That drama is what's keeping them glued to the screen. \"Reality has been eye-opening,\" says Ronald Germain, an immunologist at the National Institutes of Health in Bethesda, Maryland, who works with the technique.  \n                Set dressing \n              Konjufca started her experiment at 11:00 a.m. She took a mouse that had been genetically engineered so that several types of cell in the immune system \u2014 including neutrophils, lymphocytes, dendritic cells and macrophages \u2014 emit light in the fluorescent microscope. Then she sucked into a syringe some 10,000  Salmonella , also labelled with a fluorescent tag, and injected them into the mouse's intestine. She also injected a fluorescent label that will be taken up by epithelial cells lining the villi, painting them red. Konjufca does about two experiments like this every week as part of a project to study how  Salmonella   invades the body to cause food poisoning, work she hopes might help produce an oral animal vaccine that protects the food supply. She wants to work out the exact sequence of events during an infection: how the bacteria breach the gut wall and various immune cells arrive on the scene; how dendritic cells in the gut villi engulf and process some of the  Salmonella ; and then which cells transport the broken-down  Salmonella , in the form of antigens, to the lymph nodes and spleen. \"We don't necessarily know which cells go where and at which time point,\" says Konjufca. For her experiments she uses both wild-type bacterial strains and ones with different virulence genes mutated. Mutations introduced into  Salmonella   to make a harmless vaccine strain often make the bacteria less able to invade the intestine and provoke the immune system, says Konjufca. The trick is to make a harmless  Salmonella   that still induces a strong and lasting immune response. Konjufca is trying to understand this by watching the mouse's response to these differing strains. None of the work has been published. For now, she is still hammering out the technical details, such as how many bacteria to inject. Too few, and she may not recruit enough of the white blood cells such as neutrophils, the first responders to infection. Too many, and she will no longer be mimicking a physiological response. She also needs to figure out how long to wait after injecting the bacteria. If she waited, say, three to five hours, they may have already exited the small intestine and she would miss what happens there. Today she plans to wait around two hours. Ironing out these technical details is more than half the battle; otherwise there will be nothing to see.  \n                Special effects \n             \n               boxed-text \n             What is routine for Konjufca today was ground-breaking in 1990, when multiphoton microscopy made its debut 1 . In standard confocal microscopes, a single photon of light excites an electron in a fluorescent tag called a fluorophore, and light is emitted as the electron drops back to its ground state. But the high energy of light needed to excite the fluorophore quickly breaks it down and is damaging to cells. The innovation of Winfried Denk, now working at the Max Planck Institute for Medical Research in Heidelberg, Germany, was to excite the fluorophore using the simultaneous arrival of two or more photons of longer-wavelength light. These wavelengths are less damaging and, most importantly to immunologists, they typically penetrate at least 200 micrometres into tissue. Neurobiologists, desperate to see deeper into the living brain, seized on the technique. Immunologists were not far behind. Miller began using the technique in the early 2000s, while working as a postdoc in Michael Cahalan's lab at the University of California, Irvine, and in collaboration with neurobiologist Ian Parker, also at Irvine. He viewed the compartment of the lymph nodes where naive T cells, a type of lymphocyte, recognize antigen and become activated, a necessary step in readying the cells to respond to future infections. When Miller tried out Parker's multiphoton system on a lymph node snipped out of a mouse, his first reaction was surprise. He thought the T cells would be moving in unison along a gradient of chemicals called cytokines. Instead, he saw them weaving around very fast. \"I couldn't believe how quickly and randomly T cells moved,\" he says. \"They looked so purposeful and excited.\" Miller used these images to calculate the velocity of the cells, showing that they can reach speeds greater than 25 micrometres a minute. And he suggested that the random movement has a purpose, by helping a T cell range across a broad territory and find the 'antigen-presenting cell' carrying the precise antigen it recognizes. In 2002, the work was published in  Science 2  as part of a trio of advanced microscopy papers exploring the dynamics of T cells 3 ,   4 . At that time, Miller estimates that only a handful of immunology labs were using multiphoton microscopy. Researchers were excited to just observe the cells and get a picture of how these dynamics build up the host's defences. \"It revolutionized the ideas that immunologists had about how immune responses come about,\" says Ulrich von Andrian, an immunologist at Harvard Medical School in Boston, Massachusetts.  \n                Live action \n              These days, Miller says, it seems as though every major immunology department has set up a multiphoton system or wants to. \"I get requests for advice all of the time.\" And much of the work has gone from studying the immune cells on their own to the infection as a whole. This makes for a more complex experiment. The tissue must be kept still, insulated from the pumping of the heart or the contractions of the gut, and the pathogen must be introduced without disturbing the surrounding tissue. When microbiologist Agneta Richter-Dahlfors, of the Karolinska Institute in Stockholm, mastered the technique to watch the first few hours of a kidney infection, she worked with surgeons to inject bacteria into single rat kidney nephrons the size of a human eyelash. The images, some of the first showing an infection in real time, revealed that just two to three bacteria are enough to attach to the mucosal membrane and set up a colony 5 . Within three hours, the oxygen tension in the nephron drops to zero and blood flow to the area halts. \"These analyses cannot be performed in any setting other than a live animal,\" says Richter-Dahlfors. Researchers have now observed a range of bacterial infections, including  Listeria monocytogenes   in the mouse's footpad, a model system that Miller developed with his postdoc Bernd Zinselmeyer to show how immune cells are recruited in the early stages of infection 6 . Two and a half hours after Konjufca injected the  Salmonella , she reaches into the cage and gently picks up the sedated mouse. She puts it on the microscope stage, and covers it with a velvety, mouse-sized sheet to keep it as comfortable as possible and maintain a stable body temperature. Miller started working with  Salmonella   in the past two years. He likes studying bacterial infections in the gut because it's a common route of infection. It's also home to non-pathogenic bacteria, so he can explore how immune cells tell friend from foe. Miller built his own multiphoton imaging system at a cost of about half-a-million dollars by customizing a standard fluorescent microscope. It can probe hundreds of micrometres deep into tissue and record images at video rate. Today, he moves the microscope stage around until he finds an area of the small intestine largely empty of undigested food. Along the arc of the villus, the square epithelial cells show up like a necklace of perfect teeth. Some of the bacteria have already crossed the epithelial barrier and found their way to the lamina propria, an area at the bottom of the villi that is rich in dendritic cells and other antigen presenting cells.  \n                Towards the talkies \n              Researchers aren't certain what happens at the epithelium during an infection. One possibility is that the  Salmonella   infect and then kill the epithelial cells, releasing bacteria that dendritic cells engulf. These cells process the bacterial constituents and stick them on their surface as antigens that will stimulate an immune response. Another idea has come from Germain's work suggesting that dendritic cells reach extensions across the lumen and fish bacteria out 7 . Miller collects a stack of images from multiple villi, at intervals of 20\u201330 seconds, data the computer will compile into three-dimensional time-lapse videos. By 2.30 p.m., the team has watched most of the  Salmonella   reach dendritic cells at the base of the villi. Miller shuts off the microscope, turns on the lights, and pulls back the curtain. The team transfers the data to a computer in an adjoining room. Microscopy doesn't provide as complete a view of the infection process as researchers might like. \"There's a lot of people thinking in molecular terms for their entire careers,\" says Miller, \"and they're resistant to the idea that imaging is providing useful information because they don't feel it's quantitative.\" But he and other advocates of the technique disagree. They say that the quantification comes from measuring how fast cells move, which direction they move in and whether or how often they stick to other cells. They say that what's missing is a qualitative aspect that gives more information about why cells behave in certain ways. Following cells to an infection site is good, but what happens when they get there? Are the cells signalling, and if so, how? Immunologist Phillipe Bousso, of the Pasteur Institute in Paris, compares the footage to silent movies: showing how cells move and interact, but not how they communicate biochemically 8 . What would give them 'sound', he says, would be biochemical or genetic assays that could be used in conjunction with microscopy. Jost Enninga, a biochemist also at the Pasteur Institute, is developing and patenting such assays, including one designed to trigger fluorescence in epithelial cells when they encounter pathogens. Another idea is to combine microscopy with real-time monitoring of bacterial gene expression, by dissecting out and analysing tissues at various time points. Another limitation with microscopy is that the scene is not complete. Any cells not engineered to fluoresce disappear into the black background. \"[Researchers] know the cells are there but don't necessarily think it's a disadvantage not to see them,\" says von Andrian. \"If everything revealed itself it would be chaos.\"  \n                It's a wrap \n              Later that afternoon, and over the next week, Miller and Konjufca review the videos of the experiment and compare them to earlier ones. They are pleased to see that in the most recent movies the villi are well defined and the bacteria visible on both sides of the epithelium. It shows that they got the set-up right, and that the timing between the injection and the imaging was spot-on. A film of an earlier experiment shows some of the neutrophils in local blood vessels rushing by at several hundred micrometres per second, while other neutrophils respond to inflammatory signals and slow down. The slowed cells then crawl along the inside of the vessel until they reach one spot where they all seem to squeeze through the wall and pour out of the blood vessel like bees swarming through a crack in a fence. In her next experiments, Konjufca will wait longer before filming, so she can start to study in more detail how the cells cross the epithelium and which cells transport bacteria away from the gut. She'll then follow them to the liver and spleen to study how  Salmonella   interact with T cells and dendritic cells there. She hopes to understand how all this is different in the attenuated vaccine strains. And Miller is working with a computer scientist to develop software to track cells outside the field of view.The microscopic field is small and cells wander in and out of sight. He also wants to analyse their migratory behaviour as it's happening. Even without these special effects, the surprise and discovery that each movie brings is enough to keep him going, Miller says. \"That's the excitement that you get when you're in the lab. That's the motivation,\" he says. \"'What are we going to see today?' We may have some ideas but we may be completely surprised.\" \n                 See multiphoton movies at  \n                 \n                     http://tinyurl.com/n35ufp \n                   \n                  and  \n                 \n                     http://tinyurl.com/mb6l45 \n                   \n               Jeanne Erdmann is a freelance writer based in Wentzville, Missouri. \n                     Microscopy special \n                   \n                     Dynamic imaging of the immune system \n                   \n                     Mark Miller \n                   Reprints and Permissions"},
{"file_id": "460564a", "url": "https://www.nature.com/articles/460564a", "year": 2009, "authors": [{"name": "Erik Vance"}], "parsed_as_year": "2006_or_before", "body": "A vast supply of energy is racing around the planet far above the surface. Erik Vance meets the engineers trying to bring the power of high-altitude wind down to earth. A ride on a kite boat might just kill you from fright \u2014 if it doesn't crush you first. The vessel is essentially Fan 8-metre catamaran dragged behind a kite the size of a movie screen \u2014 a lot of horsepower for such a small craft. That concern grips me as I skitter across the surface of San Francisco Bay at 40 kilometres per hour. Sudden tugs from the kite jerk the boat from side to side and sometimes nearly out of the water. But we stay upright, thanks to the skill of the boat's inventor, Don Montague. The 46-year-old pioneer of kite surfing deftly keeps the 200-kilogram craft from flipping up with each gust and crushing its passengers beneath it. Montague has no intention of selling this floating hazard on the open market \u2014 he created it to break speed records and amuse himself. Today he has brought me out to demonstrate the raw power in the skies, just waiting to be tapped. Montague is part of the high-altitude wind movement, promoting an idea on the very fringes of energy development: tethered airborne devices that collect energy from the wind far above the surface. The designs of the aircraft vary widely, as does the height at which they would fly, but all seek to exploit the fact that the farther one goes from the ground, the stronger and steadier the available wind. For at least a century, engineers have dreamed of pulling electricity from high in the atmosphere. However, only recently have lightweight materials and computer guidance systems emerged that make the idea feasible. In the past five years, what was once seen as a crackpot scheme has entered the early stages of a research-and-development race that is attracting tens of millions of dollars from major private backers such as Google. Leading the way is a tiny, dedicated community of Californian inventors and kite surfers. As yet, they have generated little electricity and failed to win over mainstream wind experts. But several start-up companies \u2014 among them Makani Power, the one Montague has co-founded \u2014 plan to put prototypes into the air within the next 18 months. This moment is overdue for Ken Caldeira, a climate researcher at Stanford University, California, who has long promoted generating power from high-altitude wind. He says wind in some locations contains at least ten times the energy of sunlight, when measured by surface area, but that conventional surface-mounted wind turbines collect only a fraction of that because friction with the ground brakes winds at the surface. Caldeira's favourite area of study is the powerful high-altitude air currents called jet streams, which taken together contain 100 times as much energy as humans use today. They are \"the highest concentration of renewable energy in large quantities,\" he says. \"Sooner or later we will be extracting energy out of high-altitude winds.\" There's another advantage to harvesting the wind at altitude. Breezes at the surface blow intermittently, so even in windy sites turbines typically collect only 30\u201340% of the energy that would be available if they ran continuously. High-altitude devices could push that number towards 80%, thanks to the steadier flow aloft and the possibility of moving higher and lower in the atmosphere to find the best wind. Recently, Caldeira and Cristina Archer of California State University in Chico published the first comprehensive analysis 1  of global data on wind above 500 metres. They found that the most energetic winds are found in the jet streams, 10,000 metres up, above some of the spots that require large amounts of power, such as Japan and the eastern United States. However, at that height the resource is difficult to predict and even harder to reach. Most engineers are not planning to go so high. The power of wind grows as the cube of its speed, so even the moderate increase in wind strength a few hundred metres above the surface produces large power gains.  \n                Trade secrets \n              Leonard Shepard, president of Sky Windpower near Irvine, California, makes that point clear by listing the current wind speeds above Red Bluff, a small town in the northern part of the state that has detailed weather measurements. At ground level, a light breeze is blowing at under 10 kilometres per hour. At 900 metres, the speed reaches 50 kilometres per hour. But conventional wind turbines only stand about 130 metres or so above the ground. So a wind collector flying 900 metres above the town could theoretically gather 125 times as much energy as a turbine on the ground. Sky Windpower is developing an unmanned helicopter design with four horizontal rotors that would hover at altitude. Shepard says he plans to send a prototype with four 2-metre rotors aloft this year to collect electricity at a few hundred metres. Once it gets up to a certain height, the wind will spin the rotors and keep it up, but he declines to give many more details than that. A penchant for secrecy is common in the high-altitude wind world: until they have a working prototype, companies are careful not to reveal too much about their designs. \n                Kite dudes \n              Corporate secrecy mixes with surf culture at the offices of Makani Power, located on a desolate former naval air base in Alameda, California. In 2006, Montague teamed up with two avid kite surfers, inventor Saul Griffith and Griffith's former student Corwin Hardham, to mix kite know-how with engineering to harness the wind's energy. The office is in a flight control tower and feels like a 1990s Internet start-up company. Near the entrance hang a dozen high-end bicycles, and windsurfing gear lies about in the workshop. The tower's control room, now the company lounge, has microbrews on tap. And hardly anyone seems to wear shoes. Despite the Lost-Boys-in-Neverland atmosphere, Makani's design is not fantasy. It starts with a pilotless glider that resembles a giant boomerang made of lightweight carbon fibre. The glider is tethered to the ground by cords at each side, like the kite over Montague's boat. By pulling the cords a controller can send the glider sailing back and forth in a wide loop while four rotors on the leading edge spin to generate energy. Hardham \u2014 who is co-chief executive with Montague \u2014 says the kite would ideally fly at 200\u20131,000 metres. Although wind at this relatively low altitude is slowed by friction with the ground, the back-and-forth movement significantly boosts the speed of the air passing through the rotors, says Hardham. A 30-metre, 1-tonne carbon-fibre glider flying in a moderate 32-kilometre-per-hour wind could produce 750 kilowatts \u2014 about as much as a small industrial turbine, but with a fraction of the materials, he says. \"It's surprising how good the opportunity is,\" Hardham says. \"If we can make this work I think it's the best renewable conversion technology there is.\" But he quickly tempers this, saying that there is still a lot to be done. For one thing, the power needs to be transferred from the kite to the ground. Both Makani and Sky Windpower are hesitant to go into detail about the tethers they would use, but they would probably be some kind of nylon fibre cord with an electrical cable at its core. Hardham and others say that the tether is less a concern than automation. The biggest challenge, it seems, is controlling the device as it takes off, lands and flies in unpredictable weather. Their longest test run so far was a 30-hour flight for a 3-metre wing at a breezy site on the island of Maui, Hawaii. As the device gets larger, controlling it becomes easier, but the consequences of a crash grow more expensive. Californians do not have a monopoly on high-flying energy efforts. The Dutch astronaut Wubbo Ockels, who once collaborated with Montague, has developed a concept called Laddermill, which would fly multiple kites at various heights, all tethered to one turbine on the ground. As the kites rise, the turbine spins and generates power. The kites are then brought down again and the process repeats. And in Italy, another windsurfing enthusiast and engineer named Massimo Ippolito started a company called Kite Gen in 2007 to produce power by using several kites to turn a giant ring-shaped structure on the ground. Both companies have managed to create prototypes of single kites and have generated up to 10 kilowatts of electricity. Another company, Magenn, based in Kanata, Ontario, Canada, plans in the next year or two to release a spinning dirigible capable of generating 100 kilowatts that may cost around US$500,000. Even a decade ago, designs such as these would have been impossible. However, with lighter materials and advances in pilotless flight, high-altitude wind is starting to attract money. Kite Gen was promised \u20ac50 million ($71 million) by the Italian government, although that funding has fallen through because of the financial crisis. Magenn, which is backed by private investors, says it has a capital base in the region of $10 million. Makani is partially funded by the Internet giant Google as a part of its philanthropic wing, Google.org. Google put $15 million into the company in 2007 \u2014 one-third of Google's total budget for renewable-energy research. And the company recently injected another dose of funds into Makani. Geoff Sharples of Google's REC initiative, which seeks to develop renewable-energy sources that will be cheaper than coal, declines to reveal the exact amount but says it was of the same order as the original investment. It's the only wind project that the company supports. Sharples knows the wind world well, having previously served as an executive with Clipper Windpower, a major turbine manufacturer based in Carpinteria, California. The conventional earthbound approach is limited, he thinks, by the availability of conveniently located windy land. Although there is a viable alternative \u2014 erecting turbines offshore \u2014 Google is investing in high-altitude wind because the energy payoff could be so much bigger. But companies such as Makani have a long way to go. Wind-industry insiders say a new technology needs to be capable of producing at least 1 megawatt \u2014 roughly enough to supply 1,000 homes \u2014 to get the attention of energy utility companies. To date, no high-altitude wind company has generated anything close to this. In June, the US National Research Council issued a report on renewable energy that briefly mentioned high-altitude wind power as a possibility more than 25 years in the future 2 . For now, most people in the wind-power industry are apparently ignoring high-altitude efforts. \"I don't think anyone has given it a serious, objective study who wasn't already committed to the technology,\" says Robert Thresher, an engineer at the National Renewable Energy Laboratory in Golden, Colorado, which writes certification guidelines for wind technology. Wind experts at the lab are not enthusiastic about high-energy wind and say they wish inventors would focus their talents on conventional turbines. Thresher says he sees much more potential in offshore turbines than flying ones. Such naysaying has not stopped another Californian inventor from sinking millions of his own dollars into the concept. Deep in the heavily wooded Santa Cruz Mountains, JoeBen Bevirt is chasing the ultimate wind prize \u2014 the strong, steady jet streams that scream along at 200 kilometres per hour, 10,000 metres up. Projects that seek to tap this resource draw the most scepticism from conventional wind engineers. Bevirt has created a number of consumer and industrial products, such as robots for gene sequencing, mobile-phone headsets and a small camera tripod that can grip a branch or pole. In 2007, he sold his robotics company for $50 million. \"I should be retired right now. I should be kite surfing on the beach,\" Bevirt says. \"But I am so passionate about this that I am working 18 or 20 hours a day, driving as hard as I have ever driven in my life.\" He has come up with a concept that crosses Sky Windpower's helicopter rotors with Makani's sleek wings, the offspring being a diamond-shaped double-wing frame with rotors at each corner. By itself, it looks somewhat like the Wright Brothers' Flyer with an extra pair of rotors. Because of the strength of the jet streams, he expects each 11-metre rotor to produce a staggering 250\u2013500 kilowatts. However, the design is modular, with many possible configurations: a system may have 4, 32 or even 96 rotors. The system would lift off like a helicopter, with electricity from the ground used to turn its horizontal rotors to generate lift. Once it reached a desired height, the device would pitch forwards so that the wall of rotors would point into the wind and the frame act as glider wings, keeping the network aloft. Turned now by the wind, the rotors would generate electricity. The design is far from foolproof. As with Makani's wing, take-off and landing require complicated control algorithms. Any tether reaching to 10,000 metres will need to be extremely light. And because a crash could kill people on the ground, the device could fly only over an uninhabited zone at least half the size of London. Despite these challenges, Bevirt has the confidence of someone who has successfully brought products to market. He has founded a company, Joby Energy in Santa Cruz, to develop his concept. So far, he says he has invested $2 million in the project and plans to keep funding it as long as is necessary. \"I am very selective about the things that I pour my energy into. The fact that I am pouring my energy into this and that I am also willing to pour a lot of money into it lends credibility to the fact that it's not a crackpot scheme,\" he says.  \n                Coming soon? \n              Like their energy source, the prospects of success for Bevirt and other high-altitude wind aficionados are up in the air. Makani has generated little more than 10 kilowatts, although it plans to run a bigger prototype this year. Sky Windpower also aims to test an experimental version of its concept this year. The first company to bring a design to market will probably be Canada's Magenn, although the size and drag of its blimp-like units may preclude them from generating enough energy to compete with big industrial turbines. As a newcomer who started work only in 2008, Bevirt has yet to produce any energy, but he plans to start this summer and get a product to market around 2011. He says he doesn't mind that many engineers write off high-altitude wind power. The proof will come soon enough, he says. While giving a tour of his workshop, Bevirt demonstrates a 1-metre scale model of his device. During the first test, it buzzes over my head like a wobbly bird. Later, as a mortified employee looks on, the device flips over and snaps in two during take-off. Bevirt just smiles. \"Oh well,\" he says. Sometimes that happens too.\" Erik Vance is a freelance science writer in Berkeley, California. \n                     Nature Reports Climate Change \n                   \n                     Joby Energy \n                   \n                     Magenn Power \n                   \n                     Makani Power \n                   \n                     Sky Windpower \n                   Reprints and Permissions"},
{"file_id": "459632a", "url": "https://www.nature.com/articles/459632a", "year": 2009, "authors": [{"name": "Erika Check Hayden"}], "parsed_as_year": "2006_or_before", "body": "Blurry specks in the eye seem an unlikely source of inspiration for a revolutionary microscope. But 'floaters' ? tiny debris that floats inside the eyeball ? led Changhuei Yang at the California Institute of Technology in Pasadena to devise a microscope so small, cheap and mass-producible that it could, according to its inventor, transform the way that microscopy is done. The human eye registers floaters when bright light casts the shadow of debris directly onto the retina. On the tiny 'optofluidic' microscope that Yang and his colleagues invented, the sample casts a shadow directly on to an array of commercial light sensors as it floats along a microfluidic channel ( X. Cui  et al .  Proc. Natl Acad. Sci. USA    105,   10670?10675; 2008 ). The sensors feed the projection pattern to a computer, which constructs an image using relatively simple image-processing software. The device itself is assembled using semiconductor fabrication techniques and is smaller than an American dime. When mounted into a device with a USB port so that it can transfer information to a computer, it is still just 3 centimetres square. Yang says that his microscopes, which could cost as little as US$10 apiece, could have the same revolutionary impact on science that the integrated circuit has had on the electronics industry. \"When people were building transistors individually, it was still a pretty expensive proposition to build circuits out of them,\" he says. \"But the move to build integrated circuits moved the semiconductor industry forwards because you could build things comparably cheaply with high functionality. If we can start to put 10?100 microscopes on a single chip and link a bunch of them up to operate in parallel to do high-throughput processing of a large number of samples, this opens up the opportunity to do experiments you might not otherwise do.\" With cheap, high-throughput imaging, researchers could perform drug assays, genomic or proteomic screens and rapidly observe the outcome of hundreds or thousands of manipulations on the shape or behaviour of living cells. \"It's very clever work,\" says Charles DiMarzio, director of the Optical Science Laboratory at Northeastern University in Boston, Massachusetts. \"This is a way of making a [high-power] microscope that is very low cost, maybe even disposable, and that's something that we haven't had before.\" Yang recognized that it would be difficult to shrink the lens and other delicate optics in a high-end instrument ? so his 'direct projection' technique did away with lenses altogether. Other scientists have worked out similar techniques before, but they couldn't resolve anything smaller than 5 micrometres, because that's as small as the pixels on most digital light-sensing chips get. Yang coated the sensing chips with a thin layer of metal, than punched 500-nanometre holes into the metal to create apertures that are smaller than a pixel and that are patterned along the path of the microfluidic channel (see graphic). As the sample floats along, the chip captures repeated but staggered snapshots of what is passing overhead. With 500-nanometre holes, Yang's optofluidic microscope has a resolution that approaches that of a standard laboratory light microscope. He has already shown that it can capture images of the nematode worm  Caenorhabditis elegans   that are almost indistinguishable from those collected with a 20\u00d7 objective lens on a conventional instrument. He is working to narrow the holes to 300 nanometres, a resolution capable of distinguishing the finer details of cells. Besides transforming research microscopy, Yang's microscope could boost low-cost science and medicine in developing nations. The scope is rugged, works with sunlight, needs only the amount of computational power found in an iPod and, Yang wrote in his paper last July, might be \"a boon for a health worker who needs to travel from village to village\". That statement struck home for Ricardo Leit\u00e3o, a postdoctoral fellow at New York University School of Medicine, who is founding a non-profit group called Tek4Dev ? Science & Technology for Sustainable Development ? which is putting together a tool kit to enable 'telemedicine' (using networks such as the Internet to facilitate clinical care) in poor countries. Leit\u00e3o wrote to Yang on the day the paper was published to propose a collaboration. Yang, Leit\u00e3o and Ana Rodriguez, a malaria researcher also at New York University, are now testing the microscope's ability to diagnose malaria-infected red blood cells based on their shape and those of the parasites inside them. Microscopy is still the standard method for diagnosing malaria, but microscopes can be few and far between in malaria-endemic areas. \"Having a diagnostic tool as powerful as Yang's integrated with our hardware and 'tele' ability would be of tremendous clinical value,\" says Leit\u00e3o. Yang admits that by trying to do more with less, he is thinking differently from many of his peers. \"In the field of biomicroscopy, there is a very strong drive towards building more sophisticated microscopes, giving you ever better resolution,\" he says. \"But I think there's another axis to pursue, which is if you're actually building this in a comparably low-cost fashion, it can create experimental formats that are currently not doable using a traditional microscope or any other high-end microscope that other research groups are pursuing.\" What is certain is that by making microscopes exceedingly small, Yang has actually been thinking very, very big.   See also  \n                     page 629 \n                    and online at  \n                     http://tinyurl.com/microspecial \n                   . \n                     Microscopy special \n                   \n                     Cell imaging: New ways to see a smaller world \n                   \n                     Method of the Year 2008 \n                   \n                     Changhuei Yang Research group \n                   Reprints and Permissions"},
{"file_id": "459028a", "url": "https://www.nature.com/articles/459028a", "year": 2009, "authors": [{"name": "Joerg Heber"}], "parsed_as_year": "2006_or_before", "body": "Thin films of oxygen-bearing compounds could have myriad practical applications, finds Joerg Heber, if a few problems can be overcome. In late 1996, a young Bell Labs physicist named Harold Hwang told his lab director that he wanted to start a radical programme of research into oxides \u2014 the ubiquitous, oxygen-bearing compounds found in everything from granite and glass to ceramics, chalk and rust. Hwang was convinced that even the most familiar oxides might show surprising and useful properties if different ones could be stacked up into 'heterostructures': layer-cake-like arrangements in which each level is an ultrathin film just a few atoms thick. The lab director, Horst St\u00f6rmer, was slightly dubious \u2014 not about the potential, but about the practicality. \"Have you ever grown a thin film in your life?\" he asked. He knew all too well what Hwang was getting himself into. St\u00f6rmer had made his own reputation by growing and studying thin films of a very different class of materials: semiconductors. Those films had shown some remarkable properties \u2014 including a phenomenon called the fractional quantum Hall effect, in which the free-roaming electrons inside a layer condense into a liquid-like state. That discovery would later earn St\u00f6rmer a share of the 1998 Nobel Prize in Physics. But such phenomena appeared only if the layers were absolutely uniform in height, with a crystalline structure that was so pure and defect-free that electrons could race along without crashing into imperfections. It had taken St\u00f6rmer and his colleagues at Bell Labs more than 10 years to invent and perfect the techniques for fabricating such films. And oxides, he knew, would be even more difficult to master. The compounds, which form 99% of Earth's outer crust, typically consist of a larger number of chemical elements than semiconductors, and have more complex crystal structures. Still, St\u00f6rmer told Hwang to go ahead, and the younger man did not disappoint. In time, Hwang and others doing research in the field succeeded in growing high-quality oxide thin films with the same atomic precision as semiconductors. And those films do indeed exhibit interesting phenomena. In 2004, for example, Hwang co-discovered the existence of a two-dimensional (2D) electron gas, in which electrons at the interface of two oxide thin films show an extremely high mobility 1  \u2014 an effect that is particularly striking because the two oxides involved are electrical insulators. Now oxide thin films are at roughly the same stage of development as semiconductor thin-films were in the early 1970s \u2014 a period when researchers were finally learning how to work with them well enough to fabricate devices such as the thin-film lasers, which would later have their commercial breakthrough in compact-disc players. For example, the 2D electron gas that Hwang and his colleagues discovered is being explored for use in a new type of fast transistor, a device that can amplify or switch electronic signals. Another use of oxide films could be as the basis for very high-density data-storage devices in which the magnetic information is controlled with electrical fields. And that's just the beginning, says Hwang. \"The great opportunity we have now is to design and grow artificial thin film structures down to the atomic scale \u2014 using multilayers of superconductors, ferromagnets, or even a combination \u2014 and to engineer systems that may one day be used for electronics or sensing applications.\"  \n                The power of oxygen \n              The rich array of phenomena found in oxides is largely due to the oxygen, says Yoshinori Tokura, a physicist from the University of Tokyo who has worked in this field for more than 20 years. Oxygen tends to pull electrons away from other atoms in the compound, says Tokura, resulting in strong electrical fields at the interatomic scale. These fields can give rise to substantial correlations in behaviour between the electrons of one atom and those of its neighbours. And the correlations in turn can lead to effects such as ferromagnetism, in which a material's electrons spontaneously line up and produce a magnetic field. Nevertheless, for many years researchers tended to shy away from using oxides in advanced applications, because they are far more difficult to fabricate than metals and semiconductors. This situation changed in 1986 with the discovery of high-temperature superconductivity in certain oxides. The work kicked off an intense, worldwide focus on oxides that led to other discoveries. In 1993, for example, researchers encountered 'colossal magnetoresistance', in which a slight shift in the external magnetic field causes certain oxides to undergo an orders-of-magnitude change in electrical resistance 2 . Another example is the 2D electron gas that Hwang and his co-worker Akira Ohtomo stumbled on when they were studying the interface between two insulators 3 , lanthanum aluminate (LaAlO 3 ) and strontium titanate (SrTiO 3 ). \"We started to fabricate very crude-looking transistors that should not have been conducting by themselves, but found they were already conducting,\" recalls Hwang, who is now at the University of Tokyo. \"We started thinking, 'What is going on here?'.\" They soon found that everything depended on the precise crystalline structure of the interface: only when the right atomic layers met would the internal electrical fields on each side push electrons towards the junction, so that they could form the electron gas. Otherwise, no charge layer develops 4 . The interface electrons also turned out to be surprisingly mobile. In fact, as discovered in 2007 by the groups of Jochen Mannhart from the University of Augsburg in Germany and Jean-Marc Triscone from the University of Geneva in Switzerland 5 , these structures can become superconducting, meaning that the electrons can travel without resistance \u2014 albeit only below the very low temperature of about 200 millikelvin. Researchers have also been studying potential applications that would exploit the thin-film interface. One way to do that would be to place a ferromagnetic oxide next to an insulating oxide that isn't ferromagnetic. If an external electrical field is applied, it causes an electrical polarization to develop at the interface. But the field also shifts the number of electrons in the ferromagnetic material, which changes the magnetic field. As a result, electrical polarization and magnetism are both controlled by the same electrical field, and are therefore cross-linked \u2014 a coupling of properties that defines multiferroic materials 6 . Such materials are of interest both as magnetic field sensors and as memory devices, in which information is written by electrical voltages and read by magnetic read head \u2014 with the benefit that no electrical current flows through the device, significantly reducing heat generation. Indeed, says Tokura, \"the route via thin films offers the most straightforward fabrication method to realize a multiferroic material\". As well as looking at the coupling of two different properties at an oxide interface, researchers are looking for applications in which a single property, such as magnetic field 7 ,   8  or electrical conductivity, is controllably turned on and off. \"Controlling conductivity as a whole, rather than electrical current itself, in some sense is the most exciting area,\" says Stuart Parkin, a physicist at IBM's Almaden Research Center in San Jose, California. \"Contrary to conventional transistors, the required current densities could be quite small, and this is what you want for applications.\" Consider, for example, the superconducting 2D electron gas. Through the application of an electrical field it is easy to push electrons away from the interface, destroying the superconducting state and making it impossible for current to flow 9 .This is analogous to what happens in conventional transistors, in which the flow of electrons can likewise be switched on or off by an external electrical field. Conceivably, researchers could use local voltages to write complex patterns into this 2D electron gas. Where a voltage is applied, the interface would be insulating, and elsewhere it would be superconducing \u2014 potentially allowing the definition of entire electronic circuits. \"It will be exciting to see the realization of small devices such as logic and memory circuits, or even small amplifiers,\" says Mannhart. Amplifiers written into the superconducting film could enable fast switching with extremely low noise levels and thus could detect and amplify weak electronic signals. Even the logic gates used for quantum information processing could be etched into the superconducting layer this way.  \n                Just one small push \n              Unfortunately, the switching of superconductivity in the LaAlO 3 /SrTiO 3  system occurs at temperatures far too low to be relevant for most applications. So one alternative is to look at the different phases many oxides show at various temperatures or pressures. Conductivity often changes dramatically at the transition from one phase to another. \"If you go to phase boundaries, that's where you often get extremely large instabilities,\" says Parkin. \"Then you can imagine controlling those states by small modifications.\" Such a small trigger impulse can push the system from one phase to the other. This is what happens in colossal magnetoresistance \u2014 a small external magnetic field induces huge variations in electrical resistance during a phase transition. To realize high-quality oxide heterostructures for applications, researchers have had to overcome substantial obstacles to the development of suitable thin-film growth techniques. Oxides often have complex crystal structures, and films refuse to grow properly unless the right crystal layer is exposed on the top surface. Otherwise, the incoming atoms will not be able to stick to the proper chemical bonds. In 1994, this problem was identified and solved for SrTiO 3  by Masashi Kawasaki, a materials scientist then at the Tokyo Institute of Technology, now at Tohoku University in Japan, who developed a pre-growth treatment involving various acids that strip the crystal substrate down to the desired atomic layer 10 . With this advance, says Kawasaki, \"people could finally grow complex oxides\". Unfortunately, it is still impossible to obtain oxide heterostructures anywhere near as large as those used in silicon technology. \"Scaling up SrTiO 3  wafers to realistic sizes is out of the question,\" says Darrell Schlom, a materials scientist from Cornell University in Ithaca, New York. So, many researchers are now trying to integrate oxide heterostructures into silicon wafers. \"The plan is not only to integrate oxides with silicon electronics, but even more importantly to take advantage of the processing infrastructure of silicon technology,\" says Schlom. This is an arduous task, not least because there are substantial differences in crystal structure between most oxides and silicon. And worse, oxide thin films are grown by condensing a high-temperature vapour that includes oxygen \u2014 which can turn silicon into silicon dioxide at the slightest contact. This can be avoided only by carefully adjusting growth temperatures and supplying just the right amount of oxygen at precisely the right time. Still, progress has been made and the quality of oxide films on silicon has been improving steadily 11 . \"Even advanced oxide films such as LaAlO 3 /SrTiO 3  heterostructures have now been grown on silicon,\" says Mannhart.  \n                Not so crazy \n              Particularly promising in this regard is zinc oxide (ZnO), which is itself a semiconductor with a wide range of potential applications. \"In ZnO, electrons can travel up to a micrometre without scattering,\" says Kawasaki. Kawasaki and his colleagues have even observed the quantum Hall effect in ZnO \u2014 a first for an oxide 12 . The presence of such quantum phenomena suggests the use of ZnO for 'spintronics' applications, which promise ultrahigh-density storage and ultrafast processing of information using the electron's tiny magnetic moment, or spin. This isn't the end of the possible uses of oxides. \"This might be a very crazy idea, but we are wondering whether these heterostructures can be applied to new types of solar cells,\" says Tokura. Solar cells are currently made of semiconductors, he explains, and function through the absorption of light with energies larger than a certain threshold known as the band gap. If the light has an energy much larger than this band gap, the excess is wasted into heat. But if electrons are confined, for example, in semiconductor nanoparticles, they begin to interact strongly with each other, which amplifies a process in which the excess energy is not wasted but rather used to excite multiple electrons. The entire process becomes more efficient. In complex oxides, with their strong electron correlations, such an amplification could be very strong, says Tokura. Indeed, researchers already know of certain oxides in which light can excite so many electrons that the material becomes metallic. But that would still leave the problem of extracting these electrons from the oxide to put their energy to use. Even here, oxide thin-film structures may offer a solution. The layers are generally very thin, which means that electrons generated in one film could easily be extracted to an adjacent layer. \"If we can make this work, it would be really exciting,\" says Kawasaki, who is investigating this idea with Tokura. Parkin has an even more ambitious idea. He is looking for layered oxide systems in which superconductivity sets in at unprecedentedly high temperatures. \"Room temperature is, of course, the ultimate goal,\" Parkin says. \"In my mind this is entirely feasible.\" He thinks that such superconductivity might be found at interfaces similar to LaAlO 3 /SrTiO 3 , and might also involve the use of oxide compounds that do not normally exist in nature and can only be stabilized as thin films. After more than 20 years of research into oxide thin films, efforts are bearing fruit. Progress is becoming fast-paced. Thin-film-growth technology has been adapted for oxide compounds, suitable substrates have been developed and complex heterostructures are being studied for new functionality. \"Although what we have achieved as a community is still at the very early stages, we now know a lot more about the basic rules of engagement,\" says Hwang. At the same time, Hwang sounds a note of caution. \"Now the hard questions come,\" he says. Even seemingly mundane issues such as sample quality need to be tackled. \"Oxide heterostructures are still loaded with defects. Understanding how to control these is key to taking oxide heterostructures from scientific curiosity \u2014 their current position in various scientific sandboxes \u2014 to real technologies,\" says Schlom. Nevertheless, the achievements so far are strong testament to the fact that researchers in the field have begun to predict and control the phenomena that can exist in oxide heterostructures. Whether as new electronic compounds, as sensors, as memory devices, as solar cells or simply for their exciting science, oxide heterostructures are here to stay. The journey has merely begun.   Joerg Heber is a senior editor of    Nature Materials . \n                     Nature Materials \n                   \n                     Nature Nanotechnology \n                   \n                     Harold Hwang's lab \n                   \n                     Yoshinori Tokura \n                   \n                     Jochen Mannhart's lab \n                   \n                     Jean-Marc Triscone's lab \n                   \n                     Masashi Kawasaki's lab \n                   \n                     Darrell Schlom's lab \n                   \n                     IBM-Stanford Spintronic Science and Applications Center \n                   Reprints and Permissions"},
{"file_id": "459159a", "url": "https://www.nature.com/articles/459159a", "year": 2009, "authors": [{"name": "Asher Mullard"}], "parsed_as_year": "2006_or_before", "body": "Bacteria and their hosts may reside in different kingdoms, but that doesn't stop them from intercepting each other's communications. Asher Mullard reports. When Mark Lyte looked up from the podium at the 1992 American Society for Microbiology meeting in New Orleans, Louisiana, he saw two faces \u2014 and nearly 400 empty seats. Lyte, a microbiologist at Texas Tech University in Lubbock, ploughed on regardless. He had a lot to say. His experiments had shown that three species of infectious bacterium intercept the human stress-response hormone noradrenaline and use it as a cue to escalate their growth \u2014 perhaps explaining why stressed animals are more likely to die of infection despite having boosted their immune responses. Minutes into Lyte's lecture, one person got up and walked down the long, lonely aisle to the door. Only his loyal technician remained, along with the two people chairing the session. After the thin applause, one of them posed a question: \"Why would you ever want to do these experiments?\" Researchers already knew that bacteria and humans detected each other's presence through membrane receptors and cell-wall molecules \u2014 but no one thought that bacteria were sophisticated enough to eavesdrop on the long-range chemical signals of the organisms that host them. Nowadays, Lyte's lectures are packed, and a burgeoning field of researchers is studying the chemical crosstalk between bacteria and their hosts. They know that, as Lyte showed, bacteria respond to the chemical signals that their hosts use for internal communications; they've also discovered that infected hosts intercept the signals that bacteria send to each other, apparently to confound their knavish tricks. The bacteria fight back, turning off immune responses. Interkingdom espionage offers all the intrigue, jamming, fakery and subversion that you could find in a good spy thriller. Some scientists see all this subversion as something to emulate: if body cells can confuse bacterial attackers with this sort of crosstalk, why shouldn't pharmaceutical companies? Hence the search for small molecules or antibodies that could serve as new classes of antibacterials. And on top of \u2014 or beneath \u2014 these practical opportunities, there are also deeper scientific questions. How did organisms as different as bacteria and their eukaryotic hosts come to understand each other in the first place? And is it possible that the crosstalk underlies mechanisms of cooperation as well as conflict? For centuries, bacteria were thought to be loners that didn't communicate with one another, let alone with anything else. Bonnie Bassler, who studies bacterial communication at Princeton University in New Jersey, recalls a time when many of her colleagues thought that \"bacteria didn't have the genetic power to do anything interesting \u2014 they ate, they moved, they divided\". But in the 1970s, researchers discovered that  Vibrio fischeri , bacteria that live in squid, fish and the open ocean, coordinate their bioluminescence by sensing the level of signalling molecules given off by others 1 , a system that later came to be called quorum sensing. Signalling that synchronizes bacterial gene expression patterns and coordinates behaviour within a population has now been seen in all sorts of bacteria \u2014 and it is used for various purposes, including establishing infection and increasing virulence.  \n                Over the wall \n              Still fighting against the idea that bacteria were simpletons, Lyte's study 2  in 1992 was one of the first to show that bacteria also detect signalling molecules released by the organisms that they infect. In 2006, microbiologist Vanessa Sperandio, at the University of Texas Southwestern Medical Center at Dallas, and her colleagues showed how intimately the two communication systems could be integrated. Sperandio's team found that QseC, a bacterial receptor that detects a quorum-sensing signal called autoinducer 3 (AI-3), is also activated by the mammalian hormones adrenaline and noradrenaline 3 . Both cause the bacterium  Escherichia coli   to express virulence genes. Sperandio suspects that AI-3 and the human hormones have structural similarities that enable them to bind to the QseC receptor, and is looking into whether human hormone receptors can also detect AI-3. Some argue that this crosstalk is not 'signalling' at all \u2014 it did not evolve specifically as a means for two willing parties to communicate. But the fact that the same receptor performs this double duty still requires explanation. Some invoke convergent evolution, suggesting that functional requirements led bacteria and their hosts to evolve chemical messengers with some similar characteristics. An alternative possibility \u2014 a controversial one, but one to which Sperandio subscribes \u2014 is that the same receptor works for both bacterial and eukaryotic signals because eukaryotic cells acquired the genes for cellular communication from bacteria. This was proposed in 2004 by evolutionary biologist and bioinformatician Eugene Koonin, at the National Center for Biotechnology Information in Bethesda, Maryland, and his colleagues 4 . On the basis of the way the genes involved in hormone metabolism are distributed, Koonin argues that cell\u2013cell communication machinery may have been passed from bacteria to eukaryotes on several occasions by lateral gene transfer.  \n                Spooking the spooks \n              However the similarities between the signalling systems arose, they have enabled bacteria and their hosts to indulge in some deception. \"What we're looking at is not only espionage, but also hijacking,\" says Kendra Rumbaugh, who studies interkingdom signalling at Texas Tech University. Take the microbial messenger C12. This is a quorum-sensing signal that coordinates the expression of virulence genes for  Pseudomonas aeruginosa , a pathogen that can infect burn injuries or people with supressed immune systems. When Gunnar Kaufmann from the Scripps Research Institute in La Jolla, California, started working on it in 2005 he knew it was detected by mammals, and several studies suggested that it helped to trigger inflammation. \"But it's the opposite,\" Kaufmann says. When he and his colleagues treated mice with C12 they found that it actually inhibits the NF-kB signalling pathway 5 , which is crucial for immune response; studies on human cells indicated much the same. While it might make sense for the host immune system to listen out for the quorum-sensing signal, in this case the bacteria seem to have evolved the upper hand. \"C12 acts as a stealth agent,\" Kaufmann says. \"_P. aeruginosa _ might use it to shut down the immunity locally so that by the time the host realizes there is something there, it is too late,\" he says. Dirty play goes both ways. Plants and algae, for example, are master mimics of bacterial quorum-sensing signals. One of the best-known examples is found in the red alga  Delisea pulchra , which produces quorum-sensing signal lookalikes called furanones. In 2002, microbiologist Staffan Kjelleberg of the University of New South Wales in Sydney, Australia, and his colleagues showed that furanones jam signalling in  P. aeruginosa   and in  E. coli , probably either by competing with native quorum-sensing signals or by changing the configuration of the bacterial receptor 6 . Another tactic, discovered in animals, is simply to snatch the messengers off the streets. Hattie Gresham, a microbiologist at the University of New Mexico in Albuquerque, has been studying how hosts handle pathogenic  Staphylococcus aureus   for nearly 15 years. About 25% of people have these bacteria residing permanently in their nose and an estimated 1% live healthily with methicillin-resistant  Staphylococcus aureus   (MRSA). \"That means that the host has something that can keep the bacteria in check,\" says Gresham. She suspected that some component of human blood plasma was interfering with  S. aureus   communication. In 2008, after painstakingly screening serum samples, Gresham and her team found that component: apolipoprotein B (APOB), a huge lipid-binding protein that helps transport cholesterol in the bloodstream 7 . Gresham found that APOB smothers an  S. aureus   quorum-sensing molecule called autoinducing peptide 1 (AIP1), cutting the line of communication used to coordinate the onset of virulence. Mice chemically or genetically manipulated to lack APOB are more susceptible to MRSA. In this case at least, says Gresham, both sides benefit: the host prevents an infection from turning pathogenic, and the bacteria are able to live happily in the nose without threat from the host's immune system. Through mutual surveillance and manipulation, the host and the pathogen can \"arrive at a d\u00e9tente\", says Gresham. If the balance breaks down, because a patient is old, sick or otherwise immunocompromised, then the infection starts escalating out of control. Clinical studies have shown that APOB levels are lower in critically ill patients than in healthy individuals, which Gresham thinks could partly explain why these patients are highly vulnerable to MRSA infection. \"Therapeutically, is there a way to lower that risk by giving these patients APOB, or a peptide mimetic of APOB?\" she wonders. Researchers have been trying to manipulate quorum sensing to make antibacterial drugs since the 1990s. They have had little success; a fair few early start-ups based on the idea died. One problem may have been the dearth of knowledge about interactions between bacterial signals and host signals. \"It's hard to think about developing a drug that targets quorum sensing without knowing how the host deals with [quorum sensing],\" says Gresham. Researchers are wary of blocking a quorum-sensing signal if the host might already be using it to gauge its immune response, or of developing a compound that risks inhibiting both bacterial and host receptors. Still, many microbiologists and chemists are still hopeful that they can design neat little molecules to artificially stifle or manipulate microbial communication systems more effectively than the systems that have evolved naturally. \"This field is kind of like a sandbox for chemists,\" says Helen Blackwell, a chemist from the University of Wisconsin, Madison. \"If we can understand these signals better, and learn what components of these signals are necessary at the molecular level, then we can tinker with them and start to engage the bacteria in new conversations, and we can try to confuse them.\" Working in close collaboration with chemists, pharmacologists and other microbiologists, Sperandio screened 150,000 molecules for inhibitors of the quorum-sensing receptor QseC and identified one, LED209, as a potent, relatively non-toxic small molecule that protects mice from both  Salmonella typhimurium   and  Francisella tularensis , although not from pathogenic  E. coli 8 . In 2008, the group won US$6.5 million over 5 years from the US National Institutes of Health to search for LED209 analogues that provide greater protection and lower toxicity. These, they hope, will find use as broad-spectrum therapeutics to protect patients from stubborn infections associated with assisted-breathing apparatus, several of which have receptors much like QseC. \"Our idea is to have this in a preclinical form in 5 years,\" she says. For Nafsika Georgopapadakou, though, the anti-quorum-sensing approach is ultimately flawed. Georgopapadakou, a consultant in Montreal, Canada, has worked on antimicrobials at several large companies. She says that quorum sensing seems to be important for establishing infections rather than for maintaining them, and so such therapeutics are only likely to be useful as prophylactics that are given before the infection has started. Anti-quorum-sensing approaches don't kill bacteria, she adds, they just lower virulence and increase the odds that antibiotics and the immune system can clear the infection. \"If I'm going to sweat making novel compounds, I would much rather kill bugs.\" But proponents of anti-quorum-sensing approaches argue that their non-lethal approach is actually advantageous. One of the main failings of current antibiotics is that their efficient killing drives the rapid evolution of drug resistance, says Sperandio. Anti-quorum-sensing strategies, by contrast, could have a much longer shelf life. \"If you don't kill the bacteria, you're not speeding up the process of developing resistance that much,\" she says. To get the full effect, however, they will probably have to be used with other drugs, she adds.  \n                The spy who loved me \n              Some researchers are less interested in intervening in bacteria-host communication and more interested in exploring why it happens. \"Everybody, including our lab, has focused on pathogenic processes,\" says Rumbaugh. \"Unfortunately, the field might be focusing on the wrong direction.\" As Rumbaugh sees it, \"pathogenesis is the exception\". Many microbiologists believe that these cross-kingdom communication systems evolved because they served a beneficial purpose for both sides, by supporting mutually beneficial relationships between bacteria and hosts. \"So, what are the real functions of these [interkingdom exchanges]?\" Rumbaugh asks. Both Sperandio and Rumbaugh suspect that there is a host of as-yet-unidentified small molecules that pass between bacteria and humans, and that these need to be isolated and catalogued if researchers are going to understand the full scope of interkingdom communication and the purposes that it serves. Practically, this could be tough. Quorum-sensing signals can be hard to distinguish from other soluble chemicals; some of them are only produced in specific environmental conditions that can be near-impossible to reproduce in a culture dish, and some are manufactured in such minute quantities that they are difficult to collect and analyse. The structure of AI-3, for instance, has not yet been resolved for this reason, Sperandio says. There is an additional obstacle to deciphering friendly bacterial-host communication. Just as vice and espionage tend to capture the fiction market, Rumbaugh says it's easier to win research funding to study duplicity and pathogenicity than it is to study the friendly 'symbiotic' and 'commensal' interactions in which one or both sides benefit. As for Lyte, he too is pursuing the idea that an extensive and cordial dialogue is going on. He wants to examine whether bacteria use their signals to modulate complex host behaviours and functions, including learning and memory, and vice versa. \"Bacteria are conversing with us, and we're conversing with them,\" says Lyte. The question now is how to record more of the conversation \u2014 and work out what is being said. \n                 Asher Mullard is a freelance science writer based in London.  \n               \n                     Vanessa Sperandio \n                   \n                     Kendra Rumbaugh \n                   \n                     Mark Lyte \n                   \n                     Staffan Kjelleberg \n                   \n                     Hattie Gresham \n                   Reprints and Permissions"},
{"file_id": "4581097a", "url": "https://www.nature.com/articles/4581097a", "year": 2009, "authors": [{"name": "Oliver Morton"}], "parsed_as_year": "2006_or_before", "body": "Geoengineering schemes, such as brightening clouds, are being talked about ever more widely. In the third of three features, Oliver Morton looks at how likely they are to work. Something utterly insubstantial is rising above the rim of the beaker on the table. It looks like a white mist; it feels like nothing. Run your hand through it and you get no sense of warmth or cold. It leaves no moisture on the skin, no smell, no taste. It's just a whiteness. You can see that it would spur curiosity; that it might spur controversy is harder to imagine. The mist is made up of droplets of water just a few micrometres across, thousands of times smaller than a raindrop. The man who set up this beaker as a demonstration, a nominally retired professor of engineering at the University of Edinburgh, UK, named Stephen Salter, thinks that ships designed to produce such mists could whiten the low layers of cloud that hang above the sea over large areas of the globe. Established theory predicts that such whitening, if achieved, could cool Earth significantly \u2014 a thousand such ships might cool it as much as decades of carbon dioxide emissions would warm it. The beaker demonstration was part of a one-day meeting held at the University of Edinburgh in mid-March to look at how cloud whitening could move beyond the era of the tabletop. The meeting's agenda was vast, encompassing climate modelling, cloud physics, data from a field campaign studying clouds off the coast of Chile, the design of ships and the minutiae of the tiny nozzles needed to create such ultra-fine sprays. It ended up, as almost all such discussions of cooling the Earth do, heading off into questions of morality, politics and public perception. The frequency of such meetings shows how this topic, known as geoengineering, is gaining, if not acceptance, at least an enhanced currency. For a number of the participants, this was their second day of geoengineering presentations that week \u2014 there had been an all-day discussion of the topic at the International Scientific Congress on Climate Change in Copenhagen two days before. The following week, some of the key players would be at it again, this time at a workshop organized by the US Defense Advanced Research Projects Agency in Stanford, California. As yet, though, these discussions are, like Salter's mists, insubstantial. Very little funding is available for real research into whether ships are the best way to whiten clouds, or whether cloud whitening is really a workable way to cool the world. And that is cause for concern because there is a real possibility that such schemes won't work. \"The most dangerous case is \u2026 when you think that geoengineering works and you're wrong,\" said David Keith of the University of Calgary in Canada while at the Copenhagen meeting. The worry that Keith and others share is that a growing interest in geoengineering tends to promote the belief that it is a plausible option, even though there is not enough research to back that up. And interest is undoubtedly growing, in ways that go well beyond the small number of scientists trundling from meeting to meeting. The American Meteorological Society in Washington DC is consulting with its membership about a policy statement on the subject, and Britain's Royal Society is preparing a report chaired by John Shepherd, an oceanographer at the University of Southampton, UK. John Holdren, science adviser to US President Barack Obama, says that the technology should be looked at on the basis that nothing should be taken off the table; the economist Nicholas Stern, author of the influential  Stern Review on the Economics of Climate Change , says much the same in an interview on this week's  Nature   podcast . The discussions in science and policy circles are typically and appropriately couched with caveats about the unknown feasibility and safety of such ideas and the much greater desirability of cutting emissions. Although Greenpeace International has not called for a ban on geoengineering research, David Santillo, a scientist with the organization, argued at the Copenhagen meeting that such a policy might ideally be the best position, so that no creeping faith in the possibility of a last-ditch alternative would ever undercut the need to reduce emissions.  \n                First, choose your wavelength \n              Geoengineering approaches can be divided into two categories, short-wave and long-wave. Short-wave approaches reduce the amount of energy entering Earth's system by increasing the amount of sunlight that bounces back out into space without being absorbed. Long-wave approaches help infrared radiation to escape from the atmosphere, usually by reducing the concentration of CO 2  \u2014 for example by turning biomass into charcoal and burying it, or by fertilizing plankton blooms. Both approaches have their proponents, but as Timothy Lenton and Naomi Vaughan of the University of East Anglia, in Norwich, UK, have shown, the amount of cooling that might be expected from long-wave schemes is substantially less than you can get from short-wave ones ( T. M. Lenton and N. E. Vaughan  Atmos. Chem. Phys. Discuss.    9,   2559\u20132608; 2009 ). Short-wave schemes can be distinguished further by altitude. Sunlight can be reflected before it reaches the planet by some sort of shield in space. Or it can be turned away closer to home: by aerosol particles in the stratosphere; by clouds in the lower atmosphere; or by white objects on Earth's surface, such as painted buildings and roads. Lenton and Vaughan found that the last of those options \u2014 making the surface more reflective \u2014 would not be able to produce an effect large enough to counteract a doubling of CO 2 . The first \u2014 'sunshades in space' \u2014 could in principle counteract any warming effect that can be imagined. But it would require spacecraft vast in either size or number, a spectacularly ambitious and costly undertaking for a civilization that has no way of delivering 100-tonne payloads to the lowest of orbits. That leaves the stratosphere and the clouds. The stratosphere has received the bulk of geoengineering attention (see   Nature  447, 132\u2013136; 2007 ), mainly because volcanic aerosols at that height have clearly cooled the globe, and because this type of cooling is amenable to the sort of global modelling that climate researchers are good at. But such an intervention raises numerous issues. Particles injected into the stratosphere might also catalyse chemical reactions that could deplete the ozone layer. And although they \u2014 or the raw materials to produce them \u2014 could be lifted that high in a number of ways, it could be hard to make such a system operational and potentially impossible to ensure that the particles remain at the right size. Furthermore, stratospheric aerosol particles have a lifetime of a couple of years \u2014 long enough to wreck whole growing seasons if their side effects included suppressing rainfall. An attraction of the cloud-based approach is that it gets around some of those issues. It uses nothing more than seawater, it doesn't require things to be lifted tens of kilometres into the sky, it can be tested on a local or regional scale and it can be turned off instantaneously. The idea was first put forth in 1990, by John Latham, a British atmospheric scientist now based at the National Center for Atmospheric Research (NCAR) in Boulder, Colorado. For water vapour to form into clouds, the atmosphere needs to contain particles for the water vapour to condense on, called cloud condensation nuclei. Air that is well supplied with these nuclei will contain many small water droplets. In air that has a smaller number of nuclei, condensation will form fewer, larger drops. Clouds consisting of small droplets are more reflective than those with larger ones, and under some conditions they will also last longer. So if you were to inject condensation nuclei into clouds you would, other things being equal, make them brighter. Some 25% of the world's oceans are covered with thin, low-lying layers of stratocumulus cloud. Make them brighter, Latham says, and you could cool the planet. So Salter has designed wind-powered ships that use underwater turbines to drive machinery to make the fine mist that can provide the necessary particles. Such ships, which would operate, in principle, without a crew, could seed clouds over the necessary swathes of ocean. Phil Rasch, a climate modeller who until recently was a colleague of Latham's at NCAR and who is now at Pacific Northwest National Laboratory in Richland, Washington, has looked at how effective such brightening might be. By manipulating the number of cloud condensation nuclei in a global circulation model of the climate, Rasch found that seeding 25\u201350% of the ocean with droplets of the right size could offset a greenhouse warming of 3 watts per square metre \u2014 the amount that might be expected from a doubling of CO 2 . Running the model with doubled CO 2  showed that seeding could keep the average temperature where it is now, or in some scenarios actually diminish it. This might take hundreds or indeed thousands of ships, depending on their capacities. Salter, for one, thinks that such a fleet would be quite cheap. \"Do not be put off by yachts for the rich, with diamond-encrusted lavatory seats. Check out fishing boats.\" He thinks that around a thousand $2-million ships could do the trick. But even though they are of the same order of magnitude, the cloud effect does not perfectly counter the greenhouse effect. Brighter clouds cool only during the day \u2014 and do it best in summer \u2014 whereas greenhouse warming is felt 24/7. This imbalance applies to short-wave geoengineering schemes that use stratospheric aerosols, too, and it means that the net effect seen in models that include both greenhouse warming and geoengineered cooling of this type would never be just the status quo ante. Crucially, patterns of precipitation, among other things, change. But the disparity would probably be much stronger in a cloud-brightening scheme that targets only the ocean than in a stratospheric veil that operates globally. A wide range of climate phenomena are driven by temperature differences between the oceans and the land, from sea breezes to monsoons. How they would be affected by cloud brightening is something no one can yet say with any confidence. Nor can they say what the implications might be for ocean currents \u2014 a topic that Rasch is actively pursuing.  \n                Clouding the issue \n              Atmospheric scientists understand fairly well the process that creates and dissipates the clouds in question. But the relative weights of various processes and how they interact are still not understood \u2014 and those weights might be crucial to whether a cloud-seeding scheme could actually work, says Tom Choularton, who heads the cloud-research group that Latham started at the University of Manchester, UK, in the 1960s. Take, for instance, the role of convection in the marine boundary layer \u2014 the mixed layer of air that extends a few kilometres above the ocean surface. At night, convection mixes air from the bottom of the layer all the way to the top, lifting up moisture from the warm ocean to form clouds at the top of the layer. During the day, though, the clouds at the top of the layer bask in the sun, so the layer is heated from both the top and the bottom. This means that the whole-layer convection pattern can break down, with the upper part, including the clouds, 'decoupling' from the lower. The evaporation of drizzle beneath the cloud can cool the middle of the layer, exacerbating the effect. Decoupling would make it much harder to get condensation nuclei from the surface to the clouds in the first place. A linked concern, pointed out by Rob Wood of the University of Washington in Seattle, is that it is hard to gauge the effect of adding condensation nuclei on the clouds' lifetime. More cloud nuclei would be expected to make clouds persist by suppressing the growth of large droplets, which fall out of the cloud as precipitation. But suppressing precipitation might increase the vigour of the circulation in the upper part of the boundary layer. That could draw in dry air from above, leading to increased evaporation, which would thin and disrupt the cloud. The approach that Latham and Salter have laid out relies on the creation of very small water droplets, which dry out to form little specks of sea salt on which the water vapour in the cloud will condense. These salt particles will attract more water vapour than the particles that currently dominate the aerosols over the open ocean, which tend to be ammonium sulphate created by chemical reactions in the atmosphere. If the salt particles outcompete the sulphate particles, the net effect on droplet number might be small or even negative, with bigger droplets forming. To avoid this fate, the salt particles need to start off very small indeed. That makes designing the system to create the spray even more challenging. Salter has been told by various experts in industrial spraying that it would be simply impossible. He refuses to believe that, and the Edinburgh meeting saw vigorous debate between him and Armand Neukermans, a California-based innovator with a long track record of developing technology for, among other things, ink-jet printers, about ways to make very small droplets. They agreed that the smallest size feasible remains unclear \u2014 as does a great deal else associated with making such droplets day in day out, starting with unfiltered sea water, on a ship with no human crew.  \n                VOCALS support \n              At every point in the Edinburgh meeting, and at every scale from the micrometre to the global, the need for further research shone through. Many of the participants had been involved in a project that might serve as a model for future studies. In late 2008, researchers from 30 institutions used a range of satellites, aircraft, research ships and land-based observations to study cloud processes off the coasts of Peru and Chile, as part of a project called VOCALS, which is a component of an even larger study called the Variability of the American Monsoon Systems (VAMOS) project. (VOCALS, on which Wood was the principal investigator, stands for the VAMOS Ocean-Cloud-Atmosphere-Land Study). The data they gathered should improve understanding of the interaction between clouds, drizzle and aerosols, and the degree to which aerosols both above and below the clouds affect their properties. Of particular interest were holes that form in the cloud layer as a result of local decoupling. Daniel Rosenfeld of the Hebrew University in Jerusalem sees opportunity in such holes. He thinks that the aerosol concentrations will differ very little between a solid cloud bank and one filled with holes. In a poster at last December's meeting of the American Geophysical Union in San Francisco, he built on this idea to suggest that aerosol-producing ships might be able to convert a patchwork cloud layer into solid cloud quite easily. This already seems to happen sometimes in the smoky wakes of commercial ships. If it works, such a strategy would provide a much more powerful cooling effect than merely brightening clouds that are already there, giving Rosenfeld's approach remarkable leverage, he thinks, with a much smaller fleet needed. But as the more intense cooling has a more local effect, the equal-not-opposite mismatch between cooling and warming might be even more problematic. For pretty much everyone at the Edinburgh meeting, the medium-term goal for research into cloud brightening seemed to be a VOCALS-type experiment in which droplet-making technologies and their effects are studied over hundreds of square kilometres. Even that would not resolve all the issues about the feasibility of such a scheme, and it can't be rushed into \u2014 more basic research needs to be done first, in terms of cloud modelling and nozzle making \u2014 but it would be a start. And such tests could have implications for climate research more broadly. As Keith pointed out at the meeting, \"There's the potential to learn a lot more by intervening.\" Being able to experiment on clouds might reveal a great deal that climate scientists need to know, about what will happen in a warmer world even if geoengineering proves impossible, Choularton says. However, as yet there is no funding for such efforts. Of everyone using the VOCALS data, only one PhD student is doing so with a specific geoengineering-oriented goal. And what concerns researchers is that things might stay that way. It is one thing to get people to talk about geoengineering \u2014 it is another to make it a serious research topic with significant funding. Indeed, if general discussion of the possibility leads to increased polarization and opposition \u2014 as it very well might \u2014 then it may become more difficult, not less, to do the work necessary to test the possibility. Paradoxically, the intense debate over this topic could keep it alive and in the realm of possibility. At some point in the future, perhaps not too far away, society could be searching for a last-gasp response to global warming. The more that people talk about geoengineering, the more likely they will be to assume there is something solid in the idea. In fact, though, it may be as insubstantial as Salter's mist aspires to be. Since the writing of this article, the author now participates in scientific research on the topic. See also Editorial,  page 1077 , and  www.nature.com/climatecrunch . \n                     Nature Reports Climate Change \n                   \n                     David Keith\u2019s homepage \n                   \n                     Stephen Salter\u2019s homepage \n                   \n                     Tim Lenton\u2019s homepage \n                   \n                     VOCALS website \n                   \n                     Phil Rasch\u2019s homepage \n                   \n                     Daniel Rosenfeld\u2019s homepage \n                   Reprints and Permissions"},
{"file_id": "459153a", "url": "https://www.nature.com/articles/459153a", "year": 2009, "authors": [{"name": "Alexandra Witze"}], "parsed_as_year": "2006_or_before", "body": "The great Sichuan earthquake of 12 May 2008 caught Earth scientists off guard. A year on, Alexandra Witze reports from the shattered towns on how researchers have learned from their failures. Tucked below towering hillsides in Bailu, in China's Sichuan province, two school buildings face one another across a courtyard. Both are several storeys high, white with cheery light-blue trim. It's a peaceful April day, cool and humid; a rubbish bin shaped like a penguin sits at the side of the courtyard, as if waiting for someone to toss in a candy wrapper. But no one will be feeding the penguin today. That's because a nearly 2-metre-high ridge of buckled and uplifted concrete runs right through the courtyard, a manifestation of the geological faults that spawned the great Sichuan earthquake of 12 May 2008. Along the third side of the courtyard is a ghost. It is a pile of brick rubble, all that remains of another building that collapsed in the quake. There, geologists are hunting for clues to what happened on that day, digging a 40-metre-deep trench to search for signs of past quakes that emanated from these faults. These cracks in Earth's crust are deceptive pieces of geology. Both Chinese and Western scientists had mapped them before but failed to recognize their potential. \"I was astonished at this quake,\" says Xu Xiwei, deputy director of the Institute of Geology at the China Earthquake Administration in Beijing. The buildings that collapsed and the landslides and mud flows that buried towns combined to kill at least 70,000 people and cause widespread ecological damage (see  'Pandas in peril' ) in this rural corner of southwest China. More so than other quakes, this one has uncovered gaps in earthquake hazard research, both in China and elsewhere. When scientists assess seismic risk, they tend to focus on the faults that move the most and produce large earthquakes often. That strategy pays off with the many quakes that play by the rules. In western Sichuan, however, it turned out to be disastrously wrong. One year later, researchers are probing the deadly faults in the hope of finding ways to avoid repeating their mistakes. In retrospect, they say, the geology of the Longmen Shan, or Dragon's Gate Mountains, was trying to warn them.  \n                Mountains of trouble \n              The range marks the line where the 5,000-metre-high Tibetan plateau rams into the low, stable Sichuan plain. The region has the steepest topographical relief in the world, says geologist Clark Burchfiel of the Massachusetts Institute of Technology (MIT) in Cambridge: over a distance of just 50 kilometres as the crow flies, surface elevation changes by more than 4 kilometres. The Longmen Shan are a world of sloped hillsides cut by dramatic river valleys, the ideal place for quakes to trigger enormous landslides. That kind of topography does not persist without active geological forces at work, continually building the steep mountain belt. In the late 1980s, when Burchfiel and his colleagues began mapping the area, they were convinced they would find evidence for large ground movement along the Longmen Shan: perhaps 10 millimetres per year of 'shortening', in which the plateau and plain converge and push up the mountain range. But years of walking the faults unearthed no evidence for this amount of shortening in the recent geological past. By mapping rock formations, the team found evidence of just 1\u20132 millimetres of movement per year, instead of the 10 they were expecting. \"At that rate, you don't expect to have a mountain range that high,\" Burchfiel says. Nonetheless, he couldn't deny what the rocks were saying, so eventually he published a major geological overview of the region, supposing that no one would believe the low rates of shortening. Then Burchfiel moved on to map other nearby areas. Over time, however, studies have confirmed his conclusions. Researchers measured ground motion in the area using Global Positioning System receivers and found low rates of slip across the Longmen Shan, confirming the 1\u20132 millimetres per year suggested by Burchfiel 2 . To a geologist, that rate seems relatively benign, because faults store up potential earthquake energy in proportion to the speed of the regional crustal motion. Take two spots on either side of a mountain range, for example. If one is moving quickly in relation to the other, the stress on rocks in between will build up quickly \u2014 stress that has to be released by rock movement along a fault. In most cases, that movement is not steady but happens only infrequently, when the stress grows great enough to overcome the friction between rocks on either side of the fault. That sudden release is the earthquake. In the Sichuan quake, which measured 7.9 on the moment magnitude scale, there was nearly 5 metres of slip along the Beichuan fault, the biggest of the faults that ruptured last year (see  map ). Given how slowly stress accumulates in the region, rough calculations suggest that quakes of that scale should occur very infrequently, about every 2,000 to 10,000 years 3 . Large shocks in the past will have left their marks in local geology. But the record is hard to read in the Longmen Shan: heavy rains and high erosion rates have obscured much of the evidence, says Alexander Densmore, a geologist at Durham University, UK, who has mapped faults in the area. \"There aren't that many places that you can really see the past history,\" he says. Most of the recent known quakes along the Beichuan fault have been much smaller than the 2008 quake, including one magnitude-6.2 quake in 1958 and another in 1970, says Chen Zhiliang, a geologist at the Chengdu Institute of Geology and Mineral Resources. There is no archaeological evidence that the town of Beichuan itself has ever been destroyed by a quake since it was founded some 1,500 years ago. So few thought that the Longmen Shan posed a major seismic hazard. \"I don't think there was a reason to say there would have been a major quake here,\" says Leigh Royden, a geophysicist at MIT who has modelled the region's tectonics. In hindsight, it's easy to see the danger of dismissing the quake potential of the Longmen Shan. Just because something happens rarely does not mean it will never happen. It should have been obvious that the faults along that range were sleeping dragons that would awake some time. But researchers have only so much time and money to spend on seismic-risk assessments, and they therefore focus on areas that are known to have major quakes every few hundred years \u2014 not ones that might stay quiet for 5,000 years. For example, rather than worry too much about the Beichuan fault, Chinese geologists had focused on a pair of far more active fault zones to the west: the Anning He and the Xianshui He faults, both of which slip at rates of up to 10 millimetres per year. The China Earthquake Administration has spent most of its monitoring efforts on these active faults, including deploying nearly 300 broadband seismometers \u2014 ones that capture a wide range of vibrational frequencies \u2014 in the world's densest array to map the underlying crust. When the Beichuan fault broke instead, seismologists scrambled to refocus on the Longmen Shan. Some are now looking into whether a new reservoir nearby triggered the quake (see  'The reservoir link' ). The question now is what the Sichuan quake tells geologists about future seismic risk. Some say that more attention should be paid to regions with steep topographical relief, even if they have minimal ground movement. Royden points to an analogous region in Canada's Northwest Territories, but few people live there, so it is unlikely to become a priority for research. Within China and in other densely populated regions, there are few obvious analogues, although researchers will surely be taking a fresh look at mountainous zones. Beyond being deceptively lethargic, the Beichuan fault caught Earth scientists off guard last year in another way. From the surface, the fault appears to be divided into relatively short segments that were assumed to move separately in relatively small earthquakes. \"We traditionally tend to look at individual fault segments and say those are the maximum size of the earthquake,\" says John Shaw, a geologist at Harvard University. But if the segments can connect, \"the magnitude of those earthquakes is much greater than anticipated\". That is what happened last year. The Beichuan fault ruptured across several segments totalling 240 kilometres, while a secondary fault to its southeast, the Pengguan fault, broke for 72 kilometres. The segments apparently connect at depth, allowing the quake to grow larger than would have been expected. Chinese geologists are now beginning to map in detail the faults that connect with the Beichuan fault. The danger that remains is another concern. Because the Beichuan fault broke almost entirely to the northeast of its epicentre, some scientists wonder whether the segment that runs towards the southwest is ready to go. Nearby faults may also pose a risk. One study suggests the Beichuan quake increased stress, among other places, on the Xianshui He fault, and on other faults near the city of Ya'an and southeast of Chengdu, the capital of Sichuan 4 . Another study proposes that the chance of a magnitude-7 or greater quake in the area during the next decade is now 8\u201312%, higher than it was before the 2008 quake 5 .  \n                How it hit \n             The biggest city in this threatened zone is Chengdu, now teeming with 10 million people. Constant traffic jams and high demand make it near impossible to hail a taxi during working hours. Young professionals who have relocated from Beijing or Shanghai to enjoy a more laid-back lifestyle thread their way through the crush on electric bikes. Ethnic Tibetans, part of the diverse mix in southwest China, find themselves shouldered out of the boom, and many end up as beggars on the pavement. Chengdu is also home to the province's leading earthquake scientists, for whom the 12 May quake \u2014 referred to as '5/12' for short, like '9/11' in the United States \u2014 occurred practically in their backyards. In his tidy office in Chengdu, with a Chinese-language copy of  On the Origin of Species   at hand and a picture of Albert Einstein looking on, Chen recalls what it was like at 2:28 p.m. on 12 May. The office began shaking with the strongest tremors he had felt in more than 40 years in the city. Staff members evacuated the building; people poured into the streets as bricks rained down. Chen tried to call his son, but phone lines were dead, so he rushed to the nearby primary school to find his granddaughter. Then he came back to his office building, which was constructed to some of the highest quake-protection standards in the city, and within two days had posted online a history of quakes in the Longmen Shan area. Across town, when the quake hit, geodesist Du Fang hid under the sturdy wooden table in her office at the Sichuan seismological bureau. Du, the deputy director for earthquake prediction, says she had no idea the quake was coming. Although there are anecdotal reports of toads pouring into Sichuan streets as indicators in the days before the quake, scientific data justify her. Seismometers along the Beichuan fault recorded no increase in tremors that might have presaged the quake, although one station 640 kilometres south of Beichuan recorded changes that some claim were a warning. In many ways, the Chinese government is still struggling with the aftermath of the disaster. Praised initially for its quick response in sending emergency crews into the affected areas, the government soon faced angry parents asking why so many schools had collapsed. Bitterness lingers. In Yingxiu, the town closest to the epicentre, where as much as 80% of the population died in the quake, rows of temporary housing crowd up against the ruins of Xuankou Middle School where 55 people, including 43 students, were crushed to death. The government is rebuilding at breakneck pace. Above each cluster of temporary shelters rises an optimistic billboard showing gleaming plans of new houses to be built. Some are already done: fresh paint and new concrete rise from the recently cleaned-up hillsides, with red characters for good wishes inscribed over the brand-new doorways. New houses along the Longmen Shan are supposed to be able to withstand a magnitude-8 quake; previously, building regulations in Chengdu required construction to withstand only a magnitude-7 quake, which has one-tenth the intensity of shaking. In many places, however, reconstruction is taking place so quickly that no one is confident that building codes are being followed. Villagers bring hand-carts to the landslides that once blocked the road and haul away rocks to break them and use them to start building homes afresh. Piles of brick \u2014 one of the worst construction materials for a quake-prone zone \u2014 dot the sides of main roads, waiting to be mortared together into new homes. Lorries piled with construction materials cause hours-long traffic queues along the narrow roads that thread through the mountain valleys. Even as construction cranes rise from town centres, the landslide-scarred mountains above tower ominously. More than one-fifth of the people who died in the quake were killed by landslides or mud flows, says Cui Peng, a geomorphologist at the Institute of Mountain Hazards and Environment in Chengdu. Precise numbers are hard to tally \u2014 the affected area sprawls over 130,000 square kilometres in 51 counties \u2014 but estimates suggest that at least 50,000 landslides occurred, perhaps as many as 100,000 or more 6 . One, in Wangjiayan, killed 1,600 people. Another, at Beichuan High School, buried 400 students. Elsewhere, landslides did not kill directly but dammed rivers, creating more than two dozen major 'quake lakes' that threatened residents downstream. The danger of landslides, Cui warns, will be even more acute this rainy season, which begins late this month. The quake destabilized a number of slopes in the area, making them particularly prone to failure after rain. Last September, for instance, heavy rains sent a mud torrent sweeping into the empty centre of Beichuan, already devastated by the earthquake months earlier. The problem is exacerbated by large-scale damage to the landscape from mining practices that have carved out hillsides, and from deforestation that has stripped the slopes of their protective trees. If people rebuild houses in places that are prone to landslides, Cui notes, constructing them to withstand quakes won't help. \"People often forget to account for disaster prevention in reconstruction,\" he says. His team at the institute, which is part of the Chinese Academy of Sciences, has made detailed recommendations to the government to highlight areas that should avoid rebuilding. Meanwhile, new houses are springing up informally in the villages that dot the Longmen Shan \u2014 one by one, and probably not in government-approved areas.  \n                A flood of data \n              Amid the disheartening news, however, scientists say the data from the earthquake itself will illuminate the region's geology at a more fundamental level. Those data exist because in recent years the Chinese government has spent a lot of money on new equipment to try to make its Earth sciences competitive in the world arena. A crown jewel of the government's programme is the array of nearly 300 broadband seismometer stations, which was deployed in western Sichuan by Liu Qiyuan of the China Earthquake Administration and his team. The envy of Western scientists, the array boasts the densest arrangement of seismometers of any large network around the world: it has yielded more than 7 terabytes of data so far. Rob van der Hilst, a geophysicist at MIT who set out an earlier 25-station array in the same region, calls the Chinese network \"an enormous tour de force\". First deployed in October 2006 and spaced 5\u201330 kilometres apart, the solar-powered stations cover 370,000 square kilometres of mountainous terrain; someone visits each station every four months to collect the data. Originally funded with 60 million yuan (US$9 million) from the ministry of science and technology and more than 8 million yuan from the provincial government, Liu now scrapes together 1.8 million yuan per year to keep the network operating. Last May, the great quake knocked out three of the array's stations; one was squashed under a massive boulder. But the data recorded at the time by nearby stations are yielding an unprecedented glimpse into the crust of western Sichuan; a major quake has never been captured in such detail by a network like this. \"It's a very rare opportunity in the world,\" says Liu. \"This quake should play an important role in seismological history.\" Preliminary data suggest that there is a major change in the geology roughly 20 kilometres below the surface, where relatively brittle material gives way to deeper, softer rock through which seismic waves travel more slowly. This could help explain, Liu says, why the quake and all its aftershocks occurred in the upper 20 kilometres of crust. Liu is now collaborating with van der Hilst and Michel Campillo of Joseph Fourier University in Grenoble, France, to run the data through new seismic analytical techniques 7 . He is also working with scientists from Taiwan, who are interested in probing any possible analogies with Taiwan's 1999 Chi-chi earthquake. Liu originally set up the network to monitor what had seemed the biggest threat in the region: the Anning He and Xianshui He faults. After the 2008 quake, however, Liu shifted some of his stations to the north and east, onto the Beichuan fault. The array will remain in place there for a year, after which most of the stations will be moved to other areas, having collected the data he wants. Meanwhile, other researchers are trying different ways to investigate the geological history of the Longmen Shan. In a project spearheaded by the land and resources ministry, a team is drilling four holes along the fault zone to collect continuous rock cores from as deep as 4 kilometres. A pilot hole in the village of Hongkou has passed 650 metres' depth and may have already penetrated the fault zone, says Li Haibing, the project's chief geologist, who is at the Institute of Geology and Geophysics of the Chinese Academy of Sciences in Beijing. Team leaders intend to put seismological instruments down the hole for long-term monitoring. At the shuttered Bailu school, just off its tortured courtyard, the palaeoseismology trench is getting ever deeper. The pit, hand-dug by workers carrying buckets of dirt on yokes, has already revealed evidence of past tremors. Arcing layers of cinder mark the remains of fires triggered by smaller quakes like those that occurred in 1958 and 1970. The results from this trench, along with studies of the buildings still standing in Bailu, may aid future planning. Xu says that the government may intensify mapping of all the active fault traces in the country, in the hope that more precise knowledge may save lives. As they look back on the earthquake, Earth scientists in China and around the world say that they remain chastened by their lack of foresight. Although many say they could not have recognized a hazard that rears its head only once every few thousand years, the recent disaster has made researchers rethink their assumptions, especially in areas where geological forces are so evidently at work. In the future, they will be less likely to conclude that areas showing little evidence of movement are safe from large quakes. That will come as little consolation to the people of Bailu. On a spring day, a group of children swarms over a concrete court in town, shouting and elbowing each other in a game of basketball near the abandoned school. Up above, a caged songbird overlooks the playground for good luck. Rows of vegetable gardens dot the hillsides, fresh green against newly tilled dirt. But the school itself remains closed for good, a memorial park to the victims of the 2008 quake. \n                 See Editorial,  \n                 page 140 \n               Alexandra Witze is  Nature 's chief of correspondents for America. Additional reporting by Jane Qiu,  Nature 's retained correspondent in Beijing. \n                     China Earthquake Administration \n                   \n                     China Geological Survey \n                   \n                     Chinese Academy of Sciences \n                   Reprints and Permissions"},
{"file_id": "4581094a", "url": "https://www.nature.com/articles/4581094a", "year": 2009, "authors": [{"name": "Nicola Jones"}], "parsed_as_year": "2006_or_before", "body": "It's simple to mop carbon dioxide out of the air, but it could cost a lot of money. In the second of three features on the carbon challenge, Nicola Jones talks with the scientists pursuing this strategy. When Frank Zeman made a device to mop carbon dioxide out of the air of his laboratory at Columbia University in New York, it didn't look like a machine that could save the planet. Black tape held together plastic parts eaten away by lye; baking soda encrusted the outside. If someone walked behind the air intake (which looked like a grey hair dryer), their exhalations would interfere with the results. But the contraption worked. Such a device, if scaled up and perfected, could be used to dial back Earth's greenhouse thermostat by taking CO 2  straight out of the sky. Although Zeman's fully functioning desktop device has not yet made it out of the lab, others have developed parts of bigger and more ambitious devices, some of which are heading for commercialization. All are imperfect, but they all work, and that undeniable fact is turning air capture from a 'what-if' pub discussion into a serious proposal. \"Nobody doubts it's technically feasible,\" says Zeman, now director of the Center for Metropolitan Sustainability at the New York Institute of Technology. Increasingly it looks like air capture will be needed. Efforts to limit CO 2  emissions will need to be strengthened massively if they are to keep concentrations from reaching dangerous levels, so there may be little choice but to remove some of the CO 2  already in the air (see  page 1091 ) or cool the planet in other ways (see  page 1097 ). \"Without having something that is carbon negative, the possibility of avoiding high levels of CO 2  is basically zero,\" says Peter Eisenberger, former director of the Lamont\u2013Doherty Earth Observatory at Columbia University and co-founder of the air-capture company Global Thermostat. In a recent analysis, Roger Pielke of the University of Colorado in Boulder put some numbers on the task ahead. Assuming a middle-range scenario projected by the Intergovernmental Panel on Climate Change (IPCC), humanity must somehow prevent itself from emitting (or must soak up) 650 gigatonnes of carbon by 2100 to keep concentrations under 450 parts per million (p.p.m.) at that point 1 . To put that in perspective, humans added about 9 Gt of carbon to the atmosphere last year. Economic studies suggest that some reductions could come affordably, or even at a profit, from fairly obvious places. Deeper cuts would require serious money. A report from the international consultancy McKinsey estimates that energy-efficiency measures, conversion to low-carbon energy sources, and forestry and agriculture management could \u2014 with serious effort \u2014 cut about 10 Gt of carbon emissions annually by 2030, for under US$300 per tonne. But it will be much harder and more expensive to get at any fraction of the remaining 9 Gt of annual emissions expected that year in a business-as-usual scenario 2 . Pielke is one of many beginning to wonder whether mopping up CO 2  with chemicals and machinery \u2014 a strategy with an ironically un-green image \u2014 might be part of the answer. It could be an unbeatable idea. Sponging CO 2  from the air has a direct, immediate and measurable effect on the source of the problem, avoiding the possible side effects of geoengineering. Air-capture devices can be sited anywhere, although preferably on cheap land with an untapped renewable energy supply and a geological reservoir that could serve as a dump for the captured gas. In principle, there is no limit to how much CO 2  you can extract: name an atmospheric concentration you'd like to end up with, and the technology can get you there. To many in the 1990s, that cost seemed ridiculously high. In engineering circles, the dogma ran that the ease of extracting a gas was proportional to its concentration. At 0.04%, CO 2  in the atmosphere seemed exceedingly difficult; the effort and money needed to extract and store CO 2  from industrial flue stacks, where it can make up perhaps 10% of all gas, is already high, estimated by the IPCC to cost between $70 and $260 per tonne of carbon (see   Nature  442, 620\u2013623; 2006 ). The assumption was that filtering CO 2  out of the atmosphere would be 250 times harder and vastly more expensive. That assumption turns out to be wrong. The benefit of air capture is that it deals with a nearly infinite and relatively clean source, so there is no need to scrub out polluting gases before beginning and no need to take out every last bit of CO 2 . Thermodynamically, the task proves to be about twice as hard as flue-stream capture 3 . Better still, the technology to make such devices is already available. Although air capture has been ignored by the IPCC and sidelined by scientists, that is changing. Researchers in Canada, the United States and Switzerland have come up with plans, tested prototypes, filed patents and founded companies to pursue the idea. Which technology will win out is yet to be seen. The Virgin Earth Challenge, launched by airline entrepreneur Richard Branson and former US vice-president Al Gore in February 2007, offers up to $25 million for the first demonstrably viable commercial design to remove significant amounts of greenhouse gases from the atmosphere (the exact criteria are unclear). As yet the prize goes unclaimed. The bare-bones chemistry of carbon capture is simple. The simplest thing to do is to expose air to a sorbent of lye (NaOH). This reacts with CO 2  to create a solution of sodium carbonate. It's so simple that Klaus Lackner, also of Columbia University, once helped his daughter to do it for a school science project. To get the carbon out of solution, a trick can be borrowed from the pulp and paper industry: when slaked lime (Ca(OH) 2 ) is added to the mix, particles of calcium carbonate settle out. Throw this into a kiln and you are rewarded with a pure stream of captured CO 2  and quicklime (CaO), from which the sorbent can be renewed. boxed-text   \n                Crude prototypes \n              This is how Zeman's desktop device worked, and also how David Keith of the University of Calgary in Alberta, Canada, is pursuing the problem. Keith built a large-scale machine a few years ago to see how much CO 2  could be sucked up in practice. He calls it the 'Russian tractor' technique \u2014 not especially high-tech, but proven to work. A prototype featured on the Discovery Channel in 2008 mopped up a few kilograms of carbon overnight. Keith didn't build the second half of the scheme \u2014 the 900 \u00b0C kiln that spits out concentrated CO 2  \u2014 because that's already a known industrial process. It's also the energy-intensive and costly part. Nevertheless, he is setting up a company called Carbon Engineering, convinced the idea is worth pursuing, and is working to reduce costs. Keith has chosen the most obvious approach to the problem but admits that others have \"much more clever\" schemes. That includes a material being developed by Lackner for the company Global Research Technologies, based in Tucson, Arizona, and funded by a $5-million donation from the late billionaire Gary Comer. (Comer, founder of the Lands' End clothing-catalogue company, donated money to fight climate change after he sailed through the Northwest Passage in 2001 without being blocked by ice.) In April 2007, Global Research Technologies had its first demonstration of air capture with a prototype device. It was a success, widely lauded in the press, but it needed further work. For one thing, it just vented the captured carbon out the back. For another, it didn't behave as it was expected to. \"When we closed the door on it, something was happening we didn't understand,\" says Lackner. The device used a commercially available wet resin to mop up CO 2 . When its designers analysed the results, however, they realized the material was better than they thought. Not only did it turn CO 2  into carbonate, but in a dry environment it would go a step further to bicarbonate. When they exposed the resin to water, the bicarbonate flipped back to carbonate, releasing CO 2  and water vapour. They didn't need a kiln \u2014 they just needed to expose their loaded resin to water in a relative vacuum, and then pressurize the result to condense the water out. \"All you pay for is making the vacuum, pumping and pressurizing,\" says Lackner. Others argue that kiln-temperature heat isn't necessarily a problem. In Zurich, Aldo Steinfeld and colleagues at the Swiss Federal Institute of Technology are using the Sun-tracking mirrors used by solar-power plants to heat up their air-capture reactor to 800 \u00b0C. They have a fully functioning lab model, and hope to have a larger field prototype within a few years to hand to an industrial partner. Eisenberger, on the other hand, needs only low temperatures \u2014 under 100 \u00b0C, achievable using waste heat from power plants or cement factories \u2014 to run his system. To test these ideas, Eisenberger founded a company called Global Thermostat in 2006 with Graciela Chichilnisky, an economist and mathematician at Columbia University. Eisenberger imagines a future in which air-capture devices start to be deployed by 2015; by 2020, half of new power generators are matched with air capture, and by 2040, some 9 Gt of carbon is being pulled from the air per year, to a total of 650 Gt by 2100 \u2014 the amount that Pielke also estimated would be needed. (Coincidentally, that total roughly matches the IPCC's estimate of the Earth's geological capacity to act as a garbage dump for buried gas). This whole operation could be accomplished by, say, 35,000 facilities that each took a quarter of a million tonnes of carbon per year out of the air. The combined footprint of this global operation would total less than 300 square kilometres \u2014 a fraction of the size of London. Because Eisenberger assumes the world will also make substantial cuts in emissions over the same period, his air-capture scenario would return atmospheric concentrations to 380 p.p.m. of CO 2  by 2100, and they would continue to decline thereafter. The price? About $60 trillion for the air capture, or roughly $660 billion per year. That's on the same scale as the US economic stimulus package against the current recession, but every year for a century. The price is the hardest thing to estimate, since no one has yet built a full-scale device. When Lackner first put out figures of about $100 per tonne of carbon in 2006, many saw it as massively over-optimistic \u2014 some joked that the real price was one mysterious 'Lackner' per tonne, given the apparently magical capacities of his material, the identity of which was kept under wraps for commercial reasons at the time. Today, Eisenberger's estimate is slightly cheaper still.  \n                Cost competitive \n              At the other end of the scale, Keith has estimated it might cost $500 per tonne of carbon using today's technologies (see  'A way to pay for capturing carbon dioxide' ). That would rack up a bill of $325 trillion to soak up 650 Gt of carbon, but Pielke notes that such a price tag would still only be 2.7% of global economic output by 2100. That compares favourably with price estimates of the IPCC (\u20131 to 5% of global economic output) and economist Nicholas Stern (\u20132 to 4%) for stabilizing air concentrations at 450 p.p.m. without air capture. \"We should be looking into it, at least,\" Pielke concludes. To put the cost issue in perspective, he notes, if all the emissions from US cars were sucked up by air capture using today's technology, and the cost tagged onto the price of petrol, motorists in the United States would still have one of the lowest pump prices in the world. Many air-capture enthusiasts talk about countering something on the scale of global aircraft emissions, projected to reach about 0.25 Gt of carbon per year by 2030. (Technology can reduce carbon emissions from power plants and cars, but it is difficult to reduce such emissions from planes.) This is where Roger Aines of Lawrence Livermore National Laboratory in California sees air capture playing a potential part. He and his colleagues are making an overview assessment of the strategy, and estimate that the quarter-gigatonne target could be met by, say, a thousand 250,000-tonne air-capture facilities requiring a total of 900,000 gigawatt-hours of energy per year. This is slightly more than the total electricity generated by the 104 nuclear power plants in the United States. If wind were to supply the power, the world would need something like 135,000 additional 1.5-megawatt turbines. That would approximately double the current global wind-power capacity. Such a scenario is within the realm of possibility, but it demands an increase in energy production just at a time when we should be trying to break our energy addiction. For some, that's a critical problem. Every dollar spent on air capture instead of shifting to renewables is \"a long-term loss to society\", says Mark Jacobson of Stanford University in California. His concern is that researching a 'get out of jail free' card for climate change would provide an excuse to continue unabated emissions. That worry is voiced by many, but it is also dismissed by many. \"For some people there's concern that if there's hope that air capture will work, it reduces the incentive to reduce emissions,\" says Pielke. \"That makes as much sense as saying we shouldn't have open-heart surgery because it stops people from lowering their cholesterol. We need both.\" No one argues that air capture is a cure-all. Eisenberger sees it as a necessary bridge to get us more painlessly to our goal of a renewable-energy economy. Despite the 'reasonable' price tag of air capture, it is still cheaper, and more sensible, to capture large-industry pollutants at source and to reduce energy use. \"Air capture would be a back-stop technology to fill in the gap between what we can achieve and what our goals are,\" says Pielke. \"It is the most expensive climate-mitigation technology,\" agrees Zeman. \"And that's a good thing. It has this role as the upper bound on solving the climate problem.\" No matter what we have to do to get the atmosphere settled, it won't cost more than this. See also Editorial,  page 1077 , and  www.nature.com/climatecrunch . \n                     Nature Reports Climate Change \n                   \n                     David Keith\u2019s air capture work \n                   \n                     Frank Zeman paper on air capture \n                   \n                     Global Research Technologies \n                   \n                     Peter Eisenberger\u2019s patents \n                   \n                     Swiss Federal Institute project \n                   Reprints and Permissions"},
{"file_id": "459024a", "url": "https://www.nature.com/articles/459024a", "year": 2009, "authors": [{"name": "Ananyo Bhattacharya"}], "parsed_as_year": "2006_or_before", "body": "What do protein crystallographers dream of? The eukaryotic ribosome, the spliceosome, the nuclear-pore complex, the HIV trimer and almost any transmembrane protein, finds Ananyo Bhattacharya. When considered up close, the blood protein from a sperm whale is a marvellous thing. Or so it seemed just over 50 years ago, when John Kendrew and other researchers at the Cavendish Laboratory in Cambridge, UK, reported that they had used X-rays to reveal the three-dimensional structure of a globular protein for the first time. Analysis of the diffraction pattern caused by crystals of myoglobin, which was chosen for its simplicity, required one of the most powerful computers in the world at that time, and later won Kendrew a share of the Nobel Prize in Chemistry with his Cavendish colleague Max Perutz. The picture it created \"is more complicated than has been predicated by any theory of protein structure\", Kendrew and his colleagues wrote in a  Nature   article 1 . Half a century on, X-ray crystallography's techniques are in outline the same: you need a crystal, X-rays and calculating power to make sense of the diffraction pattern. In all these three areas, however, progress has been enormous. Washing machines now have more computing power than the computers Kendrew used and synchrotrons offer X-rays a trillion times more brilliant than those available in Cambridge 50 years ago. Growing crystals is increasingly routine: with many proteins it can be an automatic process, and the skills needed for the harder ones have come on in leaps and bounds. But so, too, have the ambitions of the crystallographers. They still believe, as did those earliest molecular biologists, that there is no better way to understand a complicated machine than to capture an atomic-scale picture of it. But now they dream of using their techniques on things far more challenging and complex than anyone imagined 50 years ago \u2014 including some machineries of protein and nucleic acid that dwarf myoglobin as a whale does a minnow. (For other desired structures, see ' Best of the rest '.)  \n                The building site \n             The crystals look beautiful under the microscope. Suspended in a tiny drop of solvent, each gem-like growth is a symmetrical array of copies of one of the two subunits of  the ribosome   \u2014 the cell's machine for making proteins. Each of those subunits is a tangle of many proteins and RNA. What makes the crystals especially interesting is that their constituents come not from bacteria but from eukaryotic cells. Which plant, animal or other eukaryote provided those cells, though, is for the moment a secret. Such crystals are rare and highly sought after. Unfortunately, Nenad Ban ruefully admits, these ones grown in his lab at the Swiss Federal Institute of Technology in Zurich (see photo) will not be yielding a protein structure any time soon. A crystal that looks pretty in visible light can still be a mess by the demanding standards of X-rays. \"We have fantastic-looking crystals,\" he says. \"But they are not very good in terms of diffraction.\" Ban has been here before. Nearly a decade ago he was in one of several teams striving to solve the structure of the simpler ribosomes used by bacteria and archaea. He and his colleagues in Tom Steitz's lab at Yale University published the structure of the 50S subunit of the ribosome of  Haloarcula marismortui   \u2014 a microorganism that lives in the famously salty waters of the Dead Sea \u2014 in 2000 (see ref.  2 ). The 50S subunit consists of about 30 proteins and has a mass of 1.5 million daltons, compared with myoglobin's 1,700 daltons or a carbon atom's lowly 12. Just a month later, a second team published the structure of the smaller 30S ribosomal subunit, using material from the bacterium  Thermus thermophilus 3 . The following year, Harry Noller at the University of California, Santa Cruz, and his colleagues unveiled the structure of the whole bacterial ribosome, revealing much about how it binds to the transfer RNAs that deliver amino acids to a growing protein 4 . These structures triggered an avalanche of new work in a field that had, in their absence, largely ground to a halt. They allowed researchers to see, for example, how the ribosome catalyses the joining of one amino acid to another with a peptide bond. Now, says Jennifer Doudna of the University of California, Berkeley, \"we know a lot of detail about what the bacterial ribosome looks like, how it works, how peptide bonds are made, and even a lot about how the initiation process is regulated in bacteria\". The eukaryotic ribosome takes the competition to another level. It is bigger \u2014 containing some 80 component proteins compared with the bacterium's 50 to 60 \u2014 but that is not the only, or even the main, challenge. \"There's a lot more bells and whistles; there's a lot more regulation that goes on,\" Doudna says. It's those bells and whistles that make the eukaryotic ribosome more difficult to work with than its bacterial counterpart. In mammals, for example, a host of additional proteins called initiation factors interact with the ribosome. The initiation factors are themselves complex assemblies \u2014 eukaryotic initiation factor 3 (eIF3), for example, is made up of at least 12 proteins and is only a few times smaller than the ribosome itself. Ribosomes that are purified from a cell could be in any number of combinations with these and other proteins. Getting material that is pure and homogeneous is a significant hurdle, says Doudna. Simpler proteins can be mass-produced by inserting the appropriate gene into a cell culture that then churns out proteins, but ribosomes are too large and complex to be produced in this way. Some who worked on the bacterial structures are joining the hunt for the eukaryotic one, and the competition is fierce. That is why Ban will not reveal from which eukaryote his ribosomes are harvested, nor which subunit his lab has managed to crystallize. The fact that he has crystals \u2014 albeit ones that don't diffract X-rays well \u2014 is an important proof of principle, he says. But \"it's the endgame that counts\".  \n                The editing suite \n             Noller, who with colleagues cracked the structure of the complete bacterial ribosome, says the next big thing is another signature speciality of the eukaryote,  the spliceosome  . \"The spliceosome would be fantastic,\" he says. \"That would make the ribosome look like child's play.\" Made up of 150 or so proteins, the spliceosome slices and dices freshly made messenger RNA, stitching together the 'exon' sequences that will be translated into protein and relegating the 'introns' to the cell's cutting-room floor. It can bring together sections of RNA maybe tens of thousands of base pairs apart and then snip out the intervening loop, like a movie editor running though many metres of film to splice two shots together. The activity of the spliceosome fascinates biologists because it could help explain how eukaryotic cells generate biological complexity \u2014 in the form of different RNAs and proteins \u2014 from a single DNA sequence. But that fast and continuous activity is also what makes it a gruelling challenge for crystallographers. \"Tremendous work has been done genetically and biochemically to understand how splicing works and how it's regulated,\" says Doudna. \"The big missing piece in that field is not having access to high-resolution structural information for how the spliceosome is put together and what is driving the \u2026 changes that have to occur during the splicing process.\" The problem for crystallographers is that the spliceosome is not just one machine \u2014 it is five, and all are in constant motion. Called small nuclear ribonucleoprotein particles (snRNPs, pronounced 'snurps'), these five protein\u2013RNA complexes come together transiently in a complicated, fast-moving dance, their mercurial assembly about the size of a ribosome but far less stable. \"Unlike the ribosome, you cannot simply purify a spliceosome from cellular extracts because you have many different spliceosomal complexes \u2014 different snapshots at different stages of function,\" explains Reinhard L\u00fchrmann of the Max Planck Institute for Biophysical Chemistry in G\u00f6ttingen, Germany, whose group has crystallized several proteins that are components of snRNPs. Stalling the spliceosome at a particular stage of the cycle is a \"major challenge\" says L\u00fchrmann. One way might be to use a mutant RNA message or a small molecule to arrest the spliceosome mid-splice. A team led by Kiyoshi Nagai at the Medical Research Council Laboratory of Molecular Biology in Cambridge, UK, recently published a relatively low-resolution version of the crystal structure of one of the smaller snRNPs, which they reconstituted from its RNA and seven recombinant proteins (see graphic) 5 . Reconstituting all 150 proteins in the complete spliceosome, however, \"does not appear feasible\" in the foreseeable future, L\u00fchrmann says. Instead, he is pursuing the structure using cryo electron microscopy, in which samples are flash-frozen in liquid ethane to protect them from the bombardment of high-energy electrons in an electron microscope. Recent advances in this technique have allowed it to show structures at resolutions as low as 5 angstroms, but do not yet approach the 2-angstrom resolution of good X-ray crystallography. L\u00fchrmann predicts that further advances in hardware and software will allow cryo electron microscopy to fill out the broad atomic structure of the spliceosome \"in the next few years\".  \n                The monstrous Maw \n             More than 30 times the mass of the ribosome and around 100 nanometres wide,  the nuclear-pore complex   is a doughnut-shaped assembly of hundreds of proteins that straddles the eukaryotic nuclear membrane. One of the largest protein conglomerations in the cell, the structure serves as both gate and gatekeeper, choosing which nucleic acids, proteins and other molecules to let in and out of the nucleus. With only around 200 pores in a yeast cell \u2014 compared with perhaps 10,000 to 20,000 ribosomes in a bacterium \u2014 purifying the complex from cells is \"really impossible\", says Andr\u00e9 Hoelz, a researcher who works on the nuclear-pore complex in G\u00fcnter Blobel's laboratory at The Rockefeller University in New York. Hoelz and his colleagues are taking a different tack \u2014 expressing and crystallizing single proteins or small protein complexes from the pore, and then piecing them together like a jigsaw puzzle to reconstitute the whole structure. \"When we started this work five years ago people were saying that you can't possibly get this done because of the sheer size of the nuclear-pore complex,\" Hoelz says. There are two factors working in the group's favour. First, the nuclear-pore complex has an eightfold rotational symmetry and twofold mirror symmetry (see graphic). That means that the hundreds of proteins that make up the pore are in fact made up of repetitive arrangements of only about 30 different types \u2014 around half the variety in the ribosome. Second, in many organisms the nuclear-pore complex is dismantled when cells break down their nuclear membrane before dividing, and later reassembled piece by piece. It is always broken into the same building blocks, and it is these conserved components that Hoelz hopes to crystallize and slot together. If this approach is successful, it will provide a detailed 'pseudo-atomic' picture of the nuclear pore's structure, although it may not provide the clarity that a crystal structure of the entire complex would. There's another feature of the nuclear-pore complex that is a challenge for crystallographers. The centre of the pore is a mesh of fluttering protein filaments that don't fold in a regular way, but instead flop and dangle; they are thought to play a crucial role in selecting which proteins are ferried through the pore. Because a crystal structure is really an average of the arrangements of the atoms in millions of protein molecules in the crystal, the tentacles, which are in constant motion, would be an ill-defined blur. \"Imagine one could crystallize the nuclear-pore complex; a quarter of it would be natively unfolded, and that's the business end,\" says Michael Rout, whose Rockefeller University lab also studies nuclear-pore complexes. So far, only isolated structures for some bits of the tentacles exist, solved by teams that break them off and crystallize them separately. The nuclear-pore complex, then, runs up against a fundamental limit of crystallography \u2014 it generates snapshots, not movies. And it is not alone: up to 30% of eukaryote proteins are wholly or partly disordered. To see proteins in action, some crystallographers and modellers have turned to computer simulations that jump between two or more 'frames', each obtained by crystallography. Wayne Hendrickson of Columbia University in New York says there is also a lot of excitement about technologies that might be possible at facilities such as the European XFEL (X-Ray Free Electron Laser) under construction in Hamburg, Germany (see  page 16 ). The idea here is that an extremely short burst of X-rays could be scattered off a single protein molecule, blowing it apart but revealing something about its structure before the disintegration. \"You are in principle able to capture the molecule in action,\" Hendrickson says. That's something for the future: the XFEL will not be commissioned until 2014. For now, Rout says, \"the current approach is probably the most successful, which is to continue to crystallize the structured parts and put that together with other data to build a complete picture\".  \n                The killer key \n             \"A big missing piece in the virology field is the structure of  the HIV trimer  ,\" says Ian Wilson, a structural biologist at the Scripps Research Institute in La Jolla, California. \"We don't understand what that looks like.\" It's a challenge that Wilson and his colleague Robert Pejchal have taken on relatively recently \u2014 though they're not the first. \"There have been many people in and out of the game for years because it has been so challenging.\" The trimers that Wilson and others want are protrusions from the surface of HIV, also called 'envelope spikes'. Each one has a tripartite structure: three gp41 proteins rise from the viral envelope, forming a stem that supports three gp120 molecules. It has long been thought that when the spike binds to key receptors on white blood cells it triggers massive structural changes in the trimer that drive the fusion of virus and cell. Trimers are the focus of intense vaccine research efforts, but vaccines based on the trimer so far do not stimulate enough of an antibody response to combat a later infection (see   Nature  454, 565\u2013569; 2008 ). \"There's something about the trimer that makes it difficult to mount an effective immune response,\" Wilson says. If they had the trimer's structure to work from, researchers hope they could devise better ways to turn the human immune system against it. The problem has been that, isolated from the virus, the trimer falls apart. Researchers can't adopt the nuclear-pore tactic here and break it apart before piecing it back together. The situation is more like that of the ribosome, in which the whole structure is more than the sum of its parts. Some regions of the trimer \u2014 those that are important for the virus's interactions with cell-surface receptors and for generating an effective immune response \u2014 are buried inside or in the interface between its parts. Researchers can't tell what they normally look like unless they see a trimer intact. The first structure of a pruned version of gp120, combined with a fragment of the white cells' CD4 receptor and an antibody, was published in 1998 6 . Other structures of gp120 and gp41 have followed 7 ,   8 ,   9 . And a recent cryo-electron-microscopy study of whole virus particles revealed some of the large-scale changes that occur when the trimer binds to the CD4 receptor 10 . But the structure of the trimer proper has remained out of reach. Wilson is making progress in collaboration with John Moore at Weill Cornell Medical College, New York, who has engineered a disulphide bond between gp41 and gp120 that helps to lock them together. They also used a strain of the virus that tends to form more stable trimers \u2014 called SOSIP trimers \u2014 in the first place. \"We have SOSIP trimers and we have crystals, but they don't diffract well,\" says Wilson. Still, Wilson and others will continue to plug away at the problem \u2014 because a vaccine remains \"one of the big challenges to science in the present day\", says Hendrickson.  \n                The invisible thread \n             Stephen White's website at the University of California, Irvine, features an exponential curve of which structural biologists are proud. In 1985, it shows, the first structure of a membrane protein was solved 11 . (That work, on the photosynthetic reaction centre, won a Nobel prize.) Now, the number of structures for proteins that span membranes deposited in the Protein Data Bank has climbed to more than 180. Some membrane proteins are refusing to join the cavalcade, however. Take, for example,  the epidermal growth factor receptor   \u2014 the target of Genentech's breast cancer drug Herceptin (trastuzumab). Despite a decade or more of intense study, only the bits protruding outside and inside the cell membrane have been crystallized. The connecting portion \u2014 the bit that spans the membrane and transmits information from one side to the other \u2014 has not. The same is true for the 60 or so other proteins in this family of receptor tyrosine kinases, which have central roles in cell proliferation, differentiation and disease. Solving this delicate stretch of protein would begin to explain how a signal outside the cell \u2014 such as a growth factor, hormone or other 'ligand' that binds to the receptor \u2014 can cause a change in protein conformation that leads to a response inside the cell. To crystallize a membrane protein, you have to ease it out of its normal milieu. Released from their membranes, though, the proteins easily lose their shape and precipitate out of solution. So detergents are used to keep them soluble, folded and active, and these can end up being a problem themselves. Unless just the right types and amounts of detergents can be found, their molecules can obstruct the interactions between proteins that allow them to line up and form crystals. The particular problem for the receptor tyrosine kinases is not their bulk or complexity but their flimsiness. The peptide chain that connects their extracellular and intracellular parts snakes through the membrane only once. The head or tail can wobble around on the single transmembrane stem and make it difficult for the protein to form ordered crystals. Persistence has paid off when it comes to another major class of membrane proteins. There was much jubilation among structural biologists when, in 2000, a group led by Masashi Miyano at the Riken Harima Institute in Japan crystallized rhodopsin, a light-activated protein purified from the cow retina (see graphic, showing the helices of the protein embedded in a membrane) 12 . It was the first structure to be resolved in the class of G-protein-coupled receptors, a family of membrane receptors with almost 1,000 members found in humans. Solving a second such receptor, which was done in 2007, was still a marathon task: one group set up 15,000 trials using a robot to optimize crystallization conditions 13 ,   14 ,   15 . These proteins have seven membrane-spanning regions, and may have been more tractable because this wider 'bridge' between head and tail stops their extremities wobbling around so much. There's a long way to go yet, however, according to Hendrickson. \"From my perspective,\" he says, \"these structures haven't answered the questions that I want to answer, which are about how the activation process happens \u2014 how these proteins do their job when activated by a ligand.\" A full-length receptor tyrosine kinase remains a dream structure, and one that many crystallographers doubt can ever be realized because there is no obvious way to stabilize the head and tail. \"I'm not certain that's going to be feasible,\" says Hendrickson. White says that membrane proteins in general, though, are getting less intimidating. \"There are a lot more people with the courage to tackle membrane protein,\" he says. Their courage will grow as ways to make better crystals and brighter X-rays come online. At the cutting edge, however, where crystallographers face the complexity of a nuclear pore or the wavering heart of a transmembrane protein, something extra is needed. \"Know the protein intimately,\" White says. \"So far, that seems to be a really important issue \u2014 to have somebody who loves the protein,\" he says. Ananyo Bhattacharya is  Nature  's deputy news editor. \n                     Protein Data Bank \n                   \n                     Stephen White's membrane protein site \n                   \n                     Protein Structure Initiative \n                   \n                     Nenad Ban \n                   \n                     Jennifer Doudna \n                   \n                     Rachelle Gaudet \n                   \n                     Wayne Hendrickson \n                   \n                     Andr\u00e9 Hoelz \u2014 G\u00fcnter Blobel laboratory \n                   \n                     Reinhard L\u00fchrmann \n                   \n                     Harry Noller \n                   \n                     Michael Rout \n                   \n                     Ian Wilson \n                   Reprints and Permissions"},
{"file_id": "459316a", "url": "https://www.nature.com/articles/459316a", "year": 2009, "authors": [{"name": "John Whitfield"}], "parsed_as_year": "2006_or_before", "body": "Like an alchemist of yore, Mike Russell is taking basic elements and trying to transform them \u2014 not into gold, but into the stirrings of life, John Whitfield reports. You could call the two linked aluminium containers in Mike Russell's lab the biological equivalent of a particle accelerator. But rather than simulating the birth of the Universe, he hopes that this apparatus will recreate the first moments of life on Earth, and give experimental support to his ideas about how geology begat biology. Alternatively, you could call it a machine for making 4-billion-year-old waste. One of the containers holds a liquid that mimics the oceans of the early Earth. The water is rich in carbon dioxide and iron, has a pH of 5.5 and is held at room temperature. The other container is heated to 130 \u00b0C, and its water is laden with hydrogen and sulphide. With a pH of 11, this second fluid is meant to stand in for the hot waters that spewed out of ocean-bottom springs early in the planet's history. The liquids mix in a chrome steel pressure barrel containing a catalyst of iron and nickel sulphide. It is here that Russell hopes to reproduce life's first steps, by reacting the carbon dioxide in the 'ocean' water with the hydrogen in the 'spring' water to make the simple organic molecules methane and acetate. Step by step, he thinks, the chemistry of life accreted around this reaction, until eventually, like caravels from the court of Henry the Navigator, the first cells carried it around the world. As a candidate for the spark of life, this reaction has a lot going for it. It releases chemical energy and can fix carbon \u2014 that is, convert carbon dioxide into organic compounds \u2014 two of life's most characteristic properties. It uses ingredients that are abundant and it fits with what we know about the early Earth. Moreover, it is still used today, albeit with a good deal more sophistication, by the microbes called methanogens and acetogens, which produce methane and acetate as waste. Russell has spent nearly three decades developing this hypothesis. Now, working at the Jet Propulsion Laboratory (JPL) in Pasadena, California, he's gearing up to test it, hoping to score a victory for the school of origin-of-life researchers known by the label 'metabolism first' in its long struggle with the more popular school called 'replicator first'. The latter holds that life began with a molecule \u2014 perhaps RNA or a simpler precursor \u2014 that was able to duplicate itself. Russell, however, thinks that the key breakthrough was the set of metabolic reactions that underpins biochemistry. The thermodynamic and chemical properties of the early Earth, he says, made these reactions a statistical inevitability. Despite the rivalry, however, there is widespread recognition that Russell brings a much-needed dose of geological reality to research into the origin of life. \"What I respect most about Mike's work is his keen insight into the early Earth's geochemical environment,\" says Robert Hazen, a geochemist and origin-of-life researcher at the Carnegie Institution for Science in Washington DC. \"The origin of life is the story of the emergence of complexity, and you can't have the emergence of complexity unless you have a complex environment. Mike, as much as anybody out there, has recognized this fact and incorporated it into his models. That's his strongest contribution.\"  \n                From aspirin to volcanoes \n              Studying the origin of life is not a great way to build a scientific career, so the people who work on the topic tend to have built up their reputations in other disciplines. But even by these standards, Russell's route to the JPL has been circuitous. When he left high school in 1958, he got a job making aspirin at a chemical plant in Ilford, a small town on London's northeast edge. But he continued to study at evening classes, and the Quaker-run company gave him days off to attend college. Five years later, having left the factory and taken a degree in geology and chemistry, he found himself on the Solomon Islands in the Pacific Ocean, as a volunteer geologist with the UK Mission to the United Nations. In his first week, his boss pointed out the office window to a smouldering volcano on a nearby island. \"He said: 'The chief has just radioed. He thinks it's going to explode.' I had no idea what to do,\" Russell recalls. Charged with deciding whether to evacuate the island's 3,000 inhabitants, Russell gave himself a crash course in volcanology, measured the ground temperature around the island's smoking volcanic vents, and decided, correctly as it turned out, that on this occasion the chief was mistaken. While he was on the Solomons, Russell worked with the Australian geologist Richard Stanton from the University of New England in New South Wales. On Stanton's advice, he became an ore geologist, going to Canada to work in mineral exploration, before moving into academia in the late 1960s. Stanton also recruited Russell to his then-unorthodox idea that mineral deposits were the legacy of ancient submarine hot springs. When such hydrothermal vents were found on the Pacific floor in 1977, Stanton was proved right. Many valuable mineral deposits are indeed the remains of ancient vent sites, showing various similarities to the 'black smokers' that pour out water heated to 400 \u00b0C and are loaded with dissolved zinc, copper, iron and other elements. By then, Russell was working at the University of Strathclyde, UK, and doing fieldwork in the Republic of Ireland at the mineral deposits in Silvermines, County Tipperary. There, he and his students found rocks riddled with small tubes of iron sulphide. They looked like miniature versions of the hydrothermal chimneys formed by minerals precipitating out of vent water.  \n                A child's discovery \n              Russell began trying to work out what sort of environment would give birth to these structures. His suggestion that the tubes were formed in vents 1  met with a cool reception, he says, because the chimneys at black smokers were much bigger \u2014 10 centimetres across, whereas those he saw in Ireland were less than 1 millimetre. Revelation came courtesy of Russell's 11-year-old son, Andrew. Russell had introduced Andrew to chemical gardens, toys in which pretty structures grow from a seed crystal added to a mineral solution. In a fit of destructiveness, however, Andrew had locked himself in the bathroom and started pulling the gardens apart. \"Suddenly he yelled out 'Hey Dad, these things are hollow',\" says Russell. \"I realized our little chimneys at Silvermines were actually chemical gardens,\" he says. That, he decided, must mean that black smokers were not the only kind of vent. There had to be cooler, gentler springs that would produce more delicate structures. More-or-less simultaneous with this thought came the idea that such a place was a good candidate for life's nursery. Some researchers had already suggested that hydrothermal vents had provided life's primordial source of energy and chemicals, but others protested that the extreme heat of a black smoker would break any large organic molecule into pieces. At the kind of vent Russell had in mind, however, the temperature would not have got much above 100 \u00b0C, which is much more amenable to organic chemistry. The final clue emerged during a visit to Yugoslavia in the mid 1980s, which revealed even greater diversity in ancient hot springs. The water in modern black smokers is acidic, thanks to dissolved sulphur compounds creating sulphuric acid. When life emerged in Hadean times some 4 billion years ago, the ocean, too, would have been acidic, thanks to the large amounts of carbon dioxide in the atmosphere that would have dissolved in the waters. But in the Dinaric Alps, Russell saw deposits of the magnesium-bearing mineral magnesite that had formed in the ancient ocean floor and precipitated out of alkaline springs 2 . At the time, however, no active alkaline vents were known, only black smokers. Between the late 1980s and the mid 90s, Russell and his colleagues stitched all their evidence into a portrait of the 'Goldilocks' spot, where the mineral chemistry was just right for what happens in cells today. \"We recognized that some types of mineralization had a lot in common with the chemical processes of life,\" says geologist Allan Hall of the University of Glasgow, UK, and one of Russell's principal collaborators at the time. Their theory of how life got going starts inside the tiny mineral chimneys. This protected environment would allow chemicals to become concentrated \u2014 a key problem facing anyone trying to explain how biochemistry can begin without cells. When these chimneys formed, they would have been gels rather than rocks, with membranes that would have allowed small molecules to pass through, much as cell membranes do. And the team found they could produce such a mineral gel in the lab 3 . The gel's membranes contained mineral sulphides of iron and nickel that would have catalysed organic reactions \u2014 just as these metal sulphides do inside modern enzymes. Across the membranes, gradients would have developed. Inside the vent, the water would have been hot, alkaline and rich in hydrogen, thanks to reactions between water and iron minerals in the crust, a process called serpentinization. Outside in the ocean, the water would have been cold and acidic. The vast majority of modern cells power much of their chemistry by creating similar gradients across their membranes. But they use a large variety of proteins to do it. Life's diverse ways to harness a ubiquitous energy source \u2014 the proton gradient \u2014 makes Russell think that the proteins are a secondary adaptation, and that life latched onto inorganic proton gradients before it could make its own. Left to their own devices, hydrogen and carbon dioxide form methane only slowly, because the reaction's initial steps, from carbon dioxide to formaldehyde, require an input of energy. In the ancient vents, the energy of the proton gradient accelerated this reaction, says Russell. He draws an analogy with geological convection, the churning currents of ductile rock that speed up the release of heat from Earth's interior to the surface. \"Metabolism is to geochemistry as convection is to geophysics,\" he says. Russell initially thought that the key reaction in the origin of life was a redox reaction involving iron, so called because the iron is said to be reduced and the hydrogen oxidized. In 1998, however, he saw a paper in  Nature   arguing that eukaryotes arose when a hydrogen-requiring archaeal cell engulfed a hydrogen-producing bacterium 4 . Piqued by the mutual interest in hydrogen redox reactions as a biological energy source, Russell explained his ideas to one of the authors, William Martin. Martin, now at Heinrich Heine University in Dusseldorf, Germany, loved Russell's ideas. \"I looked at it,\" says Martin, \"and I said 'Well, this is easy \u2014 the origin of life is basically solved.'\" Martin had one problem. He thought that, if you want to say anything about how life came to be, then modern organisms must harbour the descendent of the first biochemical reaction, and in modern organisms the key to success is reducing carbon dioxide, not iron. \"Is it reasonable to assume that what was possible for the very first cell has since been forgotten, and nobody is left who can do it?\" He steered Russell away from pursuing a hypothetical, forgotten reaction and towards what's called the Wood\u2013Ljungdahl pathway, also known as the acetyl coenzyme A (CoA) pathway after the energy-rich molecule that forms its end product in modern acetogens and methanogens. Martin favours this explanation because it is the only one of the five known biochemical pathways for fixing carbon that turns a net profit in ATP, the universal chemical fuel of life. In this scenario, relatively cool spots within the vent favour the formation of acetate, an intermediate in the pathway, whereas in hotter spots, the reaction goes all the way to methane \u2014 possibly creating the divide between acetogens and methanogens, and between the groups that eventually became the Bacteria and Archaea. Another advantage of the Wood\u2013Ljungdahl pathway is that its products and intermediates plug into other metabolic pathways. Some of those make amino acids and nucleic acids, through reactions with ammonia \u2014 which Russell is including in his simulated vent water \u2014 and phosphate, which is present in his simulated ocean. When nucleic acids and amino acids first formed on Earth, their initial job, say Russell and Martin, would have been to catalyse reactions involving carbon dioxide and hydrogen 5 . In a metabolism-first world, before genetic molecules had a decisive role in evolution, selection would have favoured not the best replicator, but the reaction that sucked in fuel the quickest, denying energy to other chemical processes. And what would become the network of cellular chemistry could have grown by adding links that increased energy consumption. Even though there were no organisms, a set of reactions would store information in its components and processes. And that network could be said to replicate by drawing in more molecules and more energy into itself, a process sometimes called chemical evolution. What's needed at this point is some evidence that will help to distinguish between the various hypotheses, says Robert Shapiro, a chemist at New York University. \"Basically, one has to provide a set-up and demonstrate self-sustaining and evolving chemical cycles.\" Thanks to NASA's astrobiology programme, which has provided increased visibility to origin-of-life research, Russell is now in a position to attempt that as a researcher at the agency's JPL.  \n                Giant chemical garden \n              Russell already has a natural model to copy. In 2000, the type of vent he had predicted \u2014 alkaline and not too hot \u2014 was discovered 6 . The Lost City hydrothermal field lies in the Atlantic Ocean, 15 kilometres from the mid-ocean ridge. The vents pump out water that has been drawn down into cracks in the ocean floor and heated to about 200 \u00b0C. As the fluids return to the cold ocean, calcium carbonate precipitates out of the water, building 60-metre towers like gargantuan chemical gardens. And last year, oceanographers reported finding abiotically produced organic compounds, including methane, in the water flowing from the vent 7 . Russell thinks that his reactor might produce amino acids and peptides, but first he wants to test whether sulphide minerals standing in for the ocean crust will dissolve in the alkaline hydrothermal solution. That would be the initial step toward the formation of the iron-sulphide chimneys that he believes provided a home for life's first metabolizing system. It's a high-risk strategy, says Hazen, because the early Earth would have had many sources of organic molecules, and many places where they could have become concentrated. He likens Russell's approach of testing just one detailed possibility to starting a game of twenty questions by asking \"is it Winston Churchill?\" rather than \"is it a man?\". It is glorious if you're right, but it doesn't narrow things down much if you're wrong. \"Mike Russell has a hunch, and there's nothing wrong with that,\" he says. \"Maybe it'll be like winning the lotto and his hunch will be right, or maybe it wasn't Winston Churchill after all, and that leaves the other 6 billion people on Earth to go through.\" A competing possibility is that one of the other four carbon-fixing pathways is a better candidate for the primordial biochemical reaction. Metabolism in acetogens and methanogens uses specialized enzymes, comments Eric Smith, a theoretical physicist and origin-of-life researcher at the Santa Fe Institute in New Mexico, and it is much more sensitive than other biological carbon-fixation pathways to changes in the isotope of carbon used. \"That says that you're using a very carefully refined enzymatic reaction to do something difficult,\" he says. Instead, he and his colleagues believe that the initial biochemical pathway was the reductive citric-acid cycle. This pathway reverses the normal respiratory cycle seen in every oxygen-using cell, and some microbes still use it to fix carbon. It builds acetate, which has two carbons, up into citrate, which has six, and then breaks the citrate into acetate and oxaloacetate. This cycling can provide positive feedback that functions like reproduction, drawing more and more carbon into itself \u2014 an advantage that the one-way Wood\u2013Ljungdahl pathway lacks.  \n                Indoor hot springs \n              Smith thinks that the metabolism-first viewpoint is making headway, especially with the focus on hydrothermal vents. \"The disagreements are really small compared with the basic orientation that we have in common,\" he says. He also thinks that the next move needs to be experimental; his colleagues are working to reproduce the reactions in the citric-acid cycle in vent-like laboratory conditions. Once someone gets their preferred pathway to work in the lab, \"everyone can quiet down and say 'this is something we can agree on.'\" Others, however, think that the whole metabolism-first theory is misconceived. The idea has two great flaws, says Steven Benner of the Foundation for Applied Molecular Evolution in Gainesville, Florida. In any system of organic reactions, some will make products other than those that might lead to life. \"Organic chemistry has an intrinsic propensity to make tar,\" he says. \"That tends to divert molecules out of any cycle.\" And any such set of reactions is unlikely to evolve greater complexity in a Darwinian fashion; instead it will just dissipate energy. What's more, he argues that acetyl CoA would not survive long at the temperature and alkalinity of Russell and Martin's favoured vent. Benner aligns himself with the other main school of thought, which says that life started with a gene-like molecule that could catalyse its own replication. He has done many studies on how RNA could have first been made, although he also acknowledges the formidable difficulty of creating an RNA molecule large enough to behave like both a gene and an enzyme in abiotic conditions. The RNA camp did get a boost last week with a study suggesting it would be chemically easier to produce this molecule than was previously believed 8 . In the end, there is no agreement on how to solve the problem of life's origin. Martin thinks that research on the topic is \"unfalsifiable conjecture\" \u2014 the best we can hope for is a convincing story. \"Even if you were to make a reactor in the laboratory, and put hydrogen and carbon dioxide and nitrogen in one end, and out pops something like  Escherichia coli   at the other end, you still couldn't prove that we and our ancestors arose that way. You'd just have a narrative that made it more plausible.\" Russell thinks that if his reactor produces just about anything from tar to  E. coli   it will have been worthwhile \u2014 he quotes Thomas Edison's remark that he did not build 1,000 failed prototype light bulbs; rather he discovered 1,000 ways that the light bulb wouldn't work (see  page 312  for one that did). Similarly, Russell hopes to help move the field forwards by sorting out what was possible and what was improbable around those warm vents, some 4 billion years ago. That's the most he can do, he says. \"It's just a step at a time.\"\n See News Feature,  page 312 . \n                     News and Views on Origins of life \n                   \n                     Steven Benner's website \n                   \n                     Robert Hazen's website \n                   \n                     William Martin's website \n                   \n                     Mike Russell's website \n                   \n                     Eric Smith's website \n                   Reprints and Permissions"},
{"file_id": "459312a", "url": "https://www.nature.com/articles/459312a", "year": 2009, "authors": [{"name": "Stefano Tonzani"}], "parsed_as_year": "2006_or_before", "body": "The incandescent light bulb is being phased out, but what will replace it? Stefano Tonzani investigates the technologies that are vying for our sockets. The Centennial Light, which hangs in a fire station in Livermore, California, is the oldest working light bulb on Earth. The four-watt night-light was switched on in 1901 and has been shining almost non-stop ever since, consuming roughly 3,500 kilowatt-hours of energy in total. As the picture below shows, the bulb also looks surprisingly familiar: the technology of incandescent lights has changed very little over its lifetime. Inside the bulb is a filament ? carbon in this case, tungsten in today's models ? that is heated by the flow of electricity until it glows white and lights up the room. The design is simple, versatile and cheap, just as it was when Thomas Edison first made it a commercial success in the 1880s. Nonetheless, that technology is now on the way out. In today's energy-hungry world, the devices are too wasteful: some 98% of the energy input ends up as heat instead of light. Halogen lamps, which look more high-tech, are not any better. Multiply that waste by the number of incandescent bulbs in residential, industrial and commercial settings ? an estimated 4 billion standard light sockets in the United States alone ? and it is clear why several countries are seeking to eliminate the bulbs entirely as a way to control carbon dioxide emissions. In 2007, for example, Australia became the first country to ban incandescent bulbs entirely; the phase-out is scheduled to be completed by 2012. The member states of the European Union agreed to a similar ban in 2008. And the United States has pledged to eliminate most incandescents by 2014. \"The lighting field is a fairly conservative one, so these government mandates are putting on some welcome pressure to evolve,\" says Karl Leo, an optoelectronics specialist and a founder of Novaled, a company in Dresden, Germany, that develops organic light-emitting diodes (OLEDs). But evolve into what? Although getting rid of incandescent bulbs makes environmental and economic sense, the race for a long-term replacement is wide open. At present, the only technology that is mature enough to take over from the conventional light bulb is fluorescent lighting, which can turn 10?15% of the input energy into light. Fluorescent technology has improved substantially since the days when it was synonymous with being harsh and funny-coloured, and has come to dominate in industrial and commercial settings, where energy efficiency and long life are prime concerns. In recent years, compact fluorescent bulbs that can be screwed into standard sockets have brought it ever farther into the home. But fluorescent lighting has a number of drawbacks. For example, fluorescent lamps do not work well in cold temperatures, and their lifespan can be significantly shortened if they are turned on and off frequently. Perhaps worst of all, each lamp contains a small amount of mercury, which is toxic. This presents consumers with a disposal problem at the end of the lamp's life. Some people also still complain about the colour rendering of fluorescent lights ? the way the lamps make objects look compared with their appearance in natural sunlight. Despite the substantial progress, domestic users in particular tend to prefer the warmer, slightly red-tinged tones of incandescent lights ? although that preference is highly individual, says Charles Hunt, an electrical engineer at the University of California, Davis. \"The subjective fondness for particular light shades depends on gender (women tend to prefer less harsh, warmer-coloured lights) and origin of the person (people from northern European countries prefer warmer lights whereas southern Europeans prefer colder, more blue-tinged ones).\" Another issue is that fluorescent lights require special circuitry to work with a dimming switch. And dimmable is desirable: \"Fifty per cent of home lights in the United States are dimmable,\" says Hunt. These problems may be overcome ? but they seem serious enough to encourage innovators to search out successor technology.  \n                Heavy investment \n              Perhaps the most widely anticipated of the technologies vying for centre stage is the light-emitting diode (LED), which consists of two types of semiconductors in contact. When a voltage is applied, positive charges coming from one side flow towards the junction and meet negative charges coming from the other side. As these charges combine, they release their energy in the form of light, usually as one particular colour. LEDs are long-lived, robust and roughly twice as efficient as fluorescents. Indeed, they are already widely used for computers, television sets and other consumer electronics, and are becoming a market leader for outdoor applications such as traffic lights and indicator lights on cars. \"There are so many advantages to LEDs that we think there lies the future of lighting,\" says Hans van Sprang, a senior scientist at Philips Research Laboratories in Eindhoven, the Netherlands. Philips and other big industry players are investing heavily in the technology, supporting materials-science research that has helped LED technology to evolve rapidly. Despite this, LEDs have not yet been adopted on a large scale for general lighting applications. One problem is that an LED light powerful enough for room lighting has a very high initial cost compared with an equivalent incandescent bulb. This can be a large psychological barrier for consumers, even though the cost of energy and maintenance is considerably lower. Another problem is that an LED's lifetime can reduce dramatically if it is operated at a high temperature. This makes heat dissipation an important issue, especially for powerful lamps, and complicates efforts to reduce costs. Making the LED semiconductors from substrates other than sapphire would be cheaper, and the alternatives, including silicon-based substrates, might improve heat management. Another challenge is how to generate white light from LEDs. The preferred technique for commercially available devices is to coat a blue or ultraviolet LED with a phosphorescent material that will absorb the monochromatic emissions, and then re-emit the energy as a broad-spectrum white light. Another, potentially more energy-efficient, way of generating white light is to mix the light of red, blue and green LEDs. But both of these methods have issues with colour rendering. And the latter method has the added problem that the lifespans of the three different LED types are not the same, so the light will change in colour as the lamp ages. Gain in one dimension and you can lose in another ? devices that have very good colour rendering tend to have poor energy efficiency. One potential solution is now under development by Sandra Rosenthal, a chemist at Vanderbilt University in Nashville, Tennessee. Her idea is to use an ultraviolet-emitting LED to energize the electrons in cadmium selenide nanocrystals, which respond by re-emitting a white light with very good colour rendering. \"This could be a viable alternative if we could substantially improve their efficiency,\" says Rosenthal. Organic compounds, which have already been looked at as a possible alternative to silicon in solar cells, are being investigated for use in LEDs. OLEDs produce light in much the same way that ordinary LEDs do, except that the positive and negative charges originate in organic compounds rather than in crystalline semiconductors. Typically, these organic compounds are attached to a fixed polymer sheet. The advantage of organic materials is that, at least in theory, they can be produced comparatively cheaply with the same roll-to-roll technology used to handle other types of plastic films. The main problem with OLEDs is that the organic materials are degradable by water and oxygen, which tends to give the devices a short lifespan. This can be solved, to some extent, by encapsulating the organic compounds in an inert, transparent polymer such as epoxy resin. But the compounds degrade intrinsically anyway, especially the blue OLEDs that are required for mixing with red and green to generate white light.  \n                The outsiders \n              Farther out of the mainstream ? in the sense that the technologies are being developed by smaller start-up companies ? are induction lamps and cathodoluminescence. Induction lamps, also known as electrodeless lamps, have been around since the 1890s, when Nikola Tesla invented a fluorescent light powered by currents oscillating in a coil of wires on the outside of the tube, rather than by electrodes on the inside. But some people think that these lamps are finally ready for prime time. The newest devices feature an electrodeless bulb that is filled with argon gas plus a small amount of metal halide salts. A microwave generator, much like the ones in microwave ovens, produces a wave that is channelled through a waveguide and concentrated on to the container, where it ionizes the gas to form a plasma and vaporizes the salts. The plasma and vapour together generate a broad-spectrum white light with an efficiency similar to that of LEDs. The devices are also very bright, which means they are likely to find their initial applications where intense light is required, such as in car headlights or industrial illumination. \"It will take LEDs a long, long time to catch up with the intensity of illumination possible with induction lamps,\" says Robin Devonshire, chief scientist of Ceravision, a company in Milton Keynes, UK, that is one of several developing this technology. As these lamps do not place electrodes in contact with the harsh plasma environment, they can potentially last for decades. Cathodoluminescence works like the cathode-ray tubes found in old-fashioned television sets. It uses a source of electrons to bombard a phosphorescent material coated on the inside of a glass bulb, causing the material to emit light. An electrical field, high temperature or photoelectric effect is used to make a metal surface emit the electrons. Such a light source can be quite efficient, comparable to compact fluorescent sources. It renders colours well, and the lamps can be shaped to look like incandescent bulbs. Initial applications will be geared towards home usage. However, both induction lamps and cathodoluminescence lamps have a perception problem. \"There is scepticism in industry with respect to these technologies that are not solid state,\" says Bruce Pelton, director of engineering of the University of California, Davis, California Lighting Technology Center. This is partly because solid-state devices such as LEDs ? which generate light through processes in solid material, having no moving parts or bulbs that can break ? are thought to have major advantages in the long run when it comes to ruggedness and long life. But it is also because a previous incarnation of the induction lamp, based on sulphur, failed to gain a foothold in the market. The sulphur lamp was efficient and bright, but was large and required air cooling to stop parts melting in the high temperatures reached by the sulphur plasma.  \n                Blazing competition \n              The competition to replace incandescent light bulbs is likely to be fierce. But consumer acceptance is far from guaranteed for any of the rival technologies. One problem is the confusion generated by the sheer number of alternatives. Another is that each of these devices has several parts, so that the lifespans and energy efficiencies reported for basic technology do not correspond to those of the whole device, which are, as yet, not that far ahead of incandescent bulbs. In LED lamps, for example, the electronics or the phosphors are likely to degrade much earlier than the solid-state device itself. US Department of Energy data published in 2008 found that commercially available LEDs were about half as efficient as compact fluorescent lights. And, although development since then has been fast, the problems remain. Meanwhile, because widespread acceptance of these technologies is crucial to changing the habits of consumers ? and ultimately, to saving substantial amounts of energy ? governments are keen to avoid the errors made with previous technologies, such as the early fluorescent lamps, which many end-users hated. If a technology is initially viewed negatively by the public, this can mar its subsequent evolution ? a good reason not to mandate a technology before it is market-tested and ready. Comments on websites mentioning the planned phase-outs of incandescent lights have highlighted that many people's opinions of fluorescent lights have not changed much over time. Cost, colour rendering, flicker and the presence of mercury are only a few of the issues mentioned. Fluorescent lights are still a small percentage of the market compared with more energy-intensive lamps. For all these reasons, the general-purpose incandescent light bulb might not be replaced by a single new source, but by a range of technologies, each suited to a particular use. For example, if OLED lighting can economically be produced in continuous sheets by industrial roll-to-roll techniques, it will be a natural candidate for flat panels that generate a diffuse glow for area lighting. That would make OLEDs a natural complement to the bright, directional light coming from semiconductor LEDs, which could instead be used for more light-intensive tasks such as reading. Such combinations could lead to new concepts of lighting design, so that architects could help save energy by not wasting light where it is not needed.\n   Stefano Tonzani is an associate editor at    Nature . \n                     Nature Materials \n                   \n                     Nature Photonics \n                   \n                     Sandra Rosenthal website \n                   \n                     DoE solid state lighting \n                   \n                     UK government report on lighting \n                   Reprints and Permissions"},
{"file_id": "459504a", "url": "https://www.nature.com/articles/459504a", "year": 2009, "authors": [{"name": "Geoff Brumfiel"}], "parsed_as_year": "2006_or_before", "body": "Before they were touted as invisibility cloaks, metamaterials promised a perfect lens. Geoff Brumfiel reports on the struggle for superior vision. As far as John Pendry is concerned, we are as good as blind. Sitting in his office at Imperial College London, the theoretical physicist gestures at the table in front of him. Assume, he says, that the fundamental limit of detail in the table is about the size of an atom. Then consider that the smallest feature the human eye can see, even using a high-quality optical microscope, is a few tenths of a micrometre across \u2014 roughly a thousand times bigger than an atom. That means that, even with 20\u201320 vision and the best optics, humans can access only 0.0001% of the information right before their eyes. \"That's not much,\" he observes. Nearly a decade ago, Pendry proposed a way to do thousands of times better using a film of silver just tens of nanometres thick as a 'superlens' 1 . The film would behave as a simple metamaterial, a substance with a small-scale structure that allows it to manipulate light in a way that no bulk composition could. This means that the film could capture details that elude conventional optical microscopes. In theory, the superlens could also etch nanometre-scale patterns on to a surface, which would make it very useful in the manufacture of microchips. Pendry's idea brought many researchers into the field, and for a while expectations were high. But nine years on, those expectations have yet to be fully realized. Although there have been proof-of-concept devices for both the superlens and the related 'hyperlens', industry has switched its focus elsewhere. And the field of metamaterials has itself been diverted by another high-profile potential application: cloaking devices. Making objects 'invisible' may have stolen the limelight, but one of the researchers behind that work believes that superlenses remain the most alluring idea yet proposed for metamaterials. \"I think that the superlens will probably find more applications [than cloaking],\" says Xiang Zhang, based at the University of California, Berkeley, who grabbed headlines in April for using metamaterials to create a primitive cloak 2 . The key to all this comes down to the structures of metamaterials, which directly affect their optical properties. The composition of conventional substances, such as glass, is uniform at optical wavelengths, but metamaterials feature regularly patterned structures at those scales. When light of a certain wavelength interacts within the pattern, it sets up a resonance, similar to the way a musical tone triggers vibrations in a tuning fork. The resonance can cause light beams to be deflected in the 'wrong' direction and it can also enhance certain properties. Pendry's initial work on metamaterials was aimed at beating a problem known as the diffraction limit, which says that optical instruments such as telescopes and microscopes are subject to a fundamental fuzziness. No matter how well they are made, their ability to resolve fine detail is constrained by the wavelength of the light being used. For optical microscopes, this means that any attempt to resolve features below around 200 nanometres will fail. Pendry recognized a loophole: the diffraction limit does not necessarily hold for metamaterials. Although silver is not a perfect metamaterial, he showed that it would work well enough as a lens. If a silver film were placed close enough to an illuminated object, it could catch the 'evanescent waves' \u2014 short-range electromagnetic fields that carry the detailed, subdiffraction-scale information about the object, but that cannot be picked up beyond a few tens of nanometres from the object's surface. These evanescent waves would be amplified by resonances in the silver film, and their information would be carried through the film to create an exquisitely detailed image on the far side.  \n                Window of opportunity \n              Richard Blaikie, an electrical engineer at the University of Canterbury in Christchurch, New Zealand, was excited by this concept \u2014 not as a way to see, but as a way to write. He thought that the superlens could help chip-makers in their struggle to create subdiffraction-scale circuit elements. But, as Blaikie soon learned, what looked elegant on paper was messy in the lab. \"It's very easy to do some modelling,\" he says, \"but the experiments are very hard.\" Blaikie did manage to make a prototype superlens in 2005 that captured subdiffraction information 3 . But because the evanescent waves fade away over such a short distance, the silver film had to be fixed to the object being imaged. Similarly, whatever recorded the image had to be clamped tightly to the other side of the film. And even then, says Blaikie, \"the images that we got, although good in terms of their resolution, were poor in terms of their fidelity\". Even a few nanometres of variation in the silver's surface created hotspots that made straight lines appear jagged. Blaikie says his group has struggled to follow up that 2005 demonstration. \"You haven't seen anything because we haven't had any success,\" he says bluntly.  \n                Hyper activity \n             In 2006, partly in reaction to the practical limitations of superlenses, two theoretical groups independently developed the hyperlens 4 , 5 . Instead of a single film, the hyperlens features alternating layers of a metal and an insulator. Experimentally, the layers are arranged to form a half-cylinder (see diagram). The object is placed in the centre of the cylinder and evanescent waves from it are caught by the lens and magnified as they pass through the various layers, emerging as light that shines freely. The hyperlens is more complex than the superlens, but has a major advantage in that it should be able to feed the light emerging from its surface into conventional optics. \"I'm hopeful that this hyperlens could be a revolutionary instrument,\" says Nader Engheta, a theorist at the University of Pennsylvania in Philadelphia and one of the originators of the idea. Again, experimentalists rushed into the lab to build hyperlenses, and again they met with initial success. Less than a year after the idea was proposed, two groups had made proof-of-concept hyperlenses that beat the resolution of conventional optics. But, as with superlenses, the devil is in the detail. \"The principle is working; on the other hand, you are limited by engineering problems,\" says Igor Smolyaninov of the University of Maryland in College Park, who built one of the demonstration hyperlenses. Smolyaninov's lens, for example, works in only one dimension. A two-dimensional version would be more complex to manufacture, he says. And then there is the problem of loss: imperfections at the interfaces between each metal\u2013insulator layer can cause light to scatter randomly. Progress on the perfect lens has now slowed considerably \u2014 although this is partly a result of the excitement over cloaking devices. Pendry was again the source for the idea when, in 2006, he showed that metamaterials could potentially bend light around an object, effectively shielding it from view 6 . Many researchers are now working on developing the metamaterial nanostructures required for the job. As a result, some scientists believe industry needs to start investing in super- and hyperlenses to give that technology a boost. But for both imaging and circuit etching, the lenses would be entering markets in which competing subdiffraction technologies are already further along in development. Semiconductor researchers can use nonlinear optics to write circuits below the diffraction limit. And for biological imaging, other groups are developing ways to beat the diffraction limit using a combination of fluorescent proteins and clever optics. Biologists have shown only tepid interest in superlenses, admits Nicholas Fang, a researcher at the University of Illinois at Urbana-Champaign. Still, Fang and many others remain fiercely devoted to the superlens idea. If the lenses can be made to work, they would be able to image living biological specimens with unprecedented detail. Granted, he says, \"there are many open issues that we did not realize at the very beginning\". But he points out that this is hardly unusual with new technologies. Superlenses do work in theory, he contends, and many of the problems are \"more of an engineering issue\". For his part, he believes that the field may have to draw a little more from industrial expertise. For example, he has used industrial-grade germanium to deposit atomically flat films of silver 7  that can reduce the distortion seen by Blaikie. Other groups are working to make hyperlenses more practical and, again, techniques from the semiconductor industry seem to be helping. In April, Stefan Mendach and his colleagues at the University of Hamburg, Germany, created a hyperlens by rolling up alternating layers of semiconductor materials 8 . The technique seems to be an easy way to make a hyperlens, although Mendach needs to get more layers in his system before it can work. And Vladimir Shalaev at Purdue University in West Lafayette, Indiana, is trying to build 'flat' hyperlenses. Although they are harder to make \u2014 the individual layers can no longer be uniform \u2014 they could prove easier to use, as the specimen would not have to be precisely positioned along the cylinder axis. Zhang, meanwhile, is pursuing a simpler but related concept that he believes could revolutionize data storage. Rather than using a single superlens, he is using several 'plasmonic' lenses, which focus evanescent waves. When normal light strikes the lens it interacts with electrons on the lens surface. This concentrates evanescent waves into a single point, effectively creating a subdiffraction hotspot that could be used to write a piece of information on to an optical disk. Zhang has developed a prototype array of plasmonic lenses that does indeed write information far below the diffraction limit. Back at his desk, Pendry is willing to wait for his revolution with a patience that is perhaps unique to theoretical physicists. Before he proposed superlenses, he spent years calculating the quantum mechanical forces arising between two blocks of glass flying past each other at near the speed of light, simply because the problem was theoretically interesting. Compared with that earlier work, realizing a perfect lens seems eminently practical. \"It's really a very, very simple technology,\" he says. \"It's just a question of being able to get it right.\"\n \n                     John Pendry's homepage \n                   \n                     Xiang Zhang's homepage \n                   Reprints and Permissions"},
{"file_id": "459500a", "url": "https://www.nature.com/articles/459500a", "year": 2009, "authors": [{"name": "Katharine Sanderson"}], "parsed_as_year": "2006_or_before", "body": "Could hydrogen sulphide be the new nitric oxide? Katharine Sanderson reports on the rotten-egg gas that is earning a reputation in human physiology. When Rui Wang saw the painted Easter eggs his ten-year old daughter had brought home from school in 1998, he was unaware of the stink they were about to create. The eggs were proudly displayed as objects of art in the family's glass cabinet. Then one day, says Wang, ?I came home and the whole house was filled with a stinky smell?. One of the eggs had fractured, and the distinctive whiff of hydrogen sulphide filled the air. What might be garbage for one man was inspiration for Wang, then a cardiovascular researcher at the University of Saskatchewan in Saskatoon, Canada. He was already studying nitric oxide and carbon monoxide, gases produced in tiny quantities by the body that have potent physiological effects. The stench emanating from the cracked egg coincided with Wang's desire to seek more gases that are important in human biology. ?Hydrogen sulphide jumped out from both my home and my head,? says Wang, now working at Lakehead University in Thunder Bay, Ontario. He quickly bought a canister of the gas and applied it to his cultures of rat vascular tissue. The work culminated in the discovery that hydrogen sulphide is made by the tissue, and can lower blood pressure 1 . Since then, many researchers have turned their attention to hydrogen sulphide and its role in human physiology. In the past year alone, Wang and others have demonstrated how the body manufactures the gas, along with a plausible way that it could be modifying a whole range of metabolic proteins. On the clinical side, the gas has been implicated in inflammation and shown to protect the heart. And next month will see the first conference dedicated entirely to hydrogen sulphide, organized by Wang and held in Shanghai. Some researchers now think that hydrogen sulphide is poised to have as big an impact on biology as nitric oxide did ? and that impact was very large indeed. The discovery of nitric oxide's role as a signalling molecule in the cardiovascular system won three scientists the Nobel Prize in Physiology or Medicine in 1998, opened up a huge new field of physiology and, as it turned out, helped to boost the profits of pharmaceutical companies: the erectile dysfunction drug Viagra (sildenafil) is now known to work by enhancing the release of nitric oxide. Salvador Moncada at the Wellcome Research Laboratories in Beckenham, UK, started nitric oxide's rise to fame when he showed 2  in 1987 that it accounted for the activity of a sought-after and mysterious agent known as endothelium-derived relaxing factor (EDRF), which relaxes blood vessels. The idea that cells could be making a gas in the body ? particularly one known mainly as an air pollutant ? was ?a very strange concept?, says pharmacologist Phil Moore of Kings College, London. But once it sank in, the concept generated a tremendous thrill. Moore recalls being asked by university librarians to stop lecturing about Moncada's discovery because voracious students were razoring the paper out of library copies of  Nature . Nitric oxide has since been established as a key signalling molecule that is involved in processes ranging from neurotransmission to immune-system regulation. And in the mid 1990s it became apparent that it wasn't the only 'gasotransmitter', when researchers showed that carbon monoxide also acted in cell signalling.  \n                Pungent potential \n              Hydrogen sulphide wasn't an obvious candidate to join the list of gasotransmitters, being known mainly for its smell and toxicity (a dose of more than 700 parts per million can kill a human). Wang's discovery that hydrogen sulphide could have a beneficial role in blood vessels was a ?second coming?, says Moore, who immediately started working on the gas. Moore and others say that the human body's ability to use hydrogen sulphide could be an evolutionary legacy from microbes that used it as a source of nutrients: ?Somewhere in us the hydrogen sulphide is still knocking about.? Knocking about isn't the same as doing something important, and Wang knew that he needed to show the mechanism by which the gas is made to strengthen the case that it is vital in mammalian physiology. In 2004, he contacted neuroscientist Solomon Snyder at Johns Hopkins University in Baltimore, Maryland. Back in 1990, Snyder had discovered an enzyme responsible for producing nitric oxide 3 , and Wang wondered whether Snyder could help him do the same for hydrogen sulphide. Wang arrived in Snyder's lab already armed with mice that he had engineered to remove the enzyme cystathionine-\u03b3-lyase (CSE). This enzyme is involved in hydrogen sulphide production in bacteria, and Wang hoped that the mutant mice would show it could do the same in mammals. They did. In October last year, Wang and Snyder reported in  Science   that their knockout mice had low concentrations of the gas in the heart and suffered from hypertension 4 . The paper catapulted hydrogen sulphide into the scientific limelight, says Matt Whiteman from the Peninsula Medical School in Exeter, UK, who is looking at hydrogen sulphide concentrations in patients with arthritis and diabetes: ?It has given the field more credibility.? Snyder says that nitric oxide can no longer be viewed as the only EDRF. The Nobel winners, he says, ?showed that it was an EDRF, they didn't show it was the only EDRF?. Snyder has evidence that hydrogen sulphide accounts for more of the vessel-relaxing activity of EDRF than nitric oxide does. The two gases seem to act in very different ways though. Nitric oxide activates an enzyme called guanylyl cyclase, initiating a chain of events that relaxes the muscles in blood-vessel walls. Hydrogen sulphide reaches the same end by activating ATP-sensitive potassium channels 1 . The work on CSE-knockout mice and subsequent unpublished findings are enough to convince Snyder that ?hydrogen sulphide is potentially more exciting?, than nitric oxide. Others in the field remain cautious about the risk of overstating its biological status. ?It would be like when The Beatles said they were bigger than Jesus,? says Whiteman. Snyder's unpublished work could explain his faith: he thinks he has found an important mechanism by which hydrogen sulphide acts. Snyder drew on parallels with nitric oxide to work this out. Nitric oxide often acts through a process called nitrosylation, whereby a part of the nitric oxide molecule attaches itself to a specific amino acid in a protein, often inhibiting the protein's activity. That amino acid is a sulphur-containing one, called cysteine. Snyder says that hydrogen sulphide is working by a parallel process he calls sulphydration, and that this could be an entirely new way to modify proteins and influence their activity. Sulphydration works by furnishing cysteine with an additional sulphur atom. The atom reacts with a protruding, reactive part of the cysteine that contains a sulphur?hydrogen (S?H) bond, converting it into a sulphur?sulphur?hydrogen bond that pokes even further out from the body of the protein. ?It is more exposed,? explains Snyder, making it accessible for further chemical reactions. The consequences of sulphydration are more profound than those of nitrosylation, Snyder says. His tests so far have shown that nitrosylation affects roughly 1 in every 100 protein molecules in a given sample, whereas sulphydration seems to affect one in every 10?20 proteins. This could be because hydrogen sulphide is less selective than nitric oxide about which cysteines in a protein it reacts with. Snyder says he has good evidence that sulphydration opens the potassium channels that lead to blood vessel relaxation, and that it alters the activity of at least 40 proteins in the liver. One protein he has examined closely is the enzyme glyceraldehyde-3-phosphate dehydrogenase (GAPDH) which is involved in glycolysis, the process by which glucose is broken down to supply fuel to cells. When GAPDH is nitrosylated, its ability to catalyse the reaction is lowered; with hydrogen sulphide the picture is dramatically reversed. ?If you sulphydrate GAPDH you increase its activity 700%  in vivo ,? Snyder says. ?The bottom line is sulphydration is a new mode of post-translational modification of proteins, probably comparable in prevalence and influence to phosphorylation,? he says, referring to another widespread protein modification that is vital for all manner of cell functions. Snyder says that sulphydration could be important to explain the control of many metabolic processes besides glycolysis. ?The regulation of metabolic pathways was largely worked out 50 years ago. The question is what turns them on and off? How do hormones and other signals regulate metabolism? That hasn't been adequately addressed.  \n                Wait and see \n              So far, Snyder's work has been published only in outline 5 , and researchers say that they are waiting to see it in full before judging it for themselves. Whiteman agrees that Snyder's hypothesis is plausible: ?What he's seeing makes sense with my observations as well,? he says. Sulphydration ?would be a major breakthrough,? Wang adds. But whereas Snyder puts hydrogen sulphide centre stage in cellular signalling, others in the field think that the gas could be the body's second fiddle ? there to offer support when something goes wrong with nitric oxide. ?It is surprising that there would be any system like nitric oxide without a backup,? says Giuseppe Cirino from the University of Naples Federico II, Italy. ?We are talking about a system that is important for survival.? When it comes to controlling vasculature, for example, Cirino has found that hydrogen sulphide becomes particularly important in the body if the endothelium of blood vessels ? the thin lining that regulates blood flow and produces nitric oxide ? is damaged so that nitric oxide production drops 6 . He and his collaborators studied men's corpus cavernosum, the penile tissue that fills with blood during an erection when stimulation causes the nervous system to release nitric oxide and relax the blood vessels. (Viagra mimics this by releasing nitric oxide into the corpus cavernosum.) Cirino and his colleagues took strips of smooth muscle from this tissue that lacked blood-vessel endothelium and hence weren't making nitric oxide. ?Hydrogen sulphide can relax tissue in the absence of the endothelium,? says Cirino. ?The next question is why, and the question after that is how.?  \n                Delivery details \n              The why and how would be easier to answer if researchers had a way to control the delivery of hydrogen sulphide to tissues, and Moore has developed a molecule able to do so. It is an organic, water-soluble molecule that slowly releases hydrogen sulphide in a way that can generate physiological concentrations better than a direct shot of the gas itself. Moore has applied for a patent on this molecule, which he hopes to use to study hydrogen sulphide's effects on the cardiovascular system, and perhaps to deliver it therapeutically 7 . Other groups are also seeking ways to use hydrogen sulphide in the clinic. ?We're most excited by the idea that low levels of hydrogen sulphide therapies have very, very significant effects in reducing cell damage,? says cardiac researcher David Lefer at Emory University School of Medicine in Atlanta, Georgia. In work that has yet to be published, Lefer has used single shots of hydrogen sulphide to try to shield mice from the effects of a simulated heart attack. The gas seems to promote the production of powerful antioxidants and enzymes that protect against cell damage, he explains. ?It looks like there's a programme of protection we can develop with a single exposure.? Lefer says the work might lead to a therapy that could be taken by patients before major heart surgery. Whiteman has been examining the role of the gas in inflammation. He looked at the synovial fluid from the joints of patients with rheumatoid arthritis ? a chronic inflammatory disease ? as well as osteoarthritis, a degenerative disease of cartilage. Those with rheumatoid arthritis had high concentrations of hydrogen sulphide in their synovial fluid, but those with osteoarthritis did not. Wang, though, says that researchers remain divided about the molecule's importance in the inflammatory process. ?Some have shown pro-inflammation, some have shown anti-inflammation.? There is another intriguing side to hydrogen sulphide. In 2005, Mark Roth at the Fred Hutchinson Cancer Research Center in Seattle, Washington, showed that hydrogen sulphide could put mice into a state of suspended animation. Their metabolism slowed drastically, and this is how Roth explained the deeper-than-deep sleep they were in 8 . Snyder, however, says that this experiment has little bearing on what he has been observing. ?The suspended-animation experiments used doses hundreds of times greater than normally exist and so likely involved mechanisms unrelated to the normal hydrogen sulphide physiology,? he says. ?It's very different to a tiny amount formed by an enzyme.? At the Shanghai meeting next month, which some 300 participants are expected to attend, one of the major topics for discussion will be how to translate results in animals into humans ? and, perhaps, who will capitalize first on the gas's therapeutic effects. ?I think that there are drugs to be had here,? says Moore. He and Wang are collaborating with a company called CTG Pharma in Milan, Italy, where researchers are trying to insert hydrogen-sulphide-donating groups into known drugs, including sildenafil and the painkillers aspirin and diclofenac. Researchers at the biotechnology firm Ikaria in Seattle, Washington, are developing therapies based on all three known gasotransmitters. They are testing in phase I trials whether injections of sodium sulphide, an agent that is used to deliver hydrogen sulphide to the body, can treat conditions such as blocked arteries. As such studies move forwards, researchers hope to learn much more about hydrogen sulphide ? and more about gasotransmitters as a whole. Moore and Wang think that the human body may have the ability to use other simple gases that microbes use, and one example that Moore mentions is ammonia ? another gas known for its pungent, rather than physiological, properties. This mean that Wang may be able to follow his nose to the next gasotransmitter, just as he did to the last. ?If we learn anything from nitric oxide, carbon monoxide and hydrogen sulphide that would be these three gases are not alone,? he says. ?There must be more.? This is a test to see if these entitys work \u03b3 \u03b1 \u03b2 \u03b4 \u00eb \u03ba \u00f3 \u03a0 and the old style \u03b1 and \u03b2 Katharine Sanderson is a reporter for  Nature   in London. Reprints and Permissions"},
{"file_id": "458700a", "url": "https://www.nature.com/articles/458700a", "year": 2009, "authors": [{"name": "Alison Abbott"}], "parsed_as_year": "2006_or_before", "body": "While researchers in Greece starve for government support, biomedicine is thriving at a lavish new centre in Athens, finds Alison Abbott. Athens' newest research institute, the Bioacademy, is a grandiose structure of exquisite marble and sandstone that blends modern and classical styles. Its colonnaded wings enclose a triangular courtyard, which features a shimmering pool that mirrors the activity of the 400 scientists within. Athena, goddess of wisdom, would probably feel as much at home here today as she did in the nearby Parthenon, built to honour her in the fifth century BC. Elsewhere in Greece, though, Athena might feel a little lost. The country has one of the lowest levels of national research funding in the European Union (EU) and the government has not held a competition for grants in five years. That contrast makes the Bioacademy all the more remarkable. Established by the Biomedical Research Foundation of the Academy of Athens, the Bioacademy opened for business in 2004, and has already recruited 50 group leaders and around 350 postdocs and doctoral students. Together, they have published more than 100 papers in strong journals. The Bioacademy coordinates two major EU biology infrastructure projects and has brought in more than \u20ac16 million (US$21 million) in competitive grant money, mostly from international sources. When a planned new wing is completed, it will be one of the largest centres for translational medicine in Europe. The entire credit for the Bioacademy's existence is attributed to one man: Gregorios Skalkeas, an 82-year-old surgeon who founded the first organ-transplant centre in Greece. Through political skill and force of personality, he made the \u20ac40-million building a reality. The Bioacademy's rapid scientific success, says Achilleas Mitsos, former head of the European Commission's research directorate, illustrates \"the contradictions in the Greek scientific landscape, where the level of science is generally low but you'll find extraordinary pockets of excellence everywhere\". \n               boxed-text \n             Greece is a relatively poor country where science is not a priority. In 2002, when the EU voiced its intention to raise its average research spending to 3% of its gross domestic product (GDP) within a decade, Greece was the lowest spender on research of the then 15 member states. It is the only one of those states to have actually reduced investment since that time \u2014 from 0.64% of GDP then to 0.57% in 2006. Many of the 12 states that have since joined the EU, mostly from the poor eastern bloc, spend much more. The Czech Republic, for example, devotes 1.54% of its GDP to science (see  chart ). As a result, Greek researchers depend on EU funding more than scientists elsewhere do. Without a national research council that gives out grants, small pots of money for competitive research tended to pop out of various ministries at irregular intervals, but now even these opportunities seem to have dried up.  \n                Budget woes \n              Scientists get by mostly on grants from the EU's Framework research programmes, in which Greek investigators do exceptionally well. Until 2006, Greece received the highest proportion of Framework money per researcher than any other EU country. Now it is second to Slovenia. Its relegation could be because Greece does less well in the large integrated projects that the Framework programmes now favour. \"Or it may be in part because of the running down of science in Greece,\" says George Thireos, who until last month headed another of Greece's top research institutions \u2014 the Institute of Molecular Biology & Biotechnology (IMBB) in Heraklion, Crete. EU dependence doesn't come for free \u2014 national governments are expected to match what the EU provides through its Framework and structural funds. But the Greek government says it will be able to give at most a 14% top-up this year, leaving institutes in the middle of multi-year projects with scorching debts. For example, the IMBB recently won EU money to develop a \u20ac1-million proteomics suite. The grant covered three-fifths of the costs, but then, halfway through the acquisition, the government contribution shrank. \"I don't know where we are going to find the money,\" says Thireos. \"This is a very serious problem for Greek scientists.\" Thireos is now transferring to the Bioacademy \u2014 which has proved an irresistible magnet for many \u2014 to head up a new systems-biology centre. He says that Skalkeas \"has realized a true vision \u2014 the Bioacademy will allow Greece to attract back good scientists working abroad\". The Bioacademy has already lured back many researchers, including two from Columbia University in New York \u2014 molecular biologist Dimitris Thanos, who heads the centre of basic research, and Argiris Efstratiadis who will direct a planned cancer centre. A man of uncommon political influence, Skalkeas has always thought big. He ventured into experimental and translational medicine decades before it became fashionable. In the 1960s, he set up a laboratory for experimental surgery at the University of Athens, which has since trained more than 200 research students. \"But even back then I thought that if one day I ever had the opportunity, I'd like to do something bigger for medical science,\" he says. That opportunity came with his election to the prestigious Academy of Athens in 1989. Within two years he had created the Biomedical Research Foundation under the academy's umbrella, and set about lobbying politicians to cough up money for it in order to build an internationally competitive medical research centre \u2014 the Bioacademy. \"I'm old and honest, and I have an ability to persuade,\" says Skalkeas. And his regal charm \u2014 he is one of those gentlemen who will kiss, rather than shake, the hands of ladies \u2014 no doubt also helped his quest to draw blood from what seemed to be stone. Lots of blood, in fact. The Bioacademy's 25,000-square-metre main building has state-of-the-art equipment, for applications ranging from brain imaging to simulation surgery. It is not only well appointed but also, many would argue, a model of how a modern research institution should be run. In a country where most scientists are hired as civil servants with jobs for life, only 14 of its 50 principal investigators are tenured. The rest are on tenure-track contracts. Next year, construction will begin on a 20,000-square-metre additional wing for the Bioacademy, which will house a further 300 scientists and cost another \u20ac25 million in government funds. When completed, the Bioacademy will be Greece's biggest-ever injection of research money. Is the Bioacademy, with its \u20ac14-million annual running costs, sustainable in such a research-parched general environment? Many believe so. Its unusual status as a part of the powerful Academy of Athens gives it considerable political protection. The national economy, though, is now falling through the floor.  \n                Pockets of success \n              The Bioacademy has something in common with the relatively few other success stories in Greek science, says Fotis Kafatos, a molecular biologist from Imperial College London who heads the European Research Council. \"There are a few institutes that have done well because they were established by strong people.\" But those few pockets of success in Greek science do not have a multiplier effect, says Mitsos, who is now an economics professor at the University of the Aegean. Apart from some very local connections, centres such as the Bioacademy and the IMBB are isolated from most universities and do not catalyse excellence within them. \"The problems start at the top,\" he says. \"No government has ever had any real interest in research \u2014 there are no institutions like a research council, and no serious science planning. Four years ago, the conservative government of Kostas Karamanlis launched an attempt to create an efficient national science system. What particularly excited scientists was the government's intention to establish an agency like the US National Science Foundation, which would be able \u2014 finally \u2014 to offer competitive grants on a regular basis. Filippos Tsalidis, head of the development ministry office for research and technology, consulted extensively with the academic community, and a law was passed last year. But the legislation turned out to be unworkable. It required a large number of administrative actions in a short time \u2014 something the government was not able to deliver. Tsalidis has unofficially told the directors of research institutes that the law needed some rewriting and that he had formed a committee to suggest changes. \"Amateurish,\" says Mitsos scornfully. The scientific community is bewildered and disappointed. \"We are in the dark,\" says George Kollias, director of the Alexander Fleming Biomedical Sciences Research Center in Athens. Tsalidis declined to comment to  Nature . A few positive things have happened in Greek research policy of late, though. In 1995, the government's office of science and technology started to evaluate its research institutes every five years, using foreign reviewers. Since 2000, these evaluations have been linked to distribution of EU structural funds for research. And in 2005, the government passed a law requiring that universities be similarly evaluated. Although professors and students have put up enormous resistance, the evaluations began last year. Also, Greece joined the European Space Agency (ESA) in 2005. Although the nation's small community of astronomers and astrophysicists is still finding its feet among the big players in European space science, ESA membership has raised their aspirations. Greece processes plenty of PhD students \u2014 more than 21,000 were registered in 2006\u201307 \u2014 but many go abroad when they graduate and then find it hard to justify coming back. The Bioacademy will help to stop that leakage of talent. But scientists in Greece say that the government must also create a systematic, balanced and appropriately funded national plan for science so that fewer researchers will be lost. They know that Athena would be happy to see the spirit of the great ancient Greek civilization rekindled. \n                     The Bioacademy \n                   Reprints and Permissions"},
{"file_id": "458826a", "url": "https://www.nature.com/articles/458826a", "year": 2009, "authors": [{"name": "David Cyranoski"}], "parsed_as_year": "2006_or_before", "body": "Could genes explain the remarkable rate of identical twins born in some remote villages around the world? David Cyranoski investigates a long-standing biological curiosity. In December 2008, Bruno Reversade travelled to India in pursuit of some spit. The journey took him first to the northeastern city of Allahabad, and then a further 10 kilometres to Mohammad Pur Umri, a farming village enclosed by mud walls. There he asked some of the 2,000 or so residents to deposit samples of their saliva in a cup specially designed for the purpose. Fifty-five of them complied. This is not the first time that these villagers have sacrificed their body fluids for science. Mohammad Pur Umri has become somewhat famous, not for the milk or mustard that provides the villagers with their livelihood, but for its prolific production of identical \u2014 monozygotic \u2014 twins. Globally, only 1 in every 250 to 300 births are identical twins. In Umri, roughly one in ten is of this type, births that the villagers \u2014 including the twin village leaders \u2014 call \"gifts from God\". Reversade is looking for genes that might be responsible for this gift. \"Every 50 seconds a pair of natural clones is born. It's more frequent than some of the most frequent genetic diseases,\" says Reversade, a developmental biologist at Singapore's Institute of Medical Biology. \"It can't be random.\" Many scientists disagree, arguing that chance could fully account for this cluster of cases and for every twin birth besides. Aside from genes and chance, theories abound for what causes a fertilized egg to produce a pair of monozygotic twins. None is well accepted. Nor do scientists understand how entities that are apparently genetically identical can come to have such different personalities and disease susceptibilities. Environmental factors explain some, but not all, of these differences. Over the past few years, scientists have been using new genetic and cell-biology techniques to attack these questions. Reversade is gathering samples from three 'twin towns', and using genomic analyses that, he hopes, will point to a common molecular pathway involved in twinning. Embryologists and obstetricians are looking for clues in assisted reproduction (see  'Making twins' ), which is known to promote monozygotic twinning as well as high rates of dizygotic twins, which result from the transfer of multiple embryos. Theories on how identical twins come to differ are also being overhauled, with some studies even suggesting that genetically identical twins may never, even at the earliest stage of development, have been genetically identical. \"This research just hasn't been part of people's thinking,\" says Judith Hall, a prominent twinning researcher at the University of British Columbia in Vancouver. Reversade became interested in monozygotic twinning after cutting frog embryos into halves and watching through a microscope as they developed into identical embryos. \"Pure awe,\" he says. His goal is to understand why cells that are acting together to form an embryo split off and start building a whole new organism, something they can do early, when the embryo is just a few cells big, or as late as two weeks into development. Conjoined twins can result if the embryo splits too late or incompletely. Reversade says that twinning offers the best way to study 'regulative development' \u2014 the interaction between cells that informs each one when to follow the pack and when to act alone. He moved to Singapore in February 2008 after landing the government's first A *STAR Investigatorship, a US$500,000 per year grant modelled on the Howard Hughes Medical Institute awards, having convinced the funders of the feasibility and importance of finding a twinning gene or genes. Twinning is often assumed to run in families, and in the case of dizygotic twins, caused by the release and then fertilization of two eggs, scientists agree that this is the case. So far though, researchers have only found genes that are weakly correlated with dizygotic twinning. The picture for monozygotic twins is even less definitive. In large-scale studies, family members of the mothers of monozygotic twins do not seem more likely to have monozygotic twins themselves 1 . But some families and some communities do produce identical twins in numbers that seem to defy this interpretation. Reversade is visiting them one by one. Two years ago he travelled to Jordan to collect saliva samples from members of a family with 15 pairs of monozygotic twins. The family tree fits a pattern in which a dominant gene \u2014 one that would cause monozygotic twinning even if only one copy is present \u2014 is on one of the 22 autosomal chromosomes. But to make the hereditary pattern work, the gene must have 'variable penetrance', such that some women would not bear monozygotic twins even though they carried a copy of the gene. \"Variable penetrance is of course a 'black box',\" says Reversade. \"Why don't we see more twins?\" One reason, he suggests, is that some twins 'vanish' \u2014 meaning that at least one of the two dies \u2014 during pregnancy. According to a widely cited estimate by Charles Boklage, a behavioural and developmental biologist at East Caroline University in Greenville, North Carolina, at least 12% of natural conceptions will produce twin embryos. Both twins come to term in only 2% of those pregnancies. A singleton is born around 12% of the time, and in the vast majority of cases both embryos are lost, often without the pregnancy ever being noticed 2 ,   3 . Some theories suggest that the twin conceptions that survive do so because of a healthy system that is able to support two embryos through implantation and pregnancy.  \n                Founder effect \n              In Jordan, Reversade used genetic tests to search for shared patterns of single-letter variations in the genomes of the twins that might point to a twinning gene. He found one candidate region, on chromosome four. One of the genes in the region has some promising characteristics: it is conserved through vertebrate evolution, it codes for a protein that is expressed in the blastocyst stage of mice embryos, and its activity drops once the cells that make it have differentiated. Reversade thinks that mutations in the gene or other genes working in the same signalling pathway might have been present in the founders of each twin town and then spread through the population. He says he won't publish the work \"until I have the full story, namely the gene, and a possible mechanism\". That story took on new twists during Reversade's visit to India. \"The plot thickens,\" Reversade wrote in an e-mail after returning from Mohammad Pur Umri. \"The last pair of monozygotic twins was born 3 weeks ago. There have been close to 55 pairs over the past 30 years. Not all have survived. One mother has had two pairs of monozygotic twins. It is absolutely astounding.\" Villagers there told him of three sets of twins produced by buffalo that share a pond the village uses for its water supply. Reversade took samples of the water to check for any substances that could affect reproduction. The beginning of twinning in the village coincides with the establishment of a nearby air force base some 40 years ago, but he could not find any evidence of obvious environmental pollutants. Still, some villagers are convinced there is something in the soil causing the twinning. At least one related gene hunt has been unsuccessful. In 2004, scientists from the Centre for Cellular and Molecular Biology in Hyderabad collected blood samples from the Indian village to look for genes associated with twinning and appointed a sociologist to survey food intake and social habits. \"We have not been able to conclude anything significant,\" says Lalji Singh, the centre's director. The group did not bother publishing the results. Reversade hopes that newer technologies \u2014 he plans to use next-generation genetic sequencing machines on the candidate regions in all his subjects \u2014 will give him the necessary sensitivity to find genes, but he has a long way to go to convince his peers. Edison Liu, director of the Genome Institute of Singapore, says that inbreeding, which is suspected in each of these villages (although denied by residents of Umri) supports the argument for a genetic contribution to twinning. But, he says, \"the observation should be viewed with some scepticism as geographical clustering of rare events often is a statistical fluke\". Reversade's next stop, probably this summer, will be the village of Linha S\u00e3o Pedro in Brazil, a town predominantly of blond-haired, blue-eyed people founded by German immigrants. In the 1990s, 10% of the births there were twins, and almost half of those were monozygotic. In a book published last year,  Mengele: the Angel of Death in South America , Argentine journalist Jorge Camarasa argues that Nazi doctor Joseph Mengele was responsible, using techniques he devised to rebuild a master race. The Mengele argument hasn't sold well among scientists and Reversade hopes to prove it wrong once and for all. \"That explanation makes it a perfect plot to solve,\" he says. Humans are one of the most proficient mammalian species at multiple monozygotic births. Armadillos are better. The nine-banded armadillo ( Dasypus novemcinctus)   produces identical quadruplets with every litter from embryos that have split, and then split again. Importantly for Reversade, they also show that multiple births can have a genetic basis in mammals. Evolution seems to have favoured the genes that contribute to the armadillo's reproductive strategy. Creating multiple embryos from each egg may get around physical constraints imposed by the shape of the armadillo uterus. It is not clear that reproduction in humans and armadillos would employ the same genes or have arisen for the same evolutionary reasons. Most scientists think of twinning in humans not as an evolutionary advantage, but as a breakdown of normal function in a female body evolved to carry only one fetus at a time. Some, taking into consideration the 2\u20133 fold increased risks of congenital anomalies in babies born as monozygotic twins 4 , describe it in terms of risk factors, as they would a disease. \"Most people accept that twinning is a failure, not a desired outcome,\" says Dianna Payne, an embryologist and visiting researcher at the Mio Fertility Clinic in Yonago, Japan. Why the system goes awry is not clear. One theory of twinning holds that subtle differences force cells in early embryos to repel one another and establish two separate cell masses. Reversade, hesitantly, speculates that his mutated candidate gene might deprive cells in the embryo of their ability to adhere tightly to each other, resulting in a split. Hall favours a theory based on the timing of fertilization to explain why humans twin so often. Most mammals recognize the oestrus cycle so that they mate at \"the right time\", she says, when eggs have been freshly ovulated. \"Humans do it any old time and can easily miss the right time.\" She suggests that the outer shell, called the zona pellucida, of an old egg might be more likely to fracture and split the blastocyst in two when it later hatches out 4 . In this scenario, twinning could just be a side-effect of an otherwise successful human reproductive strategy.  \n                Normal genetics \n              Isaac Blickstein of the Hadassah-Hebrew University School of Medicine in Jerusalem and Louis Keith of Northwestern University in Chicago, Illinois, think that there could be a genetic component to monozygotic twinning, although not one considered mutant or abnormal. They argue that human twinning is the result of \"a subpopulation of primary oocytes with an inborn and, as yet, unspecified propensity\" for splitting 5 . There is one thing that all scientists agree on: assisted reproduction causes higher rates of monozygotic twinning \u2014 2 to 12 times higher, according to a review published last year 6 . Ovarian stimulation with drugs, and embryos spending an extended period in culture before being transferred are most consistently linked with twinning events. The culture conditions could act directly on the embryo, encouraging it to split, or could harden the zona pellucida so that the embryo is snipped in two as it hatches. Perhaps the closest researchers have come to watching human twinning is in time-lapse images collected every two minutes during the development of embryos that had been created by  in vitro   fertilization, frozen and then thawed after they were designated as surplus. Payne's team at the Mio Fertility Clinic was surprised to find that 25 of 26 blastocoels, fluid-filled cavities that support the compacted clump of cells that will become the fetus, collapsed at least once. The more frequent and more dramatic the collapse, the less likely the embryos were to survive. Most strikingly, the clump of cells inside two of the collapsed blastocoels split and the two fragments developed as if they were on the path to becoming two separate embryos in different parts of the egg. \"The cells are quite sticky at that point and when the blastocoel reinflates itself, some get stuck on the other side,\" says Payne. She thinks that the culture system, which does not perfectly mimic the uterine environment, might bring about the collapse by causing some cells to die or by weakening cell junctions in the egg's membrane. Such mechanisms could also be taking place in naturally conceived embryos if they were triggered by faulty genes, says Payne. \"But we'll never get a camera in there to see it. Special cameras aren't needed to document another aspect of identical twinning: that the twins differ in appearance, personality and in their propensity to develop disease. \"After 50 years of epidemiological work, we cannot answer why there is such divergence in multiple sclerosis, schizophrenia, or type I diabetes between twins,\" says Arturas Petronis, who has been studying twins at the University of Toronto in Ontario. Last year, Jan Dumanski at the University of Alabama in Birmingham offered a surprising possible answer: that identical twins are not so genetically identical after all. Her group compared 19 pairs of monozygotic twins and found that the individuals within a pair have segments of DNA that are duplicated or deleted 7 . These regions might help to identify the causes of some of the disease discrepancies within the pair. Other explanations for twin differences might be found outside the genetic sequence. Manel Esteller at the Bellvitge Institute for Biomedical Research in Barcelona, Spain, looked for differences in patterns of histone acetylation and methylation \u2014 'epigenetic' marks that commonly control gene activity \u2014 in monozygotic twins ranging in age from 3 to 74. The younger twins had similar epigenetic marks \u2014 but those patterns diverged with age, or as they were exposed to different environmental factors 8 . \"When one twin starts smoking, taking drugs or moves somewhere with more air pollution \u2014 even for only a year \u2014 their epigenetic profile can diverge sharply,\" says Esteller. \"It is very dynamic.\" Petronis, however, thinks that spontaneous, random epigenetic changes are likely to contribute more than those triggered by the environment. The epigenetic differences might even start accumulating from day one of development. Researchers at the University of Cambridge, UK, found that the first four cells in a mouse embryo sometimes exhibit differences in histone methylation 9 . Petronis suggests that diverging epigenetic profiles might be what drives the cells to split into twins in the first place. His work on monozygotic twins has shown that the genomic loci that differ the most epigenetically are those involved in cell-division processes, which \"may reflect an early developmental discordance as one of the hypothetical reasons of twin formation\". If that is the case, Reversade might not ever find a twinning gene. But that is not going to stop him from packing his bag for the twin town in Brazil. The armadillo gives him confidence that there are twinning genes to be found, as does a strain of zebrafish in which a mutation can generate a double head like that of conjoined twins. And so does the Jordanian family which, he says, \"cannot be explained by these epigenetic phenomena but can be by conventional genetics\". What will he do if his results don't show a gene? \"Publish them,\" he says simply.\n \n                     Reversade Lab \n                   Reprints and Permissions"},
{"file_id": "459770a", "url": "https://www.nature.com/articles/459770a", "year": 2009, "authors": [{"name": "Amanda Leigh Mascarelli"}], "parsed_as_year": "2006_or_before", "body": "The boundaries of biology reach farther below Earth's surface than scientists had thought possible. Amanda Leigh Mascarelli delves into how microbes survive deep underground. In February, a team of American and German oceanographers set out on a ship for a little-known destination in the middle of the Atlantic Ocean called North Pond. This patch of sea floor lies on the western flank of the Mid-Atlantic Ridge \u2014 the longest mountain range in the world \u2014 where the topography of the ocean bottom drops to form a 10-kilometre-long basin rimmed by underwater peaks. For two weeks, Katrina Edwards, a geomicrobiologist from the University of Southern California in Los Angeles, and her team explored North Pond, collecting samples of the muddy sediments that fill the basin. From their ship, they dropped hollow coring tubes down through 4.5 kilometres of water and into the bottom muck. On lucky days, the equipment went straight through the sediment and struck the underlying rock, which bent the coring barrel into the shape of a banana. Although the collisions sacrificed a few pieces of pipe, they also yielded samples of the delicate interface between the rock and the sediment, one of the targets high on the researchers' wish list. Edwards had come 7,000 kilometres to look for 'intraterrestrials' \u2014 the microbes inside the sediments and the rocks beneath, where not long ago it was thought that life could not exist. She is among a group of scientists who are learning just how resilient and pervasive life is in the deep earth, both under the sea floor and inside the continental crust. Nicknamed the 'iron maiden' by her colleagues, Edwards is particularly interested in those life forms that feast on iron and that colonize some of Earth's most inhospitable terrain: the igneous crust that reaches to some 500 metres below the ocean bottom. \"What I study is essentially the tooth decay of the solid Earth, the microbes that inhabit the nooks and crannies of Earth's molars that are exposed at the bottom of the ocean,\" says Edwards. Such areas were largely inaccessible until the 1990s, when new techniques made it possible for scientists to make direct observations of this deep biosphere. In particular, oceanographers have developed sub-sea-floor laboratories known as circulation obviation retrofit kits (CORKs), which seal scientific instruments inside deeply drilled boreholes and make real-time measurements of life in the deepest, darkest realms of the marine subsurface. To date, researchers have mounted only one scientific drilling mission, in 2002, that was wholly dedicated to this biosphere, but they are poised to launch four more by 2013 through the international Integrated Ocean Drilling Program. \"We're right at the cusp of this major breakthrough,\" says Edwards, who plans to return to North Pond in a year or two. The North Pond study and others around the world are changing the way scientists think about the deep biosphere. Ten years ago, such low-life microbes were largely regarded as curiosities that represented one of the last frontiers on Earth. Now, scientists have come to appreciate these organisms as integral players in global cycles, helping to replenish key minerals in the ocean and even mediating the climate. \"As the science matures, there is an ongoing sense of wonder about what's down there, but we're also coming to understand how they are involved in the biogeochemical cycling and the health of our planet,\" says Rick Colwell, a geomicrobiologist from Oregon State University in Corvallis (see  'Mining value from deep life' ). New findings are also leading to insights about the origins of life on Earth and how life might exist on other planets. Although the microbes turn up nearly everywhere that scientists search for them, they often seem to subsist at the very brink of survival, metabolizing so slowly that it has prompted fresh ideas about the limits of life.  \n                Deep-sea sandwich \n             In 1955, Claude ZoBell, considered to be the father of marine microbiology, probed beneath the sea floor and found microbes there, decreasing in numbers down to a depth of about 8 metres 1 . At that time, researchers thought that life would peter out at some point not far below the seabed. Then in the late 1960s, an inadvertent experiment supported the notion of a depauperate deep sea, when the research submersible  Alvin   sank more than 1,500 metres after a cable snapped. The crew of three escaped safely through the hatch, but their lunches were left behind. When the vessel was recovered 10 months later, the crew was surprised to find their stranded, soaked bologna sausage sandwiches and apples in nearly pristine condition, showing no sign of microbial decay. \"This was the popular vision of the deep sea, being too extreme even for significant bacterial life,\" says John Parkes, a geomicrobiologist at Cardiff University, UK. The notion of the deep sea as an uninhabitable desert persisted for decades, colouring thinking about the sea floor and what lay beneath it. Then, in the 1980s and 1990s, some of the first missions of the Ocean Drilling Program made it possible for researchers to dig deeper than ever before. When Parkes and his colleagues tried in 1990 to publish results in  Nature   showing that bacteria could colonize much greater subsurface depths than previously thought, they were met with \"very sceptical reviews\" and the paper was rejected, he says. But in 1994, they succeeded in publishing their results and reported viable microbes living in ocean sediments at depths greater than 500 metres below the seabed 2 .  \n                Active bugs \n              That and subsequent studies showed that microbes could be cultured from samples obtained far below the sea floor. But the techniques at the time could not definitively show that the organisms were alive and actively metabolizing at such remarkable depths, leaving open the possibility that the deep bacteria were dormant, barely living. But in 2005, researchers led by Axel Schippers of the Federal Institute for Geosciences and Natural Resources in Hannover, Germany, showed the presence of intact membranes and ribosomes 3  \u2014 the first conclusive evidence that bacteria are thriving in 16-million-year-old sediments more than 400 metres deep. Last year, metabolically active microbes were reported in 111-million-year-old sediments buried as deep as 1.6 kilometres below the seabed 4 . So little is known about microbes that dwell in the deep that scientists have a hard time estimating what fraction of life they represent. A decade ago, estimates derived from work by William Whitman of the University of Georgia in Athens and his colleagues 5  suggested that one-third of all life on Earth lives in the sub-sea-floor sediments. But most of the samples of microbes from deep sediments have been collected close to shore, meaning that much of the ocean has been underrepresented. Steven D'Hondt, an oceanographer at the University of Rhode Island in Narragansett, recently sampled sub-sea-floor sediments in the North and South Pacific oceans. Those findings suggest that global cell abundances may be an order of magnitude lower than previous estimates, D'Hondt and his colleagues reported last December at a meeting of the American Geophysical Union in San Francisco. Yet Whitman's numbers did not include microbes living in the ocean crust, which would add to the estimated cell counts, says Edwards. These hidden microbes are turning up in other unexpected places. In the late 1990s, researchers plumbing the depths of the continental crust in a South African gold mine discovered microbes living at about 3 kilometres below the surface. Plans are now under way to begin drilling in the deepest mine in North America: Homestake in South Dakota, which is to house the US Deep Underground Science and Engineering Laboratory. The mine reaches down nearly 2.5 kilometres, and researchers hope to drill from that depth into rock with temperatures exceeding 120 \u00b0C. \"The deepest extent of the biosphere is currently unknown,\" says Tom Kieft, an environmental microbiologist from the New Mexico Institute of Mining and Technology in Socorro. \"If we drill deeply enough, we'll reach beyond the upper temperature limit for life, which is thought to be around 121 \u00b0C.\" Organisms that live in the deep biosphere bear little resemblance to surface bacteria such as  Escherichia coli , which can be easily cultured in the lab and divide every few minutes. In the sub-sea floor, bacteria and another group of microbes called archaea are slow by comparison, says Bo Barker J\u00f8rgensen, a biogeochemist at Aarhus University in Denmark. For organisms buried beneath the surface of the continents, the first estimates suggested that they reproduce on a timescale measured in centuries 6 . And Tullis Onstott, a geomicrobiologist at Princeton University in New Jersey who pioneered much of the exploration for deep terrestrial life in South African gold mines in the late 1990s, estimates that subsurface microbes there may reproduce once every 1,000 years.  \n                Energy crisis \n              Even with such low metabolic rates, it remains unclear how such organisms sustain themselves. \"When we do the calculations, there's not enough energy down there at all for these organisms,\" says Parkes. \"They should all be dead.\" The sparse food present in the deep-sea sediments comes from the sunlit layers. There, photosynthesizing plants and algae digest organic matter that eventually rains down in the form of dead algal cells, faecal matter and marine detritus. It settles on the sea floor and accumulates in the sediments over millions of years. Scientists estimate that the majority of sub-seabed microbial communities graze on this deeply buried organic carbon, contributing to the 'deep carbon cycle'. Only those microbes beneath the sea floor can metabolize these gritty organic leftovers. Like earthworms that plough the soil and recycle minerals and nutrients, these sub-sea-floor microbes produce carbon dioxide and methane and liberate key elements including nitrogen, sulphur and phosphorus from the sediments. And as fluids circulate through the crust, they carry microbes that can erode the rock, releasing iron and other elements. The circulating fluids take these nutrients back up into the ocean, where they can feed the growth of new biomass. \"The carbon cycle of the oceans and of planet Earth reaches deeply into the subsurface biosphere and cannot be understood without the subsurface contribution,\" says Andreas Teske, a microbiologist at the University of North Carolina in Chapel Hill. The discoveries of the past decade back up a portion of the 'deep hot biosphere' hypothesis, proposed in 1992 by the late Thomas Gold, an astronomer at Cornell University in New York. In a famous paper 7 , Gold argued that the subsurface supports a mass and volume of life rivalling that present on the surface. But Gold went further to speculate that the deep biosphere subsists on hydrocarbons rising from Earth's mantle, a vast energy source that continually refills oil deposits. That suggestion is no more accepted now than it was at the time.  \n                Climate connection \n              Even as they help recycle nutrients, the microbes below the sea floor may also have an effect on the planet's climate. Archaea called methanogens produce methane as a by-product of their metabolism. Colwell is working to quantify the rates of that methane production and says that they sometimes fall below the detection limit 8 . Yet the methane builds up over geological timescales and contributes to the formation of 'hydrates', icy cages of water molecules surrounding methane that become wedged in marine sediments. Most methane hydrates are thought to be generated by microbial processes 9 . Such deposits have apparently become destabilized at several times in Earth's past, releasing enough methane \u2014 a greenhouse gas much more potent than CO 2  \u2014 to warm the planet substantially 10 . As the globe heats up because of human pollution, scientists are becoming increasingly concerned about the potential breakdown of methane hydrates that are trapped inside thawing Arctic tundra and shallow marine sediments. Whereas the vast majority of microbes in the deep biosphere rely on leftover organic matter for food, others seem to be getting sustenance from an inorganic source. D'Hondt is investigating whether sub-sea-floor microbes may be obtaining energy from hydrogen that is produced when the radioactive decay of naturally occurring elements such as uranium, thorium and potassium splits water molecules into hydrogen and oxygen. The same process would have occurred on the early Earth more than 4 billion years ago, \"and it should be occurring on Mars today\", says D'Hondt. \"We've found some evidence that as much energy is entering these [sub-sea-floor] ecosystems from radioactive splitting of water as from burial of organic matter.\" As yet, though, the identity of any microbes making a living in this way is unclear, he says. In the terrestrial biosphere, microbes found in a South African gold mine nearly 3 kilometres underground have a similar form of metabolism. They subsist on geologically produced sulphate and hydrogen, free from any dependence on energy derived from the Sun 11 .  \n                Early Earth \n              Geomicrobiologists working in South Africa have also reported the first ecosystem that comprises a single bacterial species,  Candidatus Desulforudis audaxviator , which lives out its lifetime in pitch darkness at 60 \u00b0C, some 2.8 kilometres beneath the surface of Earth 12 . This microbe seems to obtain its energy by reducing sulphate that is formed indirectly by the radioactive decay of uranium, and it can extract carbon and nitrogen from the surrounding rocks. Dylan Chivian, a computational biologist at Lawrence Berkeley National Laboratory in California and the lead author of the study, says that this single-species ecosystem \"points to a mode of life that potentially is what early Earth might have been like\", before the atmosphere held much oxygen. The deep microbes discovered in mines are sometimes found in pockets of water that have been secluded for millions of years, practically making the organisms there living fossils. Fractures in the rocks periodically open and close due to tectonic shifts, locking the water and the microbes into what Barbara Sherwood Lollar, a geochemist at the University of Toronto in Ontario, calls a \"series of time capsules\". \"In these hydrogeologically isolated systems of very great age, it raises all kinds of fascinating questions about how long the microbes have been there, how they've evolved and what that means for our understanding of the origins of life on the planet,\" she says. Because microbes that colonize the deep biosphere have mastered the art of living on the margin, the astrobiology community has also taken a keen interest in them. \"Energy limitation is almost certainly a fact of life in extraterrestrial habitats,\" says Teske. The anaerobic metabolic reactions that deep-dwelling microbes use are the very chemical reactions that are most likely to support life on other planets, he adds. \"It's so fascinating that all this microbiology and these processes that were once considered to be in the realm of geology are really supporting a major part of life on Earth,\" says J\u00f8rgensen. \"It's like discovering a new continent.\" That sense of potential is what motivated Edwards and her colleagues during their recent trip to North Pond. In the waning days of the expedition, the team kept a frenzied pace collecting mud cores and storing them in refrigerators for the journey home. This cruise was a reconnaissance mission of sorts, which will set the stage for future expeditions. In 2010 or 2011, Edwards plans to return aboard the drill ship  JOIDES Resolution   that can penetrate rock and bore down some 500 metres into the crust. Once there, they plan to install a CORK observatory that will peer inside the intraterrestrial underworld and monitor that environment for a decade. These missions and others, Edwards hopes, will finally shed light on the deep dark biosphere and the lower limits of life itself. \n                 Amanda Leigh Mascarelli is a freelance science writer based in Denver, Colorado.  \n               \n                     Nature Reports climate change \n                   \n                     Nature Geoscience \n                   \n                     North Pond expedition blog \n                   \n                     Rick Colwell's web site \n                   \n                     Katrina Edwards' web site \n                   Reprints and Permissions"},
{"file_id": "458962a", "url": "https://www.nature.com/articles/458962a", "year": 2009, "authors": [{"name": "Monya Baker"}], "parsed_as_year": "2006_or_before", "body": "The field of induced pluripotent stem cells has gone from standing start to headlong rush in less than three years. Monya Baker charts the course so far, and the obstacles ahead. Back in spring 2007, Shinya Yamanaka thought he had a safe head start in a scientific race. Less than six months earlier he had demonstrated a technique that turned run-of-the-mill body cells into ones much like mouse embryonic stem cells 1 . Yamanaka's results were met with awe and scepticism. Few believed that a cell's identity was so flexible that the insertion of just four embryonic genes could reprogram it into a cell that could make virtually every body tissue. Yamanaka knew he would have to do more to convince others that the cells were truly pluripotent: capable of becoming any cell type, including contributing to sperm or egg cells and thus another generation of animals. So on 6 June 2007, when he published an improved version of his technique showing that these 'induced pluripotent stem (iPS) cells' could actually do this 2 , he hadn't expected that two other laboratories would announce that they had accomplished the same feat on the same day 3 ,   4 . \"It was less than ten months after our publication,\" Yamanaka recalls, \"so we were very, very surprised \u2014 and we were very, very scared.\" Surprise and fear are feelings that Yamanaka has become accustomed to since he founded the iPS field in mid-2006. At that time, it was just him and his lab at Kyoto University in Japan. Since then, Addgene, a company based in Cambridge, Massachusetts, has received more than 6,000 requests from in excess of 1,000 labs for the relevant reprogramming vectors that it supplies. In 2008, Harvard University, along with universities in Toronto and Kyoto, established entire facilities devoted to iPS cell studies. In 2009, researchers expect the field to move even faster and to become more competitive. In March alone, four papers in  Nature ,  Cell   and  Science   reported major refinements to the reprogramming technique 5 ,   6 ,   7 ,   8 . The fervour is understandable. iPS cells promise nearly everything embryonic stem cells do \u2014 including the potential for cell therapy, drug screening and disease modelling \u2014 without most of the ethical and technical baggage. Much of the early human embryonic-stem-cell work was restricted to scientists who had access to human embryos, says Peter Andrews, a stem-cell scientist at the University of Sheffield, UK. The invention of iPS technology, he says, \"opens up the area to anyone who is a competent molecular or cell biologist\". Although it took 17 years from the 1981 isolation of mouse embryonic stem cells to the isolation of their human counterparts, that transition took less than six months for iPS cells. And although stem-cell researchers have yet to make patient-matched human embryonic stem cells, they have already reached an equivalent goal in the iPS cell field, making cells from patients with conditions such as diabetes, Huntington's disease and muscular dystrophy 9 . Biologists are now jostling to reach the next obvious goals: iPS cells that represent a wider variety of diseases, and safer, more efficient ways to make them. \"It's not healthy. It's overheated,\" says Rudolf Jaenisch, a leading researcher in embryonic stem cells and iPS cells at the Whitehead Institute of Biomedical Research in Cambridge, Massachusetts. \"Every day in the lab people are worried about getting scooped.\" That means people rush to publish prematurely, he says, and are reluctant to share. \"Everyone is doing very similar things, so people aren't that open to talking about papers submitted or in press. The field risks losing sight of the big questions, says Jeanne Loring, the director of the Center for Regenerative Medicine at the Scripps Research Institute in La Jolla, California \u2014 questions such as the mechanisms by which reprogramming works and precisely what reprogrammed cells will be able to do therapeutically. \"Making the cells is not the end point,\" says Loring. \"The cells are of no value if they don't tell you something new. When it comes to studying and treating human diseases, iPS cells are potentially far more useful than embryonic stem cells. They could eventually offer a method for taking cells from a patient's body, treating them, and turning them into therapeutic cells that can be returned to the same individual without the risk of rejection. Researchers have already taken the iPS cells created from patients with neurodegenerative diseases such as amyotrophic lateral sclerosis and spinal muscular atrophy and converted them into neurons 10 , 11 . And in mice they have taken the next step, generating blood and neural cells and using those to ameliorate mouse versions of sickle-cell anaemia and Parkinson's disease 12 ,   13 . More immediately, the cells could be invaluable for researchers who want to study, say, brain or heart diseases and who can't collect enough tissue from biopsies or cadavers to conduct rigorous experiments. iPS cells made from patients with these diseases promise a limitless supply, and the ability to study the disease process in a dish.  \n                Full stem ahead \n              But in the first two years, researchers have been most preoccupied with improving the methods for making iPS cells. Yamanaka experimented with two dozen genes expressed in embryonic stem cells before hitting on a quartet (c-<i>Myc</i>,  Klf4 ,  Oct4   and  Sox2 ) capable of reprogramming adult cells when they were inserted into the genome using a virus. Although a cell dials down the activity of these genes as it assumes pluripotency, their addition nevertheless seems to make the cells less predictable and more dangerous than embryonic stem cells. Cells that are reprogrammed with the cancer gene c-<i>Myc</i> and then incorporated into mouse embryos, for example, result in animals that develop fatal tumours 1 . And Yamanaka has presented unpublished work that even mice generated from cells reprogrammed without using c-<i>Myc</i> have shorter lifespans. The rush to develop safer, more effective techniques for reprogramming cells has often resulted in prominent publications right on the heels of each other (see  'Mile markers' ). The aim has been to reprogram cells without pushing genes into the genome, where they risk causing damage. Jaenisch led one of the groups that published techniques last month for cutting out the reprogramming genes after they have finished their job 7 . Three weeks later, James Thomson and colleagues at the University of Wisconsin, Madison, reported in  Science   that they had reprogrammed human cells without requiring any genetic insertion at all 8 . They put pluripotency genes into cells using DNA rings called plasmids that did not integrate into chromosomes. But scientists want to do better. The worry is that reprogramming might shove cells so far from what is physiologically normal that they become pathological. The reprogramming process inhibits tumour-suppressing pathways and activates oncogenic ones, says Sheng Ding, a chemist at the Scripps Research Institute. It also disrupts a cell's processes for placing 'epigenetic' marks that control which genes are activated. \"Cells are undergoing very stressful conditions,\" Ding says. \"People don't talk about the hidden problems.\" Ding and many others are working to ease the transition to pluripotency with further refinements to the technique. They have found that adding drug-like molecules or starting with certain cell types can allow reprogramming with fewer types and copies of reprogramming genes as well as boosting reprogramming rates. And by the end of this year, many researchers expect to see multiple techniques for making iPS cells without adding reprogramming genes at all, instead using combinations of small molecules and proteins. But even that won't be enough to ensure that the cells are safe for therapeutic purposes, says Thomson. Any reprogramming technique runs the risk of causing mutations or problematic epigenetic changes. \"It doesn't matter whether you do it chemically or genetically, you're going to have to look at the resulting genome in excruciating detail,\" Thomson says. Yamanaka agrees. \"Everyone tends to think that if you make iPS cells with fewer factors or even zero factors, with chemicals, that those iPS cells are safer, but I'm not sure about that. We really have to test each clone,\" he says. \"Improving the derivation method is important, but I can't stress enough that how to evaluate established iPS cells is much more important.\" Researchers expect the field to start shifting towards this type of evaluation. \"It will be important now to compare the different methods and go with the one that works the best,\" says Konrad Hochedlinger of the Harvard Stem Cell Institute. What 'best' is may depend on the application. The techniques for inserting reprogramming genes are faster and technically less demanding and, for laboratories that don't have other reprogramming systems up and running, they might be a sensible choice.  \n                Side by side \n              Researchers also want to compare the iPS cells with each other and with embryonic stem cells. Embryonic stem cells are considered the gold standard. They have been studied for more than a decade, and their common origin from embryos suggests, to most scientists, that they will be less variable than iPS cells derived from different tissue types. In his recent  Cell   paper, Jaenisch characterized human iPS cells before and after the extra genes had snipped themselves out 7 . Cells that still contained extra copies of the reprogramming genes expressed 271 genes differently from embryonic stem cells; with the genes gone, that number dropped to 48. No one knows why. \"There is so much anecdotal evidence saying that iPS cells don't do as well or that they are different from embryonic stem cells,\" says Jaenisch, \"It's just unpublished.\" The cells could be intrinsically unique because they don't come from embryos, or they might differ from embryonic stem cells because current methods for creating iPS cells are inadequate. Researchers have not yet agreed how to evaluate iPS cells. The most rigorous test of reprogramming involves inserting reprogrammed mouse cells into an embryo, implanting it into a surrogate mother, letting the chimaeric mice grow to adulthood, and waiting to see if the reprogrammed cells go on to make sperm or eggs that produce healthy offspring. The ability to contribute to a brand new embryo shows that the biological settings in the original cells have been reset. Such tests are ethically unacceptable in humans, so the standard assay, borrowed from human embryonic stem cells, involves injecting human cells into an immune-compromised mouse and waiting six to eight weeks to see if the cells form a tumour called a teratoma. Naturally occurring teratomas can grow into a knot of differentiated tissues, including hair and bone, but for transplanted cells to win the iPS label, researchers just need to see a mass of differentiated cells representing all major classes of tissue. Researchers say that it is not uncommon for cells that seem fully reprogrammed in terms of appearance and surface markers to fail to form teratomas. Some researchers think that anything worthy of the iPS cell designation should demonstrate the ability to make teratomas. \"Unless we hold the field to some standard, it will muddy the literature,\" says George Daley at Children's Hospital Boston in Massachusetts, a leader in the field. Especially while the field is young and techniques are still being developed, he says, it is \"hazardous\" to say cells are iPS cells just because they express some markers typical of embryonic stem cells. \"What will it mean if we call everything that has some quality of stemness an iPS cell?\" Daley asks. \"The term will start to lose its integrity.  \n                Safety first \n             But in some cases it may not matter if a cell line can make every cell type. iPS cells that can't make teratomas but are very good at making hepatocytes, for example, might be better for modelling liver disease and safer in the clinic. The teratoma assay is also expensive, says William Stanford of the Ontario iPS Cell Facility in Toronto. His group is generating disease-specific lines from patients at the Hospital for Sick Children in Toronto, and they already anticipate having more samples submitted for reprogramming than resources to generate cell lines. \"We talked about whether we should make fewer lines and do teratoma testing on all the lines, or make more lines,\" he says. They decided on the latter. They will assess the pluripotency of reprogrammed cells using gene expression and  in vitro   tests of early differentiation, but further characterization will generally be left to individual laboratories that later use the cells. Besides evaluating the iPS cells themselves, researchers also want to see rigorous, long-term evaluations of the specialized cell types generated from them, which might be used for cell therapy, drug screening, or other applications. Because obtaining homogeneous samples of differentiated cells is difficult, says Jaenisch, no one has yet published these types of evaluations. But to screen drugs or to model diseases, researchers need to be confident that, say, neurons or cardiomyocytes coaxed from iPS cells go on to age and develop disease like the cells in intact brains or hearts. And when it comes to cell therapy, they need to know that the cells are stable, and do not contain leftover iPS cells that could generate tumours, a possibility that is also being evaluated for embryonic stem cells. Even when they have been evaluated in these ways, iPS cells will still face formidable hurdles before they reach the clinic. Regulators will need to be convinced that the risks are acceptably low and that there is a real likelihood that introduced cells will survive in the body and increase the function of a diseased brain or pancreas. It took more than a decade from the generation of the first human embryonic stem cells to the approval this January of a clinical trial of cells derived from them. Now that iPS cells can be made without genetic modification, they could make that transition much more swiftly. Yamanaka thinks that the cells will be used widely for drug screening and toxicity testing within three or four years. He hopes to see clinical trials in ten years. Much of this work is likely to be performed by companies, and a few are already trying to corner the market in iPS cells for practical applications. John Walker, chief executive of biotech start-up iZumi Bio, in South San Francisco, California, gives little away, but says the company will focus on drug testing rather than cell therapy for now. iZumi, along with the Wisconsin Alumni Research Foundation in Madison, and others, have filed intellectual-property claims around iPS cells and the techniques for making them. With more and more methods being published, the intellectual-property situation is \"more complicated than for human embryonic stem cells by an order of magnitude\", says Ken Taymor, director of the Berkeley Center for Law, Business and the Economy in California. The scientific landscape is also becoming more complex. Biologists have long assumed that one specialized cell type must be transformed back into an embryonic-like pluripotent state before it can be turned into another specialized cell. But recent work has shown that it is possible to bypass pluripotency and hop directly from one cell type to another. Doug Melton, a developmental biologist at Harvard University, did this to much acclaim in 2008 when he showed that cells in the pancreas could take on the appearance and function of insulin-producing ?-cells if extra copies of pancreatic genes are inserted into them 14 . Whether reprogramming proceeds 'backwards' or 'sideways', scientists want to understand how it occurs. For many established scientists, this is the question that brought them into the iPS-cell field in the first place. Yamanaka says he would not have attempted his initial reprogramming experiments were it not for the cloning of frogs by nuclear transfer in the 1950s or the cloning of Dolly the sheep in 1996. Before that, some thought that genes were irreversibly deactivated or perhaps even excised as cells progressed through development. Dolly \u2014 cloned from an adult cell \u2014 showed that the genes remained intact and amenable to rebooting, even in specialized mammalian cells.  \n                Rough guide \n              Researchers understand the general outlines of reprogramming. Cells loosen the tangles of DNA and protein, known as chromatin, and rearrange epigenetic marks so that the genes active in specialized cells are silenced, and those active in embryonic stem cells are turned on. They recruit an army of proteins to shift the cell machinery from one state to another. How and when all these steps occur is, despite intense study, still being worked out \u2014 and it is a question that many researchers hope that the iPS field will focus on as it matures. With iPS cells \"you can ask how reprogramming really works\", says Hochedlinger, \"This was a question that was raised 50 years ago. We have no clue.\" iPS cells don't make the problem easy, though. For one thing, it is difficult to isolate the right cells: typically, less than 1 in 1,000 cells is successfully reprogrammed in the production of iPS cells. Some cells remain trapped in differentiated states even if pluripotency genes are active 15 . \"The problem is that we don't have the intermediate states,\" says Kathrin Plath, a cell biologist at the University of California, Los Angeles. Using gene expression and cell morphology, Plath is studying a subset of cells that seem to get stuck on the way to full reprogramming. \"Partially reprogrammed cells seem to be very similar no matter how you get them,\" she says, \"but who knows if they are true intermediates of the actual reprogramming process or off on a side track?\" Understanding the reprogramming process is not just an academic exercise. Knowledge about the various states of a cell, and how cells move from one state to another, could help researchers refine their techniques for driving cells through those transitions safely, and making the cell types they want for therapies. Researchers who have seen other biological fields, such as recombinant DNA and RNA interference, go through a similar breathless period after their inception, predict that the frantic pace and competitiveness are likely to wane. The rush to optimize the reprogramming techniques will pass, predicts Martin Pera, director of the Institute for Stem Cell and Regenerative Medicine at the University of Southern California, Los Angeles, and scientists will branch out into particular types of disease or more fundamental questions. \"Activity in the field will diversify,\" he says, \"and the field will become more collaborative.\" Collaborative or not, the iPS race is heading into a new, and perhaps more intellectually rewarding, leg. Until this point, \"it's all technology, technology, technology\", says Jaenisch. \"Now we are coming to the interesting questions. And the challenging questions will be the biological questions.\" Monya Baker is the editor of  Nature Reports Stem Cells . \n                     Nature Reports Stem Cells \n                   Reprints and Permissions"},
{"file_id": "4581091a", "url": "https://www.nature.com/articles/4581091a", "year": 2009, "authors": [{"name": "Richard Monastersky"}], "parsed_as_year": "2006_or_before", "body": "The climate situation may be even worse than you think. In the first of three features, Richard Monastersky looks at evidence that keeping carbon dioxide beneath dangerous levels is tougher than previously thought. In 2007, environmental writer Bill McKibben approached climate scientist James Hansen and asked him what atmospheric concentration of carbon dioxide could be considered safe. Hansen's reaction: \"I don't know, but I'll get back to you.\" After he had mulled it over, Hansen started to suspect that he and many other scientists had underestimated the long-term effects of greenhouse warming. Atmospheric concentration of CO 2  at the time was rising past 382 parts per million (p.p.m.), a full 100 ticks above its pre-industrial level. Most researchers, including Hansen, had been focusing on 450 p.p.m. as a target that would avoid, in the resonant and legally binding formulation of the United Nations Framework Convention on Climate Change, \"dangerous climate change\". McKibben was aware of this: he was thinking of forming an organization called 450.org to call attention to the number, and his question to Hansen was by way of due diligence. As he thought about McKibben's question, Hansen, who runs NASA's Goddard Institute for Space Studies in New York, began to wonder if 450 p.p.m. was too high. Having spent his career working on climate models, he was aware that in some respects the real world was outstripping them. Arctic sea ice was reaching record lows; many of Greenland's glaciers were retreating; the tropics were expanding. \"What was clear was that climate models are our weakest tool, in that you can't trust their sensitivity in any of these key areas,\" he says. Those warning signs \u2014 and his studies of past climate change \u2014 led Hansen to conclude that only by pulling CO 2  concentrations down below today's value could humanity avert serious problems. He came back to McKibben with not 450 but 350. In 2008, he published a paper spelling out his rationale for that target 1 . The difference between 350 and 450 is not just one of degree. It's one of direction. A CO 2  concentration of 450 p.p.m. awaits the world at some point in the future that might conceivably, though with difficulty, be averted. But 350 p.p.m. can be seen only in the rear-view mirror. Hansen believes that CO 2  levels already exceed those that would provide long-term safety, and the world needs not just to stop but to reverse course. Although his view is far from universal, a growing number of scientists agree that the CO 2  challenge is even greater than had previously been thought. Several recent studies, for example, indicate that it may be exceedingly difficult to cool the climate down from any eventual peak or plateau, no matter what CO 2  concentration is chosen as a target by the international community. And by looking at the problem in a new sort of way \u2014 by tallying the total amount of carbon injected into the atmosphere across human history \u2014 two papers in this issue of  Nature   reveal how close the world has come to the danger point ( pages 1158  and  1163 ). \"It's tougher than people have appreciated. We have less room to manoeuvre,\" says Malte Meinshausen, an author of one of the papers and a senior researcher at the Potsdam Institute for Climate Impact Research in Germany.  \n                Mr Greenhouse \n              Hansen has a long history of stirring up controversy with gloomy climate prognostications. Often, they turn out to be right. In 1988, he told the US Congress that the recent warming of Earth's surface was very unusual and it was time to point a finger at the cause. Hansen said it was his opinion that \"the greenhouse effect has been detected and it is changing our climate now\". He caught a lot of flak for that statement, but the Earth continued to heat up and the rest of the scientific community eventually concurred with his assessment. He also used models to predict the amount of subsequent cooling to be expected from the eruption of Mount Pinatubo in 1991. That did much to convince people of the reliability of such models and of climate theory. The model simulations Hansen and others worked on in the 1970s and 1980s had a profound effect on both climate scientists and politicians. When nations started exploring policies to curb CO 2  emissions, the target most discussed was 550 p.p.m., in large part simply because that was what the modellers had experience with: in early studies of the greenhouse future, researchers had sought to get a sense of the scale of possible change by simulating what would happen if the atmosphere held 550 p.p.m., roughly twice the pre-industrial level of CO 2  in the air. Those studies showed a 550-p.p.m. world as warming quite a lot. In 1979, a panel of the US National Academy of Sciences led by Jule Charney, a prominent weather and climate researcher, estimated it would be 1.5 to 4.5 \u00b0C hotter. That estimate for what has become known as 'climate sensitivity' has stayed remarkably solid ever since: the most recent report of the Intergovernmental Panel on Climate Change pegged the sensitivity as being between 2 and 4.5 \u00b0C, while adding that higher values could not be excluded. Although early policy discussions focused on the 550 p.p.m. mark, researchers and politicians soon concluded that such warming would be too much. In 1996, the European Union declared that \"global average temperatures should not exceed 2 \u00b0C above pre-industrial level and that therefore concentration levels lower than 550 p.p.m. CO 2  should guide global limitation and reduction efforts\". Over the following decade, 450 p.p.m. became increasingly cited as a level to aim for, because some studies associated that concentration with 2 \u00b0C of warming. In their 2008 paper, Hansen and his colleagues offer a number of reasons for arguing that even 450 p.p.m. is too high. The most important are observational: rapid changes in the Arctic and elsewhere have demonstrated that the globe is more sensitive to even today's levels of greenhouse gases than climate models have predicted. Others depend on details of the way climate sensitivity is defined. The standard approach, going back to Charney's formulation, comes from models that allow fast-reacting components of climate to change but hold constant other, slower factors, such as forests and ice sheets. Yet evidence from the past shows that such slow players are acutely sensitive to varying levels of CO 2  \u2014 and are not so slow. By analysing how temperature and greenhouse-gas concentrations actually correlate over the past 500,000 years, as ice sheets have waxed and waned, Hansen and his colleagues find that the true climate sensitivity is 6 \u00b0C. Going even further back, the team argues there is evidence for a tipping point in the greenhouse. Some 50 million years ago, CO 2  concentrations were many times today's levels and Antarctica was ice-free. Concentrations declined slowly and crossed a crucial threshold 35 million years ago when the globe was cool enough for an ice sheet to start growing on Antarctica. Through a series of extrapolations, the researchers estimate that the threshold level was between 550 and 350 p.p.m. To avoid any risk of recrossing that threshold and losing Antarctica's ice, best keep at or below the bottom of that range: 350 p.p.m. Hansen's arguments do not convince everyone. Stefan Rahmstorf of the Potsdam Institute says that there are important distinctions between melting and forming an ice sheet, and the two processes might occur at different greenhouse-gas concentrations. In fact, a 2005 modelling study conducted at Potsdam suggests that during a simulated ice age, the amount of warming needed to melt the North American ice sheet is consistently greater than the amount of cooling needed to grow it 2 . \"You have a different threshold for the ice sheets coming and the ice sheets going,\" says Rahmstorf. Hansen, though, sticks with the new low figure. He argues that realizing the world is already in dangerous climatic territory \"completely changes the story. When you say 450 or 550, you're talking about what rates of growth you are going to allow. When you say we have to get to 350, that means you have to phase down CO 2  emissions in the next few decades.\"  \n                Peak problems \n              So how easy would it be to get back to 350 p.p.m.? Most scientists have assumed that it would not take that long to pull down CO 2  levels if humanity went cold turkey and cut off all emissions, says Susan Solomon of the National Oceanic and Atmospheric Administration in Boulder, Colorado. \"I've done a little informal poll of colleagues,\" she says. \"It was interesting, the number of smart, knowledgeable people who said if we stop emitting, things will go back maybe in 100 years, 200 tops. But they're not correct. And I didn't believe it would be so long either.\" Solomon changed her mind because of a study in which she and her colleagues used what's known as an Earth-system model of intermediate complexity \u2014 an EMIC. Although not as detailed as general circulation models, which divide the atmosphere and ocean into millions of cells, EMICs have the advantage of requiring less computing and so can run simulations lasting many centuries. They are also useful because they represent Earth's carbon cycle \u2014 the natural movements of carbon between the atmosphere, the biosphere and the oceans. Using an EMIC developed by the University of Berne in Switzerland, Solomon and her colleagues tested what would happen if CO 2  emissions immediately ceased after concentrations peaked at various values, starting with 450 p.p.m. (ref.  3 ). What they found surprised them. CO 2  levels subsided so slowly that they remained substantially above pre-industrial levels 1,000 years into the future. Global temperatures also stayed up, and had declined only slightly from their peak by the year 3000. In fact the simulations ended before temperatures dropped anywhere close to their starting point. \n               boxed-text \n             According to Solomon, the simulated climate recovers so slowly because of two factors. Natural sinks are only able to take up a fraction of the CO 2  in the atmosphere, so roughly 20% of the emitted gas will stay in the air for at least a millennium, ensuring that it continues to warm the globe long after emissions are cut off. The thermal inertia of the oceans also plays a part: the large mass of ocean water on the planet is delaying the rate of climate warming today because most of it is lagging behind the changes in surface temperature. Once it has warmed it will retard the Earth's cooling after emissions cease.  \n                Slow recovery \n              Experiments conducted with a more complex model actually make the picture look worse. In a paper this year, Jason Lowe, head of mitigation advice at the UK Met Office, and his colleagues described a study using a general circulation model at the Met Office's Hadley Centre in Exeter, UK, coupled to a carbon-cycle model 4 . He found that after emissions were curtailed, temperatures remained elevated at least to the end of the simulation, which went on 100 years past the cut-off. In fact, if CO 2  concentrations reached 550 p.p.m. or higher before the emissions stopped, temperatures actually increased for at least a century (see graphs). He would like to see other groups run similar experiments with their own general circulation models. The take-home message from his and other studies, Lowe says, is this: \"If you do end up somewhere you don't want to be, it's probably going to take you a long time to get back to lower temperature levels.\" Lowe is now looking at how long such warming would last and what kind of trouble it might get the world into by, say, melting the Greenland ice sheet. Because it will apparently take so long for the climate to recover from excessive warming, researchers are now looking at new strategies to avoid that excess in the first place. One approach is to stop thinking about the levels at which CO 2  might be stabilized and instead concentrate on something simpler: the sheer amount of CO 2  that can be emitted in total. In this issue of  Nature , Meinshausen and his colleagues present results from a coupled climate\u2013carbon cycle model that explores the effects of different emission pathways for CO 2  and the other major greenhouse gases ( page 1158 ). For the period 2000 to 2050, they find that the world would have to limit emissions of all greenhouse gases to the equivalent of 400 gigatonnes of carbon in order to stand a 75% chance of avoiding more than 2 \u00b0C of warming. Other greenhouse gases, such as methane and nitrous oxide, are expected to produce as much warming as 125 gigatonnes of carbon in the form of CO 2  would; that means emissions of CO 2  itself over the half-century have to add up to less than 275 gigatonnes of carbon. That's an extremely difficult target, admits Meinshausen, considering that emissions over the past nine years have used up almost a third of that allowance already. \"Our remaining emission budget is so small,\" he says. \"If we want to have a smooth landing and to decrease emissions in a smooth way, our options are essentially exhausted. We have to bend down our emissions by 2020.\" Also in this issue, Myles Allen of the University of Oxford, Meinshausen and their colleagues describe how they ran a series of simulations using a simple combination of climate and carbon-cycle models ( page 1163 ). They find that if humankind could limit all CO 2  emissions from fossil fuels and changes in land use to 1 trillion tons of carbon in total, there would be a good chance that the climate would not warm more than 2 \u00b0C above its pre-industrial range. Because half of that trillion tons has already been spewed into the atmosphere, and emissions now average about 9 billion tons a year and rising, the trillion-ton limit would allow the world to follow its current trend for less than 40 more years before giving up carbon emission for good, all at once. One way of looking at that challenge is put forward by Hansen. Go ahead and burn all the remaining oil and gas in conventional reserves, he says, and at the same time concentrate all efforts on quickly phasing out coal \u2014 or capturing and storing the emissions associated with it. If nations can cut off coal use by 2030 and avoid tapping unconventional fossil fuels, such as tar sands and methane hydrates, the world could limit future CO 2  emissions to 400 gigatonnes of carbon. \n               boxed-text \n             Other studies using this total-carbon-emitted approach are now appearing; a couple were presented at the International Congress on Climate Change held in Copenhagen in April. Although differing in details, they come to broadly similar conclusions. Allen says a total limit for carbon emissions, which he calls cumulative warming commitment, is a much more robust figure than a stabilization concentration of CO 2  in the atmosphere. The problem with looking for a stabilization concentration is that one must first know the globe's long-term response \u2014 its 'equilibrium climate sensitivity' \u2014 to calculate how much the planet will eventually warm for a given concentration. Estimates of what that equilibrium climate sensitivity might be are shaky, and hence so are forecasts based on it. A focus on total carbon emissions rather than concentrations, however, wipes away that problem because it demands that concentrations go up and \u2014 eventually \u2014 come back down, never stabilizing at a particular level. So the climate never reaches equilibrium and the uncertainties about its long-term response do not matter as much. \"If you assume a finite injection of carbon,\" says Allen, \"you don't need to know the climate sensitivity, so this whole debate about the equilibrium response is moot.\" Although the results of the studies might seem too daunting, they do offer a few rays of hope. Andrew Weaver, a modeller at the University of Victoria in British Columbia, Canada, says that in the new studies, what matters is how much pollution goes into the sky, not when it gets emitted. \"This allows you some flexibility,\" he says. From a political perspective, the idea of a cap on total emissions \"is a lot easier to get your head around\" than a concentration target or, say, a 20% reduction below 1990 emission levels. A cap is like a budget. Once you use it up, there's nothing left to spend. Unfortunately, the world is behaving as though it expects to be able to arrange a large overdraft. And researchers can only come up with so many ways of presenting the gravity of the carbon problem to the rest of the world. \"At some point, you begin to throw your hands up. It's very frustrating,\" says Weaver, who pulls a reference from an ancient global crisis. \"Climate scientists,\" he says, \"have begun to feel like a bunch of Noahs \u2014 thousands of Noahs.\"   See also Editorial,  \n                   page 1077 \n                 , and  \n                   www.nature.com/climatecrunch \n                 .  \n                     Nature Reports Climate Change \n                   \n                     Nature Geoscience \n                   \n                     James Hansen's website \n                   \n                     350.org \n                   \n                     United Nations Framework Convention on Climate Change \n                   \n                     Intergovernmental Panel on Climate Change \n                   \n                     Malte Meinshausen's website \n                   \n                     Global Carbon Project \n                   Reprints and Permissions"},
{"file_id": "458820a", "url": "https://www.nature.com/articles/458820a", "year": 2009, "authors": [{"name": "Eric Hand"}], "parsed_as_year": "2006_or_before", "body": "As the launch of the Planck spacecraft approaches, Eric Hand investigates what the mission could mean for the predominant theory of the moments after the Big Bang. Space is cold. But Planck will be even colder. At the heart of the European Space Agency spacecraft, chilled to 0.1 kelvin by the most sophisticated cryogenic system ever put into space, a collection of gossamer threads will be suspended in near vacuum. These threads, looking like an array of spiderwebs, will gather photons from the cosmic microwave background (CMB) \u2014 the afterglow of the Universe's creation. The extreme cold will be necessary to fulfil Planck's mission to make ultraprecise maps of the CMB. Primordial photons, the oldest light in the Universe, stream through every cubic centimetre of empty space at an average temperature of just 2.7 kelvin. The temperature of the photons will vary very slightly, depending on which part of the sky they are coming from (see ' Mapping the cosmos '). Planck's supercold detectors are designed to measure these differences in temperature at the level of less than a millionth of a kelvin (see ' Cooling Planck down '). That extraordinary precision, say the spacecraft's designers, means that once launched \u2014 a milestone now scheduled for 6 May \u2014 the probe could over its 2-year lifetime settle questions that have roiled the astrophysical community for a generation. \"We have to dig deeper and Planck is our next best chance to do that,\" says project scientist Jan Tauber, based in Noordwijk, the Netherlands. For nearly 30 years, says Tauber, the thinking of cosmologists has been guided by a theory called inflation, which tries to explain how the Universe evolved in the moments after the Big Bang. Inflation has passed every observational test to date, mainly by predicting the statistics of the temperature variations in the CMB seen by Planck's predecessors. But Planck's extreme sensitivity to the variations will put inflation to its most stringent test yet \u2014 and will either vindicate it, or demolish it in favour of some rival theory, of which there are several. Indeed, inflation is so important to modern cosmology that the ultracool, 2-tonne, \u20ac600-million (US$-800 million) spacecraft is in a very hot competition with dozens of ground-based and balloon-borne experiments, all pursuing the same goal: exquisite measurements of the CMB (see ' The race for B-modes '). \"This is a very big race,\" says Michael Turner, a cosmologist at the University of Chicago in Illinois. \"This is Swedish gold.\"  \n                A helping hand \n              The Big Bang would have stayed pretty small without some sort of boost. To reconcile quantum theory with cosmology, physicists would like to believe that the primordial Universe started out just 10 \u221235  metres across. That leads to a contradiction. The age of the Universe is reasonable well known: 13.7 billion years. If it started off as small as the theorists would like and expanded only at the rate that now prevails, it would still be able to fit comfortably within the full stop at the end of this sentence. Inflation explains this apparent paradox by postulating a spectacular expansion in the very first moments (see ' Timeline of the inflationary Universe '). There is no easy analogy for how furious and quick an expansion it was, but as an example, in one simple inflation model, that same infinitesimal Universe could have spread to something like 10 1,000,000,000,000  metres across. That's a one followed by a trillion zeros. And it would have done so in a trillionth of a trillionth of a trillionth of a second. Not even light could keep up: the farthest a photon could have travelled since the Big Bang \u2014 a scale known as the horizon distance \u2014 is 'only' about 10 27  metres. (This is not a contradiction with relativity: no two particles located at the same point during inflation ever had a relative speed greater than light. The explosive speed refers instead to the scale of the Universe as a whole.) In addition to explaining the Universe's colossal size, inflation neatly solves many other problems. It explains why the Universe appears geometrically flat rather than curved \u2014 think of a balloon that has been blown up so far that its surface looks like an infinite plane. And it explains how a Universe that easily could have looked utterly different in every direction in fact looks pretty much the same, with the same average density of galaxies and the same average CMB temperature. \n               boxed-text \n             On top of all that, inflation explains the galaxies of which we are a part. Although it made the infant Universe almost completely flat and uniform, inflation also had to obey the dictates of quantum mechanics, which produced the tiniest of fluctuations in density from point to point. So some parts of the Universe would have ended up denser than others. These denser regions would have become the seeds around which galaxies and stars would gravitationally coalesce. And earlier observatories such as NASA's Wilkinson Microwave Anisotropy Probe (WMAP), which was launched in 2001, have shown that these fluctuations in the CMB not only exist, but have precisely the kind of size distribution predicted by inflation 1 .  \n                Naming the unknown \n              Yet for all its explanatory power, inflation has its problems. For starters, no one knows what did the inflating. Theorists describe the 'force' as a field and give it a name \u2014 the inflaton \u2014 but the mystery remains. It is the same frustration that bedevils astronomers studying dark energy, an unknown force that accounts for three-quarters of the energy in the Universe and still accelerates its expansion. Could the cause of inflation also be the driver behind dark energy? It is an interesting similarity, but they act on vastly different scales; dark energy is a flea to inflation's elephant. \"It seems unlikely that they're related,\" says Turner. \"Which is a good reason to pursue that idea,\" he adds impishly. A bigger problem for inflation, according to Paul Steinhardt, a physicist at Princeton University in New Jersey, is not so much what it is, but how it stopped. \"Once it starts it never ends,\" says Steinhardt, who was one of inflation's founding fathers in the 1980s, but is now one of its chief critics. There is no obvious reason why the ultrarapid expansion should ever slow down to the much more modest rates of expansion seen today. \"At first,\" says Steinhardt sarcastically, \"this was celebrated.\" The 'celebration' of this notion of inflation without brakes is a jab at Steinhardt's colleague and sometimes rival Andrei Linde: a physicist at Stanford University in California. (Linde, Steinhardt and Steinhardt's graduate student of the early 1980s often share in the credit given to Alan Guth, now at the Massachusetts Institute of Technology in Cambridge, for originating the theory that Guth first set out in 1980 (ref.  2 ).) By 1986, Linde theorized that, because of quantum fluctuations, some portions of the Universe might feel the inflationary force more strongly than the average portion 3 . The result in each case would be a localized bulge. Because of inflation's immense strength, however, the bulge would quickly take over and inflate into a whole new universe, which would be attached to the old one by nothing more than a quantum-scale thread. To residents of the old universe, that connection would be too small to see; they would never know that a new creation had happened right in front of them. To residents of the new universe, it would look like a new Big Bang of their own. Indeed, universe could sprout on universe, like a fractal. The multiverse, as Linde calls it, would go on forever. But where Linde saw something creative, an endlessly budding tree, Steinhardt saw something more like an aneurysm, debilitating if not deadly to the overarching theory. If the new offshoot universes tend to take over, the ones left behind become rare islands. What you think is going to be a typical universe is quickly left behind by newer, emerging universes. Conceivably, moreover, the physical laws could change with each emergence, so theorists couldn't know if the physics seen in our Universe governs overall. \"The part that we observe \u2014 our Universe \u2014 exists almost nowhere. It's the rare region,\" Steinhardt says. boxed-text There is a third issue with inflation \u2014 less a problem of the theory itself than an issue of its reach or scope. As it stands, inflation is not a theory of how it all began, but a theory of how it all began just after the beginning. Call it a morning-after theory. In inflation, the Big Bang itself remains an unknowable, infinitely dense moment of time and space called a singularity. \"The biggest weakness is the notion that we can get by in a theory of cosmology without understanding the singularity,\" says Neil Turok, the director of the Perimeter Institute for Theoretical Physics in Waterloo, Canada. \"It's like, 'let's just start the clock a little bit afterwards'.\" Steinhard points out that the term Big Bang was originally coined by Fred Hoyle, a staunch opponent of the idea, as a way of mocking the notion of a cosmos suddenly appearing in the clap of a magician's hands. Ironically, the term stuck. \"Some people like the idea of there being a moment of creation,\" Steinhardt says. But he isn't one of them. Nor is Turok. In 2001, the two physicists proposed a radical alternative to inflation called ekpyrosis, from the Greek for 'out of fire' 4 . It grew out of discussions with string theorists, who see the visible world as inhabiting lower-dimensional membranes, or branes, in a universe made up of at least 10 dimensions. Steinhardt and Turok proposed two universes on separate three-dimensional branes that would oscillate back and forth along a mutually perpendicular dimension, like sheets hung out to dry on parallel washing lines. Every trillion years or so, after each universe had dissipated into darkness during an expansive phase, the two branes would approach one another and collide, releasing a fireball of energy to start each universe afresh. \"It would mean that the Big Bang wouldn't be a beginning but a collision,\" says Steinhardt. \n                Avoiding the singularity \n              Ekpyrosis mimics many of inflation's appealing features, with some key differences. Almost by definition, it avoids the singularity, because it describes the collision of branes as a continuous process. \"We have to resolve stuff from before to after,\" says Turok. \"Inflation can get away with ignoring it.\" Turok says string theory has given him tools that now allow him to describe, through ekpyrosis, a crunch that avoids a singularity altogether. Linde says he will believe it when he sees it. He has pointed out mathematical problems in ekpyrosis, and forced its proponents to revise their thinking since their original proposal. He occasionally enlists the help of string theorists, but he says he now has a hard time getting them to pay any attention to inflation's alternatives. If anything, the big advance in the past five years is that string theorists are finally finding linkages between their work and inflation. Ekpyrosis, says Linde, is \"like a house of cards\". Most theorists would say that inflation is still the best game in town. But Daniel Baumann, a postdoctoral researcher at Harvard University who has worked with Steinhardt and Turok, and who is the lead author on a recent paper that maps out the theoretical landscape of inflation and its alternatives 5 , speaks for many when he adds that it is also \"something that should be challenged\". boxed-text Generational politics might be playing a role in the debate, says Linde. Young scientists don't want a career based on polishing an existing theory, he says. They would rather strike out on their own with something revolutionary \u2014 as he did when he first published on inflation in his early 30s \u2014 whereas those who erected the framework of inflation have a vested interest in defending it. But that doesn't explain the iconoclasm of older scientists such as Turok, and especially Steinhardt, who helped establish inflation. With them, according to David Spergel, an astrophysicist at Princeton University who is on the WMAP team, it is just a matter of personal style. \"There are people who like making a mess,\" he says, \"and there are people who like cleaning it up.\"  \n                Variations on a theme \n              There could be another explanation for the restlessness among theorists. They may just be bored with a lack of new data. The WMAP data have nearly been wrung dry. Existing CMB data somewhat constrain the development of new theories, which is especially challenging if you want to invent a radical one like ekpyrosis. But there are few barriers to creating a new flavour of inflation. Variations on the theme are proposed (and often, quickly quashed) all the time: such as racetrack inflation, multi-field inflation and hyperextended inflation. Each of them satisfies the basic tenets of inflation, but the differences \u2014 primarily the shape and duration of the inflationary pulse \u2014 cannot be distinguished with existing data. Planck could change that situation. Two tests will be particularly important, not only in weeding out the imposters, but in constraining key parameters such as when inflation began \u2014 if it happened at all \u2014 and how long it lasted. One of these tests will be for the 'Gaussianity' of the CMB. The mottled temperature variations of the CMB sky seem random, but the likelihood of a hot spot sitting next to a cool spot may follow some rules. If a perfect and simple inflation governed the early Universe, then these shifts from hot to cold \u2014 the variations of the variations \u2014 should have the Gaussian shape of a bell curve. Ekpyrosis, and many of the more complicated versions of inflation, have characteristic deviations from the Gaussian curve. \"The implications of non-Gaussianity are so profound, you really want evidence that it's there,\" says Spergel. Last year, using WMAP data, Benjamin Wandelt, a Planck scientist from the University of Illinois at Urbana-Champaign, concluded to his own surprise that the CMB seemed to have significant non-Gaussianity 6 . But the result, based on months of intensive computer time, could easily evaporate even though it's supposed to be 99.5% certain. \"It's at the level of being tantalizing but not conclusive,\" says Wandelt. The WMAP team, which has performed analyses of its own, has so far resisted a declaration of non-Gaussianity. But the number of studies claiming non-Gaussianity continues to grow, including a recent one led by Christoph R\u00e4th of the Max Planck Institute for Extraterrestrial Physics in Garching, Germany 7 . \"There's a consistent picture of inconsistencies,\" he says. Without Planck, however, the existing hints are unlikely to congeal. A bigger test for inflation, the 'smoking gun' that has scientists building microwave telescopes at the South Pole and sending balloons nearly to space, is the search for B-modes. B-modes are a special type of polarization that may be present as markings on the CMB. If they exist, they would be indirect evidence for the gravitational waves that should have accompanied inflation. Ekpyrosis is clear about this: it predicts no gravitational waves whatsoever, and so B-modes should be absent. With inflation, the B-mode story, like the prospects for Gaussianity, is more complex. The simplest form of inflation should produce a relatively large B-mode signal, just within reach of Planck. But in other forms of inflation, the signal could be tiny. If Planck fails to detect a B-mode signal, but does find evidence for non-Gaussianity, inflation would certainly be \"in more of a corner\", according to Turok. Unable to resist a poke, he adds: \"It's already in a corner.\" A non-detection of B-modes by Planck would put the pressure on the ground-based and balloon experiments, some of which claim to have B-mode sensitivities about an order of magnitude better, although they cover far less of the sky. But Andrew Lange, a physicist at the California Institute of Technology in Pasadena, says there are limits to how far they can go. Lange has probably done more than anyone to pioneer the technology of bolometers, high-resolution microwave thermometers that will be used on Planck. If the B-mode signal is too weak, then the contemporary microwave emissions of the Universe (mostly from the Milky Way) will drown out the relic signal no matter how good the bolometer. At that point, the only opportunity to study inflation would be to observe the gravitational waves directly, via a mission such as the Big Bang Observer. A proposed successor to the Laser Interferometer Space Antenna \u2014 another far-off space mission to detect and measure gravitational waves \u2014 the Big Bang Observer would have the tough task of directly detecting the low hum of inflation's gravitational waves, rather than picking them up indirectly in the CMB, as the B-mode experiments try to. But the Big Bang Observer's technology, and its funding, are still just theoretical. And without hints of a signal from one of the B-mode experiments, the project will be hard to justify. If the bang was indeed ekpyrotic, there may be no Big Bang gravitational waves to observe, however subtle the instruments. And so any talk of Swedish gold \u2014 Nobel prizes \u2014 for the first finder of a B-mode, and probably also for the proposers of inflation, is either spot on or way too premature. Lange, for one, has bets on the former. In the next few years, he will have bolometers of his devising in space, on a balloon and at the South Pole. But he is also steeling himself in case of the latter. When he gives talks about the search for B-modes, he always inserts pictures of people chasing geese. If the current generation of experiments fails to detect anything, he will humbly accept that Nature has put some things beyond his reach, and his lifetime. \"Then, personally, I go do something else.\" \n                     ESA Planck \n                   \n                     NASA CMB \n                   \n                     WMAP \n                   Reprints and Permissions"},
{"file_id": "458959a", "url": "https://www.nature.com/articles/458959a", "year": 2009, "authors": [{"name": "Roberta Kwok"}], "parsed_as_year": "2006_or_before", "body": "Far from being just an accessory, mobile phones are starting to be used to collect data in an increasing number of disciplines. Roberta Kwok looks into their potential. When Martin Lukac felt a small earthquake rattle his Los Angeles apartment, he immediately thought of the mobile phone lying on his desk. Two weeks earlier, he had programmed the phone to capture readings from its built-in accelerometer, a sensor originally intended to support features such as games. Now, Lukac \u2014 a doctoral student in computer science at the University of California, Los Angeles \u2014 transferred the phone's data to his computer and saw the readings plotted as a series of tell-tale spikes. Success! His phone had become a mobile seismometer. Such moments are happening more and more often these days, as researchers seek out innovative ways to exploit mobile phones. The opportunities are tantalizing. Phones are increasingly being equipped with not only accelerometers, but also cameras, Global Positioning System (GPS) receivers and Internet connectivity. Many of them can support programs devised by anyone, not just the phone's manufacturer, which means that digitally savvy scientists can write and distribute mobile-phone software for everything from monitoring traffic to reporting invasive species. And perhaps best of all for the budget-conscious researcher, the phones are almost ubiquitous. There are now about six mobile phone subscriptions for every ten people in the world, according to a March report 1  from the International Telecommunication Union, based in Geneva, Switzerland. And the GSM Association, a mobile-communications industry trade group, announced in February that the number of mobile-phone connections worldwide had hit 4 billion and was expected to reach 6 billion by 2013. \"We've really never had a technology other than human observation itself that is as pervasively deployed out in the world,\" says Deborah Estrin, Lukac's adviser and director of the Center for Embedded Networked Sensing (CENS) at the University of California, Los Angeles. Despite the challenges in harnessing mobile phones, including privacy protection and unpredictable data flow, projects such as Lukac's are starting to emerge in a number of disciplines, from medical imaging to human behaviour.  \n                Location, location, location \n              One of the most enticing features of mobile phones for researchers is GPS, which uses satellite data to pinpoint a phone's location. Once mobile phones got GPS, says Quinn Jacobson, a computer engineer at the Nokia Research Center in Palo Alto, California, they suddenly had an \"'awareness' of where they were in the world\". This makes mobile phones a natural tool to study road traffic, says Alexandre Bayen, a systems engineer at the University of California, Berkeley, who is collaborating with Jacobson. Today, Bayen says, traffic is often monitored with equipment such as cameras, radar and sensors embedded in the pavement. But mobile phones could provide a cheaper way to collect the information, because scientists can piggyback on the phone companies' existing communications infrastructure. There's no need to \"send a crew with a truck to dig a hole in the highway\", says Bayen. In November last year, Bayen's team launched Mobile Millennium: a project to generate real-time traffic estimates with GPS-enabled mobile phones in a region near San Francisco. Volunteers download the software and transmit their position and velocity to a central computer system. The system combines the phone data with historical records and other sensor readings, reconstructs traffic flow in the area, and sends the results, suitably anonymized, back to the user's phone for display. So far, the software has been downloaded about 4,500 times, says Bayen, and the team hopes to recruit 10,000 users. Because mobile phones can travel into areas that lack other sensors, they are revealing traffic flow on smaller, previously unmonitored roads outside the highway system. \"For the first time, we're seeing very rich data on these types of roads,\" says Jacobson. The team conducted a pilot study with 100 cars driving 10\u201316-kilometre loops in February last year and found that the phones captured velocity patterns similar to those obtained by underground sensors, including the congestion resulting from a five-car accident 2 . When mobile-phone data are fused with other sensor data, \"you can get enormous gains in accuracy\", says Bayen. The idea of using mobile-phone data to monitor traffic is not new, but Mobile Millennium has managed larger-scale deployment than most academic research efforts, says Jean-Luc Ygnace, a research engineer at the French National Institute for Transport and Safety Research in Bron. The next challenge will be to recruit enough drivers to get sufficient data over a large road network, he says.  \n                All corners of the world \n              Mobile phones have managed to penetrate parts of the globe where other infrastructure is absent. In the developing world, mobile-phone subscriptions have skyrocketed from nearly zero a decade ago to 50% of today's population. Fixed telephone lines reach fewer than one-sixth of the people there 1 . Boris Rubinsky first really appreciated this reality when he visited an Indian village at the foothills of the Himalayas in 2005. Children were washing themselves in the river, and animals roamed across the roads. \"Suddenly, in the middle of the street, you see a person walking around with a cellular phone,\" says Rubinsky, a bioengineer at the University of California, Berkeley, and director of the Center for Bioengineering in the Service of Humanity and Society at the Hebrew University of Jerusalem in Israel. \"It dawned on me that cellular phones are everywhere.\" Rubinsky decided to use the phones to address a specific problem in the developing world: the lack of access to medical imaging. Imaging equipment typically includes components for data capture, processing and display, which together tend to be expensive and difficult to maintain. But with a mobile phone, Rubinsky reasoned, a doctor in a remote village could transmit raw measurements from a relatively inexpensive data-collection device, such as an ultrasound transducer, to a processing centre in a major city. The centre would then reconstruct the image and transmit it back to the phone. In a study published last year 3 , Rubinsky's team did a proof-of-principle test of their system with a procedure called electrical impedance tomography, in which a device applies current to a patient's tissues and measures the resulting voltage. In this case the voltage readings were from a simulated breast tumour. But the researchers were able to move the data through every step of the process until the image appeared on the phone. \"The data are extremely simple, and that's the beauty of it,\" says Rubinsky. He published another study 4  this month in which the system was used to classify tissue. Rubinsky is planning field trials of his system in Mexico within the next few months to see whether it can detect internal bleeding. Other groups are creating mobile-phone applications to conduct health surveys, analyse blood samples and report natural disasters. \"For those of us working in the developing world, that's the platform of choice,\" says Gaetano Borriello, a computer engineer at Google and the University of Washington in Seattle who explores how technology can improve health care in underserved populations.  \n                Eyes on the ground \n              Mobile phones also have the potential to enhance the role of the citizen scientist. Information collected by non-scientists has traditionally been seen as suspect because it is difficult to verify, says Estrin. But with the help of mobile-phone cameras and GPS, which can label data with a precise location, observations taken by everyday citizens could become more reliable. \"The fact that people can do real-time uploading of geo-tagged information changes that story,\" she says. Estrin's team is working on a set of projects that will enlist citizens to submit field observations via mobile phone to a central database. One campaign is tracking the appearance of harmful algal blooms; another, scheduled to be opened to the public in July, will monitor invasive species in California's Santa Monica Mountains. Participants will be able to send in geo-tagged pictures, with optional text messages to describe each photo, says Estrin. By next year, the team also plans to supply a phone-based reporting system for Project BudBurst, a citizen-science effort to measure the effect of climate change on plant blooming. Meanwhile, Lukac and his colleagues are hoping to deploy a test set of accelerometer-equipped phones in an earthquake-prone area and eventually establish a mobile seismic network. Mobile phones may \"open up the demographics of the people who could participate\" in citizen science, says Jeff Goldman, director of programme development at CENS. Instead of having to remember to enter information or pictures through a website after the fact, people will be able to relay their observations directly from the field. Estrin notes that mobile phones also offer two-way communication, allowing participants to receive reminders and instructions depending on the time or location. Other researchers are hoping to use mobile phones as communication hubs for external sensors. Sarah Williams, director of the Spatial Information Design Lab at Columbia University in New York, and her colleagues are working to attach air-quality sensors to mobile phones so that people can send pollution measurements via text message to a central database. And a group led by computer scientist Eiman Kanjo at the University of Cambridge, UK, has developed a system that allows phones to receive data from wireless sensors that measure carbon monoxide concentrations, temperature and other environmental conditions 5 . Kanjo's team gave the mobile sensing system to cycling couriers in Cambridge to collect readings throughout the city and is now analysing the data.  \n                Our phones, our selves \n              One obvious, but important, feature of mobile phones is that they are carried by people. So researchers such as Nathan Eagle, a computer scientist at the Santa Fe Institute in New Mexico, can use them to get glimpses into human movement and behaviour. In an experiment at the Massachusetts Institute of Technology in Cambridge during 2004\u201305, Eagle and his colleagues recorded call logs and location data from mobile phones carried by 100 students and staff \u2014 all volunteers \u2014 over nine months. They also detected when people were in close proximity to the users by scanning for radiofrequency signals emitted by nearby mobile phones. Using the phone data, Eagle's team was able to classify students studying business from those studying other subjects with 96% accuracy. If the researchers examined only the first 12 hours of a user's day, they could correctly predict the person's movements for the rest of the day 79% of the time 6 . \"We think about behaviour as a very high-dimensional thing,\" says Eagle. \"In reality, and depressingly, you can compress my behaviour down to a few vectors,\" he says. Larger-scale experiments are also starting to emerge. Last June, Albert-L\u00e1szl\u00f3 Barab\u00e1si and his colleagues at Northeastern University in Boston, Massachusetts, published a study in  Nature   that analysed the movements of 100,000 mobile-phone users 7 . Eagle is now working with Barab\u00e1si's group and others to examine phone-operator data from a range of geographic areas, including records for millions of mobile-phone users in Europe and two East African countries. Eventually, Eagle hopes to detect common behavioural patterns, such as changes in movement or calling frequency, that occur during disease outbreaks, which could help alert public-health officials to the early stages of an epidemic. Eagle's research illustrates how mobile phones can be used to collect accurate, large-scale data about real social interactions, unlike other methods such as interviews or virtual-world observations, says Jon Kleinberg, a computer scientist who studies social networks at Cornell University in Ithaca, New York. Neil Ferguson, a mathematical epidemiologist at Imperial College London, UK, who plans to collaborate with Eagle, says that although mobile-phone use in places such as Africa may not reflect a representative sample of the population, these insights into microscale social networks could help support more fine-grained models of the spread of disease. At the other end of the spectrum, some groups are exploring public-health applications at the individual level. Researchers at CENS and Intel Research Seattle in Washington, for instance, are developing mobile-phone programs to help users monitor their diet and physical activity.  \n                Hang-ups ahead \n              The scientists doing these experiments readily admit that mobile phones are not the perfect tool. For one thing, the sensors on a phone are usually not high-quality because they must be small and inexpensive. \"You don't want your phone to cost US$20,000,\" says Bayen. Because researchers cannot dictate what manufacturers include on a phone, the observations that can be performed are limited. And although features such as air-pollution sensors could conceivably be packaged into a mobile phone, people do not always carry their phones in ways that would make them useful as scientific instruments. \"Sure, I'd love some air chemistry,\" says Estrin. \"But what does the air chemistry in my purse mean?\" And as mobile phones now provide more information about their users than ever before, researchers must tread carefully to ensure they do not invade users' privacy (see  page 968 ). In some cases, the users are volunteers who agree to be studied. The data from phone operators in Europe and Africa are anonymized, says Eagle. Mobile Millennium relies on a concept called 'virtual trip lines', in which velocity readings are triggered as users pass into predetermined road segments, rather than tracking drivers throughout their journey. The data are encrypted, says Bayen, and Nokia strips out personal information about the user before passing them on to the modelling team. Once they get the data, researchers must contend with an erratic flow of information. Bayen's group is working to improve mathematical models so that they can handle traffic readings from unpredictable locations, but the reliability of the system fluctuates depending on how many measurements are submitted at the time. \"Suddenly you get tons of data, and the next hour you don't get any,\" says Bayen. Scientists may find it difficult to get enough data to validate their approaches without help from industry, says Jacobson. \"A lot of this research needs to be taken out on a commercial scale to even test the fundamental premise,\" he says. Despite the challenges, researchers are excited about the possibilities of a planet-wide network of these miniature travelling computers. About 85% of the world's population has access to a mobile signal, says Susan Teltscher, head of the Market Information and Statistics Division at the International Telecommunication Union, and there is still a \"huge potential\" for more growth. Jacobson envisions that as sensors continue to drop in size, phones could boast even more sophisticated features. Although mobile phones will not replace traditional scientific instruments, says Estrin, they make up in availability for what they lack in finesse. \"If you can't go to the field with the sensor you want,\" she says, \"go with the sensor you have.\"\n Roberta Kwok is a freelance science writer based in California. \n                     CENS \n                   \n                     Mobile Millennium \n                   \n                     Networked Naturalist \n                   \n                     Project BudBurst \n                   \n                     The MIT mobile-phone experiment \n                   Reprints and Permissions"},
{"file_id": "459634a", "url": "https://www.nature.com/articles/459634a", "year": 2009, "authors": [{"name": "David Cyranoski"}], "parsed_as_year": "2006_or_before", "body": "Some say it looks like a rocket ready for take-off. Others say a pagoda. Either description seems more fitting for the tiered and towering 13-metre structure than the diminutive word 'microscope'. The electron microscope at Osaka University in Japan is the most powerful of its kind. The technology behind it is nearly 80 years old, and the device itself has been in operation since 1995. But scientists there say that they are only just starting to show what this monstrous machine can do for biology. Transmission electron microscopes shoot out a beam of electrons, which pass through some parts of a sample and are scattered by others. The emerging beam carries data that can be used to map out the structures it has transited. Because electron beams have a much shorter wavelength than visible light, they can easily resolve structures just a few nanometres across, far beyond the resolution of a conventional light microscope and better for viewing the details of cells' internal and external topography. In the 1940s, scientists used some of the first electron microscopes to produce ground-breaking images of cells' mitochondria and other organelles. The detail was breathtaking, but researchers had to slice the tissue so that it was thin enough for the weakly energized electrons to get through: they were imaging dead, two-dimensional slices cut from living, three-dimensional structures. \"Electron microscopy gave a framework to lay out physiological biology,\" says Mark Ellisman, a neuroscientist and microscopy specialist at the University of California, San Diego. \"But it was kind of like taking road kill and analysing it.\" Ultrahigh-voltage electron microscopes (UHVEMs), which give the electron beam enough energy to pass through and analyse thicker samples, started with two machines that could reach 3,000 kilovolts, built in Toulouse and Osaka in 1970. The new Osaka scope was constructed next door to that older one between 1991 and 1995, with a \u00a52.4-billion (US$25-million) budget. It has an electron gun with a 5.7-metre accelerator tube and a maximum 3,500 kiloelectronvolt electron beam, and it can force electrons through biological samples thicker than 5 micrometres. This goes far beyond the 80-nanometre and 500-nanometre upper limits of the 100-kilovolt and 300-kilovolt machines typically found in laboratories. The Osaka UHVEM is particularly valuable for imaging the diffuse but delicate structures of neurons, which requires an image that spans several micrometres at a resolution of tens of nanometres. Naoko Kajimura, an electron microscopist at Osaka University, has captured three-dimensional images of neuronal cells in the retina to show that those lacking a protein called pikachurin cannot form synapses ( S. Sato  et al .  Nature Neurosci.    11,   923\u2013931; 2008 ). Had the 2-micrometre section been imaged by conventional electron microscopy, it would have needed to have been cut into some 30 slices, making it difficult to ensure that a part was not being missed. A decade ago, few life scientists used the Osaka UHVEM, says Hirotaro Mori, director of the Research Center for Ultra-High Voltage Electron Microscopy, which houses the machine. Materials scientists dominated them, especially those who were exploring the properties of thinly sliced semiconductors and integrated devices. But times have changed. Digital imaging has replaced film, slashing the time needed for data collection and analysis. An automated system makes it possible to better control the mechanics and flux of electrons and hence limit damage to tissues and cells. Computer software can correct much of the image distortion. And scientists have now developed electron-tomography methods \u2014 in which the sample is tilted under the microscope's electron beam \u2014 which generate a series of images from different angles that can be compiled into an accurate three-dimensional structure. Now Mori says that biologists account for half of the machine's basic-research time, which has to be booked up to a month in advance. Microscopists say that electron microscopy as a whole is undergoing something of a renaissance, with attempts to update and automate the technique so that it can image three-dimensional samples. The Osaka microscope is a brute-force solution. \"Many people might say bigger is not better,\" says Ellisman, \"but they don't have access to it.\" Ellisman and his group, who operate the device remotely from San Diego, have been working to image a Purkinje cell, an expansive neuron that measures a couple of millimetres from the cell body to the tips of the exquisitely branched dendritic arbor. The group first took light microscope images of the cell, marking its location in the cerebral cortex so that it could be indexed in a cell-based brain atlas. The team then sent thick sections to Osaka to get nanometre resolution of the spines on the dendrites. Ninety per cent of the mammalian brain's synapses are made on these mushroom-like projections but, Ellisman says, \"until now scientists have thought of them with tinker-toy-level abstraction\". The UHVEM image, which shows the number of spines as well as the size and shape of the spines' heads and necks, will be crucial for simulations on how the brain works, he says. Mori laments that such science experiments account for only half of the machine's use. To balance demand he gives the other 50% to industrial tasks, such as verification of newly designed integrated semiconductor devices, nanoscale characterization of new lubricants, or the measurement of hair damage after a cosmetic treatment. \"I would like to see more basic science,\" says Mori.   See  \n                     page 629 \n                    and online at  \n                     http://tinyurl.com/microspecial \n                   . \n                     Microscopy special \n                   \n                     Cell imaging: New ways to see a smaller world \n                   \n                     Research Centre for Ultra High Voltage Electron Microscopy \n                   Reprints and Permissions"},
{"file_id": "459629a", "url": "https://www.nature.com/articles/459629a", "year": 2009, "authors": [], "parsed_as_year": "2006_or_before", "body": "New microscopes are revealing sights that have never been seen before.  Nature   profiles five machines that are changing how biologists view the world. Close-ups of cork, lice and fly's eyes do not inspire the rhapsodies that they did more than 300 years ago when Robert Hooke first observed them under a microscope. But other pictures do \u2014 the boughs and twigs of a branching neuron in its forest; the scuttle of vesicles delivering molecular loads; the endless thrill of a cell carving itself in two again \u2014 and again, and again \u2014 as an embryo buds into being. Now, as then, microscopy is central to the understanding of living systems. In this special section,  Nature   reporters look at five microscopes that are resolving aspects of life in stunning new ways. Microscopes today still do the job that Hooke asked of his: gathering information on details that the human eye cannot resolve and magnifying them to a size that it can. The microscopes that greet undergraduate biologists in university teaching labs are remarkably similar to those that Carl Zeiss developed in the nineteenth century. On that basis of continuity, one might think that the technology was mature. But although the laws of optics have not changed, the ways they are applied are in constant flux. Part of the flux is due to the ingenuity of engineers and scientists. Applying the laws of optics to electrons, rather than to light, was possibly the greatest development in microscopy of the twentieth century \u2014 and it is still yielding dividends in the twenty first, as the ultrahigh-voltage electron microscope at Osaka demonstrates ( page 634 ). Recently some of that ingenuity has explored the possibility of doing with electronics what used to be done with carefully crafted glass, producing technologies that do away with lenses altogether. The microscope-on-a-chip featured here could turn microscopes into a disposable commodity ( page 632 ). But perhaps the greatest potential for progress is not so much in the engineering of microscopes, as in the engineering of what can be seen with them. Hooke and his successors used the microscope to see the world as it was, revealing seemingly miraculous detail at scales far too fine for human craft \u2014 powerful evidence, so it seemed, for the infinite superiority of divine craftsmanship. Today's microscopes are used more and more to look at systems that are carefully prepared to make their workings visible. The dramatic fruits of this transformation can be seen in the green-fluorescent-protein revolution of the past decades, which has made it possible to engineer appropriate illumination into organisms under study. Its full impact will only be realized when microscopes can capture everything that these organisms can reveal. This is why there is so much excitement around the arrival of the stimulated emission depletion (STED) microscope ( page 638 ) and other 'super-resolution' techniques that are allowing light microscopy to resolve details on the nanometre scale, something once thought impossible. And the unassuming single plane illumination microscope (SPIM) microscope, with its way of imaging life without killing it, could herald an era of 'systems microscopy' ( page 630 ). New ways of manipulating life will supply \u2014 and indeed demand \u2014 new ways of seeing what is going on. At the same time, there will also be those who prefer to make observations without the interference of labels. For those researchers, there are improved ways to identify molecules by their intrinsic chemical properties, such as the stimulated Raman scattering microscope sitting in a Harvard basement ( page 636 ). All these developments share one thing in common: computers. As tools for the construction, manipulation and distribution of images, whether moving or still, in two-dimensions or three, computers are almost as central to the microscope now as the lens. The startled eye at the eyepiece, as rendered on our cover, may be increasingly a thing of the past, as all that microscopes show comes to be seen on screen. The shock of new discovery, though, will remain \u2014 and perhaps, even, intensify \u2014 for as long as the workings of life become ever more variously and acutely examined.   See Editorial,  \n                     page 615 \n                   , Essay,  \n                     page 642 \n                    and online at  \n                     http://tinyurl.com/microspecial \n                   . \n                     Microscopy special \n                   \n                     Cell imaging: New ways to see a smaller world \n                   Reprints and Permissions"},
{"file_id": "459498a", "url": "https://www.nature.com/articles/459498a", "year": 2009, "authors": [{"name": "Erik Vance"}], "parsed_as_year": "2006_or_before", "body": "Chemist Allen Goldstein has spent his career tracking elusive compounds emitted by trees. Erik Vance joined him for a tour of the woods. Standing almost 20 metres above the forest floor on a scaffolding tower in the Sierra Nevada, Allen Goldstein looks over the spiky tops of a young ponderosa pine forest. At ground level, the air is warm, still and rich with the sweet smell of pine and cedar. But above the forest, a stiff breeze from the west causes the tower to sway disconcertingly. Goldstein inhales deeply before explaining the forest's daily chemical rhythm. At sunrise, the trees start pumping out a complex mix of volatile organic compounds (VOCs), such as pine-scented terpenes. By mid-morning, the westerly breeze adds a dose of the VOC isoprene from oak woodlands about 30 kilometres away. Then, as the sun reaches its peak in the sky, pollution from California's Central Valley makes its way up into the mountains. A chemist at the University of California, Berkeley, Goldstein specializes in interpreting the scents of the forest. He has built his career on finding and characterizing some of the more elusive airborne chemicals in nature. For 10 years at this site near the University of California's Blodgett Forest Research Station ( see map ), he and his team have described more than a dozen plant-released compounds that no one had previously measured or, in some cases, even known existed in the atmosphere. Working at the tops of the trees with ever more sensitive detectors, he has found that forests play a crucial part in the chemistry of aerosol particles and with pollutants such as ozone. His discoveries may help to fill in confounding gaps in atmospheric science, such as how VOCs from plants affect air quality and how they influence climate. \"Where Allen has really made his mark is in new ways of making measurements ? basically seeing things that you have never seen before,\" says atmospheric scientist Inez Fung, a colleague at Berkeley. \n               boxed-text \n             Goldstein, now in his mid-forties, has broad shoulders, a quick smile, and an unpretentious demeanour. He is popular with students and easily becomes animated when talking chemistry. He is also popular with faculty members, although this might have something to do with his hobby as a winemaker. Fung says that recent vintages are \"getting better\" ? other colleagues rave about them. Goldstein came to Berkeley in 1996 from Harvard University, where he developed one of the first devices that could track VOCs from plants continuously throughout the day. Testing his device in Berkeley, it was sensitive enough to pick up the morning spike in airborne caffeine from nearby cafes, as well as traces of marijuana. But Goldstein was not interested in tracking people's vices. His focus was the many types of VOCs emitted by forests. Estimates suggest that the vast majority, perhaps up to 90%, of Earth's VOC output comes from vegetation 1 , a fact infamously alluded to by US president Ronald Reagan, when he said that forests pollute more than cars. Goldstein scowls when asked about that statement, in part because it conflates natural emissions with car exhaust and also because journalists always ask about it. But in a way, Reagan's muddled formulation captures Goldstein's primary interest: the relationship between human pollution and plant emissions. Goldstein often says he went into chemistry not purely for science, but to make a tangible difference to society (he originally planned on being a lawyer). \"I've always tried to pick scientific problems that were interesting and challenging,\" he says, \"but also had relevance to how we manage the world around us.\"  \n                Spare the pores \n              The term VOC is a bit of a chemical catch-all, encompassing everything from exhaust fumes and the stench of solvents to the bouquet of Goldstein's wines. Some VOCs are highly reactive, existing in nature for just moments. Others float around for years. At the Sierra Nevada research site, they were a solution to a mystery. When ozone pollution from more populous regions to the west blows into the forest, a significant fraction of it seems to disappear. For years, scientists thought almost all of this missing ozone went into trees through pores in their leaves called stomata, where it could cause damage. But in his first Blodgett readings in the late 1990s Goldstein quickly discovered that in the summer the trees absorbed at most only a third of the missing ozone 2 . The key, he learned, was that when temperatures increased, more ozone went missing. To Goldstein, this implicated VOCs because warmer weather causes plants to release more VOCs (which is why forests smell more pungent on hot days). He found that some of these VOCs react in seconds, quickly stripping oxygen atoms from ozone 3 . The now oxygen-rich molecules get heavier and stickier, forming aerosols that can create haze over a forest. Joost de Gouw, an atmospheric chemist with the National Oceanic and Atmospheric Administration in Boulder, Colorado, says that Goldstein's work linking VOCs and these aerosols was a crucial breakthrough 4 . \"He was one of the first to start thinking about VOCs and organic aerosols as a whole,\" says de Gouw, leading to what he calls the provocative idea \"that the sources of organic aerosol are much [larger] than we think\". Of the 1.3 billion tonnes of VOCs released by humans and plants every year, conservative estimates say as little as 12 million become aerosol particles 4 . But Goldstein estimates that 150?200 million tonnes end up converted into aerosol. \"That creates quite a chemical soup that we are not keeping track of,\" he says. Furthermore, many of these compounds are good at hiding. For example, estragole (methyl chavicol), a liquorice-like aromatic found in aniseed, is a semi-volatile compound ? at room temperature it could be either a gas or a particle. This unpredictability means that Goldstein has to make one detector able to detect the compound in either state. It also means that the stuff is prone to sticking to the inside of tubes before it can be measured. Once Goldstein and his students worked out how to detect it, they found that it is fairly common in the air around ponderosa pines. Near his site, Goldstein picks a few needles from a ponderosa, crushes them between his fingers, and sniffs. Sure enough, there is an unmistakable hint of aniseed.  \n                Planetary power \n              Now Goldstein is starting to look at the global picture. Atmospheric modellers say that, on the planetary scale, the climatic impacts of aerosols from plants are unknown. Such aerosols may exert a cooling effect, but scientists know little about the strength of that cooling and how it might change in the future. Understanding what is happening is crucial, says de Gouw. \"If models don't have the right emissions going into them, then they don't have the right chemistry and they don't have the right products coming out of them.\" It is difficult to tell how much of the aerosol blanket over a given region comes from plants, and estimates vary widely 5 . Fung, Goldstein and their colleagues looked at aerosols that spike in the southeastern United States during hot summer days. The aerosols could not be explained by man-made sources alone and indicate, just as in Blodgett Forest, that plants are responsible for a large fraction. These aerosols in the southeast block enough sunlight that they cool the region, the researchers reported this month 6 . Although much of his work during the past decade has focused on the Blodgett site, Goldstein is getting ready to say goodbye. This summer, he will conduct his last field campaign there and then will box up his towers and shut down the site. When he first came there, Goldstein was a young professor. Now he is head of his department and well respected in his field. Looking around at the well-used equipment shacks, he seems a little sad. \"It's time to move on,\" he says. \"I am moving out of the mountains and into the valley.\" By this he means he is moving from crisp mountain air to some of the worst air quality in the country. Arvin, a small Central Valley town near which Goldstein might soon set up a tower, exceeds federal ozone levels for a record one-fifth of the year. Goldstein will be looking at the emissions in the Central Valley, hoping to learn where they come from and how to minimize them. It is one of several projects he is starting that might ask uncomfortable questions of the state's farmers. In many regions, VOCs from plants react with nitrogen pollution to form ozone and Goldstein wonders how much of the ozone in the Central Valley is due to the almonds, pistachios, oranges, lemons and grapes grown in the area. These are the questions Goldstein ponders as he inspects cables in preparation for the summer experiments at Blodgett. He says the chemistry of trees and the chemistry of man are linked in ways that scientists are only beginning to understand. But there's no time to dwell on this topic today. He has to get out of the forest, down the hill, and back through the Central Valley before the traffic hits.\n \n                 Erik Vance is freelance science writer in Berkeley, California.  \n               \n                     Nature Reports Climate Change \n                   \n                     Nature Geoscience \n                   \n                     Allen Goldstein's web page \n                   \n                     Blodgett Forest flux tower \n                   Reprints and Permissions"},
{"file_id": "458695a", "url": "https://www.nature.com/articles/458695a", "year": 2009, "authors": [{"name": "Brendan Maher"}], "parsed_as_year": "2006_or_before", "body": "From Antarctic icefish to Galapagos finches, there are some interesting characters at the fringes of developmental biology. Brendan Maher explores a world of alternative model organisms. Debate wears on as to how, exactly, it happened. Some say it could have been a global cooling event brought on by falling greenhouse-gas levels. Others argue that it was the drifting continents, which opened up Drake Passage and kick-started frigid, circumpolar currents. Whatever the reason, to the fish that lived on the Antarctic shelf about 34 million years ago, all that mattered was that it was getting cold, very cold. As the water temperatures plunged by about 5 \u00b0C to below zero over the next few million years, most fish became extinct or moved on to warmer climes. But one group, the Notothenioidei, remained. Thanks to some extraordinary evolutionary innovations, these bottom-dwellers radiated, speciated and ultimately dominated. Crucial proteins shifted shape so they could work at cold temperatures, and a digestive enzyme fragment took on a new role as antifreeze. Because colder waters hold more dissolved oxygen, red blood cells became dispensable, and the 16 species of the Channichthyidae family no longer make them at all. These are the icefish of the Antarctic: clear blooded, up to almost a metre long and with eerie, crocodilian features. To John Postlethwait, though, the icefish are \"beautiful\". It's not just their haunting looks that captivated Postlethwait, a developmental biologist at the University of Oregon, Eugene. It's what they might do for the study of osteoporosis. A quirk of their evolution, he says, may make icefish a valuable model animal for discovering genetic controls on bone density. For decades, developmental biology has been dominated by an established A-list of models including the mouse ( Mus musculus ), fruitfly ( Drosophila melanogaster ), nematode ( Caenorhabditis elegans ), zebrafish ( Danio rerio ), African clawed frog ( Xenopus laevis ), chicken ( Gallus gallus ) and mustard weed ( Arabidopsis thaliana ). It has been rare for scientists' fancies to stray beyond these supermodels. The ease with which the animals can be bred, their small size, fast generation times and the slew of laboratory tools with which to manipulate them make them irresistibly appealing. According to the Thompson Reuters Web of Science, more than 50,000 publications in 2008 mentioned mice, and around 6,000 featured  Drosophila . Icefish, on the other hand, appeared in about 20. Yet just as in advertising \u2014 where hand and foot models ply their modest trades on the basis of singular features rather than overall glamour \u2014 there is a place in science for models that have a specialist role. And the demand for them could grow as new tools make them easier to use. In November 2008, 23 alternative models, including fruit bats, comb jellies, wandering spiders and blind Mexican cave fish were featured in the first volume of  Emerging Model Organisms , a laboratory manual from Cold Spring Harbor Laboratory Press in New York. (Icefish, whose larva is pictured above, haven't yet made the cut.) Alternative models present considerable obstacles: they are often difficult to collect and maintain, and genetic and genomic tools have to be custom-built. \"You end up having to start from square one,\" says Marianne Bronner-Fraser, a biologist at California Institute of Technology in Pasadena and current president of the Society for Developmental Biology based in Bethesda, Maryland. On the other hand, barriers to working with these organisms are less than they were, thanks to rapid, cheap genome sequencing and advances in other techniques for genetic manipulation. \"The demarcation of what makes a good model is beginning to be blurred,\" says Craig Albertson from Syracuse University in New York who studies craniofacial development in African cichlids and, with Postlethwait, has begun work on icefish. As the field of evolutionary development or 'evo-devo' continues to captivate biologists, ever stranger critters may find themselves the subject of attention. Icefish and their ilk are what Albertson and his colleagues call an \"evolutionary mutant model\" \u2014 one in which evolutionary processes have produced characteristics that imitate human disease 1  (see  'The making of a model' ). Researchers can compare the genomes of closely related populations or species to find out how they changed as their bones demineralized, their eyesight deteriorated or another peculiarity arose. By doing so they may reveal the genes or genetic elements that are involved in parallel human processes, such as osteoporosis, blindness or even obesity. Russell Turner, who directs the bone-research laboratory at Oregon State University in Corvallis, says that \"evolutionary mutant models have enormous potential\". But, he adds, this kind of work is \"going to tell us more about how we got to where we are than help us find something that we can make use of for therapy, at least over the short run\". For osteoporosis, the gold standard preclinical model is an ovariectomized rat, which has mammalian physiology and bone loss analogous to that of postmenopausal women. But although the rat is very useful, the story of how it got to be that way is not. The icefish, on the other hand, has a tale to tell. As competitors in the freezing Antarctic waters disappeared millions of years ago, some icefish began to explore niches above the sea floor, something for which they needed buoyancy. They had long ago lost their swim bladders, the air sacs that perform this function in many fish \u2014 and structures rarely re-evolve. Instead, their dense skeletons began to demineralize and soften, even to the point that one can see the outlines of their brains through their translucent skulls. Thus, several species of icefish, in addition to living happily with extreme anaemia, have essentially acquired adaptive osteoporosis.  \n                Cold introductions \n              Postlethwait was introduced to icefish during a talk in 2004 by William Detrich, a marine biologist at Northeastern University in Boston, Massachusetts. Detrich studies their unusual blood development in part to understand human blood diseases such as anaemia. He has been collecting icefish for more than 25 years now, and has fished out a bevy of genes necessary for red-blood-cell development. When the two met again in February 2007, at the 2nd Strategic Conference of Zebrafish Investigators in Asilomar, California, they, Albertson and Pamela Yelick, who studies tooth development at Tufts University in Boston, laid out a plan to investigate the underlying cause of bone loss in icefish. It would be a physical as well as an intellectual challenge: they would have to acquire their specimens during Antarctica's winter when the fish are gravid. But the team suspected that the naturally occurring mutations that have led to adaptive bone loss over the past 34 million years, could teach them something important about osteopenia and osteoporosis. Their US$2.48-million proposal to the National Institute on Aging was accepted on the first try. John Williams, the programme officer on the grant, says that there has been ongoing discussion about bringing new models into the field of ageing, and this seemed to fit the bill. \"Postlethwait's work gets neatly into that and with a very well designed study,\" he says. Postlethwait and the other investigators reason that human and icefish bone loss may have evolved in similar ways. Although strong teeth and bones are necessary early in human life, the pressure to maintain these amenities wanes in later years. For the icefish expanding into niches above the sea bed, the need for strong, heavy bones also lessened. Thus conserved programs involved in bone formation and maintenance may shut off through similar mechanisms, but earlier in the development of the fish than in humans. \"You can use these slow evolutionary changes to study changes that might occur in a human in a lifetime,\" says Yelick. The collaborators have begun by comparing patterns of gene expression between developing icefish embryos and those of a closely related but dense-boned notothenioid species. They have already found delays in the expression of genes involved in bone mineralization. Postlethwait suspects that further work will uncover mutations that regulate the timing at which these bone genes act. As many of the genes are crucial for development, he says \"you can't destroy the gene. But you can destroy the regulatory elements that cause a gene to be expressed in a specific tissue at a specific time.\" If they do identify potentially important genetic elements, the group will turn to model organisms that are easier to work with to test their function by amplifying or knocking them out. Three-spined sticklebacks ( Gasterosteus aculeatus ), for example, are closely related to the icefish but they have a fully sequenced genome and tools available for genetic tinkering. Alas, recent research has suggested that regulatory sequences are less likely than the genes they control to be conserved across evolutionary time, potentially challenging the researchers' ability to translate the results to humans. \"We need to do the work to see,\" says Postlethwait, \"with the constant honest realization that an icefish does not ride a bicycle. Neither does the blind Mexican cave fish ( Astyanax mexicanus ) \u2014 but that hasn't stopped a small group of researchers from looking at them for hints about human conditions. These cave fish, deprived for more than a million years of light and the trophic abundances it brings, have evolved a slew of 'troglomorphic' traits: they are pale, eyeless and have a keen sense of smell and a slow, efficient metabolism. The 29 populations in limestone caves scattered around Mexico can interbreed both with each other and with sighted  Astyanax  populations from the surface. This sets up opportunities to understand the nature of the cave fish's peculiar qualities because researchers can cross together different cave and surface fish in order to track down the genes inherited alongside its oddities.  \n                Sight to the blind \n             Last year, Richard Borowsky, a biologist at New York University, and his collaborators showed that breeding together blind cave fish from two different populations can produce a brood in which about 40% of offspring can see 2 . The results suggest that the parental fish have lost their eyesight through mutations in different developmental pathways, and that two sets of incomplete eye-making instructions can, when combined, make up enough of a readable manual to build a working eye. William Jeffery, an  Astyanax  researcher at the University of Maryland in College Park, says that cave fish may help provide clues about human forms of blindness such as macular degeneration and cataracts. \"The cave-fish lens is one of the first things to decay in the embryo, leading to the loss of the eye,\" says Jeffery. One of the genes implicated is  \u03b1A crystallin , a factor in the lens that prevents apoptosis. It has limited expression during cave-fish development and causes cataracts when mutated in zebrafish 3 . Why cave fish lost their eyesight isn't clear. Some have hypothesized that the energetic cost of eye development and maintenance is so high that selection pressure in the dark environment quickly acts against it. Alternatively, eye loss could simply have arisen through neutral changes in each population that became fixed by genetic drift. Or it could be an inadvertent result of another adaptive change. Still, several populations have converged on this phenotype. Contrast that to albinism, which has happened in almost the same way in various cave-fish population, via a mutation to the  OCA2  gene 4 . This gene makes a protein crucial to development of pigmented 'melanocytes' and happens to be the most commonly mutated gene in human albinism. \"It may be simply that this gene is a frequent target of mutation in all organisms,\" Jeffery says. In addition to living without light, cave fish have evolved to live without much of the bounty that sunlight produces, extracting nutrients from waterborne organisms that slip through the cracks into their limestone caves, or living off the organic matter deposited by overhead bats. Both Borowsky and Jeffery have been studying how the cave-fish metabolism has adapted to the extremely low nutrient level. As it turns out, says Jeffery, they're fatter and more resistant to starvation than their surface-dwelling populations, and Jeffery thinks there could be some parallels to humans. \"We're trying to study it genetically. It would seem that this could be a very good model for obesity,\" Jeffery says. According to some theories, humans are also evolutionarily adapted to survival with little nutrition and this could explain why they easily become obese when food is abundant. Borowsky says his studies, which include an effort to build gene maps for various blind fish populations, have uncovered a variant for the growth hormone GH1 and he is currently sequencing the gene in more populations. Borowsky says of his cave-fish research. \"It's certainly not going to cure blindness tomorrow or diabetes, but it's really relevant.\" Although Borowsky and other cave-fish researchers have attempted to drum up the interest of a high powered sequencing laboratory to sequence  Astyanax  populations, they've so far been unsuccessful. Arkhat Abzhanov, at Harvard University's department of organismic and evolutionary biology, is another researcher whose move to obscure models has forced him to build his own genetic resources. Abzhanov had worked with fruitflies and chickens in his studies of  HOX  genes, which are involved in establishing the animal body plan. But to understand how these genes work in an evolutionary context, he turned to Darwin's finches. The vast phenotypic diversity of the Galapagos finch populations that Charles Darwin marvelled at includes a variety of beak shapes that seem custom built for the food source where a given finch lives: thick heavy bills for cracking seeds, or more elongate bills for probing cactus fruit. By building microarrays for the different species of finch and comparing their gene expression, Abzhanov has been able to get a picture of the genes that are differently regulated in their embryos. He found, for instance, that higher expression of calmodulin, a protein that mediates calcium signalling, is associated with longer beaks. He went back to chickens and used established experimental methods to boost calmodulin signalling in the beaks of embryos. The manipulated chickens recapitulated the finches' elongated beaks 5 .  \n                Better safe than sorry? \n              Abzhanov is also working with African seedcrackers,  Pyrenestes ostrinus , another bird with an assortment of beak shapes. He is currently applying to the National Institute of Dental and Craniofacial Research in Bethesda, Maryland, for a grant to explore whether comparisons of these birds can identify genes and regulatory pathways that sometimes go awry in human craniofacial development, resulting in conditions such as cleft palette. \"They're interested because this is a story of naturally occurring variants which genetically change the integration of craniofacial components,\" says a hopeful Abzhanov. Not everyone finds funding agencies receptive though. Bronner-Fraser says that some of her best postdocs are interested in alternative models, but \"a lot of times they come and realize that it might be difficult to get jobs and money and they might branch out to more traditional models\". Working with alternative models also requires passion and patience. Abzhanov can't order the animals he needs online. He must go on lengthy field trips to the Galapagos Islands, taking great care not to disrupt their breeding and placing mock eggs for every egg he takes from a nest. Postlethwait and Detrich arguably had it harder, spending weeks in Antarctica in the middle of its sunless winter, trawling for notothenioids. Postlethwait was able to bring back developing embryos, but he found that maintaining the fish in captivity is extremely difficult. \"You can have a freezer at minus 20 and you can have a fridge at plus 4. But the temperatures in between, especially temps at around 0 are very hard to maintain,\" Postlethwait says. Many of the embryos hatched after about six or seven months, he says, when they should take nine. For many researchers, the adventure is part of the attraction. \"I think that the people who work on the non-traditional models are often people who want to get out of the lab and into the real world,\" says developmental biologist Scott Gilbert at Swarthmore College, Pennsylvania. Gilbert, who works on shell development in red-eared terrapins, says they're a \"horrible system to use if one wants reproducibility or to have research material at any given day\". But he persists because turtles naturally turn soft tissues into bone and so hold broader lessons for these types of transition from one cell type to another. Gilbert says he is taking cues from human biology to inform his studies on turtle development. Research on fibrodysplasia ossificans progressiva (FOP) \u2014 a rare and devastating human genetic disorder in which normal muscle tissue turns into bone \u2014 has revealed some of the key proteins involved in the turtle shell development program that he studies. Frederick Kaplan, an orthopaedic surgeon at the University of Pennsylvania, Philadelphia, had discovered mutations in patients with FOP affecting the expression of bone morphogenetic protein. Similar programs seem to direct extension of bony plates from turtle ribs in forming the shell. \"I think we're benefiting in a way from his work more than he is from ours,\" says Gilbert. When it comes to the Antarctic icefish, it is unclear who, if anyone, will benefit: the researchers trying to lift them out of frozen obscurity, or the people with osteoporosis who could share some part of their biology with these fish. For Yelick though, who has worked with mouse, zebrafish and human cells prior to her work on icefish, expanding the menagerie is invigorating. \"Getting these interactions between evolutionary biologists, molecular biologists, stem-cell biologists and tissue engineers, it's just a really exciting time to be working in this field.\" \n                 Brendan Maher is  \n                 Nature \n                 's Research Highlights editor. \n               \n                     Nature's Darwin Special \n                   \n                     Turtle Power at the Society for Developmental Biology \n                   \n                     The Choice of model organisms in evo-devo from Nature Reviews Genetics \n                   \n                     Icefish educational information from Northeastern University \n                   \n                     Astyanax genetics resources from the Hubbard Center for Genome Studies \n                   \n                     Abzhanov lab at Harvard University \n                   Reprints and Permissions"},
{"file_id": "458142a", "url": "https://www.nature.com/articles/458142a", "year": 2009, "authors": [{"name": "Melissa Lee Phillips"}], "parsed_as_year": "2006_or_before", "body": "Could out-of-sync body clocks be contributing to human disease? Melissa Lee Phillips reports. Ten years ago, researchers reported on three families with an extreme 'lark' problem. Larks are people who naturally wake up early in the morning, and are the opposite of 'owls', who wake up and go to sleep late. But many of the members of these families had particularly acute larkness, waking up on average around 4 a.m. and falling asleep around 7.30 p.m. 1 . The researchers later found that the families, who were diagnosed with familial advanced sleep-phase syndrome (FASPS), all carried mutations in a gene called  PER2 , which is involved in setting the human body clock 2 . By some estimates, more than half of the population in industrialized societies may have circadian rhythms that are out of phase with the daily schedule they keep. Such people are said to have 'social jet lag' \u2014 a term coined by Till Roenneberg of Ludwig Maximilians University in Munich, Germany. Some of these are larks, some owls, and some have pretty standard human rhythms that are disrupted by shift work or travel. In modern societies, circadian-rhythm disruptions can arise from simply spending too much time indoors, something that can make such workers decidedly \"owlish\", Roenneberg says. Even the one-hour time change made by many countries at this time of year can take some adjusting to. If larks and owls are forced to follow normal schedules, they run into all kinds of problems with disabling insomnia and sleepiness. But disrupted rhythms could have graver consequences than that. In 2007, an expert working group at the World Health Organization's International Agency for Research on Cancer in Lyon, France, concluded that \"shift-work that involves circadian disruption is probably carcinogenic to humans\" 3 , after reviewing the existing evidence. Equally strong conclusions have been drawn from evidence that links circadian-rhythm problems to psychiatric disorders, metabolic syndrome and a range of other illnesses. Researchers now are working to understand those links. Some suspect that health problems arise from a third kind of jet lag \u2014 one that arises when the circadian rhythms in different body tissues lose synchrony with each other. In 2006, the European Commission started funding EUCLOCK, a \u20ac16-million (US$20-million), five-year project involving some 34 researchers whose goal is to understand how the circadian clock synchronizes with cycles in the environment. In particular, the researchers are trying to work out what type of schedules are the healthiest fit for individuals' biological clocks. \"People have been researching this for 50 years,\" says Anna Wirz-Justice at the Centre for Chronobiology in Basel, Switzerland, \"but I think the methods are only now coming up to address this properly.\" In nearly all organisms, patterns of biochemistry, physiology and behaviour oscillate with the daily cycles of light and dark, often with near-perfect timing. People forced to live in a 28-hour cycle still show fluctuations of almost exactly 24 hours in their core body temperature and levels of the hormones melatonin and cortisol 4 . In mammals, many of these cycles are directed by a 'master clock' in the brain's hypothalamus called the suprachiasmatic nucleus (SCN). The SCN receives information from the retina about light and coordinates rhythmic cycling of gene expression in the rest of the brain and body through neural signalling and hormones. Cycles of gene expression in the gastrointestinal tract, for example, ensure that digestive acids and enzymes are produced at the appropriate times. In the past decade, researchers have identified some of the genes responsible for timekeeping, showing that expression of  CLOCK ,  BMAL1 ,  PER   and  CRY   rises and falls over 24 hours in the SCN and elsewhere, and that mice with mutated versions of these genes abandon the 24-hour cycle for one that is shorter, longer or has no pattern at all. Such animals were \"the key development that brought the field to its present exciting position\", Wirz-Justice says, because it suggested that these were 'master genes' directing the clock and the physiological processes that follow it.  \n                Typecasts \n              But for some individuals, the cycles of gene expression and behaviour do not adhere well to the cycles of night and day. The tendency to be a lark, an owl, or somewhere in between is referred to as individual's 'chronotype', and although it may shift over the course of a person's lifetime \u2014 adolescents and young adults tend to be more owl-like than either children or older adults \u2014 it doesn't usually change in comparison with peers and it is thought to be determined largely by genes. FASPS was the first human circadian disorder linked to a mutation in a specific clock gene. Not all such genes have been easy to find: researchers have found no simple mutation that accounts for people with the owlish delayed sleep-phase syndrome (DSPS), who can have trouble falling asleep before 6 a.m. and waking up before 2 p.m.. People with DSPS and FASPS often also have depression 5 ,   6 , and this and other psychiatric conditions, such as bipolar disorder and schizophrenia, are commonly associated with abnormalities in circadian rhythms. \"The vast majority of people with major depression have sleep abnormalities and interestingly it can be that they sleep too much or they have insomnia and can't sleep,\" says Colleen McClung of the University of Texas Southwestern Medical Center in Dallas. This connection raises a cause and effect question: are the circadian-rhythm disorders causing the depression or the other way around? In 2007, McClung and her colleagues found some support for the former idea when they studied mice that lack a working  Clock   gene and, they observed, exhibit symptoms of mania and hyperactivity that can be reversed by the mood stabilizer lithium 7 . \"Every way we test them, they look like bipolar patients in the manic state,\" McClung says. Circadian rhythms are known to affect the most basic of metabolic pathways, including protein synthesis, glycolysis and fatty-acid metabolism. And many patients who have circadian-rhythm disorders caused by shift work also develop gastrointestinal and metabolic disorders such as glucose intolerance, diabetes and high blood pressure, says Theodore Bushnell, a neurologist at the University of Washington Medicine Sleep Disorders Center in Seattle. \"It seems like there's nobody who just has a shift-work issue.\" The conclusion that shift work can be carcinogenic has grown largely from epidemiological work. A nationwide study in Denmark, for example, found that women who work mainly at night for at least six months are 1.5 times more likely to develop breast cancer than those who work regular hours 8 . Researchers suggest that the raised cancer risk could be because these people's cells start to divide at the wrong time and run amok, an idea supported by some cell-culture studies. So far, these links haven't provided the details that biologists would like. Much like diet or stress, circadian rhythms affect so many cellular and physiological functions that it is extremely difficult to pinpoint a mechanism by which a given alteration to these rhythms could contribute to a particular disease. Much of the current work involves genetic association studies, in which scientists look for gene variants that pop up more often in people affected with a disorder than in those without it. Some such studies for psychiatric diseases, for example, have pulled out variants in clock genes. But researchers have also been making progress by dissecting the machinery of the various human body clocks. Since the late 1990s, it has become apparent that the body has 'peripheral circadian oscillators' in tissues outside the SCN. These peripheral clocks receive input from the SCN, but are also influenced by other time-keeping signals. In 2004, a team led by Joseph Takahashi of the Center for Sleep and Circadian Biology at Northwestern University in Evanston, Illinois, measured cycles of gene expression in different mouse tissues in culture when placed in constant darkness. He found that the kidney cells followed a clock of 24.5 hours, whereas the corneal cells ran at about 21.5 hours 9 . \"We used to think the clock was just in the brain \u2014 it was a neural process and the body just passively followed,\" Takahashi says. \"But that's clearly not the case. You have to view your body as a whole collection of different clocks.\" Like Greenwich Mean Time for the body, the SCN serves as a reference point for the peripheral clocks, and they usually run in sync. But sometimes \u2014 after stepping off a long-distance flight, for example \u2014 the synchrony breaks down. When simulating jet lag in rats, researchers at the University of Tokyo found that the SCN resets to local time in around one day based on light signals, but peripheral oscillators can take more than a week to adjust 10 . \"So, during jet lag, your body is literally completely out of sync,\" Takahashi says. \"Each organ shifts at a different rate.\" Could it be that loss of synchrony between the body's tissues underlies some of the health problems seen in circadian-rhythm disorders and shift workers? To test this, researchers have used genetic tools to create an artificial mismatch between the central clock and a peripheral one. In one study 11 , researchers knocked out the gene  Bmal1   in the liver of mice, effectively disabling the clock in that organ. When the animals developed low blood sugar for parts of the day, the researchers knew that the brain clock was not sufficient to maintain glucose levels \u2014 a working peripheral clock was needed too. The SCN is synchronized during the daytime by light, whereas liver metabolism is also synchronized by food intake, Roenneberg says. So perhaps shift-workers have gastrointestinal problems because their liver and intestinal tracts are gearing up for a meal at the wrong time. \"You can easily imagine that this is not exactly optimal for the system,\" he says.  \n                Out of time \n              A mismatch between central and peripheral clocks has been linked to other health conditions too. Ongoing work by Christian Cajochen at the Centre for Chronobiology in Basel, suggests that the peripheral clocks of women with depression are not as well linked to sleep\u2013wake cycles as they are in those without the condition. \"Women with depression have a greater degree of variability in the timing of different physiological and endocrine rhythms,\" Wirz-Justice says. Some researchers in the EUCLOCK consortium are planning to work out how the SCN is able to keep peripheral clocks and human physiology in time. In some experiments, mice will be put onto 'shift-work' schedules in which they are forced to 'work' and eat during the day rather than at night. The researchers will then examine how the schedule affects behaviour and the synchronization of various local body clocks. In fact, peripheral oscillators have already proved to be a useful research tool. Last February, researchers from Steven Brown's lab at the University of Zurich and Achim Kramer's group at Charit\u00e9 Hospital in Berlin identified 11 larks and 17 owls based on a 'morningness\u2013eveningness' questionnaire and then measured their molecular rhythms from the expression of  Bmal1   in their skin cells 12 . The team wanted to find out why larks and owls naturally adopt the schedules that they do. For some people, the researchers found what they expected: cells from larks showed shorter periods than those from owls. But they also found that about half of the larks and owls actually had 'normal' circadian period lengths. The researchers found that in this group, the owls had skin clocks that were more difficult to reset than those of people with more typical schedules, and that the larks had clocks that were easier to reset. This suggests that individual differences in chronotype result not just from innate differences in circadian period length but also from differences in how easily a person's rhythms can be synchronized to the night\u2013day cycle \u2014 and that some larks and owls have clocks that are not reset normally each day. So perhaps these people's peripheral clocks stray from the central one easily.  \n                Therapy options \n              With few firm mechanisms to go on, the question now is how to go about 'treating' circadian-rhythm disruptions. Intense-light therapy has been used to shift undesirable sleep schedules back to a more normal pattern. And Wirz-Justice says that researchers are working on other behavioural or pharmaceutical ways to alter circadian rhythms. Ramelteon, for instance, is a drug used to reduce the effects of insomnia by mimicking the action of melatonin \u2014 a hormone that tells the body it is sleep time. But no one knows yet whether such interventions will also prevent or reduce some of the other health risks associated with circadian-rhythm disruptions. Roenneberg is not convinced that it would be easy to solve even conventional jet lag with drugs. \"The trouble is that we are still much too naive to have pharmacological success without messing something else up in the system,\" he says. That means it could be up to individuals to consider their chronotype before choosing their schedule, or making other decisions that might affect their health. EUCLOCK researchers are developing ways to measure clock-gene expression from cheek swabs or other tissue that could provide a quick laboratory read-out of a person's chronotype. (Roenneberg says that so far these results agree with self-reported chronotype based on questionnaires.) Chronotype might be taken into account when administering drugs or medical tests: performing a blood test at 8 a.m. can yield completely different results on a morning person, who has been up for hours, than it will on an evening one. \"You can get very different clinical blood values based on their chronotypes without anything being meaningfully different, because in one person, the system is up, and in the other, it's still down,\" says Roenneberg. Working with EUCLOCK and another consortium called CLOCKWORK, Roenneberg and his colleagues are developing a computer model that works out a person's ideal schedule on the basis of his or her chronotype and sleep needs. The group is starting to test the model's predictions by asking people of each chronotype to try out a certain schedule in their daily lives, and measuring its effects in a range of physiological, behavioural and cognitive tests. They will then use their findings to refine the model. By repeating this process several times, Roenneberg says, \"we are confident that we will finally put some sense into how to do shift-work properly \u2014 that is, with the fewest side effects on health\". Being a lark or owl should not, in itself, be a problem, Roenneberg adds. \"Chronotype per se should have no health effects whatsoever. Health effects come from having to live against one's own clock.\"\n Melissa Lee Phillips is a freelance writer based in Seattle, Washington. \n                     Sleep Insight \n                   \n                     EUCLOCK \n                   \n                     Chronotype questionnaire \n                   \n                     Clockwork \n                   Reprints and Permissions"},
{"file_id": "458274a", "url": "https://www.nature.com/articles/458274a", "year": 2009, "authors": [{"name": "Geoff Brumfiel"}], "parsed_as_year": "2006_or_before", "body": "Science journalism is in decline; science blogging is growing fast. But can the one replace the other, asks Geoff Brumfiel. John Timmer's slide into journalism was so gradual even he can't put his finger on the point at which he stopped being a researcher. He started reading Internet websites and message boards a decade ago, while he was working as a postdoc in a developmental neurobiology lab at the Memorial Sloan-Kettering Cancer Center in New York. One day, one of his favourite sites, Ars Technica, announced that it was looking for someone to help with its science coverage. It was 2005, and a school board in Dover, Pennsylvania, had gone to court over the promotion of intelligent design. \"I thought, wow, it really feels like the public has completely lost touch with what science is all about,\" says Timmer. \"So I basically e-mailed the existing author and volunteered.\" Over the next few years Timmer's work on the site grew steadily, while his research career stalled. Today the 42-year-old draws a full-time salary as Ars Technica's science editor. He works with writers echoing his earlier experience: graduate students and postdocs type up brief summaries on research in their areas of expertise during down time and lunch breaks. The write-ups are more technical than you might read in a newspaper \u2014 a recent post included a lengthy discussion on 'functionalizing' cells to bind them together with DNA \u2014 but that's fine, Timmer says. The idea is to provide people already interested in science with greater insight into how research works. A typical posting can earn a writer anywhere from the price of a pair of movie tickets to around US$100, and that is often incentive enough for young academics. Timmer's tale is emblematic of a shift in the way science meets the media. In part because of a generalized downturn, especially in newspaper revenues, the traditional media are shedding full-time science journalists along with various other specialist and indeed generalist reporters. A  Nature  survey of 493 science journalists shows that jobs are being lost and the workloads of those who remain are on the rise (for full results see  http://tinyurl.com/c38kp6 ). At the same time, researcher-run blogs and websites are growing apace in both number and readership. Some are labours of love; others are subsidized philanthropically, or trying to run as businesses.  \n                It's a blog world \n              Traditional journalists are increasingly looking to such sites to find story ideas (see '\"Rise of the blogs\":#a'). At the same time, they rely heavily on the public-relations departments of scientific organizations. As newspapers employ fewer people with science-writing backgrounds, these press offices are employing more. Whether directly or indirectly, scientists and the institutions at which they work are having more influence than ever over what the public reads about their work. \n               Click here for larger image \n               [anchor a] The amount of material being made available to the public by scientists and their institutions means that \"from the pure standpoint of communicating science to the general public, we're in a kind of golden age\", says Robert Lee Hotz, a science journalist for  The Wall Street Journal . But that pure standpoint is not, or should not be, all that there is to media coverage of science. Hotz doubts that blogs can fulfil the additional roles of watchdog and critic that the traditional media at their best aim to fulfil. That sort of work seems to be on its way out. \"Independent science coverage is not just endangered, it's dying,\" he says (see  'Vox media' ). What's more, the amount of material available is not a good proxy for its reach. Press releases and blogs will not find the same broad audience once served by the mass media, says Peter Dykstra, who was executive producer of CNN's science, technology, environment and weather unit until it was closed down last year. Now at the Woodrow Wilson International Center for Scholars, an independent think tank in Washington DC, he says that science and environment news will be \"ghettoized and available only to those who choose to seek it out\". Science journalism boomed in the 1980s and early 1990s. In the United States \u2014 where by 1989 some 95 newspapers had dedicated science sections \u2014 and elsewhere, the field's precipitous rise was supported by buoyant profits in the media sector. \"The model of a major paper was that they did really serious science coverage,\" says Deborah Blum, who won a 1992 Pulitzer Prize for her reporting in the  Sacramento Bee  on the use of animals in research, and who now teaches at the University of Wisconsin at Madison. But there was a problem with the science sections, she says. \"They didn't make money.\" Most papers were willing to support their sections, even at a loss, because science was the thing to have. Today, in a harsher mass-media landscape, that has changed. Across the United States, newspaper science sections have been shut down: this month  The Boston Globe  stopped running its weekly science and health section. Nor is the written word the only casualty, as the closure of Dykstra's seven-person unit at CNN indicates.  Nature 's survey shows that, of those working in the United States and Canada, one in three had seen staffing cuts at their organization (see  'Hiring practices' ). \n               Click here for larger image \n               The European industry has not yet reached the level of crisis seen in the United States, says Holger Wormer, a professor of science journalism at the University of Dortmund in Germany. Many newspapers in Germany are considering staff cuts but, at the moment, science journalists are faring relatively well. \"Science departments are still small but they are regarded as quite important,\" he says. Because larger German papers such as  Frankfurter Allgemeine Zeitung  have science sections, smaller papers are willing to support their own science coverage, at least for now. In France, declining circulations are also creating problems, according to St\u00e9phane Foucart, a science writer at  Le Monde . In the past six months,  Le Monde  has scaled back its science coverage. Newspapers and broadcast outlets in the United Kingdom are also under pressure, and science and environmental jobs are among those that have been lost. Unsurprisingly, among the science reporters who remain, the workload is on the rise.  Nature 's survey reveals that 59% of journalists have seen the number of items they work on in a given week increase over the past five years. They are not just doing more reporting, but more types of reporting. Many are now being asked to provide content for blogs, web stories and podcasts \u2014 something they weren't doing five years ago.  \n                Fast and dirty \n              Under these straitened conditions the mainstream media's need for quick and accurate science content is being met primarily by public-relations departments, according to Fiona Fox, director of the Science Media Centre, an organization in London that supplies journalists with scientific information ( Nature 's editor-in-chief, Philip Campbell, sits on the Science Media Centre's board, and the Nature Publishing Group provides support for it). Mark Henderson, science editor for  The Times , based in London, says that he tries to avoid relying solely on releases \"as much as possible\", but \"if there's a good press release and you've got four stories to write in a day, you're going to take that short cut\".  Nature 's survey shows press releases to be a top source of story ideas for science journalists, with 39% routinely quoting from them directly. This demand for stories and ideas has been matched by an increase in supply. In Britain as in the United States, contraction in the media has made jobs in public relations particularly attractive for students at science-writing programmes. \"You'd be amazed at the diversity of places for science communicators,\" says Blum. Government agencies, universities, museums and non-governmental organizations have all hired her students, she says \u2014 almost all of whom are finding jobs, despite the woes of the traditional media. The Science Media Centre demonstrates the new opportunities that exist now. It was started in 2002 by an amalgam of non-commercial and commercial interests seeking to influence the public debate on news topics such as genetically modified foods. What began as a relatively modest attempt to connect journalists to sources of scientific expertise has expanded dramatically over the past seven years. Today, the centre's six-person staff sends out daily e-mails filled with quotes from prominent scientists on the latest news that end up in tomorrow's stories. It has also begun providing fact boxes and background documents that journalists can insert directly into their coverage. Fox is happy at the centre's success, but uneasy too. Ideally, she says, science journalists should be picking up the phone and talking to scientists directly: \"We are successful because of a serious problem in journalism, and it's not one to be celebrated.\"  \n                Straight to the masses \n              As journalists become more dependent on scientific public relations, scientists themselves have begun reaching out to mass audiences through the Internet. Such outreach is not new; but unlike books and lectures, science blogs operate with a quick turnaround that more closely resembles that of the traditional media. The most successful sites are drawing hundreds of thousands of visitors each month. Many of those blogs were started by scientists who simply wanted to reach the public with information about their research. \"I'd always find that people were interested in what I did,\" says Derek Lowe, a researcher with Vertex Pharmaceuticals in Cambridge, Massachusetts, and author of In the Pipeline, a blog about drug discovery and the pharmaceutical industry. \"Most people have no idea how drugs are actually found,\" he says. Lowe started his blog in early 2002, and now it regularly draws around 200,000 page views a week. Paul Myers, a biologist at the University of Minnesota in Morris, says that he started his blog Pharyngula \"largely out of boredom\", but now that he gets more than half-a-million weekly page views, he sees it as a valuable tool for talking to a public audience. Myers freely admits that his readers \"are not just there for the science\" \u2014 his attacks on religion are a mainstay of the blog's appeal. But he certainly considers himself a source of scientifically reliable information for his readers. Although science blogging did not start off as a business, there are attempts to make it one. Since 2006, the publisher of  Seed , a magazine about science, has gathered more than 100 science blogs \u2014 including Pharyngula \u2014 on a range of topics on to a single website, ScienceBlogs, and now pays its bloggers on the basis of how many hits their posts receive. Fabien Savenay, a senior vice-president for marketing at Seed Media Group in New York, declines to say whether the blog site makes money for the organization. But, he says, the project \"has been a successful franchise for us in that it has great traffic and engagement\". Another US magazine,  Discover , has recently been amassing a smaller but impressive stable of bloggers, too. Other magazines, such as  WIRED , prefer a more journalistic approach to blogging, using a team of reporters on their science blog to provide a pace, range and quality of posting no individual could match. Bloggers with a science background, like bloggers on most other topics, often demonstrate open scorn for the mainstream media (MSM in blogspeak). \"You get a press release that is slightly rehashed by somebody in the newsroom and it goes in the paper! It's wrong, its sensationalist, it erodes the public trust in scientific endeavour,\" says Bora Zivkovic, author of A Blog Around the Clock on ScienceBlogs and an online community manager for the Public Library of Science journals. Myers takes a similar view. \"Newspapers realize that they can get their audience by peddling crap instead of real science,\" he says. Not surprisingly, those who came to blogging from journalism \u2014 such as Carl Zimmer, who writes for a range of publications, including  The New York Times , and blogs at  Discover  \u2014 tend to disagree. But Larry Moran, a biochemistry professor at the University of Toronto, Ontario, who blogs at Sandwalk, seemed to speak for many bloggers when he recently wrote \"Most of what passes for science journalism is so bad we will be better of [sic] without it\". While journalists such as Zimmer expand their mainstream work into their blogs, bloggers with roots in the lab are moving into print. Myers will soon contribute a regular column to the  Guardian  newspaper in the United Kingdom. Derek Lowe now writes regular columns for  The Atlantic  and the trade magazine  Chemistry World  (both have also written for  Nature ). This work, though, tends towards opinion and analysis, not reporting. \"Bloggers don't want to be journalists,\" says Zivkovic. \"I want to write on my blog whatever I want. I may write a post about a new circadian paper, but the next eighty posts are about politics or what I ate for breakfast.\" Despite his distaste for how the trade is practised, he thinks that there will always be a need for professional journalists covering science. \"Somebody has to actually be paid to write about things as they come out,\" he says. That is what John Timmer is looking for new ways to do at Ars Technica. But there is a problem: the online world, both in its bloggier reaches and elsewhere, is polarized; people go to places they feel comfortable. Many of the people that Timmer originally hoped to reach when writing about intelligent design and the Dover trial probably go elsewhere for their news, he says, because \"it's easy for somebody to pick their news sources based on their politics, and get that version of scientific issues\". Dykstra worries that in a more fragmented media world, \"environmental news will be available to environmentalists and science news will be available to scientists. Few beyond that will pay attention.\" Others worry about the less questioning approach that comes with a stress on communication rather than journalism. \"Science is like any other enterprise,\" says Blum. \"It's human, it's flawed, it's filled with politics and ego. You need journalists, theoretically, to check those kinds of things,\" she says. In the United States, at least, the newspaper, the traditional home of investigations and critical reporting, is on its way out, says Hotz. \"What we need is to invent new sources of independently certified fact.\"  \n                Culture mash \n              Two Ivy League giants, Princeton University in New Jersey and Yale University, are trying to do something about the problems they see in environmental coverage with websites aimed at generating scientifically accurate news coverage. \"We're bringing something new to the table,\" says Roger Cohn, a veteran journalist who now edits the Yale Environment 360 website, which is funded in part by the William and Flora Hewlett Foundation and the John D. and Catherine T. MacArthur Foundation. The site is home to reports by journalists and opinions by scientists on subjects such as climate change, but it has \"no axe to grind on any one of these issues\", says Cohn. At the Princeton University website, Climate Central, the focus is mainly on video material. \"We're just in the initial stages of preparing a weekly series of news stories about climate based on papers in journals,\" says Michael Lemonick, a long-time science writer for  Time  magazine who now works at the site. As well as appearing on Climate Central, he says, the stories will be offered to the websites of big media outlets; some of the group's work has already been aired on the Public Broadcasting Service's evening news show  The NewsHour with Jim Lehrer , which reaches millions of viewers. Climate Central is funded by the Flora Family Foundation and The 11th Hour Project, a non-profit organization supporting climate awareness, based in Palo Alto, California. Lemonick says his new job requires him to listen more closely to researchers. \"If they say, 'you really left out this important fact,' I don't get to say, 'Sorry it's my story',\" he says. That doesn't mean that researchers make his story into a dry scientific paper, he adds. \"They have to recognize the needs of the journalist, but we have to recognize the needs of the scientists. We're kind of fusing the two cultures.\" Timmer's path has also led him to a fusion of science and journalism. In May, media giant Cond\u00e9 Nast acquired Ars Technica, and he was brought on full-time. \"When I'm interacting with press officers or researchers, I'm acting as a journalist,\" he says. \"I don't think anybody would consider me a working scientist any more.\" But when asked how he sees the scientists writing for him, he becomes more philosophical: \"Basically, however they see themselves.\" \n                 See Editorial, page  \n                 \n                     260. \n                   \n                  Full survey data are available  \n                 \n                     here. \n                   \n               \n                     National Association of Science Writers \n                   \n                     ScienceBlogs \n                   \n                     Blogs at Discover Magazine \n                   Reprints and Permissions"},
{"file_id": "458278a", "url": "https://www.nature.com/articles/458278a", "year": 2009, "authors": [{"name": "Katharine Gammon"}], "parsed_as_year": "2006_or_before", "body": "The switch to electronic medical records opens up a potential wealth of data for researchers, if major obstacles can be overcome, reports Katharine Gammon. In May 1999, the US Food and Drug Administration (FDA) approved a promising new drug. Earlier clinical trials of 5,000 patients had convinced regulators that the drug was a safe and effective means of reducing pain and inflammation in arthritis. Produced by the pharmaceutical giant Merck, the new medication became an instant hit and nearly 80 million people worldwide had taken this wonder pill by 2004. During that same period, however, evidence emerged that the painkiller, known as Vioxx (rofecoxib), significantly raised the risk of heart attacks. By the time Merck pulled the drug from the market in September 2004, \"an estimated 88,000 to 140,000 excess cases of serious coronary heart disease probably occurred in the USA\", according to a study led by an FDA scientist and published in the  Lancet  in 2005 (ref.  1 ). The drug may have caused tens of thousands of deaths in the United States alone, the researchers estimated. The digital revolution \u2014 and the switch from paper to electronic medical records \u2014 might prevent many of these kinds of deaths in the future. In a 2007 study 2 , researchers from the Children's Hospital Informatics Program (CHIP) in Boston, Massachusetts, attempted to see how quickly they could have spotted a problem with Vioxx using digital records. By homing in on two major hospitals, the researchers found an 18% jump in heart attack cases within eight months of Vioxx being introduced, which could have provided an early warning about the drug, says Isaac Kohane, an author of the study and director of CHIP. The excess cardiac cases disappeared when Vioxx was pulled from the market. The Vioxx incident shows one benefit that could emerge when health systems dump old-fashioned paper medical charts in favour of electronic records. Some countries, such as the United Kingdom, have already made the transition and the United States is moving in that direction. This trend could have profound implications for how doctors treat patients and how researchers pursue scientific questions that rely on medical data. \"In the long run, the opportunities are huge \u2014 actually transformative,\" says Ashish Jha, assistant professor of health policy at the Harvard School of Public Health in Boston. \"When the exchange of health information flows freely between doctors, we can't imagine how huge that's going to be.\" But it will take time. The move towards electronic medical records has proceeded slowly so far, and many practical challenges still lie ahead. When Jha and his colleagues surveyed US physicians in late 2007 and early 2008, they found that just 4% have a full electronic records system, and only 13% have even a basic digital system 3 . \"The organization and delivery of health care hasn't changed in nearly a hundred years,\" says Jha. That may soon change. The stimulus package that recently passed in US Congress contains US$19 billion to promote the switch to digital health records, mostly through economic incentives for doctors and hospitals to make the transition. The lofty goal is to have all health records stored and accessed electronically by 2014. President Barack Obama has promoted this policy as a way to control soaring health-care costs. An estimated $81 billion could be saved annually if 90% of doctors used electronic systems \u2014 by reducing redundant care, speeding up patient treatment and stemming mistakes and handwriting foul-ups. And the switch could cut more than just costs. By reducing the number of medication errors, electronic systems can save lives.  \n                Digging for data \n              Proponents of electronic medical records say that researchers could be among the chief beneficiaries of the conversion to digital health data. The fundamental argument for using electronic records in scientific studies is that most doctors don't see enough patients to recognize patterns. Only on the level of hundreds of thousands or millions of records do connections become apparent. What's more, research can be slow when it comes to recruiting patients, evaluating them and checking in over a long period of time, but all that information is available just a few clicks into an electronic record. But critics argue that current health files are too unstructured, too unreliable and that algorithms for sifting doctors' notes are not yet available. There's also the question of the type of information in the record. \"Depending on what you want to study, that may or may not be consistently documented,\" says Samuel Skootsky, medical director of the Medical Group and Faculty Practice Group at the University of California, Los Angeles. Kohane counters that there is enough valuable information in records to justify mining them. What's more, the gigantic amount of patient data available means researchers can choose the individuals to study. For example, investigators looking at asthma could pluck out true asthmatics from people who wheeze owing to other factors, he says. For research purposes, he says, observations in medical records more accurately reflect clinical practice. \"If you study a drug and you look at its effect on a control group, that's a very pure study,\" says Kohane. \"But everyone taking that drug might be also taking two other drugs related to their condition\", which could alter the effects of the medication being studied. That information is not always available to researchers conducting trials, but electronic medical records might help sort out the multi-drug effects. In a test of that potential, Kohane's group is using health records to look at other drugs on the market. The team is searching for effects that might have been missed in earlier tests of the medications. Even more power might emerge by combining clinical records with a genetics database. Michael Swiernik, director of medical informatics at the University of California, Los Angeles, says that the combination would allow researchers to find the right people for a study in a speedy, cost-effective way. Other research affirms that it is possible to dig into electronic records for useful clinical data. A study released in February showed that data captured in electronic records could yield the same results as data from clinical trials when evaluating the efficacy of a drug 4 . The data came from Britain's general practice research database, which holds about 10% of the records of general practitioners \u2014 mostly relating to demographics and pharmaceutical usage. The problem is that these UK databases are small, incomplete and are not yet linked together. Before researchers can truly tap digital records, the databases first have to be created and filled. That will not happen if the primary users reject them, as happened at the Cedars\u2013Sinai Medical Center in Los Angeles in 2002. After undergoing months of training with a new electronic records and communications system, doctors complained that it was too cumbersome. One day, the staff revolted, ending the hospital's $34-million effort to convert to an electronic system after just three months. Cedars\u2013Sinai hasn't been the only institution to face a rebellion. In the United States, electronic medical records spark such fierce resistance in part because those who benefit most from these systems \u2014 the insurance companies and billing department \u2014 aren't the same as the people who have to pay for the new software and training. For a medium-sized hospital, these systems can cost tens of millions of dollars just for software \u2014 with no guarantee of a return on investment for the hospital. As nearly half the doctors in the United States practise either alone or with one partner, the cost is extreme: $32,000 per physician for installation and $1,500 per month for running the system, according to estimates. When a hospital or doctor goes out to buy a system, they face another challenge: there are hundreds of vendors who each want to sell a different, proprietary product, many of which will not survive. In addition, medical records systems often have problems sharing data with each other. Taken together, these issues \"make doctors and hospitals very nervous to purchase the technology\", says Jha. That apprehension can extend to using the systems as well: studies show that doctors' efficiency actually drops 10\u201320% during the first year of using an electronic system. Then there's the thorny issue of privacy. Moving electronic patient information around between doctors, pharmacies, laboratories and billing departments means many people have access to sensitive information. That raises the risk of a large security breach. Medical-record information sells for more on the black market than credit-card information, because it is more identifying, says Swiernik. \"Security is only as good as its weakest link, and the likelihood of weak links is really pretty high,\" says Lee Tien, a lawyer at the Electronic Frontier Foundation, a digital-rights public-interest group based in San Francisco, California. He is particularly concerned about data security because the transition to electronic health records is happening so quickly. \"If you're not hard-wiring privacy and security into the DNA of the infrastructure, there will be unfortunate data security breaches.\" Despite these challenges, some institutions are successfully using and benefiting from electronic medical records \u2014 especially large organizations that act as the payer as well as the provider of health care. Two of the largest systems are VistA, the open-source system of the Department of Veterans Affairs (VA), and Kaiser Permanente's system HealthConnect. Kaiser Permanente's non-profit health plan has 8.6 million members with full electronic medical records, and nearly half of them use the online interface, where they can view part of their records. But those systems have yet to benefit researchers substantially. Because of privacy regulations, Kaiser Permanente, based in Oakland, California, limits access to its clinical records mostly to researchers within the organization. The VA has also been slow in research. \"The VA has had electronic data for 10 years now, but only a small amount of stuff is available for digital clinical research,\" says Jha, adding that the technology and the level of comfort with anonymized data sharing hasn't yet caught up to the amount of data available. The largest electronic medical-records system in the world is Britain's National Health Service (NHS), serving more than 60 million people. But the digital data amassed by UK doctors is not yet comprehensively linked up. That will change in the next five years, by which time electronic records will connect 30,000 general practitioners and include data on all UK residents. Despite that push into the digital age, the opportunity for research has been missed, says John Powell, associate professor in epidemiology and public health at the University of Warwick, UK. \"There's a massive debate going on right now about consent \u2014 whether patients are opting in or opting out of a research system. Doctors for the most part are arguing that people have to give consent to opt in, civil servants in the Department of Health are saying they have to opt out. From the research point of view, we want to have maximum people and maximum data in.\" General practitioners tend to be wary of sharing patient data, even if they are de-identified. That concern grew recently, when some officials suggested that the NHS could sell data to pharmaceutical companies to raise funds \u2014 a proposal that prompted strong criticism. Powell says many of the issues will be resolved this year, when the NHS finalizes its official constitution \u2014 a sort of terms-of-service agreement for using the system. He argues that the system should be used for research. \"In return for getting completely free health care for everything, there are certain things you have to sign up for as a resident \u2014 and one of those things should be secondary use of your data for the anonymous NHS research, but not for pharmaceutical companies,\" says Powell.  \n                Security fears \n              Britain has faced major data security problems in the past few years, so people are understandably concerned about the safety of their medical information. But there are rumblings of change: the creation in February of the National Institute for Health Research, a framework agency within the NHS, should make it easier for anonymized data to move within the system. Other European nations haven't decided what to do with their records, either. In Germany, doctors have local electronic health records, but no way to connect them to a larger system. France has a national system to hold and access electronic medical records, although clinicians can view only the part of the record that is relevant to them. Some researchers see an alternative to large, expensive systems in something closer to Apple's iPhone, with its panoply of available applications. Kohane says that the best system would be a platform available with different inter-operable, substitutable programs. If open-source platform systems aren't built because of the nature of the vendors, Kohane sees a future in the expansion of personal health records \u2014 computerized records maintained by patients, not institutions. Models of personal health records already exist in systems such as Google Health, Microsoft's HealthVault and Indivo, which was built by CHIP researchers. They can all store patient data imported from doctors' offices or laboratories, and give control of the data to the patient. When it comes to scientific research, Kohane says personal records could connect patients directly with researchers without hospitals or physicians as intermediaries \u2014 even offering the anonymity required by most review boards. Such systems could fix a major problem in medical research. Right now, people who participate in studies do so anonymously, and the results are not usually transmitted back to them. Kohane and his colleagues are testing an approach later this year that will allow for more communication in one Boston clinic with 5,000 patients, and they are adding more clinics in 2010. Patients will sign up at their health-care institution and upload clinical or genomic information to an encrypted personal health record to be shared with researchers. Then patients could choose to 'tune in' to a certain type of announcement regarding their records. If results came out that matched their genomic profile, patients would get a message about that work. They could also sign up for getting news on particular conditions, such as cancer or diabetes. And researchers could communicate with patients without knowing their identities, allowing an ongoing relationship. \"The current state of research is of self-inflicted mutual ignorance,\" says Kohane. \"Patients are committing to not knowing more about the study, and researchers can't find out more information about the patients.\" All that could change if health systems can get past the substantial hurdles. In a world where information is at our fingertips, and even the US president has his thumbs on a BlackBerry, electronic medical records are coming, along with the research opportunities they hold. For many scientists, that transition is just what the doctor ordered.\n \n                 See Editorial, \n                 page 259 \n                 Katharine Gammon is a freelance writer based in Santa Monica, California. \n               \n                     Nature Medicine \n                   \n                     Children's Hospital Informatics Program \n                   \n                     Google Health \n                   \n                     Microsoft Healthvault \n                   \n                     IndivoHealth Personally Controlled Health Record \n                   Reprints and Permissions"},
{"file_id": "458401a", "url": "https://www.nature.com/articles/458401a", "year": 2009, "authors": [{"name": "Roberta Kwok"}], "parsed_as_year": "2006_or_before", "body": "When an asteroid was spotted heading towards our planet last October, researchers rushed to document a cosmic impact from start to finish for the first time. Roberta Kwok tells the tale. Around midnight on 6 October 2008, a white dot flitted across the screen of Richard Kowalski's computer at an observatory atop Mount Lemmon in Arizona. Kowalski had seen hundreds of such dots during three and a half years of scanning telescope images for asteroids that might hit Earth or come close. He followed the object through the night and submitted the coordinates, as usual, to the Minor Planet Center in Cambridge, Massachusetts, which keeps track of asteroids and other small bodies. When the sky began to brighten, he shut down the telescope, went to the dorm down the mountain and fell asleep. The only thing that had puzzled Kowalski about the midnight blip was the Minor Planet Center's response to his report. Its website posted the discovery right away but when he tried to add more data, the system stayed silent. Tim Spahr, the Minor Planet Center's director, found out why the following morning. The centre's software computes orbits automatically, but this asteroid was unusually close to Earth. \"The computer ran to me for help,\" says Spahr. He did some quick calculations on Kowalski's data to figure out the path of the asteroid, which was now named 2008 TC 3 . \"As soon as I looked at it and did an orbit manually, it was clear it was going to hit Earth,\" he says. The brightness of 2008 TC 3  suggested it was only a few metres across and, assuming it was a common rocky asteroid, would probably split into fragments soon after entering the atmosphere. But safe as that might seem, Spahr had procedures to follow. He called Lindley Johnson, head of NASA's Near Earth Object Observations programme in Washington DC, on his BlackBerry \u2014 a number only to be used in emergencies. \"Hey Lindley, it's Tim,\" said Spahr. \"Why would I be calling you?\" Johnson's response: \"We're going to get hit?\" Spahr also called astronomer Steve Chesley of the Jet Propulsion Laboratory (JPL) in Pasadena, California, who at the time was hustling his kids out of the door for school. Chesley hurried into the office, ran a program to calculate the asteroid's orbit and \"was astounded to see 100% impact probability\", he says. \"I'd never seen that before in my life.\" Chesley calculated that the asteroid would hit Earth's atmosphere less than 13 hours later, at 2:46 UT the next day; the impact site would be northern Sudan, where the local time would be 5:46 a.m.. He sent his results to NASA headquarters and the Minor Planet Center, which circulated an electronic bulletin to a worldwide network of astronomers. A group called NEODys in Pisa, Italy, also confirmed that an impact was nearly certain. Although several small objects such as 2008 TC 3  hit Earth each year, researchers had never spotted one before it struck. Kowalski's discovery, therefore, provided a unique chance to study an asteroid and its demise in real time, if astronomers could mobilize resources around the world quickly enough. Soon e-mails and phone calls were flying across the globe as scientists raced to coordinate observations of the incoming asteroid. \"IMPACT TONIGHT!!!\" wrote physicist Mark Boslough of Sandia National Laboratories in Albuquerque, New Mexico, to colleagues, including a Sandia engineer responsible for monitoring US government satellite data.  \n                Countdown to impact \n              Peter Brown, an astronomer at the University of Western Ontario in Canada who heard the news from JPL, ran to his local observatory, fired up the telescope and began tracking the asteroid, which looked like \"a very small, faint, fast-moving streak\", he says. Alan Fitzsimmons at Queen's University Belfast in Northern Ireland called two of his colleagues, who had just arrived at the William Herschel Telescope at La Palma on the Canary Islands and were not scheduled to use the telescope until the next day. \"Listen guys, this is happening, this is going to happen tonight,\" he told the researchers, who arranged to borrow an hour of observing time from another astronomer. All day, observations poured into the Minor Planet Center, which released new data and orbit calculations several times an hour. NASA notified other government agencies, including the state and defence departments, and issued a press release that afternoon saying that the collision could set off \"a potentially brilliant natural fireworks display\". About an hour before impact, the asteroid slipped into Earth's shadow and out of view to optical telescopes. By then, astronomers from 26 observatories worldwide had already captured and submitted about 570 observations, allowing JPL to refine its predicted collision time to 2:45:28 UT, give or take 15 seconds. \n               boxed-text \n             As the countdown progressed, Jacob Kuiper fretted. Kuiper, an aviation meteorologist on the night shift at the Royal Netherlands Meteorological Institute in De Bilt, had seen an e-mail about the incoming asteroid. And he was worried that no one would see the explosion in the sparsely populated Nubian Desert. With less than 45 minutes left, Kuiper realized he could notify Air France-KLM \u2014 the airline to which he routinely issued weather reports \u2014 which probably had planes flying over Africa. About ten minutes later, pilot Ron de Poorter received a message print-out in the cockpit of KLM flight 592, flying north from Johannesburg to Amsterdam. The message gave the latitude and longitude of the predicted asteroid impact. De Poorter calculated that he would be a distant 1,400 kilometres from the collision. Still, at the appointed time he and his co-pilot dimmed the instrument lights and peered northeast. Far above the plane, asteroid 2008 TC 3  hit the top of the atmosphere at about 12,400 metres per second. The collision heated and vaporized the outside of the rock, ripping material from its surface. The impact of rock atoms with air molecules created a brilliant flash that lit the desert below. Less than 20 seconds after 2008 TC 3  entered the atmosphere, calculations suggest, pressure on the rock triggered a series of explosions that shattered it, leaving a trail of hot dust. From the cockpit of his plane, de Poorter saw flickerings of yellowish-red light beyond the horizon, like distant gunfire. The flash woke a station manager at a railway outpost in Sudan. In a village near the Egyptian border, people returning from morning prayers saw a fireball that brightened and flared out, according to accounts collected later by researchers. Electronic eyes watched, too. US government satellites spotted the rock when it was 65 kilometres above the ground. Moments later, it was picked up by a European weather satellite, which caught two dust clouds and light from the fireball. An array of microbarometers in Kenya normally used to monitor for nuclear explosions detected low-frequency sound waves from the blast, which Brown later calculated would be equivalent to about 1\u20132 kilotonnes of TNT, roughly one-tenth the size of the atomic bomb dropped on Hiroshima. Tracking of the fireball's trajectory by US satellites showed that JPL accurately predicted the object's location within a few kilometres and a few seconds. \"We have never had such a concrete affirmation that all the machinery works,\" says Chesley. But for Peter Jenniskens, an astronomer at the SETI Institute in Mountain View, California, the spectacular light show was not enough. For weeks after the asteroid hit, Jenniskens, who studies meteor showers, waited to hear whether someone had found the fallen meteorites. No news emerged. \"Somebody needed to do something,\" he says. Jenniskens flew to Sudan in early December and met with Muawia Hamid Shaddad, an astronomer at the University of Khartoum who had already obtained pictures of the fireball's trail from locals. Together, they drove north from Khartoum to the border town of Wadi Halfa, asking villagers where the fireball had exploded in the sky. These eyewitness accounts convinced Jenniskens that the rock had disintegrated high in the atmosphere \u2014 in good agreement with US satellite data \u2014 and that any fragments were most likely to be found southwest of Station 6, a tiny railroad outpost in the Nubian Desert.  \n                Desert search \n              On 6 December 2008, Jenniskens and Shaddad set out with a group of 45 students and staff from the University of Khartoum to scour the area. Team members lined up about 20 metres apart over a kilometre-wide strip, facing a sea of sand and gravel interspersed with hills, rocky outcrops and dry winding riverbeds. Flanked by two pairs of cars and trailed by a camera crew from news network Al Jazeera, the line of searchers began marching slowly east, like the teeth of a massive comb being dragged through the desert. Towards the end of the day, a car approached Jenniskens with news that a student might have found a meteorite. \"I remember thinking, 'oh no, not again',\" says Jenniskens, who had already fielded several false alarms. Still, he jumped in the car and drove to the student, who presented him with a small square fragment, about a centimetre and a half across with a thin, glassy outer layer. The surface resembled the crust that meteorites form after being melted and solidified, and the rock's deep black colour suggested it was freshly fallen. It was the team's first meteorite \u2014 and the first time that scientists had ever recovered a meteorite from an asteroid detected in space (see  Nature   458,   485\u2013488;  2009). The next day, the team walked 8 kilometres and found 5 meteorites, all very dark and rounded. On the third day, a trek of 18 kilometres yielded larger meteorites nearly 10 centimetres across. A few weeks later, a team of 72 students and staff found 32 more, and the most recent field campaign, completed in March, brought the tally to about 280 fragments weighing a total of several kilograms. Jenniskens couriered a sample to Mike Zolensky, a cosmic mineralogist at the NASA Johnson Space Center in Houston, Texas. Examining the rock, Zolensky discovered that it contained large chunks of carbon and glassy mineral grains resembling sugar crystals. Tests at other labs confirmed that the sample was a ureilite, a type of meteorite thought to come from asteroids that have melted during their time in space. Only 0.5% of objects that hit Earth yield fragments in this category. But 2008 TC 3 's pieces are strange even for ureilites: they are riddled with an unusually large number of holes, says Zolensky. \"It boggles the mind that something that porous could survive as a solid object,\" he says. The findings suggest that 2008 TC 3  broke from the surface of a larger asteroid, as the pores would have been crushed if they were near the rock's centre, says Zolensky. He suggests that future studies of the meteorites' chemistry could help reveal the history of its parent asteroid. Moreover, the new finds might eventually yield clues to how planets form, he says, because the asteroid had melted during its history, a process that young planets go through. 2008 TC 3  gave astronomers a rare chance to connect a dot in the sky with rocks in their hands. \"We have a lot of meteorites on the ground and a whole lot of asteroids up there, and forging a link is not easy,\" says Don Yeomans, manager of NASA's Near-Earth Object Program Office at JPL. Jenniskens and his team concluded the asteroid belonged to a group called F-class asteroids. These asteroids reflect very little light, and scientists had been unsure what they were made of. The new evidence \"opens a huge window\", says Glenn MacPherson, a meteorite curator at the Smithsonian Institution in Washington DC, who was not involved in the studies of 2008 TC 3 . Although not all F-class asteroids may be the same, he says, the data suggest at least some of them may contain the same material as ureilites, such as carbon and iron. Clark Chapman, a planetary scientist at the Southwest Research Institute in Boulder, Colorado, says the connection between F-class asteroids and ureilites does not surprise him. But, he adds, \"this is a proven link and we don't have many of those\". Scientists have tried to figure out the composition of asteroids by studying how they reflect various wavelengths of light and matching these features to meteorite samples in the lab. But such connections are often tenuous unless the reflection signature is very distinct. The most secure example is an asteroid called 4 Vesta, which has been associated with a group of igneous meteorites. No missions have yet returned asteroid fragments to Earth, although a NASA spacecraft orbited the asteroid Eros for a year and landed on it in 2001. Japan's Hayabusa mission attempted to collect a sample from the asteroid Itokawa in 2005; scientists will find out whether it succeeded when the spacecraft returns next year. Knowing what asteroids are made of will be crucial if we ever need to deflect one, says Yeomans. NASA aims to provide decades of warning if any killer asteroids are headed for Earth so that a strategy can be devised to avoid a collision. That strategy will differ for various asteroids, which can range from \"wimpy ex-cometary fluffballs\", to solid rock, to slabs of nickel-iron, says Yeomans. With the advent of new surveys, scientists could spot objects hurtling towards Earth more frequently. Today's surveys have found almost 90% of near-Earth objects with a diameter of 1 kilometre or larger, says Yeomans, but smaller rocks can easily slip by unnoticed. Discovering 2008 TC 3  was like finding \"a man in a dark grey suit 50% farther away than the Moon\", says Kowalski, who is part of the Catalina Sky Survey, an effort that discovers 70% of all the near-Earth objects found every year. The detection rate will increase with the next generation of surveys, perhaps up to a few Earth-bound asteroids per year, says Alan Harris, a planetary astronomer at the Space Science Institute who is based in La Canada, California. The Panoramic Survey Telescope and Rapid Response System (Pan-STARRS) in Hawaii will officially begin observations with its prototype system this year, and the Large Synoptic Survey Telescope in Chile is scheduled to begin full operations in 2016. In the meantime, Kowalski and his colleagues are still on the job. The night after spotting asteroid 2008 TC 3 , Kowalski headed back up Mount Lemmon, heated his dinner and settled down in the telescope's control room. As his discovery plunged towards the desert on the other side of the world, Kowalski was surveying another part of the sky, waiting for the next white dot. \n                 Roberta Kwok is a news intern in  \n                 Nature \n                 's Washington DC office. \n               \n                     NASA Near Earth Object Program \n                   \n                     Catalina Sky Survey \n                   \n                     JPL's account of the event \n                   \n                     Peter Jenniskens' homepage \n                   Reprints and Permissions"},
{"file_id": "458398a", "url": "https://www.nature.com/articles/458398a", "year": 2009, "authors": [{"name": "Daniel Cressey"}], "parsed_as_year": "2006_or_before", "body": "The only way to meet the increasing demand for fish is through aquaculture. Daniel Cressey explores the challenges for fish farmers and what it means for dinner plates in 2030. Sitting in an unremarkable family restaurant a short drive from his institute in Stirling, UK, Randolph Richards scans the menu's seafood offerings with an expert eye. \"The salmon is probably farmed in Orkney,\" he says, referring to an archipelago north of mainland Scotland. \"The sea-bream \u2014 probably grown in Greece.\" Thus, the head of the University of Stirling's Institute of Aquaculture reveals a secret that most diners are blissfully unaware of: farmed fish are everywhere. Roughly every other morsel of fish passing through human lips was raised under human supervision. Right now, more than 50 million tonnes of glassy-eyed livestock are corralled in underwater cages and tanks, crowded, fed, sometimes dosed with antibiotics and ultimately culled and shipped up to half-way around the world to meet the ever-growing demand for fish. It's the world's fastest growing food sector. From decadent fatty tuna belly savoured in the most expensive sushi joints to a workhorse frying fillet such as tilapia, farmed fish are becoming the norm rather than the exception, even if customers are unaware of the subtle change on their plates. Back in the 1970s only 6% of the world's food fish came from aquaculture. By 2006, that proportion had risen to almost half, according to the biennial State of World Fisheries and Aquaculture report released last month by the Food and Agriculture Organization of the United Nations (FAO) 1 . And to keep up with world appetites, the fish-farming industry will have to continue this trend. The reason is simple, says Rohana Subasinghe, a senior fishery resources officer at the FAO. \"We are not going to get adequate fish from the sea in the coming years.\" Current projections suggest that by 2030 the world's population will have exceeded 8 billion people. Maintaining today's consumption rates, of around 17 kilograms per person per year would require an extra 29 million tonnes of fish. Meanwhile, around half of all fish stocks have been deemed \"fully exploited\" by the FAO, with those deemed \"overexploited, depleted or recovering\" now around 30%. As a result, the fishing industry is casting about for anything even vaguely palatable. Although some consumers profess an aversion to farmed fish, claiming that they're bland, uninteresting or unnatural, many would choose a boring farm-raised salmon over fresh-caught pelagic delicacies such as jellyfish and krill. \"There's only so much wild stock out there,\" says Michael Rubino, manager of the aquaculture programme run by the National Oceanic and Atmospheric Administration in Silver Spring, Maryland. Most of the demand will have to be met by aquaculture, but what those future meals will look like depends very much on how much science can contribute to the trade. Predators, such as salmon and cod, are popular and command high prices at market. But their carnivorous diets rely on the same fish stocks that are under threat around the world. Tilapia \u2014 omnivorous cichlids \u2014 are fairly simple to raise. As one of the fastest growing aquaculture products, fish such as these may represent the future.  \n                From paddies to pools \n              Aquaculture has been practised in China for many thousands of years. Carp were left to grow in ponds and rice paddies and later harvested. This passive, pastoral method, with little or no attempt to actively nurture the animals, is still practised widely. China produces 67% of the world's farmed seafood, much of it carp, and much of it through these generally low-tech methods. \n               boxed-text \n             Another short drive from his institute, and Richards is showing off a landmark piece of fish-farming's technological lurch forward. The Howietoun fish farm, built in the 1800s by a Victorian landowner, is still a commercial farm today. Brick-lined ponds, set on a gentle slope, teem with brown trout waiting for a few scoops of food and destined to stock lakes for sport fishing. When the fish in the uphill ponds reach a predetermined size, they are shuttled to the next via connecting channels. The modern descendants of Howietoun have been attempting to drive up yields, along with profits, through the help of institutions such as the one Richards heads. Starting with veterinary science and progressively incorporating more and more disciplines, ranging from ecology to genetics, these research centres have helped to dramatically increase yields, but in doing so, they have created new problems. The quaint, idyllic farm at Howietoun \u2014 where interference from otters and herons represent the extent of environmental conflict \u2014 is a far cry from modern industrial farms, which can involve miles of cages off the coast or huge collections of tanks on land. The most dramatic change in aquaculture has perhaps been the explosive growth in shrimp farming in southeast Asia. Encouraged by high demand and foreign investment, more than 109,000 hectares of mangrove swamp have been cleared for shrimp farming in the Philippines, for instance, since farming started in earnest in the 1970s. That's roughly two-thirds of the nation's area for these unique watersheds. Furthermore, the nutrients added to many fish farms or produced as waste, particularly nitrogen and phosphorous, can trigger massive plant and algal blooms in surrounding waters. As the blooms die off, the bacteria taking part in the bonanza of decomposition suck the oxygen out of the system leaving it dead. \n               boxed-text \n             These environmental effects have caused many to oppose the idea of aquaculture, which the FAO says threatens future development in many regions. In response, farmers have been asking researchers to advise them on how much waste the surrounding sea can take. But farmed fish might not ever be a sustained replacement for caught fish: many farmed fish species rely on caught fish for food, in particular the ground-up solid fishmeals and liquid fish oil. According to the FAO's statistics 2 , aquaculture used 56% (3 million tonnes) of world fishmeal production in 2006 and 87% (800,000 tonnes) of fish oil production. Recent research 3  from Albert Tacon, of the Hawai'i Institute of Marine Biology in K\u0101ne'ohe puts this even higher, at 3.7 million tonnes of meal and 840,000 tonnes of oil. Although the proportion of wild caught fish fed to farmed fish is small when set against the global total of capture fisheries, it is still a sticking point if the rationale for fish farming is to relieve pressure on overfished oceans.  \n                Strange chickens \n              Of the top seven most heavily farmed fish by weight, five are carp species, which generally require less food supplementation than other species. But carp is generally unfamiliar to Western diets. Number eight on the list has been quickly rising up the ranks and in the consciousness of Westerners:  Oreochromis niloticus , the Nile tilapia. Tilapia \u2014 affectionately dubbed the aquatic chicken for their speedy and efficient growth \u2014 are to many a nearly perfect aquaculture species. The fish grow fast and are not choosy about where they live or what they eat. They occupy a low position on the food chain, so there's little opportunity for mercury to build up in their flesh \u2014 as is the case for some predatory species \u2014 and their flavour is sweet and inoffensive. Although high-capacity tilapia farms once led to rapid breeding and disappointingly small adults, interbreeding species or using hormones can create single-sex broods, effectively taking the brakes off the population's rapid growth. Production boomed from next to nothing in the 1970s and 1980s to more than 2 million tonnes by 2007. \n               boxed-text \n             But much like the land-based avian namesake, these aquatic chickens are sneered at by many fish fans for their bland ubiquity. Westerners favour animals higher up the food chain: salmon, tuna and striped bass, among others. These animals have more rarefied tastes than the grubbing tilapia, and as they fetch a high price, farmers are keen to keep them on the menu. To this end, some of the sea's carnivores have adopted a more vegetarian-like diet. In many farms, salmon, the predator farmed most heavily, have become accustomed to a diet of at least 25% soya bean, supplemented with fishmeal and fish oil at crucial times in their growth. \"Even though they look like fish they are almost more similar to pigs,\" says Carlos Duarte of the Mediterranean Institute for Advanced Studies in Mallorca, Spain. Other salmon may eat even more soya beans but such diets will reduce the levels of healthy omega-3 oils that have been a major selling point for salmon. Solutions in the pipeline include soya-bean crops that have been genetically modified to put these oils back in, and possibly even modifying the fish themselves. Traits such as fast maturation can be introduced through selective breeding. Other traits, such as disease resistance, will be harder to obtain without resorting to genetic modification, says Eric Hallerman, head of the Department of Fisheries and Wildlife Sciences at Virginia Tech in Blacksburg. The US Food and Drug Administration (FDA) is considering approval of an Atlantic salmon that has been modified with the gene encoding for growth hormone from a Pacific Chinook salmon ( Oncorhynchus tshawytscha ). The addition of the gene could cut the time taken to reach market size by between a third and a half. The decision has been in the works for a long time 4 , however, and there are no indications of a quick resolution. \"I'd be really surprised if the FDA went forward before we give in our final report [on ecological risk assessment], which will be next September or October,\" says Hallerman, who is leading the work. And there are additional costs to consider. The big issue here, as with all genetically modified organisms, is preventing the fish escaping into the wild, where they may outcompete local fish, or where the modified genes could transfer to wild stocks. To avoid this, Hallerman says that the modified salmon should only be grown onshore in tanks. At present, nearly all farmed salmon are grown in cages at sea, which puts onshore farmers at a disadvantage. However, some in the fish-farming community say that this is the best place to farm fish. By using a series of graduated tanks \u2014 small tanks for hatchery up to large 'grow out' tanks \u2014 and recirculating water with pumps it is possible to grow dinner inside what is basically a warehouse. It also removes the variability that comes with being open to the ocean, allowing every stage of a fish's development to be carefully monitored and controlled, and helping to ensure a healthy fish for the market. Although such technology costs money, some people using these systems are already making money, and any toughening up of regulations for outdoor aquaculture could make the recirculation systems even more attractive. \"Twenty years ago people would have told you we can't raise fish on land in recirculation systems. Ten years ago they would have said we can't raise them and make a dollar,\" says Hallerman. \"People laughed at me 10 years ago when I said this, they didn't take it seriously. Now they're like 'show me how it works on a spreadsheet'.\" But many people think the real opportunities lie in the deep ocean. Most fish farms are confined to the narrow strip of water near the coast, but advocates of 'open ocean' farming say that going farther out to sea is the way forwards. \"On the one side there is unlimited potential,\" says James Diana, from the School of Natural Resources and Environment at the University of Michigan in Ann Arbor, \"but it's also very expensive. It is a question of which of those two will win out.\"  \n                Out to sea \n              Going offshore would remove many of the problems of near-shore farms: water quality is generally higher and there are fewer conflicts with recreational water users. But the open ocean can be fierce, and farms will need to be engineered much more heavily than they are now. Then there is the tricky problem of licensing. In the United States, for instance, there is no regulatory system for licensing fish farms in federal waters, so farms are limited to the 3 nautical miles (around 5.6 kilometres) off-shore that fall under state control. \"We currently do not have a regulatory framework for issuing permits for aquaculture in federal waters,\" says Rubino. \"There are a few companies using offshore technologies in open ocean conditions in state waters, but none in offshore (federal) waters,\" he explains. \"It's not suitable for all species and all locations but it's certainly something that many have recommended we look at going forward.\" Although proponents think that being out at sea may cause less problems than being close to shore, some environmentalists have been fiercely opposed to open-ocean systems 5 . For Duarte, however, the opponents are missing the greater picture of food production. \"We don't need to occupy a major fraction of the oceans to grow sufficient food. We have transformed 50% of the surface of the continents into crop lands and graze lands and yet we can probably [make] do with much less than 10% of the surface of the coastal ocean.\" Consumer demand will probably push the technology as far as it can go, and in the West that means focusing on marine predators. But Duarte says that could change with the global food shortages. \"Then the demand will be for mass production of food, and not so much for specific elements of quality.\" The farm at Howietoun will still probably be producing trout in 20 years. The restaurant nearby may still be serving farmed fish, be it genetically modified salmon, humble tilapia, tank-farmed tuna, or even carp. If those who are partial to the taste of fish are lucky, an extra 30 million tonnes of fish will be on the market. Dinner won't be coming from the sea though, at least not in the traditional sense. The future is farmed. \n                     Institute of Aquaculture, University of Stirling \n                   \n                     Food and Agriculture Organization \n                   \n                     NOAA aquaculture programme \n                   Reprints and Permissions"},
{"file_id": "458568a", "url": "https://www.nature.com/articles/458568a", "year": 2009, "authors": [{"name": "Declan Butler"}], "parsed_as_year": "2006_or_before", "body": "Undergraduate textbooks are going digital. Declan Butler asks how this will shake up student reading habits and the multi-billion-dollar print textbook market. The rumble of textbooks thumping on to the desks of a university lecture theatre, the rustle of turning pages, the groan of backpack straps hoisting 10 kilograms of textbooks \u2014 these sounds may soon be an echo of the past. This semester, 1,200 students at the University of Texas at Austin (UTA) are foregoing printed textbooks in a pilot trial of Amazon Kindle e-readers stuffed with texts in electronic form. At NorthWest Missouri State University (NWMSU) in Maryville, classes are testing textbooks on Sony e-readers, as well as on the students' own laptops, as part of plans to roll out e-textbooks across all courses within 5 years. The list goes on: within the past 18 months or so, as textbook publishers have begun to make more and more titles available online, universities worldwide have begun to experiment with e-textbooks. \"E-textbooks are not yet mainstream \u2014 but they are on the edge of a breakthrough into the mainstream,\" says Kevin Hegarty, UTA chief financial officer. Indeed, textbook publishers are scrambling to position themselves for a revolution in the way they do business as they rethink their decades-old model of massive, printed tomes sold at premium prices. The resulting proliferation of new models \u2014 none of which is yet a sure winner \u2014 is being shaped by the interplay of at least three forces: new e-readers and displays for viewing and interacting with the e-textbook content; new business and licensing models for delivering quality content at prices students and universities can afford; and new concepts for the content itself, and for how it is created.  \n                Beyond black and white \n              On the hardware front, e-textbooks are reaping the benefits of rapid innovation in electronic readers for documents and novels. Most of the latest generation of e-readers, such as Amazon's Kindle 2 and Sony's PRS-700, offer displays based on technology from the E-Ink Corporation of Cambridge, Massachusetts (see  Nature   doi:10.1038/news.2009.202;  2009). These displays produce text and images that rival the brightness and clarity of ink on paper, which makes reading them far more comfortable than reading text on the liquid crystal display screens of laptops and desktop computers. They also allow an e-reader's batteries to last for days: the displays require power only when the screen is being changed \u2014 for example, by 'turning' a page. The first generation of such e-readers, launched less than three years ago, has already sparked mass uptake of e-books, and they could potentially do the same for e-textbooks. As delivery vehicles for textbooks, however, existing e-readers still leave a lot to be desired. For example, most are designed for reading books from beginning to end. But \"very few students read a textbook in that manner\", says Paul Klute, who is directing the NWMSU e-textbook project. He recalls how the school launched its pilot test of the Sony's PRS-505 reader in autumn 2008 with e-textbooks from six publishers. It was an instant flop with the 200 student testers. They wanted to do what they had always done, says Klute, and flip through to find bits they didn't grasp in the lecture, or dip in to read short sections, or find a key figure. But the e-reader wasn't built for this, so they ended up frustrated. This semester, Sony has replaced the device with the newer PRS-700. Its search and navigation functions and the ability to flip a page by swiping a finger across the touch screen have elicited a much more positive response, Klute says. Another drawback of current e-readers is that they have small black-and-white displays, just a little larger than 9 by 12 centimetres. This makes them unsuited to most science textbooks, which typically have large pages and colourful graphics. \"The market is not likely to expand until the e-readers improve,\" says Hegarty. Many large textbook companies are holding off from experimenting with e-readers until that happens. But manufacturers promise that big screen, colour e-readers are on the way within a year or two. If so, this will be the tipping point at which e-textbooks take off, predicts Hegarty. \"It will be a big leap forwards,\" he says. If the price is right. Dedicated e-readers currently start at prices of around US$350, points out Joe Esposito, a digital-media consultant and former chief executive of  Encyclopaedia Britannica  online. Reading an e-textbook on a laptop might not be as easy on the eyes, but most students already own a laptop \u2014 complete with a colour display. \"The student laptop will prove a potent competitive entry barrier to other devices for reading e-textbooks,\" says Esposito. This is why NWMSU is also piloting e-textbooks on laptops among 500 students in 11 disciplines in an effort to compare how well students learn with e-readers, laptops and print textbooks. That is probably a wise approach. Five years ago, devices such as the Kindle did not even exist. Which devices students will use for reading e-textbooks five years from now is anybody's guess \u2014 although many people are betting on some sort of convergent evolution among e-readers, laptops, portable music players and smart phones. The boundaries will increasingly blur, predicts Neelan Choksi, co-founder and chief operating officer of Lexcycle, a company based in Portland, Oregon, that makes Stanza, a popular e-book reader application for the iPhone. \"Everyone is racing to be the ultimate multi-function device,\" he says.  \n                Kindling a revolution \n              But device innovation has other implications as well. Just as the Internet brought dramatic change to the music industry, which relied on selling content on a physical medium, such as the CD, better devices could similarly disrupt the textbook industry. So it is not surprising that textbook publishers' embrace of e-textbooks is reminiscent of two scorpions mating. Like the music industry, textbook publishers have been reluctant to put content online because of concerns about piracy, and the risk that it might undermine sales of their traditional print editions. If they are now willing to do so, it is largely because such concerns have been offset by the realization that e-textbooks may give them a way to cut into the largest threat to their profits: the huge market for second-hand textbooks. Thanks to the Internet, what was once the preserve of local used bookstores is now a vast and sophisticated international online market. The US market for new textbooks is estimated at around $5.5 billion, but the parallel market for used books is around one-third of that, says Esposito. Publishers hope that by offering lower priced e-textbooks they can obliterate the used-textbook market, from which they currently get nothing, and sell electronic versions semester after semester \u2014 presumably with frequent updates, analogous to the new print editions they regularly bring out. But publishers' enthusiasm for e-textbooks remains relative, says Esposito. \"E-textbooks are too big a market for publishers to walk away from, but publishers are not willing to walk away from the print market that makes up more than 90% of their sales.\" This defence of the print market is reflected in their offerings, which are usually electronic facsimiles of printed textbooks, sold to students online, and which provide only the most basic functionality, such as printing, highlighting and making electronic annotations. By far the largest market for textbooks is the United States, and the companies that win in this space are also likely to be those that will dominate worldwide. Because of this, it is also likely to be where the evolution of e-textbook business models plays out. The biggest player is CourseSmart, a consortium in Belmont, California, created by the five publishers who together account for roughly 85% of the global print textbook market: Pearson; Cengage Learning; McGraw-Hill Education; John Wiley & Sons; and the Bedford, Freeman & Worth Publishing Group. (The last is a unit of Macmillan, which is owned by  Nature 's parent company, the Georg von Holtzbrinck Publishing Group based in Stuttgart, Germany.) \"We have brought a critical mass of textbooks together on a single common platform for the first time,\" says Sean Devine, chief executive of CourseSmart. CourseSmart sells its e-textbooks at about half the price of its print versions, and so far has made more than 5,800 e-textbooks available at its website, or about one-third of the world's most popular textbooks. Students who buy the books are constrained by digital rights management. The copy they buy usually 'expires' after their course has ended, after which it no longer accessible. CourseSmart's digital rights management also forbids students from moving a book downloaded on one computer to another device, limits printing to 10 pages at a time, and allows the whole book to be printed only once.  \n                Bulk buying \n              Nonetheless, student purchases of CourseSmart e-textbooks are growing rapidly, says Devine. A survey by NWMSU in February found that, all things being equal, about half the students would prefer print textbooks and about a quarter would prefer e-textbooks, whereas the remainder had no strong feeling. But when asked what they would do if buying a textbook themselves, almost 80% said they would opt for the cheaper e-textbook offering. Ongoing tests of CourseSmart e-textbooks by the University System of Ohio show that they reduce costs \u2014 the average US student forks out some $900 annually on print textbooks \u2014 and students using them perform just as well as when using paper versions, says Peter Murray, deputy head of new service development at the Ohio Library and Information Network in Columbus, Ohio, which assists the University System of Ohio on the project. But Make Textbooks Affordable, a coalition of US student groups, thinks that students are being fleeced, and that the price of 'renting' an electronic file, which costs little for publishers to distribute, is excessive. Indeed, if an e-textbook typically costs half that of the print version, the saving is less impressive when one considers that buyers of new print books would recoup much the same by reselling, and students might pick up used versions for the same price or less. Charging half the price of a printed textbook for an e-book that expires is \"far too costly\", says Hegarty. Rather than leaving students to act as isolated agents in the marketplace, he says, universities, or consortia of universities, should step in and use their bulk-purchasing clout to force down prices by negotiating site licences to e-textbooks, just as many do for online versions of scientific journals. E-textbooks procured this way could be made free at the point of use to all on campus, or for flat fees included in tuition fees. \"The winning model will involve licensing content broadly such that the library licenses the materials, the professors assigns them and the student electronically checks them out of the library as they do hardcopy books,\" he says. Klute also favours such a scheme. NWMSU already spends around $800,000 a year on tens of thousands of copies of print textbooks that it rents to students, who are charged $80\u2013$90 per semester for textbook provision. He thinks that using an e-textbook site licence could at least halve that cost to students. Such a model is being tested by the UK National E-books Observatory project. The project has licensed from publishers 36 e-textbooks in business and management, medicine, media studies and engineering from September 2007 to August 2009 at a cost of \u00a3600,000, and made them available free to all UK universities. It is the future, says Liam Earney, collections team manager of the Joint Information Systems Committee, based in London \u2014 a body established by Britain's higher-education funding councils to support education by promoting technological innovation \u2014 which operates the pilot.  \n                Open source \n              A more radical idea is to offer textbooks for free, without rights restrictions. A range of free, open textbooks are already available for download at WikiBooks ( http://en.wikibooks.org ); the Community College Consortium for Open Educational Resources' Open TextBooks Project; and Connexions, created in 1999 by electrical engineer Richard Baraniuk of Rice University in Houston Texas. These texts typically take the form of modules written by many expert authors. For now these free textbooks remain a cottage industry, says Esposito. Wikipedia-like volunteer efforts are much better suited to self-contained modules that are small enough for an individual to see through from A to Z. But a textbook demands a coherent overall structure and coordination between sections. That is why creating one has always been a major undertaking, demanding long-term commitments by publishers \u2014 who need to make a profit \u2014 and by authors who usually want to be paid for their effort. Still, perhaps 'free' and 'profitable' need not be a contradiction in terms. One group of veteran textbook publishing executives is trying to put open textbooks on a solid commercial footing. In 2007 they created Flat World Knowledge, based in Nyack, New York, and in January 2009 rolled out the first of the 21 textbooks they have in development so far. The texts are written by some 40 domain experts who will be paid 20% of royalties. The company also plans to make its content available via Kindle and other e-readers. All its content will be free to reuse for non-commercial purposes under a creative commons licence. Eric Frank, Flat World's co-founder, says that the strategy is to attract greater use by giving the e-textbooks away \u2014 the initial targets are the high-volume texts for first-year students \u2014 and then look for profit from students' purchase of print-on-demand versions at $29.95 for black and white, and $59.95 for colour. Students can copy and use the electronic content in any way they wish, says Frank. \"Cheap prices are the most effective digital-rights management,\" he says. \"We want to avoid a digital-rights war with students.\" The company also hopes to make money by licensing its content to commercial companies, such as distance-learning outfits and course-management software firms. By making its content free for reuse, Flat World Knowledge will allow lecturers to splice and dice its content. \"More and more professors want to teach from 'customized' textbooks, which are aggregations of various materials, not just what a publisher has aggregated in a single book,\" says Hegarty. He says that the UTA has made an electronic tool available for academics to aggregate any licensed library materials, including scientific journals, and 'publish' them to their students as their textbook materials. \"I think that this is where textbooks are headed.\" In the larger sense, of course, no one really knows where e-textbooks are headed. They just know that things are moving very fast. About all that's certain, says Klute, is that the next chapter of e-textbooks is now being written. \"E-textbooks as we currently know them will look drastically different five years from now\". \n                 See Editorial, \n                 page 549. \n               \n                     Books & Arts The learning revolution \n                   \n                     Naturejobs: Getting schooled \n                   \n                     Paper: An electrophoretic ink for all-printed reflective electronic displays \n                   \n                     CourseSmart \n                   \n                     Flat World Knowledge \n                   \n                     Connexions \n                   \n                     UK National E-books Observatory \n                   \n                     Make Textbooks Affordable \n                   Reprints and Permissions"},
{"file_id": "458564a", "url": "https://www.nature.com/articles/458564a", "year": 2009, "authors": [{"name": "Alison Abbott"}], "parsed_as_year": "2006_or_before", "body": "From a home lab to the Italian Senate, by way of nerve growth factor \u2014 Rita Levi-Montalcini is a scientist like no other. Alison Abbott meets the first Nobel prizewinner set to reach her hundredth birthday. Tiny though she is, Rita Levi-Montalcini tends to command attention. And on the morning of 18 November 2006, she had the attention of the entire Italian government. A senator for life, Levi-Montalcini held the deciding vote on a budget backed by the government of Romano Prodi, which held a parliamentary majority of just one. A few days earlier, Levi-Montalcini had said she would withdraw her support for the budget unless the government reversed a last-minute decision to sacrifice science funds. It was Levi-Montalcini versus Prodi \u2014 and Levi-Montalcini won. On the morning of the vote, immaculately turned out as always, she walked regally on the arm of an usher to her seat in the Italian senate and cast her vote. At one stroke, she secured the budget, won a battle for Italian science and snubbed Francesco Storace, leader of the Right party and part of the opposition coalition. A few weeks earlier, Storace had caused a national scandal by announcing his intention to send crutches to Levi-Montalcini's home \u2014 symbolic of her both being a crutch to an ailing government, he said, and her age, which he considered too old to be allowed to vote. Levi-Montalcini didn't consider herself too old then, when she was 97 years old, and she certainly doesn't now when, on 22 April, she will become the first Nobel laureate to reach the age of 100. Italy \u2014 and quite possibly the world \u2014 has never seen a scientist quite like her. Born into a well-to-do Jewish family in Turin in 1909, Levi-Montalcini fought hard for her career from the beginning. First there was her domineering father, who didn't believe in higher education for women. Then there were Benito Mussolini's race laws, which ejected Jews from universities and forced her into hiding. And after that there was the scientific establishment, which refused to believe in the existence of nerve growth factor (NGF), the discovery of which eventually won Levi-Montalcini a share of the 1986 Nobel Prize in Physiology or Medicine, together with her colleague Stanley Cohen. \"That discovery was huge \u2014 it opened up a whole field in understanding how cells talk and listen to each other,\" says neuroscientist Bill Mobley of Stanford University in California, an admirer for more than 30 years. Hundreds of growth factors are now known to exist and they affect almost all facets of biology. Despite her age, Levi-Montalcini still works every day, exquisitely dressed, hair stylishly coiffured, hands perfectly manicured. In the mornings she shows up at her namesake European Brain Research Institute (EBRI)\u2013Rita Levi-Montalcini, on the outskirts of Rome. In the afternoons she goes downtown to the offices of an educational foundation for African women that she created in 1992. Turning 100 is no reason to stop fighting. \"It's not enough what I did in the past \u2014 there is also the future,\" Levi-Montalcini says. She has never hesitated to use her Senate position to push for better scientific prospects in the country. And today she has something even closer to her heart to fight for \u2014 the survival of the EBRI, which she created in 2002 and which is now in financial straits. Levi-Montalcini spent a large part of her research career in the United States. But her early, and late, scientific life has been based in Italy. Three years after leaving high school, she finally persuaded her father to allow her to study medicine, and in 1930 she enrolled at the University of Turin. Her first mentor was Giuseppe Levi, a prominent neurohistologist. In her autobiography  In Praise of Imperfection , Levi-Montalcini refers to him as \"the Master\" \u2014 he was an outspoken antifascist, renowned for his alarming fits of rage. But he was also the man who introduced her to her first passion: the developing nervous system. Under Levi's attentive eye, she mastered a technique that would be key to her own successes, that of silver-staining nerve cells. Developed by Camillo Golgi in the late nineteenth century and later refined by the Spanish neuroscientist Santiago Ram\u00f3n y Cajal, the technique allowed individual nerves to be seen under the microscope with perfect clarity. Levi-Montalcini's independent research started when Mussolini's race laws were passed in 1938, and all Jews were expelled from universities and other public institutions (Levi, too, was thrown out). Inspired by the story of Cajal, who had worked alone in a makeshift lab in out-of-the-way Valencia, she set up a bedroom laboratory at her family home. When Levi returned to Turin some time later, he joined her at her bedroom bench. She had already identified her research challenge: to work out how nerves emerging from the embryo's developing spinal cord find their way to the budding limbs they will eventually innervate. She had recently come across an exciting paper 1  published a few years earlier by embryologist Viktor Hamburger at Washington University in St Louis, Missouri. Hamburger had removed the growing limbs of chick embryos and found that doing so reduced the size of the ganglia, tiny structures that cluster together the nerve fibres emerging from the spinal cord and direct them on to their final destinations. He put this atrophy down to the absence of what he called an inductive factor released by the tissue to be innervated and, he proposed, necessary to make precursor cells proliferate and then differentiate into neurons.  \n                Detailed dissections \n              Hamburger, though, could not see the nerve fibres in great detail using the light microscope. So Levi-Montalcini decided to repeat the experiment with the silver-staining method. Like Cajal, she reasoned she would need little more than an incubator and a microscope \u2014 and a regular supply of fertilized hen's eggs. Using tiny scalpels and spatulas fashioned out of sewing needles to do her dissections, she saw that the ganglia did not, in fact, wither immediately. The neurons actually proliferated, differentiated and started to grow towards their targets. It was just that they died before reaching them. She concluded that the problem was not the lack of an inductive factor, but of a growth-promoting one that would normally be released by the budding limbs 2 . Towards the end of 1942, bombing forced the Levi-Montalcini family to move into the countryside, where she continued her research undaunted, cycling to farms to buy fertilized eggs. She stopped only when Italy switched allegiance to the Allies in 1943, and Hitler's troops invaded northern Italy. After the war, Levi-Montalcini returned to Turin as Levi's assistant. But at 36, the role no longer suited her \u2014 after all, he had been an occasional assistant to her in the days of her bedroom lab. She found her way out when Hamburger, who had read the papers she had published with Levi during the war, invited her to St Louis for a semester to repeat and extend her experiments. Just as she was doing those experiments, something happened that extended her stay in St Louis from one semester to 26 years. One of Hamburger's graduate students, Elmer Bueker, was trying to see if any piece of fast-growing tissue could attract nerve fibres in the same way that fast-growing developing limbs do. He grafted a lump of proliferating mouse sarcoma tumour onto a chick embryo and found that nerve fibres grew and invaded the tumour mass more abundantly than the limb bud. He postulated that the greater surface area of the tumour allowed more nerves to grow up to it. Levi-Montalcini is renowned for her exceptional intuition, and Bueker's experiment made her antennae vibrate. To her eye, the invasion did not look quite right. Although nerves grow into developing limbs in an orderly way, their growth into the tumour was massive and wild, with the fibres branching randomly. She became convinced that the transplanted tumour tissue was releasing the same sort of factor she claimed the developing limbs released, a factor able to diffuse to the ganglia and stimulate the growth of nerve fibres.  \n                Inspired insight \n              She repeated the experiment, ingeniously placing the tumour outside the sac containing the embryo. This area, although physically separate, shares the embryo's blood supply. It was a killer experiment. Nerves sprouted and grew wildly, supporting her theory that the tumour was releasing a factor that diffused into the blood and travelled to the embryo 3 . \"She realized there was another way to interpret the data, and she knew what had to be done,\" says Lloyd Greene, who studies neuronal differentiation at Columbia University in New York, and has known Levi-Montalcini since he was a student. But to really prove her point, Levi-Montalcini needed a system that was more reliable and flexible than the fertilized egg, and one that would allow her to quantify the responses she was measuring. She wanted to learn how to culture isolated chick-embryo ganglia, and knew of only one laboratory that could do so. So she put two live, tumour-riddled white mice into her handbag and boarded a plane for Rio de Janeiro, where another of Levi's former students was running a big tissue-culture facility. In Rio she learned to culture isolated ganglia and she grew them close to pieces of mouse sarcoma. After 24 hours of culture, she was thrilled to see haloes of nerve fibres growing from the ganglia like suns, with their highest density facing the tumour. Her many letters to Hamburger include beautiful drawings of the haloes. Levi-Montalcini's strong artistic bent is also evident in her research papers, which she illustrated by hand, and in the clothes that she designs for herself. By the time she returned from Rio, Cohen had joined the Hamburger group. The pair worked together for six years trying to identify the factor released by the tumour. Both were determined to provide the sceptical scientific community with solid chemical evidence that the nerve-promoting factor was a reality. But scepticism only increased when Cohen and Levi-Montalcini proposed that snake venom and extracts of mouse salivary glands, both of which also promoted profuse nerve growth, were abundant sources of the factor they were seeking. For many scientists, it required too great a leap of the imagination to believe in this unlikely soluble factor, which was supposed to diffuse from one tissue and then potently affect specific processes in nerves. \"You have to remember that such a mode of biological action was not accepted in those days,\" recalls Ralph Bradshaw, who joined Washington University in 1969 as its first protein chemist and is now at the University of California, San Francisco. \"And Rita was saying it was in tumours, snake venom, as well as many normal tissues \u2014 well, people just didn't believe it was serious biology.\" More people started to believe when Cohen discovered another, related factor that was later called epidermal growth factor 4 . Then, in 1959, Levi-Montalcini developed with him an antiserum to purified NGF. The antiserum abolished the  in vitro   halo, and wiped out the relevant part of the nervous system when injected into newborn mice 5 . The last remaining pockets of scepticism in the scientific community dissolved when Bradshaw, together with Ruth Hogue Angeletti, the only PhD student Levi-Montalcini ever had, determined the structure of the protein in 1971 using one of the first automated protein sequencers 6 . \"Rita didn't put her name on the paper as we would have expected someone in her position to do,\" says Bradshaw. \"A typical Rita gesture.\" Although Levi-Montalcini loved the scientific atmosphere in the United States, she was always homesick for Italy and for her family. In the early 1960s, she began to split her time between St Louis and Rome, where the CNR, Italy's major research organization, created a laboratory for her. Her working style was relentless, demanding and passionate. In the decades in which her research was most intense, she would call her co-workers before seven in the morning as well as last thing at night to discuss experiments. Angeletti refers to the regime as inspiring rather than brutal. \"Even as a highly motivated young American I had never before observed this kind of dedication,\" she says. \"I realize how lucky I was to work with someone so brilliant, expansive and generous of spirit.\" When the study of growth factors finally became respectable and other scientists flooded into the area, rather than being gratified, Levi-Montalcini was annoyed by the invasion of what she saw as her territory. \"She fell out with most people in the NGF field at one time or another \u2014 including myself,\" recalls Bradshaw. At meetings, she had a tendency to educate audiences on the order in which discoveries had been made, recalls Greene. After one of his own talks, hers was the first hand raised. \"It was not a question, but a long statement about NGF and its history,\" he says. \"As she spoke, she little-by-little made her way to the stage and the podium, and the next thing I knew, she was next to me at the microphone still asking her 'question'.\" Under the circumstances, he says, he could do no more than \"step aside, cede the microphone to her, raise my eyebrows and let her finish\".  \n                Peacemaker \n              In the early 1980s, Levi-Montalcini started to bury the hatchet with everyone in the field, says Bradshaw. Their own quarrel \u2014 over a paper he had published without showing her first \u2014 was patched up when she took him aside for a chat at a meeting. \"It ended what had been a strained and difficult time for me,\" says Bradshaw. \"But Rita had to endure a great deal of scepticism in the early days and there were times when she was justifiably defensive.\" Her later discoveries faced no such scepticism. She showed, for example, that NGF had major effects on the immune system, yet another unexpected finding that became a major turning point in biology 7 . By the time she and Cohen were awarded the Nobel prize, considerable peace had been achieved. But controversy picked up again in the wake of the award. Some were upset by what they saw as her failure to acknowledge her debt to others, such as Levi and Hamburger. Hamburger, who lived to be 100, claimed that their friendship suffered after she explained publicly why he should not have shared the prize with her as some had thought appropriate. But such criticism gained no traction in Italy, where Levi-Montalcini had by now settled permanently. Many viewed her as a national treasure for her achievements, outsize personality, energy and eloquence. Her CNR institute became one of the largest biological research centres in the country. She also took it on herself to work at all levels to improve the state of Italian science. A socialist by lifelong conviction, she became good friends with Prodi, who had been prime minister in two centre-left governments. After she was made senator for life in 2001, she showed up for every parliamentary vote to support Prodi's fragile coalitions. She also champions social issues related to research, such as ethics and women in science. The Rita Levi Montalcini Foundation has supported education for more than 6,000 African women \u2014 \"to improve their chances of becoming scientists\", she says. A keen writer, she has published 21 popular books. As a young bookworm, her favourite among the classics was Emily Bront\u00eb's tale of dark passion,  Wuthering Heights . Such romantic inclinations remained literary though \u2014 despite a brief engagement while at medical school, she never had any long-term romances. In a 1988 interview with  Omni   magazine she said, tellingly, that even in a marriage of two brilliant people, \"one might resent the other being more successful\". One of her remaining desires has been to leave as a legacy a well-run research institute of international significance in her country, where underfunding, inefficiency and bureaucracy have crippled much of the state research system. The Santa Lucia Institute in Rome, keen to expand its own research activities, offered rent-free premises for the first ten years of her neuroscience institute. But the EBRI is now looking shaky. Levi-Montalcini expected the government to make funds available for running the institute, but in the event the Prodi government provided only a one-off donation of \u20ac3 million (US$4 million) just before its demise one year ago \u2014 and no other major donor was found. The right-wing government of Silvio Berlusconi has shown little interest in research and the name Levi-Montalcini cuts no ice with it. The EBRI, which now has a staff of 28, runs with an annual deficit of \u20ac200,000. Earlier this year, University of Turin neuroscientist Piergiorgio Strata took over as scientific director with a mandate to turn things around. \"We need maybe \u20ac3 million per year to survive,\" says Strata, who is confident that he'll be successful. The ever-determined Levi-Montalcini puts her trust in him. \"I'm an optimist,\" she says. \"I still hope we can find a way to carry on.\" Levi-Montalcini is now hard of hearing and sees poorly, but her mind is sharp. At the EBRI she runs a research project to see how far back NGF goes in evolution. Several young scientists are helping by trying to find out whether the factor exists in a series of invertebrates. They are gratified to be able to speak with her most days. \"She is an inspiration for us,\" says Francesca Paoletti, one of the postdocs working there. And they, in turn, make her happy. \"I am not afraid of death \u2014 I am privileged to have been able to work for so long,\" says Levi-Montalcini. \"If I die tomorrow or in a year, it is the same \u2014 it is the message you leave behind you that counts, and the young scientists who carry on your work.\" And with that, clutching her micrographs of NGF in octopus tissue, she walks away on the arm of a friend, with a slow but stately gait. With her high heels and the swing of her tailored coat, she still looks as though she stepped off the pages of a fashion magazine. \n                     The European Brain Research Institute Rita Levi-Montalcini \n                   \n                     Rita Levi-Montalcini \u2014 Nobel biography \n                   Reprints and Permissions"},
{"file_id": "457776a", "url": "https://www.nature.com/articles/457776a", "year": 2009, "authors": [{"name": "Erika  Check Hayden"}], "parsed_as_year": "2006_or_before", "body": "Geneticists looked to the human genome to understand human evolution. But it's hard to interpret without considering the inheritance of culture, finds Erika Check Hayden. Barely a decade after Charles Darwin published  On the Origin of Species , he and his long-time correspondent Alfred Russel Wallace were engaged in a fierce debate. Darwin said that natural selection had shaped the human species just like any other. But Wallace disagreed, arguing that selection alone could not account for the exceptional capabilities of the human mind. \"How could natural selection, or survival of the fittest in the struggle for existence, at all favour the development of mental powers so entirely removed from the material necessities of savage men?,\" he wrote 1 . Wallace lost out. By the mid-twentieth century most scientists had agreed that human bodies and minds were the product of genes that had evolved under the pressures of natural selection, just like everything else in the living world. One of the exciting prospects of reading the human genome was that it would reveal the ways in which this had happened \u2014 the marks left by evolution as it shaped humans into a species with language, learning and all sorts of other traits peculiarly interesting to it. \"The idea was that if we could just identify those few critical genetic differences, we could explain the differences in cognition and language,\" says Todd Preuss of the Yerkes National Primate Center at Emory University in Atlanta, Georgia. But today, nearly a decade after the human genome was sequenced, some geneticists are thinking again. Genomics has identified many sequences that are under selection, but it has not provided the simple read-out of human evolutionary history that some had hoped for. Scientists are having to rethink how genomes work, and are now pondering whether genes alone can explain the human animal. They don't think that human biology is incomplete without spirituality, as Wallace did. But they do wonder whether it is incomplete without culture. Because many complex skills and behaviours are being passed on through culture, some researchers are coming around to the view that the species has escaped the need to encode them rigidly in its genome. \"Of course the mechanisms of selection are operating,\" says Ajit Varki, a specialist in human origins at the University of California, San Diego. But perhaps \"we don't necessarily fix our behaviours, and we are letting some previously fixed behaviours deteriorate, because we can rely on cultural transmission\", he adds. These ideas are not entirely new. In 1981, geneticists Marcus Feldman and Luca Cavalli-Sforza from Stanford University in California published models to show how human behaviour results from the interaction of biological and cultural evolution. What is new for genome scientists is the realization that they will not be able to interpret the evolutionary marks they have found in the human genome without considering behaviour and environment every step of the way.  \n                Being human \n              The human species has a unique set of features, including a large brain in proportion to the rest of the body; the ability to communicate complex information through symbolic language; and physiological vulnerabilities to Alzheimer's disease, certain cancers and other conditions. Geneticists hoped to explain the evolution of these human attributes with the tools of comparative genomics \u2014 the side-by-side comparison of different species' genomes. Varki was part of a group that pushed hard to sequence the genome of human's closest relative, the chimpanzee, arguing that any genetic differences between human and chimp sequences would lead straight to the heart of humanness. But the chimp sequence, published more than three years ago 2 , hasn't delivered this. One comparison between humans, chimps and mice, for example, showed that the protein-coding sequences of genes expressed in the brain have changed very little across species 3 . The idea was that if we could just identify those few critical genetic differences, we could explain the differences in cognition and language. Todd Preuss  A few genes are interesting exceptions. Selection seems to have favoured changes in  FOXP2 , a gene involved in human speech, after humans and chimps diverged between 4.6 million and 6.2 million years ago 4 . Geneticist Bruce Lahn from the University of Chicago in Illinois has proposed that  ASPM 5  and  MCPH1 6  \u2014 both of which are thought to be related to brain size \u2014 are under selection in humans. And then there is the gene encoding DUF1220, a \"protein domain of unknown function\" that seems to be under selection, that is active in the brain and has many more copies in humans than in other species 7 . But none of these genes alone is likely to explain a single human trait. \"No real silver bullet has emerged to say, 'This is the human uniqueness gene', and there will never be one such gene,\" says Evan Eichler, a genome biologist at the University of Washington in Seattle. \"It is the collective impact of all these genetic differences that make us human.\" Some evolutionary research is leading away from protein-coding genes entirely. In 2006, a team of scientists led by David Haussler at the University of California, Santa Cruz, picked out 49 regions of the human genome 8  that had remained largely untouched throughout the evolution of fish, reptiles, birds and monkeys, and then went into mutational hyperdrive after ancestral humans emerged. The researchers found that the genomic address that has evolved faster than any other codes not for a protein, but for a small piece of RNA \u2014 human accelerated region 1 (HAR1) \u2014 that is expressed in brain cells during human fetal development. Beyond that, nobody knows what HAR1 does. Its sudden status as a belle at the evolutionary ball underscores the idea that selection could have acted most strongly on sequences outside the bounds of protein-coding genes, leaving Haussler and other researchers to work out what these sequences are doing, and why selection has acted on them. The picture could become even more complicated if, as some recent work has suggested, the statistical tests used to find genes under positive selection are themselves questionable 9 . \"There are thousands and thousands of changes to our genomes that have occurred in the past few million years that are still hidden, and the vast majority of those will not be functionally consequential,\" Haussler says. \"So it is an amazingly difficult, needle-in-a-haystack type of search.\" Genome researchers once expected that most of the genetic differences between humans would be in single letters of DNA, a type of variation called a single nucleotide polymorphism, or SNP. But in the past few years, they have discovered that large chunks of the genome can be duplicated, deleted and otherwise rearranged differently between individuals. In a paper published this week[10], Eichler's team analysed the genomes of humans, chimpanzees, orangutans and macaques and found that a burst of duplications appears to have occurred in the last common ancestor of humans and chimpanzees. The question now is why? Such rearrangements risk disrupting essential genes and have been linked to human diseases such as autism and schizophrenia. Varki and Eichler suggest that structural variations may also confer benefits by expanding the range of genetic diversity. The negative side effects might be outweighed by the advantages conferred by new genes or other beneficial arrangements. And the human genome might have been able to tolerate some of the potentially toxic variants thanks to clothing, tools, agriculture and other cultural innovations that allow individuals with these variants to survive. \"By allowing individuals to be buffered against natural selection, perhaps culture allows a wider spectrum of genetic diversity to creep in,\" Eichler says. \"Maybe the wider spectrum of diversity allows for more savants and autistic people in the same population.\" Researchers have found that the human genome has accumulated more than its fair share of other potentially harmful genetic changes too \u2014 in protein coding regions, promoters and even the loss of entire genes. One explanation is that it is a remnant of the frequent population 'bottlenecks' in human history, in which small groups that migrated to new areas established new populations that all carried the founders' mutations. But another possibility, says statistical geneticist Gilean McVean at the University of Oxford, UK, is that the human ability to learn and adapt has eased the selection pressure that would weed out some of these changes. \"When you look in the human genome, one of the things you see is that it has accumulated a lot of apparently bad mutations, and to some extent humans' inventive skills might have allowed that,\" he says. A modern example, he points out, might be the ability of humans to make spectacles to counteract poor vision. Anthropologist and neuroscientist Terrence Deacon of the University of California, Berkeley, has long argued that culture could have \"relaxed\" human selection. Apes, for instance, have lost the ability to make vitamin C because the gene that facilitates this process has broken down. Deacon suggests that the availability of fruit eased the selection pressures working against individuals who couldn't make their own vitamin C, allowing the relevant gene to accumulate mutations but also making apes dependent on external sources for the nutrient. The relaxed selection created by human culture similarly could have allowed the evolution of more diversity and complexity, Deacon says, but it has also made humans more reliant on the innovations that freed them from selection in the first place. \"We have produced symbolic communication and culture and technology, all of which play a part in shielding us from certain kinds of selective forces,\" he says.  \n                Genetic burst \n              When scrutinizing the genome, some researchers see more dramatic evidence of culture's influence. In 2007, a team led by anthropologist John Hawks of the University of Wisconsin, Madison, and genome scientist Robert Moyzis of the University of California, Irvine, proposed that culture is hastening human evolution[11]. The team combed through a set of 3.9 million SNPs from European, African and other ancestral human populations, looking for those that bore a signature of positive selection: they were relatively young, common in individuals from the same population and different from those in the other populations. Such mutations are probably being 'swept' to abundance shortly after appearing because they lie in or near pieces of DNA that are beneficial in some way. The team found that a burst of these SNPs had appeared about 10,000 years ago, suggesting that selection has been causing very rapid genetic change since that time. The researchers then tested whether evolution had sped along like this throughout human history. If so, then the different ancestral populations should have fairly similar genomes because many mutations and their surrounding regions would have swept throughout the human genome before these populations diverged. But current human populations are much more genetically diverse than this hypothesis predicts, so Moyzis and Hawks have concluded that evolution must have ramped up over the past 40,000 years. They chalk some of this acceleration up to human population growth, which exposed the species to more new mutations and created more raw material for selection. But the other reason, Hawks thinks, is culture \u2014 because although the physiology of humans has not changed much in the past 40,000 years, their expansion and migration means that lifestyles, languages and technologies certainly have. By allowing individuals to be buffered against natural selection, perhaps culture allows a wider spectrum of genetic diversity to creep in. Evan Eichler  Although not everyone agrees with Hawks's claims, the best understood example of recent human evolution does seem to fit. Genetic mutations that allow adults to digest lactose, a sugar found in milk, have emerged independently in different populations in response to the same cultural innovation \u2014 cattle domestication[12]. \"I don't see culture as an alternative to genetics, I see culture as being the explanatory factor for these genetic changes,\" says Hawks. \"There is no explanation for change without the gene\u2013environment interaction.\" Hawks and others are now looking for the beneficial sequences connected with the selected SNPs. But these sequences are hard to pinpoint, and even harder to connect up with a specific human trait. \"We know that genes involved in immunity, genes involved in brains, these are showing signals of adaptation in a way that suggests [these traits] have been important,\" says McVean. \"But that's kind of obvious \u2014 you only have to look at our biology or our physiology or behaviour to see that.\" To Varki and others, the absence of sequences that underlie specific human behaviours could itself testify to the importance of human culture. According to the 'Baldwin effect', named after the American psychologist who proposed it in the late 1800s, behaviours crucial to survival will often become 'hard-wired' into the genome to ensure that they are not lost. But most human skills are not hard-wired: people who have never lived in the Arctic would have a difficult time figuring out how to hunt a seal, skin it and stretch its hide to build a kayak. But if they moved there, they could quickly learn. The human species has spent most of its history wandering through and creating new environments, and specific skills or resources can quickly become obsolete. So it might have been helpful to cement into DNA the social and intellectual capacities to learn, but leave out the specific instructions for building a kayak. \"It is good to make learning more rapid and to improve memory,\" says geneticist Eva Jablonka of Tel Aviv University in Israel, \"but it is not good to specify it too precisely because this memory and learning allows you to cope with a wider range of environments for which you cannot be prepared genetically.\" If genes and culture are evolving together, then some difficult questions start to arise \u2014 in particular, about whether human evolution is dividing the species. Surveys of human diversity have shown that people can be classified into genetically similar groups that correlate with their geographic ancestry. And Jonathan Haidt of the University of Virginia in Charlottesville has proposed that genetic differences between these groups could underlie variations in traits such as aggression, thriftiness and spontaneity. \"Each [human] niche has its own microclimate with its own adaptive pressures,\" says Haidt, who was impressed by Hawks' work. \"The discovery of much more rapid genetic evolution leads directly to the prediction that we will find dozens or hundreds of genetically based ethnic divergences in traits, many of which will have some moral significance.\" No such ethnically linked divergence has been found to exist, but Haidt points to a hypothesis that selection has boosted the average IQ of the Ashkenazi Jewish population because of its historical reproductive isolation and history of working in the financial trades[13].  \n                Altered state \n              Most geneticists disagree with Haidt. They point out that occupations, conflicts and other aspects of culture have changed so rapidly during human history that the selective pressures associated with them would have not had time to fix changes in the genome. And Jablonka returns to her point that it is more evolutionarily advantageous for the human species to stay adaptable than to cement personality traits in the genome of different groups. \"It would not only be detrimental,\" she says, \"it cannot happen in a species that is all the time having to adapt to the changes it is creating in the world.\" Until researchers understand the evolutionary basis for human behaviour it will be easy to argue over such ideas but hard to rule them out. So geneticists are moving away from evolutionary just-so stories that are based on single genes and embracing the complexity of the cultural and biological contexts in which humans and their genes operate. Daniel Geschwind of the University of California, Los Angeles, for instance, is analysing how genes interact with each other in complex networks. If one gene in the brain occupies a central node in the human network but a peripheral one in chimps, then it is a strong hint that the gene's function has changed in some meaningful way too. This type of approach should help solve the problem that researchers have run up against repeatedly in genomic analyses: how to determine which human characteristic, or phenotype, is affected by a genetic sequence under selection. Cognitive psychologists have found in recent decades that some phenotypes once thought to be uniquely human \u2014 such as the development of moral codes, or the ability to recognize that other individuals have minds of their own \u2014 are seen in other animals. And work by Michael Tomasello of the Max Planck Institute for Evolutionary Anthropology in Leipzig, Germany, has suggested that human-specific phenotypes may be very subtle. His group found that young chimps, orangutans and toddlers performed about the same on tests designed to assess 'physical' intelligence, such as retrieving a piece of food that had been hidden under a cup and then moved. But the children were better at 'social' intelligence tasks, such as copying an experimenter's actions to retrieve a treat stuck in a tube[14]. Preuss says that such precise dissections of human-specific traits are still quite rare. \"If you go beyond the bland expression of 'advanced cognition' and try to talk about cognitive mechanisms and abilities, we don't really know that much,\" he says. This means that there is a glut of genomic data but a paucity of crucial information from other fields that would help to make sense of it. \"We need to start connecting this genetic world to the traditional anthropological approaches,\" agrees Hawks, who sees genomics as an inspiration to start collecting and sharing data on an equivalent scale in his own discipline. Long before the next centennial of Darwin's birth, these data might have closed the book on human evolution. If they do show that culture has shaped the evolution of humans in a way that has no counterpart elsewhere in the animal kingdom, then perhaps Wallace will earn some posthumous credit: this was more than natural-selection-as-usual. But culture cannot have had so strong a role in human evolution without itself being influenced by the results. In the words of Wallace, understanding this interplay will require all of the \"mental powers so entirely removed from the material necessities of savage men\". See Editorial,  page 763 , and online at  http://www.nature.com/darwin \n                     Darwin 200 news special \n                   \n                     Essay series: Being human \n                   \n                     Chimp genome focus \n                   \n                     Human uniqueness: genome interactions with environment, behaviour and culture \n                   \n                     The Complete Work of Charles Darwin online \n                   Reprints and Permissions"},
{"file_id": "457780a", "url": "https://www.nature.com/articles/457780a", "year": 2009, "authors": [{"name": "Dan Jones"}], "parsed_as_year": "2006_or_before", "body": "People's mindsets are neither fixed by evolution nor infinitely malleable by culture. Dan Jones looks for the similarities that underlie the diversity of human nature. Darwin famously gave scant attention to humans in  On the Origin of Species , contenting himself with the teasing pledge that \"Light will be thrown on the origin of man and his history\". The promised light came in the  The Descent of Man, and Selection in Relation to Sex   (1871) and  The Expression of the Emotions in Man and Animals   (1872), in which the notion of a common human origin was crucial. In  The Expression , to further his case that humans shared a great deal of their nature, Darwin \"endeavoured to show in considerable detail that all the chief [emotional] expressions exhibited by man are the same throughout the world\". Darwin often relied on anecdotal accounts of travellers and his own casual observations in drawing these conclusions. His poorly sourced claims came, in time, to be challenged by anthropologists focused on the particularities of, and differences between, various cultures. In the 1960s, guided by the prevailing anthropological orthodoxy, Paul Ekman, now retired, set out to prove Darwin wrong by asking for interpretations of facial expressions from the farthest flung people he could get to. He ended up confirming that Darwin had a point. \"The evidence is very strong, from studies of both recognition and expression in Western and Eastern, literate and preliterate, cultures that Darwin was indeed prescient,\" says Ekman. \"At least six, perhaps seven, emotions have a pancultural facial expression.\" The expression of these emotions is not the only human commonality revealed by cross-cultural studies. People everywhere form communities and pay attention to kinship systems, use complex languages to communicate, socialize and adorn their bodies. Some of these common motifs simply reflect givens of human existence. Many languages use a word related in meaning to 'small person' to describe the opening in the eye's iris, as English does, but this needs no explanation beyond the fact that pupils show the viewer a small reflection of his or herself. Others have deeper significance. In 1991, anthropologist Donald Brown, then at the University of California, Santa Barbara, published  Human Universals 1 , a survey of hundreds of candidate universal similarities from domains as diverse as language and status systems to concepts of time and incest taboos. \"It was primarily designed to disabuse anthropologists and others of the notion that universals are few and trivial,\" says Brown, and it immediately found a warm reception among the then-emerging field of evolutionary psychology, which championed the idea of a universal human nature explained by evolution. At least six emotions have pan-cultural facial expression. Paul Ekman ,  But although some anthropologists had simply denied the existence of the human universals that evolutionary psychology craved, others had put forth a subtler view. As the late Clifford Geertz wrote: The notion that the essence of what it means to be human is most clearly revealed in those features of human culture that are universal rather than in those that are distinctive to this people or that is a prejudice that we are not obliged to share \u2026 It may be in the cultural particulars of people \u2014 in their oddities \u2014 that some of the most instructive revelations of what it is to be generically human are to be found 2 . Today's research is taking that message, as well as Brown's, to heart. An emerging cadre of anthropologists and evolutionary theorists is taking a new look at the way oddities and difference shed light on the universal. Human nature, this line of thinking holds, is like the control panel on a mixing desk. It is not infinitely malleable: it has its fixed channels and its presets. But it has lots of faders and switches to play around with, and their cultural twiddling can produce a surprisingly wide range of effects.  \n                Is fair fair? \n              Trying to understand human nature this way has required some academic reconciliation. \"We've had to demonstrate to those who emphasize cultural diversity that there is still a useful and interesting way to talk about human nature underpinning the diversity,\" says Justin Barrett, a cognitive psychologist at the University of Oxford, UK. \"At the same time, we've also had to convince those on the biological side that explaining the complexity of human cultural behaviour requires more than a simple application of the rules of animal behaviour.\" And the reconciliation is still a work in progress. As anthropologist Joe Henrich of the University of British Columbia in Vancouver, Canada, puts it, \"We're all comfortable with evolved aspects of the human mind as well as the importance and power of cultural transmission \u2014 but there are large swathes of academia that haven't yet reached this constructive consensus.\" Henrich's commitment to the cultural plays out in the way he does research. He and his team have gone out and sampled humanity's cultural diversity in various far-flung reaches of the world, combining in-depth ethnographic work with experimental tools from the psychology lab. \"A lot of us are trying to be seriously quantitative about these things,\" says Henrich. A notable example of the approach comes from a staple of behavioural economics, the 'ultimatum game': one player of a pair is given a sum of money, say \u00a5100, and has to offer the other some proportion of it. If the offer is accepted, the money is split as proposed; if it is rejected neither player gets a thing. The proposer thus has to gauge what his or her partner will think fair, or at least accept. Western subjects often reject offers of less than 30% of the money. In a landmark study, Henrich's team took the game on a world tour to 15 diverse small-scale societies. In some places people rarely or never rejected very low offers; in others, surprisingly, they rejected more-than-fair offers of greater than 50% 3 . This fits with the finding that 'altruistic punishment' \u2014 people's willingness to punish free-riders who parasitize the efforts of others \u2014 varies dramatically across societies 4 , even while being present in some degree in most. The fairness fader, if there is one, can clearly be set at a wide range of positions.  \n                Enter the WEIRDos \n              These studies call into question conclusions about universals derived from looking at limited, and possibly unrepresentative, groups of subjects \u2014 which is a problem with most psychology studies. Henrich classifies them as WEIRD \u2014 Western, educated, industrialized, rich and democratic. What's worse, undergraduates, the lab rats of psychological studies, are weird even by the standards of WEIRDos. Why should this make a difference? \"Our kids grow up with a lot of active teaching compared with small-scale societies,\" says Henrich. \"Our brains are trained for the particular and strange world we inhabit, one where we're not foraging for food, hunting game or constantly under the threat of disease.\" Although WEIRDos may use universal foundations to build up their strange ideas (see  'Universal maths' ), not everyone has a culture that encourages such things. The argument is borne out by research: an as-yet unpublished review by Henrich and his colleagues Steve Heine and Ara Norenzayan that looks at available cross-cultural studies confirms that WEIRDos are outliers in many ways. Humans, for instance, have a tendency to prefer a smaller immediate gain to a larger but delayed reward. Yet to say that discounting in this way is a human universal glosses over the dramatic differences in discounting seen in different cultures. Among the Tsimane of the Bolivian Amazon, future discounting is ten times what you find in the United States; by this measure the Tsimane care even less about the future than do American drug addicts, typically regarded as pathological future discounters. Supernatural thought is so ubiquitous because it is readily accommodated by human cognitive systems. Justin Barrett ,  To try to get deeper into these issues, researchers are looking for ways to gauge not what people say, but how they actually think. \"Cognition bridges the biological and the cultural,\" says Barrett. A cognitive approach focuses attention on what human minds come equipped with, and how they develop \u2014 a process that involves much more than just unfolding according to some pre-specified blueprint. \"What you really want to study are developmental processes, and how they interact with culture,\" says Henrich. A model here is Noam Chomsky's pioneering work in linguistics (see  'Universal language' ). Chomsky, at the Massachusetts Institute of Technology in Cambridge, united the study of universals and diversity across the world's languages by proposing an innate mental system to acquire, use and comprehend language, which operates on the basis of a great deal of culturally specific input. The constraints placed on humanity's vast linguistic diversity by a shared 'universal grammar' does not make the study of languages less interesting \u2014 it just makes it more tractable. The same might apply to phenomena as seemingly complex and socially mediated as religion (see  'Universal religion' ). Ideas with a distinct Chomskyan flavour have been a stimulus to recent thinking about morality. What counts as a moral transgression, and how one should react to the transgressor, vary from culture to culture. But deeper patterns seem to lurk beneath this surface diversity. Following Chomsky's lead, a number of researchers are working on the idea that an innate and universal moral grammar might underlie human ethical judgements. A series of web-based studies led by Marc Hauser of Harvard University have suggested that moral judgements can be explained in terms of such universal and fundamental moral principles 5 ,   6 . Harm caused by direct physical contact, for instance, is generally deemed to be morally worse than harm arising as a side effect, as are harms caused by specific actions rather than omissions. But these are early days in fleshing out the tool kit of putative moral principles and parameters. \"By the time Chomsky started his work in the 1950s he already had a massive amount of descriptive linguistics from all over the world to play with,\" says Hauser. \"In the case of morality, we don't have anything like what the linguists had 50 years ago. We don't know whether the distinctions we're making are at the right level of abstraction, or whether they are principles or parameters.\" Web-based surveys are necessarily biased towards the more educated and affluent, so the answer is to go and look at less WEIRD data \u2014 which makes the picture more complex. In a recent study 7 , Hauser and his Harvard colleague Linda Abarbanell ran a variety of moral scenarios past a largely uneducated rural Mayan population. Compared with web-based subjects, this decidedly non-WEIRD group gave much less moral weight to distinction between acts and omissions. Hauser and Abarbanell think that the weight put on that dichotomy is a parameter that can be set by local culture, just as some grammatical practices are. A bird's-eye perspective on moral diversity and uniformity comes from psychologists Jonathan Haidt, of the University of Virginia in Charlottesville and Craig Joseph, of Northwestern University in Evanston, Illinois. Surveying anthropology and evolutionary psychology, they argue that evolution has built into the human mind a preparedness to care about five sets of social issues: fairness and justice; avoiding harm to and caring for others; in-group loyalty; social hierarchy and respect for authority; and the domain of divinity and purity, both bodily and spiritual 8 . \"Morality is a social construction, but each society constructs it on top of these five innate moral foundations, relying on them to varying degrees,\" says Haidt. \"Some moralities, such as those of secular Europe, rest primarily on the first two, prizing concerns about harm and fairness above all else; other cultures, such as those of traditional India, emphasize fairness less, and the virtues of respect and spiritual purity more.\"  \n                Liberal differences \n              Haidt and his graduate student Jesse Graham also argue that differential use of the five foundations illuminates the difference between American liberals and conservatives: whereas the former have hypertrophied sensitivity to fairness/justice and harm/care, and atrophied interest in the rest, conservatives are sensitive to all five channels 9 . This inevitably leads to disagreements about what counts as a moral issue, and differences in how moral debates are approached. Moral norms are a prominent, but by no means exclusive, subset of the broad class of cultural norms. These are the social rules that govern a great deal of people's behaviour in everyday life \u2014 from what to wear and how to meet and greet people, to tipping and queuing. Although invoked remarkably often, the psychology of normative behaviour still remains poorly understood. Psychiatrist Chandra Sekhar Sripada, at the University of Michigan in Ann Arbor, and philosopher Stephen Stich, of Rutgers University in New Brunswick, New Jersey, have surveyed the widely spread literature on normative psychology, and argue that humans are innately and universally equipped with two key pieces of psychological kit that underlie the phenomenon 10 . The first is a norm-acquisition device, in some ways like Chomsky's language acquisition device, which is responsible for picking out and absorbing local norms. The second generates an intrinsic motivation to comply with these acquired cultural norms, which change over time and diverge between groups. If Sripada and Stich are on the right track, then humans are biologically built to be cultural \u2014 and different. The universal design specification is not merely compatible with cultural diversity; it is one of the engines that drives it. \n                 Dan Jones is a freelance writer in Brighton, UK.  \n               See Editorial,  page 763 \n , and online at  http://www.nature.com/darwin \n . \n                     Moral Sense Test \n                   \n                     YourMorals.org \n                   \n                     Moral Foundations.org \n                   \n                     Civil Politics \n                   Reprints and Permissions"},
{"file_id": "457953a", "url": "https://www.nature.com/articles/457953a", "year": 2009, "authors": [{"name": "David Cyranoski"}], "parsed_as_year": "2006_or_before", "body": "A Chinese laboratory is the only source of a valuable crystal.  David Cyranoski  investigates why it won't share its supplies. One of Daniel Dessau's prized possessions is a small crystal of potassium beryllium fluoroborate (KBBF). Dessau, a solid-state physicist at the University of Colorado at Boulder, uses the crystal to convert the light of a US$100,000 laser into a deep ultraviolet, a good wavelength for studying the surface of superconductors. But because the laser light gradually degrades the crystal, Dessau has to save it for special projects. \"It is a beautiful crystal,\" he says. \"It would really move the field forward \u2014 if people could get it.\" But Dessau can't get any more of it. Nor can Peter Johnson, a condensed-matter physicist at Brookhaven National Laboratory in Upton, New York, who was once promised it by Chuangtian Chen, the Chinese physicist who runs the only laboratory that knows how to make the crystals. And nor can any of a host of other solid-state physicists outside China. \"There has been a limited release,\" says Johnson. \"I don't know the politics behind it.\" In fact, the politics is simple. The Chinese government is squeezing the crystal for every bit of academic and, eventually, commercial potential it can yield. In October 2008, the finance ministry sidestepped traditional scientific funding channels and started throwing 180 million renminbi (US$26 million) at a three-year national project to find better ways to produce and use KBBF. China has selected a handful of groups to work with Chen's crystal, including teams studying the newest type of superconductor, called pnictides. China's monopoly of this crystal is no fluke. At a time when materials scientists and solid-state physicists elsewhere are seeing a lack of investment, their counterparts in China are surging ahead in a wide range of materials research for much the same reasons as they did with KBBF. The nation has accumulated a great depth of crystal-growing know-how over the past three decades; it has steadfast government support; and its scientists are willing to subsume themselves in a large team effort and take on the often thankless, sometimes dangerous and always tedious trial-and-error task of synthesizing new materials. \"Many great discoveries in this field come from putting things together and getting the temperature and timing just right,\" says Christos Panagopoulos, a materials researcher at Nanyang Technological University in Singapore. The discovery process \"doesn't require genius\", he says. KBBF's ability to shorten the wavelength, and thereby boost the frequency, of laser light is an example of 'nonlinear' optics, a field that first blossomed in the 1960s as lasers became more widespread in laboratories. Under ordinary circumstances, light passing through water, glass or any other material will perturb the atoms only slightly, so that they vibrate in sync with the light wave. As a result, light can be reflected, refracted, scattered and absorbed ad infinitum without its frequency being affected. Nonlinear effects are evident only when the light is so intense that the vibrations it causes compete with the binding forces on the atoms. When highly perturbed, as in the case of high-intensity lasers, the atoms can absorb the energy of the incoming light and re-emit the light with a frequency that is double, triple or even some higher multiple of the original. A variety of materials have been discovered that can boost laser light to frequencies that the lasers alone cannot produce, and each has a set of signature frequencies that it can achieve. China might easily have fallen behind in this field, as it did in so many others. Just as nonlinear optics started coming into its own, China was caught up in the Cultural Revolution, a particularly dark period starting in the mid-1960s when many academics were criticized as being elitist or impractical and sent to do farm work for 're-education'. But Chen, now a spritely 71-year-old at the Technical Institute of Physics and Chemistry in Beijing, was lucky. \"The government always considered crystals important for industry,\" he says. So by agreeing to the government's request to switch from its theoretical studies to growing crystals, Chen's lab was able to continue doing materials science throughout this period.  \n                Crystal blueprint \n              In the 1970s, Chen developed a formula that set out the conditions needed for a material to generate nonlinear effects. In 1984, the formula led Chen and his team at the Fujian Institute of Research on the Structure of Matter to investigate barium borate (BaB 2 O 4 ), which proved to be the first material able to generate an ultraviolet wavelength of close to 200 nanometres. The crystals are now widely used in femtosecond lasers, which use ultrashort bursts of infrared light to slice through materials with extreme precision, making them ideal for some types of surgery. This was before China had any laws covering intellectual property, so Chen received no royalties, but his salary jumped from 87 renminbi a month to 147 renminbi a month. Chen thought he had hit the big time. Another seminal discovery came in 1987, when Chen's group demonstrated nonlinear optics in lithium triborate (LiB 3 O 5 ). Engineers now use the compound to create high-powered green and near-ultraviolet lasers. As the crystals are extremely resistant to damage, they are particularly useful in applications such as welding and semiconductor manufacturing. Some 80\u201390% of all solid-state lasers and laser systems now use lithium triborate crystals for frequency conversion. Castech, a corporate spin-off set up by the Fujian Institute of Research on the Structure of Matter to manufacture and develop crystals for use in lasers, has been making several million US dollars a year from the compound. But Chen and his PhD student Rukang Li were not done shortening wavelengths. Starting in 1988 on what would be a long journey, Li examined the chemistry of \"hundreds, if not thousands of compounds\" using Chen's formula. \"KBBF looked good,\" remembers Li. Eight years later, they proved that the crystal could produce a laser of an unprecedentedly short, 184.7-nanometre wavelength 1 . However, the crystal was still not ready for practical use. KBBF grows thin and plate-like, making it difficult to cut at the angles needed in lasers. It took another seven years before a collaboration set up in the late 1990s between Chen's group and Shuntaro Watanabe at the University of Tokyo succeeded in getting KBBF into a laser system 2 . That is when scientists really started to get excited. Exploiting the fact that the ultraviolet light from a KBBF-equipped laser has an extremely narrow frequency range, allowing it to measure the energy level of electrons in solids down to a resolution of just 360 microelectronvolts, Shik Shin from the University of Tokyo and his colleagues were able to show that the fine spacing of energy levels in certain superconductors depends on the direction in which the electrons travel through the lattice 3 . \"It was the first time I could really say I was a pioneer, that I was seeing something that nobody else had seen,\" says Shin. That discovery, the first to be made with a KBBF laser, opened up investigations of many different kinds of superconductors that had been impossible before because there was no laser available that had sufficient energy resolution. Since then, Shin and Chen have co-authored more than 20 papers. Xingjiang Zhou, of the Institute of Physics in Beijing, was also using KBBF to examine superconductors. He discovered an entirely new type of electron pairing 4  \u2014 the fragile coupling that allows the electrons to move though the lattice without resistance. One leading condensed-matter physicist, seeking a collaboration, told Zhou in an e-mail, \"These are the highest quality data that I have ever seen.\"  \n                Clamp down \n              By 2008, thanks to these and other stories, requests for KBBF crystals were rolling in to Chen's institute. And that, in turn, caught the attention of the institute's parent organization, the Chinese Academy of Sciences, which told Chen not to distribute the crystal any further without its permission. Chen is planning to use his share of the government's 180 million renminbi to install more large ovens in which to grow crystals. This will allow his institute to ramp up from the 15 KBBF-crystal devices it made in 2008 to 50 in 2009, and then 100 in 2010. His team will also be looking for ways to produce better KBBF crystals. Thicker crystals allow for a more powerful laser and make possible hugely profitable applications, such as replacing the bulky exciplex lasers, another type of ultraviolet laser used in surgery and in semiconductor lithography. Chen, who is discussing the possibility of technology transfer with two companies in Beijing, hopes to find some commercial applications within three years. Seven other projects will be aiming to create advanced versions of photoemission spectrometers, Raman scattering spectrometers and scanning tunnelling microscopy. Zhou will receive 20 million renminbi to head two of the projects. In one, he plans to make a tunable KBBF laser that can analyse a wider variety of materials \u2014 at present, these lasers need to be set at one frequency. In the other project, he will develop a KBBF-based laser to look at the spin of electrons in superconductors. Until now, research has focused on momentum and energy. Chen is reluctant to talk about the terms of the government's restriction. He says he would like to share the crystals with people in other countries, but first has to meet demand from Chinese KBBF projects. \"The government gives me so much money,\" he says. Crystal growers in other countries are unlikely to be able to fill the gap, mainly because of the time, people, infrastructure and know-how needed to create a good KBBF crystal. Chen's laboratory has 70 people, including technicians and some 30 students. The group has learned how to make KBBF using the same factory-like process they use to develop all their other nonlinear crystals \u2014 calculating what compounds might work; synthesizing the material; growing, cutting, polishing and coating the crystals; then testing them for their vulnerability to damage by intense laser radiation and other properties. Chen's group has put in huge resources for infrastructure such as crystal-growth ovens. It has spent US$7 million just on platinum for the crucibles used to grind up the crystals during the production process. Even now, with the KBBF process honed, it takes 3 months and about US$20,000 to produce just one KBBF device. Bruce Chai, president of Crystal Photonics in Sanford, Florida, says that anyone trying to duplicate the work would easily burn through US$10 million. \"And they wouldn't be guaranteed success,\" he adds. Perhaps the greatest obstacle to making a KBBF crystal is the beryllium it contains, which can cause pneumonia-like symptoms and cancer if inhaled. China has strict policies regarding its use, says Chen, but researchers are allowed to work with the element if they have the right infrastructure. \"You need a lot of equipment and you need to move slowly,\" he says. In other countries, the restrictions are more severe. In the United States, for example, lawsuits over beryllium poisoning led the Department of Energy to hunt down and remove even trace amounts from national laboratories. \"The demand for KBBF is spurring researchers to look for other fluoroborates that present fewer challenges,\" says Vincent Fratello, vice-president of research and development at Integrated Photonics in Hillsborough, New Jersey. But until that search bears fruit, the benefits of Chen's dogged pursuit will stay in China. KBBF is a special case, but it illustrates China's growing strength in materials science. Fratello, himself a solid-state physicist, says that he has been impressed by the \"sheer volume of work that comes out of China\". Fudan University's Dongyuan Zhao has more citations than anyone in the field of mesoporous materials (and the second highest number of citations per article). Last year, after Japanese researchers discovered high-temperature superconductivity in iron-based arsenic oxides, several Chinese groups jumped in to investigate, achieving even higher temperatures within months. Zhou says that he intends use the KBBF laser to take arsenic oxide studies even further.  \n                Material leads \n              Some researchers see China as following the lead of Japan, where significant investment in materials sciences in the 1990s has been paying off in discoveries such as the superconductor magnesium diboride 5 . Panagopoulos, who last year moved from the University of Cambridge, UK, to Singapore to continue his research on functional materials, says he tried and failed to get Cambridge to bulk up its materials-synthesis capacities in the late 1990s. \"We wait for Japan, and now China, to make it,\" he says \u2014 a sure recipe for mediocrity. \"The person who has the greatest knowledge of everything about a material is the one who is distributing it to everyone,\" says Panagopolous. There is also concern in the United States. \"It is a well-known fact that the support for basic research on crystal growth is literally gone in the United States,\" says Chai. This spring, the US National Academy of Sciences will release a report on the health of, and future opportunities for, new materials and crystal-growth research. Even in trendier, related fields such as solid-state physics, China is catching up with the traditional leaders. High-temperature superconductor specialist Hong Ding had several attractive offers last year. But neither Boston University in Massachusetts, where he had been for a decade, nor any other institution could match the deal he was offered at the Institute of Physics in Beijing. \"It is a matter of time before the United States becomes alarmed by this rapid reverse of the brain drain,\" says Ding. Dessau, who tried and failed to recruit Ding, says that \"10 years ago it would have been unheard of [for a Chinese person to turn down a position in the United States]. But I wouldn't be surprised if the trend continues.\" In fact, many credit the hard-working, selfless laboratory worker for some of China's success, and Japan's before it, in rapidly increasing their capabilities to synthesize materials. Fratello says that Chinese groups \"tend to excel at studies that require patience and looking at a lot of different systems\". But in materials sciences, labour-intensive work can often be the most productive. \"That is, after all, how you discover new materials,\" Fratello says. David Cyranoski is  Nature 's Asia\u2013Pacific correspondent. \n                     Technical Institute of Physics and Chemistry \n                   \n                     Chuangtian Chen laboratory \n                   \n                     Institute of Physics \n                   \n                     University of Tokyo, Institute for Solid State Physics \n                   \n                     Shik Shin laboratory \n                   \n                     Castech \n                   Reprints and Permissions"},
{"file_id": "457524a", "url": "https://www.nature.com/articles/457524a", "year": 2009, "authors": [{"name": "Jonah Lehrer"}], "parsed_as_year": "2006_or_before", "body": "By turning neurons technicolour, Jeff Lichtman exposed the brain's wiring. Jonah Lehrer meets the 'unapologetic cell biologist' with ambitions to map every connection in the human brain. At first glance, Jeff Lichtman seems to be hanging long strips of sticky tape from the walls of his Harvard lab. The tape flutters in the breeze from the air-conditioner. But closer inspection reveals that this is not tape: it is the brain of a mouse, rendered into one long, delicate strip of tissue and fixed onto a plastic film. When the film is tilted to the light, the tissue becomes visible, like the smear of a greasy fingerprint. These smudges are the creation of a new brain-slicing machine invented by Lichtman, a molecular and cellular biologist at Harvard University, along with Kenneth Hayworth, a graduate student at the University of Southern California, Los Angeles. Called the automatic tape-collecting lathe ultramicrotome (ATLUM), the machine resembles an old-fashioned film projector with two large reels. At its centre is a fixed diamond blade that cuts continuously into a rotating mouse brain, much like an apple parer. The end result is a seamless sliver of tissue, less than 10 nanometres thick and around 5 metres long, that is deposited on the plastic film spinning around the spools. Although Lichtman appreciates the technical precision of the ATLUM \u2014 \"That's a real diamond!\" he says \u2014 he is most excited about its scientific potential. Researchers in his lab are starting to put these slices under an electron microscope to visualize the intricate web of connections between neighbouring neurons. Lichtman eventually hopes to have a 'farm' of several dozen such microscopes scanning tissue around the clock. Even then it would take months, if not years, to capture all the connections in the strip from a single brain. \"When you cut the brain this thin, there's just such a massive amount to see,\" he says. \"It does require us to think about imaging on a different scale.\" Lichtman likes to think on a different scale. In recent years, he has become a leading proponent of a new field that is working to create a connectome, a complete map of neural wiring in the mammalian brain. Currently, such a map exists only for the nematode  Caenorhabditis elegans , which has 302 neurons. The adult human brain, in contrast, contains 100 billion neurons and several trillion synaptic connections. \"I know the goal sounds daunting,\" Lichtman says. He insists that such a wiring diagram is an essential undertaking, because it will allow scientists to see, for the first time, the path that information takes as it is shuttled from cell to cell, and how all these cells and the information they transmit weave together to create a conscious brain.  \n                All in the wiring \n              As Francis Crick and Edward James wrote in a  Nature  Commentary 1  in 1993, \"It is intolerable that we do not have [a connection map of] the human brain. Without it there is little hope of understanding how our brains work except in the crudest way.\" Thomas Insel, the director of the National Institute of Mental Health in Bethesda, Maryland, notes that many of the most common mental illnesses, from autism to schizophrenia, seem to be diseases of \"faulty wiring\", in which the brain has a set of aberrant connections. \"The brain needs a connectome, just as modern genetics needed a genome,\" says Insel. \"That's the only way we're going to understand how the brain works at a detailed level, and also what happens when something goes wrong.\" As yet, Lichtman, Insel and others have not proposed a formal connectome project \u2014 and Lichtman declines to even give a rough estimate of such a project's cost, saying only that it would be a \"scary number\". But a debate is under way about whether such an undertaking would be worthwhile. Studies have shown that a majority of synaptic connections \u2014 some estimates run as high as 80% \u2014 are extremely weak and transmit few electrical signals. If that's the case, then a map of structural connections might actually misrepresent the brain's functional organization. \"Only a very small proportion of connections seem to drive network activity,\" says John Isaac, who studies synaptic plasticity at the National Institutes of Health (NIH) in Bethesda, Maryland. \"How do you know which connections are important? A wiring diagram won't tell you that.\" It is also unclear if the connectomes of different individuals could be readily compared in the same way that their genomes can be. Whereas bioinformatics can easily identify two similar genes in different genetic sequences, it is not yet clear what comparable tools will serve for identifying functionally equivalent neurons in two brains \u2014 if they even exist \u2014 or in a diseased brain versus a healthy one. Leading connectomics scientists are not deterred by these problems, saying that they will be solved only once the research is under way. \"The point is that you don't even know what's important until you see the system in its entirety,\" says Winfried Denk, director of biomedical optics at the Max Planck Institute for Medical Research in Heidelberg, Germany, and a pioneer of advanced microscopy. \"There is a tremendous virtue in completeness.\" Lichtman says the criticisms of the connectome are similar to those put forward at the start of the Human Genome Project \u2014 and he expects them to die down once the data start coming through. Indeed, Lichtman is so convinced of the connectome's value that he hopes it could transform the way that neuroscientists study the brain. He says that the typical experimental process, in which a scientist sets out to test a specific hypothesis, is simply incapable of deciphering something as complex as the human mind. \"History has shown that it's rather tough to come up with good hypotheses about how the brain works,\" he says. He thinks that scientists should rely more on inductive reasoning \u2014 the staple of nineteenth-century scientists \u2014 in which hypotheses are generated only after careful observation. \"We need to rediscover the power of looking,\" he says. \"You can learn a tremendous amount, and generate some interesting theories, just by staring at pictures of the brain.\" Lichtman is perhaps best known for pictures generated by Brainbow, a technique unveiled in November 2007, with which his team engineered individual neurons to emit more than 90 different shades of fluorescent light, from cerulean blue to heather grey, rendering mouse brains as Impressionist landscapes 2 . Lichtman is proud of the images \u2014 \"People always ask me for screensavers and stuff\" \u2014 but he says that the beauty of the Brainbow is actually a side effect of the biology. \"The prettiness is just the structure of the brain,\" he says. \"All I did was make the structure visible.\" Because each neuron is labelled with a different hue, Lichtman and others could start to untangle the knot of neural connections in the mouse brain. In this sense, the Brainbow represents an important milestone, as it promises to advance a scientific goal that was first outlined in the 1880s, when the Spanish physiologist Santiago Ram\u00f3n y Cajal set out to trace the microscopic structure of the nervous system. He used a technique developed by Camillo Golgi that darkly stained a few cells in brain tissue with silver chromate salt. Although Ram\u00f3n y Cajal was able to decipher the layout of individual cells \u2014 he compared its shape to the branches of a tree \u2014 the monochromatic pictures made it difficult to parse the connections between two neurons that were identically labelled. Where did one cell end and the other begin?  \n                Context is everything \n              Lichtman argues that the difficulty of this parsing problem has led neuroscientists to neglect neuronal connections and circuits ever since, and instead fixate on the electrophysiological activity of individual cells and the biochemistry of synapses. \"We've developed all these powerful tools that let us see what one neuron is doing,\" Lichtman says. But in his view, a brain cell by itself can't do anything; it is defined by its web. This is why, he says, it is so essential to develop tools that allow scientists to visualize neural cells and their connections on a massive scale. In 1970, neuroscientist Paul Redfern at the University of Liverpool, UK, described a peculiar aspect of the nervous system 3 . He measured the electrical signals that stimulate muscles in a newborn rat, and watched how they changed in the first 2\u20133 weeks of its life. Based on these data, he proposed that when a rat is newborn, each muscle fibre makes connections with a few dozen neurons that grow out from the spinal cord. Then a period of competition ensues until, within a few weeks, most of these connections disappear, leaving only one victorious neuron synapsed to each fibre. Redfern's idea challenged the widespread assumption that the wiring of the nervous system is predetermined and precisely choreographed. And Lichtman, who came across the paper a few years later as an MD\u2013PhD student already interested in the brain, at Washington University in St Louis, was captivated by the question of how the nervous system figured out which neuron would pair with which muscle fibre. \"I remember reading that Redfern paper and thinking that that was just the most interesting problem,\" he says. \"I wanted to understand the rules of the game.\" Lichtman soon discovered why such a basic question remained unanswered: it was virtually impossible to visualize and track individual neurons over time. Although light microscopy had advanced radically since Ram\u00f3n y Cajal, and scientists had found fluorescent dyes and other new staining techniques, most of these still rendered every neuron the same colour and so were unable to distinguish which cell connected to which. Think of the tangle of cables running from an overloaded socket: if the wires are all black, it's difficult to know which wire connects to which appliance. But if each wire is painted a distinct colour, it's suddenly possible to find the one to disconnect. The insight, which Lichtman began working on as a young assistant professor at Washington University, was that the easiest way to distinguish these cells was with a splash of colour. His first attempt, published in 1985, involved electrically stimulating axons so that they took up various fluorescent probes 4 . The experiment was successful \u2014 each neuron was a unique colour \u2014 but painstaking, because the neurons had to be stimulated one at a time. Lichtman knew he would need to take a different tack to study neurons en masse, and the development of green fluorescent protein in the 1990s, followed by its many-coloured spin-offs, gave him the tools he needed. In 2000, Lichtman collaborated with Joshua Sanes, a professor of molecular biology who was then at Washington University but who moved to Harvard University in 2004, the same time as Lichtman. They began generating lines of mice that expressed different fluorescent proteins in some cells of their nervous system, so that some mice had neon-blue neurons whereas others had cells of red, green or yellow. The results were extremely variable, with some mouse strains expressing a fluorescent gene everywhere and others showing fluorescence in only a small percentage of cells. Sanes and Lichtman began crossing these different lines of mice, so that a strain expressing a few blue-labelled cells would be mated with a 'yellow' line with many labelled cells. The end result was mouse brains with neurons expressing three different shades: blue, yellow and green 5 . \"That's when I started to get excited,\" Lichtman remembers. \"We still couldn't see that much, but I began to appreciate how powerful these fluorescent cells could be, if only we could engineer more colours.\" \n               boxed-text \n             In 2002, Jean Livet joined the Lichtman lab as a postdoc and immediately began working on the problem. \"Jeff would always say to me, 'More colours! We need more colours!'\" says Livet, now at the Vision Institute in Paris. \"The idea of a fluorescent brain became like an obsession.\" Livet's elegant solution, which he devised after several weeks of \"doodling in the lab notebook\", was to use a DNA-engineering system called Cre/  lox . Cre is an enzyme that deletes or inverts any section of DNA flanked by  lox  sequences. (The Cre/  lox  system is often used to delete genes from specific tissues and create knockout mouse strains.) Livet realized that if he could build a genetic construct that contained multiple fluorescent proteins flanked by  lox  sites \u2014 he chose red, blue and yellow \u2014 he could force the Cre enzyme to randomly 'choose' which colours to erase and which to leave (see  'Creating a Brainbow' ). \"It would be like rolling the dice in each cell,\" says Livet, \"so you never know what colour you will get.\"  \n                At the end of the Brainbow \n              The reality, however, far exceeded their optimistic imagination. Because the mice incorporated multiple copies of the genetic construct carrying red, blue and yellow proteins, the Cre enzyme mixed up a far more extensive colour palette. \"We thought we would just get cells expressing a single fluorescent colour, such as blue or yellow,\" says Lichtman, \"but instead we got cells expressing blue-blue-blue-yellow, or blue-yellow-yellow-yellow, and so on. It was a staggering result, to see a brain expressing 80 or 100 distinct colours.\" Other scientists immediately recognized the potential of the Brainbow (a name that Lichtman devised, and with which he is clearly pleased), and have begun applying the technique to visualize the nervous systems in a variety of lab animals, including zebrafish and fruitflies. \"It's an incredibly exciting technique,\" says Insel. \"We've had such a crude map of connections for so long, and this is one of those advances that allows us to really think about how the brain is wired at a cellular level.\" With his fluorescently tagged mice, Lichtman finally had a way to solve the problem that had fascinated him since reading Redfern's paper about 25 years earlier. By making maps of blood vessels and other landmarks visible under the microscope, he could return again and again to precisely the same spot of a neuromuscular junction in a newborn mouse's neck, and know from the neuron's colours exactly which cells remained and which were pruned back 6 . Researchers had mostly assumed that the pruning process followed the classic Hebbian model \u2014 \"cells that fire together, wire together\" \u2014 and that the cell with the most synapses on the target muscle, and so transmitting the most electrical impulses, would always gain control. But by observing the process, Lichtman has found that this isn't the case. \"Sometimes, the cell with only 20% of the synaptic territory takes over,\" he says. \"The underdog wins.\" Lichtman says that neurons that are simultaneously competing to form synapses with several muscle fibres will deploy resources depending on the outcome of all those competitions. A cell that has yet to win a synaptic competition will be able to send more resources to its remaining synapses. (As Lichtman puts it, \"being a loser makes you more likely to win somewhere else\".) The end result is an exquisitely organized circuit, in which every neuromuscular junction is properly plugged into the nervous system. \"The Brainbow allows you to watch this competition in real time,\" says Lichtman, who hopes to image similar competitions in the brains of mice. \"You can see the purple cell win, or watch the turquoise cell retreat. It might seem disorderly, but this is how a brain is built, one competition at a time.\" Even as the Brainbow has been illuminating aspects of brain building, Lichtman has been working on refining the method in his pursuit of the connectome. One of his priorities is to visualize the fine-grained branches of the neuropil, the web of dendrites and axons that protrude from either end of a neuron and form synapses with neighbouring cells. The neuropil is too thin to discern with ordinary light microscopes, whose resolution is fundamentally limited by the wavelength of light. This means that objects smaller than 200 nanometres can't be reliably detected. (A typical synaptic cleft, for instance, is between 20 and 40 nanometres, making it all but impossible to see.) In the past five years, several teams have come up with methods to improve this resolution, allowing fluorescent light microscopy to approach the resolution of electron microscopy. Lichtman is now working to image the neuropil with Xiaowei Zhuang, a professor of chemistry at Harvard University who developed one such high-resolution technique called stochastic optical reconstruction microscopy (STORM). The advantage of this method over electron microscopy is that it can be readily combined with the spectral information of the Brainbow, so that the dendritic arbors of each cell are properly identified. Lichtman refers to this project as \"BrainSTORM\". The challenge, however, is turning STORM \u2014 a labour-intensive method in which each image requires extensive computer processing \u2014 into an automated method capable of capturing image after image through the entire brain of a living mouse.  \n                Gathering resources \n              Lichtman knows that mapping the brain is an epic and expensive undertaking, and it is not one that he can complete alone or with a single method. Some neural connections will be best imaged with the Brainbow, BrainSTORM and other sophisticated microscopy techniques, whereas others \u2014 perhaps those buried too deep in the brain to be seen by fluorescence imaging \u2014 will be better viewed with ATLUM and electron microscopy. Gathering and processing these images for just one brain will require a new level of standardization and industrialization, something he is working towards with the electron-microscope farm and automated BrainSTORM. \"The sheer quantity of connections means that all of these techniques need to be extremely scalable, so that we can construct and analyse massive data sets,\" he says. Lichtman is starting to collect the resources he will need to work on this scale, and is leading a 'connectome consortium' that includes Zhuang and Sanes. Last November, the group won a US$10-million grant from the Howard Hughes Medical Institute to show that a high-throughput version of BrainSTORM is feasible. The NIH is funding the development of the next generation of Brainbow mice, and Microsoft Research is assisting with some of the complex computational aspects of the project. Given the resources, Lichtman is confident that the connectome will be completed. He cites recent large-scale neuroscientific projects, such as the gene-expression map created for the Allen Brain Atlas, as proof that such an ambitious endeavour is possible. And in the meantime, Lichtman takes solace in the beauty of his Brainbow images. If other neuroscientists can appreciate their full potential, he thinks, it could change the way they think about the brain. They will see the organ not as a mass of discrete anatomical areas, or as a collection of chemical ingredients, but as a vast loom of connected cells. \"This is what we are,\" Lichtman says. \"Lots and lots of connections.\"  Jonah Lehrer is a freelance writer based in Boston, Massachusetts. \n                     Transgenic strategies for combinatorial expression of fluorescent proteins in the nervous system \n                   \n                     Jeff Lichtman's Lab \n                   \n                     National Institute of Mental Health \n                   Reprints and Permissions"},
{"file_id": "457528a", "url": "https://www.nature.com/articles/457528a", "year": 2009, "authors": [{"name": "Mark Buchanan"}], "parsed_as_year": "2006_or_before", "body": "Are people's interactions driven by a primitive, non-linguistic type of communication? Mark Buchanan looks at how modern technology can reveal the basis of our powers of persuasion. You answer the telephone and hear a perky voice: \"Hello, this is Michelle from Brown's computers. I wonder if you have a few moments to talk about some of our new products?\" Sales call! Your first impulse is to slam the receiver down. But \u2014 well, at least the woman is polite. And knowledgeable. And\u2026 Fifteen minutes later, as you give the ever-cheerful Michelle your credit-card number to purchase a new Super Zippy hard drive, you tell yourself what a good deal it is, and how much you've been needing the thing. Or do you? Computer scientist Alex Pentland from the media lab at the Massachusetts Institute of Technology (MIT) in Cambridge would argue that your decision had very little to do with what the saleswoman said, and everything to do with how she said it \u2014 her intonation, say, or the fluctuating pace and amplitude of her voice. The sale was probably made within the first few seconds, he says, guided by signals you did not even perceive. \"Human behaviour is much more predictable than is generally thought,\" says Pentland. A person's responses can often be explained by \"non-linguistic behaviours of other people and simple instincts for social display and response, without any recourse to conscious cognition\". This 'second channel' of human communication acts in parallel with that based on rational thinking and verbal communication, and it is much more important in human affairs than most people like to think, says Pentland. Yet evidence for it has been accumulating in the laboratory for decades. And now modern technology has made it possible to monitor these social signals in spontaneous, natural settings on a scale that was not possible before. \"Collecting this kind of spontaneous data is extremely hard,\" says neuroscientist Anna Esposito of the International Institute for Advanced Scientific Studies in Vietri sul Mare, Italy. \"But Alex has found a way to do it.\" Indeed, over the past decade Pentland and his team have developed a range of wearable devices that include electronic badges, modified personal digital assistants (PDAs) and specially configured mobile phones. These instruments can track not only a person's location, but also his or her behavioural details \u2014 such as intonation and upper-body movements. The resulting data can then be analysed for patterns of meaningful social signals. Pentland and his colleagues have already done multiple experiments with this technology at call centres and elsewhere, and are now moving into sites such as banks and hospitals, where they collect data on hundreds of people over many months to reveal the hidden communications in organizations. The potential reward, Pentland says, is a richer, more complete and more objective view of human interaction \u2014 with our inherent bias toward what's conscious and verbal taken out of the equation.  \n                Out of control \n              \"It is difficult for people to accept\", says psychologist John Bargh of Yale University in New Haven, Connecticut, a pioneer in the study of subliminal influences on human behaviour, but much of a person's everyday life \"is determined not by their conscious intentions and deliberate choices, but by mental processes put into motion by their environment\". \"A gazillion experiments show that I can flash something at you so fast you don't see it, yet the information does bias you towards one decision as opposed to another,\" says cognitive psychologist Michael Gazzaniga of the University of California at Santa Barbara. Moreover, he says, the literature is full of experiments showing that conscious explanations for our behaviour are often just rationalizations invented after the fact. He cites the example of a patient whose corpus callosum had been severed as a treatment for epilepsy, making it impossible for one side of the brain to communicate with the other. Gazzaniga and his colleagues presented the word 'walk' to the patient's left visual field, which corresponds to the right side of the brain. When the patient stood up and began walking, they asked him why; the right side of the patient's brain had been shown to lack the ability to process language. His left brain, which never received the walk command, but which handles language processing, quickly invented a logical explanation: \"I wanted to go get a Coke.\" Analogous effects have been found many times in healthy people. In the early 1990s, for example, psychologists Robert Rosenthal of the University of California, Riverside, and Nalini Ambady of Tufts University in Medford, Massachusetts, asked college students to give reasons for liking or disliking their instructors. The students tended to mention the instructors' friendliness or the clarity of their lectures, attributing their rating to conscious assessment. But Rosenthal and Ambady found that most of this is simply rationalization. The researchers could predict how around 70% of the students would rate an instructor just by analysing the instructor's body language in 30 seconds of soundless video ( N. Ambady and R. Rosenthal.  J. Pers. Soc. Psychol.   64 , 431\u2013441; 1993 ). Pentland's efforts to track such phenomena in the wild, so to speak, date back to the early 2000s. In one experiment, the group took data over a period of several months in 2005, as operators at a call centre in the United Kingdom tried to make sales over the telephone. The operators were fitted with small electronic devices that measured variations in their tone and pitch, but not the specific words they used. Yet the researchers were able to devise an algorithm that could predict whether a call would result in a sale from only a few seconds of data. Successful operators, it turned out, spoke little and listened more. And when they did speak, their voices fluctuated strongly in amplitude and pitch, suggesting interest and responsiveness. \"If operators do it right,\" says Pentland, \"they are almost certain to be successful.\" Indeed, the call centre now uses this new understanding to train its operators to sell more effectively, and to recruit people who have the right speech patterns. Other experiments suggest that such measures could improve sales success by as much as 20%, says Pentland, which is worth millions to large industries. And in an experiment involving a 45-minute mock salary negotiation between students in a business school, Pentland says that by combining several display signals from the first 5 minutes of the negotiation, his team could predict who would come out on top with 87% accuracy. As a result of such experiments, the MIT group has identified a handful of common social signals that predict the outcomes of sales pitches, the success of bluffing in poker, even subjective judgements of trust. These signals include the 'activity level', effectively the fraction of time the person speaks; their 'engagement' or how much a person drives the conversation; and 'mirroring', which occurs when one participant subconsciously copies another's prosody and gesture. Pentland is well aware that not everyone has embraced this vision of a hidden, pervasive 'second channel' in human affairs. Although the idea is reasonably well accepted for other animals, Pentland says that the reluctance to accept it in humans lies in part because of an inherent bias in how science looks at human behaviour versus that of other species. \"If our data were collected from ape troops,\" he says, \"and we altered the semantically loaded labels a bit, talked about 'forage' instead of 'work', 'food access' instead of 'salary', they would feel entirely unsurprising.\"  \n                Keep it simple \n              Behavioural science, as he sees it, ought to seek the simplest explanation for human behaviour first, looking to simple social signals, before constructing more complicated explanations based on language and conscious reasoning. Indeed, he says, humans lived in social groups long before language evolved, and the language function presumably exists on top of a more archaic brain system for non-linguistic social signalling. The behaviour of non-human primates supports this idea, says primatologist Frans de Waal of Emory University in Atlanta, Georgia. Apes, chimpanzees and other primates \u2014 our close evolutionary cousins \u2014 lack anything like our facility for language, yet still lead sophisticated social lives through displays of power, meaningful noises and facial expressions. So it is \"incredibly naive\", he says, to take conscious verbal communication as the primary way that people respond to each other. In the future, Pentland hopes, the type of work he and his colleagues have been doing can be expanded to look in much more detail at human social interactions in larger groups. In an ongoing experiment at a large German bank, for example, they are using their sensors to focus on personal characteristics that are thought to be linked to group creativity. According to recent work on creativity, says computer scientist Peter Gloor, who studies collaborative innovation networks at MIT's Sloan School of Management and who has collaborated with Pentland, creative people tend to be more open and agreeable, and less neurotic than others, and they tend to fluctuate more between extroversion and introversion. These traits can be assessed with standard psychological questionnaires, but Pentland's group thinks the sensors can do the job more effectively, giving managers more reliable ways to identify patterns of human behaviour, and thereby to assemble the best possible creative teams. The group had 22 bank employees wear data-collecting badges for one month, and studied how the data revealed personal characteristics relevant to creativity. For example, from the sensor data, the team was able to construct a 'contribution index' for each person that measured, roughly speaking, their pro-activeness in interpersonal interactions. Someone with contribution index of 1 looks frequently at other people. Someone with an index of \u22121 is only ever looked at, he or she never looks another actor squarely in the face. The group then compared this measure with descriptors of each individual as assessed in a standard psychological test, judging variables such as the person's 'openness' towards fantasy, aesthetics, feelings and new ideas, or their 'extroversion', spontaneous warmth, gregariousness, assertiveness and activity. High values of the index, they found, correlated very strongly with the psychological assessment of extroversion. \"The more one person looks at others,\" says Pentland, \"the more of an extrovert they seem to be.\"  \n                Neural networking \n              Pentland's team also looked at the social networks in which the employees took part. In particular, the data showed that individuals who scored more highly on the 'openness' variables were much more likely than others to shift their network position from day to day, or even within a day. Part of the time they might act as a central hub in the social network of a creative team; at other times they would have much less interaction and sit on the periphery. People who are more open to new ideas, says Gloor, \"change network position frequently, being highly central at one time and less central at others\". These techniques, he suggests, offer a fundamentally new way to look at organizations, as they give specific, quantitative measures of human traits and behaviour. One possible objection to such studies is as old as social science: people may change their behaviour when they're observed. But Pentland and his colleagues think this worry is overblown. In this particular experiment, they point out, the employees knew only that their behaviour was being monitored, and were not aware of specifically what was being recorded or all the ways in which the data would be analysed. Moreover, the measurements went on for a full month. While people can take on particular roles in the short term, says Gloor, research suggests that it is harder to do so over long periods of time. Pentland is planning experiments over the next few years that could settle this question. But psychologist Bernard Rim\u00e9 of the Catholic University of Louvain in Belgium has other reservations. Some of Pentland's findings, he says, have been known since the study of non-verbal communication first took off back in the 1960s. Back then, Rim\u00e9 recalls, call-centre operators \"were trained to smile when talking, because smiling indeed influences the vocal signal, communicating warmth and positive affect\". He acknowledges that the new technology makes this research more powerful, but wonders whether it will really help to tackle the toughest challenges of the field. In particular, he says that the relative simplicity of predicting some kinds of human behaviour may paint a false picture due to the formidable diversity and contextual variety of human signalling. The link between environmental cues and displayed responses is sometimes simple, as in the call-centre experiments. But more frequently, Rim\u00e9 suggests, many other factors interfere, such as past experiences, personal habits, inherited traits, intellectual abilities and shared knowledge about the situation. \"In this case,\" he says, \"predicting people's behaviour becomes quite a challenging task, if possible at all.\" Pentland agrees that this is the key challenge, but suggests that the sensors are orders of magnitude more powerful than what was used to study human non-verbal interactions in the past. \"People used to measure mostly short-term features such as nods or smiles, whereas we can make continuous, quantitative measurements over much longer times,\" he says. \"We can also examine precise details of frequency and amplitude, something you can't do easily without automated tools.\" The earlier work, he suggests, was akin to trying to understand written text by looking just at individual letters, ignoring words or sentences. Moreover, he suggests, almost all previous work was done in the lab, and quantification was all but impossible in anthropological-style field work. \"In contrast,\" Pentland says, \"we are taking people in real situations, in day-to-day work settings over months, and finding lots of reliable and directly predictive relationships.\" One thing he does admit is that experiments of this sort raise the spectre of Big-Brother-style intrusiveness. Pentland hopes that corporations won't look at the technology as just another way to spy on employees, which would almost guarantee resentment and loss of morale. He says that many of the problems could be addressed if the companies followed the protocol used in his own group's experiments: the data were first stored locally on each subject's machine, so that at the end of the day, he or she could review what had been collected and decide whether it would be shared or permanently deleted. Pentland also says that employees should be given the ability to turn off the device if they wish. If these kinds of protections are respected, he says, then the devices could lead to fundamentally better science, including a deeper knowledge of that second channel of human social interaction. \"People have been studying this kind of stuff for a long time,\" says Gazzaniga, \"but Alex is raising it up to another level. It is very clever stuff.\"\n Mark Buchanan is a science writer based in the United Kingdom. He is author of    The Social Atom . \n                     Alex Pentland's website \n                   Reprints and Permissions"},
{"file_id": "457950a", "url": "https://www.nature.com/articles/457950a", "year": 2009, "authors": [{"name": "Emma Marris"}], "parsed_as_year": "2006_or_before", "body": "What does it mean to save a species? For some, preserving the American bison means keeping its genome pure, finds Emma Marris. James Derr has just eaten a large grass-fed-bison steak topped with onions, the banquet dinner at a meeting of the American Bison Society in Rapid City, South Dakota. As he sips his wine, conservationists and managers of nature reserves \u2014 some wearing Stetsons and some wearing polar fleeces \u2014 approach to pay their respects to the man who could be considered the godfather of the bison genome. Derr, a geneticist, is trying to reverse more than a century of human interference with the American bison ( Bison bison ). Those shaggy symbols of the American West were driven to the brink of extinction in the last half of the nineteenth century and then saved on ranches and in zoos. But the conservation efforts came at a cost. Most of the bison alive today have cattle genes rattling around in their genomes, left over from early efforts to interbreed the two species. Derr has almost single-handedly started a movement to preserve the original bison, complete with its unadulterated genome. He has managed to persuade everyone from federal officials to private conservation leaders that they should care about the cattle genes hiding in bison. And he is convinced that his approach \u2014 managing the genome rather than the animals \u2014 could be a model for conserving other large mammals. Wildlife managers have considered the genetic diversity of animals for some time, and animals in captivity have often been bred to preserve genetic diversity. But those were blunt approaches. Now, armed with genomic tools, researchers are starting to look at specific sequences in the genome, and are raising questions about what the fundamental unit of conservation should be. Most people see preserving wildlife as a matter of saving individuals; if all the individuals die out, the species becomes extinct. But that reasoning looks simplistic when considered at the genomic level. If the genes of a species change enough \u2014 through interbreeding, for example \u2014 that species will cease to exist even if individuals that look something like the original continue to thrive. Although some species interbreed naturally, humans have forced other mix-ups, and those are the cases that most worry Derr. \"Species conservation is more than skin deep,\" he says. \"It is more than how they look, it is how they are \u2014 that's the genome.\" This purist approach has won converts, but other conservationists say that the bison is an exception, and that for many species, the situation is so dire that they don't have the luxury of worrying about a few stray genes from related species.  \n                Where the buffalo roamed \n              The troubles facing bison are relatively recent. When explorers Meriwether Lewis and William Clark voyaged across the continent at the beginning of the nineteenth century, tens of millions of bison roamed in massive herds from the Mississippi to the Rockies, and from Mexico to the Arctic. By the end of the century bison numbers had dropped to a few hundred, courtesy of long-range rifles and professional hunters. At this point, a few forward-thinking people decided that the bison should not be driven to extinction. Charles Goodnight, the famous Texas cattle rancher, and a handful of other people made room for the animal on their private ranches. In 1905, then US President Theodore Roosevelt and William Hornaday, head of the New York Zoological Society (now known as the Wildlife Conservation Society), founded the American Bison Society, which collected bison and established herds in a few reserves in Montana, Oklahoma and South Dakota. A small herd, perhaps 30 in number, was still roaming Yellowstone National Park. According to Derr, all the bison in the United States today \u2014 there are now up to a million of them, mostly on private ranches \u2014 can probably be traced back to fewer than 200 bison. Half a century after those conservation efforts started, Derr saw his first bison. He was a boy, growing up in Enid, Oklahoma, just north of a small town named Bison. His father raised cattle, and Derr lived mostly outdoors, hunting and fishing. The bison that young Derr encountered on local ranches did not act like his father's cattle, and they left a lasting impression. \"They are much more athletic, very smart and incredibly quick and strong,\" he says. In college, Derr studied biology with an eye to being a wildlife manager. But he soon decided \"that I could have a lot bigger impact if I worked in the lab\". He switched to genetics and is now a professor at Texas A&M University in College Station. In the 1990s, he put together a proposal to study how the bison had passed through that 200-individual bottleneck with seemingly no ill effects. Unlike many other species that have bounced back from small numbers, bison have retained a decent level of genetic diversity. \"It is a pretty darn amazing story,\" says Derr. At their fewest, bison were preserved by people sprinkled throughout the continent. \"Just by luck, it seems that they were able to preserve, with the few bison that they got, much of the genetic diversity that was in those areas.\" Another reason the bison came so easily through the bottleneck, he says, might be because they had already squeezed through earlier genetic restrictions. In these previous population crashes, the inbreeding required to re-expand the population exposed deleterious recessive genes, and those animals died, in effect purging the genome of lethal traits. But while doing this work, Derr found foreign genes in the bison genome. Goodnight and the other ranchers may have saved the bison, but they also experimented with them. Cattle ( Bos taurus ) and bison are not too distantly related to produce fertile offspring \u2014 they diverged between 1 million and 1.5 million years ago but have the same number of chromosomes, with many genes in the same order. Although the two animals are not known to mate voluntarily, ranchers forced a few crosses in the early twentieth century, leaving the fingerprints of cattle on the nuclear and mitochondrial DNA of most bison alive today. Derr has found cattle genes in eight of fourteen public herds in the United States and Canada, and in all but one of the 50 private herds he has looked at ( N. D. Halbert and J. N. Derr  J. Heredity   98 , 1\u201312; 2007 ).  \n                If it looks and acts like a bison ... \n              When he first started his work, Derr was worried \"that there might not be any uncontaminated bison herds\". But why should he care? These bison, apart from a few very strange specimens in Texas with white faces, look like bison. They seem to act like bison. For conservationists who are interested in restoring the relationships that once prevailed in prairie ecosystems, that is probably good enough. But for those interested in 'saving the bison', a philosophical question now presents itself: are most bison not really bison? Should managers of conservation herds cull their hybrids and replace them with uncontaminated animals that have no 'introgression' of cattle genes? And does it matter if some 'real' bison genes are lost in the process, as some of these original genes can now be found only in individuals that are part cattle? For Derr, these questions aren't hard. Although behaviour and morphology are important, Derr contends that a species is its genome. \"If you don't have the genome, nothing else you do makes a damn difference,\" he says. \"What you are preserving isn't the species; it is something the hell else \u2014 a shadow.\" There is no obligation to save every version of every gene, but wildlife managers should keep pure herds from mixing with those that have cattle genes, he says. It is a simple formulation that has caught on with a number of conservationists. \"Jim has been the lead bull in bringing about a rigorous consideration of the role of genetics in bison conservation,\" says Kent Redford, head of the Wildlife Conservation Society Institute in New York and director of the American Bison Society. \"There have been those who have not agreed with Jim but the general movement has been towards the importance of his point of view.\" And he \"would not have had the success he has had without his personality\", Redford adds. \"He is charming, slightly intimidating, tireless and single-minded. I think it helps that he is most definitely a Texan.\" But not everyone is convinced. \"There are more important things than genes,\" says Rurik List, an ecologist at the National Autonomous University of Mexico, who works with a herd that spans the US\u2013Mexico border. These bison have some cattle genes, but they also have institutional memory. If List were to remove them and replace them with pure animals, would the bison still be able to find the water holes that the current herd knows so well? \"They have been behaving like bison for 80 years,\" says List. \"They have been fulfilling an ecological role.\" Greg Wilson of the Canadian Wildlife Service in Ontario agrees. \"I am less adamant about the pure bison thing,\" he says. \"Whether you have 0% introgression or 2% introgression, it seems pretty arbitrary to me.\" Until now, Derr has used about 14 loci to look for cattle genes in bison herds \u2014 that is, 14 places in which a certain pattern of base pairs will signify a cattle ancestor. The loci were narrowed down from 200 candidates in areas on the genome that have had a relatively brisk level of mutation in mammals. Derr found short spans, each of about 100\u2013200 base pairs, that gave a clear bison or cattle signal and that were far enough away from each other to constitute independent observations. However, the function of genes that overlap with or are near to these loci is unknown. So far, the only established difference between bison with and without cattle genes is their weight, a common indicator of the overall condition of an animal. According Derr's as yet unpublished work, 5-year old male bison with cattle mitochondrial DNA were on average 4.5 kilograms lighter than those without. But with just 14 loci, Derr hasn't been able to say anything about individual animals. A bison with 1% cattle genes might have them in places besides the 14 loci that Derr tests. Statistically, he needs a sizeable sample of animals \u2014 perhaps many dozens \u2014 to say for sure whether the herd has cattle genes in it. Now that the domestic cattle genome has been sequenced, Derr is working on a 'SNP chip', a ready-made test to identify thousands of individual nucleotides that will be able to tell what proportion of the bison genome is made up of cattle genes. All he needs to do to find these nucleotides is to compare the two genomes and see where the differences are. He says he needs US$150,000 and a year and a half to make it work, and that each test will cost less than $75. Derr is a persuasive man. He has won over some government officials, but whether his efforts will translate into policy changes is uncertain, especially as many of the decision-makers are likely to be replaced under the administration of President Barack Obama. Kaush Arha, the former deputy assistant secretary for Fish and Wildlife and Parks at the Department of the Interior put together a bison working group that met with Derr and other scientists in October. Arha predicts that policy will eventually enshrine Derr's thinking. Moving animals from a hybrid herd to a clean herd, for example, might become forbidden. Peter Dratch, a specialist in endangered species at the National Park Service, agrees. \"We are now in an era where we need to look at the genetics of species,\" he says. For the bison, he says, \"introductions to a herd should take place only when they do not increase overall levels of cattle ancestry\". At the same time, small, pure herds that don't have enough animals to have healthy gene pools should be moved around so as to keep the genes flowing. \"We will probably have to artificially create immigration.\"  \n                Defining features \n              Dratch even floats the idea of differentiating between 'wild' bison and those that have been commercially bred, which might be better thought of as livestock. \"A wild bison,\" he says, \"is a member of a herd of significant size with low levels of cattle or subspecies introgression that is subject to forces of natural selection \u2014 including predation, and competition for forage and breeding opportunities.\" Such a definition might have implications for the Endangered Species Act, which is notoriously unclear about hybrids. If bison with more than a certain proportion of cattle genes don't count as bison under the act, and if, as Derr suspects, only about 8,000 animals have no detectable cattle genes, then some bison might even qualify as endangered. The American Prairie Foundation took Derr's ideas seriously when it went shopping for bison to place in its reserve in central Montana. So did the Nature Conservancy when it came to stocking a preserve in Iowa. Both organizations chose bison from Wind Cave National Park in South Dakota. The Wind Cave herd is one of only two federally owned herds that Derr is confident contain no cattle genes. To a visitor, the pure bison at Wind Cave seem remarkably tame. They will approach a stopped car to lick road salt off of its undercarriage, and seem unperturbed by camera-wielding humans. It may be that when Derr's SNP chip is ready some cattle genes will be found in the animals at Wind Cave. After all, they are separated only by a chain-link fence from bison known to have cattle genes. But if introgression levels are low, individual animals with cattle genes could be plucked out. Derr hopes that conservationists will use the sequences from domestic animals to learn about the genomes of wild species that have chequered genetic pasts. Some hybrids have arisen for economic or accidental reasons. For example, cattle have bred with the massive wild gaur ( Bos gaurus ) and Southeast Asian banteng ( Bos javanicus ). Others were created for conservation purposes: birds such as the peregrine falcon, for instance, have been bred across subpopulations for reintroduction into the wild. With a toolbox of SNP chips, wildlife managers may be able to tease apart mixed-up lineages and preserve pure genomes. Or such tests might show that it would be detrimental to try to weed out domestic genes from wild populations. But the genomic approach goes beyond just working with hybrids. Looking at species as the sum of their genetic variation could help near-extinct populations. It could help managers to decide whether to bring two isolated groups of the same species together to improve genetic variation or to keep them separate. Suddenly, the world's fauna stops looking like a bunch of animals, and seems more like a group of genomes riding around in animal-shaped containers. Derr is working on a number of species' genomes, but has a special place in his heart for the bison. They symbolize everything about the West, he says. \"Bison are incredible survivors.\"\n \n                     Department of the Interior Bison Initiative \n                   \n                     American Bison Society \n                   \n                     National Bison Association \n                   \n                     James Derr \n                   Reprints and Permissions"},
{"file_id": "4571077a", "url": "https://www.nature.com/articles/4571077a", "year": 2009, "authors": [{"name": "Richard Monastersky"}], "parsed_as_year": "2006_or_before", "body": "As change in the Arctic accelerates, scientists and indigenous peoples have pressing reasons to work together, reports Richard Monastersky. Indigenous peoples in the Arctic have long complained that the weather doesn't behave the way it used to. Climate scientists have by and large ignored them \u2014 until a few researchers looked into the data and found hints that the locals knew what was what. \"In the high latitudes, there have been a couple of studies in the past few years that find some support for the contention that the weather is becoming more variable, less predictable,\" says John Walsh, a professor of climate change at the University of Alaska at Fairbanks. In a 2005 study, Walsh and his colleagues uncovered evidence in the records of surface weather stations that Alaska and northern Canada experienced high-temperature extremes more frequently in the 1990s than they did from the 1950s to the 1980s (J. E. Walsh  et al .  Atmos. Ocean   43,  213\u2013230; 2005). Such a change would indeed make the weather less predictable for the people who live there. Walsh thinks that he and other scientists were too quick to write off the traditional knowledge of Arctic residents. Indigenous people have a deep understanding of their environment, and researchers must start paying more attention to what they say to catch the changes speeding through the far north, says Walsh. \"We need to take a hard look [at local claims], and do the confirmation where it's feasible,\" he says. Polar researchers of the past generally treated Arctic peoples as data points \u2014 or simply ignored them. Scientists would drop into a region, grab their measurements and vanish, rarely letting the residents know the results of the study. Now the relationship is getting more complex, for a raft of reasons. As well as seeing native peoples as potential collaborators and informants, scientists are also starting to see the importance of treating them as stakeholders with an interest in the results. And in some cases, governments led by indigenous people hold political power and must sign off research proposals. In the hope of bridging the gap between scientists and the native inhabitants, countries participating in the Fourth International Polar Year (IPY), which finishes in March, supported some 30 projects that sought to tap into traditional knowledge of the Arctic. David Carlson, who heads the IPY programme office in Cambridge, UK, says that the IPY \"set out with the idealistic goal of having the northern residents be partners in the science \u2014 not just subjects of study\". Such a partnership might build up research capacity among the indigenous population, advance scientific understanding of the Arctic and help native populations to adapt to climate change. Still, both scientists and native peoples say it is too soon to tell whether the IPY projects will bring much lasting change and will benefit both interest groups. Victoria Gofman is optimistic on the subject. She is the executive director of the Aleut International Association based in Anchorage, Alaska, and principal investigator on the Bering Sea Sub-Network Project, a two-year IPY project surveying more than 600 residents in six villages bordering the Bering Sea \u2014 three in Russia and three in Alaska. The project members are asking local subsistence hunters about environmental and wildlife conditions in the Bering Sea and how they are changing. The aim, Gofman says, is to find a way that local knowledge can be integrated into data sets. The US National Science Foundation is funding the work to the tune of US$600,000.  \n                Local benefits \n              The project is already starting to make a difference in the 650-person village of Gambell, on St Lawrence Island, Alaska. Iver Campbell, vice-chairman of the native village council, has interviewed about 50 residents for the project; he says they appreciated the chance to share their knowledge. \"They want to make their voices heard. Everybody wants to be a part of this investigation of global warming.\" They are also curious to learn what scientists know about the changing conditions, he says. Branson Tungiyan, the general manager for the native village, has a different take: he says the residents are tired of being studied and that the Bering Sea study will not significantly aid locals. \"We don't see much benefit for Gambell,\" he says. How much benefit governments get, in terms of information that scientists and officials can use, remains to be seen, as the project is still gathering and analysing its data. But Gofman argues that the project has already helped the local communities, most obviously by employing people in the villages as data collectors at various rates of pay, and defends the idea of such work as a good vehicle for transferring traditional knowledge in a time of change. In the Canadian territory of Nunavut, a study of polar bears is yielding more obvious mutual benefits for scientists and the native inhabitants, say participants. Moshi Kotierk, a co-investigator on the project, says the IPY has helped local communities by involving indigenous residents in the work, either as research assistants or, in his case, as a project leader. He thinks that this sort of approach will build the capacity for future scientific work in these areas. Elizabeth Peacock, the Nunavut government's polar-bear biologist and principal investigator for the study, says she often relies on local residents and their knowledge of polar-bear behaviour. Her Inuit friends and contacts can tell her the locations of polar-bear dens, for example, or explain odd behaviour. Recently, she was puzzled when a male bear with a satellite transmitter stopped moving for six weeks, acting like a female in her den rather than hunting seals as would normally be the case. An Inuit hunter solved the mystery by telling her that male bears sometimes rest if they are already fat and want to preserve their energy for the best seal season later in the winter. That's not an insight you'll find in the scientific literature. \"It took me talking to a hunter to understand that,\" Peacock says. IPY leaders point to another successful cross-cultural collaboration, studying reindeer herders and their vulnerability to change across Eurasia. The project, known as Eal\u00e1t, after the word for 'good pasture' in the Saami language spoken in Lapland and beyond, is run by Ole Henrik Magga at Saami University College in Kautokeino, Norway, and Svein Mathiesen of the Norwegian School of Veterinary Science in Oslo. Some 3 million reindeer sustain hunters and herders in more than 20 different ethnic groups across northern Eurasia. The Eal\u00e1t team is interviewing herders from a number of these groups, studying the climate at local scales, and investigating the political, social and economic factors that affect herders and their ability to adapt. By amassing such data, the researchers hope to mitigate the effects of climate change on herding societies. Because herders are so central to the research, the Eal\u00e1t project has spurred an interest in science within the herding community, says Johan Mathis Turi, secretary-general of the Association of World Reindeer Herders and a herder himself. \"Young reindeer herders are pursuing higher education and are being included in knowledge production in a way we haven't seen in reindeer society before,\" he says. Currently, five students from herding communities are pursuing doctoral degrees as part of the Eal\u00e1t project. That will help herders interact with policy-makers, says Magga. \"We need to strengthen the communication between herding communities and the government.\" With data from projects still flowing in, scientists cannot say how broadly, if at all, they will incorporate local knowledge into their data sets and analyses, and how much improvement they may see. Outside the polar community, there are grumblings about including traditional knowledge in scientific studies. Two Canadian researchers published a book last year challenging government policies that require resource managers in the northern territories to take such indigenous knowledge into account (F. Widdowson and A. Howard  Disrobing the Aboriginal Industry  McGill-Queen's University Press, 2008). That argument, however, does not win support within the polar-year leadership. Carlson applauds the projects that have involved native populations in scientific research, paving the way for future collaborations. \"We would say our short-term success has been very high,\" he says, \"and that the challenge remains to convert that into a long-term success.\" Richard Monastersky is a features editor for    Nature    in Washington DC.   See  \n                     pages 1072 \n                   ,  \n                     1074 \n                    and Editorial,   page 1057 \n                     International Polar Year website \n                   \n                     Reindeer Herders' Project website \n                   \n                     Nunavut Polar Bear Project website \n                   \n                     Bering Sea Community Based Monitoring project website \n                   Reprints and Permissions"},
{"file_id": "4571072a", "url": "https://www.nature.com/articles/4571072a", "year": 2009, "authors": [{"name": "Quirin Schiermeier"}], "parsed_as_year": "2006_or_before", "body": "As scientists celebrate the end of the International Polar Year, they see causes for concern on the frozen horizon, reports Quirin Schiermeier. It is not often that inaction earns a spot in history, but a French research team has managed to write itself into the annals of polar exploration by simply waiting \u2014 albeit in a perilous spot. The record-breaking feat happened on board a privately owned French schooner,  Tara , which was frozen solid in thick Arctic pack ice. For 17 months, the ship and its 10-member scientific crew passively rode with the ice, and on 28 May 2007, the ship came closer to the geographic North Pole than any ship ever before. At a latitude of 88 degrees and 32 minutes north, the  Tara  was just 160 kilometres away from the pole. That was not the only remarkable aspect about this expedition, one of many newsworthy cruises under the flag of the fourth International Polar Year (IPY), which runs from March 2007 to March 2009. Scientists on board the  Tara  had planned to float with the pack ice for two full Arctic winters, but their trip ended almost 9 months early because of extremely rapid ice movement \u2014 an indicator of just how fast the Arctic is changing. \"We realized after the first winter that we had drifted much faster than predicted,\" says Jean-Claude Gascard, an oceanographer at the Pierre and Marie Curie University in Paris and chief scientist of the expedition. The early exit from the ice came at an inopportune time \u2014 it halted data collection much earlier than planned and forced the crew to make a dangerous run towards the island of Svalbard. \"Sailing back to Svalbard in the middle of the Arctic winter was a pretty precarious thing to do,\" says Gascard. Its success in the face of uncertainty made the  Tara  expedition \"symbolic for the spirit of the IPY\", says David Carlson, an oceanographer who heads the IPY programme office in Cambridge, UK. \"To me it was its most iconic moment.\"  \n                Global challenge \n              This IPY has seen some 50,000 scientists, students, technicians and crew participate in expeditions and scientific programmes focusing on the Arctic and the Antarctic. Its 170 collaborative international projects have covered disciplines from ecology to astronomy, from the social sciences to human and animal health. In the Arctic, indigenous people and their traditional knowledge became part of the science wherever possible, sometimes inspiring it in unanticipated ways (see  page 1077 ). It might seem that, as so often in the past, science reigns supreme at the planet's poles. But as climate change opens up vast parts of the Arctic to commerce, nations are starting to exert their influence in the region more purposefully, and long-simmering political tensions might soon boil over. The current IPY extends a tradition that reaches back almost 130 years. Each of the three previous polar years \u2014 in 1882\u201383, 1932\u201333 and 1957\u201358 \u2014 kicked off a new era in polar science. In the most recent IPY, the main scientific challenge was to explore the most remote and hostile spots on the globe. It was an era when daring graduate students could still get their names on peaks never before seen by human eyes. The focus over the past two years, by way of contrast, has been on acronym-bearing studies that encompass the physical, biological and social components of the polar environments. As in other areas of science, many polar researchers now take a systems approach, examining, for example, interactions between glaciers, ocean circulation, sea ice, algae and seal health. These and many other parts of the polar system have been studied before. But the scope of activities carried out during this IPY would have been unthinkable without the technological advances of recent decades, including such tools as satellites, laptop computers and high-tech clothing. Most importantly, this IPY has proceeded amid galloping climate change, which has already altered the Arctic in dramatic ways and is increasingly making itself apparent around Antarctica. As the  Tara 's speedy voyage illustrates, the scale of the polar changes dwarfs what many researchers would have thought possible even just a few years ago. \"There is no final word yet as to what has made the ice drift so fast,\" say Gascard. \"But it is pretty clear that changing wind patterns and changes in the thickness, structure and concentration of ice have all played a part.\" The speed of the expedition was hardly a coincidence, he adds. On an ice floe somewhat farther south, a Russian team camped out over winter and moved even faster than the  Tara . In fact, the Russians drifted faster than any other Arctic ice station established by that country over the past 34 years. Compared with the era of the Norwegian explorer Fridtjof Nansen, who took a roughly similar cross-Arctic trip on board the  Fram  115 years ago, sea ice seems to be drifting almost twice as quickly. The fast movement means that sea ice is more likely to make it to the Atlantic, and that is one of the reasons for its dwindling extent: the ice cover reached a record low in September 2007, followed by only minimal recovery last summer. The Canadian icebreaker  Amundsen  sailed for a full year along Canada's entire Arctic coastline in flaw leads, semi-permanent patches and channels of open water between land-fast ice and the drifting pack. That expedition conducted the most extensive survey ever of how retreating sea ice affects marine ecosystems (see  _Nature_ 454, 266\u2013269; 2008 ). The 400-person, US$40-million expedition, one of the largest IPY projects, yielded a plethora of data, samples and  in situ  observations. Analysing the results will keep scientists busy for years.  \n                Lone wolves \n              The scientists involved in the  Amundsen  project and almost all those working in other IPY ventures made it through their expeditions safely, although a helicopter crash in March last year claimed the lives of a pilot and a technician near Germany's Antarctic station Neumayer II. And that was not the only setback for the polar-year programme, which also ran into rough political waters involving tensions between the East and West. The  Tara  expedition, for example, almost had to be cancelled when Russian authorities denied necessary logistical support, preventing the crew from leaving the Siberian port of Tiksi for two weeks. At almost the last possible minute, the authorities finally gave the green light for a Russian icebreaker to accompany the  Tara , allowing the schooner to set sail. Many scientists had hoped that this IPY would open doors on a new era of Arctic and Antarctic collaboration with Russia, which has in the past conducted most of its polar science activities in isolation. These hopes, however, were not fully realized. The bulk of Russia's polar-science programme during this IPY consisted of activities conceived with a national focus. Russia's planting of a flag on the Arctic seabed in 2007 symbolized its claims to a large chunk of the Arctic Ocean and reflected its national interests in the region (see  _Nature_ 448, 520\u2013521; 2007 ). \"We are proud of our long tradition in polar science and exploration,\" says Denis Moiseev, deputy director of the Murmansk Marine Biological Institute of the Russian Academy of Sciences' Kola Science Centre. \"We are going on expeditions every year to do ecosystems research in polar regions, almost independent of whether or not there happens to be an International Polar Year.\" But the IPY, to which the Russian government contributed some US$10 million, has at least facilitated some joint undertakings with Western countries. The best example is the International Siberian Shelf Study, which looked at climate change, permafrost and ocean chemistry along Russia's vast northern coastlines and involved some 30 scientists from four other nations. The expedition, which travelled aboard the Russian research ship  Jacob Smirnitskyi , came about in part because of personal contacts. Its chief scientist, Igor Semiletov, bridges two countries, as a senior researcher at the Pacific Oceanological Institute in Vladivostok and a visiting scientist at the University of Alaska in Fairbanks. Russia was one of four nations \u2014 alongside Canada, Norway and the United States \u2014 to be involved in more than half of the IPY projects. For the first time ever a Westerner was invited to Russia's drifting ice station (see  _Nature_ doi:10.1038/news.2008.956; 2008 ). Furthermore a Russian\u2013French\u2013Italian team undertook one of the traverses of Antarctica during this IPY. And scientists from Norway were involved in smaller research projects with Russia on reindeer herding and human health in Siberia. Warming in the Arctic, and the retreat of summertime sea ice, is opening up the region to interests such as mineral exploitation, shipping, fishing and tourism. Some researchers fear that the commercial potential could shift international interactions from mainly scientific collaboration to hard-nosed politics. Environmental groups such as Greenpeace have proposed a 50-year moratorium on all exploitation in the Arctic, but this is unlikely to gain much support. The shift towards economic and geopolitical competition poses a new threat for vulnerable Arctic environments, which should prompt scientists to speak out, says Oran Young, an expert on international governance and environmental policy at the University of California, Santa Barbara. \"Whither the Arctic? What's needed is science- and ecosystem-based management, rather than a race for Arctic resources with inevitable clashes of national interests,\" he says. \"Scientists can, and I believe should, intervene to influence the path taken in the years to come.\"  \n                Friction in the cold \n              It is too early to judge how IPY science can help improve the management of the poles, says Young. \"It has certainly done no harm,\" he says. One way forward is for scientists to start applying the results of their research. \"I would like to have seen more collaboration and more extensive dialogue between polar researchers and the wider environmental-change community. It is not enough to read each others' writings. We also need to bring together the scientific communities and policy-makers at venues and conferences,\" he says. With climatic and economic stakes growing at the poles, political leaders around the world are devoting more attention than ever to the Arctic and Antarctic. A record number of 63 nations participated in this IPY. Countries such as China, Belgium, Poland and Spain each took part in more than 10% of IPY projects and have joined the previously small club of nations that drive polar science. A fair number of scientists from more unlikely nations, including Bermuda, Brazil, Kenya, Mongolia, Uzbekistan and Vietnam, have also been involved. Despite that attention, funding was not spectacular. Overall, the participating nations spent some US$1.2 billion on IPY research, but only around one-third of that was money over and above typical annual investments in polar studies. Four nations \u2014 the United States, Canada, China and Norway \u2014 provided three-quarters of this additional money. But, says Carlson, even modest funding had a considerable effect in countries such as Brazil, Malaysia and Portugal, which had almost no money available for polar science before the IPY. One-third of IPY projects took place in and around Antarctica. Two of the biggest projects were the largest-ever census of Antarctic marine life and a series of six coordinated inland traverses of the unpopulated continent. Researchers made the long-distance tractor trips, at high altitudes and extremely low temperatures, to measure the conditions and drill ice cores in areas from which no  in situ  data had ever been collected. Besides gathering basic information about the climate's history and the geology below the ice sheet, these expeditions also aimed to provide 'ground-truth' data to calibrate satellite recordings. By comparing data taken at the surface with satellite-based readings, researchers can more precisely track changes in the ice thickness and its movement, among other parameters. The US\u2013Norwegian traverse of east Antarctica took a round trip route from the Norwegian Troll station on the coast, to the South Pole and back. Along the way, it passed the notorious Pole of Inaccessibility, the spot in Antarctica farthest from the Southern Ocean, much more difficult to reach than the geographic South Pole. The team took the first-ever surface-based measurements of sub-glacial lakes in the area, which lie hidden beneath three to four kilometres of ice. \"It had the air of true exploration, the way it was done in the early days,\" says Jan-Gunnar Winther, director of the Norwegian Polar Institute in Troms\u00f8, who co-led the traverse with Mary Albert of the US Army's Cold Regions Research Engineering Lab in Hanover, New Hampshire. \"Such large Antarctic projects require an awful lot of international coordination in terms of logistics, science and funding,\" says Winther. \"Without the IPY, many of the things we were able to do here just wouldn't have happened.\" The global financial crisis came towards the end of the IPY and so did not cripple operations as much as it might have, had the timing been different. However, roaring fuel prices, which peaked last summer, did hit all polar operators. Some Antarctica programmes funded by the US National Science Foundation, such as a geophysical study of the Gamburtsev Mountains in the east of the continent, got less air support than planned, for example (see  page 1062 ). It also meant delaying the deployment of seismic sensors for the Polar Earth Observing Network, aimed at measuring ice-sheet mass loss in western Antarctica. Despite these problems, this IPY has yielded enough data for decades, and a solid baseline against which future observations can be compared. But Carlson says that the IPY must do more than just reveal the current state of the poles. The task ahead will be to translate observations into more reliable predictions. \"What we've seen during the IPY shows us how unskilled our predictive ability still is,\" Carlson says. \"We knew that the system is in change, but we're only beginning to realize how deeply.\"  \n                Lost chances \n              The legacy of the IPY brings with it a few serious challenges. It remains unclear, for example, whether there will be enough support to create two proposed systems of polar monitors, the Sustaining Arctic Observatory Networks and the Southern Ocean Observing System, its Antarctic counterpart. Such basin-wide ground-observation networks are crucial for improving researchers' ability to model the ice caps and the heat flux that enters and leaves the oceans. Measurements of the thickness of sea ice, for example, are currently available from only seven points in the Arctic. And large parts of Antarctica's interior and surrounding oceans are even less well monitored. Moreover, a functioning data archive, which would allow scientists around the world to access all data and observations collected during the IPY, is not yet in place. \"In terms of data management we just didn't achieve what we wanted,\" says Carlson. \"That's a big failure on our part for which I take responsibility.\" The IPY data-management committee is sorting out the options for setting up a fully integrated data-sharing system. But the polar research communities hope that it will also be possible to maintain the general momentum and level of enthusiasm. \"Truth is there's not enough money to sustain everything,\" says Carlson. Therefore, he says, nations must invest in the most crucial measurements. Top on the wish lists of many scientists would be a permanent system to measure ice thickness in the Arctic. Even as the IPY draws to a close, nations are already attempting to secure its legacy. On 22 February, environment ministers from Norway, Russia, Denmark, Sweden, China and Britain visited the Troll station to welcome back the participants of the Norwegian-led Antarctica traverse. The gathering, says Winther, served as a reminder to politicians that even after the end of the IPY, they must not forget the scientific value of the poles. \"There's a lot we could celebrate,\" he says. \"But what's on our radar, much more than celebration, is continuity. Without a strong legacy, a lasting bonus, the beautiful lessons we've learned will be of much less value. We've seen the fruits of outreach and cooperation between research councils \u2014 we must not fall back into isolated efforts and national funding.\" Just as important, says Carlson, will be to nourish the enthusiasm of the thousands of young scientists who participated in the IPY, and to secure the funding they need to pursue careers in research. There is, indeed, no lack of interest and ambition. Since its foundation in 2006, the Association of Polar Early Career Scientists, a grass-roots organization of young polar researchers, has grown to some 1,400 members worldwide. \"We all have been hugely inspired by the IPY,\" says Jenny Baeseman, an adjunct assistant professor at the International Arctic Research Center in Fairbanks, Alaska, and co-founder of the association. \"Now we want to stay involved in the science and in the political debate.\" \n                 See Editorial,  \n                 \n                     page 1057 \n                   \n               \n                     Nature Reports Climate Change \n                   \n                     International Polar Year website \n                   \n                     Tara Expedition \n                   \n                     Association of Polar Early Career Scientists \n                   Reprints and Permissions"},
{"file_id": "4571074a", "url": "https://www.nature.com/articles/4571074a", "year": 2009, "authors": [], "parsed_as_year": "2006_or_before", "body": "The International Polar Year (IPY) has covered two full annual cycles at both poles. Consisting of 170 projects, it has involved more than 60 countries and cost about US$1.2 billion. Reprints and Permissions"},
{"file_id": "458022a", "url": "https://www.nature.com/articles/458022a", "year": 2009, "authors": [{"name": "Helen Pearson"}], "parsed_as_year": "2006_or_before", "body": "Running one of the biggest academic labs in America gives Robert Langer almost 100 people to help and advise; his BlackBerry gives him the rest of the world. Helen Pearson joins the throng. At 16.26 in the afternoon on an icy Tuesday in January, Robert Langer is in his office at the Massachusetts Institute of Technology (MIT) with a Harvard University freshman called Lulu Rebecca Tsao. Langer met Tsao last year in Finland when he and her stepfather were collecting awards from the Millennium Prize Foundation. Now she is in Cambridge, she has come to ask his advice on which projects to do, and Langer offers to show her around the lab. A quick tour would be great, she says. It will have to be quick. In my hand there is a three-page printout of the day's schedule provided by Bethany Day, the assistant who keeps Langer's diary. He has four minutes until what would be the fourteenth meeting since breakfast \u2014 if he had had breakfast. And his research lab is not a thing to tour in four minutes: try four hours. It is the biggest in the chemical-engineering department, and probably the biggest in MIT. It may well be one of the biggest academic labs in the world under a single principal investigator. Its 1,300 square metres take up most of this floor of MIT's building E25 and some of a floor above. But Langer doesn't mention any of that. He leads us from room to room pointing out postdocs and pausing at embryonic stem-cell cultures. At the doors, he peers over the top of his glasses at a list of key codes that Day has helpfully printed out while he carefully punches numbers into the locks. Langer has a lab of more than 80 people, has authored in excess of 1,000 papers and holds more than 300 patents with almost as many pending. Those patents have been licensed or sublicensed by more than 200 companies, about two dozen of which Langer took a key role in founding. His 73-page CV (in small font, single spaced) starts with a 1970 chemical engineering degree at Cornell University in Ithaca, New York, and ends with patents pending in biodegradable shape-memory polymers. I have come to spend the day with him to get a sense of how it is possible for such a monster of productivity to do what he does \u2014 and why he does it. In answer to the latter question, he says that he has only ever wanted to help people, make them happy and do good in the world. \u201cIf people feel good about themselves, they will solve problems.\u201d When I first heard this, the previous evening, I thought it sounded trite. By the time he, Tsao and I are touring the lab, I've come to think it pretty unvarnished truth. \n               06:15 \n             Langer is up and pulling on his shorts. When his father died from heart disease aged 61, Langer, then 28, gave up eating meat and started exercising, something he now does for two hours or more each day. Now 60, he uses the time in his home gym to work and read, sometimes scrawling notes on the gym-machine console. This morning, he reads  The Boston Globe , starts skimming through the nearly 200 grant proposals he is reviewing for the Bill & Melinda Gates Foundation's Grand Challenges in Global Health and listens to country music \u2014 his favourite. He skips breakfast but for a few sips of Diet Coke. These first hours, I must admit, are hearsay. I had suggested that our day together run from waking to sleeping but Langer \u2014 in consultation with his wife \u2014 understandably declined. \n               08:45 \n             Langer picks me up at the hotel near his office in his beige Mercedes-Benz E350. It's only a few blocks to his office, but the trip is long enough to pass two or three of the biotechnology companies he has started. Langer traded up from a clapped-out Ford Pinto to a Mercedes Baby Benz when he received his first consulting fee in the 1980s. He gets a new one every five years. \n               09.37 \n             I am not the only media person around today. A film crew is setting up in Langer's office; he puts on a jacket (his black jeans and brown shoes are out of shot) and starts answering questions about his achievements. It's an educational video for the website of the Charles Stark Draper Prize, a US$500,000 award sometimes called a Nobel prize for engineering. Langer won it in 2002. The woman organizing the shoot told me that some of the other winners were impossible to pin down, but Langer was happy to oblige. Langer recently read  Outliers , a book in which Malcolm Gladwell makes the case that exceptional people get where they are partly because of the exceptional circumstances in which they find themselves, rather than through exceptional ability or sheer hard work. Langer concurs. There is a personal aspect, he says, \u201ca combination of stubbornness, risk taking, perhaps being reasonably smart and wanting to do good\u201d. But there is also just the chance of what turns up. As a young chemical engineer keen not to work in an oil company he tried to find a position in teaching or at a medical school instead. He had no success until Harvard University's Judah Folkman gave him a job isolating molecules that inhibit blood-vessel growth. It was the right place: Langer says he was like a kid in a candy store, overcome by the sheer number of interesting medical problems that might yield to his engineering know-how and imagination. He isolated the angiogenesis inhibitor Folkman wanted (R. Langer  et al .  Science   193,   70\u201372;  1976) and went on to make a porous polymer that controlled the rate at which such large molecules were released (R. Langer and J. Folkman  Nature   263,   797\u2013800;  1976). The ideas took some time to catch on: both biologists and polymer chemists found them absurd. Now he is widely credited with founding the fields of controlled-release drug delivery and tissue engineering. \n               10:45 \n             Phil Hilts, who heads MIT's Knight Science Journalism Fellowships programme, wants some advice on good people in nanotechnology to invite to a 'boot camp' for journalists. He is possibly the 15th person to ask Langer for some advice so far today. Everyone wants access to his network and his experience, and he obliges. In  The Audacity of Hope , Barack Obama recounts asking Langer's advice on stem-cell research in 2006. Langer replied that more stem-cell lines would be useful, but \u201cthe real problem we're seeing is significant cutbacks in federal grants\u201d. \n               11:13 \n             On the way to and from the bathroom, Langer deals with seven or eight e-mails, including editorial advice on a paper being considered for  Proceedings of the National Academy of Sciences . A passion for his BlackBerry is another link to Obama. At every moment he is not talking to someone directly, he slumps into a characteristic stoop over the device. His computer, by contrast, has not even been switched on so far today. All its processing power would make little difference to the speed at which Langer \u2014 a one-finger typist \u2014 sends messages. Not that much difference could be made. There is rarely more than a few minutes between sending Langer an e-mail and receiving a BlackBerry reply. \n               11:45 \n             We walk across the snowy MIT campus to the room where he will be lecturing. On the way he points out the brown scaffold skeleton of the new David H. Koch Institute for Integrative Cancer Research; his lab will move there in 2010. He will increase his lab space to almost 1,900 square metres, although he says he plans to give his existing lab members more space rather than recruit more staff. \n               12:17 \n             Langer wolfs down some (vegetarian) lasagna and a chocolate cookie, then starts his lecture. (Another of his five full-time administrators, Ilda Thompson, spends much of her time putting together his talks from six or seven templates.) This one is for undergraduates, part of a programme to teach them about 'real life' skills such as starting companies. \u201cWe have the Tiger Woods, the Michael Jordan of engineering,\u201d says programme director Susann Luperfoy as she introduces him. \n               12:28 \n             Everyone who is even remotely thinking about starting a biotechnology company should listen to this lecture; it would probably save millions in wasted venture capital. Langer has boiled down the requirements for starting a biotech company to a set of clear bullet points. (Do you have a platform technology, a seminal paper and a blocking patent? If not you may be in trouble). Then he recounts six of his own success stories. \u201cDazzling,\u201d says my neighbour at the table as Langer rounds up his talk. As we walk back to his office, a small Mars rover appears to be making its way through the snow. \u201cThese kinds of things happen at MIT,\u201d he says. \n               13.35 \n             Langer is embracing Smadar Cohen, once his postdoc and now a professor at Ben Gurion University in Tel Aviv, Israel. He says that nothing makes him prouder than his 180\u2013200 former students and postdocs now heading academic labs of their own. Cohen is involved in a new biotech 'incubator' for promising academic research projects called Pharmedica, based in Haifa, Israel. She and Yoram Rubin, the chief executive, have flown here largely for 30 minutes of Langer's time; their questions are how to raise money and which field to specialize in. Langer tells them that their incubator needs to be closer to having a product if they want to persuade venture capitalists to invest the sums that they are thinking about ($10 million). \u201cI always think to put money in they need to be scared enough that if they don't, they're going to lose something big,\u201d says Langer. In terms of specializing, he says, \u201cyou have to look at what the cutting areas are and who has the intellectual property (IP) rights\u201d. Neuroscience is a huge area, they agree. But, he adds, \u201cI don't like to set boundary conditions before you need to\u201d. \n               14:00 \n             A no-show. Natalia Rodriguez, an undergraduate student who has been working in Langer's lab for the past two and a half years, has never had a one-on-one meeting with Langer. Today she had scheduled 15 minutes. Where is she? \n               14:16 \n             Langer has essentially built his own interdisciplinary research institute in E25 \u2014 chemical engineers, cell biologists, chemists, physicists, material scientists, geneticists, medical doctors, mechanical engineers and mathematicians. \u201cI don't think you could do a lot of the things we are doing without a lot of people,\u201d he says. To run the group, he has three right-hand people, senior researchers at MIT who have elected to stay in his lab. One is Dan Anderson, who has come to the office to talk about further expansion. \u201cWe need to hire more people and figure out how to get them,\u201d he says. The reason they need to expand is that the Juvenile Diabetes Research Foundation, which has awarded the lab a large grant to develop biocompatible polymers for encasing pancreatic islet cells, has recommended they get an immunologist on board. They talk about hiring a joint-postdoc with another lab, but then Langer says, \u201cit's probably easier to have our own\u201d. He picks his book of National Academy Fellows off the shelf and turns to immunology. \u201cI knew quite a few of these people. Frank Austen. Fred Alt. Irv Weissman, he's really interested in tissue engineering.\u201d \n               14:32 \n             Charles Jennings wants to consult Langer about an IP issue. As head of the neurotechnology programme at MIT's McGovern Institute for Brain Research, it has fallen to Jennings to hammer out an IP agreement for discoveries made by researchers at the institute \u2014 somewhat like a prenuptial agreement for future income. In general, MIT policy is that income from IP is split three ways between the inventors, their department and MIT. Jennings has drafted an agreement to sort out how this would work at the McGovern Institute, which often involves collaboration across several departments. Langer is concerned that MIT's Department of Brain and Cognitive Sciences (BCS) might not like it. They agree it would be bad, as Langer says, if three years from now \u201cthe head of BCS says how did you guys pull this off? We got nothing and you get 100%.\u201d \n               15:00 \n             A smell of perfume fills the air. Rob Robillard and three well-made-up young women (Jamie, Amber and Michelle) file into the office. As well as starting a bevy of companies that make biomedical devices, drugs or delivery systems, Langer also helped found Andora, which is now called Living Proof. The company, also based in Cambridge, uses chemical engineering to design hair and beauty products. Robillard is the chief executive. Its first product, No Frizz, seals the gaps in the hair shaft so that water cannot enter, thus attempting to live up to its name. The three young women will be training beauty consultants across the United States when it is officially launched there in February (and on the QVC shopping channel). \u201cIt's spreading through the MIT campus,\u201d says Robillard. Through starting this company, Langer now has his (frizzy) hair cut for free by a top hair stylist. \n               15:30 \n             Langer parcels out wisdom and contacts in 15- and 30-minute slots. To the undergraduate student wanting advice on courses: \u201cThe most important thing you can learn is fundamentals.\u201d If you want a placement in a company, \u201cwe can arrange that\u201d, he says. He gives Tsao the lab tour. A button has come undone, unnoticed, on his shirt. Langer told me earlier that he does wonder whether he needs to be better at saying no to things. But \u201cI don't like to hurt people's feelings\u201d, he says. \n               16:38 \n             We are slightly late to meet Rodriguez, whose appointment was rescheduled. She missed her original slot because she was trapped in an elevator for 45 minutes. She looks close to tears, but not because of the elevator; she's having difficulty deciding whether to accept a job offer from Merck, or whether to go to graduate school. Langer tells her there is no wrong choice. \u201cWhat do you feel in your heart you want to do?\u201d he says. \u201cI think I'm gonna work,\u201d she says eventually, looking unconvinced. \n               17:10 \n             Langer walks upstairs to a conference room filled with a throng that, in most other labs, would be an all-hands meeting. But this is just the undergraduates who work here; he has organized a pizza and soda session in an attempt to make himself accessible. As they introduce themselves (he doesn't know all their names), their varied projects outline the sheer scope of the lab's activities: stem-cell regeneration; contact lenses that release drugs; lipid parcels that deliver small RNAs; biomaterials for insulin delivery; DNA vaccines. What is your favourite discovery, they ask. (His 1976  Nature  paper.) Where do you like to travel best? (Paris. Maui. Where the food is good.) Are you still intimidated by talks? (No.) Where do you find your inspiration? (TV shows, music, reading, no single mechanism.) How do you balance everything? (Exercise a lot.) If you did it all over again, what would you do differently? (He wouldn't change anything.) Will America still be a power in future research? (Yes.) What was your worst mistake? (Even mistakes teach you to be better.) He answers them all, between three pieces of pizza. \n               18:50 \n             Langer's computer is still sitting unused as we leave the office. He drops me back at the hotel \u2014 I'm exhausted. On his way home he stops for an ice cream \u2014 coffee chip frozen yoghurt with hot fudge sauce. He spends an hour on the exercise bike. Sometimes he reads  People  magazine or watches the Boston teams play. Tonight he reads CVs, revises a paper for  Angewandte Chemie  and prepares his talk for the World Economic Forum in Davos, Switzerland, later in the month. He listens to his daughter \u2014 one of three teenagers \u2014 practise the presentation for her Friday chemistry class on smart polymers. \u201cShe did it all herself,\u201d he says. \u201cShe has four citations and an interview with me. I don't do her homework. I explained some of the chemistry.\u201d Then he does another hour on the cross-trainer and treadmill. No dinner speech today, and his own bed to look forward to: comparatively relaxed. \n               23:10 \n             While packing for tomorrow's trip to Tampa, Florida, for the Armed Forces Institute of Regenerative Medicine, Langer panics: where are his passport and phone? They're in his coat. \n               23:50 \n             Langer's BlackBerry is charging in the bathroom. He is in bed. I can't tell you what he dreams of. But if I had to guess it would be about happy, helped people.\n \n                     Transdermal drug delivery \n                   \n                     Small-scale systems for in vivo drug delivery \n                   \n                     Moving smaller in drug discovery and delivery \n                   \n                     Langer Lab \n                   \n                     Millennium Technology Prize \n                   \n                     Living Proof \n                   Reprints and Permissions"},
{"file_id": "458138a", "url": "https://www.nature.com/articles/458138a", "year": 2009, "authors": [{"name": "David Lindley"}], "parsed_as_year": "2006_or_before", "body": "Waste heat from industrial plants and electricity-generating stations represents a huge amount of lost energy. David Lindley finds out what engineers and regulators need to do to get it back. The power flows freely at West Virginia Alloys. This factory, tucked into the Kanawha River valley about 50 kilometres upstream from Charleston, West Virginia, is the second largest consumer of electricity in the state. It routinely pulls more than 120 megawatts of power from the grid, directing it into five electric arc furnaces that heat quartz to about 1,500 \u00b0C to make high-grade silicon for computer chips, solar cells and other uses. Then the plant takes the furnaces' 800 \u00b0C exhaust, which still contains much of the original energy, and sends it billowing out of the smokestacks. Unfortunately, West Virginia Alloys is not unusual, says Thomas Casten, an industrialist who has been preaching against this kind of energy waste for 30 years. Consumers around the world have embraced personal energy conservation in compact fluorescent light bulbs, home insulation and hybrid vehicles. But the industrial sector \u2014 including much of the electric power industry itself \u2014 continues to waste energy at a staggering rate. Thanks to generations of cheap energy, industrial process designers have typically had neither the expertise nor the economic incentive to do otherwise. Now, however, West Virginia Alloys is one of a small but increasing number of companies trying to change their throwaway habits. By 2011, equipment being installed by Recycled Energy Development (RED) in Westmont, Illinois, a company that Casten co-founded in 2006, will start converting the plant's waste heat into electricity at a rate of more than 40 megawatts. That's enough to reduce the plant's power consumption by a third. Such opportunities are everywhere, says Casten. There are 300 or so silicon plants in the world; each of them could repeat the West Virginia Alloys' story. Then there is steel. About half the steel in the United States is made by recycling scrap in arc furnaces; each furnace could recycle perhaps 15 megawatts of electricity. And for the manufacture of tyres, huge amounts of carbon black are used as a toughening agent for rubber. Carbon black is made by the partial combustion of tars, and the heat wasted could generate about 1 gigawatt of electricity. Improving the energy efficiency of traditional fuel use is at least as good a way of protecting the environment as pushing for greater use of renewables, says Ahmed Ghoniem, a mechanical engineer at the Massachusetts Institute for Technology's Center for 21st Century Energy in Cambridge. Burning fossil fuels accounts for around 85% of the world's energy use, he says, so that a 20% increase in efficiency, \"which is not unachievable\", would reduce pollution and emission of greenhouse gases as much as would doubling the global proportion of 'green' energy generation \u2014 a goal that's years if not decades away, he adds. And in the United States alone, Casten says, energy recycling could offset the equivalent of about 200 gigawatts of electricity generation \u2014 about one-fifth of the nation's total capacity. The challenge will be to get there \u2014 through a labyrinth of technological issues, costs, national energy policies and the often byzantine regulatory structures that govern electric power generation. \"It's inordinately complicated to put all this together,\" says Casten.  \n                Technology old and new \n              The good news for Casten's company and others in the energy-recycling field is that more and more potential customers are starting to pay attention. After last summer's spike in the price of oil, there is a widespread conviction that the price of energy has nowhere to go but up, however low oil prices might have fallen at the moment. The bad news is cost: the price tag for retrofitting waste-heat recovery technology into a plant that was not designed for it tends to send potential customers into shock. The West Virginia Alloys installation will cost about US$100 million. That is why Casten's RED, for one, supplies both the capital and the expertise for installing energy-saving systems, in return for an agreement with the company to share the benefits. Plant managers don't have the knowledge to tackle the task, says Casten. \"If they look at doing this on their own they will tend to overestimate the risks and underestimate the rewards.\" Still, the technology itself is straightforward; in many cases, waste heat can be turned into electricity in exactly the same way that most power plants do it. A standard plant starts by taking heat from some primary source \u2014 coal, gas, biomass, enriched uranium or even concentrated sunshine \u2014 and using it to vaporize water into high-pressure steam. The steam is then directed through a turbine: essentially an arrangement of high-tech fan blades that are forced to spin by the steam's forwards motion. The turbine's shaft rotates an electrical generator, which produces the electricity. And the steam coming out on the far side is passed through a chiller that condenses it back into water again. This Rankine cycle \u2014 named after William Rankine, the nineteenth-century Scottish engineer who devised it \u2014 is a practical and efficient way to turn a temperature difference into useful power (see 'Power from heat'). But as the French engineer Sadi Carnot showed in 1824, there is a fundamental limit on how efficient any such cycle can be. The bigger the temperature difference, the bigger that maximum efficiency is; thus engineers' frequent references to 'high-quality' heat, by which they mean that the source is much hotter than its surroundings. But no matter how good the technology, there will always be some heat energy left over. In the case of the Rankine cycle, this is primarily the energy given up by the steam as it condenses back into water. In principle, further mechanical work can be extracted from leftover heat, as long as there is at least some temperature difference with the surroundings. But usually, says Ghoniem, \"because it's low quality you have to work a bit harder to get that energy out\". \n               boxed-text \n             That's why energy-recycling companies go after high-quality heat first. At West Virginia Alloys, for example, the 800 \u00b0C exhaust heat from the furnaces will be used to boil water, which will then generate electricity via a standard Rankine-cycle system. And there are plenty of other examples, says Casten. He vividly remembers visiting a lime plant in Nevada that has enormous cylindrical kilns fed by limestone, pulverized coal and oxygen. \"You can feel the heat 30 feet away,\" he says. Large-scale industries such as ore-smelting and metal-refining have conventionally pumped out energy that does nothing except heat the surrounding air. Tempting as these targets may be, there are many more opportunities to reclaim lower-quality waste heat given off in chemical manufacturing, paper making and countless other industries. RED is looking at the cavernous ovens used to dry great sheets of plasterboard (drywall) \u2014 an energy consumption that represents 80% of the plasterboard's cost. The exhaust temperature is less than 200 \u00b0C, making it hard to run a water-based Rankine cycle with any efficiency. But it is now possible to buy off-the-shelf Rankine-cycle systems that replace water with more volatile liquids such as freon, propane or butane, which can extract mechanical work from more modest temperature differences. At the high-tech frontier, meanwhile, are waste-heat recovery systems that rely on solid-state thermoelectric materials that directly generate electricity from a temperature gradient. Commercially available thermoelectric devices can attain 15\u201320% of the Carnot efficiency for a given temperature difference, says Jeff Snyder, a materials scientist at the California Institute of Technology in Pasadena, making them both less efficient and more expensive than a Rankine device doing the same job. Nonetheless, their compact size and lack of moving parts make thermoelectric generators an attractive option for the automotive industry. BMW and Volkswagen, for example, have both announced prototype systems that would create electricity from the engine's exhaust heat. Researchers are exploring more-exotic chemical compositions and materials engineered to have nanostructures to decrease thermal conductivity, slow the flow of heat and make its energy easier to capture. Any heat that flows all the way through the material is lost, says Snyder. Such systems probably won't be able to compete with Rankine-cycle technology in a factory setting, he says, but it's realistic to think of adding thermoelectric generators to vehicles at a cost a few hundred dollars.  \n                The biggest offender \n              Globally, one of the most glaring inefficiencies comes from the burning of fossil fuels to generate electricity. About two-thirds of the world's electricity is made this way, according to a 2005 tabulation from the US Energy Information Agency, and the conversion efficiency in typical coal- or oil-fired plants is about one-third. Newer electric power plants running on natural gas or on 'syngas', a mixture of mostly hydrogen and carbon monoxide created by gasification of coal, can achieve greater efficiencies through a combined cycle system, in which combustion of the gas directly drives a first set of turbines, while exhaust heat from that stage generates steam that drives a second set of turbines. Combined cycles deliver efficiencies of 55% or more, says Ghoniem. Gasification is a cleaner but more expensive use of coal, whereas natural gas is mostly used in 'peaker' plants that can be turned on and off quickly as changing demand requires. Combined cycle systems have therefore yet to make a significant improvement in the average efficiency of converting fossil fuels to electricity. Still, no method for generating electricity by burning a fuel can avoid substantial inefficiency through the loss of heat \u2014 which puts utilities in the ludicrous position of venting huge amounts of waste heat to produce electricity that is often turned back into heat somewhere else, via arc furnaces, hair driers and home heating systems. So why not use that waste heat directly? \"Separate generation of electricity and heat is utter madness,\" says Casten, who likes to point out that the plant built by Thomas Edison in Manhattan in 1882 to supply electricity to the burgeoning city also sent its waste heat through pipes to warm nearby buildings. Such a combined heat and power (CHP) system remains in place in New York City, operated by Consolidated Edison and feeding steam heat to about 100,000 buildings. The emphasis is on 'nearby', however; heat doesn't travel as well as electricity does. So in the United States, where the power system evolved through the twentieth century towards large power plants located in remote areas, this municipal deployment of CHP remains rare. But other countries do things differently. Perhaps the most notable example is Denmark, which has pursued national energy policies that have put it at the forefront of energy efficiency as well as use of renewable sources, principally wind power. About half of Denmark's electricity comes from CHP plants \u2014 a few large ones, which switched from oil to coal following the oil crisis of the 1970s, and thousands of small to medium units, serving smaller communities and mostly running on natural gas. Some local units burn wood chips, straw, other biomass and household waste. \"Everything that can be burned is burned,\" says Per Lund, head of system development at Energinet.dk, the state-owned corporation in Fredericia that runs Denmark's electricity grid. Part of Denmark's success with CHP is a matter of geography: its large CHP plants sit close to the shore and use cold sea water for cooling. As larger temperature differences allow greater efficiency, these plants achieve efficiencies of more than 90% in terms of the electricity and useful heat obtained from their fuel. Over the past 25 years, Denmark's energy consumption has remained essentially constant even as the country's gross domestic product (GDP) has grown by 75%, giving it the European Union's lowest 'energy intensity', measured in terms of energy consumption per unit of GDP. But Denmark's success also derives from a mix of tight regulations, government investments and open-market responses. For example, says Lund, there was significant national spending to install the buried pipes that distribute hot water around communities of as few as 500 houses. And in CHP districts, which include about 60% of the country's population, residents must draw their domestic heat from the centralized system. But at the same time, private investors who see a profitable opportunity in providing additional energy are free to propose building a new CHP unit. Likewise, industries that invest in equipment to capture and divert their waste heat can recoup some of their costs by selling it to the local CHP district.  \n                Keeping up with the competitors \n              In the United States, Kathey Ferland, project manager at Texas Industries of the Future, an advisory organization in Austin, points out that energy-intensive oil refineries and chemical plants in Texas have long made use of CHP to achieve internal efficiencies. \"What mostly seems to motivate them is making sure that they are at the least doing what their competitors are doing,\" she says. However, in Texas, Ferland says, \"a lot of the really good economic sites are done\", and for smaller operations the economics of CHP can be hard to work out, or outright unfavourable. For example, a plant that uses CHP to meet its basic power needs may greatly reduce its consumption of off-peak electricity \u2014 a significant fraction of which comes from wind power in Texas \u2014 while still drawing on the grid when its needs are higher. CHP, therefore, competes disproportionately against the cheapest and most environmentally friendly electricity, without eliminating the need for conventional power plants. So an electricity-pricing system with higher daytime prices to discourage peak consumption can create a \"misincentive\" by acting against other desirable goals, says Chris Marnay of the Lawrence Berkeley National Laboratory in California. \"The form of the technologies that are successful is determined at least as much by the regulatory situation as it is by the engineering reality.\" In one study of medium-scale commercial buildings, Marnay and his colleagues found that CHP made economic sense for a hypothetical building in California, but not for the same building in New York, mainly because of differences in the electrical tariff structure.  \n                National oversight \n              These regional variations arise because electricity regulation has traditionally been in the hands of the states. The 1978 Public Utility Regulatory Policies Act modified that picture, obliging US electric utility companies \u2014 at that time, a collection of local and regional monopolies \u2014 to buy electricity from independent producers. Oversight was put in the hands of individual states, which took on the task with varying degrees of enthusiasm, and the terms of the act gave utilities considerable influence over the price they were supposed to pay for independently produced electricity. The Danish government, by contrast, mandated expansion of CHP as a national policy beginning in the 1980s, requiring grid operators to pay for the power generated at a set rate. The guaranteed ability to sell electricity stimulated the construction, by private entities, of relatively small-scale CHP plants. The same strategy lies behind the rapid growth of wind and solar power in Denmark and also in Germany. In both cases, grids were legally obliged to buy renewable power. Wind power, which now accounts for about 20% of Denmark's electricity supply, began as a grass-roots movement of \"happy amateurs\", says Lund, with farmers installing turbines made by the same companies that forged ploughs and built water tanks. Today, however, wind turbines are huge business, manufactured by a handful of large companies. At the time, power engineers accustomed to working with grids that transmitted power over wide areas from a handful of enormous generating plants doubted that the new system could work reliably, says Thomas Ackermann, chief executive of Energynautics in Langen, Germany, an energy-consultancy company. A number of Danish studies argued that distributed generation \u2014 with electricity coming from many smaller-capacity suppliers \u2014 couldn't be done. And \"it wasn't done, in the sense that anyone actively planned it\", he says. \"Instead, the regulatory system allowed private investment, and the network people couldn't stop them.\" As a result, Denmark is moving towards an increasingly distributed generation system, in which electricity doesn't simply flow in a 'top-down' manner, but can also move in a 'bottom-up' way from multiple small generating units. This required many changes to the way power moves through transformers from low-voltage to high-voltage parts of the network. A unified national policy of private investment aided by government subsidy spurred the construction of moderate-capacity electricity plants, and the grid operators were able to pass on costs to customers through tariffs. But in many countries, such transformations falter on the question of who will pay for them, says Ackermann. That is typically the case in the United States, where efforts to deregulate the old monopolies have happened piecemeal. \"One of the first problems is that people don't understand each other, everyone has a different mindset, everyone has a different way of looking at the system,\" says Ackermann. A common question is whether a distributed generating system with many small seasonally variable suppliers can ensure that peak demand will be met. But, says Ackermann, in a reversal of the standard clich\u00e9 about the free-market United States and socialist Europe, \"we don't talk about guaranteeing power from wind or other sources all the time \u2014 we just believe that the market will cope\". In Germany, he says, peak demand is about 74 gigawatts and the market can supply about 120 gigawatts, so that as demand waxes and wanes, more expensive producers enter and leave the market, and the price of electricity rises and falls. \"The system is one of the best in the world in terms of reliability,\" says Ackermann. He argues that the greater regulatory burden in the United States has inhibited development of diverse generating capacity and of the grid, so that the infrastructure that gets built is the bare minimum. \"The US system looks like our system did maybe 25 years ago,\" says Lund.  \n                Change we need? \n              But change is in the air. In his Senate confirmation hearing, Steven Chu, former director of the Lawrence Berkeley National Laboratory and now US energy secretary, declared that \"more efficient use of energy in the United States is the one big factor that can most help us reduce our dependency on foreign oil\". The economic stimulus package signed by President Obama on 17 February includes a modest federal subsidy for CHP and recycled-energy projects. \"This is the first federal inducement of generation efficiency since 1986,\" says Casten, although he adds that the bill promises much greater support for renewable energy sources, \"which is not as good a way to spend money\". He hopes that when Chu talks about efficiency, he doesn't just mean better appliances and more insulation. \"The bottom line is that we will not have a prayer of mitigating climate change unless we address the efficiency of generating thermal and electric energy.\"\n \n                 David Lindley is a freelance science writer based in Alexandria, Virginia. \n                 See Editorial,  \n                 \n                     page 125. \n                   \n               \n                     RED (Recycled Energy Development) \n                   \n                     Danish Energy Agency \n                   \n                     US DoE Office of Energy Efficiency and Renewable Energy \n                   Reprints and Permissions"},
{"file_id": "458025a", "url": "https://www.nature.com/articles/458025a", "year": 2009, "authors": [{"name": "Jim Schnabel"}], "parsed_as_year": "2006_or_before", "body": "Alcoholics Anonymous and its spin-off programmes have been helping people with addictions for decades. Jim Schnabel talks to the neuroscientists who are looking deeper into the approach. In the depths of the Depression, in a Manhattan alcoholism clinic, a ruined Wall Street speculator named Bill Wilson had a vision. His room suddenly blazed \"with an indescribably white light\" and he experienced euphoria and a godlike \"presence\", followed by a \"great peace\" 1 . Like St Paul after his experience on the road to Damascus, Wilson soon turned away from his old, inebriated life and became an evangelist \u2014 preaching a radical, spiritual cure for alcoholism. That cure grew into the modern addiction rehabilitation industry, which even today is dominated by Wilson's Alcoholics Anonymous (AA) paradigm and its 'twelve-step' approach to recovery. Perhaps unsurprisingly, given its spiritual origins, this approach has had an uneasy relationship with the evidence-based culture of medical research. Both perceive addiction as a chronic disease; but whereas scientists seek rationally targeted interventions to blunt drug cravings, AA and related programmes tend to feature group therapy, tearful confessions and the call to \"surrender to a higher power\". In the past few years, however, these two cultures have been finding common ground. Neuroscientists have begun to recognize that some of the most important brain systems impaired in addiction are those in the prefrontal cortex that regulate social cognition, self-monitoring, moral behaviour and other processes that the AA-type approach seems to target. \"A lot of the treatment programmes out there are targeting these systems without necessarily knowing that they are doing it,\" says Nora Volkow, director of the National Institute on Drug Abuse in Bethesda, Maryland. Researchers are now searching for ways to boost these prefrontal systems even further \u2014 not to remove the need for twelve-step and other behaviourally oriented treatment programmes, but to enable people with addictions to get more out of them. \"It completely changes the way that we look at medications,\" Volkow says. Until recently, addiction researchers focused almost entirely on 'limbic' circuits in the brain that mediate fear and desire. These dopamine-fuelled networks are effectively hijacked by addictive drugs and behaviours so that the person ends up wanting, and compulsively seeking, little else but the next fix. Drugs such as methadone and naltrexone can blunt the activity of these circuits, but they are not a cure.  \n                Impulse management \n              While doing neuroimaging studies at the Brookhaven National Laboratory in Upton, New York, in the 1990s, Volkow was one of the first researchers to suggest that abnormalities in the prefrontal cortices of drug users might weaken the systems that normally counteract drug cravings 2 . Since then, the prefrontal regions and their links to the limbic system have garnered more and more attention, and researchers are now attempting \"a very extensive evaluation of how the different areas in the prefrontal cortex participate in the process of drug addiction\", Volkow says. The prefrontal cortex \u2014 the most recently evolved set of structures in the brain and the one that most clearly differentiates humans from other species \u2014 is the headquarters for the circuits that help shape feelings and behaviour according to long-term goals, moral strictures and social cues. These systems are extensively wired into limbic regions, and are often portrayed as a 'braking' system to resist impulsive behaviour. The slow development of prefrontal structures after birth tracks the maturation of children into adults, and people whose prefrontal areas are damaged by trauma or stroke, for example, seem to have lost some control of the brakes and are apt to be childishly impulsive and uninhibited in their behaviour. With tools such as psychological tests and brain imaging, researchers have been finding similar braking problems associated with drug use and are starting to tease apart the mechanisms involved. Some have shown that people with drug addictions are poor at monitoring their own behaviour 3 , making appropriate decisions and inhibiting impulses \u2014 and these behavioural findings have been matched to functional magnetic resonance imaging (fMRI) data that show reduced activity in the corresponding prefrontal areas. Animal studies have supported the human ones by showing, for example, that monkeys given cocaine swiftly develop prefrontal impairments 4 . And other researchers have found that stress, which frequently triggers drug use and relapse in people with addiction, seems to do so at least in part by shutting down prefrontal functions 5 . \"We're really starting to understand the molecular basis of why this cortex falls apart with drugs of abuse, and during stress, and how those two interact,\" says Amy Arnsten at Yale University School of Medicine in New Haven, Connecticut. If the cortex falls apart with drug abuse, then it may be impossible to recover from an addiction without putting it back together. In unpublished studies, Hugh Garavan and his colleagues at Trinity College, Dublin, have found that cocaine users and tobacco smokers who go through treatment and are able to stay abstinent for more than a year \"seem to show hyperactivity in these prefrontal control centres\" in fMRI images. Garavan says that this extra activity seems to be especially prominent during the first few weeks of abstinence, hinting at \"a heavy reliance on these prefrontal centres to avoid falling off the wagon\". The recognition that prefrontal systems might need boosting in people with addictions has helped fuel a new interest in whether AA and similar behavioural treatments are already having these kinds of effects. \"It behooves us to try to understand how [twelve-step approaches] link to what we're addressing in terms of intervention,\" Volkow told the annual meeting of the Society for Neuroscience in Washington DC last November. So far, these treatment programmes have been difficult to study formally, says Martin Paulus, a psychiatrist who is researching addiction at the University of California, San Diego. \"It's very much a voluntary-based programme, with little standardization, and the whole programme thrives on anonymity.\" But much of what is known about the AA approach suggests that it aims to protect or enhance prefrontal circuits. In the protected environment of a rehab centre, drugs and other cues associated with drug taking are gone and stressful situations that suppress prefrontal activity are minimized. Volkow notes that the feeling of ceding control to a higher power is also likely to \"enhance your sense of security, decreasing stress and anxiety\". Similarly, says Garavan, the confessions of bad behaviour and other \"strategies that push users to become more aware of their drug-related actions presumably aim to boost their capacity for self-monitoring, which is largely a prefrontal function\". The social environment in rehab is another factor that works in part through prefrontal systems. \"Our brains have evolved to be very sensitive to social cognition and social reinforcers,\" says Volkow. By putting people with drug addictions into a group with anti-drug values, \"you are providing them with a very powerful reinforcer\", she says.  \n                Spiritual control \n              And then there is religion, which has been shown to have a strong inverse association with drug addiction. Psychologist Michael McCullough, who studies religion and behaviour at the University of Miami in Florida, calls this inverse association \"one of the most unsung findings in the entire literature on drug and alcohol abuse\". Both adults and children deemed religious by various measures \"drink, smoke and do drugs less often\", McCullough says. \"If they get into trouble with drinking and drugs and smoking, they're more likely to be able to get away from those problems.\" McCullough suggests that when a person commits to any cultural system that regulates behaviour, the psychological effort to conform strengthens the brain systems that mediate self-monitoring and self-control. \"What makes religion unique, I think, is that the code of conduct isn't just laid down by your parents or your friends or your principal at school, but ostensibly by the individual who is superintending the Universe, so it has an extra moral force.\" Some religious rituals, he says, have been shown to provoke enhanced activity in prefrontal regions 6 . \"It's as if certain forms of prayer and meditation are pinpointing precisely those [prefrontal] areas of the brain that people rely on to control attention, to control negative emotion and resolve mental conflict.\" However the twelve-step strategies actually work on the brain, \"there is now excellent documentation that those who attend AA-type programmes regularly do very well by anyone's standard\", says Thomas McLellan, director of the Treatment Research Institute in Philadelphia, Pennsylvania. The problem, McLellan says, is that the vast majority of people who enter such programmes do not go regularly \u2014 they drop out after a few days or weeks \u2014 and are more than likely to relapse. Anna Rose Childress, a psychiatrist at the University of Pennsylvania School of Medicine in Philadelphia, has encountered a similar resistance to treatment in the crack cocaine users she has studied. In her lab she uses a cognitive behavioural training technique \u2014 like \"prefrontal pushups\", she says \u2014 that tries to make these users more aware of their drug-related actions and the consequences. But her studies indicate that \"most of our cocaine patients are not great at it\". Results such as these raise what Childress and others call the \"chicken or egg question\" \u2014 is drug use the cause of users' prefrontal problems, or do they have pre-existing defects that make them susceptible to addiction? As Garavan puts it: \"A lot of people might be able to enjoy drugs but there's only a certain percentage who actually go on to become addicted. And maybe part of that is because these people lack that prefrontal-mediated control over behaviour.\" Some research already links prefrontal-related conditions such as impulsivity and attention deficit hyperactivity disorder (ADHD) to a heightened risk of later drug use. But to really start answering the chicken or egg question, says Childress, \"you would need some good large-scale developmental studies for one thing; you would like to look at adolescents before they've ever touched drugs\". Garavan and several dozen other European researchers are now participating in a project that aims, in part, to do just that. Known as IMAGEN and begun in late 2007, the five-year, \u20ac10-million (US$14-million) project funded by the European Commission will ultimately enrol 2,000 14-year-olds and follow them through their late teens. Principal investigator Gunter Schumann, a psychiatrist at Kings College, London, says that the testing will include fMRI and structural MRI, as well as a full genome scan. He expects to start publishing findings in the next few years.  \n                Quenching the flame \n              In the meantime, researchers are pursuing other ways to boost prefrontal systems \u2014 and medicines for ADHD seem an obvious place to start. Attention-enhancing drugs such as methyl-phenidate and atomoxetine boost the activity of key receptor systems in the prefrontal cortex, in particular those for noradrenaline and dopamine. Some evidence already suggests that patients with ADHD are less likely to go on to abuse drugs if they are receiving medication for their condition 7 . And earlier this year, a team led by Daina Economidou at the University of Cambridge, UK, reported that atomoxetine helped rats with an ADHD-like impulsivity to resist a relapse to cocaine-seeking 8 . The National Institute on Drug Abuse has also been supporting studies of cognitive and behavioural strategies, and Volkow says that she is particularly enthusiastic about an approach that involves \"real-time fMRI feedback\". Developed by researcher and entrepreneur Christopher deCharms earlier this decade, the technique involves placing drug users in an fMRI machine and showing them a symbolic representation \u2014 a flame \u2014 of the fMRI-measured brain activity that corresponds to their cravings. The users are then asked to apply their own cognitive exercises, such as imagining their child is with them, to quench their cravings and douse the flame. After half a dozen sessions with this feedback the user will, in principle, develop cognitive circuitry that is more efficient at suppressing craving and that can then be used in ordinary life. A version of the technique, used for pain relief, has already shown some efficacy in a small clinical trial 9 , and deCharms and his Silicon Valley start-up, Omneuron, are currently running a small trial in smokers \u2014 with plans for a follow up with some of Childress's cocaine users. For some people, even the most sophisticated therapies may not be enough to rescue a prefrontal cortex that has been damaged by genetics, development and perhaps decades of drug use. \"It's like somebody who has had a stroke and is paralysed,\" says psychologist Antoine Bechara at the University of Southern California, \"and you tell them, well, you should walk, you should exercise. But the part of the brain that allows them to do that is not there and they just cannot do it.\" To Bechara, a more efficient approach would be to protect and strengthen these critical brain regions as they are developing. As an example, he cites preliminary data from a study in China. \"There are children who grow up whose parents make all the decisions for them, and others who are encouraged to make decisions and are rewarded or punished for their bad decisions,\" he says. \"The latter children grow up to show better performance on measures of decision making, and there is even a hint of evidence from fMRI that the kids with that latter kind of parenting style have better prefrontal cortex function.\" Even for those beyond the influence of parenting style, researchers hope that a little lift in prefrontal efficiency could go a long way. Such a boost, says Paulus, could be \"the critical piece that helps prevent the person from getting onto a very destructive pathway\". The question now is how best to give that boost. As researchers come to understand the neural mechanisms of addiction better, the twelve-step approach may give way to more secular strategies. But it seems unlikely that all behavioural approaches will soon be replaced by a pill. \"I think most researchers would say, and I know I would say, that medicines should be used in the context of a good behavioural programme,\" says Childress, \"because a person is essentially trying to restructure a lot of behaviour, and the more support that you can provide for that, the better.\"\n \n                 Jim Schnabel is a freelance writer based in Maryland. \n               \n                     Alcoholics Anonymous \n                   \n                     NIDA \n                   Reprints and Permissions"},
{"file_id": "457376a", "url": "https://www.nature.com/articles/457376a", "year": 2009, "authors": [{"name": "Lea Winerman"}], "parsed_as_year": "2006_or_before", "body": "Messages appear on Internet-based social networks within minutes of disasters occurring. Lea Winerman investigates how to harness this trend to create official community-response grids. The messages began to fly almost as soon as the bullets stopped. Starting at 7:00 on the morning of 16 April 2007, an undergraduate named Seung-Hui Cho had carried a pair of semi-automatic pistols through the campus of the Virginia Polytechnic Institute and State University in Blacksburg \u2014 better known as Virginia Tech \u2014 gunning down dozens of students and professors as he went. By 9:51 it was over: Cho had turned one of his guns on himself. But the survivors, lacking any official word from the university other than the total death toll, were still in the dark about which of their friends had lived or died. So they turned to the best information source they had: the Internet \u2014 notably, the social website Facebook. Posts appeared in quick succession, indicating the names of suspected casualties. Those here have been edited for privacy reasons. \"CH, as reported by a sorority sister,\" read a post on one Facebook page. \"I just finished speaking with his girlfriend, and it appears JH is a fatality as well,\" read another post. As the information accumulated, the participants spontaneously began to develop their own norms to ensure accuracy. Anonymous or vague posters were asked for clarification. People had to identify themselves when they put forward the name of a shooting victim, for example, and explain where they had got the information. By the time the university released the names one day later, it was old news to the online community: they had identified all 32 of the deceased already. The Virginia Tech story is hardly unique. \"When people are under threat, perceived or actual, they go into this intensified information-seeking period,\" explains Leysia Palen, a computer scientist at the University of Colorado, Boulder. And these days, they are increasingly doing so through social networking sites. But social-network users often end up bypassing the authorities \u2014 a tendency that has left officials scrambling to use this information and integrate it into traditional responses. \"Emergency managers have this desire to control the flow of information,\" says Jeannette Sutton, a sociologist who also studies disaster communication at the University of Colorado. \"But you can't control it. The best we'll be able to do is figure out how to harness it.\" Palen calls her research 'crisis informatics', and it has taken her to several disaster hotspots. In the Virginia Tech case, for example, she and her colleagues began monitoring websites within hours of the shooting, then travelled to Blacksburg five days later to interview people. Back in Colorado, they created a detailed timeline of official communications (university e-mails, press conferences) and unofficial communications (Facebook messages, Flickr photos) that stretched across a giant white-paper diagram on their office wall. This allowed them to track the emergence of those informal norms. \"One of the concerns from the emergency-management point of view is 'how can we know the information [posted on websites] is accurate?'\" Palen says. \"We saw that here it was self-correcting.\"  \n                Spreading like wildfire \n              In October and November 2007, Palen and her colleagues examined the online response to another crisis. During that time, more than 20 wildfires raged from Santa Barbara to San Diego counties in southern California, eventually burning 200,000 hectares, destroying about 1,500 homes and forcing many more households to evacuate. Palen and her colleagues monitored local news websites and online forums including Craigslist, Facebook, Twitter and Flickr. By the tenth day of the fires, they also began to survey and interview area residents. \"National news websites were completely worthless as they ignored everything except the comparatively minor Malibu fire that burned near some celebrity homes,\" one evacuee wrote. And official government communications, although sometimes useful, couldn't be relied on either. \"The county so-called emergency site was always crashed,\" another wrote. Instead, many people turned to websites run by local media, such as the National Public Radio station KPBS, based in San Diego, or by individuals. On these sites, the updates came from any local resident with an Internet connection and information to share. Some hosted Google maps on which users could overlay information such as the location of the fires. Others hosted discussion boards on which people who hadn't evacuated, or who had made it back to their homes, could share what they were seeing.  \n                Worldwide webs \n              Online information-sharing during crises isn't limited to the United States, and other media agencies are already using the concept. In early January, for example, Al Jazeera, a satellite television network headquartered in Doha, Qatar, launched an experimental website that aggregates text messages, mobile-phone reports and Twitter feeds about the conflict in the Gaza Strip on a Microsoft Virtual Earth map ( http://labs.aljazeera.net/warongaza ). And Yan Qu, Philip Wu and Xiaoqing Wang, researchers at the University of Maryland in College Park, observed information-sharing trends after the 12 May 2008 earthquake in China's Sichuan province. \"The earthquake happened at 14:28. Within one minute there was a message posted on a Tianya forum,\" says Wu. Tianya is one of China's most popular websites \u2014 a bulletin-board system with more than 20 million registered users. Within 10 minutes, 56 discussion threads reported feeling the earthquake in 22 cities throughout the region. The researchers read and classified the thousands of threads that appeared on the site in the days following the quake. \n               boxed-text \n             They found that Chinese citizens used the site for many of the same purposes as Californians did during the wildfires: seeking information about their homes, hometowns and family, and coordinating action to help victims of the crisis. And there were some widely reported success stories. In one, a college student saw news reports about the military having a difficult time finding a spot to land a rescue helicopter near a destroyed village in the mountains. The woman, who had grown up in the area, posted a detailed description of a potential landing spot in an online forum, and begged users to forward it to authorities. The post eventually found its way to the military, who landed a helicopter in the spot she described. Unfortunately, says Wu, that kind of story is rare. In general, professional emergency responders are only vaguely aware of how citizens use social media during disasters. \"Even in the United States, professional emergency responders are just beginning to think about this. In China I don't think there's any kind of explicit effort,\" he says. It's the lack of official involvement that Ben Shneiderman, a computer-science professor at the University of Maryland and his wife Jennifer Preece, dean of the university's College of Information Studies, are trying to change. In 2007, they published a one-page article in  Science  called '911.gov' (B. Shneiderman and J. Preece  Science   315 ,  944;  2007). Taking their title from the emergency telephone number for much of North America, they envisioned a web-based 'community response grid' that would combine the power of social networking sites with official government emergency-response systems. An emergency telephone system works terrifically under normal circumstances, says Shneiderman. \"But when you get a Hurricane Katrina or a 9/11, [the call centres] become overwhelmed.\" Potentially, at least, a website can handle such sudden surges more gracefully by tapping more servers as needed. As Shneiderman sees it, people would report incidents via the Internet or by sending text messages from their mobile phone rather than by calling 911. Software would aggregate those reports into a constantly updated map of the situation, which citizens and emergency responders could check without encountering clogged phone lines. People could also register to receive block-by-block information \u2014 again via e-mail or text message \u2014 about whether to stay in place, to evacuate or to respond in some other way. And people would be able to coordinate with their neighbours before, during and after emergencies on community message boards. So, for example, a family could agree to take responsibility for a wheelchair-bound neighbour during an evacuation. Shneiderman is collaborating with Wu, Qu and others to explore how a small-scale version of the system would work on the University of Maryland campus. Wu says that the official emergency responders in the university Department of Public Safety are very interested in the idea \u2014 but they are also wary. They are concerned that they wouldn't have the staff to run the system, he says, and that it might confuse people who are already well-trained to call 911 in an emergency. Then there's the possibility that people could use the system to spread bogus information and rumours. \"If you have everybody able to do peer-to-peer, then there is widespread panic,\" one safety officer told the researchers. That reaction \u2014 interest tinged with scepticism \u2014 mirrors the reaction of many professional emergency responders. Federal and local disaster-response agencies have long operated under the Incident Command System, a standardized protocol developed in the 1970s by firefighters battling California wildfires, and later adopted by federal agencies including the Federal Emergency Management Agency (FEMA). The system, with a clear, top-down chain of command, views communication with the public as a one-way street: information is supposed to flow from officials to the public via warnings sent out over TV, radio and other media. That view fits well with older academic research in disaster sociology. \"Thirty years ago no one talked about crisis communication,\" says Dennis Wenger, a sociologist at the National Science Foundation who has studied disasters for more than three decades. \"Back then the term was warnings research.\" The questions that preoccupied researchers then were how to write an effective warning \u2014 instructing people to evacuate before a hurricane, for example \u2014 and how best to make sure everyone heard it. \"But then all of a sudden came the Internet revolution, and it blew apart this notion of a linear chain,\" says sociologist Kathleen Tierney, director of the Natural Hazards Center at the University of Colorado. Social-networking and photo-sharing sites, mobile phones and text messages have turned the chain into a web. In hindsight, that was only to be expected. The chain was never as linear as the models made it out to be. One of the first to realize that was Thomas Drabek, a disaster researcher at the University of Denver, Colorado, who surveyed evacuees after the South Platte River flooded near Denver in 1965. More than 60% of the people he spoke to told him that even after they received a warning telling them to evacuate, they tried to confirm it \u2014 checking with family and friends, talking things over, watching to see what their neighbours were doing \u2014 before taking action.  \n                Reaching an understanding \n              Researchers have observed the same information-sharing inclination in many disasters since. After the bombing of the World Trade Center in New York in 1993, Benigno Aguirre, a sociologist at the Disaster Research Center of the University of Delaware in Newark, found that people who were in large offices took longer to evacuate than people who were in smaller groups, because it took them longer to come to a common understanding of what was happening. \"It is actually very difficult to get human beings to perceive that they are at risk,\" says Dennis Mileti, a disaster-management researcher at the University of Colorado. \"How do you convince people that they are at risk? Only through other people.\" And there, as long as the process isn't too time-consuming, is one of the advantages of the Internet. \"Long before technology, you could check in with neighbours next door,\" says Sutton. \"But now you can check in with peers around the country.\" Garry Briese \u2014 a regional administrator for FEMA in Denver and, for 22 years before that, the executive director of the International Association of Fire Chiefs \u2014 is excited about the opportunity that public feedback offers. He recounts the example of a wildfire in Deckers, Colorado, last summer, when he was able to find photos of the fire posted online by local residents within 30 minutes of hearing about the fire himself. \"It expands our ability to have situational awareness,\" he says. \"Someday, in a fast-moving wildfire, if we had the right system, we could ask people by a text alert to take pictures and send them to an emergency centre. And our ability to understand what the scene looks like would be enhanced.\" That scenario is still a long way from reality, says Briese, who, in July 2008, talked to an official from a large city's emergency operations centre about the information already available on the web. \"A few days later, the guy called me and said 'you know, my IT department has me blocked from those sites',\" he says. \"So here we have an emergency manager who wants to learn, and the first thing he has to do is go to the IT department and convince them.\" Sutton encountered something similar when she spoke about her research at a conference of state-level emergency managers in September 2008. \"I had a couple of people say to me, 'This is so new to us, we don't even know what to ask',\" she says. In any case, answers might not have been easy to supply. \"This information is out there, and you can find it online,\" Sutton says. \"But how do you aggregate it, and where would it get plugged into? Would it go back to an emergency operations centre, a joint information centre, a public information office?\" When Shneiderman proposed community response grids in 2007, he thought that it might become a reality within 3\u20135 years. Now, with so much research still to do, he thinks 5\u201310 years might be more accurate. But he is used to waiting. He helped to develop the concept of hypertext links in the 1980s and saw them become integral to the web more than a decade later. Not everyone has Internet access or the technical knowhow to take advantage of it. But, during a disaster, a community-response grid could benefit almost everyone, as family, friends, neighbours and authorities share what they know. \"I think this is inevitable,\" says Shneiderman. Lea Winerman is science editor for    The Online NewsHour with Jim Lehrer . \n                     Leysia Palen's website \n                   \n                     Ben Shneiderman's Community Response Grids website \n                   Reprints and Permissions"},
{"file_id": "457144a", "url": "https://www.nature.com/articles/457144a", "year": 2009, "authors": [{"name": "Paroma Basu"}], "parsed_as_year": "2006_or_before", "body": "If growing forests in India can generate lucrative carbon credits, then why isn't everyone planting trees? Paroma Basu reports. On a humid morning in August 2008, Harshkumar Kulkarni strolled through a thicket of young eucalyptus trees, surveying their slender forms. Stroking each silver trunk as he passed, Kulkarni, the general manager for plantations at the conglomerate Indian Tobacco Corporation (ITC), seemed like a concerned father checking on his children \u2014 albeit more than 3 million of them. Kulkarni has spent the better part of two decades breeding fast-growing, disease-tolerant varieties of eucalyptus, leucaena and casuarina trees that can survive in the swampy, inhospitable soils of Khammam district, in India's southeastern state of Andhra Pradesh. Farmers who had struggled to grow traditional crops such as cotton can now earn up to US$500 per hectare from timber that they sell to paper mills, including the one owned by the ITC. But for the past few years, Kulkarni has been grooming some of his trees for a new role: carbon offsetting. Kulkarni and his colleagues realized that they might benefit from the global carbon market established in 2001 as part of the Kyoto Protocol. The Clean Development Mechanism (CDM) is a system by which participating countries can meet some of their greenhouse-gas reductions by buying certified carbon credits from projects in developing countries that reduce emissions. Kulkarni decided to devote a portion of the ITC's plantations to supplying those credits. Seven years, mountains of paperwork and almost $70,000 of the company's funds later, the project is set to become one of only two forest-based CDM projects in India \u2014 and one of only a handful in the world \u2014 to win the necessary approval from the United Nations Framework Convention on Climate Change (UNFCCC). If it is approved as expected in the next few months, families will be paid $65 per year for each hectare they commit to grow for four years rather than logging every year. The owners still get money from timber sales when the trees are logged, as long as they immediately replant the plot. Kulkarni considers the project one of his biggest triumphs \u2014 but the past seven years have been difficult. \"I lost 11 kilos and ruined my health trying to get this CDM project registered,\" he says. India should be thick with forestry carbon-offset schemes: it was one of the first nations to move aggressively into carbon offsetting, has robust forestry policies and has vast swaths of land that could be planted. The challenge for Kulkarni and others trying to establish CDM forestry projects has come from the convoluted, expensive and bureaucratic approval process. Some of the biggest difficulties lie in documenting how much carbon the growing trees will absorb. Piecing together the patchwork of land involved is hard too; the 3,070 hectares in Kulkarni's project is owned by around 3,000 local tribal families. \"Forest-based CDM has not yielded the desired results so far,\" says a high-ranking forestry official in India's Ministry of Environment and Forests, who did not want to be named. \"The required methodologies and calculations are too complex and the costs are too high to justify. There is certainly huge scope to simplify the whole business.\" Despite the complex system, India and other countries are keen to grow more forests for carbon credits. Those efforts that plant native trees could aid biodiversity conservation, and all such schemes can provide a potential source of income for impoverished families who depend on forests for their livelihoods. Soyam Nagamani, a community leader of the Koya tribe in Khammam district's Bhimavaram village, says that ever since she switched from growing traditional cotton crops to the ITC's eucalyptus trees, a lot more money has come in from the sale of timber. Now the promise of additional income has convinced her to devote a portion of her land to carbon offsetting. \"We don't understand that much [about global warming],\" says Nagamani, \"but before we ate only two small meals a day whereas now we eat three full meals of rice.\"  \n                Grand potential \n              India is among the most densely forested countries in the world and already has robust conservation policies requiring that trees are replanted as they are removed. Some 24% of the country's land mass is now forested, with the government aiming to reach up to 33% by 2012. India also has vast stretches of land degraded by agriculture, development and pollution, plus remote and uncultivable lands, all of which could be planted with trees. It is here that the CDM could make a big difference. Project developers and forest-dependent communities now have an economic reason to plant and preserve trees. \"Through the CDM, we need to create situations in which a standing tree makes more business sense than a felled tree, for only then will poor communities retain the tree,\" says Mohit Gera, a government forestry officer who has carried out research at the Energy and Resources Institute in New Delhi. \"In developing countries such as India, carbon sequestration and poverty alleviation must happen side by side.\" Trees can only ever be a temporary carbon sink. They will eventually release all their carbon when they die from old age, if not before, because of logging, fires, pests, floods or cyclones. \"I feel that in forestry projects you are not actually mitigating climate change but just 'buying' time, or postponing the problem, because of the non-permanence issue,\" says Jos\u00e9 Domingos Miguez, a member of the UN's CDM executive board and an adviser on climate change in Brazil's ministry of science and technology. But forestry projects are still considered to be valuable as temporary 'carbon banks' that soak up greenhouse gases until researchers work out ways to make longer-lasting emissions cuts. When the CDM was formally established, some expected forestry schemes to thrive because tree-planting is a relatively cheap and easy way to mitigate emissions compared with establishing a wind farm or hydropower plant. But although the incentives are there, few projects have materialized. Only one, in China's Pearl River basin, has received the green light from the UNFCCC. Around 25 more are awaiting final approval, according to UNFCCC figures: six in India (see map, \"below\":#map), five in China and the rest scattered through Asia, Africa and South America. By contrast, the organization has approved 1,270 schemes worldwide \u2014 including 378 in India, more than any other country \u2014 to offset carbon in other sectors, ranging from wind farms to the destruction of non-CO 2  greenhouse gases. China's Pearl River project aims to reforest 4,000 hectares of watershed areas along the Pearl River basin in Guangxi province and to generate around 600,000 carbon credits by 2036. It was an inspiration for the Indian forestry community, but it also exposed serious bottlenecks in the system, says Promode Kant, a former forestry official and director of the Institute of Climate Change and Ecology in New Delhi. Kant learned this first-hand as a technical adviser to a small, 370-hectare reforestation project in the north Indian state of Haryana that is, along with Kulkarni's project, on the brink of getting UNFCCC approval for carbon offsetting.  \n                Contrasting fortunes \n              Where the Pearl River project enjoyed full government support \u2014 including a multi-million-dollar budget and the contribution of its mostly government-owned land \u2014 the Haryana scheme received none. The project was funded with less than $75,000 by the state's cash-strapped forestry department, and Kant and his colleagues had to corral together 227 farmers across many villages, each owning tiny patches of a hectare or less. State forest departments ought to be the most enthusiastic proponents of India's forest-based carbon offsetting, but they lack financial resources and the experience to compile complicated project applications, says Jagdish Kishwan, director-general of the government's Indian Council of Forestry Research and Education. There are a number of bureaucratic hurdles to forest-based carbon offsetting. Many of them arise from the UNFCCC's stringent approval process. Before they can sell carbon credits, potential CDM projects have to pass through three rounds of evaluation: a government task force in the home nation, an audit of methodologies, finances and documentation by a UN-accredited body, and a final review by a CDM executive board staffed by scientists and officials. After the CDM executive board was established in 2001, Indian entrepreneurs moved aggressively into the carbon-trade business and an industry of domestic and international carbon-consulting firms soon emerged to help Indian clients prepare technically sound projects in renewable energies and other areas. But the same is not true for forestry. The CDM guidelines for the forestry sector weren't figured out until 2003, two years after other emissions-reducing sectors, says Martin Schr\u00f6der, lead auditor for forestry projects at T\u00dcV S\u00dcD Industrie Service in Munich, Germany, a UN-accredited firm that evaluates CDM projects. \"Very few consultants are actually experienced in the field, so there are more project developers who are trying to make a go of it on their own,\" says Schr\u00f6der. \"They are discovering the hard way that the process is not so easy, because UNFCCC rules [for forest-based schemes] are rather complex, maybe even more complex than regular CDM projects.\" \n               boxed-text \n             [anchor map]One of the complexities lies in documenting \u2014 to the UNFCCC's satisfaction \u2014 how much carbon a forest will actually store for the duration of a project. Different tree species take up carbon at different rates and store it for varying amounts of time, depending on lifespans and harvesting cycles. Although researchers have fairly good estimates for these figures, it has been challenging for people such as Kulkarni, who are going it alone, to apply the CDM's methods on the ground. To ensure he made all of his calculations accurately, Kulkarni spent a lot of time travelling to conferences, reading scientific papers and consulting with CDM specialists, climate scientists and forestry researchers. But Miguez says that such an involved approval process is unavoidable. \"The complexity of this type of project is necessary to guarantee that uptake of CO 2  from the atmosphere is real and measurable,\" he says. Managers of carbon-offset ventures must also meticulously account for 'leakage' \u2014 greenhouse gases that will be released into the atmosphere in the process of setting up the project. For example, a World Bank-funded reforestation project in the north Indian state of Himachal Pradesh had to account for emissions that would have been released by resident cattle, and would continue to be released when the cattle were moved elsewhere, along with those caused by land preparation, clearing bushes, petrol-burning trucks, water pumps and labourers. More challenging, say proponents, is demonstrating that projects meet the crucial 'additionality' concept, meaning that the greenhouse-gas reductions will be more than those that would have occurred anyway. As part of his application, Kulkarni had to prove that the land he wanted to plant on stood no chance of being otherwise reforested \u2014 by locals who own the land or by government 'social forestry' programmes \u2014 because the cost was too high and the soil so poor that it could support nothing but eucalyptus and other specially bred trees. Offset projects also have to describe the exact Global Positioning System coordinates for each of the land patches included, a job that, in India, must be contracted out to the only government agency that can produce such maps. This can cost thousands of dollars. Even if Kulkarni and others do get approval for their projects, they cannot be sure of getting a good price for their carbon credits \u2014 or of finding buyers for them at all. This comes back to Miguez's 'non-permanence issue', the fact that the carbon in the forest will eventually be released, however many times it is logged and replanted. Because of their uncertain environmental value, forest-generated credits are expected to fetch only $4\u2013$5 apiece in the global markets, compared with the $20\u2013$23 fetched by carbon credits from other offset schemes. So far, only a few buyers, including Japan, Canada, Spain and the World Bank's BioCarbon Fund have expressed interest in buying these credits, and China's Pearl River project is yet to sell a single one.  \n                On the way to market \n              Demand for forestry credits also remains low because the European Union does not yet allow their sale on its emissions trading system, or carbon stock exchange, ostensibly fearing that cheap forest credits would swamp the market and bring down prices. Miguez says this exclusion may be reconsidered during the next review by the system's governing committee, although it is still uncertain when that will be. Kulkarni and other forestry managers are hoping that pricing issues and the other problems they have encountered will be ironed out in the post-Kyoto negotiations, so all eyes are on the 2009 Copenhagen talks, when discussions are expected to begin in earnest. Kant says he would like to see a simplification of the rules and regulations for forest-based CDM projects so that \"the process becomes accessible to larger numbers of poor communities across the world through, for example, the reduction of transaction costs. This is one area that has to be tackled and will be tackled.\" Kant now conducts CDM training workshops for forestry officers in several developing countries and says that the situation is already improving. \"I constantly used to hear that the CDM just can't be done,\" he says. \"But now awareness and actual training efforts are growing, so there is increased understanding and confidence about the process.\" As many as six groups around India are currently in the early stages of planning forest offset schemes. Forestry managers still think that such ventures, with their UNFCCC stamp of approval, will provide an international, reputable business for the long term, says N. H. Ravindranath, an ecologist at the Indian Institute of Science, Bangalore, who serves on the government's CDM task force. \"The whole CDM process is maybe a bit too rigorous, but it is generally the right way of doing projects as it is introducing accountability, encouraging community participation and trying to set good standards.\" Kulkarni's experience in meeting those standards means that he is now in regular contact with others in the CDM community and has come to be known as something of an expert on forest-based carbon offsetting. \"I still say the future [of forest offsetting] is bright in India,\" he says, \"provided we line up simpler methods and practices.\" On the ITC plantations, more and more farmers are agreeing to grow eucalyptus trees for carbon storage, persuaded by the additional income. But as for Kulkarni, he says that his motivations have changed. \"By this point, earning money [from carbon credits] is not even the issue,\" he says. \"We just want others to feel that if we can succeed, they can too.\" \n                 Paroma Basu is a freelance writer based in New Delhi, India. \n               \n                     Nature Reports Climate Change \n                   \n                     Clean Development Mechanism \n                   \n                     Indian Tobacco Corporation \n                   \n                     CDM India \n                   Reprints and Permissions"},
{"file_id": "457141a", "url": "https://www.nature.com/articles/457141a", "year": 2009, "authors": [{"name": "Mark Schrope"}], "parsed_as_year": "2006_or_before", "body": "The best way to study life beneath the waves is to live there. Mark Schrope describes his experiences in the world's longest-running undersea laboratory. It is night on Conch Reef in the Florida Keys, and Niels Lindquist is running a thin green plane of laser light through a lone sponge. The psychedelic display captures the attention of a rather large, green moray eel. As Lindquist watches, the eel inspects the laser light, decides it doesn't approve and finds another way around, swimming right down the side of Lindquist's leg. It is only when the eel has vanished into the night that Lindquist notices that his dive buddy has risen to a safe distance with the wariness most divers accord such beasts. Lindquist, though, had been completely at ease. Lindquist's sangfroid comes from the sort of experience and familiarity most divers can't even dream of. A chemical ecologist from the University of North Carolina in Chapel Hill, he has spent the past ten days underwater, living and working at the world's only underwater long-term research station \u00e2\u20ac\" the Aquarius Undersea Laboratory, which sits next to Conch Reef. Aquarius is roughly the size of a school bus, 15 metres under the waves and 6.5 kilometres off the Florida coast. Managed by the University of North Carolina in Wilmington for the US National Oceanic and Atmospheric Administration (NOAA), the laboratory began operating in 1993. Over the years, according to its director, Andrew Shepard, it has sometimes been hard to convince people that the facility is worth its annual operating budget, currently about US$2 million. Since 2007, in part to provide clearer justification, Aquarius's chief scientist, Ellen Prager, and her colleagues have been working to better define the facility's mission. One research focus she and her colleagues are highlighting is technology development for long-term ocean observing \u00e2\u20ac\" an established priority for NOAA. The laser that was illuminating that sponge is one such technology. But there is more to the sponge work than a technological shakedown cruise. It is the latest part of a set of Aquarius-based studies of the roles that sponges play in reef ecology. Sponges have long been known to be prodigious pumpers of water 1 , but these expeditions have provided the first precise figures on quite how prodigious the pumping powers are \u00e2\u20ac\" findings that could have implications for reef research worldwide. Such work can be done without a home under the waves, but it takes a lot longer, and it might not lead to the same insights and achievements. An 'aquanaut' in Aquarius can spend 12 hours a day diving, a tally that could take weeks to accomplish with regular scuba dives from the surface. The sustained access that Aquarius offers also makes researchers much more responsive to anything untoward. Aquarius is not just about convenience, though. Most scuba divers will tell you that when you pause to really watch a section of a coral reef you are sure to spot subtle details you would have missed if you had kept swimming. Researchers who have lived in Aquarius say that experience is magnified many times over. \"You're looking at the complete ecosystem if you're working from Aquarius,\" says Chris Martens of the University of North Carolina in Wilmington. \"You gain so much, so fast.\" Aquarius is not just a home for aquanauts \u00e2\u20ac\" and the occasional astronaut who comes for some zero- g   simulation time and experience of cramped conditions. There are plenty of snapper and other fish living in the waters below the coral and algae-bedecked lab, utterly at ease with the human residents. Divers making their way into the hole at the end of the lab that leads into the 'wet porch' entryway are often greeted by chub fish swimming between their legs. The lab is filled with normal air under enough pressure to keep water from entering. Burps of air periodically escape the entrance hole, resonating inside the wet porch like the sound of a draining tub. From the wet porch, visitors enter a small workroom that houses research equipment and a curtained toilet. Every other room is visible to the world via webcams, creating a sort of human fishbowl effect. \"You get used to it,\" says Lindquist, with a laugh and in a voice of slightly higher pitch than normal, due to the extra atmosphere-and-a-bit of pressure. The next chamber is known as the main lock. On one side is a bank of monitors and controls for the lab's life-support system, and a computer. On the other is a simple kitchen where aquanauts prepare mainly freeze-dried meals, and a table that sits next to one of the portholes. \"We get friends coming to the view ports all the time, especially at night,\" says Lindquist. Soon after, a large barracuda swims by. The final space is the kind that someone seeking to make a sale might describe as cosy: triple-decker bunks on either side of the structure for four researchers and two technicians. \"There's not a lot of personal space,\" says Jim Hench, a veteran aquanaut from Stanford University in California. \"Unless you're outside \u00e2\u20ac\" then it's great.\" The North Carolina team's sponge-studying mission last September lasted eight working days \u00e2\u20ac\" a typical length for the facility's eight or so missions a year \u00e2\u20ac\" largely devoted to just four individual star sponges on Conch Reef. Like patients in an emergency department, each sponge was wired up at one time or another with a full complement of monitors and sampling devices that linked back to Aquarius: a device for measuring water flow, a mass spectrometer to calculate the isotopic signatures of nitrogen and other compounds going into and coming out of the sponge, and an autoanalyser to record nutrient concentrations.  \n                Pump it up \n              The team's most startling results have been on the sheer amount of water that the sponges pump. Below the waves and away from fairly well understood upwellings, ocean water tends to move only horizontally. \"A few millimetres of vertical velocity gets people's attention,\" says Hench. That's why, on a previous mission, the team had been astonished to measure a vertical-flow rate of 25 centimetres per second from a single giant barrel sponge \"When I first saw that I couldn't believe it,\" says Hench, \"because you just don't see that in nature.\" More measurements, though, proved that the rates were real. In fact, some vase sponges can pump more than 100,000 times their own volume in a single day. A modest-sized giant barrel sponge can pump 15,000 litres per hour, giving a weekly volume roughly equal to that of an Olympic-sized swimming pool. Some sponges have since proved less siphontastic; but in terms of flow for a given body size, some were even more vigorous in their pumping than the giant barrel sponges. The plumbing that enables the sponges to work at these impressive rates consists of a system of channels and chambers spread throughout the sponges' bodies. Cells known as choanocytes, which line the pumping chambers, are equipped with waving flagella that move water past filter-feeding components and out of the sponge. For decades, researchers thought that water currents assisted the sponge's pumping action. But work on Conch Reef suggests that the choanocytes work alone to establish those impressive rates. More surprising still is that barrel sponges in various parts of the reef will occasionally stop pumping in unison for periods of up to several hours for no apparent reason. \"There's a rhythm out there nobody has ever seen before,\" says Lindquist. What triggers this synchronized action is not clear. In the September mission, the team recorded that synchronized slow-downs, but not complete stoppages, seemed to be happening in the reef's stinker sponges \u00e2\u20ac\" so called because of the sulphurous smell they exude when brought to the surface The ways of the sponge are important because what comes out is not what goes in. Sponges are remarkably efficient at pulling particles and plankton out of the water during the few seconds that the water remains in their bodies 2 . This means that they are crucial for cleaning the water and helping to create the clear conditions that many reefs require to thrive. But they don't just change the particulate content of the water \u00e2\u20ac\" they also change its chemical content. Much of the responsibility for these changes can be laid at the door not of the sponges but of the microbes that live within them \u00e2\u20ac\" high-microbial-abundance (HMA) sponges can be 50% bacterial by mass. In past research, the team has showed that the microbes in HMA sponges process massive quantities of nitrogen from plankton and dissolved organic matter 3 . And researchers in the Netherlands have similarly found large nitrogen contributions from sponges in the cracks and crevices of coral reefs 4 . The nitrates and ammonia that these microbes churn out can serve as fertilizer for some types of seaweed. \"This definitely shifts the whole way that ecosystems function,\" says Martens. On many reefs in the Florida Keys, and around the world, seaweeds are increasingly smothering coral, with devastating effects. Whether the nitrogen that fuels this growth comes from the ocean itself or from the land, in the form of run-off and sewage, remains a topic of intense debate. Although they don't have a firm answer, the Aquarius researchers have shown that the sponges and the seaweed on Conch Reef often have similar nitrogen isotopic signatures. This serves as circumstantial evidence that wherever the nitrogen is coming from, the processing done by sponges to liberate nitrogen from other components of dissolved organic matter is important for the subsequent growth of algae. The team is now looking for direct evidence of the role of sponges in enhancing that growth. Martens and Lindquist believe that by increasing nitrogen fertilization, growing sponge populations could change the basic ecology of some reefs in a way that favours seaweed at the expense of corals. Unfortunately, there is relatively little solid evidence of historical sponge populations to test this hypothesis against \u00e2\u20ac\" although Martens, who lived in the Keys for several years while growing up, thinks that sponges were less common in the 1960s. But more sponges might not always be bad news. Preliminary studies from Aquarius suggest that some sponges and their microbes have a denitrifying effect, changing nitrate into nitrogen gas and thereby making what used to be a fertilizer into something that can then escape into the atmosphere. This could counteract some of the fertilizing tendency and thus possibly help to maintain the health of the reef. This side of the work began when the researchers realized that the nitrate and ammonium concentrations they were measuring amounted to only about half of what would be predicted given how much oxygen the sponges were taking up to process organic matter. For four years the team has been working to find this 'missing' nitrogen.  \n                Bagging data \n              In 2008 the North Carolina group finally established a way to clearly measure the conversion to nitrogen gas. Aquanauts wrestled a plastic bag over individual sponges to temporarily cut them off from the surrounding water, and the sponges were then exposed to ammonium ions with unnaturally high levels of a heavy nitrogen isotope. The team found that this 'labelled' ammonium was converted to nitrogen gas in one HMA species, but not in a low-microbial-abundance species. \"This is the most exciting discovery we could have made because of the potential to actually get rid of some nitrogen from reefs,\" says Martens. A second variation on the enclosed sponge experiment helped to answer another crucial question. Conversion to nitrogen gas was once thought to take place only through conventional denitrification, in which ammonium and nitrite are converted into nitrate and then to nitrogen gas. But in the past two decades scientists have found that in the ocean, most of this conversion involves a sort of shortcut known as anaerobic ammonium oxidation, or anammox 5 ,   6 . To determine the type of conversion at play in the sponges, the team also exposed the sponges to labelled nitrate. The team found that the sponges missed out the step of conversion of nitrate to nitrogen gas, supporting the idea that anammox is the denitrifying process that goes on in the low-oxygen microenvironments within the sponges. The researchers hope to run additional experiments to confirm this, then move on to determine rates and, by extension, how important sponge denitrification is to reefs. \"I think we've got the tiger by the tail, but we'll have to see if it turns out to be a big tiger or a little tiger,\" says Martens. If the group can show that anammox is occurring widely, he says, it could change people's views of how important sponges are on reefs. Cristina Diaz, a sponge expert at the Margarita Marine Museum in Venezuela, says that the group's research is reviving interest in sponges and their environmental impacts on reefs. \"I'm really excited,\" she says, \"I think it's fantastic work.\" Nonetheless, she says there is still much to be learned: \"We are still in diapers.\" Martens and Lindquist agree, and hope that Aquarius will remain available to support their own research and that of a growing number of other groups. Shepard says the lab is designed to last for at least another ten years, and that he is encouraged by growing support inside and outside NOAA. \"I think we're going in the right direction.\" Mark Schrope is a writer in Melbourne, Florida. \n                     Aquarius homepage \n                   \n                     World Porifera Database \n                   \n                     Spongebob Squarepants \n                   Reprints and Permissions"},
{"file_id": "457250a", "url": "https://www.nature.com/articles/457250a", "year": 2009, "authors": [{"name": "Declan Butler"}], "parsed_as_year": "2006_or_before", "body": "In the first of three features on the legacy of the Bush administration, Declan Butler looks at the United States' failure to deal with the risks of nuclear proliferation. \"Where do I start? One could write a book,\" sighs Frank von Hippel, a nuclear-weapons expert at Princeton University in New Jersey. He rattles off a litany of the ways in which he believes the administration of George W. Bush has harmed the cause of nuclear non-proliferation. The list includes the Iraq war, the administration's scuttling of the Nuclear Non-Proliferation Treaty (NPT), its opposition to the Comprehensive Test Ban Treaty, its pursuit of missile-defence schemes and a nuclear deal with India, and its foot-dragging on reductions in nuclear weapons. \"The Bush administration did about as much damage to non-proliferation as one could imagine anybody doing,\" says von Hippel. That gloomy assessment is largely shared by other experts. \"For non-proliferation, the Bush administration has represented at best stagnation, and in many places retrogression,\" says Christopher Paine, a nuclear-weapons expert at the Natural Resources Defense Council in Washington DC. \"All in all, not a good record.\" And for Daryl Kimball, executive director of the Washington-based Arms Control Association, the Bush administration has \"left the world demonstrably less secure today than it was a decade ago with respect to nuclear-weapons-related threats\". The international nuclear non-proliferation regime depends on an interconnected, fragile system of trust, confidence, and diplomatic and regulatory checks and balances built up over decades around the NPT, Kimball explains. Although cheats of the NPT regime, such as North Korea, have rightly attracted prominent concern, eight years of less-conspicuous non-proliferation deregulation under Bush has undermined the regime itself, say experts. Indeed, some find the situation depressingly similar to the recent carnage in the financial markets. One such example is Joseph Cirincione, president of the Ploughshares Fund, a grantmaking facility in San Francisco, California, that supports projects to prevent the spread and use of nuclear weapons. Writing last November in  Arms Control Today , Cirincione said that Bush non-proliferation officials are leaving \"office like financiers fleeing busted Wall Street banks, with precious assets squandered on risky ventures, once-solid institutions crumbling, surpluses turned into gaping deficits, and a string of problems mismanaged into crises that threaten to bring down a decades-old global regime\". \"The Bush administration effectively walked away from proven arms-control and disarmament agreements,\" says Kimball \u2014 most obviously the NPT. Kimball and other arms-control experts are the first to admit that the NPT regime and its related legislation are far from perfect. But they argue that the treaty has succeeded in creating a diplomatic space and norms that have largely prevented the appearance of new nuclear-weapons states. Although India, Pakistan, Israel and North Korea have developed nuclear weapons since the treaty came into effect in 1970, these experts say, many more nations have renounced their programmes to develop nuclear weapons. Without this non-proliferation system, the world might contain 30\u201340 nuclear-weapons states, they say. And should the system founder, they add, many new nuclear-weapons states would probably emerge, and this in turn would increase the risk of terrorists getting their hands on a bomb. Despite that risk, says Rebecca Johnson, founding director of the London-based Acronym Institute for Disarmament Diplomacy, the Bush White House viewed multilateral arms-control agreements as constraining US action yet offering the nation few benefits. So after the terrorist attacks of 11 September 2001, the administration effectively repudiated the 'grand bargain' that is the core of the NPT.  \n                Give and take \n              That bargain calls for the United States, Russia, the United Kingdom, France and China \u2014 the five states that had developed nuclear weapons before the NPT came into effect \u2014 to work to eliminate their stocks. In addition, these states undertook, albeit unofficially, to use their weapons only as a last resort in the event of a nuclear attack. In return, other states agreed not to develop nuclear weapons, and were guaranteed an 'inalienable right' to use nuclear energy for peaceful purposes. In 2002, however, reviews of US defence and nuclear policy enshrined a new 'Bush doctrine'. On the plus side, the doctrine formalized the post-cold-war reality: nuclear weapons were no longer central to foreign policy with Russia and China, who had ceased to be the nation's mortal enemies. And it accurately reflected the new, twenty-first-century reality: the immediate threat was no longer full-scale nuclear war, but that terrorists might obtain and use a nuclear bomb. The problem was how the doctrine responded to this new reality. It has been a \"disaster\", says Paine. It has \"completely backfired\" in terms of non-proliferation. For example, the doctrine authorized preemptive war against states considered to be developing weapons of mass destruction. It also left open for the first time the possibility of using nuclear weapons against states that did not have them, and blurred the historic red line between nuclear and conventional weapons by allowing tactical nuclear weapons to be used to attack enemy targets such as hardened bunkers. In doing so, the doctrine directly encouraged proliferation by countries in the US sights, says Bates Gill, director of the Stockholm International Peace Research Institute. He argues that it helped to convince countries such as Iran and North Korea that they needed to have nuclear weapons as deterrents to stave off potential US attacks. But if these two countries acquire nuclear arsenals it could trigger new nuclear-arms races in Asia and the Middle East, and perhaps beyond. Bush initially refused to negotiate with either country, which \"exacerbated\" the problem, says Kimball. At least partly as a result of that missed opportunity, North Korea exploded a nuclear test device in 2006, and Iran now stands just months away from having enriched enough uranium for a bomb. Only recently has the United States started talks with North Korea, and it has now made tentative moves to negotiate with Iran. On a broader level, say non-proliferation experts, the Bush doctrine's undermining of the NPT had a chilling effect on global non-proliferation efforts. It contributed directly to the collapse of the 2005 review conference of the NPT, in which member states failed to make any progress on a series of 13 measures that they had agreed to at the previous review conference five years earlier. These measures included the Comprehensive Nuclear-Test-Ban Treaty and a Fissile Material Cut-Off Treaty to outlaw the production of new weapons material. Bush delivered another blow to the NPT when he signed a deal with India to force the lifting of an international ban on nuclear trade with the country, says Gill. The deal set a dangerous precedent by rewarding a country that has refused to sign the NPT and that has developed nuclear weapons, he adds. It also alienated countries that had abandoned their nuclear-weapons programmes to join the NPT. \"In a word it has been negative,\" Gill says. Dismantling the multilateral non-proliferation system further, in 2002, the United States withdrew from the Anti-Ballistic Missile Treaty negotiated with the Soviet Union in 1972, so that it could pursue its plans to build missile-defence shields against nuclear warheads. \"This set a very dangerous precedent of a major country withdrawing from a major arms-control agreement,\" says Pavel Podvig, an expert on Russian nuclear affairs at the Center for International Security and Cooperation at Stanford University in California. The Bush administration has also \"basically dropped the ball\" on nuclear-arms reductions with Russia, says Podvig. Large reductions in arsenals were negotiated as part of the Strategic Arms Reduction Treaty initiated by Ronald Reagan in 1982. But the treaty expires at the end of this year, and no follow-up has been pursued. \"The Bush administration consciously made the decision that it was not going to seek any new agreement,\" says Podvig. And the beneficial scientific and political dialogue that the agreements opened between the signatories has atrophied, he says.  \n                Positive progress \n              There are some bright spots in the Bush legacy. Behind-the-scenes US and British diplomacy helped to persuade Libya to give up its programme of weapons of mass destruction in 2003. That same year, the United States helped to dismantle the vast black-market network of uranium enrichment and nuclear-weapons technologies run by Abdul Qadeer Khan, former head of Pakistan's nuclear programme. And initiatives to secure stocks of nuclear-weapons material worldwide have also progressed somewhat, says von Hippel. Another positive development \u2014 albeit one started during the previous administration of Bill Clinton \u2014 is the maturing of the stockpile stewardship programme launched in 1994 to maintain the US arsenal through simulation and physics rather than nuclear testing. The programme has defied sceptics, says von Hippel, and has been largely successful in demonstrating that existing US nuclear warheads are reliable enough not to need imminent replacement. But perhaps the most significant development in non-proliferation has emerged in the United States not because of the Bush administration, but despite it. This is the bipartisan groundswell for a major new commitment to disarmament, says Johnson. Consensus is growing that nuclear weapons are more of a liability than an asset now that the major threat to the country is a terrorist nuclear attack. The continued existence of massive nuclear arsenals, and the huge and inadequately secured stockpiles of weapons-grade fissile material around the world make this threat even more severe. The result is a movement to develop a comprehensive roadmap catalysed by Sam Nunn, former chairman of the Senate Committee on Armed Services and co-chair of the Nuclear Threat Initiative, a non-profit organization that aims to reduce the threat of nuclear and other weapons of mass destruction; George Schultz and Henry Kissinger, former secretaries of state; and William Perry, a former secretary of defence. The project aims to disarm the nuclear-weapons states, and in so doing gain multilateral support for a tougher international regime to counter nuclear terrorism and nuclear proliferation. Adding momentum, the Global Zero campaign put in place to back the plan was launched in Paris last month by prominent world figures, including former US president Jimmy Carter and other former heads of state. The next NPT review conference, in 2010, will test whether such efforts are paying off. Experts have high expectations for the meeting, as US president-elect Barack Obama made the reinforcement of multilateral non-proliferation and disarmament efforts a central part of his election campaign (see  page 235 ). \"We have an opportunity here in the United States to shift directions, and to repair our nuclear non-proliferation and disarmament strategies,\" says Kimball. \"You can never make up completely for lost time,\" says von Hippel, \"but there certainly is a feeling that we now have to do the best we can to make up for the lost time of the Bush administration.\" \n                 See \n                 Editorial \n                 Click here for Bush's legacy in \n                 science \n                 and the \n                 fight against AIDS \n                 Click here for more on the \n                 challenges facing Barack Obama \n               \n                     Nuclear Proliferation \n                   \n                     Acronym Institute for Disarmament Diplomacy \n                   \n                     Arms Control Association \n                   \n                     Global Zero \n                   \n                     Nuclear section of the United Nations Office for Disarmament Affairs \n                   \n                     Ploughshares Fund \n                   Reprints and Permissions"},
{"file_id": "457254a", "url": "https://www.nature.com/articles/457254a", "year": 2009, "authors": [{"name": "Erika Check Hayden"}], "parsed_as_year": "2006_or_before", "body": "Was setting up PEPFAR \u2014 a massive HIV treatment programme \u2014 the best thing that President Bush ever did? Erika Check Hayden investigates. On 1 December 2008, a parade of luminaries appeared on screen in Washington DC to pay video tributes to President George W. Bush on World AIDS day. It was the twilight of his administration, and an obvious time for reflection. But for these people \u2014 including former US president Bill Clinton, United Nations secretary-general Ban Ki-moon, rock star Bono and US President-elect Barack Obama \u2014 this was not dutiful lip-service. They were heaping praise on Bush's signature programme to fight AIDS, and what many view as his most significant positive achievement of the past eight years. By the next day though, the compliments had been eclipsed. The front pages of major newspapers were dominated by photos of Obama embracing Hillary Clinton, his pick for Secretary of State. None featured the accolades for Bush, or the new figures showing that his programme \u2014 the US President's Emergency Plan For AIDS Relief, or PEPFAR \u2014 had put more than two million HIV-positive people on antiretroviral treatments since Bush established it in 2003. The episode underscores the complicated legacy that Bush has left with his HIV programme, variously praised, criticized and overshadowed. PEPFAR is credited with being the first and largest bilateral foreign-aid programme to try to treat chronic disease on a mass scale, with US$18.8 billion spent so far. But it has also been highly controversial because of stipulations on how its funds should be spent. And now, as Obama takes over and with PEPFAR's leadership likely to change, the programme faces a challenging future. Unlike smallpox or polio, which were brought under control by vaccines that could be administered in a just a few doses, HIV drugs have to be delivered for life. \"Once you start people on treatment, you can't stop \u2014 you have made a long-term commitment to supporting therapy for these people,\" says Chris Beyrer, director of the Johns Hopkins Center for Public Health and Human Rights in Baltimore, Maryland. Such therapy could get increasingly expensive as those already on treatment become resistant to their current medications and have to switch to pricey alternatives. And many hope that in addition to covering these drugs, the programme will expand to reach the millions who are still not receiving any treatment at all \u2014 a costly scale-up at just the time when the world's economy is in sharp decline and the United States is in a recession. How to sustain this scale-up, says Anthony Fauci, director of the US National Institute of Allergy and Infectious Diseases in Bethesda, Maryland, \"is something that I dream about, think about, while I'm eating, sleeping \u2014 all the time\". For Obama, then, PEPFAR could prove a complicated inheritance. In 2003, only 400,000 people in poor countries had access to antiretroviral treatment, and the world was spending less than half of what was needed to reach their goals for combating AIDS by 2005, according to the Joint United Nations Programme on HIV/AIDS (UNAIDS). When Bush announced in his January 2003 State of the Union address his intention to create the $15-billion PEPFAR programme, he \"completely changed the landscape\", says Peter Piot, founder of UNAIDS. \"The most powerful man in the world moved from the 'm' word to the 'b' word \u2014 from millions to billions. In that sense, PEPFAR not only brought money, but elevated AIDS issues to one of the big political themes of our time.\"  \n                Stormy start \n              The programme was dogged by controversy from the outset. Bush became interested in putting together a large AIDS programme partly because the disease was becoming a big issue among Republican leaders and some of his conservative supporters. Yet some of those same supporters baulked at the prospect of supplying condoms, and with it the implicit endorsement of premarital sex. They found a compromise in a public-health approach called ABC \u2014 Abstinence, Be faithful, Correct and consistent condom use \u2014 that had been credited with helping to cut HIV prevalence in Uganda 1 . When lawmakers enacted the legislation that enabled PEPFAR, they dictated that one-third of the 20% spent on prevention must be used for abstinence education programmes. They also required organizations who would receive aid from PEPFAR to pledge their opposition to prostitution \u2014 ruling out support for any group trying to reach out to sex workers. And none of the money could go to groups that support abortion, under the 'gag' rule enacted by Bush on his first day in office. \"It's probably true that PEPFAR never would have gotten through Congress had it not been for these political compromises,\" says Lawrence Gostin, faculty director of the O'Neill Institute for National and Global Health Law at Georgetown University in Washington DC. But for many activists, these compromises confirmed their suspicions that PEPFAR was little more than a ploy to curry favour for the United States abroad after its internationally unpopular invasion of Iraq. If that was the intention, it backfired, says Thomas Coates, director of the Program in Global Health at the David Geffen School of Medicine at the University of California, Los Angeles. \"The directives made the United States look ridiculous to the world,\" he says. \"It was like, 'There they go again \u2014 being generous on the one hand and then earmarking these moral dictates on the other.'\". The taint of moral hypocrisy deepened when Randall Tobias, the first head of PEPFAR, who left in 2006, resigned from government a year later after being linked to a prostitution ring. Tobias, a former chief executive of the pharmaceutical company Eli Lilly, had publicly questioned the effectiveness of condoms and the reliability of generic drugs. He was replaced at PEPFAR by Mark Dybul, an intense young doctor who specialized in infectious disease and who had helped to work out the nuts and bolts of the PEPFAR programme with Fauci. Dybul was seen as being more in touch with the reality of AIDS. \"As a gay man who lived through the epidemic, he had a professional and personal connection to it in a way that Tobias didn't have,\" says long-time activist Gregg Gonsalves, now with the International Treatment & Preparedness Coalition in New Haven, Connecticut. \"He helped to give [PEPFAR] a more clinical focus and strip it of some of its ideology.\" By 2008, PEPFAR was estimated to have prevented infection in 240,000 babies born to HIV infected mothers and provided health care to 9.7 million people, on top of the two million receiving antiretroviral treatment. The programme targets 15 of the most stricken \"focus countries\" in Africa, Asia and the Caribbean. In addition to providing an overwhelming sum of money, PEPFAR also seems to have succeeded in setting and meeting targets from the outset, establishing a type of accountability that is often missing from aid efforts. \"The PEPFAR strategy was to make sure that promises were kept, and that was new in international health,\" Gostin says. Yet the controversy surrounding PEPFAR never let up. In 2006, a study 2  commissioned by the World Health Organization (WHO) found \"little evidence of the effectiveness of abstinence-only programmes in developing countries\". And in Uganda, critics say, restrictions on PEPFAR-funded organizations compelled the groups to place too much focus on abstinence and too little on condoms, helping to trigger a resurgence of the epidemic. PEPFAR officials counter that they always promoted the ABC strategy as a whole. \"Our policy was never abstinence only,\" Dybul says, \"anyone who read any of our documents could see that \u2014 we supplied more than 2 billion condoms.\"  \n                Poaching partners \n              The abstinence provisions aren't the only source of contention. Some have alleged that PEPFAR is poaching scarce workers from countries' own health programmes. Others complain that by focusing resources on AIDS alone, the programme neglects other, equally vital aspects of the health system, such as childhood vaccinations or other infectious diseases. PEPFAR was also criticized for introducing its own process for approving generic drugs, meaning that drugs bought with PEPFAR money were mostly expensive brand-name ones. But some PEPFAR beneficiaries have said that such policies were possible to work around. Agnes Binagwaho, former head of Rwanda's AIDS-control commission, says that her country purchased cheap generic drugs with money from other donors, such as the Global Fund to Fight AIDS, Tuberculosis and Malaria and the World Bank. Rwanda used some of its $30 million from PEPFAR when it needed to purchase brand-name drugs to treat patients who had developed resistance to their original treatments. \"There are so many needs, there is always something else the money can be used for,\" Binagwaho says. The 'B' part of ABC \u2014 be faithful \u2014 has also received some credit. Interest in how to encourage people to change their sexual behaviour is growing throughout the world, led by public-health researchers such as Edward Green, director of the AIDS Prevention Research Project at the Harvard Center for Population and Development Studies in Cambridge, Massachusetts. Green and others argue 3  that AIDS spreads more slowly in regions where people are encouraged to favour monogamous relationships \u2014 even if they are serial relationships \u2014 over multiple concurrent long-term sexual relationships. The theory is that the virus will spread more quickly between multiple partners in the acute infectious stage. And Green, who has been an adviser to PEPFAR, says that the organization has been in the vanguard here by advancing the idea that people need to change their behaviour in ways more radical than the wider use of condoms. \"PEPFAR is the only major donor that has promoted this,\" he says.  \n                Congressional credit \n             Three influential reports released since 2006 \u2014 two by the non-partisan US Government Accountability Office 4 ,   5  and one by the US Institute of Medicine 6  \u2014 have praised the programme's results, but faulted its spending directives for lessening the initiative's potential impact. And in July 2008, Congress finally responded, stripping PEPFAR of the abstinence provision during the programme's required re-authorization. The new law authorized $48 billion in new PEPFAR spending over the next five years \u2014 more than three times the original sum and a massive affirmation of the initiative's success. It still asks countries to explain themselves if they spend less than half of their prevention funds on abstinence and fidelity projects, but critics hope that this will not restrict distribution of the money. It also includes provisions that seem to counter other arguments against PEPFAR, setting targets to train 140,000 health workers, link AIDS and nutrition programmes and authorize $5 billion for malaria and $4 billion for tuberculosis. The challenge now is to build on PEPFAR's success. Programme officials acknowledge that the people they have reached so far may be the 'low-hanging fruit' \u2014 those that can travel to clinics, for example. Many more live a long way from roads or services and will be difficult to reach. The WHO estimates that two-thirds of the nearly 10 million people who need treatment in developing countries still have no access to it. There is also a rising chorus of calls for PEPFAR to expand its remit beyond HIV and begin providing services such as maternal care, clean water, basic sanitation and food. Some even want it to be folded into a new US department of international development with a broader portfolio. \"No matter what we do with PEPFAR, it will still ignore many of the fundamental things we need,\" says Gostin. But such broad ambitions seem unlikely to be realized. Tight financial times leave no room for Obama to start bold new initiatives in global health, and some worry that Congress won't even appropriate the $48 billion authorized for PEPFAR. Who will lead the programme also remains uncertain. Observers say that Dybul's strong defence of the ABC initiative may have doomed his chances of staying on under Obama, and several people have been rumoured as potential replacements. Among the most prominent names is Jim Yong Kim, director of the Fran\u00e7ois-Xavier Bagnoud Center for Health and Human Rights at Harvard School of Public Health in Boston, Massachusetts. Whoever takes over, observers anticipate that the programme will improve under the Obama administration, and that many of the problematic aspects, such as the prostitution pledge and the anti-family planning bent, will be eliminated. \"The good stuff will be salvaged, the bad stuff will be thrown out and PEPFAR will emerge a very different beast,\" predicts Stephen Lewis, co-director of the advocacy group AIDS-Free World, based in Boston. Even as the beast it is, the PEPFAR legacy continues to win Bush a level of regard that was only rarely voiced during his administration. \"I can't stand him,\" Gonsalves says, \"but Bush has done something unprecedented.\" And for Fauci, the praise is most apparent in the places where HIV is hitting the hardest. \"If you go to Africa,\" he says, \"into villages where PEPFAR has had a major impact, people look at you and say, 'Thank you, and thank God for the United States of America.'\". \n                 Erika Check Hayden is a senior reporter for  \n                 Nature  \n                  based in San Francisco.  \n               \n                 See  \n                 Editorial \n               \n                 Click here for Bush's legacy in  \n                 science \n                 and  \n                 nuclear non-proliferation \n               \n                 Click here for more on the  \n                 challenges facing Barack Obama \n               \n                     International AIDS Conference blogs \n                   \n                     UNAIDS \n                   \n                     PEPFAR \n                   \n                     NIAID \n                   Reprints and Permissions"},
{"file_id": "457252a", "url": "https://www.nature.com/articles/457252a", "year": 2009, "authors": [{"name": "Mark Schrope"}], "parsed_as_year": "2006_or_before", "body": "A look at George W. Bush's legacy in science. Although a president's influence extends only so far in determining the United States' research agenda, most if not all areas of science and environmental management feel some impact. In the eight years since he became the United States' 43rd president, George W. Bush has supported major increases in portions of the federal research budget, but he has also attracted serious criticism for some environmental policies and the degree to which policy decisions in his administration have been informed \u2014 or not \u2014 by the best available science. A full appreciation of his policies' impacts on a given field may take years. However, the raw data available now lend some insights as to what history is likely to judge as high and low points of the Bush administration's scientific legacy. \n               Alternative energy \n             The Bush years saw significant increases in energy production from such alternatives as wind, solar and biomass, driven in large part by skyrocketing oil and natural-gas prices. The Bush administration supported incentives to encourage continued expansion through tax credits and other approaches, but no provisions for permanent incentives were established. Total US renewable-energy production \u2014 which is dominated by hydropower and biomass \u2014 remains less than 7% of the country's overall energy consumption. Increase in total installed capacity for wind, solar, geothermal, hydroelectric, wood and waste power generation during the administration of Bill Clinton (1993\u20132001):  3.07% \n               Increase during the current Bush administration, to 2007: 12.5% \n             \n               Marine reserves \n             In 2006 Bush declared the Northwest Hawaiian Islands as the Papah\u0101naumoku\u0101kea Marine National Monument \u2014 the culmination of a long process that began well before he took office. On 6 January, the administration declared three additional, massive marine national monuments in remote areas of the Pacific that include coral reefs around several uninhabited islands and the Mariana Trench. No president has ever protected more ocean area, but the value of the protections will be largely determined by the extent to which the areas are ultimately managed and policed under future administrations. \n               Area of the Papahanaumokuakea Marine National Monument: 362,072 square kilometres \n             \n               Area of the new Mariana Islands, Pacific Remote Islands, and Rose Atoll Marine National Monuments: 505,773 square kilometres \n             \n               Stem cells \n             [image 13 right] Academics have struggled to quantify the detrimental effects of Bush's decision in 2001 to restrict federal funding for research on human embryonic stem cells to those lines in existence at the time. Aaron Levine, a public-policy professor at the Georgia Institute of Technology in Atlanta, recently studied the stem-cell research contributions of various countries. His work suggests that, relative to other similar research areas such as RNA interference, the US contribution to the field is deficient. Perhaps not surprisingly, all five countries showing the highest performance rates in the field \u2014 Singapore, the United Kingdom, Israel, China and Australia \u2014 permit the creation of new embryonic stem-cell lines. In 2006, Bush kicked off a focus on physical-sciences research by announcing the American Competitiveness Initiative. Among other goals, this aimed to double by 2016 the budgets for the National Science Foundation (NSF), the Department of Energy (DoE) Office of Science, and core funding for the National Institute of Standards and Technology. But whereas the president's proposed budgets each year were on track towards doubling, the budgets enacted by Congress have fallen well short of this mark for the NSF and the DoE. The fate of all agency budgets will remain unclear until the new president makes his annual budget request in early February. \n               Endangered species \n             Endangered-species listings slowed to a trickle during the Bush administration; critics have blamed an increase in bureaucratic hurdles and a failure at times to act according to scientists' recommendations. Officials say the decrease is due largely to litigation that forced the Fish and Wildlife Service to focus most available funding and energy on completing critical habitat designations \u2014 which did increase dramatically \u2014 for previously listed species. Having addressed much of this backlog, the agency says that more than 50 endangered-species listings are in the works for 2009. \n               Average number of endangered species listed per year \u2014 Clinton administration: 65; Bush administration: 8 \n             \n               Biosecurity \n             [image 15 right] One clear result of the 9/11 terrorist attacks was an increased focus on potential means of terrorism and a drastically heightened interest in research aimed at preventing and responding to attacks. From 2001 onwards there were huge increases in funding for civilian biodefence programmes (see  chart ), although many of those also serve broader purposes in preparing for infectious-disease outbreaks or other disaster scenarios. Even so, experts in the field say additional money is needed to address key needs such as development of medical countermeasures for attacks. A recent report on terrorism threats \u2014  _World at Risk_ , put out by the bipartisan Commission on the Prevention of Weapons of Mass Destruction Proliferation and Terrorism \u2014 concluded that a biological attack is likely somewhere in the world in the next five years. \n               Space \n             As part of his new 'Vision for Space Exploration', Bush announced in 2004 that the United States would send humans back to the Moon by 2020. The resulting surge in US lunar interest and activities, combined with a cadre of Moon probes launched by other countries such as China, Japan and India, mean that the coming years will see a massive flood of Moon-related data to analyse. The Lunar Reconnaissance Orbiter (LRO) and the Lunar Crater Observation and Sensing Satellite (LCROSS) are slated to launch together in late April. LRO will characterize the Moon's surface from orbit with an eye to finding safe landing and base sites for future astronauts; LCROSS will send two impactors plummeting to the surface to hunt for water ice in the shadowy craters near the lunar poles. Whether the larger vision of returning to the Moon is sustained depends on how Barack Obama crafts his space policy \u2014 an unknown at this time. \n               Number of US missions under development for lunar research, in 2001: 0 \n             \n               Number in 2008: 2 \n             \n               Research funding \n             [image 11 right] For the first few years of his administration, despite tight budgets and calls to expand into other research areas, Bush continued the substantial funding increases at the National Institutes of Health (NIH) that were begun under Clinton. By 2005, the NIH budget had levelled off, and the past few years have seen flat to negative funding once biomedical inflation is accounted for. \n                 Reported by Mark Schrope, a freelance writer in Florida. \n                 See \n                 Editorial \n                 Click here for Bush's legacy in \n                 nuclear non-proliferation \n                 and the \n                 fight against AIDS \n                 Click here for more on the \n                 challenges facing Barack Obama \n               \n                     US election special \n                   \n                     White House \n                   Reprints and Permissions"},
{"file_id": "457372a", "url": "https://www.nature.com/articles/457372a", "year": 2009, "authors": [{"name": "David Cyranoski"}], "parsed_as_year": "2006_or_before", "body": "After spurning wind power, China has swung around and embraced this clean energy. But the nation's love affair with wind may be spinning out of control, finds David Cyranoski. As he drives along a two-lane highway skirting the Gobi Desert, Hubert Beaumont sees nothing but wind. The road in this part of China's Gansu province runs completely flat \u2014 a monotonous route past grey rocks, a few shrubs and some 100 sleek turbines towering over the empty terrain. \"I can't think of anything else you'd want to do here except wind energy,\" says Beaumont, a Beijing-based wind specialist at Ecofys, a green-energy consulting firm. Indeed, the region is pegged to become the core of a 'mega wind-power base', a massive collection of wind farms with 5 gigawatts of capacity by 2010. Even considering that wind turbines generally produce only about a quarter of their advertised capacity, these would still generate enough juice to satisfy about a million energy-guzzling US homes. It will be the biggest wind-power development in the world. \n               boxed-text \n             For a country with such a bad environmental reputation, China is fast amassing green credentials in wind. Wind-energy generation capacity has nearly doubled in each of the past three years, and in 2007 the country surpassed its goal to achieve 5 gigawatts by 2010 ( see graph ). With plans to build four more mega-bases like the Gansu project, China is poised within the next decade to blow past four other nations, including Germany and the United States, the current number one and two in wind-energy capacity. China is also manufacturing and, increasingly, designing turbines. Domestic turbine manufacturers numbered just four in 2004. Now there are some 70. Within three years, says Haiyan Qin, secretary-general of the Chinese Wind Energy Association in Beijing, China will manufacture more turbines than other country. But experts wonder whether the superlatives applied to Chinese wind farms will also include 'least reliable' and 'most inefficient'. Wind-turbine manufacturers and wind-farm developers everywhere have faced teething problems, but China has perhaps faced more difficulties than most. Its wind farms are much less efficient than those in other leading countries, manufacturing defects have plagued Chinese equipment and the nation's electrical grid cannot carry all the wind power the country is generating today, let alone the huge amounts planned for the next few years. Some critics, including several from international turbine companies, blame a lack of planning and poor Chinese manufacturing. Chinese engineers, entrepreneurs and government officials are working to improve the situation but they have a long way to go. As Beaumont passes the Gansu farm, he sees both the promise and the perils of China's surge in wind power. A strong breeze blows across the desert but half of the turbines are standing still.  \n                Storm chasers \n              Although China is currently chasing wind power, its leaders had little affection for it just a few years ago. Over the 1980s and 1990s, efforts by the World Bank, the Asian Development Bank and others to get energy from wind in China fizzled out. A goal to have 1 gigawatt of capacity by 2000 passed without notice, or achievement. \"Before 2004, leaders thought wind was too small. They didn't think it was real energy,\" says Qin. But the sizzling economy boosted demand, energy prices soared and, in 2004, 24 of China's 27 provinces were hit by blackouts. At around the same time, environmental pressures set in ahead of the 2008 Olympics. The government responded in February 2005 with a Renewable Energy Law and subsequent guidelines that called for all major power companies to create a percentage of their energy from renewable sources other than hydropower: 3% by 2010 and 8% by 2020. With biomass resources too sparse and photovoltaics too expensive (although China is the biggest producer in the world), most of that renewable energy will be achieved through wind. \n               boxed-text \n             Beginning in 2004, farms started to open up in the windy northern and eastern perimeter of the country ( see map ). Total installed capacity climbed steeply: 1.3 gigawatts in 2005, 2.6 gigawatts in 2006, 5.9 gigawatts in 2007 and, according to early estimates, 10.6 gigawatts in 2008. But there is a hitch \u2014 China's wind farms are underperforming. All power installations have a 'capacity factor', which is calculated by dividing the energy actually produced by what the installations could maximally generate. According to data from the Beijing branch of London's New Energy Finance, a consultancy firm that advises investors on developments in renewable energy, on-shore turbines in other leading wind power countries have capacity factors of around 30%. China's is just 23%. \"China's numbers are not good,\" says the firm's Justin Wu. \"It might not seem significant, but a few percentage points could make the difference between a farm that is economically viable and one that is not.\" Wind experts blame several factors, starting with the turbines themselves. When wind-farm developers began gearing up around 2004, they imported turbines from established overseas manufacturers. Recently they have relied more on domestic makers. In 2005, Beijing added a requirement that 70% of turbine parts be made by domestic manufacturers. Major international turbine makers have established manufacturing plants in China, but they are losing out to local firms. According to data collected by the China Wind Energy Association, 2008 will be the first time that the installed capacity of Chinese-made turbines will exceed that of foreign ones. The 3.8 gigawatts already assigned to developers for the Gansu project, for example, does not include a single turbine from a foreign maker. That's great for China's wind-turbine late-comers, but is it good for the wind farms? According to Wu, Chinese wind farms using foreign models have a 5% higher overall capacity factor than those using domestic turbines. Domestic turbines are especially unproductive when first set up, says Wu. Because the technology is newer and less tested, Chinese turbines are also more likely to be shut down for maintenance, according to anecdotal evidence. A turbine's average down time is a closely guarded trade secret. But Xiliang Zhang, director of Tsinghua University's Institute of Energy, Environment and Economy in Beijing, says he hears reports that domestic turbines are standing still while foreign models nearby are humming away. Domestic models are 15\u201320% cheaper, and the quality has been rising quickly as they have either used proven foreign technology or have hired foreign engineers to help with designs. But as the lifetime of a turbine averages 20 years, the more-productive foreign models would be better buys overall, says Wu. (Representatives of two major Chinese turbine manufacturers did not return phone calls or e-mails requesting comments.) So why do wind-farm developers in China mostly choose domestic turbines? It is a controversial subject. Despite the mandate to source 70% of parts from domestic manufacturers, turbines used in major national projects, including the recently started wind megabases, are supposed to be purchased through open bidding. But the bidding process is a tricky business and many in the industry assume an opaque policy of favouritism, especially considering that most of the wind-farm developers and domestic turbine-makers are state-owned enterprises. \"The Chinese government cannot be taken as a model of transparency,\" says Paulo Fernando Soares, chief executive of Suzlon Energy in Beijing, a subsidiary of a major turbine manufacturer in India.  \n                Spin tactics \n              Turbine quality is one problem; finding the best turbine for the available wind is another. Every turbine has a 'load envelope' that defines roughly what wind speed, turbulence, wind shear and other conditions it will function best in. Some turbines work better for wind that comes in short bursts whereas others work better with long consistent spells of low wind. Poorly chosen turbines will be more likely to break down. A turbine with a wide blade can catch low winds but could be destroyed by typhoons. If turbines are poorly chosen for a site, their efficiency will plummet. \"Decisions made in the first year will affect the 20-year life of the turbine,\" says Sebastian Meyer, who researches wind-energy resources in China for Ecofys. \"Serious companies won't put turbines in sites for which they are not suitable,\" says Soares. But until now, he says, Chinese developers have too often relied on inadequate wind data. China is now trying to address this problem. By June, it will finish installing 400 masts to measure wind-energy resources in various regions throughout China. The masts will stand 70 metres, 100 metres or 120 metres tall, to match the height of the turbines. China plans to spend more than 200 million renminbi (US$30 million) on the four-year project, which started in 2007. \"This project is the key to make the national plan of wind-power development work,\" says Zhenbin Yang, deputy director of the wind-resource laboratory at the Chinese Academy of Meteorological Sciences in Beijing. Yang will analyse the data provided by the masts. The plan has its sceptics. Because wind can vary greatly over short distances, even this survey of broad areas of wind flow will be inadequate and 'micrositing' data will still be necessary from the specific regions where the turbines are planned, says Soares. More scepticism is aimed at a proposed secondary use of the data \u2014 to create new standards for turbine design specifically based on Chinese weather. \"Wind turbines are not like refrigerators sitting in a house,\" says Zhang, \"China is very different from the United States and Europe.\" But although it is still not clear what form the Chinese standards might take, some question whether China really needs its own standards. \"It's no colder in Mongolia than it is in Minnesota,\" says Soares. Given China's plans to make five huge mega-bases, the number of turbines in its future is staggering. By 2015, the Gansu project alone will boast 10,000 turbines with a combined capacity of 12 gigawatts. China's four other megabases \u2014 in Xinjiang, Inner Mongolia, Hebei and the Shanghai\u2013Jiangsu region \u2014 will total 60 gigawatts. But to make use of all that energy, China's wind hopefuls must tackle an even more intractable problem \u2014 the electricity grid. China's energy supply has been hamstrung by a fragmented and underdeveloped grid system that makes it difficult to get energy from coal-rich rural areas to the cities on the east coast. Wind has the same problem: it is produced mostly in sparsely populated regions that cannot use all that energy. The problem is compounded by the unreliability of a power source that is based on the weather. \"Grid companies don't like wind. It changes too fast,\" says Qin. Steve Sawyer, secretary-general of the Brussels-based Global Wind Energy Council, says that other countries, particularly the United States, have struggled with their grids. This month, in fact, Democrats introduced a bill in the House of Representatives that allocates US $11 billion to modernize the grid.  \n                Hurry up and wait \n              China's rapid expansion has caused delays down the line. Turbines often have to sit idle \u2014 on average for four months \u2014 before they get hooked up to the grid. The backlog is huge. Of the 5.7 gigawatts of turbine capacity installed by the end of 2007, only 4 gigawatts was plugged into the grid. Getting connected is just the first hurdle. Many grids are simply too puny to carry all the electricity being made. At peak production times, turbines often have to shut down so as not to overload the electrical networks. Newer turbines can alter the angle of their blades to miss the wind, and will slow to a halt. The only loss is the energy. For older turbines, the operator has to slam on the brakes. Turbines in China wear through their brakes at remarkable speeds, says Meyer. Elsewhere, the brakes are usually used \"once a month or less, but in China they're using it every four days\", he says. The delays and losses could drive people away from wind power, says Qin. \"If there is not a serious plan for the grid soon, we will not be able to develop wind energy beyond the next few years,\" he warns. So why hasn't the Chinese government's legendary ability to get things done kicked in. According to a December 2008 report by New Energy Finance, China's National Development and Reform Commission \u2014 the body that oversees national economic and social development \u2014 is concerned about the \"lack of supervision in China's rapid wind-power growth\", especially when it comes to the grid. But the government is afraid to do anything that would raise prices, says Zhang. The problem is low demand for the expensive energy. \"The solution is to force places, like Beijing, to buy it,\" says Zhang. But the government hasn't brought itself to do that. It's easier to let them stick with coal. Help might be on the way. The megabases have an economy of scale that will make them more attractive targets for grid developers. And in November last year, the country's biggest power supplier, the State Grid Corporation of China, which provides some 88% of the nation's power, announced plans to more than double its investment in grid infrastructure for 2009 and 2010 from 550 billion renminbi to 1.16 trillion renminbi. Regional projects will also help. In November this year, Gansu will put 20 billion renminbi into its grid to support its megabase, and Inner Mongolia, also set to get a megabase, plans to add 30 billion renminbi to its grid operations by 2010. These will feed into a scheduled upgrade from 110 kilovolts to 750 kilovolts for a transmission line running from Xinjiang in the northwest, through Gansu, to the eastern metropolises. \"It gives a feeling of optimism,\" says Beaumont. Of course, hooking up wind turbines might not be the main thing on grid developers' minds. Wind energy is currently only a drop in the bucket in China's 793-gigawatt energy supply. In 2008, wind accounted for just 0.3% of China's total electricity production, most of which was powered by coal. But if the five megabases go as planned, their combined capacity would provide more electricity than the controversial Three Gorges dam, even considering the dam's much higher capacity factor. Meeting government targets for renewable energy for 2020 will, according to most estimates, require a prodigious 100 gigawatts of wind energy \u2014 about 5% of China's total energy supply. Some estimates note a potential of 500\u2013600 gigawatts by 2040 to 2050. For now, the Chinese government has thrown its considerable weight into exploiting this resource. \"It is its flagship renewable-energy industry, so the government is going to support it,\" says Wu. But unless some dramatic changes are made soon, China's plans for wind energy might get blown far off course.  \n                 See Editorial, \n                 page 357 \n               \n                     American Wind Energy Association \n                   \n                     Ecofys \n                   Reprints and Permissions"}
]