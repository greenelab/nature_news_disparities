[
{"file_id": "540153a", "url": "https://www.nature.com/articles/540153a", "year": 2016, "authors": [{"name": "Marissa Fessenden"}], "parsed_as_year": "2006_or_before", "body": "Sensitive mass spectrometry and innovative cell-sampling techniques allow researchers to profile metabolites in single cells, but the field is still in its infancy. Sitting in his first-floor office, in an industrial lab space that opens onto a field of grazing cattle, analytical chemist Renato Zenobi explains one of the fundamental problems facing today's cell biologists. He traces out a curve representing the average concentration of a molecule in a theoretical cell population \u2014 a simple bell-shaped distribution. That distribution, he explains, can obscure complexity. To prove the point, he sketches two curves overlapping either side of the single peak, each representing a distinct phenotype in the population \u2014 and also consistent with that bell-shaped curve. \u201cTo really figure out if the distribution is multimodal or bimodal, you need to go down to the single-cell level,\u201d says Zenobi, at the Swiss Federal Institute of Technology (ETH) in Zurich. Cell heterogeneity is why some bacteria in a clonal population can develop antibacterial resistance. It also gives rise to the different cell subpopulations in the brain. And it explains tumour relapses. The tools to detect those differences are only just becoming available. \u201cRecent technological advancements \u2014 especially those made just in the past two years \u2014 have revealed that individual cells within the same population may differ dramatically,\u201d says Ananda Roy, programme leader of the National Institutes of Health (NIH) Common Fund working group for single-cell analysis in Bethesda, Maryland. \u201cAnd these differences can have important consequences for health and disease.\u201d Funders worldwide have queued up to support single-cell research. The NIH has invested in special initiatives to support single-cell profiling, starting with US$2 million in 2014, and almost 60 groups have now received awards under the programme. A collaboration between universities and companies in Japan launched the Society for Single-Cell Surveyor, which awards grants and holds symposia on single-cell analysis and technologies. And in October, experts discussed launching the International Human Cell Atlas Initiative, which aims to chart every type of human cell and its properties \u2014 an ambitious task that relies heavily on single-cell analysis. Much of this work focuses on revealing cell-to-cell differences at the DNA level. Yet profiles of genes and epigenetic modifications merely outline a cell's potential. The rapid, dynamic responses a living cell has to its environment are better reflected in the metabolic transformations \u2014 and the resulting patterns of small molecules \u2014 that keep the cell powered, cycling and communicating with others. \u201cThe metabolome is most directly related to the phenotype,\u201d says Caroline Johnson, an analytical chemist at Yale School of Public Health in New Haven, Connecticut. It reveals the products of the genome and its protein output, as well as metabolites from diet, drugs and toxic compounds. Yet its complexity means that metabolomics has lagged behind other 'omics'. Unlike DNA and RNA, metabolites cannot be amplified. Although some metabolites can reach millimolar concentrations, a single cell offers a limited volume for analysis. Exquisitely sensitive methods are needed to detect rarer compounds \u2014 and the abundances can change in seconds in a living cell. Metabolites also have a bewildering variety of structures. The Human Metabolome Database contains records for more than 42,000 metabolites, from sugars to peptides to cofactors. But the total may be significantly higher, and single analytical methods often struggle to capture the chemical diversity. Still, the field is advancing, thanks to leaps in detection capabilities, increasingly sophisticated ways of isolating and handling single cells and developments in bioinformatics. \u201cWe are getting close to making single-cell metabolomics robust,\u201d says Jonathan Sweedler, an analytical chemist at the University of Illinois at Urbana\u2013Champaign. \u201cI can't see exactly how it is going to work, but I look at the increase in detectability and throughput of mass spectrometry and I say it will happen.\u201d \n               Pushing sensitivity \n             Profiling individual yeast cells allowed Zenobi's group to spot two phenotypes lurking in a genetically identical sample \u2014 one characterized by low levels of a metabolite called fructose 1,6-bisphosphate, and another with high levels 1 . The difference probably comes down to different glucose-utilization strategies. The insight doesn't have immediate biomedical applications, Zenobi admits, but it does illuminate a fundamental way in which cells work. To draw such insights, Zenobi's team uses sophisticated techniques to isolate cells and to boost the sensitivity of its analytical approaches. Researchers generally use either mass spectrometry or nuclear magnetic resonance (NMR) to drive metabolomics studies. But because NMR is less sensitive, mass spectrometry has emerged as the method of choice, and there are many variations to improve its detection abilities, throughput or simply make it easier to pull the contents out of a cell. \u201cThe whole arsenal of mass spectrometry has been thrown at this problem,\u201d Zenobi says. Mass spectrometry involves ionizing a sample to lend a charge to its constituent molecules. The charge then means that magnetic plates can nudge the molecules as they fly through a vacuum. Each molecule deflects by a different amount, depending on its mass-to-charge ratio, such that by the time the molecules reach the detector, they have resolved into their component parts. The data appear as a complicated plot of unidentified peaks, each corresponding to a different molecular entity. The method is straightforward \u2014 except when applied to single cells. Attempting to detect just a few molecules in a vanishingly small volume pushes the limits of modern instrumentation. Zenobi uses a specialized silicon slide to individually deliver hundreds of single cells into the mass spectrometer. To the naked eye, the slides seem to be covered with a fine black mesh. The mesh is a coating of a polymer called polysilazane, which has been laser micro-machined to create hundreds to thousands of reservoirs, each a couple of hundred micrometres in diameter. When the researchers add a dilute solution of cells to the slide, the coating's repellent properties ensure that some liquid, and one or two cells, end up in each reservoir. The researchers then direct the spectrometer's laser to target each well in turn. One configuration of these slides, called microarrays for mass spectrometry (MAMS), is available from MilliporeSigma in St. Louis, Missouri. Across the hall from Zenobi's office, graduate student Robert Steinhoff demonstrates how the slide fits into a mass spectrometer. The researchers use matrix-assisted laser desorption/ionization (MALDI) coupled to a time-of-flight analyser, which requires them to spot their slides with a chemical matrix to drive ionization. By using a matrix that minimizes interference with the signal given off by small molecules, the team can detect metabolites in the low attomole (10 \u221218  moles) range \u2014 and do so for about 1,000 cells per chip, which is relatively high throughput in the single-cell world. Steinhoff is also working to alter the substrate used in his reservoirs. \u201cMaking nanostructures \u2014 pillars \u2014 within the spot helps us see things more reliably,\u201d he explains. The mechanisms behind this effect aren't yet clear, but Steinhoff says that the cells end up clinging to the tops of the silicon columns. Sweedler has developed a high-throughput approach that uses a computer to guide the laser to individual cells spread across a slide. His team can process about 10,000 cells per slide in this way. But for a more comprehensive look at the metabolome, Sweedler takes it one cell at a time. He uses a modified patch-clamp tool, which typically records cellular electrical signals, to withdraw roughly three picolitres of cytoplasm (about 10\u201340% of the total volume) from individual brain cells and deliver it into a mass spectrometer. The limited throughput restricts analysis to a few dozen cells per experiment. Still, Sweedler's group has used it to detect about 60 metabolites from 30 neurons and astrocytes in slices of rat brain 2 . The team focused on neurochemicals such as glutamate, but also detected amino acids and derivatives of ATP, among others. From that, they compiled unique profiles for the different cell types, providing a window into the cell-to-cell heterogeneity that makes the brain so complex, Sweedler says. \n               Different strokes for different sizes \n             When dealing with single cells, size matters. Plant cells can be anywhere from 10 to 100 micrometres across. Mammalian cells tend to be smaller, on the scale of 10\u201320 micrometres. Microbial cells are smaller still, reaching down to the submicrometre range. As cell size varies, so too does the volume and thus the absolute number of metabolites. \u201cIt is obvious from an analytical perspective there will be no single method that can deal with all these volumes,\u201d says Akos Vertes, a chemist at George Washington University in Washington DC. His lab uses different methods for different cell sizes. For the largest cells, the team uses a sharpened optical fibre to deliver infrared light directly into the cell. The light excites the oxygen\u2013hydrogen bonds in the water of a cell, causing the cell to explode and eject its contents. The escaping material then meets an aerosolized, ionized liquid called an electrospray, which charges the molecules for mass spectrometry. The advantage of this technique is that single cells can be profiled while still embedded in tissue. But it also can be very slow, because each cell generally needs to be poked with the fibre by \u201ca very patient graduate student\u201d, Vertes says. He recently automated the process, using a computer to manoeuvre the sample stage. The smallest cells are deposited on a nanopillar-covered surface, also made of silicon although fabricated differently from Steinhoff's. Imaging the entire surface reveals where the instrument's ion beam needs to aim to target single cells. With this method, the team can routinely detect metabolites at femtomole (10 \u221215  moles) levels 3 . But the investigators estimate their lower limit of detection at about 800 zeptomoles (one zeptomole is 10 \u221221  of a mole), or about 482,000 molecules. It is possible to go even smaller. Bioanalytical chemist Andrew Ewing at the University of Gothenburg in Sweden examines the small-molecule content of neural vesicles. These vesicles are key to delivering and releasing chemicals into the space between cells to facilitate cell-to-cell communication. \u201cIt's tricky because you get very little signal with so few molecules in there,\u201d he says. Ewing uses an approach called nanoscale secondary ion mass spectrometry (NanoSIMS), which bombards a sample surface with a high-energy beam of caesium ions. This assault ejects charged particles from the surface, and they can then be analysed in a mass spectrometer to determine the material's composition. Ewing's group uses the method to assess distribution of the neurotransmitter dopamine. And by correlating the NanoSIMS data with transmission electron microscopy to observe a single vesicle as it loads and unloads dopamine, the researchers determined that a vesicle's inner shape might regulate how quickly this process happens 4 . \n               Seeing is believing \n             At the RIKEN Quantitative Biology Center in Osaka, Japan, chemist Tsutomu Masujima uses a video feed to target single cells for mass spectrometry. Individual cells behave very interestingly and unexpectedly. \u201cIndividual cells behave very interestingly and unexpectedly, so I like to see [them] as much as possible,\u201d Masujima says. His approach involves inserting a nanospray needle directly into a cell under video observation, sucking out the contents and then using the same needle to inject the contents into the mass spectrometer. Adding the visual component allows his group to complete delicate manoeuvres, such as capturing and analysing the amino acid and lipid contents of single white blood cells and tumour cells circulating in the blood 5 . But it also allows his team to estimate molecular abundance. Often, such a seemingly trivial calculation is complicated, because researchers aren't sure exactly how much volume they are analysing. So Masujima and his colleagues use 3D microscopy to observe the cell before and after removing a small amount of cytoplasm. By measuring the resulting deformation, they can deduce the volume they removed as well as the location from which it was taken 6 . In one case, they sampled a cytosolic metabolite called methionine sulfoxide, determining that they had captured 5.9 zeptomoles of it. \n               Making sense of the data \n             Fortunately, says Gary Siuzdak, a chemist who heads the Scripps Center for Metabolomics and Mass Spectrometry in La Jolla, California, bioinformatics tools are available to make sense of such findings. Siuzdak's centre runs a cloud-based metabololomic analysis platform called XCMS Online (see  'Selected metabolomics software' ), whose 12,000-plus users have collectively shared more than 120,000 jobs. Few of those jobs have involved single cells, Siuzdak acknowledges, but that doesn't mean that they are inherently incompatible with the software. \u201cOn the bioinformatics side, I don't see a major issue with doing these experiments,\u201d he says. Rather, the main challenge with single-cell metabolomics is one of instrumentation: devices that can analyse enough single cells, and enough metabolites per cell, for the results to be statistically meaningful. \u201cThe primary issue with single cells is that the hardware still needs to be improved,\u201d Siuzdak says. Studies tend to profile tens or maybe a hundred different molecules. But a single yeast cell can have some 600 metabolites 7 . As a result, even the most sensitive analytical techniques are picking up only the easiest-to-detect, most prevalent molecules in a cell; less-common ones fall below the radar. A possible solution to this problem, says computational biologist Jianguo Xia of McGill University in Montreal, Canada, may lie in population-based tools and in various 'omics. \u201cThe pipelines and tools that have been developed for metabolomics data sets generated from bulk cell populations can be reused,\u201d he says. All that may be needed are slight modifications to the data-processing and normalization methods that researchers use. \u201cSingle-cell transcriptomics is already established, and we can learn from it to accelerate bioinformatics development for single-cell metabolomics,\u201d he adds. Other researchers are pursuing single-cell strategies that are based on methods other than mass spectrometry \u2014 in particular, using living cells. Matthias Heinemann, for instance, who collaborated with Zenobi during his postdoc, now heads a molecular systems-biology group at the University of Groningen in the Netherlands. There, he uses his background in biochemical engineering to \u201cfind other ways to zoom into single cells\u201d. In one approach, his group uses fluorescent molecular sensors to quantify metabolites, such as ATP. In a forthcoming paper in  Molecular Cell , his group has used time-lapse microscopy to watch levels of ATP and another metabolite, NADH (which is autofluorescent), oscillate as the cells go through the cell cycle 8 . Key technical challenges have already been solved when it comes to single-cell metabolomics, Heinemann says. \u201cWhat is needed now is to do tedious development and validation work.\u201d The process might not be glamorous, he admits, but it is essential if single-cell metabolomics is ever to make the leap from proof of concept to answering fundamental biological questions. \u201cWe are often so happy to detect unique molecules, but my point is: why?\u201d Masujima says. \u201cWhat is behind this finding, why does this molecule come up?\u201d Without that insight, techniques run the risk of being mere gimmicks that fail to address questions of biological significance. \u201cI don't want to be a scientist who does that kind of science,\u201d he says. Reprints and Permissions"},
{"file_id": "537433a", "url": "https://www.nature.com/articles/537433a", "year": 2016, "authors": [{"name": "Monya Baker"}], "parsed_as_year": "2006_or_before", "body": "Numerous variables can torpedo attempts to replicate cell experiments, from the batch of serum to the shape of growth plates. But there are ways to ensure reliability. When Alastair Khodabukus tried to engineer muscle fibres in his new laboratory, he saw something strange: the tissue was convulsing. He had been growing fibres from the same mouse-derived clone for years, but these were different. They burned more glucose, contained lower amounts of a protein that promotes faster relaxation and fatigued less readily than those he had grown before his lab moved from Dundee, UK, to the University of California, Davis. The difference, he thinks, was due to how cows are raised in the United States 1 . Most academic labs culture cells by using fetal bovine serum (FBS), a liquid extracted from clotted cow blood and collected from abattoirs when pregnant cows are slaughtered. What ends up in the serum depends on factors such as diet, geographical location, time of year, whether the animals receive hormones or antibiotics and the gestational age of fetal calves. Substantial amounts of FBS are added as a supplement to the culture media in which cells grow; 5\u201315% of the volume of growth media is typical. FBS composition can affect how thick an engineered tissue becomes, cause spontaneous artefacts that mimic cell activity and even influence how surface receptors respond to a given compound. \u201cFBS is like a big dark cloud over our heads, not knowing what's real and what's not,\u201d says Khodabukus, now a postdoctoral researcher at Duke University in Durham, North Carolina. And serum is just one of many factors that researchers have to consider when studying cells. At a US National Institutes of Health (NIH) workshop on cell culture and reproducibility last year, Richard Neve, a cancer biologist at the biopharmaceutical company Gilead Sciences in Foster City, California, worried that researchers could become overwhelmed. \u201cA lot of labs see the magnitude of the problem and the complexity of the problem, and enter the primordial part of their brain and shut down.\u201d With the right mindset, however, and some obsessive checking and planning, researchers can gain confidence in performing their experiments. The most basic step is to ensure cells' genetic identity. Journals and funders now ask researchers to disclose whether they have checked to make sure that, say, cell lines representing corneal or skin tissue are not actually a fast-growing line derived from human cervical cancer. But cells' behaviour can also change with density, proliferation rates, growth media, the presence of contaminants and the time kept in culture 2 . Serum is arguably the most common supplement in cell-culture media, and also the least consistent. Human serum harbours thousands of distinct proteins originating from a wide range of cells and tissues, as well as thousands of small-molecule metabolites, all in varying concentrations. FBS probably has similar complexity, with plentiful factors to support a fast-growing fetus, too. FBS is not only variable, it also differs from the fluid that cells are exposed to in their natural environment. Most cells are in contact not with blood directly but with the interstitial fluid that bathes organs, says Adam Elhofy, chief science officer at Essential Pharmaceuticals in Ewing, New Jersey, a company developing a serum replacement for multiple cell types. Hormones, growth factors and other signalling molecules are abundant in serum, but tightly regulated in organs, he says (see  'Bovine serum's wide range' ). \n               Going serum-free \n             To overcome such concerns, reagent firms have developed serum-free growth media. Scientists pursuing 'bioprocessing' applications \u2014 such as the manufacture of therapeutic proteins and vaccines, a process in which animal products are frowned on \u2014 have embraced the serum-free alternative. Stem-cell researchers, who know these cells are sensitive to even small changes in growth conditions, are also enthusiasts. Many more researchers are now beginning to pay attention to how they treat their cells, driven by concerns about consistency and a push into translational medicine. These priorities are encouraging more scientists to avoid serum, says Ken Yoon, who is head of strategic marketing in the research division of MilliporeSigma, a life-science reagents company in Billerica, Massachusetts. Chemically defined, serum-free media is one of the fastest growing segments in the cell-culture space, he adds. But serum-free media are not always possible, or pragmatic. \u201cEveryone agrees it would be a great thing if we can move away from FBS and to something more defined,\u201d says Jon Lorsch, head of the US National Institute of General Medical Sciences in Bethesda, Maryland. \u201cThe question is how feasible it is, and we don't know the answer to that question.\u201d Most serum-free formulations apply only to a specific cell type or closely related group of cell lines. Vendors sell one serum-free medium for, say, Chinese hamster ovary cells, an epithelial cell line that is often used to produce therapeutic proteins, and others to expand particular types of blood cells. Formulations don't work for all cell types: many 'primary cells' \u2014 those taken directly from living tissue \u2014 require serum to grow after they are removed from the cues that the body provides, says Jennifer Welser-Alves, associate director of research and development at ScienCell Research Laboratories in Carlsbad, California. \u201cAnything you can do to boost the cells and keep them growing is necessary,\u201d she says. Some formulations require adding just 2% FBS to primary growth media, a low volume of serum that helps cut down on variability. Even if the option is available, many researchers are unwilling to take the time, or the risk, to wean their cells off serum, says Paul Price, a culture-media consultant in Mount Pleasant, South Carolina, who has designed serum-free formulations. \u201cEvery year since 1980, people have been saying that serum is dead,\u201d he says. \u201cSerum is still very popular because people like the idea that they can grow cells and not have fabulous technique.\u201d Culture is tough on cells: researchers pipette them from dish to dish, freeze and thaw them, add digestive enzymes to detach them from substrates and more. Serum is a balm for such abuses, says Price. \n               Stuck with serum \n             No commercial formulation is available for skeletal muscle fibres, says Khodabukus. He has spent two years tinkering with recipes that combine dozens of growth factors and other signalling molecules. When the fibres' performance changes with each lot of serum, it disrupts his own projects and muddies collaborations, he says. \u201cI'm going to spend the rest of my life working with this system, and as a scientist I want control. If we can get this to work and be consistent, we can get this to work in every lab around the world.\u201d Keith Baar, Khodabukus's former postdoc adviser at the University of California, Davis, relies on a more common solution: he keeps a freezer in his lab that's dedicated to storing serum. When serum starts to run low, he orders and tests at least four batches, and watches the cells' performance to find the closest match to that in his current experiments. Then he buys 100 bottles from the same lot of serum. That can drain US$25,000 from his lab budget, but it means that his lab members can continue their experiments without stopping every few months to test more lots of serum. Researchers who don't test their serum could run into trouble, says Matthew Sikora, a cancer biologist at the University of Colorado, Denver. He uses breast cancer cell lines to work out the effects of 'weak oestrogens', which include certain drugs and industrial chemicals such as bisphenol A. Sikora buys serum that has been treated with charcoal to strip out steroid hormones and other greasy molecules. Then he tests the serum on cells that have or lack oestrogen receptors; if the hormones in the serum have been effectively removed, the proliferation rates should be the same. Last year, he and others in his laboratory were stalled for about six months when sequential batches of serum failed this initial screen. Differing hormone content \u201ctotally flipped\u201d the interpretation of how a cancer drug worked 3 . Sikora thinks that unrecognized variation in serum might explain why he and a potential collaborator could not get consistent results. But even when researchers do batch testing, they don't always know what to look out for. Some laboratories simply buy the serum lot in which their cells grow the fastest. Instead, they should tailor screens to the intended study. Researchers also need to report exactly how they screen serum to enable others to reproduce the work, says Sikora. Some cells and experiments will be more sensitive to the effects of serum than others. The 'transformed' cell lines selected over decades for robust growth tend to vary less than 'diploid' lines or primary cell lines that more closely resemble natural tissue. Researchers always need to be careful, says Mariella Simon, a cell and developmental biologist at the Children's Hospital of Orange County in California. Ideally, they should have enough serum to last an entire study. And when they do move to a new bottle, they should make sure that no other reagents have changed and that they have enough old serum stockpiled to test whether any strange results can be attributed to the switch. It is easy, for example, to conclude that something is going wrong with a protocol to introduce DNA into cells when, in fact, a new batch of serum has affected division rates. Researchers should also record the information supplied by vendors about serum, including lot numbers, says Simon. \u201cYou can't just use your labmate's serum that might have been aliquoted a long time ago and labelled FBS.\u201d Contaminants can confound experiments, too. One of the most insidious is  Mycoplasma . This tiny bacterium can slip through sterilizing filters and is unfazed by many antibiotics. It depletes cells' nutrients and alters DNA and protein synthesis. An analysis of nearly 10,000 rodent and primate samples found that more than 10% contained RNA sequences unique to  Mycoplasma 4 . Conventional  Mycoplasma  testing can take several weeks and still miss rare strains, but PCR-based tests are now providing swifter, surer answers, says Yvonne Reid, who leads standards-setting efforts at American Type Culture Collection, a non-profit repository for cell lines in Manassas, Virginia. To avoid serum contamination, some researchers are opting for gamma irradiation. Several common contaminants, including  Mycoplasma , are sensitive to even low levels of radiation. But this requires a balancing act: radiation also damages growth proteins and bioactive molecules that help cells thrive. Many vendors offer gamma-irradiated serum, and the International Serum Industry Association has set up a working group to elucidate its effects 5 . Cell-culture consultant Raymond Nims of RMC Pharmaceutical Solutions in Longmont, Colorado, advises anyone who plans to work with gamma-irradiated serum to first test that cells perform as expected, and to remember that even contaminant-free serum cannot prevent infection by other sources. \n               Errors come from everywhere \n             The cells' physical environment is a profound influence. Researchers at the Wyss Institute in Boston, Massachusetts, found that mechanical peristalsis-like deformations and fluid flow changes alone could, without any alterations to the growth media, induce functional villi from cells that otherwise grow flat 6 . Lab dishes of different brands leach different chemicals into cell-culture media, and can confound studies of cell metabolites. Deliberate additives can change cell metabolism in unappreciated ways: antibiotics in particular frequently impair mitochondrial activity. Even a glass door on a lab refrigerator can ruin experiments, because some chemicals in growth media are sensitive to light. Just changing the laboratory plates, and thus the height of media in which cells are sitting, can alter how cells behave. What's more, cells growing in a given culture are not identical, and the subset of cells that thrives the most can quickly dominate a population. That means cells may not revert back to former behaviour if a researcher decides to restore previous experimental conditions. In all these experiments, the cells themselves are the most important variable. There is no quick, simple way to know that cells are fit for purpose, says John Masters, a cancer researcher at University College London, and author of cell-culture reference books. \u201cGet to know your cells,\u201d he urges. \u201cThe best assay you have for knowing how happy the cells are is looking at them.\u201d Leland Foster, a cell-culture consultant in Salt Lake City, Utah, and former chief executive of HyClone Laboratories, the cell-culture reagents company now owned by GE Healthcare Life Sciences, thinks that trainees cannot, generally, be expected to take the care required. \u201cOne way that labs could get away from variability is having some expertise that is resident in that laboratory,\u201d he says. Growing cells is, he says, best left to \u201can expert cell culturist\u201d who can tell when cells are \u201csmiling or frowning\u201d, and who will ask serum and other vendors tough questions about the products they buy for their cells. Cells react differently when they are growing rapidly or persisting in a stationary phase. If they are 'overpassaged' (that is, kept in culture too long), other changes can occur and affect reproducibility. Even when the genetic identity of a cell line has been authenticated \u2014 as is now broadly recommended \u2014 other crucial attributes, such as the growth state, number of doublings and checks for contamination, too often remain undocumented. \u201cAuthentication means more than identity,\u201d Reid says. Researchers hoping to reproduce experiments should not have to \u201cact like a detective\u201d to work out what state the cells were in when building on a reported study. Given these unknowns, researchers should take a week or so to optimize their cells' growth and plot a growth profile before launching experiments, says Reid, who is coordinating an open-access series about best practices in cell culture. A growth profile can inform researchers when to harvest cells, when to do assays and when to go back to a distribution bank for a fresh batch of cells. It can also warn scientists if they are overlooking important variables. Most of all, researchers must be alert and creative to make sure the cells they are using are consistent across a study, Neve says. \u201cThere is no single set of experiments that works for everyone.\u201d Concrete data, like good microscope images or expression data, can help researchers recognize when the cells used in their experiment have changed, says Anne Plant, a division chief at the US National Institute of Standards and Technology in Gaithersburg, Maryland, who hopes to find quantitative ways of making cell-culture experiments comparable across laboratories. \u201cOne of the hardest things to assess is what constitutes a healthy cell,\u201d she says. Even harder can be the consequences for researchers who neglect to think of cells as \u201clive beings that need to be looked after and cared for\u201d, says Masters. \u201cSomeone's PhD goes down the pan, or a grant is lost, or years of work are wasted because they are not doing fairly simple quality control.\u201d \n                     Hallmarks of pluripotency \n                   \n                     Cell culture: A better brew \n                   \n                     Biotech firm announces fast test to unmask imposter cell lines \n                   \n                     Contamination hits cell work \n                   Reprints and Permissions"},
{"file_id": "530367a", "url": "https://www.nature.com/articles/530367a", "year": 2016, "authors": [{"name": "Asher Mullard"}], "parsed_as_year": "2006_or_before", "body": "Drug discovery is a daunting process that requires chemists to sift through millions of chemicals to find a single hit. DNA technology can dramatically speed up the search. Nestled in a plastic box, in an ordinary laboratory freezer on the second floor of a concrete building in Waltham, Massachusetts, is a clear test tube that contains a concoction of astronomical proportions. The library frozen within, a collection of chemical compounds owned by London-based pharmaceutical company GlaxoSmithKline (GSK), contains as many as 1 trillion unique DNA-tagged molecules \u2014 ten times the number of stars in the Milky Way. This and other such libraries are helping pharma companies and biotechnology firms to quickly identify candidate drugs that can latch onto the proteins involved in disease, especially those proteins that have proved difficult to target. They enable screening to be performed much more rapidly and cheaply than with conventional methods. And academic scientists can also use them to probe basic biology questions and investigate enzymes, receptors and cellular pathways. Drug discovery often starts with researchers assembling large libraries of chemicals and then testing them against a target protein. Compounds are added individually to wells that contain the target to see whether they affect its activity. This approach, known as high-throughput screening (HTS), is automated using robotic equipment to test millions of chemicals, but it's still laborious, expensive and not always successful. Over the past few years, medicinal chemists have been increasing the odds of finding potentially useful compounds by labelling chemical compounds with bits of barcode-like DNA. These DNA-encoded libraries \u2014 which can dwarf conventional small-molecule libraries \u2014 offer all sorts of advantages to drug discovery. For a start, rather than testing each compound individually, researchers can put all of the DNA-tagged small molecules into a single mixture and then introduce the target protein. Any compounds that bind with the target can be identified easily thanks to their DNA barcodes. DNA-encoded libraries were first proposed in 1992, in a thought experiment 1  by molecular biologist Sydney Brenner and chemist Richard Lerner, who were then at the Scripps Research Institute in La Jolla, California. They have been gaining momentum ever since. In 2007, GSK acquired one of the firms that pioneered these libraries, Praecis Pharmaceuticals in Waltham, for US$55 million. The drug firms Novartis and Roche, both in Basel, Switzerland, have started their own in-house DNA-encoded-library programmes. A burgeoning group of biotech companies \u2014 including X-Chem in Waltham; Vipergen in Copenhagen; Ensemble Therapeutics in Cambridge, Massachusetts; and Philochem in Zurich, Switzerland \u2014 has meanwhile built up a who's who list of industry and academic partners that are eager to use the technology. \u201cPeople understand now that this is not a fad,\u201d says Robert Goodnow, executive director of the Chemistry Innovation Center at AstraZeneca in Boston, Massachusetts, which collaborates with X-Chem. \u201cIt's for real.\u201d DNA-encoded libraries will not replace HTS: companies have already invested heavily in HTS screening, and there are some compounds that cannot be synthesized using DNA-encoding technologies. Rather, they offer a complementary way to quickly, efficiently and cheaply find chemical structures that bind to new or historically challenging targets, such as ubiquitin ligases, which flag proteins for disposal and could be targeted in cancer therapy. \n               Big is beautiful \n             GSK currently has the world's biggest DNA-encoded library: it is an impressive 500,000 times larger than the company's 2-million-compound HTS library. There are several ways to build DNA-encoded libraries: the biggest ones, like GSK's, are made using an approach called 'DNA recording' 2  (see 'Building barcodes'). Chemical building blocks, such as amino acids, amines and carboxylic acids, are synthesized and then tagged with a unique DNA barcode through a chemical reaction. A second building block is added to the mix to make a new small molecule, and the DNA barcode is then lengthened. By joining up to four blocks, chemists can create drug-like molecules. And because they have thousands of building blocks to play with, the number of potential combinations is enormous.\n Compared with conventional HTS libraries, for which chemists have to test each compound individually, DNA-encoded libraries are easier to maintain and use. A DNA-encoded library can be stored in a single test tube, whereas an HTS library requires robot-filled facilities that are large enough to store each compound individually. But the true beauty of DNA-encoded libraries, says Chris Arico-Muendel, a manager at GSK in Waltham, is the sheer number of chemical structures it is possible to synthesize. The company's drug-discovery team now uses the DNA-encoded library as frequently, if not more frequently, than the HTS library for new and difficult protein targets. The most advanced compound to emerge from the company's DNA-encoded library so far is GSK2256294, which blocks epoxide hydrolase 2 , an enzyme that is involved in breaking down lipids. This drug candidate came out of GSK's collaboration with Praecis and has completed first-in-human safety studies that may support further evaluation of its use in diabetes, wound healing or as a therapy for chronic obstructive pulmonary disease 3 . \u201cWe are pleased with how things are going with DNA-encoded libraries within GSK,\u201d says Arico-Muendel. And as more chemical building blocks are created, along with extra ways to attach them to one another, the breadth of these libraries will continue to expand. In the near future, DNA-encoded libraries will not only become bigger and broader but might also provide hits that can quickly be moved into the clinic, says X-Chem chief executive Richard Wagner. With conventional screening, medicinal chemists sometimes have to spend many years tweaking compounds to make them specific, potent and safe enough to enter the clinic. \u201cThis is just a game of odds,\u201d says Wagner. By contrast, the large size of DNA-encoded libraries means that, by chance, some of the compounds they include will be more clinic-ready than others. Although the compounds will still require optimization, \u201cwe can get things that are pretty close\u201d, he says. X-Chem, which holds 120 billion compounds in its DNA-encoded library, is already starting to see this in practice. It took just one year to move its most advanced candidate \u2014 an autotaxin inhibitor that blocks the conversion of one phospholipid into another \u2014 from a screening hit to a clinical candidate. A spin-off company of X-Chem, X-Rx in Wilmington, North Carolina, now plans to start clinical trials of the compound for fibrosis in 2017. Interest in X-Chem's library is spreading across the industry: in the past five years, the company has forged collaborations and licensing agreements with several major pharmaceutical firms \u2014 including Roche, AstraZeneca, Bayer, Johnson & Johnson, Pfizer and Sanofi \u2014 as well as with a host of biotech and academic partners. \n               Made to order \n             Other biotech firms have added an interesting twist. They use the DNA tag not only to identify a compound but also as a template to make it. David Liu, a chemist at Harvard University in Cambridge, and his students developed this 'DNA-templated' approach and used it to build a library of circular molecules called macrocyles 4 . These larger, more-stable, ring-shaped molecules interact with the target at multiple sites, boosting the specificity of the binding reaction. (GSK and X-Chem also have extensive macrocycle collections in their DNA-encoded libraries.) Liu first creates single-stranded DNA templates that act as guides \u2014 these consist of several regions that are complementary to the DNA tags on his chemical building blocks. He then sequentially adds the DNA-tagged building blocks into a reaction vessel, relying on DNA base pairing to physically bring the tagged building blocks close enough together to bind to one another. A final reaction then converts the strings of building blocks into rings, producing macrocycles that are each tethered to a unique DNA barcode. Constructing a DNA-templated library involves a hefty workload because researchers must design a template for each molecule as well as tagging thousands of building blocks with DNA. As a result, DNA-templated libraries are smaller than DNA-recorded ones, but they still eclipse HTS libraries in terms of size \u2014 and they have other advantages, too. Because scientists know at the outset what compounds they are producing, they can purify the DNA-templated libraries to remove compounds that are tagged inaccurately. This step translates into a high degree of confidence in the hits. By contrast, colossal DNA-recorded libraries may still contain wrongly tagged compounds, and thus might yield hits that will send researchers on wild goose chases. Liu's 14,000-strong library has already led to a few triumphs. In 2014, his team reported that it had solved a problem that researchers had been struggling with for decades when it found 5  a specific and stable small molecule that can block insulin-degrading enzyme (IDE), which has been linked with type 2 diabetes. He and others have started to unravel the role of IDE in both health and disease, which has led to the identification of other IDE inhibitors. Discussions are under way to develop these into drugs. Liu has also screened more than 100 other targets, many brought to him by academic collaborators who need small-molecule inhibitors of their pet proteins. \u201cI never would have thought seven years later that this first-generation library would still be providing us with interesting biological discoveries,\u201d he says. \u201cBut it has proved to be very fruitful. We have had more hits against targets from our first library than we can follow up on.\u201d He is nevertheless putting the finishing touches to a second-generation, 256,000-macrocycle library that could open up even more biology. Ensemble Therapeutics, which Liu founded in 2004, now has more than 10 million macrocycles in its library. The company is focusing on targets among the immune checkpoint proteins, which modulate the immune system, and the ubiquitin ligases. It has also granted a license to Novartis to develop one of its finds, a molecule that targets the inflammation-linked protein interleukin-17. \n               Sweet screens \n             Once the library is built, the fun of identifying which molecules stick to a target begins. Most researchers rely on 'affinity-based screening' to find those compounds. For this, they engineer the protein target to include a purification tag. They then pass the mixture containing the library and target through a purification column, using the purification tag to pull out the bound pairs. The last step is to read the DNA identifiers linked to the small molecules using a DNA sequencer. This method can yield results even with minute amounts of a target protein. In one project, remembers Arico-Muendel, academic collaborators wanted to screen an unstable protein that they could produce only in tiny quantities. \u201cThey flew it here on dry ice overnight, and we immediately did the entire screen on it,\u201d he says. \u201cAnd that actually gave some really good hits.\u201d Such experiments are impossible with HTS, because the target protein must be stable and abundant enough to be added into millions of wells before the experiment can begin. But affinity-based screening has its shortfalls. The clunky DNA tag can sometimes impede interaction with the target, and some potential candidates may be lost. But because the DNA-encoded libraries are so big, screeners are not typically too concerned with these losses. More problematically, small molecules and their tags can bind to the purification column and generate false-positive hits. The purification tag can also interfere with the structure of the target protein, introducing confusion in the data. Several groups have developed solutions for this problem. Vipergen, a biotech firm that has a DNA-templated library with 50 million molecules, has put its hope in a 'binder trap enrichment' strategy. Imagine, says Nils Hansen, the company's chief executive, that you could freeze your protein\u2013library mixture and cut it into super-small ice cubes. If the ice cubes are small enough, each will be able to contain only a single target protein. At this size, small molecules that bind to the target will be consistently overrepresented in ice cubes that contain targets, even without a purification strategy. Vipergen has achieved the same effect by putting its screens into water-and-oil emulsions, in which minuscule water droplets stand in for ice cubes 6 . \u201cIt's pretty cool,\u201d says Hansen. Currently, screens of DNA-encoded libraries work best with free-floating, soluble proteins. But many appealing drug targets are embedded in the cell surface, making them impossible to probe with traditional affinity-based screening. For example, an estimated 40% of approved drugs target membrane-bound G-protein-coupled receptors, which sense molecules outside the cell. The technology for screening membrane-bound proteins is evolving, says Goodnow, \u201cbut is still kind of a challenge\u201d. One way forward is to mix a DNA-encoded library with intact cells that overexpress a membrane-bound target. The small molecules can then bind to the targets on the surface of the cell. After the researchers wash away the unbound library, they can identify the bound small-molecule hits by heating up the cells and reading the eluted DNA tags. GSK has used this approach to identify potent inhibitors of a receptor that has been implicated in schizophrenia and disorders of the central nervous system 7 . X-Chem, too, is starting to see success with screens of membrane-bound proteins. \u201cHistorically, the majority of our programmes were on soluble proteins. But there is shift given the recent data we've been able to generate with really difficult membrane-bound proteins,\u201d says Wagner. With DNA-encoded libraries continuing to expand, and new screening approaches opening up uncharted biological space, he adds, \u201cDNA-encoded libraries are set to become one of the pillars of discovery in the pharmaceutical industry.\u201d\n Reprints and Permissions"},
{"file_id": "529423a", "url": "https://www.nature.com/articles/529423a", "year": 2016, "authors": [{"name": "Kelly Rae Chi"}], "parsed_as_year": "2006_or_before", "body": "Little is known about the function of most long non-coding RNAs. But a suite of new tools might change that. In 2013, a group of researchers decided to dig deeper into a human embryonic stem-cell line called H1 \u2014 and uncovered some surprises. H1 is one of the best known stem-cell lines, yet the team managed to unearth more than 2,000 previously uncharacterized stretches of RNA 1 . What is more, 146 of those were exclusive to human embryonic stem cells, offering tantalizing leads into pluripotency \u2014 the ability to become any cell type in the body. These transcripts had gone unnoticed because they contain repetitive stretches of code that sequence analysers had tended to filter out. It was a big blind spot. Other labs had uncovered early evidence of RNAs that are rich in repetitive codes and important in human stem cells. As the researchers, who were based mostly at California's Stanford University, examined their haul, they realized that they had hit on exactly these kinds of RNA. Among their list of 146 RNA sequences, says team member Vittorio Sebastiano, three of the most abundant \u2014 which they named HPAT2, HPAT3 and HPAT5 \u2014 seemed to be necessary for establishing the pluripotent cells that develop into a human fetus: those that comprise the 'inner cell mass' of an embryo that has yet to implant in the uterus 2 . These RNA stretches are examples of long non-coding RNAs (lncRNAs) \u2014 sequences at least 200 bases long that do not encode proteins. lncRNAs are present in many different kinds of tissue and are often found in specific spots inside a cell. But most lack a defined function and, until recently, were thought to be little more than transcriptional noise. That view began to shift as more data rolled in showing that the genomic regions from which lncRNAs are transcribed are more highly conserved through evolution than was imagined, implying that they had some function. But, to this day, a neat and sensible classification system for lncRNAs remains out of reach. They are still 'each their own snowflake', says John Rinn, who discovered lncRNAs as a graduate student about 15 years ago and now runs a lab specializing in the molecules at the Broad Institute of MIT and Harvard in Cambridge, Massachusetts. Now, revolutionary tools such as the genome-editing platform CRISPR\u2013Cas9 are making the task of discovering what individual lncRNAs do much easier. Some lncRNAs are thought to act as scaffolds for proteins to hang off while they manipulate the packaging of DNA. The functions of others are merely hinted at by their co-occurence with proteins (see 'Guilt by association') or by the effects of their absence \u2014 cancers start to spread to other parts of the body, and developmental disorders such as autism spectrum disorder arise.\n Today, it is broadly assumed that the molecules do have biological functions, says geneticist John Mattick, who heads the Garvan Institute of Medical Research in Sydney, Australia. \u201cThe evidence is on the table. That's been the big change \u2014 and it's palpable,\u201d he says. There are still doubters, but arguments now tend to caution against assuming function rather than dismiss its likelihood altogether. In the cautious camp is Michael McManus at the University of California, San Francisco. In 2013, his group mined published data sets of RNA sequences and found tens of thousands of new human lncRNAs, although many were present in cells only in very low amounts 3 . Most of these still need to be followed up, McManus says. And even with the latest tools, figuring out what the myriad lncRNAs do will be a slog requiring an army of experts. \n               Function first \n             Because the list of lncRNAs is so long, a key step is deciding which ones to prioritize for study. Rinn advocates starting with those from regions of the genome that have been already linked to disease. Another idea is to look at where the lncRNAs are located \u2014 finding one near a transcription start site might mean that it is involved in regulating the nearby gene, for example. These days, researchers can track the location of molecules inside cells with relative ease. Rinn, along with others at the Broad Institute and at the University of Pennsylvania in Philadelphia, has managed to discern the positions of 61 lncRNA molecules within skin, lung and cervical tumour cells using a technique known as single-molecule RNA-FISH (RNA fluorescence  in situ  hybridization) 4 . Scientists can also now test lncRNA function by using CRISPR\u2013Cas9 and other gene-editing techniques to interfere with part of the DNA sequence from which it is transcribed or with the promoter that directs its transcription. Some of these technologies allow labs to quickly screen huge numbers of lncRNAs. The logic is the same as when CRISPR\u2013Cas9 is used to look at the function of a protein-coding gene: introduce single-base deletions or substitutions into the DNA and watch the effects of the altered transcript. The only problem is that lncRNAs are less likely than proteins to be disabled by subtle alterations, says cancer biologist Howard Chang of Stanford School of Medicine, who develops and applies new methods for studying how lncRNAs work. The sequence alterations often need to be more drastic. This is where RNA researchers have made CRISPR\u2013Cas9 their own. They have expanded the CRISPR toolbox to include ways to block or prompt the transcription of a specific lncRNA. Rinn and his team have developed yet another approach: a tool known as CRISPR-Display (or 'CRISP-Disp'). Rinn compares it to a drone that can deliver an item \u2014 in this case, a specific lncRNA \u2014 anywhere in a cell. If a role in gene activation is suspected because a lncRNA normally lies alongside a certain part of the genome, then that role can be tested by moving the lncRNA to a different genomic location and watching for gene activation in the new spot. His group had been trying for years to make this happen using older genome-editing methods. Then, once CRISPR\u2013Cas9's crystal structure was published in 2014 (ref.  5 ), the team was able to tweak CRISPR's machinery to carry large packages, and had CRISP-Disp up and running within months. \u201cIt's very high-throughput: we can put 100 different lncRNAs at different sites and ask what they do at once,\u201d says Rinn. But figuring out function using this and other CRISPR techniques that block lncRNAs can be more complicated than it seems. For some lncRNAs, the DNA code overlaps with regions that are important for protein-coding genes, so destroying those makes the effects tricky to interpret, says Andrew Bassett, a genome-editing specialist at the University of Oxford, UK. And just because a functional change isn't seen doesn't mean that there is no function; the effect may be very subtle, or perhaps revealed only when the cell is faced with a particular threat. Two often-cited examples of this involve lncRNAs known as NEAT1 and MALAT1. They are abundant in cells and are highly conserved across mammals. Researchers know that they bind DNA to protein, but deleting the DNA stretches has no observable effects in mice. It is a familiar story to researchers in the field. \u201cThere are mysteries everywhere,\u201d says Mattick. \n               The RNA interactome \n             An entirely different approach is to find out what the lncRNAs are interacting with. \u201cIt's still believed, despite the importance of lncRNAs, that, ultimately, they can't carry out their functions without accessory factors,\u201d says molecular biologist Jeannie Lee of Massachusetts General Hospital in Boston and co-founder of the company RaNA Therapeutics in Cambridge, Massachusetts. These accessory factors are almost always proteins. Lee and others have set about unveiling lncRNA interactions using a lncRNA \u2014 called Xist\u2014 that is known to be necessary for silencing one of the two X chromosomes in the cells of female mammals to stop females from having twice as many X-chromosome gene products as males. The proteins that bind to Xist silence gene expression through multiple mechanisms. But in the past year, scientists have finally made inroads into pinning down the identities of these partners. Now, the proteins are known to not only pull in other molecules that silence transcription, but also to repel some that promote it. A host of techniques are available for probing a lncRNA's crowd of protein partners: broadly, researchers link RNA and protein together using agents such as formaldehyde or ultraviolet (UV) light, then use mass spectrometry to parse what is bound to what and come up with the 'interactome'. Often, these analyses have many steps, and therefore require the scientist to make many strategic choices. How should the RNA and protein be linked? How can real signals of interactions be distinguished from artefacts? And hanging over all such studies is the problem that RNA often behaves differently  in vitro  from how it does inside a cell. This is why those working on Xist interactions have focused on techniques for identifying RNAs bound to proteins inside living cells. Helping them are recent improvements in the sensitivity of mass spectrometry. In the past year, Chang's group has combined an assay that uses formaldehyde as a linking agent with the latest mass-spectrometry techniques to demonstrate that Xist binds to 81 proteins  in vivo 6 . Guttman's group used UV light instead \u2014 and revealed ten partners, including one not found by Chang's group 7 . Lee has been working on another UV light method called iDRiP (identification of direct RNA-interacting proteins) and has revealed an Xist interactome of about 100 proteins 8 . That is a complex that rivals the ribosome for size. Lee thinks that iDRiP could be used to look at other lncRNAs but that the protocol would probably need tweaking for each lncRNA. The chosen linking method, she adds, will depend on a number of factors, including where the lncRNA is in the cell and how abundant it is. Even for the well-studied Xist, the full interactome is far from settled. One of the central debates concerns whether, and how, Xist interacts with a structure called polycomb repressive complex 2 (PRC2), which influences gene expression by modifying proteins called histones. Some groups, including Guttman's, have found little evidence that Xist binds to PRC2; others insist that it does. Guttman thinks that the reason could be down to different experimental protocols: \u201cXist may bind more strongly  in vitro  than  in vivo . It's also a question of how one separates the strongest binding interactions from background interactions,\u201d he says. The PRC2 debate highlights the importance of following up interactome assays with tests of whether breaking a lncRNA\u2013protein interaction changes the RNA's function. Guttman has found that PRC2 deletion does not seem to affect Xist's ability to silence the X chromosome. By contrast, he says, perturbing the interaction between Xist and another protein implicated in gene silencing \u2014 called SHARP \u2014 does. \n               Secrets in structure \n             A third avenue for probing the function of lncRNAs is to study their structure. This doesn't predict function as directly as it often does for proteins, but knowing more about an RNA's arches and folds is likely to inform nonetheless. \u201cIt's a wide open field that needs a lot of work,\u201d says structural biologist Karissa Sanbonmatsu of the Los Alamos National Laboratory in New Mexico. \u201cThere are so many different lncRNAs that there's going to be a large zoo of different classes and motifs.\u201d Methods for establishing the secondary structure of a lncRNA include chemical probing strategies, such as one called SHAPE. It involves attaching acetyl groups to the RNA, modifying its backbone only at flexible regions. The modified sites block the enzyme that 'reads' RNA to create a complementary DNA sequence so that short DNA fragments are generated rather than long strands. The fragments can then be sequenced or sized on a gel. Sanbonmatsu's group was the first to describe, in 2012, the secondary structure of a human lncRNA: the steroid receptor RNA activator (SRA), which had been known for more than a decade to associate with oestrogen receptors 9 . By chemically probing the bound and unbound SRA, as well as its various domains, Sanbonmatsu revealed the lncRNA in its full glory, including all of its stems, loops and bulges. It looked a lot like the 16s ribosomal RNA, a highly conserved molecular machine. Sanbonmatsu's team has since found other strongly structured lncRNAs, but it is unclear whether most lncRNAs are like this or whether they are floppy, or somewhere in between, she says. The structural approach, too, has to cope with the problem that RNA behaves differently in test tubes and cells. And as with binding studies, the latest techniques are being done  in vivo . In 2012, a team that included Chang described a version of SHAPE that can work inside living cells 10  and has since improved it to characterize thousands of RNA structures simultaneously. Structural studies, like the others, require a large time investment \u2014 so careful choices must be made to focus on the lncRNAs that are most likely to have functions. Luckily, researchers are getting better at such triage, Sanbonmatsu says. She suggests to determine the likelihood of functional significance, scientists should start with lncRNAs that have known phenotypes, then chemically probe them to obtain secondary structures and check the extent to which they are conserved across other species. Sebastiano already has those boxes ticked for the three lncRNAs that seem to be key in establishing the pluripotent inner cell mass of human embryos: HPAT2, HPAT3 and HPAT5. Now he plans to delve further into the mechanistic details of these factors and has a raft of planned experiments on his list, including assays to ascertain their interactomes as well as structural analyses. \u201cThere's a ton of work to do, and this is just the beginning,\u201d he says. \u201cBut considering that these sequences may explain a lot of our unique features as humans and as primates, the effort is well worth it.\u201d\n \n                     Long noncoding RNAs: the search for function \n                   \n                     A cellular puzzle: The weird and wonderful architecture of RNA \n                   \n                     Genetics: The production line \n                   \n                     Molecular biology: A second layer of information in RNA \n                   \n                     Insights into RNA structure and function from genome-wide studies \n                   \n                     Nature Reviews Genetics  article series on non-coding RNA \n                   \n                     Nature RNA sequencing subject area page \n                   \n                     Science forum: Considerations when investigating lncRNA function  in vivo \n                   \n                     Symbiosis with Ancient Viruses Critical for Human Development \n                   \n                     Long Noncoding RNA Database v2.0: The Reference Database For Functional Long Noncoding RNAs \n                   Reprints and Permissions"},
{"file_id": "539315a", "url": "https://www.nature.com/articles/539315a", "year": 2016, "authors": [{"name": "Amber Dance"}], "parsed_as_year": "2006_or_before", "body": "Neurobiologists are coming up with innovative ways to get high-resolution pictures of the whole brain at work. Rosa Cossart thinks she knows what a memory looks like. In a study published in  Science  in September, Cossart, a neurobiologist at the Institute of Neurobiology of the Mediterranean in Marseilles, France, opened up mouse brains to visualize their neural activity as the animals raced on treadmills and rested. As the mice ran, some 50 neurons in their hippocampi fired in sequence, possibly to help the animals measure the distance travelled. Later, when the mice were resting, certain subsets of those neurons turned on again 1 . This reactivation, Cossart suspects, has to do with encoding and retrieving memory \u2014 as if the mouse is recalling its earlier exercise. \u201cThe power of imaging is really to be able to see the cells, to see not only the active ones but also the silent ones and to map them on the anatomical structure of the brain,\u201d she says. It has not yet provided proof for Cossart's hypothesis, but the microscope and neural-activity markers behind the techniques represent the very latest in methods to study brain connectivity. In the past, researchers studied just a few neurons at a time using electrodes implanted into the brain. But that gives a fairly crude picture of what is going on, like looking at a monitor with just a couple of functioning pixels, says Rafael Yuste, director of the NeuroTechnology Center at Columbia University in New York City. But new techniques are fleshing out the picture. Scientists can now watch neurons live and in colour, helping them to work out which cells work together. Methods such as Cossart's zoom in at the microscopic scale to catch individual neurons in the act; others provide a whole-brain, or mesoscopic, view. And although it is possible to perform these experiments with an off-the-shelf microscope, scientists have been customizing them to suit their specific purposes; these devices are in various stages of commercialization. The field of live-brain imaging is flourishing thanks to innovations such as two-photon microscopy, which allows scientists to image deeper into brain tissue, and indicators that flash as neurons fire; Cossart combined the two in her study. Major funding initiatives are also pushing the field forward, particularly the US Brain Research through Advancing Innovative Neurotechnologies (BRAIN) Initiative, which aims to improve researchers' ability to map the brain. The US National Institutes of Health has partnered with groups in Canada, Australia and Denmark to co-fund investigators from other countries involved in the BRAIN Initiative. In Japan, the Brain Mapping by Innovative Neurotechnologies for Disease Studies (Brain/MINDS) programme includes funding for projects such as functional magnetic resonance imaging (fMRI) analysis of the marmoset brain. Nevertheless, the scientists involved in these projects face major challenges. The biggest is the brain matter itself. \u201cBrain tissue has the optical properties of milk,\u201d Yuste says. The light waves that microscopists use to visualize neurons tend to bounce off surrounding tissue and scatter in multiple directions. That means that most studies cannot penetrate much more than a millimetre below the brain's surface. But researchers can now use both crude surgical techniques (removing part of the brain to discern what happens underneath, or poking in fibre optics, for example) and tricks of light to sneak their laser beams deeper into the tissue. Other challenges include the incredible speed at which mammalian neurons communicate as well as how to integrate data all the way from the meso- to the microscale. \u201cThe dream is obviously every neuron \u2014 every axon, dendrite, synapse \u2014 in the whole brain flashing away,\u201d says Columbia biomedical engineer Elizabeth Hillman. \u201cWe can do it in the fruit-fly brain, and in the zebrafish, just not yet in the mouse.\u201d But despite its limitations, live-mouse-brain imaging is already starting to reveal how neural connections can be silenced, or regrow, in studies of brain disease and ageing. \n               Of calcium and circulation \n             Take the work that has been done on stroke at the mesoscale, for instance. Blood clots in the brain damage neurons and thus the routes of neural communication. The damage can easily be seen in people: fMRI has shown that stroke affects the flow of blood between mirror-image parts of the two hemispheres, a cross-talk that is crucial for activities such as coordinated movement. But probing the details of stroke is difficult to do in people, so researchers including neurologist Jin-Moo Lee at Washington University School of Medicine in St. Louis, Missouri, are keen to use mice as a model to study the disease and possible treatments. However, mouse brains are so small that fMRI signals get lost in the noise, so Lee had to turn to a different technique to track blood flow. His colleague Joe Culver, a biomedical engineer, introduced him to a technique called optical intrinsic signal imaging (OIS), which picks up colour alterations that are linked to changes in blood oxygen levels. Oxygenated blood is reddish and deoxygenated blood bluish, and the different colours can be detected through the thin skull of a mouse using fairly basic scientific equipment, or even a wearable consumer camera known as a GoPro. Well-oxygenated areas are likely to be more active than others. To study neural connectivity, Culver and his colleagues zoomed out to look at the entire cortex, and presumed that highly oxygenated spots that are flashing in sync are likely to be connected. He calls the new method \u201cfunctional connectivity optical intrinsic signal imaging\u201d, or fcOIS 2 . In 2014, Culver and Lee used this technique to show that strokes in mice affect connections between mirror-image parts of the two hemispheres, just as they do in people 3 . Culver has also applied fcOIS to a mouse model of Alzheimer's disease and found that cross-hemisphere communication not only drops, but is also correlated with plaque deposition and with ageing. The loss of connectivity seems to happen first, foreshadowing which areas might be vulnerable to plaque accumulation 4 . The technique provides a good first-pass screen for changes in connectivity, Culver says, because it works on any mouse; some markers, including those used by Cossart, need to be genetically engineered into mouse neurons. Yet it's still only a surrogate marker of brain activity. A step closer is calcium indicators. When neurons receive a signal, an electrical current passes through them. That depolarizes the plasma membrane and opens ion channels, allowing calcium to flood into the cell. Indicators in the cytoplasm change shape and fluoresce when calcium flows in, providing more-immediate visual feedback. Among the most popular calcium indicators are the GCaMP proteins (see  'An imaging palette' ) developed at the Janelia Research Campus in Ashburn, Virginia, which are now in their sixth generation. \u201cWe're switching everything we can over to calcium,\u201d says Culver. \n               Flashing quasars \n             Calcium indicators have become the workhorses of live-brain microscopy. Scientists can see every neuron \u2014 at least in the plane imaged by their microscope \u2014 and follow their activity over time. For this kind of focus, scientists often use two-photon microscopy. In standard microscopy, a fluorophore is excited by just one packet of light, so any fluorophore that receives a packet will light up, even if it is outside the focal plane. In two-photon microscopy, scientists use a longer-wavelength laser, so the fluorophore must absorb two photons simultaneously to fluoresce. Because the chance of two photons hitting the same spot is high only at the laser's focal point, the signal is effectively limited to the focal plane. As an added bonus, the longer-wavelength, lower-energy light can penetrate deeper into the tissue. By scanning the laser across the brain, microscopists can build up a high-resolution picture of the brain at a depth of up to one millimetre, Yuste says. Yet calcium indicators are still only a proxy for the electrical spikes that mediate neural signalling. And they're relatively slow to reflect neural communication \u2014 \u201cthe smeared-out remnant of a spike\u201d, says David Kleinfeld, a neurophysicist at the University of California, San Diego. It takes about 100 milliseconds after the membrane depolarizes for the calcium to bind to the indicator and cause it to change shape and fluoresce, estimates Karel Svoboda, a neurobiologist and biophysicist at Janelia. It also takes half a second or so for the fluorescence signal to decay back to the unlit state, so two or three electrical impulses, or 'action potentials', could pass in the time that the calcium system can indicate only one. \u201cYou probably miss stuff,\u201d Svoboda says. The seventh generation of GCaMP indicators, anticipated within the year, should improve the response speed by an order of magnitude, as well as boost its sensitivity, Svoboda says. But no calcium indicator will ever measure action potentials with the same speed and range as electrodes, he says. Adam E. Cohen, a biophysicist at Harvard University in Cambridge, Massachusetts, is pursuing a faster type of visual indicator \u2014 one that fluoresces as the membrane depolarizes. The sensors \u2014 called genetically encoded voltage indicators (GEVIs) \u2014 are based on a protein that allows a Dead Sea microbe to harvest solar energy. Luckily for Cohen and his collaborators, that protein \u2014 called Archaerhodopsin \u2014 also fluoresced in response to changes in membrane voltage. And Cohen's team was able to enhance the mechanism to create a pair of GEVIs called QuasArs. But the QuasArs were fairly dim, so the group fused them to brighter fluorophores to create pairs that perform fluorescence resonance energy transfer (FRET), with one fluorophore influencing the emission of the other 5 . Cohen's team used these indicators, which are available in a handful of colours (see 'An imaging palette'), to monitor spontaneous and induced voltages in cultured rat neurons. Yet, like calcium indicators, GEVIs are neither fast enough nor bright enough to faithfully report neural firing, Cohen acknowledges. And the available colours are restrictive: ideally, he says, GEVIs should fluoresce in the far-red part of the light spectrum, leaving the rest of the rainbow open for the use of other proteins. In particular, neuroscientists often use light-activated proteins to control neuronal activity, using a technique called optogenetics. \n               Advancing microscopy \n             Other researchers are focusing on the microscopy itself, and specifically on imaging in 3D. Because the neurons that work together are not conveniently organized in a single plane, the scanning process must be able to keep pace with signalling across the volume of the brain. Ten frames or 'volumes' per second is a good benchmark, says Fritjof Helmchen, who co-directs the Brain Research Institute at the University of Zurich in Switzerland. \u201cThis is one of the clocks the brain is working on\u201d \u2014 millisecond resolution would be even better, he adds. That means that microscope designers must minimize the moving parts that slow things down, says Diego Restrepo, co-director of the Center for NeuroScience at the University of Colorado Anschutz Medical Campus in Aurora. He and his collaborators have eliminated the up-and-down motion required for focusing by using a liquid objective lens that is controlled by electric field. \u201cWhen you throw oil on water, you form a lens,\u201d explains Restrepo. By making the lens very small, he and his colleagues have managed to make it very stable, so that it doesn't bobble about as an animal moves. And they can change the lens' shape and focal plane by altering the electrical field. Restrepo's team has used this lens in combination with a confocal microscope and a fibre-optic system to image brain slices 6 , and now plan to attach the device to a mouse's head. At University College London, neuroscientist Angus Silver found a way to accelerate the focus changes while imaging across multiple focal planes. He uses an acousto-optic lens that transmits megahertz sound waves through tellurium dioxide crystals to focus the laser beam. \u201cThe limitation to speed is the speed of sound across crystal, basically,\u201d Silver says. The technique still isn't ideal for quickly imaging every neuron in a volume, he says, but it can move from one region to the next in about 25 microseconds 7 . That makes it useful for viewing all of a sparse population, such as inhibitory interneurons in a volume of brain, he suggests. Another solution to quickly sampling different depths is a modification of light-sheet microscopy, which typically involves moving multiple lenses to continually refocus a sheet of light. The technique can image one or two volumes per second, Hillman estimates. But by turning the sheet on an angle and using a single mirror to sweep it across the volume of interest, Hillman's group achieved a rate of 20 times per second. Hillman calls the technique swept confocally aligned planar excitation, or SCAPE, and her team has used it to visualize dozens of distinct firing patterns in the brains of awake mice 8 . The technology has been licensed to Leica Microsystems in Wetzlar, Germany. Yuste's group offers yet another option. It uses a spatial light modulator, which splits the laser beam into many beamlets, each of which is aimed at a different part of the tissue. \u201cImagine a comb of light that's hitting the sample,\u201d Yuste explains. The microscope picks up any light that comes back, so it can capture multiple planes at once 9 . It can collect about ten sets of images per second, and the researchers are already speeding that up, Yuste says. Yuste has licensed the technology to Bruker in Billerica, Massachusetts, and Olympus in Tokyo, and is contemplating starting his own company. \n               Zooming in, zooming out \n             Most 2D and 3D techniques remain hampered by how the brain scatters light, but scientists have ways of circumventing that limitation, too. At Cornell University in Ithaca, New York, applied physicist Chris Xu and his colleagues reasoned that if two photons could push the imaging depth to a millimetre or so, then three should go even deeper. Indeed, Xu's three-photon imaging can reach two or three times further down than two-photon imaging can, he says, although the limits depend on the properties of the tissue being imaged. His group managed to use the technique to image the mouse hippocampus, without removing any of the cortex above 10 . Xu's team still can't penetrate all the way through the brain \u2014 \u201cWe're literally still scratching the surface,\u201d he acknowledges \u2014 but there's plenty of room for improvement, he says. There's also room to develop live-brain imaging in other ways. A number of researchers, including Kleinfeld and Svoboda, have devised systems that combine the wide mesoscopic field of view with the single-cell resolution achieved by two-photon imaging, allowing them to zoom out on much of the brain or zoom in, Google Earth-style, on individual neurons 11 , 12 . Kleinfeld's field-of-view covers an 8\u00d710 millimetre section of cortex; Svoboda's group can manage a cylinder of brain about 5 millimetres in diameter and 1 millimetre deep, and that's about 25 times the typical field-of-view in two-photon microscopy, he says. Svoboda has now trained several labs to build their own versions of his microscope, and licensed the technology to Thorlabs in Newton, New Jersey. Ultimately, these diverse technologies could realize Yuste's dream for neuroscience: to \u201ccrack the code\u201d that links neural firing patterns with behaviour and sensation. The technology can't yet be used to look at and interpret the activity in a mouse's visual cortex, for instance, but it has certainly added plenty of pixels to the screen. \n                     Focus on the neuroscience toolbox \n                   \n                     Micromanagement with light \n                   \n                     Neuroscience: Connectomes make the map \n                   \n                     The New Century of the Brain \n                   \n                     Imaging: Show me where it hurts \n                   \n                     Method of the Year 2014 \n                   \n                     Guide to light-sheet microscopy for adventurous biologists \n                   \n                     Nature Neuroscience  Focus: Neuroscience toolbox \n                   \n                     Nature Reviews Neuroscience  commentaries on 'scaling up neuroscience' \n                   \n                     GCaMP info page at Janelia Research Campus \n                   \n                     BRAIN Initiative \n                   \n                     Adam Cohen lab resources page on GEVIs \n                   Reprints and Permissions"},
{"file_id": "531401a", "url": "https://www.nature.com/articles/531401a", "year": 2016, "authors": [{"name": "Michael Eisenstein"}], "parsed_as_year": "2006_or_before", "body": "Scientists are designing cells that can manufacture drugs, food and materials \u2014 and even act as diagnostic biosensors. But first they must agree on a set of engineering tools. From an evolutionary perspective, yeast has no business producing a pain killer. But by re-engineering the microbe's genome, Christina Smolke at Stanford University in California has made it do precisely that. Smolke and her team turned yeast into a biofactory that, by starting with sugar as a raw ingredient, makes the potent pain-relief drug hydrocodone 1 . This feat is a prime example of synthetic biology, in which scientists reprogram cells to replicate products found in nature \u2014 or even make more-specialized materials that would never normally be produced by a natural organism. Synthetic biologists are ambitious. \u201cWe'd all love to imagine a world where we could adapt biology to manufacture any product renewably, quickly and on demand,\u201d says Michael Jewett, a synthetic biologist at Northwestern University in Evanston, Illinois. Groups around the world are engineering yeast, bacteria and other cells to make plastics, biofuels, medicines and even textiles, with the goal of creating living factories that are cheaper, simpler and more sustainable than their industrial counterparts. For instance, the biomaterials company Spiber Inc. in Tsuruoka, Japan, has reprogrammed bacteria to churn out spider silk for use in strong, lightweight winter clothing. But synthetic biologists are going beyond simply producing materials \u2014 they are creating complex systems by 'wiring up' genetic parts into circuits. This approach has already resulted in various living switches and sophisticated sensors. For example, Martin Fussenegger's group at the Swiss Federal Institute of Technology (ETH) in Zurich has built biomedical sensors that can detect disease-relevant metabolites in the blood and trigger the production of therapeutic compounds. In mice, these biosensors successfully staved off gout and obesity, and treated the skin disease psoriasis 2  (see  'Living pills' This young field has already spawned some success stories, but making and putting together genetic parts currently involves substantial guesswork and unpredictability. For the field to advance, academics and industrial players must agree on a toolbox of reliable genetic parts and the best strategies for assembling them. To build an artificial product, synthetic biologists begin by selecting DNA parts on a computer and manufacturing them with specialized instruments. The parts can then be inserted into the DNA of microorganisms and cells to reprogram them. Thanks to the plummeting cost of DNA sequencing, there is now a vast collection of genetic data through which synthetic biologists can sift to find useful genes. \u201cBiology has given us this big, crazy library of stuff to choose from,\u201d says Christopher Voigt, a synthetic biologist at the Massachusetts Institute of Technology (MIT) in Cambridge. One leading database, the US National Center for Biotechnology Information's GenBank, contains more than 190 million DNA sequences from 100,000 organisms. Some of the most widely used genetic parts encode enzymes \u2014 proteins that are essential for manufacturing. To transform glucose into hydrocodone, for example, Smolke's team took 23 enzyme-encoding genes from diverse species and put them into yeast 1 . Other favourites in the genetic designer's palette are promoters \u2014 stretches of DNA that regulate the activity of nearby genes and cause them to be expressed. When proteins called transcription factors bind to a promoter, the process of transcribing a gene begins. But promoters operate too slowly for some synthetic-biology applications. \u201cWe're trying to build things that operate fast \u2014 on millisecond time-scales,\u201d says biologist Pamela Silver of Harvard Medical School in Boston, Massachusetts. Scientists are therefore examining alternative mechanisms that allow gene expression to be controlled directly by signals in the environment, such as toxins or antibiotics. With myriad synthetic DNA pieces at their disposal, synthetic biologists can indulge their creativity. Voigt is enthusiastic about the possibilities: \u201cThe nice thing about biology is that there are lots of ways to do the same thing \u2014 and as an engineer, you can pick the way that is easiest to design.\u201d But genetic parts must perform consistently if the goal of setting up industrial processes is to be realized. \u201cOne of the key problems for biology in general is the lack of reproducibility,\u201d says Richard Kitney, chairman of the Institute of Systems and Synthetic Biology at Imperial College London. \u201cIn synthetic biology this is totally unacceptable \u2014 you have to have reproducibility if you're going to do industrial translation.\u201d Many researchers deposit their discoveries into shared repositories, such as the Registry of Standard Biological Parts and the Inventory of Composable Elements. But those parts are often poorly defined or lack crucial information about how they were experimentally tested. \u201cThe only quality control is in the person who deposited the information,\u201d says Voigt. The US National Institute of Standards and Technology (NIST) launched the Synthetic Biology Standards Consortium in March 2015 with the aim of standardizing the design, documentation and assembly of synthetic-biology parts across academic institutions and industry. In the United Kingdom, Kitney is coordinating a similar effort in which the DICOM (Digital Imaging and Communications in Medicine) standard for sharing medical information will be expanded to include synthetic biology 3 . In parallel, an international team has developed SBOL (Synthetic Biology Open Language) 4  to provide researchers with a standardized vocabulary to describe genetic parts and circuits. \n               Cellular software \n             Thanks to greater automation, it is now simpler and cheaper than ever before to make synthetic DNA parts (see  'Manufacturing DNA has never been easier' ). But connecting those parts to form genetic circuits that can work together to provide sophisticated, computing-like behaviours is still a challenge. \u201cAny time you physically connect DNA you're creating a new sequence at that interface \u2014 as DNA is so information-rich, you could create a new promoter or change the beginning of the RNA,\u201d says Voigt. Even carefully designed circuits can malfunction and cause unwanted expression of a gene or interference between the genetic elements in the biological circuit \u2014 outcomes that cannot be foreseen in computer models. \u201cThe community is very much operating in a world where we cannot predict what is going to happen in our systems when we build them,\u201d says Reshma Shetty, co-founder of the synthetic-biology company Ginkgo Bioworks in Boston, Massachusetts. This uncertainty means that many of the steps in engineering a synthetic system need to be tested and optimized. Software tools and robotics are speeding up each part of this process, from building the artificial DNA to inserting it into a microbe. \u201cYou can use high-throughput prototyping to just build every variant, and hopefully one of them will hit,\u201d says Jay Keasling, a biochemical engineer at the University of California, Berkeley, and a pioneer in the field. The push for automation has led a number of synthetic-biology research centres and firms to install 'biofoundry' facilities in which robotic assembly lines create, test and optimize microbes at a much larger scale than could be done by hand. Biofoundries are enabling synthetic biologists to embark on ambitious projects. For example, Voigt, who co-directs the MIT-Broad Foundry, cites a collaboration with the Swiss pharmaceutical company Novartis to manufacture a huge range of molecules that are produced by bacteria in the human gut. Other institutions pursuing the biofoundry model include the SynbiCITE programme at Imperial College London and the National University of Singapore's Synthetic Biology Foundry. The US Defense Advanced Research Projects Agency (DARPA) has also invested heavily in the MIT-Broad facility, including a five-year, US$32-million contract that began in October 2015. Some biologists remain sceptical about the rush to scale up and automate, and favour a more theory-driven strategy. But Kitney, who co-directs SynbiCITE, considers automation to be an inevitable step in the evolution of synthetic biology. \u201cYou can rapidly run a whole series of experiments in parallel to see which configuration works best,\u201d he says. \n               The perfect host \n             Species that are commonly used as model organisms in the lab, such as brewer's yeast ( Saccharomyces cerevisiae ) and the bacterium  Escherichia coli , have also been pressed into service by synthetic biologists. Many breakthroughs in biosynthesis have been achieved with these organisms, such as when Keasling and his collaborators at Amyris, a company that he co-founded in Emeryville, California, in 2003, reprogrammed  S. cerevisiae  to manufacture the antimalarial compound artemisinin 5 . But these common lab organisms are not necessarily suited to being grown on an industrial scale. The hunt for better alternatives has led scientists to search in obscure places. \u201cMore and more labs are taking on arcane organisms \u2014 I think the  S. cerevisiae  and  E. coli  dominance is dropping,\u201d says Voigt. In some cases, the choice organisms will be those that can withstand harsh manufacturing conditions, says Keasling. \u201cMaybe you're producing something that's toxic but volatile, so if you have an organism that can produce it at relatively high temperatures, you could boil it off while you're producing it.\u201d Scientists are also testing whether it is possible to feed microbes with carbon sources other than sugars to make products. Synthetic-biology company Intrexon in Germantown, Maryland, is working with bacteria that feed on methane, a cheaper and more efficient means for producing carbon-based products than is sugar. \n               Medical cells \n             When it comes to medical applications, synthetic biologists are engineering mammalian cells rather than microbes. Such designer cells could produce drugs in response to disease or take over certain physiological tasks in people with metabolic disorders such as diabetes. But engineering mammalian cells introduces a new set of challenges. \u201cAll the tools we have in yeast are just not there in mammalian cells,\u201d says Smolke. \u201cWe don't have as many promoters, or tools for regulation of gene expression or protein modification.\u201d The easiest cells to cultivate are tumour-like, immortalized cell lines, which are inherently 'defective' and therefore not representative of healthy tissues. Conversely, tissue-derived primary cells are hard to cultivate and manipulate, and differences between cell types confound efforts to build toolkits that can be applied across the body. \u201cSomething that works in a kidney cell will not necessarily work in the lung or liver,\u201d says Fussenegger. To get around this, the ETH team is engineering 'prosthetic gene circuits', which are introduced into host cells that can be implanted at the site of disease. Tinkering with genomes can also present problems. Even 'smart' genome-editing tools \u2014 such as CRISPR\u2013Cas9, a system for introducing targeted modifications at specific DNA sites \u2014 can have unpredictable outcomes. \u201cWe don't know enough loci in human cells where you can put things in without any interference,\u201d says Fussenegger. His team is exploring whether it is possible to avoid this uncertainty by introducing gene networks that are embedded in synthetic loops of DNA known as plasmids rather than integrated directly into chromosomes. As an extra precaution, his experiments with mice generally make use of engineered cells trapped in implanted capsules, rather than modifying the animal's tissues. Others want to do away with the cell altogether. Jewett is studying cell-free systems in which bacterial extracts are purified to obtain only the 'useful' parts of the cellular machinery. \u201cYou get all the enzymes necessary for energy and cofactor regeneration as well as protein synthesis,\u201d says Jewett. \u201cThis gives you unprecedented freedom to directly manipulate reaction conditions.\u201d This allows researchers to establish chemical conditions that maximize manufacturing productivity without worrying about keeping cells healthy. Jewett's team has shown that this approach can efficiently churn out medically useful proteins such as erythropoietin 6 , a hormone that stimulates red-blood-cell production. \u201cIt's not yet a replacement for existing technologies, but the yields are sufficient to serve as a complement,\u201d he says. The field is still in its infancy \u2014 indeed, the earliest demonstrations of engineered genetic circuits appeared only in early 2000 \u2014 and it can be dauntingly complex. Even so, a growing number of scientists grounded in conventional molecular biology are keen to give genetic design a try. Synthetic biologist Ron Weiss at MIT is teaching an online course on the field that proves its popularity. \u201cWe've had about 14,000 people sign up,\u201d he says. The pay-off for those entering the field could be huge. \u201cI'm in this space because the frontiers are endless for what biology can do,\u201d says Shetty. \u201cIt's just a matter of the technology advancing to a point where those new horizons open up.\u201d\n \n                     Synthetic biology lures Silicon Valley investors \n                   \n                     How to turn living cells into computers \n                   \n                     Synthetic biologists seek standards for nascent field \n                   \n                     Cellular 'computers' gain a hard drive \n                   \n                     Synthetic biology: How best to build a cell \n                   \n                     First synthetic yeast chromosome revealed \n                   \n                     Nature Methods  collection on super-resolution microscopy \n                   Reprints and Permissions"},
{"file_id": "532269a", "url": "https://www.nature.com/articles/532269a", "year": 2016, "authors": [{"name": "Kelly Rae Chi"}], "parsed_as_year": "2006_or_before", "body": "Liquid biopsies can detect cancer signs from a blood sample, without the need for invasive procedures. But further work is needed before they can become reliable diagnostic tools. A lung biopsy is an invasive and uncomfortable procedure \u2014 especially for an 80-year-old grandmother. But by profiling his elderly patient's tumours in this way, lung oncologist Geoffrey Oxnard could target them with a matched drug. After treatment, his patient's tumours seemed to disappear. Adapted from C. B. Meador & C. M. Lovly Nature Med.  21 , 663\u2013665 (2015)/M. R. Speicher & K. Pantel Nature Biotechnol.  32 , 441\u2013443 (2014) But Oxnard, who is at the Dana-Farber Cancer Institute in Boston, Massachusetts, offered the woman an alternative: \u201cLet's just check your blood.\u201d He performed what's known as a liquid biopsy, using nothing more than a blood sample. Within a day, he spotted minuscule amounts of tumour DNA that revealed a mutation that causes resistance to treatment. Luckily, a drug that targets the mutation was being tested in clinical trials. With the genetic profile in hand, Oxnard managed to enrol his patient into the study, and her tumours went into remission again. The discovery that parts of tumour cells, or even whole cells, break away from the original tumour and enter the bloodstream led to the idea of liquid biopsies. With this approach, cancers can be genetically characterized by analysing tumour DNA taken from a blood sample, thus bypassing the need to extract solid tumour tissue. Now, the rise of rapid genome-sequencing techniques has made it practical to translate this concept to the clinic. Three main approaches are being pursued: analysing circulating tumour DNA 1 , examining whole tumour cells in the bloodstream 2  and capturing small vesicles called exosomes that are ejected by tumours 3  (see 'Scalpel-free biopsies'). And scientists have found that blood platelets might be able to offer up cancer clues, too (see 'Platelets ingest tumour data'). The allure of liquid biopsies is that they are quick, convenient and minimally painful, and they allow clinicians to closely monitor how tumours respond to therapies and to forecast cancer recurrences. In the long term, clinicians might even be able to use liquid biopsies to catch tumours at the earliest stages, before a person shows any symptoms. The genomic information in DNA circulating in the bloodstream could provide a snapshot of cancer genes in the body and may even point to where the cancer originated. Investors are excited, and funds are pouring into start-ups focused on liquid biopsies. Sequencing firm Illumina of San Diego, California, for example, launched a spin-off company in January called Grail that will develop a plasma-based genetic screen for the early detection of multiple cancers. But extensive testing is required before liquid biopsies can supersede surgical biopsies in the clinic. And there are still concerns from regulators about the sensitivity and accuracy of these procedures \u2014 for example, Oxnard's patient was required to have another, conventional biopsy to confirm the results of the liquid biopsy before she was allowed to enrol in the clinical trial. boxed-text Nevertheless, researchers say that it is no longer a question of whether liquid biopsies will one day replace surgical biopsies, but when and in what form. First, however, costs need to go down and sensitivity needs to rise. \n               Free-floating DNA \n             Bits of DNA are constantly flooding into the bloodstream. This genetic flotsam is present even in healthy people and could come from anywhere in the body. Dennis Lo, a chemical pathologist at the Chinese University of Hong Kong, realized that if the placenta of pregnant mothers released fetal DNA, then tumours may also shed DNA. Lo pioneered non-invasive prenatal screening for identifying chromosomal abnormalities in unborn babies, a test now in widespread use. But these screens have also yielded unexpected information \u2014 a few mothers-to-be found out that they had cancer. The advent of more-accurate next-generation sequencing has enabled liquid biopsies on free-floating tumour DNA, because it can distinguish such sequences from normal DNA. In addition, digital polymerase chain reaction (dPCR) allows researchers to detect or quantify specific stretches of tumour DNA even at levels as low as 0.1% of total DNA in the blood. \u201cThese are very sensitive tests,\u201d says dermatologist David Polsky of the New York University Langone Medical Center, who has shown that droplet-based dPCR can be used to monitor metastatic melanoma after treatment by tracking circulating DNA 4 . \u201cIt's because they're so impressive in the lab that clinicians are excited about them,\u201d he says. Digital PCR, especially the droplet-based versions, has become so easy and cheap that many liquid-biopsy assays have the potential to become widely adopted. But the drawback of dPCR is that clinicians must know what aberrant DNA sequence they are looking for. As a result, sequencing could be preferable for some clinical indications because it allows clinicians to search for mutations without any pre-conceived notion of what genetic changes might be driving the cancer. At the moment, liquid biopsies are mostly confined to basic- and clinical-research settings, although some blood tests are creeping onto the commercial market. Pathway Genomics, a medical-diagnostic company in San Diego, California, came under fire from the US Food and Drug Administration (FDA) in September 2015 for marketing its blood-based test directly to consumers. The regulator said that the CancerIntercept Detect screening test, which costs US$699 and is aimed at people who are at high-risk of developing cancer but are otherwise healthy, was not approved for direct marketing to consumers and had not been adequately clinically validated. The company countered that it had physician involvement and did not follow a direct-to-consumer model. Lo is now finishing a 20,000-person screen for nasopharyngeal cancer, a rare type of head and neck cancer that is nevertheless common among men in southern China. In a 2013 study, Lo's team screened 1,300 healthy individuals and found 3 with early-stage cancer (stage 1). Lo will soon report the full results of his study. \u201cIt's a big deal that we are able to get them at stage 1,\u201d Lo says, because 95% who are treated at that stage survive. Those three people were promptly treated and are still healthy, he adds. It helps that nasopharyngeal cancer is particularly easy to spot. The cancer is caused by Epstein\u2013Barr virus, which leaves behind a specific genomic footprint that a simple, dPCR-based assay costing roughly $25 per test can pick up. But most other cancers have more undefined patterns of mutation. And although scientists can now access a growing catalogue of tumour signatures thanks to large-scale cancer-genome sequencing projects, there are many more to find, says Lo. To track the source of the DNA fragments, scientists are starting to take size into account. Of the DNA bits floating in the bloodstream, those derived from normal cells are roughly 100\u2013200 base pairs long, and are still wound around proteins called histones. Histones package DNA into the nuclei of different cells in different ways, so the length of the DNA may indicate the organ from which it was derived. Tumour DNA (like fetal DNA) is shorter than normal blood DNA fragments across multiple types of cancer, and large concentrations of very short fragments have been linked to metastasis (the migration of cancer through the body). The presence of tissue-specific transcription factors and other markers can also reveal clues about the tumour's provenance 5 . With all of this genetic information, clinicians might be able to make an informed guess as to where to look for a tumour, Lo says. \n               Spotting relapse \n             Tracking a patient's circulating DNA also opens up the game-changing possibility of detecting metastasis at the very early stages, something that would otherwise require repeated invasive procedures. In the past few years, scientists have been studying circulating DNA for signs of cancer recurrence in women who were treated and presumed cancer-free. This is an important step towards early detection of metastatic relapse, says Muneesh Tewari, an oncologist and researcher at the University of Michigan Health System in Ann Arbor. In a 2015 study 6 , a blood test in 20 people with breast cancer revealed signs of metastasis 3 years before it could be diagnosed with standard clinical tools. The biomarker was chromosomal rearrangements in circulating DNA. These rearrangements seem to occur early in cancer development, and scientists can use them to design primers for running dPCR tests. \u201cWe thought that [circulating tumour] DNA should be a good biomarker for recurrence of breast cancer, but it had not really been shown before. When we saw that, it was very exciting,\u201d says Lao Saal of Lund University in Sweden, who led the study. \u201cTo think that you could pick it up three years beforehand, one could speculate that perhaps that could make a clinical difference.\u201d Another study 7  of 55 people with breast cancer identified relapse about 8 months before symptoms appeared, also using dPCR to detect mutations. \u201cTwenty per cent of women with breast cancer will go on to die of their cancer,\u201d says Nicholas Turner of the Institute of Cancer Research in London, whose team published the results. \u201cWe need to get an awful lot better at identifying the 20% who aren't cured by their current treatment.\u201d Both Saal and Turner compare DNA from patients' primary tumours with circulating DNA collected after treatment. Saal has formed a company, SAGA Diagnostics in Lund, Sweden, that expects to partner with cancer specialists to validate the assay in the clinic. Saal is also working to improve the sensitivity of dPCR for identifying other point mutations to diagnose and monitor disease. Turner's group is starting a clinical trial to assess whether its assay can predict a person's response to a new class of immunotherapeutic drugs, the 'checkpoint inhibitors', which kick start the body's immune system. Although liquid biopsies are beginning to gain a foothold in some clinics, it's still unclear what impact the search for pre-defined genetic alterations will have on patients' lives, Tewari says. Tumour resistance can emerge from mutations that are not covered by today's tests, so next-generation sequencing approaches will probably be needed to monitor all possible cancer mutations, Turner says. \n               Breakaway cells \n             Whereas free-floating tumour DNA was discovered fairly recently, scientists first reported finding tumour cells in the blood in 1869. More than 40 different devices for isolating tumour cells are described in the literature (although only one, CellSearch by Veridex in Raritan, New Jersey, is approved by the FDA). For a long time, scientists pursued the notion that the number of tumour cells in blood might be used to assess the aggressiveness of a patient's cancer. However, results at the bedside have been disappointing. In 2014, for example, the National Cancer Institute's Southwest Oncology Group published a study 8  showing that it was possible, by capturing and counting tumour cells in the blood of people with metastatic breast cancer, to identify a subset of those individuals with a more aggressive cancer. But although these women received another round of chemotherapy, this did not improve their outcomes. From this, people concluded that counting tumour cells had no practical value as a predictor, says Stefanie Jeffrey, a clinical oncologist at Stanford University in California. But, she adds, this conclusion was too swift: rather, it was the extra treatment that did not work. Nevertheless, the American Society of Clinical Oncology does not recommend counting cells to help guide treatment. Rather than just counting tumour cells, a better option is to sequence the DNA or RNA from them to provide more insight into the gene alterations driving the patient's cancer. And because isolation techniques are improving, greater numbers of viable cells can be harvested from a patient \u2014 enough to allow researchers to culture the cells or implant them into mice to study their functional attributes. Any insights they gain can then be used to guide the patient's treatment. Klaus Pantel, director of the Institute of Tumor Biology at the University Medical Center Hamburg-Eppendorf in Germany, foresees the potential of this technique for predicting a tumour's response to targeted treatment \u2014 for example, researchers might spot PD-L1 on the cell surface, a protein that helps tumours to avoid the immune system and that can be targeted with immunotherapeutic drugs. \n               Exosome exam \n             A third liquid-biopsy approach targets exosomes. These are tiny vesicles that are shed by all living cells, including tumours, and just like their parent cells, they contain DNA, RNA and proteins. Much about them remains unknown, but a cancer test based on exosomes has already been commercialized. In January, a blood test developed by Exosome Diagnostics of Cambridge, Massachusetts, for detecting lung cancer by analysing tumour-exosome RNA was certified for laboratory use under the Clinical Laboratory Improvement Amendments (CLIA) quality programme in the United States. Exosomes can be harvested from a patient's blood, and potentially from other bodily fluids such as urine, which are even more convenient and easy to access than blood. However, the isolation of tumour-derived exosomes, which are variable in size, and their separation from normal exosomes, remains particularly challenging when compared with isolating free-floating DNA or tumour cells, so it will take some time before the technique is mature enough to detect other forms of cancer. Each type of measurement gives a different window into the biology and course of disease, Tewari says. Much refinement of the techniques remains to be done, but clinicians are still excited about what liquid biopsies can do today. \u201cThey have a powerful role in helping patients get to the right treatment,\u201d says Oxnard. Adapted from C. B. Meador & C. M. Lovly Nature Med.  21 , 663\u2013665 (2015)/M. R. Speicher & K. Pantel Nature Biotechnol.  32 , 441\u2013443 (2014) \n                     Cancer blood test venture faces technical hurdles \n                   \n                     Cancer biomarkers: Written in blood \n                   \n                     Tumor signatures in the blood \n                   \n                     Liquid biopsies reveal the dynamic nature of resistance mechanisms in solid tumors \n                   \n                     Beyond counting tumor cells \n                   Reprints and Permissions"},
{"file_id": "536361a", "url": "https://www.nature.com/articles/536361a", "year": 2016, "authors": [{"name": "Stephen Ornes"}], "parsed_as_year": "2006_or_before", "body": "Structural biologists are at last living the dream of visualizing macromolecules to uncover their function. But it means integrating different technologies, and that's no easy feat.  Like other structural biologists, Eva Nogales works in extraordinary times. The University of California, Berkeley, faculty member now has the tools to tackle important questions about cells' molecular machinery that would have been impossible to answer just a few years ago. A recent project with Berkeley colleague Jennifer Doudna, the molecular biologist who co-pioneered the CRISPR\u2013Cas9 gene-editing method, is a case in point. Both were intensely interested in the R-loop, a structure made of nucleic acids that forms in cells in many situations, but also just before DNA is snipped by CRISPR\u2013Cas9. Nogales and her team revealed an R-loop in  Streptococcus pyogenes  bacteria, and from the near-atomic-resolution images, deduced how the Cas9 enzyme opens up the DNA conformation at specific sites and makes them accessible to CRISPR's molecular scissors 1 . The work is remarkable for the speed with which the scientists assigned a function to the structure, but also because they arrived at the solution by combining imaging methods \u2014 an increasingly popular approach in structural biology. For more than a century, the field's premiere method has been X-ray crystallography. But some biomolecules are simply too big or small to crystallize, and the technique doesn't work on others. And some biomolecules change shape or orientation as they work, which isn't captured by static crystallization. Now, scientists have a dazzling suite of different imaging techniques with which to build on crystallographic findings. Some of the approaches, such as cryogenic electron microscopy (cryo-EM) or chemists' stalwart nuclear magnetic resonance (NMR) imaging, reveal molecular shapes, size and orientation at near-atom-level resolution without the need to make crystals. But not every method works for every protein, nucleic acid or other biomolecule inside a living cell. Growing wisdom in the field suggests that no single method is likely to be sufficient to probe the dynamic behaviour or intricate interactions taking place in a cell. The most powerful insights will come from hybrid methodologies that integrate the images from several different tools. The approach is rapidly gaining followers. \u201cEach [method] brings something important to the table, and the combination is very much larger than the sum of the parts,\u201d structural biologist Roger Kornberg of Stanford University in California. Kornberg won the 2006 Nobel Prize in Chemistry for his work detailing the machinery of gene transcription. For that ground-breaking research, he generated crystallographic pictures. Now, like other crystallographers, he has moved on to hybrid methodologies. Kornberg continues to analyse RNA polymerase II, but now he combines crystallography with cryo-EM, in which an electron beam probes the structure of biomolecules. Cryo-EM can be used on molecules that don't crystallize easily and can reveal larger structures than can X-ray crystallography, but \u2014 for the moment at least \u2014 it lacks crystallography's high resolution. Kornberg's lab also uses chemical crosslinking and mass spectroscopy to reveal relationships between nearby proteins, and homology modelling to construct representations using information from known proteins 2 . Nogales and Doudna's team also took the hybrid route to study R-loops. \u201cThe full R-loop could not be seen by the high-resolution X-ray crystallographic structure,\u201d says Nogales. So they also used cryo-EM to reveal the full R-loop structure at lower resolution. Only by combining the two methods could the researchers work out how R-loops fit into the larger CRISPR\u2013Cas9 picture 1 . Such hybrid, or integrative, approaches help researchers to probe deep basic-science questions, but also reveal details that are useful to drug developers. Large proteins found in cell membranes are often targets for therapeutic drugs, and high-resolution hybrid methods have the potential to show in atomic detail how a drug interacts with a receptor. Similarly, hybrid methods might be able to aid vaccine development by showing how proteins on the viral envelope of HIV, Ebola and other pathogens interact with immune cells to induce protective responses. \u201cThese structures are super-important to understand how our immune system works,\u201d says structural biologist Jens Meiler at Vanderbilt University in Nashville, Tennessee. Nogales sums it up: \u201cThis is a golden time to do hybrid methodologies.\u201d \n               Living the dream \n             The current era in structural biology promises to fulfil the \u201cdream of many life scientists\u201d, says Jan Ellenberg, head of the Cell Biology and Biophysics Unit at the European Molecular Biology Laboratory (EMBL) in Heidelberg, Germany. That dream is to seamlessly scale up from what scientists see at the atomic level to the cellular level. Such deep understanding of the cell's macromolecules naturally leads to answers to the overarching question in structural biology \u2014 how is a molecule's structure connected to its function? Each technique in a structural biologist's toolbox offers a different perspective. Models that use hybrid methods can boost biologists' confidence that a model accurately reflects how the molecule or ensemble acts in the cell. \u201cYou need all of them in combination to really get a full understanding of your biological question of interest,\u201d says Meiler. X-ray crystallography has long reigned as the standard way to determine the atomic structure of proteins. Of the 120,000 or so models in the Protein Data Bank (PDB), established in 1971, about 90% were derived from crystallographic studies. But structural biologists' workhorse, even with its high resolution, has limitations. Crystallography requires highly purified samples that produce a well-ordered crystal. Scientists fire X-rays at a crystal to determine its structure by analysing how the atoms scatter light. The technique needs a specimen with enough atoms to produce a measurable diffraction pattern, and every crystal must be static. As a result, the method can't reveal how a molecule moves or functions in a cell, or its connections to other systems. A protein \u201cis not just a single static structure\u201d, says Gunnar Schr\u00f6der, who leads the computational structural biology group at the Institute of Complex Systems in J\u00fclich, Germany. \u201cOftentimes, what you want is to see how the whole protein works.\u201d Schr\u00f6der uses hybrid methods to understand the movements and connections of proteins. Crystallography provides a snapshot of a protein in one configuration, removed from its normal environment. He says that structural biologists need other methods to boost the structural information from crystallography and improve their understanding of the form and function of proteins. Many proteins, such as drug targets on a cell membrane, are flexible and often unstable. To get these proteins to form crystals, researchers often have to change them in some way. Meiler says the altered specimen may not accurately reflect the native state of the molecule or how it is arranged in the cell. He mixes experimental and computational approaches to better understand molecular structure. \u201cIt takes time for people to understand that for many biological systems, the model from crystallography is a good starting point,\u201d he says, but it may not be suitable for providing information about function. Biologists are now leveraging a range of tools to build richer, more accurate models of biological structures. Hybrid approaches have the power to do more than a single technique ever could. One particularly useful partnership joins cryo-EM and X-ray crystrallography. This microscopy method has been around since the 1980s, but in recent years it has achieved a resolution of 2.2 \u00e5ngstr\u00f6ms, edging close to the 2 \u00c5 average resolution of X-ray crystallography 3 . It can produce models in two or three dimensions of proteins and other macromolecules that have stubbornly resisted other approaches. \u201cOne of the really exciting things about cryo-EM is that you can start a biochemical process and freeze those samples at multiple states,\u201d says Jeffrey Lengyel, principal scientist for life sciences at FEI, a company in Hillsboro, Oregon, that designs and manufactures cryo-electron microscopes. \u201cYou can determine the structure of multiple conformations.\u201d Researchers can also combine cryo-EM images to see molecules in motion. John Rubinstein of the Hospital for Sick Children in Toronto, Canada, led work published in May 2015 that used image analysis to combine 100,000 cryo-EM images into a film, showing changes in the structure of eukarotic V-ATPase, an enzyme that pumps proteins across membranes and, changes over time 4 . In papers published earlier this year, Nogales and her collaborators used cryo-EM with homology models to describe the structure of TFIID, a large, horseshoe-shaped protein complex that is required to initiate gene transcription 5 . \n               With a little help \n             The hybrid strategy of Nogales and her team led to an overall resolution of better than 10 \u00c5 \u2014 a significant improvement over their previous analysis of the same protein at 30 \u00c5. That resolution has led to new insights: \u201cWe can see what amino acids are interacting with DNA,\u201d Nogales says. But cryo-EM requires specimens to be snap-frozen. That's not ideal for biological samples, as the conditions are far removed from a macromolecule's dynamic, natural state. NMR spectroscopy can help on that front. \u201cNMR has a big advantage in that you can look at proteins at room temperature, and get information on dynamics,\u201d says Schr\u00f6der, whose lab builds experimental models that combine NMR data with those from cryo-EM and crystallography. First used experimentally in the 1940s, NMR reveals macromolecular structures by exciting atoms in an external magnetic field. When the atoms relax, the changes in their internal magnetic fields can be mapped to each atom. However, NMR spectroscopy works only on relatively small macromolecules or ensembles.  Structural biologists are also using hybrid methods to tackle supersized ensembles, a task that would have been impossible in the past. Kornberg's latest research, which has not yet been published, extends his ongoing RNA polymerase II studies and uses hybrid methodologies to describe a giant assembly made of more than 50 proteins and transcription factors. \u201cThe entire assembly could now be visualized for the first time through the combination, and I would say equal contribution, of all the methods,\u201d he says. Another supersized target is the nuclear pore complex. This collection of membrane proteins acts as a gatekeeper for information and molecules passing in and out of the nucleus. In 2015, Ellenberg and his colleagues used a hybrid approach to study the structure of this protein behemoth 6 . In the past, researchers had probed the complex with crystallography and electron microscopy, but they weren't able to image the entire thing at molecular resolution, and its overall structure largely remained a mystery. Ellenberg's team first imaged the nuclear-pore complex using super-resolution fluorescence microscopy, which he says can identify features measuring less than 30 nanometres. To improve the resolution, they combined it with an image-processing technique called single-particle averaging that uses information from thousands of pores, bringing down the resolution to about 10 \u00c5. Comparisons with cryo-EM maps of the same complex validated their work. The result is a zoomed-in view of a supersized protein complex. The EMBL team \u201cgenerated models of the nuclear pore that were unthinkable in the past\u201d, Nogales says. Similarly, Rubinstein and Lewis Kay at the University of Toronto used hybrid methods to push the boundaries of what was deemed possible. By combining cryo-EM with NMR spectroscopy, they mapped previously unidentified conformational changes of an enzyme called VAT, which has an important role in breaking down proteins in a cell. Cryo-EM revealed the structure, and used together with NMR, they were able to show how the enzyme changes shape, painting an elegant portrait of a protein at work 7 . \n               Hybrid drawbacks \n             Although biologists are gaining clarity from merging different tools, each technique also contributes its own error rates. Mixing them, therefore, presents a potential problem because it multiplies the sources of error. \u201cHow can I combine these different ways of analysing error into one holistic approach that gives me a measure of confidence, accuracy and precision in model?\u201d asks Meiler. Yet another hurdle is melding different data sets to make them accessible and useful to other researchers. The rich level of information from any one technique makes this a formidable challenge. \u201cYou can literally generate terabytes of data per day,\u201d says Lengyel. He hopes the structural-biology community might benefit from the approaches in astronomy and high-throughput genetics to grapple with data overload. Although software exists that can neatly combine high-resolution crystallographic data into cryo-EM maps, other hybrid methodologies aren't as straightforward to merge. Electron paramagnetic resonance spectroscopy, for example, measures distances and orientation in a macromolecule, whereas cryo-EM produces a density map. Although those two measurements would be useful together, they don't speak the same language. \u201cHow do I combine these very different metrics? How do I share these data?\u201d asks Meiler. To discuss the best ways to organize, share and use data from hybrid approaches, dozens of structural biologists gathered in October 2014 at the European Bioinformatics Institute in Hinxton, UK 8 . The meeting was the first of its kind, organized by a task force set up by the Worldwide Protein Data Bank. At present, the PDB stores data from individual protein structures, says Schr\u00f6der. \u201cWe should get to the point where we have all the information that we know about this protein \u2014 all the different conformations it can take,\u201d he says. Such rich data, he says, will help to reveal the bigger picture of proteins and other big molecules. There have been steps in that direction: archives exist for electron-microscopy models in two dimensions (the Electron Microscopy Pilot Image Archive) and three dimensions (EMDataBank). Established with funding from the EMBL and other sources, these archives contain data that can be shared, archived and distributed. Yet another challenge threatens to forestall progress in the field: human expertise. \u201cInvestments are needed in technology, but it's equally important to invest in educating scientists,\u201d says Meiler. He recommends that students learn the limitations and challenges of each method \u2014 and become an expert in at least one. \u201cWe need to train a new generation of scientists who are capable of understanding how to integrate these different technologies,\u201d he says. Finally, structural biologists have to learn to ask new, complicated biological questions that may seem impossible. Thanks to hybrid methods, says Ellenberg, \u201cthings have come within reach that even five years ago I wouldn't have been able to dream of doing until retirement\u201d. \n                     Super-resolve me: from micro to nano \n                   \n                     The revolution will not be crystallized: a new method sweeps through structural biology \n                   \n                     Microscopy: Bright light, better labels \n                   \n                     Guts of giant virus imaged in 3D \n                   \n                     Through the nanoscope: A Nobel Prize gallery \n                   \n                     Is super-resolution microscopy right for you? \n                   \n                     Nature Methods  collection: Super-resolution microscopy \n                   \n                     Nature  special: Crystallography \n                   Reprints and Permissions"},
{"file_id": "534421a", "url": "https://www.nature.com/articles/534421a", "year": 2016, "authors": [{"name": "Michael Eisenstein"}], "parsed_as_year": "2006_or_before", "body": "Ways to directly convert one mature cell type into another may eventually offer a safer, faster strategy for regenerative medicine. Until the day it dies, a cell that has become a skin cell remains a skin cell \u2014 or so scientists used to think. Over the past decade, it has become clear that cellular identity is not written in stone but can be rewritten by activating specific genetic programs. Today, the field of regenerative medicine faces a question: should this rewriting take the conventional route, in which mature cells are first converted back into stem cells, or, where feasible, a more direct approach? 'Terminally differentiated' is a term that sums up the old way of thinking \u2014 that skin, muscle or other mature cells cannot be coaxed to adopt a drastically different fate. That idea began to falter a decade ago, when cell biologist Shinya Yamanaka of Kyoto University in Japan showed that a handful of genes could transform adult fibroblast (connective tissue) cells into induced pluripotent stem (iPS) cells 1 . Like embryonic stem cells, iPS cells can develop into any cell type, a property called pluripotency. They can also be produced in unlimited quantities, unlike embryonic stem cells, which must be harvested from human embryos and therefore come with considerable political baggage. Just a few years after Yamanaka's discovery \u2014 which earned him a share of the 2012 Nobel Prize in Physiology or Medicine \u2014 researchers began uncovering shortcuts for switching cell types that they called 'direct reprogramming'. Mature cells of one kind could be coaxed to directly become another, with no pluripotent middleman. Researchers have learned how to turn skin cells into neurons or heart cells, and stomach cells into insulin-producing pancreatic \u03b2-cells. \u201cIt's amazing to watch the cells change right before your eyes,\u201d says Benedikt Berninger of the Johannes Gutenberg University of Mainz in Germany, who uses direct reprogramming to generate neurons. Research into direct reprogramming is more preliminary than work on iPS cells, but it is stirring excitement in regenerative medicine. Directly reprogrammed cells might be safer than cells that pass through a pluripotent state, because the latter share with tumour cells a capacity for extensive proliferation \u2014 making them potentially cancer-causing Trojan horses. Clinical interventions based on iPS cells must be done carefully to ensure that no pluripotent cells are transplanted along with the fully mature cells. \u201cThere's a risk that you could lose control of these cells and that they start proliferating uncontrollably after transplantation,\u201d says Malin Parmar, a neurobiologist at Lund University in Sweden who hopes to use direct reprogramming to reverse the loss of neurons in people with Parkinson's disease. \u201cBut if you bypass the pluripotent stage, it's a lot quicker and potentially safer.\u201d \n               Changing programs \n             Rewriting cellular identities first requires an understanding of how those identities are established. Every cell in the body can trace its ancestry back to a single progenitor: the fertilized egg. As embryonic cells divide and mature, their destiny is determined by the specific genes that are switched on and off over the course of development. Proteins called transcription factors regulate this process by binding certain DNA sequences in the genome, and subsequently activating or suppressing adjacent genes. The ones that govern the fate of a developing cell are often called master regulators because they operate at the summit of complicated cascades of gene activity. \u201cThese master regulators are basically all defined by their pivotal roles in embryogenesis in the development of certain cell types,\u201d says Qiao Zhou, a cell biologist at Harvard Stem Cell Institute in Cambridge, Massachusetts. \u201cPerhaps a progenitor cell can become cell A or B or C, but if you force it to express a certain master regulator, it will inevitably choose A.\u201d An early demonstration of the usefulness of master regulators for direct reprogramming came as far back as 1987, when Harold Weintraub, Andrew Lassar and their colleagues at the Fred Hutchinson Cancer Research Center in Seattle, Washington, showed that forcing fibroblasts to express a certain portion of DNA put them on a developmental path to become muscle cells; they later discovered that the single gene responsible encodes the transcription factor MyoD 2 . \u201cThat was a paradigm-shifting observation, and people in the field thought that most other cell types would have that one key factor that would be powerful enough to convert the fate of a cell,\u201d says Deepak Srivastava, a heart development researcher at the Gladstone Institute of Cardiovascular Disease in San Francisco, California. But it wasn't that simple. The hunt for individual master regulators that could initiate reprogramming would yield many years of disappointment \u2014 until Yamanaka's work on iPS cells revealed that the secret of effective reprogramming was not a single factor, but rather combinations of multiple genes. As researchers started to mix and match different sets of master regulators, success stories began to emerge. In 2008, Zhou was part of a team led by Harvard scientist Douglas Melton that transformed one type of pancreatic cell into another, generating the insulin-secreting \u03b2-cells that are needed by many people with diabetes. \u201cOur study concluded that you need a minimum of three master regulators to make that happen,\u201d says Zhou. In 2010, a group led by stem-cell scientist Marius Wernig of Stanford University in California turned fibroblasts into neurons, also using a trio of genes 3 . Further refinements and extensions of this work gave rise to a host of different, specialized neurons, with each type producing or responding to distinct neurotransmitter signals. Most of these pioneering demonstrations of direct reprogramming have been achieved with cultured cells. Yet many researchers see much greater promise for regenerative medicine if cell conversions can be prompted inside the body. Pools of cells that are relatively abundant in an organ could be transformed into other kinds of mature cells that are more desperately needed. So far, there have been a handful of triumphs in animal experiments. Parmar's group, for example, found that glial cells can be converted into functional neurons by injecting viruses laden with genes for reprogramming factors into the brains of mice. And Srivastava has likewise turned mouse fibroblasts inside the heart into beating cardiac muscle cells, a strategy that may offer a way to repair damage caused by a heart attack. \u201cYou've got this vast pool of cells that are already in the organ that you can harness for regeneration,\u201d he says. But no one has so far tried direct reprogramming inside a human. \n               Identity crisis \n             For now, most research is focused on ensuring the success of the reprogramming process. Investigators not only have to work out a successful combination of master regulators that turns on the genes that define a certain cell type: they also, ideally, have to discover the smallest possible set. This is because the most reliable way to force a cell to express master regulator genes is to deliver additional copies of these genes to it, and delivering many genes into a cell is a much tougher technical challenge than providing just a few. Working out the minimal set of master regulators can be a slog: often the roster of candidate combinations is huge, and the only way through a thicket of options is to systematically test each one. Parmar's team started with 12 candidate genes for generating dopamine-producing neurons, for example, before eventually narrowing it down to 2. Some researchers have started to create software specifically for direct reprogramming that incorporates information about which master regulators control the formation of tissues. A team spread over three continents has developed an experiment-planning tool called Mogrify 4 , which brings together large quantities of gene-expression data from a long list of cell types with rules about the gene networks that different master regulators control. Mogrify uses these to predict the combination of reprogramming factors that will cause a desired cellular identity change. The idea is to provide researchers with a way to computationally identify the fewest possible master regulator genes that can directly reprogram one particular cell type into another. But providing active master regulator genes isn't always enough to ensure complete reprogramming: the master regulators may successfully set a cell on a developmental path, but then leave it stranded in an immature, precursor state. Then the task is to identify which additional genes must be active to finish the process, and add them to the delivery package. Stem-cell biologist Hongkui Deng at Peking University in Beijing struggled with this problem for years. His initial efforts to directly turn skin cells into liver cells through the forced expression of master regulator genes alone yielded cells that failed to perform key, liver-like functions. Then, during a second round of screening, he identified additional genes that could complete the reprogramming 5 . He calls them maturation factors \u2014 genes that are unimportant for initiating the conversion but crucial for obtaining functionally mature cells. Other researchers have found that they can boost the success rate of direct reprogramming by augmenting the effects of master regulator genes with chemicals that act on cellular signalling pathways to promote reprogramming \u2014 occasionally, chemicals alone can prompt a cell-type transformation (see  'Better modifying through chemistry' ). Even with the appropriate gene and chemical deliveries, it is hard to prove that any direct reprogramming is truly complete. Peering through a microscope can reveal whether a transformation has taken place \u2014 for example, whether flat, star-shaped fibroblasts have formed long, axon-like projections \u2014 but deeper analysis of the cell's inner workings is also needed. Put simply, how can one be certain that a reprogrammed skin cell has truly become a neuron, and is not merely 'neuron-like'? Measuring the downstream activity of master regulator genes can offer insights into how well reprogramming has succeeded. If the introduced master regulators are doing their job, they should cause grand shifts in the overall patterns of gene expression in the cell nucleus, which should match the patterns found in mature cells of the target tissue. There are several ways to survey a cell's total gene expression \u2014 for example, sequencing all of the RNA molecules in it. Researchers at Boston University and Harvard University in Massachusetts have drawn on this kind of data in their development of CellNet, a software program that can assess how well the gene activity in reprogrammed cells matches that of target cells 6 . Still, the identity test that really matters is whether reprogrammed cells can functionally replace naturally differentiated cells. \u201cIf they look like neurons and have gene expression like neurons, that doesn't mean they're really neurons,\u201d says Chun-Li Zhang, a neurobiologist at the University of Texas Southwestern Medical Center in Dallas. Convincing proof requires a battery of assessments, such as electrophysiological measurements that confirm whether a newly formed neuron is firing and is therefore capable of activating other neurons that are linked to it by synapses. No one characteristic can provide sufficient evidence in isolation, says Zhou. His group's attempts to reprogram liver cells into pancreatic \u03b2-cells yielded only dysfunctional intermediates. \u201cThey synthesized and released insulin in large quantities \u2014 so much so that the animals died from hypoglycaemia,\u201d he says. This is because the cells lacked pancreatic cells' ability to sense and respond to blood glucose levels. One of the findings of these diagnostic tests is that prompting reprogramming within a target organ often works better than efforts with cultured cells. \u201cMost of our cells only partially reprogram to cardiac muscle when they're on plastic,\u201d says Srivastava. \u201cBut in their natural environment, the majority go all the way to a beating state, where they're electrically coupled with their neighbours.\u201d This may be due to chemical cues generated by other neighbouring cells in the organ, or because of features of the 3D tissue environment that are hard to replicate in the lab. Whatever the reason, it bodes well for developing clinical applications. \n               Path to the clinic \n             Researchers agree that there are many hurdles to overcome before these methods can be tested in people. In general, human cells have proven more challenging to directly reprogram than mouse cells: they tend to take longer to go through the reprogramming process and often require additional transcription factors to those that are sufficient in animal experiments. Gene delivery also poses formidable challenges, especially into organs such as the brain. In some cases, viruses that preferentially infect particular cell types could help to guide reprogramming factors to specific sites of disease or injury, but delivery to unintended sites may still pose risks. Then there's the issue of 'robbing Peter to pay Paul'. Transforming glia into neurons in the brain reduces the number of glia there \u2014 which might pose a hazard. \u201cThese cells are not just for decoration,\u201d says Berninger. \u201cThey have important functions, and how do we replace them if we take them away?\u201d One possibility is to reprogram cells to a proliferative \u2014 but non-tumorigenic \u2014 neural progenitor state. That way, a few glia could yield numerous neurons. Heart treatments are probably closest to the clinic. Srivastava's team has already begun studies to turn fibroblasts inside pig hearts into cardiac muscle cells. \u201cWe have initial proof of concept that even in a big heart like ours, we can achieve efficacy,\u201d he says. The team is now carrying out safety studies and refining their gene delivery method with the aim of gaining regulatory approval for human trials. Importantly, heart fibroblasts are self-replenishing, so concerns over cell loss are less acute. Zhou's team is also making headway towards the clinic in its attempts to switch cultured human gastrointestinal cells directly into \u03b2-cells. The gut cells are easily obtained by biopsy, and after cultivation and reprogramming they could, in theory, be transplanted into the pancreases of volunteers who have diabetes. Direct reprogramming is beginning to garner interest from industry, although biotechnology and pharmaceutical companies are not quite ready to jump in with both feet. Although research into iPS cells and embryonic stem cells has a head start in this respect, the gap may close as the advantages of direct cell-type switching come into focus. \u201cThere is not yet a comparable amount of resources and manpower going into this approach,\u201d says Zhou. \u201cBut the field is quickly catching up, and I can't wait to see where it's going.\u201d \n                     Brain cells made from urine \n                   \n                     Stem cells: Reprogramming finds its niche \n                   \n                     Stem cells: Reprogramming in situ \n                   \n                     Diabetes forum: Extreme makeover of pancreatic \u00ce\u00b1-cells \n                   \n                     A predictive computational framework for direct reprogramming between human cell types \n                   \n                     Dissecting direct reprogramming from fibroblast to neuron using single-cell RNA-seq \n                   \n                     Nature  subject area page for stem cells \n                   \n                     Nature  Insight: Stem Cells \n                   \n                     Cell Fate Plug and Play: Direct Reprogramming and Induced Pluripotency \n                   \n                     Cell Stem Cell  Collection on Direct Reprogramming \n                   \n                     Generation of induced neurons via direct conversion in vivo \n                   Reprints and Permissions"},
{"file_id": "533565a", "url": "https://www.nature.com/articles/533565a", "year": 2016, "authors": [{"name": "Marissa Fessenden"}], "parsed_as_year": "2006_or_before", "body": "A suite of tools now enables scientists to see proteins at work in living cells at the single-molecule level.  Biophysicist Joerg Bewersdorf says that 2006 was fluorescence microscopy's  annus mirabilis  \u2014 a 'miraculous year' as momentous in its own way as 1905, when Albert Einstein revolutionized physics in the realms of relativity, quantum theory and atomic physics. In microscopy's case, the revolution consisted of three papers 1 , 2 , 3  that, for the first time, gave scientists the power to peer down into the cell and track the behaviour of individual molecules. \u201cEvery molecule is a machine, a little nano-machine,\u201d Bewersdorf says. Proteins, in particular, are complex molecules that twist, flex, open and shut in a multitude of ways to perform the reactions necessary for cell metabolism and growth, sending messages and providing structure. \u201cThat is what we are ultimately interested in understanding,\u201d says Bewersdorf: \u201cHow do all these little machines work together for the global function of the cell?\u201d Until scientists could observe that world, however, they had only the cloudiest idea of how to answer that question. Light microscopes were no help; beyond a certain magnification, diffraction causes light waves to spread out instead of converging to form an image. Any features closer together than about 200 nanometres, or about 40 times the width of a typical cell membrane, become a hopeless blur. Images made using electron microscopy can resolve fine structures \u2014 but they are static and almost impossible to obtain from a live cell.  The three laboratories that independently circumvented the 'diffraction barrier' in 2006 adopted a similar strategy: studying the sample with specialized fluorescence probes that can be selectively switched on, a few at a time, until all of the probes are captured in a series of images. Combining the data from those images builds a picture, in a similar way to an impressionist painter building up a scene with dots of colour (see 'Connecting the dots'). The three techniques \u2014 photoactivated localization microscopy (PALM) 1 , fluorescence PALM (FPALM) 2  and stochastic optical reconstruction microscopy (STORM) 3  \u2014 can differentiate between points just 20 nanometres apart, producing the sharpest-ever fluorescent images at the single-molecule level. Researchers have rushed to take advantage of these capabilities. At Bewersdorf's lab at Yale University in New Haven, Connecticut, for example, he has filmed proteins moving across the surface of living cells 4 . In the decade since 2006, the three techniques have inspired a wave of technological and methodological innovations. Researchers are designing fluorescent probes that shine brighter and are robust enough to image cellular processes as they unfold. They are also developing methods that cause less disruption to living cells. Several illumination strategies seek to reduce the visual noise caused by background fluorescence, whereas computational methods and strategies are allowing researchers to combine multiple imaging approaches to see molecular interactions in real time. \u201cThe big excitement over these past few years is that these technologies have become doable in living cells,\u201d says Jennifer Lippincott-Schwartz, a cell biologist at the Howard Hughes Medical Institute Janelia Research Campus in Ashburn, Virginia. \u201cThe time is definitely ripe for being able to image individual proteins using fluorescence.\u201d \n               Lasting longer, shining stronger \n             All three of the super-resolution microscopy techniques invented in 2006 rely on the light emitted by probe compounds such as green fluorescent protein (GFP), which was first isolated in a bioluminescent jellyfish. The genes for these probes can be inserted into the DNA coding for a cellular protein of interest. Then, when that protein is produced, it will have the fluorescent compound attached, and will reveal its presence by glowing. The time is definitely ripe for being able to image individual proteins using fluorescence. But these techniques have some severe limitations. A big one is that many of these probes can emit only a finite number of photons before they are irreversibly damaged by the intensity of the lasers that are used to excite them into emitting light. Even before this photo-bleaching effect takes hold, the probes are quite dim when imaged individually. Synthetic versions of these probes known as organic (that is, carbon-containing) dyes are brighter, but they cannot be genetically encoded to their target and manufactured inside the cell. Instead, they are often linked with antibodies that can seek out the protein of interest. However, that combination can make the probes too large to pass through the cell membrane or bulky enough to interfere with a protein's function. \u201cProbes are really limiting us and really defining to some extent where this technology can go,\u201d says Bewersdorf. Fortunately, alternatives are emerging. Bewersdorf's group is working with two 'clickable chemistry' probes: SNAP-tag, from New England Biolabs in Ipswich, Massachusetts, and HaloTag, from Promega in Madison, Wisconsin. These technologies involve a short target sequence that can be encoded into the protein of interest and a dye molecule that clicks into place with its target protein through a simple chemical reaction. Bewersdorf and his colleagues demonstrated that the two tags can be used with organic dyes in living cells to achieve a resolution below 50 nm 5  \u2014 almost as precise as the 20-nm resolution of the original techniques, with the advantage that they combine the specificity and leanness of genetically encoded probes with the brightness of synthetic dyes. Researchers have also turned to quantum dots, nanoscale semiconductors that are not only bright and stable for a month or more, but can also link to biological molecules. Diane Lidke, a biophysicist at the University of New Mexico in Albuquerque, uses quantum dots in her lab's work on cell signalling. For her, the benefits outweigh the major disadvantage of quantum dots \u2014 their size. Commercially available quantum dots are surrounded by a shell that can link the dot to other molecules but that expands the dot's diameter to 15\u201325 nm. \u201cThey are quite large and bulky,\u201d she says, at least when compared with a fluorescent protein, which can be just 4 nm wide. That pitfall means that researchers have difficulty getting quantum dots into the cell or other tight spaces, but they work well for the kind of extracellular, membrane-bound proteins that Lidke targets. In collaboration with her husband Keith Lidke, a physicist at the same university, she has developed a multiple-colour, fast, single-molecule tracking method that uses quantum dots to produce images on a custom microscope 6 . Still, much of the cell's internal processes remain locked inside the membrane, difficult to reach with fluorescent probes. \n               Cracking open the cell \n             Getting past the cell's membrane is one of the most daunting hurdles that fluorescence microscopy faces. \u201cEven though it is only five nanometres thick, [the cell membrane] has had a few billion years of evolution to separate the inside of the cell from the outside, and it does this amazingly well,\u201d says Paul Selvin, a biophysicist at the University of Illinois at Urbana\u2013Champaign.  Selvin's lab has developed its own version of quantum dots that are smaller, closer to 9 nm in diameter 7 . That size reduction helps him to slip quantum dots into the approximately 20\u201340 nm gap between nerve cells, the synapse, where signalling molecules pass on messages to nearby nerve cells. Once in that cleft, the quantum dots can bind to and advertise the presence of receptors that facilitate memory formation. Selvin hasn't sent these smaller quantum dots inside living cells yet, but he says that this could be possible. Selvin's lab is also working on a strategy to punch holes in the plasma membrane and then quickly reseal them so that the cell isn't disturbed. \u201cWe effectively drill little microscopic pores into the membrane that are about five nanometres,\u201d he says, using a bacterial enzyme called streptolysin O. That's just wide enough to let a fluorescent protein, even one linked to an antibody, slip inside and find its intracellular target. The method, which Selvin has yet to publish, then patches up those holes within 20 minutes. Yet there is always the concern that the added probe could interfere with the target protein's typical function. An alternative strategy that doesn't impair the protein comes from Jie Xiao, a biophysicist at Johns Hopkins University in Baltimore, Maryland. Her probe molecules are genetically encoded, but instead of hanging on to the molecule of interest, they are cleaved by an enzyme as soon as they are produced and scurry off to a particular part of the cell membrane. That means that they no longer carry any information about the target molecule's position, but they are in a position where Xiao can count them precisely and thus get an exact tally of the proteins produced, while the proteins themselves are free to go about their business unencumbered. She calls the method co-translational activation by cleavage (CoTrAC) 8 . \u201cBeing able to quantify the absolute protein level in a living cell is very important,\u201d Xiao explains. \u201cMost of the time people use the fluorescence to indicate relative change.\u201d However, only a few of the gene-regulation proteins she studies are produced at a time, which makes them difficult to image with most super-resolution techniques. Furthermore, small changes in the exact number of those proteins can determine whether the cell changes states or not. The advantage of CoTrAC is that, by gathering probes for different proteins at different locations on the cell membrane, this technique can be used to count multiple protein products using the same fluorescence molecule and colour. This is a crucial ability when producing different probes can take varying amounts of time and may obscure the timing of cell processes. \n               Lighting the way \n             Crisper, striking images can come from brighter probes, but another way to make the probes stand out is to reduce the background light. \u201cYou have many different planes that you observe simultaneously but you can see only one plane sharply \u2014 that in the focus plane of your camera,\u201d says Ulrich Kubitscheck, a biophysical chemist at the University of Bonn in Germany. \u201cBut you still have the diffused background of everything else in the cell.\u201d Non-target proteins can even have their own, natural, dim fluorescence that contributes to this noise. If that background can instead be kept in the dark, the contrast and clarity of images can be enhanced. Therefore, researchers are constantly improving their illumination strategies. Kubitscheck's lab uses light-sheet microscopy to generate a very thin beam of precisely focused light that slices through the sample from the side. \u201cWe have a glass chamber that is transparent from below and the sides,\u201d he says. By shining light through the side rather than the top of the sample, his group illuminates a section only 200\u2013300 nm thick and observes the sample from below. In this way, the group has seen RNA molecules as they are exported out of the nucleus through a protein complex called the nuclear pore and into the cytoplasm, where they go on to instruct protein synthesis 9 . Microscopes equipped to carry out light-sheet imaging are commercially available from companies such as Leica Microsystems in Wetzlar, Germany, and Carl Zeiss in Oberkochen, Germany. Groups with the necessary expertise can even do as Kubitscheck's team has done and build their own, custom microscopes. The next iteration of such selective plane illumination microscopy is called lattice light-sheet microscopy and hails from the lab of Eric Betzig, a physicist at Janelia who also developed PALM microscopy. The technology can generate an illumination plane 300\u2013500 nm thick, says collaborator Zhe Liu, a cell biologist at Janelia \u2014 but the real advantage of the method is the structure that the light takes. The lattice forms a three-dimensional grid that moves, illuminating successive sections though the sample. \u201cYou can image for a much longer period of time because the [out-of-focus] molecules are being preserved,\u201d Liu says. Capturing 3D images is also possible. Liu first started using the technology three years ago, while Betzig was still developing it, to examine the organization of protein clusters that are necessary for maintaining a stem cell's ability to self-renew 10 . Lattice light-sheet microscopy is not yet commercially available, although Carl Zeiss is working on a microscope. Until this goes on sale, groups that are interested in using the technology need to assemble their own custom microscopes. \u201cAlignment is tricky,\u201d Liu cautions, but Betzig and others provide workshops to help those willing to tackle the challenge. \n               Motion in miniature \n             These probes and illumination strategies can be combined for truly novel insights. \u201cTypically, the field has defined the structures in cells,\u201d says Lippincott-Schwartz. \u201cNow we are getting a handle on the underlying mechanisms that allow these structures to move and interact.\u201d In her lab at Janelia, Lippincott-Schwartz and her colleagues are leveraging lattice light-sheet microscopy \u2014 which she calls a \u201ctruly transformative technology\u201d \u2014 to look at the way cell organelles and proteins interact. She and her colleagues have watched enzymes repeatedly interact with the endoplasmic reticulum (ER), a network of membranes in the cell where proteins are synthesized and folded 11 . Canonically, the ER has been thought of as only a site of protein secretion, but these observations have \u201creally made me start thinking very differently about the primary or major function of this organelle\u201d, she says. The ER might be communicating with other structures more than was previously thought. This kind of work requires expertise even beyond that needed to align and use the microscopes properly. Lippincott-Schwartz explains that it requires the appropriate algorithms to reconstruct the image from the data acquired by the microscopes. \u201cThis is not just something you can pick up,\u201d she says. Typically, labs without substantial experience in super-resolution imaging and the statistical methods it relies on can turn to the specialized knowledge held by experts in the shared imaging facilities that some institutions have. But she also sees a need for standards related to image acquisition and analysis to be set for the field. \u201cOtherwise, there will be information that will be improperly interpreted,\u201d she says. The complexity of single-molecule imaging means that clean data, controls and proper analysis are invaluable. \u201cIf I could recommend something to people entering the field,\u201d says Antoine Triller, a neurobiologist at the Institute of Biology of the \u00c9cole Normale Sup\u00e9rieure in Paris who studies the movement of molecules in the synapse, \u201cit would be either to have a good background in statistical physics or to work with people with a very good knowledge in the field.\u201d Making this effort is worth it, however, to gain access to the 'black box' of molecule-scale life occupying the space between classic light microscopy at the microscale and electron microscopy at the nanoscale. Single-molecule approaches in living cells offer a way to find new biological parameters to measure and observe, Triller says. \u201cThese parameters will allow us to develop new theoretical fields and a new understanding of living matter.\u201d \n                     Microscopy: Bright light, better labels \n                   \n                     Nobel for microscopy that reveals inner world of cells \n                   \n                     Super-resolve me: from micro to nano \n                   \n                     Eric Betzig\u2019s laboratory \n                   \n                     Joerg Bewersdorf\u2019s laboratory \n                   \n                     Ulrich Kubitscheck\u2019s laboratory \n                   \n                     Diane Lidke\u2019s laboratory \n                   \n                     Jennifer Lippincott-Schwartz\u2019s laboratory \n                   \n                     Jie Xiao\u2019s laboratory \n                   Reprints and Permissions"},
{"file_id": "538275a", "url": "https://www.nature.com/articles/538275a", "year": 2016, "authors": [{"name": "Kelly Rae Chi"}], "parsed_as_year": "2006_or_before", "body": "Scientists are uncovering the hidden switches in our genome that dial gene expression up and down, but much work lies ahead to peel back the many layers of regulation. Fifteen years ago, scientists celebrated the first draft of the sequenced human genome. At the time, they predicted that humans had between 25,000 and 40,000 genes that code for proteins. That estimate has continued to fall. Humans actually seem to have as few as 19,000 such genes 1  \u2014 a mere 1\u20132% of the genome. The key to our complexity lies in how these genes are regulated by the remaining 99% of our DNA, known as the genome's 'dark matter'. From efforts such as the massive Encyclopedia of DNA Elements (ENCODE) project 2 , launched in 2003 by the US National Human Genome Research Institute, it's clear that copious regulatory elements are at play, tuning gene expression in ways that scientists are only starting to unravel. By uncovering regulatory instructions in the genome beyond protein-coding genes, scientists are hoping to yield new ways to understand and treat disease. \u201cIt's not overstating to say that ENCODE is as significant for our understanding of the human genome as the original DNA sequencing of the human genome,\u201d says cell biologist Bing Ren of the University of California, San Diego, Institute for Genomic Medicine in La Jolla, who is a member of the ENCODE team. Ren is also part of a subsequent consortium called the Roadmap Epigenomics Project 3 . These two initiatives \u2014 both funded by the US National Institutes of Health (NIH) \u2014 aim to map and predict the existence of elements in the genome, including in the vast stretches of non-coding portions, that drive when and where genes are expressed. Scientists have generated a list of such elements by using biochemical assays to probe DNA sequences, RNA transcripts, regulatory proteins bound to DNA and RNA and epigenetic signatures \u2014 the chemical tags on DNA and the proteins packaging it \u2014 that also affect gene expression. So far, the data suggest that there are hundreds of thousands of functional regions in the human genome whose task is to control gene expression: it turns out that much more space in the human genome is devoted to regulating genes than to the genes themselves. Scientists are now trying to validate each predicted element experimentally to ascertain its function \u2014 a mammoth task, but one for which they now have a powerful new tool. Since the gene-editing technique CRISPR\u2013Cas9 entered the scientific arena, the speed at which researchers can test functional elements in the non-coding regions has ramped up. But it is still a daunting endeavour: more than 3 million regulatory DNA regions, thought to contain some 15 million binding sites for regulatory proteins called transcription factors, control gene expression in the human cell types studied thus far. About 150,000 may be active in any given cell type. These could be crucial to understanding disease, because most single-nucleotide changes associated with common diseases fall in regions outside protein-coding genes, and they often overlap with DNA sites highlighted by ENCODE as having regulatory function. Certain regulatory elements that normally drive gene expression are thought to underpin the mechanism of cancer, for example. Disrupting a gene's regulatory elements, the data suggest, could thus have as drastic an impact on cell function as disrupting the gene itself. Using CRISPR\u2013Cas9, scientists now have an opportunity to test that premise by introducing targeted mutations into non-coding sequences and observing the consequences. \n               Decoding a complex world \n             How much of DNA's dark matter has a function in gene control is still up for debate. In 2012, ENCODE scientists proposed on the basis of biochemical-assay predictions that 80% of the non-coding genome has a function 2 . But this figure soon proved to be an overestimate as researchers narrowed the definition of 'function' and devised experimental methods, such as reporter assays, to test these functions. \u201cThe number still isn't fully known\u201d, in part because the mapping isn't complete, says Michael Snyder, a geneticist at Stanford University in California and a member of ENCODE. \u201cMost people would say between 10% and 20% of the [non-coding] genome is likely to have a function where, if you disrupt it, you will affect something.\u201d But regulatory elements have a bewildering array of functions and forms, which makes tackling them a formidable challenge. Even the best-known types, such as spots in the genome known as promoters, which lie next to a gene where transcription begins, and enhancers \u2014 regions that when bound by specific transcription factors alter the likelihood of a gene being read \u2014 are hard to study. In addition to the sheer number of these sites, estimated at 15 million, enhancers may be positioned thousands of base pairs away from the gene that they control. This makes it tough to predict where their target genes are located and what they do. Thus far, ENCODE and Roadmap have offered up important clues, but the real proof that these predicted regulatory elements actually do something comes from a functional test. For genes, this mostly entails deleting them one at a time and observing the consequences in a cell assay or animal model. This is less easy to do for the non-coding genome because many of the elements are redundant, and so deleting just one might not alter gene expression or produce an obvious change. \u201cIt's a huge challenge that we have at the moment to really distinguish between functional and non-functional elements detected by ENCODE,\u201d says geneticist Ran Elkon of Tel Aviv University in Israel. CRISPR\u2013Cas9 is particularly accelerating scientists' exploration of enhancers. The technology enables scientists to alter large numbers of regulatory elements in a high-throughput way, using libraries of RNA guide fragments that target and disrupt different regions in the genome, to observe the outcome. Not only is the method relatively fast, but researchers can also run the assays directly in human cells. Experiments of this type have already turned up some unexpected findings. While a postdoc working with cancer biologist Reuven Agami at the Netherlands Cancer Institute in Amsterdam, Elkon was involved in performing the first screen of regulatory elements using the advanced editing system 4 . The CRISPR\u2013Cas9 approach enabled them to test individually those enhancers predicted by ENCODE to bind a transcription factor called p53. Interest in p53 is high because the protein is a known tumour suppressor that is mutated in more than 50% of human tumours. The researchers were able to pinpoint two enhancers from more than a thousand genomic sites that affect p53's tumour-suppressing function, located near the p53-encoding gene. A predicted third enhancer has yet to be located because it is far from any gene, let alone one related to p53. In a separate screen, the group targeted binding sites for oestrogen receptor-\u03b1 \u2014 which is implicated in breast cancer \u2014 and identified three enhancer sequences that influence tumour growth; these elements could thus have a role in the development of resistance to breast-cancer therapy. At the Broad Institute of MIT and Harvard in Cambridge, Massachusetts, bioengineer Feng Zhang and his group also used CRISPR\u2013Cas9 to identify genes essential to the survival of cancer cells. Using a melanoma model, they first screened around 18,000 genes in human cells to pinpoint ones that might underlie resistance to the melanoma drug vemurafenib. Then, in a follow-up study published last month 5 , they described a new screen that identified regulatory regions on either side of several resistance genes. Their findings fit well with ENCODE data that predict regulatory regions at these locations \u2014 and they also reveal new functional elements, says molecular biologist Neville Sanjana, who conducted the research as a former postdoc in Zhang's group and now works at the New York Genome Center at New York University. Other CRISPR\u2013Cas9 screening data have challenged ENCODE predictions. Richard Sherwood of Harvard Medical School in Boston, Massachusetts, and his collaborators created an approach called a multiplexed editing regulatory assay 6  to screen for non-coding regions that might influence gene expression in well-known mouse embryonic stem-cell lines. Using this technique, they obtained quantitative information about the extent to which these regulatory regions might contribute to gene expression. Some of their results are discordant with regions flagged by ENCODE as potential enhancers because, when mutated, these areas did not affect gene expression. Moreover, the researchers also discovered mysterious sections that they dubbed 'unmarked regulatory elements', or UREs, that do not fit into any category of functional elements. The team is currently exploring how widespread these UREs might be in the genome. This new type of assay, along with other gene-editing-based screens, will play an increasingly important part in the validation of ENCODE candidates, says Sherwood. \n               Technique tweaks \n             Investigators working on the ENCODE and Roadmap projects have relied mostly on a biochemical technique called DNase-seq, which sequences and maps all exposed regions of the genome. In these sections, the DNA is relaxed instead of tightly coiled around histones, and thus is more likely to facilitate transcription-factor binding that drives gene activation. By mapping these areas, investigators can pinpoint candidate enhancers, promoters, silencers, insulators and other regulatory elements in the non-coding genome (see 'Spot the regulators'). Another method, ATAC-seq, detects and sequences sites in the chromatin that are accessible to the transposase enzymes used for the assay. Both DNase-seq and ATAC-seq produce a genome-wide view of regions of open chromatin. According to the researchers, because such epigenomic profiles can map the extent to which genes are activated in certain cell types, they could be useful for clinical decision-making, and ATAC-seq is fast enough for this purpose 7 . Many, however, consider a technique known as chromatin immunoprecipitation (ChIP)-seq to be the most reliable for this purpose because it is the only one that can identify all potential binding sites for a given transcription factor.\n Even so, biochemical assays can only hint at function. CRISPR\u2013Cas9 cell screens, by contrast, are more concrete because scientists can introduce a mutation or deletion at a particular site in the genome and observe how it influences gene expression. The disadvantage is that these tests cover smaller portions of the genome. If the full genome of 3 billion base pairs were represented, for example, by three copies of Leo Tolstoy's classic novel  War and Peace  (1869), such screens would barely cover a single page, Sanjana says \u2014 although he is optimistic that future gene-editing approaches will scale this up. \u201cIn the short term, I think CRISPR will serve mainly as a tool to validate functions predicted by those biochemical signatures,\u201d says Ren. Once enough of these kinds of screens have been done, their data could be fed into a machine-learning tool to improve its predictive power, Sherwood says. New computational tools are already providing scientists with smart ways to interpret biochemical mapping data. Algorithms can predict transcription-factor binding sites, which researchers can then probe for function. But even with algorithms, predicting which enhancers are active in a given context is harder in human genomes than in yeast or worm genomes, says computational biologist Michael Beer of Johns Hopkins University in Baltimore, Maryland. Beer and his collaborators have developed a computational model 8  to predict which tissue-specific networks of gene-regulatory elements are operating in a given cell type and to what extent they are perturbed in complex diseases. They trained their open-source algorithm, called deltaSVM, on human lymphoblastoid cell lines using gene data from ENCODE in 2012, followed by mouse ENCODE data in 2014. Scientists have initially focused on cancer to probe the links between functional elements and disease because cancer is a simpler condition to study at the cell level than, say, a neuropscyhiatric disorder \u2014 cancer cell lines reveal simple-to-measure outcomes, such as cell multiplication, death or senescence. But the data that have streamed in from the Epigenome Roadmap consortium are shifting scientists' thinking about how cancers arise. A study published last year by geneticist John Stamatoyannopoulos of the University of Washington in Seattle and his collaborators showed 9  that mutations in a given cancer cell type cluster in inaccessible chromatin regions rather than in the exposed ones \u2014 possibly because the open regions can be accessed by DNA-repair enzymes. The scientists also found that mutation density in a tumour is defined by the epigenomic profile specific to each type of cell. Consequently, the DNA sequence can be informative about tumour origin, which ushers in the possibility of using epigenomic data to trace cancer provenance in patients for whom it remains unknown. It could also open up new approaches to cancer treatment. \u201cCancer is essentially a regulatory or epigenetic program that is superimposed on a cell, and the result of that program is the development of genetic and genomic instability,\u201d says Stamatoyannopoulos. \u201cAs we've analysed lots of cancer genomes, all of these patterns now are starting to come out that were previously not imagined to exist.\u201d It's possible that there are still elements in the genome that existing assays have missed. After all, regulatory signals still crop up unexpectedly, such as the UREs in Sherwood's screen. And a team of scientists led by Harvard Medical School immunologist Daniel Tenen discovered 10  a potential new class of regulators that seem to control whether a gene is turned on or off by blocking the enzyme DNA methyltransferase 1, which adds methyl groups to silence genes. These elements are dubbed 'extracoding RNAs', and because they can influence silencing in a gene-specific way, have therapeutic potential. Earlier this year, neuroscientist Jeremy Day of the University of Alabama at Birmingham and his colleagues showed in rat neurons that an extracoding RNA influences the transcription of a gene important for memory formation 11 . The ENCODE team will continue to map the non-coding space in the genome and expects to cover most of the regulatory DNA by 2020, Stamatoyannopoulos says. A spatial understanding of how DNA is packaged into a cell, and of the 3D folding that positions genes in close contact with their regulatory elements, will be key to predicting an element's target genes. The NIH Common Fund has begun the '4D Nucleome' project, for instance, which aims to predict the target genes for every regulatory element. That knowledge will help to fill in the picture of how a given regulatory element influences health and disease. Next-generation sequencing has been \u2014 and still is \u2014 the technological engine of ENCODE. But looking ahead, researchers might be able to roll out high-resolution live-cell imaging on a large scale to watch the state of the genome change in real time using specific markers. This technology could be disruptive. \u201cIf we had a better microscope, we wouldn't be sequencing anymore,\u201d says Stamatoyannopoulos.\n Footnote  1 \n                     ENCODE: The human encyclopaedia \n                   \n                     CRISPR: gene editing is just the beginning \n                   \n                     High-throughput mapping of regulatory DNA \n                   \n                     Role of non-coding sequence variants in cancer \n                   Reprints and Permissions"},
{"file_id": "535453a", "url": "https://www.nature.com/articles/535453a", "year": 2016, "authors": [{"name": "Amber Dance"}], "parsed_as_year": "2006_or_before", "body": "Research into ageing requires patience, but a small cadre of scientists is angling to speed up answers by developing the flamboyant, short-lived turquoise killifish as a new model.  Physiologist Alessandro Cellerino has always been an aquarium enthusiast, but fish were not originally part of his research plan. One afternoon in 2000, hanging out in a tank-filled cellar in Canossa, Italy, with breeder Stefano Valdesalici, Cellerino idly asked him which fish were the shortest-lived. Valdesalici pointed to a tank with brightly speckled African turquoise killifish: \u201cThey don't make it any longer than three months.\u201d \u201cAre you kidding?\u201d asked Cellerino, who works at the Scuola Normale Superiore in Pisa, Italy. \u201cOK, I want them.\u201d So in March 2004, Cellerino and his graduate student Dario Riccardo Valenzano found themselves bouncing through Mozambique in a four-wheel-drive truck with Valdesalici, who chairs the Italian Killifish Association in Canossa. They donned chest-high waders and gloves to net killifish from the seasonal, cow-pat-spattered mud holes where the fish live. Like the tank-bred versions, these wild strains were exceptionally short-lived. Of the many varieties of killifish, the turquoise killifish ( Nothobranchius furzeri ) has the shortest lifespan \u2014 the briefest of any vertebrate bred in captivity, ranging from 3 to 12 months depending on strain and living conditions. Using killifish to study ageing is not a new idea. In the late twentieth century, scientists studied ageing in one species,  Nothobranchius guentheri , that lives for about 14 months. But given techniques available at the time, they could come up with only basic descriptions of ageing features. When Cellerino encountered  N. furzeri , timing and luck were on his side: advances in molecular analysis had set up excellent conditions in which to develop the model and investigate mechanisms behind its dotage. The killifish's brief lifespan, relative to those of longer-lived models such as mice and zebrafish, enables ageing research to progress apace. And because the fish is a vertebrate, the research is more directly relevant to people than are studies of short-lived organisms such as fruit flies or nematodes (see 'The long and short of it'). To set up a killifish model, researchers have taken advantage of modern genomic tools and drawn techniques from well-established zebrafish protocols, rather than starting from scratch. In 2015, the publication of CRISPR\u2013Cas9-based gene-editing techniques for the killifish 1 , as well as two complementary genome-sequencing efforts 2 , 3 , boosted the fish to the status of genetically tractable model. The killifish \u2014 or  Notho , as some scientists affectionately call it \u2014 is certainly gaining fans. Interest has \u201creally exploded over the past few years\u201d, says Valenzano, who now works at the Max Planck Institute for Biology of Ageing in Cologne, Germany. He estimates that about two dozen scientists have visited his group in the past year to learn killifish husbandry. In June, about 70  Notho  aficionados attended the second  Nothobranchius  Symposium in Jena, Germany. But the challenges of keeping killifish \u2014 such as their lack of a standardized diet \u2014 and a want for basic reagents, such as  Notho -specific antibodies, mean that the fish has a way to go before it reaches the utility of lab mice. The ephemeral existence that so appeals to scientists is an evolutionary adaptation to the fish's natural environment: their accelerated development enables them to live and reproduce in transient mud pools during the wet season in equatorial Africa. The eggs survive in a dormant state during the dry season, and once the rains come and pools form, they hatch. The fish have only a few weeks or months to grow up and spawn before the water dries up. Hobbyists, attracted by the males' flashy appearance, have been collecting turquoise killifish since they were discovered in Zimbabwe in 1968. Consequently, Cellerino's first challenge was to confirm that their lifespan was not a side effect of decades of breeding in tanks. Most of the wild  N. furzeri  that Cellerino's team caught in Mozambique lived for about eight months \u2014 not as brief a time as the inbred Zimbabwe line, but still short enough to interest scientists. But that begged another question: would killifish age in a way that parallels the human process? Yes, says Valenzano: the fish do get 'old' before they die. \u201cThey don't drop dead after four months,\u201d he says. \u201cThey slowly deteriorate.\u201d The fish become duller in colour, lose muscle mass and body weight, develop cancers and swim around less. The brain shows typical signs of ageing, too, says Livia D'Angelo, an anatomist at the University of Naples Federico II in Italy. Glia \u2014 brain cells that provide support and protection for neurons \u2014 upregulate the glial fibrillary acidic protein GFAP, as happens in mammalian ageing, and age-associated, lipid-rich pigment granules called lipofuscin accumulate. Neurons degenerate and deposit amyloid molecules that aggregate, resembling the plaques seen in people with Alzheimer's disease, Valenzano adds. He has also found that old fish don't learn as well as young ones. Young fish rapidly work out how to avoid an unpleasant stimulus, such as a plastic stick swirling in their tank, whereas old-timers take longer to catch on 4 . \u201cIt's a very good model for neuroscience research,\u201d D'Angelo says. The fish also respond to anti-ageing interventions much as some short-lived vertebrates do. Resveratrol \u2014 the stuff in red wine that prolongs life in nematodes and fruit flies \u2014 can lengthen their lifespan by up to 59% 4 . Restricting feedings to every other day creates a caloric deficit known to extend lifespan in organisms ranging from yeast to rodents, and it does the same for killifish, although the effects vary by strain 5 . \n               Gone fishing \n             Having shown that killifish decline with age, scientists now want to understand how the process occurs. One key resource is the collection of several strains from Africa whose genomes are not identical.  Notho  scientists have four main strains to choose from, Cellerino says: the original Zimbabwe line, and three derived from fish caught in Mozambique in 2004 and 2007, which have slightly longer lifespans. By cross-breeding two strains, Cellerino and his team created fish with a range of lifespans. They then compared the genomes and longevities of parent and second-generation progeny, and identified a few chromosomal regions, each with hundreds of genes that might influence ageing. Although these did not directly reveal genes involved in longevity, they suggested possible candidates. From this study, the scientists estimated that about 32% of variation in lifespan among turquoise killifish results from genetics, a figure comparable to the 20\u201335% estimated genetic contribution in mice 6 . From then on, the killifish's transformation into a valid research model accelerated. Anne Brunet, a geneticist studying ageing at Stanford University in California, had longed for a short-lived vertebrate and was delighted to hear about killifish when Valenzano visited Stanford for a summer course. She recruited him to her lab for a postdoc, and in 2006, Valenzano brought the killifish to California. There, he copied and modified protocols for zebrafish to transfer in foreign genes, starting with the green-fluorescent-protein gene from jellyfish 7 . In 2015, Brunet and her colleagues reported the successful use of CRISPR\u2013Cas9 gene editing in killifish, generating fish with mutations in 13 genes involved in key ageing events such as telomere shortening and mitochondrial dysfunction 1 . As enthusiasm for  Notho  grew, two groups tackled its genome sequence: Brunet's lab at Stanford, and Cellerino and collaborators at the Leibniz Institute on Aging\u2013Fritz Lipmann Institute in Jena, where Cellerino worked for a time and still maintains a cooperative group. Both groups published genome sequences 2 , 3  in December 2015. \u201cThe two papers are complementary,\u201d says molecular geneticist Matthias Platzer at the Leibniz Institute, who collaborates with Cellerino. Researchers from the teams now plan to make a consensus sequence. Beyond the genome, scientists are exploring which genes are transcribed into RNA and used for protein production during different stages in the life cycle. Platzer and his colleagues are interrogating messenger RNA molecules \u2014 the killifish transcriptome \u2014 to find out. To put together a transcript catalogue 8 , they sequenced RNA from killifish whole body, brain and skin, taken at a range of ages, from the embryonic period to 39 weeks old. Cellerino's team used similar techniques to track what happens in tissues from the same killifish as it develops. By taking small fin clips, they let the fish live long enough to be sampled again. They found that the transcriptomes of short- and long-lived killifish differ when those fish are only ten weeks old, and identified a protein that is a key controller of lifespan 9 . Because the killifish is not a mammal, linking fish genes to human ones will require a leap. Fish genes often have a human counterpart, but these can be difficult to find. This is in part because the killifish's ancestor underwent a whole-genome duplication: where human DNA has one copy of a gene, the killifish often has two. But at the Jena meeting, geneticist John Postlethwait of the University of Oregon in Eugene offered a potential solution. The trick, he explains, is to use an intermediate genome from another fish: the spotted gar ( Lepisosteus oculatus ). The gar's ancestors diverged from the killifish's before the duplication event, so its genome is in some ways more similar to that of a mammal. Scientists may be able to find a killifish gene's counterpart in the gar, and from there, find a match in people 10 . \u201cThe killifish work clearly is very innovative and potentially could be a really valuable model,\u201d says Matt Kaeberlein, a molecular biologist at the University of Washington in Seattle who studies ageing. But he is unsure how popular the fish could become, noting that its adoption will depend on how difficult it is to work with and whether killifish scientists can obtain sufficient funding. Ron Kohanski, programme officer at the US National Institute on Aging in Bethesda, Maryland, says that the agency is not funding killifish research, but is interested in the fish: \u201cThe killifish constitutes a good model for ageing on several levels,\u201d he says. \n               Blue thumbs \n             Yet the African fish has its disadvantages. For one, it's not as easy to keep in a lab as other fish, such as zebrafish. \u201cYou need to have a 'blue thumb',\u201d says Cellerino. \u201cYou need at least one person who is 100% of the time taking care of these fish.\u201d They also need more space than zebrafish, which thrive in crowded conditions; killifish males sometimes fight and might interfere with each other's growth. Because killifish develop so quickly, they eat a lot \u2014 and so produce a lot of waste, leading to water-quality challenges. \u201cOne of the things we joke about is we don't keep fish, we maintain biofilters,\u201d says Mickie Powell, a comparative physiologist at the University of Alabama at Birmingham. Killifish spawn readily; a couple can produce 20\u201340 eggs a day. But then things get tricky, because the eggs need to develop in a fairly dry place. Scientists often transfer the eggs to peat for a couple of weeks, but the eggs don't hatch at the same time, so require a watchful eye. Many researchers feed their killifish bloodworms, but the quality of that foodstuff varies with season and by supplier. Food matters, points out Powell, who is working on a standardized killifish food; for example, diet affects epigenetic markers that in turn influence longevity. She thinks that food choice might explain why some labs report different killifish lifespans. Researchers also need a better understanding of how to keep lab collections healthy. Brunet's lab was blindsided in 2008 when several fish started to act weirdly, rolling awkwardly instead of swimming straight. A veterinary surgeon diagnosed the parasite  Glugea , which the scientists suspect came in with other species of killifish that they bought from a fish store. \u201cThat was the lowest point,\u201d Brunet says. \u201cWe had to bleach everything and start from scratch.\u201d Scientists still hanker after tools that are easily obtainable for other model systems. Valenzano and Brunet wish for antibodies to study fish proteins, and Valenzano also dreams of more strains and a stock centre to provide them. These will probably come as the community of killifish researchers grows. That community is growing beyond those who study ageing, Platzer says. Developmental biologists are interested in the suspended animation, or diapause, that the eggs undergo, and evolutionary geneticists are intrigued by the killifish's use of XY chromosome sex selection. Many other fish use mechanisms such as population density, ambient temperature or ZW chromosomes, in which the egg, not sperm, determines offspring gender. The Jena meeting attracted scientists interested in using killifish to study epigenetics during blood formation, toxicology and shift-worker biology, says co-organizer Christoph Englert of the Leibniz Institute. Valenzano says that discussions among  Notho  researchers have shifted from tool development to biology. For example, in a study posted on the preprint server bioRxiv 11 , Cellerino and his colleagues describe how a microRNA involved in controlling excessive iron levels is upregulated in ageing killifish to protect the brain from iron accumulation. The human version of this microRNA is associated with Alzheimer's, a condition in which high iron levels have been implicated, he adds. \u201cThe fun part is just about to start,\u201d says Valenzano. \n                     Short-lived fish may hold clues to human ageing \n                   \n                     Pet dogs set to test anti-ageing drug \n                   \n                     Monkeys that cut calories live longer \n                   \n                     Medical research: Treat ageing \n                   \n                     Nature  Outlook: a collection of articles on the mechanisms of ageing \n                   Reprints and Permissions"}
]