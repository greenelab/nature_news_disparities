[
{"file_id": "444961a", "url": "https://www.nature.com/articles/444961a", "year": 2006, "authors": [], "parsed_as_year": "2006_or_before", "body": "Each stage in the building of a protein chip \u2014 expression, purification, immobilization \u2014 adds a layer of experimental complexity, as each feature may need its own optimization process to ensure consistent quality. \u201cWe thought we needed a better way to do this,\u201d says Joshua LaBaer, director of the Harvard Institute of Proteomics. The solution that he and his team arrived at was the 'nucleic acid programmable protein array' (NAPPA), in which cDNAs encoding GST fusion proteins are arrayed on chips alongside antibodies that recognize GST. The array is then subjected to cell-free transcription and translation; as protein is produced, it gets bound by an antibody and presented for analysis. According to LaBaer, NAPPA has simplified his group's research. \u201cYou don't have to purify proteins \u2014 you just purify DNA, so it's pretty easy, and it's been successful for printing about 95% to 96% of the things we make,\u201d he says. \u201cAnd when we do protein\u2013protein interactions, we're getting interactions that make sense and not a lot of false positives. Although initial arrays were limited in size, LaBaer and his team have since generated NAPPA arrays with up to 2,000 features, and they hope to surpass this soon. Other techniques even bypass DNA immobilization. In the protein  in situ  array (PISA) developed by Michael Taussig of the Babraham Institute in Cambridge, UK, cDNAs are amplified  in situ  with primers that encode polyhistidine tags, so that proteins can be captured on a nickel\u2013NTA-coated surface. More recently, Philipp Angenendt of the German Cancer Research Center in Heidelberg transferred the PISA principle to a microarray set-up, integrating cell-free production of histidine-tagged protein from unpurified PCR fragments with a multiple spotting technique (MIST) previously developed by his group. MIST uses automation to apply array reagents precisely and sequentially to specific spots. As a result, each transcription/translation reaction is confined to a tiny, sub-nanolitre droplet, allowing greater density \u2014 up to 13,000 spots at present. \u201cThe nice thing about it is that the proteins expressed remain in a liquid environment,\u201d says Angenendt, \u201cand the structure should be as intact as it can be with a solid-phase immunoassay format.\u201d Both Angenendt and LaBaer are now fine-tuning their processes. \u201cWe're really working on large-scale screens, doing biomarker and protein interaction studies,\u201d says LaBaer. \u201cWe've also got some preliminary enzymatic data that look promising.\u201d \n               Michael Eisentein \n             Reprints and Permissions"},
{"file_id": "4411183a", "url": "https://www.nature.com/articles/4411183a", "year": 2006, "authors": [], "parsed_as_year": "2006_or_before", "body": "Never mind Star Wars, lasers are good for more than burning and cutting, and in fact can be surprisingly gentle. Take \u2018optical tweezers\u2019, for example \u2014 a laser focused on a microscopic object creates an optical force trap that enables the precise manipulation of that object within a three-dimensional space. Since their development in 1986, optical tweezers have generated a veritable bounty of valuable information, including insights into the physical properties of DNA molecules and the fundamental mechanisms of various enzymes and molecular motors. They have also shown considerable promise for cell manipulation, although biologists have yet to fully explore their potential. \u201cWe have to educate people and make them comfortable with optical technology,\u201d says Kishan Dholakia, who heads the optical-trapping group at the University of St Andrews, UK. Many current users build their systems from scratch \u2014 a daunting and expensive prospect for the optics novice. To remedy this, Dholakia's team has lent its expertise to develop an entry-level, single-beam optical-tweezers workstation \u2014 the E3100 \u2014 for biologists looking to get their feet wet. The E3100 is available from Elliot Scientific in Harpenden, UK. Cell Robotics of Albuquerque, New Mexico, also offers an off-the-shelf system: LaserTweezers, an adjustable single-trap system that is designed for integration with a standard inverted microscope, and which can also be incorporated as a module of a larger workstation to allow computerized control and full automation. Single-beam traps enable impressive experiments, but the future clearly lies in higher-throughput platforms. \u201cThe technology has always been associated with one to ten particles,\u201d says Dholakia. \u201cI would like to see thousands of particles being ordered and sorted in a really rapid fashion.\u201d Enter Arryx of Chicago, Illinois; its BioRyx 200 system uses holographic technology to expand the number of tweezers simultaneously available to users. \u201cIt can generate up to 200 traps in a three-dimensional working volume, each of which is independently movable in real-time,\u201d says chief technology officer Dan Mueth. \u201cYou can pull on cells and sense or measure how they stretch, grab cells and move them around, probe the adhesion of cells, position cells for investigation, or isolate cells.\u201d The BioRyx 200 also offers a software interface that enables real-time manipulation or automation of the traps. But multiple traps are not the only benefit of holographic optical trapping. \u201cYou can select from a variety of trap shapes with different properties,\u201d says Mueth. \u201cThere are plenty of other advantages that are more subtle but can have an impact, and have to do with optimizing performance and the ability to work with particular samples.\u201d \n               M.E. \n             Reprints and Permissions"},
{"file_id": "4401238a", "url": "https://www.nature.com/articles/4401238a", "year": 2006, "authors": [], "parsed_as_year": "2006_or_before", "body": "Reprints and Permissions"},
{"file_id": "4401233a", "url": "https://www.nature.com/articles/4401233a", "year": 2006, "authors": [{"name": "Laura Bonetta"}], "parsed_as_year": "2006_or_before", "body": "Understanding the regulation of gene expression in stem cells and neurons, and finding ways to manipulate it, are important challenges. Laura Bonetta searches out the tools for the job. Researchers face a difficult task as they try to realize the therapeutic potential of stem cells and neurons. To better understand how to manipulate these cells, they need to monitor the gene-expression patterns, as well as working out how these genes are controlled. But both stem cells and neurons are not easy to maintain in culture, and it is hard to introduce DNA or RNA molecules into them to target specific genes or pathways. Nonetheless, researchers have been successful in determining the expression of thousands of genes and comparing expression patterns between different cells or cells grown under different conditions. Such work has allowed them to identify, for example, master regulators of stem-cell differentiation or neuronal survival. In addition, a wide variety of tools has been designed specifically for use in stem cells or neurons to control the expression of a gene of interest and study its function. One useful technique allows scientists to identify all the genes involved in a particular process, such as the migration or differentiation of stem cells. Serial analysis of gene expression (SAGE) is an open platform for monitoring the expression patterns of thousands of transcripts in one sample and can lead to the discovery of novel genes. The technique relies on the generation of a library of short cDNA \u2018tags\u2019 each corresponding to a sequence near the 3\u2032 end of every transcript in a cell or tissue sample. The tags are sequenced to reveal the identity and quantity of the corresponding transcripts. The National Institutes of Health's Cancer Genome Anatomy Project has generated several SAGE human embryonic stem-cell libraries and offers web-based tools to analyse the expression of genes in these libraries (see  \u2018Tools for expression analysis\u2019 ). To aid researchers with the construction of SAGE libraries, Invitrogen of Carlsbad, California, sells the I-SAGE and I-SAGE Long kits. A single SAGE experiment generates about 50,000 tags. But in mid-June Solexa in Hayward, California, will launch a sequence-based expression-analysis platform that can analyse more than one million cDNA tags. The Genome Analysis System uses a tag amplification step on the surface of a glass flow cell and features Solexa's Sequencing-by-Synthesis chemistry for automated sequencing. In principle, the number of tags sequenced in one experiment should be enough to provide very deep coverage of the transcripts expressed in a human cell and so should capture those expressed at very low levels, according to Solexa. Methods such as SAGE and the ever-popular microarrays look at the complement of transcripts isolated from a population of cells. Using a different method, based on gene-trapping technology, scientists at the Salk Institute for Biological Studies in La Jolla, California, have achieved real-time monitoring of gene expression in individual, living mouse neuronal stem cells. \u201cWhen you work with a therapeutically relevant cell system you want to keep it as close to its natural state as possible,\u201d says Carrolee Barlow, who helped devise the system. Barlow, who is now at Brain Cells in San Diego, California, and her colleagues created a library of stem cells, each with a single retrovirus randomly integrated in its genome. The virus was often integrated within or near a \u2018trapped\u2019 gene, and carried with it a reporter gene that gave off a fluorescent signal when expressed. By detecting fluorescence, the researchers could correlate the expression of a trapped gene with a specific phenotype. \u201cBy PCR or microarray you are looking at genes associated with a specific phenotype,\u201d says Barlow. \u201cBut by analysing expression in real time you can identify the genes actually driving those phenotypic changes.\u201d \n               Narrowing the search \n             Large-scale gene-expression screens usually result in a subset of genes that warrant further analysis, with the polymerase chain reaction (PCR) being the usual choice for the first round of follow-up studies. In particular, real-time PCR allows products to be detected as they are being made, which provides a quantitative measurement of expression levels. With some platforms, it is also possible to look at four or five gene targets in a single reaction tube, increasing throughput and lowering costs. For example, QIAGEN in Venlo, the Netherlands, sells a proprietary PCR buffer solution that increases the specificity of each primer for the respective target sequence, even when several different primer sets are present in a single reaction mix. \u201cIt does not matter what primer\u2013probe combinations you use; our product allows for specificity,\u201d says Kenneth Dwyer, marketing manager at QIAGEN. Seegene in Seoul, South Korea, has developed a primer based on dual specific oligonucleotides that allows the length of the primer sequence to be longer than traditional primers in real-time PCR, thereby increasing specificity. \u201cYou can use more than five sets of primers in one reaction tube and never have any problems,\u201d says Seegene's founder and chief executive, Jong-Yoon Chun. \u201cThere is no need for optimization.\u201d The product, called GeneXP, is sold in kits for studying the expression of specific gene families. For research that may lead to drug development, Gene Express in Toledo, Ohio, markets a technology known as  StaRT -PCR. Although similar in principle to many other quantitative PCR assays, \u201cthe key differentiation is the ability to have an internal standard\u201d, according to the company's chief executive Gerald Vardzel. The technology relies on a standardized mix of competitive cDNAs included in all the reaction mixtures, which allows numerical values to be assigned to gene-expression levels and for comparisons to be made across the drug-development pipeline. Gene Express provides the technology as a service through its Standardized Expression Measurement Center. SuperArray Bioscience in Frederick, Maryland, sells 96-well plates that include real-time PCR primer sets for different panels of pathway- or disease-focused genes. \u201cEven people in the field may not know all the genes related to a particular process. We have done the work for them in identifying the genes of interest,\u201d says Sean Yu, vice-president of operations. The company also sells cDNA and oligonucleotide microarrays for specific sets of genes, including one containing cell-type specific markers for human embryonic stem cells and another that carries representative markers for some of the neural phenotypes. The latter can distinguish between dopaminergic neurons, glial cells and pluripotent stem cells by their gene-expression profiles in a concentration-dependent manner. \u201cThe goal was to provide a practical test for stem-cell differentiation,\u201d says Yu. Along the same lines, OriGene in Rockville, Maryland, offers TissueScan Real-Time PCR panels to study gene expression in human and mouse tissues. Each array contains PCR-ready cDNAs normalized with beta-actin. For example, the Human Brain TissueScan array contains first-strand DNA from 24 tissues in the human brain. The company markets TissueScan Real-Time PCR disease panels for gene-expression analyses across many stages of disease. \u201cIt is prohibitively expensive to get hold of high-quality disease tissues,\u201d says Rich Hamer, vice-president for business development at OriGene. \n               Comparing expression \n             For comparing genes expressed by two different cells, for example an embryonic stem cell and one that has begun to differentiate, the usual method is differential display. Developed in 1992, the technology works by systematically amplifying portions of mRNAs from two or more samples and resolving the amplified products by denaturing polyacrylamide gel electrophoresis (PAGE). This allows sequence information to be recovered and corresponding cDNAs to be isolated for further molecular and functional characterizations. \u201cIt is especially good for samples with limited RNA or species that have not been sequenced,\u201d says Jonathan Meade, product manager for GenHunter in Nashville, Tennessee. GenHunter sells two kits for differential display: the RNAimage Kit uses radioactive detection, and the RNAspectra Kit uses fluorescence. Seegene's GeneFishing DEG (differentially expressed gene) discovery kit also relies on amplification with randomly designed primers. But this method uses annealing control primers, which are longer than primers used in differential display and so provide greater annealing specificity during PCR. The resulting PCR products are sufficiently long to be detected on an agarose gel. \u201cThe big advantage is that you can use agarose gels, which are easier to prepare than PAGE,\u201d says Chun. In most cases, microarrays and quantitative PCR technologies cannot assign a gene-expression status to a specific cell type. A better choice for this purpose is fluorescent  in situ  hybridization (FISH). This uses fluorescently labelled DNA oligonucleotide (or RNA) probes to visualize the expression of genes in a single cell with a microscope. Gene expression measured by FISH can be combined with other visual properties of the cell in a high-throughput, automated fashion (see  \u2018High-content screening\u2019 ). Aureon Laboratories in Yonkers, New York, has developed a method called peT-FISH (paraffin embedded tissue FISH) for detecting signals from nascent RNA molecules localized at the transcription site of genes being expressed. \u201cWe are looking at the presence of nascent RNA that correlates with early events of gene regulation in response to stimuli,\u201d says Paola Capodieci, one of Aureon's scientists. \u201cThe transcript is still in the nucleus and we don't know if it will become mRNA or protein. So we can see what happens at the beginning of the expression chain.\u201d So far, the method has detected up to five genes in a single cell. One of the advantages of this technology is that tissue is not destroyed. \u201cWe preserve the tissue morphology,\u201d says Capodieci. \n               Homing in on individual genes \n             To study the function of a gene, researchers typically alter its expression, either overexpressing it or knocking it out. A popular way to turn down the expression of specific genes is through RNA interference. This uses synthetic double-stranded RNA oligonucleotides known as small interfering RNAs (siRNAs), short-hairpin RNAs (shRNAs), which are expressed from a polIII promoter on a plasmid vector, or microRNAs (see  \u2018A micro perspective on stem cells\u2019 ). But slow-growing stem cells and neuronal cells are difficult to transfect with these molecules. One way to introduce plasmids or siRNAs into cells is to use lipid-based transfection products. These work by forming a complex with DNA or RNA that interacts with the cell membrane. Invitrogen's Lipofectamine 2000 works with many mammalian cells, and can introduce an siRNA corresponding to the transcription factor Oct-4 in stem cells. \u201cWe got about 80% delivery,\u201d says Peter Welch, director for research and development. Several companies have developed transfection reagents to introduce plasmids and siRNAs specifically into cells that are hard to transfect. Mirus Bio in Madison, Wisconsin, has the  Trans IT LT reagent for neuronal cell lines, which is a combination of an endogenous cellular protein, histone H1 and lipoamine. \u201cThe histone H1 is the primary cationic carrier and reduces the amount of lipid needed, reducing toxicity,\u201d says James Hagstrom, vice-president for scientific operations at Mirus. This year, Panomics in Fremont, California, will launch a peptide-based reagent called DeliverX, which it claims will have high efficiency and low toxicity. \u201cIt has been validated with one primary cell and this summer more primary and suspension cells will be added,\u201d says Ian Ley, the company's vice-president for marketing. When transfection agents fail, electroporation is the brute-force approach for delivering genetic material inside cells. The nucleofector technology marketed by Amaxa Biosystems of Cologne, Germany, is aimed at cell lines that are difficult to transfect. A series of gentle pulses allows transfected DNA directly to enter the nucleus. \u201cOther non-viral transfection methods rely on cell division for the transfer of DNA into the nucleus. Nucleofection provides the ability to transfect even non-dividing cells such as neurons and resting blood cells,\u201d says spokeswoman Kimberly Stevenson. Depending on the cell type, a researcher would choose one of many proprietary nucleofector solutions and program the machine with the appropriate parameters. For experiments that require stably transfected cells or  in vivo  work, viral vectors are the obvious choice (see  \u2018Hitching a ride on a virus\u2019 ). Regardless of the method of gene expression used, some steps in the process will probably require growing cells in culture, using formulated liquid media supplemented with growth factors and other substances that promote cellular replication and govern differentiation. The conditions and the reagents used may affect the gene-expression programmes of a cell, a concern that is particularly relevant to stem-cell research. \u201cFor the stem-cell field to move forward it is necessary to have standardized reagents and assays to allow results to be compared,\u201d says Sharon Louis, senior scientist at StemCell Technologies in Vancouver, Canada. \u201cWhere we see ourselves is providing standardized tissue-culture reagents.\u201d The media reagents provided by the company come with detailed protocols for either maintaining cells in an undifferentiated pluripotent state or for inducing differentiation towards a specific lineage. Each in their own way, stem cells and neuronal cells present problems for the scientists who want to manipulate them. But many scientists and companies have risen to this challenge by developing increasingly sophisticated tools and technologies. With this tool kit in hand, studying gene expression in stem and neuronal cells is a realistic goal that has been embraced by researchers with an enthusiasm matching the promises of their field. Reprints and Permissions"},
{"file_id": "4411181a", "url": "https://www.nature.com/articles/4411181a", "year": 2006, "authors": [], "parsed_as_year": "2006_or_before", "body": "Researchers have used electrical fields to manipulate nucleic acids and proteins for more than 50 years, but similar systems have only recently begun to emerge for working with whole cells. Dielectrophoresis is nevertheless quickly gaining appeal as a basis for microfluidic cell-sorting. Hyongsok (Tom) Soh and Patrick Daugherty at the University of California, Santa Barbara, recently demonstrated the feasibility of dielectrophoresis-activated cell sorting, or \u2018DACS\u2019, with a microfluidic chip that uses dielectrophoretic forces to steer bacteria tagged with polystyrene beads into a collection channel (X. Hu  et al .  Proc. Natl Acad. Sci. USA  102, 15757\u201315761; 2005). Initial experiments showed that one round of DACS could achieve more than 200-fold enrichment of a rare subpopulation of cells at rates of 10,000 bacterial cells per second. They initially tested a single-stage, single-channel device, but Soh believes DACS is ideal for parallel operations. \u201cIt is relatively straightforward to design cascaded, sequential sorting stages that operate in parallel,\u201d he says. \u201cThis allows high purity and cell recovery without sacrificing throughput.\u201d Soh is quick to point out that DACS is in no position to usurp fluorescence-activated cell sorting, because of the binary nature of its sorting mechanism; but it shows great promise for high-throughput screening, he says. \u201cWe just completed screening the first molecular library and performed epitope mapping with DACS,\u201d Soh explains, \u201cand we've shown that it can be faster, cheaper and simpler than commercial assays.\u201d Evotec Technologies in Hamburg, Germany, is also taking advantage of dielectrophoresis for its Cytocon 400 system. The key to this is the CellProcessor microfluidic chip, which contains a three-dimensional array of electrodes that allow users to design and control electrical-field configurations for cell manipulation. \u201cWe developed the CellProcessor platforms for precise and fully automated sorting in a microfluidic environment,\u201d says Gabriele Gradl, Evotec's vice-president of cell handling and analysis. \u201cThe underlying technology makes cell analysis and isolation reproducible and predictable down to the single-cell level.\u201d The resulting platform allows for the delicate manipulation of small numbers of cells, in which the gentle handling provided by combining dielectrophoresis with hydrodynamic flow can be useful. \n               M.E. \n             Reprints and Permissions"},
{"file_id": "4411179a", "url": "https://www.nature.com/articles/4411179a", "year": 2006, "authors": [{"name": "Michael Eisenstein"}], "parsed_as_year": "2006_or_before", "body": "A key element of performing good cell-biology experiments is starting with exactly the right cells. Michael Eisenstein takes a look at the technologies that can make this possible. What do you get when you cross an ink-jet printer with a Coulter counter? It's not a riddle; scientists at Los Alamos National Laboratory in New Mexico asked the question 40 years ago, and the answer turned out to be the cell sorter. Los Alamos researcher Mack Fulwyler created a prototype in 1965, which used vibration to generate tiny droplets from a jet of solution containing red blood cells; the individual cells in each droplet could then be subjected to rapid volumetric analysis and sorting. Fulwyler's prototype came to the attention of Stanford University researcher Leonard Herzenberg. \u201cI was working on immunofluorescence in immunology and genetics,\u201d he says, \u201cand had realized that there was a need for the means to sort cells according to the molecules they display on their surface.\u201d Herzenberg and his colleagues adapted Fulwyler's design to produce an instrument that could sort cells depending on the presence or absence of molecules identified by fluorescent labels \u2014 the first example of fluorescence-activated cell sorting (FACS). Today, FACS has not only aged gracefully but is arguably in its prime, embraced by nearly everybody looking to pluck specific cells out of complex mixtures. Herzenberg's patent was licensed by Becton Dickinson (BD), which developed the first commercial instrument and currently holds the trademark for the term \u2018FACS\u2019. Early FACS fans include Dutch researcher Gerrit Van den Engh, whose refinements enhanced sorting rates. \u201cI designed a different, digital parallel post-processing scheme,\u201d says Van den Engh. \u201cBy digitizing the signal as quickly as possible, we could take the cells in parallel and we could put an error-tracking code on the information, so that we could check whether events were being dealt with properly.\u201d Cytomation, later acquired by Dako in Glostrup, Denmark, made Van den Engh's patent the foundation for MoFlo, the first high-speed commercial sorter. MoFlo pushes the speed envelope, sorting up to 70,000 objects per second \u2014 although researchers often use lower rates to optimize recovery and sort accuracy. Dako has also ensured that new expansions and components are suitable for use both with current and older systems. \u201cOur modern upgrades are reverse-compatible, even with MoFlo machines from eight or nine years ago,\u201d says Cytomation's founder, George Malachowski. Meanwhile BD Biosciences, a segment of BD in San Jose, California, recently introduced its most advanced cell sorter to date. The BD FACSAria benefits from its small size, being one of the few bench-top systems on the market, and from an alternative approach to pre-sorting analysis. Most sorters use the \u2018jet-in-air\u2019 approach, in which optical analysis is performed on a rapidly moving stream of fluid, but the BD FACSAria instead uses a sorting flow-cell. \u201cThis gives you the much higher sensitivity that you need, but maintains extremely efficient sorting,\u201d says marketing director Tony Ward. \u201cAnd you can sort cells that have lower levels of antigen expression than you might be able to see using a jet-in-air approach.\u201d Several alternative systems are available. Beckman Coulter of Fullerton, California, another early entrant into the field, offers its EPICS ALTRA, an established platform for cell sorting. In 2000, Van den Engh launched a new company, Cytopeia, based in Seattle, Washington, whose inFlux Cell Sorter is based on ideas from his academic work. \u201cIt's an open system, so you can have access to all the modules and can configure it freely for whatever experiment you want to do,\u201d he says. \u201cWe're not competing with the other manufacturers for well-established applications; we work with the 10\u201320% of researchers who have applications that are not done as well on the other machines.\u201d And for researchers working with larger objects, Union Biometrica of Holliston, Massachusetts, offers a FACS-like platform for sorting embryos and multicellular clusters (see \u2018The gentle touch\u2019,  page 1179 ). Most observers agree that cell sorting has probably reached its speed limit, and some scientists are now looking to expand the breadth of flow cytometric analysis and sorting. Mario Roederer of the US National Institutes of Health (NIH) in Bethesda, Maryland, has been a leader in this regard, combining fluorescent dyes and quantum dots to perform experiments involving simultaneous analysis of up to 17 different intracellular and cell-surface markers. Many cell biologists have yet to explore these outer limits, but current commercial sorters can typically accommodate optics for analysing a dozen or more fluorescent parameters. As an immunologist examining very specific cell subtypes, Roederer finds this flexibility invaluable: \u201cWe're routinely doing 12- or 15-colour flow-cytometry to try to look for important subsets or functions. In the end, I'm hoping we'll be able to reduce it to a 4- or 5-colour assay with the correct combination of markers.\u201d With all the power that these cell-sorting systems offer, there are still problems to be resolved. \u201cSoftware is the issue that requires the most effort,\u201d says Roederer. \u201cWe need tools that can automate the discovery or the analysis of subsets of cells that are present in complex data sets.\u201d Ward agrees: \u201cThe faster you count particles, the more data get generated and the resulting high degree of data complexity and intersections mean that current approaches to software can be limiting.\u201d Both Roederer and Herzenberg have worked to address this, developing two commercially available software packages, FlowJo and FacsXpert, intended to improve the quality of cell-sorting analysis. Another big factor for many is cost: power and efficiency don't come cheap, and access to high-end machines may be restricted to limited slots in a shared core facility. \u201cI'd like to see cheaper machines that give you five, six or seven colours but that are much less expensive than the mammoth machines,\u201d says Herzenberg. Nevertheless, these instruments receive strong acclaim from users. \u201cI don't want to say it's a way of life,\u201d says Roederer, \u201cbut it is a way of biology.\u201d \n               Working in bulk \n             Sometimes, however, all a scientist needs is a way to separate two groups of cells quickly. \u201cFACS can do pretty much everything, but it's expensive,\u201d says Steven Woodside, a scientist with StemCell Technologies of Vancouver, British Columbia. \u201cIf you want to do more bulk separations, then immunomagnetic separation is a really good option.\u201d The principle is simple. Cells are incubated with paramagnetic beads tagged with antibodies, after which a magnet or array of magnets can be used to either purify cells of interest or remove unwanted cells. Dynal Biotech's Dynabeads \u2014 currently available through Invitrogen in Carlsbad, California \u2014 were among the first such commercial products, and remain a popular option. Miltenyi Biotec, based in Bergisch Gladbach, Germany, also offers reagents for performing column-based \u2018magnetic-assisted cell sorting\u2019 (MACS) using its MACS Separator. This is available in an automated version to simplify the purification process. StemCell's offerings for magnetic separation include the EasySep system, which uses a specially designed high-gradient magnet to separate nanoparticle-labelled cells. These paramagnetic particles are particularly small, to an extent where they will not interfere with flow cytometry performed on purified cells. StemCell also offers an automated version, the RoboSep, which gives users a variety of options for configuring separation protocols. Magnetic separation's usefulness is limited by the inherent constraints on the number of sort parameters and its reliance on cell-surface antigens for sorting. Nonetheless, it excels at applications in which bulk separations need to be performed quickly, or as a prelude to more extensive cell-sorting procedures. \u201cIn a lot of cases for immunology research, the level of purity that you get with magnetic separation is more than adequate,\u201d says Woodside. \u201cBasically, people want to do the fewest steps possible and get the purest cells back \u2014 that's what's driving this technology.\u201d \n               Bridging the gap \n             With all the interest in optimizing the efficiency and cost of cell sorting, it is understandable that James Leary, head of the molecular-cytometry facility at Purdue University in West Lafayette, Indiana, is disappointed at the chilly reception that microfluidic platforms tend to receive in the community. \u201cFlow cytometry has had microfluidics at its core for 40 years,\u201d he says. \u201cBut it's interesting, because the flow-cytometry groups and the microfluidics groups don't talk to each other very much. If they did, progress would be a whole lot faster.\u201d Leary, a cell-sorting pioneer, is doing his part to bridge that gap through collaborations with colleagues such as Rashid Bashir, from his university's Birck Nanotechnology Center. The two see great advantages at the microscale, including portability, disposability and improved biosafety for handling pathogenic samples. They are exploring next-generation sorting technologies such as dielectrophoresis, in which a nonuniform electrical field is used to separate charge-neutral objects based on their size or chemical properties (see \u2018Playing the field\u2019,  page 1181 ). \u201cThe dielectrophoretic approach is attractive because it is electrical, it is integrated and you don't have to have lots of mechanical valves,\u201d explains Bashir. \u201cMoving the cells, instead of the fluid, makes more sense.\u201d The use of laser light for cell manipulation is well-established (see  \u2018The guiding light\u2019 ), and lasers are now being exploited for cell sorting. Kishan Dholakia, leader of the optical-trapping group at the University of St Andrews, UK, recently developed a system in which two- or three-dimensional patterns are generated by an optical-tweezers laser to create a \u2018passive\u2019 cell-sorting matrix. \u201cWe put cell samples on to an optical corrugation or landscape,\u201d he says, \u201cand this light pattern acts rather like an optical sieve.\u201d This proved effective for sorting lymphocytes from erythrocytes by size and shape, although Dholakia is still exploring how this might be used to sort cells with subtler internal differences. Stanford University's Stephen Quake tested a more active sorting approach for his \u00b5FACS chip, integrating optical detection with electroosmotic flow manipulation; when cells of interest are identified at a detection \u2018window\u2019, fluid flow at an adjacent T-channel is switched to divert the cell for collection. \u201cWhat we have been using these cell sorters for is not as independent stand-alone things, but as an integrated component of a more complicated microfluidics system,\u201d explains Quake. His team has also developed micromechanical valve-based chips that function as part of a larger platform for single-cell genetic analysis. Micromechanical valves are also the foundation of a chip developed by Innovative Micro Technology (IMT), a manufacturer of microelectromechanical systems (MEMS) based in Santa Barbara, California. IMT's rare-cell purification system (RCPS) is designed to purify stem cells from patient samples for transplantation, and chief executive John Foster suggests that its small size and disposability could make it ideal for clinical settings. RCPS chips feature 32 parallel channels that use tiny valves, optics and electromagnetic actuation to divert cells for rapid collection following detection of appropriate fluorescent markers; initial results have been promising. \u201cWe've shown that the human cells that we're sorting survive and reproduce, and we've got the speed, sterility, ease of use and disposability required for human-cell therapies,\u201d says Foster. Microfluidics remains a hard sell for many, a niche dominated mainly by \u2018do-it-yourselfers\u2019, who design and build chips from scratch or with the help of companies specializing in biological microfluidics. Among other reasons, the perceived speed sacrifice of going micro remains a deterrent. But Bashir cautions that \u201cthe emphasis on speed is overplayed\u201d. Leary agrees. \u201cWe can do multiple channels and we can do sorting and re-sorting in a continuous-flow device instead of a droplet-based device \u2014 microfluidics can provide many other features,\u201d he says. In the end, the biggest problem may just be getting people to see an old problem in a new way. \u201cIt's hard to convince people, when there are already FACS machines that work very well, that they need another FACS,\u201d concludes Quake. \u201cBut people will start to use microfluidic FACS for all kinds of creative things if it's out there as a low-cost, personal alternative.\u201d \n               Laser precision \n             The preceding systems offer a wealth of options for working with cells in suspension, but many scientists face the need to work with especially \u2018fresh\u2019 cells. \u201cThe process of disaggregating and sorting cells, and treating them over a time period will profoundly change the expression profile or protein-signalling pathways,\u201d explains Lance Liotta, co-director of the Center for Applied Proteomics and Molecular Medicine at George Mason University in Manassas, Virginia. In the mid-1990s, while working as a lab chief at the NIH, Liotta's frustration with mechanical methods for microscopic tissue dissection led his team to develop laser capture microdissection (LCM), a quick and precise system for excising cells from fixed tissue samples or even live adherent cultures. The patents from this work were developed by Arcturus Bioscience, and two descendents of the invention \u2014 PixCell and Veritas \u2014 are available from Molecular Devices of Sunnyvale, California. PixCell is a simpler, microscope-based platform for the manual dissection of cells, whereas Veritas offers a fully automated alternative for performing LCM. Both systems use a gentle near-infrared laser to partially melt an adherent layer of polymer film over selected cells; these cells can then be mechanically transferred with the film for analysis or further culture. Veritas also features a more powerful ultraviolet laser for rapidly cutting larger groups of cells or working with more difficult tissues or live cells. Laser microdissection systems from Molecular Machines and Industries (MMI) of Glattbrugg, Switzerland, use a \u2018sandwich\u2019 approach in which cells or tissue samples are prepared between a layer of microscope slide glass and a polymer membrane; selected sections are cut with a brief laser pulse and then recovered with specialized collection tubes with adhesive caps. \u201cThe big advantage is that it's totally contamination free,\u201d explains Stefan Niehren, a senior development engineer at MMI. MMI offers two systems, the smaller and simpler SmartCut, and the CellCut, which offers full automation and can be expanded by integration with other MMI components, such as the CellManipulator optical-tweezers system. Optical trapping and manipulation is also a feature of the CombiSystem, one of the PALM microlaser systems made by Carl Zeiss of Bernried, Germany. PALM systems use a non-contact process called laser microdissection and pressure catapulting, in which cells or tissue sections are precisely cut with a UVA laser and then catapulted into a collection vessel by a single beam pulse. PALM systems can be integrated with Zeiss microscopes and software for automated recognition and isolation of individual cells. \u201cThere are studies out now in which the PALM system was used to isolate an individual cell's clonal expansion, or to separate single embryonic stem-cell clones from other clones,\u201d says Richard Ankerhold, managing director of Zeiss subsidiary PALM Microlaser Technologies. Leica Microsystems of Wetzlar, Germany, is another imaging specialist offering a platform for laser microdissection, the LMD6000, which has a powerful diode laser for precise cutting of thicker tissue samples and comes integrated with an automated research microscope. Although these systems were first developed with fixed tissues in mind, most will also work with live cells, an area Liotta believes will grow in importance. \u201cThe exciting thing will be to actually do this with embryonic tissue or biopsies of living tissue from surgery,\u201d he says. \u201cWe'll see advances in molecular staining, stabilizing and extracting tissue macromolecules and being able to work with a thick piece of living tissue.\u201d \n               Increasing options \n             The longevity of the fluorescent cell sorter is a clear testament to its power, but subsequent years have also shown a need for complementary methods that can be applied for more specialized experiments \u2014 for example, sorting through dozens or hundreds rather than millions of cells, or plucking a handful of cells from a slice of brain tissue. Meanwhile, growing interest in stem-cell isolation, clinical cell sorting and single-cell analysis are fuelling the drive to develop microscale cell sorters. Even though this field is still in its infancy, specialists in industry and academia are coming to recognize that microfluidic systems could one day handle many of the tasks now reserved for FACS. With their speed and proven reliability, it seems clear that modern cell sorters will remain cell-biology monarchs for some time \u2014 but they must also make room for what looks to be a growing court. Reprints and Permissions"},
{"file_id": "4391018a", "url": "https://www.nature.com/articles/4391018a", "year": 2006, "authors": [], "parsed_as_year": "2006_or_before", "body": "Structural genomics, or structural proteomics, aims to provide three-dimensional information for all proteins. This information can be used to ascribe function to a protein and to reveal or invalidate drug targets. There are structural proteomics projects in both the public and private sectors around the world, including Germany, Japan and the United States. In 2000, the USNational Institute of General Medical Science in Bethesda, Maryland, launched an initiative to determine the structures of about 10,000 proteins representing different structural (\u2018fold\u2019) families over the next decade. The studies will result in a public resource linking amino-acid sequence, structural and functional information, eventually allowing scientists to make the three-dimensional atomic structures of most proteins easily obtainable from their corresponding sequences. One of several participating centres is the Genomics Institute of the Novartis Research Foundation in San Diego, a member of the Joint Center for Structural Genomics (JCSG). Scott Lesley (pictured below, left), who heads the institute's proteomics unit, says his group has processed 50,000 protein samples from more than 20,000 unique expression clones. The pipeline starts with a small screen protocol that uses samples in a 96-well format to evaluate how the targets behave at different steps in the procedure. \u201cThose that are not behaving well after expression, purification or crystalization are set aside and then run through one of several salvage pathways,\u201d says Lesley. Such pathways include using different expression systems (vectors and cells), making point mutations in the cDNA to increase the likelihood it will be expressed, and trying different tags. \u201cAll the bacterial expression and purification is done in parallel and the majority is automated by custom instrumentation,\u201d says Lesley. The targets that perform well are applied to a large-scale purification protocol to yield 10\u201320 mg of protein. The JCSG has already deposited 239 structures in the protein database \u2014 mainly from the bacterium  Thermotoga maritima , the mouse, and the yeast  Saccharomyces cerevisiae  \u2014 representing at least 19 new fold families. \u201cMost of the tools are in place, and we can increase both output and success rate by implementing salvage approaches in high throughput,\u201d says Lesley. Similar efforts are underway in other countries. The Protein Structure Factory in Germany, an initiative of the German Human Genome Project and structural biologists from the Berlin area, targets human proteins for structure determination. \u201cOur current focus is to determine the structures of particular protein\u2013protein or protein\u2013ligand complexes,\u201d says Christoph Scheich of the Max Planck Institute for Molecular Genetics in Berlin. \n               L.B. \n             Reprints and Permissions"},
{"file_id": "4421067a", "url": "https://www.nature.com/articles/4421067a", "year": 2006, "authors": [{"name": "Michael Eisenstein"}], "parsed_as_year": "2006_or_before", "body": "Doubt is often cast on the reliability of DNA microarrays, but resources are becoming available to help researchers overcome many of the problems inherent in this technology. Michael Eisenstein reports. It is more than 15 years since DNA microarrays were developed, and in that time they have been adored, attacked and, in an effort to look beyond the hype, appraised. One outcome of this 'soul searching' has been the realization that the flaws inherent to gene-expression arrays are similar to those of other high-throughput platforms. \u201cI don't think microarrays are different from other technologies in that respect, and it's important for people to keep that in mind,\u201d says Janet Warrington, vice-president of molecular diagnostics and emerging markets research and development for Affymetrix, based in Santa Clara, California. \u201cI try to point out 'microarray exceptionalism' wherever I find it.\u201d John Quackenbush, a computational biologist at the Harvard School of Public Health, sees several fundamental errors in the way many researchers tackle microarray gene-expression studies. \u201cPeople tend to go out blindly and do experiments, then go back and try to analyse them and figure out what the question is afterwards. I think that's the first thing you have to avoid,\u201d he says. \u201cIt's also important to make sure you remove confounding factors from the experiment wherever possible.\u201d Systematically sorting out sources of error can be a daunting process, but a recent series of investigations by people such as Quackenbush into cross-experimental, cross-platform and cross-laboratory variability between array experiments has helped clarify some issues that were preventing comparisons being made between experiments. Several multi-institutional projects are now under way to develop more reliable experimental protocols and controls (see  'Standards and practices' ). Meanwhile, many scientists, manufacturers and programmers are working to develop practical tools that could help eliminate unwanted variability from experiments and analyses. The biggest problems often occur early on. Existing kits for RNA preparation are effective, but many are designed for a 'best-case scenario': large amounts of fresh biological source material. Many researchers are now interested in studying gene expression in a small number of cells, necessitating efficient systems that can work with limited samples. Various systems have been developed, many of which are based on a linear-amplification procedure known as the Eberwine method. Labelling strategies typically fall into two main categories: direct and indirect. Direct methods, used in systems such as CyScribe, available from GE Healthcare Life Sciences of Little Chalfont, UK, and ChipShot, from Promega of Madison, Wisconsin, typically involve the incorporation of fluorescent-dye-conjugated nucleotides during complementary DNA (cDNA) synthesis. One favoured indirect-labelling strategy involves incorporating an aminoallyl-modified nucleotide into cDNA or amplified RNA transcripts, and then labelling these with chemically functionalized fluorescent dyes. EPICENTRE Biotechnologies in Madison, Wisconsin, has incorporated this approach into its TargetAmp kits for RNA amplification. The company says these kits can deliver up to 5-million-fold amplification, and even allow the study of single cells. \u201cWe can get down to 10 picograms of starting RNA,\u201d says Shervin Kamkar, a technical-sales specialist at the firm, \u201cso it's really useful for people doing stem-cell work or laser capture.\u201d \n               Designer labelling \n             Genisphere of Hatfield, Pennsylvania, uses a unique labelling approach for its 3DNA kits that is based on fluorescently tagged DNA-based dendrimers. These contain sequences complementary to 'capture' sequences added to cDNA during sample amplification. Different dendrimers are available with varying quantities of linked fluorophore molecules that determine the limits of detection, down to less than a microgram of starting material. Detection can be further augmented with the company's SenseAmp kits, which use one or two rounds of a non-Eberwine amplification strategy to produce sense-strand RNA. \u201cWe've gone as low as 0.1 nanograms of total RNA,\u201d says Bob Getts, Genisphere's director of research and development, \u201cwhich is typically from ten cells.\u201d Another problem is posed by the increasing use of microarrays for analysis of clinically prepared formalin-fixed paraffin-embedded (FFPE) samples, such as tumour biopsies. These may have been in storage for anything from months to decades, and the resulting RNA degradation is a serious challenge for reagent designers. Several companies, including EPICENTRE and Genisphere, are working on this. According to Getts, SenseAmp is well-suited to FFPE work. \u201cWe routinely use samples that have been degraded to between 50 and 250 bases long,\u201d he says. Array manufacturer Illumina of San Diego, California, offers an alternative with the DNA-mediated annealing, selection, extension and ligation (DASL) assay. This is an adaptation of its GoldenGate genotyping technology that takes advantage of a universal array consisting of large numbers of specific tag sequences in order to quantify PCR-amplified primer-extension products. \u201cIt's been shown to work on samples as old as 20 years,\u201d says Shawn Baker, Illumina's scientific product manager for gene expression. Still further optimization will be needed as researchers continue to target smaller and more biologically relevant specimens. \u201cThere's just so much variation in the material that you're given,\u201d says Kamkar, \u201cand it's hard to know how effective a technology is until people have tried your approach on their own samples.\u201d When asked about concerns regarding microarray experimental reliability, many in the field are quick to defend the hardware. \u201cThe microarray instrument itself, if used correctly, is precise and accurate,\u201d says Rafael Irizarry, a biostatistician at the Johns Hopkins University in Baltimore, Maryland. \u201cThe quality of commercial arrays is improving, whereas their price is dropping, so commercial arrays are supplanting the 'home-brew' approach more and more,\u201d says Quackenbush. Different manufacturers have opted for various strategies to improve experimental quality and to minimize opportunities for human error (see  'Hands off!' ). John Blume, vice-president of assay and application product development at Affymetrix, cites ever-increasing probe density and genome coverage as a secret of Affymetrix's success. Their Human Genome U133 Plus 2.0 GeneChip microarrays feature 11 different oligonucleotide probes for each transcript, which confer a number of benefits. \u201cAs annotation shifts, people's bets on which sequence is useful for a gene can prove wrong,\u201d says Blume. \u201cThis design philosophy of multiple sequence probes per gene provides a buffer that single sequences cannot.\u201d It also protects against unexpected glitches at the hybridization stage. In addition, Affymetrix offers broadened coverage through its genomic-tiling arrays and, more recently, new exon arrays that allow users to assemble detailed, genome-wide exon-usage profiles for human, mouse and rat studies. \n               Probing for answers \n             Illumina also uses redundancy to maintain experimental quality control in its bead-based arrays. \u201cEach of our arrays has about 30 replicates of each probe,\u201d says Baker. \u201cBecause these 30 measurements are spread randomly across the chip, we don't have to worry about little things like smudges on the array \u2014 any outlier measurements get removed.\u201d These arrays also benefit from the combination of high probe density with the inclusion of multiple arrays on a given chip. This allows users to simultaneously profile a number of samples \u2014 up to 96 parallel arrays \u2014 in an 'array of arrays' format. Agilent Technologies of Santa Clara, California, touts the use of long probes \u2014 60mers, compared with Affymetrix's 25mers \u2014 as an advantage for enhancing sensitivity to low-abundance transcripts, typically a weakness for microarray platforms. Agilent's instruments incorporate a multiple-scan approach, further extending the sensitivity of detection. \u201cYou can look at a broader range of transcripts and still get linearity with regard to the signal recorded,\u201d explains Kevin Meldrum, director of genomics marketing. Agilent has also incorporated proprietary 'spike-in' controls into its platform, which allow monitoring of experimental quality. An efficient and cost-effective production process gives NimbleGen Systems of Madison, Wisconsin, particular flexibility in the generation of its arrays. These combine a maskless photolithography method with a proprietary chemical process for efficient and accurate  in situ  synthesis of high-density probe arrays. The company's latest generation chips contain more than 2 million probes. NimbleGen also favours the use of long, typically 60mer, probes. \u201cWe are the only company that combines long oligomers with high density,\u201d says vice-president of business development Emile Nuwaysir. NimbleGen's rapid production process also allows it to continually update its probe sequences to align with the latest genome-annotation data. Affymetrix is currently taking advantage of this process for the production of NimbleGen-manufactured NimbleExpress custom GeneChips. A relatively recent entrant into the gene expression array field, Applied Biosystems of Foster City, California, has used years of experience in genomic work \u2014 and access to the proprietary genome databases of Celera Genomics, based in Rockville, Maryland \u2014 to good advantage in the design of its oligonucleotide arrays. \u201cWe've basically front-loaded all of the bioinformatics work,\u201d says staff scientist Chris Streck. \u201cWe do all the curation and annotation of these particular genes, and we make sure we have the most comprehensive and complete view of the genome to begin with.\u201d Applied Biosystems also benefits from a chemiluminescence-based approach to detection, with considerably reduced background noise relative to standard fluorescent systems. \n               The number crunch \n             However, high-quality samples and high-tech instrumentation alone won't save the microarray experiment. Some of the most fundamental challenges lie in gleaning biological significance from mounds of data and designing experiments with a statistically sound foundation. David Allison, a biostatistician at the University of Alabama at Birmingham, remembers the early days of microarray work with horror. \u201cThe sample sizes were way too small, unjustified statements were made, and the analyses were primitive,\u201d he says. Fortunately, he adds, \u201cthe field recognized this, and a lot of people started weighing in with their own methods\u201d. According to Irizarry, an important first step for good analysis is the effective pre-processing of raw data, using algorithms that accurately convert spot fluorescence to gene-expression estimates. \u201cChanging those algorithms can make a difference,\u201d he says, \u201cand you can turn an experiment that looks so-so into something that looks powerful and precise.\u201d Irizarry has also called attention to the importance of data normalization, and designed an online tool, Affycomp II, which allows users to benchmark their normalization methods using 'known' data sets from Affymetrix GeneChip experiments, and makes those benchmark results publicly available \u2014 extending an ongoing trend in the community of increasing data sharing (see  'Share and share alike' ). Most major chip and instrument manufacturers also market software packages with which to analyse raw microarray data. Agilent offers the GeneSpring suite, whereas Illumina has developed BeadStudio, which is specifically designed for its array format and can also interact with other analytical tools. Affymetrix distributes a variety of programs, and has also established its 'GeneChip Compatible' program with various other companies. \u201cIt's not practical for us to be experts at everything\u201d, says Blume. \u201cBy working with our partners, we can provide better solutions for a more diverse range of users.\u201d The open-source movement has also taken firm hold in this field, and a particularly strong contributor has been the Bioconductor program, now in its fifth year. Bioconductor was launched to make high-quality, community-developed and community-tested tools for statistical analysis freely available. The foundation language for Bioconductor is 'R', an optimal choice for statistical analyses. \u201cWhen a statistician develops a method and wants people to use it, he or she will carefully create software for people to implement this method in R,\u201d explains Irizarry. \u201cAnd now, any method that's good, that people like and want, will be implemented in R and made available in Bioconductor.\u201d \n               Pathfinders \n             Unfortunately, Bioconductor can be difficult for scientists lacking programming skills to use effectively. In an effort to bring R's analytical capabilities to these users, Quackenbush's group has developed the TM4 suite. \u201cThese programs are the biologist-friendly version of what people are doing at Bioconductor,\u201d he explains. A user-friendly solution is also offered by Insightful of Seattle, Washington, in the form of its S+ArrayAnalyzer software, which ports the complete set of Bioconductor statistical tools. But this is just scratching the surface, and the variety of analytical tools available can be confusing. \u201cWe're facing too many options for analysing the same data set,\u201d says Leming Shi of the US Food and Drug Administration in Rockville, Maryland, \u201cand there has not been adequate scientific vetting of the capabilities and limitations of available methods.\u201d Scientists may cringe at the effective long-term solution to this problem \u2014 acquiring a solid background in practical statistics. Many in the field recognize that biologists have an unfortunate tendency to 'plug and play' in analytical methods without understanding the underlying principles, which results in misuse of otherwise effective strategies. Ultimately, good maths may become the key to good science. \u201cSoon, you're probably not going to be able to say that you're a molecular biologist if you don't understand some statistics or rudimentary data-handling technologies,\u201d says Blume. \u201cYou're simply going to be a dinosaur if you don't.\u201d Of course, the objective of microarray experiments is not to generate endless spreadsheets and scatter-plots, but to produce data that can be used to formulate an understanding of biological events. This requires a way to predict the impact of gene-expression shifts on networks of interacting gene products, and this, in turn, requires detailed databases in which the function and behaviour of these individual gene products has been accurately defined and annotated. Several such databases now exist, thanks to projects such as Gene Ontology (GO) and the Kyoto Encyclopedia of Genes and Genomes (KEGG). These resources serve as the foundation for a number of different tools for 'second-order' microarray analysis. Some of these, such as Gene Set Enrichment Analysis (GSEA), which attempts to identify significant shifts in sets of interacting or associated gene products, or GenMAPP, a software tool for the assembly of interactive, graphic maps of biological pathways based on gene-expression-array data, are academic in origin and freely available. Several others, such as Ingenuity Pathways Analysis, from Ingenuity Systems of Redwood City, California, PathwayStudio, from Ariadne Genomics of Rockville, Maryland, and BiblioSphere, from Genomatix of Munich, Germany, have been commercially developed. Ingenuity's efforts at pathway assembly complemented GO and KEGG with a large-scale 'knowledge base' that was assembed, from scratch, for its Pathways Analysis software. \u201cIt took us about four and a half years to reach critical mass in terms of ontology and content,\u201d says chief technology officer Ramon Felciano, \u201cand then go through a decade's worth of literature and manually structure millions of pathway relationships using those ontologies.\u201d This knowledge base now incorporates definitional data from GO as well as a number of user-uploaded pathways defined from new or unpublished experimental data, and continues to be closely curated. \u201cIf you're building pathways based on biological-data models, the quality, accuracy, richness of detail and breadth of coverage of the biological content are critical,\u201d says Felciano. BiblioSphere uses a multi-pronged approach for its analysis that, as its name indicates, starts in the library. \u201cThe program's first line of analysis is literature\u201d explains Martin Seifert, vice-president of microarray business at Genomatix. \u201cWe build up a literature network from the co-citation of genes in abstracts from PubMed.\u201d This is followed by an overlay of other lines of evidence, including a curated pathway database and information from ontology databases such as GO and the US National Library of Medicine's Medical Subject Headings. \u201cThe most important step,\u201d says Seifert, \u201cis finding the biological aspects that are buried in data from chips.\u201d Effective pathway-building systems may offer the promise of making gene-expression arrays a potent tool for performing detailed diagnostic analyses in fields such as toxicology and pathology. \u201cOur customers see a lot of promise in generating  de novo  pathways that may not be exactly like the ones you see in your textbooks, but may be more specific to the disease or tumour stage that you are looking at,\u201d says Felciano. This approach may also serve as a model for further integration of microarray findings with other data collections, ranging from the combination of different sets of chip data \u2014 such as associating genome-wide expression patterns with transcription-factor binding and DNA methylation \u2014 to more ambitious syntheses with massive databases such as PubMed, OMIM and DrugBase. \u201cThe way to deal with the problem of big data is to beat it senseless with other big data,\u201d says Quackenbush. \u201cThere's a host of information out there on how biological systems function that has been collected over the past 300 years. What we want to be in a position to do as a community is leverage that information, synthesize it and discover things that we couldn't discover using any technology on its own.\u201d Affycomp II \u2192 affycomp.biostat.jhsph.edu Bioconductor \u2192  http://www.bioconductor.org  GenMAPP \u2192  http://www.genmapp.org  GSEA \u2192  http://www.broad.mit.edu/gsea  TM4 suite \u2192  http://www.TM4.org Reprints and Permissions"},
{"file_id": "4411179b", "url": "https://www.nature.com/articles/4411179b", "year": 2006, "authors": [], "parsed_as_year": "2006_or_before", "body": "Sorting individual cells is not a problem, but what if a researcher wants to sort clusters of cells, or even whole embryos? Union Biometrica of Holliston, Massachusetts, offers a potential solution in the shape of its COPAS family of instruments. These, it says, can work with objects ranging from pancreatic islets up to zebrafish hatchlings. The principle is similar to fluorescence-activated cell sorting (FACS) \u2014 objects in solution pass through a specially designed flow-cell, where they are optically profiled according to as many as five different sorting parameters (which can include three fluorescent wavelengths), and specimens of interest are diverted and collected. But some design adjustments were necessary to protect the integrity of the multicellular objects being sorted. For one thing, rather than using charge-based sorting of droplets, COPAS uses rapid puffs of air to gently divert \u2018slices\u2019 from the flow stream that contain objects of interest. COPAS also uses lower flow-rates than FACS, sorting between 100 and 300 objects per second. \u201cWe sacrifice speed for gentle flow, which results in increased viability and less destruction of what we're analysing,\u201d says Rock Pulak, Union Biometrica's director of life sciences. \u201cIt's still much faster than trying to analyse these same numbers of cell clusters by microscope.\u201d In addition, the lower speeds mean that it is possible to do limited analysis of fluorescence localization within the objects being sorted. Pulak speculates that future instruments may even be able to perform actual imaging during the sorting process. Sometimes even single cells prefer a lighter touch, and an ongoing collaboration between the company and the Joslin Diabetes Center in Boston has shown that COPAS is also useful for sorting adipocytes and hepatocytes. \u201cThese are larger cells that are very delicate and subject to sensitivities with respect to shear force,\u201d says Pulak. \u201cWe are finding that our technology is appropriate for these kinds of applications.\u201d \n               M.E. \n             Reprints and Permissions"},
{"file_id": "4421071a", "url": "https://www.nature.com/articles/4421071a", "year": 2006, "authors": [], "parsed_as_year": "2006_or_before", "body": "Reprints and Permissions"},
{"file_id": "4431017a", "url": "https://www.nature.com/articles/4431017a", "year": 2006, "authors": [{"name": "Michael Eisenstein"}], "parsed_as_year": "2006_or_before", "body": "Light microscopy is undergoing a renaissance, with a huge range of tools and techniques for gathering biological data with unprecedented speed and resolution. Michael Eisenstein takes a closer look. Confocal microscopy is now a well-established technique for the three-dimensional imaging of cellular structures. But despite its success, the technique has its limitations when imaging live cells. The scanning process can greatly reduce imaging speed, for example, and the powerful lasers involved can damage the cells. Some manufacturers have addressed these problems by designing alternative systems such as 'restoration' microscopy (see  'Achieving clarity' , below), whereas others have strived to improve confocal technology. Leica Microsystems of Wetzlar, Germany, for instance, still uses standard point-rastering in its high-end TCS SP5 confocal instrument, but it incorporates a resonant scanner for real-time 'true confocal' imaging with little cell damage. An alternative approach uses a rapidly rotating disk, called a Nipkow spinning disk, which has numerous apertures to illuminate hundreds of spots simultaneously. This allows faster imaging with reduced photobleaching, although it does suffer from some loss in resolution. PerkinElmer Life and Analytical Sciences of Boston, Massachusetts, was among the first companies to develop instruments using this technology. Its UltraVIEW ERS microscope uses parts from leading Nipkow-disk manufacturer Yokogawa in Tokyo, Japan. The system is designed for live-cell confocal imaging and can be fitted with a sensitive electron-multiplying CCD (EMCCD) camera for detection. \u201cThis offers frame-rates in excess of 100 frames per second, if you're looking at relatively small areas in your sample,\u201d says product leader Paul Orange. Olympus of Tokyo also makes a spinning-disk system, called the DSU, which features disks with slits instead of holes. Five different disks are available that vary in terms of slit number and spacing. \u201cYou can match the slit spacing to the numerical aperture of your objective,\u201d says product manager Nicolas George. The DSU can also incorporate high-resolution EMCCD cameras for imaging live specimens at up to 150 frames per second. The LiveScan SFC from Nikon Instruments in Melville, New York, uses arrays of pinholes or slits for multipoint imaging. These remain stationary while mirrors sweep the beam spots over the sample \u2014 a process known as swept-field confocal imaging, developed by Prairie Technologies of Madison, Wisconsin. Nikon is also introducing controlled light emission microscopy, which uses feedback from the detection process to modulate laser intensity to prevent oversaturation or unnecessary illumination of signal-free regions. \u201cYou sacrifice a little bit of temporal speed,\u201d says Stan Schwartz, vice-president of Nikon's microscopy division, \u201cbut the casual user can make correct and beautiful images, and you significantly reduce photobleaching and phototoxicity.\u201d The LSM 5  LIVE  from Carl Zeiss MicroImaging in Jena, Germany, uses a line-scanning approach that can image an area 512 \u00d7 512 pixels at up to 120 frames per second, or even faster for smaller sections, which means that rapid physiological events can be visualized. According to Bernhard Zimmermann, head of product management at Zeiss, this shift from point- to line-scanning has minimal impact on resolution for most studies. \u201cIf you're looking at intracellular vesicles, you will hardly see a difference,\u201d he says. \n               Adventures in the  \n               n \n               th dimension \n             These days a confocal microscope has to do more than simply track fluorophores. Multidimensional imaging is microscopy's new catchphrase, and the latest instruments gather information that goes beyond basic spatial orientation to provide time-lapse images and detailed fluorescence profiles. The expansion in the range of available fluorescent labels means far more biological information can be colour-coded, and manufacturers have responded with advanced strategies for enhancing spectral resolution. For example, the LSM 510 META from Zeiss uses a 32-channel photomultiplier array behind an optical grating to generate full spectral signatures for each pixel; this is followed by further software-based separation. \u201cYou can even separate fluorescent emissions that peak at the same wavelength,\u201d says Zimmermann, \u201cas long as the individual emission curves are different.\u201d Nikon offers a similar strategy with its C1si confocal instrument, which can achieve spectral resolution as low as two nanometres. Leica enhances confocal spectral precision with proprietary technologies that include the acousto-optical beam-splitter, which controls excitation illumination in a filter-free manner, and SP module, which uses a prism-based approach for precise spectral separation. These will be enhanced by its forthcoming introduction of white-light lasers, which provide coherent, continuous-spectrum illumination. \u201cThis will give us total freedom in excitation and detection for all available dyes,\u201d says Frank Olschewski, Leica's manager for confocal product development. Fluorescence lifetime imaging (FLIM), which involves measuring the emission decay rate for an excited fluorophore, is also gaining interest for imaging. It can, for example, improve the quality of fluorescence resonance energy transfer (FRET) experiments. \"When there is a FRET interaction, the donor lifetime gets shortened \u2014 this is an absolute, and it doesn't depend on the quantity or intensity,\" says John White of the Laboratory for Optical and Computational Instrumentation (LOCI) at the University of Wisconsin at Madison. \"If you're doing other types of FRET measurements there are other reasons why you might get reduction in intensity or increased intensity, such as differential bleaching or compartmentalization.\" FLIM requires extremely high-speed photon detectors; Becker & Hickl of Berlin were an early leader in this regard, and Zeiss uses these detectors in its instruments. Leica also offers a FLIM attachment for its confocal instruments, which takes advantage of the SP detection system to provide 'spectral FLIM', with precise wavelength selection. Standard fluorescence microscopes produce a focal spot that is ovoid rather than spherical. So although high-end instruments can produce images with  x \u2013 y  resolution of up to 180 nanometres, the resolution is considerably poorer along the  z  axis \u2014 around 500 to 800 nanometres \u2014 which reduces the quality of three-dimensional reconstructions. One way round this uses two objective lenses, where the interference between the two optical wavefronts produces an effectively spherical focal spot. Stefan Hell and his colleagues at the Max Planck Institute for Biophysical Chemistry in G\u00f6ttingen, Germany, demonstrated the effectiveness of this approach with their confocal 4Pi instrument, which can achieve a resolution of 100 nm in all directions. The original 4Pi design was relatively slow, barring its use in live specimens, but Hell and his team have since developed a multifocal, multiphoton version \u2014 MMM-4Pi \u2014 that scans samples with 64 foci, and images at a rate that makes high-resolution three-dimensional imaging of live cells possible. A 'user-friendly' commercial version of Hell's 4Pi system, the TCS 4Pi, is now available from Leica. A related method, I5M, was developed by Mats Gustafsson of the University of California, San Francisco. I5M also achieves axial resolution below 100 nm, but is based on a wide-field configuration, with interference taking place over the entire field of view rather than at a point, and can be faster and potentially brighter than confocal 4Pi. Interference imaging produces artefactual 'lobes', which must be removed by deconvolution; these tend to be more prominent with I5M, and lobe removal is made easier by imaging with lenses that have a high numerical aperture. These are typically oil-immersion, and use of I5M is presently restricted to fixed specimens. Gustafsson's group has also developed a variant technique, I5S, which incorporates structured illumination to surpass the 'diffraction limit' in the image plane. Together, techniques such as I5S and confocal 4Pi represent important early steps in 'super-resolution' microscopy, an increasingly vibrant area of research and technological development (see  'Thinking big, seeing small' ). \n               Nonlinear thinking \n             The real rising star of imaging is multiphoton microscopy (MPM), a nonlinear imaging method. In MPM, target molecules are stimulated by a pair (or more) of low-energy photons virtually simultaneously, providing sufficient energy in combination to induce excitation and photon emission. This approach uses long-wavelength, near-infrared pulsed lasers that can penetrate deep into tissues with heavy scattering properties and that are less likely to produce phototoxic side effects. Additionally, multiphoton excitation results in dramatically reduced background fluorescence. Price remains a major obstacle to adoption, as pulsed lasers are extremely expensive, but those who have tried the method generally walk away impressed. \u201cI think it's the best technique in town for looking deep into solid tissue and figuring out the dynamics of what's going on,\u201d says White. Zeiss currently controls the patent for femtosecond pulsed-laser MPM, and has made this technology available with its LSM 510 NLO. Several companies have also sublicensed this technology, and Olympus has made such an arrangement for its FV1000-MPE instrument, which was launched earlier this month. Leica also offers an MPM configuration for the TCS SP5, based on a separate patent that makes use of picosecond-pulse lasers. Current MPM systems offer reasonable speed for live-cell work, but LaVision's TriMScope unit further improves imaging frame-rates with a multifocal approach that simultaneously scans samples with up to 64 foci, enabling the imaging of a 1004 \u00d7 1002-pixel field at 31 frames per second. TriMScope can also be adapted to perform FLIM or second-harmonic generation imaging, another nonlinear method gaining interest for its label-free imaging capabilities. \n               Scratching the surface \n             Some of the most interesting biological events take place near the cell surface, and for imaging beauty that goes only membrane deep, total internal reflection fluorescence (TIRF) microscopy is a strong option. A sample on a glass coverslip is illuminated either through a prism or an objective lens of high numerical aperture. When light enters the coverslip at an angle greater than the critical angle, as determined by the difference between the refractive indices of the glass and the specimen, the incident beam does not enter the specimen, but instead produces an evanescent electromagnetic field. This field can excite fluorophores close to the surface (within about 200 nm) of the coverslip, allowing non-destructive, live-cell imaging at the cell membrane with single-molecule resolution. Most major microscope manufacturers now offer multi-wavelength TIRF modules for their inverted microscopes. These typically benefit from precise computer control of the angle and position of the excitation beam. Zeiss offers a compact, stand-alone option with its Laser TIRF system. TIRF instruments typically use oil-immersion lenses with a numerical aperture of 1.45, an effective minimum for good imaging. Recently, Nikon broke this barrier with a set of Plan Apo 60\u00d7 and 100\u00d7 magnification objectives. \u201cWe achieved a numerical aperture of 1.49,\u201d says Schwartz, \u201cand that's just under the theoretical limit for imaging with glass.\u201d \n               Putting it all together \n             Modern imaging experiments have become very complex, generating huge amounts of multidimensional data. Although microscope manufacturers will typically provide reasonably effective tools for image processing and analysis, many users also opt for third-party software with more extensive analytical capabilities. MetaMorph from Molecular Devices of Sunnyvale, California, integrates hardware control with a large toolbox of resources for multidimensional imaging, including a range of 'application modules' for image processing. \u201cThese are specific segmentation and analysis modules where, rather than having the user write macros, we've automated the entire process by having one dialogue box that helps segment your image and gives you relevant measurements,\u201d says product manager Magali Trani\u00e9. Imaris from Bitplane in Zurich, Switzerland, also uses a module-based approach for segmentation and analysis of multidimensional images. \u201cYou can click on any object that you see and immediately, in the same screen, get the statistics for that particular object,\u201d says vice-president and director of sales Michael Wussow. \u201cThe same is true in reverse \u2014 we have an interactive sorting tab, where you move a bar on a histogram and things will appear or disappear depending on whether or not they meet your criteria.\u201d Bitplane also allows users to code their own routines, and hosts a number of user-generated plug-ins on its webpage. Improvision of Coventry, UK, recently released the latest version of its Volocity package, which consists of four products \u2014 Acquisition, Visualization, Quantitation and Restoration \u2014 that can also be integrated. Among Volocity's strengths are its object detection and tracking capabilities, and powerful rendering techniques. \u201cEvery voxel within the image set is computed by the Volocity rendering engine,\u201d says senior marketing specialist Nicky Francis. \u201cSo it isn't a surface rendering technique, but a fully interactive volume-rendering technique. This makes it much more true-to-life.\u201d Numerous other powerful commercial options are available, but the academic community has also stepped up to provide a variety of open-source solutions. Among the most popular is ImageJ, developed at the US National Institutes of Health by Wayne Rasband. ImageJ is a multipurpose imaging tool, maintained by Rasband but powered by a highly active user community. \u201cThere are about 400 plug-ins on our webpage, contributed by more than 100 people,\u201d says Rasband. Another popular program is VisBio, developed by the LOCI's Curtis Rueden for working with multidimensional data. According to Kevin Eliceiri, co-director of the LOCI, such tools complement commercial products. \u201cThe goal is to fill in the gaps,\u201d he says. \u201cThere are needs that often cannot yet be met by the commercial community because they either represent too small a market or are emerging techniques.\u201d Such projects have also meshed with the efforts of the Open Microscopy Environment (OME) to develop tools that aid collection and sharing of complex image data (see  'Tower of Babel' ). Software \u2014 and hardware \u2014 requirements will only grow more severe as scientists attempt to coax increasingly complex data from smaller numbers of photons. Technology aside, the underlying challenges for this field remain clear and simple. \u201cThere are three issues in microscopy that never change \u2014 I want the best resolution, I want the best sensitivity and I want the best speed,\u201d says Nikon's Schwartz. \u201cAnd it's really difficult to get all three.\u201d Reprints and Permissions"},
{"file_id": "4421069a", "url": "https://www.nature.com/articles/4421069a", "year": 2006, "authors": [], "parsed_as_year": "2006_or_before", "body": "Many working with microarrays now recognize that one way uncertainty about experimental findings can be dispelled is by being more transparent about methodology and data. This realization has transformed the field. For instance, after some initial resistance, almost every major commercial vendor has made the sequences and annotations of their probes publicly available \u2014 to the considerable benefit of the community as a whole. This awareness has also manifest itself in the drive to develop shared resources for pooling experimental data and systems for clearly defining how these data were obtained. A leading force in this regard is the Microarray Gene Expression Data (MGED) Society, which put forward a proposal in 2001 for experimental annotation standards known as minimum information about a microarray experiment (MIAME), designed to record key details about factors such as sample preparation and experimental design. These standards were embraced by many, and several leading journals, including  Cell ,  The Lancet  and  Nature , demand MIAME compliance from all microarray research submissions. However, some aspects of MIAME have proved problematic. \u201cI think almost all academic biologists embrace the concept of openly sharing data,\u201d says Catherine Ball of Stanford University in California, the current president of the MGED Society. \u201cBut embracing the process and actually taking part are very different, and it can be difficult to fully annotate your data.\u201d According to Gavin Sherlock, also of Stanford and MGED, part of the problem was MAGE-ML (microarray and gene expression markup language), the XML-based language initially developed for MIAME data recording. \u201cNobody can look at it, nobody can read it, nobody can edit it,\u201d he says. \u201cIt's very difficult to use.\u201d This is reflected in the uploading of data to public databases, another process strongly advocated by MGED. The ArrayExpress database of the European Bioinformatics Institute in Cambridge, UK, is strictly MIAME-compliant, and receives considerably fewer submissions than the non-MIAME-compliant Gene Expression Omnibus (GEO) of the National Center for Biotechnology Information in Rockville, Maryland. MGED is now poised to release a considerably simpler format for data submission, and Ball is hopeful that this, along with other user-friendly software tools, will make a difference. But, fundamentally, compliance comes down to the effort scientists can and will put in. All of the MicroArray Quality Control project's data are being deposited into both GEO and ArrayExpress, and although this has proved an onerous task, Leming Shi of the US Food and Drug Administration sees clear rewards in the effort. \u201cDepositing the data may be a painful process, but we have to do it for the sake of the community,\u201d he says. \u201cThe more information we have in the future, the better.\u201d \n               M.E. \n             Reprints and Permissions"},
{"file_id": "4421068a", "url": "https://www.nature.com/articles/4421068a", "year": 2006, "authors": [], "parsed_as_year": "2006_or_before", "body": "To err is human. Scientists are keenly aware of this, and the multistage nature of microarray experiments provides ample opportunity for the human aspect of experimental error. Robotics could offer a solution, as well as potential for greater experimental efficiency. \u201cIt used to be that the only people who thought about automation were the drug screeners, the diagnostic folks, people who had to do things hundreds of thousands of times,\u201d says John Blume, vice-president of assay and application product development at Affymetrix of Santa Clara, California. \u201cBut a lot of early-stage discovery-type work now happens at the level of hundreds of things at a time.\u201d Affymetrix uses a modular approach, developing units that automate individual experimental stages. Several other companies have taken a similar path after finding that many customers had already taken matters into their own hands. \u201cA lot of them have already adopted automation infrastructure,\u201d says Kevin Meldrum, director of genomics marketing at Agilent Technologies, Santa Clara, California. \u201cThey don't want to have to go out and buy a completely new system.\u201d San Diego-based array manufacturer Illumina offers an 'arrays of arrays' format inherently designed for higher-throughput, and so further automation is a lower priority, although the company offers an AutoLoader that can process the scanning of up to 32 BeadChips \u2014 containing up to 256 arrays in total \u2014 in 24 hours. Chip-builder and service provider NimbleGen Systems, based in Madison, Wisconsin, has taken an opposing approach. \u201cOur mission is to make everything automated,\u201d says Emile Nuwaysir, vice-president of business development. \u201cSo that analysis is monitored and quality controlled by humans, but not run by humans.\u201d He projects near-complete automation of the full process by the end of the year. febit Biotech in Heidelberg, Germany, brings this philosophy to the benchtop with the GENIOM, a system that fully automates most processes, including array synthesis, hybridization and analysis. Current GENIOM arrays are limited to 6,000 features but will soon expand to 15,000, and Peer St\u00e4hler, vice-president of marketing and sales at febit, believes that the instrument's speed and flexibility offer benefits for rapid experimental probe design and optimization. \u201cYou can easily import and synthesize the results of other people's bioinformatics,\u201d he says, \u201cand you can export any capture probe that you have evaluated.\u201d \n               M.E. \n             Reprints and Permissions"},
{"file_id": "4421067b", "url": "https://www.nature.com/articles/4421067b", "year": 2006, "authors": [], "parsed_as_year": "2006_or_before", "body": "Leming Shi of the US Food and Drug Administration (FDA) in Rockville, Maryland, is unabashed in his affection for microarrays. But he was disconcerted by the recent publication of several papers challenging the reliability of gene-expression microarray experiments. One article, for example, reported such disagreement that an analysis of 185 genes using three different technologies revealed concordant readings for only four transcripts (P. K. Tan  et al .  Nucl. Acid Res.   31,  5676\u20135684; 2003). Subsequently, Shi and his colleagues found that an alternative analytical approach greatly improved cross-platform concordance for these data sets (L. Shi  et al .  BMC Bioinformatics   6 (Suppl. 2),  S12; 2005). But the lingering climate of uncertainty, and concerns about the potentially serious implications for the use of microarray data in the FDA drug-approval process, led them to launch the MicroArray Quality Control (MAQC) project. The MAQC brought together research leaders from government, academia and industry to establish tightly controlled 'gold standard' comparisons of microarray systems. They began by identifying commercially available, trustworthy 'standard' RNA samples. But this was just the start. \u201cThe MAQC's main goal is to generate a vast reference data set,\u201d says Shi. \u201cWe have conducted more than 1,000 array hybridizations with these reference samples, plus we're using three alternative technologies and we requested that each system be evaluated at three testing sites.\u201d The MAQC recently completed a review of its final data, and will present its findings in a series of articles to be published in  Nature Biotechnology  next month. In the meantime, many in the field are awaiting the outcome of a complementary initative: the External RNA Controls Consortium (ERCC). This evolved from a 2003 meeting headed by the US National Institute of Standards and Technology in Gaithersburg, Maryland. It aims to identify and help make commercially available a collection of reliable RNA 'spike-in' controls, which can be included in any microarray experiment to assess variables such as labelling and hybridization efficiency. Participation has grown rapidly, and ERCC leader Janet Warrington, who is a vice-president at Affymetrix in Santa Clara, California, finds the early progress promising. \u201cA number of organizations that were already using their own controls have donated these \u2014 no strings attached \u2014 for testing,\u201d she says. \u201cSo we have a collection of 100 to 150 controls that will be tested across platforms and we have eight sites that have volunteered to carry out the testing.\u201d Both projects have benefited from collaborative environments that have allowed even direct competitors to work together towards a shared goal. \u201cWe all share the belief that if we're successful, we'll expand the marketplace for everyone,\u201d says Warrington. \n               M.E. \n             Reprints and Permissions"},
{"file_id": "4431019a", "url": "https://www.nature.com/articles/4431019a", "year": 2006, "authors": [], "parsed_as_year": "2006_or_before", "body": "Most optical imaging techniques are subject to the tyranny of the diffraction limit, where the optical properties of conventional objective lenses make it impossible to distinguish two objects separated by less than 180 nanometres in the focal plane. But limits were made to be broken. In 1993, Stefan Hell of the Max Planck Institute for Biophysical Chemistry in G\u00f6ttingen, Germany, developed the concept of stimulated emission depletion (STED). Fluorescent molecules are activated with a laser spot, which is then overlaid with a ring-shaped beam of low-energy photons that shrinks the effective area of excitation to an extent determined by the brightness of the ring. This means that STED can generate almost arbitrarily small excitation areas. The instrument lives up to its promise, achieving resolution of well under 100 nanometres along all axes, and representing the first 'super resolution' technique to break the diffraction limit. This platform has evolved rapidly \u2014 Leica Microsystems of Wetzlar, Germany, plans to release a commercial instrument in 2007 \u2014 and Hell has also integrated STED with his confocal 4Pi instrument to push the resolution limit further. \u201cWe've got a resolution of 15 to 30 nanometres in the focal plane,\u201d he says. Proof-of-concept studies suggest that STED should be suitable for live-cell imaging, although imaging rapid events with small focal volumes could prove tricky. Fluorophore concentration is a major concern in super-resolution imaging. Mats Gustafsson, of the University of California, San Francisco, has demonstrated the use of patterned light in a wide-field 'structured illumination' scheme that is effective for planar imaging at less than 50-nm resolution, but he feels that such techniques may be less ideal for smaller focal regions. \u201cAs your resolution volume shrinks, it holds a smaller and smaller number of molecules,\u201d he says, \u201cand the individual stochastic response of each molecule becomes more apparent. Eventually, you reach a point where it is better to exploit, rather than fight, this independent molecular behaviour. This is precisely what Eric Betzig, for example, has done beautifully.\u201d Betzig, of the Howard Hughes Medical Institute (HHMI) Janelia Farms campus in Virginia, cites the research of Hell and Gustafsson as fuelling his interest in super-resolution. His efforts recently yielded a technique for single-molecule imaging: photoactivated localization microscopy (PALM), which he developed in collaboration with Harald Hess of NuQuest Research in La Jolla, California. Subsets of individual photoactivatable fluorescent molecules are activated, and subsequently bleached, within a sample through laser exposure, and then imaged by total internal reflection fluorescence (TIRF) microscopy. This process is repeated, and the resulting images are superimposed into a stack that is computationally resolved into a fluorescent molecule 'map' with resolution below 10 nm. The technique is not intended for live-cell work, but could offer a powerful super-resolution imaging tool for fixed cells. \u201cThere's good hope that it will be fairly easy to adapt for commercial TIRF,\u201d says Betzig. \u201cI have some more grandiose ideas about how to do three-dimensional PALM, but the first order of business is to get a more usable instrument for cell biologists.\u201d HHMI investigator Xiaowei Zhuang, at Harvard University in Cambridge, Massachusetts, working with students Mark Bates and Michael Rust, recently described a similar technique, stochastic optical reconstruction microscopy (STORM). STORM emerged from the discovery by Zhuang's group that the commonly used Cy5 fluorophore has photoswitchable properties. \u201cIt can be switched in a controlled way, back and forth hundreds of times, between light and dark states,\u201d says Zhuang. Like PALM, STORM involves cycles of selective excitation and imaging followed by computational reconstruction of the full image. As the fluorophores can be turned on and off, it is possible to do time-resolved imaging and faster imaging cycles. Resolution as low as 18 nanometres has been demonstrated, and Zhuang thinks this is just the beginning. Right now, her top priorities are exploring the potential for multicolour imaging, and improving imaging speed. \u201cIf we can bring STORM to one-second resolution, that's going to open up a big door and enable many different things,\u201d she says. Surveying the field, Hell views these various approaches as complementary tools that advance a common goal \u2014 forcing researchers to re-examine what is possible in imaging. \u201cI think breaking the diffraction barrier is a fundamental step forward,\u201d he says. \u201cThis is an idea whose time has been coming for a long time now.\u201d \n               M.E. \n             Reprints and Permissions"},
{"file_id": "4431021a", "url": "https://www.nature.com/articles/4431021a", "year": 2006, "authors": [], "parsed_as_year": "2006_or_before", "body": "The Open Microscopy Environment (OME) first emerged from the recognition that an explosion in imaging data was imminent. Image files were becoming bigger and more complex, representing copious data on numerous parameters as well as experiment-specific 'metadata'. \u201cThe ability to provide links between an image, any processed versions of an image, the data describing the acquisition of that image, and any analytical results generated about that image is a critical capability,\u201d says Jason Swedlow of the University of Dundee, UK. When Swedlow, along with Ilya Goldberg at the US National Institute on Aging, Peter Sorger at the Massachusetts Institute of Technology, and researchers at the Laboratory for Optical and Computational Instrumentation (LOCI) at the University of Wisconsin at Madison, officially launched the OME in 2001, the development of a universal file format was a top priority. \u201cProprietary file formats are one of the biggest problems in modern microscopy,\u201d says the LOCI's Kevin Eliceiri. \u201cThere are about 30 or 40 major microscopy file formats, so we're speaking all of these different languages \u2014 it's the Tower of Babel.\u201d To combat this, the OME has developed a file format called OME-XML, which retains both image pixel data and experimental metadata in a readable XML-based file. The LOCI has since refined this format as the OME-TIFF, which still encapsulates metadata in XML but stores pixel data in a TIFF format. \u201cWe have the best of both worlds now, in that TIFF is probably the closest there is to a universal image format,\u201d says Eliceiri. The OME also offers open-source software for image management and analysis, which uses these new formats. Swedlow acknowledges that OME software may prove challenging for less computer-savvy users, but says the developers are working hard to make it easier to use. Meanwhile, a growing number of commercial packages now not only recognize the OME formats, but also record data into them. Indeed, the efforts of Swedlow and his colleagues have won accolades from several manufacturers. \u201cMaybe up to 15 or 20% of our development resources go towards reader updates,\u201d says Michael Wussow of Bitplane in Zurich, \u201cso we really like this idea of an Open Microscopy Environment.\u201d The OME also intends to tackle another weighty issue: handling and browsing increasingly bulky data sets. \u201cWe currently store about 50 terabytes of data from our imaging facility,\u201d says Swedlow, \u201cand the OME's goal is to provide software to handle data on this scale.\u201d Of course, software isn't the whole answer, and significant investment in hardware and network infrastructure will be necessary for labs serious about imaging. \u201cAn enterprise-scale data-storage facility doesn't sound like hypothesis-driven research,\u201d says Swedlow, \u201cbut it is a required tool for hypothesis-driven research using imaging.\u201d \n               M.E. \n             Reprints and Permissions"},
{"file_id": "4431017b", "url": "https://www.nature.com/articles/4431017b", "year": 2006, "authors": [], "parsed_as_year": "2006_or_before", "body": "A key strength of confocal imaging is the elimination of emitted light from outside the focal point, such as that from tissue autofluorescence. But there are situations when non-confocal imaging is a better option. Deconvolution or restoration microscopy, in which computational algorithms remove artefactual blurring from actual image fluorescence while restoring out-of-focus fluorescence to its proper location, is one solution. Applied Precision of Issaquah, Washington, was the first company to offer a complete real-time restoration-microscopy instrument, the DeltaVision RT. This combines a motorized stage with deconvolution software to collect and process two-dimensional image sections for the real-time assembly of 'restored' three-dimensional image projections. Although restoration microscopy is sometimes presented as an economical alternative to confocal, many confocal users find advantages in computational image correction, including compensation for potential resolution loss with spinning-disk instruments. Confocal manufacturers have responded by incorporating deconvolution into their software packages. For example, Nikon Instruments in Melville, New York, offers a '2D-RT decon' module that removes blurring from two-dimensional sections to clean up three-dimensional confocal images in real time, and Olympus of Tokyo uses deconvolution tools developed by Intelligent Imaging Innovations in Denver, Colorado. Many users also opt for dedicated deconvolution software, such as the Huygens suite from Scientific Volume Imaging of Hilversum, the Netherlands. \u201cOur software offers as much knowledge as we dare to put in about microscope image formation and noise characteristics,\u201d explains founder Hans van der Voort, \u201cand with that we can recover as much as possible about the original object.\u201c Huygens is regularly updated to tackle the data being generated by new imaging techniques \u2014 an onerous task for an expanding field. Above all, the challenge is keeping the final image clean and true-to-life. \u201cWhat everybody hates is a restored image that is an artefact,\u201d says van der Voort. \u201cYou can make an image like a photograph, which is nice to see. But at second glance, if you really want to analyse your data, what you want is reliability.\u201d \n               M.E. \n             Reprints and Permissions"},
{"file_id": "4391017a", "url": "https://www.nature.com/articles/4391017a", "year": 2006, "authors": [{"name": "Laura Bonetta"}], "parsed_as_year": "2006_or_before", "body": "Proteomics raises new challenges in protein purification. Technologies well adapted to isolate individual proteins get a makeover to tackle large numbers of samples. Laura Bonetta investigates. Proteomics aims to identify the cellular functions of all proteins encoded by the genome of an organism. Protein structure determination, proteome-wide functional screens, and the identification of protein interactions are just a few of the proteomics applications requiring thousands of purified proteins as a starting point. Researchers now have many reagents and off-the-shelf kits for the isolation of proteins to 90% purity. The challenge facing researchers is to adapt these methods to purify hundreds of different proteins in parallel using robotic systems. Several years ago a small number of academic institutions and biotechnology companies responded to the challenge by developing pipelines for high-throughput protein purification (see  \u201cProteomics at Harvard\u201d ). Some of the reagents and instruments required for these methods are making their way to the marketplace. \n               HT purification \n             One widely used method for purifying hundreds of proteins in parallel involves their expression in a heterologous system. Once a target protein is identified, the corresponding complementary DNA is cloned into an expression vector for producing the protein in  Escherichia coli  or another organism. The purification of the expressed recombinant protein from bacteria typically involves cell lysis, incubation of the lysate with an affinity resin, washes and elution. Additional purification steps, such as ion-exchange and size-exclusion chromatography, are required for protein crystallography (see  \u201cProtein purification for structural proteomics\u201d ). One bottleneck in high-throughput protein production is the expression of enough properly folded protein in bacterial cells. Proteins expressed in  E. coli  can accumulate as inactive, misfolded aggregates called inclusion bodies. Proteins in this form can be purified under denaturing conditions, but need to be refolded into their native conformations. A number of groups have developed procedures for expressing proteins in insect cells or cell-free extracts to get around some of these problems (see  \u201cCell or cell-free?\u201d ). Each protein is expressed in different amounts and has different properties. Thus, to be able to apply the same purification protocol across the range of proteins, researchers have engineered proteins with generic tags that will bind to an affinity ligand. Widely used tags include a small peptide of six histidine residues (6xHis), a calmodulin-binding peptide, the streptavidin friendly biotin, the cellulose-binding tag, the maltose-binding protein (NusA), and glutathione  S -transferase (GST). Vectors designed for expressing tagged proteins can be purchased from several manufacturers including Invitrogen (Carlsbad, California), Novagen (Madison, Wisconsin), Roche Applied Science (Indianapolis, Indiana) and others. Some companies are now developing high-throughput methods for purifying proteins that are not tagged. \u201cTypically most scientists work with tagged proteins through the discovery process. However, it is possible to lose some information because of tag interference in the biological assays. Removing the tag means removing some of the question marks,\u201d says Lisa Bradbury, director of R&D for proteomics and cell therapy at Pall Corporation of East Hills, New York. \n               Purification tool bag \n             Several companies sell reagents for every step of the purification process \u2014 from cell lysis, to affinity purification resins, to desalting and concentrating reagents \u2014 adapted to high-throughput protocols that use standard 96-well plates. \u201cWe were recognized for our protein affinity-purification line. Now we are configuring it into a format that is automation-friendly,\u201d says Craig Smith, vice-president of R&D at Pierce Biotechnology of Rockford, Illinois (part of Fisher Scientific). Among the company's offerings are the SwellGel Discs, pellets of dehydrated support that can be distributed into filter plates. When the protein is added, the disc rehydrates to an affinity gel that binds tagged proteins. \u201cWe have taken out variability by having a dried pellet,\u201d says Smith. More recently, the company's Zeba Micro Desalt Spin Columns were configured to 96 well-plate format for desalting many small samples in parallel. Pall Corporation also sells reagents for each step of the purification process. The AcroPrep 96 Filter Plate can be used for any type of chromatography, including immobilized metal affinity chromatography, in a multiwell platform. The format facilitates the optimization of various purification parameters, including the choice of metal ion and resin, the sample-to-resin ratio, and the elution conditions. Other reagents adapted to a high-throughput environment include the Mustang 96-well ion-exchange plates and Nanosep Centrifugal Devices. QIAGEN, based in Hilden, Germay, offers the Superflow 96 BioRobot, a medium-scale purification kit for 6xHis-tagged proteins. \u201cYou load your pellets onto a robot, press a button, and you will have product in two hours,\u201d says Frank Sch\u00e4fer, associate director for R&D protein expression/proteomics. According to the company, purification can be done in denaturing or native conditions and yields 4 mg of protein for each of 96 wells. \u201cIt is one system and one process for every sample,\u201d says Sch\u00e4fer. QIAGEN sells a similar system that runs up to 24 samples in parallel to purify up to 30 mg of protein for large-scale applications. Novagen's host of reagents for high-throughput protein purification starts with the expression step. The Overnight Express Autoinduction Systems allow fully automated, parallel protein expression from  E. coli  cultures, and achieve high cell densities without monitoring growth or induction by the addition of isopropyl-beta-D-thiogalactopyranoside. \u201cThe medium contains a blend of carbon sources optimized for tightly regulated, uninduced growth and automatic induction of protein expression,\u201d says Anthony Grabski, R&D group leader for protein purification. The RoboPop Purification Kits contain reagents for the extraction and purification of His or GST fusion proteins by magnetic or filtration-based affinity purification protocols, all in a 96-well format. The kits have been validated on both PerkinElmer and Tecan robotic liquid handlers. Other providers of reagents for high-throughput protein purification include GE Healthcare of Little Chalfont, UK, with its Tricorn High Performance Columns designed for high-resolution purification of proteins, peptides and other biomolecules. Sartorius of G\u00f6ttingen, Germany, introduced the Vivapure 8-to-96 well cobalt chelate kit for the simultaneous purification of multiple His-fusion proteins by affinity chromatography. And St Louis-based Sigma-Aldrich's HIS-Select iLAP Plates are plates coated with cell-lysis reagents and a HIS-Select nickel chelate matrix allowing for cell lysis, protein capture, and assay of a His-fusion protein in a single well. As well as purification kits, several companies offer products to help screen the outcome of each step in the purification pipeline. Novagen's RoboPop Solubility Screening Kit contains a filtration plate that retains insoluble inclusion bodies while allowing soluble proteins to be collected for rapid quantitation and analysis. \u201cYou can screen expression conditions for soluble protein before you proceed to purification,\u201d says Grabski. In January, the company released the iFold Protein Refolding System 1 to screen different refolding conditions in parallel. The system includes inclusion-body purification reagents and 92 different refolding buffers. \u201cStructural proteomics groups have harvested most of the low-hanging fruits, so now they will have to focus on difficult proteins that are insolubly expressed,\u201d says Grabski. Another product for monitoring the purification procedure is the Protein 200-HT2 assay of Agilent Technologies (Palo Alto, California), which allows the identification, sizing and quantification of proteins from 14 kD to 200 kD in size. \u201cIt allows researchers to replace SDS\u2013PAGE procedures in the lab,\u201d says product manager Carsten Buhlmann. Finally, some reagents and instruments have been designed to help researchers hone in, for example, on medically relevant targets, which can then be purified and studied (see  \u201cTarget identification\u201d ). \n               Getting automated \n             The available reagents and kits can be used to purify several hundred proteins in a 96-well format at considerable speed. However, the tedious and error-prone nature of manually performed high-throughput operations calls for automation of the process. \u201cWhen a robot is doing the pipetting there are no mistakes, so you can have greater consistency and reproducibility,\u201d says David Daniels, applications marketing manager for Beckman Coulter in Fullerton, California. Liquid handlers perform all the steps of protein purification from loading cell cultures to obtaining a protein in solution, in under four hours. Two leading liquid handlers are Beckman Coulter's Biomek 3000 and Biomek FX instruments, with plate-deck configurations that can handle 10 and 18 plate positions, respectively. The Biomek 3000 will process two 96-well plates in 2\u20133 hours; the FX twice as many. Promega in Madison, Wisconsin, automated high-throughput protein purification using its MagneHis Protein Purification System and the Biomek FX instruments. The MagneHis reagent contains a cell-lysis solution that allows resuspension and lysis of bacterial cell pellets without sonication and centrifugation. Magnetic pre-charged nickel particles are then used to isolate His-tagged proteins from the cell lysate. The MagneHis particles bound to the target proteins are captured on a MagnaBot 96 Magnetic Separation device \u2014 a plate that can be hooked up to the Biomek instruments. In January, PerkinElmer (Boston, Massachusetts) launched its modular and scalable JANUS Automated Workstation. \u201cMany units can be configured at the time of sale,\u201d says Nance Hall, business unit leader for automation and liquid handling. \u201cWith JANUS, you can scale and change the system not only when you buy, but also in the future.\u201d Users can also program the selection of numerous dispense heads and formats. \u201cIt is as though you go into a kitchen and you are asked if you need a cup or a teaspoon,\u201d explains Hall. \u201cIn the kitchen you will use both.\u201d Likewise, the JANUS allows users to choose different tools automatically depending on the volume range and microplate densities. Another popular choice among liquid handlers is the Freedom EVO automated liquid handler from Tecan in Durham, North Carolina. Others include the Sciclone ALH Workstation from Caliper Sciences (Hopkinton, Massachusetts), the BioCube System from Proteodyne (Windsor, Connecticut) and the 925 PC Workstation and GX-281 Liquid Handler from Gilson (Middleton, Wisconsin). \u201cYou can use over 300 standard racks and many more custom ones,\u201d says Gilson's spokesperson Greg Robinson. \n               Going large \n             For crystallization and other functional studies, a researcher needs more protein than is possible in 96-well plate format. For such purposes, a number of parallel-purification systems that can accommodate larger volumes of cells are starting to come onto the market. GE Healthcare's AKTAxpress is a fully automated chromatography system for purification of His- and GST-tagged proteins, yielding up to 50 mg of target protein. Affinity chromatography is the first step of all protocols, but as a second step it is possible to choose between desalting or gel filtration, and an ion-exchange step can be added too. Automatic tag removal is possible in all multi-step protocols. Teledyne Isco (Lincoln, Nebraska) launched the BioOptix 10 last year, a ten-channel parallel-purification system for high-capacity protein purification with subsequent fraction collection. This means it can purify protein from ten different samples by ion exchange, affinity or size-exclusion chromatography. The instrument includes a high-capacity fraction collector (20 to 60 fractions per sample). Ten independently controlled pumps can be programmed with different gradient conditions for rapid identification of columns and conditions that give optimal results. \u201cInstead of doing it sequentially, a process that can take 4\u20135 days, you can load different columns and run the instrument over lunch,\u201d says John Urh, product manager for chromatography Another player in this arena, QIAGEN's BioRobot Protein LS System, has the capacity for the parallel purification of up to 24 large-scale cultures in less than three hours, and PerkinElmer's JANUS system can be configured for large-scale purification protocols. New reagents and instruments have allowed researchers to set up pipelines for high-throughput protein production of a scale and capacity that match the needs of their individual labs. As a result of these advances, hundreds of proteins from different organisms have been purified and their structures and functions determined. But keeping abreast of this rapidly changing field will require ongoing innovation. \u201cSeveral years ago everyone was talking about genomics, now the focus is on proteomics. More and more we are seeing research moving toward specific proteins,\u201d says PerkinElmer's Hall. \u201cThe challenge is trying to keep up with the technologies.\u201d Reprints and Permissions"},
{"file_id": "4391022a", "url": "https://www.nature.com/articles/4391022a", "year": 2006, "authors": [], "parsed_as_year": "2006_or_before", "body": "Reprints and Permissions"},
{"file_id": "4391019a", "url": "https://www.nature.com/articles/4391019a", "year": 2006, "authors": [], "parsed_as_year": "2006_or_before", "body": "Only a fraction of proteins can be overproduced in  E. coli  in sufficient yield and without the formation of inclusion-body aggregates or the proteolytic degradation of expressed proteins. Alternative expression systems include cell cultures from eukaryotic organisms, such as insect cells, and cell-free,  in vitro  protein expression. The latter is the focus of John Markley's group at the Center for Eukaryotic Structural Genomics at the University of Wisconsin, Madison. Collaborating with Ehime University and CellFree Sciences, both in Japan, Markley and colleagues developed a pipeline using wheat germ cell-free protein translation as a way to produce proteins for nuclear magnetic resonance (NMR) structure determination. Cell-free systems simplify the purification efforts, as \u201conly the protein of interest is expressed and labelled, thus the background is cleaner\u201d, says Markley (pictured). For the NMR studies, his group has been getting twice as many folded proteins using the cell-free system than with expression in  E. coli . The cell-free method also requires smaller volumes, avoiding lengthy concentration steps, and it lends itself to the labelling of proteins, which is a requirement for NMR. \u201cBut there is a steep learning curve for learning how to do a cell-free system,\u201d says Dmitriy Vinarov, who is responsible for high-throughput production at the centre. One downside of cell-free systems is the expense of the reagents, especially when success rates are not high. \u201cAbout 79% of human proteins will be expressed, about one half of those will produce protein in sufficient quantities for structural studies, and half of those will remain stably folded for NMR studies,\u201d says Markley. Cell-free and cell-based systems are not mutually exclusive. \u201cEach has unique advantages and capabilities so we will continue to do both,\u201d explains Markley. By using a new cloning system from Promega the researchers have been able to transfer target cDNAs from cell-free to cell-based expression systems to achieve greater flexibility. \u201cWe still have a lot to learn. It is working quite well for us but we still have improvements to make,\u201d says Vinarov. \u201cThe technology is not quite primetime.\u201d Cell-free systems are available from companies such as Roche Applied Science, QIAGEN and Invitrogen, as well as from CellFree Sciences. \n               L.B. \n             Reprints and Permissions"},
{"file_id": "4391017b", "url": "https://www.nature.com/articles/4391017b", "year": 2006, "authors": [], "parsed_as_year": "2006_or_before", "body": "Founded in the spring of 1999, the Harvard Institute of Proteomics (HIP) at Harvard Medical School aims to provide tools to determine the function of every protein encoded by the human genome and appropriate model and disease organisms. To this end, HIP scientists are building collections of genes from humans and organisms including  Saccharomyces cerevisiae ,  Vibrio cholerae ,  Yersinia pestis ,  Pseudomonas aeruginosa  and  Bacillus anthracis , as well as some mouse and viral genes. In addition, a number of projects are focused on specific groups of medically relevant genes. The Breast Cancer 1000 Project has developed a repository of clones for 1,000 genes that contribute to the onset of breast cancer, a subset of which have already been tested in functional assays to identify cDNAs that induce cancer-like phenotypes. Another project aims to clone the entire range of human kinases. The expression clones generated at HIP, along with the technology to use them, will be made available to all researchers. Joshua LaBaer (pictured), co-founder and director of the institute, says his group has developed methods for the high-throughput purification of proteins expressed in  Escherichia coli  and downstream processes to determine their functions or properties. \u201cIn one project, we are purifying all the proteins produced by the organism  Francisella tularensis , the parasite that causes tularaemia,\u201d says LaBaer. The organism's proteome consists of about 1,600 proteins, all of which have been expressed in  E. coli  and purified. The purified proteins are then used in high-throughput functional screens \u201cto find proteins that produce an immune response\u201d. A similar study focuses on the genome of the bacterium  V. cholerae , which can cause cholera in humans. Such studies are not only yielding important drug targets but laying the groundwork for studies targeting the much more complex human proteome. LaBaer says that two bottlenecks affecting the purification of proteins from  Y. pestis , the plague bacterium, are the \u201cthe ability to express large or hydrophobic proteins in bacteria, and avoiding the inclusion-body problem.\u201d Partly for this reason the institute is now moving towards the development of protein arrays, with proteins being synthesized on the array rather than spotted on them. In their protocol, plasmid DNA is spotted on the array and genes are then transcribed and translated in a cell-free system. The resulting proteins, hundreds of them per chip, are immobilized  in situ  and can be used to test for protein\u2013protein interactions and other functional assays. Although the technology is not yet ready for primetime, LaBaer says they are having success with it. The development of these types of arrays may alleviate the need to produce and purify proteins from  E. coli , as scientists will be able to conduct functional studies directly on the arrays. \n               L.B. \n             Reprints and Permissions"},
{"file_id": "444959a", "url": "https://www.nature.com/articles/444959a", "year": 2006, "authors": [{"name": "Michael Eisenstein"}], "parsed_as_year": "2006_or_before", "body": "Protein microarrays are coming of age, and the development of specialized technologies is extending their high-throughput capabilities. Michael Eisenstein reports. Like the younger child who has had an older sibling to 'soften up' his or her parents and make life a little easier, protein microarrays have benefited from lessons learned during the noisy adolescence of DNA microarrays. \u201cIntellectually, the assays are identical to DNA microarrays,\u201d says microarray pioneer Mark Schena, visiting scholar at TeleChem International in Sunnyvale, California. \u201cThe basic tenets of miniaturization, automation and parallelism all hold for DNA and protein.\u201d Indeed, some tools of the trade, such as array spotters and readers, have needed virtually no changes to make the move to proteins. But the differences are there nonetheless, and the protein-microarray field is rapidly developing its own identity, with new challenges that require a diverse set of specialized tools and tricks. One key distinction emerges early in the analysis process: the selection of array substrate. Unlike oligonucleotides, proteins are broadly heterogeneous in size, shape and chemistry, and the diversity of applications for protein arrays means that users need to shop around for the appropriate substrate. \u201cWe usually test about three or four surfaces, and see which one gives us the best signal-to-noise ratio for a given assay,\u201d says Michael Snyder of Yale University. There has been an explosion of options in the market, and several vendors \u2014 such as Xenopore of Hawthorne, New Jersey, and Schott of Jena, Germany \u2014 specialize almost exclusively in slides and substrates. Functionalized glass surfaces are a strong choice, as they virtually eliminate background fluorescence. According to Schena, TeleChem's SuperEpoxy glass slides are among the company's most popular products for protein-array construction. \u201cThe key benefit of the epoxide is that it's highly reactive at physiological conditions,\u201d he says. \u201cJust depositing proteins on an epoxide-coated surface results in covalent linkage to the glass.\u201d Three-dimensional substrates are also popular. For example, the Nexterion Slide H from Schott is coated with a covalently linked three-dimensional hydrogel with reactive groups that readily bind to proteins or peptides. \u201cSlide H preserves the three-dimensional structure of proteins, providing a hydrophilic cell-like or cytosol-like environment, thereby maintaining stability and functionality,\u201d says R\u00fcdiger Dietrich, Schott's director of research and development and technical support. PamGene of 's-Hertogenbosch in the Netherlands also takes advantage of a three-dimensional environment for its flow-through array platform. In its set-up, samples are pumped back and forth through a porous inorganic substrate where capture probes are immobilized. Nitrocellulose, a timeless classic for protein work, remains among the most popular substrates. \u201cWe still don't have a substratum that's better than nitrocellulose for its binding capacity \u2014 it's cheap and can be mass-produced,\u201d says Emanuel Petricoin, co-director with Lance Liotta of the Center for Applied Proteomics and Molecular Medicine at George Mason University in Manassas, Virginia. \u201cThe problem is that it has high background fluorescence and you have to come up with different labelling strategies to get around that.\u201d One solution uses an ultrathin nitrocellulose layer \u2014 such as the PATH slides from GenTel BioSciences of Madison, Wisconsin \u2014 which maintains binding capacity but reduces background noise. More specialized surfaces enable orientation-specific presentation of protein probes, such as nickel-NTA (nitrilotriacetic acid) surfaces for use with polyhistidine-tagged proteins. Lumera in Bothell, Washington, uses a protein\u2013protein interaction-based immobilization format for its arrays. \u201cWe have a proprietary protein-tag technology that's composed of two coiled coils that bind together with picomolar affinity,\u201d says product group manager Ronald Dudek. \u201cOne can be engineered into a protein or antibody library, the other is part of the surface chemistry.\u201d The pairing of streptavidin-coated slides with biotinylated proteins is also widely used. \n               Seeing the light \n             One aspect of arrays that has changed relatively little is the dominance of fluorescence as a method of detection. \u201cPeople generally use the standard Cy3 and Cy5 fluorophores, which are cheap, reproducible, and scanners are already configured to read them,\u201d says Petricoin. His group has done initial studies with semiconductor quantum dots, which offer the benefits of robust long-term fluorescence and enhanced potential for multiplexing. But for many investigators, generically reactive fluorescent dyes are sufficient. \u201cIn some ways, labelling is easier than with DNA,\u201d says Schena, \u201cbecause proteins are naturally highly reactive with fluorescent reagents.\u201d Some manufacturers improve the quality of fluorescent-array experiments with specialized instrumentation. The ZeptoREADER from Zeptosens in Witterswil, Switzerland, uses 'planar waveguide' technology, in which an evanescent electromagnetic field is generated by directing light into a specially designed array substrate through a diffraction grating. \u201cThe key benefit is that you excite only fluorophores that are bound on the surface,\u201d says managing director Markus Ehrat. \u201cIn many conventional assays, you don't need to separate bound from unbound fluorophores.\u201d Alternatively, in PamGene's flow-through system, samples are repeatedly cycled through porous arrays while being imaged by a sensitive charge-coupled device camera, making it possible to take real-time kinetic measurements. For experiments involving particularly small samples or scarce protein targets, it might be necessary to amplify the signal. One way to do this is rolling-circle amplification (RCA), first adapted for use with protein arrays by David Ward at Yale University. In RCA, circular DNA molecules are hybridized to capture probes that are conjugated to a detection antibody. These are then repeatedly replicated by DNA polymerase and detected with complementary fluorescent probes. Another alternative involves enhanced chemiluminescent detection with tyramide signal amplification, a system available from PerkinElmer of Wellesley, Massachusetts. As effective as these techniques have proven, they all require a degree of tagging or modification, which can be time-consuming or even impractical for certain samples, such as clinically derived preparations. So many researchers are keeping their eyes on the evolution of 'label-free' technologies that could greatly simplify future studies (see  'Losing the label' ). As with genomic arrays, the earliest application \u2014 and still the main use \u2014 of protein arrays has been the detection or quantification of targets from a research sample or clinical preparation. This typically involves a 'capture array' in which selected antibodies are spotted on to a substrate and are then exposed to the sample. Detection can be performed directly with a labelled sample, although this can increase the background noise. Many researchers prefer 'sandwich' formats in which a second, tagged antibody is applied that recognizes a different epitope from the capture antibody. This considerably improves sensitivity and the signal-to-noise ratio, but also requires two high-quality antibodies for every target. Antibody arrays have been commercially available for some time, but companies have taken different approaches to them. Sigma-Aldrich of St Louis, Missouri, for example, has several arrays in its Panorama product family, each containing 100\u2013200 antibodies targeting proteins involved in processes such as signal transduction or gene regulation. The company also plans to release a broad-content array with more than 700 antibodies. \u201cOur current pathway arrays will be included, in addition to various other signalling and regulatory gene-product antibodies,\u201d says market-segment manager Richard Pembrey. TeleChem, on the other hand, leaves antibody selection to the users for its ArrayIt microarrays. \u201cFor now, it's custom work just because a full proteomic complement of antibodies is still expensive for most researchers,\u201d says Schena. \u201cBut our printing technology scales pretty nicely, and our products can span a pretty wide spectrum, between 100 elements and 50,000.\u201d After encountering limited success with capture arrays in their clinical research, Liotta and Petricoin developed the reverse-phase protein array, in which samples are spotted on the array and then probed with detection antibodies, requiring only one antibody per analyte. Much of their work has centred on signalling pathways in human disease, and this technique has worked well with their research, they say. \u201cFrom a few thousand cells obtained by laser microdissection, we can look at hundreds of phosphorylation endpoints quantitatively, and look at a target and all the downstream signalling around it,\u201d says Liotta. Such sample-specific arrays are difficult to commercialize, although Zeptosens is attempting to address the needs of this community with its cell lysate array (CeLyA) product line, which provides users with protocols, reagents and equipment to prepare chips for reverse-phase experiments. The company also has an active service division. \u201cWe deposit roughly the content of one or two cells per spot,\u201d says Ehrat, \u201cand we can monitor changes of 15% from control to a treated sample.\u201d But both approaches face a key limitation: antibodies. \u201cA lot of users have two problems,\u201d says Mathias Uhl\u00e9n, a researcher at the Royal Institute of Technology in Stockholm, Sweden, and chair of the Human Antibody Initiative of the Human Proteome Organisation (HUPO). \u201cOne is that you buy reagents, and half of them don't work in your application, and the second is that it's not easy to buy 200 antibodies that work on a single platform.\u201d Part of the solution lies in thorough validation, but other problems arise from the feature density of today's arrays and the broad range of protein expression. \u201cEven if you have an antibody with picomolar affinity for your target, where the background is micromolar; if you have cross-reactivity against a protein that is 10 6  times more abundant, you will see that protein first,\u201d Uhl\u00e9n says. Some are exploring alternative affinity reagents, such as recombinant single-chain antibodies or nucleic-acid aptamers (see  'An apt solution?' ), but most in the field still see the limits of antibodies as secondary to their strengths. \u201cSo far, the good old antibody is still going strong,\u201d says Uhl\u00e9n. \n               All together now \n             High-content protein microarrays have brought the classic protein\u2013protein interaction assay to levels of throughput previously only possible with two-hybrid assays. These 'proteome chips' originated in Snyder's lab, in the form of arrays composed of protein products from nearly 6,000 yeast open reading frames. These chips are now available from Invitrogen of Carlsbad, California, which has continued to develop these and other arrays as part of its ProtoArray product line. Invitrogen sells both human and yeast proteomic arrays in a variety of formats, designed for use in protein\u2013protein interaction assays as well as functional studies. Developing arrays of soluble \u2014 and ideally, functional \u2014 protein at this scale poses considerable challenges, says Paul Predki, vice-president of proteomics at Invitrogen. \u201cOur latest array product has more than 8,000 human proteins, and you can imagine the challenges of doing quality control for the functionality of all of these proteins, especially when for some 15% we don't even know the function, and other proteins have multiple functions,\u201d he says. Invitrogen bypasses some of the risks of misfolding or improper processing by expressing proteins in insect cells rather than in bacteria. \u201cAlmost all of our human proteins are expressed using baculovirus and we've found, for example, much higher success rates in obtaining active kinases,\u201d says Predki. Snyder is also continuing his work with the yeast arrays, which now cover roughly 75\u201380% of the yeast proteome. These proteins have been cloned as both C- and N-terminal glutathione  S -transferase (GST) fusions to improve the odds of obtaining well-folded fusions, although some proteins refuse to comply. Several groups are now attempting to resolve this issue with chip formats in which transcription and translation of cloned genes are performed on the chip (see  '(Almost) no assembly required' ). Proteome chips also have clinical promise, and Snyder suggests that they could be valuable for drug screening. \u201cAny drug company with a set of lead compounds should put them on a protein chip and see what the possible side effects might be, or what their targets are,\u201d he says. \u201cI think that will be the future.\u201d Pathogen-derived proteome chips may also have clinical value \u2014 Snyder recently developed a coronavirus protein array for diagnostic use, and Joshua LaBaer, director of the Harvard Institute of Proteomics in Boston, Massachusetts, has developed full proteomic arrays for the pathogens  Francisella tularensis  and  Vibrio cholerae . \u201cThe idea would be to take serum from individuals who have been infected, both pre- and post-immune response, and look at the proteins against which the patient has mounted a response,\u201d says LaBaer. JPT Peptide Technologies in Berlin, Germany, is examining similar applications, with microarrays displaying overlapping peptides comprising the full  Mycobacterium tuberculosis  proteome. \u201cWe started with a small subset of 10,000 peptides from tuberculosis markers,\u201d says managing director Michael Schutkowski, \u201cand found that the chip is more predictive and sensitive than all the other tuberculosis tests on the market.\u201d \n               Where form meets function \n             Functional information is available for only a subset of the more than 23,000 proteins in the human proteome, and much of this is incomplete or even anecdotal. A truly comprehensive understanding of biological processes requires the building of functional proteomic maps, and protein arrays could be a perfect tool for this. Snyder and his team made a step forward when they applied their proteome chips to the assembly of a yeast phosphorylation map, incubating their microarrays with 87 different yeast protein kinases, and using radiolabelled ATP to identify proteins that were being modified. \u201cWe showed that closely related kinases had different specificities,\u201d says Snyder. His lab was also able to combine its findings with other genomic and proteomic data to identify regulatory 'modules' of interacting proteins that seem to be conserved in other eukaryotic species. \u201cIn my mind, that's the power of '-omics',\u201d he says. \u201cWe can come up with new principles by looking at these large data sets. Invitrogen's arrays are also suitable for such assays. \u201cThe same application is fully validated on our human array,\u201d says Predki. And Sigma-Aldrich has a product for kinomic analysis, the Panorama Human Kinase v1 array, based on technology licensed from Procognia in Maidenhead, Berkshire, UK. This array incorporates 152 different human kinases for the identification of interacting partners and substrates, or the analysis of putative therapeutic compounds. On the clinical side, Liotta and Petricoin have found success using phosphospecific antibodies with reverse-phase arrays to characterize differential phosphorylation in biopsy specimens, and their arrays are now undergoing clinical trials as a diagnostic or prognostic tool for cancer. Several groups are also developing array formats for glycan analysis. Brian Haab of the Van Andel Institute in Grand Rapids, Michigan, has used lectins \u2014 a family of proteins with specific binding preferences for different types of glycans \u2014 as a probe for monitoring glycosylation patterns of array-bound proteins. \u201cWe now work with about 20 different lectins,\u201d says Haab, \u201cand we recently demonstrated that we can do partial digestion of glycan structures using glycosidases to expose additional underlying structures to lectins, allowing us to get more complete information.\u201d These are mainly being used to characterize glycosylation changes in disease states, and Haab has licensed his platform to GenTel for commercial development. Snyder, meanwhile, is interested in using such arrays for discovery, and has used polyclonal antibodies against yeast glycans to reveal hundreds of previously unidentified glycoproteins. QIAGEN of Venlo, the Netherlands, will soon launch a glycomics array product, the QProteome GlycoArray kit, based on technology licensed from Procognia, which uses immobilized lectins as a capture reagent. Peptide arrays are a useful alternative for functional assays, and JPT has applied a peptide-synthesis process to develop a variety of array products. \u201cThe peptide chip is much more stable compared with a protein chip,\u201d says Schutkowski, \u201cand because we have the peptides beforehand in a microtitre plate, we can analyse each peptide for purity and whether it has the right content and the right modifications before we immobilize it.\u201d JPT has used literature- and data-mining to design peptide libraries containing thousands of putative kinase targets, which can be used to identify recognition sequences for tyrosine or serine/threonine kinases. The company also offers similar arrays for phosphatase and protease target identification, as well as 'random' arrays that can potentially reveal previously unidentified target sites. PamGene uses a peptide-based format in its PamChip arrays for analysing the kinetics of tyrosine and serine/threonine kinases against hundreds of arrayed probes. \u201cIC 50  results, selectivity data, mechanism of action information and multiple rate constants can all be obtained in one experimental run,\u201d says PamGene's vice-president of technology, Rinie van Beuningen. As these various applications strive towards full maturity, those in the protein microarray field are wrestling with the same issues that have confronted users of DNA arrays \u2014 experimental standardization and accurate quantification. The former issue is now being investigated by HUPO, which is looking to develop guidelines for experimental design and data annotation. But the key to accurate quantification will most likely lie in technological evolution. \u201cRight now these data are just semi-quantitative, but I think that with the right technologies they could be made completely quantitative,\u201d says Snyder. \u201cI'd like to see a future where people can just think up an experiment, buy an array, and do it.\u201d Reprints and Permissions"},
{"file_id": "4391021a", "url": "https://www.nature.com/articles/4391021a", "year": 2006, "authors": [], "parsed_as_year": "2006_or_before", "body": "High-throughput protein-purification pipelines are becoming part of many proteomics efforts. But how to choose the targets to put through the pipeline? For those who want to focus on targets relevant to a disease-related process, it is important to catalogue where, when and to what extent a protein is expressed. Currently the main method for determining the protein complement of a given cell or tissue uses two-dimensional (2D) gel electrophoresis or liquid chromatography to resolve the proteins in the sample, and mass spectrometry to then identify the individual proteins. A problem with this approach is that it is difficult to identify rare proteins, because cell extracts are dominated by a few very abundant proteins. Beckman Coulter has developed a number of innovations for protein fractionation to address this problem. \u201cWe first selectively remove the most abundant proteins from a sample,\u201d says Jerry Feitelson, marketing manager for the company. The ProteomeLab IgY-12 partitioning kits selectively remove the 12 most abundant proteins \u2014 albumins and immunoglobulins \u2014 from human/primate serum or plasma. These proteins make up 95% of the total protein expressed by cells. So removing them increases the chances of identifying proteins that are not highly expressed and are probably more interesting to researchers. The technology uses antibodies bound to inert beads, which are then packed in liquid-chromatography columns or, for smaller samples, spin columns. \u201cThe advantages of our reagents are the greater capacity, increased specificity and the ability to bind across species,\u201d says Feitelson. The enriched material can then be collected and further fractionated with the Beckman Coulter ProteomeLab PF 2D protein fractionation system. The instrument divides proteins in two dimensions: first, in a 2D gel, samples are separated by isoelectric focusing and size, and then automatically injected into a liquid-chromatography column. The process is fully automated. \u201cOne run takes ten hours, so you can do two in one day,\u201d says Feitelson. Detailed protein maps can be constructed for easy comparison between two samples using the Proteome Lab software suite. Interesting proteins can then be further analysed by cutting out the corresponding band, for example, and putting it into a mass spectrometer. Pall Corporation has also introduced the Enchant Life Sciences kits for albumin depletion and immunoglobulin G purification. \n               L.B. \n             Reprints and Permissions"},
{"file_id": "444960a", "url": "https://www.nature.com/articles/444960a", "year": 2006, "authors": [], "parsed_as_year": "2006_or_before", "body": "Larry Gold has a solution for the problems faced by users of antibody arrays \u2014 stop using antibodies. Gold, founder and chief executive of SomaLogic in Boulder, Colorado, is outspoken in his belief that aptamers \u2014 small nucleic-acid molecules with specialized functional properties \u2014 should replace antibodies in capture arrays. Years ago, Gold and his team tried to calculate how much multiplexing could be done with an antibody array before background noise became a significant issue. \u201cWe guessed that sandwiched antibody arrays were going to start running into serious noise problems when there were 20 analytes per array,\u201d Gold says. He thinks that aptamers could provide a level of specificity that surpasses what most antibodies achieve. More than 15 years ago, Gold was one of the inventors of a now widely used procedure called SELEX \u2014 systematic evolution of ligands by exponential enrichment. In this system, multiple rounds of selection and amplification can be used to select for DNA or RNA molecules with high specificity for a target of choice. Gold and his colleagues have since enhanced the procedure, and SomaLogic uses a high-throughput version of SELEX to generate 'photoaptamers', which can be covalently crosslinked to bound targets following irradiation. As a result, aptamers with high affinity and specific crosslinking can be used to measure proteins in complex samples without needing the extra specificity provided by secondary antibodies. Another advantage of the aptamer platform is simplicity of detection \u2014 once protein targets are bound, they can be labelled with a generic protein-binding fluorescent dye, eliminating the need for analyte-specific sandwich detection reagents. Early platform tests have been promising \u2014 SomaLogic is achieving success rates of about 80%, Gold says, and he believes a product launch is imminent. \u201cWith the right partner, we could launch a product for the research market within some months,\u201d he says, \u201cbut we think diagnostics is key, and we're even more interested in transforming evidence-based healthcare.\u201d Some antibody experts are impressed by aptamers, but doubt whether they will unseat the current king. \u201cWhat Larry Gold and others have shown is that when it works, it works very, very well,\u201d says Mathias Uhl\u00e9n of the Royal Institute of Technology in Stockholm, Sweden, \u201calthough I would be surprised if aptamers are the dominant scaffold in the future, due to limitations in the chemical space.\u201d But SomaLogic is banking on the use of novel nucleotides to increase this chemical space, and believes that a full proteome complement of aptamers is unnecessary \u2014 so that high specificity against a few thousand well-chosen targets could be more than sufficient. \u201cWe think that, for diagnostics, there's so much redundancy in biology that you'll be able to do useful biomarker discovery with an incomplete proteome that's still quite large,\u201d says Gold. \n               Michael Eisentein \n             Reprints and Permissions"},
{"file_id": "444959b", "url": "https://www.nature.com/articles/444959b", "year": 2006, "authors": [], "parsed_as_year": "2006_or_before", "body": "The appeal of using arrays free from fancy adornments \u2014 fluorescent, radioactive or otherwise \u2014 is fairly obvious. Such arrays could eliminate extra work and reduce errors in detection and analysis. Surface plasmon resonance (SPR) is a promising technology for this approach. In SPR the interactants are fixed to a gold-coated substrate, and sample binding is detected as mass concentration-dependent changes in refractive index at that spot, which makes it possible to monitor binding in real time. \u201cYou can look at specificity, affinity, kinetics, make concentration measurements, and work in a range of different sample environments,\u201d says Gary Franklin, industrial-sector specialist at Biacore. Recently acquired by GE Healthcare in Little Chalfont, UK, Biacore has pioneered the development of SPR platforms for a wide variety of proteomics applications. It has two main options for array users. In the Flexchip platform, 400 interactants can be spotted on a slide and then screened against a single sample. In contrast, the A100 is limited to 20 immobilized interactants but, thanks to parallel flow systems, it can perform up to four simultaneous screens with large numbers of samples. \u201cThe A100 can look at up to 3,800 interactions per day in a variety of different, multiplexed ways,\u201d says Franklin. Lumera of Bothell, Washington, is a relative newcomer to the market. It takes advantage of extremely rapid optical-switching technology, originally developed for telecommunications, in its ProteomicProcessor SPR instrument. Throughput was a key limitation of early SPR array experiments, and Lumera says its switching technology has reduced the problem. \u201cWe're basically limited by the size of the slide and how small you can print your spots,\u201d says Tim Londergan, director of the company's bioscience business unit. \u201cYou can really see this tracking to DNA microarray scale.\u201d Lumera has nearly finished testing its 'beta' instrument and plans to launch its first commercial system in January 2007. But SPR is not without limitations, and reduced sensitivity remains a common complaint. \u201cMy feeling is that there may be a somewhat limited dynamic range,\u201d says Joshua LaBaer, director of the Harvard Institute of Proteomics. \u201cIt's a fairly narrow window, and you have to be able to figure out how to get your chemistry within that window.\u201d Other technologies are also emerging, such as the arrays based on microcantilevers made by Protiveris of Rockville, Maryland, or the atomic-force microscopy system from BioForce Nanosciences of Ames, Iowa. Most of these platforms are in their infancy, but technology development is under way and many microarray users foresee a label-free future \u2014 one way or another. \u201cIt's going to revolutionize the way we think about things,\u201d says LaBaer. \u201cIt won't happen in ten days or a year, but it's going to happen.\u201d \n               Michael Eisentein \n             Reprints and Permissions"}
]