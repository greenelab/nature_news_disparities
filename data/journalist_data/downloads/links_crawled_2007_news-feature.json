[
{"file_id": "4501150a", "url": "https://www.nature.com/articles/4501150a", "year": 2007, "authors": [{"name": "Gabrielle Walker"}], "parsed_as_year": "2006_or_before", "body": "In 2007, the Intergovernmental Panel on Climate Change delivered its most thorough and authoritative assessment yet \u2014 and shared a Nobel prize for its efforts. Gabrielle Walker profiles its indefatigable leader. \n                     Bali conference \n                   \n                     Nature's IPCC coverage \n                   \n                     Climate feedback blog \n                   \n                     Nature Reports Climate Change \n                   \n                     The IPCC \n                   \n                     TERI \n                   \n                     2007 Nobel Prizes \n                   \n                     CricInfo India \n                   Reprints and Permissions"},
{"file_id": "450342a", "url": "https://www.nature.com/articles/450342a", "year": 2007, "authors": [{"name": "Jeff Tollefson"}], "parsed_as_year": "2006_or_before", "body": "For the first time, the US Congress has begun crafting comprehensive legislation to tackle global warming.  Nature brought together five experts with various backgrounds to discuss the current political climate as the United States moves towards mandatory emissions caps. A full transcript of this discussion can be downloaded  here  . See Editorial,  page 319 \n                     Energy for a cool planet \n                   \n                     Climate politics special \n                   \n                     Full transcript \n                   \n                     World Resources Institute \n                   \n                     National Commission on Energy Policy \n                   \n                     Climate Institute \n                   \n                     Duke Energy \n                   Reprints and Permissions"},
{"file_id": "450021a", "url": "https://www.nature.com/articles/450021a", "year": 2007, "authors": [{"name": "Ishani Ganguli"}], "parsed_as_year": "2006_or_before", "body": "When most people look at lobsters, they see dinner. Eve Marder saw a key to the theoretical underpinnings of animal behaviour. Ishani Ganguli reports. Larry Abbott was a theoretical particle physicist at Brandeis University when he first met Eve Marder at an off-campus retreat to celebrate the induction of a new centre for neuroscience. It was the late 1980s and he was just starting to dip his toes into the statistical mechanics of neural networks. But he wasn't sure what to expect from Marder, who had already made a name for herself as an iconoclast in the field. Abbott talked about his mathematical models, and Marder told him he needed to see a 'real' neural network in person, so she invited him to visit her lab. A few weeks passed back at the Brandeis campus in Waltham, Massachusetts, and Abbott's student needled him about the invite. ?I finally listened,? he says, and he was blown away. Along a bench, tiny grape-like clusters of nerve cells pulsed away in Petri dishes. Their activities weren't directly visible, but the rhythmic discharges between nerves ticked away on a chart recorder attached to the cells by electrodes. ?It was so fascinating ? what was going on ? that I thought, 'I should become a neuroscientist,'? says Abbott. And, with help from Marder, he did. Marder spent hours with him, meeting daily to answer his questions and later to discuss theories that would define both of their careers. Something for which Abbott, now a long-time collaborator of Marder and a professor of theoretical neuroscience at Columbia University in New York, is grateful: ?I was a nobody,? he says. ?She could have easily given me a quick tour and forgotten about it. But she didn't. She's quite remarkable that way in not giving up on unlikely sorts of people.? For nearly four decades, Marder has taken on all kinds of unlikely sorts in the hope of forging a path of her own design in neuroscience. The 59-year-old has crafted a tremendous body of work on ? of all things ? the stomachs of lobsters and crabs bought from local fish markets, using the creatures' gastric wiring as an investigational model to launch several fields of inquiry and dozens of careers. Her work has explained how even simple neural networks can create diverse functional repertoires, and how the homeostatic mechanisms within allow these networks to produce reliable behaviour patterns despite the constant turnover of nerve-cell components. Marder first found herself at Brandeis as a teenager in autumn 1965. Any scientific aspirations she had before going to college withered in the political heat of the time, she says. Demonstrations and voter-registration drives led her to dream of becoming a civil-rights lawyer. She declared a major in politics. But in her junior year, Marder followed a room-mate into a course on abnormal psychology that changed everything for her. These were the days when the blame for schizophrenia and many other psychological ills was placed squarely on parents. Marder was intrigued by the idea, then heretical, that schizophrenia had a biochemical cause and that dysregulation of neurotransmitters ? which relay electrochemical signals from nerve to nerve ? could vastly influence a person's grip on reality. So she holed herself up in the Brandeis library and read every book she could find on how the brain seemed to use the nerve signal-dampening inhibitory neurotransmitter, gamma-aminobutyric acid (GABA) to keep itself in check. This first introduction to neurotransmitters sparked a lifelong curiosity about nerve-cell communication. She switched her major to biology and never looked back. In 1969, Marder started graduate school at the University of California, San Diego. It was there, under Allen Selverston, that she first encountered the lobster stomatogastric-ganglion (STG) system. This neural network comprises just 30 neurons that control the gut muscles of lobsters and other crustaceans ? allowing them to grind food using gastric 'teeth' and then pass it down their digestive tract. It has become the best-studied example of a central-pattern generator, the same type of rhythmic neural circuit that controls breathing and chewing in humans. Excised and secured on a Petri dish, this network will fire rhythmically to the beat of its own drum for hours, without any outside input. The STG's large neurons, easy to identify and record from the preparation, provide a ready glimpse into the tiny but powerful circuit.  \n                The big picture \n              Whereas Marder says that most people in neurobiology were studying single neurotransmitters such as serotonin and GABA in isolation, she had broader questions in mind. As many as eight neurotransmitters had been identified by then, and she wanted to know why so many were required in the brain. As she was discovering the system that would define her research career, Marder was also working to define her values as a scientist. As a graduate student, she watched fellow neuroscientists scramble to purify the nicotinic acetylcholine receptor ? a critical part of nerve and neuromuscular signalling ? and quickly learned something about herself. ?[With] a consensus problem that everybody thinks is interesting, you have a built-in audience and built-in peer group approval. But I realized I would never choose to work that way,? she says. Her own vision would matter little, Marder says, in an ?enterprise where the goal was so well defined that it was going to get done no matter what?. So she went after the circuitry of the STG system. Glutamate had already been established as the neurotransmitter at play at many of the synapses ? the connections between nerve cells. With painstaking measurements of electrical activity in the 30 cells of the ganglion, and by searching for biochemical clues, she discovered that it wasn't alone. Acetylcholine ? the neurotransmitter of choice in the nerve to muscle interfaces of vertebrates ? was also at play in half of the neuromuscular junctions in this arthropod system. She found that like glutamate, acetylcholine could excite or inhibit depending on context, adding to a growing body of evidence that a neurotransmitter's effect could vary based on its targets 1 . The findings earned her her first publication ? in  Nature  ? and a PhD. Marder continued her pursuit of neurotransmitters over four years of postdoctoral training at the University of Oregon in Eugene and the \u00c9cole Normale Sup\u00e9rieure in Paris, France, where she honed her biochemistry, electrophysiology and biophysics techniques. In 1978, Marder turned down a tenure track position at Cornell University in Ithaca, New York, to return to her alma mater as an assistant professor, and has not left since. Neuroscientists had always treated networks as electrical systems with fixed connectivity. But an offhand observation Marder had made as a graduate student led her to question this dogma. In the process of figuring out which neurotransmitters the ganglion cells use to communicate, Marder had dumped candidate after candidate onto the system. She had noticed at the time that many of these molecules had an effect on the output of her tried and true lobster preparation, even those that weren't acting as neurotransmitters. But she hadn't known what to make of this at the time. Newly situated at Brandeis, she began to investigate with her budding lab team. What she soon began to realize was that these substances ? acting on a time course of hundreds of milliseconds to hours, a glacial pace in the firestorm of STG signalling ? were 'neuromodulators'. This was heresy. ?People working in vertebrate systems still thought that vertebrate neurons ? were very simple on?off figures.? But modulators meant that the systems didn't have to be hard wired. Neurons can release one or several neuromodulators ? some of which are also neurotransmitters, such as serotonin, and others, such as the peptide proctolin. Like hormones, they bind to receptors on other neurons, triggering long-lasting changes in how the neurons respond to the fast-acting neurotransmitters that allow cells to communicate. ?They can do all sorts of interesting things,? she says, such as alter the intrinsic excitability of cells, the amount of neurotransmitter released each time a neuron 'fires', or the firing patterns as a whole. She began to parse out which neuron types have which receptors for these functions 2 . New antibodies against potential modulators eased the process of searching intact tissue for these modulators through the late 1980s and early 90s. ?Every year or two would add one or two more,? says Marder. Mass spectroscopy accelerated the search in the late 1990s. Within the next 10 years, she says, the full cast of 50-odd characters in the neuromodulation story should be assembled ? by her lab and others. At that point, she says, they will get a clearer picture of how all the parts interact.  \n                From lobsters to humans \n              The applicability of her work on crustaceans to human neuroscience is a point that animates her. Neuromodulators have been found in all animal brains.?If you don't think they're doing the same thing, you're just silly,? she says. ?As far as I'm concerned, evolution never throws away a good mechanism.? Although experts were not open to the neuromodulators' snail-paced mode of communication between neurons, Marder considered it a logical step. ?It was so obvious to me that this is the way it was working that I was always surprised when people didn't think it worked that way.? Abbott, at least, had no reason to doubt her, and months after they met, their collaboration spawned the birth of the 'dynamic clamp': a neurophysiological method that allows researchers to simulate finely tuned neural networks using computer-manipulated nerve cells. Scientists can adjust different parameters, tweaking conductance through ion channels triggered by ligands or voltage changes, and see the effects on the circuit in real time 3 . The clamp is now used worldwide in diverse applications such as simulating heart muscle cells. At the same time, her collaboration with Abbott led her to ask new questions about the neurons she had studied for decades. The neurons were resourceful in modifying their patterns, but how did they manage to keep their basic performance intact over time? Marder had been working with lab members to build a model of the lateral pyloric motor neuron, an important controller of the stomatogastric system. ?I found it frustrating beyond belief,? Marder recalls. Coordinating the individual current contributions of ion channels in each membrane to predict the system's overall behaviour proved unwieldy. ?I said if the model is so sensitive to variations of each type that it takes a smart postdoc months and months to play with, how does the neuron get it right?? ?There have got to be some simple rules that let the neuron self-tune even though ion channels are turning over continuously over time,? she remembers venting to Abbott. It turns out there were. The independent variable was the output activity, not the number of channels mediating this activity. Marder and Abbott discovered that neurons were remarkably resilient when perturbed, changing their conductances to maintain a given output 4 . What are these rules by which the neuron gets its activity right? This question of homeostasis continues to drive her work. She is trying to figure out how cells maintain stable network performance over long periods of time despite the ongoing replacement of ions and ion channels. Today, as when she started, Marder says that about a dozen labs are working on the stomatogastric ganglion system. The niche field suits Marder's style: turning unexpected questions into mainstream ones, then passing the pursuit of their answers onto researchers working on model organisms that can best handle them, such as mice and flies. ?One of the dangers of working on a prep such as the STG is if you're not careful, you can fall into studying the system for the system's sake,? says Adam Taylor, a postdoctoral fellow in Marder's lab. ?She has an amazing ability to come up with ways to get at big questions that are relevant to neuroscience writ large within the stomatogastric ganglion of crustaceans.? And there are many more such questions to answer, says Marder. For example, her lab has shown that different mechanisms produce similar output patterns, but to what extent does this concept extend to more complex systems? ?Your respiratory system keeps you breathing, mine keeps me breathing. How different are they really?? she asks. She is also trying to understand how this built-in variability fits into the preservation of circuits as animals grow and develop. The neurons of adult lobsters are much bigger than those of juveniles, yet she has measured the same motor patterns in both generations, suggesting that the animal retunes properties such as cell-membrane resistance or the distance between nerve cells to produce the same effect.  \n                Do unto others \n              Taylor says Marder leads by example. ?It's quite a trick,? he says, how she ?manages both to make you feel like you should work harder and not make you feel depressed about your boss being a slave-driver?. Such skills may come in handy in November when she adds president of the Society for Neuroscience to her extracurricular activities, which already include editing the  Journal of Neurophysiology  and service on several advisory committees and review boards. Marder has a habit of downplaying her accomplishments ? which include induction into the National Academy of Sciences earlier this year, and winning the 2005 Ralph W. Gerard Prize from the Society for Neuroscience. During an interview at her home in down-town Boston, her husband ? Arthur Wingfield, also a professor of neuroscience at Brandeis ? gently chides her for her reticence as he lists her honours and responsibilities. But her spotlight-shyness is part of a desire to be so far out on the leading edge that no one notices her. At least not right away. It's a challenge she revels in. ?If you work on a non-consensus problem, you have the additional burden of having to do something that's interesting enough, novel enough or articulate enough to change the way people think,? Marder says. Certainly her work has articulated enough questions to keep others busy. ?There are parts of these problems that will be much better solved by people who can work on mice and flies and real genetic organisms.? For her own part, though, Marder's model loyalty is firm. It even extends beyond the bench. ?I don't like to eat lobsters anymore,? she says, ?because I find I just feel bad for them.? Ishani Ganguli is a freelance writer in Boston, Massachusetts. \n                     Neuroscience stories on Nature Network \n                   \n                     The Marder Lab \n                   \n                     The Abbott Lab \n                   Reprints and Permissions"},
{"file_id": "450336a", "url": "https://www.nature.com/articles/450336a", "year": 2007, "authors": [{"name": "Stephen Pincock"}], "parsed_as_year": "2006_or_before", "body": "Climate is shaping up as an issue in the 24 November Australian elections, as Stephen Pincock reports. Any campaign veteran will tell you that voters are fickle, switching from candidate to candidate and issue to issue as the whim takes them. But in Australia, voters may have changed their minds once and for all on the issue of climate change. In mid-2006, something seemed to shift climate from an 'issue of concern' to the top of the list of people's most serious considerations. Frank Muller, an expert on sustainability policy at the University of New South Wales in Sydney, recalls that the change took place while he was on a four-month visit to America last year. \u201cI went to the United States in June and was back in October, and in that period, this big switch had taken place,\u201d he says. Others pinpoint it even more precisely. \u201cI think in about September of last year there was a global shift in awareness of this issue,\u201d says Tim Flannery, a zoologist at Macquarie University in Sydney who won this year's Australian of the Year award for his climate activism. Blame any number of factors for the switch: Al Gore's visit to Australia in September 2006 (and Prime Minister John Howard's refusal to meet with him); the October 2006 release of Nicholas Stern's review of the economics of climate change, which estimated vast costs if global warming is not stemmed soon; and, looming over all, the drought that is plaguing Australia, by some measures the worst in a century or more. The timing could not be worse for Howard (pictured above), the Liberal prime minister who has led Australia for 11 years and is behind in opinion polls in his quest for a fifth term in office. On 24 November, Howard will face his Labor opponent, Kevin Rudd, in a federal election to determine who will form the next national government. And climate is shaping up as a major election issue. One of Howard's defining foreign-policy stances has been his refusal to ratify the Kyoto Protocol to control greenhouse-gas emissions; Rudd has vowed to ratify it immediately if elected. As the election date draws closer, both parties have released a slew of climate-related policies and promises in an effort to woo voters. They range from Rudd's Aus$200-million (US$185-million) plan to save the Great Barrier Reef, to Howard's vow to ratify an international agreement to replace Kyoto after it expires in 2012 \u2014 that is, as long as the agreement applies to major emitters such as China and India. What happens in Australia's election could shape the international dynamics on climate change for years to come. A week afterwards, representatives will gather in neighbouring Bali to start thrashing out details of a post-Kyoto strategy for limiting emissions ( see page 319 ). A change in Australian leadership could bolster morale, if not much else, among the delegates. And with a presidential election due the following November in the United States ( see page 340 ), some envisage a not-so-distant future in which the leaders of the two major countries that did not ratify Kyoto are instead in favour of mandatory reductions in greenhouse gases. For Australians, the potential consequences of climate change have been driven home by the country's water crisis. The five-year drought, often described as the worst in living memory, has left rural communities in the southeast of the country reeling and many urban areas with severe water shortages. Average annual inflows into Sydney's dams for the period 1991\u20132006, for instance, were 71% less than for 1948\u20131990. The drought is hitting home in many parts of the country, says Mike Young, professor of water economics and management at the University of Adelaide. \u201cPeople are looking at water shortages,\u201d he says, \u201cand farmers are finding that their water entitlements have been dramatically debased.\u201d Young says that in their minds, Australians are linking the drought to the longer-term issue of climate change. Opinion polls bear that out. In August, a poll conducted by the Lowy Institute for International Policy, a Sydney-based think-tank, suggested that climate change ranks ahead of nuclear weapons, Islamic fundamentalism and international terrorism as the external threat most Australians are concerned about. And earlier this month, the Climate Institute in Sydney commissioned a poll of 877 voters in 9 key marginal electorates. It found that 73% of voters thought climate change would have either a strong or a very strong influence on their vote at the election, an increase from 62% in August.  \n                All change \n              Howard's anti-Kyoto stance did not prevent him from being re-elected in 1998, 2001 and 2004. But that may be about to change. Although Australians aren't likely to cast their vote solely on the basis of climate-change policies, the divergent positions of Howard and Rudd are becoming a major force in the current election campaign, says veteran pollster David Briggs from Galaxy Research in Sydney. \u201cIt is one of those areas where Rudd has been able to extract some point of difference,\u201d says Briggs. \u201cIt does make a contribution to the perception that Rudd is the man of the future, and that he has a vision for the future that includes proper environmental planning, whereas John Howard is a man of the past.\u201d For much of his time in office, Howard has questioned the scientific basis of anthropogenic climate change, says Peter van Onselen, a political scientist at Edith Cowan University in Perth and co-author of a biography on the prime minister. \u201cHe was comfortably within the category of being a climate-change sceptic,\u201d van Onselen says. \u201cHe felt that the science on it, at least what was presented to him, went both ways as to the significance of it or not.\u201d Guy Pearse, a former speechwriter for Robert Hill, one of Howard's previous environment ministers, argues that the prime minister's perspective was heavily influenced by Australia's mining and energy sectors. Pearse's book  High & Dry details, on the basis of interviews with key players in the country's greenhouse-gas policies, what he calls the 'greenhouse mafia'. According to Pearse, lobbyists and advisers from industry groups sold Howard the idea that Australia's economy rests on the supply of cheap fossil fuels. Their message, he says, was simple: \u201cWhen it all boils down, it's about avoiding cuts in Australia's emissions for as long as possible, delaying as long as possible.\u201d But even within Howard's cabinet, there has been dissent on the subject of climate. In late October, the  Australian Financial Review broke a story that Howard's environment minister, Malcolm Turnbull, had six weeks earlier failed in his attempt to convince the cabinet to ratify the Kyoto agreement. Howard has not denied the charge, and Turnbull has not spoken publicly about it. Analysts agree that Howard tends to view the issue of climate change through the prism of economics. Like US President George W. Bush, Howard has long said that he thinks mandatory emissions reductions could hurt his country's economic growth, and that excluding developing countries such as China from the agreement would hurt Australia competitively. This came to the forefront in 2002, when Howard decided not to ratify the Kyoto Protocol, despite having signed it.  \n                Clean green \n              In place of mandatory emissions reductions, Howard opted for several clean-energy initiatives. In June 2004, for instance, his Liberal\u2013National coalition government announced a new policy called Securing Australia's Energy Future. It includes a Aus$500-million fund to encourage the energy sector to develop lower-emission technologies, and Aus$75 million targeted at solar-energy projects. Four months later, Howard was re-elected. Environmental groups have criticized Howard's continuing focus on fossil fuels. According to the Australian Bureau of Agricultural and Resource Economics, in 2004\u201305, 93% of the country's electricity was generated from fossil fuels (coal, oil and gas), and 7% from renewables such as hydroelectricity, wind, biomass and biogas. That could have something to do with the nation's 73-billion-tonne coal deposits, found mostly in New South Wales and Queensland. Coal also forms the backbone of a robust trade with nearby China. \u201cWhatever may be the merits of renewables,\u201d Howard told ABC radio in 2004, \u201cthe reality is that the older fuels of which we have large supplies are going to contribute the bulk of our energy needs and what we have to do is to make them cleaner.\u201d Hence his government's focus on emphasizing clean-coal technologies, such as carbon capture and sequestration, and on nuclear power as an alternative low-emission energy source in the future. The Howard government has made several major, if criticized, initiatives in the field of climate. In 2005, Howard took Australia into the Asia\u2013Pacific Partnership on Clean Development and Climate, a non-binding partnership established with China, India, Japan, South Korea and the United States to foster cooperation on climate-change action. Australia pledged Aus$100 million to the group over five years, from which it has so far allocated Aus$60 million to 44 projects. These include deployment of high-efficiency solar-power stations, improved efficiency standards for appliances and a mobile system that can test carbon capture at coal-fired power stations. In June 2007, Howard announced that the government would introduce a cap-and-trade emissions-trading scheme, which observers consider his most significant backflip on climate-change policy in the lead-up to the election. And in September, Howard placed climate change on the agenda for a meeting of the Asia\u2013Pacific Economic Cooperation (APEC) forum, which includes the United States, China, Russia and Japan. Without setting any hard targets, the APEC leaders signed up to a statement, which said they agreed \u201cto work to achieve a common understanding on a long-term aspirational global emissions reduction goal to pave the way for an effective post-2012 international arrangement\u201d.  \n                Conservative approach \n              The question for many observers is whether such actions will be enough to make a difference. In the run-up to the elections, Australian climate scientists are cautious about seeming overly partisan, and even the outspoken Flannery is reticent about openly criticizing any political party. But if pressed, he notes that Howard's climate announcements lack teeth. \u201cIf you add up all of those policies, all of the pronouncements,\u201d he says, \u201cthey make no difference in terms of emissions.\u201d In 2005, greenhouse-gas emissions in Australia reached 559 million tonnes of carbon dioxide equivalent, which is 2% higher than 1990 levels. Government figures show that per capita emissions fell between 1990 and 2005, from 32.3 to 27.6 tonnes of carbon dioxide equivalent \u2014 but they remain the second highest in the developed world, after Luxembourg. Dave Griggs, a climate scientist at Monash University in Melbourne and former head of the science working group secretariat of the Intergovernmental Panel on Climate Change (IPCC), argues that Australia's position in the ranks is not acceptable. \u201cThe IPCC fourth assessment report is very clear that we need to mitigate greenhouse-gas emissions by a very large fraction,\u201d he says. \u201cI don't see that the policies that have been put in place to date have been aiming to get an Australian economy in which emissions have gone down to a level that is probably required globally. Have current policies put us on the right track? The answer is clearly no.\u201d Climate is, of course, just one issue facing voters in the upcoming elections; the Howard and Rudd campaigns have also included ongoing concerns such as the economy, health care and taxation. For some, the prominence of climate echoes previous elections in which environmental issues played a role. In 1983 for example, a controversial proposal to dam a Tasmanian river helped to bring down the government of Malcolm Fraser. In the upcoming election, \u201cthe difference between the major parties over whether or not to ratify the Kyoto Protocol is as sharp as the difference in 1983 over the Franklin dam\u201d, says Muller. Before the election was called, for example, the Howard government committed to ensuring that about 15% of Australia's electricity would come from low-emission sources by 2020. It also launched a taxpayer-funded advertising campaign, featuring television commercials that focused on simple things families could do to reduce their greenhouse footprint, such as drying clothes on a washing line or switching to compact fluorescent lights (incandescent bulbs are being phased out entirely across the country).  \n                Power and the passion \n              Both parties have also opted for charismatic, high-profile figures as their environment representatives. In the case of the Labor party, the candidate is Peter Garrett, who was the front man for the rock group Midnight Oil and former president of the advocacy group Australian Conservation Foundation in Melbourne. The government's choice for environment minister is Turnbull, the minister who reportedly argued for ratifying Kyoto. Howard and Rudd themselves have focused on climate change at key moments. During the only televised debate between them in the campaign, on 21 October, Howard chose climate change for one of two new policy announcements (the other being on troops in Iraq). He vowed to establish a fund to pay for development of clean energy and to compensate low-income earners for hikes in power costs, funded by the revenue raised by auctioning emissions permits. For his part, Rudd made a point of reminding voters that his party had set a concrete target for a 60% reduction in emissions from 2000 levels by 2050. The Labor party has, however, refused to commit to a target for 2020 until after it receives an economic report it commissioned on climate change from Ross Garnaut from the Australian National University in Canberra. Last month, the Australian Conservation Foundation released an election scorecard designed to compare the parties on their approaches to climate change, sustainability and the environment. With four weeks to go, both parties scored poorly: Labor at 56%; Howard's coalition just 21%. All this leads some commentators to despair about whether the election can truly turn around the major climate issues facing Australia. \u201cWe love to romanticize about a sunburnt country,\u201d says Pearse, referring to the well-loved Australian poem  My Country. \u201cBut people need to start realizing that what we're talking about is a country burnt beyond recognition in our lifetimes \u2014 and unless we start acting, that's what we're going to cop.\u201d\n Stephen Pincock is a writer based in Sydney. \n                     Climate politics special \n                   \n                     Australian Prime Minister \n                   \n                     Australian Labor party \n                   \n                     Australian Conservation Foundation \n                   \n                     Australian Climate Exchange \n                   Reprints and Permissions"},
{"file_id": "450152a", "url": "https://www.nature.com/articles/450152a", "year": 2007, "authors": [{"name": "Emma Marris"}], "parsed_as_year": "2006_or_before", "body": "Not all species can be saved from extinction. Emma Marris talks to conservation biologists about prioritization and triage. Richard Cowling was playing with maps of South Africa on a computer screen when he had his epiphany. He was designing a conservation plan for the Cape Floristic Region, or fynbos, an arid landscape of shrubs and flowers that contains some 9,000 species, many unique to the area. Some of these, such as the mandala-like sunset blooms of the protea flowers, are spectacular. Some ? like the geometric tortoises, whose fetching shells help them hide from baboons and secretary birds ? are seriously endangered. Cowling, a conservation biologist at Nelson Mandela Metropolitan University in Port Elizabeth, was working on defining a set of reserves that would maximize the chances of conserving all those species. The project was so large that it would end up as a series of 16 papers by 36 authors that occupied all 297 pages of  Biological Conservation' s July?August 2003 issue. And it was also, Cowling realized as he stared at the screen, ?sheer nonsense?. ?I had to click on a couple of grid squares and the project would be complete,? Cowling says. ?And it dawned on me: complete for whom? There was no way that this reserve would ever happen. It had to be linked to some social realities on the ground.? In the preface to his 1981 book,  Extinction 1 , Paul Ehrlich, a biologist at Stanford University in California, provided a powerful parable for conservation biology: the story of the rivet popper. A passenger inspecting the plane he is about to fly in notices someone popping rivets out of the wings. When challenged, the rivet popper says that the passenger shouldn't worry because not all the rivets are necessary. For Ehrlich the rivets represent species and the rivet popper represents humanity, indifferent to the looming danger of ecosystem collapse and the end of the natural processes that supply raw materials of life such as clean water, wild food, carbon sequestration and climate regulation. In the apocalyptic style for which he has become famous, Ehrlich predicted that continuing to pop the rivets of ecosystems would lead to ?a crumbling of post-industrial society?. He demanded that the rivet popping be stopped. There aren't many, if any, conservation biologists who would disagree with that conclusion. In principle. The problem is that they don't have the resources to back up such ambition in practice. Spending on conservation by major international and non-governmental organizations has been estimated at around US$2 billion a year 2 . Given constrained resources, the biologists have to set priorities. 'Triage' is a dirty word in some conservation circles, but like many dirty words, it describes something common. Whether they admit it or not, conservationists have long had to make decisions about what to save. As more and more admit it, open discussion about how the decisions are best made ? by concentrating on particular species, or particular places, or absolute costs, or any other criterion ? becomes possible. Whichever criteria come into play, one thing remains constant. The decisions have to be made quickly. In the bloody business of conservation biology, the longer you pause to reorder your list, the more species will become extinct.  \n                Superfluous species \n              Perhaps the most controversial basis for triage is redundancy ? prioritizing those species that provide a unique and necessary function to the ecosystem they live in and letting go of those that are functionally redundant. It might seem sensible to lose a few rivets around the plane's over-engineered windows if that saves the rivets actually holding the wings to the fuselage. This idea was raised in the early 1990s by Brian Walker of the Australian Commonwealth Scientific and Industrial Research Organization 3 . ?Regrettable as it might be,? he wrote, ?it is most likely that global biodiversity concerns will ultimately reduce to a cost?benefit analysis. Without knowledge of redundancy, or more broadly, the relationship between the levels of biodiversity and ecosystem function, we cannot estimate either the costs or the benefits.? The majority opinion among conservation biologists today is that they still understand too little about ecosystem functions to say for sure which species are the 'load-bearing' ones whose presence keeps a complex, multi-tiered ecosystem from collapsing into some worst case dull scenario of rats, roaches and invasive grass. ?We are so fundamentally ignorant,? says Norman Myers, a fellow of the University of Oxford, UK, and adjunct professor at Duke University in Durham, North Carolina. ?We cannot afford, by a long, long way, to say which species are dispensable.? Andrew Balmford, a conservation biologist at the University of Cambridge, UK, tends to agree: spotting key species is ?an interesting exercise intellectually ? but by the time we've figured it out the forest will have gone anyway?.  \n                Save the genes \n             Not everyone is quite so convinced the problem is ineluctable. ?I think there are a lot of systems where we know more than we think,? says Reed Noss, a conservation biologist at the University of Central Florida in Orlando. ?If you can get naturalists to open up and talk about what they know, we can at least generate some testable hypothesis and do some manipulation if we have time.? Kent Redford, head scientist at the New York-based Wildlife Conservation Society, agrees, up to a point. ?Our big problem is that we have been raised to believe that unless you have complete information you cannot make recommendations, and I think that is something we are going to be put on trial for by our children. It's baloney.? But his belief that science might make this sort of prioritization possible doesn't mean he approves of it. ?I don't care if something is redundant,? he says, ?I want to save it for all these other reasons.? Perhaps aware of the resistance that functional prioritization might encounter, Walker's forthright paper suggested a complementary approach: taxonomic distinctiveness 3 . This turns out to be less contentious; although there are no organizations dedicated to sorting the load-bearing species from the non-load-bearing, there is at least one that dedicates its resources to saving the mammals that are phylogenetically distinct. The EDGE programme ? its intials stand for evolutionarily distinct and globally endangered ? of the Zoological Society of London argues for giving priority to endangered species of mammals that are far out on their own on the tree of life, without close relatives. The EDGE scheme gives each species a score derived from its position on a phylogenetic tree. A lone species out on a long branch gets a higher score because it is the sole bearer of genes that represent a very long period of evolution. Take the three-toed sloths, which parted company with the rest of the sloths some 15 million years ago. ?There are two species of three-toed sloth that only diverged 1 million years ago. If one went extinct, we would lose 1 million years, but if we lose both, we lose 15 million years,? says Nick Isaac, a research fellow at the Zoological Society who helps to run the EDGE programme 4 . ?You could make an analogy with art,? says Isaac. ?You are in a spaceship leaving Earth with three paintings. Do you take three Rembrandts, or do you take one Rembrandt, one Leonardo and one Picasso?? The group's top five targets for funding ? which at this point amounts to paying for a student in the countries where the animals live to study their conservation ? are the Yangtze River dolphin ( Lipotes vexillifer),  the long-beaked echidna ( Zaglossus bruijni)  of New Guinea, the riverine rabbit ( Bunolagus monticularis)  of the Karoo desert in South Africa, the Cuban solenodon ( Solenodon cubanus)  and its cousin, the Hispaniolan solenodon ( Solenodon paradoxus).  Similar to each other, but distinct from anything else, the solenodons merit two slots. Conservation favourites such as tigers, pandas and gorillas are noticeably absent from the list. There are variations on this theme floating about. Redford suggests that when a species is identified as endangered, a priority list of populations within the species should be drawn up based on genetic diversity. And a biologist who considers his idea a little too hot to put his name to suggests putting species that have future evolutionary potential at the top of the list. This means prioritizing current species according to their capacity for future speciation. Big, long-lived species face inherent disadvantages under this idea: such a list would have little room for elephants or whales. Or redwoods.  \n                Battle of the maps \n              A much more popular alternative to prioritizing species is prioritizing areas. There is less need to know how the ecosystem works ? just identify an area of interest and try to preserve it in its entirety. The first such scheme to gain real influence was Myers' hotspot map, which has been published in several incarnations since its inception 5  in 1988. The original version, which prioritized tropical forests above all other places, was persuasive enough for Conservation International, headquartered in Arlington, Virginia, and the MacArthur Foundation, based in Chicago, Illinois, to adopt it as a framework for their efforts. But like all prioritizing, it had its critics: ?I was told it was immoral, that all species are equal,? Myers recalls. The criteria he has used to define the hotspots are, Myers freely admits, somewhat arbitrary, and have evolved over time. In the 2000 version an area makes the grade if it contains at least 0.5%, or 1,500, of the world's 300,000 plant species as endemics ? that is, species that are seen nowhere else ? and has lost 70% or more of its primary vegetation 6 . In this iteration the Brazilian cerrado, the fynbos and other mixed grasslands joined the forests. Myers' hotspot map set a trend: it is now practically compulsory for every conservation organization to have its own priority map. The Cape Floristic Region received its journal-filling loving-care from Cowling and his peers in part because it had made it onto so many of these prioritization lists. As well as being an accredited hotspot under Myers's scheme it had also made it into conservation group WWF's 'Global 200' scheme. Birds found nowhere else, such as the protea canary and the orange-breasted sunbird, had propelled the area onto Birdlife International's Endemic Bird Areas list 7 .  \n                Priority actions \n              The fynbos demonstrates the extent to which maps will agree about things, which raises the question of why there should be so many. ?It has been a not terribly profitable exercise over the last ten years to have such a proliferation of schemes that are basically very similar,? says Georgina Mace, who runs the Centre for Population Biology at Imperial College in London, UK. ?They act as sort of branding for the organizations. It still surprises me that the big conservation organizations have not gotten together under a single banner, like Make Poverty History.? At the same time, partisans can detect ? and defend, debate and disparage ? various differences in approach. ?We have been arguing, or certainly jockeying, to present one piece of science as more legitimate or stronger than another,? says Jon Hoekstra, a senior scientist at the Nature Conservancy in Seattle, Washington. These squabbles are framed to suggest that there is one right answer ? one most valid way to prioritize areas. But different starting assumptions and different goals mean that many of the schemes are not directly comparable. ?We have to remember that they reflect the philosophical decisions made at the beginning,? says Hoekstra. The approach that currently enjoys perhaps the highest level of acclaim, at least scientifically, is that taken by Hugh Possingham of the University of Queensland, in Brisbane, Australia. His one goal is maximizing number of species conserved, and he loathes scoring systems. Instead he uses algorithms that measure real-world costs against benefits in terms of species number, and the resulting papers, colleagues say, are in a league of their own 8 . In his latest work he compares different actions in different places with each other, which is more complex than one might think. Land prices vary around the world, as does species richness. Many investments have diminishing returns over time: once a large chunk of one ecosystem is protected, turning a bit more into a park won't save many additional species. On the other hand, some interventions begin to pay off seriously only after a certain investment threshold is reached. ?If you were trying to get all the rats off an island, unless you invest enough to get them all off, you might as well not even bother,? explains Possingham. On top of all this is the problem that data on costs are infamously scanty ? so much so that many earlier analyses just used land area as a proxy, an astonishing simplification. ?In a sense, it is just about good problem definition,? says Possingham. ?If you don't do that right, you head down these scoring paths. The people who make them just have a feeling of which facts are important, and they throw them in.? Possingham tries to be as rigorous as possible, and sometimes that means not everything gets saved. ?A lot of people get upset with that. It basically says some regions aren't working at all. They are too expensive, the threats are too huge, or there are not enough species in them.?  \n                Mount Lofty's short straw \n             Consider, for example, the Mount Lofty woodlands of Australia, where eucalyptus trees shelter rare orchids, spiny echidnas and cockatoos. Surely it is worth preserving them from the invasive predators such as foxes and cats that threaten them? But in a trade-off between spending on the Mount Lofty ranges and on the montane regions of the fynbos, Possingham's algorithms give the money to the fynbos ? among other regional investments. The Australian woodlands get nothing, despite the fact the fact that Possingham, an avid birder, would bitterly regret losing part of the original range of the endangered regent honeyeater ( Xanthomyza phrygia);  he's particularly keen on honeyeaters. Putting this sort of insight into practice is not simple. Most Australian money isn't transferable to South Africa, any more than money given to preserve pandas can be spent on solenodons. But some of those who administer the sliver that is fungible ? people at the World Bank, the Global Environmental Facility and other large foundations ? are taking an interest in Possingham's approach. Peter Kareiva, the head scientist of the Nature Conservancy, is one of many researchers who has been co-authoring papers with Possingham on such return-on-investment models of conservation 9 ; a few years ago, as it happens, he rubbished the whole idea of hotspots in  American Scientist [10]. Balmford, too, is excited about these approaches. ?Possingham's new techniques on setting priorities dynamically, allowing you to shift from one to another, are really exciting,? he says. The difficulty is getting them adopted by managers and decision-makers on the ground. ?We have got to get away from conservation scientists handing down ideas from on-high to practitioners and expecting them to be received gratefully. It has got to be through examples, and from realizing from their peers that those things make sense.? This is the dreaded implementation gap, in which theory ignores practice and practice ignores theory. In the end, it may not matter which prioritization scheme is most scientifically defensible. What matters is that the people carrying out a scheme feel that it makes sense and will save species. On this pragmatic basis, many schemes shouldn't even be considered for implementation, says Hoekstra ? including some of his own work. ?I wrote this crisis eco-regions paper. It gives some real interesting perspective on the world. It highlights the crisis in temperate grasslands. But I don't think it is as useful to look at the map I generated to decide where to work; you could end up trying to restore something that is lost.?  \n                Armchair scientists \n              ?So much of this stuff is done by well-meaning people sitting as it were in their armchairs,? says Stuart Pimm, a conservationist at Duke University. Pimm recently eschewed the priority list for his own expertise and invested in some land in the Amazon he knew was ripe for conservation. ?You have to do what you think you can do. It is going to be based on imperfect information and it is going to be very, very strongly conditioned by local politics and economics and social conditions,? he says. Pimm aside, the armchair approach can seem deeply entrenched. Redford points out the perennial problem of papers that follow pages of science with a cursory command, ?that deadly last paragraph that begins 'managers should'?. For Noss, one solution is educating those managers. ?We need a system that can provide mid-career training to people who are going to be working in land-management agencies, ocean-management institutions, and in environmental consultancies. Otherwise they are going to keep using these more outdated and less defensible approaches to prioritization.? For an on-the-ground conservationist, such as Stuart Cowell, project coordinator with Bush Heritage Australia, the many different schemes have been influential, but not immediately applicable. ?We haven't taken those approaches off the shelf,? he says. Bush Heritage buys land with conservation value, but unlike the ideal maps on paper, some land is never going to come up for sale. What Cowell and his colleagues are asking themselves, he says, is: ?Is there a benefit to an organization spending the time and resources in doing this sort of prioritization, which looks good in theory but perhaps does not take us as far as just some good expert knowledge?? There are some small successes. Possingham has had some luck impressing government bureaucrats with the rigour of his analyses; some spending decisions in Australia have been made on the back of his work. And South Africa has had real success in bridging the implementation gap. ?The US and European style is that the scientists write it and hope someone picks it up, but the South Africans are trying to get the people who are going to implement it to help with the priorities,? says Redford.  \n                The messy reality \n              Since his conversion experience over the digital maps of the fynbos, Cowling has been one of those attempting to build the input of decision-makers and local people into his schemes from day one. ?The plans [I've worked on] were done not because they appealed to anyone's curiosity in an academic sense but because they were needed,? he explains. He's more interested in determining the possible than mapping the ideal. ?Through the process of negotiation [with stakeholders] you end up with a series of projects, and funding is sought.? And sometimes that which is sought is actually found. Cowling says that getting all conservation biologists to do their prioritization work with both feet on the ground ?will require a substantial change in how researchers operate?. ?Getting involved in the slushy stuff takes time. The kind of research is not likely to appear in the pages of high impact journals. You might get it into the pages of  Ecology and Society ,? he says. But his work is not going unrecognized, whatever its impact factor; Balmford singles Cowling out for praise as someone ?not just concerned with getting the algorithm to get the best bang for the buck, but with the more messy, more real, more interesting reality?. There is no reason why, in theory, one could not include the slushy stuff of real life as inputs in a prioritization scheme. ?People say that this mathematical approach can't account for anything, but it can,? says Possingham. ?The question is, can you put it in with a plausible number?? Imagine a platonic scheme in which one could include the intransigence of a particular politician, the likelihood of a coup in a certain country, the relative value of the US dollar, the effect of eco-fatigue among the donating public, and the looming spectre of climate change, each quantified and slotted into equations (along with values representing their uncertainty, of course). Such a marvel might give you the best tactics. But it would be no help in setting fundamental goals for future conservation ? a subject on which unanimity seems about as likely as a full recovery for the Yangtze River dolphin. \n               See  \n               Editorial \n               \n               \n             Emma Marris writes for  Nature  from Columbia, Missouri. \n                     Evolution & Ecology \n                   \n                     Biodiversity insight \n                   \n                     Society for Conservation Biology blogging at Nature \n                   \n                     Biological Conservation special issue on the Cape Floristic Region \n                   \n                     EDGE \n                   \n                     CI Page on Hotspots \n                   \n                     Peter Kareiva of The Nature Conservancy on hotspots \n                   \n                     Make Poverty History \n                   \n                     Ecology and Society \n                   Reprints and Permissions"},
{"file_id": "450024a", "url": "https://www.nature.com/articles/450024a", "year": 2007, "authors": [{"name": "David Cyranoski"}], "parsed_as_year": "2006_or_before", "body": "The Japanese make few charitable donations. David Cyranoski meets a patient advocate and scientist working to change a cultural reticence about giving. \n                     Philanthropy special \n                   \n                     Nature Reports Stem Cells \n                   \n                     Philanthropy in science \n                   \n                     Japan Spinal Cord Foundation \n                   \n                     The Japan Foundation Center \n                   \n                     Japan Philanthropic Association \n                   \n                     The Kidney Foundation, Japan \n                   \n                     Foundation Center \n                   \n                     Bill & Melinda Gates Foundation \n                   \n                     Juvenile Diabetes Research Foundation \n                   \n                     Wheels in Motion \n                   \n                     Nippon Keidanren \n                   \n                     The Miami Project to Cure Paralysis \n                   Reprints and Permissions"},
{"file_id": "450156a", "url": "https://www.nature.com/articles/450156a", "year": 2007, "authors": [{"name": "Ewen Callaway"}], "parsed_as_year": "2006_or_before", "body": "Ocean wave energy is trying to break into the renewable-energy market, but many challenges remain. Ewen Callaway reports. The North Sea is not known for calm days, and neither is its inlet called Nissum Bredning, 300 kilometres northwest of Copenhagen. On a typical afternoon, windsurfers skate across the grey-green water while birds seem to hover, frozen, in mid-air. Along the horizon stretch rows of giant white windmills, their long blades whirring in the gusts. ?There's really some good action today,? Per Steenstrup shouts over the gust as a mix of sea water and rain pelts his face. Steenstrup, an engineer from Copenhagen, is 200 metres offshore, aboard a steel platform with 20 large floats on each side lined up like oars on a Viking ship. A knee-high wave washes under the platform, and the floats move up and down a dozen centimetres in quick succession. Steenstrup opens the door to a small prefabricated structure on the platform, and suddenly a mechanical roar rises above the noise of the wind and sea. ?It's a 40-piston engine running on waves,? he yells. In the control room next door to the turbine, Steenstrup peers at a computer that keeps an instantaneous pulse on the Wavestar, as the platform is called. ?At the moment, the output of power is around 800 watts,? he says ? enough to run a large-screen plasma television. The wind turbines on the horizon, for their part, are components in a well-established system that produces 41 megawatts of wind power. That's enough on some days to power the entire agriculture-dense region. Yet Steenstrup and dozens like him think that power harvested from ocean waves will one day be competitive with other methods for extracting energy from the physical environment such as wind, solar and hydroelectric. Wave energy is applicable only in a few regions of the world, and uses technologies that, for the most part, remain unproven. But given the scale of the energy challenge facing the world, supporters say that wave energy could supply enough electricity to make it part of a green-energy portfolio. The European Ocean Energy Association in Brussels, for instance, estimates that the global resource for wave energy lies between 1 and 10 terawatts; the world currently produces about 13 terawatts from all sources. Others see a more realistic number of 0.2 terawatts, or less, coming from wave energy; that's still three times the current installed capacity for wind power worldwide. Whether wave energy becomes economical depends heavily on a new round of open-ocean tests that are under way from Portugal to Wales to Oregon. Engineers and entrepreneurs are field-testing machines that until now have been scale models in water tanks. Few are looking for profits; they just want to see if the technologies can produce a consistent amount of power from the ocean. Success could attract funds from investors, industry and utilities; failure could set the field back years. ?A lot is riding on how well the first sets of large-scale devices work,? says Tom Thorpe of the consultancy firm Oxford Oceanics in Grove, UK, which advises prospective wave-energy investors and developers. ?They've got to be either successful or, if they fail, there has to be a good reason why.?  \n                Blowing in the wind \n              Wave energy's most obvious parallel ? and, perhaps, competitor ? is wind energy. ?We are where wind was 25 years ago,? says Alla Weinstein, director of ocean energy at Finavera Renewables in Vancouver, Canada. Finavera's prototype, the Aqua Buoy, sank off the coast of Oregon last month after operating for two months and before it was scheduled to be taken out of the water later this month. A quarter of a century ago, world capacity for wind energy was around 90 megawatts. But that was a ninefold jump from just two years previously, and today countries such as Denmark and Germany get more than a tenth of their power from wind ? although it still accounts for just 1% of energy produced worldwide. Global figures for the power that waves produce are hard to pin down, as most projects are still in the testing phase. But when the industry's leading company, Pelamis in Edinburgh, Scotland, gets a new project online in Portugal ? as is expected within weeks ? that will add 2.25 megawatts from three machines. And wave-energy proponents think that the growth could be exponential after that. Waves offer several improvements over wind, although they are trickier to harvest. Wind is notoriously fickle; when gusts fail, utility companies have to deliver power to their customers from other sources. Waves can be fickle too, but are easier to predict, says George Hagerman, an engineer at Virginia Tech's Advanced Research Institute in Arlington, who forecasts wave-energy days in advance using weather satellites. Knowing when waves are coming and how big they will be can save utilities money by cutting down on the power they need to keep ready in reserve, he says. Water is not only more reliable than wind; it is also 800 times the density of air. Aboard the Wavestar, it's not a hard concept to grasp. ?This is a big guy,? Steenstrup says, eyeing an approaching metre-high curl like a surfer would. It hits head-on, and sea foam gushes over the floats. The steel platform shudders, even though it's anchored to the seabed by concrete piles. Like other renewable energies, wave power works better in some locations than others. It takes more than just a shore to harness the power of waves. Because of the planet's prevailing winds, the best spots are on the west coast of continents in the mid-latitudes of the Northern Hemisphere, or on the east coast in the Southern Hemisphere. Not coincidentally, most wave-energy tests are being installed in those spots in the North Atlantic and North Pacific oceans. Some of the strongest waves hit the Orkney Islands in Scotland, where the European Marine Energy Centre has established a wave-energy test site two kilometres offshore. The centre, which receives both government money and private funding, offers developers steady waves and easy connection to electricity grids to field-test machines. Pelamis began testing a 750-kilowatt wave machine there in 2004, although it is now gone. Four other manufacturers plan to join the site in the next two years. In the Southern Hemisphere, Australia-based Oceanlinx has been testing a 600-kilowatt machine off Port Kembla, New South Wales, since 2005, and is working on a larger, 2-megawatt model. In such places, wave energy could provide an alternative source of renewable energy to the usual standbys of wind and solar. Given recent government mandates to increase the power generated from renewable-energy sources ? the European Union is aiming for 20% from renewables, and California 33%, by 2020 ? wave power could be another much-needed option. The targets are aggressive enough that all options could be needed, says Dan Kammen, director of a renewable-energy laboratory at the University of California, Berkeley. Some energy companies and industrial giants are already starting to take notice. At a September conference in Porto, Portugal, representatives from the national utilities of France, Denmark and Portugal attended the usually science-focused meeting. In California, the utility behemoth Pacific Gas and Electric has sought permission to establish wave-energy test sites off the coast of northern California. And in 2005, the hydroelectric firm Voith Siemens in Heidenheim, Germany, purchased Wavegen, a Scottish developer.  \n                Patents and promises \n              For much of its 200-year history, wave energy has been flush with ideas but short on results. In 1799, French engineer Pierre Girard and his son filed the first patent to harness power from waves. Never constructed, the device was to work by linking the bobbing of moored ships to heavy machinery ashore via a plank and fulcrum. Through the nineteenth and twentieth centuries, patents trickled out of inventors' workshops, but no machine ever produced enough power to gain widespread use. Wave energy's supporters began moving out of garages and into government ministries in the early 1970s. The embargo imposed by the Organization of the Petroleum Exporting Countries (OPEC) propelled the price of crude oil from $7 in 1970 to $38 by 1974. Many countries saw independence from Middle East oil in renewables, beefing up their research into wind and solar energy. When it came to ways to get energy from the sea, the United States mounted an ultimately unsuccessful effort to capture thermal energy from oceans by exploiting the temperature difference between deep and surface water (see 'Energy from the sea'). Meanwhile, Britain led the way in wave energy. In 1974, the UK government commissioned academia and industry to draw up plans for how the country could bring wave power into the mainstream. The first prototypes for wave-energy machines were ?hopelessly uneconomic?, says Thorpe. The plans called for massive machines that cost more than $100 million each to generate 2,000 megawatts of electricity ? roughly the output of the nuclear and oil-powered plants they were designed to replace. No rationales were given as to how such a technological leap would be made, says Thorpe. None of the proposed plants was ever built. A 1983 progress report effectively ended Britain's foray into wave energy, saying that the technology was unproven and too costly. Developers felt betrayed by the criticism, prompting the government to commission Thorpe to repeat the review; yet he came to a similarly dim conclusion in 1992. As wind energy took off in the 1980s, wave energy went back to its roots, in university laboratories and inventors' workshops. Lessons learned from the early failures and from offshore oil rigs would guide the designs of a new generation of machines. Currently, at least 50 wave-energy projects are in development, with more appearing every year. Analysts divide the machines into more than half a dozen breeds (see graphic), each with a different trick to turn waves into electricity. Pelamis' resembles a giant snake with three segments that shimmy back and forth. Oceanlinx's looks like a giant steel bagpipe that's played by a rising and falling water column. And Finavera's are oversized buoys that use waves to drive hydraulic pumps. Such heterogeneity is natural for a field in its early days ? but within a decade the various designs should shake out into those that are practical and those that aren't, says analyst Roger Bedard of the Electric Power Research Institute, a think-tank based in Palo Alto, California. ?It's still anybody's game.? Thorpe is more sceptical. ?There are lots and lots of ideas out there and hundreds and hundreds of patents,? he says. ?Some of these actually defy the laws of physics, many of them will not be technically viable, even more of them would not be economically attractive ? and we are left with very, very few designs that I think have a chance, on a 10- to 15-year timescale, of becoming economic.? Even now, the most promising designs can be washed under by the smallest technical glitch. A short drive from Steenstrup's Wavestar rests a competing project: 237 tonnes of crimson-painted steel and concrete dotted with barnacles. Curled up onshore, the Wave Dragon resembles a giant piece of playground equipment, its steep, curved walls sloping up to a large concrete bed with an opening at its centre. When operating, the Wave Dragon floats in open water; waves gush over its wall and into a hole, where they power a turbine. Many see the machine as one of the industry's leading prospects. Installed in Nissum Bredning in 2003, the 20-kilowatt device ran for 20,000 hours, says Lars Christensen, a developer in Wave Dragon's Copenhagen office. But the machine has been ashore since early this year after a rusted screw put it out of commission. The screw, it turned out, should have been made of stainless steel. And Finavera's Aqua Buoy was apparently sunk by a pump that failed to remove water once the device started leaking.  \n                Trial by error \n              Such minor errors underscore the difficulty of engineering devices to withstand the demands of the open ocean. Waves come in all shapes and sizes, and most devices are designed to run on average ones. Yet to last years without regular maintenance, they must withstand swells twenty times more powerful. To counter such storms, many of the new machines are moored loosely to the ocean floor, allowing them to better absorb a pummelling. ?The ocean is really going to beat these things up,? says Hagerman. ?They need to be out there for a few years to demonstrate that they can survive.? In 1988, a severe storm destroyed a 600-kilowatt pilot plant made by Kvaerner Brug, a Norwegian firm. The shore-based machine ? one of the few built in the 1980s ? produced power for just three years, and the company later abandoned wave energy. Not to be deterred, Wave Dragon plans to install an even larger model next year in Wales, where the waves dwarf those in Denmark. Although technical hurdles could torpedo any one machine, engineering alone is unlikely to sink the whole field, says Thorpe. He sees greater challenges in the potential costs of developing and delivering wave energy in the face of competition from traditional and other renewable sources. ?Technically it will work,? he says. ?Getting the cost down is a significant challenge, and I think some of them are going to be successful ? but not that many.? The UK-based Carbon Trust has estimated that wave power costs between 25 and 91 US cents per kilowatt-hour. Investing some \u00e32.2 billion (US$4.6 billion) could bring the cost down to 12 cents per kilowatt-hour, it estimates, although that will depend on cost-cutting innovations that will make or break the field. Wind energy, however, can cost as little as 4 cents per kilowatt-hour. The price of solar energy varies with location, but averages around 19 cents per kilowatt-hour for utility installations. Desperate to diversify their energy supplies and narrow the gap between wave energy and other sources of power, European governments have started to offer subsidies and grants. The Portuguese government pays developers 32.5 cents for every kilowatt-hour they put on the grid. It uses the same strategy to support other renewable sources. In Britain, the Marine Renewables Deployment Fund will give out up to \u00e350 million to push devices to field-testing and commercialization. The United States offers no specific support to wave energy, but a bill to provide $50 million in funding per year for five years has been introduced in Congress by Representative Darlene Hooley (Democrat, Oregon). Unsurprisingly, many developers complain that governments haven't been generous enough towards the field. According to a report from the UK consulting firm AEA, the members of the International Energy Agency, which includes nearly every country investing in wave power, spent just 0.3% of their renewable-energy budgets on ocean energy between 1974 and 2004. That's equivalent to US$800 million adjusted for inflation. So some private investors have opened their wallets. Nearly three-quarters of Wavestar's recent funding round came from private investors such as the chief executive of Danish industrial giant Danfoss, and several wave-energy start-ups are publicly traded. Yet Thorpe worries that backers will flee if the field doesn't return money quickly. And with so many firms competing for attention and few side-by-side comparisons available, investors could throw their money at doomed projects. ?A lot of people have invested in wave energy without taking a serious look at the economics,? he says. Energy companies with outdated power grids could also dash the hopes of wave-energy supporters. Designed to handle large, centrally located power plants, many utility company networks are unprepared for the dispersed nature of renewable energies. ?Historically, their role in this was to say 'over my dead body',? says Thorpe. ?The last thing they wanted was a wave-energy device on their network.? In some countries, such as the United Kingdom, power infrastructure is minimal in the remote coastlands with the biggest waves. The situation is better in Portugal, which boasts an extensive grid up and down its coast, which wave-energy developers could theoretically plug into.  \n                Go with the flow \n              Although most environmentalists see wave energy as a valuable green alternative to fossil fuel, opposition has emerged from groups that compete for the ocean. In Oregon, local fishermen wary of being pushed out of waters they have fished for decades initially opposed test projects proposed by Finavera and other developers. ?There's no law that says that renewable power supersedes renewable food,? says Terry Thompson, a county commissioner in Newport, Oregon, and a retired crab fisherman. After Thompson brokered discussions between Finavera and his county's $100-million fishing industry, the firm installed its Aqua Buoy out of the way of fishing grounds. Yet Thompson worries about what might happen when companies move from tests to full-scale wave farms. Surfers have also aired concerns over efforts to cut into waves before they've ridden them. In Oregon, a non-profit advocacy group, the Surfrider Foundation, has filed a complaint with the US government over Oceanlinx's plans to build a wave farm off its coast. And in Cornwall, UK, surfers have complained about a wave-energy farm that is under discussion for instalment off the coast. But, as with fishing, wise planning could side-step any conflicts with surfers, especially in hotspots such as Hawaii. ?I wouldn't put it in the north shore of O'ahu,? jokes Christensen. ?That would be suicidal.? Back in Nissum Bredning, though, the windsurfers and gulls remain unperturbed, and Steenstrup's most pressing concern is funding. To make the leap to commercialization, the Danish engineer needs to build a Wavestar five times larger than the platform his hopes now rest on. The first prototype will cost $11 million, he says. Waves will continue to crash on the shores of Nissum Bredning, Oregon and Portugal no matter what happens in the new round of tests. It should soon be clear whether the technology aimed at moving wave energy forwards can keep the concept afloat.\n \n                     Focus on energy \n                   \n                     Wavestar \n                   \n                     Wave Dragon \n                   \n                     Pelamis \n                   \n                     Finavera \n                   \n                     European Marine Energy Centre \n                   Reprints and Permissions"},
{"file_id": "450340a", "url": "https://www.nature.com/articles/450340a", "year": 2007, "authors": [{"name": "Jeff Tollefson"}], "parsed_as_year": "2006_or_before", "body": "The next US president could lead the country into meaningful action on controlling greenhousegas emissions, but only if he, or she, can seize the moment. Jeff Tollefson reports. Jeff Tollefson covers climate, energy and the environment for  Nature. \n                     US Climate Change Science programme \n                   \n                     UN Framework Convention on Climate Change \n                   Reprints and Permissions"},
{"file_id": "450345a", "url": "https://www.nature.com/articles/450345a", "year": 2007, "authors": [{"name": "Richard A. Muller"}], "parsed_as_year": "2006_or_before", "body": "If you want to lead the free world, you'd better know your physics. That's the lesson from a popular undergraduate class, called 'Physics for future presidents', taught by Richard A. Muller at the University of California, Berkeley. Here he sets some typical questions. An interactive version of this quiz with extended answers is online at  http://www.nature.com/news/specials/climatepolitics/index.html \n                     Climate politics special \n                   \n                     Extended answers \n                   Reprints and Permissions"},
{"file_id": "450606a", "url": "https://www.nature.com/articles/450606a", "year": 2007, "authors": [{"name": "Eric Hand"}], "parsed_as_year": "2006_or_before", "body": "Hanging bright in the morning sky, Venus's allure is obvious; but its blasted surface looks too hot to handle. Eric Hand investigates the difficulties of returning to the closest planet - and new plans to reap the rewards of doing so. The panorama is distorted and claustrophobic, overcast by a thunder-in-the-air gloom. Flat rocks fill the view, framed by the menacing, serrated teeth of the spacecraft and a patch of yellow sky. The lens cap lies discarded in the dust as if the cameraman had been mugged. Or melted. It was 1982 when Venera 13, one of many Soviet missions to Venus, landed in this hellish place. The four cell-phone-quality panoramas it and its sister probe sent back (see image, above) are still the only colour pictures of the surface that researchers have. As the morning or evening star, Venus is the brightest and loveliest of the planets, but as a subject for science it is barely an also-ran. The European Space Agency's Venus Express, launched in 2005, is the first mission to Venus since NASA's Magellan mission, launched in 1990, was brought to a fiery end in the planet's atmosphere in 1994. Between the two, Mars got all the love, with a mission almost every year. NASA's planetary database now contains about 25 terabytes of martian data; America's space-age Venus data total only 400 gigabytes ? a tenth of the return from the two Mars rovers launched in 2003. ?Venus has been like the forgotten planet,? says H\u00e5kan Svedhem, project scientist on the Venus Express team. ?Mars has completely taken over.? There are various explanations for the neglect, but as Venus Express is showing (see articles starting on  page 629 ) a dearth of beguiling questions is not one of them. The runaway greenhouse effect that boiled away the oceans the planet seems to have started off with draws a lot of attention and speculation. But scientists are also interested in the rocks below. Venus, unlike Mars but like Earth, retains enough internal heat to drive large-scale planetary processes ? flows in the mantle and the crust. But the ways these processes have played out on Earth and Venus are vastly different ? in part, perhaps, because of that vicious greenhouse effect and the absent oceans. To address these questions, planetary scientists want a surface mission that picks up where the Venera probes left off. ?You reach a point where you want to do more than fly-bys and orbiters,? says David Grinspoon, astrobiology curator at the Denver Museum of Nature and Science in Colorado. ?We need to push for these more comprehensive missions that explore the deep atmosphere and surface  in situ .? Now Grinspoon and other venusian diehards may be getting a few crucial rungs higher on NASA's to-do list. ?From my perspective, we can't ignore Venus any longer,? says NASA's director of planetary exploration James Green. ?We're not going to ignore it. It's gathering momentum.? Earlier this month, Green announced that NASA would consider a 'flagship' mission, its highest, multi-billion-dollar mission class. He has set up a team to study the science questions to be asked and the technology that would be needed. One explanation for the neglect of Venus is simple. Human exploration is a stated priority for NASA; human exploration of the surface of Venus is next to inconceivable. Mars is by far the most plausible outpost beyond the Moon ? and it has the added bonus of being considerably more likely than Venus to offer a discernible record of past life. Extraterrestrial life and human exploration excite the public. And if that drives science policy, then so be it, says Steve Squyres, a planetary scientist at Cornell University in Ithaca, New York, and lead scientist for NASA's Mars rovers. ?We are doing science with public money and lots of it. We have to keep in mind not just our own academic interests,? he says. Although he says he is equally interested in both planets ? as a young professor he worked on the Magellan mission to Venus ? his academic career is now centred squarely on Mars. ?To a certain extent, you have to go where the data are,? he says. Which leads to the other advantage of Mars. Scientists study Mars because they can. Mars orbiters can map its surface across the whole electromagnetic spectrum using radar, a wide range of infrared and visible wavelengths, ultraviolet and even ?-rays. So far, the only thorough mapping of Venus's shrouded surface has been by radar, first by some of the Venera probes, then by Magellan. And Mars science isn't limited to orbiters. The thin atmosphere makes slowing down landers hard, but once you're down, life's not too difficult: Squyres's rovers have trundled along for years now. It's far easier for engineers to design instruments to survive the cold of Mars than it is to immerse a probe in Venus's crushing atmosphere and scalding temperatures. That hasn't stopped Kevin Baines. ?I've been trying since 1992 to get this country back to Venus,? says Baines, a planetary scientist at the Jet Propulsion Lab in Pasadena, California, who believes that he holds the record for the greatest number of NASA-rejected Venus mission applications. His proposals in the 1990s included an infrared imager that would map the surface, at night, using spectral gaps in the atmospheric greenhouse to glimpse the hot surface below ? a technique Venus Express is now using in the hopes of finding evidence of contemporary volcanism. Spotting active eruptions might kindle interest ? Mars's volcanoes, though monumental, are thought to be almost entirely dormant ? and might also help explain the chemical structure of the atmosphere. But so far, Venus Express has not found any lava hot spots. And an instrument that was supposed to locate tell-tale plumes of sulphur rising from volcanoes has failed to send back any useful data. With Venus Express pursuing much of the science of his rejected orbiters, Baines has moved on to balloons ? something the Soviet Union deployed on Venus more than 20 years ago with the Vega probes. In terms of ballooning, the charms of Venus and Mars are reversed. The hardly-there atmosphere on Mars is deeply inimical to the idea of floating lazily from place to place (which has not stopped such missions being suggested). The thick atmosphere on Venus, on the other hand, is ideal. A balloon 55 kilometres up in the venusian atmosphere would operate at comfortable temperatures and pressures, ferried around the world by ample winds. Baines says his silvery-green balloon would be 7 metres across and five layers thick. With a coating of Mylar to reflect the Sun and a layer of Teflon to protect against sulphuric acid, a balloon could last months, perhaps a year, he says. A high priority for such a mission would be to measure the abundance of various isotopes of noble gases, which would allow scientists to deduce new details of the planet's history. Baines is also considering dropping 5-kilogram probes, like ballast, which would take pictures of the surface as they fell to a crash landing. The probes might target specific spots on the surface. Most of the surface of Venus is basalt, and apparently rather young basalt at that ? eruptive evidence of the heat within. The age is known because Magellan found very few large impact craters on the surface compared with the surfaces of Mars and the Moon. Fewer craters, other things being equal, mean younger crust. On Earth, large parts of the crust are kept young by plate tectonics. On Venus, though, there is no evidence of the faults that a surface made of shifting plates should have. One proposed explanation for the oddity is that instead of producing plate tectonics as on Earth, convection currents in Venus's mantle knead the crust continuously, thickening and thinning it in a way that leaves it crater-free. Another, more dramatic, hypothesis is that every so often the crust gets so thin and cool and dense that it cannot support itself on the mantle. This 'catastrophic resurfacing' theory holds that, 750 million years ago, all the crust sank at once, turning the entire planet into a sea of magma. ?That's a good way to get rid of a lot of heat,? says Steve Mackwell, director of the Lunar Planetary Institute in Houston, Texas, and a master of deadpan delivery. Without new data, it is hard to decide between theories. Dropping probes into the right places might help ? and might reveal clues to what the surface was like before. If, for example, they were to find some ancient granite, a rock that on Earth is produced by water-assisted recycling of the crust, they would be opening a window into a time when Venus was more like Earth, a time when it might have had continents, oceans and plate tectonics. ?That would be a revelation,? says Baines.  \n                The nuclear fridge option \n              A lander or rover might also search out minerals or structures associated with now-closed chapters in the planet's history. Although tricky, a surface mission is not impossible, says Larry Esposito of the University of Colorado at Boulder. There are three choices: insulate your probe and do the science quickly (as the Venera probes did, and Baines's drop-probes would); use temperature-resistant electronics; or refrigerate the craft. The technologies of deep-sea submersibles that investigate black-smoker chimneys on Earth could translate well to venusian probes of the first sort, Esposito says, whereas heat-resistant electronics, which might be required for, say, a network of passive seismometers listening for quakes and eruptions, would be expensive to develop. A rover mission, or anything else intended to lead an active life of longer than a day, would need refrigeration, which requires a continuous power source. That means nuclear power of some sort. And that means a very expensive mission. A nuclear rover might stretch the budget of even a flagship mission (the Mars Science Laboratory, a non-nuclear, non-refrigerated proposition, is currently budgeted at US$1.7 billion). A simple lander, though, or a balloon, might be cheap enough to fall within NASA's $700-million New Horizons mission category. And a bargain basement $425-million Discovery-class orbiter is still a possibility. Vesper, a Venus orbiter that would explore the carbon chemistry of the atmosphere, was proposed for the next round of Discovery missions by Gordon Chin, a planetary scientist at NASA's Goddard Space Flight Center in Greenbelt, Maryland. It has survived to the last stage of the selection process, but as Baines and Chin know from experience, a long history as bridesmaids has yet to see a Venus mission win its own special day. With a flagship mission on the cards, might NASA be less likely than ever to approve a smaller Venus mission? Baines thinks otherwise. Venus orbiters and balloons could act as scouts for a major mission, he says. ?If we can get those to work, people will be more comfortable spending the big money of a flagship mission.? Other nations may also quicken the pace of discovery. Although Europe has no firm plans to follow on from Venus Express ? itself something of an afterthought in the wake of Mars Express ? Japan has an orbiter slated for 2010. And Russia, which has not launched a successful planetary mission since the fall of the Soviet Union, has proposed a Venera-D mission that might include a lander with a launch window of 2016?18.  \n                A farewell to fantasy \n              Engineering a viable Venus probe, and paying for it, is only part of the challenge. To justify a flagship mission, NASA's Green says it is up to scientists to ask compelling questions that demand new research. Beyond that, they need to engineer a new narrative, one that rallies other scientists and excites the public. In pre-space-age science fiction, that wasn't a problem: Venus was a young, oceanic planet shrouded in cloud that acted as a counterpoint to the ancient clear-skied deserts of Mars. Sometimes it was a swampy home to dinosaurs, sometimes to frog people. Mars, though, largely through chance, has managed to hold on to the narrative that has held sway since the days of Percival Lowell and his canals ? the story of a once-watery planet transformed into desert, with vestiges of the life of its early wetter days possibly still preserved. Venus, on the other hand, lost its allure, its swamps and oceans evaporated by the evidence of prohibitive surface temperatures that emerged in the 1960s. (The dream lingered on ? the Venera landers were designed to float if they landed in the sea.) ?Venus suffers from not fulfilling our pre-space-age expectations,? says Grinspoon. ?We put Venus up on this pedestal. And it disappointed us.? In the past decade, an alternative narrative for Venus with obvious appeal has sprung up: Venus as a cautionary tale of greenhouse warming run amok. ?Now, I think, the environment is a bigger concern,? says Esposito. ?It's causing people here on Earth to pay more attention to Venus.? Chin goes so far as to say that the case for Venus exploration has been helped by Al Gore's Nobel prize for campaigning on global warming. Gore himself might be wary of exaggerating the links too much. Earth's atmosphere is 0.04% carbon dioxide; that of Venus is 95%. If humans burned all of Earth's fossil fuel in one go, the atmosphere would still be only about 0.2% carbon dioxide, says James Kasting, a planetary scientist at Pennsylvania State University in University Park. Besides, Kasting says, models show that distance from the Sun is the critical factor in pushing a carbon-dioxide greenhouse over the edge into an ocean-evaporating runaway state. Only in a billion years, when the Sun is 10% hotter, will such a thing be likely on Earth, he says. But perhaps Venus doesn't have to be of direct relevance to Earth's future to be exciting. Perhaps its very difference will be the key to its importance. For a long time the study of Earth-like planets has been limited to just Earth and its two nearest neighbours, siblings with life histories to compare and contrast. But those days are coming to an end. Current missions such as France's Corot, and future missions such as NASA's Kepler, due to launch in 2009, may discover a far greater range of Earth-sized planets outside the Solar System. To understand the thin streams of data from those far-off worlds, an understanding of the full range of possibilities for an Earth-like planet's evolution will be important. ?What if we find a couple dozen planets ? and they're all dead planets?? asks Sara Seager, an astrophysicist at the Massachusetts Institute of Technology. ?We're going to want to understand them.? Studying Venus, she says, ?might help us understand what leads to a habitable planet and what leads to a dead planet.? In the context of the farthest planets ever studied, the story of the planet closest to hand may take on a new importance.\n See also News & Views,  page 617  . Eric Hand writes on physical sciences in  Nature's  Washington DC office. \n                     Venus Web Focus \n                   \n                     The Venus Exploration Analysis Group website \n                   \n                     The European Space Agency's Venus Express website \n                   Reprints and Permissions"},
{"file_id": "450472a", "url": "https://www.nature.com/articles/450472a", "year": 2007, "authors": [{"name": "Mark Schrope"}], "parsed_as_year": "2006_or_before", "body": "Far below the surface of the ocean, beyond the reach of the Sun's rays, organisms still have eyes. Mark Schrope investigates seeing without sunlight. It is a calm August day in the azure waters of the Bahamas, and Erika Raymond, a doctoral student in Oceanography at Johns Hopkins University in Baltimore, Maryland, has been screening hours of video footage inside a shipboard laboratory. The footage comes from a camera system that was stationed on the sea floor some 600 metres down. Hardly a trace of sunlight makes it that deep, and trying to film in the darkness is a tricky feat. Artificial lights might scare creatures away, or attract the wrong kind. So the camera is fitted with a red light, thought to be invisible to the eyes of most deep-sea creatures. But the system does more than film the depths. Eye-in-the-Sea, as it is called, is equipped with an LED lure, a cluster of tiny lights designed to flash and flicker in very specific patterns, all in the hope that something might respond. Raymond is getting excited about what is on the screen. She calls over Edith Widder, her PhD adviser, who becomes similarly animated. \"That is so great,\" says Widder, co-founder of the Ocean Research and Conservation Association in Fort Pierce, Florida. Others gather round, expecting to see another good clip of one of the giant sixgill sharks that have been nosing around the rig. But the duo's excitement stems from something more subtle and more profound. First there's a repetitive flash of light from the lure in the centre of the screen. Then, in the distance, there's a similar burst, but this one is from an animal. Raymond and Widder jump forward to other clips in which the lure was making that same flashing pattern, one of five they have been using. Again they see the response, sometimes multiple responses. It is official. Through their lure, the scientists have begun to communicate with deep-sea organisms using what might be the loudest form of expression in that dark void, the language of bioluminescence \u2014 biochemically produced light (see  Living light - how it works ). What exactly had been said is another issue entirely. The response could have been the equivalent of a flirtatious wink or, more likely, a warning call. But simply making that connection for the first time is a good start in a field of study that, of necessity, operates in the dark. True behavioural observations of bioluminescent activities in the deep are rare, because of the difficulties inherent in keeping deep-sea species alive in the lab, and the severely limited access to the depths of the oceans. Moreover, the tools of on-site exploration \u2014 remotely operated vehicles and submersibles \u2014 generally fall short of being unobtrusive. Ron Douglas, who studies deep-sea vision at City University in London, compares exploration by submersible to taking a \"Land Rover and going out into the savannah in the middle of the night with the stereo on full blast, the lights on full, with a rotating siren and expecting to see normal lion behaviour\".  \n                Plumbing the depths \n              More restrained research such as that conducted this summer in the Bahamas, the third in a series of expeditions funded by the National Oceanic and Atmospheric Administration's Office of Ocean Exploration, is gradually revealing new information about what light can be found in the depths, how the organisms there use it, and even how they see their world. Although much uncertainty remains, one thing is abundantly clear: bioluminescence is nearly everywhere, and for inhabitants of the deep sea, it seems to play key roles in everything from eating, through mating, to staying alive. Tantalizing recent discoveries also suggest that at least some deep-sea organisms are seeing more than anyone expected. Descending into the crystal waters of the Bahamas, observers are invariably struck by the remarkable blues not only of the water, but of everything in it. The long wavelength red light is quickly absorbed and extinguished by the uppermost layers of water. This is why most deep-sea creatures don't see red \u2014 there is hardly any of it. Travel a bit deeper, say 200 metres or so, and there is still enough light for a passable twilight. But as surface light fades, the first small flashes of bioluminescence from dinoflagellates appear. Go deeper still and the flecks become more prevalent. Larger flashes, perhaps from a shrimp or a jellyfish, punctuate the scene. Eventually, the lightshow grows into a veritable fireworks display against an ever blacker background. By 500 metres or so humans can't detect much if any sunlight. The animals equipped for this realm might still detect some light below that level, but at 1,000 metres, absolutely every trace of sunlight is extinguished.  \n                Seeing without sunlight \n              Yet even beyond sunlight's reach, eyes are still common. Bottom-dwelling creatures that spend their entire lives shielded from the light of the Sun actually tend to have enlarged eyes. Those eyes have to be seeing something and that something is the bioluminescence. Indeed, ongoing surveys of bioluminescence led by Monty Priede, from the University of Aberdeen, UK, show that although its frequency decreases with depth, bioluminescence persists thousands of metres beneath the surface. \"People lose sight of the fact that light is the most important variable in our environment, and it is probably as important in the marine environment,\" says Widder, \"but we have to understand they're not seeing like we do.\" Researchers suggest that although other senses are clearly important, some deep-sea organisms depend on bioluminescence for every life function. Not surprisingly, given the scarcity of hiding places, one of the most common uses is simply survival. One established theory is that certain organisms use bright bioluminescent flashes like burglar alarms to startle predators, or to attract their predators' predators 1 . Widder says that one of the earliest deployments of Eye-in-the-Sea may support the second option. In 2004, just more than a minute after the lure was activated in a jellyfish-like pattern for the first time at depth, a 2-metre squid arrived on scene. The next year, hundreds of miles away, a squid of the same as-yet-unidentified species responded similarly. Widder says that it was terrific proof that unobtrusive observation could capture novel behaviour. \"I don't think anything can top that squid,\" she says. Circumstantial evidence suggests that deep-sea animals have other uses for their bioluminescent organs, known as photophores, as well as bioluminescent tissues and various forms of 'spew'. Sometimes it's a deterrent. Some organisms seem able to light up a predator's stomach when they are eaten, broadcasting its location to other predators 2 . Bioluminescence similarly could be used like the colours on a poisonous snake or frog to warn would-be predators of toxicity or unpalatability 3 . On the 2005 Deep Scope expedition, Widder discovered the first known bioluminescent anemone, which produces a sort of glowing slime. Its use is not known, but she speculates the slime could be just such a warning. There is even evidence of batesian mimicry, where perfectly edible organisms have evolved to impersonate noxious species. Many animals can hide from predators using light. In the ocean's twilight zone, where some light still penetrates, a dark silhouette is easily spotted from below, and is hence a dangerous liability, says S\u00f6nke Johnsen, a biologist at Duke University in Durham, North Carolina, and the 2007 Deep Scope chief scientist. So, many animals use photophores to counterilluminate, or light up their undersides and blend 4 . On the flip side of bioluminescence, there is strong evidence that many animals use light to attract and find others. Lots of fish and squid, for instance, use bioluminescence as a form of searchlight. Others attract mates using sex-specific bioluminescent patterns 5 . And female deep-sea anglerfish are famous for their lures, filled with bioluminescent bacteria, that they dangle in front of imposing fangs. Bioluminescent bacteria are common throughout the ocean, and researchers have proposed that the bacteria use their luminosity to find good homes. Bacteria that are lit up, whether hitching a ride on a bit of detritus or on a fecal pellet, are more likely to be eaten by fish, in whose guts their needs are well met.  \n                Look deep into the eyes \n              Understanding what deep-sea animals can see is, of course, integral to understanding how bioluminescence might be used and to what degree of success. Tammy Frank, a Deep Scope leader and a visual ecologist at the Harbor Branch Oceanographic Institution in Florida, which owns the ship and submersible used, has done extensive work studying deep-sea eyes. She speculates that the size of the eyes of bottom-dwelling deep-sea creatures, relatively larger than those of the occupants of the water column above them, could be tied to energy expenditure. The advanced vision requires energy. Creatures in open water cannot afford to waste energy that might be better spent outswimming predators. Bottom-dwelling animals can use the sediment or rock structures to help them hide, which could allow them to funnel more energy towards vision. To date, most deep-sea animals studied seem able to see light only in the blue-green range. These shorter wavelengths penetrate the water farther and are where the majority of bioluminescence falls. But there are notable exceptions. In 2005, on the second Deep Scope expedition in the Gulf of Mexico, Frank discovered a bottom-dwelling crab species sensitive not only to the standard blue-green range, but also to ultraviolet light. The utility of this ability is not clear, but could, by some unknown means, allow the crab to detect its favoured hiding spots \u2014 soft corals \u2014 or to distinguish between different types of bioluminescent light, which does at times dip very slightly into the ultraviolet range. If such an ability is proven, it would not be the first time scientists have discovered visual skills that seemed initially bizarre or improbable. Because of red light's weak ability to travel in water, conventional wisdom had long held that all bioluminescence in the deep was blue or green. But in 1981 Peter Herring, a now semi-retired bioluminescence pioneer at the University of Southampton, UK, reported the strange discovery of red bioluminescent 'searchlights' under the eyes of certain dragonfish 6 , a common family of elongated deep-sea fish with menacing fangs appropriate to their name (see picture, top). Herring, and others, have shown that these searchlights are the result of fluorescent proteins that shift bluish bioluminescence to the red range. The eyes of these fish are adapted to see red light as well, suggesting the animals can secretly attract mates or hunt using flashlights that few other fish can see 7 ,   8 . Strangely, one dragonfish species initially did not seem to have the pigments necessary to see the red light. Douglas and his colleagues eventually discovered that these fish use a chlorophyll pigment from bacteria for red vision. There are still significant uncertainties about how the pigment is obtained and how it accomplishes the energetic shift that leads to red vision. But other groups have confirmed that when the chlorophyll is added to the eyes of animals such as mice, it boosts red vision 9 .  \n                Seeing red \n              Steven Haddock, at the Monterey Bay Aquarium Research Institute in Moss Landing, California, and others have proposed that there may be still more uses for red light in the deep. In 2005, his group made the controversial proposal that at least one fish-eating species of siphonophore, a jellyfish relative, combines bioluminescence with red fluorescent proteins to create red fishing lures on its tentacles 10 . Challenges to the theory revolve around the low number of deep species so far known to see red, although the vast majority have not been tested. Physics provides another challenge. Fluorescence is inefficient, because energy is lost in the transfer from bioluminescent reactions to fluorescent proteins, meaning a relatively weak signal is produced or received. \"My physics head says, 'No,'\" says Justin Marshall, another Deep Scope participant, from the University of Queensland in Brisbane, Australia, \"But my biology head says, 'Well, Why not?' Biology is weird, so it could be.\" Haddock, and Mikhail Matz, another Deep Scope leader, based at the University of Texas in Austin, are involved in a related research project focused on the red fluorescent protein found in the dragonfish searchlight photophores, and, according to preliminary results, also in the chin barbels of two species. There it is not associated with a bioluminescent organ, making its presence all the more intriguing. \"I think red fluorescence attracts some sort of prey,\" says Matz, but what and how are not clear. The dragonfish protein seems to be novel, and, as with several collected from shallower-dwelling animals, Matz is exploring the potential use of the protein as a new tool for biomedical research, where other fluorescent proteins are used frequently. Questions such as the extent to which red fluorescence might play a role in the deep are likely to remain unanswered for some time, but Widder hopes that there will soon be a new tool to advance research on a number of the deep's mysteries. Thanks to military funding, most research on bioluminescence has focused on shallow waters where it can reveal the presence of submarines and other vessels. But Widder has now secured funding from the National Science Foundation for a new version of Eye-in-the-Sea, scheduled for deployment as part of an undersea observatory array off Monterey, California, in early 2008. This system will provide researchers with the first ever perpetual, unobtrusive view of the depths. That could mean the first effective, long-term study of true deep-sea bioluminescent behaviour. Among numerous other potentials for discovery, this extended view may allow Widder and her colleagues to decode just what was said between their lure and the unidentified creatures they were communicating with in the Bahamas. \"I feel like we'll be able to address some of these issues for the first time,\" she says.\n Mark Schrope is a freelance writer in Florida. \n                     Insight: Bio-Oceanography \n                   \n                     The Bioluminescence Web Page \n                   \n                     Edith Widder's website \n                   Reprints and Permissions"},
{"file_id": "450475a", "url": "https://www.nature.com/articles/450475a", "year": 2007, "authors": [{"name": "David Cyranoski"}], "parsed_as_year": "2006_or_before", "body": "With keen immunological insight and a knockout mouse 'factory', Shizuo Akira leads by quiet example. David Cyranoski visits the world's most-cited scientist as he prepares to run one of Japan's premier research centres. \n                     Innate Immunity: The Unsung Heroes \n                   \n                     World Premier Immunology Frontier Research Center \n                   \n                     Fast Breaking Comments by Shizuo Akira and collaborators \n                   \n                     Bruce Beutler's lab \n                   \n                     Alan Aderem's lab \n                   Reprints and Permissions"},
{"file_id": "450778a", "url": "https://www.nature.com/articles/450778a", "year": 2007, "authors": [{"name": "Declan Butler"}], "parsed_as_year": "2006_or_before", "body": "Technology will soon allow the world to be mapped in near-real time and at high resolution. Declan Butler investigates the potential for operational monitoring of the planet. Forecasting is a tricky business. You can be let down by your initial data or your model of the processes, by an unrecognized bias or just bad luck. But a dramatic forecast still has the power to grab the attention. Take this one, about the state of Earth monitoring in a couple of decades: ?A user will be able to get, on demand, climate, or any other information for any place on the planet, on the land, in the oceans, or in the atmosphere, at any time, past, present and future.? The speaker is Rick Anthes, president of the University Corporation for Atmospheric Research in Boulder, Colorado, and chair of a US National Academies panel that in January released an influential 428-page blueprint on the future of Earth monitoring. The forecast he is making is based on clear and established trends: satellites are getting more cost-effective in their capabilities, and the computers and supercomputers that make use of their data are speeding up exponentially. If that increase in technological capability can be turned into usable systems, then the ability to monitor Earth's environment will be revolutionized. Real-time and near-real-time data will be available on soil moisture, greenhouse-gas concentrations, biological productivity, aerosol concentrations and so on, all around the world. With those data, scientists will be able to build and study models of Earth as a system far beyond what they have today. To make that real, though, will require coherent and sustained political and institutional support, and on this front the news from the National Academies is less compelling. The US Earth-monitoring programmes are adrift without leadership, warns the academies report; the number of observational satellites and instruments has already peaked, and is set to decline over the next two decades (see  page 782 ). No repairs are likely before the next administration, even if then. Anthes and other US scientists are keenly aware that to get a glimpse of the sort of sensible and forward-looking Earth observation strategy the academies panel proposed, they have to look to the European Union (EU). Europe's approach to Earth monitoring is not flashy. Its underlying philosophy flows not so much from cutting-edge research as from what amounts to weather forecasting writ large, building ever more capacity for monitoring the planet onto the day-to-day activities of meteorology ? delivering data, images and products to users 24 hours a day, 365 days a year. The idea is to roll out similar easily used maps, models and forecasts on an ever-increasing range of data and processes ? for example flood risks, soil and coastal erosion, crop and fish resources, air pollution, greenhouse gases, iceberg distribution and snow cover. The systems that do so would, like today's meteorological systems, generate continuous, cross-calibrated, long-term data sets on the state of the planet and its atmosphere. It is by embedding scientific Earth-observation needs within an operational system that meets the needs of customers that the financial case can be made for the sort of sensors attuned to various climatic and other parameters now seen only on research satellites ? sophisticated spectrometers, sounders, lidars and radars. The operation of these sensors entails the recurrent costs of running fleets of satellites and sensors for decades, with regularly scheduled replacements. A key part of the European process is the Global Monitoring for Environment and Security (GMES) programme run by the European Commission and the European Space Agency (ESA). GMES is explicitly charged with bringing the sorts of data that have previously been the province of research satellites to the citizens of Europe and beyond.  \n                Sentinel senses \n              The GMES suite of 'Sentinel' satellites will be operated around the clock to routinely supply data similar to those now provided by research satellites into the foreseeable future. Sentinel 1, slated for launch at the end of 2011, will be designed for radar and build on some of the databases from Envisat, an 8-tonne ESA research behemoth that has tested out a wider range of instruments; the Sentinel 2 series will be imaging satellites with fine spectral resolution, building on the SPOT satellites; the Sentinel 3 satellites will carry forward the ocean-observing aspects of Envisat; Sentinels 4 and 5 would monitor atmospheric chemistry. The data provided by these assets would be integrated with data from future research satellites, as well as with national and international data from airborne, ground and ocean sensor webs (see  Nature  440,  402?405  2006). The idea is that by the mid-2020s, Europe would have monitoring systems akin to those now in place for meteorology for all areas of environmental monitoring, says Josef Aschbacher, head of the GMES space office in Frascati, Italy, with forecasts and data on everything from global climate change to town-by-town air pollution levels. The programme is loosely modelled on that of the European Organisation for the Exploitation of Meteorological Satellites (EUMETSAT), which supplies weather data to national met offices, and other government and commercial users. EUMETSAT is not a large organization ? its annual budget is normally in the ?300-million range (about US$440 million) ? but unlike ESA, or for that matter the commission, it has a track record of running operational systems in a way that works for users. The first of its Meteosat weather satellites was slotted into geosynchronous orbit in November 1977. It also now runs a weather satellite in a low-Earth orbit that complements US satellites in similar orbits. EUMETSAT recently agreed to join GMES, and is discussing directly operating future GMES satellites and ground operations, as well as hosting GMES instruments on its own weather satellites. But GMES is a much more ambitious undertaking. It has ?1.97 billion in approved funding from the EU and ESA to carry it through until 2013, covering the launching of the first three Sentinels (the contract for the first of which was signed earlier this year). In 2008 ESA will ask its member states (which differ slightly from both those of the EU, and of EUMETSAT) for a further ?700 million ?900 million to cover development of the Sentinel series, operations and spare spacecraft, and the commission will ask the EU for the ?2.5 billion needed to operate the system until 2023. These requests for funding are not the only reasons that 2008 will be the crunch year for GMES, says Paul Counet, head of EUMETSAT's Strategy and International Relations Division. Many governance issues remain to be resolved, such as who is responsible for which aspects of operations or services. And then there is the vast task of integrating national observing systems into GMES, and of finding ways for private industry to manage or add value to specific data sets and provide new services. The resolution of these governance issues, and the forging of the long-term relationships needed to underpin operational systems, will determine whether GMES blossoms into a full-blown system or goes belly up, says Counet. Regardless of its implementation, the operational logic of the GMES programme gets the thumbs up from across the Atlantic. ?The Europeans certainly have a robust programme and are moving to make it happen,? says Scott Goetz, a researcher at Woods Hole Research Center, Falmouth, Massachusetts, who uses remote sensing to model ecosystems. ?GMES is a step in the right direction,? agrees Kevin Trenberth, a climate researcher at the National Center for Atmospheric Research in Boulder, Colorado, who says that he would like to see a similar approach expanded in the United States and internationally, to generate a global 'climate-information system'. The fact that operational satellites must meet needs other than those of scientists ? a cornerstone of the GMES approach ? is no obstacle to research, he says. Researchers just need to get involved to make sure their needs are taken into account. The data that such information systems could make available will have implications both for how well scientists can run environmental models and for what those models can do. Operational systems launched in the late-2010s and onwards are likely to generate order-of-magnitude improvements in both temporal and spatial resolution. Today, a typical weather model might have 30?50-kilometre horizontal resolution. Climate research models are even coarser ? often 100?200 km. But by 2025, improvements in both data and computing will mean that weather will be modelled at 1-km resolution, and climate models at 5?10 km, predicts Anthes. Anthes has high hopes for the impacts such improvements could allow. ?We have seen beyond a doubt in weather prediction that as the resolution increases, something almost magical starts to happen in the models, even without an increase in the observations,? says Anthes. ?As we go for example from a 30-km resolution model to a 5-km model, hurricane prediction, precipitation patterns and so forth become far more realistic.? More data obviously demand more computing power to make sense of them ? and computing power is already the overriding limitation on how realistic models are, say many researchers. ?It's a major issue,? says Trenberth. To get good results you need to run the same model again and again with slightly different inputs, which eats up computing power. And every twofold increase in resolution requires a tenfold increase in the teraflops required. ?Climate prediction is probably the most computationally challenging problem in science,? says Tim Palmer, a scientist at the European Centre for Medium-Range Weather Forecasts (ECMWF) in Reading, UK. But Palmer is optimistic that the bottleneck will soon be alleviated. He points to the arrival next year of the first petaflop computers, running at peak speeds of up to 3,000 teraflops (see Nature 448,  6?7  2007). By way of comparison, the ECMWF's fastest machines today run at less than a hundredth of that. Palmer predicts that by 2010, 10-petaflop machines will allow climate scientists who can get hold of them to run century-long simulations of the climate at 10-km resolution. That could be 1 km within a decade after that. Accurate modelling of cloud processes at the 1-km level, a key component currently missing from global climate models, could vastly improve predictions of regional climate change. Trenberth is one of a group of researchers planning to propose the creation of one or more international multi-petaflop computing facilities for climate prediction, with a ball-park cost of $1 billion over 5 years. The idea will be presented at the international climate negotiations opening in Bali, Indonesia, this week, with a formal proposal to be published next year in  Bulletin of the American Meteorological Society.   \n                Integrate and accumulate \n              The data expected will not just be more precise ? they will also be more wide-ranging, providing new impetus to models that seek to treat the Earth system as an integrated whole. Until recently, Earth observation has been less than the sum of its oceanic, terrestrial and atmospheric parts, according to Stephen Briggs, head of science, applications and future technologies at ESA. ?Integrating the components is something we are really bad at,? says Briggs. ?This is where we are going to see the major advances.? Incorporating more geophysical observations made from multiple instruments obviously makes models more complex. To integrate such disparate data sets, which differ not just in their spatial and temporal resolution but also in their error profile, modellers are borrowing the 'data assimilation' techniques used by weather forecasters. A model producing a weather forecast will start off with reasonable best estimates of initial global conditions informed by the data to hand. As more come in ? as low-Earth-orbit satellites pass over new places, for example ? the model's evolution is reiteratively compared with reality. So sparse and infrequent sources of data can still play a role. Counter-intuitively, more data sources can also often simplify modelling, as they can help to better define other variables, adds Trenberth. For example, raw measurements from a buoy might be misleading if it were in an eddy of the warm Gulf Stream rather than somewhere more representative of the Atlantic as a whole. A system that could use other data to know that the buoy was in the Gulf Stream would not be misled so easily, and the model would be made more realistic. ?As one can resolve features better,? he predicts, ?one can utilize data better.? The resolution and the data can be provided, if the institutions allow; it will then be up to Trenberth and his colleagues to make good on that forecast.\n See Editorial,  page 761  . Declan Butler is a senior reporter for  Nature , based in France. For all our content on Earth observation visit our online  special  . \n                     Earth observation special \n                   \n                     Global Monitoring for Environment and Security (GMES) \n                   \n                     GMES services \n                   \n                     European Space Agency (ESA), Earth observation site \n                   \n                     EUMETSAT \n                   \n                     Envisat \n                   \n                     Planned Sentinel satellites \n                   Reprints and Permissions"},
{"file_id": "450603a", "url": "https://www.nature.com/articles/450603a", "year": 2007, "authors": [{"name": "Erika Check Hayden"}], "parsed_as_year": "2006_or_before", "body": "Are ageing and disease two sides of the same coin? Erika Check Hayden reports from an institute that is betting that they are. ?Don't forget,? Robert Hughes reminds a colleague in the hallway, ?beer on the landing later to celebrate my impending death.? Hughes isn't ill, but rather kicking off festivities for his 45th birthday. And working at the Buck Institute in Novato, California, an independent institute devoted to ageing research, tends to alter the way one thinks about birthdays. Their work makes the Buck's 15 principal investigators peculiarly, if playfully, aware that they are adults getting older. But their institute is on the other side of the ageing divide. ?It is a toddler becoming a juvenile,? says Dale Bredesen, the Buck's director. Youth, like age, has its problems, and after a rough infancy, the Buck has emerged as a player with the potential to change the way people think about ageing. The Buck was founded on the premise that ageing and disease are manifestations of the same biological processes, and they can be understood only by working across disciplines. It is a modern take, but it has its supporters, including the US National Institutes of Health (NIH). In 2005, the agency named the Buck as one of five national Nathan Shock Centers of Excellence in the Basic Biology of Aging. And in September, it gave the institute US$25 million to create a new 'interdiscipline' called geroscience: defined as the study of connections between ageing and age-related disease. Now, the Buck is embarking on a growth spurt that will add 10 labs in 3?4 years. Plans have been drawn up for a long-term goal of three new buildings on its sunny hilltop campus and 20 more investigators, bringing the total number of labs to 45. But the expansion will depend on funding, which, despite recent votes of confidence from the NIH, may be hard won. Peers in research on ageing see the Buck as a pioneer, testing the links between ageing and disease. ?It's clearly an experiment worth doing,? says molecular biologist Stephen Helfand of Brown University in Providence, Rhode Island. But research into ageing has historically faced scepticism. ?We're all watching to see whether it is working,? says Helfand. ?Are they going to do something good and bold, or are they going to be doing the same old boring things we're doing?? To understand the novelty of the Buck's approach, historical perspective helps. In the 1950s, ageing was thought to be intractable. The rather crude tools available to those who studied it didn't engender respect. Caloric restriction was known to extend lifespan in some organisms but was enigmatic. There weren't a lot of ways to study ageing beyond simple observation. ?People were essentially grinding up old and young rats? and coming up with descriptions rather than learning about mechanics, says Judith Campisi, a cellular and molecular biologist who splits her time between the Buck and the Lawrence Berkeley National Laboratory. Around 1961, researchers established that ageing was a bona fide biological process, separate from, and as important as, the pathologies associated with it. By 1974, the NIH had set up a specific branch ? the National Institute on Aging ? signalling that the field had earned a place in mainstream biomedical research. But soon after, the advent of genetic engineering and modern molecular biology upended the tidy distinctions between ageing and disease. A new generation of researchers embraced molecular biology. In the 1990s, they discovered that knocking out certain genes endowed organisms such as yeast, worms and flies with unusually long lifespans 1 . But oddly enough, researchers started to find that mutating these genes to slow ageing could also stave off ailments such as cancer and diabetes 2 , which claim many more old people than young but that aren't usually seen as related to the ageing process itself. These studies raised the question: could ageing and disease be part of the same process after all? Some researchers have reverted to the earlier course, says the Buck's Gordon Lithgow. ?We're coming around in a circle and saying, it's the same thing. We don't need a National Cancer Institute and a National Institute on Aging in separate buildings with separate funding streams, for example,? says Lithgow. ?If ageing is the biggest causal factor for adult cancer, they should be together.? And thanks to a trust fund left by Beryl Buck, a nurse who died in 1975, the institute can pursue this integrated approach. Buck left behind a trust to benefit the citizens of Marin County, a wealthy enclave of towns, including Novato, just across the Golden Gate Bridge from San Francisco. The fund was to be used, in part, ?to extend help towards the problems of the aged?. The trust's funds ballooned from $11 million to several hundred million dollars after its core stock holdings shot up in value. And dispensation of the funds fell into a messy legal battle between San Francisco and Marin County. But even before this brawl resolved in Marin County's favour in 1986, a board of advisers convened by the Buck Trust met to figure out how to meet Beryl Buck's goal of aiding Marin County's aged and needy. ?The joke was that the trust was for the poor, elderly people of Marin ? both of them,? recalls Jack Rowe, who was studying ageing at Harvard Medical School in Boston, Massachusetts, at the time, and led the board of advisers. The group settled on the idea of an independent research institute focused on gerontology, and modelled on places such as the Salk Institute for Biological Studies in San Diego.  \n                A design for life \n              The advisers set about duplicating two Salk characteristics: its enviable location and remarkable architecture (see  page 592 ). The trust bought a 200-hectare tract of land atop scenic Mount Burdell, an undeveloped hilltop dotted with oak trees and boasting views of both the Pacific Ocean and the San Francisco Bay. The board commissioned renowned architect I. M. Pei to design the institute's $59-million headquarters. And despite Marin County residents' objections to the development, construction started in 1996. To recruit a director, the advisers once again looked down the California coast ? this time to La Jolla, where Bredesen, then a neuroscientist and neurologist, was working at yet another private institute, the Burnham. Bredesen agreed to accept the job, on one condition: the Buck would have to revise its mission. ?They wanted to set up an institute related to neurology and neurological disease, and I said, 'That's not right. That's not where we are in ageing today,'? Bredesen says. ?It was clear that we needed to think about ageing and its relation to disease.? Bredesen envisaged a self-sufficient, interdisciplinary research institute built on three cores: the biology of ageing, the study of disease and the development of technology. The idea that growing old and growing ill are two sides of the same coin remains controversial. Backers of the concept include David Sinclair of Harvard Medical School who made headlines with findings that a chemical in red wine called resveratrol extends lifespan and might prevent diabetes-like symptoms in mice 3  (see also  page 712 ). ?I don't see ageing as a disease, but as a collection of quite predictable diseases caused by the deterioration of the body,? Sinclair says. But others don't see it that way. The University of Michigan's Richard Miller says that Sinclair's characterization is ?missing the point in a subtle but important way?. Ageing is a major cause of many diseases, but not the only one, Miller argues. And, he adds, ageing has some effects that aren't considered disease states. ?It's important to make a distinction between ageing and disease,? Miller says. Still, those who differ agree that interfering with the ageing process could help patients who are suffering from age-related disease. Sinclair is already running a clinical trial using resveratrol to prevent diabetes in humans. Ageing researchers' shift towards treating disease, rather than attempting to lengthen lifespan, has legitimized the field ? and attracted scientists who would have eschewed ageing research in the past. Research on ageing has always been dogged by a flake factor ? the suspicion that scientists pursuing the fountain of youth were merely involved in a self-serving and impossible quest. Today, researchers studying ageing have largely distanced themselves from that perception. Hughes, for instance, studies protein?protein interactions and uses drug-screening technologies to look for potential therapies for protein-misfolding conditions, such as Huntington's disease. ?I'm totally uninterested in the fountain of youth,? Hughes says. ?What's interesting to me is the idea that pharmaceutical drugs that extend lifespan will be an interesting new class of drugs to treat disease.?  \n                Risky business \n              To Hughes and his colleagues at the Buck, the new geroscience grant offers a means to turn that idea into reality. ?The essential thing that will come from this grant is speed,? says Lithgow. The geroscience grant enables risky work that, by and large, will fail. But in a few cases, it might succeed, leapfrogging years of painstaking studies and hitting pay dirt much faster. Lithgow and Campisi are going after checkpoint genes, cellular traffic cops that survey for damage and prevent faulty cells from dividing. These have long been a target in cancer research because the products of such genes prevent cancer. Last year, Lithgow's lab reported that deactivating checkpoint genes in the nematode worm  Caenorhabditis elegans  extended its adult lifespan 4 . Deactivating such genes in mammals results in an early death as cells that can divide become cancerous, but practically no  C. elegans  cells divide. To Campisi and Lithgow, the finding suggests that losing certain checkpoint genes in non-dividing cells could preserve the cells' health. They think that this concept could extend to, for example, the brain cells of human stroke patients, as adult human neurons don't divide either. Normally, it would take years of work even to begin to address such a question ? further studies in worms, then bridging studies in other model organisms. Finally, perhaps, someone would fund an extension into human cells. But Campisi and Lithgow vaulted over these hurdles and set up a three-way collaboration with their Buck Institute colleague, Xianmin Zeng, who specializes in human embryonic stem-cell research. Zeng will grow human neurons from stem cells, and Lithgow and Campisi will study cell-division proteins in the resulting neuronal tissue. Such a leap would probably be panned as too speculative by review committees that mete out NIH funding. ?It would be difficult to get grants like these reviewed in the traditional way,? Lithgow says. The geroscience grant is a boon for the Buck's brand of high-risk research. But it does not guarantee success. The Buck is up against a few obstacles ? for example, it must adhere to numerous building restrictions, limiting the potential for expansion. And although its location is idyllic, Marin County is isolated; the nearest universities, Berkeley and the University of California, San Francisco, are both 50 kilometres away. ?Their slight weakness is that the Buck is a standalone institute not connected to other universities or hospitals,? says Leonard Guarente, who studies ageing at the Massachusetts Institute of Technology, Cambridge, and serves on the Buck's board, ?but their strength is they have very good scientists and they do good work.? But good funding doesn't necessarily follow good work. And although certain types of ageing research have captured the popular imagination, that hasn't translated into bountiful support for basic studies such as those that go on at the Buck. The Buck draws $6 million a year from the Buck Trust, and the $25-million geroscience grant will be spread over five years. With a $30-million annual operating budget and a big expansion planned, that leaves a gap of more than $20 million a year to fill through other grants and private funding.  \n                Public profile \n              But money is flowing to the multibillion-dollar industry of anti-ageing products, many of which are of a dubious nature. And controversial personalities, such as former computer scientist Aubrey de Grey, are attracting public interest and dollars with sexier goals than that of the Buck's. De Grey is convinced that all ageing is caused by cellular damage; fix that damage and humans can live for ever. This tantalizing idea has convinced private citizens to donate more than $4.5 million to support his Methuselah Mouse Prize, an incentive for researchers looking to extend mouse lifespan. But mainstream researchers are split on whether his simplification of the ageing process ? and his ability to draw large donations, such as a $3.5-million gift in 2006 from American entrepreneur Peter Theil ? is helping or hurting their cause. An invited talk by De Grey at the Buck, for instance, divided faculty members between those who label his work 'pseudoscience' and want the institute to distance itself from him, and those who are glad to see the public interested. ?Aubrey has his weak points, and he grandstands, and that turns people off,? says Campisi. ?But I think the fact that he has stimulated a wealthy donor to give money to basic research on ageing is a good thing for all of us.? Still, donors seem more excited by the hunt for a fountain of youth than they are about working out the details of normal ageing. ?When I say I study ageing, people say, 'You must be rich.' But people aren't rallying to try to understand ageing. They're rallying to cure their son of diabetes,? says Helfand. ?The disease people get the money.? The National Institute on Aging's $1-billion budget this year is smaller than that of the major disease-specific branches of the NIH, including those that fund cancer, cardiovascular and brain research. And NIH budgets overall are stagnant ? a trend that has hit independent institutes such as the Buck perhaps harder than universities and medical centres, which can count on alumni and patients. But the Buck's vision is to change that picture. If geroscience works, it will produce drugs that may one day cure or prevent afflictions such as cancer and Alzheimer's disease. And that will benefit regular people ? maybe even people such as Hughes, who, upon celebrating his 45th birthday, estimates that he is halfway through his own lifespan. ?I don't think the point of what I'm doing here is to live to 120, but to live for as long as I'm going to live and be healthy, and to help the field think more clearly about real things affecting real people in real time,? Hughes says.\n Erika Check Hayden writes for  Nature  from San Francisco. \n                     Buck Institute for Age Research \n                   \n                     National Institute on Aging \n                   \n                     Methuselah Mouse Foundation \n                   Reprints and Permissions"},
{"file_id": "450780a", "url": "https://www.nature.com/articles/450780a", "year": 2007, "authors": [{"name": "Quirin Schiermeier"}], "parsed_as_year": "2006_or_before", "body": "As names go, it's cleverer than most. At the mundane level, Argo stands for Array for Real-time Geostrophic Oceanography. But the name comes with not just one but two classical allusions attached. There's the  Argo , the ship in which Jason sailed to find the golden fleece, a nod to the fact that the buoys that make up Argo were conceived as a counterpart to a satellite called Jason (see 'The Jason project'), which measures the surface topography of the oceans. And then there's Argos the giant, who was blessed with 100 eyes to see in all directions, with only a handful asleep at any one time. The modern Argo puts even that spectacularly distributed sensory system to shame ? it has 3,000 different sense organs spread all around the globe.  The array's 3,000-odd autonomous floats, which look like upended torpedoes, are equipped with sensors for recording temperature and salinity in the upper 2,000 metres of the ocean (see enlarged map). Each float sinks, drifts, bobs up and transmits data to satellites on a regular basis. At its current size the array provides more than 100,000 temperature and salinity profiles each year, regardless of the season or weather. This is 20 times greater than the comparable annual measurements by research vessels and merchant ships, which in the past have been the main data source for this type of oceanography. In the past five years, Argo has more than doubled the total database on some seas that ships steer clear of ? such as the seas around Antarctica in winter. Over the next years, the array's design will be reassessed with an eye to the need for additional floats and sensors. Already, 60 of the floats carry oxygen sensors ? will that number be increased? What about sensors for particulate carbon, or for chlorophyll? Such extras are attractive, but always come at the cost of increased demand on the floats' batteries and thus a shorter lifetime. Another idea is to design floats that can dive to greater depths. And special floats for polar latitudes ? tethered to ice floes or equipped with acoustic sensors that tell the instrument where it can safely surface ? are already being tested in seasonally ice-covered regions.  \n                Plugging the gaps \n              Predicting and closing the gaps that will constantly appear in the array will be a major logistical challenge. The types of floats currently in use have a lifetime of 4 years, meaning that around 800 instruments need to be replaced each year. And replacements need to maintain the system's global coverage. That means dropping floats out of low-flying aircraft or chartering ships to optimal sites for replenishment. The array's annual costs ? around US$24 million ? are being shared by more than 30 nations. Only Germany, however, has so far made a firm long-term funding commitment. In the United States, which contributes 50% of the overall programme costs, funding currently comes through the National Oceanographic and Atmospheric Administration. But it is not yet clear which agency will be responsible for maintaining the array once Argo becomes a routinely operational system. Proof of definite advantages ? for fisheries, merchant shipping, oil-spill management or naval purposes ? could help secure long-term funding from more governments, says John Gould, a consultant and former director of the project. ?But it's still rather early days to evaluate the benefits.?  \n                Raft of achievements \n              The first scientific achievements using data from Argo include the detection of accelerated circulation in the subtropical South Pacific 1 , the tracking of the deep water that forms in the Labrador Sea 2  and the observation of local sea-temperature changes during hurricanes. But there have also been setbacks. A surprising suggestion that the oceans' heat content was diminishing, rather than increasing as expected 3 , turned out to be an artefact caused by a software glitch 4 . Gould says that this underlines the need for experienced scientists ? who know both the oceans and the technology ? to be involved in data management. ?We're still learning,? he says. Argo data are already incorporated into models for seasonal weather prediction. To initialize such forecasts, scientists 'tell' their models about the here and now and then cast them off into the future. Precise knowledge of the initial state of the ocean ? which has a longer 'memory' than the atmosphere ? could greatly improve the accuracy of longer-term weather and climate prediction. At the UK Met Office's Hadley Centre in Exeter, Argo data have been used to initialize the centre's new decadal temperature-forecasting system, which combines observations and models on longer timescales 5 . ?The data are absolutely invaluable,? says Matt Huddleston, a climate scientist at the Hadley Centre. ?We now use Argo data continuously for forecasting everything from European winters to tropical storms and El Ni\u00e1o events.? The success of the Met Office in correctly predicting some very cold and wet conditions during the 2005?2006 European winter seems to have been made possible in part by Argo data, which revealed an abnormal subsurface temperature pattern in the North Atlantic. But statistical proof of such benefits will be available only when the Argo network has been maintained for much longer. Perhaps Argo's most valuable contribution will be in facilitating studies of year-to-year and decade-to-decade variability in the oceans. At present, it is hard to distinguish climate-change-driven shifts in ocean circulation from natural fluctuations. Argo will provide the continuous time-series needed to solve such puzzles. It should also help settle the big question of the extent to which the oceans participate in climate variability. ?We keep learning that it is dangerous to infer changes in the oceans from only a few years of measurements,? says Brian King, a physical oceanographer at the Southampton Oceanography Centre, UK. ?But if anthropogenic forcing does leave a mark on the ocean, Argo should definitely help us find out.?\n For all our content on Earth observation, visit our online  special  . \n                     Earth observation special \n                   \n                     Argo Website \n                   \n                     Argo Information Center \n                   Reprints and Permissions"},
{"file_id": "450782a", "url": "https://www.nature.com/articles/450782a", "year": 2007, "authors": [{"name": "Alexandra Witze"}], "parsed_as_year": "2006_or_before", "body": "The capacity of the United States to monitor Earth's vital signs is being stymied by tight budgets and poor coordination. Alexandra Witze reports. See Editorial,  page 761  For all our content on Earth observation, visit our online  special  . \n                     Earth Observation special \n                   \n                     Group on Earth Observations \n                   \n                     Committee on Earth Observation Satellites \n                   \n                     National Polar-orbiting Operational Environmental Satellite System \n                   \n                     NASA's earth science site \n                   \n                     NOAA's satellite and information service \n                   \n                     Landsat \n                   Reprints and Permissions"},
{"file_id": "450940a", "url": "https://www.nature.com/articles/450940a", "year": 2007, "authors": [{"name": "Rex Dalton"}], "parsed_as_year": "2006_or_before", "body": "After decades of war, looting and destruction, Afghanistan's archaeologists are scrambling to restore their country's cultural heritage. Rex Dalton visited Kabul to see how they are faring. From a hillside overlooking Kabul, a dozen Afghan archaeology students have a monumental vista of their nation's ancient heritage. Domes of tombs of past kings dot the skyline; a stone wall topped with battlements snakes along a ridge; and nearby looms the fifth-century  AD  fortress Bala Hissar, site of countless battles and events, including the massacre of a British envoy and his staff about 125 years ago and the retaliatory series of public hangings of Afghans. But it is a less dramatic, century-old home site, on a knoll on the hillside, that is getting the attention today. Afghan archaeologist Zemaryalai Tarzi, of Strasbourg University in France, has brought the students here to teach them basic excavation skills. The group was to have been the inaugural class of Afghanistan's first graduate programme in archaeology at Kabul University. But instead it has turned into an impromptu field school; the challenges of setting up a master's programme in the country are too great now. It is six years since the fall of the Taliban regime, the force that routinely and ruthlessly smashed artefacts it deemed idols. And although normality is returning to many parts of Afghan life, archaeologists are still struggling to recover the country's heritage and rebuild its academic community. The threat of violence keeps many researchers from doing field projects, as funding agencies often ban archaeologists from going out to sites. Looted artefacts are being recovered, but only slowly. Many artefacts that did make it through the strife ? including a priceless collection of gold relics, called the Bactrian hoard ? are now being exhibited abroad, although critics claim that Afghanistan is not being sufficiently compensated. Taliban memories haunt nearly every aspect of Tarzi's instructional dig. The excavation site lies below a steep-walled, rocky gorge that cuts to the top of a high ridge and also holds a spring, a source of fresh water for the poor community just down the hillside. There, supplied with water that could be traded for food, the Taliban set up a command post from which to strategically control the western approach to Kabul. ?No one could come here during Taliban days,? says Hafiz Latify, an assistant at the Afghan Institute of Archaeology now studying in Greece, as we climb to meet Tarzi's group.  \n                Kidnap threat \n              But the spectre of violence has returned. Tarzi has suspended the dig because of security concerns; travelling to the site, along dirt alleys through the teeming city of Kabul, has become too dangerous. With the kidnapping of French citizens in Afghanistan earlier this year, officials in France, where Tarzi now lives and works, say he can conduct studies next summer only in Bamiyan, where the situation is more secure. Over the years, Tarzi's excavations at Bamiyan, about 125 kilometres west of Kabul, have yielded an array of artefacts, including life-like sculpted heads modelled after individuals from the past two millennia. But he wants to return to Kabul so that he can resume his teaching efforts. In some areas of Kabul, though, reconstruction is already under way. West over the ridge from Tarzi's instructional dig, the Afghan national museum in Kabul has undergone a transformation. The museum was ransacked under Taliban rule, and statues were pounded into smithereens in a rampage that did not garner as many headlines as the destruction of the huge Buddha statues at Bamiyan. By the time they left, ?the museum was a depressing ruin ? no roof, no glass, everything broken into little pieces?, says Gitta van Buuren, a Dutch cultural anthropologist who has visited Kabul since 2003 to document the city's recovery photographically. Today, the national museum is completely refurbished, the beneficiary of aid from UNESCO (the United Nations Educational, Scientific and Cultural Organization) and numerous countries. It sits in stark contrast to the Darul-Aman Palace, a bombed-out stone edifice still majestic on a nearby hilltop. The museum restorers, says van Buuren, ?did an amazing job?. The museum is now open for visitors ? although few come ? and in a laboratory on the second floor, museum staff spread out broken pieces, putting artefacts and statues back together like jigsaw puzzles. The pieces fill buckets. But the staff is turning them back into life-like forms, and the restored statues are making their way downstairs along with other salvaged objects. Omara Khan Masoudi, the museum's director, says the restoration team is making progress, but is short-staffed when it comes to skilled workers. ?We need more Afghans ? or any scientist ? to help,? he says. Facing such realities, Afghanistan has turned to touring some of its most precious artefacts in international museums. Afghanistan's key treasures include the Bactrian hoard ? about 21,600 gold coins, ornaments, pieces of jewellery and funerary relics ? discovered in northern Afghanistan in 1978 at a 2,000-year-old burial site called Tillia Tepe. They reflect a m\u00e9lange of styles ? from Greek to those of Asian tribes. When the Taliban tightened authoritarian screws, archaeologists worldwide feared these relics were lost. But in 2003, the Bactrian hoard emerged from a vault of the central bank in Kabul, where it had been successfully secreted away from the Taliban. After inventory and cataloguing, the gold artefacts went on the global museum circuit. In the past year, more than 200 artefacts from the Bactrian collection have been exhibited in Paris, and Turin in Italy. On 22 December, the exhibition is to open at the Nieuwe Kerk in Amsterdam, the Netherlands, in what is being called a 'blockbuster show', before moving to a US tour that takes in Washington DC, San Francisco, Houston and New York. But simmering behind the glitz is anger and resentment about whether the Afghans are being properly compensated. ?The Afghans were taken advantage of,? charges Lynne Munson, a former deputy director of the US National Endowment for the Humanities, who helped arrange endowment funding for the Bactrian inventory. The European exhibitions typically paid about ?150,000 (US$220,000) to Afghanistan; in Paris and Turin, about 130,000 people visited the exhibition, at an admission fee of ?8 apiece. The four upcoming US exhibitions ? potentially the richest revenue producer ? are to provide the Afghans with a total loan fee of $1 million. The Afghans will also receive 40% of merchandise sales, after expenses, for the US tour.  \n                Making the deal \n              Munson argues that the Afghans should have received substantially more, and she worries that they will see nothing from the merchandise deal given the way it is structured for payments after expenses. She blames the US tour's organizer, the venerable National Geographic Society, which will receive any additional monies from the tour. ?They took advantage of the Afghans for their own selfishness,? she says. But Terry Garcia, an executive vice-president of the National Geographic Society, says his organization worked hard to make sure the Afghans were getting a good deal, and modelled US financial arrangements on those of the European tour. ?Throughout every step of the process, we have responded to the needs and wishes of the Afghans,? he says. Munson also says that she recently learned that the document lending the artefacts was signed on behalf of the Afghan government by Omar Sultan, who took on the responsibilities of acting minister for the Afghan culture ministry after the minister was injured in a bombing. But Sultan has also worked as a paid consultant to National Geographic. ?It's a conflict of interest,? says Munson. The arrangement has been debated and criticized in the Afghan parliament, although Garcia says the negotiations had the full support of the Afghan government. The need for funds is desperate, says Ana Rosa Rodriguez, executive director of the Society for the Preservation of Afghanistan's Cultural Heritage. Her group, based in Kabul, trains staff and raises funds to protect sites by actions such as trying to restrict detrimental development at the 'city castle' of Bala Hissar. More money would translate into more conservation, she says. But some Afghan supporters say that sending the nation's artefacts on tour is a good idea ? as there is limited security to protect them at the Kabul museum. Afghan artefacts of suspicious provenance ? potentially sneaked out during the Soviet or Taliban years, or even since ? regularly show up for sale at international auction houses. In April, the International Council of Museums (ICOM) in Paris presented a 'red list' guide to Afghan artefacts that may appear for sale illegally. Afghan treasures that have been up for auction have been scrutinized, but the list hasn't yet resulted in any seizures by law-enforcement agencies such as Interpol, says ICOM's Jennifer Thevenot. This isn't surprising, as both the ICOM and Interpol have skeleton staffs for looking into questionable artefacts. If an auction house has legitimate-looking documents, an artefact is likely to be sold with little inquiry. In November, for instance, the Boisgirard auction house in Paris offered two Bactrian sculptures at prices of up to US$100,000. Thevenot says that questions were asked, but no action was taken because the house had documents. No one contacted Afghan experts such as Tarzi, who helped to draw up the red list. The items were sold, disappearing into a private collection much as they might once have gone down the old Silk Road. Those trying to save Afghanistan's culture hope that it doesn't all follow that route. Rex Dalton is a US West Coast correspondent for  Nature . \n                     Association for the Protection of Afghan Archaeology \n                   \n                     Society for the Preservation of Afghanistan's Cultural Heritage \n                   Reprints and Permissions"},
{"file_id": "449532a", "url": "https://www.nature.com/articles/449532a", "year": 2007, "authors": [{"name": "Alison Abbott"}], "parsed_as_year": "2006_or_before", "body": "International collaboration and a can-do spirit have allowed some Russian scientists to flourish. Alison Abbott watches an extraordinary field test for mutant mice in the Russian wilderness. Alison Abbott is Nature's senior European correspondent.See also the Russian science web focus at www.nature.com/nature/focus/russianscience/index.html \n                     The Mouse Genome \n                   \n                     Russian science web focus \n                   \n                     Hans-Peter Lipp \n                   \n                     Martin Wikelski \n                   \n                     International Fund for Animal Welfare video on brown bear orphans in Russia \n                   Reprints and Permissions"},
{"file_id": "449528a", "url": "https://www.nature.com/articles/449528a", "year": 2007, "authors": [], "parsed_as_year": "2006_or_before", "body": "Russian researchers, and those who have worked in Russia, share their thoughts with Nature on the problems faced by the country's scientific system ? and how they could be addressed. See also the  Russian science web focus \n                     Russian science web focus \n                   Reprints and Permissions"},
{"file_id": "450942a", "url": "https://www.nature.com/articles/450942a", "year": 2007, "authors": [{"name": "Alison Abbott"}], "parsed_as_year": "2006_or_before", "body": "Are their brains not wired to feel what others feel, or do they just not care? Alison Abbott joins researchers looking into normal neurobiology through the scope of psychopathy. It is a rare event that patient 13 is let out of the high security Dr S. van Mesdag Clinic in Groningen, the Netherlands, and he is making the most of the attention he is getting. Already, the prison guards have had to accompany him from the University of Groningen's functional magnetic resonance imaging (fMRI) scanner to the toilet four times in two hours. The guards indulge him with a shrug. Research psychologist Harma Meffert, who has recruited him for her study, is just as tolerant. That can't be easy, given that she has to spend at least 20 minutes resettling him into the scanner after each interruption Wearing nothing but blue cotton surgical pyjamas and a constant smile, patient 13 doesn't seem to present much of a threat. In fact with his jewellery removed and his tattoos covered he looks decidedly small and vulnerable. But no one is forgetting why he was recruited to Meffert's study. Patient 13 has scored the maximum possible on the Psychopathy Checklist-Revised (PCL-R) rating scale, the ubiquitous tool psychiatrists use to identify the personality and behavioural traits that define the clinical syndrome 'psychopathy'. Lack of empathy is a key feature. As it happens, Meffert's lab chief, Christian Keysers, the 34-year-old director of the university's neuroimaging centre, is not primarily interested in psychopathy per se. The major focus of his research is empathy ? the way we can't help feeling awful when we see a loved one cry, or can't stop our stomach sinking when someone's face darkens in anger. One major theory holds that we understand what another person is feeling by activating the same neural circuitry in our brains that activates when we are experiencing that emotion first hand. To investigate this trait, Keysers is comparing 'normally' empathic people with those who lack empathy, such as people with autism, and psychopaths. He suspects that psychopaths may be able to recognize emotions in others but that they are also able to disconnect that recognition from their own emotions. ?Our question is: do they do terrible things to other people because, unlike most of us, they do not share the pain they inflict?? says Keysers. His sophisticated trial design is intended to test whether this is the case (see 'Letting fingers do the talking'). Although not all diagnosable psychopaths are criminally inclined or in prison, places such as the Groningen clinic serve as a concentrated source. And they provide screening. The PCL-R scale is practically the only tool available for this purpose. In PCL-R assessment, specially trained psychiatrists discuss hundreds of issues with the patient during semi-structured interviews. On the basis of these interviews, and information about past behaviour from independent sources, such as social workers' reports, they build up a four-part assessment. The headings are: 'interpersonal', covering behaviour such as manipulativeness and lying; 'affective', covering irresponsibility and lack of empathy and remorse; 'lifestyle', tracking impulsivity and need for stimulation; and 'antisocial', which looks for records of things such as juvenile delinquency. Those on the receiving end of the assessment find it tiresome. It is, of course, not easy to put together a group of imprisoned psychopaths for an academic research project, but the Dutch Ministry of Justice provides generous access. ?We have a legal duty to try to treat all those criminals who are found guilty but not responsible for their actions due to insanity,? says Jacqueline Hochstenbach, a department head at the ministry. Although the Groningen project doesn't aim to treat or cure psychopathy, there's a general sense even among the subjects that such basic research could at least help illuminate what is wrong. For psychopaths who are deemed dangerous, there are no therapeutic options. ?We know there is no effective treatment for psychopathy,? says Hochstenbach. Pharmaceuticals don't help and those who receive behavioural therapy have a higher ? not lower ? rate of recidivism. In 2004, like other countries, the Netherlands institutionalized PCL-R testing in forensic psychiatric centres, where it is used as a risk-assessment tool for patients being considered for parole. Developed over the past three decades by psychologist Robert Hare from the University of British Colombia in Vancouver, Canada 1 , it has proved to be a powerful predictor of the likelihood that a criminal will reoffend.  \n                A qualifying score \n              To qualify for the empathy study, participants must score higher than 30 on the PCL-R scale, out of a maximum possible score of 40. Qualifiers are told they will participate, but not when ? to give no opportunity to plan escape. On the morning of the test, they are asked to confirm their consent and Meffert goes through the protocol again in more detail. She does not, however, explain the detailed scientific aims of the study in case the subjects try to manipulate the outcome. Inside the clinic, Meffert is often alone with her subjects but wears an alarm around her neck. ?Once I pressed it by accident and was amazed to find myself surrounded by several guards who seemed to spring from nowhere within seconds,? she says. ?I feel safe.? Meffert is a calm person, who works well with her subjects by talking and listening to them seriously. But she says that psychopathic people can be very tiring to work with because they command, and need, intense attention. On the morning of his test-day interview, patient 13, although taken by surprise, is looking what must be close to his best. His hair and beard are fashionably trimmed, and his clothes are casual but coordinated. Walking to the small interview room, he says he wishes he had more notice, but he is laughing. He listens to Meffert's detailed explanation of how the day will run and gives his agreement. An hour later he is on the road, in an armoured van. No metal is allowed near the scanner. Even his tattoos nearly ruled him out as a subject, but they are small, and also recent enough that the red in them is likely to be from newer, iron-free dyes that won't affect the imaging. The guards don't carry guns, the rod fitted down patient 13's trouser leg, preventing him from running, is security enough. Handcuffs are made of a special non-metal material. Patient 13 doesn't seem to think much of the experiment itself. The sequence of film clips, which are projected inside the scanner directly above his face, only run for ten minutes or so. But he finds it hard to concentrate and his eyelids, observed remotely by the researchers in the adjacent control room, begin to droop. Meffert runs the clip again ? the experiment requires the subject's full attention. Most of the others who have taken part in the study were much more compliant and easier to handle in the scanner than patient 13 ? often they are more cooperative than the average student volunteer, says Meffert. All the subjects seemed to find the experiment to be nonsense. ?It was stupid, boring,? says inmate Willem Boerema (not his real name), who claims to have taken part only because he likes Meffert. Then, contradicting himself, he adds that ?if they say the study can help people then it's good?. Boerema, smart, articulate and multilingual, has a PCL-R rating of 35 ? and a big problem with the term 'psychopath'. He views it as a fashionable label abused by the judicial system to keep people like himself from being released. ?The courts look at your PCL-R rating and add two years to your sentence, then another two years, and then another.?  \n                Damaging label \n             When he entered the prison five years ago, Boerema says, 'borderline personality' was the fashionable term, and his designated pigeon-hole. ?The psychopathy label is more damaging though ? it prompts everyone to see you as a potential serial killer, which I could never be.? (Note, in reporting this article it was agreed that inmates' crimes would be neither asked about nor reported on.) But Boerema also wears the score as a badge of honour: ?I think my high psychopath score is a talent, not a sickness ? I can make good strong decisions, and it's good to have some distance with people.? There is some truth in this, says Hare. As well as developing the PCL-R, he has also developed a shorter version suitable for screening the general population (PCL-SV, with the SV standing for screening version). He has used it to estimate that maybe 1% are psychopathic, even if they have never committed a crime, according to research presented recently at a meeting on psychopathy research. ?Some psychopathic features are not necessarily a bad thing for society ? in some professions they may even help,? says Hare. ?Too much empathy, for example, on the part of a police officer or a politician would interfere with the job.? In theory, scientists like Keysers could recruit high-scoring psychopaths from the general population as control subjects for studies on empathy. But identifying enough of them would be extremely time consuming. Some scientists without access to the captive population in prisons ? a fifth of whom may be psychopathic according to Hare 2  ? have turned to populations on the outside with specific behavioural problems. Neuroscientist Jorge Moll, for example, from the Labs-D'Or Hospitals network in Rio de Janeiro, Brazil, screened and recruited 'troublesome' outpatients of a civil psychiatric centre for his ongoing neuroimaging study to identify the neural circuits involved in moral judgement. Drug use, which can interfere with results, is a bigger hazard in those outside prisons than those inside, he concedes, ?but the standards of security in Brazil don't make a prison study feasible here?. James Blair at the National Institutes of Health in Bethesda, Maryland, gets around the drug problem by using children with behavioural problems who score highly on the PCL-SV rating, and whose parents have responded to his advertisement. Kent Kiehl, now at the University of New Mexico in Albuquerque, has worked with parole populations in Connecticut, but found the subjects so unreliable that he spent three-quarters of his time getting them to keep appointments. New Mexico is one of several US states that, like the Netherlands, are keen to promote psychopathy research. Kiehl has made the most of the supportive environment and developed a mobile fMRI machine to conduct a dozen or so different studies ? from empathy and moral reasoning to cognitive function ? on 300 inmates. ?Going into the prison means you can get many more subjects than would be possible by bringing them out individually with all the arrangements that requires,? he says. ?Larger subject numbers mean a more definitive study.? All of these psychopathy researchers believe that their work will lead to a level of understanding of the condition that could eventually lead to a treatment. Keysers does too ? even though his prime motivation in recruiting psychopaths was to support his empathy research. He now finds himself ?fascinated by the phenomenon of the untreatable psychopath?, and also convinced that there will one day be a fix. Patient 13, meanwhile, has finished his test day wearing the same smile he set out with. If a form of therapy were ever to emerge, it is not clear whether people like him ? who do not consider themselves sick ? would be willing to take it. Alison Abbott is  Nature's  Senior European Correspondent. \n                     Christian Keyser's Lab Home Page \n                   \n                     Robert Hare's Web Page \n                   Reprints and Permissions"},
{"file_id": "449652a", "url": "https://www.nature.com/articles/449652a", "year": 2007, "authors": [{"name": "Daemon Fairless"}], "parsed_as_year": "2006_or_before", "body": "India, like many countries, has high hopes for jatropha as a biofuel source, but little is known about how to make it a successful crop. Daemon Fairless digs for the roots of a new enthusiasm. With a top speed of about 110 kilometres an hour, India's Shatabdi Express is not much to brag about by the standards of a French TGV or a Japanese Shinkansen train. Nonetheless, as the stock for one of the country's fastest and most luxurious passenger lines, the Shatabdi trains have a certain prestige. So when, on New Year's Eve 2002, the Shatabdi train from New Delhi to Amritsar was powered in part with biodiesel for the first time, it was a clear statement of the government's desire to wean India off imported petroleum. Diesel is India's main liquid fuel: the country burns roughly 44 million tonnes, or 320 million barrels, of the stuff a year, as opposed to about 94 million barrels of gasoline. The trains account for a significant part of that. Kunj Mittal, who heads the government-operated rail service's engineering and traction division, says its fleet of 4,000 engines currently burns about 1.7 million tonnes a year, and that he wants to replace at least 10% of that with biodiesel at some unspecified point in the future. But he would need 200 million litres of biodiesel a year. Which is a problem. ?At this stage,? says Mittal, ?there is no mass production of biodiesel.? Like many others around India, the rail service is looking to an unprepossessing, poisonous scrub weed to try to do something about that. It has planted a million  Jatropha curcas  seedlings on unused land along its tracks and elsewhere. It's just one symptom of the jatropha fever that is spreading around the country and the world ? to the slight bewilderment of some of the scientists who best understand the shrub. Jatropha, a member of the euphorbia family, originated in Central America. It has long been used around the world as a source of lamp oil and soap, and also as a hedging plant. One of its great selling points as a biofuel is the fact that growing it need not compete with the cultivation of food. Of 306 million hectares of land considered in a report by India's Ministry of Rural Development, 173 million are already under cultivation but the rest is classified as either eroded farmland or non-arable wasteland. That's the sort of land that jatropha can thrive on, with bushes living up to 50 years, fruiting annually for more than 30 years and weathering droughts with aplomb 1 . In the early 2000s then-president A. P. J. Abdul Kalam repeatedly endorsed the plant for its potential contributions to energy security and as a route to greening barren land. Jatropha has been held to promise a reliable source of income for India's poor rural farmers and energy self-sufficiency for small communities ? all while reducing fossil-fuel greenhouse-gas emissions and soil erosion. In 2003, India's Planning Commission recommended a national mission on biofuel, a two-phase project for wide-spread cultivation of jatropha on wasteland across much of India. The first phase of the mission aims for 500,000 hectares of jatropha grown on government land across the country. The biodiesel would be produced primarily by panchayats ? local governing bodies ? at the village level, coordinated at the national level by a consortium of government departments. Should the first phase go according to plan, India's central government would embark on the second phase of the mission ? planting a total of 12 million hectares of the plant and privatizing the production of jatropha biodiesel. Although it seems likely to go ahead eventually, various ministerial meetings that might have given the national mission on biofuel the seal of approval have been postponed in favour of higher-priority issues. Despite this, several states have enthusiastically hopped aboard the jatropha express, providing free plants to small-scale farmers, encouraging private investment in jatropha plantations and setting up biodiesel processing plants. The Ministry of Rural Development, which is set to coordinate the national mission on biofuel when it is approved, estimates that there are already between 500,000 and 600,000 hectares of jatropha growing across the country. And India is not alone in its hopes for the shrub. In February 2007 China, which claims to have 2 million hectares of jatropha already under cultivation, announced plans to plant an additional 11 million hectares across its southern states by 2010. Neighbouring Myanmar (Burma) has plans to plant several million hectares; and the Philippines, as well as several African countries, have initiated large-scale plantations of their own. India looks forward to encouraging more such schemes and quite possibly profiting from them. ?Once we have an operational programme and have something to offer the world,? says Krishna Chopra, the recently retired principal adviser to India's Ministry of New and Renewable Energy, ?I think exporting the know-how would certainly be one of the first areas to develop.?  \n                The great unknown \n              Although there is reason to be enthusiastic about jatropha's potential as a biodiesel feedstock in India and beyond, there is one rather sobering concern: despite the fact that jatropha grows abundantly in the wild, it has never really been domesticated. Its yield is not predictable; the conditions that best suit its growth are not well defined and the potential environmental impacts of large-scale cultivation are not understood at all. ?Without understanding the basic agronomics, a premature push to cultivate jatropha could lead to very unproductive agriculture,? says Pushpito Ghosh, who has been working on the plant for the best part of a decade, and who is now director of the Central Salt and Marine Chemicals Research Institute (CSMCRI) in Bhavnagar. When Ghosh first arrived at the CSMCRI, the United Nations Development Programme (UNDP) had already given the institute funding for the cultivation of a modest jatropha plantation, although not for biofuels work. The idea was to see ?how to make use of waste land, coastal areas and sand dunes?, Ghosh says. The plantation started off as an unirrigated, unfertilized, 20-hectare patch of exhausted scrub: Ghosh wasn't particularly impressed when he first saw it. ?There were shrubs and they were growing,? he recalls, ?but it didn't look to me that it had what was required to make a successful plantation. 'Where are the seeds?' I said to myself. I didn't see too many of them. Merely planting and letting jatropha grow doesn't necessarily lead to productive growth.? Nonetheless, the fact that jatropha lived up to its reputation as a shrub that could eke out a living on relatively barren land piqued the interest of India's Department of Biotechnology, which provided a little further funding for exploration of biofuel possibilities using cuttings from three of the most productive plants in the UNDP trial. The seedlings were planted in small plots spread over patches of degraded, untended land in the eastern state of Orrisa. ?The results were not outstanding,? says Ghosh, ?but they were consistent.? Several plants yielded around 1.5 kilograms of seed, enough for about 0.4 litres of diesel. As modest as the results were, says Ghosh, they created a lot of interest. ?For the first time,? he says, ?we were doing something in a systematic way.? The CSMCRI's work also caught the imagination of Klaus Becker, who arrived at the institute in 2000 as a visiting agricultural scientist from the University of Hohenheim in Germany. The original UNDP plot inspired him far more than it had the sanguine, measured Ghosh. ?I saw all this green in what is otherwise a complete desert. There was absolutely nothing else around it. 'Look,' I told Ghosh, 'if you get this working, you'll be the first in the world'.?  \n                From seed to oil \n              Becker returned to Germany and set about fund-raising. By 2003 he had cobbled together a ?1.7-million (US$1.9-million) research fund comprised of grants from DaimlerChrysler, the German Investment and Development Company in Cologne, India's Council of Scientific and Industrial Research and the University of Hohenheim. With these funds, Ghosh and his team ? working in collaboration with Becker and scientists at DaimlerChrysler ? began exploring the transesterification needed to turn jatropha into biodiesel. The process had already been established by Nicaraguan researchers during the 1990s 2  and it wasn't long before Ghosh and his team were producing small batches. ?You could tell simply by looking at it that it was fairly good quality,? says Ghosh of their first attempts. Chemists at DaimlerChrysler's Stuttgart labs analysed it in more detail than the CSMCRI was able to and judged it easily good enough to meet European standards. Further tests at the Austrian Biofuels Institute (ABI), which pitted the CSMCRI's jatropha biodiesel against fuels from other feedstocks, showed that it ?clearly outperformed biodiesel from rapeseed, sunflower and soya bean oil in [its lack of a propensity to oxidize],? says the ABI's Werner K\u00e9rbitz, adding that the fuel ?showed a fully satisfying performance concerning power, efficiency and emissions?. Ghosh's vision ? and part of the CSMCRI's mandate ? was to create a version of this transesterification process that was both inexpensive and easily replicable at the village level. Nearly 80,000 of India's 600,000 villages currently have no access to fuel or electricity ? in part because there isn't enough fuel for a fuel distribution network. ?If people can grow oil directly in villages and produce biofuels themselves in decentralized plants,? says Ghosh, ?then they can achieve energy self-sufficiency. My colleagues and I are deeply committed to this principle.? ?The constant urge to simplify and to ensure that every gram of jatropha is turned into something valuable was a tremendous motivator,? he says, looking back at the project. But while he and his colleagues were still congratulating themselves on a job well done, the  Times of India  ran a story announcing that DaimlerChrysler was set to test two of its Mercedes C-Class cars on a 6,000-kilometre road test across the length and breadth of India using the CSMCRI's jatropha biodiesel.  \n                Up the Khardungla pass \n             It was the first Ghosh had heard of it. ?Our focus all along has been biodiesel as a fuel for village folk,? he says, ?not for fancy urban folk.? And on top of that there was an obvious practical difficulty. Up to this point, Ghosh and his team had only ever produced a few litres of it at a time: you can't get across India on that. Within a few months, though, Ghosh's team had developed a transesterification unit capable of producing about 250 litres a day ? adequate for use in villages and small-scale industry 3 . The Mercedes ran entirely on 100% jatropha biodiesel from this unit throughout April and May 2004 without any significant engine modifications. In the summer of 2005, DaimlerChrysler had several automotive journalists take the cars on a high-altitude test through the Himalayas, including Khardungla pass, which, at 5,359 metres above sea level, is one of the world's highest motorable roads. While Ghosh and his colleagues were making sure that jatropha could be processed as a reliable source of biodiesel, several of India's state governments were busy promoting their own jatropha cultivation campaigns. The state of Chhattisgarh, which has the most well-developed biodiesel programme in the country, has distributed 380 million jatropha seedlings to farmers, free of charge, over the past 3 years, enough to cover 150,000 hectares with the shrub. Shailendra Shukla, executive director of the Chhattisgarh Biofuel Development Authority (CDBA), says the state has also provided 80 oil presses to various village panchayats, and guarantees to buy back jatropha seeds ? which have to be hand-picked off the shrubs ? at 6.5 rupees (about US$0.16) per kilogram in order to stimulate confidence in the crop. Several local businesses have popped up across the state, says Shukla, that are now operating micro-refineries. ?These are small businesses that provide biodiesel for the use in tractors, irrigation pumps, jeeps and village power generators.? Ghosh says that the CSMCRI has received an order for a refinery from the country's Defence Research and Development Organisation, part of India's Ministry of Defence. He explains that the unit would be capable of producing about 1,000 litres a day and would cost about 14 million rupees to install. In such a plant, he says, each litre of biodiesel would have a net production cost of about 26 rupees if the seed pods are bought at 6 rupees per kilogram and every scrap of seed and seed pod is converted into something valuable, with the seed going into oil, the bi-product seed cake into fertilizer and the seed husk into a high-density brick that can be burnt for fuel. The wide governmental support has also attracted substantial business interest. D1 Oils, a UK-based biodiesel producer, is the world's largest commercial jatropha cultivator, responsible for around 81,000 hectares of jatropha in Chhattisgarh and in the southern state of Tamil Nadu, with plans for an additional 350,000 hectares over the next few years. ?The entire programme revolves around the government-funded jatropha seeds,? says Sarju Singh, until recently managing director of D1 Oils India. ?The government gives farmers free or subsidized seedlings and D1 Oils guarantees to purchase the seeds at the price prescribed by the state.? The company claims to have invested more than \u00e93 million (US$6 million) in plant science and financing its share of the plantings, which are joint ventures.  \n                Cautious approach \n             Yet most of these plantings have yet to reach whatever maximum level of productivity they might eventually attain ? the plants need a few years to bed in. And Ghosh is wary of subsidizing jatropha too much before mass cultivation of the plant is fully understood. ?A lot of government funds may go down the tube,? he warns. Ghosh doesn't want the farmers to take on too much risk, so he is suggesting that they intersperse jatropha between their current crops, rather than banking on it as a cash crop. Shukla has similar reservations. ?My immediate concern,? he says, ?is that because the seeds are derived from wild plants there is no assurance of yield.? Shukla says the CBDA, like Ghosh, is promoting jatropha as something farmers limit themselves to planting between their rice fields. The only situation where all are agreed that it makes sense for small farmers to cultivate whole fields of jatropha is on farm land that has become or is becoming unproductive. It is a good fallow crop, says Becker: ?It has a deep root system which stops ground erosion and increases water storage in the soil.? This, he says, leads in turn ?to more biomass growth and an accumulation of organic carbon in the soil?. Henk Joos, D1 Oils' director of plant science and agronomy, agrees that assured yields and the techniques needed to achieve them on a large scale need a lot more research. Yield estimates currently vary a great deal. India's Planning Commission estimates about 1,300 litres of oil per hectare, but Ghosh, conservatively, foresees a figure of about half that. Yield research is the main focus of D1 Oils' Indian operations, he says. The company is currently testing a number of jatropha varieties to see which ones grow best in India's varied climatic zones. ?It will be two or three years before we get real scientific data to base an industry on,? he says. ?We are not there yet, we have a lot of work to do.? This is the sort of work Ghosh is currently overseeing at the CSMCRI's test plots. ?It isn't the most glamorous work, but the mass multiplication of reliably producing plants is key to developing an industry, he says. Ghosh and his team are looking at precisely what kind of soil conditions and just how much water jatropha needs in order to reliably pump out oil-bearing seeds. The fact that jatropha plants can survive droughts does not mean they will not be more productive if they get more water. The optimum amount of water is still unknown. The team is also continuously on the lookout for plants that could be potential progenitors for a generation of a high-yield crop. ?We have one plant which has given us 5 kilograms of seed,? says Ghosh. ?We have yet to get that from any other plant.? The CSMCRI is trying to perfect the use of shoot-tip cuttings as a means for mass-replication of jatropha plants so it can capture their best attributes. Culturing tissue cuttings from the plant's growing tip, says Ghosh, is the most reliable means of propagating exact copies of a parent plant, an important step in creating an army of dependable high-yield clones. It's a common enough technique ? but like so much technology, it hasn't yet been reliably adapted to jatropha. ?The problem is, we just don't have the protocol right,? says Ghosh. These various efforts are not part of any overarching plan. Despite the general enthusiasm for India's national mission on biofuel, there is a definite lack of cohesion at the national level. ?Right now, ad-hoc research is being done by different agencies,? says Chopra, ?but it doesn't add up, because they each do their own thing.? A national biofuel policy that was written by Chopra and his colleagues shortly before his retirement might help. It envisages an authority that would coordinate research and provide funding through various government agencies in order to cultivate jatropha on an industrial scale. But this policy, like the national mission on biofuel, has yet to go through the cabinet. In this case, it has been stalled by disagreements between various ministries on how to price jatropha ? the Ministry of New and Renewable Energy suggests subsidizing seeds; other government ministries suggest subsidizing biodiesel itself. But, says Chopra, ?I expect it will come together, perhaps this year or early next year?. Ghosh remains cautious and optimistic in level-headedly equal measure. ?We must neither get carried away by hype nor get despondent if the initial results of cultivation are not as per expectation,? he says. ?The future will depend on how seriously and scientifically we pursue our goals.? See Editorial,  page 637  . Daemon Fairless is this year's winner of the IDRC-Nature fellowship. \n                     India's Ministry of New and Renewable Energy \n                   \n                     Central Salt & Marine Chemicals Research Institute \n                   \n                     D1 Oils \n                   \n                     Bioresource Technology \n                   \n                     Natural Resources Forum \n                   \n                     International Journal of Environmental Studies \n                   Reprints and Permissions"},
{"file_id": "450785a", "url": "https://www.nature.com/articles/450785a", "year": 2007, "authors": [{"name": "Amanda Haag"}], "parsed_as_year": "2006_or_before", "body": "See Commentary,  page 789  For all our content on Earth observation visit our online  special  . \n                     Earth observation special \n                   Reprints and Permissions"},
{"file_id": "450018a", "url": "https://www.nature.com/articles/450018a", "year": 2007, "authors": [{"name": "Alison Abbott"}], "parsed_as_year": "2006_or_before", "body": "Is blasting into a river bluff any way to do palaeontology? Alison Abbott reports on an unusual expedition into the Alaskan wilderness in search of the bones of polar dinosaurs. The rabid fox was almost the last straw. In the middle of the Alaskan wilderness, it was tugging at the guy ropes of the mess tent so ferociously that it had torn open its mouth, leaving a trail of bloody spittle across the snow. The gun was in another tent. One person distracted the fox by swinging at it with a frying pan, while another slipped out the back to retrieve the gun. But the ammunition was missing. It took fifteen terrifying minutes to locate it and shoot the fox. \"A comedy of errors\" is how Australian palaeontologist Thomas Rich describes the scene. Not funny at all, counters Ruth Berry, the filmmaker who had travelled with Rich to the field site to film his unorthodox plan for getting to dinosaur fossils with the help of some well-placed dynamite. By the time the fox appeared, during the second of three intense expeditions, relations were such that Berry was close to walking away. Five years ago, the project had looked distinctly more promising. \"There are three themes you can always sell at a film market,\" says Berry, who has won assorted awards for her science documentaries. \"Hitler, mummies \u2014 and dinosaurs.\" Rich was offering what seemed like a once-in-a-lifetime opportunity: exclusive filming rights to his plan to blast a tunnel into the northern Alaska permafrost above a bed of fossilized dinosaur bones. At the time, Berry was making a film about the dinosaur bed that Rich, who holds a post at Museum Victoria in Melbourne, had worked for decades on the south coast of Australia with his wife, Patricia Vickers-Rich of Monash University. As Berry researched and shot the film that would be released as  The Terrible Lizards of Oz , Rich told her how, in the mid-1980s, he had built a tunnel into a fossil-rich cliff face in Australia \u2014 and how he wanted to do something similar in Alaska. Such a tunnel, if it worked, would improve access to bones containing unprecedented information about polar dinosaurs. Seventy million years ago the Alaskan landscape was covered with a rich mixture of vegetation including deciduous conifers and ferns, and was home to diverse species of polar dinosaurs. Rich argued that a tunnel could reveal much about the Alaskan dinosaurs and the environment in which they lived 1 . When Berry, an Australian living in Munich, began shooting her new film, she was convinced of the purity and nobility of science and its processes. She lost this naivety quickly \u2014 but her documentary will have a happy ending. This summer she was able to film palaeontologists \u2014 although they did not include Rich \u2014 working the floor of a sturdy permafrost tunnel and finding what they describe as a treasure of fossilized bones. \"The film may have an edge to it I hadn't predicted, though,\" says Berry. \"The tensions in the field were often awful.\" Palaeontologists traditionally rely on the natural forces of wind and water to expose fossils, and then work the bones out by hand using pick or brush. On rare occasions they will use earth-moving equipment to get closer to a fossil-rich rock formation. But Rich planned to use dynamite to blast a tunnel above a well-established bone-bed, in order to penetrate deep into the frozen ground to where fossilized bones had not been subjected to seasonal cycles of freezing and thawing.  \n                New digs \n              Rich told Berry how difficult it had been to get research-funding agencies to pay for a tunnel. \"But somehow I didn't believe him,\" she says. \"I was convinced that the importance of learning about the past history of the Earth would be evident to any research body and that there would be no problem.\" Hawking the idea at film markets was as easy as Berry had thought, and she rounded up around $800,000 \u2014 mainly from Film Finance Corporation Australia and from NOVA, a series produced by WGBH in Boston, but also from smaller investors, including the Australian Broadcasting Corporation and ARTE France. This, she reckoned, would cover the costs of three expeditions to bring film crews and scientists to the remote site. Their target was the Liscomb bone-bed, located more than 900 kilometres north of the university town of Fairbanks, on the banks of the Colville River on Alaska's North Slope. The site is named after Robert Liscomb, the Shell Oil geologist who discovered fossils there while prospecting in 1961. Liscomb misidentified the bones as belonging to mammoths and died in a rockfall the next year; the fossils languished for years in Shell's archives. They came to light only in 1984, when Shell handed them over to the US Geological Survey, which identified them as 70-million-year-old fossilized dinosaur bones. The Liscomb bed turned out to contain a whole trove of dinosaur fossils \u2014 mostly fragmented skulls and bones belonging to hadrosaurs, plant-eating dinosaurs known as duckbills. Throughout the 1980s and 1990s, the site had been worked by Roland Gangloff, a palaeontologist at the University of Alaska in Fairbanks, who then handed over key work to Anthony Fiorillo, a palaeontologist at the Museum of Nature & Science in Dallas, Texas, who specializes in Alaskan dinosaurs 2 ,   3 . By spring 2007, Berry was set to go, but Rich had still not secured funding to build the tunnel itself. So Berry made the daring decision to pay for it with $167,000 out of her film budget. This meant cutting back on computer animation \u2014 a high price to ensure the continuation of her filming project. \"But, to be honest, I was seduced by the excitement of the science and the purity of scientific values,\" she says. \"I felt I was doing something good for the world.\" NOVA was keen to have an American protagonist in the film as well as an Australian. Berry got in touch with Fiorillo, widely seen as the leading expert on Alaskan dinosaurs, and arranged to film his team at a nearby bone-bed in August 2006, when the group planned to helicopter out a plaster jacket containing a huge horned dinosaur skull. And because the Australian Broadcasting Corporation wanted Rich to be the film's main protagonist, Berry paid for him to fly up and be filmed talking palaeontology with Fiorillo. Only then did she learn that you can bring scientists together, but you can't make them spark. During two days in the field, they didn't speak, in part because Fiorillo was ill. \"I hadn't appreciated there would be rivalry,\" she says. \"I'd imagined them talking together excitedly about the great science that might emerge.\" Fiorillo and his colleagues clearly felt their site was being invaded by Rich's plan to test the feasibility of building a permafrost tunnel. \"It's a bit audacious to go into a well-occupied site solely to see if a technology works,\" says Fiorillo. That was a taste of problems to come. In March 2007, Berry left on her second expedition to the Colville, planning to bring the heavy mining equipment up on the still-frozen river. When she arrived in Fairbanks with her small film crew and a team from the University of Alaska, she found that the permits were not yet ready \u2014 delaying the start for two weeks. Mike Kunz, the Bureau of Land Management field officer responsible for the Colville area, was also starting to worry that Rich did not understand the difficulties of working in the extreme conditions of the Arctic at that time of year. \"There was consternation about this among the expedition principals,\" he remembers. Getting out to the site involves a two-day drive from Fairbanks through increasingly wild locations with names such as Coldfoot, Gobblers Knob and Last Chance. The road, built to follow and service the massive Alaskan oil pipeline, finishes at Deadhorse. From there, it is a four-hour drive southwest to get to a site maintained by the oil company ConocoPhillips, where Kunz arranged for the expedition to stay while a skeleton team travelled the remaining distance to set up the final camp.  \n                High stakes \n              Team spirit broke down almost as soon as the full group reached the Colville. One of the stakes Rich had planted in summer a few years earlier to mark where the tunnel should be drilled had disappeared, and the remaining one seemed to be in the wrong place; squabbling ensued. And as temperatures sank as low as \u221240 \u00b0C, and 20 \u00b0C colder with wind chill, Rich worried team members by failing to adhere to safety instructions, often leaving his tent without the regulation hat and gloves. Rich says that his experience working in Antarctica had taught him what was acceptable and what was not in extreme weather conditions. Within ten days, a team of permafrost miners led by Robert Fithian had blasted into the river bluff and created the tunnel. Checking frequently that they were not hitting bones, and altering the angle when that seemed to happen, they carved a volume roughly ten metres deep, three metres wide and three metres high, with insulated thermal walls, a thermal door and a portico jutting out of the cliff face. The tunnel's floor rose towards the back so any water entering in the event of spring flooding could flow out again. On the last morning, Rich decided to baptize the freshly minted tunnel with a bottle of champagne he'd brought for the occasion. The others were too busy at the time, so Rich went and opened the bottle alone. Not helpful, as far as the filmmaker was concerned. \"We tried to redo the celebration later in the day,\" says Berry. \"I didn't film it with the big camera because I thought it would look insincere.\" It's a decision she now regrets. The final expedition, in August 2007, was to be the climax of the filming. Berry had hoped to capture a glorious moment when the door was opened. She would have liked to film Rich with Kevin May, a palaeontologist from the University of Alaska, locating the bone-bed after some careful digging \u2014 and ideally pulling out an extraordinary fossil specimen. But the August trip was no less traumatic than the spring one.  \n                Floor plan \n              When the expedition assembled at Deadhorse, it emerged that Rich planned to use power tools to cut half of the tunnel floor into 40-centimetre blocks and transport them to a laboratory in Australia, leaving the other half for May to excavate with his more conventional methods. May was worried that blocks of permafrost might disintegrate when they thawed; Berry was furious at what she saw as a personal betrayal. \"I'd invested a lot of time, energy and money in the project,\" she says, \"and there would be nothing to film if the bed was just carted away.\" Kunz was also concerned. He put Rich's Bureau of Land Management licence on hold while he checked with experts to ensure that the large air compressor and its supply of diesel Rich planned to have helicoptered to the top of the bluff would not endanger the environment. He cleared that aspect, but put his foot down when it came to cutting the blocks. \"These issues had not been addressed in the permit application,\" he says, \"and when Rich had mentioned it to me a few months earlier I'd informed him it wasn't going to be acceptable. The blocks would melt into glob and we'd have no idea of the position of bones in relation to each other or to other items in the excavation unit.\" Rich, for his part, wasn't happy with what he considered \"micromanagement\". He says that none of the permit officers had asked for details about the method of excavation and that those on site in Alaska \"did not understand the pioneering nature of the project\". He says he thought he could do more work in carefully controlled lab conditions away from the site. \"The tunnel is so cold and cramped, it makes everything very slow,\" he says. With the disgruntlement following the team to the bone-bed, the next disappointment was finding the tunnel chock-full of debris and ice \u2014 much more than had been anticipated \u2014 from spring flooding. May and his team were unable to help dig it out, having had no safety training with a jackhammer. The unskilled workers took as long to clear the tunnel as the miners had taken to dig it from scratch. With no chance of removing the blocks, Rich left with his team of four a fortnight earlier than planned, even before the team had reached the bone-bed in the tunnel, and transferred his permit to May. By the time the film crew left, Berry did not have her hoped-for 'eureka' moment in the can. May then had only six days left to dig with his remaining team, comprising just one museum colleague, his 11-year-old son and Anne Pasch, emeritus professor at the University of Alaska. But he says that was long enough to dig 30 centimetres deep and find around 80 bones in good condition \u2014 \"much better,\" he claims, \"than what we would have expected to gather outside, where they would have been subject to successive freeze\u2013thaw cycles.\" Although very cold, the tunnel was \"a fabulous place to work\", he says. \"And although we lost a lot of time this season, next year we'll be better prepared and we'll hit the ground running.\" But Fiorillo, who was working on the same bone-bed 75 metres from the tunnel in August, insists that it offers no advantage. \"I have been finding intact fossils outside the tunnel without problem,\" he says. He adds that the expedition disturbed his own team: \"The noise of the generator they were using to light the tunnel was making us crazy while we were working the bone-bed, not to mention all the helicopter traffic.\" Fiorillo and his team were so harried they shifted their camp \u2014 \"hard to imagine how it could get crowded in the Arctic, isn't it?\" he asks with dark sarcasm \u2014 and he says that they couldn't excavate exactly where he wanted to. He also objected to Rich's arriving at his technology project with a group of scientists. But Kunz says that the tunnel worked \"spectacularly well\". Berry, although still bristling, also concedes that the tunnel was a success. \"In fact, it's beautiful, but no one can say that it has proved its scientific value yet,\" she says. \"That will maybe happen next year.\" She says she'll still have her film, even if the shots from Alaska were not all she had imagined. The back story \u2014 about how palaeontologists showed that dinosaurs really existed at the poles and how scientists are now trying to work out how they managed to survive in the cold, dark extremes \u2014 is exciting enough for most networks, she says. \"In showing the world that dinosaurs really did live at the poles, I'll have achieved my aim.\" Alison Abbott is  Nature  's senior European correspondent.  Watch a short video of excerpts from Ruth Berry's film  Dinosaurs on Ice . \n                     Video of footage from 'Dinosaurs on Ice' \n                   \n                     University of Alaska Museum of the North \n                   \n                     Alaska Museum of Natural History \n                   \n                     Ruth Berry's company \n                   Reprints and Permissions"},
{"file_id": "450937a", "url": "https://www.nature.com/articles/450937a", "year": 2007, "authors": [{"name": "Emma Marris"}], "parsed_as_year": "2006_or_before", "body": "Accustomed to adapting to nature's whims, gardeners are more prepared than most to take on the challenge of climate change. Emma Marris asks them what to grow in a greenhouse world. ?This concept that gardening puts you in harmony with nature is a big lie,? says Peter Del Tredici, a botanist at Harvard University in Cambridge, Massachusetts. ?Gardening is really about preventing nature from doing what it wants to do, which is to destroy your landscape, and gardeners know this at their core. Climate change is just another challenge.? At The Royal Botanic Gardens in Kew, London, the English oaks are ailing. High temperatures and dry conditions over consecutive years have stressed the trees, and wood borer beetles have been taking advantage. ?A number of oaks are looking very sad. The weakening of the tree means that beetles come and finish them off,? says Nigel Taylor, Kew Gardens' curator of living collections. The leaves, too, are eaten, by the caterpillars of the oak processionary moth,  Thaumetopoea processionea.  To add irritation to injury, these invaders from southern Europe shed hairs that can cause severe allergic reactions in park visitors. ?If we get to the stage of a major epidemic, I can see us having to close substantial parts of the park,? says Taylor. In ten years, he says, all of London could be affected as the caterpillars become established in northern climates. Meanwhile, a stroll through Kew Gardens reveals many tender plants and Mediterranean species that would not have been grown outdoors a few decades ago. ?The last great winter was 1963,? says Taylor. ?I remember it from when I was a boy. These days, about a quarter of the plants we grow outdoors would not have survived that winter.? In many places a warmer and less predictable climate seems to be remaking the context in which gardeners sow and reap. Blooming, sprouting and frost times are shifting unexpectedly. Traditional plants are suffering, whereas exotic species are thriving, and unfamiliar pests and weeds are showing up. Gardeners have no choice but to respond to the challenges ? and opportunities ? offered by their climate-changed gardens. You can see their responses in the latest trends in British urban gardens: subtropical and vegetable gardening. Olive trees and even tropical avocados have been seen growing in London. In 2006, sales of vegetable seeds in the United Kingdom overtook sales of flower seeds for the first time since the Second World War, according to the Royal Horticultural Society. One reason could be that, for the eco-conscious gardener, home-grown vegetables avoid the carbon emissions associated with importing produce from overseas.  \n                Breaking with tradition \n              Yet the growth of the new brings with it reasonable fears for the old. Some gardeners worry that much-loved traditional species are being adversely affected by the changing climate. And the timing of seasonal events does seem to be shifting. In the Shangai Botanic Garden, cherries and gardenias reportedly bloomed 15?20 days early this year. Del Tredici notes that the annual lilac festival at Harvard's Arnold Arboretum has been brought forward a week after a couple of Lilac Sundays (as the festival is known) nearly missed the peak bloom. And some of the famous cherries in Washington DC bloomed in January rather than their usual April this year. And one startled man on the street stopped me, saying, ?Do you see this? It's not natural!? It's not easy to say whether these shifts are caused by global warming or are just the result of natural climate variability. According to Simon Brown, head of climate extremes research at Britain's Met Office Hadley Centre, all that scientists can say for sure about climate change in the United Kingdom is that it increases the probability of extreme events, such as hot, dry summers and mild winters. Events such as the 2003 summer heat wave in Europe, he says, are now at least three times more likely. He adds that most climate models also predict fewer frost days across the country.  \n                Redrawing the map \n             Changes in the plant hardiness zones used by gardeners to choose what plants to grow are seen by some as evidence of systematic change (see 'US Plant Hardiness Zones'). These zones map the possible growing areas for different plants as defined by regional average annual minimum temperatures. In the United States, the best known map is produced by the Department of Agriculture (USDA). The current map, released in 1990, is based on 15 years of temperature data, and an eagerly awaited revision will be based on 30 years. But impatient with the USDA's progress, in 2006, the national Arbor Day Foundation produced its own update of the 1990 map, based on the 15 years prior to 2006. In it, the zones were shifted noticeably northward, and many chalked that up to climate change. But USDA spokesperson Kim Kaplan insists that data sets of just 15?30 years are not good enough to diagnose climate change. ?Call back in 50 years and I'll let you know,? she says. And she points out that temperature is not the only important variable, especially in the activity that is the USDA's main focus: farming. ?It is not the temperatures that tend to change what farmers grow, but their effect on moisture,? she explains. ?One or two degrees isn't enough to affect most plants, but where we are seeing shifting patterns of rainfall, that has a major impact on what farmers can grow.? Climate change is expected to increase rainfall in some parts of the world and decrease it in others ? with potentially disastrous results for agriculture. The challenge faced by farmers is much more serious than that faced by gardeners. But precisely because less rides on gardening, horticulture can be seen as a way to experiment with strategies to adapt to changes in the climate, some of which might then have broader relevance. ?Home gardeners tend to be kind of adventurous,? says Peter Raven, director of the Missouri Botanical Garden in St Louis, ?so they will continuously be pointing the way to what can be grown.? They have the luxury, not available to other land managers, from farmers to city planners, of changing what they do each season. ?It is a lot easier in gardening than it is in many other spheres,? Raven says, ?You can adapt with new plants every year.? And some are already adapting to a warmer and less-predictable climate. ?The professional gardening community is beginning to think carefully about what it is going to plant,? says Taylor. ?Some have even written contingency plans.? Used to suffering from the vagaries of the weather, gardeners might be better prepared than many for the changes that will occur as humanity fills its atmosphere chock-full of heat-trapping carbon dioxide and other gases. ?As any gardener knows, the weather was engineered to make us miserable,? says Todd Forest, vice-president for horticulture and living collections at the New York Botanical Garden, which recently held a symposium on gardening and climate change. But he adds, ?Gardeners love to experiment. They love to try new things. You might be able to grow things in New York that you couldn't grow before. We will look at those opportunities.? Not everyone is so positive. Scott Aker, a horticulturist at the US National Arboretum in Washington DC says that climate change is likely to be mostly bad for gardeners. ?I don't believe that global warming is going to allow us to grow things that were previously not hardy enough here,? he says. In fact, Aker, explains, because plants go into a state of dormancy for the winter, which is triggered by gradually lowering temperatures, a warm winter and then a cold snap will be much more damaging than the same cold snap after the rigours of a cold autumn and early winter. ?We can dispense with the idea that we are going to be growing coconut palms in Washington any time soon,? he says. And some of the knock-on effects of climate change will be too complex to predict. At Kew, they have been watering more in the rash of dry summers they've seen, and all that London tap water is turning their soil alkaline. Changes in precipitation and microclimate will vary, and perhaps the only firm certainty is that the weather will be less predictable. According to Aker, the uncertain weather wrought by climate change may narrow rather than broaden the range of plants that can be grown in any one place. At least until plant breeders are able to produce tougher varieties. ?That is going to be the focus of breeding,? says Aker. ?The plants that we put in our gardens 20 years from now are going to have to be able to withstand a lot more extremes of temperature and drought.? Apart from changing temperatures and moisture patterns, climate change also expands the ranges for many pests and pathogens. ?I would say that perhaps the most significant things affecting horticulture are the new pests and diseases,? says Taylor. At Kew they are seeing one or two new pests every year. Scott Ogden and Lauren Springer Ogden are landscape gardeners who, by virtue of maintaining gardens in two unpredictable climates ? Colorado and Texas ? are now advising gardeners in heretofore meek and mild climes such as the northeast and northwest. Their advice for handling climate change? Plant more species, so even if some fail others will flourish. They say that if gardeners try to hold on to species they've always grown, they may have to water, fertilize, and generally manage them more. ?It's a much more mixed bed,? says Ogden. ?Instead of maintaining the plants artificially, find the plants that are going to work.? Other advice: forget about relying on long springs to bring out your show-stopping flowering trees ? they might bloom in February and then get zapped by a cold snap in March. And watch those formal gardens that rely on broadleaf evergreen hedges. They don't like erratic freezes. Instead, go layered and diverse. Then, ?when things ebb and flow there is always something looking good,? says Springer Odgen. The message is to take control by not being too controlling; to worry less about traditional species and to embrace well-adapted species whatever their source (as long as they aren't destructively invasive). The new look in the climate-adapted garden is rambunctious, diverse and more like wild spaces.  \n                Up to date \n              No matter what their local climate does, gardeners will notice. ?If you ask a gardener what the ten-day forecast is or whether it has been a wet or dry fall, chances are that they will know,? says Forest. In a sense, every year is a new mini-experiment in each garden. And in some regions gardeners are being asked to put their famous attention to detail to scientific use by recording and reporting data. Phenologists, who study seasonal phenomena, are enlisting citizens to record data in projects such as the US-based Project Budburst, the United Kingdom's Nature's Calendar, the Netherlands' De Natuurkalender and Canada's NatureWatch. Kayri Havens, a conservation biologist at the Chicago Botanic Garden in Illinois, helps run Project Budburst. She says that in its pilot year, about two-thirds of their 1,800 observations of blooming times were from children under 12. Havens hopes that the data will be used to predict where plants may need to migrate so that their blooming coincides with their pollinators' cycles. Data from an older volunteer project run by the University of Wisconsin, Milwaukee, which monitors lilac bloom times, have already been used in more than two dozen scientific papers. Gardeners are moving into a space where many others are still loath to go. When it comes to climate change, says Raven, gardeners can afford to experiment. They accept climate change as fact, and they work with it (see 'The green gardener'), some even do so cheerfully. ?I call this a brave new ecology,? says Del Tredici, ?The reality is that climate change is already happening, so we have to learn how to live with it.? This approach might be a model for managing other activities, beyond the backyard. After all, says Stephen Hopper, head of Kew, ?Some people argue that the world is managed so much that we are all gardeners.?\n Emma Marris is keeping an eye on when the redbuds bloom in Columbia, Missouri. \n                     Nature Reports Climate Change \n                   \n                     Evolution & Ecology \n                   \n                     Climate and Water \n                   \n                     Sustainable Development \n                   \n                     Science in Culture \n                   \n                     Scott Ogden and Laura Springer Ogden's page \n                   \n                     New York Botanical Garden climate change symposium \n                   \n                     UK Forestry Commission's page on Oak processionary moth \n                   \n                     Douglas Kent and Associates \n                   \n                     Nature's Calendar \n                   Reprints and Permissions"},
{"file_id": "448988a", "url": "https://www.nature.com/articles/448988a", "year": 2007, "authors": [{"name": "David Chandler"}], "parsed_as_year": "2006_or_before", "body": "When it was trying to catapult satellites into orbit the private launch business didn't get very far. Can it do better now that it's focused on giving the rich the ride of their life, asks David Chandler. In the 1990s, the 'new space' vision was simple. Constellations of communications satellites in low-Earth orbit would require launch systems that could pop up little replacement satellites easily and cheaply. Small entrepreneurial companies would be able to meet the demand better than conventional launch providers such as Lockheed Martin, Boeing, Arianespace or even the cheaper options from the post-Soviet states. Unfortunately, the vision didn't get very far. The would-be contenders were short of capital, and the technological challenges were greater than appreciated. Worst of all, the vast expected market did not materialize. In the past couple of years, however, the new-space sector has been booming, thanks to a change in its business model. Private spaceflight has always been a business for dreamers, and now one group of dreamers \u2014 the people building the spacecraft \u2014 has realized that another, much larger group of dreamers might be willing to pay a lot to ride on those spacecraft even if they can't do useful things such as get into orbit or launch satellites. A company called Space Adventures, based in Vienna, Virginia, pioneered the marketplace for space tourism. Since 2001, it has sent five private individuals into orbit on Russian spacecraft, at US$20 million a trip. Then, in 2004, Scaled Composites of Mojave, California, became the first venture to build and launch a privately funded piloted rocket up to the edge of space. Entrepreneur Richard Branson immediately signed on. Starting in 2009, Virgin Galactic, a US subsidiary of the Virgin Group founded by Branson, plans to offer sub-orbital trips with a three-minute ride in weightlessness and an out-of-this-world view of black skies and curved horizons. At 1% of the fee for flying to the International Space Station, the trips are cheap only in a relative sense \u2014 but the number of people with $200,000 to spend on such an adventure is growing, and the company says that it has taken at least $25 million in advance bookings. \n               Dream teams \n             Numerous companies are now working on similar projects \u2014 a big change from the days of satellite dreams (see  'Private spaceflight: the different approaches' ). \u201cIt's a very different situation now,\u201d says former space-shuttle astronaut Jeffrey Hoffman, now a professor of astronautics at Massachusetts Institute of Technology in Cambridge. Even traditional industry giants, such as Paris-Based Astrium and Northrop Grumman in Los Angeles, California, are joining the game. But these developments do not mean that the business will be an easy one, or risk free. On the morning of 26 July, a loud blast ripped through the Mojave desert in southern California as a routine propellant test at Scaled Composites triggered an explosion. Three people died and three others were injured. But the industry as a whole seems not to have been badly shaken by the accident. For one thing it was not a problem with the spacecraft that Scaled Composites is developing for Virgin \u2014 SpaceShipTwo \u2014 nor with its engines, but with gas storage. For another, the companies are all working with very different technologies. And for a third, most of the leading companies have relatively few investors to chase away \u2014 they tend to be dependent on the interest and investment of a particular multimillionaire or billionaire. As long as a small number of investors keeps the faith, the boom can continue. Scaled Composites has a clear lead over the rest of the competition. No other company has yet managed to emulate the feat with which its SpaceShipOne, a small rocketplane launched from beneath a larger carrier jet, won the $10 million Ansari X-prize. Scaled Composites is \u201cthe main chance here\u201d for succeeding with a passenger-carrying rocket, says Robert Zubrin, an aerospace engineer from Colorado who in the 1990s worked to develop spaceplanes to serve the satellite market. Actually getting a vehicle into space, Zubrin says, \u201cis a very tough thing. You put all this money in, and then something fails for the most trivial reason. You really have to be able to take a lot of punches before you make it through the door.\u201d \n               Leader of the pack \n             Scaled Composites has already taken most of those punches \u2014 cushioned in no small part by the bankroll of Paul Allen, the co-founder of Microsoft, who paid for most of the $20-million-plus cost of SpaceShipOne. SpaceShipTwo is meant to extend the success into a business with a small fleet of rocketplanes that can each carry two pilots and six paying customers. Whether that sort of business plan can sustain itself over the long term remains to be seen; the industry, however, is banking on it. Jane Reifert, president of the adventure-travel business Incredible Adventures in Sarasota, Florida, arranges trips for people who want to fly Russian MiG jets or cage-dive among great white sharks. She says that she has a long list of customers willing to pay for suborbital space flights. \u201cPeople just want to do it,\u201d she says. \u201cThey don't care whether they're the first or the three-thousand-and-first.\u201d Incredible Adventures has a contract to sell space rides with Rocketplane Kistler in Oklahoma City, Oklahoma \u2014 a company that is happy to make money from suborbital flights, but actually has its eye on getting people all the way to orbit. In this case, though, the people are government employees, not wealthy thrill-seekers. NASA's space shuttle is set to be retired in 2010, and its replacement \u2014 the Ares/Orion system \u2014 won't be ready to carry cargo and astronauts to the International Space Station until at least 2015. For those five years, NASA will have to rely on the Russians for transporting people and the Russians, Europeans and Japanese for transporting cargo. It would like to be able to supplement that capability with launches by private companies. So the agency has set up a Commercial Orbital Transportation Services (COTS) programme. In the first phase of this programme NASA is providing up to $500 million in the form of matched funds to private investments in two companies \u2014 Rocketplane Kistler and Space Exploration Technologies (SpaceX) of El Segundo, California. The idea is to develop systems that can take supplies and, further down the line, crew members to the space station. In a major departure from the way NASA has always done business, the agency is specifying the end result but leaving design choices and related trade-offs to the companies; the companies will end up owning the technology and charging NASA, and others, for their services. Of the two, SpaceX seems to be the most likely to make it. \u201cMost knowledgeable people think SpaceX has a reasonable chance of success,\u201d says John Logsdon, an expert in space policy at George Washington University in Washington DC. SpaceX was founded in 2002 by Elon Musk, who by the age of 30 had made hundreds of millions of dollars through Internet start ups, most notably PayPal. SpaceX has developed a small, single-engine rocket called Falcon 1, which has so far been launched twice: the first launch, in 2006, careened into the ocean; the second, earlier this year, soared to an altitude of 200 kilometres \u2014 well into space \u2014 but didn't make it into orbit. The problem was traced to fuel sloshing in a second-stage tank. But all the most difficult parts of the mission \u2014 rocket ignition, liftoff, guidance, second-stage ignition and separation \u2014 worked without a hitch. With the sloshing fixed, SpaceX plans to place a satellite into orbit with its third test flight in early 2008. And by the end of 2008, Musk hopes to launch a nine-engine, 54-metre-high version that will ultimately be capable of carrying a crew of seven to the space station; on return, the crew capsule would parachute down to the ocean. Musk hopes to get into orbit for about a tenth of the $200 million cost of launching the similar-sized Atlas V, which is sold by a joint venture between Lockheed and Boeing. A key to making things cheaper, says Musk, is optimizing efficiency in every possible way. The Falcon craft has been designed from scratch, and every component \u2014 the propulsion, ground operations, electronic systems and overhead \u2014 has been streamlined as much as possible. \u201cWe've made substantial improvements that would be considered breakthroughs in all those areas,\u201d says Musk. \u201cThere's no one thing that is really the fundamental improvement.\u201d To achieve all the cost savings he thinks possible, though, Musk will also need to find ways to reuse the rocket engines by bringing the spent booster stages safely back to Earth. \n               Off target \n             Rocketplane Kistler, the other potential COTS beneficiary, also hopes to make things affordable by making its stages fully reusable. Every part, including the crew capsule, is designed to parachute down to dry land for recovery. The struggling company could be at the end of its rope financially, however. It has missed several target dates for securing private financing, and the  Wall Street Journal  reported in late August that the firm had not been able to line up the institutional investors it needed to continue operations. The company was formed in 2006 through the merger of Kistler Aerospace, which had been developing a two-stage orbital system similar to SpaceX's, and Rocketplane, which had been working on a suborbital rocketplane similar to Scaled Composites'. Its K-1 design, a traditional multi-stage rocket not that unlike the SpaceX design, is the work of a space veteran \u2014 George Mueller, who was head of NASA's Apollo programme in the 1960s and lead designer of the space shuttle in the 1970s. K-1 also uses veteran rocket motors, albeit in a new configuration \u2014 Russian NK-33 and NK-43 engines, developed in the 1960s for the Soviet Union's planned heavy-lift N-1 Moon rocket. The company's chairman and chief executive officer, George French, disputes reports of its financial woes, telling  Nature:  \u201cWe will continue to pour in our blood, sweat and money until the funding is secured.\u201d Many more companies are working on suborbital approaches. Although most are interested in rocketplanes, two are using an approach familiar to watchers of science-fiction movies but never before used for operational spacecraft: the same engine system for a vertical take-off and a vertical landing. \n               Straight up and down \n             Perhaps the most ambitious of these designs comes from Jeff Bezos, founder of Amazon.com and owner of a space company called Blue Origin, based in Kent, Washington. With a personal fortune estimated at more than $3 billion, Bezos has the resources to develop a serious vehicle, and his company's motto \u2014  gradatim ferociter , \u201cstep by step with spirit\u201d \u2014 suggests that he is in for the long haul. So does his choice of a vertical take-off, vertical landing design. Such an approach works best in vehicles with orbital and near-orbital performance, when the ability to slow down with rockets rather than with heat shields could be very handy. Some clues about Blue Origin's near-term plans can be gleaned from an environmental impact statement it had to file last year about its plans to launch from Bezos' ranch in west Texas. The company's first cone-shaped craft will be 15 metres high, automatically piloted and carry up to three passengers to heights of more than 100 kilometres. Passengers would experience about three minutes of weightlessness, and return to a landing pad just a few hundred metres from the take-off point. The company has been doing flight tests since November 2006. Operating on a much less lavish scale, out of a warehouse near Dallas, Texas, Armadillo Aerospace was founded in 2000 by videogame designer John Carmack. Last October, the company \u2014 staffed in part by volunteers \u2014 fell just short of winning a prize NASA set up to inspire designs the agency hopes to use on the Moon, where vertical take-offs and landings are the only possibilities. The lunar landing challenge promises $1 million to the first craft that can soar 50 metres high, hover for three minutes, land 100 metres away, and repeat the process in reverse within two and a half hours. Armadillo's small test rocket, named Pixel, has since met the requirements, but not yet under the scrutiny of judges. And, in the second 'new space' accident of the year, Armadillo's other vehicle, Texel, crashed and burned during a test flight on 18 August, taking it out of the competition and leaving its twin without a back-up. Meanwhile, another company, Benson Space Company in Poway, California, run by businessman Jim Benson, is looking at a hybrid approach \u2014 a rocketplane that takes off vertically but glides back horizontally. A survey by Incredible Adventures has shown that most potential passengers would prefer a horizontal landing of this sort, and Benson says that it will provide a gentler ride. Benson's rocket design is powered by a hybrid engine, which combines solid fuel with a liquid oxidizer. That gives it some of the stability of solid rockets, the motors of which can be stored fully fuelled for long periods, but with the controllability of liquid engines, which can be turned on and off during flight. The craft uses six hybrid motors, but needs only three to take off successfully. Parts of the hybrid engine technology developed by SpaceDev \u2014 another company founded by Benson \u2014 have already been used in SpaceShipOne, although just how much it contributed has been a bone of contention between Benson and Burt Rutan, the fabled aerospace engineer who created SpaceShipOne. \n               Under pressure \n             Another company building on previous success with rocket motors to plan a foothold in the spaceplane business is Xcor Aerospace of Mojave, California. Xcor already makes a small profit by selling rocket engines and low-altitude racing rocketplanes to other companies, and was included on  Inc Magazine's  list of the 500 fastest growing companies in the United States this August. It has plans for a simple spaceplane that would use a cluster of four such engines to take off from its runway and ascend to space. Xcor has avoided finalizing the details of the design or timetable for its Xerus passenger rocket, but it is expected to be lightweight and small, with room for just one passenger and a pilot. \u201cWe don't want to feel rushed to market by tying ourselves to particular deadlines,\u201d says the company's president, Jeff Greason. \u201cWe never anticipated that we'd be the first to market.\u201d The company is also planning for a suborbital research market, in which their passenger spaceship could be reconfigured as a space laboratory for short-duration scientific research \u2014 the kinds of projects in astronomy, environmental monitoring, materials research and so on that are currently done on sounding rockets, which are expensive. \u201cWe've already got a customer [the Southwest Research Institute in Boulder, Colorado] waiting in the wings that wants more flights than we had thought the entire market would be,\u201d says Greason. With these rivals at various stages of development, many are watching to see what is going on with Scaled Composites. The explosion in July was clearly a blow, although Rutan has said the company will forge ahead. Investigations into the accident are ongoing, and formal findings have yet to be released. The other major development in the company's fortunes is a buy-out by aerospace giant Northrop Grumman, which in July bought the 60% of the company that it did not already own. The price has not been disclosed, and the deal is still waiting for regulatory approval. But John Pike, the space and defence analyst who runs Globalsecurity.org, suggests that it might have had less to do with the space-tourism business than with the other projects Scaled Composites works on. The company has several patents and designs, including pilotless aircraft, that Northrop Grumman could benefit from. \u201cIt has basically bought a skunk works,\u201d says Pike, referring to the secretive, advanced-design companies pioneered by Lockheed Martin's famed 'skunk works' in California. \u201cThat kind of agility is hard to develop, so you buy it.\u201d It's even conceivable, he speculates, that Northrop could float off the personal spaceflight business as a separate company. \n               Growth spurt \n             Rick Tumlinson, cofounder of the Space Frontier Foundation in Nyack, New York, says that the acquisition could signal that traditional businesses are becoming more familiar and respectful of the fledgling space-tourism industry. But he worries that a full buy-out of Scaled Composites could dampen some of its entrepreneurial spirit. \u201cWhen a big aerospace company comes in and takes over one of these startups before the frontiers have been opened,\u201d he says, there's a risk that \u201cthe old-school mentality will suffocate [the new industry] even before it gets born.\u201d Tumlinson and other advocates of a new approach to spaceflight are also concerned about the other big player entering the field: Astrium, part of the European Aeronautic Defense and Space Company. Astrium announced in June that it hopes to raise \u20ac1 billion ($1.3 billion) to develop a rocketplane that would take off and land on a runway. It would use a single vehicle with two propulsion systems, conventional jet engines to take off and climb to the cruising altitude, and a rocket to soar into space. Charles Lurio, an aerospace engineer based in Boston, Massachusetts, and publisher of an insider newsletter on the new-space movement, says the announcement shows that \u201cthe field's become serious enough that even a company such as Astrium feels like it has to\u201d offer a competing vision. Astrium concedes that its plans depend on raising outside funding, hopefully from private loans and regional development funding, but says that it is serious and expects to go ahead. Jeremy Close, the company spokesman, says that its technology could also be developed into vehicles that could go beyond the space tourism market. \u201cThere's a possibility of point-to-point travel \u2014 for example, from Europe to Australia at significantly reduced travel times,\u201d he said. \n               The extra mile \n             The fact that supersonic passenger planes never got farther than the massively subsidized Concorde project of the 1960s suggests there might not be much of a civilian market for such things. But if Astrium really does manage to develop such a vehicle, it will probably catch the attention of the armed forces. The US military, for instance, plans to use scramjet engines, which use jetplane-like intakes to extract oxygen from the air to ignite their fuel, rather than carrying a supply of oxidizer as rockets do. Codenamed Blackswift, the planes being studied would be unmanned fighter-like craft that can reach any point on Earth within a few hours. Although the technological approach is different, Astrium's spaceplane, with its similar capabilities, might have military applications. With so many businesses and technological approaches to achieving similar goals for private spaceflight, many analysts have a positive outlook for the industry. There's a good chance that the robust competition between these players will help to hone the systems and drive down the costs. And Logsdon adds, \u201cI hope one or more of these folks will succeed. A little competition could be a good thing.\u201d See Editorial,  \n                     page 970 \n                   . \n                     Is this the future of space tourism? \n                   \n                     The inflatable space hotel \n                   \n                     Internet star shoots for a rocket revolution \n                   \n                     Making space for small businesses \n                   \n                     SpaceShipOne scoops X prize \n                   \n                     X Prize special \n                   \n                     Astrium \n                   \n                     Armadillo Aerospace \n                   \n                     Benson Space Company \n                   \n                     Blue Origin \n                   \n                     InterOrbital Systems \n                   \n                     Rocketplane-Kistler \n                   \n                     Scaled Composites \n                   \n                     Space Exploration Technologies \n                   \n                     Xcor Aerospace \n                   Reprints and Permissions"},
{"file_id": "449133a", "url": "https://www.nature.com/articles/449133a", "year": 2007, "authors": [{"name": "Navroz Patel"}], "parsed_as_year": "2006_or_before", "body": "Particle accelerators that use plasma technology promise to shake up the fields of high-energy particle physics and cancer treatment. Challenges remain, but smaller, cheaper machines are within reach. Navroz Patel reports. Beyond the theoretical and engineering challenges of building particle accelerators, sheer cost is a concern for physicists whose work involves accelerating and smashing subatomic particles together at great speed. Many particle physicists think that if the planned International Linear Collider \u2014 a US$7-billion electron\u2013positron collider that could begin operation within a decade \u2014 gets the go ahead, it may be the last large accelerator to be built for many decades as governments put a squeeze on funding. The cost of accelerators is a concern not just for those who crave bigger and bigger machines to probe ever higher energy scales. Some oncologists think that proton beams could offer superior results to conventional X-ray treatment of some tumours, yet they say the size and cost of the accelerators has limited the number of studies into their clinical effectiveness. \u201cIf we can reduce an accelerator's size, we can reduce the cost of proton therapy to something very small,\u201d says Charlie Ma, director of radiation physics at the Fox Chase Cancer Center in Philadelphia, Pennsylvania. Building a proton-treatment centre with conventional cyclotron or synchrotron accelerators costs between $100 million and $200 million, which explains why there are so few of these facilities (see  'Targeting tumours' ). But if accelerator research continues to progress at the rapid rate seen in recent years, the economics could be about to change for the better. A handful of groups are working on a new way to accelerate particles \u2014 known as wakefield acceleration \u2014 that should not only help push physicists towards the next energy frontier, but also provide affordable, table-top accelerators that could revolutionize cancer treatment. The technique involves passing either a laser beam or a beam of particles through a plasma. The beam scatters electrons, causing an uneven distribution of charge between the scattered particles and the plasma ions. To restore an even distribution, the electrons are pulled back towards the positive plasma ions that have congregated towards the rear of the beam pulse. But the electrons overshoot their original positions, creating a wake-like disturbance called a wakefield oscillation. Within this wake are pockets of plasma ions, which physicists refer to as bubbles, thanks to their spherical shape. The wake of a breaking wave causes turbulence, and the wake generated in a plasma is no exception. But as surfers and boat owners know, if you hit the wave at just the right spot, you can be accelerated by its surf. So some electrons can surf the plasma wakefield, as can other particles, such as protons, injected into the beam, accelerating them to very high energies. When particle beams are used to create the wake, it is often simply referred to as 'plasma wakefield acceleration', and the disturbance is created through electromagnetic repulsion between the beam and plasma electrons. For laser wakefield acceleration, the radiation pressure from the laser beam causes the wake formation. \n               Bubble effect \n             In the past three years, wakefield acceleration has generated its own bubble of excitement. Swapan Chattopadhyay, director of the Cockcroft Institute, a collaborative accelerator-research centre opened last year in Warrington, UK, says that a wakefield experiment at the Stanford Linear Accelerator Center (SLAC) in California this year has opened up a new chapter in accelerator physics. Using a 400-metre extension of the 3.2-kilometre main accelerator at SLAC \u2014 the longest linear accelerator in the world \u2014 researchers have managed to double the energy of the electron beam over a distance of just 85 centimetres 1 . Much of the beam loses energy in setting up the plasma wakefield, but a few (just 0.02%) of the electrons were accelerated from 42 gigaelectronvolts to around 85 gigaelectronvolts. Conventional technology would have to accelerate the electrons for around three kilometres to achieve this pick-up in energy. \u201cThis trick of sending the SLAC's electron beam through a plasma jet to double its energy without having to double the size of the facility is truly remarkable,\u201d says Chattopadhyay. One of the SLAC team, accelerator physicist Chandrashekhar Joshi based at the University of California, Los Angeles (UCLA), says that taking laser wakefield accelerator research to SLAC was the logical next step for the field. \u201cShort-pulse lasers are powerful, but beams typically contain energies of tens of joules,\u201d he explains. The energies of particle beams, on the other hand, are of the order of kilojoules. In other words, particle-beam technology can reach much higher energies than contemporary reliable laser technology. \n               Splitting ions \n             In theory, there is no limit to the energies that plasma wakefield accelerators could reach. In conventional accelerators, particles are accelerated by an electric field \u2014 the steeper the electric gradient, the greater the acceleration. But the field can only increase so far before the surrounding cavity material, such as copper or a superconducting material, starts to break down as electrons are stripped from its atoms. Because plasma, although electrically neutral overall, is already broken down into its atoms and electrons, it can support much stronger electric fields. The SLAC experiment was a breakthrough on several fronts. It showed that the technology can work at larger distances \u2014 reaching almost a metre, rather than the couple of centimetres previously achieved with laser technology. It also produced enough energy to be of interest in high-energy particle physics. But the energy of the accelerated electrons and the distance over which they continue to accelerate are not the only important properties of an accelerator. Other key factors also need to be addressed: the number of particles accelerated, or energy density, should be as high as possible, and the particles need to have a low energy spread, which means that they all have similar energies. With an energy spread of 100%, the SLAC experiment still has some way to go. Experiments with laser wakefield accelerators, although operating at lower energies and over shorter distances than plasma accelerators, are making progress with these key factors. In 2004, three groups used lasers to accelerate electrons so that they had similar energies and reasonable energy densities, exceeding 10 9  electrons per beam. These experiments reinvigorated interest in wakefield acceleration, which was first proposed 2  by physicists Toshiki Tajima and John Dawson at UCLA a quarter of a century earlier. But to do particle collision experiments, such as those at SLAC, the beams need to reach energy densities of 10 34  particles. The tiny fraction of electrons accelerated at SLAC is nowhere near enough for a collision experiment. Late last year, researchers took wakefield acceleration a step further. The 2004 experiments had accelerated electrons over the 0.1 gigaelectronvolt range, but a collaboration between researchers at the Lawrence Berkeley National Laboratory in California and a team led by the University of Oxford's Simon Hooker in Britain has now boosted electrons to more than 1 gigaelectronvolt 3 . \n               Small steps \n             This is not yet the high-energy frontier, which sits in the region of teraelectronvolts and beyond, but it is still a respectable gain on earlier experiments. \u201cOur next goal is to go up to 10 gigaelectronvolts, for which we will need a bigger laser \u2014 around one terawatt,\u201d says Wim Leemans, head of the group at Lawrence Berkeley National Laboratory. What's more, the researchers were able to create narrower particle beams with tight beam spreads \u2014 the energy spread divided by the peak energy. Tight spreads are essential in cancer treatment, as the energy determines how deeply the protons will deposit their maximum energy in the body. The researchers achieved a beam spread of less than 5%, compared with 10% in 2004 and 100% just a few years earlier. But there's still room for improvement. Karl Krushelnick, a wakefield accelerator physicist at the University of Michigan in Ann Arbor says: \u201cFor many processes that we would like to use these electron beams for, this figure needs to be well below 1%.\u201d Also last year, Victor Malka and his team at the Ecole Polytechnique in Palaiseau outside Paris developed a technique that uses a second counterposing laser beam to create an electron beam that can have its energy changed on the fly 4 . The second laser beam is used to control the injection of the electrons that surf the wakefield. The resulting accelerated electrons had an energy spread of less than 10%, and by changing the way that the two lasers overlap the researchers could tune the energy of the beam from 15 megaelectronvolts to 250 megaelectronvolts. Importantly, the beam was much less prone to fail than in previous experimental set-ups. \n               Particles to the people \n             \u201cWe now have a good understanding and much of the science worked out,\u201d says Malka. \u201cIn a sense, what we are left with is the technological work needed to improve and stabilize the machines to create a commercial product.\u201d The commercial application that Malka has in mind for his group's research is cancer treament. Since 2004, he has been collaborating with a group led by oncologist Uwe Oelfke at the German Cancer Research Center in Heidelberg to perform rigorous simulations comparing proton therapy with X-ray therapy for targeting tumours 5 . The team hopes to apply its results to patients within the next 5 years. If wakefield researchers make the advances they hope to over the coming years, then table-top accelerators could become much more powerful than they are now. Many experiments that are currently the preserve of relatively few, typically large and costly, facilities will be carried out in the basements of universities using compact and cheap technology. \u201cExperiments over the next few years could make or break our field,\u201d says Leemans. \u201cStill, I'm hopeful that we will be able to further address issues such as beam quality and that wakefield acceleration will really prosper.\u201d Even at the high-energy frontier, the next generation of very large accelerators will probably incorporate plasma. According to Krushelnick, plasma wakefields are the only affordable way to achieve the very large acceleration gradients needed to get to extremely high energies, perhaps even the terascale. Plasma techniques may initially be used to boost existing accelerator technology, as with the SLAC experiment, or in the staging of multiple modules to build a plasma wakefield accelerator from scratch. The SLAC team is already trying to work out how numerous small plasma accelerators can be combined to create a reliable machine. And Joshi says that he hopes that he and his team can address all the remaining critical scientific issues and propose an accelerator that is entirely based on plasma within 10 years. \n                     Laser physics: Extreme light \n                   \n                     An atom-smasher on your desk? \n                   \n                     Plasma physics: On the crest of a wake \n                   \n                     High-energy physics: Now that's cool! \n                   \n                     Accelerator physics: Electrons hang ten on laser wake \n                   \n                     Accelerator physics: In the wake of success \n                   Reprints and Permissions"},
{"file_id": "448984a", "url": "https://www.nature.com/articles/448984a", "year": 2007, "authors": [{"name": "Brendan Maher"}], "parsed_as_year": "2006_or_before", "body": "Physicists interested in the mechanics of single molecules are helping open one of the blackest boxes in biology. Brendan Maher discovers how the disciplines are working together. When trying to explain why he has become fascinated by physics lately, Kerry Bloom, a cell and molecular biologist at the University of North Carolina, Chapel Hill, pulls a handful of paper clips from his pocket and links them together. Stretching a small chain across the surface of his palm, he says: \u201cImagine this is DNA. You can stretch it to its full length, but each link in this chain is vibrating all the time.\u201d Bloom jiggles his hand, causing the paper clips to dance. The links twist at random, but once a couple of kinks or bends are introduced some force is needed to stretch the chain out again. After a few seconds of simulated brownian motion, the paper-clip chain collapses back into his hand. Bloom's paper clips are a demonstration of the properties of an 'entropic spring', a system where thermodynamics favours a resting state in which all the chain's components are bunched and tangled. Rubber bands and silk share these properties, and so does DNA. It is not a new insight, but to Bloom, who has spent most of his career speaking the language of genetics, it's a powerful one. He gets a wistful look when talking about it \u2014 he's even written poetry about it (\u201cFrancis and Jim lasted 50 long years/Isn't it time for some big new ideas...\u201d). Spring theory, as he calls it, might help explain a phenomenon he is deeply interested in \u2014 how the tiny biochemical machinery of the cell can manage billions of bits of information stored on vast polymer strings that need to be read, copied and packaged into an incredibly small space. In the cartoon models that illustrate textbooks on cell- and molecular biology, purposeful proteins orchestrate neat, stepwise molecular dances as they react to coloured blobs and bind a perfect DNA staircase. Everyone finds their partner easily and does their job efficiently in a scale-free rendition of an otherwise empty space. The reality is something much more chaotic. \n               Eye of the storm \n             In the cell there's no eye-soothing white space to separate things. Water molecules are a constant omnidirectional hailstorm, van der Waals forces glue things together and viscosity rules. Within this molecular maelstrom, gravity is imperceptible, and there's more or less no inertia; all purposeful movement degenerates into random jittering the moment no further power is available. Bloom has a dramatic illustration of the strangeness: if a bacterium stops beating its flagella to move forward, it comes to a stop in \u201cless than the width of a hydrogen bond\u201d, just a fraction of a nanometre. For those who find these complications fascinating, the tools of modern physics are making them ever more amenable to study. Theoreticians and experimentalists are devising predictive mathematical models for the mechanical properties of cells at a molecular level, and starting to expose the formulae under which these tiny chaotic environments function. DNA, operating at the centre of this maelstrom, is of particular interest. \u201cDNA is mechanically manhandled inside the cell,\u201d says John Marko, a condensed-matter physicist now working in Northwestern University's molecular-biology department in Evanston, Illinois \u2014 and that manhandling is important for replication, transcription, regulation, packaging and pretty much everything else DNA does or has done to it. Oliver Rando, a biologist at the University of Massachusetts Medical School in Worcester who studies DNA packaging, hopes that physics may answer questions other approaches haven't touched. \u201cYou have these machines that appear all over the nucleus but only happen to act at a couple of loci,\u201d he says, referring to specific places in the genome. \u201cIn some cases the detailed mechanism underlying that difference might be biophysical in nature.\u201d He's not certain that the physicists can solve the problem \u2014 but he's happy to see them try. To find out how DNA works in the strange world of the cell, the first step is to look at how it behaves in simpler places. Carlos Bustamante, a pioneer in single-molecule biophysics at the University of California, Berkeley, got started in the field simply by thinking about the most everyday lab procedure: gel electrophoresis. DNA fragments loaded into a gel and then subjected to an electric field will migrate along the field lines, and the speed at which the different fragments do so reveals their size. In the late 1980s, when he was at the University of New Mexico, Albuquerque, Bustamante began to wonder about the details of the process, and used a microscope to watch fluorescently labelled DNA fragments migrating through a gel 1 . \u201cWhat was amazing was how elastic they were,\u201d he says. As the electrical field pulled on the negatively charged strands, they folded and curled, crawling like caterpillars through the gel's molecular obstacle course of crosslinked polymers. \n               Let's twist again \n             Bustamante started devising new experiments to stretch or twist the DNA and see how much force was needed to make the familiar double-helix structure break, unwind, or knot up like an old telephone cord. Key to these investigations were new and constantly improving ways of seeing and manipulating the molecular structures \u2014 by attaching beads of polystyrene to the ends or sides of long DNA molecules he could hold them in a magnetic field, or trap them with laser light and watch as the DNA squirmed and recoiled in reaction to what was done to it. Because DNA is double-stranded and twisted, says Bustamante, it's quite rigid. But it also bends and folds \u2014 in fact it does so to an astonishing degree. The DNA in human cells is packed so tightly that two metres of it squeezes into every nucleus. The trade-off between rigidity and flexibility depends on scale: on small scales the molecule seems stiff, on larger ones bendy. Jonathan Widom, at Northwestern University, compares DNA to a garden hose; easy to wrap around your waist, impossible to wrap around your finger. The key to the difference between stiff and flexible is the chain's 'persistence length' \u2014 the distance that, as Widom puts it, \u201cdefines how far you need to go along a polymer before it forgets which way it was going\u201d. For a strand of DNA left to itself, studies have pinned the persistence length at about 50 nanometres, which corresponds to 150 bases; below this length DNA is difficult to bend. Results from dozens of studies fit fairly well a pre-existing 'worm-like chain' model of DNA, which predicts that it behaves somewhat like a chain of tiny paperclips. But key cellular processes, including packaging and genetic regulation, require looping on a scale much smaller than 50 nanometres. And some experiments by Widom and his collaborators showed tiny sequences spontaneously forming loops \u2014 'cyclizing' \u2014 at a much higher frequency than would be predicted by the worm-like chain 2 . These are the sorts of mismatch between theory and reality that excite Philip Nelson, a theoretical physicist at the University of Pennsylvania, Philadelphia. Nelson says he took notice of the work by people such as Bustamante and others in the mid-1990s because it put biological problems into a language he could understand. \u201cIf you knock out a gene and suddenly a rabbit doesn't like broccoli,\u201d he says, speaking as a physicist, \u201cthat's not helpful to us.\u201d DNA wrapping itself up in knots that the models seemed to preclude, though, was a problem he could get his teeth into. A group including Nelson and Widom recently approached the problem of tight looping using atomic-force microscopy, a powerful visualization technique that allows them to look at the shape of DNA strands directly, rather than looking at beads or fluorescence associated with them. They found that at lengths of between 5 and 10 nanometres (just 15 to 30 of the nucleotide subunits from which the double helix is built) the flexibility of the DNA was several orders of magnitude higher than that predicted by the worm-like-chain model. They proposed a new model called the sub-elastic chain 3 . Others, such as Marko and Jie Yan at the University of Illinois at Chicago had also been proposing models that allow for breakdown of the worm-like chain at short scales 4 . But it's still a contentious area in the field. \u201cYou have to say that this is very much in flux at the moment,\u201d says Widom, who notes that some of the assumptions in his cyclization experiments have come into question 5 . Widom has nonetheless found that he can make useful predictions of the bending and looping proclivities of a piece of DNA on the basis of its sequence. \u201cThat makes a link between bioinformatics and mechanics,\u201d he says. A DNA section with a specific nucleotide pattern might bend more or less than another strand, and some proteins seem to read this 'code' from DNA's bendability rather than directly from the sequence. Histones, the barrel-shaped packaging proteins around which DNA winds in tight curls are a prime example. According to Widom, the histones seem to prefer specific DNA sequences based on their flex. Rando, who works on histone dynamics, says this is where physics influences his work: \u201cThat's a case where something super-important to biologists is directed at least partially by something biophysics-y.\u201d And histones aren't the only proteins known to manage DNA looping. The lac repressor, a tiny V-shaped protein that grabs two specific sequences of DNA about 10 nucleotides apart and pulls them together, forms a very tight coil in the intervening material \u2014 too tight for any proteins that might want to unspool and read the DNA to cope with. Jeff Gelles, a self-described \u201cdyed in the wool  in vitro  biochemist\u201d at Brandeis University in Waltham, Massachusetts, helped to develop a physical way to visualize this looping. In 1991, he and his co-workers devised a simple way of looking at DNA mechanics visually 6 . They took a DNA strand that was being transcribed into RNA by a protein polymerase, fixed the polymerase to a glass slide and then attached a 40 nanometre gold bead to the free end of the DNA. The scale of the particle means that brownian motion has it dancing wildly \u2014 think balloon on a string in a hurricane \u2014 but the bead is big enough to see under a microscope. With time-lapse microscopy, the researchers could extrapolate the length and movement of the DNA from the bead's random positions around a central tether point. If these tethered particle experiments can reveal DNA length, they can reveal DNA looping, as the looping, by taking a hitch out of the tether, shortens its effective length. In unpublished work, Oi Kwan Wong, a former graduate student with Gelles, now at Stanford University in Palo Alto, California, used tethered particles to investigate how much the looping caused by a lac repressor shortened a sequence of DNA that was equipped with the relevant binding sites. The only problem is that she saw three different lengths: one stretched out, unlooped length and two looped lengths. \u201cWe think that the most likely explanation for that is not something to do with the structure of the DNA, but rather that the repressor itself can undergo a large change in the three-dimensional structure,\u201d says Gellis. \n               Internal workings \n             Rather than using microscopic gold beads, Bloom is trying to study the dynamics of DNA inside the cell itself \u2014 using the cell's own machinery to do the work. During cell division a protein-motor complex called a spindle separates identical copies of each chromosome, pulling one towards one end of the cell and the other in the opposite direction, allowing the eventual daughter cells to each get a complete set. The spindle latches on to the chromosomes at structures called centromeres, which have been the focus of Bloom's work for decades. Some years ago he engineered a cell chromosome with a second centromere that he could turn on and off. When the extra centromeres are activated during cell division, the spindle will sometimes latch on to two centromeres on the same copy of a chromosome, stretching it across the cell rather than separating it from its twin. Occasionally, the stretched chromosome snaps like a wishbone, with the DNA recoiling to one end of the dividing cell. Bloom originally used the method to study how the cell responds to DNA breakage 7 . But he is now starting to look at the dynamics of the break itself, using a laser to snip the stretched chromosomes and measuring the rate at which they recoil. \u201cThis is now where it gets complicated, and I'm not an expert,\u201d says Bloom. \u201cWe can see it stretch, we can see it recoil. How do I deduce force?\u201d Elements of the worm-like-chain model predict his observations fairly well, he says, and with additional genetic manipulation, he can begin to look at how histones disassemble and reassemble in the stretched and recoiling DNA. The moxie required to study DNA physics inside the cell itself elicits both admiration and scepticism. \u201cI would say that it is really important to understand how these models work inside the cell,\u201d says Bustamente. \u201cBut let's not forget that the best biophysics is always done outside the cell.\u201d Hermann Gaub, a professor at Ludwig-Maximilians University in Munich, Germany, says he thinks observing systems that include some of the DNA's biological setting, but that stay away from the messiness of the cell proper are likely to be the most fruitful: approaches such as that of Marko, who pokes, prods and pulls the peculiarly large chromosomes of newts. \u201cDoing it right inside the cell is what I would call heroic,\u201d says Gaub. But he's nonetheless intrigued. Bloom admits that the work is preliminary. He says he's been boning up on polymer theory and is probing for potential collaborators \u2014 but he wants the data to show them first. \u201cThis is the stuff physics brings to the table,\u201d he says. The packaging of all that DNA into the nucleus of cell, he continues, \u201cis another one of these big mysteries\u201d. A physical sense of how a hierarchy of folding patterns can pack DNA into the cell but also allow its sequences to be accessed remains far off. \u201cBy most estimations, it's packaged 10,000 fold,\u201d says Bloom, and no one knows how. \u201cThat's the attraction.\u201d It's far from the only problem that physicists have their eyes on. \u201cPeople are now able to do extremely quantitative and extremely reproducible and precise experiments on individual biomolecules,\u201d says Marko, \u201cand that's very attractive to physics people.\u201d And there are many biomolecules to choose from. To biologists, a text such as  Molecular Biology of the Cell  by Bruce Alberts and his colleagues is a well-trodden rendition of that which is known. But to a physicist first approaching biomolecules, it's an Aladdin's cave of shiny and captivating phenomena. Nelson says he's drawn to the sensation when something in biology \u201cmakes you ask 'How the heck does that happen?' Physicists start reading Alberts' and they say that every three pages.\u201d See Editorial,  \n                     page 969 \n                   . \n                     Genetic information: Codes and enigmas \n                   \n                     Molecular microscopy: Focus on the living \n                   \n                     DNA: Beyond the double helix \n                   \n                     How to rip apart molecules \n                   \n                     Genes get knotted \n                   \n                     Bloom lab \n                   \n                     Bustamante lab \n                   \n                     Widom lab \n                   \n                     Phil Nelson\u2019s website \n                   Reprints and Permissions"},
{"file_id": "449016a", "url": "https://www.nature.com/articles/449016a", "year": 2007, "authors": [{"name": "Jerry Guo"}], "parsed_as_year": "2006_or_before", "body": "Dubious science and looming legalization of the tiger trade threaten to derail China's efforts to save the Siberian tiger. Jerry Guo goes to the world's largest tiger-breeding facility to investigate. The automated gates chug and clatter open as a jeep, its windows ribbed with steel, noisily announces its arrival in the tiger park. Without the usual gaggle of tourists to impress, the occupants of a neighbouring jeep toss out a skinny pheasant as the driver shouts obscenities at a dozen lounging Siberian tigers. For sightseers they would have released a bull, but they cost US$250 each. One tiger finally takes notice and lunges at the fluttering fowl, which has enough brains to scuttle under one of the jeeps. The tiger, neither as sharp nor as small as the pheasant, slams into the vehicle with a thud. And as the hulking beast shakes off the dust and disappointment of his failed attempt, the pheasant dashes into the brush. The striped leviathan promptly settles back down, seemingly deciding that the prey isn't worth the effort. And why not, for these tigers are already well-fed, particularly by the 300,000 tourists who flock every year to the tiger park at the Hengdaohezi Feline Breeding Centre on the outskirts of Harbin in northeastern China's Heilongjiang province. By most accounts, the place is an enviable success. Started in 1986 with 8 Siberian tigers, it is now home to 800 of the big cats. Compare that with the estimated 150 Siberian tigers in US zoos. The largest tiger-breeding facility in the world, Hengdaohezi \u2014 like its cousin down south at the Wolong Panda Reserve \u2014 has learned the art of churning out cubs, 100 this year alone. But this year the centre has been subject to all sorts of media attention, from gruesome videos on the Internet of tigers eviscerating a bull as tourists gape, to reports of plans to reintroduce 600 of the cattle-fed, people-friendly tigers into the wild. At the Convention on International Trade in Endangered Species of Wild Fauna and Flora (CITES) meeting this June in The Hague, the Netherlands, China's tiger-breeding programme was criticized for at best creating tourist traps, and at worst being flat-out farms for the animals. Indeed the Hengdaohezi facility was developed as a government-run enterprise to capitalize on the tiger-bone trade. Since the practice was outlawed in 1993, the park has depended on tourism for 80% of its $4 million per year operating costs. But shortly after the CITES meeting, Wang Wei, a deputy director at the State Forestry Administration in Beijing threatened the imminent reopening of the tiger trade, inviting 70 international tiger experts to Hengdaohezi in July to hear the merits of such a move (see ' Another pickle for Siberian tigers '). So far, Western scientists are unconvinced that the proceeds from farming the animals might fund conservation efforts. Moreover they doubt whether conservation is something the facility is interested in or even equipped to do. \u201cThey want to use their bones and parts,\u201d argues Sybille Klenzendorf, who toured the facility in 2006 as director of species conservation for the conservation group WWF. \u201cIt's basically a tiger farm operating under the pretence of a research facility.\u201d In a concrete bunker off-limits to gawking tourists, mother tigers nurse their cubs in tiny cages. The park's chief scientist, Liu Dan, proudly surveys his charges. For him, the objective is straightforward. \u201cOur goal is to reintroduce them into the wild,\u201d he says. He denies media reports of an earlier failed reintroduction. The centre did, however, send ten tigers to a small area resembling alpine forest in the Changbaishan reserve, close to the North Korean border. \u201cIt's a very good wild habitat. A good exercise in all aspects of training, but still a big difference to the wild,\u201d says Liu Dan. \n               Genetic sleuths \n             The park contains roughly twice the number of Siberian tigers that exist in the wild, and letting loose even a few captives would have widespread conservation implications \u2014 especially in the small remaining natural range in northeastern China where perhaps ten tigers reside. But reintroduction wouldn't be just about bolstering the wild population. In 2004, Michael Russello and his colleagues at Yale University in New Haven, Connecticut, published a study in  Conservation Genetics  indicating that all but 2 of the roughly 60 wild Siberian tigers they sampled shared a single mitochondrial haplotype 1  \u2014 a set of genes that is inherited en masse. \u201cThe genetic diversity was about as low as it gets,\u201d Russello says. In particular, his data suggested that captive tigers, at least those in North America, may be more genetically diverse than their wild counterparts. But he doesn't know if this corollary holds true for Hengdaohezi's 800 tigers. If so, \u201cthey could re-inject variation that has been lost in the wild\u201d. At Wolong Panda Reserve, keepers are increasing the population to maintain a healthy genetic reservoir in case of a sharp drop or extinction in the wild. Three hundred pandas is apparently the magic number, and tourists are no less impressed. There, as in Hengdaohezi, even keeping the animals caged can benefit conservation, as long as pedigrees are tracked and specific pairs matched to maximize diversity, says Shujin Luo, a conservation biologist at the National Cancer Institute in Frederick, Maryland. That's not the case at Hengdaohezi. Although Luo is currently analysing the genetic diversity of wild versus captive Siberian tigers, like Russello and other Western researchers, she has not been able to obtain any samples from Hengdaohezi. One person who has obtained samples says he is confident in the diversity of the Hengdaohezi stock. Across town from the facility, forensic geneticist Xu Yanchun of Northeast Forestry University in Harbin, has collected 500 blood samples from the place and plans to have a completed 'genebank' of the captive population by this winter. \u201cI'm sure genetic diversity here is higher than in the wild,\u201d he argues, citing indicators such as heterozygosity and allelic distribution from his unpublished data. \u201cThis population is quite high in genetic diversity because they are well managed,\u201d Xu says. \n               Fuzzy breeding \n             The tourists love the liger enclosure \u2014 they can't snap enough pictures as the tour bus slowly rolls past lions, tigers and their enormous hybrid offspring, all basking next to each other. The huge animals, a cross between a male lion and a female tiger, are a dramatic sight, but such disregard for intermixing could lead to bigger problems. The property also contains Bengal tigers, technically a different subspecies from their Siberian cohabitants, and the subspecies could produce harder-to-spot hybrids together. In general, Hengdaohezi's breeding strategy is crude compared with Western practices. Unlike US and European zoos that use computer models to calculate exactly which animals should mate together \u2014 and stud books to track every individual at Hengdaohezi, Xu's genetic pedigrees are mostly ignored. Liu Dan concedes that the centre doesn't control or record which tigers breed together (as long as they're not brother and sister). \u201cWe don't have the resources,\u201d he says. \u201cNow they're all breeding haphazardly,\u201d says Liu Yutang, a cryogeneticist at Northeast Forestry University. \u201cWe have to wait until our technology is better so that we can control which tigers will mate.\u201d In captivity each female may mate with several male tigers when sexually receptive, confusing keepers on the paternity of the resulting cubs. The centre also rarely exchanges tigers with other breeding facilities. US zoos regularly shuttle tigers across the country to breed, explains Kathy Traylor-Holzer, the Siberian tiger stud-book keeper for American zoos. \u201cThe fear is to accept an animal from Harbin as they may carry genes from other subspecies,\u201d she says. \u201cIf you don't manage the population, you automatically lose genetic diversity.\u201d The breeding facility is not a member of the international stud book for Siberian tigers, which includes almost all US and European zoos. \u201cIt's because they breed the same animals over and over again, which you would never do as a registered stud-book zoo,\u201d says Klenzendorf. Xu, who serves as a breeding consultant to Hengdaohezi, refuses to use the international stud book's standard software program, SPARKS, which calculates the degree of inbreeding for each individual. \u201cThe prediction is not accurate,\u201d he says, citing more unpublished data that indicate that sperm genetically similar to the female's genotype stand less of a chance in the oviduct, a case of 'selective fertilization'. \u201cThis model has the precondition that all alleles are passed on randomly, which is not accurate because of my selectivity theory,\u201d says Xu. But Xu's claims about the population's high genetic diversity draw doubts even from colleagues such as Liu Yutang who says physiological problems from inbreeding already run rampant. \u201cSome tigers are very weak, have different stripes, high cub mortality rates, and reduced immune systems,\u201d he says. \u201cAnd when they're all related, then it's easier for something to wipe them all out,\u201d he adds, citing the waves of deaths from bird flu and canine parvovirus. Liu Dan says he's not aware of these tiger deaths, but Xu says they numbered in the \u201cseveral tens\u201d in 2005. Luo says she reviewed one of Xu's papers on the genetic fitness of Hengdaohezi tigers and didn't consider it good enough for publication. \u201cEvery few months we receive papers from China about tigers; overall the quality is disappointing,\u201d she says. So far, Xu's data from the past decade at Hengdaohezi have not been published in a Western journal, although he did publish a paper in  Forensic Science International  detailing a genetic fingerprinting method to combat Siberian tiger poaching 2 . \n               Siberian sperm bank \n             In his university office, Liu Yutang grabs a giant syringe equipped with a video camera and a light. To help bring the centre to Western standards by controlling exactly which tigers mate, he has developed this gadget for artificially inseminating the females. He tried it earlier this year with no luck. Still it was an improvement on the prototype, which, he says with a grimace, was \u201ctoo sharp\u201d. Liu Yutang says he is attempting to assemble a Siberian tiger sperm bank with samples from the entire Hengdaohezi population for artificial insemination. He's even eyeing wild tigers as possible targets for collection, but there's still a lot to learn. He admits his team doesn't yet know all of the basics, such as when tigers produce sperm. \u201cA lot of this work is through trial and error,\u201d he says. For example, 8 of 23 semen samples he has extracted so far from the captive tigers yielded no sperm. Liu Dan's plans to reintroduce tigers into the wild have faced further criticism. \u201cThey'll wreak havoc in the villages after being fed chickens and getting used to jumping on cars,\u201d says John Goodrich, a conservationist with the Wildlife Conservation Society in New York. \u201cAnd there's absolutely no need to release tigers at all when tigers from Russia will move into China.\u201d For now, the Siberian tiger's foothold seems sturdier than that of its cousin, the South China tiger. As a result of poor breeding and poaching, the South China population now numbers 66, all caged in a handful of zoos. Xu says the existing population is extremely inbred, with high mortality and low fertility. A paper under review at  Current Biology  paints an even bleaker picture. Yue Bisong of Sichuan University in Chengdu, notes that of 45 tigers he sampled, only 13 were pure South China tigers, the rest were hybrids with other tiger subspecies. With its baffling breeding techniques and plans to open a market in tiger parts, Hengdaohezi hardly seems the safest place for Siberian tigers, but how they would fare in the wild is even more uncertain. So perhaps it is fortunate that the reintroduction campaign is mainly hype for now. Although media reports mention plans to release 600 of the captive tigers (apparently hoping to coincide it with the Beijing Olympics), the centre has not yet separated any group for eventual reintroduction, selected any potential release sites, or built specialist training enclosures. As Liu Dan broods over his nursing mothers, he defends the conservation work of the centre, posing the rhetorical question that if they weren't keeping the tigers around for a greater purpose, wouldn't they be just another tiger farm? \u201cFrom breeding to reintroduction is a long process,\u201d Liu Dan says. \u201cThe programme isn't mature yet.\u201d See Editorial,  \n                     page 2 \n                   . \n                     Traditional medicine: A culture in the balance \n                   \n                     Conservation biology: The tiger's retreat \n                   \n                     Siberian tigers fight for freedom \n                   \n                     Doing conservation by numbers \n                   \n                     Snapshot: A night out in the park \n                   \n                     WWF - Amur (Siberian) tiger \n                   \n                     World Conservation Society: Siberian Tiger Project  \n                   \n                     AMUR: preserving leopards and tigers in the wild  \n                   Reprints and Permissions"},
{"file_id": "449278a", "url": "https://www.nature.com/articles/449278a", "year": 2007, "authors": [{"name": "David Cyranoski"}], "parsed_as_year": "2006_or_before", "body": "The world's biggest, best-equipped research drilling vessel is about to set off on its first scientific voyage. David Cyranoski previews its quest to catch a formidable earthquake in the act. When it comes to natural disasters, the Japanese government is good with numbers. It expects, for instance, a magnitude-8.1 quake to strike in the next 30 years with an epicentre in the Nankai trough \u2014 a depression in the sea-floor 100 kilometres off the country's east coast. And when it hits, it is likely to kill between 12,000 and 18,000 people. The Nankai trough lies in a subduction zone, a perilous region in which one tectonic plate dives under another, building up the sort of rock strain that can unleash the world's most powerful earthquakes. All earthquakes with a magnitude of greater than 9 have occurred in these zones. And although the next earthquake at Nankai is not expected to be quite this big, the region could prove key in understanding why earthquakes in subduction zones release such vast amounts of energy. On 21 September, a brand new research ship is due to depart from the city of Shingu in Japan on the first leg of a five-year project. The iniative, called the Nankai Trough Seismogenic Zone Experiment (NanTroSEIZE), is the latest and most ambitious of a series of deep-drilling research projects that stretch back decades (see  'Staying afloat' ). Everything about the ship, named  Chikyu  for 'Earth', is large: its 210-metre length, its 10 kilometres of drill string and its \u00a560-billion (US$526-million) price tag.  Chikyu  is the first research ship to use a massive pipe known as a 'riser' to encompass the main drill pipe \u2014 making the rig more stable and enabling it to drill more than three times deeper than any other scientific drill ship 1 . But once  Chikyu  gets down to its ultimate goal \u2014 an earthquake-generating zone some six kilometres below the sea-floor \u2014 its work will become very small-scale. Scientists onboard the vessel will be looking at minute changes in the porosity and other characteristics of the rocks drilled from the depths. Eventually, they will place instruments in a deep borehole that will gather data over several years. The goal is to monitor the build up of strain in the rocks \u2014 to see an earthquake in the making. \n               Down under \n             Previous attempts have been made to monitor earthquake zones \u2014 for instance, at the Parkfield site in California atop the infamous San Andreas fault \u2014 but  Chikyu  will be the first to take such a precise look in a subduction zone. \u201cIt will be the first chance to see how such an earthquake is being prepared,\u201d says Asahiko Taira, director-general of the Center for Deep Earth Exploration in Yokohama, Kanagawa, which manages  Chikyu  and its attempts to unearth the very origins of earthquakes. \u201cHaving an exposed system to observe is like being able to examine a live squid rather than a dried one to understand its biology,\u201d he says. Japan, which footed the bill for building  Chikyu , has a good reason to focus on the Nankai trough. Here, the Philippine tectonic plate dives beneath the Eurasian plate, on which Japan sits, at a rate of about four centimetres per year. But at some points along the boundary, the plates 'stick' together and pressure builds. Two of these sticky patches, both roughly 100 kilometres wide, were responsible for earthquakes in 1944 and 1946 (ref.  2 ) that each killed around 1,300 people. And it is these patches that are thought to be where pressure is building for the next big quake. It's a good bet. For the past 1,300 years the Nankai trough has unleashed a large earthquake, of magnitude 8 or greater, every 90 to 210 years. This regularity offers scientists an opportunity for a before-and-after look at an earthquake in a subduction zone. \u201cThere's no place in the world like it,\u201d says Taira. Going deep is the best way to study the trough. Using the riser system,  Chikyu  scientists intend to dig the 'ultimate' borehole. (The record for the deepest scientific hole in the ocean is held by the  JOIDES Resolution  vessel, which drilled to a depth of 2,111 metres in 1993.)  Chikyu  will also drill at least 5 other boreholes along a 70-kilometre line, spanning a range of depths above the plate boundary (see graphic above). \u201cWe can look at temperature, pressure, material composition, as well as the degree of dehydration, and see how these properties change from the shallow part of the plate boundary into the deep,\u201d says Masataka Kinoshita, a researcher at the Japan Agency for Marine-Earth Science and Technology (JAMSTEC) in Yokosuka and a chief project scientist for NanTroSEIZE. But those comparisons will have to wait until the project is completed, which won't be before 2012. The leg starting on 21 September is an 8-week-long rapid survey of the six planned borehole sites. The 16 scientists on board will use sensors attached above the drill bit to pick up signals such as \u03b3-radiation, electric currents, and sound waves transmitted from the drill to obtain information about the porosity and density of the surrounding rock. \u201cWe can record information about the rock types before we've disturbed them,\u201d says Harold Tobin of the University of Wisconsin in Madison, the project's other chief scientist. \u201cThis is about as close to pristine conditions as you can get.\u201d To keep the project moving at a fast pace, no cores will be taken. That will be the task of the next leg \u2014 a 4\u20135 week mission scheduled for late November. One of the cores will go down to 1,000 metres, and they will all be analysed with  Chikyu 's plush scientific facilities, which include a computed-tomography (CT) scanner that can reveal the internal structure of the core without destroying it. In late December, a third leg will involve penetrating to a depth of 1,000 metres at two other sites. After that,  Chikyu  will be temporarily side-lined, partly because of an agreement with Japan's fisheries, partly to save money and partly to do maintenance work. Drilling is slated to resume again in October 2008, when the massive riser system will come into play. A riser, common on oil-drilling vessels but rare for scientific missions, surrounds the drill pipe all the way from the ship down to the sea-floor and below. Heavy mud is circulated at high pressure between the riser and the drill pipe to stabilize the rocks and stop them from collapsing. The ship will recover cores in continuous nine-metre stretches, which will provide valuable information about the geological history of the region. The layers within the cores \u201care like tree rings\u201d, says Tobin. \u201cYou don't want to miss any dates.\u201d But coring is a slow process \u2014 every nine-metre section must be hoisted up into the ship before drilling can continue. Raising a core can take as little as 15 minutes, to more than an hour, depending on the depth of the water at the drill site. Because of the premium on ship time, the crew will work around the clock. A helicopter will make runs every two weeks from the shore to exchange scientists and drilling crews. In the end, Tobin says, it will take six or more legs of concentrated eight-week drill stints to reach all the way down to six kilometres. Half of the cores will be analysed on board; the other half will be stored in a facility at Kochi University, on the island of Shikoku, for permanent archiving. Onboard researchers get first dibs at studying them, but a year after being extracted anyone can apply to study them. \n               Detectors in the depths \n             But many scientists are more excited about the possibilities once the drilling has finished. Project scientists plan to place long-term observatories down some of the boreholes to measure rock tilt, seismic activity, strain, pore pressure and temperature \u2014 key variables for understanding how the rocks behave. The sensors must be designed to withstand very high temperatures, and will cost around \u00a51.5 billion over the next five years to develop. It's not yet clear whether they will be ready before  Chikyu  finishes its drilling. The researchers hope to operate the sensors for at least five years after they have been installed, perhaps uploading their data to remotely operated submersibles or sending them back to shore via cables on the bottom of the ocean. The observatories will measure changes that are surprisingly small given what can be felt on the ground during earthquakes. But \u201cthese measurements will be the key to getting a quantitative description of how the earthquake is building its energy,\u201d says Kinoshita. Over the long term, Taira adds, the observatories might even be able to provide a way to identify the very start of earthquakes, and perhaps even to warn areas that have not yet been hit. The measurements should also shed light on some important research questions. What happens to the rocks, and the water they carry, in the subducting plate? How does the strain released during an earthquake propagate to the surface? Under what conditions do earthquakes trigger tsunamis? Previous drilling has helped to answer other questions about the behaviour of plate boundaries. At the Parkfield site, for instance, scientists have drilled three kilometres into the San Andreas fault and found that rocks there contain talc, which could explain the ease with which the plates slide along each other at those points 3 . At Nankai, excitement has grown in the past five years, after seismologists there discovered earthquakes that generate very-low-frequency waves. Before that, it was generally thought that a subduction system had 'creeping' regions that slid past other, and 'locked' zones in which pressure builds up. But using broad-band seismometers, Japanese scientists found seismic events at lower frequencies than had previously been detected, and in places thought to be devoid of seismic activity. Taira speculates that these low-frequency earthquakes, which are typically of magnitude 3 or 4, relieve strain over the long term. \u201cIt is clear they have something to do with the earthquake cycle,\u201d he says. The new-found earthquakes could be partly caused by water carried by the subducting tectonic plate, says Kazushige Obara, at the National Research Institute for Earth Science and Disaster Prevention in Tsukuba. The water creates a clay-like formation that \u201cacts like a cushion\u201d to slow the action of the earthquakes, he says. When Obara and his colleague Yoshihiro Ito discovered these low-frequency earthquakes in the shallower region of  Chikyu 's drilling area, it gave the mission a whole new target to study 4 . In 2001, when  Chikyu's  drilling was being planned, \u201cno one had heard of these earthquakes\u201d, says Greg Moore, also of the JAMSTEC. \u201cWe now know there is a lot of seismic activity.\u201d By 2012,  Chikyu  may have spent as much time as it needs to study these and other details of the Nankai trough. After that, project managers expect that the ship will be in high demand for other missions. Two vying to be next are a palaeoclimate study in the Indian Ocean, and a seismogenic study in the Middle America trench off Costa Rica. Both require such deep drilling that only  Chikyu  can do it. And eventually,  Chikyu  could achieve one of scientific ocean-drilling's greatest dreams. In the 1960s, scientists envisioned an ocean drilling project that could pierce Earth's mantle. The project, called 'Mohole', never got deeper than 200 metres beneath the sea-floor \u2014 let alone to 10 kilometres, where the crust borders the mantle. But one day  Chikyu  might try its own version of Mohole. To do so, it would need a costly extension of the riser from its current 2.5 kilometres to 4 kilometres so that it could operate in the deep waters where the mantle is closest. But it would be worth it, says Kinoshita. \u201cThis is a long-held dream of all mankind, or at least of all Earth scientists.\u201d See Editorial,  \n                     page 260 \n                   . \n                     https://doi.org/10.1038/442964b \n                   \n                     https://doi.org/10.1038/426492a \n                   \n                     NanTroSEIZE \n                   \n                     Chikyu Hakken/Center \n                   \n                     Integrated Ocean Drilling Program \n                   \n                     Japan seismic hazard information \n                   \n                     San Andreas Fault Observatory at Depth \n                   Reprints and Permissions"},
{"file_id": "449136a", "url": "https://www.nature.com/articles/449136a", "year": 2007, "authors": [{"name": "John Whitfield"}], "parsed_as_year": "2006_or_before", "body": "Above ground, plants compete for life-giving sunlight, but below the surface a more complex picture emerges. John Whitfield explores the role of mycorrhizae in plant ecology. Under the deep shade of northern forests in North America and Europe, a tiny plant ekes out an existence.  Orthilia secunda , the serrated wintergreen, is found huddled in the understorey of pines and birches, sending up a drab bundle of yellow-green flowers. Beneath ground, however, this meek plant hides a secret. Starved of sunlight,  Orthilia  has found another source of nourishment. Its roots tap into a network of soil fungi, taking up to half the carbohydrates it needs from organisms that normally form mutually beneficial relations with plants but giving nothing back. \u201cYou could say that the plant is eating the fungus,\u201d says Marc-Andr\u00e9 Selosse of the Centre of Functional Ecology and Evolution in Montpelier, France, one of the team that detected  Orthilia 's thieving 1 . But  Orthilia  is also, indirectly, eating other plants \u2014 probably the very trees that tower over it. After all, it is their photosynthetic efforts that the fungus is feeding on. Indeed,  Orthilia 's freeloading may reveal the actions of an invisible hand in ecology; that is, the fungal network that underlies the forest and participates in a cooperative give-and-take with hundreds of plants. Emerging clues suggest that this covert subterranean interplay influences many aspects of the forest community, including which plants live, which die, the effects of physical stresses such as heat and drought, and what happens after the introduction of new species. Add the controversial possibility that fungi mediate resource sharing between different plant species and a picture emerges of a Robin Hood of the soil, subsidizing those less able to make food, and by so doing, helping its own cause by promoting a diverse range of plant partners. Just how interactive this fungal support system is remains unclear. The key processes happening underground are carried out by microscopic players and are hard to track. Moreover, no one knows how to measure fungal fitness. Is it reflected in the number of mushrooms \u2014 spore-bearing, fruiting bodies \u2014 above ground, the size of the vast network below, or something else entirely? Even the process by which the plants and fungi exchange nutrients is unknown. \u201cWe know so little that it's possible to propose some very naive hypotheses,\u201d says Martin Bidartondo of the Royal Botanical Gardens in Kew, London. Researchers are now looking to fill some of the huge gaps in their knowledge of the basic biology and natural history of fungal networks and to improve understanding of the their ecological consequences. Most can agree that fungal networks are real, and important to the lives of plants, but they disagree on how the effects manifest themselves. \n               Give or take? \n             About 80% of land plants have fungi called mycorrhizae growing in and on their roots. The fungi extend throughout the soil matrix; it has been estimated that a single gram of soil can contain 100 metres of mycorrhizal filaments. By vastly expanding root surface area, the fungi help plants extract water and nutrients, most importantly nitrogen and phosphorus, from the soil; they also protect plants against soil pathogens. In return, the plant provides carbohydrates. As much as 20\u201340% of the products of photosynthesis can flow back into the fungus. Most plants and fungi are promiscuous in their associations, mingling with a range of partner species, and creating the potential for one fungus to link the roots of dozens of different plants. DNA fingerprinting has shown such multiple links with matsutake fungus 2 . But more than 400 species of plant have reneged on this contract. They do not photosynthesize at all and have no chlorophyll. Initially, scientists thought that they were decomposers; in fact they parasitize mycorrhizal fungi. This way of life has evolved many times in different plant groups, but it is particularly common in the orchid, heather and gentian families 3 . The more recent discovery that green plants such as  Orthilia  can also use fungal carbon suggests that the ability may be more widespread and ecologically significant than was once thought. \u201cIt was a dogma in botany that green plants were autotrophic, but this is no longer valid,\u201d says Gerhard Gebauer of the University of Bayreuth, Germany. A team including Gebauer and Bidartondo recently found five green orchid species that can use mycorrhizal carbon to thrive 4 . \u201cThis allows the orchids to move into the deepest shade in the forest,\u201d says Gebauer. \u201cThey can live as pioneers without any herbaceous competitors.\u201d Bidartondo suspects that all orchids, which have very small, poorly provisioned seeds, take from fungi before they become self-sufficient. This start-up funding could have a huge ecological impact, he says: \u201cIf flow from fungi to plants happens when plants are getting established, it can really affect competition, even though the amounts moving might not be massive.\u201d \n               Wood-wide web \n             That some plants can take advantage of the fungal network is not in dispute \u2014 although it is not known how they do it. More contentious is whether the flow of nutrients between plants via fungi is a general and significant feature of the 'wood-wide web' of forest ecosystems. Suzanne Simard of the University of British Columbia in Vancouver, Canada, says fungal networks may allow trees to support their own seedlings, perhaps providing the trees with an evolutionary benefit. \u201cThere's lots of evidence that mature trees facilitate the growth of conspecific seedlings beneath them, and the evidence is growing that networks are important,\u201d she says. Experiments with oaks, for example, show that acorns planted near their own kind do better than those planted near maples 5 , and Simard has found something similar with fir trees. Like Bidartondo, she thinks that these boosts to seedlings are mediated by fungi and are ecologically significant. \u201cEarly growth sets the trajectory of the ecosystem,\u201d she says. Such effects can be seen even between different species. In a study 6  published in  Nature  in 1997, Simard's team provided the leaves of paper birch ( Betula papyrifera ) and Douglas fir ( Pseudotsuga menziesii ) seedlings growing half a metre apart with carbon dioxide containing either carbon-14 or carbon-13 \u2014rather than the usual carbon-12. They could thus detect the element moving in either direction between birch and fir. Move it did; what's more, trees growing in shade received more from the other plants in their network. After 9 days, an average of about 4% of the carbon isotopes given to each plant had shown up in the leaves of the other species, and the amount of carbon flowing from birch to pine doubled when the pine seedlings were shaded. Although laboratory studies had shown carbon moving between plants via fungi, Simard's study was a watershed in showing that the phenomenon happened in the field. The work demonstrated that carbon could flow both ways, that a significant amount of carbon moved, and that the quantity depended on environmental conditions. Since the work was published, other studies have shown that the timing of plant growth, as well as the light environment, affects the dynamics of transfer, with sugar maple seedlings gaining resources from a fast-growing perennial, the trout lily, in spring, and returning the favour in the autumn 7 . It has also been suggested that one thing that makes spotted knapweed a pernicious invasive weed in the United States is its ability to steal resources, via fungi, from other plants \u2014 one study found that as much as 15% of the carbon in the knapweed's shoot came from Idaho fescue, a native grass 8 . Simard is currently working on the interactions between fungi and Douglas fir at the edge of the tree's range, in the dry regions where forest shades into grassland. In such places, water can also move between trees via fungi, she has found. \u201cMycorrhizae are more important in more stressful climates,\u201d she says. By helping plants cope with stress, and by helping seedlings survive, she thinks that fungal networks make plant communities more stable in the face of environmental stress, and quicker to recover from damage. By distributing resources between different species, says Simard, fungi can preserve a variety of plant partners and insure against the effects of plant disease or herbivores. \u201cIf the fungus can form a bigger or more diverse network, its chances of survival are better,\u201d she says. Selosse says he thinks that fungi might help young plants to get established because it helps them compete with other fungi in the soil \u2014 nourishing an existing partnership might be a more effective strategy than seeking out new hosts. Alternatively, he suggests, it could be that some plants provide trace amounts of vitamins or even hormones in return for fungal carbon. \n               Up-rooting claims \n             But not everyone is convinced. \u201cThere's been some wishful thinking, and the evidence hasn't been looked at critically,\u201d says David Robinson of the University of Aberdeen, UK. \u201cI don't think there's any convincing evidence for resource sharing between plants by mycorrhizal transfer.\u201d Robinson, working with Alistair Fitter of the University of York, has suggested that the carbon might have moved through the soil, rather than the fungi \u2014 in Simard's experiments, a small amount of radioactive carbon also showed up in a plant species that did not share mycorrhizae with the fir and birch. And experiments by Robinson, Fitter and other groups have found that, although elements do move between plants via fungi, they stay in the root system, and never make it into leaf and stem, suggesting that the resources are stored in the fungal tissue, and not released to the plant 9 . \u201cThe moving carbon is primarily a fungal resource,\u201d says Robinson. Fitter adds that, from a darwinian viewpoint, it is \u201chighly implausible\u201d that a plant would benefit from helping its neighbours. \u201cThere's no doubt that carbon moves through the soil,\u201d says Simard. \u201cI think it goes through both mycorrhizae and soil.\u201d But she believes that evidence for transfer between plants has strengthened over the past decade \u2014 her group has recently redone the experiments with birch and fir, for example, and found a similar result. Experiments such as Robinson and Fitter's were conducted using grassland plants \u2014 a \u201cquite different system\u201d from woodlands, she says. Simard adds that, rather than simply shunting carbohydrates from one plant to another, the fungus might first use the carbon to make amino acids \u2014 nitrogen-containing compounds that form part of the usual carbohydrates-for-nitrogen exchange between plant and fungus. This avoids the problem of explaining why a fungus would want to give back its hard-won nourishment. Even if resources do not flow from plant to plant, the mycorrhizal network has other ways to influence plant interactions. \u201cYou don't need direct resource translocation to have benefit or disadvantage moving between plants via a fungal network,\u201d says Minna-Maarit Kyt\u00f6viita of Oulu University in Finland. She has found that some seedlings do worse when hooked into the mycorrhizal network. In greenhouse experiments using herbaceous plants, seedlings do best with mycorrhizae. But when adult plants are present, seedlings do no better with mycorrhizae than without them. The fungi seem to be making the competition more intense \u2014 Kyt\u00f6viita says they might be supplying more to the adult plant that gives more in return, and withhold favours from the seedling. She has also found that seedlings do better when their adult competitors are defoliated, and so less able to supply their fungi 10 . It's in this spectrum of positive and negative interactions between plant and fungus that we should be seeking the influence of mycorrhizal networks, says John Klironomos of the University of Guelph, Canada. His experiments testing different combinations of plants and fungi have found that outcomes can range from exploitation, to mutualism, to neutrality 11 . A fungus might nourish one plant it links to, exploit another and be cheated by a third. It's the diversity of possible interactions between one fungus and the many plants in its network, not transfer between plants, that is ecologically significant, says Klironomos. \u201cI'm convinced that mycorrhizal networks exist, but I'm not sure the mechanism of action is carbon transfer. What's more exciting is the other resources that the fungus is transferring to different plants, and the different amounts of carbon the fungus demands from plants. When you put all that into the equation you get some interesting dynamics.\u201d \n               Filling in gaps \n             What is clear is that researchers have their work cut out for some time to come. Studies so far have tended to look at two plant species linked by one fungus \u2014 a gross simplification of real-world diversity. Such studies have been snapshots, but fungi and trees can live for centuries. And biologists don't know how the links between plants and fungi affect the survival and reproduction of each party. For fungi, says Bidartondo, we're not even sure how to measure that. Fitter believes that the priority should be to start filling in the large gaps in the understanding of mycorrhizal fungi. We don't know the extent of fungal diversity, he points out, or of the mechanisms of exchange between plants and fungi. \u201cIt seems almost certain that mycorrhizae have a huge importance in biodiversity and a number of ecosystem services. But we're a long way from knowing what that is,\u201d Fitter says. \u201cUntil we've really got a proper understanding of basic fungal biology, we'll find it difficult to understand the ecological mechanisms.\u201d \n                     Hungry fungi chomp on radiation \n                   \n                     Conservation at a distance: Atomic detectives \n                   \n                     Mycorrhizae \n                   \n                     Tree employs fungal hitman \n                   \n                     Nature Biotechnology: Struggling to see the forest through the trees \n                   \n                     Marc-Andr\u00e9 Selosse \n                   \n                     Suzanne Simard \n                   \n                     David Robinson \n                   \n                     Gerhard Gebauer \n                   Reprints and Permissions"},
{"file_id": "449020a", "url": "https://www.nature.com/articles/449020a", "year": 2007, "authors": [{"name": "Rex Dalton"}], "parsed_as_year": "2006_or_before", "body": "The whole world felt the effects of the dinosaur-killing mass extinction 65 million years ago. But a spot in Colorado may have the best record of it. Rex Dalton reports from Denver. It's no wonder palaeontology is so popular in Colorado: fossils practically permeate the landscape. Hiking trails run through hills known for  Triceratops . Part of the local interstate is named after  Tyrannosaurus rex . And around the east of the city of Denver you can drive out into the grassy plains and walk the landscape where dinosaurs once lumbered. But for some, the thrill isn't just in finding the giants \u2014 it's in dating precisely when they disappeared. Here, in the rolling expanses east of the city, lies what could be the world's best spot for understanding what transpired 65 million years ago, when the Cretaceous geological period ended and the Tertiary one began. This 'K/T boundary' marks a crucial moment when some cataclysmic event \u2014 probably an asteroid impact (see  page 48 ), or massive volcanic eruptions \u2014 wiped out most creatures worldwide. The going date for this event is 65.5 million years ago, with a margin of error of 100,000 years 1 . But one group of scientists is aiming to get this margin down to just 25,000 years. Their work is part of a major international geochronology effort called EARTHTIME, which aims to calibrate the geological timescale for all of Earth's history 2 . \u201cWe hope to use the hundreds of ash layers to develop an unprecedented time sequence that will allow us to read the geological record of deep time like never before,\u201d says palaeobotanist Kirk Johnson, leader of the team and head curator at the Denver Museum of Nature and Science. Colorado has taken centre stage in geology after some serendipitous discoveries in the Denver basin over the past 15 years. In 1994, plant fossils of an ancient rainforest were unearthed as a highway was being built near the town of Castle Rock, about 40 kilometres south of Denver. Dating techniques pegged them at 64.1 million years old. That suggests that a tropical rainforest had established itself less than a million-and-a-half years after the cataclysmic extinction \u2014 earlier than some had suspected 3 . More discoveries followed, defining the Cretaceous underbelly of Denver. Then in 2003, Johnson and his co-workers published a report 4  that detailed the wealth of K/T sediments at a site called West Bijou Creek, about 65 kilometres east of Denver. Suddenly, Colorado took on world-class importance. Sediments marking the K/T boundary can be found across the globe, but the West Bijou site is one of the most complete locations, with plant and animal fossils encased within layers of volcanic ash. In an eroded knob along a gully overlooking the grassy plains, Johnson points to a layer just below the surface that is rich in the element iridium, which is often used as an indicator of an extraterrestrial impact. Below the iridium layer marking the K/T boundary are dinosaur fossils; above it there are none. The region's rich geological history makes it ideal for studying the events leading up to, at and just after the K/T extinction. Volcanoes once dotted the eastern side of the Front Range of the Rocky Mountains, showering ash over the countryside. Eventually, the volcanoes eroded away, but they left behind thick layers of sediment that piled up in the basin. It is these alternating white and dark layers that provide geologists with clues about the past. On past field trips, Johnson and his crew scoured the sites for plant fossils \u2014 finding about 500 leaves and 2,000 pollen samples, which together show what the environment was like around the cataclysmic transition. The sizes of leaves can indicate rainfall 5 , and the edges of leaves denote temperature \u2014 the smoother the edge, the warmer the temperature must have been 6 . \u201cLeaves are a fossil thermometer and rain gauge,\u201d says Johnson. \u201cThey also are a chalk board for insect tracks.\u201d By studying insect markings on the leaves, scientists can see which species went extinct and when. For instance, the team found little damage due to insect feeding on leaves dated at 63.8 million years old \u2014 suggesting that food webs were still out of kilter between a million and 2 million years after the extinction event 7 . \n               Dating service \n             Now, though, the researchers are moving beyond fossils and into geochemical, magnetic and other evidence to help them improve the chronology of what happened across the K/T boundary. Johnson has been joined by Sam Bowring, a specialist in geochemical dating at the Massachusetts Institute of Technology in Cambridge and William Clyde, a geologist from the University of New Hampshire in Durham who analyses palaeomagnetic fields. Their quarry is tiny zircon crystals in the layers of ash. By measuring the ratio of uranium to lead in individual zircon crystals, Bowring can date the ash layers and help the team pinpoint the time at which the K/T extinction occurred. Hiking to the bottom of a gorge called My Lucky Number, Johnson leads Bowring and Clyde to an eroded outcrop. Here they dig a shallow trench, revealing a dozen layers of ash. \u201cYou are looking at the surface of the swamp where the ash fell\u201d about 500,000 years after the K/T boundary, says Johnson, sweeping clean the first whitish layer. Every time a new layer of ash is uncovered, Bowring kneels \u2014 trowel in hand \u2014 and scoops out samples for the collection bag. \u201cOh, baby,\u201d is his frequent refrain. A field crew can work for years before finding a single ash layer. Here, more and more appear beneath the trowel. Meanwhile, Clyde uses clues from the periodic reversals in Earth's magnetic field. He takes a wood rasp, flattens a section of the soft ash layer, and aligns a compass along its plane. Then he digs out a fist-sized piece and tucks it away in the collection bag. These pieces will later be tested to see whether they fall in the same polarity as occurs today, or in the 'reversed' direction. By working out the date of the reversals around the K/T layer, the team can narrow down the time at which the extinction event must have occurred. \n               Back in time \n             For the rest of the day the researchers work their way up the gorge, scaring off deer as they go. They repeat the exercise half-a-dozen times, eventually taking readings on some 40 layers of ash. As they sample the rising sediments, Clyde may find five consecutive normal palaeomagnetic readings, then five consecutive reversed ones. Bowring's zircon dates can perhaps be used to bracket and date that reversal point. By comparing the findings to a research core stored at the US Geological Survey's office in Denver, near-surface dates can be correlated with the same sediments found deeper at other locations. And by comparing the new Denver dates to records from the ocean cores, the researchers might even be able to improve the correlation of sediment layers found around the globe. For instance, European researchers have found evidence 8  in Spain for changes in Earth's orbital cycles near the K/T boundary, which they date to 65.8 million years ago. The Denver studies will test that result, and help to refine the chronology of the past climate changes caused by changes in the Earth's orbit. These observations will be plugged into the larger EARTHTIME project, which is just beginning to get under way. But already, Johnson is looking at short-term benefits of the research \u2014 in particular, tying it back to issues concerning the health and development of the Denver area. His team's work on local aquifers, which run through sediments from the Cretaceous period, has helped to clarify the region's geology and hydrology. And that, in turn, could help city planners make better decisions about how to manage water supplies for the rapidly growing metropolis \u2014 to keep itself from going extinct. See Editorial,  \n                     page 1 \n                   . \n                     Ancient fossil forest found by accident \n                   \n                     Burrowing dinosaur unearthed \n                   \n                     Telling the time \n                   \n                     Meteor theory gets rocky ride from dinosaur expert \n                   \n                     EARTHTIME \n                   \n                     Denver Basin Project \n                   \n                     Denver Greenprint \n                   \n                     Plains Conservation Center \n                   Reprints and Permissions"},
{"file_id": "449274a", "url": "https://www.nature.com/articles/449274a", "year": 2007, "authors": [{"name": "Heidi Ledford"}], "parsed_as_year": "2006_or_before", "body": "As several lucrative protein-based drugs are poised to go off patent, makers of biopharmaceuticals argue that their products are too complex to be reproduced as generics. Heidi Ledford investigates how close 'biosimilar' drugs can get to the original. In 2006, Craig Wheeler, then president of Chiron BioPharmaceuticals in Emeryville, California, received a call from across the country that would challenge his perspective on the biotechnology industry. Momenta Pharmaceuticals, a small firm in Cambridge, Massachusetts, was looking for a new chief executive. The company planned to develop new drugs, in part relying on its ability to detect and manipulate the carbohydrate molecules that decorate proteins. But Momenta also intended to create generic versions of therapeutic proteins, something that Wheeler says he thought was impossible. Unlike the straightforward industrial chemistry techniques used to make small-molecule drugs, the methods of producing and isolating 'biologics' \u2014 complex drugs, vaccines or antitoxins made by or from living cells \u2014 can be complex and fickle. \u201cThe process is the product\u201d was the mantra of the biopharmaceutical world, says Wheeler. Even those who developed drugs in the first place were loath to play around with their methods. \u201cWe were deathly afraid of changing anything because we couldn't tell where it would lead,\u201d he says. Debate has flared over whether proteins are too complex to be copied. Even nomenclature for the replicants has changed as a result. Many have discarded the term 'biogenerics' in favour of 'biosimilars', saying that the word 'generic' unfairly implies a perfect replication. And companies and lobbyists on both sides are battling over whether biosimilars should be allowed to follow the fast track to approval available for small-molecule generics, or whether they should undergo expensive clinical trials beforehand. Pending US legislation on the matter could result in billions of dollars being won or lost by companies such as Momenta and the larger biotechnology and pharmaceutical companies that own the ageing patent rights to biologic drugs. Epoetin alpha or 'EPO', for example, is a recombinant form of the protein erythropoietin used to treat anaemia. It is marketed by several companies under different names, and currently commands a $12-billion market. EPO has already lost patent protection in Europe, and European regulators approved the first epoetin biosimilar in August. EPO and other drugs set to lose patent protection in the near future (see  table ) are attractive targets for the generics market. Europe has only recently determined a regulatory pathway for such generics, reaching the conclusion that the expedited path for small-molecule drugs is not directly applicable to protein-based therapeutics. Neither the United States nor Japan has a policy in place, and expectations of US legislative action during this session of Congress are fading. Ultimately generics companies want to leave the door open for accelerated reviews that would decide on a case-by-case basis. \n               Complex challenge \n             Wheeler certainly had his doubts both about biosimilars and Momenta. The sheer complexity of proteins presents a challenge. For small-molecule drugs, structure can be determined with certainty, and 'the process' is not in itself crucial. As long as the end product is the same as the original and there are no worrisome contaminants, the generic form of a small-molecule drug may often proceed to market without clinical trials. But proteins are much bigger \u2014 sometimes hundreds to thousands of times as large. In some cases the precise structure made by the atoms in the protein and the various chemical adornments it may have picked up cannot be determined. Moreover, the cells that are used to produce the protein sometimes leave a unique fingerprint of sugars and phosphate patterns reflective not only of the cell type but also the conditions under which they are grown (M. Gawlitzek, U. Valley, M. Nimtz, R. Wagner and H. S. Conradt  J. Biotechnol.   42,  117\u2013131; 1995). Genzyme for example, another Cambridge biotechnology company with a number of biologic products, recently struggled to gain regulatory approval to scale up production for one of its own drugs. Growing the cells in large tanks was found to change the drug's carbohydrate composition. A change in the arrangement or type of these sugars can profoundly affect protein activity, directing it to a new tissue, altering its function or alerting the immune system to its presence. Momenta claims that its generics will be aided by new methods to precisely monitor the sugars that decorate many protein surfaces. Understanding the arrangement of these sugars can be crucial to creating a copy of a protein that bears them, but they have been notoriously difficult to study, says Wheeler. \u201cI said, 'They can't know this stuff'.\u201d Nevertheless, on a whim, he decided to pay the company a visit. Wheeler studied the approach, toured the laboratories and came away convinced. Having made the leap to become Momenta's chief executive, he knows he is a rarity among his peers. \u201cThe generics people still really hate me because I was on the other side,\u201d says Wheeler with a laugh. \u201cI know all the counter arguments.\u201d \n               The dangers of change \n             The arguments can be compelling, as even small changes in biopharmaceutical production have resulted in tragic consequences. In 1998, European regulators asked Johnson & Johnson, based in New Jersey, to remove human serum albumin from its brand of EPO, called Eprex. Serum albumin, which was purified from human blood at the time, was there only to stabilize the protein during storage, and regulators wanted to eliminate the risk that Eprex might spread infectious agents. So Johnson & Johnson replaced it with polysorbate 80 (also known as Tween 80), a detergent and emulsifier commonly used to keep proteins in solution. Around the same time, the company also introduced a line of pre-loaded syringes. Nicole Casadevall, a haematologist at H\u00f4tel-Dieu Hospital in Paris, remembers when the first Eprex patients began to get sick. Doctors shipped blood samples to her so that she could test for antibodies against the drug. \u201cI began to see one case, then another, and then I was receiving cases from all of Europe,\u201d says Casadevall. In some patients, the immune system branded Eprex a foreign invader and produced antibodies to neutralize the drug. The antibodies not only rendered the therapy useless, they also attacked the endogenous protein \u2014 erythropoietin \u2014 causing a life-threatening anaemia in at least 200 patients. It has taken years to determine just what went wrong with Eprex. Johnson & Johnson says that polysorbate 80 caused compounds to leach from rubber stoppers in some of the pre-loaded syringes. Those compounds, the company argued, may have served as an adjuvant, boosting the recipients' immune response to the protein. In the United States, the original legislation covering generic drugs simply did not anticipate biological therapies, says Janice Reichert, a research fellow at Tufts Center for the Study of Drug Development in Boston, Massachusetts. The only protein therapeutics on the market in that era were insulin and growth hormone. \u201cThere was no reason to believe that they would have EPO on the market,\u201d says Reichert. \u201cNow we're sort of reaching a critical mass.\u201d Biopharmaceuticals represented a $30-billion market in 2005 and are expected to net $70 billion by the end of this decade. The US Congress introduced legislation to carve out a regulatory path for biosimilars but has been slow to act on it. Patent-holding companies have vigorously opposed the legislation, which is intended to assess biosimilars on a case-by-case basis rather than require trials for every drug. Meanwhile, Europe has approved only two drugs, EPO and human growth hormone, via its biosimilars pathway. Insulin and growth hormone, the only biosimilars marketed in the United States, are covered under present generics legislation because of their long history of use and their relative structural simplicity. All eyes are looking to companies such as Momenta to see what will happen to their first applications for approval in Europe and, eventually, the United States. \n               Three steps \n             Momenta Pharmaceuticals occupies a building in Cambridge's Kendall Square, just a stone's throw from the Massachusetts Institute of Technology, where the company's founders first developed the carbohydrate technology. On a recent summer afternoon, one of those founders, senior vice-president of research Ganesh Venkataraman, leans against a fume hood and outlines the company's three-step plan to introduce its technology to the US Food and Drug Administration. The first step is M-Enoxaparin, he explains, a generic form of Sanofi-Aventis's Lovenox. Lovenox is a complex mixture of sugars \u2014 but no protein \u2014 produced when the long carbohydrate polymer heparin is isolated from pig intestines and chemically shattered into short sugar chains. The resulting fragments are used to treat deep-vein thrombosis and pulmonary embolism, and Lovenox pulled in \u20ac2.4 billion in sales in 2006. Venkataraman says that Sanofi-Aventis declared it impossible to determine more than 70% of the carbohydrate composition of Lovenox. \u201cWe can account for every species in that mixture,\u201d says Venkataraman. Momenta chemically recreated that blend, and submitted the drug for approval in August 2005. Lovenox is not considered a biopharmaceutical, however. So Momenta's partner, Sandoz \u2014 the biogenerics arm of the Swiss pharmaceutical company Novartis \u2014 based in Holzkirchen, Germany, has filed for approval under the standard, existing generics pathway. That application, originally projected to take two years or less, is still pending. The second step is a generic version of Copaxone, a complex peptide mix marketed by Teva Pharmaceuticals in Petach Tikva, Israel for the treatment of multiple sclerosis. Momenta, together with Sandoz, will also file for approval of its version of Copaxone under the existing generics pathway. But after that comes the challenge: the third step involves two protein biologics, still in development, both of them complex proteins, complete with their adorning sugars and other modifications. By then Momenta will have walked through the entire regulatory process, says Venkataraman. For now, the researchers are working on characterizing complex mixtures and proteins. Using enzymes that cut sugars in specific places, they feed the fragments into one of the whirring mass spectrometers scattered throughout the lab, ready to pick apart a protein's knots of amino acids and forking carbohydrate chains. \u201cThese instruments can give you femtomolar resolution,\u201d says Venkataraman, proudly indicating the mass spectrometers. Momenta has also developed a computer algorithm that allows its researchers to feed in results as they are revealed. The algorithm then calculates all the structural possibilities, which can then be narrowed down and confirmed through additional experiments. Momenta is counting on its in-depth structural analysis to promote acceptance of its biosimilars. \u201cWe think that chemical characterization is a door opener,\u201d says Venkataraman.\u201cYou first need to establish that you at least chemically understand the molecule.\u201d Both Wheeler and Venkataraman are quick to note that they are not opposed to clinical trials of biosimilars. But whether a trial is necessary and what form that trial must take should be decided on a case-by-case basis, they say, not made mandatory. \n               Original variability \n             Biopharmaceuticals are often a mixture of protein variants, differing from batch to batch. \u201cOne of the key things that one has to do is to accumulate enough information on the original product to understand its own variability,\u201d says Cartikeya Reddy, head of the biologics division at Dr Reddy's Laboratories, a pharmaceutical company based in Hyderabad, India. Since 2001, Dr Reddy's has been producing a biosimilar version of granulocyte-colony stimulating factor (G-CSF), a protein drug used primarily to stimulate white blood cell production after chemotherapy or bone-marrow transplants. Understanding the variability in the original product is crucial but reproducing the precise mixture poses another challenge. Generics companies may know only the sequence of the protein of interest, and what they can glean from published material, says Reddy. Whenever possible, Reddy says his company uses the same cell line, or another cell line from the same species used by the innovator. Wheeler says that Momenta's intensive characterization ahead of time gives it a competitive advantage when it comes to making the drug. \u201cWe would have a far greater ability to design a work-around than others because of our analytical capability,\u201d he says. But Venkataraman admits that the company has not yet worked out a rational design for how to, for example, force a cell to reproduce a particular sugar pattern once it has been identified. \u201cWe're trying to get to it,\u201d says Venkataraman. \u201cIt's a frontier that hasn't been tackled.\u201d Venkataraman's ultimate goal is to use the structural information about a product and its biosimilars to rationally predict whether the differences between the two are likely to have an impact on toxicity or efficacy. Nevertheless Momenta's analyses will easily be overshadowed by real-world examples of the dangers involved. Johnson & Johnson's Eprex is so often cited in reference to biosimilars that the details of the incident sometimes get forgotten. Eprex was not a biosimilar, and problems associated with manufacturing changes are endemic to all biopharmaceuticals, not just biosimilars. Moreover, a clinical trial wouldn't have revealed Eprex's problems. But a simple chemical analysis might have shown the presence of the leachates in the syringes. \u201cEprex is an example that's good to scare people,\u201d says Venkataraman. \u201cIt raises this fear of the unknown.\u201d But the tale of Eprex also highlights the unpredictability of the human immune system. With the exception of G-CSF, every known protein-based drug tested prompts antibody production, usually at a subclinical level. Models that aim to determine whether a particular change in protein structure will tip the immune response from subclinical, to clinical, have performed poorly. \u201cThere doesn't seem to be any underlying pattern to what's immunogenic and what's not,\u201d says Robin Thorpe, head of the biotherapeutics group at the National Institute for Biological Standards and Control in Potters Bar, UK. \u201cYou're going to have to do some kind of study in humans.\u201d Because of the low frequency of immunogenic responses, as seen in the Eprex case, such trials are likely to include post-marketing surveillance. Venkataraman agrees that post-marketing surveillance will be important for biosimilars. \u201cThere are still several leaps that have to happen to get to the same level as small molecules,\u201d says Venkataraman. \u201cThe science is evolving to get there, but the lawmakers need to create the incentives. They shouldn't base legislation on today's technology.\u201d See Editorial,  page 259 . \n                     https://doi.org/10.1038/438154a \n                   \n                     https://doi.org/10.1038/447629a \n                   \n                     https://doi.org/10.1038/nbt0107-13 \n                   \n                     https://doi.org/10.1038/nbt0705-765 \n                   \n                     https://doi.org/10.1038/443496a \n                   \n                     Nature Biotechnology \n                   \n                     Nature Reviews Drug Discovery \n                   \n                     Statement by FDA Deputy Commissioner Janet Woodcock before the house committee on energy and commerce, subcommittee on health \n                   \n                     The Biosimilar Medicinal Working Party for the European Medicines Agency \n                   \n                     The Biotechnology Industry Organization \n                   \n                     The generic Pharmaceutical Association \n                   Reprints and Permissions"},
{"file_id": "449656a", "url": "https://www.nature.com/articles/449656a", "year": 2007, "authors": [{"name": "Geoff Brumfiel"}], "parsed_as_year": "2006_or_before", "body": "How do nuclear inspectors know when all is not as they are told? Geoff Brumfiel joins some inspectors-in-training as they learn the ropes at the Los Alamos National Laboratory. At half-past eight in the morning, the New Mexico sun already hot on our necks, we gather outside what could be the entrance of a high-security prison. Past the guard post and fence lies a tan, windowless building that covers about a city block. Gill-like vents along the building's side give it the look of a leviathan beached unnaturally in the high desert. This is the Chemistry and Metallurgy Research building at Los Alamos National Laboratory, the United States' oldest nuclear-weapons laboratory. The 51,000-square-metre building is the weapons programme's main centre for the study of nuclear material. But for the next two weeks the building will serve another purpose. It will be a training base for a fresh crop of nuclear inspectors for the International Atomic Energy Agency (IAEA). The IAEA is the United Nations' body charged with ensuring that the world's civilian nuclear facilities are being used for peaceful purposes, and it relies on about 250 highly trained individuals to do the job. The seventeen inspectors waiting at the gate are here as part of the agency's on-going training programme, honing the skills needed to stop the spread of nuclear weapons. The inspectors are ?the eyes and ears on the ground?, says David Albright, head of the Institute for Science and International Security, a non-proliferation group based in Washington DC. ?And they're quite effective.? When I was younger, I entertained the thought that I, too, might want to be a nuclear inspector. I imagined travelling the world with a diplomatic passport, turning up unexpectedly at secretive facilities and matching wits with local despots. So when the IAEA offered to let me tag along on the first few days of this August's course I jumped at the chance. My two days as a student there taught me that the reality is at once more tedious and more demanding than my Ian Fleming fantasy would have led me to believe. Inspectors are part scientist, part detective and part diplomat, says Jean Maurice Andre Crete, who heads the IAEA's training programme at their headquarters in Vienna, Austria. They also need to be part accountant, keeping meticulous logs of reactor operations, waste-pool inventories and research-material stocks. And they must be personable enough to win the trust of local plant managers. They must be Jacks and Jills of all trades. To reach that goal, inspectors undergo a gruelling three-month induction and take continuing education courses in subjects as diverse as international law, psychology, satellite imagery analysis and environmental sampling throughout their career. But at the heart of it all is the need to be able to verify whether the canister in front of them contains the radioactive material that its custodians claim is inside. And that's why they train at Los Alamos. In the belly of the metallurgy behemoth, they have an opportunity to get to grips with pure weapon's-grade uranium and plutonium.  \n                Blending in \n              If it wasn't for the fact that we're shuffling about in front of one of the world's most fortified scientific laboratories, we'd be an unassuming group. We come from 11 countries, spanning the globe from Argentina to Indonesia. All but two of us are men, and we're mostly middle-aged and dressed in polo shirts, khakis and tennis shoes. Most are nuclear engineers, but a few are scientists. One of the younger inspectors, called Giuseppe, is an Italian radiochemist who was working at a national research institute near Milan before joining the agency last year. Another, named Valeriy, has a PhD in particle physics, although he has worked in the field of uranium enrichment for decades, first in the Soviet Union and later in the Russian Federation. Security is tight, so much so that we have to keep in our escort's line of sight at all times. We cross an immaculate front lawn distinctly out of place between the razor-wire and the formidable walls. A lone cooler sits on a picnic table. ?That's the good thing about working behind the fence,? jokes Peter Santi, one of our instructors. ?Nobody's going to steal your lunch.? We wind down the staircase to our basement lab. Blast barriers line the hallways to protect heavily armed guards if attackers try and storm the building. But in contrast to the imposing exterior, the room in which the inspectors will spend the next two weeks is more like a college physics laboratory than a secure nuclear facility. The main room has eight tidy lab stations, each with its own instruments and worksheets. Friendly looking instructors chat casually over coffee. Only two things hint that something more dangerous is at play: a line of wall safes that hold canisters of high-purity uranium-235 and plutonium-239, and a plainly printed sign reminding users to be aware of 'criticality limits' ? that is, the maximum amount of material that can be brought together without triggering a nuclear incident.  \n                Indirect comparisons \n              After introductions, David Bracken, head of training at Los Alamos, gives a brief overview of the next two weeks. The inspectors will learn about what is known in the business as a non-destructive assay, or NDA. An NDA is a suite of measurements that allows inspectors to determine both the quantity and composition of a material without ever sampling it directly. Done right, it can verify a country's stocks of nuclear material quickly and cheaply. ?It's the foundation of safeguarding,? Bracken tells us. An NDA basically comes down to measuring two types of radiation ? ?-rays and neutrons. The ?-rays, energetic photons emitted during nuclear decays, give a distinctive energy spectrum that is a fingerprint of a material's elemental composition. The neutrons provide a measurement of the quantity of material involved. I haven't studied physics in nearly a decade, but I can follow the morning's lecture on neutron measurements without difficulty. It's a fairly simple technique: the material is placed at the centre of a doughnut of polyethylene foam. The polyethylene slows down the neutrons sputtering out of the sample so that they drift gently into a set of 18 or so cylindrical detectors placed around the foam. When the helium-3 atoms inside these detectors are struck by a neutron, they release a cascade of charged particles that goes on to be picked up by a high-voltage wire at the centre of each detector. It's a low-tech, cheap and durable way of counting neutrons. Nothing fancy, nothing too complicated; reliable and portable. ?The techniques aren't that new, and they're not that challenging,? Santi says. ?The challenge is to understand how they fit into the real world.? That afternoon, Santi's warning starts to make sense. A multitude of factors can affect the measurements. The distance of the detectors from the source; the thickness of the foam; the shape of the sample; the quality of calibration in the field: all these can trick an inspector into thinking that there is less, or more, or a different kind of material than is actually present. Measuring ?-rays is also quite simple ? we are given detectors just like the ones I used in my undergraduate lab work. But in the real world, shielding can block crucial features of a material's spectrum, and a dizzying array of common impurities, such as caesium and cobalt, can confound even the simplest detection. This is exactly why the inspectors are brought to Los Alamos, Bracken tells me. Only in a facility such as the metallurgy centre will the inspectors get the chance to work directly with realistically large quantities of weapon's-grade materials. ?The way we teach here isn't just to push a button,? Bracken says. ?We try to teach the physics, teach them how to think.? Over lunch with Giuseppe, I learn that taking these measurements in the field is tougher still, because there's very little support. Only in the most confrontational cases, such as Iran and North Korea, do inspectors work in teams of the sort that appear on television. On visits to places such as Germany or Mexico they travel alone.  \n                Field work \n              In nuclear power plants, especially, the inspectors must also work quickly. Commercial reactors are generally checked during refuelling outages, which companies try to complete as quickly as possible to maintain their profits. ?You have just a few minutes,? says Giuseppe. ?And you have ten people standing behind you, talking in a language you don't understand.? Sometimes, the staff at a facility will try to restrict the inspectors' access, not necessarily to hide anything but simply to try and speed the inspection along. ?If you're not prepared, people can sometimes prevent you from doing what you're trying to do,? he says. ?You have to be very sure about your rights.? Indeed, the rights of a nuclear inspector have changed radically over the past decade, says Crete. Inspectors used to have no authority to ask questions or search for additional, undeclared stocks. But all that changed in 1997, with the amendment of the Nuclear Non-proliferation Treaty ? the international agreement under which the IAEA operates. ?Now we are allowed to ask some questions,? he says. They also have the right to check for hidden stock. After my two days, I head back down from the mesa. I can see the attractions of the world my sort-of classmates are learning to negotiate. ?It's a very dynamic kind of life,? says the 37-year-old Giuseppe, happy with his career shift, with three months of every year on the road and a bag packed at all times for unexpected trips. ?You don't know exactly what to expect,? he says. But as someone who's both indiscreet and not much of a detail person, I wouldn't be the man for it. Better for me and non-proliferation both that I gave up the fantasy and took up the notepad. \n                     Nuclear Proliferation Special \n                   \n                     International Atomic Energy Agency \n                   \n                     Los Alamos National Laboratory \n                   Reprints and Permissions"},
{"file_id": "449773a", "url": "https://www.nature.com/articles/449773a", "year": 2007, "authors": [{"name": "Brendan Maher"}], "parsed_as_year": "2006_or_before", "body": "Despite a training in clinical genetics, Hugh Rienhoff didn't know what was wrong with his daughter. So, as he tells Brendan Maher, he set about finding out. Nearly four years ago, Hugh Rienhoff watched as his baby girl was pulled from a small incision in his wife's belly. It was their third child \u2014 the two boys had also been delivered by caesarean \u2014 and Rienhoff was there for all three births. But this child seemed different. He remembers her looking a little dark and sort of floppy, possibly attributable to the stress of delivery. Then he caught a glimpse of her feet, which were just a little longer than normal. For an instant, his training as a clinical geneticist kicked in. Could she have Marfan's syndrome? In the joy of the moment the question vanished as quickly as it arose. \"I didn't really think about anything from that point on medically, at least for that day,\" says Rienhoff. \"I did all the usual things you do when you have a baby, which is cry and call my family.\" When the paediatrician handed his new daughter to Rienhoff, she offered some technical terms \u2014 nevus flammeus for a port-wine-stain birthmark down the middle of her face and arthrogryposis for the reluctance of her tiny fingers to extend all the way. Rienhoff had to write them down to remember them. Although in the weeks and months after his daughter's birth the port-wine stain receded, it soon became clear to Rienhoff that she wasn't developing normally. Her fingers and toes wouldn't uncurl. More worryingly, in spite of ample feeding, the girl just wasn't gaining weight. Fleeting first impressions aside, she didn't have Marfan's, a disorder affecting maybe 1 in 5,000 births that arises from alterations in the gene for a protein called fibrillin-1. Skinny and birdlike with long fingers and flat feet, his daughter fit the physical characteristics of Marfan's fairly well, but she lacked some hallmark clinical criteria. In particular \u2014 thankfully \u2014 the typical defects in the cardiovascular system seemed to be absent, at least for now. Rienhoff's daughter is one of the thousands of children born every year who have a congenital defect that resists satisfactory diagnosis. Such cases could be a known disorder that presents in an unusual way, or they could arise from a mutation rare enough not to have made it into textbooks or databases. Detailed genetic analyses are rarely undertaken on such cases unless a group of families with compelling commonalities can be found. Instead, these children are cared for as best can be. But for Rienhoff that wouldn't do. Although he had largely left his practice, he had trained as a physician under Victor McKusick, the father of clinical genetics. Rienhoff knew genes, and he wanted to know his daughter's. For almost four years he has been trying to understand what makes her different at a molecular level, hoping that such knowledge could inform her care and treatment. He's quizzed experts, gone to meetings, and even set up gene-amplification equipment at home so that he can test his hypotheses with sequence data. He has also begun sharing the information he's found, telling his story on the Internet in the hope of helping others and of learning more. He may even have found a treatment that improves his daughter's condition. +++++++++++++++ As a medical student, intern, resident and research fellow, Rienhoff trained and worked at Johns Hopkins Hospital in Baltimore, Maryland, during the late 1970s and most of the 1980s. In 1992 he put clinical medicine and research largely behind him, leaving Johns Hopkins to become a partner with a Baltimore venture-capital firm, New Enterprise Associates.After years of helping biotech companies get off the ground, he decided to start one of his own. In 1998, he and his wife Lisa Hane moved to San Francisco where Rienhoff founded Kiva Genetics, later named DNA Sciences, a company aimed at developing a high-throughput sequencing platform for use in genetic discovery and diagnostics. He left his post there in 2001, and has continued to advise and found biotech ventures. Confounding some expectations for a corporate type who has danced at the dizzying pace of start-ups for more than a decade, the 54-year-old Rienhoff is patient, thoughtful and soft spoken. He talks in lists as if every thought has been backed up by a careful tabulation of pros and cons. That's certainly how he has managed his daughter's care. With every new doctor she's seen, every test she's been given, there's been a meticulous calibration of the risks and of the benefits that she might receive. One of his first carefully weighed decisions is one he remains adamant about: \"I didn't want to be my daughter's doctor.\" (And in the con part of the table, he is quick to point out that he's not a paediatrician.) Even though he has begun to practise medicine again, Rienhoff has one relationship with her \u2014 as her father \u2014 and wants no other. He's happily drawn his own blood, but when he wanted to sequence his daughter's DNA, he took her to a phlebotomist. He couldn't bear to put her through pain. +++++++++++++++ Because of her arthrogryposis, the first doctors Rienhoff took his daughter to see were orthopaedists. They saw one ten days after her birth. \"He was a thoughtful guy,\" remembers Rienhoff. \"He said: 'This reminds me of Beals', but it's not complete'.\" Beals' syndrome is a congenital disorder largely characterized by contracted joints, like those curling fingers and toes. Aside from that, the symptoms are quite similar to those of Marfan's, from which it was first distinguished some 35 years ago. The cause is similar to Marfan's, too: but in Beals' the mutation is in the gene for fibrillin-2 rather than fibrillin-1. Being familiar with the genetics community has its perks. Rienhoff read some papers on the disorder and contacted the authors. One put him in touch with the eponymous Rodney Beals at Oregon Health and Science University in Portland. Beals, also an orthopaedist, responded that it didn't look like the syndrome he had described in 1971. Among other things, Beals' patients typically have their 'contractures' in larger joints than those of fingers and toes; but Rienhoff's daughter's knees and elbows were lax \u2014 indeed hyperextensible. Beals didn't think that he could help. That said, Rienhoff knows all too well the difficulty in definitively ruling out disorders such as Marfan's and Beals'. They are genetically dominant, arising from a mutation in just one of the two copies people have for most genes. The mutant gene can be inherited from either parent, but that's not necessarily the case; sometimes a new mutation will crop up in sperm or egg. And because not all mutations in a gene will affect its expression, or the structure of its associated protein, in the same way, the symptoms associated with such a mutation can be quite different from the 'classic' form of the disease. Moreover, they may take years to manifest themselves. So Rienhoff's daughter may have a defect in fibrillin-1 or 2 that no one has ever seen before, and thus be a cryptic case of Marfan's or Beals'. But because there has never been enough evidence that his daughter has either of these diseases, she has not been sequenced for these genes, although she might be in the future. Rienhoff's first visit with a geneticist didn't provide much more clarity. The doctor suggested amyoplasia congenita, a diagnosis Rienhoff calls a relic, a \"dustbin\" for kids with various symptoms. And the collection of problems associated with this condition is so heterogeneous as to be useless. Like arthrogryposis, the term was merely a description of his daughter's symptoms. Thousands of children receive diagnoses like these, which don't shed light on what causes the problem or how it might be treated. \"I couldn't go very far with that particular diagnosis. It was clear as we went forward that she had a syndrome,\" Rienhoff says. He believed her symptoms were related to each other and that they were probably caused by something specific in her genes. Rienhoff's need for clarity was not purely intellectual. About five months after their daughter's birth, Rienhoff and Hane became very concerned about her failure to thrive. Although growing taller, she wasn't putting on weight. \"She was just melting away,\" says Rienhoff. The gastrointestinal specialists they went to see advised them to stuff her with calories, but it didn't do any good. The doctors drew up a list of things that might be causing her problems \u2014 disorders of the metabolism, of the way nutrients were absorbed from gut and stomach, of the way that mitochondria in her cells produced energy. One possibility that arose was an unusual form of cystic fibrosis, but her symptoms looked quite unlike this condition. Rienhoff thought that a mitochondrial disorder was a particularly plausible cause. The typical symptom is muscle weakness, which his daughter clearly had, but making a precise diagnosis is very tricky. Rienhoff dove into the literature and talked with the experts, quickly finding himself in what he calls a very messy field. \"That really ate up a lot of time \u2014 eight or nine months,\" says Rienhoff. As Rienhoff studied the murky world of the mitochondriacs, his daughter had her first birthday and took her first steps. She was developing \u2014 and, as a result, so was what could be said about her condition. When she stood up from a squatting position, she needed to brace her hands on her thighs. This behaviour, known as Gowers' sign, is common in children with muscle-wasting diseases such as Duchenne's muscular dystrophy. Sometimes, says Rienhoff, in a hard-to-determine diagnosis, you try to find a guiding principle. The inability to form muscle mass and tone, he says, \"became the North Star of the case\". +++++++++++++++ In the spring of 2005, Rienhoff and his daughter visited family and friends in Baltimore. He made an appointment with David Valle, a paediatric clinical geneticist whom he had met while working under McKusick. Valle, now director of the Institute of Genetic Medicine at Johns Hopkins, knows the limitations of his field as well as anybody. \"Although there's been great progress in recent years, it still comes down to a careful [case] history, family history and physical exam, looking at the standard laboratory data and trying to put all these clinical features together to come up with some sort of diagnostic probability,\" he says. And even after a battery of standard genetic screening, \"we're still left with maybe a third of patients who come in with morphological abnormalities for whom we're unable to make a diagnosis\". But when Valle was looking at Rienhoff's daughter with a couple of colleagues, something clicked. Her widely spaced eyes and marfanoid features, which are admittedly common in genetic disorders, looked strikingly similar to a syndrome that had just been defined. They asked the girl to open her mouth wide, and when they looked down her throat, they thought they'd cracked the case. In January of that year Hal Dietz and Bart Loeys, both at Johns Hopkins at the time, had published a paper defining another condition similar to, and previously confounded with, Marfan's \u2014 Loeys\u2013Dietz syndrome, to which they ascribed a related but distinct cause 1 . Fibrillin, the protein affected by mutations in Marfan's, is a structural component of the extracellular matrix, the protein netting that holds cells together. As a result it had long been assumed that the long, thin physical features and cardiovascular problems found in Marfan's were a result of the extracellular glue being structurally unsound. More recently, various lines of evidence, including research on Marfan's by Dietz and others, have suggested that the extracellular matrix does more than passively hold cells together; it mediates communication between them 2 . Fibrillin binds and sequesters the intercellular signalling molecules in the transforming growth factor-\u03b2 (TGF-\u03b2) superfamily, which plays an important role in development. Fibrillin defects, it seems, free up the TGF-\u03b2 signalling system with a range of effects: bones may grow extra long; vascular tissue may degrade. Loeys and Dietz found that some people who seemed to have Marfan's harboured mutations not in the gene for fibrillin-1, but in the genes for two TGF-\u03b2 receptors. Exactly how the mutations, which seem to disable the TGF-\u03b2 receptors, have an activating effect on the pathway is an ongoing puzzle. But the result is a syndrome that, because it disrupts the same bodily system, is quite similar to that caused by the fibrillin-1 defects in Marfan's. In addition to the molecular details, Loeys and Dietz had found three obvious bodily symptoms for their syndrome: the widely spaced eyes; a cleft in the palate and/or the uvula (the soft tissue that hangs down at the back of the throat); and severe structural defects in the arteries. Strikingly \u2014 although it had never been noticed before, despite a great deal of medical and parental inspection \u2014 Rienhoff's daughter had a forked uvula. Valle and Loeys took blood samples to sequence the TGF-\u03b2 receptor genes and suggested that the girl be given an echocardiogram as soon as possible. By chance, Rienhoff had scheduled one months earlier at the suggestion of David Clapham at the Children's Hospital in Boston, who suspected that her failure to thrive might be related to a heart defect. On the plane trip back Rienhoff read the Dietz and Loeys paper, which showed detailed pictures of the patients and their devastating aortic defects. His own heart sank. +++++++++++++++ \"The problem is,\" Rienhoff said to me with a levelling look the first day I met him at a caf\u00e9 in San Francisco, \"she's amazingly cute.\" He smiled with the look of someone who knows that a desperate search for a diagnosis can sometimes end with a bad diagnosis. The average age of death for someone with Loeys\u2013Dietz is 26. Over the year and a half of draining doctors' visits, Rienhoff and his daughter had developed a special bond. He has almost filled an entire notebook documenting his research on her case. But he has two more that are personal logs of his experiences with her, and casual observations of her and the funny things she says and does \u2014 he has similar notebooks for her brothers. One time, she asked him about a benign growth in the corner of his right eye. She calls him \"Poppy\", so his growth became a \"poppy-oma\". On the Thursday after their return to the West Coast they went in for the heart exam. Rienhoff watched every moment of the echo, and her aorta came back \"clean as a whistle\" \u2014 a huge relief. The sequence data on the TGF-\u03b2 receptors arrived a few weeks later; they showed none of the mutations Loeys and Dietz had identified for the syndrome. Dietz, whom Rienhoff went to visit the next year, says he wasn't completely surprised that the genetic testing came back negative. Rienhoff's daughter didn't have all the 'classic' symptoms associated with the syndrome. It was reassuring that one dreadful diagnosis could largely be ruled out. But there was still the matter of what was actually going on. Inspired by Loeys, Dietz and Valle, Rienhoff found himself newly focused on the TGF-\u03b2 signalling pathway. Maybe his daughter's disorder looked like Marfan's and Loeys\u2013Dietz because related molecules were damaged. Rienhoff threw himself into the literature on TGF-\u03b2 activation, once again guided by his North Star, his daughter's inability to build muscle. There are dozens of different growth factors in the TGF-\u03b2 superfamily and one of them, myostatin, is predominantly expressed in skeletal muscle. Defects in the gene for myostatin can result in overly muscled animals \u2014 notably the extraordinarily chunky Belgian blue cattle. In 2004, researchers in Germany and the United States described a young boy born to a former professional athlete 3 . He was remarkably muscular; at four-and-a-half he could hold two 3-kilogram weights at shoulder height with arms fully extended. Both his copies of the gene for myostatin were defective; his skeletal muscle was out of control. Myostatin works through three activin receptors: ACVR1B, ACVR2A and ACVR2B. These look similar in sequence to the TGF-\u03b2 receptors mutated in Loeys\u2013Dietz. Rienhoff thought that a mutation in one of these specific receptors might explain why his daughter's skeletal muscle was so dramatically affected while her blood vessels were not. But as far as he knew no one had ever looked at them in relation to a disease. So he bought a used PCR machine, a microcentrifuge, some small-volume pipettes and a brand new gel box. All told, the equipment cost him about $2,000. With these simple tools and some sequence-specific DNA primers of his own design, he could pick the relevant genes out of his daughter's genome and amplify them enough for sequencing. Freezing the samples and packing the tiny tubes on ice, Rienhoff sent them off for sequencing at about $3.50 a pop. He prepared upwards of 200. If he was right, the data he got back would show a mutation in one of the genes for the activin receptors analogous to the mutations seen in Loeys\u2013Dietz. When he got the sequences, Rienhoff compared them to the human reference sequences in GenBank. In the gene encoding the ACVR1B receptor he found a variant. But it was a long way upstream of where he would have expected it to be, far from the active domain where many of the Loeys\u2013Dietz mutations are found on the TGF-\u03b2 receptor genes. An obvious way to clear the mutation of any blame is for Rienhoff to sequence the copies of the gene in both his genome and his wife's. If one of them has the mutation too it is probably irrelevant \u2014 a harmless change, not one that explains the syndrome, because if it did the parentwith the faulty copy would share the symptoms. Rienhoff says that he plans to sequence his and Hane's genes when he gets the time. +++++++++++++++ His notebooks are not the only record of Rienhoff's journey into his daughter's DNA. There's also mydaughtersdna.org, where he presents the clinical facts of his daughter's case both in layman's terms and also with the details required by a more medically astute audience. Although in some ways the website is a conduit for bloggish catharsis, Rienhoff sees it as an attempt to serve others with unidentified genetic disorders who are looking for answers. He hopes it will give others a chance to share their experiences, bring together parents and patient advocates, and perhaps even identify other patients with symptoms similar to his daughter's. The site got off to a slow start, but a few people have now begun posting their stories, including Rienhoff's colleague Clapham, who recounts the heartbreaking loss of his son, Ben, to an inexplicable neurological disorder. With help from George Church, a Harvard Medical School professor with an extensive track record in new technologies for sequencing and synthesizing DNA, Rienhoff developed a sort of 'phenotype spreadsheet' on which to record his daughter's clinical history. The idea is that such data representations might someday allow a computer to search through his daughter's symptoms along with those of others with unidentified genetic disorders looking for clinical commonalities. Church plays down his contribution as just a seed of an idea that he thought worth testing, but says he is intrigued by Rienhoff's gumption: \"I'm interested in cases of altruists who, rather than hiding from genetics, are using the opportunity to be sort of social activists, working to raise consciousness and maybe raise money for diseases affecting their family and friends.\" Such activism is not new. Parents quite frequently become advocates for research on behalf of their children \u2014 when they are rich, famous, persistent, lucky or very well placed, they can make a difference. But with a sequencer and a website, Rienhoff has stepped over the threshold of personal genomics in a way set to catch the imagination. As sequencing gets ever easier and knowledge bases ever larger, it may not be fanciful to imagine more and more people following him, developing theories about abnormalities and testing them through sequencing. Such attempts will often fail, and in some cases lead to frustration and heartache. But some may make significant contributions to our understanding of the function of various human genes. Rienhoff recognizes that he has benefited from his training and connections. But he told me part of his mission is to empower others. \"I think probably the most important thing that people could take away from this is that the process is not mysterious,\" he says. His enthusiasm is not universal. In the course of my reporting, Rienhoff gave his daughter's doctors permission to speak to me, and not all of them agreed that he was doing the right thing. Dietz says he worries that Rienhoff's example may lead some parents down the wrong path, searching for answers in the genes and diverting resources from the important goal of making sure their children are receiving proper care. Rienhoff has heard these criticisms, and understands the discomfort. \"There is a certain sense that all of this will unravel, meaning all of this will become driven by the people,\" he says. In deference to Dietz he has removed from his website a folder called 'How to sequence DNA' that he had never filled. \"The purpose of the website is not about teaching people how to sequence DNA, at least not now,\" he says. But he still believes that patients and patient advocates can usher in what he calls a golden age in genetic research. It won't be for everyone. Rienhoff's search has been slow and methodical, and as yet inconclusive. Still, it has been fulfilling. \"I'm really being given an opportunity, if you will, with this site and at this time in the history of genetics.\" And the journey continues. Despite his daughter's DNA revealing not quite what he had expected, Rienhoff is still hopeful about his myostatin hypothesis, and it has led him to the most difficult decision he's ever made about his daughter. In May, based on the hypothesis that errantly activated signals might account for her inability to build muscles, Rienhoff, Hane and their daughter's cardiologist decided to put her on losartan, a drug for treating high blood pressure. Recent evidence suggests that it reduces the activity of secondary messengers triggered by TGF-\u03b2 receptors 4 , and that marfanoid mice are helped by the drug. Rienhoff knows it is a controversial move, but there are two quite powerful factors on his pro list. First, if his daughter does have some aberrant form of Loeys\u2013Dietz or Marfan's, the drug could forestall the vascular disease associated with the condition. Although he may eventually sequence her fibrillin and other genes for these disorders, the most definitive answers will come from regular cardiograms. The side effects of losartan are minor and reversible, he says, but vascular disease isn't. Second, her muscles might get a little better. \"I tortured myself over that one,\" he says. \"I took a Hippocratic oath \u2014 but I also took a parental oath \u2014 to do no harm.\" Meanwhile he devours any literature on TGF-\u03b2 signalling he can find. He has begun looking for scientists with whom he might collaborate on related projects, such as finding other patients with similar symptoms who might have mutations in the genes he's been looking at, or creating knock-out mice. He's waiting for more people to start using his website. He keeps a close eye on his daughter's progress, where he sees grounds for hope. \"Her more proximal muscles seem to be growing,\" he says. \"She can walk upstairs with a little assistance.\" But he's cautious not to overinterpret. \"I can't ascribe it to anything. I just keep my fingers crossed that she doesn't have a vascular disease. It's a quiet vigil.\"   See Editorial,  \n                     page 755 \n                    . Brendan Maher is a features editor at  Nature . \n                     Human Genome Collection \n                   \n                     MyDaughtersdna.org \n                   \n                     National Marfan Foundation \n                   Reprints and Permissions"},
{"file_id": "449398a", "url": "https://www.nature.com/articles/449398a", "year": 2007, "authors": [{"name": "Jane Qiu"}], "parsed_as_year": "2006_or_before", "body": "China's railway to Tibet is an engineering marvel or an environmental menace \u2014 or perhaps both. Jane Qiu takes a ride to find out. It is a glorious, crispy cold summer morning on the Qinghai\u2013Tibet plateau, and already my lips are turning purple. At more than 4,500 metres above sea level, the air is thin and I can feel a light headache coming on. In front of me a pair of rail tracks stretch into the distance, looking as thin as silver threads as they negotiate a landscape filled by expansive glaciers and mountains with needle-sharp peaks. The rattling sound of an approaching train jolts me to alertness and in no time it roars past us at 100 kilometres an hour, passengers waving gleefully from its windows. I'm painfully aware that the pressurized air behind those windows offers a great deal more oxygen than the stuff I'm breathing in the open air. \u201cYou will be fine,\u201d my Tibetan host, Tsega, pronounces after inspecting me up and down a few times. I feel obliged to trust his judgement. The railway in front of me is the world's highest, and is a 1,142-kilometre stretch that connects Golmud, in China's Qinghai province, with Lhasa, capital of the Tibet Autonomous Region. It is also widely regarded as one of the great engineering achievements of the world. Its course crosses four mountain chains and five major rivers \u2014 with nearly all of it more than 4,000 metres above sea level. Successive Chinese governments, keen to tighten their grip on Tibet, have dreamed of such a railway for nearly half a century, and more than 20,000 workers laboured for 5 years to complete the project at a cost of 33 billion yuan (about US$4 billion). Now, more than a year after it opened, the railway remains a source of bitter controversy. Supporters say the scheme will bring major opportunities to China's underdeveloped west. Critics fear that China will use it to assert control over a contested border region, and to exploit its natural resources. The railway's long-term impacts on the plateau \u2014 direct and indirect \u2014 remain unknown, and I have embarked on a journey between Golmud and Lhasa \u2014 sometimes on the train, sometimes off it \u2014 to seek answers. The trip reveals how well the engineering is standing up after a year's worth of exposure to the harsh seasons; how the attempts to minimize its environmental effects have fared; and what the railway means to ordinary Tibetans. \n               Earthquakes and permafrost \n             Tsega, who runs the Kekexili nature reserve from the office in Golmud, has kindly agreed to take me along on one of his regular surveillance trips to the reserve. After driving for 100 kilometres, we approach the gigantic Kunlun fault, a 430-kilometre surface rupture, ripped open by a magnitude-8.1 earthquake 1  on 14 November 2001. The scar from that quake is clearly visible in the landscape, and sends a shiver down my spine as we drive across it. The earthquake didn't cause much damage to the railway because construction had just started, says Wang Lanming, director of the Lanzhou Institute of Seismology, China Earthquake Administration, in Gansu province. \u201cBut it was certainly a wake-up call for those of us involved in advising the engineers of the region's seismic activity,\u201d he adds. Two other quakes of magnitude 7.5 or greater have shaken the plateau in the past century, and seismic activity poses one of the major threats to the new railway. The question is what to do about it. Railway engineers have avoided building stations, tunnels and bridges in areas containing active faults, and these main structures are equipped with anti-quake safety measures such as additional steel reinforcement bars to keep the concrete from cracking during an earthquake. But the tracks themselves do not have any extra reinforcement. \u201cThe cost would be astronomical to install anti-quake measures all through the route,\u201d says Wang. However, researchers and engineers are in talks with the ministry of railways about installing an earthquake-detection system along the tracks on the plateau. The project, estimated to cost tens of millions of yuan, could, in theory, notify the train of a substantial earthquake in progress by detecting the primary seismic waves propagating from a quake, then bringing the train to a halt before the damaging secondary waves arrive. As Wang notes, stopping a train before it derails in a quake is particularly important for high-altitude railways: \u201cIt would be very dangerous if passengers were exposed to the harsh conditions of the Qinghai\u2013Tibet plateau,\u201d he says. The harsh weather here also calls for innovative approaches to railway engineering. As we crest the Kunlun Range, ahead of us we see how the rail tracks disappear into a 1.6-kilometre-long tunnel. \u201cWe are entering a region rich with warm permafrost,\u201d Tsega explains. The temperature of warm permafrost stays within a couple of degrees below freezing, and its top layer thaws more readily in the summer than that of normal permafrost, making the ground more unstable. Regular permafrost is bad enough; engineers have to raise the railbed or use insulating materials to keep the tracks from warping during seasonal freeze\u2013thaw cycles. Construction is even harder with the mix of permafrost types on the Qinghai\u2013Tibet plateau. The railway traverses 275 kilometres of warm permafrost, 221 kilometres of ice-rich permafrost, and 134 kilometres of a mix of both. \u201cThis is the worst combination for any permafrost engineering project,\u201d says Max Brewer, a permafrost researcher at the University of Alaska in Fairbanks. \n               Keeping cool \n             A group of researchers, led by Cheng Guodong of the Cold and Arid Regions Environmental and Engineering Research Institute, Chinese Academy of Sciences, in Lanzhou, has developed a series of measures to cool down the railbed. All the approaches capitalize on using the benefits of the cold environment, and try to keep the railbed as naturally cold as possible 2 . In most places, for instance, the railway embankment is raised by between 2 metres and 10 metres, helping insulate the ground from the heat created by the tracks. The slopes of the embankment are also covered with a layer of crushed rocks \u2014 a technique inspired by reports that permafrost can occur beneath blocky and coarse materials even when the air temperature is well above 0 \u00b0C. \u201cThis is the first time a large-scale project has used the technique as one of its primary solutions,\u201d says Zhang Tingjun, a permafrost researcher at the National Snow and Ice Data Center in Boulder, Colorado. It seems to be working; tests suggest that the crushed layer keeps temperatures up to 2 \u00b0C cooler, and in some places, the permafrost has even increased in volume and pushed up into the embankment structure. As we drive along, passing Tibetan wild asses grazing by the road, I spot other gadgets along the tracks for cooling the embankment. The most prominent ones are 'thermosyphons' \u2014 a series of thin metal tubes standing upright on the railbed shoulders every 3 metres, which use evaporative cooling to transfer heat from one end to another. Thermosyphons are costly to install and maintain, but the Chinese government is not shy in lavishing on such gimmicks, with 18,200 of them installed in 'high-risk' permafrost areas. Elsewhere, engineers buried ventilation pipes through the embankments, which can also cool the temperature by up to 2 \u00b0C. And some of the south-facing slopes hold giant shading-boards. Over the first winter, these allowed shaded areas to freeze solid while non-shaded embankments remained unfrozen. More than 200 sensors constantly monitor ground temperatures on both sides of the railbed over permafrost regions, in a system costing 40 million yuan. One year and a seasonal cycle on, researchers suggest that the permafrost regions are recovering from the disturbance caused during the construction of the railway. But there have been several cracks along the track in some regions, which may be due to distorted foundations. Cracks are repaired as soon as they are discovered by the crew regularly monitoring the track. Zhang Luxin, chief engineer for construction and maintenance of the railway, would not disclose the total number of cracks since it began operating. But he says that all of the cracks so far have been superficial and have had no impact on the railbed. Brewer says he is not alarmed by such incidents. \u201cIt would be naive to expect that construction of this scale could be free of problems,\u201d he says. Indeed, it has been more than a century since Siberia's first railway was built, but frost-damage still affects 30% of the tracks passing across permafrost regions, according to a recent survey conducted by Russian engineers. Some experts, however, fear that the problems could get worse in a warmer climate. Research shows that global warming has affected the ground temperature on the plateau to a depth of 40 metres; permafrost is degrading as temperatures rise and the 'active' layer that freezes and thaws every year gets thicker 3 . According to a recent report by the Chinese Academy of Sciences, 10% of the permafrost regions on the plateau have degraded in the past 20 years. Before 2001, when the railway constuction started, the consensus was that the air temperature on the plateau would increase by 1 \u00b0C over the next 50 years. Scientists now think that it may rise by 2.2\u20132.6 \u00b0C during that time. Cheng, however, says that engineers could adapt the railways to additional temperature increases by adding more of the cooling measures to the railbed's slopes. Lack of oxygen sends excruciating pulses burrowing in through the top of my head and deep into my brain. I turn to the view outside, seeking comfort in the widening, sparsely vegetated valley, sprinkled with streams, rivers and enormous alluvial fans. In the distance, my gaze comes to rest on a gigantic overpass perching high above the plain. \u201cThat's the famous Qingshuihe Bridge,\u201d Tsega says, noticing my sudden enthusiasm. The 11.7-kilometre-long overpass, consisting of 1,367 piers and costing 24 million yuan to build, straddles the most unstable stretch of permafrost along the railway. The overpass removes direct contact between permafrost and the tracks \u2014 the engineers' last resort for protecting fragile ground. \n               Effect on migration \n             Tsega explains that the bridge is also one of 33 passageways designed to enable wildlife on the plateau to go under the arches to get from one side of the railway to the other. Chiru, endangered Tibetan antelopes known for their speed and stamina, are shy creatures and particularly prone to disturbances. Every June, more than 3,000 pregnant chiru travel hundreds of kilometres westward from the Three-River Headwater nature reserve to give birth in the Kekexili reserve, then migrate back with their young in August. Both the Qinghai\u2013Tibet railway, and the highway that follows the same route, cut right across their migratory paths. Experts are divided on the subject of how effective such wildlife passageways are. According to a government report released ahead of the first anniversary of the line's operation in July, most of the passageways are used by plateau animals and the railway has had no impact on the Tibetan antelopes in those areas. Other research is less positive. Contrary to reports by the state-run Chinese media, as many as 1,500 antelopes couldn't make the crossing in 2003 and had to give birth locally, says Yang Xin, president of Green River, a non-governmental organization based in Chengdu, Sichuan province. This happened despite the fact that railway workers suspended construction and cleared out of the sites for a couple of days during the animals' peak migration period. Although antelopes are able to cross the railway, research by Yang and Su Jianping, a zoologist at the Northwest Institute of Plateau Biology, Chinese Academy of Sciences, in Xining, Qinghai province, suggests that antelopes don't use most of the passageways. Before the railway construction began, antelopes crossed the highway between the Kunlun Pass and Wudaoliang at multiple locations along a 100-kilometre stretch. Now, a large majority of them funnel under the 200-metre-long overpass at the Wubei Bridge, just north of Wudaoliang. It's not yet clear what these changes in the animals' migration habits might mean in the long term. The researchers conjecture that some of the new overpasses might be too low or narrow for animals to feel comfortable, which pushes them into new migration routes. Yang, for instance, has seen herds of antelope approaching the Chumaer River \u2014 which used to be a key crossing site \u2014 then hesitate, turn away and go south to make the crossing near Wudaoliang. Knowing why some overpasses seem to work better than others could be valuable for future construction work, says Su \u2014 but only long-term monitoring can find that out. \u201cIt would be surprising if the highway and railway didn't affect movements of plateau animals,\u201d he says. \u201cIt's unclear how this could affect their other behaviours or their survival as a species.\u201d Yang Qisen of the Beijing-based Institute of Zoology \u2014 leader of the team that studies the railway's impact on plateau wildlife \u2014 declined  Nature 's request for an interview. Throughout the journey, the plateau is otherwise little marred by construction debris or the landscape scars usually associated with major projects. Railway planners developed a 'green policy' to ensure protection for soil, vegetation, animals and water resources along the route 4 . Nearly 4% of the 33 billion yuan spent on construction was allocated to restoration of ecosystems and environmental protection, according to Hao Qingjun, an official at the Xining-based Qinghai Environmental Protection Agency. Where possible, the railway was rerouted around sensitive natural zones such as wetlands. In other places, much of the turf that was dug up during the construction phase was preserved and then replanted once that section of the railway was completed. \n               Accumulating rubbish \n             The sun has almost set by the time we pull off the road at the reserve's protection station near Tuotuohe for the last stop of the day. The lingering light sets the snow-capped mountain peaks ablaze, while long blue shadows creep down from their dark stony slopes. Tuotuohe, a main town along the railway, lies in the region that is the headwater of three of Asia's largest rivers \u2014 the Yangtze, Yellow and Lancang (Mekong) rivers. Standing at the Tuotuohe Bridge, the first bridge over the Yangtze, I recall how Yang told me that the town is also the first rubbish dump along the river. The highway and railway are bringing more people and industrial products to the plateau. They have also led to more roads and other constructions, such as hotels and restaurants, along the route. \u201cThose constructions do not have the same kind of resources as the railway project,\u201d says Yang, referring to the lack of money and manpower. \u201cSo environmental protection is not on the agenda.\u201d Few towns along the route have the capacity to treat their accumulating rubbish, so it is left to moulder in open containers on the street and along the river banks. This is not only a source of disease, says Yang, but it also pollutes the environment and endangers wildlife. In addition, the introduction of commerce into the plateau life has led to an increasing need for more livestock, such as yaks, cows and sheep, which nomads can exchange for industrial products. \u201cOver-grazing is a serious problem on the plateau,\u201d he says. In the Three-River Headwater region, some nomads have to go as high as 5,500 metres to find fresh pasture for their animals. \n               Change in Tibet \n             It soon turns pitch-dark and the temperature drops sharply. In the protection station we sit around a stove, enjoying the waves of warmth it radiates. A television is showing news on Phoenix TV, a popular Hong Kong channel. There is a computer in the corner with an internet connection. Tashi, a young Tibetan man who looks after the station, brings us supper: bread, yoghurt and Tibetan tea, along with some dried beef Tsega brought with us. Over the meal, I ask Tashi how the railway has affected his life and that of his family who are nomads in the region. \u201cOur life is still the same,\u201d he says after a moment of hesitation, a sense of indifference palpable. None of his family or any of their friends sees a need for taking the train. \u201cWe are nomads.\u201d His indifference, and that of many others living in rural Tibet, is in stark contrast to China's claimed purpose of the railway: to promote economic development. On the surface, Tibet is doing well, and the railway has certainly helped; during the first 10 months of its operation, trade between Tibet and outside regions increased by 75%, to 2.6 billion yuan. Tibet will receive an estimated 3 million tourists this year and, by 2010, the number is expected to reach 5 million. The billions of yuan that Beijing has poured into Tibet flow almost exclusively along the handful of roads in the region, pooling in the towns along the way and finally ending up in three major cities \u2014 Lhasa, Shigatse and Chamdo, says Kabir Heimsath, a Lhasa-based anthropologist at St Cross College, University of Oxford. The towns and cities are the few places in which serious development has taken hold and, consequently, are also the only places where Han migrants \u2014 mostly itinerant workers marginalized by the same system in other parts of China \u2014 and businessmen form a majority. This increasing disparity between the urban rich and the rural poor \u2014 a general problem of China's economic development \u2014 means that most Tibetans are left behind, unable to reap the benefits of modernization, he says. Even in towns and cities, Tibetans are in danger of being marginalized. They find it increasingly difficult to compete in the job market against skilled and more business-oriented Han, who, privileged by ethnicity and language, make up more than 92% of the population of China. Intentionally or not, Tibet has seen a surge of Han migrants, which is further boosted by the railway. And emerging with this wave of migration is the cultural imprint of Han Chinese on many aspects of Tibetan life \u2014 even religion, as an increasing number of Tibetan monasteries are funded and built by Han Chinese. \n               On the train \n             A few days after visiting Tuotuohe, I board the train from Golmud to Lhasa for the uninterrupted experience of the entire train journey. The coaches are completely sealed so no rubbish can be thrown out and have a wastewater-storage system. They have windows with ultraviolet filters to keep out the sun's glare, and their underbellies are enclosed to protect wiring from snowstorms and sandstorms. The oxygen level in every coach is well-adjusted, and additionally, passengers can plug in to the oxygen supply anywhere on the train. Every coach is equipped with digital displays that show continuous updates of the train's elevation and speed, the outside temperature and the distance to the next station. Travelling at about 100 kilometres an hour, the trip is surprisingly smooth. I have the opportunity to review the scenery \u2014 the magnificent Mount Yuzhu reaching up to 6,178 metres, the turquoise Namucuo Lake shimmering under the unearthly Tibetan light, nomads dressed in colourful robes, turrets with red, blue and green prayer flags fluttering near a Communist flag. Fourteen hours later, at the end of an uneventful journey, the train pulls into the great vault of Lhasa's new railway station. The next morning I wander through the city's various districts. Apart from the Tibetan quarter centred on the holiest Jokang temple, most of the shops, hotels and restaurants are run by Han Chinese. I come across Bhuchu, a Tibetan pilgrim from a village in Sichuan province. He has prostrated himself on the ground after every step for more than 1,000 kilometres over the past 6 months. Now only steps away from Jokang, he is radiant with joy, hope and a deep sense of heightened existence. I finally arrive at the Potala Palace, the historic landmark of Lhasa, which used to be home to successive Dalai Lamas. The sight of the gigantic red-and-white structure perching serenely on Potala Hill is simply breathtaking. Marvelling at its architectural splendour and spiritual richness, I can't help but wonder what the railway will bring to Tibet and its people. Already, China is planning to extend the railway even farther, to Zhangmu to the west and to Dali to the east. Another extension would link Shigatse with Yadong, near the China\u2013India border. What will this expanded network mean for the fragile plateau? Here in the rarefied environment of Lhasa, the question hangs shimmering in the air. \n                     Climate change: The long-range forecast \n                   \n                     Tibetans show 'evolution in action' \n                   \n                     Neuroscientists see red over Dalai Lama \n                   \n                     Official Qinghai-Tibet railway website \n                   \n                     Xinhua news agency site on the Qinghai-Tibet railway \n                   \n                     The Government of Tibet in Exile \n                   \n                     Free Tibet campaign \n                   Reprints and Permissions"},
{"file_id": "449395a", "url": "https://www.nature.com/articles/449395a", "year": 2007, "authors": [{"name": "Brendan Maher"}], "parsed_as_year": "2006_or_before", "body": "Can a stage spectacular based on a TV documentary bring science to life and please the punters too? Brendan Maher joins a palaeontologist to watch the dinosaurs walk. As the seating in the arena slowly fills up the children can't hide their giddy anticipation. Neither can Ken Lacovara, chattering away about dinosaurs and digs. Admittedly, he has a soul patch on his chin, a beer and a couple of graduate students, which marks him out from the majority of enthusiasts here to see Walking with Dinosaurs: The Live Experience. But there's no mistaking the kinship between the professor of geology and palaeontology from Drexel University, Philadelphia, and the children around him. \u201cEverybody I know in the field wanted to do this since they were very young,\u201d he says, looking around. \u201cYou never know what future scientists might be in the audience.\u201d The stage show now touring America \u2014 watched by  Nature  and Lacovara at Philadelphia's Wachovia Spectrum sports arena in August \u2014 was inspired by the BBC documentary series, which, according to Lacovara, set a gold standard for edutainment (see  'Origin of a Species' ). Using computer animation, animatronics and the authoritative tones of the actor Kenneth Branagh, the series told the tale of the dinosaurs' 160-million-year lease on Earth. The programme was stunning to watch yet stuck close enough to scientific understanding not to upset an expert. At least, not Lacovara. The $20-million stage show has a lot to live up to, and Lacovara's excitement \u2014 especially over the prospect of a fully fleshed-out brachiosaurus \u2014 is tinged with scepticism. How well can a theatrical presentation relay natural history? Will spectacle triumph at the expense of information? The house lights dim and a booming voice reminds the crowd that, as cell phones and pagers didn't exist 65 million years ago, they should be turned off. Score one for realism. 'Huxley', a palaeontological P. T. Barnum strides on to the stage to serve as our guide and scale bar. He bends down by a nest from which plateosaurus hatchlings emerge, in the form of squirming green hand-puppets. The eggs, Lacovara notes, aren't shaped quite right \u2014 too \u201cchickeny\u201d \u2014 but his criticism is cut short by lilliensternus. A two-metre-tall carnivore enters the arena. Actually, it's a suit worn by an actor with the mettle to carry 40 kilograms of foam, lycra and animatronics on his back and yet still look nimble. Once you learn to ignore the craftily camouflaged extra set of human legs, it's pretty convincing. In the interest of conflict, a full-grown plateosaurus appears next, eager to defend its babies. This large dinosaur is a puppet, operated by three people. One drives a slim car camouflaged beneath the dinosaur; two others are in a control room moving its neck, tail, jaws and the like by manipulating a smaller version, evocatively known as a voodoo rig. Prosauropod and predator settle into a carefully choreographed stand-off. The slow, deliberate tempo is the result of much trial and error, Matthew McCoy, the head of puppetry, later explains. McCoy tells the tale of a tragic show in Sydney, Australia, in which  Tyrannosaurus rex 's head fell off after a tight turn at high speed. The audience was sympathetic, he says with some gratitude, but the troupe learned its lesson. In addition to slowing down the action in later shows, the team built a spare  T. rex . With 15 dinosaurs taking the stage every night, losing one wouldn't necessarily stop the show, but turning up in front of hundreds of children without a working  T. rex  just doesn't cut it. \u201cWe might as well just go home,\u201d says McCoy. There are contingency plans for other disasters, too. Had lilliensternus been toppled by plateosaurus's heavy whiplike tail, for example, he would have needed help getting back up. That, says McCoy, is when they send in the dinosaur clowns. No disasters strike in Philadelphia. After several minutes of a mock battle and trotting about, lilliensternus and plateosaurus dutifully leave the stage, and Huxley eases the crowd through geological time into the Jurassic period. Bright inflatable plants explode around the stage. Lacovara gleefully elbows one of his students; he thinks giving a round of applause to the Jurassic just for starting is pretty amusing. With the Jurassic period come the brachiosaurs. The young one is perhaps two storeys tall; the adult, more than ten metres. They make an impressive pair as they stretch their long necks deep into the stands, delighting the audience. As one of them almost lays its head in Lacovara's lap, he notes that a full-grown adult would have been a bit taller, but he's still impressed. The palaeontologist on stage rattles off statistics about the beast, which may have weighed as much as 40 tonnes. The expert in the stands notes, with a mischievous grin, that the titanosaur his group is excavating in Patagonia weighed 60. But despite a little professional one-upmanship, Lacovara likes the show. During the intermission, he confers with his students, who agree that it doesn't pull any educational punches. \u201cIt's just packed full of content,\u201d Lacovara says, noting that it introduces concepts such as deep time, plate tectonics, climate change and evolution: all ingredients, he says, that presented too dryly would spell certain death. This concept brings us ineluctably to the show's finale. In the climactic Late Cretaceous, rife with volcanic drama (cue the light show), a  T. rex  mother and son take the stage triumphantly. After some play-fighting with ankylosaurus and torosaurus, they turn their attentions to the crowd. While baby  rex , another actor in a heavy dino suit, mugs for the crowd, mum is scaring the life out of them. A blonde boy just behind Lacovara chats nervously with his father about the seating arrangement as the  T. rex  approaches. \u201cNo Daddy, don't tell him I'm here.\u201d A bright strobe with booming audio represents the extraterrestrial  coup de gr\u00e2ce  at the end of the Cretaceous. The dinos exit, the plants deflate, and bows are taken. The crowd drains from the Spectrum, and the children are laden with bright and blinking palaeoparaphernalia. After three more shows, the crew will pack its 27 truckloads of equipment and move on to the next stop. Lacovara is beaming, satisfied with the production's portrayal of the work he does. Dinosaurs, he says, are \u201ca gateway drug for the sciences\u201d. A lot of kids scored tonight. \n                     Dinosaur protein sequenced \n                   \n                     Dinosaur embraced vegetarianism \n                   \n                     Happy hunting predicted for dinosaur seekers \n                   \n                     Dinosaurs in Focus \n                   \n                     Kenneth Lacovara at Drexel University \n                   \n                     Walking With Dinosaurs The Live Experience  website \n                   Reprints and Permissions"},
{"file_id": "449524a", "url": "https://www.nature.com/articles/449524a", "year": 2007, "authors": [{"name": "Quirin Schiermeier"}], "parsed_as_year": "2006_or_before", "body": "The Russian Academy of Sciences has resisted pressure from czarists and communists. Can it thwart the reforms planned by Putin's government? Quirin Schiermeier reports. Muscovites have a complex relationship with the bold architecture they call the \u201cbuilding with the golden brain\u201d. The widely visible tower near Gagarin Square in the southwest of the Russian capital, topped by a futuristic chaos of glistening copper pipes and air shafts, is the modern headquarters of the Russian Academy of Sciences, founded in 1724 by Peter the Great. For many it symbolizes the pride and glory of science in a nation that has traditionally held researchers, inventors and explorers in high esteem. But for others the overly ornate superstructure can all too easily be seen as emblematic of the arrogance of an isolated and ageing academic \u00e9lite that has little to offer Russia as it moves towards a knowledge-driven modern society. The feeling that the academy, which employs some 50,000 scientists across 418 research institutes, is in dire need of reform is widespread even among scientists affiliated with the institution. Critics say that it is controlled by an old guard of 1,250 academicians raised under a Soviet culture who cling to their privileges and rituals. The academy, they say, has become a comfortable refuge for tens of thousands of unproductive 'shadow researchers' who await their pensions while producing little or no science of any merit. In the absence of proper quality control and competition, pessimists warn, Russia will risk falling further behind up-and-coming science nations in Asia, and could even struggle to maintain its status as a second-rate science nation. The response from many leading academicians within the academy is that Western-style research with its 'publish or perish' mentality offers a rat race unworthy of the noblesse of true science. \u201cTen years ago Russian scientists published ten times more papers in journals such as  Nature  and  Science  than Chinese researchers did \u2014 now the Chinese have twice as many as we do,\u201d says Alexander Sobolev, a geochemist at the academy's Vernadsky Institute of Geochemistry and Analytical Chemistry in Moscow, who is frustrated by the slow pace of reform. \u201cThe problem is that the academy just doesn't support a system in which productivity counts. Scientific careers in Russia don't depend on results, and nobody really cares about international expertise. We simply don't provide the right motivation for scientific work.\u201d Motivation understandably waned during the 1990s, after the break up of the Soviet Union, when scant scientific salaries forced talented Russian scientists to find second or third jobs outside science, or to leave research altogether. Many more emigrated to research positions abroad. During the hardest years, even top academicians officially earned little more than US$100 per month. Without generous support from foreign governments and organizations, such as the Soros Foundation in New York, set up by Hungarian billionaire George Soros, the Soviet Union's scientific heritage might have decayed beyond repair. During this period of crumbling science budgets and galloping inflation, many were concerned about Russia's nuclear heritage and its emigrating scientists. The nation was living permanently in debt, and could hardly afford to maintain some 4,000 Soviet research organizations. As economist Boris Saltykov, the first minister of science in the post-Soviet government, said in 1992: \u201cIn Russia we have too much science.\u201d But he, like others after him, struggled to reform the academy. Against the odds, the number of academy institutes actually rose after 1990, from 330 to 418 today. The number of academicians has also grown, at the expense of other scientific employees, and despite frequent criticisms of inertia and unproductivity. Some wonder if the academy is unreformable \u2014 a feature of the Russian landscape as enduring as the Siberian steppes. The current science minister, Andrei Fursenko, formerly a physicist at the renowned Ioffe Physico-Technical Institute in St Petersburg, is determined to make the academy more accountable. Sources close to the ministry say that long-standing discussions over the reform of the academy are turning into a real power struggle. In January, Fursenko suggested a 'model charter', which the academy's general assembly rejected in March on the grounds that it would give the ministry bureaucratic control over the academy. The academy has proposed its own charter, but a compromise is unlikely, and so the deadlock continues. Whether the academy can resist all attempts at external and internal reform remains to be seen.  \n                Paper trail \n             What is the state of Russia's scientific output today? Certainly its publication record remains bleak. \u201cToo many people here call themselves scientists without having published a single paper in the past decade,\u201d says Boris Stern, an astrophysicist at the P. N. Lebedev Physics Institute in Moscow. In 2001, Stern received a small grant from the Russian Foundation for Basic Research in Moscow to build up a database of the scientific productivity of Russian scientists and scientific institutes. He regularly updates this 'Who's who in Russian science' with the latest information from Thomson Scientific, a citation service based in Philadelphia, Pennsylvania. \u201cThe problem is that we have a totally biased system for scientific reputation in Russia,\u201d says Stern. \u201cThe formal administrative hierarchy has nothing to do with informal reputation.\u201d For example, Stern's list of the most productive institutes is topped by Lomonosov Moscow State University, which is not an academy institute. And Stern found that fewer than 50,000 Russian scientists \u2014 one-eighth of the total workforce \u2014 publish at least one paper a year. According to the 2006 World Bank Russian Economic Report, researchers in Poland, India, Brazil and South Korea generate 2\u20133 times as many papers per person. And patent numbers tell a similar tale: patents produced per capita are 60 times higher in South Korea than Russia. The low productivity has not escaped the attention of the Kremlin. As the government invests in the country's diminished science base \u2014 the overall number of researchers has declined by more than 100,000 since 1995 \u2014 expectations will rise and so focus new attention on the academy's sagging performance. President Vladimir Putin is determined to give the Russian economy, which leans towards exploitation of the country's abundant oil and gas resources, a more solid industrial base. Science will be a tool on the road to a new economy Putin said in May in a speech to the State Duma, the lower house of the Russian parliament. In July, the Duma dutifully approved a massive $7-billion investment in nanotechnology over the next five years, which Putin and his advisers think will lessen Russia's dependence on petrodollars (see  Nature 448, 233; 2007 ). The nanotech initiative, which almost matches the $1.3 billion per year the academy receives from the government, can be seen as an attempt to develop a new state sector of science outside the academy, says Saltykov. In fear of being passed over, the academy rushed to appoint Mikhail Kovalchuk, a physicist with close ties to Putin and the head of the nano-initiative, as its 'acting' vice-president for nanotechnology. The move has been criticized as a violation of the academy's statutes, which state that only full members can be elected into leadership positions \u2014 Kovalchuk is only a corresponding member.  \n                State control \n              Having already threatened the academy's pre-eminence as Russia's main basic-research organization, the government is also starting to encroach on its internal operations, which remarkably survived the communist era intact. Last December, Putin signed the new 'Law on Science and Technology State Policy' which, among other things, modified the legal status of the academy to give the government more control over its operations. The new law includes a provision that allows Putin to approve the elected president of the academy \u2014 a requirement that has dismayed some leading academicians. Alexander Nekipelov, one of the academy's chief vice-presidents, says he thinks it was 'merely a symbolic act'. \u201cI don't think [Putin] can actually disapprove of an academy president who has been properly elected according to our rules,\u201d he adds. Still, last November, in anticipation of the new rules, the academy postponed the election of a successor to its current president, Yuri Osipov, who has reached the age limit of 70. It's not yet clear when a new academy president will be elected \u2014 it may be as late as 2008, when the country will also choose a successor to Putin. Nekipelov does admit to \u201cmajor disagreements\u201d with the science ministry over the future course and governance of the academy. According to Fursenko's charter, important decisions, such as creating new laboratories or closing existing ones, and on allocation of funds, would be handed over to supervisory committees, in which government representatives would hold the majority. Research money would be given to individual teams on a purely competitive basis rather than as a lump sum to the academy presidium. The academy leadership, backed by most of the full and corresponding academicians who make up the general assembly, vehemently opposes the new law and subsequent charter. It has prepared its own charter, which is currently being reviewed by the government, in which the academy would remain a self-governed organization, free to allocate funds independently, and with the right to set up, modify or liquidate institutes as it sees fit. The academy is keen that most of its budget remain as basic institutional funding \u2014 as a fixed lump sum. But it has suggested that 20% of the money it receives from the government in future will be distributed though a 'competitive' interdisciplinary programme. The ministry would like to reduce institutional funding to a minimum, and introduce performance-related funding of projects and individuals, including consideration of scientists' international publication record. Many academicians hate this idea because their low publication rate in international journals would exclude most of them from getting funding under any competitive programme. If competition is to be introduced, they would prefer the programme to be managed 'in-house', using internal expertise to evaluate grants. But the government is unlikely to accept this proposal, and a compromise is currently not in sight. To outsiders, the academy seems to be fighting a losing battle. The ministry holds the purse strings, and can force through legislation to restructure the academy. Indeed, the government has threatened to block next year's round of promised pay rises to increase pressure on the academicians, says Nekipelov. Average salaries for academy scientists were to jump to $1,200 per month by 2008, in exchange for the academy cutting staff by 20%. Two years ago, average monthly salaries were still as low as $200; they are now at around $500, roughly similar to that of the average industrial worker. To justify even higher salaries, the government wants to impose on the academy a performance-related system that it hopes will improve the scientific output of its workforce. Future salaries would be composed of a fixed part \u2014 the tarif \u2014 plus bonuses for measurable scientific achievements. Nekipelov gets animated when it comes to what he calls 'paying by results'. He disagrees that a simple indicator, such as the number of publications in international journals, can measure researchers' performance, as has been proposed by the science ministry. \u201cPeople would start working for 'indicators' only,\u201d he says. \u201cSome people think that our scientists just want to pocket salaries but not work hard, and that there is no such thing as competition in our institutes,\u201d he says. \u201cThat's wrong. The truth is that scientific work as such is a very competitive activity. All academy scientists compete for funds allocated to their institutes, while institutes compete for funds allocated to different fields of science, and so forth.\u201d Nekipelov also disputes the widely held view that most academy researchers are isolated from, and rather indifferent about, international trends in science, and refuse to give talks or write papers in English. \u201cWe do have many unofficial relations with colleagues abroad,\u201d he says. He is also critical of the influence of international journals and foreign reviewers: \u201cThe working language in this country is indeed Russian, scientists will not start speaking English here.\u201d Saltykov is dismayed to hear this: \u201cIf that is so, then what we are doing here has nothing to do with international science.\u201d Saltykov is in favour of radical reorganization of the academy. With the exception of a few young scientists with ties to the West, he says, the members of the academy continue to live in a Soviet-type economy and state of mind. \u201cThey say that science and the market are incompatible, whereas in fact they consider money more important than principles. And they keep demanding budget money, but would like to remain totally unaccountable.\u201d  \n                Fresh blood \n              Only about one-quarter of academy scientists are truly competitive, hard-working researchers, says Saltykov. He cites an unpublished review of 1,330 academy researchers done in 2005 by sociologist Sergei Belanovsky on behalf of the Moscow-based Centre for Strategic Research, a think-tank close to the ministry of economics. The study showed that almost half of the academy's scientific workforce was unproductive dead weight. Nekipelov accepts that something needs to be done: \u201cWe do have quite a strong potential,\u201d he says, \u201cbut it's true that we need more young people, and that we need to free ourselves from some of the elders who don't do much science any more.\u201d \u201cPeople here are very experienced in the art of imitation,\u201d says Mikhail Feigel'man, a researcher at the Landau Institute for Theoretical Physics near Moscow. \u201cEven though open scientific competition is officially encouraged, 99% of it is fake. You write a proposal, say, and then a so-called 'competition' is created, but in which only your project will fit.\u201d The nanotech initiative, for which Kovalchuk will effectively have control over the whole budget, makes no difference he says. \u201cIt's a very feudal system in Russia, and I fear this will be impossible to change in our current situation.\u201d However, government funding is starting to become more transparent. Its $5.3 billion federal targeted programme for 'Research and Development in Priority Areas of Russian Science and Technology 2007\u201312' supports mostly applied research on a competitive basis in seven fields, including the life sciences, energy and nanotechnology. The scheme is modelled after Europe's Framework Programmes and is intended to increase competitive funding for research projects. In recent years, the academy has experimented with a similar funding model \u2014 on a smaller scale and with mixed results. The 'Scientific Programmes of the Presidium of the Academy' is a scheme to enable small teams, selected on the basis of their international publication record during the past five years, to do independent research. Sixteen fields were selected for pilot projects, but only one, the $5-million cellular and molecular-biology project, has had much success. When they were first set up in 2002, the idea of competition was almost a revolution in the academy system, and it was attacked from many sides. Only a few 'good' people applied in the first round, and most did not believe that there would be honest competition and no corruption, says Georgii Georgiev, former director of the academy's Institute of Gene Biology in Moscow and project coordinator for the cellular and molecular biology scheme. But news got around that the programme could work, and that the grants \u2014 of $150,000 per year \u2014 could provide talented scientists with a level of scientific independence and flexibility formerly unknown in Russia. About 100 life scientists, of which 10 have since returned to Russia from abroad, are being funded in the cellular and molecular biology programme, and the second round, from 2004\u201308, was oversubscribed five-fold. \u201cWe improved the system by and by, so that we are considered an example as to how competition can help make our science better and more transparent,\u201d says Georgiev. The third round of the competition will start in December.  \n                Isolated innovators \n             Sobolev, a corresponding member of the academy who has close ties with the Max Planck Institute of Chemistry in Mainz, Germany, has lobbied hard in the past few years to set up and expand competitive programmes to support international-level research groups in Russia. But he got little support for his ideas from decision-makers within the academy. The problem is that only a small fraction of Russian scientists \u2014 those well integrated in the international community and who regularly go to international meetings \u2014 really want to change the system, explains Evgeny Antipov, a chemist at Lomonosov Moscow State University. \u201cBy and large, impact factors and publications have zero impact here on grants and careers,\u201d he says. Konstantin Severinov, a young molecular biologist at the Institute of Molecular Genetics in Moscow who is also a tenured professor at Rutgers University in New Jersey, is one such scientist who has recently returned to Russia. He would like to transform the system from within \u2014 with the help of the next generation of Russian scientists. \u201cThe students here are just wonderful,\u201d he says, but he points to a worrying increase in state interference in scientists' lives (see  Nature   449 ,  122\u2013123  ; 2007). Scientists now have to report all contacts with foreign visitors to their institutes, for instance. Severinov was himself interviewed by the Federal Service Bureau in May about claims or rumours that DNA from Russians could be used to build some kind of 'genetic weapon'. In June, the government ordered that Russian DNA must no longer leave the country. But the order was revoked just two weeks later after an outcry by scientists and the media. Severinov adds that a newly emerging anti-Americanism, and widespread aversion to Western scientific culture and journal editors are also major obstacles on the road to reform. More money will not help without proper peer review and competition, he says. \u201cToo many people here think that grant writing is just a detraction from daydreaming and doing 'wonderful' science.\u201d Scientists, such as Severinov, who return to Russia, are still rare. A special federal programme for Human Resources in Science and Technology, aimed at attracting and enticing young people back to scientific careers, is to be launched in 2009. But Sobolev fears that Russian researchers will continue to go abroad, and stay there, because they have no chance of starting an independent and reasonably well-paid group in their home country. \u201cI am not very optimistic that things will improve soon,\u201d he says. \u201cThe scientific establishment is still opposed to true reform, and the next generation is absent. We can only hope that many young scientists will return one day and proceed to the political level.\u201d In the long term, he says, there is no way to escape the necessary changes: \u201cIsolation is not an option.\u201d See  Editorial  ,  News Feature  , and  Commentary  . Quirin Schiermeier is a reporter for  Nature  based in Munich. \n                     Russian science web focus \n                   \n                     Russian Academy of Sciences \n                   Reprints and Permissions"},
{"file_id": "449778a", "url": "https://www.nature.com/articles/449778a", "year": 2007, "authors": [{"name": "Nick Lane"}], "parsed_as_year": "2006_or_before", "body": "The nitrogen cycle rarely features in the grim litany of things at risk from global warming. Nick Lane reports on research that might change this ? with grave consequences for ocean chemistry. Colossal bridges bestride the waters of Narragansett Bay. Towns, interstate highways and suburbs sprawl along its shores and its waves are studded by thousands of pleasure boats. Yet the estuary retains a quiet beauty that befits the pious settlers who, 350 years ago, named the islands within the bay Prudence, Hope and Patience. Extending its briny fingers through much of Rhode Island, Narragansett Bay has long juxtaposed man and nature, pollution and purity. Industrialization in the nineteenth century led to the rapid growth of cities such as Providence, a port at the bay's north end. The area now supports about a million people, all flushing their waste into the pastoral watershed. Reactive nitrogen compounds from treated sewage, industrial waste and fertilizers have poured in for decades, but remained at a relatively constant level for the past 25 years. Nevertheless, set against this steady background, a silent microbial and biochemical transformation has occurred in the bay that could have devastating ecological effects. The cause is pollution, but of an indirect sort ? the changes seem to be down to global warming. For the past three decades, the bottom sediments of the bay have mopped up much of the reactive nitrogen that humans have dumped into it. Although the fraction sequestered had been falling, this valuable natural sink has been protecting the bay and the coastal oceans from the effects of nitrate runoff. But last year the sediments abruptly stopped performing this service. Worse than that, they went into reverse. In a single summer, the bay switched from being a net sink to a net source of nitrates 1 . Robinson ?Wally? Fulweiler, an oceanographer at Louisiana State University in Baton Rouge, and her colleagues at the University of Rhode Island in Kingston, were among the first to notice the turning environmental tide. If Narragansett is typical of other bays, they argue, it could be the harbinger of a new threat. Shifting the effect of anthropogenic nitrogen loading beyond the immediate coastal zone could destabilize ocean ecosystems by acidifying the waters, exacerbating harmful algal blooms, killing fish and shellfish, or perhaps even powering a vicious new cycle of global warming. The studies are currently hard to interpret and some say the system is poised to rebalance itself. But if they are wrong, global warming may do more to the oceans than make them rise. According to Fulweiler, the root of the problem is a disconnect between life in the water column and that in the bottom sediments ? the pelagic and benthic ecosystems, respectively. Under normal conditions, phytoplankton in the water column generate new organic matter through photosynthesis, some of which filters down to the bottom sediments ? the benthos ? where it provides food for bacteria. These benthic bacteria detoxify the water, removing excess nitrates, phosphates, and other pollutants while adding various micronutrients back to the water column. Healthy coastal ecosystems tend to have a good interchange ? a tight coupling ? between the pelagic and the benthic zones.  \n                A silent switch \n              In Narragansett, the pelagic ecosystem is failing. Primary productivity, as measured by chlorophyll concentration, has fallen by 40% during the past 30 years, reflecting a dwindling of the spring bloom of phytoplankton. Unlike harmful algal blooms ? in which excess nutrients such as nitrates provoke a strangling growth of weed-like algae, ultimately leaving the water full of dead, rotting matter ? normal seasonal blooms of phytoplankton are necessary to maintain the health of the estuary. The decline, Fulweiler says, could have been caused by warmer winters, either because they provide thicker cloud cover or because they allow more grazing zooplankton to flourish. Either way, rising temperatures are hitting primary productivity. Falling productivity means a decline in the quantity and quality of organic matter reaching benthic bacteria, notably the denitrifiers. Denitrifying bacteria convert nitrates back to inert nitrogen gas. Just as organic molecules from food are reacted with oxygen to generate energy in animals, these denitrifying bacteria glean energy when organic remains react with nitrate. The benthic denitrifiers are choosy eaters. They live on 'labile organic carbon', essentially, fresh food. A decline in primary productivity was likely to hit them hard. What came as a shock was the switch to a completely new population ? the nitrogen-fixing bacteria. These organisms take nitrogen dissolved in water and convert it into ammonia, in a process known as nitrogen fixation. This new organic nitrogen is ultimately converted into nitrates by a third group of bacteria, the nitrifiers. So the sediments not only stop mopping up excess nitrates, they start adding more to the pot (see  'The changing cycle' ). What flipped the switch is unknown. The nitrifying bacteria at the end of this chain have been shown to have unpredictable population patterns 2 . If that is the case here, then the change may not be significant. But a more troubling explanation relates to the composition of non-labile organic matter. According to Bess Ward, a biogeochemist at Princeton University in New Jersey, organic matter can become so depleted of nitrogen that it no longer provides enough sustenance for denitrifying bacteria. That stops their growth. Because nitrogen fixers don't face this constraint, they thrive. If Ward is right, then the switch could be both persistent and widespread. The overall equation is not trivial. Under normal circumstances, the sediments in Narragansett Bay decontaminate around one quarter of the reactive nitrogen compounds running off from farmland and sewage ? something in the order of 1,000 tonnes of nitrogen, or 5,000 tonnes of nitrate, every year ? making the failure of denitrification significant in its own right. Fulweiler notes that the excess nitrogen is not accumulating as dissolved nitrates in the bay, nor is it stimulating algal blooms. Presumably, she says, at least some is simply being flushed out to sea.  \n                An unbalanced equation \n             In addition to this substantial flux, in the three summer months of 2006, the rate of nitrogen fixation by the thriving nitrogen fixers was estimated to be around 1.5 times greater than the total input from rivers, sewage and atmospheric pollution combined ? nearly 3,000 tonnes of nitrogen converting into 12,000 tonnes of nitrate. Even allowing for more typical conditions throughout the rest of the year, this still represents 20?60% more nitrogen input annually. Last year, in Narragansett Bay, some 17,000 tonnes of nitrate went unaccounted for. So, what's happening to it all? A switch from denitrification to nitrogen fixation is utterly unexpected, which in itself shows just how much remains to be learned about the nitrogen cycle. Fulweiler notes that nitrogen-fixing bacteria in estuarine sediments have not been seen as important players in the nitrogen cycle before. But balancing the nitrogen cycle has been a bit of an embarrassment for years. It should be easy enough ? just sum up the total nitrate inputs, from natural and anthropogenic sources, and subtract the flux back to the atmosphere as nitrogen gas. Instead, it's a giant mismatch. The calculated rate of global denitrification is twice the known inputs from fixation and anthropogenic sources. As Lou Codispoti at the University of Maryland's Horn Point Laboratory (HPL) just outside Cambridge says, either the cycle doesn't balance at all ? that is, today's oceans are in some sort of a 'transient' state ? or scientists have overlooked a whole lot of nitrogen fixation somewhere. Most suspect the latter. And set against this background, Narragansett Bay might just represent a small fraction of that missing nitrogen. On the other hand, if global warming really does alter the nitrogen cycle, it could throw the global equation off balance. Fulweiler is the first to admit that a correlation is not proof of causality. Nevertheless, she's concerned with what might be a trend. There are hints from elsewhere that nitrogen fixation is picking up. A similar uncoupling of the pelagic and benthic ecosystems was noted in the arctic last year. Jackie Grebmeier at the University of Tennessee in Knoxville, and her collaborators reported 3  a 75% drop in benthic oxygen consumption (a surrogate for carbon supply) between 1988 and 2004. More recently, oceanographer Judy O'Neil at the HPL measured nitrogen fixation in tributaries to Chesapeake Bay, a few hundred miles south of Narragansett. Although she's yet to quantify their total contribution, her data incriminate cyanobacterial blooms, rather than benthic nitrogen-fixation for flipping the switch. The reason is that cyanobacterial blooms are notoriously full of toxins, which makes them unpalatable to grazing zooplankton, and the animals that eat them in turn. As a result, more carbon is flushed out in a dissolved state rather than passing down to the bottom sediments via faecal pellets. The trend appears new according to Ward, who, working with Todd Kana and his colleagues at the HPL, looked for evidence of nitrogen fixation in the Chesapeake 5 years ago; they found no activity. Later, working with Jon Zehr and others at the University of California, Santa Cruz, Ward found copies of the genes required for fixation in the sediments there 4 . What's more, they were associated with the most unusual suspects, such as proteobacteria, which have never before been associated with nitrogen fixation. Nitrogen fixation requires a suite of 10 to 20 genes; if lying fallow, these are costly to maintain and so ought to be lost. Ward says that if they're there, they're being used.  \n                Some like it hot \n             In the Chesapeake, too, the rise in nitrogen fixation probably relates to global warming, albeit for different reasons. Here there is no evidence of failing spring blooms, but rather a gradual takeover by cyanobacteria, which apparently ?like it hot?. Don Canfield, a geochemist at the University of Southern Denmark in Odense, says he thinks the trend is global. If he's right, nitrate export could rise substantially in a matter of a few years. Of course verifying the export of nitrogen hinges on balancing that stubborn nitrogen cycle equation. Paul Falkowski, a biogeochemist at Rutgers University's Institute of Marine and Coastal Sciences in New Brunswick, New Jersey, is sceptical about nitrogen export from Narragansett Bay. If ammonia and nitrates are being produced in such large quantities, he asks, then why are we seeing a steady decline in primary productivity, rather than an increase in blooms? And if the nitrates are all being flushed out into coastal waters, then the more buoyant, fresher water from the bay (with its river inputs) ought to float in the sunny surface waters above the denser saline, and we should see blooms there instead. Satellite images, he says, don't show dramatic blooms. Fulweiler has plausible answers, which need to be tested: in the bay itself, nitrates may be swallowed up by other bacteria competing for resources with phytoplankton in the water column. If so, then the export of nitrates to the oceans would be more limited, and the problem would revert back to one of local ecology. On the other hand if more nitrates really are exported out to sea, Fulweiler argues, they wouldn't necessarily stimulate large algal blooms. The salinity gap between Narragansett Bay and the ocean is quite small, she says, so the waters emerging from the bay might not float. In the absence of dramatic blooms, the water might instead be getting more acidic, according to Scott Doney and his collaborators at Woods Hole Oceanographic Institute in Massachusetts. Both nitrates and sulphates acidify ocean waters. The impact on the open ocean is limited, compared with the acidification caused by rising carbon dioxide levels, but Doney's marine modelling suggests that the effects of nitrates in coastal waters may be serious. The organisms most likely to be affected by ocean acidification are those with shells or skeletons made from calcium carbonate, including many algae that would otherwise bloom 5 .  \n                No laughing matter \n             Another possibility, if the waters emerging from Narragansett sink, is a bloom of denitrifying bacteria lower in the water column. Denitrification in coastal waters tends to be dominated by the classic bacterial pathway; and a major by-product of this is nitrous oxide ? laughing gas. Nitrous oxide is 200?300 times more potent as a greenhouse gas than carbon dioxide. In 2000, Wajih Naqvi and his colleagues at the National Institute of Oceanography in Goa, India, reported an alarming accumulation of nitrous oxide in the Arabian Sea, along the western Indian continental shelf, following a monsoon washout of nitrate fertilizers 6 . The calculated emissions from the region in just 6 months accounted for as much as 5% of annual ocean emissions from a region that makes up only 0.05% of the world's oceans. Although there is much to learn about the interplay of factors controlling oceanic nitrous oxide emissions, Naqvi says that emissions are generally greatest in oxygen-minimum zones. Because these are spreading as a result of global warming (oxygen is less soluble in warmer waters) and eutrophic conditions (an increase in chemical nutrients) nitrous oxide emissions will in all probability rise. Most observers anticipate that restrictions in the use of nitrates, along with better facilities for treating sewage and industrial waste, will reduce oceanic nitrogen contamination. But a widespread switch to nitrogen fixation, as a result of global warming, could raise nitrate levels and nitrous oxide emissions despite human intervention, driving a vicious cycle. Balancing the nitrogen cycle as the world warms suddenly looks more critical and more uncertain than ever. Nick Lane is a science writer and honorary reader at University College London. \n                     Climate and Water \n                   \n                     Focus on Climate Change \n                   \n                     Robinson Fulweiler's web page \n                   \n                     Horn Point Laboratory \n                   \n                     Woods Hole Oceanographic Institution harmful algae page \n                   Reprints and Permissions"},
{"file_id": "449968a", "url": "https://www.nature.com/articles/449968a", "year": 2007, "authors": [{"name": "Rex Dalton"}], "parsed_as_year": "2006_or_before", "body": "Under the rubble of war-torn Afghanistan lie natural resources worth billions. Rex Dalton reports from Kabul on the scientists risking their lives to see them developed for the good of the country. In a canyon just outside Kabul, the rocky terrain is strewn with debris symbolizing the troubled past and tenuous future of war-torn Afghanistan. Exploratory cores, drilled decades ago by Soviets probing for minerals, are scattered across a landscape peppered with landmines. A line of bomb craters crosses the basin, which was home to a terrorist training camp until late 2001, when US B-52s swept overhead, dropping bunker-busters in retaliation for the terrorist attacks of 11 September. Among other things, the Americans destroyed a building that had been used to store geological cores, later turned into an ammunitions dump. Below this rubble lies a potential economic and social boon for the troubled nation ? a massive copper deposit estimated to be worth US$30 billion at today's high prices. The deposit, called Aynak, has never been developed into a viable mine, but international corporations are now competing to win a major mining concession there. What happens at Aynak could eventually serve as a model for developing Afghanistan's other natural resources, ranging from mineral wealth to reserves of coal and petroleum. But concerns about the Aynak bidding process have set off a behind-the-scenes scramble among consulting scientists, diplomats and aid agency officials to try to ensure its success. In June, the top World Bank geological consultant to the Afghanistan government sent a report to the office of President Hamid Karzai that sharply criticized how the country's ministry of mines was handling the competition. The consultant, James Yeager, called for new analysis of the bids, more emphasis on social and economic benefits and a stronger analytical role by the inter-ministerial council that administers the process. The government is soon expected to announce two finalists for the concession, narrowing the field from the current five. Because he raised the alarm, Yeager thinks that he was targeted for assassination. A capped beer bottle of hydrochloric acid was slipped into the refrigerator of his heavily guarded apartment in Kabul; he stopped just short of drinking it. Yeager did not renew his World Bank contract and instead returned home to Denver, Colorado, joining numerous other consulting scientists leaving Afghanistan at a time when their experience is sorely needed. Meanwhile, researchers who remain there face a range of threats, from kidnapping to landmines to booby traps. Nevertheless, some are optimistic that Afghanistan's natural resources can be developed in a stable and sustainable manner. Officials at the World Bank, for instance, say that the inter-ministerial council has started to strengthen its role in the Aynak bidding process by asserting power over the ministry of mines and shifting the competition onto a steadier course. Using the phrase that has been a byword for conflict in the region since the days of Rudyard Kipling's  Kim , World Bank mining engineer Michael Stanley says: ?We are into a new phase of the Great Game.? Afghanistan is a key player in the game because of its panoply of geological riches, created as the Indian subcontinent rams into Asia, and thrust into the air and exposed in the Hindu Kush mountain range. Coal, rare industrial metals and precious stones abound at various points along the range. The northern provinces of the country also have oil and gas reserves.  \n                Back to work \n             For centuries, Aynak has been known for its copper, used for weapons, tools and trade along the Silk Road. Before withdrawing from Afghanistan in 1989, the Soviets drilled countless cores to assess the deposit, now estimated to hold 240 million tonnes of the metal. Work halted for years during Taliban rule, but after 2001, reconstruction teams started to identify the country's assets. In the United States, the Bush administration encouraged Afghan expatriates to help develop their homeland; some scientists who had fled the country returned (see 'Science after the Taliban'). Work in post-Taliban Afghanistan wasn't easy. In Kabul, the Afghanistan Geological Survey building had been reduced to a shell, pockmarked by rocket blasts. Its equipment, samples and library were destroyed; anything burnable had been used for fuel. US officials helped to reconstruct the building, spending at least $6.2 million to modernize the facilities with computers, labs, sample storage racks and a library housing old, rare and sometimes bullet-scarred reference volumes. The UK government also chipped in with US$8 million and a three-year contract for scientists from the British Geological Survey (BGS) to analyse natural resources. Although Aynak is only about 35 kilometres southeast of Kabul, the BGS scientists were forbidden to go there without military protection. Even when they did manage to get there, they couldn't sample for minerals in the scattered cores. The cores had been blasted apart by bombs aimed at the old mining tunnels, which had been suspected of housing Osama bin Laden. Back in Kabul, the team managed to patch together a detailed picture of the copper deposit from surviving Soviet core reports. Courageous staff from the Afghanistan Geological Survey had hidden the 20-year-old documents during the Taliban regime. ?The Taliban would have killed them if they had found the reports,? says Antony Benham, a mineral specialist at the BGS. Working with these formerly hidden records, BGS scientists plugged in data from the cores to create a computerized model of copper distribution at Aynak. ?It was a remarkable job,? says geologist Richard Ellison, the official in charge of the agency's contract, which ended on 1 September. The BGS is now negotiating for a new contract with the World Bank.  \n                Power problems \n              The firm that eventually wins the Aynak concession will face many difficult tasks, but perhaps the most daunting will be to secure electricity for mining and smelting equipment. The villages around Aynak have only generators as a source of power, and building an enormous copper production facility will require lots of power from coal-fired plants. But before hundreds of millions of dollars are invested in power plants and mining facilities, coal supplies must be located, assessed and graded for development. To evaluate coal resources in Afghanistan, the US Geological Survey (USGS) sent in a task force led by geologist John SanFilipo. SanFilipo had earned an international reputation for assessing coal in dangerous environments, particularly in adjacent Pakistan. There, he discovered one of Asia's largest coal reserves, the Thar deposit in the Sindh province of southeastern Pakistan. But much of Thar's coal is difficult to mine because of political difficulties in the country. SanFilipo and his colleagues faced similar challenges in assessing Afghanistan's coal resources. The Soviets and officials from the Afghanistan Geological Survey had previously found a massive coal band running across much of northern Afghanistan, with mining centred around the town of Pul-I-Khumri, northwest of Kabul. Another known coal band runs along the country's southeastern border with Pakistan, in the Katawaz Basin. Historically, this coal has been tapped by artisanal methods ? in mines called 'dog holes', dug by locals who use donkeys for underground hauling. With little shoring or proper ventilation, the tunnels regularly collapse, killing villagers. In these warrens, the geologists went coal-hunting, sometimes using old ways. Yeager, for instance, carried a bird to test air quality. ?When the bird died, I left,? he says. Details on the locations of the coal deposit were extremely sketchy, and map conditions worse. ?The Afghans had squirrelled the maps away during the Taliban days,? says SanFilipo. ?When I got there, they had brought them back to the Afghanistan Survey building, where they were piled like junk. We set out to organize, scan and digitize them for a permanent record.? The quality of Afghanistan's coal deposits varies greatly. Some contain coal that burns hot and clean; other coals are more problematic, rich in sulphur and fluorine, and emitting noxious gases when burnt. Just north of Aynak lies a coal deposit called Chalaw. The coal there could fire a power plant for the Aynak mine, officials say, but it is rich in fluorine ? which requires added measures to limit pollution from power plants, and protective venting if burned inside houses. Another challenge is to find coal that is not buried so deep that it can't be extracted. ?The critical step is to determine where coal is not at the surface, but is still easily minable,? says SanFilipo. Geologists thus search for a relatively flat area where coal is just below a weathered surface. Dating the coal is also crucial, because coals of the same age will tend to be of the same quality. Older geological maps show coal reserves that might have been dated incorrectly. ?We want to create a stratigraphic picture of coal deposits across the entire country,? says SanFilipo. The USGS team uses palaeobotanical clues such as pollen to date the coal. It relies on scientists such as Rahman Ashraf, a palaeobotanist who fled Afghanistan after the Soviet invasion but now serves as special adviser to President Karzai. ?It was a dream that I could return to work in my country,? says Ashraf, who has also been appointed chancellor of Kabul University.  \n                Road blocks \n              The power lines of Afghan politics run through nearly all attempts to characterize the country's natural resources. In one case, sources allege that SanFilipo was blocked from returning to Afghanistan for coal exploration by the actions of another USGS geologist ? Afghanistan-born Said Mirzad. Originally trained in France, Mirzad was director of the Afghanistan Geological Survey before the Soviet invasion in 1979. After that he ran computer services for a small USGS office in San Diego, California. After the terrorist attacks of 11 September, Mirzad's Afghan friendships vaulted him to the USGS headquarters in Reston, Virginia, to help coordinate resource development in Afghanistan. Mirzad has deep and historic connections in Afghanistan, where his brother-in-law is the minister of defence. Mirzad is also the mentor of the minister of mines, Mohamad Ibrahim Adel, who was one of those criticized for the handling of the Aynak copper bidding competition. And Mirzad has powerful allies in Washington DC; both the US state and defence departments awarded him medals for outstanding service in 2005. In Afghanistan, Mirzad has aided multiple projects, such as an airborne geological assessment he urged the Karzai government to fund after aid agencies declined. But some also see him as an obstructionist. Beginning in early 2005, SanFilipo attempted unsuccessfully to return to Afghanistan to continue his fieldwork and geological map inventory. His repeated requests to US officials in Kabul for clearance to return were denied, keeping him out of the country for 15 months. He was finally allowed to return three times in 2006, but not since then. ?A geologist must go out in the field to see,? says Ashraf, praising Yeager and SanFilipo's expeditions. Sources say that denials for SanFilipo's travel to Afghanistan were traced to Mirzad, who was in Kabul advising Zalmay Khalilzad, then the US ambassador to Afghanistan. Khalilzad is arguably the Bush administration's most-favoured Afghan and has since been appointed as the US ambassador to the United Nations. Mirzad's historic friendships also extend to the presidential palace in Afghanistan: he used to play bridge with President Karzai's father. Mirzad, though, denies hindering SanFilipo's work in any way. ?This is all gossip,? he says. ?There is not a shred of evidence.? But neither he nor the USGS officials could explain why SanFilipo was refused access to Afghanistan during the time in question. In October 2006, SanFilipo lectured at the annual meeting of the Geological Society of America in Philadelphia, Pennsylvania, on the poor state of mining in Afghanistan. Not long afterwards, he was removed as the project leader for the USGS effort. Since the meeting, he has declined to discuss the issue publicly. These events set back coal exploration in Afghanistan substantially, say several sources in Afghanistan and the United States, who requested anonymity so they may continue to help the country without reprisals. ?It is unforgivable what has happened, a disaster,? says Mary Louise Vitelli, a US attorney in Kabul who has worked extensively in war-torn regions. ?Guys like SanFilipo are rare; he produces quality analysis under difficult circumstances.? And some scientists with long-term experience in the subcontinent saw the tapping of Mirzad for a reconstruction role as counterproductive ? as were other selections by the Bush administration in Afghanistan and Iraq. Jack Shroder, a geologist at the University of Nebraska in Omaha, has worked in Afghanistan for 35 years, conducting glacial, mapping and global-positioning-system studies. He has been integrally involved in the American Institute of Afghanistan Studies, a multidisciplinary organization to foster research. But Shroder says that he and his fellow institute leaders were never consulted about the Bush administration's science policy for Afghanistan. ?We were the boots-on-the ground guys ? in and out of Afghanistan before the terrorist attacks,? he says. ?They completely ignored us; they think academics are all left-wingers.? Shroder also says that he has repeatedly encountered difficulties dealing with Mirzad, whom he calls a hard-core nationalist. ?He didn't want foreigners to get access to maps, even if they were helping,? says Shroder. But Mirzad expressed surprise that he would be seen as an obstructionist. ?I believe the only thing that can save Afghanistan is its indigenous wealth. I am completely behind that,? he says. USGS managers of international programmes, such as Asia project chief Jack Medlin, praise Mirzad for fighting to secure funds for the agency to work in Afghanistan. Even so, the USGS wanted $12 million a year for five years to develop resources in Afghanistan, but scrapes by with about $9 million a year. On 13 November, the USGS is scheduled to release a status report on minerals in Afghanistan, after some delay. The main coal report isn't to be released until next year, albeit short of data as few USGS scientists have gone to Afghanistan this year. Expatriate Afghan geologist Shah Wali Faryad, now of Charles University in Prague, repeatedly invited USGS scientists to attend a conference on geological opportunities on 15?16 October in Kabul, but the agency didn't respond. Medlin cites security issues as the reason.  \n                Competitive streak \n              As the coal debacle simmers in the background, bigger questions arise about the Aynak copper project. Nine corporations originally sought the concession, which includes an option on the nearby Darband deposit. By June, the field of contenders had been narrowed to five firms, all mining heavyweights: Strikeforce, part of Russia's largest private employer, the Basic Element Group; China Metallurgical Group, a Chinese government-owned conglomerate based in Beijing; London-based Kazakhmys Consortium, which mines and processes copper in Kazakhstan; Hunter Dickinson of Vancouver, Canada, which mines minerals internationally; and Phelps Dodge, a leading US copper mining firm based in Phoenix, Arizona. An informed source says that a few months ago the favourite of the ministry of mines' technical group was the China Metallurgical Group, with Hunter Dickinson a distant second. In his critique of the process, Yeager wrote that Afghan expertise wasn't being used to its fullest extent, and that officials controlled by Adel, the mining minister, had too much influence in the process. No economists, attorneys, environmentalists or foreign-affairs specialists had been involved in the technical analysis, he asserted, which violates the laws Afghanistan implemented after the Taliban were ousted. Yeager also noted the importance of the bidders' track records: the top-ranked company has come under fire for poor environmental records in mining in nations other than its native China. Yeager also contends that the strategic implications of selecting either an Eastern or Western firm have not been addressed. If Afghanistan were to choose a Russian, Kazakh or Chinese bid, Yeager wrote, firms from Western nations might not seek other mineral concessions in the region in the future, fearing that Afghanistan's neighbours may have undue influence. But Adel counters that the tender bids have been ?very strong, and everyone is happy with the progress.? He adds that he has not seen Yeager's report, but considered its transmission to Karzai's office ?a breach? of the adviser's duties. ?He is not directly responsible for the bidding,? says Adel. For environmental specialist Daud Saba, a human development adviser to President Karzai, the difficulties with Aynak have been particularly painful. Developing such a rich natural resource should be spearheaded by the country's leading scientists, he feels. ?It breaks my heart when I see what is happening,? he says. And unless Afghanistan puts resource development on a steady course, many more hearts may also be broken by the opportunities lost. Rex Dalton is a US West Coast correspondent for  Nature . This article is part of the Global Theme on Poverty and Human Development, organized by the Council of Science Editors. All articles from the Nature Publishing Group are available free at  http://www.nature.com/povhumdev  The content from all participating journals can be found at  http://www.councilscienceeditors.org/globalthemeissue.cfm \n                     Afghanistan Geological Survey \n                   \n                     US Geological Survey in Afghanistan \n                   \n                     Centre for Policy and Human Development \n                   \n                     American Institute of Afghanistan Studies \n                   Reprints and Permissions"},
{"file_id": "449964a", "url": "https://www.nature.com/articles/449964a", "year": 2007, "authors": [{"name": "Daemon Fairless"}], "parsed_as_year": "2006_or_before", "body": "M. S. Swaminathan transformed agriculture in India in the 1960s. Now Daemon Fairless finds him at the heart of another high-tech scheme to help the rural poor. Monkombu Sambasivan Swaminathan has trouble making his way across a crowded conference room, not because, at 82, he walks with a slight stoop and an even slighter shuffle, but because he is intercepted at every step by a handshake and a request for a snapshot and an autograph. It is the kind of adulation normally reserved for Bollywood celebrities, not plant geneticists. But, as the father of India's Green Revolution, Swaminathan holds a revered spot in the national pantheon of public figures. Swaminathan is credited with introducing new varieties of high-yield wheat to India during the 1960s and 1970s, catapulting the country from dependence on foreign-grain shipments to food independence within a few years. With about 60% of India's population employed in agriculture, it is difficult to overstate the significance of Swaminathan's contribution to his country. It is not unusual for former doctoral students to prostrate themselves before 'The Professor' and touch his feet ? a sign of respect unparalleled in Western academia. Despite this, Swaminathan shows no sign of resting on his laurels; instead he uses his position to further rural development in new ways. Over the past decade he has become the driving force behind a different revolution ? a national movement to bring Internet and telecommunications to each of India's 600,000 rural villages. By building on existing networks of village telecom kiosks, including 80 kiosks set up by his non-profit institute, the M. S. Swaminathan Research Foundation (MSSRF), Swaminathan hopes to create, with government and business support, enough kiosks to serve one in six villages. Swaminathan's belief is that information and communications technologies (ICTs), if properly implemented, will help bridge India's growing urban?rural divide and forge better links between researchers and rural poor people. ?Those of us working in agriculture, health and environmental sciences,? he says, ?have a moral and ethical responsibility to ensure that we not only do good work, but more importantly, that that work reaches the people for whom it is intended.? The growing disparity between India's urban economy, with its white-hot annual growth rate of around 9%, and its sagging rural economy yoked with massive unemployment, is of profound concern. India, with more than 1.1 billion people, remains the country with the largest number of poor people, 70% of whom live in rural areas. Moreover, the percentage of gross domestic product the government spends on rural infrastructure has been steadily declining since the late 1980s. According to the World Bank, improving the accessibility and quality of education, health care and basic infrastructure such as water, electricity, sanitation and roads are among India's biggest challenges. How will ICT help? A significant barrier to rural development in India is that, although the government has launched multiple development schemes, each designed to aid rural poor people, they are spread across so many different departments that even a seasoned bureaucrat would have trouble keeping track of them, let alone an illiterate farmer. Basheerhamad Shadrach, Asia programme officer of a company called telecentre.org and secretary of the MSSRF's national ICT project, says that rural economic progress is hindered by a disconnect between farmers and researchers. ?Agricultural extension workers,? says Shadrach, ?are supposed to go into the field every day and seek out the needs of farmers. But the moment they become government workers, their job is guaranteed; they simply aren't motivated enough.? He says that the same is true of government employees whose job it is to provide rural health care, education and basic municipal services such as sanitation. The hope, says Shadrach, is that ICT will provide ?a fresh approach? to agricultural extension, putting the information directly in the farmer's hands. Swaminathan sees the same attitudes today that he faced during the 1960s, as a researcher with the Indian Agricultural Research Institute (IARI). In India, he says, ?there is a reluctance to pass on authority from the bureaucracy to the local representatives at the grass-roots level?. In the mid-1960s, eager to turn India's grain crisis around, Swaminathan bypassed the orthodox procedure of growing novel varieties in a controlled environment for several years before handing the seeds over to agricultural extension workers who would, in turn, instruct the farmers on how to cultivate them. Instead, he headed straight for the farmers' fields and convinced them to become collaborators. ?Some of my colleagues would say: 'why are you going to the villages? That is the duty of the extension officer.'? he recalls. ?I didn't believe in that. I felt that it was my responsibility.? Between 1964 and 1966, Swaminathan, his colleagues at the IARI and local small-scale farmers planted more than 1,000 crops on farmland around Delhi and in adjoining states. The harvest was, on average, nearly 300% higher than traditional wheat varieties. ?It had an electric effect on the farmers,? says Swaminathan. He is hopeful that ICT will now propagate similarly throughout the country ? creating a groundswell of demand for the technology.  \n                Mixed success \n              This sounds good in principle, and no one doubts Swaminathan's enthusiasm or ability to secure political backing, but India's past experience with rural ICT schemes has been rife with disappointments. Ashok Jhunjhunwala, head of the Telecom and Networks Group (TeNeT) at the Indian Institute of Technology in Chennai, has been working to bring ICTs to rural India for more than 15 years. Jhunjhunwala says that the dozens of ICT projects across the country are a series of ongoing experiments, ?some of which have worked?, he says, but ?most of which haven't?. ?You'll hear about a village where ICTs have helped farmers get a better price for grain, or a village where someone has got better access to health care, but these are all anecdotal cases and don't represent the majority of ICT projects,? says Jhunjhunwala. For one of its biggest projects, Jhunjhunwala's group helped to launch n-Logue, a Chennai-based company that set up about 3,500 Internet-kiosk franchises starting in 2001. ?Half of them are now closed,? says Jhunjhunwala, ?The other half are only partially functioning.? The root of the problem, he says, is that although n-Logue provides the franchisee with the equipment and training needed to run an ICT kiosk, there aren't enough services to create viable market demand. Most franchises close because they don't get adequate return on their investment. And many kiosk operators, having received computer training, end up leaving their village to seek fortunes in the cities. Jhunjhunwala says that n-Logue's experience is not unusual. Indeed, a 2006 study by the United Nations Development Programme of 18 ICT projects in India ? representing some 6,500 ICT kiosks across 10 states ? found that many faltered because they didn't address rural needs. The study found that several of the projects failed to ?understand the importance of cultivating close relationships with their beneficiary community?. In other words, the projects failed to listen to the villagers. Despite these failures, Jhunjhunwala is optimistic that ICTs can eventually help. His group has had some success with a distance-education programme in 70 villages over the past 2 years. The programme more than doubled the number of rural children who passed the exams they take at 15 years old. ?It was a marvellous success.? But, he notes, scaling up to a national level is a different problem altogether. ?This was for 60 or 70 villages,? he says. ?How do you make it happen in 100,000?? This is the challenge that Swaminathan has taken on. In the late-1990s, his institute set up some of the first telecom kiosks in his home state of Tamil Nadu, with the goal of linking farmers and fishermen to the basic information they need. After a rocky start, which saw the first 4 centres close, the MSSRF's network has now grown to a total of 80 kiosks across 3 states. In 2004, Swaminathan rallied the ICT troops, creating what he calls a National Alliance, a coalition of more than 400 organizations, including state governments and various business, academic and non-governmental organizations, with the collective aim of providing ICT access to every villager in India. Nationally, there are now more than 20,000 ICT kiosks operating in nearly all of India's 28 states, run by several dozen ICT providers, and that number is set to double by December of this year. The alliance has been enormously successful in getting government support for ICT infrastructure. In addition to US$420 million that central and state governments are pledging towards the physical infrastructure for 100,000 telecom kiosks by 2008, roughly $850 million is also being invested to bring broadband connectivity to administrative groups of villages. They have also approved close to $565 million for the creation of state data centres as hubs for government services. It is an ambitious project considering that 80,000 villages are still without electricity and 65,000 villages have no telephone line.  \n                People power \n             Swaminathan says that although he is encouraged by the government's commitment to rural ICTs, he is concerned that the plan is overly focused on providing equipment and physical infrastructure. As part of their more people-centred approach, the MSSRF established the National Virtual Academy (NVA) in 2003, as a distance-learning program for training villagers to become advocates for the ICT needs of their community. Over the past 4 years, the NVA has recruited more than 1,000 such villagers ? 'NVA fellows' ? each nominated by their peers and each with a track record of community service. Kandeepan Selvarani, an NVA fellow from Embalam, a farming village with a population of 4,500, not far from the city of Puducherry, says that she is becoming recognized as a local problem-solver. Recently, she was approached by a cashew farmer who feared he would lose his harvest because his trees were losing their flowers. Selvarani visited the village telecom kiosk ? one of the first the MSSRF set up ? and managed to track down an agricultural scientist who taught Selvarani how to prepare panchakavya ? a traditional biopesticide comprised of cow-derived products: dung, urine and milk, as well as curd and ghee. ?The farmer sprayed it on his trees,? says Selvarani, ?and the crop was saved.? The Embalam kiosk is a barren, concrete room with four ageing computers and a broadband connection. Yet the kiosk manager, Indra Gandhi, says that in the 8 years since it opened, her community has seen substantial changes. Before, most of the villagers were unaware of the various government support programmes for small farmers and fishermen. But now, she says, ?everyone in the village knows about them?. Farmers regularly come to the centre to get information on livestock management and crop diseases or pests. And the centre has helped more than 50 village cooperatives to apply for microfinancing loans.  \n                The hub of the matter \n              A few kilometres away, the MSSRF Puducherry Village Resource Centre serves as a coordinating hub for eight village kiosks, including the one in Embalam, relaying the needs of villagers such as Selvarani to various experts and government institutions. Importantly, says Anburaj Thiagarajane, the centre's director, all of their activities ? including a local-language community newspaper, daily weather reports and regular workshops ? are determined by a collaboration between villagers, NVA Fellows and the centre's staff, who are all from local communities. The Puducherry Centre is one of 15 such hubs run by the MSSRF, each of which links with government institutions, universities and businesses, and with one another, via a satellite connection donated by the Indian Space Research Organization in Bangalore. But focusing on the high-tech component of the project, says Thiagarajane, is missing the point. ?The linkage between people is the most important part of an ICT programme, not the technology. If you really want the information to reach villages, you have to have people who are capable of taking it there.? This is especially true for the most ambitious aspect of the national ICT project. Government funding for the telecom centres runs out in 4 years, so to continue operating, the kiosks must secure private investment to ensure that each kiosk will be a self-sustaining public?private outlet. Jhunjhunwala doubts that the government will achieve the national goal of building 100,000 telecom kiosks across the country by 2008, let alone achieving sustainability by 2010. So far, the government has attracted the interest of several large companies, including the Mumbai-based telecoms giant Reliance Communications, each of which has placed bids for government funds to operate several thousand kiosks. Jhunjhunwala is sceptical that these companies will be able to make a success of it, even though there are businesses running profitable rural ICT centres. The Imperial Tobacco Company of India, one of the country's largest private corporations, selling products from cigarettes and clothing to fertilizers and pesticides, has established 6,500 kiosks that it says serve 38,500 villages in 9 states. The company built its first Internet kiosks in 2000 to buy grain directly from farmers. The kiosks provide farmers with market prices so they can decide when best to sell their harvest, and they sell directly to the company for an immediate cash payment. Imperial Tobacco says its system has reduced its procurement costs by 25?30% and claims to put more money in farmers' pockets. The company's kiosks are not part of the National Alliance, and it is likely that in India's fast-moving telecom sector, the government-funded infrastructure will be leapfrogged by new technologies. Tata Consultancy Services in Mumbai, India's largest computer software exporter, is developing mobile-phone software for farmers. India boasts the fastest-growing mobile-phone market in the world. One-fifth of its 218 million mobile-phone users live in rural areas and the country's service providers are rapidly expanding wireless coverage to villages. Arun Pande, who heads Tata's Innovation Labs in Mumbai, says the company has developed mobile-phone applications that give farmers a local 7-day weather forecast, pesticide and fertilizer advice and crop prices at nearby markets, in their local language. ?When we talked to farmers,? he says, ?we realized that their questions were very simple and also extremely specific to the conditions in their field.? Pande says that Tata has paired up with the MSSRF to launch trials in four farming villages in Maharashtra and Uttar Pradesh. He says the business model for such mobile-based services hasn't been developed yet but he sees them working alongside village kiosks. Swaminathan welcomes these new initiatives. He says that he feels they complement the national movement. ?Let many flowers bloom,? he says, ?whichever one works, great.? After all, rural India still has more problems than solutions. According to O. P. Bhatt, chair of the State Bank of India, traditional banking is limited by having only 20,000 bank branches in rural areas, where 70% of India's population reside. He is keen to see the ICT kiosks used to help villagers obtain basic financial services, particularly microfinancing. Bhatt says that unless the disparity between the rural poor and urban nouveau riche is remedied, India's economic success will be short-lived. He points to the growing influence of the Naxalite-communist groups, and recent terrorist attacks, as examples of violent responses to social inequity: ?It can and will lead to social strife and political backlash,? he warns. Daemon Fairless is this year's winner of the IDRC-Nature fellowship. This article is part of the Global Theme on Poverty and Human Development, organized by the Council of Science Editors. All articles from the Nature Publishing Group are available free at  http://www.nature.com/povhumdev  The content from all participating journals can be found at  http://www.councilscienceeditors.org/globalthemeissue.cfm \n                     Nature Outlook: India \n                   \n                     Indian Department of Information Technology's e-Governance site \n                   \n                     The Telecommunications and Computer Networking Group (TeNeT) \n                   Reprints and Permissions"},
{"file_id": "448530a", "url": "https://www.nature.com/articles/448530a", "year": 2007, "authors": [{"name": "Heidi Ledford"}], "parsed_as_year": "2006_or_before", "body": "The ethics committees that oversee research done in humans have been attacked from all sides. Heidi Ledford recounts the struggle to come up with alternatives. Fourteen years of treating people with tuberculosis has taught physician William Burman what to expect when a patient walks through his door. Tuberculosis is not typically a disease of the well-heeled. Many patients in the United States are foreign born. English is their second language. Fewer than half have completed a high-school education, and many have spent time in jails or homeless shelters. So when Burman, of the University of Colorado in Denver, joined in two studies run by the Tuberculosis Trials Consortium, he knew that the consent forms needed to cater to people with an eighth-grade reading level (comprehensible to an educated 13-year-old). The trials involved multiple institutions, and the forms were sent to 39 institutional review boards (IRBs) \u2014 committees designed to determine whether a proposed experiment is ethically sound. The final approvals came in 346 days later, but what the IRBs sent back, Burman found disturbing. \u201cThe consent forms were longer. The language was more complex,\u201d Burman says. \u201cAnd errors were inserted at a surprising frequency.\u201d In one case, a potential negative side effect of the treatment had been accidentally edited out. Burman responded to the problem as any researcher would: he studied it. He had an independent panel review the changes. The reviewers found that 85% of the changes did not affect the meaning of the consent forms, but that the average reading level had jumped from that of an eighth grader to that of a twelfth grader (around 17 years old) 1 . His results confirmed something he'd suspected for some time. \u201cI started to think about what was happening and it just seemed like the system was flawed.\u201d It was time to change the system. Burman is not alone. In the 40 years since their birth (see ' Time for ethics '), IRBs, also known as research ethics committees, have faced criticism from all sides. They're too slow, or too hasty, overprotective, or they flout basic safety. They're bureaucratic, wasteful and unavoidable. So, what are researchers to do? The will for change exists, says Sarah Greene, a researcher at the Group Health Center for Health Studies in Seattle, Washington. But recent attempts to fix the system have struggled to gain a foothold. \n               Obstacle course \n             In many countries, a complex network of local ethics committees handles the approval of research on humans. This focus on local resources allows committees to account for specific laws or cultural concerns in a particular region. But it leads to problems in multicentre trials, such as Burman's, which are becoming more frequent. When IRBs were first founded, multicentre trials were almost unheard of. A 1998 report 2  from the inspector-general of the US Department of Health and Human Services in Washington DC stated that a rise in the number of multicentre studies was throwing the system into crisis. And a recent analysis 3  showed that five of 20 trials seeking IRB approval reported significant delays as a result of IRB negotiations. Seventeen noted inconsistencies both in IRBs' review process and in their recommendations. In one case, negotiations between 65 IRBs delayed the study by a year. More frighteningly, the cumbersome system could even endanger the health of the studies' participants. The higher the hurdles \u2014 and the more unfair they seem \u2014 the less inclined researchers will be to jump them. \u201cIt's slow and frustrating to researchers,\u201d says Ezekiel Emanuel, chair of the US Department of Bioethics at the National Institutes of Health in Bethesda, Maryland. Researchers have reported that they are more likely to violate the regulations set by ethics committee if they feel that they or their application have been mishandled 4 . And even though IRBs are made up mostly of volunteers, they are expensive to run. In 2002, the median cost of running an IRB, taking into account the time spent by IRB members, was $742,000; the maximum was over $4 million 5 . So, for every protocol they assess, they charge a fee to cover support staff, facilities, and outside consulting. These fees are typically pulled from grants as part of the institutional overhead, or as direct charges to commercial sponsors, and they average just over US$1000 (ref.  5 ). A proposed solution to the copious problems with IRBs is outsourcing, especially for multicentre studies, to some form of centralized review. That movement has met with resistance from those who say that local review provides valuable local context. But Burman counters that local context had little bearing on the changes in his consent forms. In his trials, only 1.5% of the tweaks to the consent forms were made to account for local context 1 . \n               Risky process \n             But that's not enough to rule out the importance of local review argues David Wynes, vice-president for research administration at Emory University in Atlanta, Georgia. \u201cI agree that the vast majority of changes are editorial, but I think there's a value in an institution having a process for identifying when local context is an issue,\u201d he says. \u201cYou might have to review a hundred protocols before you can see the value of local context. Is it OK if only 1% of the time you put subjects at risk?\u201d Burman argues that expertise with a specific patient group or disease should trump local context from detached review boards. \u201cThe local IRBs don't know the patients I take care of, because if they did, the last thing they would do is increase the length of the consent form and make the language more complex,\u201d he says. Instead, Burman and his colleagues have worked to create a designated panel at the Centers for Disease Control and Prevention (CDC) in Atlanta, Georgia, which keeps track of disease epidemiology in the United States, to review all tuberculosis studies. \n               Top trumps \n             But a centralized system will work only if the local boards are not allowed to overrule the decision of the central board, says Emanuel. It's a lesson, he adds, that the United Kingdom has had to learn the hard way. In 1997, the United Kingdom created a system of regional review boards in which trials needed approval from just one board to proceed. In 2000, the system was brought under the auspices of the Central Office of Research Ethics Committees (COREC), based in London. The trouble was, local ethics committees refused to surrender control, and instead of expediting review, COREC had created a new layer of bureaucracy. \u201cResearchers were very upset with the way things were going,\u201d says Emma Cave, a lecturer at the Leeds School of Law, UK. \u201cThey thought the regulations were making the United Kingdom a bad place for research.\u201d In April, Britain dissolved COREC in favour of the new National Research Ethics Service, and changed the regulations to restrict the ability of local ethics committees to change the protocol approved by the national office. In 2001, the US National Cancer Institute (NCI) in Bethesda, Maryland, launched a similar experiment \u2014 a central review board to review all NCI-funded research on humans. Local review boards retained the power to do a full review, but could opt instead for an expedited review in which they merely adjust for local context. (The NCI formed a similar review board for paediatric studies in 2004.) The project immediately ran into trouble. The central board spent too much effort on scientifically reviewing proposals that had already been reviewed by the granting arm of NCI, says Richard Schilsky, chairman of Cancer and Leukemia Group B, an NCI-sponsored cancer clinical trials group. And at first, few local IRBs were willing to cede control to the central review board. \u201cThe concept is good,\u201d says Schilsky, \u201cbut the devil has been in the details of the implementation.\u201d Since 2001, the number of participating institutions has climbed to 300. More than half of those have accepted the reviews of NCI's central board; the remainder are still developing ways to incorporate the central boards review into their own review process.. But Schilsky says that only about 20% of the institutions involved in his clinical-trials group \u2014 the largest in the United States \u2014 have signed on. The result was similar to what happened in Britain, adding to the bureaucracy. Schilsky estimates that the system has added two to three months to the time it takes to activate a new study. Lainie Ross, a paediatrician and member of the IRB at the University of Chicago Medical School, says that she is opposed to surrendering local control. \u201cA national IRB could fail to recognize different needs of different communities. I'm not just going to accept someone else's word for it.\u201d Without fail, IRB members interviewed by  Nature  who were opposed to ceding control to a centralized board cited concerns about patient safety as their main reason. But Emanuel, who has also served on an IRB, says that there's another cause for concern. \u201cThere are no good data suggesting that there are local factors that are ethically relevant,\u201d says Emanuel. \u201cIt's really liability that's driving this.\u201d \n               Vulnerable populations \n             Liability is a thorny issue for local IRBs contemplating handing over control to NCI's central IRB, says Wynes. If a participant in a clinical trial felt that he or she were unjustly harmed during the course of the research, they could not hold the NCI legally responsible because it is a branch of the federal government. That leaves the local IRB legally vulnerable, says Clint Hermes, general council at St. Jude Children's Research Hospital in Memphis, Tennessee. It is rare, but IRBs and even individual IRB members have been sued in the past. Bioethicist Arthur Caplan at the University of Pennsylvania in Philadelphia says that he has served on two IRBs that were sued but still thinks it's important to have a mechanism in place to hold negligent IRBs accountable. Meanwhile, a profitable industry in private, commercial IRBs has sprung up. Although commercial IRBs can provide a sense of security by assuming legal liability, the institution doing the experiment will bear ultimate responsibility. But partial indemnity seems sufficient to comfort many researchers: commercial IRBs serve hundreds of companies, hospitals and research institutions. In 2005, the consulting firm Deloitte named Chesapeake Research Review of Columbia, Maryland \u2014 a commercial IRB and consulting service \u2014 as one of the fastest-growing technology companies in North America. The firm increased its revenues by 244% in five years, to reach nearly $5.5 million in 2004. Proponents of commercial IRBs say that larger companies have good reputations for speedy turnaround and thorough reviews. Several, including the two largest players, Chesapeake Research Review and Western IRB in Olympia, Washington, have been officially accredited by the Association for the Accreditation of Human Research Protection Programs in Washington DC. Such societies provide a stamp of approval for IRBs, providing oversight and standardization to the field. \n               Financial gain \n             Still, others worry about the potential conflict of interest inherent to commercial IRBs, who could benefit financially from pleasing their customers and passing protocols with minimal fuss. \u201cI'm a little cautious about this drive towards commercial IRBs,\u201d says Richard Bianco, associate vice-president for regulatory affairs at the University of Minnesota in Minneapolis, and a 15-year IRB veteran. Nevertheless, Bianco and other critics acknowledge that local IRBs also have a conflict of interest \u2014 clinical trials can bring in serious cash and prestige to the institutions they serve, and IRB members that are also scientists at the institution may feel pressured to approve a trial. \u201cI'm somewhat surprised that no one has ever pushed to reform the private side of IRBs,\u201d says Caplan. \u201cIt's growing like crazy. Industry hires them because they're fast and efficient. It doesn't mean that they're right.\u201d Bianco also says that his colleagues have been under \u201cintense pressure\u201d by industry collaborators to relinquish control to commercial IRBs. Some trial sponsors, he says, even issue ultimatums: use the commercial IRB that we recommend or don't participate in the trial. \u201cThat was pressure,\u201d says Bianco. \u201cBut I've been around a long time. You come to know what to ignore.\u201d None of those threats ever came to fruition, he says. For others, initial scepticism of commercial IRBs has given way to acceptance. \u201cWhen I first came across independent IRBs, I questioned them, too,\u201d says Wynes. \u201cBut I've taken the time to get to know how they operate, and my comfort level has changed.\u201d In November 2005, while Wynes was still at the University of Iowa in Iowa City, he helped the university to switch to outsourcing industry-sponsored trials to Western IRB. Prices vary, but outsourcing to industry can cost twice as much as processing the application in-house, although commercial IRBs typically boast a quicker turnaround time. Whereas many commercial IRBs aim to review applications within a week of their receipt, non-commercial IRBs may meet only once a month. Some see commercial IRBs as a stop-gap measure in lieu of real regulatory change. But despite the roadblocks, substantial change is inevitable, says Emanuel. The lingering problem, he adds, is that it will probably take a new scandal to push reform to the top of the agenda. \u201cI think we're just one accident away, but it will still take the accident,\u201d he says. \u201cIn my opinion, that's the sad fact.\u201d See Editorial,  page 511 . \n                     Pharma firms told to end secrecy in drug trials \n                   \n                     Bioethics: Dial \u2018E\u2019 for ethics \n                   \n                     Researchers break the rules in frustration at review boards \n                   \n                     Journals lack explicit policies for separating eds from ads \n                   \n                     Nature Medicine blog \n                   \n                     Nature Reviews Drug Discovery \n                   \n                     US Office for Human Research Protections \n                   \n                     UK National Research Ethics Service \n                   \n                     The Belmont Report \n                   \n                     Declaration of Helsinki \n                   Reprints and Permissions"},
{"file_id": "448642a", "url": "https://www.nature.com/articles/448642a", "year": 2007, "authors": [{"name": "K. S. Jayaraman"}], "parsed_as_year": "2006_or_before", "body": "India's new Ministry of Earth Sciences is at the helm of ambitious plans to advance deep-sea and polar research. K. S. Jayaraman reports. India, despite being the only country with an ocean named after it, has not always been a global driving force in the field of oceanography, with most of its research focused on the seas close to its shores. Now Indian researchers are pushing beyond this traditional base, with a $100-million-a-year drive deeper into their own waters and farther afield to the poles (see  'Push for the poles' ). For instance, the Indian government is planning a new deep-sea research vessel to complement the  Sagar Kanya , the flagship of the Indian research fleet. In collaboration with Russia, India is also building a robotic underwater vehicle that can dive up to 6 kilometres deep. And on the drawing board are dreams for a manned underwater submersible that Indian scientists believe would put the country on a par with more developed nations. \u201cI am really excited about the idea of India becoming a major blue-water research institution,\u201d says Henry Dick, a senior scientist at the Woods Hole Oceanographic Institution in Massachusetts, who is on the editorial board of the  Indian Journal of Marine Sciences . \u201cIndians have a tremendous contribution to make, and I see that as a great asset to the international community.\u201d India is, after all, in an excellent geographical location \u2014 a suitable launching point for expeditions to the relatively little-explored Indian and Southern Oceans. Both play important roles in the formation of the Indian monsoon, and both contain underwater geological features of interest \u2014 such as the Southwest Indian Ridge, where oceanic plates are pulling apart at one of the slowest rates in the world. \u201cThis portion of the global plate-boundary system is the most poorly explored region on the globe,\u201d says Dick. \u201cBasically, this is where the frontier of oceanographic research is, and India is located next to it.\u201d In addition, nearly 40% of the Southern Ocean can be reached only from the launching-off points of India and Australia, adds Maruthadu Sudhakar, senior scientist at the National Centre for Antarctic and Ocean Research (NCAOR), which together with India's other government-funded laboratory of oceanography \u2014 the National Institute of Oceanography (NIO) \u2014 is located in the state of Goa. \n               Out of disaster \n             Still, Indian oceanography is a relatively new field. \u201cSystematic oceanography in India really started with the International Indian Ocean Expedition in the 1960s,\u201d says Satish Ramnath Shetye, director of the NIO. That expedition produced the first oceanographic atlas of the Indian Ocean, and the core group of scientists set up the NIO in 1966. Today, with more than 200 researchers, the NIO is \u201cthe largest institution dedicated to ocean sciences in this part of the world\u201d, says Shetye. The entire field got an unexpected boost after the official monsoon forecast failed to predict the driest season for decades in 2002 (see  Nature   418 , 713; 2002). The Indian government asked atmospheric physicist Roddam Narasimha to suggest ways to revamp the country's meteorology department, and his report concluded that India needed an agency more along the lines of the US National Oceanic and Atmospheric Administration. The recommendation might have remained just that but for C. N. R. Rao, chair of the science advisory council to the prime minister, who pushed hard to implement it \u2014 and whose group met for the first time to discuss it just after the devastating tsunami of 26 December 2004. That disaster apparently convinced the government that oceans and atmosphere should be brought under the same umbrella. India's space and nuclear scientists have left their marks on global science, says seismologist Harsh Gupta, former secretary of the government's Department of Ocean Development and a driving force behind the new initiatives. \u201cIt is now the turn of oceanographers\u201d, he says. In July 2006, the new Ministry of Earth Sciences was born, bringing together national programmes in ocean science, meteorology, climate, environment, seismology and polar research. In its first year, the ministry has been allocated US$240 million and is staffed by some 8,000 scientists. India's interest in the ocean that surrounds it is economic as well as scientific. The country's exclusive economic zone, or EEZ, extends by almost two-thirds of the land area, and India has long set its sights on deep-sea mining. In October, the state-funded National Institute of Ocean Technology (NIOT) in Chennai anticipates getting a new $70-million research vessel, which is being built in Italy. It will be the mother ship for a $5-million unmanned submersible, expected to be ready in two years with Russian help, whose main goal is to recover manganese-rich nodules of rock and sediment from the sea floor. A prototype dived to 205 metres last October, says Subramaniam Kathiroli, director of NIOT. A manned exploration vehicle remains something of a dream, with a token allocation of $1.25 million in this year's budget for 'development'. \n               Ocean secrets \n             Although some researchers now see deep-sea mining as economically unfeasible (see  Nature   447 , 246\u2013247; 2007), India continues to pursue it. A pilot plant in Udaipur that extracts copper, nickel and cobalt from manganese nodules dredged from the sea floor, has been operating for several years at a rate of 500 kilograms of nodules processed per day. India has also been looking at underwater volcanic mountains \u2014 seamounts \u2014 as potential sources for mining cobalt. The cobalt-rich crusts capping the seamounts are available at shallower depths than the manganese nodules, which are up to 4 kilometres deep. The cobalt content of the crusts is three to six times greater than that of nodules. Virupaxa Banakar, a senior researcher at the NIO, says there are promising results from the Afanasy-Nikitin seamount in the equatorial Indian Ocean \u2014 which is in international waters and cannot be mined without additional permits \u2014 and at several sites within India's EEZ. Yet the secrecy with which India treats the oceanographic data from its EEZ is not going down well with the international community. \u201cThat is a great way to retard progress,\u201d says Dick. Indian researchers say they are bound by the guidelines of a 1975 parliamentary committee report that calls for \u201cstrict scrutiny\u201d of foreign participation \u201cin all aspects of oceanographic research related to ocean resources and coastal areas\u201d. For instance, India's forthcoming petition to claim an additional 1.5 million square kilometres of seabed will not contain magnetic or seismic data on the region, says Sivaramakrishnan Rajan, a senior scientist at the NCAOR. Indian collaboration with foreign scientists might be helped, though, by the fact that India is on the verge of joining the Integrated Ocean Drilling Programme (IODP). The United States, Japan and a coalition of 17 European countries are the primary members of this deep-sea drilling programme; India is looking to join as an associate member, as China is already. As part of the IODP, the Indian government plans to explore its energy and mineral resources beneath the sea and also investigate the geology of the Indian plate. Of particular interest are three potential drill sites: one each in the Arabian Sea, the western Andamans, and the Bay of Bengal, whose floor is carpeted by the world's thickest sediments, at 22 kilometres thick. The drilling, however, has to take place far offshore. \u201cWe cannot allow drilling by foreign ships within our EEZ unless there is a policy change,\u201d says Rasik Ravindra, director of the NCAOR. But interacting with the scientific community of the IODP could be valuable for young Indian scientists. And India desperately needs to alleviate a creeping manpower shortage in this field. Last year, Mangalore University, one of four universities in India that teaches oceanography, closed its marine-geology programme because it had only two students. \u201cWe need 20 fresh scientists a year,\u201d says Shetye, \u201cbut it is hard to find them.\u201d Many of the marine geophysicists trained at the NIO are poached by offshore oil companies, he adds. The NIO gives 12-week courses to 300 university students each year in the hope that some will opt for a career in this area, and the new ministry has created nine ocean science and technology cells in universities. But the results are yet to be seen. \u201cWe will not survive,\u201d says Shetye, \u201cif we don't produce enough manpower to do research.\u201d See Editorial,  \n                     page 623 \n                   . \n                     Digging deep \n                   \n                     India plans third Antarctic base \n                   \n                     Rival monsoon forecasts banned \n                   \n                     Climate model under fire as rains fail India \n                   \n                     National Institute of Oceanography \n                   \n                     National Centre for Antarctic and Ocean Research \n                   \n                     National Institute of Ocean Technology \n                   \n                     Integrated Ocean Drilling Program \n                   Reprints and Permissions"},
{"file_id": "448638a", "url": "https://www.nature.com/articles/448638a", "year": 2007, "authors": [{"name": "Claire Ainsworth"}], "parsed_as_year": "2006_or_before", "body": "No longer just cellular janitors, cilia are making a clean sweep for biological greatness. Claire Ainsworth explores how they may hold the secret of multicellular development. As orgies go, it's pretty wild. Hundreds of whip-wielding participants pile into a seething ball. Stripping naked, they entwine and embrace, striving to make an intimate connection and consummate the union. If beaten to it by a rival, they move on to another partner until they get lucky. And chances are, it's all carrying on right now in your garden. But don't call the police or reach for your camcorder yet. These swinging debauchers aren't human, they're single-celled algae called  Chlamydomonas , commonly found in soil and water. Affectionately nicknamed 'chlamy' by the scientists who study them, these slimy green organisms and their rumbustious sex lives have a surprising connection with us and how our bodies work. Dissecting that connection is leading researchers to uncover a story that starts more than half-a-billion years ago and ends in modern-day illnesses such as diabetes, cancer and obesity. Along the way it touches the origin of bodies, beauty and symmetry, even helping reveal what makes individuals unique. The link that brings these together is the whips brandished by our revellers; they're actually flagella that propel chlamy through the environment and are integral to their reproduction. In addition to providing locomotion, this tail-like structure acts as an antenna, allowing the cells to sense their environment by detecting signals that indicate the presence of food, predators or mates. When mates meet, their flagella intertwine, sticking together and triggering a cascade of chemical signals that directs the cells to fuse. This signalling, cooperation and clustering is reminiscent of a momentous biological step. \u201cIt's a flirtation with multicellularity as far as I'm concerned,\u201d says Bill Snell, a cell biologist and expert on chlamy at the University of Texas Southwestern Medical School in Dallas. About 700 million years ago, single cells clubbed together perhaps using flagella and other similar structures to cooperate and communicate, forming the first multicellular organisms. Now, biologists are realizing that signs of this unicellular ancestry are etched on almost every single cell of our bodies in the form of cilia, shortened versions of flagella that our unicellular ancestors used to flit through Precambrian seas. As is the case with chlamy, these primitive flagella probably also worked as antennae, receiving signals transmitted by other cells as well as channelling information from the environment. There is growing evidence that cilia are performing similar tasks in our bodies today, sensing and responding to fluid flow and physical stress, helping cells to navigate and move as our bodies develop, and acting as communication conduits. \n               Stirring things up \n             Where they once ignored or overlooked cilia, biologists are now seeing them everywhere and are having to rethink aspects of their fields. \u201cI think it's quite a surprise to developmental biologists,\u201d says John Wallingford, a developmental biologist himself at the University of Texas, Austin. And, he adds, \u201cI think a lot of the cell biologists are saying, 'I told you so'.\u201d For a long time, cilia were regarded as lowly janitors, lining our airways and beating in coordination to sweep up dirt and mucus, or wafting eggs down fallopian tubes. True, they had shown flashes of brilliance: single cilia, called kinocilia on the hair cells in the inner ear, for example, help us to hear. Cilia play similar roles in sight and smell, but biologists have only recently started to realize that cells carrying cilia are far from exceptions. Almost every cell in the human body carries a cilium, even neurons buried deep in the brain. The first hint at the primacy of cilia came in 1976, when a Swedish biologist called Bj\u00f6rn Afzelius reported the first link between faulty cilia and human disease. Afzelius was studying four infertile men. They were prone to chronic bronchitis, and, bizarrely, three of them had their internal organs placed on the wrong side of their bodies, a rare condition known as situs inversus. Using electron microscopy Afzelius showed that their sperm flagella lacked a key protein component, the dynein arms that help give flagella their kick. The cilia lining the men's airways had similar faults and did not sweep their lungs effectively. But the situs inversus was more puzzling 1 . Afzelius proposed that during development, cilia in particular positions in the embryo determined the placement of the organs by their coordinated beating. But no one knew where these cilia might be. More than two decades later, developmental biologist Nobutaka Hirokawa and his team at the University of Tokyo in Japan pinpointed them. Working on mouse embryos that lacked a protein needed for cilia to form properly, they showed that a patch of cilia in a tiny pit on the surface of very young embryos beat to create a leftward flow of fluid 2 . The team suggested that the fluid contained molecules that could accumulate on the left side of the embryo, breaking its bilateral symmetry and allowing the embryo to tell left from right. \u201cThis was a very big surprise for us,\u201d recalls Hirokawa. In the intervening decades more clues to the roles of cilia have come to light. Scientists studying patients with faulty cilia found a number of puzzling symptoms. Some patients developed multiple cysts in their kidneys. Others suffered from hydrocephalus, an accumulation of fluid on the brain. Cilia seemed to have their fingers in a lot of pies, but no one could work out how or why. \n               Dissecting cilia function \n             Biologists needed to find out what cilia were made of and how all the pieces worked together. This is where model organisms like chlamy helped. \u201cCilia are turning out to be really important organelles in cells, but we don't have any way to biochemically study them in vertebrates,\u201d explains Snell. Vertebrate cells are hard to separate from their cilia, but chlamy cells give up their two flagella more readily, making it possible to perform studies on intact organelles. \u201cJust lower the pH and off they come,\u201d says Snell. Genetic experiments on chlamy can then determine the functions of flagella components. Thanks to a number of studies in chlamy and other organisms such as fruitflies, nematodes and mice, biologists have pieced together a detailed picture of the structure of cilia and are now learning more about how they function (see  'The cilium dissected' ). One key to understanding their role in multicellular bodies came from work in chlamy on the system that builds and maintains cilia intraflagellar transport (IFT). In this system, motor proteins rapidly move bundles of cilium components and other molecules up or down the cilium's microtubule scaffolding. The system is balanced between motors that build and those that dismantle a cilium and if IFT fails, a cilium will eventually break down. Mutating IFT genes in mice, unsurprisingly, disrupts their ability to form cilia. Curiously, it also leads to kidney cysts similar to those found in patients with faulty cilia syndromes and to those found in patients with polycystic kidney disease, the most common inherited single-gene disease in humans 3 . Biologists now know that cilia in the kidney bend in response to passing fluid, allowing the cells to sense flow. Many other cell types also have flow-sensing cilia, including those in the liver. Already drawing attention, research on cilia yielded more surprises in the early 2000s. One revelation was a clutch of studies linking faults in ciliary proteins with a complex human genetic disease called Bardet\u2013Biedl syndrome, a condition characterized by obesity, kidney failure, blindness and extra fingers (see  'Cilia roles in obesity and diabetes' ). Another was the realization that a number of signal receptor proteins \u2014 such as receptors for the cell-growth control signal PDGF and the neurotransmitters serotonin and somatostatin \u2014 seemed to be concentrated in cilia. This raised the possibility that cilia might be involved in receiving molecular messages from other cells. So, are cilia just passive receptor-holders, or are they actively involved in transmitting signals to the inside of the cell? Snell and his team tackled this question by exploiting a quirk of chlamy genetics so they could switch IFT on and off at will. \u201cIt allows us to have cells with a full-length flagellum, but which are completely depleted of intraflagellar transport machinery,\u201d he explains. IFT-depleted cells can stick to each other, but none of the signals reach the inside of the cell, leaving them unable to fuse 4 . Last year, Snell's team showed that IFT proteins are needed for a key signalling molecule to move to the correct locations within a cilium during the signalling process, implying a direct role for IFT in transducing the signal 5 . \u201cThe IFT system doesn't just deliver things,\u201d says Snell. \u201cIt participates directly.\u201d As well as helping single cells find love, there is now strong evidence that IFT and the structural components of cilia are also needed to relay signals in multicellular bodies, such as those of vertebrates. Many of these signals are involved in coordinating body plans. One such set of signals is that of the oddly named hedgehog protein family. Hedgehog was first discovered in fruitflies, and is so named because larvae lacking it have disrupted body plans and are covered in prickles. \n               From hedgehogs to guinea pigs \n             Similar molecules exist in many animals, including mammals, where they coordinate a vast range of cell behaviours. For example, hedgehog signals direct the correct ordering of fingers, the development of lungs and neurons and the spacing of facial features. Stem cells also rely on hedgehog signalling to repair damaged tissue, and several cancers result when it goes wrong. And it seems that it all comes down to cilia. \u201cI think the idea that hedgehog depends on cilia is quite compelling,\u201d says Kathryn Anderson, a developmental biologist who studies hedgehog signalling at the Sloan-Kettering Institute in New York. Anderson and her team stumbled across the link between IFT and hedgehog when they were looking for mutations that disrupted the development of mouse embryos. They found a set of mutants that looked like hedgehog mutants, and were surprised to find that the mutations were in genes coding for IFT proteins, which indicated that intact cilia and IFT were needed for signalling to work 6 . \u201cWe weren't looking for cilia, they found us,\u201d says Anderson. Exactly how the system works is unclear. One suggestion is that cilia concentrate signal-ling components together in one place, an idea supported by the findings that hedgehog signalling proteins are localized in cilia. \u201cI think cilia are more complicated than that,\u201d says Anderson. Earlier this year, her team showed that hedgehog-signalling components are tethered or released from the internal scaffolding of cilia in response to signalling, pointing to the spatial organization of the cilium being important 7 . And just last month, Matthew Scott and his team at Stanford University School of Medicine, California, showed that key components of the hedgehog -signal reception system move in or out of cilia as part of the signalling process 8 . Hirokawa has added another twist to the tale. His team found that, as well as generating flow, cilia respond to the signals being wafted to the left-hand side of the embryo. Cilia located on left cells do this by seizing and opening large fatty packets of the signalling molecules needed for creating the asymmetry and then relaying the signal to their cells. \u201cSo the cilium works as a capturing mechanism,\u201d says Hirokawa. But cilia do even more during development: they help individual cells determine where they are within the plane of a sheet of cells. This phenomenon, called planar cell polarity (PCP), ensures, for example, that the hairs in a cat's coat point sleekly in the same direction, or that the hair cells in your inner ear stack the right way to let you hear. It also helps cells to navigate when they move, such as when the neural tube, which forms the developing brain and spinal cord, closes. When PCP is disrupted, the results range from the quirky, such the rosetted fur beloved of guinea-pig fanciers, to the devastating, such as the neural-tube defect spina bifida. Wallingford was studying PCP and neural-tube defects in  Xenopus  frogs when he stumbled upon cilia. He and his team had isolated the frog equivalents of two PCP-related genes found in flies. When they knocked out the functions of these genes, they found neural-tube defects, as expected. But, oddly, the embryos also had problems consistent with faulty hedgehog signalling 9 . This, and the fact that the genes were expressed at high levels in ciliated skin cells made Wallingford consider cilia. \u201cIf you had told me two years ago that I was going to be up to my eyeballs in cilia, I'd have laughed,\u201d he says. \n               A cellular compass \n             Looking at the cell's internal skeleton, or cytoskeleton, Wallingford found that the PCP-related genes controlled the behaviour of actin \u2014 the main cytoskeleton protein \u2014 during cilium formation and that actin controlled where in the cell the cilium was positioned 9 . This work and that of other scientists supports the idea that the cellular signalling that underlies PCP somehow interacts with the systems that build cilia. The jury is still out on whether, like hedgehog, PCP signals are transduced via cilia, or whether cilia need PCP to form. Wallingford favours the suggestion that the PCP system governs a cilium's position on the cell, denoting its up-down axis, perhaps angling the basal body in the right direction, allowing the cell to determine its orientation. This compass is also important in making sure cilia, in the airway for example, all beat in the same direction. \u201cWhen cilia are beating, if there's directed fluid flow across it based on cilia, there has to be a planar polarity there,\u201d says Wallingford. His team is now probing possible links between PCP and lung diseases such as asthma, where cilia go awry. The team has turned to the cilia that line the slimy skins of  Xenopus  tadpoles \u2014 uncannily similar to the lining of our airways \u2014 and in unpublished work, they have already identified a suite of new genes involved in generating them. Although cilia's relationship with cell signalling has come as a surprise to biologists, it may have been signalling that drove them to evolve in the first place. Detlev Arendt and his postdoc G\u00e1sp\u00e1r J\u00e9kely at the European Molecular Biology Laboratory in Heidelberg, Germany, were running some bioinformatics studies on IFT proteins when J\u00e9kely noticed that they bore a striking resemblance to some of the protein complexes involved in transporting vesicles around the cell 10 . This led them to speculate on the origins of cilia, which have been debated for many years. Lynn Margulis of the University of Massachusetts, Amherst, and her colleagues have suggested that, like mitochondria \u2014the powerhouse of the cell \u2014 cilia are derived from bacteria subsumed by other cells. Arendt and J\u00e9kely's findings suggest an alternative: that cilia arose from within cells, evolving from an existing signalling system. \u201cIt's a good assumption that the original function of the cilium was sensory,\u201d says Arendt. Further analysis indicated that IFT structures and sequences of IFT proteins were related to cellular proteins involved in transporting receptors rather than bacterial proteins 10 . J\u00e9kely and Arendt propose that cilia started out as a sensory patch on the surface of a primitive cell. Natural selection would favour patches that bulged out from the surface of the cell, as this would increase the patches' sensitivity. As signalling between cells probably played a role in the shift to multicellular life, cilia are likely to have helped. Nicole King, an evolutionary biologist at the University of California, Berkeley, has been investigating the role of signalling and the origins of multicellularity by studying choanoflagellates, the closest living single-celled relatives of multicellular animals. \u201cWe think that the common ancestor of choanoflagellates and the first animals had a single flagellum,\u201d she says. For the moment, little is known about the choanoflagellate flagellum, but more insights will come from genome projects now underway. Although choanoflagellates share some signalling systems with multicellular animals, they lack many others. For example, they have fragments of gene sequence that are similar to hedgehog, but, as yet, there is no evidence that they have hedgehog signalling themselves. The full range of signals first shows up in sponges \u2014 primitive multicellular animals \u2014 suggesting that the raw material of signalling was there in single-celled animals, and expanded upon as multicellularity developed. \u201cIt's easy to see how signalling may have been a pre-adaptation to multicellularity and that it might have been co-opted,\u201d King says. It's an intriguing thought that a tiny structure sitting on almost every cell in our bodies links back to a time when cilia were helping cells first get together, and that it still plays a key part in keeping cells together today. As biologists discover yet more roles for cilia, these once obscure organelles seem set for the limelight. \n                     Marine worm sports two kinds of 'eyes' \n                   \n                     Embryos grow with the flow \n                   \n                     Two become one \n                   \n                     William J. Snell, Ph.D. \n                   \n                     The Wallingford Lab, University of Texas \n                   \n                     Kathryn Anderson, PhD \n                   \n                     King Laboratory, University of California at Berkeley \n                   Reprints and Permissions"},
{"file_id": "448855a", "url": "https://www.nature.com/articles/448855a", "year": 2007, "authors": [{"name": "Erika Check"}], "parsed_as_year": "2006_or_before", "body": "Researchers in San Francisco have findings that suggest a whole new side to RNA interference. Erika Check reports on their attempts to make a revolutionary field more revolutionary still. \u201cThat looks perfect,\u201d murmurs Robert Place as he watches a smooth line trace across his monitor. \u201cIt never works this well.\u201d Place, a molecular biologist at the Veterans Affairs Medical Center and University of California, San Francisco, is using a spectrophotometer to measure the purity of a series of samples. Every sample contains a tiny drop of micro RNA (miRNA), a type of genetic regulator that dampens gene expression \u2014 or so the story goes. The experiment Place is absorbed in is the last he must complete before he submits a publication that could upend that story. He and his colleague, Long-Cheng Li of the University of California, San Francisco, think they have found some miRNAs that boost, rather than silence, gene expression in cells. Their work could shake the foundations of one of the hottest topics in biology \u2014 RNA interference \u2014 which studies how short pieces of RNA regulate the expression of genes. Place knows that his experiment will draw intense scrutiny from other researchers, and therefore it has to go perfectly. And as far as he can see, it has. His spectrophotometer displays a series of flawless curves, free of impurities; it looks as if he and his colleagues are finally seeing the pay-off after a three-year saga of frustration and exhilaration. Since about the turn of this century, scientists have realized that 50 years of focus on DNA had blinded them to the wide range of biological roles held by its chemical cousin, RNA. The old view was that DNA contained life's instructions, proteins carried them out, and RNA served as little more than a go-between. It's now become clear that RNA has vast potential for controlling how cells interpret the instructions embedded in the genome. The RNA revolution began in 1998 with the discovery honoured by last year's Nobel Prize in Physiology or Medicine that small strands of RNA stuck together in pairs, like the strands that make up DNA, could turn off specific genes in roundworms 1 . RNA interference was born. In 2001, scientists discovered that the process works in mammals, too 2 . They found that interference could be triggered by 'short interfering RNAs' (siRNAs) or 'short hairpin RNAs' (shRNAs). Both work by using a pair of scissors, actually a complex of proteins known as RISC, to cut apart the longer messenger RNAs (mRNAs) that take information from the genes to the cell's protein-making machinery. The small RNAs target mRNAs because their sequences match, and by destroying the messenger, RNA interference stops genes from making proteins. Both siRNAs and shRNAs are being tested in clinical trials against conditions in which an overactive gene needs to be shut down 3 . Scientists then discovered miRNAs, which also trigger the interference pathway, but unlike siRNAs and shRNAs are naturally encoded by cells' DNA. So far, more than 500 miRNAs have been identified in the human genome 4 . They can act like siRNAs, using the same protein complexes that slice through messenger RNA. But for the most part, miRNAs are content to muzzle the message or mark it for degradation, rather than chop it up ( see graphic ). All these techniques were still quite new back in 2004, when Li began to investigate RNA at the San Francisco Veterans Affairs Medical Center in the laboratory of Rajvir Dahiya, a urologist. Li, also a urologist, was studying epigenetics \u2014 stable modifications to the genome that change how it is read without altering its sequence. He was particularly interested in DNA methylation, the addition of chemical tags called methyl groups to regions of DNA. Methyl tags often silence nearby genes, which can be disastrous for genes that usually suppress tumours. So Li was trying to find ways to reverse this process. He decided to try an RNA-based technique to control methylation that hadn't yet been used in animals, as far as he knew, although it had been demonstrated in plants a decade earlier 5 . \n               Unexpected boost \n             Li purchased two pieces of double-stranded RNA that were complementary to a DNA control sequence \u2014 known as a promoter \u2014 upstream of the gene that encodes the tumour-suppressor protein E-cadherin. Then, his colleague Hong Zhao added the double-stranded RNA to two lines of prostate cancer cells and measured how it affected E-cadherin expression. Three days later, Zhao noticed something shocking: the experiment had boosted levels of the E-cadherin protein by 4- to 14-fold. \u201cI couldn't believe it,\u201d Li says. The work defied everything that scientists had ever reported about small, double-stranded RNAs. They were only supposed to dampen gene expression. Li seemed to have stumbled on a phenomenon that could rewrite the textbook understanding of RNA interference and might offer new therapeutic potential. Li re-ran the experiment many times, each time with the same result. He found that two other cancer-related genes,  VEGF  and  p21 , could also be activated by double-stranded RNAs. The evidence seemed solid. At least in the prostate cancer cells he was working with, short, double-stranded RNAs activated genes. \u201cIt was so easy to observe \u2014 I didn't understand how people could have ignored it for such a long time,\u201d Li says. But apparently they had. So Li knew he was going to have to put together a watertight case to convince other scientists. He started to realize how difficult that would be when he first submitted his work for publication to  Science  in August 2004. It was promptly rejected. He then submitted his paper to  Nature  that December; when it was rejected, he resubmitted it with new data in April 2005. He presented his findings at the annual conference of the American Association for Cancer Research in Anaheim, California, in May 2005, and the reception wasn't warm. \u201cI got a lot of sceptical questions,\u201d Li says. Then, after an extensive delay,  Nature  again rejected Li's paper in December, 2005. Without evidence for a mechanism, he says he was told the results weren't convincing enough. Unsure how to proceed, Li got some unexpected help in the form of Place, who joined the lab in October 2005. Fresh from earning his PhD, Place was excited by the novelty of Li's findings. But he could tell that the lab had a lot of nitty-gritty molecular biology work to do. Place helped plan experiments to help convince the sceptics. For instance, he helped Li and his technician Deepa Pookot to perfect assays, called immunoblots, that detect proteins so that they could measure how RNA activation boosts levels of E-cadherin and other proteins. \n               Filling in the details \n             Place and Li also started to think about the mechanism behind RNA-directed gene activation, a major stumbling block for the editors and reviewers who had seen the paper. If it worked in the same way as gene silencing, the sequence of the trigger RNA should matter, and the pair found that it did. Changing five letters at one end of the 21-letter sequences rendered them inactive. The pair also found that activation used some of the same key proteins that are involved in silencing, such as Dicer, which cuts up strands of RNA so that they can be used by RISC to target mRNA. They began to experiment with activation, trying to find out what worked best. \u201cWe started to modify the RNA duplexes to get optimal activation, tweaking their chemical structures,\u201d Place recalls. \u201cThere came a point when we were working in synchronization, and we really started to click.\u201d Li resubmitted the work to  Science  with Place's additional molecular biology results, but it was again rejected. The letter he received said that because the work \u201cwould represent a substantial paradigm shift\u201d, the evidence just wasn't strong enough. Again, editors required demonstration of a mechanism. So, in June 2006, Li talked to leaders of the RNA-regulation field at a meeting in Cold Spring Harbor Laboratory, New York. Li recalls asking David Bartel of the Whitehead Institute for Biomedical Research in Cambridge, Massachusetts, if he thought RNA activation \u2014 not just inhibition \u2014 was possible. Bartel said he didn't think so, Li says. Bartel says he doesn't remember the conversation, \u201cI was probably just trying to find out how solid the evidence for activation really was,\u201d he explains. Li was after all a junior researcher in an unrelated field, and he hadn't been able to publish his findings. Exasperated, Li considered sending his findings to an online journal with less stringent criteria than  Nature  or  Science . But Dahiya thought that the work would get buried in obscurity, and convinced Li to try the  Proceedings of the National Academy of Sciences  instead. In August 2006, the lab submitted its work there \u2014 and finally got its breakthrough. The journal reviewed it in weeks and published the work online in November 6 . Two months later, vindication: another lab, led by biochemist David Corey of the University of Texas Southwestern Medical Centre in Dallas, published a paper essentially duplicating the results in  Nature Chemical Biology 7 . \u201cIt was really good for us,\u201d Place says. \u201cBefore that, it seemed like everyone just thought we were crazy.\u201d Back at the Veterans Affairs Medical Center, Place and Li were now working closely together, although Li was preparing to move to his own lab at the University of Califorinia, San Francisco. They had already devised a series of experiments that they hoped would bolster their activation hypothesis. Although some changes to the sequence can render RNA useless, others fine-tune the activation. The trigger doesn't have to match the target sequence exactly, and a few tweaks to the double-stranded RNA produces less activation. And that's exactly how miRNA works. So the pair started hunting for a miRNA that might activate E-cadherin. It wouldn't prove that activation was a natural phenomenon, but it would strengthen the case, because miRNAs are encoded in the genome. Place and Li used a bioinformatics tool to search for miRNA in the human genome that had sequences roughly complementary to the E-cadherin promoter sequence. They found a handful of candidates and transferred them into the prostate cancer cell lines. The experiment was so simple they half-expected it wouldn't work \u2014 but it did. The pair found one miRNA, miR-373, that boosted E-cadherin expression. \u201cWhen that worked, we were totally pumped, because it was a potential example of natural function,\u201d Place says. \n               On the right track \n             But it still wasn't proof. So Place designed an miRNA precursor that, according to the model of miRNA biogenesis, should work as well as the miRNA itself. It did. Place also found that if he knocked out Dicer, then miR-373 stopped working, and that Dicer could also activate another protein with a promoter sequence similar to E-cadherin's. All these experiments supported the idea that miRNA could use the interference pathway to activate genes. It was a strong hint that Li and Place were on the right track. On 6 August, they submitted the work to  Nature Chemical Biology . The question now is how this work will be received. Many are still sceptical about activation, as David Corey can attest. \u201cLast time I talked about this at a meeting, a couple of leaders in the field jumped all over me and told me it wasn't true,\u201d Corey says. \u201cWe've received grant and manuscript reviews that seem to express irritation more than anything else. I just wish they would look closely at our data.\u201d Indeed, it sometimes seems as though the idea of activation is struggling because it contradicts the interference dogma. The RNA interference field is quite young, but already seems to have acquired a certain amount of inflexibility. On scanning through archives of biology message forums, Place has found that other scientists \u2014 often graduate students \u2014 have also seen evidence of RNA activation. But they have been encouraged to discount it. Phillip Sharp, a Nobel-prize-winning biologist whose lab at the Massachusetts Institute of Technology in Cambridge pioneered much of the work on RNA interference, admits that RNA-mediated activation might be possible, but says that \u201cthe results I have seen do not prove this\u201d. Echoing the demands of manuscipt editors, he says, \u201cI think further papers on the subject must address the mechanism if they are to be published in a high-profile journal.\u201d \n               A new pathway \n             As the field has moved apace, such requests are warranted, and providing proof of a mechanism is Li and Place's main challenge. Some have suggested that activation is simply inhibition in disguise. It could be the accidental result of silencing an upstream repressor or of blocking another silencing RNA. Although they can't rule out these possibilities, from their experiments, they say that these mechanisms look unlikely. For instance, they can elicit activation predictably at specific genes by targeting their promoters. There are intriguing differences between the known silencing pathways and the observed activation phenomenon. Silencing is triggered within hours and ceases in about seven days, whereas activation takes days to appear but can last for weeks. The different kinetics suggest that some mystery process is involved, Place says. \u201cPeople say this could be RNA interference with another name, but it's so blatantly different.\u201d Major questions also remain about how RNA regulation might act at gene-promoter regions. In the classic interference pathway, the RISC complex guides siRNAs or miRNAs to a target mRNA in the cell's cytoplasm. But to regulate a promoter, a small RNA would have to sneak into the cell nucleus, where DNA is transcribed. There is mounting evidence that this happens in silencing pathways, but again the mechanism is unclear. In 2004, two groups published papers that suggested that siRNAs that target gene-promoter regions can silence genes if they are delivered into cell nuclei. Although one group has since retracted its paper, the other group, who published in  Science 8 , showed that the inhibition was accompanied by epigenetic marks associated with silencing. Kevin Morris, of the Scripps Research Institute in La Jolla, California, an author of that paper, has continued to study how this occurs, and sympathizes with Li and Place's position. \u201cI was there in 2004, when 50% of the people love your work and the other 50% think you're full of it,\u201d Morris says. \u201cIt's a frustrating place to be.\u201d Frustrating, indeed. As the deadline for this news feature approached, Place received word that the group's manuscript on miRNA activation had been rejected. The reason given: without proof of a mechanism, the evidence isn't substantial enough. Li is still adamant that the field will come around. \u201cWe have no doubt that RNA activation is an endogenous mechanism,\u201d he says. And Place seemed unsurprised. \u201cWe knew the mechanism would be the sticking point. That's the hardest part to prove.\u201d But, he predicted, if the group can just get someone to just take a look at its data, the strength of its evidence will prevail. \u201cIf we can get it into review, we'll be okay,\u201d Place says. When RNA interference first hit the scientific radar, it was a slow climb from something written off by many as artefact to a revolutionary paradigm. That same uphill battle confronts Place, Li and their collaborators as they try to rewrite, or at least refine, the revolution. Call it what you will \u2014 stubbornness, confidence or optimism \u2014 this group just isn't going to give up. \n                     RNAi scoops medical Nobel \n                   \n                     Silent running: the race to the clinic \n                   \n                     Further accusations rock Japanese RNA laboratory \n                   \n                     Negotiating the RNAi patent thicket \n                   \n                     http://www.nature.com/news/infocus/rnai.html \n                   \n                     Rajvir Dahiya's website \n                   \n                     Long-Cheng Li's website \n                   \n                     David Corey's website \n                   Reprints and Permissions"},
{"file_id": "448122a", "url": "https://www.nature.com/articles/448122a", "year": 2007, "authors": [{"name": "Nick Lane"}], "parsed_as_year": "2006_or_before", "body": "Studies of mass extinctions tend to emphasize the sheer scope of the carnage. But subtle differences between the species that died and those that survived can be crucial, finds Nick Lane. The extinction at the end of the Permian period, some 251 million years ago, is the most fascinating mass-murder mystery in Earth's history. Simply put, a party or parties unknown killed off up to 96% of all species then alive. Palaeontologists and Earth-system scientists have suggested any number of possible culprits, from a comet or asteroid collision to a collapse of the ozone layer, from a dramatic suite of volcanic eruptions to ocean waters bubbling with sulphurous fumes. Given that the evidence for an impact, although championed by some, seems sketchy and inconclusive to the majority, the most dramatic of these suspects is volcanism \u2014 the sequence of eruptions that gave rise to the basalts of the Siberian Traps. The scale of the eruptions was vast, with something like 3 million cubic kilometres of basalt flowing on to the surface, and the main lava flows took place at almost exactly at the same time as the end-Permian extinction itself, give or take a few hundred thousand years. The vast clouds of gas and ash they spewed out look like the smoke from a very large gun indeed 1 . But if the erupting traps look increasingly like the trigger for the great die-off, how did they actually do their damage? The fiery birth of a small continent's worth of new landscape can change everything from the brightness of the sky to the chemistry of the oceans. How can scientists work out which of these effects were inconveniences, and which were fatal? One answer is to study not the volcanoes, nor their victims, but the creatures that survived. A mortality rate of 96% sounds pretty indiscriminate \u2014 but recent physiological comparisons between the creatures that died and those that survived reveal intriguing patterns 2 . And these patterns do not just provide clues as to how the great dying actually unfurled. They also reveal how the extinctions at the end of the Permian period shaped the subsequent history of the world that the survivors inherited. Until recently, perhaps the most popular 'kill mechanism' evoked for the extinction was global warming 3 . Volcanic carbon dioxide, so the story goes, raised temperatures enough to destabilize vast reserves of methane hydrates in the sea floor, pumping temperatures up further. But although there's little question that global temperatures rose at the end of the Permian by as much as 6 \u00b0C, there are problems with this picture. Among the things to go extinct at the end of the Permian are a lot of plants. There are plausible arguments as to how temperature could kill off animals \u2014 for example, it would force a higher metabolic rate on them, requiring an increase in their calorie intake that they might simply not have been able to satisfy. But there's no obvious reason why such heat would damage plants. What's more, the physiology of the survivors doesn't fit the story. Animals that seem to have started off with higher metabolic rates suffered less than those with slow metabolisms. Other things being equal, you might expect animals that are already generating more heat in their own bodies to do worse when things heat up further. Another possible kill mechanism is a collapsed ozone layer. Mutant pollen and spores in sediments from the time of the extinction hint at an increase in ultraviolet levels brought about by a dearth of ozone. David Beerling, a plant physiologist who concentrates on palaeoclimate issues at the University of Sheffield, UK, says his eyes were opened to this risk when he learned that a NASA aircraft flying through the plume above the 2000 Hekla eruption in Iceland showed substantial amounts of hydrochloric acid had been injected directly into the stratosphere. Simulations by Beerling and his colleagues at Sheffield and the University of Cambridge, UK, suggest that the Siberian Traps could have removed 70% of the ozone shield from the Northern Hemisphere, enough to do serious environmental damage 4 . Even so, Beerling doubts that an ozone collapse was responsible for the mass extinction: \u201cAnimals have too many places to hide on land, and in the oceans they could easily escape the worst of it,\u201d he says. And as with global warming, an excess of ultraviolet radiation does not seem to fit the pattern of survival that is actually seen. Living deep in the sea doesn't seem to have helped. But better ways of breathing, which are often seen in creatures with a higher metabolic rate, did. In the sea, creatures with gills and an active internal circulation, such as snails, clams, crabs and fish, fared better than stationary filter feeders. A similar pattern holds on land, with groups that were not so good at pumping air through their respiratory systems \u2014 such as the giant dragonflies iconic of pre-Permian times \u2014 suffering disproportionately. These patterns focus attention not on harsher sunlight, but on tainted air. \n               Oxygen collapse \n             One possible factor is a worldwide dearth of oxygen. It used to be thought that today's oxygen level of 21% had remained fairly constant over geological time. But in the late 1980s, Robert Berner of Yale University in New Haven, Connecticut, shocked the establishment by arguing that in the Carboniferous period that preceded the Permian, 300 million years ago, the oxygen level had reached 30% or more, and that it had then fallen dramatically to a mere 13% in the late Permian and the early part of the subsequent Triassic. But after a few refinements, Berner's models for this period have been widely accepted. Berner blames the oxygen collapse on another spate of volcanism that predates the Siberian Traps by eight million years or so. The warming those eruptions triggered, he argues, would have brought about more arid conditions, drying out some of the coal swamps of the time. It had been the capacity of those swamps to bury organic carbon where it could not be respired back into carbon dioxide, which had accounted for the high oxygen levels. As the swamps dried out, less organic carbon got buried this way, and so more was respired back to carbon dioxide \u2014 using up oxygen in the process. Over millions of years the slower rate of carbon burial drew oxygen levels down to an unprecedented low point 5 . The falling oxygen levels Berner describes would have restricted the geographical range of many land animals 6 . Most animals have an 'altitude ceiling' above which they can't flourish (for humans it's 5.1 kilometres). As oxygen levels dropped in the later Permian, so these ceilings would have pressed down on the creatures beneath them. According to palaeontologist Peter Ward of the University of Washington in Seattle, for creatures that had evolved in a high oxygen world, modest hills of just a few hundred metres could have become impassable barriers. In the oceans, too, the habitable zone would have shrunk as oxygen levels fell. Global warming would have made matters worse, as oxygen is less soluble in warm water than in cold. Such effects are still visible in the world today. This January a study of fish populations in the North Sea showed that, among eelpouts (Zoarcidae), the size of both populations and individuals shrinks when increased temperatures lessen the oxygen supply 7 . Palaeontologist Richard Twitchett, at the University of Plymouth, UK, thinks that something similar happened in Permian times. His evidence comes from the species that seem to vanish at the end of the Permian, only to start cropping up again after a hiatus of millions of years \u2014 the 'Lazarus taxa'. A startling 70% of the gastropods found in the mid-Triassic seem to have 'risen from the dead' in this way. Twitchett thinks that their populations became so small that random chance expunged them from the fossil record. Things didn't pick up for the gastropods until oxygen levels started to pick up, well into the Triassic 8 . And when they did re-emerge, they were smaller \u2014 they have something of Lilliput, as well as Lazarus, about them. But for all this it is hard to pin the mass extinction proper on oxygen. From a physiological point of view, the species most likely to survive low oxygen would seem to be those with a low metabolic rate \u2014 and yet the stationary filter feeders, which fit that bill, fared disastrously. And then there's the timing. The trends in atmospheric oxygen levels were slow, reflecting changes in the rate at which organic matter was buried over millions of years. By contrast, detailed analyses of the strata in which the fossils are preserved show that the extinction was abrupt, with most species disappearing in a few tens of thousands of years 9 . As Andrew Knoll, a palaeontologist at Harvard University in Boston, Massachusetts, puts it: \u201cIf you've got gradual changes taking place over millions of years, I'd expect organisms to adapt; but if the changes take place in 10,000 years, I'd bet on the environment to club them.\u201d \n               Killer carbon \n             Low oxygen must have placed life in a precarious position, but something else pushed it over the edge. And a good clue to the culprit is a series of unusual shifts in carbon isotopes found in strata worldwide, corresponding closely to the exact time of the extinction. This series of spikes in the carbon-isotope level suggests that strange things were happening to the carbon cycle at this time. The anomalies \u2014 sudden upswings in the ratio of lighter carbon-12 to heavier carbon-13 \u2014 are too big to be explained by changes in the total amount of biomass (which tends to be carbon-12 rich). The most plausible explanation would be bursts of methane. But sea-floor methane hydrate reserves would not be up to the job. There are five distinct spikes in the record between the latter part of the Permian and the early Triassic. According to Jonathan Payne at Harvard University and his colleagues, although sea-floor hydrate reserves might account for one such spike, they could not be replenished quickly enough for a whole succession 10 . Greg Retallack of the University of Oregon in Eugene points to a different source. In both the Siberian Traps and the earlier volcanism, lava rose to the surface through both carbonate rocks and coalfields. Hot lava passing through coal measures breaks down some of the heavy hydrocarbons into methane, and although this 'thermogenic' methane is not as light, isotopically speaking, as the stuff on the sea floors, it is light enough to explain the isotope spikes 11 . At a time when oxygen levels were already stressing animal life, such large injections of methane, which would oxidize into carbon monoxide and carbon dioxide, could have killed animals through simple asphyxiation. As for the plants, Retallack points out that those that did the worst were in swamps, which were low in oxygen anyway, so their roots asphyxiated. The need to deal with high levels of carbon dioxide could explain various aspects of the survivors' physiology, including subtleties at the molecular level. Knoll is intrigued by a subtle difference in pigment use between the squid-like ammonoids, which died out, and the creatures from which today's spiral-shelled nautilus is descended. Animals use respiratory pigments such as haemoglobin to get oxygen from the lungs or gills to the tissues that need it, and to remove the resulting carbon dioxide. When carbon dioxide levels get too high, some such pigments will cease to work properly, unable to remove the bodily carbon dioxide with which they are encumbered or to pick up more oxygen. The nautilus, Knoll notes, has a pigment that deals well with high levels of carbon dioxide. The closest living relatives of the ammonoids, the coleoids \u2014 soft-bodied cephalopods such as squid and octopus \u2014 do not. Their unhelpful pigments might not have been enough to doom the ammonoids \u2014 but as shell makers they had other problems too. One of the effects of increasing carbon dioxide is that sea water, like soil, becomes more acid. Shell formation, Knoll argues, is repressed if carbon dioxide levels cause organisms to lose control over their systems for dealing with such acidity. The groups least good at regulating acidity seem to have been particularly badly hit at the end of the Permian. So were those creatures that needed their shells to channel water over their gills. \n               Sulphur survivors \n             The case for a significant amount of asphyxia is strong. But global warming and low oxygen would have brought another deadly gas into play. Hydrogen sulphide is produced in vast quantities by bacteria that gain their energy from sulphate, and that thrive only in very low oxygen conditions. Today, these conditions are prevalent only in closed basins, most strikingly that of the Black Sea. But there's plenty of evidence to suggest that such conditions applied much more widely at the end of the Permian. Lee Kump, a geochemist at Pennsylvania State University in University Park, argues that sulphidic deep waters could have crept higher and higher up the water column, killing everything they encountered and creating what is evocatively named a 'Strangelove ocean'. When they reached the surface they would have released poisonous gas straight into the air 12 . Last year, biomarkers unique to 'green sulphur' bacteria, which make use of sulphide for photosynthesis and cannot tolerate oxygen, were found in some sediments from the end of the Permian, suggesting that oxygen-free waters got close enough to the surface for photosynthesis to take place 13 . The effects of hydrogen sulphide escaping from the oceans would in all likelihood have been local, not global, and Beerling's models suggest that no plausible level of the gas could have damaged the ozone layer, as had been suggested. But it would have been another challenge for stressed ecosystems. So with dangerously low atmospheric oxygen levels, ecosystems were compressed and fragmented. The deep oceans were largely uninhabitable. Land plants were dying back in the arid greenhouse climate, making food hard to come by. And then came the hammer blows of fate, the greatest volcanic outpourings in the history of our planet, releasing vast quantities of methane and carbon dioxide, raising global temperatures by 6 \u00b0C, and lifting the Strangelove conditions to the very surface. Head for the hills and there's no oxygen; stay on the shores and you risk breathing hydrogen sulphide. High carbon dioxide levels sabotage your respiratory pigments and choke you from within. Even if death doesn't get you right away, you're unlikely to have much spare energy for sex. Population sizes fall; so do body sizes. Even for those that survive the immediate toxicity, slow extinction was likely over a few generations \u2014 the blink of an eye in geological terms. And the resultant catastrophe shaped the rest of the planet's history. According to research by Peter Wagner and his colleagues at the Chicago Museum of Natural History in Illinois, life in the seas before the extinction was split roughly 50\u201350 between simple marine ecosystems dominated by sessile filter-feeders, such as today's sea lillies, and more complex systems in which larger numbers of species moved around and interacted. After the extinction and the subsequent recovery, complex ecosystems outnumbered simple ones by three-to-one 14 . That has remained pretty much the case ever since, the patterns of biodiversity reflecting the extinction bottleneck that a few creatures squeezed through with frantically fluttering gills. The situation was similar on the continents. The hardiest group of reptiles was the cynodonts, from which mammals are descended. The squat, flat-faced Lystrosaurus, with its breathing aided by a bony palate separating the mouth from the nose and a barrel-chest with a muscular diaphragm to help deep breathing, went on to dominate the Triassic. Cynodonts also enjoyed the benefits of nasal turbinate bones. Some turbinates are linked with fast metabolic rates in warm-blooded creatures, as they restrict water loss during rapid respiratory ventilation. In the early Triassic, though, they might have restricted water loss in hyperventilating animals on the verge of suffocation 15 . Other survivors included the archosaurs, which benefitted from breathing made easier by air sacs. Their descendants became the dinosaurs, birds and crocodiles 16 . It's unlikely that any of these respiratory adaptations evolved in response to the suffocating conditions at end of the Permian; rather, the animals most likely to survive the disaster were those already adapted to deal with suffocating conditions on a daily basis \u2014 creatures used to burrowing on land, or scavenging in the stagnant muds of ocean shelves; or just those that came up trumps with an air-sac system that happened to be particularly good. Take a deep breath, fill your lungs with a toxic brew of gases, gasp for the thin oxygen. Small wonder most life perished; wouldn't you? But your ancestors survived and repopulated the world in new ways that we still see around us today. The genetic memory of those times is etched in our physiology \u2014 and in the living fabric of our world. \n                     Plant pollen records ozone holes \n                   \n                     Comet impact theory faces repeat analysis \n                   \n                     Suffocated or shot? \n                   \n                     David Beerling's White Rose Palaeobiology group \n                   \n                     Robert Berner's homepage \n                   Reprints and Permissions"},
{"file_id": "448742a", "url": "https://www.nature.com/articles/448742a", "year": 2007, "authors": [{"name": "Corinna Wu"}], "parsed_as_year": "2006_or_before", "body": "For 30 years scientists have believed that there are no organic molecules in the martian soil. Will NASA's Phoenix probe prove them right or wrong, asks Corinna Wu. In the summer of 1976, humanity first got its surrogate hands on the soil of another planet. Two Viking probes scooped up samples from their landing sites and fed them into an array of instruments carried from Earth for the purpose. Perhaps the most technically sophisticated of those instruments were the gas chromatograph-mass spectrometers (GC-MS) designed to detect evidence of organic compounds in the soil. After months of operation they had found none at all. The Viking missions were accepted almost unanimously as showing that Mars was a sterile planet, and the GC-MS data were a crucial part of the evidence. This month, another mass spectrometer is on its way to Mars. NASA's Phoenix mission was launched on 4 August, and if all goes according to plan, it will touch down 1,200 kilometres from the north pole of Mars on 28 May 2008. Phoenix is essentially a sister ship to the Mars Polar Lander (MPL) lost in December 1999 \u2014 hence the imagery of its name \u2014 and the first in NASA's new line of low-cost 'Scout' missions. It is a Buick to the Bentleys of the Viking missions, but some chemists are all but certain that it will find evidence for organic chemicals close to the martian surface even though the better equipped Vikings found none. If it does it will raise the hopes of those who cling tenaciously to the idea that there might be evidence for past, or even present, life somewhere on that frigid, radiation-battered and deeply inhospitable planet. The Viking landers each carried a suite of three experiments designed to detect micro-organisms in the dust and sand that, along with pumelled rocks, made up the surface at the landing sites. Their results were inconclusive. When watered and fed nutrients, the martian soil gave off gases in ways that had not been expected, some of which might be consistent with biological activity. But the GC-MS data were unequivocal. The GC-MS is a workhorse lab instrument used to pick out the various components in mixtures of organic molecules. It works by first passing samples through a thin capillary column \u2014 the chromatograph. Small molecules move through quickly; heavier ones more slowly. When the compounds emerge from the column, sorted by size, they pass through the mass spectrometer, which measures the mass either of whole molecules or their fragments. The Viking GC-MS team was led by Klaus Biemann, a chemistry professor at Massachusetts Institute of Technology (MIT), Cambridge, and a pioneer in the development of such instruments. Biemann was not a space scientist; his work concentrated on determining the structure of protein fragments \u2014 laying the foundations for today's 'proteomics'. He agreed to work on the Viking instrument only as an act of \u201cscientific charity\u201d, he recalls. \u201cI said if it needs to be done, it might as well be done well.\u201d And it was. At the time, the GC-MS Biemann had in his laboratory was the size of a room. The instrument needed to analyse the compounds that might be given off when martian soil samples were heated had to fit into a box just thirty centimetres long on each side. Biemann's team produced a fully automated system that met the constraints in size and available power and yet was still sensitive to compounds present only at a parts-per-billion level. \u201cTo get two machines a hundred million miles out and both of them working is just a marvel of engineering,\u201d says Steven Benner of the Foundation for Applied Molecular Evolution in Gainesville, Florida, who has worked on chemical approaches to various astrobiological questions. \n               Organic ball game \n             Biemann's team expected to have organic compounds to analyse when its instrument reached Mars. Mars's cratered face showed it had been bombarded by meteorites, and by the 1960s some meteorites were known to contain organic compounds. So organic compounds were to be expected, even if Mars itself had produced none of them. \u201cYou should be sitting in a sea of this stuff,\u201d says Benner. Indeed, according to Benner some Viking developers worried that the GC-MS would be swamped by the sheer amount of organics. Yet the only things that came out of the heated soil samples on Mars were water and carbon dioxide, which were taken to have been either physically trapped in soil particles or released from inorganic minerals. Although the experiments did see some organic compounds, they were those that had been used to clean the equipment back on Earth, and had already been detected when the instrument was tested in deep space during the probes' transit to Mars. With no evidence for organic molecules in the soil, the results from the life detection experiments were to some extent rendered moot. \u201cThat's the ball game,\u201d said Jerry Soffen, Viking's project scientist, at the time. \u201cNo organics on Mars, no life on Mars.\u201d The activity seen in the samples was subsequently interpreted as being due to chemistry, not biology. Bombardment by ultraviolet light would make the soil rich in oxidants such as peroxides, and also lead to some unusual \u2014 on Earth \u2014 compounds such as carbon suboxide (C 3 O 2 ); scientists proposed various ways that reactions with such chemicals could have provided the results seen by the life detection experiments. Only one of the investigators, Gil Levin, continued to think that what they had seen was best explained by biology. In 2000, though, Benner, then at the University of Florida, Gainesville, suggested that Mars might indeed have contained some organic material, but that Viking could have missed it. He proposed that the organics might take the form of mellitic acid, which is seen when the mess of polymerized carbon that makes up much of the organic component in meteorites is not completely oxidized. \u201cIt is readily formed under the oxidizing conditions you'd expect to find on Mars,\u201d Benner says. \u201cIt is quite stable to further oxidation, and it's also refractory \u2014 it doesn't dissolve in anything. And when you heat it, it doesn't give off anything volatile. So they could have been sitting in a sea of this stuff and not seen it.\u201d Benner's study estimated that this process could have generated 2 kilograms of mellitic acid per square metre of martian surface over the course of 3 billion years 1 . Unfortunately, it takes a lot of heat to break down mellitic acid \u2014 and when it breaks down, the primary product is benzene, says Benner, which was one of the solvents that had been used to clean the instrument. If the Viking experiments had heated their samples to 600 \u00b0C instead of 500 \u00b0C, they might have picked up traces of something distinctive. But they didn't reach 600 \u00b0C . In October 2006, a team including chemist Rafael Navarro-Gonz\u00e1lez of the National Autonomous University of Mexico's Institute of Nuclear Science, Mexico City, and planetary scientist Christopher McKay of NASA's Ames Research Center in Mountain View, California, reported that the experiment might not have picked up some other types of organic compound in the soil 2 . Navarro-Gonz\u00e1lez was inspired to revisit the Viking tests for organic molecules after the Opportunity rover discovered jarosite, a hydrated iron sulphate that forms in the presence of water, on Mars in 2004. Studying jarosite-containing soils in the Rio Tinto area of Spain, he found that getting organic material out with chemical approaches was relatively easy \u2014 but getting it out just by heating was not. \u201cWhen I repeated the Viking experiments,\u201d he says, \u201cI was surprised to see that despite the huge amount of organic matter present, there was virtually no detection of organics in the sediments. This was quite strange.\u201d Independently, McKay had been doing research on soils from the Atacama desert in Chile, and had also started to suspect that the Viking experiments weren't telling the whole story. Alison Skelley, a graduate student at the University of California, Berkeley, had asked McKay to review a paper on a device she had developed for detecting amino acids in soil 3 . McKay found the paper striking, noting that \u201cit found that there were a thousand times more amino acids released by chemical extraction than pyrolysis\u201d \u2014 the heating method used by the Viking experiments. Then McKay says, \u201cWithin a month, Rafael told me about his puzzling result with the jarosite. That's when I suggested that we ought to see if this effect was widespread.\u201d In addition to the Rio Tinto sediments and Atacama desert samples, they tested soil samples from other inhospitable and vaguely martian environments \u2014 the Dry Valleys of Antarctica and the Libyan desert. Chemical extractions revealed low levels of organic compounds \u2014 between 20 and 90 micrograms of carbon per gram of soil, in Antarctica for example. But heating samples from most of the sites to 500 \u00b0C did not produce organic volatiles that their GC-MS setup \u2014 a combination of several commercially available instruments \u2014 could detect. Only at 750 \u00b0C did they start to get a signal from more than half the samples \u2014 a temperature the Viking systems were not designed to reach. \n               Heated debate \n             Navarro-Gonz\u00e1lez and McKay think that during the heating step of the Viking experiment, any organics given off at moderate temperatures would have been turned into CO 2  before they reached the GC-MS, thanks to catalytic iron compounds in the martian soil. \u201cWe suggest that a small portion [of the carbon dioxide seen by the Viking experiments] could have resulted from the oxidation of organics,\u201d says Navarro-Gonz\u00e1lez. \u201cEven if it's just a small percentage, this could mean levels of organics on the surface of Mars a thousand times higher than expected.\u201d Before publication they sent the paper to Biemann for his feedback and \u201che was completely upset,\u201d says Navarro-Gonz\u00e1lez. \u201cBut he did bring up some interesting points that we had to resolve.\u201d Navarro-Gonz\u00e1lez says they sent further versions of parts of the paper; Biemann says he received no satisfactory answer to his questions. In the finished article, the authors thank Biemann \u201cfor comments on an earlier version of the manuscript\u201d. Biemann, who denies he was upset, says he heard that the Navarro-Gonz\u00e1lez paper had finally been published only after his daughter read a story about it in the  Wall Street Journal . Being thanked in the paper, he feels, implies that he agreed with their second version, which he says he never saw. And so in a strongly worded critique of the revisionist work, Biemann argued that the experimental setup used by the Navarro-Gonz\u00e1lez and McKay team was a thousand times less sensitive than the Viking device 4 . \u201cTo say in their experiment that they don't find things at 500 \u00b0C that they find at 750 \u00b0C doesn't mean anything,\u201d he says. If their instrumentation had corresponded more closely in its performance to the carefully-tailored Viking GC-MS, it would have been easily sensitive enough to detect what was going on at 500 \u00b0C, he says. Biemann thinks that a misplaced zeal to find life on Mars has driven scientists, including the Navarro-Gonz\u00e1lez team, to try and prove the GC-MS results wrong; they want to \u201cget rid of that obstacle\u201d, he says. McKay, for his part, expresses frustration over what he feels is a misunderstanding of the thrust of their paper. \u201cIf I were rewriting the paper, I would emphasize that the GC-MS operated flawlessly. The problem is the pyrolysis release of organics.\u201d He says the debate over the Viking results has been so narrowly focused on the GC-MS that the pyrolysis step has been ignored. Even accepting the group's analysis that there might be some organic matter in the soil, \u201cit's not a rich soil by any stretch of the imagination\u201d, says McKay. \u201cWhen we say [the organic fraction] could be as high as a part per million, it's important to note that it could also still be zero.\u201d What's important, he says, is to take the new work into account on future missions. Indeed, the scientists in charge of the instruments on NASA's Phoenix lander have paid heed. The primary goal of the mission is to characterize ice and minerals in the martian soil, but the lander will also have the ability to detect organics. The Thermal and Evolved Gas Analyzer (TEGA), originally developed for MPL, heats soil samples at a constant rate, measuring changes in the rate of warming so as to detect phase changes \u2014 when things melt or evaporate they can absorb heat without changing temperature. But Phoenix's TEGA, unlike that on MPL, also boasts a small mass spectrometer which will be used on the output from samples heated as high as 1,000 \u00b0C \u2014 twice the temperature of Viking's ovens, and hot enough to decompose the most refractory compounds. \u201cIf we don't see organic compounds, we will have at least answered the question that it's not because they are of a refractory nature,\u201d says William Boynton of the University of Arizona in Tucson, who is lead scientist for the TEGA project. \u201cIt will also mean that this particular environment was not suitable for protecting organic molecules from being destroyed.\u201d \n               The search for life continues \n             Benner is more or less convinced that the Phoenix ovens will find the mellitic acid and associated salts he has predicted \u2014 which would make it third time lucky for him. He originally hoped to see evidence for the compounds from Raman spectrometers that would have been carried on the Spirit and Opportunity rovers, but a tight schedule saw the necessary lasers cut from the payload. The Beagle 2 mission should also have measured organics in the soil, and had a cunningly contrived device that would have let it take samples from underneath rocks, where the ultraviolet-induced oxidation might not be so bad. But contact with the spacecraft was never established after it left its mothership, the European Space Agency's (ESA) Mars Express. If TEGA does detect organics that reveal themselves only at high temperatures, the chances are strong that they will be from meteorites, not native to Mars. But their persistence would show that the soil was not quite as powerful an oxidant as the post-Viking consensus supposed, which might offer hope that in some places native organic matter might be preserved. What's more, there is a possibility that the icy soil on which Phoenix is hoping to land might be such a place. If the source of the ice \u2014 the presence of which was confirmed by the gamma-ray spectrometer that Boynton flew on NASA's Mars Odyssey orbiter \u2014 is ground water that has welled up and then frozen, it might contain organics derived from reservoirs that are below the reach of the soil's harsh oxidizing properties and subsequently protected by the ice, Benner suggests. If such organics were detected it would not necessarily prove the Viking results wrong \u2014 it would just show that different environments in different parts of Mars offer different levels of comfort for such molecules. Navarro-Gonz\u00e1lez and McKay are both working on the Sample Analysis at Mars (SAM) package that will be part of NASA's Mars Science Laboratory, scheduled for launch in 2009. SAM will contain the first GC-MS since Viking \u2014 Phoenix has no chromatograph \u2014 as well as a laser spectrometer, and it will use chemical extraction as well as pyrolysis, allowing the two techniques to be compared and to be used in a complementary way. In the longer run, some scientists are wondering if there are robust ways to tell whether any organics found are the product of living beings, but they recognize it will be a hard problem. On Earth, it is possible to distinguish life's organic products by the ratio of different carbon isotopes they contain, and SAM should be able to do this \u2014 Phoenix might, too. But scientists do not expect that interpretation of any such results from Mars would be straightforward, since non-biological processes can also have isotopic signatures. An instrument on the ESA's ExoMars mission, slated for launch in 2013, will have the capacity to measure the 'handedness' of any organic molecules it finds. On Earth, life uses left-handed amino acids and right-handed sugars, and samples that reflected a similar prejudice might be seen as good evidence of a living source. But if the molecules have been around for millions of years, they may well have spontaneously rearranged themselves into a random mess, and thus become indistinguishable from molecules created through non-biological processes. Before that next level of uncertainty and debate can be reached, though, evidence is needed that there are organic molecules out there to study. If Phoenix, unlike its ill-fated sibling, survives and sends back data, it will at least have moved the debate on. \u201cIt's more important to be looking forward to future missions than to be stuck on a debate about Viking,\u201d McKay says. \u201cIf we get results from Phoenix and from SAM, then people will have something new to argue about.\u201d Benner, meanwhile, just looks forward to what he sees as the inevitable surprises: \u201cEvery time we go back, it's like going for the first time.\u201d \n                     Phoenix mission on the launch pad \n                   \n                     Mars rover web focus \n                   \n                     Phoenix mission \n                   \n                     Landing sites for MSL \n                   Reprints and Permissions"},
{"file_id": "447900a", "url": "https://www.nature.com/articles/447900a", "year": 2007, "authors": [{"name": "Andreas Trabesinger"}], "parsed_as_year": "2006_or_before", "body": "Can motor racing go green? Andreas Trabesinger asked Max Mosley, head of Formula 1, how he wants the sport to develop energy-efficient technology that will also work in road cars. When Robert Kubica moved to overtake a rival at the hairpin bend in the Canadian Grand Prix on 10 June, he lost control of his Formula 1 car and smashed head-on into a wall in a truly horrific-looking crash. His BMW-Sauber was travelling at 280 kilometres per hour as he tried to pass Jarno Trulli's Toyota, and after ricocheting off the barriers, the car somersaulted along the track before coming to rest with only one wheel still attached. Remarkably, Kubica emerged from the crushed shell of his car with mild concussion and a sprained ankle. His slight injuries are a testament to safety improvements in Formula 1 cars, and to the commitment made by the F\u00e9d\u00e9ration Internationale de l'Automobile (FIA), the sport's governing body, to safety standards. Kubica almost certainly would not have survived a similar crash 15 years ago. The Montreal event, as with all 17 races held in this year's Formula 1 championship, is about the thrill of pushing automotive technology to the very edge of reason. Making sure that the speed seekers are reined in and the sport stays within sensible limits is a difficult task in a contest of such extremes. This task is the responsibility of the FIA, which until recently worried mostly about drivers' safety while keeping the race exciting enough to satisfy the tens of thousands of spectators at the circuit and the tens of millions of television viewers. But this heady mix of reason and adrenaline can have unexpected results. Last year, the FIA set out a 'green agenda' for Formula 1, announcing its intention to turn a sport in which cars guzzle 60 or 70 litres of petrol every 100 kilometres into a catalyst for greener technology for road cars. Max Mosley is the man behind the wheel of the green agenda. In a penthouse high above London's Trafalgar Square, he lays out goals for the FIA to reach by 2009 and beyond. Now in his sixties, Mosley graduated from the University of Oxford, UK, with a physics degree, before going on to study law. He admits that he is no expert when it comes to car technology, but he has been active in motor sports as a driver and team owner since the mid-1960s, and has been president of the FIA since the early 1990s. Mosley's vision of how Formula 1 will contribute to green technologies is simple: make the research done in Formula 1 relevant to road cars, in particular reducing their emissions of carbon dioxide. So how does Formula 1 plan to get there? The FIA has a powerful advantage in that it can rewrite the technical rules for the championship every year. In the past, the FIA restricted the power a car's engine was allowed to produce for safety reasons, typically by limiting the engine size. For the race engineers, the task was to extract the maximum possible power from a given size of engine (see  'Racing through the decades' ), thereby ensuring that Formula 1 remains the fastest form of racing on a twisted circuit. But by the start of the 2011 season, Formula 1 teams will have to crack a new technological nut: making the most of a given amount of energy. From then, the amount of fuel the cars can use in each race will also be restricted. For Mosley the link with road cars is obvious: \u201cThis is precisely the problem that the car industry is trying to solve and indeed the world is trying to solve.\u201d He adds, \u201cAs soon as you look at it like that, you say 'why didn't we do this years ago?'\u201d The reason, he says, is the same as why the road-car industry hasn't done it and that the public hasn't demanded it, because energy is still very cheap. The links between Formula 1 and road cars have strengthened over the past decade. Of the 11 teams racing today, six are sponsored directly by major road-car manufacturers, only two of which \u2014 McLaren-Mercedes and Ferrari \u2014 were running their own teams in 1997, although many manufacturers were involved in the sport as suppliers of engines and other parts. The change came as the road-car industry embraced Formula 1 as a marketing platform, and its involvement has in turn benefited the sport as the costs of racing started to outstrip the available resources. Owning a Formula 1 team is a luxury few can afford, with running costs of up to hundreds of millions of dollars a year. Thirty years ago, the change of a single gearbox could require extra fundraising, but today the sport is flush with money from big-name sponsors and advertising. \n               Over-engineering \n             What has that money achieved? According to Mosley, until the FIA froze engine development at the end of last year's season, an average of 4 milliseconds of lap time were gained for every million dollars spent on engine development, and 20 milliseconds for every million dollars spent on optimizing the aerodynamics. Mosley is clear in his verdict: \u201cBrilliantly clever, amazing engineering but utterly pointless, and irrelevant to the real world, because the engines were inherently inefficient.\u201d He points out that the teams have massive wind tunnels, supercomputers and model shops and they work 24 hours a day just to refine known technology. \u201cThis I want to stop,\u201d he says. \u201cLet's get the really clever people working on the problem the whole world is trying to solve \u2014 which is just as good for Formula 1.\u201d There are two areas in which Mosley thinks Formula 1 can make a lasting contribution to road-car technology, and that in turn will ensure the lasting success of the sport. These will be to recover energy lost through waste heat and braking. About two-thirds of the fuel energy in a car is lost as heat into the atmosphere \u2014 through exhaust gases and coolants. The other third propels the car forwards, but some of that kinetic energy is also lost, ultimately turned to heat, when the driver brakes. From 2009, new regulations for Formula 1 will allow, and thus force, the teams to recover a restricted amount of energy lost in braking, and use it to propel the car. The harder task of recovering the two-thirds of heat lost to the atmosphere is deferred until new regulations are introduced for 2011. At present, the teams are not allowed to recover braking energy because of concerns about how the technology would perform under the extreme forces experienced by a Formula 1 car. The technology that does this is called a kinetic energy recovery system (KERS), better known to drivers of hybrid vehicles as 'regenerative braking'. In a modern hybrid car \u2014 which has both a petrol engine and an electric motor \u2014 the motor's batteries can be charged by either the petrol engine or regenerative braking. The energy can be stored in different forms, but the most viable options for Formula 1 seem to be electrical storage in batteries or capacitors or the use of a flywheel. Although the 2009 regulations will not limit the cars' consumption of fuel directly \u2014 and refuelling will still be allowed during the race \u2014 the ability to regain kinetic energy means extra power for racing. In short, the car gains energy without having to carry extra fuel, and therefore weight. Another advantage of KERS is that the stored energy can be used to improve performance, especially during acceleration out of corners or overtaking of other drivers, giving racing fans a more exciting spectacle. Together, these factors make KERS extremely attractive to Formula 1 engineers. \n               Electric dreams \n             Burkhard G\u00f6schel, chairman of the FIA Manufacturers' Advisory Commission and former board member of BMW, is the 'technical brain' behind FIA's green agenda. He expects that most teams will go for electrical storage systems, either in the form of so-called supercapacitors (which have very high energy density and can store and release energy quickly) or new battery technology based on lithium-ion batteries. Long term, both Mosley and G\u00f6schel are betting that the car industry will move towards using more electric power. \u201cThe electrification of the automobile can be anticipated, there is no way back. We are exactly on the right track with Formula 1, and road cars will follow this track,\u201d says G\u00f6schel. He is convinced that Formula 1 will make electrical energy-storage systems more efficient, smaller and lighter, and that the technologies developed on the way will be directly relevant to road cars. For example, the batteries used by hybrid fuel\u2013electric vehicles are still too heavy, and the amount of energy that can be put in and taken out of a storage device is limited \u2014 problems that Formula 1 research, with its short design cycle and high-performance goals, seems ideally suited to fix. Mosley is confident that the race engineers will deliver: \u201cYou can't say you'll have it ready in two years, because the teams say they need it next week. The people in the next garage will have it next week.\u201d So is Formula 1 heading in the same direction as the road-car industry? Paul Eisenstein, publisher of  TheCarConnection.com  in Pleasant Ridge, Michigan, and observer of the automotive industry since 1979, has no doubt that the car business is under enormous pressure to improve fuel economy; at the same time however, consumers are not willing to compromise on car size or performance. For these reasons, says Eisenstein, hybrids are not doing as well in practice as on paper: \u201cThe cost is high, and the performance of many models mediocre, in particular in terms of fuel economy. Most existing hybrids don't deliver what they promise; that's not good.\u201d The next breakthrough for hybrid vehicles will have to come from making the interplay between the electric motor and the petrol engine more efficient, says Eisenstein. What will race engineers contribute to hybrid technology? \u201cI see no reason why race-car technology shouldn't make it into road cars,\u201d says Eisenstein, \u201cbut such technology will have to meet tough criteria: What is it going to cost? How long is it going to last? Nowadays, such components are expected to deliver at least 100,000 miles.\u201d Whether the technologies developed for Formula 1 will deliver both performance and durability, at reasonable cost, remains to be seen. \n               Heat treatment \n             Hybrid vehicles would benefit from improved regenerative braking, but the recovery of kinetic energy is still playing with only a third of the energy contained in the fuel that the car burns. There is still the two-thirds lost as heat to think about. Getting that energy back is attractive, says G\u00f6schel, but not as simple to address. Formula 1 cars have previously harnessed 'turbocharging' technology to improve the engine efficiency. In a turbocharger, exhaust gases drive a turbine that compresses the air flowing into the combustion chambers, and thus, eventually, allows fuel to be burned more efficiently. Turbochargers were used by the teams during the 1980s, before being banned in 1989 because they gave engines dangerously too much power. The changes to the FIA regulations for 2011 onwards could provide a chance to bring the turbochargers back, but they have yet to be framed. What Formula 1 will bring to turbocharger technology for road cars \u2014 widely used in vehicles from turbodiesels to high-performance sports cars \u2014 is far from clear. \n               Charging up \n             More speculative are new ways to transform waste heat directly into electrical energy by use of physicochemical processes, but G\u00f6schel notes that such devices have very low efficiency. Further developed is a steam turbine that BMW introduced under the name of 'turbosteamer' \u2014 who would have thought that one of those could ever be discussed in the context of a Formula 1 car? \u2014 which is powered by the heat created by the petrol engine, so mechanical energy is recovered from heat. In a similar device, known as a 'turbo-compound', the exhaust gases drive not only the turbine of a turbocharger, but also a turbine in the stream of exhaust gases whose extra power can be used either directly or stored electrically. Unlike turbochargers, none of these devices is yet in production for road cars. How have the Formula 1 engineers reacted to these rule changes? \u201cThe teams don't like it, because we ask them to stop doing things they understand, and do things they don't understand,\u201d says Mosley. G\u00f6schel has noticed a more positive trend: \u201cIn the very beginning, our engineers had some concerns, but now there is a lot of excitement in working on new technology.\u201d Nick Fry, head of the Honda Racing F1 Team, hopes that the rule changes will challenge young engineers, in particular, to come up with new solutions: \u201cIt's an investment in people, in learning and in intellectual property. By pushing this type of technology where we have to perform publicly every two weeks, we must advance very quickly.\u201d \n               Alternate take \n             But why stop with efficient energy recovery? Formula 1 could switch to using biofuels, maybe starting in 2011, says Mosley: \u201cWe would like to use a biofuel. The question is, which one. There are so many competing biofuel systems.\u201d What Formula 1 might end up doing is taking whatever fuel becomes adopted more widely, rather than picking a fuel in advance. Fuel cells relying on hydrogen are not yet being considered for Formula 1, although a small Dutch company is trying to launch a fuel-cell race series (see  'One formula for zero emissions' ). In addition, the FIA has an Alternative Energies Commission that organizes an annual cup race with vehicles that use alternative energies. Mosley is planning to step down as FIA president at the end of his fourth consecutive term in October 2009, so is this green agenda all about his legacy? He admits that it plays a part, but he compares today's environmental concerns (See  'Carbon credentials' ) with the safety concerns that dominated Formula 1 when he first became president of the FIA. \u201cIt's a little bit like the safety debate, in that you work on safety because you don't want to kill anybody, you don't want anybody to get hurt, but also, society won't permit you to kill people like we did in the 1960s.\u201d During that period a driver died every year in Formula 1. \u201cSo, you've got two reasons: you want to do it yourself, but also you have to have regard to what society allows you to do.\u201d Will Formula 1 be perceived as a 'green sport' in the future? \u201cI don't know whether the fans will like it,\u201d says Mosley but he doesn't think that reason and adrenaline are incompatible. As people become increasingly conscious about carbon emissions and fuel economy, he hopes they will still be fascinated by a very fast, very powerful \u2014 but fuel efficient \u2014 Formula 1. In general, Mosley is pragmatic about the effect of the rule changes: \u201cIf it's technically interesting, that's fun, and if it makes a contribution to society, that's good,\u201d but ultimately he thinks Formula 1 needs public support in order to survive. \u201cThe number one thing is to make it so attractive and interesting that the public continues to pay for it.\u201d \n                     Formula 1 racing: Science in the FAST LANE \n                   \n                     F1 \n                   \n                     FIA \n                   \n                     FIA Foundation \n                   Reprints and Permissions"},
{"file_id": "448015a", "url": "https://www.nature.com/articles/448015a", "year": 2007, "authors": [{"name": "Mark Buchanan"}], "parsed_as_year": "2006_or_before", "body": "Fifty years ago, a physics student dissatisfied with the standard view of quantum mechanics came up with a radical new interpretation. Mark Buchanan reports on the ensuing debate. In 1957, a young physicist from Princeton University published his first paper \u2014 it went virtually unnoticed \u2014 and then disappeared from academia. He worked as an engineer and analyst in the defence industry until he died in 1982, at the age of 51. But Hugh Everett's lasting contribution to science, some physicists argue, stands far out of proportion to his paper tally. His first paper, they say, provided a new way to understand one of the most enduring puzzles of quantum physics. Quantum theory has had many spectacular successes, but physicists have always been unsettled by its logical consistency. The theory, exemplified in Schr\u00f6dinger's wave equation, insists that quantum particles such as electrons evolve into weird states of 'superposition' in which, among other possibilities, they can be in two places at once. This equation helped to explain the behaviour of atoms, but ordinary objects such as chairs are also made of quantum particles, so why don't we ever see them in two places at once? One answer to this conundrum \u2014 as expressed in the late 1920s by Niels Bohr and Werner Heisenberg in their famous Copenhagen interpretation \u2014 was that we don't see these weird states because they collapse whenever we try to measure them. Everett, in bold contrast, suggested another solution \u2014 that the superpositions do affect our world, we simply don't notice them. As he pointed out, the maths of quantum theory suggests that when we encounter an object of superposition of say, here and there, that superposition draws us in too; splitting us into one being who sees the object here, and another who sees it there. In essence, as a later physicist put it, Everett claimed that quantum physics reveals a Universe that perpetually splits into \u201cmany worlds\u201d co-existing side by side. This idea was largely dismissed as being too weird, and many alternatives have been suggested since. Empirical tests are unhelpful. Critics argue that experiments alone cannot distinguish between the many-worlds idea and some of the alternatives. \u201cIf experiments continue to verify quantum theory, we're going to be in a very difficult position, having to decide between theories not on evidence but on something else,\u201d says physicist and philosopher David Albert from the University of Columbia in New York. Supporters, though, argue that experiments are indeed helping their case \u2014 by continuing to find no evidence for the mysterious 'collapse' required by the Copenhagen interpretation. Later this year, a select gathering of a few dozen physicists, philosophers and mathematicians will meet at two conferences in Canada and England to explore the status of Everett's ideas, 50 years on. \u201cThe idea,\u201d says philosopher of science Simon Saunders of the University of Oxford, and co-organizer of the meeting in England, \u201cis to assemble the key arguments for and against the interpretation, and to come to a verdict \u2014 if not on the interpretation itself, then at least on the precise nature of its strengths and weaknesses.\u201d \n               Out of control \n             The interpretations of quantum theory \u2014 and the puzzle over why we never experience superpositions \u2014 has challenged physicists ever since the theory became indispensible for explaining atomic and nuclear physics early in the twentieth century. Bohr and the early interpreters of quantum theory got around this puzzle simply by dividing the world into classical and quantum parts, and supposing that quantum theory applied only to the latter. By their thinking, electrons and other quantum particles should evolve with quantum wave-like dynamics, but only until they interact with a classical object, such as a measuring device, at which point any superpositions would collapse, leaving just one outcome. Bohr pointed to the inherently uncontrollable nature of quantum interactions to account for the unpredictability of actual measurements. In the 1930s, Hungarian mathematician John von Neumann formalized these ideas in his 'collapse postulate', which subsequently became one of the standard rules of quantum mechanics. Using the postulate, one can calculate the probabilities of the different outcomes when a 'measurement' takes place, with incredible accuracy. But quantum theory still does not explain in precise detail when or why collapse should occur. For all its success, the theory seemed incomplete. \u201cAs they appear in the textbooks\u201d, says physicist Wojciech Zurek of the Los Alamos National Laboratory in New Mexico, \u201cthe axioms of quantum theory are inconsistent. \n               Bridging the divide \n             And for many physicists, most prominently Albert Einstein, Bohr's split between classical and quantum regimes introduced an unacceptable and arbitrary divide in physicists' view of the world. Quantum theory ought to be able to make sense of classical measuring devices, because these too are made of quantum particles. Restoring such wholeness has been the aim of quantum theorists ever since. Starting in the 1950s, with American physicist David Bohm, theorists have tried their best to develop a coherent quantum theory. Bohm proposed a so-called hidden-variables interpretation, in which quantum particles have unique positions and velocities at all times. Bohm's theory doesn't require collapse, but its unusual mathematical structure \u2014 and the apparent impossibility of ever measuring these hidden variables experimentally \u2014 has made it attractive to only a few physicists. Over the past two decades, various researchers have proposed interpretations that effectively side with Bohr's view, but make it more specific. These theories propose that Schr\u00f6dinger's equation should be modified so that superpositions collapse naturally and, importantly, very rapidly in large, classical objects \u2014 thereby providing a more precise explanation of why we never see such superpositions at our level. They do away with the collapse postulate by effectively bringing it into the Schr\u00f6dinger equation in a mathematically consistent way. Everett's alternative takes yet another perspective. He simply insisted that collapse never happens, and that everything in the Universe always runs in tune with the wave-like dynamics of the Schr\u00f6dinger equation. We seem to see things that look like collapse, he argued, only because we're part of the quantum world too, and so get caught up in its branching superpositions. Take quantum theory seriously, Everett insisted, and it supplies its own interpretation of reality \u2014 with the Universe splitting into multiple parallel worlds. \u201cIn this view,\u201d says theorist David Deutsch from the University of Oxford, \u201cour Universe is only a tiny facet of a larger multiverse, a highly structured object that contains many universes. Everything in our Universe \u2014 including you and me, every atom and every galaxy \u2014 has counterparts in these other universes.\u201d All three approaches have their adherents, but for what seems to be a growing number of physicists, especially those working in quantum information and cosmology, it is Everett's alternative that wins out. \u201cThese fields follow the idea that there's nothing wrong with taking quantum theory to its logical limits,\u201d says Zurek. \u201cAnd it was Everett who gave us the permission to do this.\u201d \n               Two-timing particles \n             Everett's idea is gaining popularity, at least in part, because quantum theorists have made progress in solving some of its earlier problems, which were not long ago thought to be fatal. One problem is that although Everett's view implies that the world splits into multiple branches, it does not, on its own, give a clear recipe for how this branching should take place. In standard quantum theory, for example, an electron in a fuzzy ill-defined state can be thought of as a superposition of multiple states of well-defined position, or, alternatively, as a superposition of states with fuzzy positions but well-defined velocities. Neither picture is more correct than the other. Similarly, Everett's theory doesn't provide an obvious right way to identify the multiple co-existing branches that make up a superposition. \n               Deceptive behaviour \n             A body of ideas known as decoherence theory was developed by physicist Dieter Zeh of the University of Heidelberg in Germany in the 1970s, and has been expanded on by Zurek since then. The theory notes that because quantum systems interact with their environment they do not remain in superpositions for long, but instead tend to 'decohere'. In effect, environmental interactions make quantum systems behave as if their superpositions had collapsed, when in fact they have just become so entangled with the surrounding environment that no experiment, for practical purposes, would be able to detect them; and this is especially true of systems that involve many particles. Many physicists suggest that, when coupled with decoherence, the many-worlds idea provides an attractive package. That's because decoherence singles out the familiar states of classical objects as being more robust and therefore observable. The idea even has some experimental support, and seems to remove much ambiguity from the many-worlds view. Another long-standing problem for Everett's view is probability, one of the bedrocks of quantum physics. Using the collapse postulate, the magnitude of the wavefunction can be used to calculate probabilities for the position or velocity of an electron. But the many-worlds perspective, with its insistence on the never-collapsing evolution of quantum systems, says nothing at all about probabilities. \u201cIn Everett,\u201d says philosopher of physics Wayne Myrvold of the University of Western Ontario in Canada, \u201cthere seems to be no room for statements of probability at all.\u201d Even so, recent work by Deutsch and by philosopher of physics David Wallace, also at the University of Oxford, suggests that quantum probabilities can find a way into the theory \u2014 through consideration of how human brains, or those of other organisms, register changes in their environments. They used decision theory \u2014 the logical science of optimal decision making \u2014 to explore how an individual living in a branching many-worlds universe would behave if he or she were trying to anticipate real-world outcomes as accurately as possible. They conclude that such individuals, confronting new outcomes as they entered new branches, would be driven to assigning probabilities to those branches, and would end up using the very same probabilities prescribed in the collapse postulate. Although not everyone is convinced, some physicists think that Deutsch and Wallace succeeded in showing how the probabilities of quantum theory arise naturally from Everett's theory. \u201cI'm not fully convinced that it all makes sense,\u201d says Myrvold, \u201cbut great strides have been made, and it's less implausible than I used to think. \n               Bigger and better \n             Everett enthusiasts also point to the increasing sophistication of modern experiments, which continue to verify the predictions of quantum theory, with no known exceptions. Using the techniques of atom trapping and quantum optics, experimentalists have observed quantum superpositions in systems containing more and more particles, as Everett's theory predicts ought to be possible. \u201cThe many-worlds view is slowly becoming the majority preference among physicists,\u201d says Tony Sudbery, a mathematician at the University of York, UK, \u201cat least in part because we see quantum theory working, without any hint of collapse, for systems that are larger and larger.\u201d But Albert says that the idea that experiments lend support to the many-worlds view is wishful thinking. As he points out, all experiments that verify quantum mechanics are consistent with the many-worlds theory and with several of the alternatives, including Bohm's interpretation of hidden variables. Observing wavefunction collapse in an experiment would contradict the Everett view, but the failure to do so might simply reflect the difficulty of doing controlled experiments on quantum superpositions. Physicists hope that future experiments will challenge the idea of wavefunction collapse more directly by testing some of the theories that modify the Schr\u00f6dinger equation. \u201cUnfortunately,\u201d Albert says, \u201cat this point, we're really very far from being able to do the necessary experiments.\u201d The current record for a quantum superposition is around 1,000 particles, although some researchers have bold plans to search for superposition collapse in a macroscopic object, perhaps a virus containing as many as 10 6  particles. \n               Seeing is believing \n             This is where the debate at the two meetings this year is likely to kick off: with arguments over what experiments can ever prove and whether theorists have really fixed all the conceptual holes in the many-worlds perspective. Although both events have been organized by Everett enthusiasts, they're unlikely to be one-sided affairs, as many researchers still have grave doubts about his ideas. \u201cEven if one accepts 'many worlds',\u201d says physicist Roger Penrose of the University of Oxford, \u201cone needs a theory of consciousness, in effect, to explain the physics that we actually perceive going on in the world.\u201d Without it, he argues, the many-worlds theory is putting the cart before the horse. Even Albert, who is lukewarm about the many-worlds view but positive about recent theoretical work, says, \u201cMy own guess is that, in the end, it's not going to work. But at least the people nowadays visibly understand what the worries and problems are.\u201d Saunders and other Everett supporters suggest that those who hold out against the many-worlds picture are often driven, whether they say so or not, by an instinctive dislike of its non-intuitive consequences. \u201cQuantum theory under the Everett interpretation is fantastic \u2014 too fantastic for the great majority of physicists to take seriously,\u201d Saunders says. But however they do it, as physicists struggle with the many-worlds interpretation, and the many alternatives, they are at least paying respect to the central idea of Everett's thesis: to take quantum theory very seriously indeed. See Editorial,  \n                     page 1 \n                   , and Commentary,  \n                     page 23 \n                   . \n                     2020 computing: Champing at the bits \n                   \n                     Natural selection acts on the quantum world \n                   \n                     The Perimeter Institute meeting \n                   \n                     Conference at Oxford \n                   \n                     Stanford Encyclopedia of Philosophy entry on many worlds \n                   Reprints and Permissions"},
{"file_id": "4471049a", "url": "https://www.nature.com/articles/4471049a", "year": 2007, "authors": [{"name": "Kris Novak"}], "parsed_as_year": "2006_or_before", "body": "Smoking was banned in Californian bars a decade ago, and this week England follows suit. But Kris Novak finds that epidemiologists are still arguing about the effects of second-hand smoke. The reactions to the English ban on smoking in all enclosed public places on 1 July are almost as predictable as a chain smoker's second pack of the day. \u201cFirst you'll read that all the pubs are going broke, then that people are blocking the street when they stand outside smoking, creating traffic congestion, and then everyone will be told that the ban is wildly unpopular,\u201d says Stanton Glantz, director of the Center for Tobacco Control Research and Education at the University of California, San Francisco. \u201cIt happens every time these bans go into effect. After the wave of bad press, everything will calm down and next year everyone will say, 'What was the big deal?'\u201d Glantz should know \u2014 he was involved in the first widespread ban on smoking in workplaces, including bars and restaurants, which has been in effect in California for almost ten years. Lately, he has advised public-health officials from around the world on how to enact similar legislation. Back in 1998, banning smoking in bars was not easy, even in clean-living California. The ban met with resistance from groups claiming that making bars smoke-free would devastate businesses, deny adults the freedom to smoke and be too difficult to enforce. The tobacco industry made nine attempts to repeal the law and spent US$18 million on a public-relations campaign against it. However, a smaller but well-organized public-health campaign overcame resistance to the ban. Glantz says that it's important that governments explain the dangers of second-hand smoke to non-smokers. In his view, passive smoking has been the Achilles' heel of tobacco companies, taking the focus off the rights of smokers and placing it on the health of non-smokers. True to this script, over the past six months, the UK government's health department has spent \u00a38.7 million (US$17.3 million) on an advertising campaign to educate English citizens about second-hand smoke and prepare businesses for the change. \n               Coronary concerns \n             Despite all the strongly worded advertising campaigns, the health benefits of smoking bans for non-smokers have been controversial. One of the biggest rows concerns the links between second-hand smoke and heart disease \u2014 an argument in which Glantz has been a key player. Richard Peto at the University of Oxford, who has been studying tobacco use for more than 30 years, says: \u201cPassive smoking must kill some people, but the big question is how many.\u201d Smoking has been banned in all workplaces, including bars and restaurants, in more than 20 countries (see table) and in many US states. In March 2004, Ireland became the first country to enact such a ban. \u201cIreland is a huge success story,\u201d says Glantz, paving the way for bans in Scotland, Wales, Northern Ireland and now England, the largest nation to do so. Glantz cited the active support of Ireland's health minister and television campaigns that warned people of the dangers of second-hand smoke. Because California has the longest history of smoking restrictions, epidemiologists often turn to the state's public-health data to determine the effects of smoking bans, on both smokers and non-smokers. Back in 1988, California voters approved the California Tobacco Control Program (CTCP), which increased tax on cigarettes to fund tobacco education programmes, and went on to ban smoking in workplaces (including restaurants) in 1995 and in bars in 1998. Several studies show that the CTCP helped Californian smokers to quit. In 1988, 23% of all Californians smoked and by 2006 that number had dropped to 13% 1 . Similar effects have been seen in Europe: in Scotland, cigarette sales fell by 8% in the first year of its ban, and Ireland has seen about a 5% drop in the number of smokers since the 2004 ban, although 25% of Irish people still smoke. But were these changes on their way, with or without a ban? Peto notes that smoking-related deaths in the United Kingdom have already declined by almost 50% since the late 1960s. \u201cSmoking bans will probably affect some additional people,\u201d he says, but separating that effect from the overall decline in smoking is a complex task. The California experience suggests that legislation has a role. More than half of California's former smokers agreed that the smoke-free workplace and bar laws made it easier for them to quit, and most smokers reduced their cigarette consumption because of the bans 1 . According to the European Commission, the United Kingdom has the highest percentage of European smokers who are trying to quit (46%), so the 1 July ban could be a step in the right direction. \n               Tips for non-smokers \n             If smokers are giving up in droves, is their health also improving after the bans? Once again, the California experience is helpful because public-health data can be compared with those for US states without smoking bans. Between 1988 and 2002, rates of lung and bronchial cancer declined four times faster in California than in the rest of the United States 1 . Peto, however, is sceptical of studies that associate mortality trends in large populations with a single event. \u201cThe number of deaths from vascular disease and cancer have been falling for multiple reasons, such as improved treatments; at the same time, you have increases in disease rates because of obesity \u2014 there are just too many things going on to attribute changes in mortality to one single thing,\u201d says Peto. He thinks that it takes decades for population-wide changes in health trends to become apparent. But what about the health benefits of smoking bans for non-smokers? These are even more uncertain. Banning smoking does clean up the air in bars \u2014 an Italian study comparing air quality before and after their 2005 ban found a 95% reduction in airborne nicotine 2 . Within months of bans on smoking in bars in Ireland and Scotland, bar workers reported fewer respiratory problems, and breath, saliva and blood samples contained fewer biomarkers of tobacco exposure than before the bans 3 , 4 . According to Smokefree England, an educational website funded by the UK health department, second-hand smoke contains more than 4,000 different chemicals, such as carbon monoxide, and more than 50 carcinogens, including chromium, vinyl chloride and benzene. So does cleaner air prevent disease in non-smokers? Smokefree England claims that exposure to second-hand smoke increases a non-smoker's risk of getting lung cancer by 24% and risk of heart disease by 25%. But where do these numbers come from? \u201cThese are generally accepted numbers,\u201d says epidemiologist Alfredo Morabia, at Queens College in New York, who cites the 2006 US surgeon general's report,  The Health Consequences of Involuntary Exposure to Tobacco Smoke , the US government's most detailed statement ever on second-hand smoke 5 . For lung cancer, there is general agreement with the report's assessment. Studies of non-smokers who live with a smoker suggest that the risk of developing lung cancer is about 25 times higher in smokers than non-smokers. Because non-smokers' exposure to second-hand smoke is estimated to be about 1% that of a smoker, a 24% increase in lung-cancer risk (0.24-fold) fits with expectations. \n               Cardiac effects \n             But the data supporting the link between second-hand smoke and cardiovascular disease are more controversial. The surgeon general's report states that \u201cpooled relative risks from meta-analysis indicate a 25\u201330% increase in risk of coronary heart disease from exposure to second-hand smoke\u201d. Although most epidemiologists think there is a link, it's the size of the effect that surprises them. \u201cIt seems to me that a 25% increase is not plausible,\u201d says John Bailar, a biostatistician at the National Academy of Sciences in Washington DC, who thinks the effect should be proportional to exposure, as it is for lung cancer. \u201cRegular smoking only increases the risk of cardiovascular disease by 75%, so how could second-hand smoke, which is much more dilute, have an effect one-third that size\u201d? Bailar says that even if a non-smoker took in 10% as much smoke as a smoker, which is a high-end estimate, his increased risk would be only 7.5%. One explanation offered by Glantz and other heart researchers for the higher-than-expected effect of second-hand smoke on coronary heart disease is that the 'sidestream' smoke a non-smoker breathes is more toxic, per gram of total particulate matter, than the 'mainstream cigarette smoke' that a smoker inhales 6 . So small amounts of sidestream smoke might be more likely to trigger heart disease than the smoke inhaled through a filtered cigarette. Another explanation is that a very low threshold of exposure to second-hand smoke is required for disease risk to escalate. But this idea is hard to test. Epidemiologists lament that exposure is one of the hardest factors to quantify. \u201cThere is no unit of exposure, and levels of exposure vary based on what size room people are in, the number of smokers in the room, the level of ventilation and so on,\u201d says Morabia. Despite these concerns, the surgeon general's report takes a hard line on exposure, stating that there is no 'safe' level. According to Terry Pechacek, one of the authors of the report and associate director at the Office on Smoking and Health at the US Centers for Disease Control and Prevention in Atlanta, Georgia: \u201cExposure to second-hand smoke for even a short time can have adverse health effects \u2014 this is not subject to debate. Compounds in tobacco smoke have the ability to cause cancer in humans, it's just a probabilistic game of whether they will cause death in a certain individual.\u201d \n               Burning opportunity \n             In principle, smoking bans provide a unique opportunity to study populations before and after reductions in second-hand smoke exposure. But if research into the incidence of heart attacks is any guide, the results of such studies are often far from clear and can cause more controversy. Glantz reported in 2004 that during a six-month smoking ban in Helena, Montana, the number of heart attacks dropped by 40% compared with the same months in other years 7 . The study, cited in the surgeon general's report, was criticized for the small number of cases studied and the large month-to-month variations in incidence of heart attacks. Since the report, several studies have reported a drop in hospital admissions for heart attacks after smoking bans in the Piedmont region of Italy (11% drop) 8 , and in small cities in Colorado (27%) 9  and Ohio (39%) 10 . But not all epidemiologists are impressed. \u201cIt's quite common to see major year-to-year changes in heart attacks, sometimes as much as a 50% increase or decrease,\u201d says Michael Siegel, an epidemiologist at Boston University School of Public Health. Attributing such changes to a ban is impossible, he says. Worse, none of the studies recorded whether the changes occurred in non-smokers or in smokers, yet the effects of the ban are frequently attributed to reductions in second-hand smoke exposure. The lead author of the Ohio study agreed that a prospective study that collected data on groups of non-smokers and smokers in advance of a ban \u2014 and then followed them up for a few years after \u2014 would be ideal, but this would take longer and be costly. Glantz has heard all these concerns before: \u201cWhile the numbers were small in our original Helena study,\u201d he says, \u201cthe more recent ones have been in larger places.\u201d In his view, the effects have now been well documented by several studies, all of which accounted for monthly variations in heart attacks: \u201cThat is not a legitimate criticism of any of them.\u201d \n               Lethal limits \n             Certainly, public-health officials seem convinced. Pechacek says that the general consensus in tobacco researchers is that the drop in heart attacks occurs mainly in non-smokers. Smokefree England suggests that just 30 minutes of breathing in second-hand smoke can raise your risk of having a heart attack. But can you really become ill just by sitting in a bar next to a smoker? \u201cSaying that just a little exposure is killing people is going overboard,\u201d says Siegel, who worries that when researchers exaggerate their findings, they lose credibility with the public. \u201cI agree that second-hand smoke is a tremendous health hazard, but no one is going to have a heart attack from 30 minutes of exposure.\u201d Siegel thinks that banning smoking in outdoor places is going too far and risks losing support for smoking bans overall. Smoking has been banned on 25 California beaches, and this week the Beverly Hills City Council approved a ban in nearly all outdoor dining areas. \u201cWe should focus efforts on the remaining areas in which workers are not protected,\u201d he says. \u201cMy biggest concern is for the waiters and bartenders who spend 40 hours each week in very smoky environments.\u201d He's also worried about the health of smoking research itself, which he sees being compromised by methodological flaws and over-interpretation of results. And although prospective studies would be costly, Siegel argues that funding for a large prospective study of the effects of smoking bans on non-smokers is warranted. \u201cAlthough I wholeheartedly support smoking bans, I still believe that we must use solid science to advocate for such bans and that a noble end \u2014 improving public health \u2014 does not justify the compromise of our scientific principles.\u201d Other epidemiologists worry that with all the focus on second-hand smoke, the attention is being drawn away from the real issue: the dangers of smoking itself. \u201cThe key point to remember is that smokers kill themselves,\u201d says Peto. \u201cA few are probably killing other people, but half of all smokers will be killed by their own tobacco.\u201d \n                     Passive-smoking study faces review \n                   \n                     Tobacco money sparks squabble at California universities \n                   \n                     Passive smoking may speed cancer growth \n                   \n                     All in a puff over passive smoking \n                   \n                     Smokefree England \n                   \n                     Smoking mortality and statistics \n                   \n                     The Health Consequences of Involuntary Exposure to Tobacco Smoke \n                   \n                     US Centers for Disease Control: Smoking and Tobacco Use \n                   Reprints and Permissions"},
{"file_id": "447905a", "url": "https://www.nature.com/articles/447905a", "year": 2007, "authors": [{"name": "Josie Glausiusz"}], "parsed_as_year": "2006_or_before", "body": "From acid mine drainage to the bowels of the Earth, Josie Glausiusz reports how researchers are taking great pains to grow recalcitrant bacteria. To reach one experimental site \u2014 inside Lechuguilla Cave, New Mexico \u2014 Diana Northup and her team must venture 350 metres down into the Earth, hiking along several kilometres of obstacle-strewn passages. For days they rappel down pits, traverse narrow ledges bordering steep drops and pristine lakes, and clamber over boulders \u2014 all the while carrying heavy packs of equipment. When they finally arrive at their sampling areas, they must change into clean suits, including bonnets and sterile gloves. \u201cIt's hard to convey to you how filthy you can become in four to five days,\u201d Northup says. \u201cYou're just constantly drenched in sweat. These crusts we study get all over you, gypsum gets all over you, and the sweat helps plaster it all onto you. It's just lovely.\u201d Northup, a microbiologist at the University of New Mexico in Albuquerque, and the inter-university SLIME team (Subsurface Life in Mineral Environments) are looking to find how cave bacteria deposit oxidized iron and manganese crusts on the walls of underground caverns. Her first attempts to grow these microbes in the lab were thwarted. All that appeared on her agar plates were 'weeds', fungi that hikers had tramped into the caves on their boots. Hence the clean suits, to prevent contamination. To get the right incubating conditions, Northup came up with a simple solution: cultivate the bacteria in glass tubes inside the caves themselves, providing the precise environment they prefer: total darkness, low temperatures and high humidity. Northup's efforts may seem extreme, but they illustrate the lengths to which some microbiologists will go to culture the seemingly unculturable. According to a common estimate, some 99.9% of microbes will not reproduce in Petri dishes lined wth nutrient-rich agar \u2014 a culturing technique virtually unchanged since its invention in the early 1880s. Most have more stringent criteria, including rare minerals, specific biochemical signals or the organisms with which they usually cooperate. In other words, they need the comforts of home. Difficulties in culturing bacteria have helped spur metagenomics, in which researchers descend on a sample \u2014 a bucket of sea water, a patch of human skin or a handful of soil \u2014 and sequence all the microbial DNA within. Such techniques can start to reveal how many bacterial species can be found in any spot, or how many gene variants appear in the collective population. For example, the J. Craig Venter Institute announced in March 2007 that its  Sorcerer II  Global Ocean Sampling Expedition had discovered 6 million new genes and thousands of new protein families. Mitchell Sogin's group at the Josephine Bay Paul Center of the Marine Biological Laboratory in Woods Hole, Massachusetts, has been sequencing 16S ribosomal RNA from seawater samples to measure species numbers and found microbial diversity much higher than had been reported before. Exciting as these results are, they also highlight the culture gap. Some biologists remain dedicated to taming recalcitrant bacteria in the lab, challenged and sometimes aided by metagenomic findings. Using specially designed diffusion chambers, mud-filled vessels, microarrays and bioreactors, they mimic the conditions in which the bacteria live naturally. They are growing what no one has grown before, which should reveal more about the organisms than a mere gene census, says evolutionary biologist Lynn Rothschild of NASA's Ames Research Center in Moffett Field, California. \u201cJust because they have a gene, they may not use it. Or we may not learn when they use it,\u201d she says. \u201cUnculturability is not a biological characteristic, it's a human failing. They're not unculturable; they're just not cultured yet.\u201d Some researchers have what Rothschild calls \u201ca green thumb for microbes\u201d. Belinda Ferrari of Macquarie University in Sydney, Australia, has developed a novel microcultivation method, using a mud slurry as a medium for growing soil bacteria. Using this system, she has cultivated a wide diversity of bacteria: they include those called TM7, found in soil, bioreactor sludge and the human mouth. Nutrients come only from the mud, a source ideally suited for these microbes, which may suffer in the high sugar levels that suit lab 'weeds'. \u201cIf you're in a pot full of jam and not much else, you'll just overdose and get sick from too much sugar,\u201d Ferrari explains. \n               From culture to cash \n             There are commercial spinoffs for extreme culture. In the Netherlands, microbiologists Mike Jetten and Marc Strous of Radboud University Nijmegen have enriched anaerobic ammonium-oxidizing (anammox) bacteria and nitrate-dependent anaerobic methane-oxidizing bacteria. The latter, collected from a canal contaminated with agricultural runoff, were grown in a bioreactor with a nutrient-limited supply of carbon dioxide, nitrate and methane. The two succeeded in culturing colonies of these microbes after a year of trial and error. Anammox bacteria are now being used to clean waste water on a large scale in the Netherlands and Japan. Meanwhile, Kim Lewis and Slava Epstein of Northeastern University in Boston, Massachusetts, are trolling the depths of the uncultured for new antibiotics using a diffusion chamber in which microbes are suffused in the conditions of their natural environment \u2014 soil, for example, or sea water plus marine sediments. Lewis and Epstein have founded the company NovoBiotic to capitalize on their cultures. \u201cThe practical benefits are enormous,\u201d Lewis says. \u201cIf you want to discover new stuff, you want to go to organisms you haven't seen before. It's reasonable to assume that 99% of the remaining bacteria will have at least some useful antibiotics.\u201d The spelunking Northup has begun to find antibiotics in her samples. Initially, though, she strove to grow crust-forming cave bacteria to understand their basic biology. \u201cOne of the reasons we culture rather than do DNA sequences is because we want to catch them in the act of precipitating the minerals, so that we can say definitely, 'These guys can do it',\u201d she says. While underground she and her team scrape crust off the cave wall and stab it into a glass tube filled with 'sloppy' agar. An iron carpet tack or reduced manganese at the bottom of the tube provides metal, and rock dust from limestone in the caves provides trace minerals that the microbes need. The bacteria form bands in the agar at different oxygen levels and precipitate iron and manganese deposits. Because these microbes grow so slowly, Northup and her colleagues usually leave their cultures percolating for years. She has already cultivated a variety of microbes from New Mexico caves, including iron- and manganese-oxidizing species of  Bacillus ,  Caulobacter  and  Alcaligenes : odd-looking bacteria whose strange morphology she refers to as \u201cbeads on a string\u201d and \u201chairy sausages\u201d. She has also found samples of  Actinomycetes , bacteria that are known to produce more than 4,000 antibiotics, as well as enzymes that could be of use in biotechnology. \n               Antibiotics and ecology \n             Northup now has a student searching her  Actinomycetes  samples for novel antibiotics. \u201cBecause this is fairly unexplored habitat, we're hoping to find some rarer ones,\u201d she says. \u201cWe'd also maybe like to characterize what habitats in caves tend to be richer in organisms that produce antibiotics \u2014 to test the hypothesis that in low-nutrient environments they're more likely to produce these secondary metabolites to keep their neighbours at bay.\u201d Such ecological questions could have widespread implications. A group led by Steve Giovannoni, a microbiologist at Oregon State University in Corvallis, has cultivated at least 11 strains of SAR 11, a marine microbe that is not only the smallest independently replicating bacterium known but also has the smallest genome ever seen in a free-living cell. What's more, SAR 11 \u201cis possibly the most abundant organism in the oceans\u201d, says Giovannoni. It converts organic matter into carbon dioxide, and therefore plays a huge role in global biogeochemical cycles. All the more reason, therefore, to understand its basic biology \u2014 something Giovannoni thinks is best done through pure culture. \u201cMetagenomics is fantastic,\u201d he says, but \u201cwith an organism like this, it's vastly superior to have a culture. If hypotheses emerge from a genome sequence, you can then test them. Also, at least 30% of the genes in every microbial genome cannot be identified under the best of circumstances.\u201d Even as some microbiologists seem aligned with pure culture techniques, others have used metagenomics as a guide in raising obstinate microbes. Jill Banfield of the University of California, Berkeley, and her colleagues used 'shotgun' sequencing methods to characterize five species of iron-oxidizing bacteria and archaea in biofilms from an acid mine-drainage system in Iron Mountain, California, and discovered that only one carried the genes that code for enzymes to fix nitrogen. \u201cWe got a pretty complete inventory of the pathways in each organism that's present,\u201d Banfield says. \u201cBecause we had near-complete genomes we could say, 'This organism \u2014 ha ha! \u2014 has the ability to also fix nitrogen, whereas the others don't.'\u201d By raising them in a sulphuric-acid bath supplemented with iron, with no nitrogen except the gaseous form, the team was able to isolate the nitrogen-fixing bacterium,  Leptospirillum ferrodiazotrophum . As metagenomic efforts ramp up, more of these extreme-culture techniques are likely to follow because the two approaches inform each other. \u201cYou get totally different bits of information,\u201d says microbial ecologist Anna-Louise Reysenbach of Portland State University in Oregon, who for the first time has cultured an acid-loving, thermophilic, sulphur-reducing species of archaea called  Aciduliprofundum boonei  from deep-sea vents at Valu Fa Ridge between Tonga and Fiji. \u201cI love being able to have the organism in culture, but also knowing what its distribution is like. I definitely think that both have great value. There shouldn't be one without the other.\u201d \n                     Microbes reveal extent of biodiversity \n                   \n                     European funding targets big biology \n                   \n                     Genomics: Discovery in the dirt \n                   \n                     Gut bugs sequenced \n                   \n                     Antibiotics Hot Topic \n                   \n                     Antibacterials Web Focus \n                   \n                     Slime team site \n                   \n                     Sorcerer 2 expedition \n                   \n                     Ames Research Center \n                   \n                     Jill Banfield\u2019s site \n                   Reprints and Permissions"},
{"file_id": "4471046a", "url": "https://www.nature.com/articles/4471046a", "year": 2007, "authors": [{"name": "Daemon Fairless"}], "parsed_as_year": "2006_or_before", "body": "How did a little Spanish province become one of the world's wind-energy giants? Daemon Fairless reports. From a wheatfield overlooking the village of Iratxeta in the Spanish region of Navarre, you can appreciate the unique countryside and its contrasts: the pale green of ripening grain setting off the dark-green mountain forests behind, country houses newly rebuilt on medieval foundations, and giant wind turbines, brilliantly white and strikingly erect, their slowly moving blades driven by the breeze that ripples the wheat. \u201cThat was the first one,\u201d says Jos\u00e9 Roman G\u00f3mez, pointing south to the Guerinda wind park, a phalanx of turbines running along the ridge of a nearby hill. He draws an arc along the horizon with his finger: \u201cThat one was built after. And there is a different one. And, over there, another.\u201d Iratxeta, an isolated cluster of stone houses nestled into the hills about 30 kilometres south of the regional capital, Pamplona, is encircled by wind farms. Their turbines loom over the hills like an invading army. But it's an occupation for which G\u00f3mez is grateful. G\u00f3mez, who has been managing the local council of the villages of Iratxeta and neighbouring Leoz for the past two decades, is a tranquil and friendly man in his mid-50s. Fifteen years ago, before the wind boom hit this area, the two villages had only about 150 residents between them. \u201cThere was no running water here,\u201d G\u00f3mez explains. \u201cThere was no waste collection or buses. Most of the villagers were old and waiting to die.\u201d Now, however, the villages have nearly doubled in size compared with before the wind boom. In addition to providing running water and waste collection, the rent from wind-farm operators has enabled the local council to hire four full-time health workers to care for the community's elderly people. \u201cNow,\u201d G\u00f3mez says, \u201cpeople from Pamplona are investing here.\u201d Wind farms have transformed Spain's landscape over the past decade and Navarre, the least populated of the nation's 15 mainland autonomous regions, has been at the forefront. Located in the northeast, between the Basque country and the French border, Navarre generates almost 60% of its electricity from renewable sources \u2014 the vast majority from wind. The region's most recent energy plan, released in May, aims to increase this proportion to more than 75% by 2010. \n               From water to wind \n             Although it would hardly be fair to attribute Navarre's renewable-energy boom to just one person, one name invariably arises when discussing wind power, that of Estaban Morr\u00e1s. Morr\u00e1s is the executive director of Acciona Energia, a subsidiary of the Spanish engineering group Acciona, which has grown to be the world's foremost wind-park developer, according to the Madrid-based Spanish Wind Energy Association, an industry body. Dark-suited and seated behind a boardroom table at the company's headquarters on the outskirts of Pamplona, he looks very much the person you would expect to see running a company with first-quarter revenues in 2007 of \u20ac252 million (US$337 million). But when Morr\u00e1s founded the company in 1989, he had more of an Indiana Jones air about him. \u201cI did so much research, running through rivers and forests looking for locations for new hydroelectric projects, my sweaters, my clothes, disintegrated,\u201d he says. Until the early 1990s, small, decentralized hydroelectric stations dotting the rivers that flow out of the Pyrenees were the only significant form of renewable energy in Navarre. Many had been around since early last century, providing power to local small industry. Morr\u00e1s founded his company, originally called EHN, with the plan of buying up the existing mini-hydro-stations, connecting them to the grid and building more. But it was becoming harder to find new sites. \u201cI spent all year in 1992 desperately looking for new locations,\u201d he says. \u201cBut it was impossible. I understood my company had no future.\u201d But, in 1994, on a trip to France, he saw the future. \u201cThe most significant moment for me,\u201d he recalls, \u201cwas visiting a wind farm in Montpellier. My first thought was that a single turbine was capable of producing more energy than the last mini-hydro plant we had built. It was really difficult for me to sleep that night.\u201d He took to the new technology with gusto and ambition. Morr\u00e1s says that in his first meeting in the mid-1990s with Vestas, the Danish company that he selected as his turbine supplier, he told the then chief executive, Johannes Poulsen, that Spain would have 1,000 megawatts of wind-power capacity by the end of the century. At the time, it had virtually none. Poulsen, he says, was incredulous: \u201c'You are crazy,' he told me: 'Denmark has only 400 megawatts after 20 years of effort.'\u201d Morr\u00e1s's answer was \u201cwith Denmark's experience, we can go faster than you.\u201d And they did. With 11,615 megawatts of installed wind capacity at the end of last year, Spain is second in the world to Germany in capacity, beating the United States by just 39 megawatts (see graphic). The energy actually generated is much less, however, thanks to the caprice of the wind and maintenance downtime for the turbines. The 23,372,000 megawatt-hours generated in 2006 is only 23% of the turbines' full capacity. Nevertheless, it provided just over 8.5% of Spain's electricity: only Denmark boasts a higher percentage. On a particularly blustery day this March, wind generation was providing 27% of Spain's electricity supply \u2014 surpassing, for a fleeting moment, the contributions of both nuclear and coal-fired plants. \n               Power to the people \n             The wind boom in Navarre is among the most intensive in Spain. Throughout the 1980s and 1990s, the government of Navarre was eager to develop new industries, especially as the region seemed overly dependent on its single large industrial employer, a Volkswagen car plant. But with a limited local power supply, Navarre's infrastructure wasn't particularly appealing to outside investors. Morr\u00e1s's wind-power plan offered a solution, and promised an industry in its own right \u2014 more enticing for the government at the time than any environmental argument. Well before Spain's national government started to guarantee profitable prices for selling wind-generated power to utilities, Navarre had bought into Morr\u00e1s's vision. His first wind farm, El Perd\u00f3n, was built south of Pamplona in 1994; since then the government of Navarre has approved well over a thousand turbines in 32 wind farms. From 1995 to 2004, it invested more than \u20ac136 million in renewable-energy enterprises, contributing up to 30% of the initial funding and providing tax credits for investors. The current renewable-energy plan (2005\u20132010) has earmarked an extra \u20ac240 million for investment in renewables. Navarre's installed wind capacity is about 950 megawatts \u2014 8.5% of the country's total on 2% of its land, and nearly two-thirds that of its neighbour, France. For a while it looked as though Navarre might become entirely self-sufficient by 2010, producing as much electricity through renewables as it consumed. \u201cOur goal in this area was more aggressive than it is now,\u201d admits Jos\u00e9 Javier Armend\u00e1riz, the region's minister of industry, technology, commerce and labour. Morr\u00e1s's aspirations haven't changed, however. \u201cI think it's possible for Navarre to obtain 100% of its energy with renewables,\u201d he says. \u201cThis project is essentially one of companies and people in the private sector. I don't understand the position of the government. The target for me is to obtain 100%.\u201d His goals to expand renewable energy go well beyond the region. \u201cI am convinced that it is possible for the world to have an energy model that is 100% renewable. Navarre could do nicely out of this. By 2003 the region had more than 50 companies active in various aspects of renewable energy (J. Faulin  et al .  Energy Policy   34,  2201\u20132216; 2005), and that year it was commended as having the best regional policy in Europe at the European Conference for Renewable Energy in Berlin. EHN became lucrative enough to attract the attention of the Acciona Group, which after first taking a small stake, took it over entirely in 2005. The Navarrese wind boom also gave birth to Gamesa E\u00f3lica in Pamplona, initially created as a local supplier for EHN, which has grown to be one of three turbine producers vying with each other for second place to Vestas in the world turbine market. Pamplona is on its way to becoming the official hub of Spanish renewable energy. The city is home to the National Renewable Energy Centre, a new research facility that focuses on developing commercial applications in wind, solar and bioenergy, and to two national training facilities in renewable technologies. Last autumn, the Public University of Navarre in Pamplona launched the country's first graduate programme for electrical engineers in wind and solar electricity, to meet the growing demand for specialists. \n               Running out of land \n             The region's success has led it into uncharted waters, however. Although much of the rest of the world is still waking up to renewables, Navarre has reached possible saturation point, at least for wind power. In 2004, the regional government stopped approving proposals for new wind farms. One reason, it says, is the effect on the landscape. According to Armend\u00e1riz, the government sensed that further development might lose it the public support that, he says, was one of the reasons wind power could gain such traction in the region. Another reason is that wind-power capacity can grow without building new farms, by installing newer, higher-capacity turbines. Several of the region's first wind parks, those with 500-kilowatt machines, will be fitted with newer 3-megawatt models, starting this summer. Even so, having so much wind energy contributing to the network comes with technical concerns. The amount of energy that each wind turbine produces depends entirely on what the wind is doing, and can vary from moment to moment. This makes it difficult for power utilities, which need to match the amount of energy generated with the amount being consumed moment-by-moment. If you rely on wind power, you need spare capacity for when the wind drops, and in 2004, when new farms stopped being developed, the regional government approved two 400-megawatt natural-gas plants near Castej\u00f3n, a town in southern Navarre. Luis Mar\u00eda L\u00f3pez Gonz\u00e1lez, an engineer specializing in renewable energy at the University of La Rioja in Logro\u00f1o, suggests that the intermittent nature of wind generation was one of the reasons that the plants were built. These 'combined-cycle' plants, which harness their own waste heat, can be fired up quickly to meet a sudden need. They are also efficient, emitting a little over half of the carbon dioxide per watt that a coal-powered plant does. But that's still a lot more carbon dioxide than a windmill. With the wind sector maturing, Spain as a whole is starting to look elsewhere for renewable-energy sources. Last month, the national parliament lowered the subsidies that wind operators get and boosted subsidies for the production of biomass and biofuels and solar photovoltaics. One new scheme is so-called 'solar gardens', large expanses of solar panels in which individual investors can buy as few as a single panel, enjoying both the income it provides and the feeling that they are greening the grid. Morr\u00e1s has no complaints about the government shifting funds away from wind and into other forms of renewable energy, even though it means that making a profit from wind is harder. \u201cIt will be an effort for the sector,\u201d he says, \u201cbut we can do it.\u201d And there is still plenty of room for wind-farm growth elsewhere. \u201cIn my opinion,\u201d he says, \u201cthe next years will be the most important, with the extension of wind farms in China, India, South America, Europe and Russia.\u201d Closer to home, in Iratxeta, G\u00f3mez has been overseeing a more modest expansion \u2014 the erection of a new building for the village council, one of several infrastructure projects the village has invested in with the income they receive from the wind parks. For G\u00f3mez, wind power is a renewal as well as a renewable: it has brought Iratxeta and its neighbouring villages back from what seemed to be an inevitable decline. He is disappointed with the government's decision to make no new wind parks. \u201cI think if you ask people around here,\u201d he says, \u201cthey'll tell you they want more because it means more money. Wind power,\u201d he says with the emphasis of someone who has seen science change his world for the better, \u201chas been a copernican revolution.\u201d \n                     Wind map shows top sites for turbines \n                   \n                     Urban wind power: Breezing into town \n                   \n                     Energy for a cool planet \n                   \n                     European Wind Energy Association \n                   \n                     Spanish National Renewable Energy Research Centre \n                   Reprints and Permissions"},
{"file_id": "448126a", "url": "https://www.nature.com/articles/448126a", "year": 2007, "authors": [{"name": "Jane Qiu"}], "parsed_as_year": "2006_or_before", "body": "Traditional Chinese medicine and Western science face almost irreconcilable differences. Can systems biology bring them together?  Jane Qiu  reports. Liu Wen-long's modest Beijing practice looks no different from most clinics. But he is no ordinary doctor. Liu never orders lab tests, nor does he prescribe high-tech imaging diagnostics. He relies on simple observations, checking a patient's pulse, complexion and odour, and asking about habits and medical history. At 69 years old, he has been practising traditional Chinese medicine for 43 years and he is resolute about its benefits. \u201cPeople keep coming back because it cures them and improves their well-being,\u201d he says. Indeed, patients trickle in to see Liu all morning for conditions ranging from allergies to lung cancer. Some are nervous first-timers, others are regulars, confident in what traditional Chinese medicine has to offer. Ms Huang, an accountant from the outskirts of Beijing, is delighted that her migraines, which haunted her for years, disappeared after three herbal regimens. \u201cI used to live on painkillers and felt tired all the time,\u201d she says. \u201cI am now a totally different person.\u201d In a country that is fiercely embracing modernity, clinics such as Liu's, which have been operating the same way for thousands of years, seem vulnerable and out of place. Indeed, attitudes on traditional Chinese medicine have divided the country. Last year, Zhang Gong-yao, from the Central South University in Changsha, Hunan, published an article in a Chinese journal calling traditional Chinese medicine a pseudoscience that should be banished from public healthcare and research 1 . The article caused uproar in the country, and earlier this year the government announced an ambitious plan to modernize the millennia-old practice 2 . But should such a formidable gap be bridged? Modern Western medicine generally prescribes treatments for specific diseases, often on the basis of their physiological cause. Traditional Chinese medicine, however, focuses on symptoms, and uses plant and animal products, minerals, acupuncture and moxibustion \u2014 the burning of the mugwort herb ( Artemisia vulgaris ) on or near the skin. But whether these methods are effective and, if they are, how they work remain a source of some derision. The greatest divide is in the testing. In the West, researchers test a drug's safety and efficacy in randomized, controlled trials. Traditional Chinese treatments are mixtures of ingredients, concocted on the spot on the basis of a patient's symptoms and characteristics and using theories passed down through generations. The mainstream medical community, in China and abroad, has been highly critical of the underlying theories. Traditional Chinese medicine is based on ideas such as  qi  (meridian), in which illness is caused by blocked energy channels;  yin  and  yang , which emphasizes the balance of energy; and  wuxing  (five elements), in which people's organs and health status are categorized according to their 'elemental characteristics': fire, wood, water, earth and metal. Pharmaceutical companies have become more interested in traditional Chinese medicines over the past decade. But their approach has been characteristically Western: isolate the active ingredients and test them one at a time. This reductionist approach has led to the approval of drugs such as artemisinin for malaria, which is used to treat fever in traditional Chinese medicine, and arsenic trioxide, which has been carried over from Chinese medicine for treatment of acute promyelocytic leukaemia. But identifying the active ingredients isn't easy. Most remedies in traditional Chinese medicine, as it turns out, are compound formulae \u2014 or  fufang  \u2014 that contain as many as 50 species of herbs, and thousands of chemicals therein (see  'Knowledge mining' ). To tap into the deeper well of traditional Chinese treatments, researchers think they may need to look at how the mixtures of ingredients act in concert. \n               Relaxed regulation \n             The criteria for approval of herbal mixtures as medicines are now starting to relax, at least in the United States. In June 2004, the US Food and Drug Administration (FDA) issued new guidelines that permit the approval of herbal mixtures if they can be shown to be safe and effective, even if the active constituents are not known. Last October, the FDA approved the first such botanical drug under the new rules, a proprietary mixture of green-tea extracts called Veregen developed by the German company MediGene for treating genital warts. These new regulations have helped to renew industry's interest in the complex formulae. And a buzzing new Western field could be poised to capitalize on the deeper secrets of traditional Chinese medicine. Systems biology attempts to understand the function and behaviour of an organism by studying the interactions between its components. It has been called a more holistic approach to biology and is seen by some as a perfect match for traditional Chinese medicine. By measuring many genes, proteins and metabolites at the same time, systems biology may provide a measure of the entire body's response to a complex mixture of herbs. \u201cIf there is any technology that could lead to a breakthrough in traditional Chinese medicine, it will be systems biology,\u201d says Robert Verpoorte, head of the pharmacognosy department at the University of Leiden in the Netherlands. But not everyone agrees that the new technology is equipped to test old ideas. Jia Wei, a pharmacologist at the Shanghai Centre for Systems Biomedicine at Jiao Tong University, and Tang Hui-ru at the Wuhan Institute of Physics and Mathematics, part of the Chinese Academy of Sciences, want to understand more fully how herbal extracts affect the whole body. They are collaborating with Jeremy Nicholson, head of the department of biomolecular medicine at Imperial College London, and using technologies such as nuclear magnetic resonance spectroscopy and mass spectrometry to profile the metabolites in a person's urine or blood \u2014 a discipline they call metabonomics. Jia and his colleagues found that rats given the compound 1,2-dimethylhydrazine to induce tumours in their colons had different metabolic profiles in their urine from those in the control group. And by feeding the rats a combination of two herbal extracts \u2014  Coptidis rhizoma  and  Evodia rutaecarpa , which are widely used in traditional Chinese medicine to treat gastric conditions \u2014 the researchers were able to reverse these changes in metabolism. Their results have not yet been published, but the researchers say that by looking at the changes in metabolites in detail, they have pinpointed the metabolic pathways that the herbs affect. \n               Culture shift \n             Jan van der Greef from SU Biomedicine in Zeist, the Netherlands, and his colleague Wang Mei are using a similar approach. In a mouse model of metabolic syndrome \u2014 a cluster of conditions such as insulin resistance and high blood pressure that often occur together \u2014 they and their team looked at the effect of an undisclosed formula used in traditional Chinese medicine on lipid profiles. When these mice are fed a high-fat diet, they become more resistant to insulin. The lipid profiles of these mice were clearly distinguishable from those of mice fed a normal diet, and they shifted towards the healthy state when the mice were given traditional Chinese medicine 3 . The researchers noticed that the profile shift resembled that caused by the Western obesity drug Rimonabant, which acts on proteins called CB-1 endocannabinoid receptors. And their unpublished work with cell culture suggests that herbal extracts can affect lipid metabolism through the same receptor, says van der Greef. The team is now testing the formula in clinical trials. Although one active ingredient may act as the Western drug, the uncertain role of additional ingredients and the variability of active ingredients confounds Western sensibilities. \u201cVariations worry people,\u201d Nicholson says. The same plant species grown in different regions and harvested in different seasons could have distinct chemical compositions. This has always been a vexing issue for herbal-medicine researchers. While at Nicholson's lab, Tang and his colleagues analysed the molecular components in chamomile plants from Egypt, Slovakia, and Hungary, and could classify them easily 4 . Using similar approaches, the team from the Wuhan Institute of Physics and Mathematics found significant variations in the same herbal medicines produced by different companies and even between different batches produced by the same company. \u201cThis is an issue China must tackle for its herbal medicines to raise their game in the world market,\u201d says Tang. To many self-purported systems biologists, several approaches are needed to build a complete picture of a living organism and to understand the effect of traditional Chinese medicine. Nevertheless, systems biology has been a conspicuously hard field to define. Many have used the term loosely, and pioneers in the field contend that the technologies haven't been honed to the point that they could be used for these approaches. \u201cIt's conceivable that systems biology could find applications in trying to sort out components in Chinese herbal medicine, but it's very early days,\u201d says Leroy Hood, president of the Institute for Systems Biology in Seattle, Washington, and regarded as the field's founding father. \u201cIt would be an enormous challenge at this point and time.\u201d Systems biology has been successful in model organisms, according to Hood, but is much less so in human studies. Many hurdles need to be overcome before researchers could even begin to contemplate how to deal with subjects as complex as traditional Chinese medicine. For example, better detection systems are needed to measure metabolites, especially proteins, accurately in the blood, and more powerful computational and statistical tools are crucial for dealing with large and complex data sets. \u201cThose technologies are at early stages of maturation,\u201d Hood says. There are also broader concerns about the modernization of traditional Chinese medicine, from both advocates and sceptics of the practice. Some are uncomfortable with separating the study and development of Chinese herbal medicines from the theories that underlie its normal practice. \u201cTraditional Chinese medicine is not just a medical system, but a branch of philosophy and healing art that is an important part of Chinese culture,\u201d says Fu Jing-hua, a retired researcher at the Chinese Academy of Chinese Medicine Sciences in Beijing and president of the Chinese Ancient Books Publishing House in Beijing. \u201cDevoid of that cultural context, it would become a tree without roots.\u201d \n               Lofty ideals \n             But Zhang and Fang Shi-min, a US-trained biochemist who now runs a society called New Threads that is known for fighting pseudoscience and research misconduct in China say that it is exactly those traditional Chinese medicine theories that should be abolished. Conceits such as  yin  and  yang ,  wuxing  and  qi  \u201care inaccurate descriptions of the human body that verge on imagination\u201d, he says. Inevitably, cultural factors may be the biggest obstacle in bridging the East\u2013West gap. \u201cThe field of traditional Chinese medicine is notorious for being averse to criticism,\u201d says Yuan Zhong, a philosopher of medicine at the Chinese Academy of Medical Sciences. \u201cIf people are not allowed to disagree or voice their opinions, there would be no hope of progress for any discipline.\u201d But although heated exchanges are boiling over in debates on the future of traditional Chinese medicine, it's business as usual in Liu's practice. He is sanguine about the convergence between traditional Chinese medicine and Western medicine, but has a pragmatic attitude towards it. \u201cWhether from the East or the West, we share the same goal of improving human health. As long as it works, anything goes,\u201d he remarks. But Liu says that he is yet to see any real progress in the merging of the two philosophies and, until then, his intuition and experience \u2014 as well as traditional Chinese medicine's seemingly arcane theory and practice \u2014 will serve him and his patients just fine. See Editorial,  \n                     page 106 \n                   . \n                     Break with tradition \n                   \n                     Scientist supporting herbal HIV remedy suspended \n                   \n                     Malaria breakthrough raises spectre of drug resistance \n                   \n                     Ginkgo is living fossil \n                   \n                     Chinese medicine homes in on cancer target \n                   \n                     Medline plus information on drugs and supplements \n                   \n                     Chinese Academy of Sciences \n                   \n                     Institute for Systems Biology \n                   \n                     New Threads \n                   Reprints and Permissions"},
{"file_id": "448018a", "url": "https://www.nature.com/articles/448018a", "year": 2007, "authors": [], "parsed_as_year": "2006_or_before", "body": "Time machines, spaceships, atomic blasters \u2014 the icons of science fiction tend to come from the physical sciences. But science fiction has a biological side too, finding drama and pathos in everything from alien evolution to the paradoxes of consciousness.  Nature  brought together four science-fiction writers with a background in the biological sciences to talk about life-science fiction. \n               Who's who \n             \n               Nature has given biologists a lot of weird things to study \u2014 how easy is it making aliens that are even weirder? \n             Peter Watts:  Every now and then I think I'm taking a shot at it, but it always trips my ass ultimately. In my latest book,  Blindsight , I thought I had come up with the ultimate alien: they didn't even have genes, and most of their metabolic processes were mediated by external magnetic fields. I thought I was really striking off into new and unexplored territory. But when it comes right down to it, I had described, at least in terms of gross morphology, brittle stars. Something that essentially had a whole series of pinhole cameras across its entire body surface, something like a very large telescope array. Joan Slonczewski:  I ended up in microbiology because microbes are the most fantastic creatures, more diverse than any other kind of creature in the natural biosphere. In fact, for much of my career it's been hard to tell which is more bizarre, the kind of research that I'm doing or the kind of science fiction I'm trying to write. In the textbook I'm doing now I've been writing about organisms that live 2 kilometres below the earth in gold-mines and that live off hydrogen atoms produced by uranium decay. I have yet to see nuclear-powered creatures much in science fiction. \n               Can science fiction work as a medium to put across important scientific ideas? \n             Joan:  Yes. I actually teach a course on biology and science fiction for students who have trouble with a standard science course. There are certain books that do a wonderful job of teaching science through science fiction \u2014 Kurt Vonnegut's  Galapagos  is a great example. Peter:  I've got to second that. I think that was Vonnegut's best: it got evolution right. The idea that what is left of our civilization a million years hence is that when one of our seal-like descendents farts on the beach, the others just laugh and laugh \u2014 that's a wonderfully ironic and potent summation of human achievement. Paul McAuley:  Evolution is a keynote that runs through most of H. G. Wells's science fiction. The human race was going to slip down into unthinking Morlocks and Eloi or we could continue to rise and become the big-brained, small-limbed creatures that are the kind of epitome of science-fiction clich\u00e9 of future man. Wells was taught by Huxley, had a zoology degree and so on, so he had a good grounding in it. But in Wells's time, evolution was some blind force. We've now got the opportunity to start directing evolution ourselves. Joan:  We can change our genes based on cultural views, what we believe are better genes. That's what the aliens in Octavia Butler's books are doing when they mate with humans. But what happens when it turns out the environment changes and that's not the best gene or we make a mistake? And what happens if we lose the variation that's required in the environment? \n               Those are great questions, but is science fiction good at answering them? Take cloning \u2014 how helpful has the vast amount of pre-existing science fiction about cloning been in informing the post-Dolly debate? \n             Joan:  My impression is that for the students it was more helpful than not. That is, if Dolly had happened and there was no context at all, where would you begin to discuss what had happened? Whereas because there was a whole science-fiction tradition of questioning the ethics of cloning and the ethics of making people for spare parts, you had at least somewhere to start. Ken Macleod:  I think the prior art provided by science fiction was distinctly unhelpful in dealing with Dolly and cloning: ludicrous drek about cloning armies of soulless robots and  The Boys from Brazil  cloning Hitler, and the whole Frankenstein mess. And the actual ethical issues that arose with cloning were essentially none of the above. Joan:  I used to agree with your point that bad science fiction was an obstruction to learning, but as a biology professor I learned that sometimes bad science fiction is better than none at all. For example, you could poke Michael Crichton's portrayal of dinosaur cloning full of holes, but those stories encouraged a whole generation of molecular-biology students. As a result, we have a molecular-biology programme now at Kenyon College. And that's really due to    Jurassic Park ? Joan:  Yes, we call it the  Jurassic Park  generation. Paul:  The big problem I have with Michael Crichton is he's basically anti-science. That old clich\u00e9 of things that man wasn't meant to know embodies most of his work. Science is always running out of control, with people coming in to mop up afterwards. I think that the effect that Joan is seeing is from the film more than the novel, the wonderful scene of the dinosaurs up there on the screen. In the novel the hero was a lawyer. That's how anti-science it was. My stuff gets compared to his stuff occasionally and I just have to say, 'Well, no'. Because I like science, I like scientists. I like what science does and I think that on the whole it's a good thing \u2014 and I think Crichton thinks the opposite, mostly. But the good thing about Crichton's work is that he does show, usefully I think, that science is not ethically valueless. Some scientists tend to argue that knowledge is knowledge for knowledge's sake and that we should just find out what we can and damn the consequences. Crichton does actually cast the shadow of what we find back on to society and what's going to happen to it if we take these things to their logical conclusion. Peter:  When I start writing, I like to think of it as a sort of thought experiment. I go where the data lead, and I do not explicitly start off with a goal of writing a cautionary tale of saying that the world is turning to shit. That does seem to be where my stories all end up ultimately, but that's just because I'm following the data and there is an inertia to big systems and we can't realistically imagine a situation in which things would be better by 2050 unless we had actually had gotten serious about cleaning things up 20 years ago. Ken:  I think there's a distinction between science fiction and techno-thriller. The sort of thing that Michael Crichton writes is different from the sort of thing that Paul McAuley writes. Even when what Paul McAuley writes looks like a techno-thriller, he's actually sneakily writing science fiction in disguise. I attempt now and again to do the same thing. The difference is that in the techno-thriller, the lab eventually gets burned down, the genie gets back in the bottle, the evil scientist is defeated and so on. That's not the spirit of science fiction at all. Paul:  Ken's hit the nail on the head there. Science fiction posits that change is good and that change will happen and doesn't necessarily say in which direction change will go. The great power of science fiction is that, first of all, it's able to do that, and second, that it's able to get away without causing so much fuss. We can sneak in under the radar with all these outrageous notions and these manipulations and speculations about human nature. Peter:  Just to play devil's advocate for a moment, how would you guys react to the argument that if we can get away with it, we're not really doing the job? That if nobody gets pissed off by what you're doing, you are essentially conceding defeat when it comes to actually trying to provoke action. Ken:  I don't see science-fiction writers as agents for change. I think we're here not so much to bring about change but to investigate and imagine change. What happens as a result is the responsibility of the readers, not of the writers. Joan:  I think the imagination of change has to come first. If you don't do that, you're not a science-fiction writer. But I think there are some science-fiction writers who attempt to be scandalous. I think Robert Heinlein's  Stranger in a Strange Land  was considered extremely scandalous when it first came out \u2014 it posited a religion where there is cannibalism at the core of it, which was an attempt to imitate the Eucharist, and had all kinds of things that were then considered scandalous. Ken:  Yes, but have you ever come across anyone who was genuinely scandalized by  Stranger in a Stranger Land ? I mean, I was a little Christian fundamentalist when I first read it as far as I can remember and I wasn't scandalized. I was kind of mildly titillated, but... Joan:  In Ohio we're a little more easily scandalized, perhaps. \n               Science fiction has always been interested in 'the other' \u2014 and these days that other is as likely to be a computer program as an eight-legged alien. How does the interaction between biology, technology and artificial intelligence feature in your work? \n             Joan:  One of the things that fascinates me is how people react to ideas of aliens or of artificial intelligence, and it seems to me that the way we treat artificial intelligences has a lot in common with the way we treat immigrant labour and the lower classes, or slaves. We think about these machines as slaves to do our work, but the more like us the machine is, the more effective a servant it is. This is the kind of dynamic we don't like to think about, but that has gone on in the way we treat either immigrant labour or slaves historically. Some studies have shown that even people who are very computer literate will treat their desktop computer as if it were another person. What if their computer were to become so powerful it actually wakes up and demands human rights? In my book  Brain Plague , there are technological entities that have sentience, whether they're robots or an entire transit system and it's just assumed that although they're robots, they're also just another ethnic group. My view as a molecular biologist is that our own bodies are machines composed of molecules and the computer on my desk is a machine composed of molecules, and the only difference is which one has woken up. Ken:  When I wrote my first novel,  Star Fraction , I wrote it partly under the impulse of the feeling that Richard Dawkins and his selfish gene and the propagation of memes were something not widely enough known. I had to spread the word about these new and exciting ideas, and you know that whole thing of darwinian evolution going on in electronic systems seemed to me to be an enormously exciting and fruitful line of work and now it's pretty much all-pervasive on the talk shows. I think I was pretty much behind the curve even when I wrote it, though it didn't seem like it. Peter:  In my  Rifters  trilogy I wrote about Maelstrom, which was a far-future, massively super-evolved descendent of the Internet; all I did there was apply darwinian principles, assume you get your computer viruses, set them loose to breed in the wild, and end up with a seething electronic ecosystem that reproduces 200 times a second. It didn't strike me as a particularly radical innovation and I don't think any biologist would find that. But people who were in AI [artificial intelligence] found this a massively innovative idea. I started getting letters from this guy who works in the Lawrence Livermore lab who told me that he had found my portrayal of digital ecosystems inspirational in his own work, which I found a little bit creepy because of the kind of things they do at the Lawrence Livermore labs and the fact that he couldn't tell me exactly what his work was. The way my ideas about marine biology fall by the wayside and my ideas about AI get taken up makes me think that our imaginations are hamstrung in our own area of expertise. We know too many reasons why this, that or the other wouldn't work. We're perhaps a little too cognizant of our colleagues peering over our shoulders and ranking us as one or two steps above child pornographers because we write that sci-fi stuff in the first place. I wonder if some of the most innovative stuff comes when you retain the respect for logic and the respect for consequence, but you leave behind that infestation of fact and dogma that you used in getting your degree. Paul:  Well, that's why I quit science, folks. To get away from that self-censorship thing. I'm partly joking, but only partly. One of the useful things that science fiction does is to get out from under self-checking circuits that scientists must use when they're doing their work and just let rip and dance away with it. Doing science is like slogging through mud. Science fiction straps on mud shoes and dances off over the surface and onto the horizon, gesticulating madly and doing all sorts of silly little dances, but sometimes doing useful stuff. Joan:  I actually find science to be inspirational for science fiction. I can still remember seeing an isolated photopigment that a grad student had got in a test-tube that was purple, and he shone light on it and it bleached white and this idea of the colourful switch enabled me to imagine: 'What if people had symbiotic microbes that would turn a switch depending on the environmental situation'. Later, that same pigment was used in a molecular switching device to make biochemical computers. So I think that science can be inspiring if you're doing it; you just have to be willing to not be inhibited in taking it a little farther. Ken:  In the novel I'm working on, one of the assumptions in it is that some AIs become self-aware because they're combat-robots and they're required to have ever more sophisticated theories of mind to work out what the guys they're about to shoot are going to do. But the other AIs, the ones that do our dirty work for us, like the police national artificial intelligence, which is one of the characters in my story, don't necessarily have self-awareness in the human sense at all. Peter:  The creepy thing about self-awareness is that 'the other' may in fact have been inside us all along, it may really be the one in control. The conscious decision to move your arm occurs half a second after the motor nerves have started firing, the conscious event is an executive summary received after the fact. This little self-aware homunculus behind the eyes doesn't seem responsible for nearly as much as it gives itself credit for; the heavy lifting seems to be done by something deeper, something we don't have conscious access to. I played with this idea in  Blindsight , in which although there are aliens, there is no 'other'; the things our heroes meet are hyperintelligent but utterly nonsentient. And maybe that thing inside us that we can't feel, that makes the real decisions, that lets us think we're in control \u2014 maybe that's the same way. \n               Let's end with your favourite biological moment in science fiction. \n             Paul:  In  Blood Music , by Greg Bear, when the protagonist looks down the microscope and he sees the bacteria have created little circuits that look like cities. A moment of 'Wow' like that is quite rare in science fiction, even though the 'Gosh, wow' thing is something we all aspire to. And it happened really early on in the novel, as well, which got even weirder after that. So I knew I was in for a good time. Peter:  Mine was Alice Sheldon's  The Screwfly Solution . The idea is a rampant, literally epidemiological spread of homicidal hatred towards women, which society insists on treating as mass hysteria, whereas in fact it's been pheromonely introduced by aliens who want to clean up the real estate without using radioactive devices. So it's essentially a form of biological pest control. Joan:  For me, if it's a defining moment, it's the moment in Vonnegut's  Galapagos  in which the narrator of the story has the opportunity to decide whether to stick around for the next million years of evolution or to be taken off to heaven. And he decides that observing the next million years, no matter what, no matter how bad it is, that the next million years of human evolution are more compelling to him than going off to heaven. That to me is an inspiring moment. Ken:  I think that my sort of favourite biological science-fiction story is  Sunken Universe  [aka  Surface Tension ] by James Blish. It's an absurd idea that in the far future there are engineered human beings on another planet who are the size of protozoans. They're living in a puddle and they build what they call a spaceship, a little device made of bits of leaf and twig and so on that has wheels propelled by paramecium, and they laboriously drag this device across the dry land to the next puddle and at the end of it wonder if they have actually built this spaceship and crossed space like their ancestors did. And I loved that as an image of where we are and what we can do. \n                 See Editorial,  \n                 \n                     page 1 \n                   \n                 . An expanded version of this conversation is at  \n                 \n                     http://tinyurl.com/224s24 \n                   \n               \n                     Undead again \n                   \n                     The Oort crowd \n                   \n                     Tuberculosis bacteria join UN \n                   \n                     Danger \u2014 hard hack area \n                   \n                     Natures Futures stories \n                   \n                     Joan Slonczewski\u2019s homepage \n                   \n                     Peter Watts\u2019s homepage \n                   \n                     \u201cThe early days of a better nation\u201d -- Ken Macleod\u2019s blog \n                   \n                     \u201cEarth and other unlikely worlds\u201d \u2013 Paul McAuley\u2019s blog \n                   Reprints and Permissions"},
{"file_id": "448245a", "url": "https://www.nature.com/articles/448245a", "year": 2007, "authors": [{"name": "Geoff Brumfiel"}], "parsed_as_year": "2006_or_before", "body": "Why is dark energy, hailed as a breakthrough when discovered a decade ago, proving so frustrating to the scientists who study it? In 1998, two teams of astronomers reported that the Universe was pulling itself apart. This came as something of a shock. That the Universe was expanding had been known since the 1920s, but conventional wisdom held that this expansion was slowing and was likely, in the distant future, to come to an all but complete halt. Then, in the late 1990s, observations of distant supernovae showed that the expansion was not slowing down at all. It was speeding up. This discovery was incredibly counterintuitive, recalls Charles Bennett, an astronomer at Johns Hopkins University in Baltimore, Maryland. \u201cI just didn't believe it.\u201d Within a few years, however, he and almost all his peers could withhold their belief no longer. The observations became stronger. And the expansion provided a way out of a theoretical impasse. Observations of the Big Bang's afterglow made by various groups, including Bennett's, indicated that the Universe's gravity had flattened it out. But other observations suggested that it simply didn't contain enough matter to have that much of a gravitational effect \u2014 even when as-yet-undiscovered forms of dark matter were included in the sums (see  page 240 ). Happily, the theory of relativity requires energy, as well as matter, to have a gravitational effect. And it turned out that the amount of energy needed to drive the acceleration was pretty close to that needed to solve the flatness problem by means of its gravity. 'Dark energy', as it quickly became known, seemed poised to provide great insight into the origin and future of the cosmos, says Michael Turner, a cosmologist at the University of Chicago in Illinois. \u201cThis seemed to be the piece that made everything else work.\u201d But a decade further on, researchers seem to have swapped one theoretical conundrum for a bigger one. Follow-up measurements have revealed little about the nature of dark energy, and theories to explain it have failed to gain traction. And although astronomers are trudging forwards with a battery of new measurements, there is little guarantee that any will solve the problem \u2014 and thus no clear consensus on how much effort to put into them. \u201cThe issue is: how much information do we get from these future observations?\u201d asks Avi Loeb, an astrophysicist at Harvard University. \n               Hidden depths \n             The big problem is that dark energy is not, in itself, something that astronomers can see. Like dark matter, it is known only by its effects \u2014 in this case, the effect it has on the Universe's acceleration. The acceleration is related to dark energy through a quantity known as the 'equation of state'\u2014 the ratio of the pressure dark energy exerts to the energy per unit volume involved. An accelerating expansion means that the equation of state has to be negative. And a value of \u22121 would mean that dark energy was an unchanging feature of the cosmos \u2014 a 'cosmological constant'. Such a constant had been a feature of Einstein's general theory of relativity, one that he had added, ironically, as a way of guaranteeing that the Universe would stay the same size. When Einstein came to accept that the Universe was, in fact, expanding he removed the term, calling it his \u201cgreatest mistake\u201d. But if the equation of state had a value of \u22121, dark energy would fit the cosmological constant bill perfectly. And current measurements make it quite possible that the equation's value is \u22121. If dark energy's equation of state is indeed \u22121, then there's one obvious way to make sense of it, says Leonard Susskind, a cosmologist at Stanford University in California. For decades, physicists have postulated the existence of something known as 'vacuum energy' \u2014 a primordial froth of quantum particles that flit in and out of existence in the vacuum of space. This vacuum energy could drive the observed accelerating expansion, and it would do so in a constant manner. Because vacuum energy is an inherent property of space, Susskind explains, an expanding Universe would create more of it, meaning that the ratio of energy density to pressure would never change, their ratio fixed for ever at \u22121. There's just one theoretical discrepancy: the vacuum energy as calculated by physicists is more than 10 100  times larger than would be needed to explain the relatively weak effects of dark energy as observed by astronomers. If it were as big as physicists suggest, then our Universe would fly apart in the blink of an eye. \u201cEvery calculation indicates that vacuum energy should be enormous,\u201d says Turner. \u201cThere's no natural way to get such a tiny number.\u201d So most physicists have hoped that some yet-to-be-discovered effect based on some hidden symmetry of nature would cancel out the vacuum energy. Such a hope-it-goes-away approach is used by physicists quite a lot, and can be the only way to make progress in some circumstances. At the same time, applying it to the vacuum energy was, admits Susskind, \u201ccompletely illogical\u201d. \u201cAnd I must say I shared that illogical attitude myself,\u201d he continues almost apologetically. Now, he thinks differently, and is one of those who has proposed a solution of sorts to the conundrum. 'String theories', popular with many particle physicists, make it possible, even desirable, to think that the observable Universe is just one of 10 500  universes in a grander 'multiverse', says Susskind. The vacuum energy will have different values in different universes, and in many or most it might indeed be vast. But it must be small in ours because it is only in such a universe that observers such as ourselves can evolve. This sort of anthropic argument irks many scientists. Critics say such reasoning is almost impossible to verify and doesn't provide any deeper insight into the cosmos. \u201cAnthropics and randomness don't explain anything,\u201d says Paul Steinhardt, a theorist at Princeton University in New Jersey. \u201cI'm disappointed with what most theorists are willing to accept.\u201d The trouble is that no other approaches are proving any more fruitful. Some suggest that the problem lies with Einstein's idea of gravity, which they then seek to modify in a way that fits in with dark energy. \u201cIt would be very fortunate if the dark energy were a modification of gravity,\u201d says Georgi Dvali of New York University, \u201cbecause it would address fundamental questions of physics.\u201d But others see little mileage in such changes. Leaving aside the cosmos, \u201cit's not so easy to get those theories to be consistent with our Solar System\u201d, says Turner. Another possibility is that dark energy is some sort of evolving property of the Universe. Some postulate that dark energy is a fifth force (the others being electromagnetism, the two nuclear forces and gravity) that works at the largest scales of the cosmos. Others suspect that it is the aftermath of the inflation that many see following directly on from the Big Bang. Inflation was, after all, a period of extreme expansion \u2014 might it not have some sort of 'long tail' that stretched away down cosmic history? These solutions and others, although different conceptually, are equivalent mathematically. And they share a requirement that dark energy changes over time \u2014 that its equation of state is not locked in as \u22121. Such a change would help to explain why dark energy is apparently so weak today, says Steinhardt. And changing values for dark energy might affect other features of the Universe, including some parameters now seen as fundamental constants, in detectable ways, which could be a plus. But critics claim that these ideas require extreme amounts of special pleading. \n               Starring role \n             In general, the theoretical side of the debate is not a pretty thing. \u201cWe've tried a whole bunch of things and nothing has sprung forward,\u201d says Sean Carroll, a theoretical physicist at the California Institute of Technology in Pasadena. What's needed, Carroll says, are a few more good clues. Astronomers are planning a new generation of dark-energy probes that will refine measurements of the equation of state. They are already pushing ahead with further measurements of type 1a supernovae. These stellar outbursts occur when a stream of material being sucked from a larger star onto a smaller one pushes the smaller star's mass over a threshold, precipitating a massive thermonuclear explosion. Because each star explodes at the same mass threshold, they should all give off the same amount of energy. And so, in absolute terms, each should be as bright as any other. By comparing their relative brightnesses when seen from Earth, it is possible to measure the distance to the explosion with precision, says Saul Perlmutter, the astronomer at Lawrence Berkeley National Laboratory in California who led one of the original dark-energy supernova teams. And by measuring distance in this way and speed by means of the 'red shift' of the supernova's light, astronomers can understand acceleration over time. Perlmutter and others are now working to increase both their understanding of the supernova mechanism and the size of their sample to improve on their original calculations. Supernovae, although the best understood, are not the only way to measure acceleration. Another option is to study X-rays from distant clusters of galaxies. As in the case of supernovae, a cluster's temperature and brightness should have a standard relationship, so it should be possible to measure the speed at which those at a given distance from Earth are receding, says Steve Allen, an X-ray astronomer at Stanford University. It is also possible to measure the effects of dark energy in subtler ways. The gravitational field of a cluster or group of galaxies makes light shift towards the blue as it falls into the galaxies' gravity well, and reddens it as it climbs back out. According to Ryan Scranton, an astronomer at the University of Pittsburgh in Pennsylvania, dark energy should affect the way these effects show up in the cosmic microwave background, radiation left over from the Big Bang. \n               A tangled web \n             Combining these different sorts of measurement should offer ways of constraining the value of the equation of state better than any single measurement can manage (see  'Closing in on dark energy' ). Perhaps the most promising new realm of research, say many in the field, lies in surveys that will look at how the largest structures in the Universe have been shaped or distorted by dark energy. Galaxies are not spread evenly across the cosmos, but instead clump into a three-dimensional cobweb. The structure of that cobweb is sensitive to dark energy. And the sort of error to be expected in measurements of the structure are completely different from those that plague measurements of supernovae, according to Adam Riess, an astronomer at Johns Hopkins University who led the original supernova team that competed with Perlmutter's. That makes the new approach pleasingly independent of the old one. Several ambitious surveys are now being planned to further map the large-scale structure of the Universe (see ' The search for structure '). But none of these techniques can do more than narrow the frustratingly uninformative equation of state down further. To prove that dark energy is a cosmological constant requires showing that the equation of state is indeed \u22121. Merely showing that it is close doesn't cut it. Astronomers could basically go on measuring dark energy for ever without eliminating other possible theories, says Simon White, director of the Max Planck Institute for Astrophysics in Garching, Germany: \u201cIf it's just a constant, then you need infinite accuracy.\u201d Lawrence Krauss, a theoretical physicist at Case Western Reserve University in Cleveland, Ohio, goes further. If the equation of state is indeed \u22121, and dark energy is a constant, then the only way to measure it will be through its effect on the Universe's acceleration. \u201cIf it is \u22121, we won't know what dark energy is,\u201d he says. \u201cIt doesn't give us any theoretical guidance whatsoever.\u201d Carlos Frenk, a theoretical physicist at the University of Durham, UK, agrees that probing a single number without a strong theoretical case for doing so is not the way forward. \u201cIt's like trying to learn something fundamental about biology by measuring the height of every tree,\u201d he says. \u201cJust measuring something for the sake of measuring it is pointless.\u201d Frenk questions how much money should be spent on such measurements, and Loeb agrees. \u201cOne should put money in this direction,\u201d he says, \u201cbut not excessive amounts.\u201d But for all the worries of some theoretical physicists, observational astronomers think that carrying on with the equation of state measurement is the most sensible next step, not least because it is the only one on offer. \u201cMy feeling is that we should measure it to the limits,\u201d says Bennett. \u201cWe may see things that surprise people, that often happens.\u201d Perlmutter, too, sees room for a few more results to narrow things down rather than shaking them up. \u201cIt seems like you'd want to get a couple of boring results before you decide 'we're done',\u201d he says. Bennett and Perlmutter's enthusiasm for further measurements is evidenced by the fact that they are heading up rival proposals for spacecraft to observe galactic structure (Bennett) and distant supernovae (Perlmutter), seeking to get the money that NASA and the US Department of Energy are considering spending on a dark-energy probe. And even if their endeavours contribute no more than some incremental precision to the debate on dark energy, the observations will still tell astronomers quite a bit about other things in the Universe. A space-based supernova probe, for example, would provide a high-quality survey of infrared objects throughout the sky. \u201cThese are not special-purpose instruments,\u201d says Roger Blandford, director of the Kavli Institute for Particle Astrophysics and Cosmology in Stanford, California. \u201cThey will revolutionize a whole range of fields.\u201d And there is always a chance that some other area will reveal the next much-needed clues as to the nature of dark energy. When it starts taking data in 2008, the Large Hadron Collider at CERN, the particle-physics laboratory near Geneva, might conceivably make relevant discoveries about the nature of space-time (see Insight,  page 269 ); \u201cWe may learn more from accelerators than we do from the sky,\u201d says Krauss. Similarly, measurements of fundamental constants and gravitation at short distances could have some unexpected connection to the dark-energy problem; and detecting some sort of dark matter might help, too (see 'Welcome to the dark side',  page 240 ). So far, though, the revolution promised by dark energy's discovery a decade ago hasn't materialized. Although researchers are more certain than ever of the existence of a cosmic push, they know as little about what it means physically as they did in 1998. \u201cRight now there are two possibilities,\u201d says Carroll. \u201cDark energy is vacuum energy, or it's something else.\u201d Observers are slightly more upbeat. \u201cIt feels to me like a very early discussion of all this,\u201d says Perlmutter. Still, he concedes, without a measurement of the equation of state that deviates from \u22121 it will be difficult to learn much of anything. \u201cIf you don't see those ripples,\u201d he says, \u201cit's going to be hard to play the game.\u201d For now, many in the field are left with a sense of unease: the tantalizing clue they thought they had discovered has turned into an exasperating mystery. And with no clear explanation of something that could be up to three-quarters of everything out there, it's hard not to feel like you're missing a big part of the picture, Susskind says. \u201cWe could be wrong about cosmology for the next thousand years. Deeply wrong.\u201d \n                 See Editorial,  \n                 \n                     page 225 \n                   \n               \n                     Is dark energy changing? \n                   \n                     Cosmology: What is dark energy? \n                   \n                     Hubble sees dark energy's youth \n                   \n                     Cosmology gets real \n                   \n                     Universe can surf the Big Rip \n                   \n                     Focus \n                   \n                     Supernova Cosmology Project \n                   \n                     Riess's homepage \n                   \n                     More on Dark Energy \n                   Reprints and Permissions"},
{"file_id": "447772a", "url": "https://www.nature.com/articles/447772a", "year": 2007, "authors": [{"name": "Philip Ball"}], "parsed_as_year": "2006_or_before", "body": "When two objects separated by a vacuum are barely a whisker apart, a strange attraction comes into play. Philip Ball meets the physicists who are trying to make something out of nothing. \u201cNothing can come of nothing,\u201d wrote Shakespeare, but Harvard physicist Federico Capasso aims to prove him wrong. Because of the fluctuating nature of the sub-microscopic quantum world, 'nothing' \u2014 a vacuum \u2014 can generate an attractive force between two objects that are very close to each other. This Casimir force, named after its discoverer, Dutch physicist Hendrik Casimir, has long been regarded as a scientific curiosity. But Capasso believes that it can be tamed and modified in ways that could benefit technology at the microscopic scale. Thanks largely to his vision, the field of Casimir engineering is beginning to take shape. The Casimir force is so weak it is almost undetectable. As a result, much of the work on this ghostly interaction has been concerned with simply detecting and characterizing it. Hardly anyone has thought about whether it could be put to good use. But Capasso believes that there is \u201ca whole zoo of interesting stuff\u201d that can be done with the force. He wants to use it to make new microscopic devices, such as motion and position sensors 1 . \u201cI love to think of myself as a designer,\u201d says Capasso, \u201cand here the question is, can we design quantum fluctuations?\u201d These fluctuations lie at the heart of the attractive force. All materials \u2014 and vacuums too \u2014 are pervaded by fluctuating electromagnetic fields. These fluctuations in two closely separated surfaces can get in step, leading to an electrical attraction between them. That gives rise to the familiar van der Waals force, famous for helping geckos to climb glass walls: an illustration that this fundamentally quantum-mechanical effect has observable everyday consequences. But experiments in the 1940s showed that this attractive force falls off more rapidly than expected at separations of more than 10 nanometres or so (see  'Into the gap' ). In 1948, Casimir explained why. The fluctuations of one surface are 'communicated' to the other by fluctuations of the electromagnetic field in the vacuum in between. But as the gap gets bigger, it takes longer for this 'signal' to cross, and so there is a time delay: the surface fluctuations get out of step, and the force therefore gets weaker. This weaker attraction is now generally called the Casimir force, although in a sense it is simply a modification of the van der Waals force. It took several more decades for this elusive effect to be measured directly. The Casimir force falls off rapidly with increasing separation and is tiny beyond a few tens of nanometres. But its strength increases with the area of the interacting surfaces, so it becomes detectable if the surfaces are big enough. This does, however, mean holding two parallel surfaces only a microscopic distance apart, which is technically demanding, especially given that such surfaces are generally rather rough and so may not have a well-defined separation. That's why the Casimir force wasn't detected unambiguously with high precision until 1997, when Steve Lamoreaux, then at the University of Washington in Seattle, measured the interaction between a gold-plated hemisphere and a gold plate attached to a torsion pendulum: a horizontal bar suspended by a wire. As the objects were brought to within a few micrometres of each other, the force caused the pendulum to twist 2 . \n               Weak and feeble? \n             It is difficult to imagine that a force so weak that it is hard to measure at all is likely to be significant in applied science, either as a problem or an opportunity. But the small scales on which engineering is now being conducted have revitalized interest in the Casimir force. Mechanical devices such as vibration sensors and switches are now routinely made with parts that are just a few micrometres big. These microelectromechanical systems (MEMS) are just the right size for the Casimir force to exert itself: they have surface areas big enough, yet gaps small enough, for the force to draw components together and perhaps lock them tight \u2014 an effect called stiction. Such permanent adhesion is a common cause of malfunction in MEMS devices, and in 1998 Jordan Maclay and his co-workers at the University of Illinois in Chicago suggested that the Casimir force might be responsible for it 3 . This makes sense to MEMS researchers such as Ho Bun Chan at the University of Florida in Gainesville. \u201cThe components in MEMS are designed to be very close to each other,\u201d says Chan, who has worked on Casimir forces in these systems. \u201cUnder the right circumstances, the Casimir force can become significant and affect the operation of the device. It can initiate the pull-in of components that eventually leads to stiction.\u201d He admits that so far there have been no reports of this, but adds that most researchers in this field have probably seen it unknowingly. One attempt to look for it explicitly was made in 2001, when physicists Eyal Buks and Michael Roukes at the California Institute of Technology in Pasadena investigated stiction between a nanoscale gold beam, fixed at both ends, and an adjacent gold electrode 4 . \n               Sticking point \n             They brought the two metal surfaces into contact using the capillary forces of a droplet of water placed between them, and found that the beam stayed stuck after the water had evaporated. The interaction was a combination of the van der Waals force where the surfaces were in contact, and the Casimir force where there was a gap. Chan notes that in experiments he has done, the smallest possible separation between two MEMS surfaces is about 60 nanometres or more. \u201cIf we go beyond this distance, we find that the plates jump into contact, presumably due to the Casimir force,\u201d he says.\u201cHowever, this is a phenomenon that most researchers try to avoid rather than study.\u201d Although more work is needed to find out whether the Casimir force triggers stiction in MEMS, Capasso and his co-workers have verified that it can be detected in such devices. They think that the force could in fact be used to start mechanical motion. \u201cAs you load more and more MEMS devices on to a chip, at some point you have to contend with this effect,\u201d Capasso says. \u201cEither you must avoid it or you must use it.\u201d Capasso's fascination with the Casimir force began during the 1990s, while he was working at Bell Labs in Murray Hill, New Jersey. In 2001, he and his Bell colleagues devised a MEMS device that enabled them to probe the Casimir force in an accurate and controlled way 5 . The device measured the attraction between a gold-coated sphere and the surface of a see-saw made from two square silicon plates that were coated in gold (see picture, below). The Casimir force between the plates and sphere drew one side of the see-saw upwards. The tilt increased sharply at separations of less than about 150 nm, but the researchers could detect it for separations of at least 300 nm. The amount the plate tilted matched very closely that predicted from Casimir's theory. In principle, the researchers can exert precise control over the Casimir interaction because electrodes beneath the see-saw (used to measure its movement) offer the option of tilting the plate using electrostatic interactions. Because this would entail balancing electrostatic forces, elastic forces in the see-saw and the Casimir interaction, the researchers can control the interaction in ways that might be exploited in future MEMS devices. Further opportunities for engineering the Casimir force arise because the interaction depends on the composition of the interacting materials. There have even been predictions of a repulsive Casimir force. \u201cBetween two flat plates with vacuum in between, the Casimir force is always attractive,\u201d says physicist Astrid Lambrecht of the Kastler Brossel Laboratory in Paris. But with another medium in between the plates, such as certain types of liquid, she says, you can have repulsion. No one has yet seen a repulsive Casimir force experimentally. It requires picking materials that have the appropriate electromagnetic responses at different wavelengths. But earlier this year, Capasso reported experiments on two gold surfaces immersed in ethanol 6 . The force between them remained attractive, but it was only half as strong as that when the surfaces were separated by air. \u201cThis is a stepping-stone experiment,\u201d he says. \u201cBy replacing one of the surfaces with silica or Teflon, we might expect to see a repulsive force. But it is very difficult stuff \u2014 you have to be paranoid about possible sources of error.\u201d Capasso believes that engineering a repulsive force could be used to induce 'Casimir levitation', which might be used to make friction-free micro-bearings. It could also provide routes for avoiding Casimir-induced stiction in microengineering. Because the Casimir force stems from electromagnetic fluctuations, it is sensitive to the way materials interact with light. So optical properties such as birefringence, in which the refractive index is different in different directions, can also affect the force. This means that the interaction between two birefringent plates should depend on their orientation: the plates will tend to align with each other. If they are moved out of alignment, a torque will act to restore it. Capasso hopes to observe this effect by suspending a birefringent disk over a plate, perhaps levitating it using a repulsive Casimir force 1 , 7 . \u201cWe can give the disk a kick with laser light, then shut off the light and watch it rotate back,\u201d he says. It should be possible to detect this rotation by looking at how light bounces off the disk. Capasso and his colleagues are planning an experiment like this, using disks of barium titanate tens of micrometres across suspended in ethanol above a calcite crystal. \n               Attractive proposition \n             Capasso thinks that such studies could bring together hitherto separate communities \u2014 those who work on quantum optics and electrodynamics, and those who work in materials physics. He suspects, for example, that interesting things might happen to the Casimir force close to phase transitions in materials that alter their electronic or optical properties \u2014 such as the transition between an insulating and a metallic material, or between a normal metal and a superconductor. His group is currently looking for these effects in the interaction between a gold sphere and a thin slab of a high-temperature superconductor. Any such effect is likely to be small, but he thinks it is worth a try. A former co-worker of Capasso, Davide Iannuzzi, now at the Free University of Amsterdam, proposed one such experiment that involved measuring the force between a gold plate and a sphere coated with stacked thin films of magnesium and nickel, which switches from being reflective to transparent when exposed to hydrogen. To their surprise, the researchers saw no change in the strength of the force on adding hydrogen 8  \u2014 they had expected the change in the material's reflectivity to affect the Casimir force. \u201cIt was a negative result that probably told us more than a positive result would have,\u201d says Capasso. One explanation may be that for this material, wavelengths much longer than the visible range contribute significantly to the Casimir force, and that adding hydrogen doesn't alter the reflectivity much at these wavelengths. So, as they continue to investigate this quirk of nature, researchers are finding that the Casimir force is more slippery than they first imagined. But this exploration of how, and how much, empty space can be engineered has only just begun. It looks sure to demonstrate that there's a lot you can make out of nothing. \n                     Popular physics myth is all at sea \n                   \n                     Quantum physics: Casimir force changes sign \n                   \n                     Research highlights \n                   \n                     Making light of the Earth \n                   \n                     Measuring gravity with an atomic fountain \n                   \n                     Capasso lab \n                   \n                     Iannuzzi group \n                   \n                     Chan lab \n                   \n                     Casimir force \n                   Reprints and Permissions"},
{"file_id": "448240a", "url": "https://www.nature.com/articles/448240a", "year": 2007, "authors": [{"name": "Jenny Hogan"}], "parsed_as_year": "2006_or_before", "body": "Physicists say that 96% of the Universe is unseen, and appeal to the ideas of 'dark matter' and 'dark energy' to make up the difference. In the first of two articles, Jenny Hogan reports that attempts to identify the mysterious dark matter are on the verge of success. In the  second , Geoff Brumfiel asks why dark energy, hailed as a breakthrough when discovered a decade ago, is proving more frustrating than ever to the scientists who study it. We're underneath 1,400 metres of Italian mountain, walking through cavernous halls that lead from a 10-kilometre-long road tunnel. The scientists working within the Gran Sasso National Laboratory near L'Aquila seem ant-like in scale against the backdrop of vast metal spheres, towers and scaffolding that house their underground experiments. Physicist Elena Aprile is hurrying the group along, pointing out one project after another. She stops to take a photo of one, exclaiming at its size. We finally reach Aprile's XENON10 experiment, which is tucked away at the end of a small side tunnel. This is the project into which Aprile has poured her energy over the past few years, one of several experiments at Gran Sasso and around the world that are waiting for a passing piece of 'dark matter' to show itself. Once upon a time, waiting for new particles to reveal themselves was a major endeavour. Scientists in the 1940s would also head to the mountains \u2014 to their tops, not to underground caverns \u2014 carrying emulsion-covered plates to capture strange new cosmic rays. But as particle accelerators became more powerful, physicists became adept at making their own novelties, and lying in wait for chance discoveries fell out of fashion. In this, dark-matter searches are something of a throwback. They are a reminder of the past in another way, too. Ever more powerful accelerators require ever vaster detectors and ever larger teams of people to make sense of their output. The Large Hadron Collider (LHC) under construction at CERN, the European particle-physics laboratory just outside Geneva, will cost \u20ac3 billion (US$4.1 billion) and is the work of thousands of scientists and engineers. The XENON10 detector is run by just 30 scientists, and that's part of its attraction. \u201cIt's a last chance to do physics like it used to be done,\u201d says Aprile. In the hunt for dark matter, a small team can make a big difference. XENON10 has steamed ahead of older collaborations to become the most sensitive detector for a category of dark matter called weakly interacting massive particles, or WIMPs. Other collaborations are keen to wrest the lead back, and over the next two to five years, sensitivity records look set to fall repeatedly. \u201cA few years ago, I would have been surprised if dark-matter detectors had found a WIMP,\u201d says Leszek Roszkowski, a theorist from the University of Sheffield, UK, on sabbatical at CERN. \u201cIn a few years' time, if our ideas are correct, I will be surprised if they don't.\u201d \n               The dark titans \n             The first hints of dark matter came in the 1930s, when astronomer Fritz Zwicky spotted something odd about the behaviour of galaxies in the Coma cluster. His measurements of the galaxies' velocities suggested that the cluster was held together by more mass than he could see. He wrote: \u201cIf this is confirmed, we would arrive at the astonishing conclusion that dark matter is present with a much greater density than luminous matter.\u201d Cosmologists now believe that dark matter provides the scaffolding around which all other cosmic structures, from galaxies to galaxy clusters, superclusters and more, have taken shape. Astronomers are building big telescopes that can map its distribution in the heavens (see  'The search for structure' ). But this dark stuff cannot be the everyday matter of which stars, gas clouds and planets are made. Detailed measurements of the microwave radiation left over from the Big Bang suggest that such ordinary matter makes up just 4% of the Universe. The rest is thought to be divided between dark matter \u2014 outweighing normal matter by five to one \u2014 and a strange repulsive force dubbed dark energy (see 'A constant problem',  page 245 ). \u201cIf you look at the history of the Universe, it's been the battle of the two dark titans,\u201d says Michael Turner, a cosmologist at the University of Chicago, Illinois. \u201cFor the first 10 billion years, dark matter reigned, and it shaped all the structure in the Universe, and then, about five billion years ago, dark energy took over, shut off the formation of structure and got the Universe accelerating.\u201d The idea that dark matter might not just be dark but fundamentally different from other matter gained ground in the 1970s. Planning for underground detectors similar to Aprile's began in the 1980s; but the field has heated up only recently as the sensitivity of WIMP detectors has improved \u2014 and as competing experiments have emerged to attack the problem from other angles. \n               Producing particles \n             Most of the particles that theorists have suggested as possible WIMPs are massive \u2014 at least 100 times the mass of a proton. Their size has kept them beyond the reach of particle accelerators, but the LHC could well change that, and produce in the lab what the dark-matter detectors have so far failed to capture in the field. While researchers at the LHC have a new collider to tackle the problem, astronomers are taking yet another approach. Some of the WIMP particles that theorists are fond of might give off distinctive bursts of \u03b3-rays or other odd signatures when they interact with each other; satellites and telescopes are now looking for such signals. But the scientists trying to catch a piece of dark matter using their underground experiments still hope to get there first. \u201cIf you don't get your act together now,\u201d says Aprile, \u201cthe feeling is that you're going to be too late.\u201d Even those who have worked at CERN in the past, such as dark-matter researcher Bernard Sadoulet, now at the University of California, Berkeley, hope for a return on their investment in direct searches. \u201cOf course after putting 20 years of my life into this thing, I would like to see it first,\u201d says Sadoulet. Any of these experiments could individually provide evidence for a dark-matter particle. To date, there has been only one claim of direct particle detection, and that remains controversial (see  'Contested results' ). So multiple lines of evidence will be essential for scientists to claim with confidence that they have discovered a new ingredient of the Universe \u2014 especially because that particle might point to a new framework of physical laws. \u201cThere's never been a more fun time to wonder about dark matter,\u201d says physicist Max Tegmark of the Massachusetts Institute of Technology in Cambridge. \u201cIt really feels like we are on the brink. There are these different roads to dark matter and they are all on the verge of coming through.\u201d No one knows what dark matter is, but they know what it's not. It's not part of the 'standard model' of physics that weaves together everything that is known about ordinary matter and its interactions. The standard model has been hugely successful, but it also has some problems, and in trying to fix these, theorists have predicted hordes of new fundamental particles. At first, these hypothetical particles were viewed as unwelcome additions, but now some of them are leading candidates for dark matter. \u201cThese days a theory without a dark-matter candidate is not considered an interesting one,\u201d says Roszkowski. \u201cThe existence of the dark-matter problem is perhaps the most convincing evidence for physics beyond the standard model.\u201d Many of today's leading theories for physics beyond the standard model are variations of 'supersymmetry', which posits that each ordinary particle has a heavier supersymmetric partner. Several of these partners have been put forward as candidate WIMPs, and, remarkably, calculations of the number of such WIMPs expected to be left over from the Big Bang match cosmological observations of dark matter. This coincidence helped to strengthen the case for dark matter being a new kind of particle, although the numbers take some getting used to. Assuming a typical WIMP has a mass 100 times heavier than a proton, models of the dark matter in the Milky Way predict there will be roughly ten billion WIMPs passing through one square metre of Earth every second. For these particles to zip by unnoticed requires ordinary matter and light to barely register their presence. It also makes WIMPs incredibly difficult to spot. Calculations suggest that almost a million billion dark-matter particles pass through Aprile's XENON10 detector every week \u2014 yet only the tiniest fraction would ever be detected. The XENON10 experiment works on the principle that a passing WIMP should very occasionally bump into a xenon atom \u2014 a fat target, with 54 protons and 54 electrons. Such collisions would release energy through a handful of photons and electrons, which can be detected by sensitive instruments. In common with other experiments that aim to directly detect dark matter, XENON10 is housed underground because the rocks above it absorb particles and radiation, such as cosmic rays from outer space, that might otherwise confuse the data. The challenge for the experimental teams is to block out as much of this 'background' as possible and see what's left. Aprile announced XENON10's first results earlier this year 1 . The findings are yet to be published, but they took the community by surprise, not least because the previous best result belonged to an underground experiment using totally different technology. Rather than trying to trap dark-matter particles in a 15-kilogram vat of xenon liquid as the XENON10 detector does, the Cryogenic Dark Matter Search (CDMS) in Minnesota looks for vibrations and charge created by particle collisions in a very cold crystal of germanium and silicon. Until Aprile's result, experiments with noble liquids had been lagging behind. In its first run, XENON10 registered 10 events over 60 days that couldn't be instantly dismissed. \u201cYou could jump up and down and say we've found ten WIMPs, but of course we haven't,\u201d she says. Aprile's team ruled out half of the events on closer inspection, and the rest were assumed to be background signals that slipped through the analysis. The researchers would have needed at least 15 events that could not be explained in other ways to think they'd caught a whiff of a WIMP, and Aprile says that, even then, they would need to understand the background better before claiming a direct detection. \n               Setting limits \n             In the meantime, a negative result is still important. The sensitivity of the XENON10 detector allows the researchers to set limits on the properties that a hypothetical WIMP might have \u2014 such as how heavy it is and how much it interacts with matter. This is crucial information when what you are hunting for is as mysterious as a dark-matter particle. XENON10 now claims a tighter limit than the previous best result. The experiments are starting to eat into the regions where supersymmetry predicts WIMPs should be (see  'WIMP hunting' ). Other projects are close on XENON10's heels, searching for particles that might interact even more feebly. To improve their chances of finding a particle, dark-matter detectors need to do two things: get bigger and reduce the background. Designers of a rival xenon-based experiment, Zeplin-III, have taken great care to minimize stray signals reaching their detector, and this experiment is expected to yield results within the next year or two. The Zeplin project, a UK collaboration, started life in the early 1990s and the team had been in talks with Aprile before she decided to strike off on her own. \u201cIt's frustrating that they came into this game when we already had the Zeplin designs and picked off the best bits,\u201d says project spokesperson Tim Sumner of Imperial College, London, \u201cbut we should take the positive out of it; it shows the technology works.\u201d With their latest 12-kilogram detector installed in the corner of a potash mine in Cleveland, UK, the group is confident: \u201cWe have faith that Zeplin-III will be better than XENON10,\u201d says Sumner. This will help set even tighter limits on background noise, but \u201cthe glory will come with the first detection\u201d. Closer to home, XENON10 has several rivals at Gran Sasso. The container that Aprile snapped a picture of during her tour will soon be the centrepiece of WARP, the WIMP Argon Programme, an experiment led by Carlo Rubbia, who won a share of the Nobel Prize in Physics in 1984 for his part in the discovery at CERN of the force-carrying particles known as the  W  and  Z  bosons. WARP will use a large vat of liquid argon to trap dark matter. \u201cThis is serious competition,\u201d says Aprile, who is a former student of Rubbia's. Aprile is also upgrading XENON10 over the next few months by reducing the background and increasing the detector's size to 60 kilograms of liquid. A bigger vat boosts the chances of finding a WIMP, because having more mass makes it more likely that a dark-matter particle will interact. \u201cThe next step that is sensible is to go to one tonne; it doesn't make any sense any more to screw around with these little things,\u201d Aprile says. \n               Paying the price \n             But bigger detectors require more money. In a report published on 13 July, the Dark Matter Scientific Assessment Group, established last year by the US Department of Energy and National Science Foundation, recommended that US funding for dark-matter research be bumped up from less than $4 million per year to $10 million annually. This won't fund experiments at the tonne-scale, but it should accelerate testing of the different technologies, says Hank Sobel, chair of the group. The panel recommends that another review be carried out in 2009 to decide how to move to large-scale detectors. It is easier to scale up the liquid approaches than the solid-state experiments, but the CDMS detector can more easily identify and eliminate background. The CDMS did undergo an upgrade a year ago, from one kilogram of germanium to four. That should mean that its next result, due at the end of the summer, will equal or better that of XENON10. \u201cOver the next year, we should be able to increase our limits by at least a factor of 10 or 15, or discover something,\u201d says Sadoulet, who is the spokesperson for the CDMS. Despite the enthusiasm, there is still a chance that nature will refuse to cooperate, and the experiments will chase ever better limits but never detect a particle. Some of the WIMP candidates predicted by supersymmetry are \u201cessentially undetectable\u201d, warns Roberto Trotta, a theoretical physicist at the University of Oxford, UK. The particles may be too heavy to be created by the LHC and at the same time too weakly interacting to be detected by the underground experiments. \u201cI think in the next ten years, we are going to see big discoveries; if not we are going to be in big trouble,\u201d says Trotta. When the LHC smashes together its protons in 2008, WIMPS might be created in the messy outpouring. The collider's detectors wouldn't be able to register these directly, but they would show up as 'missing mass' when the physicists piece together the energy budget of the collisions. Because such evidence is indirect, finding a WIMP signature at the LHC would not confirm it to be dark matter. \u201cThere would still be a window open,\u201d says Roszkowski. For example, a particle might be stable for the fractions of a second that it takes to fly out of the collider, but then decay elsewhere. That means a second route \u2014 direct detection \u2014 would be necessary too. \u201cWe absolutely need both to resolve the dark-matter problem,\u201d he says. But the collider can provide additional theoretical context. For instance, should it be lucky enough to identify a particle, the LHC should be able to place it in a supersymmetric family tree. Different supersymmetry theories predict superpartners with different masses \u2014 including the axino, gravitino or neutralino \u2014 as WIMP candidates. The dark-matter candidate is always the lightest partner, because this can't decay into anything else. Many of the simplest models predict that the neutralino, a particle that is a combination of the superpartners of four other particles, will be the lightest, and thus stable enough to have survived from the Big Bang until now. Consequently, this strange particle \u2014 it acts as its own antiparticle \u2014 is the most popular WIMP candidate among particle physicists. A third route to detecting neutralinos will search for evidence of their destruction. Because neutralinos are their own antiparticles they should annihilate each other in regions where they are close enough together to bump into each other. This self-destruction could show up in the form of \u03b3-rays, neutrinos or matter and antimatter particles. For example, models of dark matter in the Milky Way suggest that annihilation of neutralinos concentrated in the Galaxy's dark-matter halo and in the galactic core could generate a \u03b3-ray glow (see picture). Satellites such as the Gamma Ray Large Area Space Telescope, due for launch in early 2008, could spot this. Moreover, annihilation of neutralinos clumped in the core of the Sun could be inferred from measurements made by neutrino telescopes, such as the IceCube Neutrino Detector being built at the South Pole, because such annihilation would generate higher-energy neutrinos than expected from other processes. But what if dark matter isn't a neutralino or even a WIMP? Some proposals to explain dark matter don't depend on supersymmetry or WIMPS at all. These range from doing away with the need for dark matter by modifying the laws of gravity (see  'Bending the rules' ) to suggesting other types of particle altogether. The main rival to the neutralino is the axion, first proposed by particle physicists in 1977 to resolve a glitch in the standard model. Many theorists believe that the axion will eventually be found, but it is unclear whether its mass and interactions will match cosmological expectations. Already the axion mass is constrained on one end by theory and on the other by observations of supernovae. It is consequently predicted to be at least 10 million million times lighter than a typical neutralino. The Axion Dark Matter Experiment, at the Lawrence Livermore National Laboratory in California, aims to give a definitive answer to the axions' part in dark matter. Leslie Rosenberg, co-spokesperson for the experiment, says the team expects results by 2011, after the detector moves to its final home at the University of Washington in Seattle. Axions interact so weakly that they are rarely produced in particle colliders, so the experiment will look for signs of axions using a radio receiver that can detect tiny particle energies. There are far fewer searches for axions than there are searches for WIMPs. Rosenberg thinks this is partly because the technology needed to find them is less familiar to particle physicists. In addition, he says, \u201cthere's a tremendous bandwagon to supersymmetry, and WIMPS are riding on that\u201d. Even if dark matter turns out to be something completely different, the experimental teams are determined to track down their particular quarry and get an answer, one way or another. \u201cI live with that with the impatience of the Italian woman that I am,\u201d says Aprile. \u201cI am just going fast ahead with the next step, making the detector better.\u201d But success for one type of experiment doesn't have to mean failure for another. Scientists prefer simplicity: if they find WIMPS, then they don't need axions, and vice versa. But why not have both? Dark matter might prove to be a richer problem than anyone is expecting. Tegmark hopes for this outcome. \u201cThis could be a wonderful surprise. It's very arrogant of us humans to say that just because we can't see it, there's only one kind of dark matter.\u201d See Insight,  \n                     page 269 \n                   , and Editorial,  \n                     page 225 \n                   . \n                     Space telescope spies dark matter \n                   \n                     Dark matter has a ring of truth \n                   \n                     Space probe backs up dark view of the Universe \n                   \n                     Dark matter warms up \n                   \n                     Dark haloes pepper the Universe \n                   \n                     Dark matter highlights extra dimensions \n                   \n                     Focus \n                   \n                     Zeplin-III \n                   \n                     XENON experiment \n                   \n                     Gran Sasso National Laboratory \n                   \n                     Cryogenic Dark Matter Search \n                   \n                     UK Dark Matter Collaboration \n                   \n                     The Large Hadron Collider \n                   \n                     GLAST \n                   Reprints and Permissions"},
{"file_id": "448402a", "url": "https://www.nature.com/articles/448402a", "year": 2007, "authors": [{"name": "Michael Hopkin"}], "parsed_as_year": "2006_or_before", "body": "A project that gives Congolese pygmies new ways to tell logging companies about the trees that are important to them, and their own radio station to discuss community issues, is really putting their interests on the map, says Michael Hopkin. In June in the rainforests of the northwest of the Republic of Congo, the trees rain caterpillars. For the Mbendjele pygmies who live there, it's boom time. For a few weeks each year, children climb the 45-metre-tall sapelli trees and shake the branches, sending hundreds of newly hatched caterpillars down to the waiting women, who dry and cook the creatures to eat or sell. But someone else has their eye on the 'caterpillar trees' of the Congo. Their wood is one of the most valuable of the African mahoganies \u2014 a fact that has not escaped the attention of commercial loggers. One tree might yield five sacks of caterpillars per year, potentially fetching US$500 at the local market, but the timber from the same tree could bring in more than $1,500 of profit to a logging company. Now, though, the Mbendjele have a new way to help keep the trees standing. Scientists, conservationists and technicians have put together an innovative set of technologies to allow the forest people to mark trees that are important to them, saving them from the logger's axe. Logging is a given in these regions. A logging company called Congolaise Industrielle des Bois (CIB) \u2014 a subsidiary of the Danish timber multinational DLH Group \u2014 holds logging rights to nearly 10% of the Congo. And although the government has been fastidious in setting aside reserves such as the 4,000-square-kilometre Nouabal\u00e9 Ndoki National Park (see map) to preserve wildlife, it has not allocated any land to indigenous semi-nomadic people. The Mbendjele are one of these groups, which together constitute about 3,000 people living in the CIB concession. Activists worry about the collision between local people and logging companies. In a report released after a 2004 fact-finding trip to the country, a band of non-governmental organizations led by the environmental-activist group Greenpeace strongly criticized the CIB's activities in the area, saying that there were \u201cno mechanisms by which the indigenous community as a whole is kept informed about logging plans, or by which they can have an input\u201d. But appealing to loggers' pockets may help resolve these issues, some say. Consumers in the developed world are increasingly demanding that their wood or furniture comes with a stamp of approval from the Forest Stewardship Council (FSC), an organization based in Washington DC that aims to ensure that the world's timber is harvested in a sustainable, environmentally friendly and socially responsible way. Not engaging with local people means no FSC certification and, potentially, fewer buyers. Greenpeace's report found that engagement with the local people was one of the main areas in which CIB was wanting. For its part, the company said it had wanted to work on the issue, but didn't know how to go about doing it. \u201cWe had no ideas on the issue of traditional people,\u201d says Lucas van der Walt, who oversees the company's environmental management practices. \u201cWe looked elsewhere but found no examples of community projects that we thought would work.\u201d So in 2004, CIB turned to the Tropical Forest Trust, an organization based in Crassier, Switzerland, that helps logging companies win FSC certification by fostering more responsible practices. The result, now coming to fruition, blends simplicity with technology to help CIB achieve its aims and at the same time give the Mbendjele a chance to protect their way of life. The task has not been easy. Pygmies in the Congo have long been disenfranchised from the rest of the population, says Scott Poynton, executive director for the forest trust. Loggers typically pass them over for employment, and even having national parks in the region doesn't always help. \u201cThere were incidents where ecoguards with Kalashnikovs beat up pygmies,\u201d says Poynton. \u201cPolitically, the pygmies are really not a powerful people.\u201d \n               Inside knowledge \n             Somehow, the pygmies had to find a way to let the loggers know the locations of forest sites that are important to them, such as hunting and foraging grounds, water sources, burial places and sacred sites. The idea is not new; mapping of valuable sites with global-positioning systems (GPS) has been tried before with Amazonian tribes and elsewhere in the Congo river basin, such as in Cameroon. But these efforts have been hampered by the lack of literacy, both traditional and technological, among the indigenous people. \u201cBefore, it was a question of going around with a notebook and an old-fashioned GPS unit, numbering the important areas and then marking them on a map,\u201d says Marcus Colchester of the Forest Peoples Programme, an advocacy group based in the United Kingdom. \u201cIt was pretty laborious.\u201d Now, the forest trust has a different approach. With a $150,000 grant from the World Bank, they enlisted the help of Jerome Lewis, an anthropologist at the London School of Economics, UK, who has lived on-and-off with the Mbendjele for more than a decade. He knew how to go about getting the information the logging company needed. \u201cI sat down and spoke with Mbendjele friends who were really pissed off when loggers drove bulldozers over cemeteries and water sources,\u201d Lewis says. To the loggers, many of the sites look the same as any other part of the forest; their significance is known only to the pygmies, who don't want to share the sites with outsiders. \u201cThe only successful engagement of local people happens when they start doing the monitoring themselves,\u201d explains Lewis. How to do that is the tricky part. So Lewis designed a set of electronic icons to help the Mbendjele record the locations of important sites using a portable, palm-pilot-style device. The simple pictorial menu allows the pygmies to identify different types of sites as they wander through the forest; the sites are then automatically plotted onto a computerized map with GPS. The Mbendjele choose from four categories to classify their important sites: hunting, gathering, social/religion and farming. From there, each category branches off into more specific details. For instance, to signify the importance of a sapelli tree, one would select 'gathering' and then 'caterpillars' from the next list of choices. Similarly, the system can log areas where yams grow, where herbal medicines are found, and where the Mbendjele camp while travelling through the forest. The results can easily be plotted on a piece of mapping software such as Google Earth. And the pygmies know the terrain so intimately that they have no problems visualizing it as depicted from a birds-eye view on a map. \u201cPeople are very quick to understand it,\u201d Lewis says. Other groups of pygmies, in regions such as Cameroon, were also quick to adopt the technology, he says. \u201cThe younger ones play it like a video game,\u201d adds Poynton. Mapping began on a trial basis in June 2006, and CIB says that it has since mapped all of the area it was planning to survey for this year's logging activities \u2014 in about one-third of the time it would previously have taken with traditional mapping. Of course, there are trade-offs from the logging company's point of view. CIB has only ever logged the forest selectively \u2014 it takes only about 1.5 trees per hectare, which is why its concessions are so large \u2014 and it has no plans to change that. But they have pledged not to cut down trees deemed by the Mbendjele to be of value. \u201cThey have been able to respect the trees that people want without harming their profit margins,\u201d says Lewis. \n               Radio pygmy \n             Meanwhile, the Mbendjele are getting another technical toy: a community radio station that, although still in its preliminary stages, should turn locals into roving reporters, and provide information about CIB's plans for logging. Perhaps inevitably nicknamed 'Pygmy FM' at its inception, the station has now formally been named 'Bisso na Bisso', which means 'between us' in the Lingala language. Only a few pilot programmes have been made, but the plan is to eventually broadcast six to eight hours a day of music and public-service broadcasts, about both logging and wider issues such as AIDS awareness. The pygmies currently get their information about when and where logging is to occur from representatives, called 'animateurs', that CIB sends into the villages \u2014 but they usually talk only to men; women and children are kept in the dark. The station already has its headquarters in the logging town of Pokola, but a planned series of satellite bases and broadcasting towers have not yet materialized. Government broadcasting licences have yet to be finalized, and it must all be done on an annual budget of less than $90,000, says Lewis. Funding so far has come from the World Bank grant, and CIB has promised to match the funding. In May 2006, the project plus other engagement with the indigenous peoples earned CIB its FSC certification, at least for one of the five areas of its concession. But its plans to open a sawmill in Loundoungou, one of the remotest parts of its concession, has been condemned by Greenpeace as \u201cunacceptable\u201d; activists say that the mill will raise the number of transient workers in the area and exacerbate the problem of bushmeat hunting. But the Mbendjele are finally getting the chance, over the airwaves and by satellite, to let the loggers know what they think about the impact on their own lives. \n                 For more on the project and to hear some of Bisso na Bisso's pilot show, go to  \n                 \n                     http://www.nature.com/nature/podcast/index.html \n                   \n               \n                     Biodiversity: Logging: the new conservation \n                   \n                     Brazilian Amazon being cut down twice as fast \n                   \n                     Tropical Forest Trust \n                   \n                     Forest Peoples Programme \n                   \n                     Forest Stewardship Council \n                   \n                     DLH Group \n                   Reprints and Permissions"},
{"file_id": "448406a", "url": "https://www.nature.com/articles/448406a", "year": 2007, "authors": [{"name": "Meredith Wadman"}], "parsed_as_year": "2006_or_before", "body": "Alan Krensky has been put in charge of a controversial new office responsible for charting the progress of the NIH Roadmap for Medical Research. Meredith Wadman catches up with him in his first few days on the job. Hurrying in to an interview on his second official day as the de facto roadmap czar at the US National Institutes of Health (NIH), Alan Krensky is absently clutching a piece of paper; he's proud to hand it over for inspection. On it, the physician-scientist has charted a year's worth of trans-NIH 'rounds' \u2014 expert talks on cross-agency topics from network theory to pharmacogenomics to health economics. He has scheduled 11 lectures. The first is in two days. Krensky, a respected paediatric immunologist who spent the past 23 years at Stanford University, has been recruited to institutionalize the Roadmap for Medical Research, a brainchild of NIH director Elias Zerhouni that was launched four years ago to decidedly mixed reviews. The map is intended to foster trans-NIH, large-scale and high-risk research. This year, it will spend $483 million, roughly 1.7% of the $29-billion NIH budget. But Krensky's job \u2014 his official title is deputy director for the Office of Portfolio Analysis and Strategic Initiatives (OPASI) \u2014 doesn't stop at the map. Krensky will be assessing its effectiveness, trying to come up with ways, for instance, to evaluate whether the coveted Pioneer Awards for high-risk research are delivering the intended spoils. He will be scoping out new scientific opportunities and emerging public health risks, to recommend for future trans-NIH projects. And he and his staff will be completing an exhaustive, unprecedented cataloguing of the NIH's research portfolio aimed, in a tight budget era, at identifying both gaps and redundancies in all the science that the NIH funds. \u201cThe roadmap needed a home and someone to be responsible for it. But OPASI is much bigger than that,\u201d says Krensky, whose broad grin and tousled grey hair belie the earnest energy required of a man squeezing an interview between meetings with senior staff and directors of 27 institutes and centres. Zerhouni sees Krensky and the new office as lending the roadmap permanence and continuity. \u201cYou need a permanent director,\u201d he says. \u201cSomebody who will be there through NIH directors and ensure that things are done well, without any temptation of having this dictated from the top.\u201d An outgoing Chicago native, Krensky graduated from the University of Pennsylvania Medical School, trained in Boston in nephrology and immunology and landed at Stanford in 1984, where, along with seeing patients, he shared a lab with his wife, Carol Clayberger, a Yale-educated cell biologist. During his last six years at Stanford, Krensky was tapped to lead a $526-million campaign to transform the Lucille Packard Children's Hospital. In 2001, Packard was a regional hospital struggling to chart a course after Stanford University Medical Center's failed merger with the University of California, San Francisco. Krensky recruited 47 new faculty, oversaw the growth of the hospital's endowment from $20 million to $200 million and earned the hospital thirteenth place last year in  US News and World Report 's national ranking of children's hospitals. What he acheived at the hospital, says Harvey Cohen, until recently its chief of staff, \u201cis nothing short of astounding.\u201d Cohen, along with hospital chief executive Christopher Dawes and Stanford dean of research Ann Arvin, consider Krensky a perfect fit for the NIH job. At the hospital, he developed disease-based centres glued together by cross-cutting functions from imaging to informatics. In the process, they say, he proved himself an outstanding strategist with a vision broad enough to take in the massive complexity of the NIH and deep enough to understand how to tackle the thorny problems involved in incubating trans-institute research. \n               Light leadership \n             Krensky is quick to note that OPASI \u2014 a $3.3 million, 15-person office intended to ramp up to 70 people in coming months \u2014 doesn't exercise executive power over individual institutes or roadmap spending. As in the past, roadmap projects will continue to run a gamut of review by senior NIH staff, with Krensky's office one of many participants. He sees his role instead as coordinating, advising, greasing inter-institute wheels and providing information. \u201cA lot of my job is cajoling,\u201d he laughs. Krensksy's experience at Stanford went a long way with the hiring committee at the NIH. Top administrators there are scrambling to get into synch with a new NIH-governing law enacted in January that, among other things, enshrines the roadmap as an NIH fixture, backed by a 'common fund' that can comprise up to 5% of the NIH budget in any given year. \u201cWe were very impressed with his package of experience and knowledge and vision,\u201d says Raynard Kington, the agency's principal deputy director, who until this month was doing Krensky's job in an acting capacity. \u201cHe understood that a big part of the responsibility of this office is to integrate sciences that cut across the agency. He got that in a very concrete way.\u201d And by the end of 2008, Krensky's job will grow. In the recent law, Congress created a new NIH division \u2014 the Division of Program Coordination, Planning and Strategic Initiatives (DPCPSI) \u2014 that is similar to, but not the same as, Krensky's current shop. A sort of OPASI-plus, DPCPSI (which Krensky will direct) will incorporate offices including OPASI, the Office of AIDS Research and the Office for Research on Women's Health, with a combined annual budget of $150 million. Although the new law explicitly states that these offices will keep their current powers, it has caused some to complain about reporting to Krensky rather than directly to Zerhouni. Krensky will also oversee an exhaustively detailed accounting of the agency's science spending that will for the first time allow members of the public \u2014 and Congress \u2014 at the click of a mouse to examine every NIH project being funded in a given disease area, with dollar amounts and other details attached. This 'portfolio analysis' tool, intended to go public in Febuary 2009, is using sophisticated computer software that will capture elusive disease connections not immediately obvious in research grants, along with a new, pan-NIH set of disease definitions laboriously crafted with input from hundreds of scientists. \u201cThe idea is to identify gaps in funding but also redundancies,\u201d Krensky says. \u201cIt has the potential to find out that various people are funding similar things.\u201d Such talk can be unsettling to institute heads. \u201cPeople are worried that the historic independence of individual institutes' leadership could be undermined by having this kind of central analytic and oversight capacity within the NIH director's office,\u201d says David Korn, the senior vice-president for biomedical and health sciences research at the Association of American Medical Colleges. But Korn says that worry is misplaced. A former dean of medicine at Stanford who knew and admired Krensky when he was on the faculty there, Korn contends that Zerhouni \u201cis on exactly the right track\u201d in appointing Krensky to take a more sophisticated and rigorous approach to analysing the research portfolio of the massive agency. In a time of fiscal duress on Capitol Hill, he says, the NIH needs just such numbers to assure the public its money is being spent as wisely and effectively as possible. Others, including scientists who have spent their lives in the trenches of investigator-initiated research, see Zerhouni, Krensky and the entire enterprise of the new office as well-meaning but misguided. \u201cThe function of a federal agency that funds science is to respond to the innovative, novel and exciting ideas generated by individual scientists,\u201d says Gerald Weissmann, a longtime NIH grantee who is the director of the Biotechnology Study Center at the New York University School of Medicine. \u201cConsidering the enormous lassitude of large organizations, I tend to doubt that we will get more bang for the buck from big science.\u201d Indeed, senior agency administrators have been perennially tempted to direct science, rather than letting it come to them, says Murray Goldstein, a former director of the National Institute of Neurological Disorders and Stroke who spent 40 years at the NIH. There are many first-class grant applications from individual scientists that are not being funded, he says. \u201cWhat's the priority?\u201d Krensky is unapologetic. \u201cHaving the kinds of infrastructure that OPASI will develop doesn't take away from individual investigators \u2014 it buoys them,\u201d he says, \u201cby giving them tools that modern science requires.\u201d What's more, he argues, \u201cIt's only 1.7% of the budget. Investigator-initiated awards are still the mainstay.\u201d As if to remind himself of this, Krensky intends to keep his hand in at the bench, working in the lab his wife will be running at the National Cancer Institute. There, he's hoping to find applications in tuberculosis for the cytolytic molecule granulysin. But for the moment, he has to dash to a meeting of institute directors. There, he and NIH chiefs will be deciding the fate of $60 million in unspent 2007 roadmap funds. A top aim is to jumpstart the new epigenetics and microbiome initiatives from the latest phase of the roadmap, which is rolling out this autumn. Mark Lively, a biochemist at Wake Forest University in Winston Salem, North Carolina, says he hopes Krensky's priorities stay in the right place. Recalling a recent Krensky speech to the Federation of American Societies for Experimental Biology, he says he has a \u201cgood feeling\u201d about Krensky: \u201cHe spoke to us all like a scientist and not yet like a government bureaucracy leader. I hope he can maintain his optimism and not find the government rules and regulations too stifling for his ideas.\u201d \n                     NIH 'roadmap' charts course to tackle big research issues \n                   \n                     Early success claimed for Zerhouni's NIH roadmap \n                   \n                     New NIH project could be road to ruin for basic research \n                   \n                     US elections \n                   \n                     NIH Roadmap for Medical Research \n                   \n                     Lucile Packard Children's Hospital \n                   \n                     OPASI \n                   Reprints and Permissions"},
{"file_id": "448746a", "url": "https://www.nature.com/articles/448746a", "year": 2007, "authors": [{"name": "Katharine Sanderson"}], "parsed_as_year": "2006_or_before", "body": "Space exploration usually means leaving Earth's orbit. But chemists are now burrowing inside solids to open new vistas. Katharine Sanderson reports from the internal frontier. Solids have a reputation for permanence and reliability. Astronauts and sailors rejoice in returning to solid ground. And few things are more durable than solid gold. Unlike their flightier cousins, the gases and liquids, solids have regular structures that generally resist deformation. But there are materials that challenge our notions of a solid. They are rigid and strong, yet flexible, incredibly light yet so porous that the surface area of one gram could cover an entire football pitch. These solids are known as metal\u2013organic frameworks, or MOFs. They have no internal walls, just a bare molecular scaffold, creating a regular, sturdy, crystalline structure that is packed full with empty space. But if the scientists building these structures get their way, the materials will soon be packed full of useful gases, such as compressed hydrogen or methane, making it easier to store and transport these alternative fuels. Chemists expected that the vast, open structures of MOFs \u2014 more air than solid \u2014 would collapse readily. Yet the structures can be designed such that they are held together entirely by strong chemical bonds \u2014 carbon\u2013carbon, carbon\u2013oxygen or metal\u2013oxygen. They are composed entirely of joints, made from metal oxide units, and struts, which are organic groups. As long as those metal oxide joints don't shear or buckle under pressure, the structure stands firm, like builders' scaffolding. \n               Structural gymnastics \n             The resulting wall-less structures produce solids that are incredibly light with very low densities \u2014 sometimes as low as 0.2 gram per cubic centimetre \u2014 allowing a chunk of the material to float on water. Some cleverly engineered MOFs can even flex and bend slightly without collapsing. With all that space to play with, these solids are more easily penetrated than most. Chemists have used porous materials, including clays and zeolites, to trap and store gases for decades. But the pores and channels in naturally occurring porous materials are of varying sizes, so researchers have sought to make porous structures with uniform openings. In trying to construct such materials in the 1990s, Omar Yaghi at the University of California, Los Angeles, hit on a recipe for making frameworks with precisely controlled pore sizes. In 1998, Yaghi engineered the first MOF structure by mixing together two molecular building blocks \u2014 namely metal oxide and organic groups 1 . Since then, researchers have created materials with larger and larger pores. In doing so, they have repeatedly broken records for internal surface area in solids, and for low density, making zeolites look stodgy by comparison. But record-busting is not the only motivation to get bigger surface areas. \u201cIt's not just an obsession of mine to get higher numbers, it's the way to compact more and more gases into smaller volumes,\u201d says Yaghi. Every advance has made it easier to stuff more gas into the structure, attracting the interest of German chemicals giant BASF, which is about to move its MOF research into small-scale production. Yaghi is thrilled by BASF's investment in his work. In Yaghi's lab, MOFs can now be constructed to order. He has compiled a list of metal fragments to act as the scaffold joints, and a compatible range of organic links. If these linking groups are linear \u2014 that is with connecting points on both ends \u2014 they can be used to make chains or cube-like structures. More exotic networks can be made from triangular or square groups with connectors sticking out from each corner. BASF supports another MOF chemist, G\u00e9rard F\u00e9rey at the Lavoisier Institute in Versailles, France. F\u00e9rey claims to hold the world record for the internal surface area of a MOF 2 , published in 2005, at 5,900 square metres \u2014 or, the same area as an average-sized football field \u2014 for every gram of material. His material was built from triangular groups of chromium atoms linked together by terephthalic acid molecules. Yaghi could soon beat F\u00e9rey's record. His latest MOF \u2014 MOF-200 \u2014 looks likely to have a whopping surface area of 8,000 square metres per gram, once the synthesis is complete. But Mark Thomas, an expert in porous solids at the Newcastle University, UK, cautions against reading too much into these claims, which are based on calculations developed for materials with much smaller pores, and with internal walls. \u201cIt's a pore volume converted into a surface area,\u201d he explains, \u201cit is an apparent surface area.\u201d F\u00e9rey agrees. \u201cThe numbers don't represent anything physically,\u201d he says. \u201cWe have to find a new definition for the surface of very large pores.\u201d Still, Yaghi says that the numbers do have meaning, and they can be used to compare MOFs with each other and with other porous materials. The surface area of a single layer of porous carbon, when calculated in a similar way, comes out at a relatively tiny 2,600 square metres per gram. \n               Storage space \n             Yaghi's hopes for trapping and storing gases within the cavernous space provided by MOFs are focused on hydrogen storage. Hydrogen offers a low-polluting alternative to petrol as a fuel for road vehicles, but it faces many technical and infrastructure barriers. Not least of these is storing enough hydrogen on a vehicle in a safe and affordable way. The US Department of Energy says the ultimate hydrogen car will need enough fuel for a 480-kilometre trip, and yet the fuel tank must not be too heavy or too bulky. It has set a target of 2010 for a viable storage system in which hydrogen makes up 6% of the system's total weight. In addition, the system has to operate over a range of \u201cexpected ambient conditions\u201d \u2014 storing and releasing hydrogen at temperatures between \u221230 \u00b0C and 50 \u00b0C and at a maximum onboard operating pressure of 100 bar. None of the existing options achieve this. Conventionally, hydrogen has been stored as a gas by keeping it in a high-pressure canister, or stored as a liquid at a chilly 20 kelvin (\u2212253 \u00b0C). But high-pressure tanks are bulky, and liquid hydrogen is expensive to keep cool. In principle, porous materials can increase the amount of hydrogen stored in a given volume, without relying on extremely high pressures or low temperatures. In the late 1990s, carbon nanotubes generated much excitement as a way to store hydrogen. Researchers in industry and academia claimed storage levels ranging from 3% to 10% by weight at ambient temperatures and pressures. But most of these results \u2014 including one claim that graphite fibres can adsorb their own weight in hydrogen \u2014 have not been reproduced. Researchers are now more cautious of spectacular claims about hydrogen storage. Today's front-runners for hydrogen storage in materials are metal hydride systems. These operate on the principle of chemisorption \u2014 whereby hydrogen is trapped by forming a chemical bond with the material, so forming a metal hydride. They operate at moderate pressures, but the material needs to be heated to release the hydrogen, thereby consuming more energy. The current storage record for metal hydrides is 9% by the weight of the material alone, but the Department of Energy's target includes the fuel tank and accessory parts in the calculations, so that value translates into just 4% by weight for the overall storage system. A different process, known as physisorption, is responsible for the weak electrostatic attractions that hold gases inside MOF pores. Weak interactions mean that releasing the gas from the material requires less energy \u2014 nothing like that needed to break chemical bonds. \u201cThe good thing about physisorption is that it's easily reversible,\u201d explains chemist Matthew Rosseinsky from the University of Liverpool, UK. And the great thing about MOFs that enhances that easy reversibility is their openness. \u201cThings move in and out with great facility,\u201d says Yaghi. For now, though, trapping hydrogen in MOFs still requires low temperatures. Although MOFs need pressures of only 70 bar to get hydrogen into the materials, they typically need temperatures around that of liquid nitrogen, at 77 kelvin \u2014 to keep them there. This is because of the weak interaction between hydrogen and the scaffold. As it starts to warm up, the energy in the system breaks the interactions, leaving the hydrogen free to exit the framework. \n               Out cold \n             Yaghi claims the record for hydrogen storage in MOFs. He says that at 77 kelvin, and at a pressure of 70 bar, his MOF-177 material can store 7.5% hydrogen by weight 3 . Although that sounds promising, carrying tanks of liquid nitrogen as well as hydrogen is not a viable option. \u201cThat's not very practical when you want to run an automobile,\u201d Yaghi admits. And even at these temperatures, MOF-177 still misses the 2010 target \u2014 which sets a goal for the material alone of 9% by weight. Yaghi is optimistic about achieving the 2010 target with a MOF that can operate at room temperature. F\u00e9rey is more sceptical. He thinks that the first step should be to move to slightly warmer temperatures: \u201cIf we reached 150 kelvin instead of 77 kelvin it would be a great improvement.\u201d And recent work has challenged the idea that bigger pores are always better. Martin Schr\u00f6der at the University of Nottingham, UK, has calculated the optimum pore size for stuffing hydrogen into a MOF \u2014 with the surprising result that medium-sized, rather than giant, pores were the winners 4 . With smaller pores, the thinking goes, the scaffold is more curved, improving its chances of interacting with gas molecules. It is the lure of engineering MOF structures to enhance these interactions that attracts Yaghi. If MOF structures could be tweaked to strengthen physisorption, they might be able to operate at higher temperatures. But this is a delicate art: if physisorption is too weak, the hydrogen won't stick, yet as it increases more energy is needed to release the gas. What's needed is something in between a physical electrostatic interaction and a chemical bond. \n               Weighty advantage \n             Yaghi proposes modifying the organic groups in his MOFs by adding a light metal such as lithium. The extra electrons provided by lithium would strengthen the interactions between the gas and the MOF. Yaghi predicts that by simply adding some lithium into his system he could improve the current room-temperature record of 1% hydrogen by material weight to 4%. Others are exploiting MOF's ability to flex and bend to create clever hydrogen-trapping devices. Thomas and Rosseinsky and their colleagues constructed a cylindrical MOF from a network of large cavities separated by small windows 5 . The windows are too small to let hydrogen through. But when the structure is completely dried out under vacuum, it can flex and the windows open slightly so that hydrogen can enter. As the pressure is raised to 1 bar, roughly atmospheric pressure, more hydrogen stuffs into the cavities, attaching itself to the scaffold and making it more rigid. This forces the windows to close, trapping the hydrogen inside. At this point, the pressure can be reduced to as low as 0.01 bar and the inherited rigidity keeps the hydrogen trapped. This all takes place at the temperature of liquid nitrogen, but it means that when the temperature is raised, the windows can open again and set the hydrogen free. The system is not perfect by any means, admits project scientist Darren Bradshaw, at the University of Liverpool, noting the low storage level achieved so far: just 1% by weight. But, he says, \u201cit's a proof of principle that small windows that dynamically open are a good way to store hydrogen\u201d. If MOF technology is going to be a serious contender for hydrogen storage, industry needs to get involved, not least for scaling-up production. MOFs are not yet cheap, easy things to make. They also contain metals, which can be expensive. This is where BASF comes in. BASF has been involved in MOF research for eight years and has switched its activities from the research division into BASF's daughter company, BASF Future Business in Ludwigshafen, Germany. \u201cThe transfer from academia to industry is really a tough job,\u201d says BASF project manager Thomas Danner. Researchers need only small amounts of product, so the solvents and starting materials can be more expensive than those used in industry. In the short term BASF is not looking at MOFs for storage of hydrogen, but rather for natural gas or methane. \u201cSo many questions remain to be answered for the hydrogen economy,\u201d says Danner. Natural gas, however, is an existing market, with millions of vehicles running on compressed natural gas (CNG) already on the roads. Today, most CNG vehicles are used for public transport, and buses can tolerate the bulky storage tanks and shorter driving distances. But as the number of CNG vehicles grows, BASF predicts that longer driving distances and lighter storage will become key demands. More importantly, MOFs can trap complex gases, such as methane, more easily than they can hydrogen. These gases have more electrons available to interact with both the organic and metal groups. Existing MOFs can store methane and carbon dioxide at room temperature, circumventing one of the biggest challenges of hydrogen storage. \n               Scaling up \n             BASF can now make kilograms at a time of MOFs. Danner says that the development stage will last until at least 2009, at which point he expects to see a MOF product on the market. The challenge is not necessarily the technology, but finding the right market for the right product. According to Danner, adding MOFs to a standard CNG fuel tank operating at 200 bar pressure can already increase the distance travelled with a single tank by 25%, but he sees no reason why this cannot be increased to 45% in future. For now, the practical potential of these super-sponges seems as vast as the space they contain. And whether MOFs ultimately deliver on their gas-storage promise, or disappoint, as carbon nanotubes did, researchers will find other uses for them. For example, their ability to absorb large amounts of guest molecules makes them ideal for catalyst applications. F\u00e9rey remains amazed that almost no research has been done on MOF catalysis. He is also developing a biomedical MOF for delivering vast quantities of drugs to the bloodstream, while Yaghi is pursuing carbon dioxide storage in MOFs. \u201cIt's the very beginning, the limits are in our imagination,\u201d says F\u00e9rey. \n                     Middle-sized holes best for storing hydrogen \n                   \n                     Hydrogen cars will save lives \n                   \n                     The hole story \n                   \n                     Nature Materials \n                   \n                     DOE targets \n                   \n                     DoE hydrogen storage \n                   \n                     Omar Yaghi \n                   \n                     CNG vehicles \n                   Reprints and Permissions"},
{"file_id": "448404a", "url": "https://www.nature.com/articles/448404a", "year": 2007, "authors": [{"name": "Michael Hopkin"}], "parsed_as_year": "2006_or_before", "body": "Part of The  Simpsons ' greatness is a willingness to find the humour in absolutely everything \u2014 including science. Executive producer Al Jean, the show's head writer and a Harvard mathematics graduate, talks to  Nature  about how to get a laugh out of Euler's formula. \n               The Simpsons Movie \n                comes out this week \u2014 is there much science in it? \n             The crisis that precipitates the plot is environmental \u2014 Lisa's trying to warn the town about it and she gives a lecture entitled 'An Irritating Truth'. She's often the voice for the writers, even though she's eight years old a lot of us identify with her. But she's also depicted as socially unpopular, and is not always listened to. \n               As writers do you set out to satirize public attitudes to science? \n             Our general agenda is to show both sides of an issue and to let the viewer make up his or her own mind. In my lifetime I've seen science viewed as the saviour for everything, but now it's almost come full circle, because nothing can completely solve everyone's problems, and the disappointment when that happens is extreme. So now people are casting scientists as villains and not listening to them, which I think is tragic. But we make fun of everything, so if a scientist appears on the show we make fun of them too. Generally our depiction of scientists is that they're insular and have bad social lives, and say things in an obscure fashion that isn't always comprehensible to the layman. From my limited experience in the scientific world I wouldn't say it's completely off the mark. \n               One episode in which the show does take sides is the one in which Lisa protests against creationism in her school. \n             What we say is that there are conservatives, like Pope John Paul II, who believe in the theory of evolution, and that it's far from a liberal theory: it's scientific, it's as close to a fact as can be. We did say that Flanders, who opposed the teaching of evolution, is sincere in his beliefs. We tried to take his emotions seriously. What's really funny is that they had a debate here between the Republican candidates [for the presidential nomination], and the moderator said \u201cso, which of you believe in evolution?\u201d And you could see a couple sort of raising their hands and then changing their minds, and I'm going \u201chow can you not be sure whether you think that's true or not? It's not a matter of opinion.\u201d \n               You've had several famous scientists, including Stephen Hawking and Stephen Jay Gould, as guest stars on the show. \n             People seem thrilled that we've had Stephen Hawking on the show, and no one could be more thrilled than I. Although one joke that reflects the attitude of the public to science is when he appeared in front of the people of Springfield and told them what they should do, and Homer said, \u201cYeah, Larry Flynt is right!\u201d \n               Do you have a dream scientific guest who you'd love to have on the show? \n             Living or dead, it would be Isaac Newton. But living would be a tough question. Fifty years ago, Albert Einstein was the epitome of a scientist as far as the public were concerned, and was regarded as a hero. But there isn't anybody comparable today. I think it shows how science has been made by some to appear in a more ambiguous light. \n               The Simpsons \n                also boasts staggeringly obscure mathematical references. \n             One that always makes me laugh is in a recent episode in which Homer and Marge are at a baseball game where the public has to guess the attendance, and each of the options is a different mathematical irregularity \u2014 one's a perfect number, one's a sum of four squares. They're all in the thousands and they're numbers that nobody except a mathematician would, at face value, recognize as anything unusual, but if you're really sharp you'll pick it up. I love the fact that we can throw that sort of thing in. My favourite mathematical reference on the show was when we did an episode where Apu was a witness in a courtroom and the lawyer asked if he had a good memory. He said, yes I do, I've memorized pi to one million decimal places, and Homer said \u201cmmm... pi\u201d and started drooling. We did call Caltech [the California Institute of Technology in Pasadena] to check that whatever we said the millionth decimal place was was correct. My favourite mathematical equation is e i\u03c0  + 1 = 0, and we threw that into one episode even though not all of our audience would necessarily understand it. \n               In one episode Professor Frink shocks a lecture theatre into silence by shouting \u201cPi is exactly three!\u201d \n             One thing I always thought was funny was that in the 1890s, I believe, the state of Indiana declared that the value of pi was 22/7. I mean, it's just... the idea that you could change a mathematical concept to suit a legislative whim is nutty. \n               So is there a formula for writing good jokes? \n             I look at comedy writing mathematically, it's sort of like a proof in which you're trying to find the ideal punchline for a setup, and when you get it it's a very elegant feeling. It's a little like the feeling I used to get on completing a proof when I was doing maths at college. \n               Several Simpsons writers have backgrounds in maths and science. Are you a bunch of geeks at heart? \n             When we're alone we talk about maths [chuckles], but we've learned that there's a wider world, so we don't always expose others to it, and we do it in a subtle way. \n               Do you ever regret not pursuing your mathematical studies? \n             I wouldn't have been among the top people in my field; there were guys I knew in maths who were just the best. I don't know if I would ever have achieved what they did, so I'm very happy doing what I'm doing. And what does    Nature    have to do to get a mention on    The Simpsons ? We should mention  Nature ! There's still time, we're only 400 episodes young... (see  Box ) \n                 Interview by Michael Hopkin. \n               \n                     Freudian quips \n                   \n                     http://www.nature.com/news/specials/oscars2006/index.html \n                   \n                     The Simpsons Movie \n                   Reprints and Permissions"},
{"file_id": "447251a", "url": "https://www.nature.com/articles/447251a", "year": 2007, "authors": [], "parsed_as_year": "2006_or_before", "body": "Donations from philanthropists and private foundations are increasingly finding their way into biomedical research. Lucy Odling-Smee takes a look at some of the richest and most influential funders. \n                     Biomedical philanthropy: State of the donation \n                   \n                     Biomedical philanthropy: Love or money \n                   \n                     Biomedical philanthropy: The giving machine \n                   \n                     Stowers Institute \n                   \n                     Bill & Melinda Gates Foundation \n                   \n                     Howard Hughes Medical Institute \n                   \n                     Wellcome Trust \n                   Reprints and Permissions"},
{"file_id": "448860a", "url": "https://www.nature.com/articles/448860a", "year": 2007, "authors": [{"name": "Emma Marris"}], "parsed_as_year": "2006_or_before", "body": "Elephant populations are soaring in some parts of Africa. Emma Marris discovers there's no single way to fit them in amid the people. The car creeps towards the left as the driver tries to get as close as possible to the group of elephants foraging at the road's edge. The elephants walk silently, communicating in companionable purrs. The loudest noise is the crackle of the tough foliage they are eating. \u201cYou're crowding Agatha and she's going to have to press into the bush to get by,\u201d says Katie Gough from the back seat. Gough, based at Nelson Mandela Metropolitan University (NMMU) in Port Elizabeth, South Africa, has been studying these animals for four years, and can tell most of the elephants apart by the wear and tear on their ears or by idiosyncratic wrinkles. She is right about Agatha, who, slowly moving her enormous body into the thorny shrubs, turns her head and gives the occupants of the car a look that everyone reads \u2014 scientific prohibitions on anthropomorphization be damned \u2014 as reproachful. Agatha's home, Addo Elephant National Park in South Africa, has too many elephants. In 1954 there were 22 animals in a park of about 2,300 hectares. They were the remnants of a herd hunted nearly to extinction by one hired hunter in 1919. He was carrying out the orders of local orange growers who were sick of elephants gorging themselves on their crops. Today, there are around 460 animals in about 26,000 hectares \u2014 or roughly double the 1954 density. Throughout its history, densities have waxed and waned; in the main camp of Addo they now stand at about three elephants per square kilometre. One estimate of the maximum number of elephants this area can sustainably support is about 0.5 elephants per square kilometre 1 . \u201cTwenty or thirty years ago, things were black and white,\u201d says Graham Kerley, the elephant expert at NMMU who drove too close to Agatha. \u201cYou had elephant management and you had  laissez-faire .\u201d Although faced by an excess of elephants, Kerley is clearly still charmed by them, as he is by all the carefully managed animals at Addo \u2014 among them the elegant kudu, zebras with tawny rumps, and the immense ostriches sprinting along the road. These days no one believes in  laissez-faire . The effects that too many elephants have on their environment are easy to see. Climbing the stairs over a fence to cross from an elephant area into a pachyderm-free zone, the landscape switches from patchy shrublands to a lilliputian forest containing a wide variety of plants, including spectacular aloes. \n               Giant appetite \n             In places where the Addo elephants have already consumed the juicier vegetation, they now feed on bushes such as spekboom, sweet thorn and the bee-sting bush \u2014 plants as tough and spiny as their names suggest. In the Addo area, elephants may threaten some 168 species of plants with extinction 2 . These include the geophytes and the aptly named succulents, including the jade plant. And as the miniature forest turns into miniature savannah, other animals will see their habitats change. Animals that live in areas dense with vegetation, such as the cape grysbok and bushpig, will have their habitats fragmented. As the Addo experience shows, if your goal is to preserve an entire ecosystem and not just the elephants within it, pachyderm numbers have to be controlled. But where park managers, scientists and government experts disagree on the best means of managing elephant populations, the elephants can find themselves left in limbo. For an outsider, it's a challenge to switch from seeing African elephants as much of the Western world usually does \u2014 as majestic, rare animals dying at the hands of villainous ivory poachers \u2014 to a more nuanced, more African conception. Depending on who and where you are in Africa, elephants may also be livelihood-destroying crop-raiders, expensive tourist lures, environmental menaces, or dinner. \n               Different countries, different stories \n             So it comes as no surprise that Africans can become irritated when told how to manage their elephant populations by interested parties abroad. Well-meaning and sometimes technically expert outsiders are constantly offering opinions on whether to cull animals, whether to try contraception, whether some kinds of ivory should be legally sold, and so on. Often, onlookers will treat the whole continent as a single case, when each country and each area has its own problems to solve. Most African elephants still live outside protected areas, and so they increasingly come into fraught contact with Africa's expanding population. Botswana has an estimated 150,000 elephants roaming without major barriers between them and pastoralists herding livestock. In Namibia, elephants range free of people in the western deserts, but they butt heads \u2014 sometimes literally \u2014 with agriculturalists elsewhere in the country. In other countries, elephants have been fenced into reserves or hemmed into protected areas by development as more land is built on. Across southern Africa as a whole, elephant populations are increasing by about 4% annually, while herds in central and western Africa continue to struggle with poaching and habitat fragmentation 3 . \u201cI could tell you a different story for every African elephant range,\u201d says Holly Dublin, an elephant expert who is chair of the Species Survival Commission at the World Conservation Union. Nowhere do humans and elephants pack in more tightly than in South Africa. So it's tempting to see the country as an example of how elephant management might look across Africa in the future. But the country is different to others in that there are fewer than half-a-dozen unfenced elephants nationwide. These are the elephants of the Knysna forest, which have held on to their freedom only by virtue of living in an inaccessible valley. All the other elephants are descendants of remnant herds, living behind strong fences built with materials such as railway ties and mineshaft cabling. \u201cSouth Africa cannot be considered indicative of the challenges facing elephant conservation in the rest of Africa,\u201d says Dublin. \u201cIt is more like living in America and trying to take care of elephants. For the most part, rural communities are not trying to make their livelihoods right next door.\u201d South Africa itself has only a small fraction of the continent's elephants. But to judge from media reports, they might as well have almost all of them. Coverage of the culling controversy is largely responsible for this. Until 1994, managers at South African National Parks (SANParks) would shoot elephants in places such as Kruger National Park, far north of Addo on the Mozambique border, whenever their numbers got too high. The slaughtered elephants would be given to local people as a free meal. Other elephants were transported to parks that wanted more elephants or sold to private reserves. Before the modern practice of culling whole family groups was introduced, culling occasionally resulted in orphans that grew up to behave antisocially. For example, some orphan elephants, which were moved from Kruger to other parks, famously became maladjusted juveniles that went around killing rhinoceroses 3 . \n               Getting in a flap \n             In 1994, culling was stopped because of the objections of local and foreign animal rights groups. Elephant densities have since increased, and in Kruger, where there are now about 0.63 elephants per square kilometre 4 , large bulls flex their muscles by pushing over baobab and marula trees. And after a decade of debate, in 2004, SANParks announced that a whole suite of tools \u2014 including culling \u2014 should be considered for managing elephant numbers. This news was not received well, especially by wildlife conservation groups, who claimed that culling was inhumane, even unethical. Hector Mogame, executive director of Conservation Services at SANParks, Pretoria, doesn't see it this way. \u201cThe issue of ethics is about power,\u201d he told the Society for Conservation Biology in Port Elizabeth this July. \u201cThe viewpoint of opposition to lethal control is usually favoured by affluent people \u2014 people with money.\u201d Citing the furore over SANParks' plan to resume culling, he says the debate is about the people on the ground being trumped by the rich and powerful. SANParks officials have historically had a relatively free rein to manage the animals within their park boundaries. But after the 2004 outcry, South African government ministers stepped in and asked SANParks to provide scientific justification for its culling plan. In 2005, Marthinus van Schalkwyk, minister of tourism and the environment \u2014 and these two are very closely linked in South Africa \u2014 called for a scientific round table representing different views on the elephant management debate. Mogame and Kerley were two of those invited. In January 2006, the Elephant Science Round Table decided that Kruger didn't have enough data at that moment to show culling was necessary, but smaller reserves experiencing \u201cbigger pressures\u201d might need to do something immediate. The group has continued to meet unofficially to advocate for a major research push on elephant management. SANParks has moved away from suggestions that there is an ideal number of elephants. \u201cThe maintenance of biodiversity is best achieved by permitting \u2014 or if appropriate actually encouraging \u2014 variation in time and space, rather than attempting to manage for stability,\u201d says Wanda Mkutshulwa, head of communications at SANParks. As of now, the official management strategy of SANParks is \u201cwait and study\u201d. In the meantime, the elephants are continuing to restructure their habitat. Their movements are dictated by the presence of food and, crucially, water. Around any surface water where elephants drink, you will find a 'piosphere', a circle of reduced vegetation cover demonstrating the landscape-changing power of these animals. The shrubs remaining have just a few leaves, and seem as if they are clinging to life. There is no grass, and warthogs are pale from dust bathing. The word 'desertification' springs to mind. Adrian Shrader of NMMU argues that if you pack water-holes and elephants in so densely that the piospheres begin to overlap, landscapes may begin to suffer irreversible losses. But even without such pressures, a high number of elephants will over time reduce plant diversity. \n               Which path to tread? \n             For now, park managers still have several other options besides culling that they can actively pursue. But which is best? Most elephant experts have a pet intervention they study or favour. Round-table member Bruce Page, at the University of KwaZulu-Natal, Durban, is keen on contraception, which has promise but is extremely awkward in execution. It requires identifying elephants, and then shooting hormones into them from a truck or helicopter. Field trials at Kruger recorded reasonable success rates. In Page's population models, contraception and culling affect population structure in different ways. The proportion of the population under ten years of age increases under culling, but decreases under contraception. When population control ceases, you get a baby boom after culling that you don't get after the cessation of contraception. Still, Page estimates that under operating conditions, the best one can hope for is to prevent about 75% of the females from giving birth. Other experts call for the removal of artificial water sources, which are sometimes used to lure the animals to within sight of tourists. In Addo, one can have lunch at the park restaurant and then stroll across the car park to a viewing area overlooking a water-hole, and expect to see at least one or two massive grey beasts having a drink. But there are at least two problems with removing the water- holes. At Addo, water -holes had to be put in because the river, which would have been the water source for elephants in the area, is not part of the park. Without the water provided by SANParks, the Addo elephants would perish along with all the other mammals. Another problem is that when you use water scarcity to limit elephant numbers, they die of thirst. This would be an even worse public-relations disaster than shooting them, according to Shrader. \u201cIt would hit the evening news: skeletal elephants and their babies.\u201d More controversially, Rudi van Aarde of the University of Pretoria advocates knocking down all the fences you can and allowing natural metapopulation dynamics to manage elephant numbers. The idea is to get as close to 'natural' as possible. \u201cIt is when you put up hard fences and dig the land full of water-holes, that we are creating problems that we shouldn't try to solve through the barrel of the gun or through contraception,\u201d says van Aarde. Critics note that many people would be angry, not to say frightened, if the fences between their village and a herd of elephants were removed. Van Aarde insists that enough unpopulated land could be cobbled together to avoid this, and that if elephants caused problems at the extremes of their range then they could be shot by villagers. But this would put the onus of management on local people with a lot to lose. \u201cMaybe with time this will prove too extreme to be applicable,\u201d he admits. The argument over elephant ranges ultimately depends on how you draw the maps. Many people feel that the areas currently occupied by elephants in Africa are as large as can be reasonably expected. With human numbers growing as they are, there is no space for the elephants to expand into. And even when range expansion has been tried, there's scant evidence that taking down the fences encourages elephants to populate a new area. In many cases, they like it where they are. If they can get enough water, why move? The removal of a fence on one side of Kruger to create a park that crosses into Mozambique has reportedly boosted the transit of people in stolen cars much more than it has increased the movement of elephants. \n               Packing their trunks \n             There are also proposals to move elephants from areas where they are living in high densities to areas where numbers are fewer, but according to Dublin, interest in that idea has \u201ccooled down\u201d in recent years. Moving an elephant is \u2014 no surprise here \u2014 an enormous undertaking. One must knock them out and then heft them around in trucks chaperoned by experienced personnel. It is very expensive. Another concern, according to Dublin, is the destination. For example, if it is sparsely populated because poachers are killing elephants there, why move the beasts into a death trap? Some non-governmental groups such as the Massachusetts-based International Fund for Animal Welfare and the South Africa-based Peace Parks Foundation have funded relocations, but usually on the scale of tens of animals. The Kenya Wildlife Service, which manages elephants in that country's protected areas, relies heavily on relocations, as it does not cull. Ask any elephant expert about the various management options, and they will usually say they may need to use all or some, depending on the situation. \u201cYou cannot expect that one size fits all,\u201d says Dublin. \u201cIf you take away any of the tools, that is not the best thing for elephants or for Africa.\u201d Above all, elephant managers \u2014 be they governments, park managers, or consulting scientists \u2014 have to decide the reasons for managing elephant numbers. If it is to achieve maximum biodiversity, one strategy and population density of elephants might be best. If it is for iconic savannah landscapes, or to increase the genetic diversity of elephant populations, other routes may be needed. As Kerley says, \u201cif you just want an elephant viewing park, you might as well just tear all this out and plant alfalfa\u201d. Part of the challenge is that we have only vague ideas of elephant numbers and movements in Africa prior to Europeans showing up with flintlock elephant guns and sailing away with holds full of ivory. Some elephants migrate. Females and their offspring move about together; bulls often live alone. It is no easy task to establish a baseline \u2014 whether for elephant numbers or behaviour, or for the landscapes in which they lived. Even the parts of Addo park that are untouched by elephants are not a good guide to what the forests looked like before European colonization, because there would have been some elephants around. \u201cWhat we don't have information on,\u201d says Kerley, \u201cis what the landscape should look like and how many elephants will achieve that.\u201d Some argue that culling, although the most controversial intervention, may also be the most 'natural'. Those who study elephants say that they do not change their reproductive behaviour much in the face of food scarcity \u2014 although van Aarde and others maintain that elephants do this in savannah ecosystems. Other herbivores will have their first offspring later in life and have subsequent offspring farther apart in lean times. Not so the exuberant elephant, which, given enough foliage to start with, will reproduce and consume its way right into a nasty population crash. So perhaps something else was limiting their numbers in ages gone by. \n               Mammoth story \n             We do know that before the Europeans arrived, Africans hunted elephants in pit traps, or, in what must have been a pretty spectacular manoeuvre, running up and slashing them in the Achilles tendon. David Cumming of the Percy FitzPatrick Institute of African Ornithology at the University of Cape Town speculates that human predation has been the key limit on the population of elephant-like animals \u2014 proboscideans \u2014 in Africa for more than a million years. He is of the camp that favours predation over climate change as an explanation for the extinction of earlier proboscidean species. The fossil record confirms that the number of proboscidean species dropped from nine to two after humans appeared on the evolutionary scene. Cumming mentions, too, the many large mammals that humans almost certainly did kill to extinction. So can South Africa's elephants prosper without culling? Is it possible that by protecting elephants in special areas, we are removing what once limited their numbers? Animal rights activists and tourists don't like hunting or culling. But it might be the simplest and most direct way to reduce elephant numbers. Not all experts agree that pre-gun human predators were capable of taking down enough elephants to directly regulate the population. But even if it's not the 'natural order', it might still have a place in management. And as Kerley explains, hunting isn't as simple as reducing the total number. \u201cIt's not how many elephants you kill, but how you influence resource use through fear. Elephants are incredibly risk averse.\u201d So by killing a few elephants near a certain village or farm or water-hole, you may be able to keep the rest of the herd, especially the more timid females and calves, from using that area. Kerley argues for exploring all options: \u201cI suspect that in the end we will come up with a uniquely African solution. We might reinstate some level of predation \u2014 call it predation rather than culling \u2014 but we will also make more space for elephants.\u201d And the inescapable conclusion from all across Africa is that elephants do have to be actively managed. It is interesting to look into their large dark eyes \u2014 famous for seeming wise, even world-weary \u2014 and wonder if they have any inkling that they are no longer in charge. \n                     The tusk detective \n                   \n                     Black-market boom for ivory \n                   \n                     South Africa expected to propose elephant cull \n                   \n                     The elephant vanishes \n                   \n                     IUCN African Elephant Status Report \n                   \n                     South African National Parks \n                   Reprints and Permissions"},
{"file_id": "447368a", "url": "https://www.nature.com/articles/447368a", "year": 2007, "authors": [{"name": "Alison Abbott"}], "parsed_as_year": "2006_or_before", "body": "It is 50 years since Arvid Carlsson showed dopamine to be a neurotransmitter. Alison Abbott profiles a chemical and its champion. They were conscious but you wouldn't know it: able to perceive the world around them but powerless to look around, sniff the air or to cry out. So when the young scientist injected them with a chemical called  L -dopa, he witnessed what seemed to be a miracle. They stirred, opened their eyes and began roaming around as if nothing had happened. This may sound familiar from the book  Awakenings 1  \u2014 the true story of how, in 1963, the neurologist Oliver Sacks used  L -dopa to spectacularly revive patients with sleeping sickness who had been 'frozen', speechless and motionless, for more than 40 years. But the unwritten and equally startling prequel took place in Lund, Sweden, several years earlier. The protagonists were rabbits; their saviour a young Swedish pharmacologist called Arvid Carlsson. In his experiment, Carlsson showed that dopamine \u2014 the chemical manufactured from levodopa, or  L -dopa \u2014 acts as a neurotransmitter in the brain, passing signals between neighbouring neurons. Injection of  L -dopa restored the propagation of electrical signals in the brains of rabbits that had been rendered catatonic, allowing the animals to move. But the pharmacological establishment was scornful of Carlsson's claim. At a London meeting in 1960, the foremost experts in neural transmission made it clear that they didn't believe him \u2014 dopamine was thought to be the metabolite of another neurotransmitter rather than one in its own right. Within years the critics were silenced. Dopamine was shown to be a pivotal chemical in the neural circuits that drive pleasure and addiction, as well as in illnesses such as Parkinson's disease, for which  L -dopa quickly became a first-line treatment. It remains so today. In 2000, Carlsson shared the Nobel Prize in Physiology and Medicine for his discovery. And next week neuroscientists will gather at a meeting in Carlsson's home town of Gothenburg, Sweden, to celebrate the 50th anniversary of his formative paper on the awakened rabbits 2 . During the past half century, Carlsson and dopamine have followed intertwined paths. Researchers now understand that the way dopamine works is subtle and complex, and its mechanisms of action are central to the function of many neurological and psychiatric drugs. And Carlsson, now a sprightly 84-year-old, still spends hours pondering the mysteries of brain chemistry. But he feels marginalized in Gothenburg and, last year, the institute established in his name closed prematurely after bitter feuds about funding. The field of biomedicine has also evolved during this time, and much has changed. \u201cWe used slide rules and manual calculators back then, so statistical calculations were quite time consuming,\u201d Carlsson says. But a willingness to embrace new techniques and fight for controversial ideas, as Carlsson has done throughout his career, remains essential to success. \u201cHe was one of my heroes when I was a young neurology resident,\u201d says William Langston, head of the Parkinson's Institute in Sunnyvale, California. \u201cWhat impressed me was his brilliant deductive reasoning.\u201d Carlsson is a polite, soft-spoken and humorous gentleman with a hallmark bow tie. He grew up in a family of academics in Lund that focused on the humanities but, in what he refers to as a minor act of rebellion, he chose science over the arts because he thought it more useful to society. He is also single-minded and uncompromising, features that no doubt contributed to his success as a scientist against stiff opposition \u2014 and perhaps also to his later problems. Carlsson calls himself a lucky man, and says his first stroke of luck came when he arrived in the lab of chemist Bernard 'Steve' Brodie at the National Institutes of Health in Bethesda, Maryland, in the summer of 1955. Brodie's lab pioneered studies with reserpine, one of the very first drugs to be introduced for the selective treatment of schizophrenia, and hence one of the hottest molecules for pharmacologists of the day. Reserpine injections made rabbits catatonic, but how the drug worked was a mystery. \n               Direct measures \n             At the time, pharmacologists typically tested the potency of neurotransmitters with assays of their biological activity \u2014 for example, applying them to a piece of animal gut under tension in an organ bath to see how much they could contract or relax the muscle. Brodie's team instead developed the spectrophotofluorimeter, a machine able to measure how much neurotransmitter was synthesized from fluorescently tagged precursors. This allowed the researchers to measure the precise level of a compound extracted from tissue rather than an indirect measure of its activity, and later became a standard instrument in biological labs. When Carlsson returned to Lund after his five-month visit to Brodie's lab, the first thing he did was order a spectrophotofluorimeter. \u201cI didn't want to be confined by the indirectness of bioassays,\u201d he says. Carlsson's work with this device showed that reserpine completely drains the stores of several neurotransmitters in the brain. Loss of one of these was causing the rabbits to become catatonic \u2014 the question was, which one? Carlsson reasoned that he could answer this question by adding back the missing neurotransmitter to rabbits that had been frozen with reserpine \u2014 the crucial awakening experiment. The blood\u2013brain barrier prevents the neurotransmitters noradrenaline and serotonin from passing into the brain from the blood, so Carlsson instead injected precursors of these molecules that can enter the brain and are then metabolized into the relevant neurotransmitter. One of these precursors was  L -dopa, which is converted into dopamine and then, in turn, into noradrenaline. But when Carlsson examined the revived rabbits' brains after injecting  L -dopa he saw a lot of dopamine and very little noradrenaline. At this point it dawned on him that dopamine could be a neurotransmitter in its own right, a memory that still summons astonishment to his face. Within months his graduate students \u00c5ke Bertler and Evald Rosengren had found that dopamine is normally concentrated in areas of the brain known to be involved in movement, such as the basal ganglia 3 . Knowing that high doses of reserpine cause side effects that are similar to some of the movement difficulties experienced by patients with Parkinson's disease, Carlsson proposed that this disease might be caused by a lack of dopamine. \n               Sparking opposition \n             In late 1958, Carlsson travelled across the Atlantic to explain his ideas to a symposium in Bethesda. There, his story was well-received. \u201cBut this was not the case when I presented my results at the Ciba meeting in London,\u201d says Carlsson, who is still clearly a bit hurt. The prestigious 1960 Ciba Symposium on Adrenergic Mechanism attracted all the key European players in the field. At the time, a vigorous debate was going on between the 'soups' \u2014 who thought that nerve transmission occurred through chemicals \u2014 and the 'sparks', who argued that it was all electrical. The soups had more or less won their case for neurotransmission outside the brain but, owing to lack of experimental evidence, the sparks' view still dominated the brain itself. \u201cIt's hard to imagine now, but when I was an undergraduate student at Cambridge [in the late 1950s] we were taught categorically that there was no chemical transmission in the brain \u2014 that it was just an electrical machine,\u201d recalls pharmacologist Leslie Iversen, professor emeritus at the University of Oxford, UK. In this setting, Carlsson's ideas went down like a stone. The meeting unceremoniously rejected his interpretation of his data and, to his mortification, the single comment in the discussions praising his work was excluded from the symposium book. \u201cThe Ciba meeting might have been the opportunity to tell the world how things really were, but there was uniform hostility from the community,\u201d says Iversen. Carlsson stuck to his guns, and data began to amass to a point that made denial impossible. Later in 1960, for example, the Austrian pharmacologist Oleh Hornykiewicz published studies on postmortem brains from patients with Parkinson's disease, showing the absence of dopamine in the basal ganglia 4 . And a few years later, Annica Dahlstr\u00f6m and Kjell Fuxe at the Karolinska Institute in Stockholm, Sweden, showed that in the healthy brain neurons in this region contain high levels of dopamine 5 . In 1961, neurologist Walter Birkmayer, working together with Hornykiewicz, injected the first Parkinson's patients with  L -dopa to dramatic effect, allowing previously immobile patients to move freely 6 . Carlsson recalls the penchant of his collaborator Tor Magnusson for testing drugs openly on himself \u2014 something that would be regarded with horror today. Expecting to see some kind of neurological effects, Magnusson hooked himself up to intravenous dopamine, but, says Carlsson, \u201call we saw was emesis!\u201d Many other aspects of practising science were different then. Fax and e-mail did not exist, and all papers were read in the library rather than on a computer. \u201cWe certainly wore white lab coats and we addressed secretaries and assistants more formally as Miss,\u201d Carlsson says. Over the next few years, clinicians learnt to start patients on low doses of  L -dopa to avoid side effects such as nausea and vomiting. They also noticed other intrinsic imperfections of the drug. After a few years of therapy, its effects temporarily, and unpredictably, switch off in some patients. In severe cases, patients can suddenly become frozen and be stuck, immobile, for minutes or hours. Neurologists were also starting to learn that dopamine is involved in much more than the control of movement. They noticed that high doses of  L -dopa cause some of the symptoms of psychoses such as schizophrenia, and that the antipsychotic drugs used to treat schizophrenia can cause the same movement problems that are indicative of dopamine deficiency in Parkinson's. This led to the idea that schizophrenia could be related to a disturbance of dopamine neurotransmission in areas of the brain other than those involved in movement. \n               The molecular multitasker \n             Carlsson reasoned that antipsychotic drugs could be blocking dopamine receptors, and that neurons might be spitting out more and more dopamine to try to compensate for the blockage. He was right. This type of feedback control of neuronal activity is now a well understood, and critical, phenomenon in neurotransmission. And this way of thinking won Carlsson many fervent admirers. He is \u201cjust about the most creative, intuitive scientist I've met\u201d, says Solomon Snyder of the Johns Hopkins University in Baltimore, Maryland, who discovered opiate receptors in the brain. Another side effect of  L -dopa treatment is that patients may develop an irrational tendency to gamble. It is now well known that the neural pathways controlling behaviours such as motivation and feelings of reward pivot around dopamine. These pathways drive pursuit of food and sex \u2014 and are hijacked by addictive drugs and addictive behaviours such as gambling. In the 1960s, Carlsson was among the first to spot that drugs of abuse work by boosting dopamine transmission in particular brain areas. Too little dopamine in one area produces Parkinson's, too much dopamine in others can cause psychoses. Over recent decades the importance and complexity of the dopamine system have mushroomed in scientists' eyes. Dopamine acts on many types of receptor, at varying levels and in different brain areas, and in concert with other neurotransmitters. Another of Carlsson's legacies has been the development of dopamine stabilizers. These are dopamine-like molecules that have been chemically modified so that they activate dopamine receptors to only a certain degree, effectively constraining the level of dopaminergic activation in the brain to within the healthy range. The theory is that a stabilizer could compensate for lack of dopamine in Parkinson's without causing overactivation; or block the overactivity in schizophrenia without too much depletion. Several companies have dopamine stablizers in development, and one, aripiprazole, has been approved by the US Food and Drug Administration for use as an antipsychotic for schizophrenia and bipolar disorder. When Carlsson reluctantly retired aged 66 \u2014 then the law in Sweden \u2014 he kept his research going by forming a company called A. Carlsson Research where, among other things, he developed a dopamine stablizer. Then, in 2000, Carlsson's work on dopamine and its control of movement was recognized with a share of a Nobel prize. \u201cI won it 40 years after my discovery,\u201d he jokes. \u201cEinstein won his some 20 years after his discovery. So I guess my work was twice as complicated as Einstein's.\u201d Until this time, Carlsson and dopamine had both followed stellar trajectories. But after the Nobel prize, Carlsson's luck began to falter \u2014 and the uncompromising side of his nature, which had served him so well in his scientific career, failed to help. Just a few months before winning the Nobel, Carlsson was voted off his own company's board of directors. And in the next few years, plans for his namesake institute also went awry. \n               Difference of opinion \n             The Arvid Carlsson Institute was launched in November 2004 with SKr630 million (US$92 million) funding over five years \u2014 a tribute from Gothenburg University and the regional authorities to the city's only Nobel prizewinner. Its original mission was to promote health care and neuroscience research in the region, and Carlsson was named honorary chairman of its developmental council. But disagreements began almost immediately about how the money should be divided up. Carlsson wanted a significant proportion to support his field of neuropharmacology, but others argued it should go to neuronal stem-cell research. Scientists decline to discuss details of the arguments, but the hostilities became so bitter that the institute was dissolved in April 2006. Carlsson's daughter Maria, who is also a neuropharmacologist, receives a small proportion of the funds \u2014 too small in the opinion of her father, \u201cgiven that the research was stated to be done to honour my contributions to science\u201d. Carlsson's contributions to science continue. He is closely involved in his daughter's work and they sometimes publish together, although he wishes he could still work in the lab. He also jets around the world to meetings. Much in neuroscience has changed during Carlsson's time, but he believes at least one thing has remained constant. \u201cWhen I started my career, the most important generator of science was the human mind,\u201d he says. \u201cThis has probably not changed much during the past half-century.\u201d \n                     Ecstasy eases Parkinson's in mice \n                   \n                     Registry could nail causes of Parkinson's \n                   \n                     Parkinson's trial halted \n                   \n                     Neuroscience: Addicted \n                   \n                     Nature 407, 661 (12 October 2000) | doi:10.1038/35037760 \n                   \n                     www.nature.com/nrn/journal/v1/n2/full/nrn1100_084a_fs.html \n                   \n                     http://nobelprize.org/nobel_prizes/medicine/laureates/2000/ \n                   Reprints and Permissions"},
{"file_id": "447374a", "url": "https://www.nature.com/articles/447374a", "year": 2007, "authors": [{"name": "Eric Sorensen"}], "parsed_as_year": "2006_or_before", "body": "When you win a Nobel prize, you become much in demand. Eric Sorensen takes a look at how laureates decide which worthy causes to lend their name to. Half a century has passed since chemist Linus Pauling spearheaded one of the biggest petitions ever in science. More than 11,000 scientists, including 36 of Pauling's fellow Nobel laureates, signed on to call for the \u201cultimate effective abolition of nuclear weapons\u201d. The petition led to the first international attempt to control nuclear weapons \u2014 the Partial Test Ban Treaty. And on the same day in 1963 that the treaty went into effect, the Norwegian Nobel Committee announced that Pauling would receive the peace prize to go with his 1954 Nobel Prize in Chemistry. Scientific petitions graced by laureates have become common tools of activism \u2014 clamouring to free the unjustly imprisoned and cure a myriad of perceived ills, from drug laws to inadequate research funding to nuclear proliferation. Having a Nobel laureate's name on a petition almost guarantees it extra attention: in a newspaper story's first paragraph, if not its headline. The past year alone has seen laureates' signatures on petitions to make publicly funded academic research available for free on the Internet; decriminalize homosexuality in India; raise the US minimum wage; decry the Bush administration's alleged politicization of science; and restrict the US president's authority to order nuclear strikes against nations without nuclear weapons. And last week in Jordan, about 35 laureates gathered at the third Petra conference to discuss major world issues; it concluded with the launch of a US$10-million fund to bolster scientific projects in the Middle East. As the high-powered scientific petition has grown, signature gathering has become its own industry. Leading the way is the watchdog group Union of Concerned Scientists in Cambridge, Massachusetts, whose 1992 World Scientists' Warning to Humanity on the environment was signed by about half of the living Nobel laureates in the sciences, for a total of roughly 1,700 researchers. Five years later, no fewer than 110 laureates signed the group's Call for Action on global warming. Politicians also routinely summon laureates \u2014 or at least their signatures \u2014 to their pet causes. During the 1996 presidential race, Bill Clinton had seven Nobel laureates backing his budget plan; his Republican rival Bob Dole had four. In 2004, George W. Bush's campaign mustered only six Nobel laureates to deride the tax plan of Democratic nominee John Kerry, which had the backing of 10 Nobel economists. And that illustrates a fundamental problem with the Nobels: tease out the inner workings of matter, and you become a Nobel laureate; sign a petition, and you become a number. Roald Hoffmann of Cornell University in Ithaca, New York, won the 1982 Nobel Prize in Chemistry. He says that laureates become a sort of commodity from the moment he or she is asked if their name can be used. \u201cIt's a kind of detachment of the person from the subject,\u201d he says. \u201cDo they really want to know what I think? Or do they just want my name?\u201d As the number of Nobel-signed petitions has risen, their value has decreased, says Peter Agre, who won the Nobel Prize in Chemistry in 2003 and is now vice chancellor for science and technology at Duke University School of Medicine, Durham, North Carolina. \u201cThe more you sign and respond to, the less valuable your service is,\u201d he says. Since winning the prize, Agre has signed petitions opposing the inclusion of intelligent design in science curricula and seeking leniency for an infectious-disease specialist charged with mishandling lethal biological agents. He also supported the candidacy of Kerry \u2014 along with 47 other laureates. \n               Great minds think alike \n             So how does a Nobelist, newly inundated with fame and requests, sort through the competing, well-meaning demands for his or her time and name? For Agre, it means looking at who else is already involved; if he sees other respected names on a petition, such as Harold Varmus from the University of California School of Medicine in San Francisco and winner of the 1989 prize for medicine, then he's in. Nicholaas Bloembergen, emeritus of Harvard University in Cambridge, an honorary professor at the University of Arizona and winner of the 1981 prize for physics, says that he is asked to sign petitions half a dozen times a year. He signs about once a year, acting as a physicist on scientific issues such as federal funding for research and as a well-read citizen on social issues such as overpopulation. He was, for instance, one of 41 laureates signing a protest against the Iraq war and one of 100 laureates to warn in 2001 that world security hangs on environmental and social reform. Robert Solow, from the Massachusetts Institute of Technology in Cambridge and winner of the 1987 prize for economics, signs no more than half the times he is asked and he tries to stick to economics issues, recently advocating for a rise in the minimum wage. \n               Stairway to heaven \n             Solow says that: \u201cThe big difficulty is usually that you're asked to put your signature to some statement that someone else has written.\u201d If the statement is not in line with his thinking, he figures he has no business signing it; if he agrees with it in broad terms but not in specifics, he then asks if his disagreement with the author is minor enough. And he breaks little sweat over \u201cgeneral statements about peace and things like that. It's not my specialty but I read it over and figure when I get to the pearly gates, St Peter won't turn me away for favouring peace\u201d. Aaron Ciechanover of the Ruth and Bruce Rappaport Faculty of Medicine in Haifa, Israel, and winner of the Nobel Prize in Chemistry in 2004, faces a lot of local demands for his attention. He is often asked and signs a few petitions \u2014 for instance, a petition calling on Israeli prime minister Ehud Olmert to open contacts with Syria and Hamas, or a call to the government in Sudan to stop the murder in Darfur. \u201cI do not think that as a Nobel laureate my opinion is better or carries any extra weight than that of anybody else,\u201d he says. \u201cYet I am guided by my principles and conscience and am voicing my opinion on issues I think are important. At the end, it may add up.\u201d Yet for all their celebrity, Nobelists seem to be decidedly weak instruments of social change. Leslie Gelb, Pulitzer prizewinning reporter for the  New York Times  and president emeritus of the Council on Foreign Relations, a non-partisan think tank based in New York City, says that he has seen petitions come and go over the past four decades. \u201cI have not seen evidence that petitions change [the minds of] decision-makers,\u201d he says. Gelb has routinely asked people in power if they had seen petitions in full-page advertisements, and nine times out of ten, he was told they had missed it. And perhaps that's not always a bad thing. \u201cThe big difference in life before and after you win a Nobel prize is there's nothing you can say that's so stupid that some magazine or newspaper won't print it,\u201d notes Solow. Others question the importance of some of these issues. \u201cThere is no petition so stupid that it cannot get at least a handful of signatures from Nobel laureates,\u201d economist George Stigler told a group of students at the University of Chicago in Illinois in 1970 \u2014 12 years before he himself won the Nobel prize. Even the best-intentioned petition can fail to measure up to its signatories' hopes. For instance, the 1992 'warning to humanity' encompassed a wide range of environmental issues, including ozone depletion, water pollution, declining fisheries, degraded soils, destroyed rainforests, species extinctions, overpopulation and poverty. Its release coincided with United Nations debate over actions outlined at the Earth Summit in Rio de Janeiro, Brazil, earlier that year. Yet few newspapers gave it more than a brief mention. \u201cIt is a very powerful and beautifully written document and it was just totally ignored,\u201d says Canadian biologist and broadcaster David Suzuki. \u201cTo me that is a stunning indictment of the kind of society we have that scientists are marginalized by the media,\u201d he says. Gelb, for his part, thinks that the laureates watered down their message by asking for too many things at once. Still, laureates interviewed for this story would like to think that their support counts for something. The Nobel is a brand that many argue confers prestige and honour on petitions and their sponsors. \u201cPeople like to put movie stars' and sports figures' names on petitions,\u201d says Philip Anderson of Princeton University in New Jersey, who won the 1977 Nobel Prize in Physics. \u201cIs there any reason a sports figure would know more about famine or any other issue?\u201d To many, the consequences of remaining silent are too great to ignore. \u201cThe majority of Nobel prizewinners are willing to use the 'power of shame' to inform the public about developments that should not be accepted,\u201d says German physicist Klaus von Klitzing, director of the Max Planck Institute for Solid State Research in Stuttgart, and winner of the Nobel Prize in Physics in 1985. \n               History repeats \n             Dudley Herschbach of Harvard University and winner of the 1986 prize for chemistry, sees his activism as part of a long American tradition that stretches back to Benjamin Franklin, the eighteenth-century statesman and scientist. As an active laureate, Herschbach sits on the board of the Council for a Livable World in Washington DC. This political-action group was created by physicist Le\u00f3 Szil\u00e1rd, the person who first imagined a nuclear chain reaction and the leader of the Manhattan Project petition that failed to keep President Harry Truman from using the atomic bomb on Japanese civilians. Herschbach, for his part, worries about the 1,000 tonnes or so of weapons-grade enriched uranium that exists in the former Soviet Union, and the very real possibility that the uranium will fall into the hands of terrorists. \u201cThings like that,\u201d Herschbach says, \u201cyou have to do what you can to get some attention.\u201d But even he is well aware of a laureate's limits. Herschbach holds many of his field's highest honours, yet accepts that many people were more interested when he appeared on an episode of the television show 'The Simpsons'. And perhaps it is just a symptom of democracy that a laureate may hold no more sway than any one else. \u201cEach man counts for one,\u201d says economist James Buchanan, \u201cand that is that.\u201d He should know. He won a Nobel prize. See Editorial,  \n                     page 354 \n                   . \n                     Nobel laureates: Close encounters \n                   \n                     Nobel Prizes \n                   \n                     Union of Concerned Scientists \n                   \n                     Council for Livable World \n                   \n                     Scientists and Engineers for America \n                   Reprints and Permissions"},
{"file_id": "447372a", "url": "https://www.nature.com/articles/447372a", "year": 2007, "authors": [{"name": "Geoff Brumfiel"}], "parsed_as_year": "2006_or_before", "body": "Quantum cryptography is theoretically unbreakable, yet a handful of physicists are finding ways to hack into its secrets. Geoff Brumfiel finds out how. On an otherwise quiet Saturday evening in 1946, Frederic de Hoffman briefly thought he had lost the secrets of the atomic bomb. De Hoffman was a physicist at Los Alamos National Laboratory in New Mexico. As part of his job, he kept the design for the weapon in nine filing cabinets in his office. When he came into work and opened one, he found an enigmatic note: \u201cWhen all the combinations are the same, one is no harder to open than another \u2014 same guy.\u201d De Hoffman thought that the 'same guy' was the person who had tried to break into the lab's secure facility earlier that summer, but, as it turned out, the thief was standing next to him. It was Richard Feynman (pictured above), a leading quantum theorist who had a reputation as an incorrigible prankster. He had broken into de Hoffman's filing cabinets earlier that day to grab a few documents he needed to write a report. Security has come a long way since 1946, but some things never change. Quantum physicists are now learning how to crack what is arguably the most secure form of data transmission ever conceived: quantum cryptography. This encryption method is theoretically unbreakable, but nevertheless, groups are finding weaknesses that may require rethinking the design of commercial systems. The work, says Seth Lloyd, a physicist at the Massachusetts Institute of Technology (MIT) in Cambridge, is a cautionary tale. \u201cNothing is unassailable,\u201d he warns. Quantum cryptography uses the fundamental laws of physics to encode information in the quantum states of particles, usually photons. Most existing systems use a protocol known as Bennett\u2013Brassard 1984, or BB84, which generates a secure quantum 'key' that can be used to encode messages sent between parties. BB84 works like this: the sender transmits an encoded key by polarizing single photons along one of two axes \u2014 up and down or tilted at 45\u00b0 \u2014 and sending them along a fibre-optic cable to the receiver (see diagram). The receiver then randomly chooses an up-and-down or tilted filter to read each photon. If the filter they choose is aligned with the sender's original polarization, the receiver will be able to read one bit of the sender's data, but if they choose the wrong alignment, then the photon, by virtue of quantum mechanics, will pass through the filter in a random orientation. After the original key has been transmitted, the sender and receiver compare the filters they used for each photon. They throw away the random bits and keep the rest as part of a new, secure shared key, which is then used to encode a longer message sent over regular channels. At first glance, the BB84 protocol may seem complicated and highly inefficient. But the method behind it means that it can't be intercepted without the sender and receiver finding out. Suppose an eavesdropper were to try to listen in on their conversation. As she reads each photon with her own two filters, she passes it along to the receiver, but not necessarily in the same orientation as it was originally sent. Therefore, when the sender and receiver compare their keys, they will find a lot more random bits created by the eavesdropper and they can immediately cut off their communication or try to send a fresh key through a different channel. What sets the BB84 protocol apart from other forms of cryptography is that the code should be impossible to crack. Most of today's keys are encrypted with a mathematical technique that depends on large prime numbers. Security hinges on the idea that large numbers are hard to factor into primes, but there is no way to be sure of that assumption, says Daniel Gottesman, who studies quantum cryptography at the Perimeter Institute in Waterloo, Canada. \u201cWe don't think there's a way to do it on a classical computer in any reasonable amount of time, but there is no way to prove that,\u201d he says. By contrast, the security of BB84 and other quantum protocols hinges on the immutable laws of physics: \u201cGiven that quantum mechanics is correct, then we can mathematically prove that this idealized BB84 protocol is actually secure.\u201d But if the idealized version of the BB84 protocol is secure, the real version can be anything but, according to Charles Bennett, a computer scientist at IBM Research in Yorktown Heights, New York. Bennett is one of the 'B's in the BB84 name, and he and other researchers built the first demonstration unit in 1989. In that very first quantum-cryptographic system, Bennett recalls, the polarization of the photon was switched by use of a high-voltage power supply. \u201cThe power supply hummed differently depending on whether or not the voltage was being applied,\u201d Bennett says. \u201cIf you listened, you could hear it.\u201d \n               No escape from reality \n             Nobody was planning on sending state secrets over the experimental set-up in Bennett's office. But while showing that quantum cryptography could work, he and his collaborators inadvertently demonstrated something else: idealizations are often far from reality. \u201cIt's hard to ensure that any box that you build is entirely secure,\u201d he says. Such real-world security is the key to moving quantum cryptography from the lab to the commercial sector, and it has been a slow process. BB84 protocols require sending single photons, but early technology often sent more than one photon at a time, raising the possibility that an eavesdropper could read one without disturbing the others. Single-photon systems became commercially available a few years ago, but they remain modest in their capabilities. Error rates can be high, data speeds slow, and they can only be transmitted as far as a single photon can travel along a commercial fibre-optic line. Meanwhile, researchers are stepping up their attacks on quantum cryptography. The most scientifically sophisticated strike was conducted earlier this year by MIT physicists led by Jeffery Shapiro and Franco Wong 1 . The team stole information from a passing photon by entangling its polarization with its own momentum. This quantum-mechanical entanglement allowed the team to read about 40% of the key while leaving the polarization relatively untouched. But Shapiro and Wong both admit that an eavesdropper would be defeated just by increasing the length of the key. Commercial systems already use long keys to deflect such attacks. Other vulnerabilities could be even more dangerous because they have been overlooked by theorists. For instance, theoretical physicists assume that the sender and receiver will have absolute control over their equipment. But the real world is less precise, says Nicolas Gisin of the University of Geneva in Switzerland. Gisin and his colleagues have shown that an eavesdropper could learn a sender's polarizations by shining a light down the fibre and into the sender's set-up 2 . Because the cryptographic protocol assumes that light will only come from the sender, it doesn't take into account such dirty tricks. And still other attacks can take advantage of simple flaws in individual components. Earlier this year, Hoi-Kwong Lo and his colleagues at the University of Toronto in Canada showed that they could steal a commercial system's quantum secrets by exploiting a small defect in the receiver's photodetectors. The protocol under attack was different from BB84 in that it required two photons. The system switched on the two highly sensitive detectors only when it was expecting photons from the sender to avoid false alarms. But the detectors switched on at slightly different times. By delaying photons so that they arrived just as a detector was turning on or off, Lo showed that an eavesdropper could modify the measurement, which blinds the receiver to the eavesdropper's presence. Not everyone agrees on how serious the threats are to commercial systems. \u201cWe are quite confident that our system will remain impervious,\u201d says Robert Gelfond, chief executive of MagiQ, a quantum-cryptography company in Somerville, Massachusetts. Gelfond says MagiQ's government and military customers regularly try to breach their systems' security. \u201cThey want to see it and test for themselves,\u201d he says. So far, MagiQ has not had to modify any of its cryptographic technology. \n               Weak spots \n             But others think that more needs to be done to ensure that the systems live up to their theoretical reputation. \u201cThere's been a lot of lip service,\u201d says Gottesman. \u201cBut I don't think enough attention has been paid to vulnerabilities.\u201d The researchers are more concerned with getting their set-up to work than they are with finding ways to cheat the system, he says. That may be changing. Gisin thinks that the number of studies on potential attacks has risen over the past few years. \u201cThe entire field is getting more mature,\u201d he says. \u201cNow is really the time to think about these things.\u201d As quantum cryptographic networks grow in size and complexity, they are also at risk from new kinds of attacks. Gisin would like to see the industry develop standards for detectors, transmission lines and other equipment that would help to close future gaps in security. But there will always be a dirty trick to try, as the quantum-theorist-turned-safecracker Richard Feynman knew all too well. Feynman didn't rely on his theoretical brilliance to open the safes holding America's atomic secrets. He simply guessed the combination. He knew that his friend, a physicist, would undoubtedly choose a number he already had memorized, and the sly theorist got it on the second try: 27-18-28, the first six digits of the mathematical constant  e . \n                     Quantum cryptography is hacked \n                   \n                     Quantum cryptography goes wireless \n                   \n                     Quantum cryptography: Code-breakers confounded \n                   \n                     Quantum cryptography: Can you keep a secret? \n                   \n                     MagiQ \n                   \n                     Shapiro and Wong's Homepage \n                   \n                     Hoi-Kwong Lo's Home Page \n                   \n                     Nicolas Gisin's Page \n                   Reprints and Permissions"},
{"file_id": "447525a", "url": "https://www.nature.com/articles/447525a", "year": 2007, "authors": [{"name": "Kendall Powell"}], "parsed_as_year": "2006_or_before", "body": "No longer viewed as inert packets of energy, fat cells are two-faced masterminds of metabolism. Kendall Powell weighs up the differences between 'fat' fat cells and thin ones. Unwanted, unloved, yet often overabundant, few have much regard for fat. Scientists, too, long thought of fat cells as good-for-nothing layabouts unworthy of attention; containers stuffed with energy to be released at the body's command. So stuffed, in fact, that many other parts of the cell were thought too squeezed to function. So when, in the early 1990s, graduate student G\u00f6khan Hotamisligil at Harvard Medical School in Boston caught fat tissue doing something biologically remarkable, at first he did not believe his own data. He repeated the work many times, but it always came out the same: fat from obese mice was producing TNF\u03b1 \u2014 the hot inflammatory molecule of the day because of its role in autoimmune disorders such as arthritis. After he and his colleagues published their observation in  Science 1 , others in the field remaned sceptical. Hotamisligil says he was invited to speak at meetings \u201cfor entertainment purposes\u201d. Since then, fat cells have had an image change. This started with the 1995 discovery that fat secretes leptin, a hormone that tells the brain \u201cI'm full, stop eating\u201d. In retrospect, it makes sense that fat should tell the body how much energy it is storing and how much more to take in. But when it came to obesity-related problems such as type 2 diabetes and cardiovascular disease, fat was still not seen as an active player. These conditions were thought to be caused by an excess of nutrients from overeating, or a glut of fatty molecules spilling out of storage into the bloodstream. More than a decade on, fat has a higher status. Scientists know that fat cells pump out ten or more molecules called adipokines that carry messages to the rest of the body. And 'fat' fat cells \u2014 those common in the obese and which are themselves bloated with lipids \u2014 send different molecular messages from 'thin' fat cells. The signals from 'fat' fat are thought to directly promote insulin resistance and to trigger inflammation, which may, in turn, cause type 2 diabetes, cardiovascular disease, increased cancer risk and other obesity-associated problems. This means that it might be possible to treat these conditions without shedding the fat itself. Some remedies might be as simple as using anti-inflammatory drugs that have been around for more than a century; others might involve persuading obese fat cells to behave like skinny ones. Society may still view fat with resignation or even revulsion, but biologists have moved on. \u201cNo one appreciated the higher functions of fat,\u201d says Barbara Kahn, a diabetes and obesity researcher at Harvard's Beth Israel Deaconess Medical Center in Boston. \u201cThe fat cell's got to be put on the map now.\u201d An animal fat cell, or adipocyte, is a giant droplet of triglyceride molecules \u2014 each composed of fatty acids and glycerol \u2014 plus a life-support system of other organelles squashed to the side. After a meal, fatty acids and glucose enter the blood. Fat cells absorb the fatty acids, and the liver converts excess glucose into more fatty acids, which fat cells take up and store. In obesity, when the body is swimming in excess fats and glucose, fat cells pack more in and expand. \n               Sugar daddy \n             Fat and glucose control are linked: overweight people tend to develop insulin resistance and then type 2 diabetes, conditions in which the hormone insulin no longer promotes normal glucose uptake and fails to stem glucose production by the liver. But researchers generally assumed fat was a bystander in these problems. They thought the defect must lie in muscle and the liver, which take up and metabolize the vast majority of blood glucose. Then, in 2001, Kahn's group showed that fat tissue was managing much of the body's response to insulin. The researchers used genetic engineering to eliminate a glucose transporter molecule from the surface of mouse adipocytes, and found that the animals' muscle and liver cells also became insulin resistant \u2014 just as much as those from obese mice 2 . This suggested that obese fat cells make a circulating factor that makes other tissues resistant to insulin. In 2005, Kahn identified that molecule as RBP4 (ref.  3 ), which works partly by blocking the action of insulin in muscle and liver. The team went on to show that obese and type 2 diabetic patients have higher levels of RBP4 in their blood compared with healthy controls 4 . \u201cFat cells are really conducting the orchestra, telling the sugar where to go,\u201d says Evan Rosen, who also studies obesity at Beth Israel Deaconess but was not involved in the studies. Because of its link to human disease, RBP4 is one of the stars of a growing list of molecules secreted at higher levels by 'fat' fat cells compared to 'thin' ones \u2014 or vice versa. They include adiponectin, which is the opposite of RBP4 in that it improves the action of insulin, and is secreted in large quantities by normal fat cells but less so by 'fat' fat cells. Last year, a group led by diabetes researcher Philipp Scherer of the University of Texas Southwestern Medical Center in Dallas showed that mice lacking adiponectin suffer severe insulin resistance in the liver 5 . To some researchers, it now seems obvious that fat should control the uptake of glucose, because it may limit further fat accumulation. Lean fat cells produce signals such as adiponectin that promote glucose uptake into tissues and its accumulation as more fat. \u201cWe view it as a starvation signal, in essence, signalling that the fat cell is willing to accept additional [stores],\u201d says Scherer. When the fat cells fill up, the signal drops. Cells that become full of fat produce signals such as RBP4 that help stop cells in the body from sucking up glucose and laying down more fat. The glucose instead stays in the bloodstream, where it triggers the problems associated with diabetes. \n               Inflammatory acts \n             Engorged fat cells are complicated characters. Over the past decade it has become clear that they send out distress signals, such as TNF\u03b1, that can trigger inflammation, and that these contribute to insulin resistance. In their  Science  study, Hotamisligil and his adviser, Bruce Spiegelman of the Dana Farber Cancer Institute in Boston, used a molecule to soak up the TNF\u03b1 from 'fat' fat cells and found that obese animals got back their insulin sensitivity. We now know that obese fat cells also produce other inflammatory substances such as the cytokine interleukin-6 (ref.  6 ). In addition, immune cells called macrophages invade obese fat tissue 7 , where they begin contributing to a downward spiral of inflammation. There is overwhelming evidence that obesity and type 2 diabetes are accompanied by chronic, low-level inflammation of fat tissue, says Spiegelman. Researchers are now trying to work out what causes a 'fat' fat cell, one clogged with lipids, to start secreting a different set of molecules. Hotamisligil's team thinks that the answer may lie in the endoplasmic reticulum (ER) \u2014 the cell's centre for folding and processing many proteins. Perhaps because it is crammed with excess lipids and its metabolism is overworked, the ER is unable to properly fold proteins 8 . This type of cellular stress makes overstuffed fat cells stop making 'healthy' molecules such as adiponectin and begin making 'unhealthy' ones such as RBP4 and TNF\u03b1. These trigger insulin resistance in other cells and start the cycle of inflammation, increasing insulin resistance. This chronic inflammation and insulin resistance is now thought to be largely responsible for the plethora of higher disease susceptibilities that obesity brings, including cardiovascular disease, diabetes and cancer. Fat cells' dynamic new image is changing the way that some researchers think about treating these obesity-related conditions. Treatment usually focuses on losing the fat. But if the problem lies in the adipokines produced by 'fat' fat and the insulin resistance and inflammation they cause, it might be possible to deal with these problems while leaving the fat intact. This approach is needed, some experts say, because of the large number of patients who cannot or will not lose weight. Reducing RBP4 or raising adiponectin levels in obese patients are two potential treatments for type 2 diabetes. But clinical trials are a long way off because the exact way in which these proteins work is unknown. More likely in the near future is that molecules such as adiponectin and RBP4 may be used as biomarkers in the blood to indicate which overweight patients have higher levels of misbehaving 'fat' fat, and are therefore at risk of diabetes. Steven Shoelson at Harvard's Joslin Diabetes Center is working on the idea that the chronic inflammation found in the obese causes their diabetes, and he leads a clinical trial to test whether this can be reversed. Starting this year, his team will give half of 120 type 2 diabetics a 14-week course of salsalate, an anti-inflammatory drug similar to aspirin that has been used for more than 150 years. Shoelson was partly inspired to carry out the trial by research published in 1876 \u2014 long before the discovery of insulin \u2014 suggesting that salsalate could treat diabetics 9 . \n               Hot fat \n             Treatments that make 'fat' fat cells act like lean ones may be another way to tackle obesity. Diet and exercise do this by starving the body, prompting fat to release its stores into the blood for use in the muscles. But diet and exercise are hard to stick to. And liposuction, which sucks out some of the 'fat' fat cells, is probably not the answer, because the remaining cells are still 'fat' and produce disease-causing molecular signals. So some researchers are trying to exploit the molecular differences between different types of fat cell to convince 'fat' fat cells to reduce their own girth. In one such effort, Dominique Langin, a clinical biochemist at the Louis Bugnard Institute in Toulouse, France, hopes to exploit the properties of brown fat, a tissue that burns fuel to create heat. Human newborns have a small pad of brown fat on their back, but it quickly disappears. Adults are thought to have only a tiny number of brown fat cells among a mass of white ones. By switching on a gene called  PGC-1\u03b1 , Langin's team converted human white fat cells to brown-like ones in the lab dish 10 . The gene seems to switch on others normally active in brown fat. Langin suggests that white fat cells could be converted into brown ones, so that they burn up their fat inside the cell without releasing it into the bloodstream and clogging up arteries. But a drug that can turn on  PGC-1\u03b1  in human white fat is still distant, he says. Hotamisligil hopes to correct aberrant fat cells by relieving the stress on their ER. Last year, his laboratory fed obese and diabetic mice two chemicals that help proteins fold correctly in the ER of fat and liver cells. This improved the animals' ability to control glucose levels 11 . Because both chemicals are already used to treat other diseases, Hotamisligil says he wants to try testing them as a diabetes treatment in the near future. If this works, it will show that fat cells are not only doing something interesting \u2014 as Hotamisligil suspected 14 years ago \u2014 but that they can be manipulated for medical use. It takes a lot of willpower to convert a fat body into a thin one. But 'fat' fat cells may be more amenable to a change of character. (see  Location, location, location ) \n                     How fat genes differ from thin ones \n                   \n                     Medicine: Sleep it off \n                   \n                     Dietary advice: Flash in the pan? \n                   \n                     Science of dieting: Slim pickings \n                   \n                     Food In Focus \n                   \n                     NIDDK Weight-control Information Netweork \n                   \n                     American Diabetes Association \n                   \n                     International Association for the Study of Obesity \n                   Reprints and Permissions"},
{"file_id": "447522a", "url": "https://www.nature.com/articles/447522a", "year": 2007, "authors": [{"name": "Quirin Schiermeier"}], "parsed_as_year": "2006_or_before", "body": "How the oceans mix their waters is key to understanding future climate change. Yet scientists have a long way to go to unravel the mysteries of the deep. Quirin Schiermeier reports. The absence of data is not always a bad thing; it can sometimes make a job a lot easier. In decades past, for instance, climate modellers simply didn't worry too much about how turbulent processes in the ocean (known as ocean mixing) affected the outputs of their models. Not knowing how ocean mixing worked may not have been intellectually fulfilling, but it certainly made the models simpler to run. But data are now beginning to emerge \u2014 and they show, peskily, that mixing is not so simple after all. Information flooding back from instruments such as microstructure profilers, which are towed behind ships to gather data on factors such as temperature and conductivity, has started to quantify exactly how parcels of water mix with each other. In particular, the amount of mixing seems to be much less than models of the ocean would suggest, especially given the temperatures measured at the ocean's depths. Some of the mixing is missing. As a result, ocean mixing is an increasingly hot topic at conferences. \u201cThe field is changing quicker than many would have thought,\u201d says Raffaele Ferrari, an oceanographer at the Massachusetts Institute of Technology (MIT) in Cambridge. \u201cIt seems unlikely that we know the full story.\u201d The importance of mixing is that it helps the oceans move heat from A \u2014 normally near the Equator \u2014 to B \u2014 which will typically be nearer one of the poles. The best known mechanism is the global conveyor belt, or thermohaline circulation; warm waters move north through the Atlantic before becoming cool and salty enough to sink to the ocean bottom and flow back south, from where they are distributed worldwide 1 . The energy involved is prodigious: 2,000 trillion watts, or two petawatts, some 200 times the rate at which mankind uses energy, and a significant chunk of the energy flow needed to drive Earth's climate. Since early in the twentieth century it has been known that, without mixing, such a system would stall. The depths would fill up with cold water that, because it was dense, would not resurface, leaving a thin warm layer on top. Warmth is needed to add buoyancy to the bottom waters, and that means mixing them with the waters above. But no one can say for sure where the energy needed that drives this mixing comes from. Some have suggested plankton as paddles (see  'Could tiny creatures stir the whole ocean?' ). Others have called down the Moon in support of their theories, proposing tides as the driving force. The prevailing trade winds between the tropics of Cancer and Capricorn were perhaps the most obvious candidates for stirring the oceans, in principle providing sufficient energy to create a well-mixed upper layer in the tropical oceans. But the amount of mixing near the ocean's surface, as observed by techniques such as microstructure profilers and tracer experiments, is not as high as theoretical calculations would suggest, and not enough to keep the global circulation going. \n               Stormy weather \n             One recent idea is to look not at the regular winds that are the sailor's friend, but the rare ones that spell his doom \u2014 specifically, hurricanes. On  page 577  of this issue, modeller Matthew Huber of Purdue University in West Lafayette, Indiana, presents evidence supporting the notion that tropical cyclones play an important part in ocean mixing. Huber's work follows on from a 2001 study that suggested much the same thing 2 . In that earlier work, Kerry Emanuel, a hurricane researcher at MIT, proposed that tropical cyclones mix the upper ocean so well that they are responsible for a staggering amount of heat transfer: 1.4 petawatts. As most other researchers had ignored the role of cyclones completely, perhaps thinking them too sporadic to have such a sustained effect, the idea was met with some scepticism. The new study, based on more and better observations, dials this back to an estimated 0.26 petawatts per year \u2014 a number that Emanuel says is still \u201cin the right ballpark\u201d for the level of mixing he would expect from cyclones. Even with the lower mixing rates cited in Huber's study, the very idea that cyclones could substantially contribute to ocean mixing is so surprising that many are still likely to reject it. \u201cAt least in the present climate the effect is unlikely to be very large,\u201d says Ferrari. But that would not necessarily be the case in the climates of tomorrow (or, for that matter, the climates of yesteryear \u2014 see  'Lessons from the past' ). What is thought provoking about Emanuel's ideas is that he has also shown that high sea-surface temperatures could push up the strength of tropical storms. Others have speculated that global warming might also make storms more frequent. If cyclones are indeed playing a key role in ocean mixing, then increases in their power and frequency could change that role, and thus the climate. Hurricanes could matter far farther afield than the coasts they batter \u2014 their effects could be worldwide. There have long been debates on what global warming means for hurricanes; now we may see one on what hurricanes mean for global warming. \n               Tales from topographic oceans \n             Another source of mixing that was long ignored but is now increasingly accepted is down to the Moon and the tides, and the effect they have, not at shores, but in the oceans' depths. In the 1960s and 1970s, conventional oceanographic wisdom held that the considerable energy represented by the ebbing and flowing of tides was dissipated entirely through friction in the shallow coastal areas above continental shelves. But then pioneering oceanographers Walter Munk of the Scripps Institution of Oceanography in La Jolla, California, and Carl Wunsch of MIT proposed that tides penetrate all the way into deep waters, where their flow across the rough seafloor topography helps the water to mix and overcome the forces of gravity 3 . Studies from the Topex/Poseidon satellite that maps sea-surface elevation have since confirmed that tidal energy does indeed get dissipated in the deep ocean 4 . Other experiments have lent further support to this idea. The Hawaii Ocean Mixing Experiment has quantified the amount of mixing over undersea ridges 5 ; it found tides with amplitudes of up to 300 metres sweeping across the flanks of one ridge. Separately, the AnSlope project off Antarctica has looked at the continental shelf surrounding Antarctica and how it affects ocean mixing throughout the Southern Ocean. These projects have shown that most of the mixing takes place at the ocean's boundaries, from the drop-off of the continental slope to the tall mid-ocean ridges that run along the middle of the sea floor. Mixing doesn't occur everywhere to the same extent. It happens more around and above rough mountainous regions on the sea floor than, say, in the vast abyssal plains. This variability from place to place further complicates modellers' efforts to capture the mixing accurately. As always, new observations could help. Researchers are unleashing more microstructure profilers, along with remote sensing of sea-surface temperatures and sea-level anomalies, to keep trying to track down the missing mixing. Among other things, they need better to understand the role of 'mesoscale' eddies, some 50 to 100 kilometres across, that swirl around some parts of the oceans mixing water and affecting biological productivity 6 . Meanwhile, modellers are far from keeping up. Until now they have mostly pretended that mixing occurs uniformly throughout the ocean, and everywhere at the same rate. No one believes that is how the real ocean behaves; few, if any, think it is even a reasonable approximation for a functional climate model. \u201cIt's a really serious weak point,\u201d says Olivier Marchal, an ocean modeller at the Woods Hole Oceanographic Institution in Massachusetts. Without capturing the complexities of mixing better, the models will be hard put to capture how climate can be expected to change thanks to the greenhouse effect. For example, most researchers expect the oceans to become less dense. But that could change, depending on a key mixing parameter, called diffusivity, that most computer models have so far regarded as fixed. A changing mixing in the models could ultimately alter our view on key components of the climate system, such as the thermohaline circulation; in some models increased mixing could render it more powerful 7 . Researchers would be better off accepting this idea, many say, and moving forward with new ways to quantify the problem. But working the fine details of mixing into global models will remain a tall order. With so many new findings, those who study ocean mixing are scrambling to incorporate them all. And scientists may soon have a rule book by which to conduct their future research. Robin Muench, an oceanographer with the Earth and Space Research Institute in Seattle, Washington, leads a working group set up by the International Council for Science. Its goal is to identify the rate of mixing in the oceans as a whole as well as in specific regions where mixing is known to occur (or not). In July, the group is slated to deliver its final report in Perugia, Italy, which should help clarify the research agenda. And with that, the modellers will have their next couple of years' work laid out for them. \n                     Only mother nature knows how to fertilize the ocean \n                   \n                     Decades needed to tell whether ocean currents are slowing \n                   \n                     Polar core is hot stuff \n                   \n                     Great rivers of the ocean \n                   \n                     In Focus Climate Change \n                   \n                     SCOR working group on ocean mixing \n                   \n                     Hawaii Ocean-Mixing Experiment \n                   \n                     Hurricanes page from NOAA \n                   Reprints and Permissions"},
{"file_id": "447630a", "url": "https://www.nature.com/articles/447630a", "year": 2007, "authors": [{"name": "Alison Abbott"}], "parsed_as_year": "2006_or_before", "body": "Long a symbol of East German pride, the Charit\u00e9 medical school is flourishing in the twenty-first-century shake-up of German universities. Alison Abbott reports. The concrete high-rise of the historic Charit\u00e9 hospital was the pride of communist East Germany's medical sciences. Built in 1982, its 21 stories were a riposte to the Verlagshochhaus \u2014 a 19-storey tower that the Springer publishing group built close to the wall in west Berlin, and that was seen as a way to taunt people in east Berlin with visions of western freedom, progress and wealth. The monolithic response from the other side of the wall was a showcase building for a top research institute \u2014 one of the few institutes in the former republic that gave some scientists the freedom to travel abroad and provided a certain independence from the all-pervading communist ideology. The 'Charit\u00e9' sign on the top of the building could be read kilometres away \u2014 a proclamation that one of the city's proudest and oldest scientific institutions stood tall in the east. Today, the Charit\u00e9 still holds its head up high. Last year it was ranked top university medical school in Germany by an independent research assessment, creeping ahead of schools in Heidelberg and Munich. Much of its success is due to its having grabbed, with an alacrity not shown by most other universities, the opportunities offered by recent government reforms. Founded in 1710 as a plague house outside the city gates, the Charit\u00e9 was later converted into a hospital, and developed close links with the University of Berlin (now Humboldt University). It was the intellectual home of, for example, Rudolf Virchow, the father of modern pathology, and Emil von Behring, the bacteriologist who discovered the diphtheria toxin. The Charit\u00e9 officially became Berlin's university hospital in 1927. This was a time when Germany was the world leader in science and medicine; when it was unthinkable, for example, that anyone planning a research career would not learn German. But only a few years after the Charit\u00e9 won its university hospital status, the Nazis came to power, and by the end of the Second World War the picture had changed entirely. Physical and moral destruction had left the country's scientific landscape in ruins. The Charit\u00e9's historic buildings had to be rebuilt brick by brick from the rubble. Reading German would never again be compulsory for scientists from Chicago to Shanghai. On both sides of the wall, German science eventually recovered from the war and from the subsequent shock of reunification. Today the country's scientific impact is among the world's top five, thanks in large part to the network of Max Planck Institutes that serves as home to the majority of Germany's most-cited scientists. But the country's politicians think that things would be even better if the university sector, too, were to house similar excellence. They are aware that Germany boasts no heavyweights to rival the giants of Cambridge in Massachusetts \u2014 or, closer to home, of London or Paris. Although Germany spends more on research than either France or the United Kingdom, Heidelberg, Munich and Berlin do not shine in the world rankings. German universities rarely feature in lists of the top 50 worldwide. The shackles holding back the universities, critics say, were forged from a mixture of traditional stodginess and utopian zeal. In the postwar years, the strong hierarchy within German universities meant, among other things, that young scientists were rarely able to run their own research. Herr Professor \u2014 or the very occasional Frau Professorin \u2014 made all the decisions, applied for all the research grants and received guaranteed levels of research money from the university, no matter how productive he, or she, happened to be. The spirit of '68 and the political awakening it brought to Europe's students dovetailed with the postwar generation's concern that many of those in charge of the universities had served the Nazi regime. The result was a drive to democratize academia. For example, student, administrative and technical representatives were included on all university committees, including the selection boards for faculty appointments. Decisions became painfully slow. The university president had very few powers, and it was never clear who was responsible for anything. \u201cIt is really time to say goodbye to this collective irresponsibility,\u201d says physicist J\u00fcrgen Mlynek, former president of Humboldt University, where the professors have a majority of just one in the highest academic committee, the senate. \n               All things being equal \n             In the 1970s, a series of developments enshrined the concept of the 'equality of universities' in law. A degree from one university was supposed to be as good as one from any other university and the law prevented universities from selecting their own students and from charging for tuition. The roughly equal funding of the roughly equal universities provided adequate education for all but it didn't promote competition or innovation. And some of the biological sciences suffered other political constraints. The idea of genetic engineering was mostly rejected \u2014 an understandable reaction to the appalling abuses of Josef Mengele and his cohorts \u2014 and that held back the development of molecular biology. The first German factory for producing genetically engineered human insulin was supposed to open in the early 1980s, but was famously delayed for nearly a decade. Although the regulations for genetic engineering are now in line with those elsewhere, research on embryonic stem cells remains among the most restricted of European countries. Meanwhile in eastern Germany, research had been more or less stamped out of most universities. The freedom of thought it required was not seen as sitting comfortably with the teaching of young minds. Most research took place in the Soviet-style institutes of the German Academy of Sciences. The Charit\u00e9 was one of the few institutions spared. In the 1990s, reunification, with its bankrupting costs, forced the restructuring of East German institutes. The upheaval also provided an opportunity to take a closer look at the stagnating system in the west, and in particular at the perceived loss of the best young researchers to the United States.The result was a series of schemes to enliven the universities, for example by cutting the time it took students to graduate and by increasing competition between institutions. But the effect of federal initiatives is limited by the 16 state governments that hold sway over Germany's 100 or so research universities. In 2001, the research minister at the time, Edelgard Bulmahn, pushed through legislation aimed at removing the obstacles to innovation and reform that had been put in place by some of the state governments. Among its provisions was a new salary system for professors that allowed performance-related pay. Another important reform was the creation of fixed-term 'junior professorships' also paid for by the federal purse. These independent positions have allowed 1,145 young academics to set up their own research groups at a university. At the same time the  Habilitation  qualification required for university teachers stopped being mandatory, removing years from potential qualifying times. \n               Competitive streak \n             Since 2001, the federal government has succeeded in forcing research organizations over which it has more direct control, such as the Helmholtz Association (see  'Uncoiling the dead hand' ) to become more competitive. But despite the relaxation of rules on salaries,  Habilitation  and hiring of young academics, the universities were not obliged to change their ways, and at first most didn't \u2014 even when some state governments, such as those of Bavaria, Baden W\u00fcrttemberg and North Rhine-Westphalia, joined the reform bandwagon and passed local laws that encouraged their universities to modernize and to use their budgets flexibly. The Charit\u00e9, by contrast, has been one of the few universities to take up all opportunities for reform without hesitation. Unlike many of its counterparts in western Germany, the medical school was never burdened by inertia. Quite the opposite \u2014 it had been turned upside down and inside out by reunification. Alongside major restructuring, the Charit\u00e9 staff had their political backgrounds examined: those found to have collaborated with the Stasi to the detriment of their colleagues were dismissed. Many others were dismissed simply because of massive overstaffing, particularly among the technical-support staff. \n               Joining forces \n             Staff morale might have been low, but funding prospects were better at the Charit\u00e9 than at the two medical schools of the Free University in west Berlin, which lost their lavish federal subsidies in the cutbacks after reunification. In 2003, the cash-strapped Berlin state government decided to merge the city's medical schools under the umbrella of the Charit\u00e9, making it a university in its own right, but demanding an additional 33% budget cut to be phased in by 2010. The fresh if traumatic start forced on the Charit\u00e9 made it easier to make the necessary changes. Commercial activities, such as clinical-trial services, add 10% to the \u20ac200 million (US$270 million) budget it gets from Berlin's state government. And it is pulling in enough grant money to make up for the reduction in state funds. \u201cThe early years after reunification were psychologically difficult on both sides,\u201d recalls Detlev Ganten, head of the merged medical schools. \u201cBut I'm optimistic now \u2014 we are really starting to identify with the scientific spirit in Berlin before the Nazi period.\u201d To weaken the rigid academic hierarchy, whenever any academic staff left the Charit\u00e9, Ganten pooled their institutional money and support staff and used the shared resources competitively, for example to offer good starting packages for young academic staff, to improve career opportunities for women and to support top performing staff (see  'Flexibility at the Charit\u00e9' ). Already a third of faculty resources are shared out in these ways, and the beneficiaries have to reapply every year for their share. The pool will grow further at the end of this year when all packages agreed over the decades will be cancelled and renegotiated. Today, the Charit\u00e9 hosts 25 junior professorships, the highest number of any medical school. Unlike some other institutions, it has appointed to those positions people whom it truly intends to see tenured at the end of the process. Elsewhere, the reforms have been less impressive. By 2004, only two-thirds of the junior professors had gone on to get tenured positions, fewer than had been anticipated. Most universities didn't want to offer the few open faculty positions they had to junior professors (see  'The Emmy awards' ). \n               Attractive incentives \n             In 2005, the federal government adopted new tactics and decided it could best persuade universities to adopt reforms by dangling carrots \u2014 in the form of competitions that could be won only by universities able to attract the best students and faculty and to network effectively with their neighbours. The most influential of these competitions is the massive three-part Excellence Initiative \u2014 \u20ac1.9-billion over 5 years. For comparison, the country's science-funding agency, the DFG, has an annual budget of \u20ac1.3 billion. The initiative is now halfway through. Eighteen graduate schools and seventeen 'clusters of excellence' have been rewarded in the first funding round. The initiative can name up to five '\u00e9lite universities' from among the winners of the other categories. In the first round, three universities \u2014 two in Munich and one in Karlsruhe \u2014 earned the \u00e9lite label. This should mark the end of the pervasive myth that all German universities are equal. \u201cThe competition has challenged the dogma of equality, making the difference between universities apparent,\u201d says Peter Strohschneider, chair of the German Science Council in Cologne. \u201cA new paradigm in Germany science policy has been established and it has far-reaching effects for the whole university system.\u201d \u201cThe competition's really put new momentum into all the universities,\u201d says DFG president Matthias Kleiner, whose agency is helping to administer the initiative. \u201cFor one thing it got people interacting \u2014 faculties had previously behaved like little kingdoms but they had to cooperate for the Excellence Initiative.\u201d The Charit\u00e9 is already reaping the benefits of the more competitive structure it has created. It won an Excellence Initiative award for its graduate school in neuroscience \u2014 the Berlin School of Mind and Brain \u2014 worth \u20ac1million a year for five years, and has been short-listed in the second round, to be decided this autumn, for a neuroscience research cluster worth around \u20ac6.5 million per year for five years. This success adds to a prestigious award it won from the federal research ministry last year to create the Berlin-Brandenburg Center for Regenerative Therapies, which comprises 23 research groups and is funded with \u20ac45 million over four years. Also last year it was ranked top German medical school in impact and grant money by the independent Bertelsmann Foundation in G\u00fctersloh. \n               Slowly but surely \n             Some of the older Charit\u00e9 professors were not happy with the rapid pace of change, says Ganten. But the young faculty members are. \u201cI'd be a liar if I said the atmosphere here was quite the same as San Francisco and Berkeley but there is already a huge difference compared with when I studied here in the mid-1990s,\u201d says Charit\u00e9 neuroscientist Dietmar Schmitz. \u201cIt's getting there slowly.\u201d Schmitz returned to Germany in 2002 with both a junior professorship and an Emmy Noethar award, designed to attract back emigr\u00e9 scientists, and now has tenure and coordinates the Charit\u00e9 neuroscience cluster. Two main things attracted him back from the United States. \u201cThe Charit\u00e9 offer was tenure track with a good package,\u201d he says. And he was aware of a growing neuroscience buzz around the city and its academic community. The city had chosen neuroscience as a focus and built up appropriate infrastructure. Top neuroscientists had already started to arrive there from elsewhere. \u201cGerman universities are getting more professional,\u201d observes Christian Spahn, a structural biologist with tenure who also joined Charit\u00e9 as a junior professor in 2002 \u2014 despite the offer of a faculty position in the United States. He is now being courted by other German universities and knows he will get the facilities he needs to match his research ambitions. But Spahn doesn't think that the changes in Germany go far enough in trying to improve competition. \u201cWhat we really need now is for funding agencies to introduce [payment to cover] overheads,\u201d he says, \u201cso universities know that if they employ a good scientist he or she will bring in regular money \u2014 more regular than the occasional competition like the Excellence Initiative.\u201d As things gradually improve for German universities, the Charit\u00e9 is planning to upgrade its concrete high-rise to celebrate its rise in status. An \u20ac86-million restoration project has been launched to give it a new fa\u00e7ade and seven additional floors. The reasons are practical \u2014 the medical school needs more space. But the extension and expansion are not without their own propaganda purpose. The newly heightened building will go by the name of the Leuchtturm der Lebenswissenschaften Berlin \u2014 the Beacon of Life Sciences. See Editorial,  \n                     page 613 \n                   . \n                     Striving for excellence \n                   \n                     Grand ambition \n                   \n                     Helmholtz Society prepares itself for strategic reforms \n                   \n                     Tough measures bring a scarred science back to the world stage \n                   \n                     The Charit\u00e9 \n                   \n                     The Helmholtz Association \n                   \n                     The Junior Professorships programme \n                   \n                     The Emmy Noether awards \n                   Reprints and Permissions"},
{"file_id": "447635a", "url": "https://www.nature.com/articles/447635a", "year": 2007, "authors": [{"name": "Carl Gierstorfer"}], "parsed_as_year": "2006_or_before", "body": "Brought up in the Congo basin, Jonas Eriksson has worked through a war and battled poachers to help reveal the secrets of bonobo societies. Carl Gierstorfer reports. In 1998 in Lomako, a study site in the northwestern \u00c9quateur province of the Democratic Republic of Congo, a peace-loving primate closely related to the chimpanzee showed its darker side. A group of bonobos ( Pan paniscus ) was feeding when a male started to act aggressively towards a female with an infant \u2014 an unwelcome act in the typically female-dominated primates. Suddenly, all hell broke loose. The females banded together to attack the male, and beat him viciously for more than a half hour. The other males fled, and the wounded aggressor disappeared, never to be seen again. The event epitomizes a paradox in bonobo societies. DNA studies 1  done at the site have shown that the females aren't related, so cooperation would not benefit their kin directly. So why would females cooperate to exclude aggressive males? That is one thing that Gottfried Hohmann and Barbara Fruth from the Max Planck Institute (MPI) for Evolutionary Anthropology in Leipzig, Germany, had been studying at the Lomako site for eight years before the thrashing. But soon after the incident, violent raids from a different primate \u2014 human rebels from nearby Rwanda \u2014 evolved into a full-blown war that eventually reached Lomako and forced the researchers to leave. A year before the event, Jonas Eriksson (pictured), a former graduate student at the University of Uppsala in Sweden had joined the research team. The son of Swedish Baptist missionaries, Eriksson had spent his childhood in the pristine forests of the Salonga National Park in the central Congo basin and had gained a detailed knowledge of the region. While working on his degree, he learned about primate behaviour and field studies. The softly spoken 38-year-old says that he thought of his childhood hunting trips with bow and poison arrow and knew he could contribute something to the field. He was to prove instrumental in keeping the research going during the crisis. In 2000, Hohmann and Eriksson set out on a trip worthy of Henry Morton Stanley's epic exploration of the Congo basin in the 1870s. They combed the better part of the bonobo's range \u2014 around 200,000 square kilometres \u2014 by foot and bicycle, hunting for bonobo faeces, scooping them from the forest floor, sealing them in plastic bags and sending them to Leipzig to sequence their DNA. Although a dirty job, this way of collecting DNA samples puts as little stress on the bonobos as possible. Their analysis of 34 males from four distinct sites 2  showed that males from the same site had more similar Y chromosomes than did those from different sites, indicating that related males stay together, as they do in chimp societies. But mitochondrial DNA from these males, which is inherited down the female line, did not show such clustering, indicating that females tend to leave the group. Combined with their observation that females will work together to maintain their dominant status within their society, these findings further challenged the idea that genetic relatedness plays any part in female cooperation. Brenda Bradley, an evolutionary geneticist at the University of Cambridge, UK, says that many researchers realized that they were \u201coverestimating genetic relatedness when they see cooperation.\u201d Eriksson and colleagues' work helped to clarify that issue by providing data on long-range gene flow in the apes, she says. Chimpanzees ( Pan troglodytes ) have a similar kinship pattern but behave differently. Like the bonobos, female chimps in a group are generally unrelated. But unlike the bonobos, chimp societies tend to be dominated by the males. Whereas violent encounters are the norm in the chimp society, conflicts such as that observed at Lomako are rare in bonobos. Perturbations to bonobos' social order are generally defused through sexual acts, often in homoerotic encounters between females. \n               Secret for success \n             Eriksson and Hohmann had been hunting for more than just bonobo droppings on their trek. They had also been looking for a new study site and settled on the southern reaches of Salonga National Park. Eriksson's mastery of the Congolese language and culture were integral to securing permission from villagers to use the site. \u201cHe has a strong emotional attachment to Congo and the Congolese people,\u201d Hohmann says. Fruth says that she admires Eriksson's ability to penetrate the Congolese culture. But his intimate link also has its downsides: Fruth says that Eriksson's 'Congolese' way of approaching things means that he refuses the pace of the western world and prefers a more laid-back lifestyle. \u201cHe has to be pushed to bring things to an end,\u201d he says. Nevertheless, the team managed to secure the study site in 2000, and work could resume. For Hohmann, Fruth and Eriksson, a new opportunity to explore the bonobo paradox began to take shape. The researchers think that the coopperation between unrelated females to keep aggressive males in check was to protect against infanticide, which is common in male chimps \u2014 bonobos closest relatives. Moreover, the females may pool their efforts to collect high-value resources such as meat. Hohmann and Fruth have found that at Salonga, meat consumption is much more pronounced than previously thought in the normally fruit-eating apes. The prey is caught by females, possibly even in groups, and males rarely share in their spoils \u2014 a striking contrast to chimpanzees. \u201cThey are just sitting there, begging for meat, or even guarding the kids only to score well with the females,\u201d Fruth says. The lack of male aggression could be down to the plentiful supply of good-quality resources. Meat might be a delicacy enjoyed only by the females, but fruit is abundant and sex is readily available, reducing the need for competition. But while Eriksson was in Leipzig sequencing the bonobo droppings, a new problem erupted. The bitter war that shook the country and cost an estimated four million human lives had ended, but leftover weapons were being put to use in the bush-meat trade. \u201cSuddenly, in 2005, I got these reports from my friends in Congo that the poachers were coming closer and closer to that area that's really fond to me,\u201d says Eriksson. For more than two years poachers had been moving steadily into the Salonga National Park, mainly targeting the abundant and easy to kill red colobus monkey. \u201cThey pick them off like fruit,\u201d Eriksson says. As colobus numbers dwindle, the bonobos are more likely to be targeted. \n               Trading places \n             So, with support from his mentors, Eriksson abandoned his research to protect the site. He convinced local park rangers and villagers to help him chase out the poachers, armed with automatic weapons. \u201cI think the combination of being foreign, white-skinned, but speaking to them in a way that penetrates their culture and language is the key,\u201d Eriksson says. His approach has been effective in keeping the poachers out of the study site, at least for now. Having put down his pipette for an AK-47, Eriksson says that he's determined to return to science, but not necessarily in the same role. \u201cI probably won't spend too much more time in a lab; it's a waste of time. There are other people who are much more skilled than me.\u201d Hohmann chides that Eriksson's \u201cacademic ambitions are easily outrun by his liking for adventures\u201d. Nevertheless, Salonga is still in danger and the conflict is bound to escalate as the poachers take greater risks. Eriksson says that he has already received death threats. Having seen their Lomako site collapse, the team is determined to hold on to the one in Salonga. Too many questions remain about how bonobos manage to avoid violent conflicts. Ironically, saving the peaceful bonobos from the poachers may require more aggressive displays. Eriksson says: \u201cI did not spend years studying to run around in the forest with a Kalashnikov and my finger on the trigger. But emotionally, it is very easy to convince myself that these steps are necessary. I have to try to do something.\u201d A  video  of Eriksson discussing his adventures can be seen in this feature on the Nature website. \n                     Bonobos face extinction \n                   \n                     Hunting for hope \n                   \n                     What the chimp means to me \n                   \n                     www.nature.com/news/specials/chimpgenome/index.html \n                   \n                     blogs.nature.com/news/blog/2006/08/abs_chimps_in_a_world_of_their.html \n                   \n                     www.nature.com/nature/journal/v443/n7114/full/443915a.html \n                   \n                     www.nature.com/nature/journal/v436/n7053/full/436916a.html \n                   \n                     Max Planck Institute Department of Primatology \n                   \n                     Bonobo Project News \n                   \n                     Bonobo Fact Sheet from the World Wildlife Federation \n                   Reprints and Permissions"},
{"file_id": "447026a", "url": "https://www.nature.com/articles/447026a", "year": 2007, "authors": [{"name": "Michael Cherry"}], "parsed_as_year": "2006_or_before", "body": "Two institutes on opposite sides of South Africa are intent on tackling HIV. But they are separated by more than geographical distance, finds Michael Cherry. Sandwiched between the green Hluhluwe-Umfolozi game reserve and the national motorway running north from Durban is a 435-square-kilometre dustbowl called Umkhanyakude. In the apartheid era it was designated a part of the Zulu homeland, and today each of the small plots is scattered with thatched huts and is home to a family. Umkhanyakude has one of the highest levels of HIV prevalence in South Africa, peaking at 51% for women aged 25\u201329, compared with about 30% for pregnant women nationally 1 . These data were collected by researchers at the Africa Centre for Health and Population Studies, a ten-year-old institute housed in an incongruously modern building in the centre of this impoverished region. At the southwestern tip of the African continent, 2,000 kilometres away, lies fair-weathered and affluent Cape Town \u2014 another city suffering from the HIV epidemic but home to a rather different research centre. In Cape Town, HIV has spawned a higher incidence of tuberculosis (TB) than in any other city in the world. This disease is the subject of study by researchers at the five-year-old Institute of Infectious Disease and Molecular Medicine (IIDMM), part of the University of Cape Town. Last year, a team there sequenced DNA from the preserved organs of TB victims and showed that the prevalence of a particularly virulent strain originating from southeast Asia has exploded since 1996. This strain is now causing 20% of TB deaths, perhaps because it acquires drug resistance particularly quickly. Both the Africa Centre and the IIDMM are at the heart of South Africa's battle against HIV and both have received significant funding from Britain's Wellcome Trust medical charity. \u201cClearly there is some overlap between the two institutions,\u201d says Jimmy Whitworth, head of international programmes at Wellcome. But the two also have their differences: the Africa Centre focuses on understanding the health of the local population and the best interventions to help this community, whereas researchers at the IIDMM are interested in understanding HIV on a molecular level and turning that knowledge into clinical practice. And despite the institutes' overlapping remits and the scale of the HIV crisis in South Africa, there is very little collaboration between them. No one is prepared to say that this remote relationship is holding back research into the country's HIV epidemic. But it is \u201ca cause for regret\u201d, says Robert Wilkinson, a senior fellow at the IIDMM who led the TB study, and it is an issue that he and others want to address. \u201cI don't think that the lack of collaboration is intentional,\u201d says IIDMM director Greg Hussey. \u201cThe reality is that we tend to collaborate with overseas researchers because, with a few exceptions, they provide our funding.\u201d The Africa Centre was founded by the Wellcome Trust in 1997 and is now part of the University of KwaZulu-Natal (UKZN). About 80% of the institute's running costs \u2014 currently US$5 million \u2014 are covered by a grant from the trust. The centre faces a difficult balance between the demands of funders \u2014 who want high-quality publications in order to justify their support \u2014 and the needs and expectations of the poor rural community in which it is based. \n               Location, location, location \n             The Africa Centre's location has proved a major barrier to recruiting and retaining senior staff, particularly skilled South Africans who tend to be more interested in working in cities or abroad. Another problem in this regard is that the centre offers no security of tenure; like other Wellcome-funded institutes, all of the staff are on three- to five-year contracts \u2014 centre director Marie-Louise Newell is herself on a five-year secondment from University College London in the United Kingdom. The IIDMM, by contrast, has proved to be a magnet for drawing South Africans abroad back to the country and for attracting foreign talent. The institute was established in 2002 as a virtual entity comprising researchers from departments in the University of Cape Town's science and medical faculties. Two years ago, a new building was completed to house the institute. Its annual budget is approaching US$20 million, about 40% of which is funded by the university and the rest of which comes from research agencies. The Wellcome Trust has so far supplied more than US$12 million, mostly by funding senior fellowships. Although these are for only five years, the university has agreed to convert them into tenured appointments at their termination, subject to the fellowship recipients' good performance. The combination of the IIDMM's location, its secure and generous funding, world-class facilities and access to HIV patients all serve to attract good researchers. The institute's mission is to translate molecular laboratory research into the clinic and it is having some of its greatest impact in African initiatives to develop vaccines against TB and HIV. Virologist Carolyn Williamson, now a principal investigator at the IIDMM, was central to the initial elucidation of the virus's diversity in South Africa. In 1999, her group showed that the country's epidemic is dominated by a strain of HIV known as subtype C, whereas in the developed world subtype B is dominant 2 . This suggested that vaccines developed against foreign strains might not work well in South Africa. \n               The big picture \n             One of the Africa Centre's main achievements, meanwhile, are its HIV surveillance data. These represent one of the few precise estimates of HIV prevalence in South Africa, because they are collected by visiting and testing everyone in a household rather than, more typically, by testing pregnant women who attend antenatal clinics. In one study, the researchers compared their surveillance figures with those from an antenatal survey in the same region and confirmed that HIV prevalence was similar across the data sets in almost all age groups. But according to the surveillance figures, women aged 15\u201319 had an HIV prevalence that was only half that suggested by the antenatal survey. This disparity has been attributed to the fact that some of the cohort are not yet sexually active 3 . Another widely acclaimed accomplishment of the centre is a study on HIV transmission during breast-feeding. Previous studies estimated that breast-feeding is associated with a 10\u201320% probability of HIV transmission, so the World Health Organization (WHO) formerly recommended feeding infants with formula milk as a first choice, where it was safe to do so. But as in much of Africa, most women in Umkhanyakude lack access to clean water and cannot safely avoid breast-feeding, although many supplement it with formula or solid food. Previous studies had not distinguished between transmission risks from exclusive breast-feeding and this type of mixed feeding. The centre's study showed that, in this rural setting, exclusive breast-feeding is a much better strategy. Babies who received breast milk and formula were nearly twice as likely to acquire HIV \u2014 and when mixed with solids, the risk was almost 11 times higher 4 . One reason for this is that infant formula and solid food can cause microscopic damage to cells lining a baby's immature gut, helping HIV to enter the body, says author Nigel Rollins of UKZN. The study helped prompt the WHO to refine its infant feeding guidelines earlier this year, so that exclusive breast-feeding is the default option where feeding with infant formula alone is not possible or safe. While the Africa Centre has focused on means of HIV transmission in the local community, Williamson's group at the IIDMM set to work developing vaccines against the dominant subtype C. The team surveyed HIV from newly infected patients and selected genes that best represented South African subtype-C strains 5 . The idea is that individuals immunized with vaccines containing genes from dominant local viruses are more likely to recognize and launch an immune response when infected by one of them. The IIDMM is now carrying out one arm of a clinical trial in Cape Town of an HIV vaccine that includes these genes, funded by the International AIDS Vaccine Initiative. The results of this trial should be available by the end of the year. Building on this work, the HIV vaccine-development group at the institute, headed by Anna-Lise Williamson, recently developed two new, refined vaccines incorporating additional subtype-C genes and modifications to make them more potent 6 . These will go into clinical trials next year \u2014 the first HIV vaccines developed in Africa to enter trials. \n               Dual aspect \n             The Africa Centre presents a huge opportunity to do meaningful research in an HIV-stricken area, says Newell, and some researchers are hopeful that the relationship between the two institutes could yet blossom. Populations studied by the Africa Centre could prove an ideal testing ground for interventions developed at the IIDMM. \u201cAn obvious area of collaboration is the clinical testing at the Africa Centre of candidate vaccines that were developed at the IIDMM,\u201d says Lynn Morris of the National Institute for Communicable Diseases in Johannesburg. And, says Wilkinson, \u201cit might well be possible to develop concrete plans for collaboration in the future.\u201d As yet, there are no firm plans for such a partnership. But perhaps in time the 2,000 kilometres between Umkhanyakude and Cape Town will be bridged. See Editorial,  \n                     page 1 \n                   . \n                     AIDS: On the Brink \n                   \n                     South Africa takes steps to tackle HIV \n                   \n                     AIDS in Africa: A question of trust \n                   \n                     Extreme TB strain threatens HIV victims worldwide \n                   \n                     Hunt for AIDS vaccine tackles genomes \n                   \n                     HIV special \n                   \n                     Africa Centre \n                   \n                     IIDMM \n                   \n                     UNAIDS \n                   Reprints and Permissions"},
{"file_id": "447022a", "url": "https://www.nature.com/articles/447022a", "year": 2007, "authors": [{"name": "Haim Watzman"}], "parsed_as_year": "2006_or_before", "body": "Archaeologists are unearthing remarkable finds in Jerusalem. But the digs have sparked an argument over who should run the site and present the results to the public. Haim Watzman reports. In the ancient heart of Jerusalem, the one-kilometre-square walled area known as the Old City contains some of the most sacred sites for three religions, and as such it is no stranger to religious and political conflict. Yet some of the latest disputes are centred just outside the massive walls, in a Palestinian village known as Silwan that is now a neighbourhood within greater Jerusalem. Here, archaeologists are battling over the interpretation of major ongoing excavations. The site, known to Israelis as the City of David, lies under part of Silwan and is operated by a Jewish settler organization. Some Israeli archaeologists are openly critical of this organization's aims, while many Palestinians claim that the digs are damaging their property. At the heart of the debate is the question of who should be allowed to control the site, oversee excavations, and present the findings to the public. The part of Silwan that lies on top of the site contains some four dozen homes of Palestinian Arabs, and 20 homes of Israeli settlers. The houses stand on the ruins of centuries of Muslim and Byzantine habitation, which in turn cover the Jerusalem that was sacked and burned by the Babylonians in 586  BC , and before that besieged by the Assyrians in 701  BC . Farther down lies evidence that might help confirm \u2014 or refute \u2014 the Bible's account of a prosperous united Israelite kingdom in the tenth century  BC , ruled by the kings David and Solomon. And deeper still are the remains of an even more ancient city that prospered around 1800\u20131700  BC , during the Middle Bronze Age. So it's hardly surprising that archaeologists have been excavating here for the past 140 years. Digs are now under way at several sites in Silwan \u2014 both between and beneath the homes of the village's inhabitants. Each month, busloads of Israeli schoolchildren and Israeli and foreign tourists flock to see the unearthed finds. But to some, the way in which these finds are presented and explained to the visitors is a major cause for concern. The visitors' centre at the site is run by a non-profit foundation called Ir David (which means 'City of David' in Hebrew). The group was established in 1986 to promote excavation and tourist development of the site, and is popularly known by its Hebrew acronym, Elad. The foundation often helps to organize funds and support for archaeological excavations at the site. But Elad also has another goal: promoting Jewish settlement in the village of Silwan. To that end, it has reclaimed formerly Jewish houses, evicting the Palestinian residents and replacing them with Jews, and has purchased Palestinian houses \u2014 sometimes using means that its Palestinian and Israeli critics charge are legally questionable. Renovation of these homes and development work for residential and tourist purposes have necessitated salvage excavations that have inconvenienced the Palestinian residents and sometimes caused damage to their property. Elad's members and supporters are nearly all nationalist Orthodox Jews who believe that Jewish settlement in the territories captured by Israel in the Six-Day War of 1967 is God's will and a precondition for the arrival of the Messiah. Although archaeologists digging at the site say that Elad has not pressed political interpretations on their work, some of them have raised concerns that the organization's religious and political goals are incompatible with the role of running a national park containing an important archaeological site. Indeed, they charge that Elad is using its position to promote a distorted version of history \u2014 merging myth and legend with archaeological fact. This apparent conflict of interest has prompted a group of archaeologists to initiate legal moves to get the Israeli government to take control of the site from Elad. \u201cThey are taking over public land,\u201d says Rafi Greenberg, an archaeologist at Tel Aviv University who is one of the organizers of the initiative. But Ronny Reich, an archaeologist at the University of Haifa who has excavated at the City of David, notes that Elad does not try to dictate who can work there. \u201cI don't think anyone can tell a group of people that wants to participate in the excavations that they can't,\u201d he says. Elad's spokesman refused  Nature 's request to interview a representative of the organization and asked that questions be submitted in writing. When they were, the spokesman failed to provide answers, despite repeated promises to do so. \n               Fact or fiction? \n             One way to experience the Elad view of the City of David is to tour the site with an Elad-trained guide. It is possible to visit the excavations on your own or with a guide you've brought yourself. But the default option for tourists and school groups is to hear the narrative that asserts the Jewish claim and historical connection to the site, say Greenberg and his colleagues. There is some truth to these claims, as a  Nature  visit to the site suggests. The tour guide provided by Elad was well-spoken and knowledgeable, but mixed myth and fact in her presentation. For example, she asserted that the reason David chose the site for his capital is that it lies just below the Temple Mount, which is identical to Mount Moriah, the site where, according to the Bible, Abraham took his son Isaac to offer him as a sacrifice to God. Although the identification of the Temple Mount with Mount Moriah is well-established in Jewish tradition, there is no archaeological evidence for Abraham's presence on the site \u2014 or indeed for the existence of Abraham and Isaac. In fact, a handful of archaeologists go so far as to say that David and Solomon may also be largely mythical characters. This view is rejected by most experts on the period \u2014 they tend to agree that it is likely the two ancient rulers did reign in Jerusalem. But many scholars argue that the evidence discovered so far \u2014 both at the City of David and at other sites in the region \u2014 indicates that the biblical description of the extent and wealth of their kingdoms is exaggerated. Furthermore, the Elad guide made no mention of Byzantine and Muslim settlement, giving the impression that the site is solely a Jewish one. But this may not be too surprising, given that she had only about an hour to explain the site and that her audience consisted of Israeli Jews, including a number of easily bored children and teenagers. Under those conditions, the presentation of any archaeological site would no doubt be geared more towards storytelling than to the detailed technical facts of what the archaeologists have found and how they interpret the evidence. Few dispute that this complex site has yielded some major discoveries in recent years. Near the top of the hill in Silwan is an ongoing excavation led by Eilat Mazar of the Hebrew University and sponsored by Elad, the Shalem Center (a Jerusalem-based research institute), the Israel Antiquities Authority and the Society for the Study of the Land of Israel and Its Antiquities. In early 2005, Mazar's team uncovered a large stone structure, and dated pottery found inside the structure to early in the Iron Age IIa period (around 1000  BC ), which corresponds to the time of King David. So far, several large rooms have been uncovered, as well as walls two to three metres wide. In March, Mazar announced the discovery of another 20-metre section of the structure's outer wall, further evidence of its huge size. She believes that the massive nature of the structure indicates that it must have been an important public building. And because it is located close to the Temple Mount and at a commanding position in the city, she believes that it is the palace that, according to the Bible, David built after conquering Jerusalem and making it his capital in the early tenth century  BC 1 . \n               Age concerns \n             But some archaeologists dispute her dating and interpretation. Israel Finkelstein of Tel Aviv University, for example, is a leading proponent of the view that many archaeological remains throughout Israel dated to the early tenth century  BC  \u2014 the time of David and Solomon \u2014 are actually nearly a century younger. On the basis of his later dating of the artefacts in question, and of the lack of references to a large Israeli kingdom centred in Jerusalem in the records of near-Eastern cultures, he argues that the rulers of Jerusalem were not significant players on the international stage until much later. \u201cMazar has done fine and important work,\u201d he says, \u201cbut interpretation is another matter. The structure she found can't be dated unambiguously.\u201d Finkelstein thinks that the large stone structure instead dates to the ninth century  BC  \u2014 the period in which Omri and his son Ahab organized the northern Israelite tribes into a powerful kingdom. He argues that the influence of the more powerful kingdom to the north, to which the rulers of Jerusalem were probably vassals, may have been the spur for major construction. Or perhaps, he suggests, the collapse of Omri's dynasty in the mid-eighth century  BC  may have created a power vacuum that the kings of Jerusalem were able to fill. Another collection of artefacts, found in 2005 near the Gihon spring at the bottom of the Silwan hill, also touches on this debate. Reich and Eli Shukron, who conducted salvage excavations for the Israel Antiquities Authority around the spring, discovered a large number of bullae \u2014 clay seals placed on ancient letters. The researchers dated the bullae to the late ninth or early eighth century  BC . The figures on some of the seals are Phoenician, a sign that Jerusalem was trading with the coast at the time. Further evidence is provided by some 10,000 fish bones found with the bullae \u2014 around 90% of which come from saltwater fish. \n               Tunnel vision \n             Ironically, although the Gihon spring was long the city's sole source of water and is mentioned in the Bible, it attracted relatively little archaeological attention until Reich and Shukron began digging there. Reich thought it so unpromising that he initially resisted being assigned to dig there by the antiquities authority, his employer at the time. \u201cEleven or twelve years ago Elad received a permit from the municipality to build a visitors' centre by the spring,\u201d says Reich. \u201cThe head of the antiquities authority, Amir Drori, told them that, by law, they had to fund a salvage excavation at the site before building.\u201d Reich and Shukron's work on the spring allowed them to work out that a tunnel dug by King Hezekiah in the eighth century  BC  to bring the water safely into the ancient city was used in a different way from that previously supposed by archaeologists. In addition, they uncovered a pair of massive towers dating from the eighteenth and seventeenth centuries  BC  \u2014 demonstrating that the city was large and wealthy at that time 2 , 3 . The bullae came from a new salvage excavation near the spring, this one initiated by the need to unblock the sewerage line that dumps the Old City's waste in the Kidron riverbed. In 2005, the pair made another major find \u2014 the Pool of Siloam, mentioned in later Jewish sources and the New Testament. This excavation ran up against the site's political and legal complexities. The excavation goes into the hill and under a mosque and a kindergarten, whose walls have cracked. The Palestinians say that the excavations were carried out without regard for their property and caused the damage. Yigael Ben-Ari, district manager for the Israel Nature and National Parks Protection Authority's central region, says that the damage is unrelated and rejects the charge of indifference to the Palestinians and their property. \u201cWe agreed in advance to take care of any damage that the sewerage work and excavations would cause,\u201d he says. \u201cAn engineer we brought to examine the damage said there is no connection between our work and the cracks in the mosque. The work has turned up a find of worldwide importance.\u201d Ben-Ari says he and his staff have met with the Palestinian residents of Silwan and will help to repair the damage. \u201cWe can't run any site if the residents don't want us,\u201d he says. Meanwhile, Greenberg and his colleagues maintain that the extent of the salvage excavations has been dictated in part by Elad's desire to create tourist attractions to present to the public. They argue that the site should be run by the parks authority or some other national body that is subject to public oversight and does not have a political or religious agenda. In fact, Greenberg's group claims, such a transfer of authority was supposed to happen in 1998, when a group of 33 archaeologists petitioned Israel's Supreme Court to cancel the permit that allowed Elad to operate the site. The suit was withdrawn after the state attorney told the court that Elad's permit had been cancelled. But in 2002, under legal circumstances that are still unclear, Elad resumed managing the park. This has spurred Greenberg and his group to prepare to renew the 1998 suit. According to Greenberg, the petition has been delayed for technical reasons but will be filed with the Supreme Court in the near future. For his part, Reich says he disagrees with Elad's politics and belongs to the group of archaeologists who think that the evidence on the ground fails to support much of the Bible's narrative. But he has not joined Greenberg's efforts to remove Elad from involvement. The digs and the resulting tourism have provided employment for the Arab residents, he notes. Unlike many other archaeologists, Reich has not used student volunteers, and instead employed 20\u201330 local Palestinian residents \u2014 whose salaries were paid by Elad. \u201cIt's the residents' luck that they happen to live here,\u201d he says. \u201cSo they should live off the site as well.\u201d The excavations could not be accomplished without the money and sponsors Elad brings in, Reich maintains. He says that the group does not impose its politics or religion on him or other archaeologists. Furthermore, Elad has been willing to pay for the kind of unexciting, but crucially important, technical work for which it is difficult to find national and academic funding. For example, Elad has helped Reich to find a private funder to pay a draftsman to copy the marks on the bullae, so that they can be analysed and compared. Greenberg and his associates are still in the process of preparing their court petition. In the meantime, the conflict between science and politics at this most sensitive of archaeological sites has not affected either its interest to scholars or its attraction for tourists. Excavations continue in the City of David and the archaeological park is filled with Israeli and foreign visitors. And whatever the final decision on who runs the site, that is likely to continue. \n                     Antiquities fraud: Reality check \n                   \n                     Automatic archaeology \n                   \n                     Ir David Foundation \n                   \n                     The Shalem Center \n                   \n                     Israel Antiquities Authority \n                   Reprints and Permissions"},
{"file_id": "447018a", "url": "https://www.nature.com/articles/447018a", "year": 2007, "authors": [{"name": "Ichiko Fuyuno"}], "parsed_as_year": "2006_or_before", "body": "Neuroscientist Ryuta Kawashima promotes the idea that computer games can boost the ageing brain \u2014 but others in the field remain sceptical. Ichiko Fuyuno investigates. On a chilly, rainy day last November, Ikuyo Narawa climbed on a tour bus in Tokyo in the hope of revitalizing her brain. She knew that she drank too much and never exercised, and she thought that her memory was weakening. So, along with 40 companions, she travelled to the scenic Tateshina highland in Nagano Prefecture, where she tried hiking and handicrafts, ate organic meals and bathed in hot springs. At the start of the tour, the 37-year-old dental assistant took several simple computer tests, such as clicking a mouse as soon as a lamp lit red, and was surprised to be told that her 'brain age' was 61 years. After two days of activities, she took the test again and found that her result was, disappointingly, unchanged. But on her fifth time, attempting the test at home, her brain age dropped to 20. \u201cI was very relieved,\u201d she says. The brain-train tour, organized by the region's local chamber of commerce and a travel agency, is the latest example of a brain-exercise boom that is consuming Japan and spreading across the globe. In Japan, TV quizzes aimed at boosting the brain are popular, and fashion magazines run rejuvenating advice for the mind alongside that for the skin. And throughout the world, a proliferation of books, websites and software is available claiming to preserve our mental capabilities. At the centre of this craze is a ragingly popular video game developed by Japanese video game maker Nintendo and based on the research of Ryuta Kawashima (pictured above), a neuroscientist who specializes in brain imaging at Tohoku University in Sendai. Players of the game \u2014 called 'Brain Age' in the United States and 'Dr Kawashima's Brain Training' in Europe \u2014 use a console to complete simple tasks such as reading aloud, multiplication and memorizing words. In promotional material for the game, Kawashima says that daily training on these activities can \u201chelp to prevent a decrease in brain function\u201d. He promotes the idea that these types of activity enhance blood flow to the brain's prefrontal cortex \u2014 the region of the brain that regulates aspects of memory, reasoning and some of the other complex behaviours that deteriorate with age. Many neuroscientists and gerontologists are sceptical of the claims made for Brain Age and similar games, saying that there is scant evidence that any type of brain exercise can halt mental ageing. Kawashima has published a paper 1  suggesting that the type of mental activities in the game can help elderly people with dementia, but critics say that these findings do not necessarily apply to healthy adults. A player's score could well improve with practice, but whether that translates into an improvement in other mental tasks or everyday life skills remains unknown. \u201cThat's what I think is a big question,\u201d says Timothy Salthouse, a researcher of cognitive ageing at the University of Virginia, Charlottesville. \u201cI don't think there is scientific evidence that the improvements after mental exercise can be generalized beyond what you have trained on.\u201d In a rapidly ageing population, people are understandably keen to live a long life with their mental capacities intact. Neuroscientists think that from as young as 30, changes in the chemistry and connections between nerve cells cause some cognitive functions to decline. Brain-training programmes typically claim to slow this mental descent or even to recover some lost ground. Some studies have shown that physical exercise can prevent some of the brain's deterioration (see  'Body and mind' ), and the idea that simple mental activities could do the same is enticing. Brain training has already earned Kawashima celebrity status in Japan. He studied to be a physician but, after learning about imaging techniques at Sweden's Karolinska Institute in Stockholm, he became one of the first to pursue brain-imaging research in Japan. Using techniques such as functional magnetic resonance imaging (fMRI), he showed that blood flow to the brain's prefrontal cortex increases when people are calculating sums quickly or reading aloud. He has published more than 100 drill books with such exercises for adults. \n               Mental work-outs \n             In 2001, Kawashima won a coveted \u00a560-million (US$500,000) grant from the Japan Science and Technology Agency in Saitama to study whether mental exercises improve cognitive function in elderly people. He and his team investigated 32 individuals who had been diagnosed with Alzheimer's dementia in Eiju-no-sato nursing home, Fukuoka. Over six months, half of the people were asked to do simple calculations and language tests, such as arithmetic division or reading fairy tales aloud; the other half received no training. Kawashima's team measured their cognitive status before and after the training with two widely used tests to diagnose dementia \u2014 the Mini-Mental State Examination (MMSE) and the Frontal Assessment Battery (FAB). The tests included questions such as \u201cWhat day of the week is it?\u201d and \u201cWhat do bananas and oranges have in common?\u201d People in the training group improved their FAB score, maintained their MMSE score and became more communicative and independent than they had been before the training. The control group, however, showed no change in FAB score and a decline in the MMSE score 1 . In addition, a 75-year-old man in the treatment group regained his ability to go to the toilet by himself, and a 77-year-old woman who used to come to the learning centre in her pajamas started to arrive fully dressed. Kawashima and the other authors acknowledge the study's limitations. They could not tell whether the cognitive improvements were attributable to the training itself, or to the extra attention and social interaction the individuals received from the experimenters and nursing staff. But Kawashima thinks that the method may have stimulated the prefrontal cortex, causing improvements in general cognitive functions such as communication. He called the method 'learning therapy', and it has now been introduced at 300 nursing homes across Japan at a monthly cost of \u00a51,575 per user. Kawashima has conducted two more unpublished studies of the same training on elderly people with and without dementia and says that the training improved their MMSE scores. \n               Adult entertainment \n             At the end of 2004, Kawashima was contacted by Nintendo. The company thought that his drill books could be turned into a stimulating game that would attract those adults who usually shy away from conventional video games. Working with Nintendo to develop the game, Kawashima says that he studied 120 Japanese people aged from their 20s to their 70s. He used a technique called optical coherence tomography to examine blood flow in their brain while they tackled dozens of exercises, such as adding up numbers and memorizing Chinese characters. The tomography technique is not as accurate as fMRI, but Kawashima says that he used it because it was quicker and easier for the volunteers. He and his research team selected 15 tasks that boosted blood flow to the prefrontal cortex and calculated an average score for each age, which they used as a basis for the Brain Age game. The video game debuted in Japan in May 2005. Sales of the \u00a52,800 product, together with its sequel, have reached more than 3 million units in Japan \u2014 in a market in which 1 million units is considered a hit. The game was introduced in the United States, Europe and Australia in 2006 and in South Korea earlier this year. Kawashima says that he uses all of the royalties from Nintendo and other companies \u2014 more than \u00a5400 million in 2006 \u2014 on his research, including construction of a new laboratory near his office. \n               Lost in translation \n             Other neuroscientists, many of whom say that they respect Kawashima's work, express discomfort with his Brain Age concept. Although the game could work in principle, they question whether simple mathematics and reading tests of the type in Brain Age are any more effective than other cognitive tasks at boosting blood flow to the prefrontal cortex. And they say that there is little evidence that a brief boost in blood flow would improve brain processes or the everyday skills that decline in normal ageing adults. \u201cI see no reason to believe that Brain Age gains will transfer to other kinds of cognition, or to real-world function,\u201d says Michael Marsiske, who also studies interventions to improve cognitive performance, at the University of Florida, Gainesville. Earlier this year, 41-year-old Marsiske played the game himself. He says the game was entertaining and fun and that his brain age dropped from 78 to 26 after three days. But much of the improvement could be attributed to practice, he says. \u201cUsers may get the illusion of huge gains when starting with Brain Age, but these have more to do with learning the device than actual mental improvements.\u201d Marsiske and other neuroscientists say that they would like to see well-controlled, published studies to show that Brain Age benefits those who are buying it. \u201cPeople are paying for it. They deserve to know whether it really works,\u201d says Dorothy Bishop, a neuroscientist at the University of Oxford, UK. Kawashima is indifferent to this criticism and says that he does not intend to conduct more detailed studies on the game's effects because his earlier study on those with dementia, and his additional unpublished work, demonstrated that his learning therapy works. \u201cI am sure it works because all the data we obtained so far have been showing positive results,\u201d he says. \u201cThe most important thing is that behaviours of older people have got better with our training method.\u201d Yasuhiro Minagawa, a spokesman for Nintendo, says that the company is not in a position to comment on the scientific evidence behind the game, and the company is confident that Brain Age provides high-quality entertainment. Other scientists are eager to test more rigorously whether mental work-outs can enhance the ageing brain. \u201cA lot of people are interested in this area and are working hard to see whether or not this kind of short-term training has a long-term benefit,\u201d says Marilyn Albert, an expert on Alzheimer's disease at Johns Hopkins School of Medicine in Baltimore, Maryland. A firm called Posit Science in San Francisco, California, is conducting research into Brain Fitness Program, its best-selling computer-based exercise (costing from $395), in which users distinguish similar sounds, reconstruct sequences of words and do other mental exercises. A team led by Michael Merzenich, the company's chief scientific officer and a researcher on cortical plasticity at the University of California, San Francisco, assigned 182 participants aged over 60 to one of three groups. The first group performed brain exercises on a computer, the second watched and listened to educational DVDs on their computers and the third had no computer time. After the 8\u201310 weeks of the study, the training group had improved at the tasks in the program and in other standardized assessments of memory it had not been trained in 2 , whereas those in the control groups showed no improvements. Perhaps the best study on mental training so far, Albert says, was that published by a team including Marsiske late last year 3 . It suggested that healthy elderly people given a cognitive work-out can gain long-lasting mental benefits, which may have transferred into improvements in their daily living activities. More than 2,800 mentally healthy adults aged over 65 were randomly assigned to receive ten hour-long training sessions over five weeks or to a control group that received no training. The team found that the training groups had improved cognitive ability for the specific tasks they had trained on, and that the benefit lasted for as long as five years. And compared with the untrained controls, participants in one of the training groups reported that they had less difficulty in performing routine tasks, such as preparing meals and using the telephone. Many neuroscientists are optimistic that brain training will have proven benefits. And they see no more harm in doing computer exercises than in completing brain teasers such as crosswords or sudoku, and can at least give a warm glow of accomplishment. At the Eiju-no-sato nursing home, those with dementia are given basic exercises in reading and calculation for 15\u201320 minutes a day. Head of the nursing home Ritsumi Yamasaki says that this schedule has improved patients' behaviour more than therapies she had tried before, such as gardening and karaoke, and it helps their interaction with carers. \u201cI tell people to do it if it's enjoyable,\u201d agrees Salthouse. \u201cThere's little evidence that it's damaging or harmful, and we may eventually find out there are some benefits.\u201d \n                     Social sciences: Life's a game \n                   \n                     Computer games could save your brain \n                   \n                     Bionic brains become a reality \n                   \n                     Science in the movies: From microscope to multiplex - An MRI scanner darkly \n                   \n                     Brain and behavior news \n                   \n                     Nintendo \n                   Reprints and Permissions"},
{"file_id": "447132a", "url": "https://www.nature.com/articles/447132a", "year": 2007, "authors": [{"name": "Oliver Morton"}], "parsed_as_year": "2006_or_before", "body": "Long marginalized as a dubious idea, altering the climate through 'geoengineering' has staged something of a comeback. Oliver Morton reports. In the first week of June 1991, Michael MacCracken, a climate physicist from Lawrence Livermore National Laboratory in California, was attending a small conference in Palm Coast, Florida, to discuss technological approaches to cooling the Earth. There he gave a paper that looked at various approaches that had been suggested in the decades before, from burying carbon dioxide underground to increasing the proportion of sunlight that bounces off hazes in the atmosphere and back into space. At the same time half a world away, something like 20 million tonnes of sulphur dioxide dissolved in searingly hot magma a few kilometres underneath the Philippines was preparing to show him and his audience how it's done. The day after the conference ended, the first of that magma emerged from the crater of Mount Pinatubo. After a week of intensifying eruptions, on 15 June the volcano exploded cataclysmically, blowing a plume of molten rock, ash and gas as high as 40 kilometres into the atmosphere. Much of the plume's sulphur dioxide ended up in a cloud of tiny particles spread around the stratosphere, more than 20 kilometres up, and there it remained for years. The thin global veil of sulphates made the planet's sunlight more diffuse, its skies a touch whiter, its sunsets more spectacular \u2014 and its climate a little cooler. The Pinatubo particles cooled the Earth more or less exactly in line with the figures that MacCracken had offered at the meeting for the effects of 'artificial volcanoes'\u2014 any technology for injecting sulphur high into the atmosphere. Had there not been a simultaneous El Ni\u00f1o, 1992 would have been 0.7 degrees cooler, worldwide, than 1991. And this demonstration of cooling power took place at a crucial time. The first report of the Intergovernmental Panel on Climate Change (IPCC) warning of greenhouse warming came out the year before Pinatubo; the UN Framework Convention on Climate Change was opened to signatures while its aerosols were still enlivening the skies. In a world awakening to the prospect of global warming, you might have expected such an object lesson in global cooling to sharpen the debate over artificial volcanoes of the sort that MacCracken had reviewed. \n               First cut is the deepest \n             But things went the other way. Once global warming started to be seen as real and important, climate scientists shied away from such speculation, preferring to hammer home the message that greenhouse-gas emissions had to be cut quickly and deeply. 'Geoengineering' the climate through artificial modifications was seen as a dangerous distraction from the business of slashing emissions. In the decade and a half that followed Pinatubo, talk of geoengineering went into eclipse. From 1995 to 2005, more research went into technological responses to asteroids that might one day endanger the Earth than into direct responses against the sunlight already heating the planet. Much of the climate community still views the idea with deep suspicion or outright hostility. Geoengineering, many say, is a way to feed society's addiction to fossil fuels. \u201cIt's like a junkie figuring out new ways of stealing from his children,\u201d says Meinrat Andreae, an atmospheric scientist at the Max Planck Institute for Chemistry in Mainz, Germany. But in the past year the idea has begun to re-emerge, and it now seems to be making up for lost time. In particular, the idea of blocking some of the Sun's light before it gets to the Earth \u2014 sometimes euphemistically referred to as 'radiation management' \u2014 is receiving more attention now than ever before, with new ideas about how, why and when such an approach might be taken. The most recent IPCC report, released last week, scoffs at such notions \u2014 but underlines the need for drastic approaches to stave off the effects of rising planetary temperatures. And in the context of the drastic, curiosity about geoengineering looks likely to grow. \u201cIt's a natural question to ask,\u201d says MacCracken, now chief scientist for the Climate Institute in Washington DC. \u201cIf we can do something inadvertently, can we do something deliberate to counter it?\u201d This new interest in geoengineering was set off by an article by Andreae's friend and colleague Paul Crutzen, published in the journal  Climatic Change  in August 2006 (ref.  1 ). The article contained relatively little that wasn't already in the literature when Pinatubo blew its top, but it had a major impact because of who was saying it. \u201cIn this case, the messenger is the message,\u201d says Stephen Schneider, a climate scientist at Stanford University in Palo Alto, California, and editor of the journal. \u201cNobelist and general environmental worrier Paul Crutzen \u2014 someone who showed the world the risks of ozone depletion very early on \u2014 is a natural to get big attention for thinking about the environmentally unthinkable.\u201d It was for exactly this reason that Crutzen's colleague Andreae urged him not to publish. \n               Pollution to save the world \n             If the identity of the author was striking, so too was the matter-of-fact way that he chose to frame the issue. Mankind, Crutzen pointed out, already puts more than 100 million tonnes of sulphur dioxide into the atmosphere every year \u2014 the equivalent of at least five Pinatubos. Unfortunately, the aerosols that this sulphur produces sit in the lower atmosphere, the part we breathe, and they do us no good; they are estimated to contribute to 500,000 premature deaths every year. But clearing away this pollution has the unintended consequence of increasing the rate of global warming, because even in the lower atmosphere the sulphates stop sunlight from reaching the surface. Crutzen looked at the idea of introducing one or two million tonnes of sulphur into the stratosphere every year, where it could produce a long-lived aerosol, as a way to keep the protective effects while getting rid of the short-lived aerosols in the lower atmosphere. At both the beginning and end of his article, Crutzen stressed that he would rather see global warming controlled by a reduction in emissions. But he admitted that, so far, he saw little cause for optimism. He also pointed out that sulphate aerosols can act to cool the climate immediately; reducing emissions, on the other hand, takes decades or generations. If something really bad starts to happen, aerosols could provide a prompt cooling response in a way that emissions control simply could not. On hearing of Crutzen's paper, Tom Wigley, a veteran climate scientist at the National Center for Atmospheric Research in Boulder, Colorado, decided to look at what such a programme might achieve in the short term. He realized that the almost instantaneous cooling effect of the sulphates could be used to buy the time needed for emissions reductions to start having an effect. Using a very simple climate model, Wigley looked at the possibility of capping atmospheric carbon dioxide levels at 450 parts per million around the middle of the century. (Before the industrial revolution the level of carbon dioxide was 280 parts per million, and today it is 381 parts per million.) Never going above 450 parts per million would offer a decent chance of limiting future warming at or below 2 \u00b0C. But such restraint looks increasingly implausible to many. A little geoengineering might make an equivalent objective a lot more achievable, Wigley argued 2 . Imagine an aerosol effort that starts fairly soon and is quickly ramped up to a Pinatubo's worth of sulphates being injected into the upper atmosphere every two years, before being phased out completely after 80 years. The resulting cooling effect would allow carbon dioxide emissions to keep climbing for a few more decades without the world warming any more than if they levelled immediately. In Wigley's model the peak level of atmospheric carbon dioxide could climb to well over 500 parts per million without the Earth's temperature getting any higher than it would with stabilization at the much-harder-to-obtain 450 parts per million. Emissions would still have to be cut very steeply from the middle of the century on. But for Wigley, those extra decades of room to manoeuvre are all important. \n               Realms of the unknown \n             If a burst of sulphates might allow the world to postpone the effects of emissions control for a few decades, would a consistent effort allow the world to do without control altogether? Wigley points to at least one reason why not. Carbon dioxide does more than just warm \u2014 it also acidifies the ocean 3 . Even if the warming effects of ever-increasing carbon dioxide could be cancelled out, the effects on corals, shellfish and eventually the entire marine food web would still be disastrous. And even the most vigorous proponents of geoengineering do not suggest that it can defer any need to reduce emissions indefinitely. \u201cIf you are digging a hole and want out of it, certainly slowing your digging rate is good,\u201d says Gregory Benford, an astrophysicist at the University of California, Irvine, who is also a noted science-fiction writer and something of a geoengineering enthusiast. \u201cBut,\u201d he continues, \u201cyou need a ladder.\u201d Even a strictly term-limited scheme has potential pitfalls. Wigley's model deals only with average global temperatures, and there is much more to the climate than that. For decades, climate scientists dubious about geoengineering schemes have pointed out that the pattern of warming expected from carbon dioxide, and the pattern of cooling expected from aerosols, would differ in both space and time. Aerosols cool things only when the Sun is shining, and they cool things most where the Sun shines brightest. They thus cool only in the day and more in summer and the tropics. Greenhouse gases warm things night and day, and their effect is greater at the poles. The two factors could thus cancel each other out in terms of global average, while fundamentally changing the way that the climate works region by region. In 2000, Ken Caldeira \u2014 then of the Livermore lab \u2014 decided to look in detail at how strong the mismatch was. With his colleague Bala Govindasamy he used a general circulation model (GCM) to compare a world with doubled carbon dioxide to a world with both doubled carbon dioxide and an offsetting 1.8% drop in sunlight. In the carbon-dioxide only world, 97% of the surface had statistically significant warming; in the world with a cooling aerosol, that figure was cut to just 15% (ref.  4 ). \n               Simple solutions \n             The result surprised Caldeira, who had undertaken the research in part to show a colleague, Lowell Wood, that geoengineering was more complex than Wood imagined. Wood is a forceful spokesman for extreme ideas, most notoriously the proposed X-ray laser that was to have formed the cornerstone of Ronald Reagan's Star Wars programme. In the 1990s, he had become enamoured of radiation management, as had his mentor, Edward Teller, Livermore's hydrogen-bomb-begetting eminence gris. If geoengineering had not already had a bad name among climate scientists concerned about the environment, Teller's championing of the idea in the pages of the  Wall Street Journal  would have won it one. Caldeira had wanted to show that the world was more complex than simple physics suggested. His results, though, edged things the other way, making geoengineering look more plausible, rather than less. Perhaps as a result, they were hardly followed up at all. Only six years later, under the influence of the Crutzen paper, are other researchers with GCMs starting to look at radiation management. Last month, for instance, Wigley's colleague Phil Rasch unveiled some preliminary results in a seminar at the National Center for Atmospheric Research. Again, the amount that warming from emissions and cooling from aerosols cancelled each other out was surprising. But the differences were not zero. Temperature shifts in some places, and precipitation in others \u2014 although the differences were not as large as those to be expected in a greenhouse-only world. Caldeira, too, while stressing that he is not an advocate of moving ahead with geoengineering, has recently revisited the topic using a different GCM to the one he used in 2000. He finds similar results, with somewhat larger shifts in precipitation than in temperature. His new work also suggests that natural sinks for carbon might expand in a geoengineered world. With more carbon dioxide, plants are more productive and thus suck up more carbon dioxide. In a greenhouse world, this tendency is counterbalanced by the effect of temperature increases on the respiration of soil microbes \u2014 warmer microbes produce more carbon dioxide. But in a greenhoused-and-cooled world, the plant effect remains while the respiration effect is capped, and so significantly more carbon dioxide gets used up. \n               Unstable foundations \n             Climate modellers at NASA's Goddard Institute for Space Studies in New York have also started to study the potential effects of geoengineering in GCMs. The people who run similar models at the Met Office Hadley Centre in the United Kingdom and the Max Planck Institute are looking on with interest, and will probably follow them. But Rasch cautions that these are early days. A confident understanding of geoengineering's promises and problems would require years of dedicated work from groups all over the world, an effort comparable to that reflected in the IPCC's massive reports on the natural science of climate change. And even that, say critics, would not be enough. GCMs are useful tools, but they do not provide a perfect understanding of the climate system. And it is the lack of such an understanding that critics point to as geoengineering's biggest scientific problem. The very thing that motivates people like Crutzen to study geoengineering \u2014 the risk of large surprises that require immediate action \u2014 leads others to see the whole idea as fundamentally unworkable. Although models agree that the world will warm and climatic patterns will change as carbon dioxide rises, they don't agree on the amount of warming or the patterns of change. Indeed, that uncertainty is one of the reasons that climate change is such a difficult issue. \u201cHow can you engineer a system whose behaviour you don't understand?\u201d asks Ronald Prinn, a climate scientist at the Massachusetts Institute of Technology in Cambridge. One answer to this question is \u201cas carefully and reversibly as you can\u201d. Caldeira and MacCracken have now joined Wood and Benford to investigate a radiation-management proposal aimed at the Arctic. It is in the Arctic, Caldeira thinks, that they can get the greatest effect for the least effort, because cooling the Arctic will encourage the growth of sea ice \u2014 which will itself cool things even further, both by reflecting away sunlight in the summer and by acting as an insulating lid on the warmer water below. The Arctic has endangered ecosystems with inhabitants that might benefit from the cooling \u2014 as did the polar bears born in the winter of 1991\u201392, who grew big and strong on the particularly long-lived sea ice of the following spring, and who scientists dubbed the 'Pinatubo cubs'. And it is in the Arctic, the team suggests, that greenhouse warming might spring one of the 'surprises' not foreseen in models but endlessly speculated about elsewhere: the sudden pell-mell melting of the Greenland ice cap. \n               Polar focus \n             Caldeira and his colleagues reason that cooling the Arctic requires much less material than cooling the planet as a whole. What's more, they propose putting it low enough in the stratosphere that much of it will fall out less than a year after it is lofted up in the spring \u2014 as there is no point having a reflecting layer up there in the sunless winter. Engineering a year at a time, in a small and sparsely populated region, might be as low-impact an option as the geoengineer's toolbox offers. The technology could be quite simple: cargo aircraft towing sulphur-distributing parasails behind and above them, or very high-altitude blimps pumping sulphur dioxide up from the ground through 20-kilometre-long hoses. As Wood points out, you really only need a few dozen litres per second of output to do the job \u2014less if you use something more reflective than sulphate particles. But even modest, local geoengineering could have disproportionate effects far away. Alan Robock and his colleagues at Rutgers University in New Jersey, working with climate modellers at the Goddard institute, have studied the effects of volcanic eruptions that belch out sulphur at high latitudes \u2014 natural analogues to the sort of thing Caldeira and colleagues are talking about. These eruptions seem to have an unfortunate side effect; the 1783 Laki eruption in Iceland, for instance, weakened the Indian monsoon and cut rains in the Sahel, in Africa, to boot 5 . The fact that that is what seems to have happened in the past does not necessarily mean that it would happen in a geoengineered future. But it is easily argued that betting the monsoon on the ability of models to accurately capture such subtleties would require a foolhardy level of trust, a remarkable lack of concern for hundreds of millions of livelihoods or a startling desperation in the face of the alternative. One source of such problems is the fact that the stratosphere is not just a sheet of glass to be tinted at will. It is a circulating system in which physics and chemistry interact; it is tied to the troposphere below in complex ways that greenhouse warming is already changing; and aerosols warm it or cool it in different ways depending on the size of the particles involved. True, compared with most other components in Earth's system it is relatively simple. (For a start, nothing lives there.) But it still has its subtleties. A tempting way around this problem is to put the sunblock even higher \u2014 in orbit, where among other things it can be turned off at will. Discussions of orbital sunshades have been around almost as long as those of artificial volcanoes. The most technically sophisticated was published by Roger Angel of the University of Arizona, Tucson, last year 6 . \n               Up and away \n             Angel was looking for a way to put up a sunshade that, unlike earlier proposals, did not require humans in orbit or the resources to be found on the Moon or nearby asteroids. His solution was to use a fleet of almost-transparent 'fliers', the size of dustbin lids, that would be launched from Earth in prepacked stacks by means of a vast electromagnetic cannon. Once in orbit, the gossamer-thin fliers would peel off these stacks and arrange themselves in orbits that keep them between the Earth and the Sun at almost all times. The shadow of this cloud of spacecraft 1.85 million kilometres away, Angel calculated, would be a little larger than the Earth, and would cut down sunlight by about 1.8%. The details of Angel's proposal are meticulously worked out, and their cost is suitably astronomical \u2014 about $5 trillion, or a decade's worth of US defence spending. The cannons, and the power systems required to pulse gigawatts through them on demand, are impressive but borderline plausible. The really mind-boggling bit is the sheer number of fliers required to do the job: 16 trillion. The US military gets through 1.5 billion bullets a year. If fliers could be mass-produced at a hundred times the rate that those bullets are, it would still take a century to produce enough of them. \n               Setting the standard \n             Nevertheless, Ralph Cicerone, a climate scientist and president of the US National Academy of Sciences, singles the paper out for praise for the painstakingly careful way it was done. \u201cHe went back to it again and again,\u201d Cicerone says. \u201cIn its standard of elegance and completeness it was exemplary.\u201d For him and many others, such academic excellence is the main point of publishing research on geoengineering. For these researchers, the aim is not to find feasible solutions but to do good science that provides a standard against which to judge the less good, or flatly foolish, schemes that might otherwise accrete around the idea. Cicerone points to quack schemes for ozone replacement in the 1980s as the sort of thing that needs to be forestalled: back then, he says, \u201cpoor ideas got as far as they did because of [the community's] silence.\u201d Cicerone says he would welcome a body of work on geoengineering that is substantial enough to deserve a chapter of its own in the next IPCC assessment report, due in about six years. At the same time, he favours a moratorium on any moves towards deploying such a system, and agrees with the consensus of the climate community that much greater efforts towards mitigation of emissions remain the highest priority. After all, no one thinks that, in the short term, a world cooled by engineering would be preferable to one cooled by a reduction in carbon dioxide levels. And no one thinks that, as yet, we know enough to embark on any sort of large-scale engineering. Models of geoengineering's benefits need to be a lot more accurate than models of the harm that will be done in its absence. As Caldeira puts it, if you can be no more precise about the chances of harm under the status quo than to give them as 50%, that's still something to worry about. But if a proposed intervention has a 50-50 chance of doing good or harm, that's something to avoid. A few voices argue that it is too late for this thinking \u2014 that we are already engineering nature by exerting a vast influence over the nitrogen cycle, the carbon cycle, the radiative balance of the atmosphere and everything else. In this sense we live in an engineered world, and the question is simply how to engineer it better. But in the scientific community this argument has achieved little traction. The key point, articulated by climate scientist David Keith from the University of Calgary in Canada, is that making a mess is not the same as engineering. Humanity has shown a great capacity to make a mess, mostly as a side effect of just trying to make a living. But that is not engineering. Engineering involves intention. That is why economist and philosopher Herbert Simon famously grouped it with the social and some of the human sciences under the rubric of 'the sciences of the artificial', a category created as a deliberate counterpart to the intention- and imperative-free natural sciences. \n               Artificial intelligence \n             Although in the past two decades climate scientists have been confronted with the social, technological and economic implications of their work, they are not scientists of the artificial. Hans Feichter, a climate modeller at the Max Planck Institute for Meteorology in Hamburg, speaks for the vast majority of his colleagues when he says \u201cthe role of a geoscientist is to understand nature, not to change it.\u201d Climate scientists have proved themselves happy to advocate massive changes aimed at shifting the climate. But they are massive changes in technology, in geopolitics, in social norms \u2014 changes that require the sciences of the artificial. Not changes in the workings of the stratosphere. Not changes in the natural. In the past year, climate scientists have shown new willingness to study the pathways by which the Earth might be deliberately changed, although many will do so in large part simply to show, with authority, that all such paths are dead-end streets. But they are not willing to abandon the realm of natural science, and commit themselves to an artificial Earth. See Editorial,  \n                     page 115 \n                   . \n                     Only mother nature knows how to fertilize the ocean \n                   \n                     You can't do it all with mirrors \n                   \n                     Should we flood the air with sulphur? \n                   \n                     Volcanoes cool climate through bacteria \n                   \n                     Climate Change In Focus \n                   \n                     Geoengineering Insight \n                   \n                     Paul Crutzen's home page \n                   Reprints and Permissions"},
{"file_id": "447138a", "url": "https://www.nature.com/articles/447138a", "year": 2007, "authors": [{"name": "Helen Pearson"}], "parsed_as_year": "2006_or_before", "body": "Imaging fluorescent molecules in live cells is revolutionizing cell biology. But a pretty image is not necessarily a good one, and many biologists are learning this the hard way, finds Helen Pearson. The satellite imagery of Google Earth offers homeowners the chance to zoom in from outer space and hover above their rooftops. For biologists, a microscope gives a similarly exhilarating view of a cell's innards: the omnipotent eye of the nucleus, the bustling traffic of the cytoplasm and the elaborate architecture of the cytoskeleton. This is the detailed, shifting topography that cell biologists spend their lives trying to comprehend. But looks can be deceiving, as Jack Fransen at Radboud University Nijmegen Medical Centre in the Netherlands and his colleagues found out. Using a powerful fluorescence microscope, they watched cells pulse from acid green to mellow blue when bathed in the chemical fuel ATP. The purpose of the project was to test whether ATP could cause proteins to change shape and trigger their fluorescent tags to change colour. \n               In the eye of the beholder \n             But the beauty of the imagery dimmed when the researchers checked their control protein, which had been carefully constructed to fluoresce at a constant level. This, too, pulsed prettily with ATP \u2014 as did every other control protein they could lay their hands on. \u201cIt was a complete surprise,\u201d says Fransen. The problem lay with some mysterious behaviour of ATP that so far remains unexplained and hard to spot. Researchers less knowledgeable or meticulous about microscopy could easily have their results thrown off by such phenomena. This is why the team published a cautionary note about the discovery earlier this year (Willemse, M.  et al .  Nature Biotechnol.   25,  170\u2013172; 2007). To correctly capture images using a modern microscope, researchers must have a good grasp of optics, an awareness of the microscope's complexity and an obsession for detail. Such skills can take months or even years to master, and yet, owing to inexperience or the rush to publish, are all too often squeezed into hours or days. Popular methods such as fluorescence microscopy are particularly fraught with dangers. Most researchers are not intentionally cutting corners; they may simply be unaware of the possible pitfalls. And most oversights are harmless \u2014 for example, making a fluorescent protein appear dimmer or fuzzier than it is. But inept microscopy, and subsequent analysis, can easily generate results that are misleading or wrong. It is difficult to gauge how much published microscopy is of poor quality, and it is a rare biologist, such as Fransen, who will be able to identify, let alone admit to, a specific problem. But one expert contacted by  Nature  estimated that as many as half of all experiments that report two proteins in one spot have not been performed properly. Another estimated that 5\u201310% of images don't match what is reported in the text. \u201cIt's easy to pick up any journal \u2014 even  Nature  \u2014 and see poor microscopy data,\u201d says Jennifer Waters, who directs the Nikon Imaging Center at Harvard Medical School. \u201cI don't know how often the results are blatantly wrong, but I do worry about the accuracy.\u201d The modern light microscope comes with the accoutrements and price tag of a high-speed racing car and offers an exhilarating ride. It can boast numerous knobs, a foot pedal, winking lights and touch-control climate. Such microscopes can cost anything from US$50,000 to $1 million. But not everyone should be allowed behind the eyepiece. \u201cIt's much more complicated than sitting down and pressing the buttons,\u201d says Simon Watkins, who runs a biological-imaging centre at the University of Pittsburgh in Pennsylvania. \u201cIf you got into a fast car but didn't know how to drive it, you'd crash very quickly.\u201d And that's what has happened. During the past 10\u201315 years, these souped-up machines have become a mainstay of most cellular and molecular biology laboratories. But many biologists' ability to handle the instruments has not kept pace with the technology, and the road to results is becoming littered with scrapes, prangs and outright wrecks. Only 20 years ago, light microscopy was very different. Most biologists used conventional cameras to take snapshots of illuminated slices of dead tissue. That changed in the early 1990s with the discovery that proteins could be spliced onto jellyfish green fluorescent protein (GFP), allowing their location to be tracked in living cells. Since then, a rainbow of fluorescent proteins has become available, as have highly sensitive digital cameras that can detect signals invisible to the naked eye. \n               Getting a look in \n             It is now a routine part of many studies to investigate, using microscopy, where in the cell a fluorescently labelled protein is concentrated and where it goes. This type of microscopy has hooked cell biologists because it allows them to gaze inside living tissues and monitor molecules in their native environment. But although most biologists graduate with some training in chopping and splicing DNA, few will have laid their hands on a pricey fluorescent microscope. \u201cYour average molecular biologist can make all these fantastic fluorescent tools,\u201d says Kurt Anderson of the Beatson Institute for Cancer Research in Glasgow, \u201cbut then imaging is just a little bit tacked on the end.\u201d \u201cThe only time I end up in stand-up fights with users,\u201d says microscope specialist Alison North, \u201cis when they say, 'I need to get a picture, I've never used that microscope before but I'm sending out the paper tomorrow.' Then I scream at them, because that's terrible science. How do they know what the results are if they haven't got the images yet?\u201d North knows the perils of microscopy all too well. She runs a 12-microscope facility at Rockefeller University in New York and gives a two-hour lecture on general microscopy and its pitfalls. After I sat through an abbreviated version of the talk, my brain felt heavy and my palms damp, and the chance of capturing a good image seemed near impossible. That's precisely the point, says North. She aims to scare users enough that they will consult her before embarking on a doomed microscopy project: \u201cIt's quite cruel of me isn't it?\u201d The list of potential mistakes in fluorescence microscopy is long and complex (see  'Top tips for taking images' ). Seemingly small steps, such as using the correct thickness of glass coverslip, are crucial to obtaining a good image. Even the intermittent cooling of an air conditioner can cause a microscope to drift in and out of focus. \u201cThere are an infinite number of settings that a poor microscopist can make mistakes on,\u201d says microscope expert Michael Davidson at the Florida State University in Tallahassee. One of the most common uses of fluorescence microscopy \u2014 and therefore the source of many problems \u2014 involves looking for two proteins labelled with different coloured tags in order to determine whether they sit in the same place in a tissue or cell. Each fluorescent protein is excited by a particular range of wavelengths and emits at different wavelengths that are collected through microscope filters. If a researcher uses GFP in combination with a tag that emits red light then, in places where the two proteins are close together, combining digital images of these two tags will create a yellowish signal. Things go wrong if users select an inappropriate pair of tags or incorrect microscope settings with which to detect them. Problems can arise if the light used to excite one tag also partially excites the second and a poorly chosen filter lets some of that unwanted light in. This phenomenon, known as bleed-through, can wrongly suggest that two proteins are located together, because one of the tags will fluoresce under both excitation conditions and thus appear to be two tags in precisely the same spot. Such a mistake can be avoided by choosing tags with non-overlapping emission and excitation spectra, and by a control experiment in which only one tag is used, to see if it fluoresces under both conditions. \n               Not for the faint-hearted \n             Microscopists save most of their expletives for more sophisticated techniques such as FRET (fluorescence resonance energy transfer), one of several four-letter acronyms for methods that are both popular and treacherous. FRET is the technique that Fransen and his colleagues stumbled over. It's so temperamental that Waters says she advises new graduate students \u201cto turn around and run away\u201d if a prospective supervisor suggests FRET for their thesis work. FRET is highly susceptible to both false-positive and false-negative results because it is used to detect very close interactions between two proteins or parts of the same protein and users are not always aware of its limitations. So who is responsible for ensuring that microscope users are competent, and what can be done to help those who are not? Most scientists are willing to admit their inexperience and accept that it is their responsibility to operate microscopes correctly. They can turn to information-packed websites and books for help. But it is hard to beat hands-on experience from the many highly regarded \u2014 and oversubscribed \u2014 crash courses in microscopy. A handful of universities are starting to offer graduate courses in biological imaging, but more are needed. And microscope manufacturers such as Olympus and Nikon provide training and troubleshooting. A growing number of institutions also host a central imaging facility similar to that run by North, with a suite of machines and one or more dedicated specialists. These experts urge researchers to consult them at the planning stages of an experiment to ensure that their imaging will be successful. But even with all this advice on offer, biologists can remain oblivious to the mistakes they are making. In such cases, some believe that journals and reviewers could do more to police poor-quality microscopy. One improvement journals could make would be to require more details about microscopy techniques in methods sections or supplementary information. Experts say that few journals require enough detail to properly judge the quality of data or to reproduce them. For example, including the type of filters used would help others judge whether a result could be due to bleed-through. Several experts also support the idea of asking a microscopist to review imaging data in papers that rely strongly on imaging to support their conclusions. Editors at the  Journal of Cell Biology  and  Nature Cell Biology  say they already consult reviewers with microscopy expertise when necessary. But journals and reviewers are too often impressed by pretty images. North tells the story of one postdoc who slaved to capture images of her small cells only to have the paper turned down because the reviewer said the images were not good enough. \u201cI've seen how long she spent getting the highest quality images she could possibly achieve,\u201d says North. \u201cI think it's a big problem when the reviewers are more concerned with how aesthetically pleasing an image is than whether the scientific content is clear.\u201d Looking ahead, the situation could get both better and worse. For those who just want to point-and-shoot with their microscope, manufacturers are building machines with less room for mistakes. Last year, Nikon launched a microscope called BioStation IM \u2014 effectively a foolproof microscope for imaging live cells with camera, software and incubator all in one box. Joseph LoBiondo, an expert in bioscience microscopes at Nikon in Melville, New York, predicts that microscopes will become even more automated in future, and says, \u201cit takes the mystique out of it.\u201d But others worry that such automation will encourage sloppy experimentation. At the same time, microscopy is becoming still more complex. Even those who run imaging facilities say that they struggle to keep up with the latest technology as new imaging techniques are introduced. Many potential problems, such as that encountered by Fransen, are only now being discovered. It could be argued that biologists should focus on generating hypotheses and analysing results, rather than mastering sophisticated machinery. So microscopy could become a specialized service that is outsourced to technical experts, collaborators or even companies, says John Runions, who specializes in bioimaging at Oxford Brookes University, UK. Others disagree, saying that many biological questions simply cannot be answered without a working knowledge of microscopy. \u201cIn competitive biology, you don't necessarily need to be a mechanic but you need to be able to operate the machine,\u201d Davidson says. \u201cIf you don't know how it works you'll get creamed in the race.\u201d \n                     Cells come into focus \n                   \n                     Molecular microscopy: Focus on the living \n                   \n                     Forensic software traces tweaks to images \n                   \n                     Image manipulation: CSI: cell biology \n                   \n                     Nature Cell Biology \n                   \n                     Molecular Expressions \n                   \n                     Nikon MicroscopyU \n                   \n                     The Olympus Microscopy Resource Center \n                   \n                     European Light Microscopy Initiative \n                   Reprints and Permissions"},
{"file_id": "447141a", "url": "https://www.nature.com/articles/447141a", "year": 2007, "authors": [{"name": "Emma Marris"}], "parsed_as_year": "2006_or_before", "body": "Why do chemists make compounds that could blow up in their faces? Emma Marris finds out... from a safe distance. Explosives come in many varieties, from military munitions to rapidly inflating airbags. But useful explosives share one thing: stability. A clear advantage of trinitrotoluene, or TNT \u2014 whose punch is used as a yardstick for all other explosives \u2014 is that it remains safe and solid until detonated. So why would anyone want to make a highly unstable explosive? One that will release its energy on the slightest provocation? Because they are chemists, and they like explosions, is the popular answer. Because they are chemists, and they like a technical challenge, is what those doing the work say. How convincing is that? Explosives release energy stored in chemical bonds in a runaway process that often turns solids into gases, expands material massively and creates heat. In big explosions, pressure waves radiate out from the origin, keeping the reaction going throughout the material. When detonated, TNT decomposes violently into a gas, some soot, and a boom. Many explosive compounds are less stable than TNT \u2014 some are so temperamental or hard to make that they will probably never be used in practice. Consider this warning for tetraazidomethane, a particularly wild member of the group of compounds known as polyazides, which have a general reputation for removing student eyebrows. \u201cTetraazidomethane is extremely dangerous as a pure substance. It can explode at any time \u2014 without a recognizable cause.\u201d Klaus Banert at the Chemnitz University of Technology in Germany was the first to synthesize this compound. He says that less than a drop of it destroyed the glass trap and the Dewar flask of the cooling bath they used to isolate it (K. Banert  et al .  Angew. Chem. Int. Edn   46,  1168\u20131171; 2007). \u201cAlthough we had expected explosive properties of tetraazidomethane, we were deeply impressed by its destructive force,\u201d he says. His team had to work behind a safety shield and wear gloves, face shields and ear protectors. Banert says that when it was all over, he was relieved. The lab had taken all reasonable safety precautions but he had still been worried while the experiment was underway. So why did they do it? Was it the adrenaline? The childhood lure of explosions? Banert says that it was the pure challenge of the synthesis. \u201cI received my first chemistry set at the age of 11 and continued very intensively for several years performing chemical experiments at home. I was also interested in explosives at that time,\u201d he says. \u201cBut explosions were only of secondary importance.\u201d For tetraazidomethane, Banert says that it was an ambitious target to fill this gap in the family of high-energy density materials. \u201cThe structure of tetraazidomethane had already been calculated, and it was predicted that the compound theoretically should be able to exist.\u201d Derek Lowe, a medicinal chemist and author of the popular chemistry blog 'In the Pipeline' runs an occasional item on 'Things I Won't Work With'. Among them are the polyazides. But he can see the appeal of making highly explosive compounds. \u201cThese molecules do not want to exist. They are never going to form naturally or spontaneously. These things are teetering right on the edge of not being feasible, and you can be the first to make it.\u201d To strengthen the case that it is the synthesis, not the destruction, that excites such minds, consider the work of Philip Eaton at the University of Chicago, Illinois. In the 1960s, Eaton made cubane \u2014 a cube with a carbon at each corner. Then, at the suggestion of an army general, he went on to synthesize a highly explosive compound called octanitrocubane (M.-X. Zhang, P. E. Eaton & R. Gilardi.  Angew. Chem. Int. Edn   39,  401\u2013404; 2000). Octanitrocubane has the same pattern, but with nitrogen dioxide bound to each corner carbon atom. \u201cThe problem,\u201d says Eaton, \u201cwas how the devil to make it.\u201d The tricky synthesis has, he explains, many, many steps. \u201cIn the course of the whole thing we made less than a gram.\u201d Eaton can't estimate how much more explosive it is than TNT, except to say \u201ca lot\u201d. The idea was that the density of the structure would pack a high explosive power into a small volume \u2014 something that was important to the military when bulky guidance-system computers were hogging too much space in missiles. But octanitrocubane is just too hard to make for it to have any role in the military for the foreseeable future. Eaton is just pleased he figured out how to synthesize it. And he did it, he repeats, for the pure love of the challenge. \u201cThe explosiveness has no allure for me at all. I was not the kind of kid who made explosives.\u201d The proof? He never set off so much as a milligram of the stuff. \u201cThere may be some folks who like that sort of thing, but they don't tend to last very long,\u201d agrees Lowe. \u201cChemists have a reputation for being closet pyromaniacs, but the real crazies blow themselves up.\u201d \n                     How to rip apart molecules \n                   \n                     Chemistry: What chemists want to know \n                   Reprints and Permissions"},
{"file_id": "447248a", "url": "https://www.nature.com/articles/447248a", "year": 2007, "authors": [{"name": "Meredith Wadman"}], "parsed_as_year": "2006_or_before", "body": "Wealthy philanthropists and private foundations are supporting biomedical research on a grand scale. Meredith Wadman asks what they get for their money. Scientists arriving at the Stowers Institute for Medical Research in Kansas City, Missouri, might think they've chanced on Xanadu. Limestone floors and fine furniture seduce the eye, and from the expansive gardens comes the soothing sound of fountains. Inside this research palace, funded by a $2-billion endowment from local mutual-fund magnates Jim and Virginia Stowers, scientists pursue research on fundamental cell biology. Dozens are flush with US$1 million funding a year, and their work is often destined for stellar publications. The Stowers are not alone in their generosity; philanthropic foundations have long had an important role in biomedicine, from the birth of the Carnegie Institution in Washington DC in 1902 to the biomedical activism of the Rockefeller Foundation in the 1930s and 1940s. But during the past decade, philanthropists \u2014 and the foundations that they establish to distribute their money \u2014 have begun funding biomedical science on a particularly striking scale. Dominating this landscape is the Bill & Melinda Gates Foundation with its plans to nurse the globe to better health, boosted last year by billionaire financier Warren Buffet (see  page 254 ). But there are other donor organizations, many of which fund more basic biological research, ranging from the United States' mighty Howard Hughes Medical Institute (HHMI) to Britain's Wellcome Trust, the world's largest charity exclusively devoted to biomedicine (see  page 251 ). Their cash is being lapped up by researchers parched by flat funding from the US National Institutes of Health (NIH) and many other sources. These new givers \u2014 the gigaphilanthropists \u2014 are perceived to be making an impact on the research landscape that is much greater than the sum of their dollars. \u201cThe effect of the private foundations is not reflected in the total funding they supply. They have disproportionate influence,\u201d says Hamilton Moses of the Alerion Institute, a Virginia-based think-tank that focuses on innovation in biomedical research. They can, and do, take financial and scientific risks unthinkable with tax-payers' dollars. They fill gaps left by government and industry, dictate exactly what their money is spent on and act quickly compared with the sometimes glacial pace of government agencies. But although those running the organizations are sure that private money buys more and better science than public money, there is little concrete evidence they are right. The new wealth also comes with strings attached: some funders keep a businesslike control over the direction of the research they pay for and demand a level of accountability that can make researchers uncomfortable (see  page 252 ). Some observers worry about the growing power wielded by the gigaphilanthropists over the research agenda if, as is predicted, charitable giving reaches new heights in the future. They are concerned that too many important decisions with an impact on biomedicine will be made in the boardrooms of foundations with little scientific expertise \u2014 and no public input or accountability. \u201cYou may have foundations with assets larger than almost 70% of the world's nations making decisions about public policy and public priorities without any public discussion or political process,\u201d says Pablo Eisenberg, a senior fellow and philanthropy-watcher at the Georgetown Public Policy Institute in Washington DC. \n               Value for money? \n             By all accounts, the amount of money from non-profit foundations and philanthropists is growing strikingly. In Germany, for example, the Frankfurt-based Hertie Foundation has spent more than \u20ac90 million (US$122 million) on nurturing neuroscience since 2000, compared with a total of \u20ac30 million in the previous quarter century. In Britain, charities fuel more than half of the biomedical research enterprise, led by the Wellcome Trust, which served up \u00a3484 million (US$960 million) in research funding last year compared with \u00a3270 million just over a decade ago. In the United States, a wave of philanthropic giving over the past decade has been fuelled by a buoyant stock market and a generation of ageing, affluent baby boomers. Investing in biomedicine allows them to do something that might support the health of their children, and gains them significant tax breaks. In 2005, a study led by Moses showed that private, non-corporate support for US biomedical research leapt 36% to $2.5 billion between 1994 and 2003 (H. Moses  et al .  J. Am. Med. Assoc.   294 , 1333\u20131342; 2005). Today, \u201c$5 billion is probably an undercount\u201d, Moses says, when one includes philanthropic funding in all its varieties. This still accounts for no more than 5% of the roughly $100 billion spent annually in the United States on biomedical research (the biotechnology and pharmaceutical industries account for about 60% of this total and the government, led by the $29-billion budget of the NIH, for most of the rest). But even so, the massive donations and influence of the US foundations, along with the Wellcome Trust, bear examination. Linheng Li was one of the first scientists through the door after the Stowers Institute opened in late 2000. Li was intent on ending a 25-year quest by stem-cell scientists to find the 'niche' in bone marrow that harbours blood-forming stem cells. In 2003, Li delivered, with a paper in  Nature  describing the cells' physical and biochemical environment in mice (J. Zhang  et al .  Nature   425 , 836\u2013841; 2003), a discovery that had the potential to help researchers grow stem cells outside the body. Scores of such anecdotes suggest that the munificent backing of a gigaphilanthropist generates more, and more influential, scientific results. But they are just that: anecdotes. There are few if any studies rigorously comparing the productivity of an HHMI investigator, for example, against that of an NIH-funded colleague down the hall. Such assessments are difficult to make, partly because there are few researchers who rely solely on a single source of funds, philanthropic or public, and so could be sensibly compared. There is also an 'apples and pears' problem, adds Mark Walport, the Wellcome Trust's director. The trust focuses much of its support on young scientists and building research capacity in the developing world. \u201cIt would not be meaningful,\u201d he says, to try to compare the scientific outputs of these programmes with those from senior, independent investigators supported by Britain's government-funded research councils. The gigaphilanthropists do find other ways to gauge whether they are getting scientific value for their money. The Wellcome Trust, for instance, has combed global citation indices to establish that it funds five of the world's ten most-cited malaria researchers, and four out of the top ten in the cognitive sciences. The Ellison Medical Foundation, based in Bethesda, Maryland, funds ageing research. It is building an electronic archive of every application it receives and it plans \u2014 many years hence \u2014 to use citations and other measures to analyse the impact of researchers it backed and those it turned down. Meanwhile, executive director Richard Sprott says he attends as many national and international meetings as he can. \u201cI listen to who's doing the cutting-edge, exciting stuff. If it's our people, I think we're doing okay.\u201d Tom Cech, the Nobel-prizewinning chemist who has directed the HHMI since 2000, says that the number of plaudits won by its scientists \u201cprove that our investigators are far more successful than average\u201d. Between 1994 and 2007, 89 HHMI investigators were elected to the National Academy of Sciences (NAS) and seven won Nobel prizes. The HHMI has calculated that its investigators were more than ten times more likely to be elected to the NAS than US biologists funded by the NIH, and over 16 times more likely to win a Nobel prize in chemistry or medicine. \u201cWhat we can't rigorously prove is whether they are more productive because of our support, or simply because we know how to choose winners,\u201d Cech says. \n               Taking risks \n             Not every foundation does know how to choose winners, points out Eisenberg. \u201cThere are foundation officers who are sharp and knowledgeable, and those who are not,\u201d he says. Without more rigorous comparisons, some observers question whether, dollar for dollar, philanthropic donations guarantee more good science than government or industry does. \u201cI don't think the data overall would hold up,\u201d says Mary Woolley, president of Research!America, a health-research advocacy group in Alexandria, Virginia. \u201cPlenty of people who have received the Nobel prize were funded by the NIH.\u201d Philanthropic organizations can certainly put great pressure on their grant recipients to ensure they deliver. At the Stowers Institute, the senior scientists are appraised after five years' generous funding, when the institute enlists leaders in a researcher's speciality to evaluate their performance. \u201cThe only question we ask is whether the leaders in the field can say definitively, specifically and discretely: 'This is what this person has done at Stowers that has changed how people think about the field',\u201d says Stowers' president Bill Neaves. Since the institute opened its doors, eight of nine senior scientists who have been evaluated have passed that test. Private foundations have a flexibility and agility with their spending that industry and government agencies do not. They are not answerable to shareholders or venture capitalists; nor do they labour under the political and public scrutiny experienced by the NIH and other spenders of public money. \u201cWhen I was at the NIH, we had to ask ourselves a question when contemplating every award: 'Can I live with it on the front page of  The Washington Post ?'\u201d says Sprott, a former director of the National Institute on Aging. That kind of thought process, he says, \u201ctends to make the NIH very conservative\u201d. The NIH, under director Elias Zerhouni, has launched an ambitious effort to battle this conservatism. Since 2004, for instance, Zerhouni has awarded 35 Pioneer awards to individual investigators for high-risk research, delivering around $2.5 million to each over the course of five years. But for philanthropists, risk-taking is often the rule rather than the exception. \u201cWhen you are a very small slice of a large pie you not only have the opportunity, but I would say the responsibility, to do something out on the edge,\u201d says Cech. \n               Brain storm \n             Lately, Cech's organization has been pushing the edge at Janelia Farm, the $500-million research complex near Washington DC. Since September a cadre of top-tier scientists has been set loose here to try to unravel how information is processed by neuronal circuits \u2014 a departure for the HHMI, which until now has supported researchers in their home institutions. At Janelia Farm, researchers work in small groups without the benefit of tenure or outside funding \u2014 but with a freedom from the hassles of grant-writing, teaching and administrative duties that is almost unique in US science. Director Gerry Rubin says that the best science comes from making such an expensive but risky investment, even if nine out of ten projects fail. \u201cWe are venture capitalists here,\u201d he notes. The gigaphilanthropists can also move fast. When the US Postal Service was beset by anthrax attacks in the autumn of 2001, researchers quickly realized that they needed to know the background level of anthrax in post offices around the country. \u201cIt would have taken the NIH two or three years to solicit and award a grant answering that question,\u201d says Sprott. \u201cWe could pick up the phone and call the world's top anthrax expert and ask him to design and carry out the needed study. We had an answer within six months.\u201d Ellison's investigator found 15 different strains of anthrax of the kind that infect cattle, sheep and horses; none was the strain being sent through the mail. Generally, the new philanthropists are not the type to write a cheque and walk away. They are determined to identify and fill key gaps in public funding \u2014 and to make sure the work gets done. Take Paul Allen, the Seattle billionaire who co-founded Microsoft with Bill Gates. In 2001, Allen summoned the best and brightest minds he could find in genomics, neuroscience and psychology and asked them what could and should be done to change the field of brain science. In 2003, he and his sister, Jody Allen Patton, signed a cheque for $100 million to launch the Allen Institute for Brain Science in Seattle Footnote  1 . Last year, researchers there unveiled the Allen Brain Atlas, a three-dimensional map showing where thousands of genes are active in the mouse brain. Some 800 scientists are using the atlas daily, according to the institute. Like the Allens, foundations \u201call want missions accomplished\u201d, says Donald Brown, president of the Life Sciences Research Foundation, a non-profit organization based in Baltimore, Maryland, that solicits grants from foundations and industry to support postdocs. Targeted philanthropic spending gets big, expensive projects done fast, but it also draws criticism. Because US charitable foundations are required by law to spend 5% of their assets each year, money can be thrown at projects too quickly for some people's tastes. Gigaphilanthropists can choose to fund research at the whim of their benefactors and the advisers they choose, very different to the extensive consultation with the scientific community that occurs before government money is committed to a big project. In the case of the Allen Brain Atlas, some researchers grumble that the money would have had a far greater impact on neuroscience if it had been spread among a group of top investigators. More recently, some foundations' alacrity has been taxed as they find themselves scrambling to compensate for the funding freeze at the NIH and keep individual investigators afloat. Some are concerned that this constrains their flexibility \u2014 and that they risk being taken for granted. \u201cI worry that we are allowing the government to say: 'We don't have to pay the bill. Private philanthropy will step in',\u201d says Sprott. \u201cThat's not what we want to do.\u201d As Paul Schervish sees it, the current flow of philanthropic money may look a mere trickle by the time today's postdocs retire. In a widely cited 1999 report, Schervish, director of the Center on Wealth and Philanthropy at Boston College, and his colleague John Havens predicted that by 2052 at least $6 trillion in wealth would be transferred from the estates of older Americans to charity \u2014 some $100 billion more per year than today. That would amount to a significant chunk of cash for biomedicine if, as is the case today, roughly 20% of that money goes to health. \u201cWe are going to see substantial foundation growth,\u201d says Schervish. If his prediction pans out, many more researchers will find themselves relying on \u2014 and answerable to \u2014 the gigaphilanthropists. At the Stowers Institute, the founders have already announced plans to add 56,000 square metres of facilities and 600 people every decade in perpetuity. But in the rich biomedical landscape of the future, it may be just one of many palatial shrines to research. For more on philanthropy, see our online special at  http://www.nature.com/news/specials/philanthropy . See also Editorial,  page 231 . \n                     Biomedical philanthropy: The money tree \n                   \n                     Biomedical philanthropy: Love or money \n                   \n                     Biomedical philanthropy: The giving machine \n                   \n                     Giving it away \n                   \n                     Centre stage in Missouri \n                   \n                     In praise of Gates \n                   \n                     Universities face cash shortfall as stock-market slide hits charities \n                   \n                     Biomedical philanthropy, Silicon Valley style \n                   \n                     Stowers Institute \n                   \n                     Bill & Melinda Gates Foundation \n                   \n                     Howard Hughes Medical Institute \n                   \n                     Wellcome Trust \n                   \n                     Boston College Center for Wealthy and Philanthropy 1999 paper \n                   Reprints and Permissions"},
{"file_id": "447254a", "url": "https://www.nature.com/articles/447254a", "year": 2007, "authors": [{"name": "Lucy Odling-Smee"}], "parsed_as_year": "2006_or_before", "body": "Flush with Microsoft's fortune, the Bill & Melinda Gates Foundation is the largest charitable foundation in the United States. Tadataka Yamada, executive director of its Global Health Program, tells Lucy Odling-Smee how the organization aims to save lives with its wealth. \n               Empowering and enriching the developing world requires tens of billions of dollars a year. How can the Gates foundation hope to make a difference? \n             The Gates foundation contributes roughly 10% of the US$12.7 billion a year spent on health-related aid to developing countries by donors such as the United States, United Kingdom and France. The world needs to commit a lot more funding to improving global health. Estimates of the additional resources that are needed to meet the United Nations Millennium Development Goals for health range from $25 billion to $70 billion per year. We believe that our funds have been catalytic in many ways. For example, our initial $750 million contribution to the GAVI alliance \u2014 formerly known as the Global Alliance for Vaccines and Immunisation \u2014 prompted further donations of more than $2 billion. GAVI has helped to increase immunization rates in millions of children in 70 developing countries. \n               How do you decide where to allocate your money within the Grand Challenges in Global Health initiative? \n             The Grand Challenges initiative was created to support ground-breaking research on some of the most fundamental scientific problems in global health. Its scientific board, which includes 20 scientists and public-health experts from around the world, identified 14 major challenges in global health and then reviewed proposals for research to overcome these challenges. Guided by the board's recommendations, we selected 43 projects to fund \u2014 ranging from heat-stable vaccines that don't require refrigeration to insect repellants that interfere with disease-carrying mosquitoes' sense of smell. \n               Are there any areas you have specifically decided not to fund? \n             We focus most of our resources on the health problems that disproportionately affect developing countries and that don't receive enough attention and resources \u2014 particularly infectious diseases; maternal, newborn, and child health; and nutrition. Although many important health issues fall outside of this scope, we believe that this focus will help us have the greatest impact on the people in greatest need. \n               What is the best way for a biomedical researcher to win funding from the Gates Foundation? \n             We fund projects that fit within our global-health strategy, which is covered in detail on our website. We are very focused on real-world outcomes, so we tend to favour research that, if successful, can be translated quickly to the field. \n               As a private foundation, what can you do with your money that other funders cannot? \n               We often support 'high-risk, high-reward' projects that can be difficult for governments or the private sector to fund. For example, the Medicines for Malaria Venture, one of our major grantees, is partnering with industry to conduct clinical trials of new treatments for malaria. Private companies would not be able to fund these trials alone, because of the high financial risk of conducting expensive trials on products that do not have a market in wealthy countries. \n               How are scientists who are funded by the Gates Foundation held to account? \n             We require grantees to report on their progress against agreed-on milestones, and we often support third-party evaluations of our grants. 'Productivity' is a tricky concept \u2014 we know that some of the research we support will fail, but that doesn't mean those researchers weren't productive. Even a failed study can contribute to the body of knowledge in its field and help point the way forward. We also provide funding to outside organizations to measure the effectiveness of our investments. \n               The foundation has, it has been reported, no guidelines for judging the ethics of companies its funds are invested in. Is that a tenable position for the world's leading philanthropic fund? \n             The foundation's outside investment managers have no involvement in our grant-making decisions, and our programme teams have no involvement in investment decisions. A detailed explanation of the investment strategy for the endowment is available on our website. \n               Is there a risk that your dominant role in funding global health will skew the research agenda? \n             It is not our goal to set the global health agenda, and we can succeed only if we help encourage greater funding from other donors. It is important to note that although our resources are significant, we account for a small portion of what is spent on health globally. For example, in 2005, our global-health budget was about $1 billion, whereas the National Institutes of Health's budget was nearly $30 billion. \n                     Biomedical philanthropy: State of the donation \n                   \n                     Biomedical philanthropy: Love or money \n                   \n                     Biomedical philanthropy: The money tree \n                   \n                     Giving it away \n                   \n                     Biomedical philanthropy, Silicon Valley style \n                   \n                     Global vaccine project gets a shot in the arm \n                   \n                     Gates ploughs millions into plan for assault on killer diseases \n                   \n                     Biomedical philanthropy \n                   \n                     Bill & Melinda Gates Foundation \n                   Reprints and Permissions"},
{"file_id": "447252a", "url": "https://www.nature.com/articles/447252a", "year": 2007, "authors": [{"name": "Erika Check"}], "parsed_as_year": "2006_or_before", "body": "Biomedical scientists want funding; private foundations want cures. Erika Check hears the joys and tensions that arise when the two hook up. Relationships are complicated beasts and they come with highs and lows. After the initial flush of attraction comes the getting-to-know-you stage, when the two parties realize they share common interests. Problems flare when things get serious. Each side has its own baggage, expectations and annoying habits, but needs to figure out how to live and work closely with the other. And once money gets involved, well \u2014 that's when things can get really convoluted. Robert Hughes, a postdoctoral researcher at the University of Washington in Seattle, had little idea of the challenges ahead when he first met leaders from the Hereditary Disease Foundation, a philanthropic organization in New York that focuses on curing the fatal neurological condition called Huntington's disease. Hughes' adviser asked him to attend a workshop the foundation had organized and, after spending two days talking with other scientists, Hughes was smitten. He now receives funding from the High Q Foundation in New York \u2014 a successor to the Hereditary Disease Foundation \u2014 and he finds them a gratifying but demanding partner. High Q expects its researchers to make clear contributions to the search for cures, and leaves little room for academic digressions. \u201cBut the benefit of working with High Q is that you get a sense that you're working as part of a community towards a common goal,\u201d he says. \u201cThere's a kind of excitement to that.\u201d Biomedical researchers have long flirted with private foundations and wooed them into bankrolling their research. Many of these foundations are small entities focused on rare conditions and supported by patient advocates and donations. And in the past, such groups were happy to dole out money, giving academic scientists a relatively free reign to investigate the fundamental roots of disease. But today the relationships between funders and scientists have become more passionate, needy and complex. Today's disease-focused foundations are becoming frustrated that basic research is not being converted into drugs for sick patients. They prefer to back 'translational' research, which aims to speed progress towards that goal and, to do this, they select research projects carefully and demand that scientists deliver concrete results on time. \u201cThe day of simply giving small grants is over. Now, you raise the money, you become educated about the science, and you take a seat at the table and become part of the conversation. It's absolutely new,\u201d says Susan Fitzpatrick, vice-president of the James S. McDonnell Foundation, based in St Louis, Missouri. Researchers, for their part, are keen to help. But they are also attracted by government grants and the greater financial security and intellectual freedom these tend to offer. Through interviews with scientists and the numerous foundations that fund them,  Nature  gained a broad picture of the trade-offs involved in these relationships \u2014 a picture best illustrated by putting one foundation under the microscope. \n               Fatal attraction \n             Jenny Morton first became seriously interested in Huntington's disease in 1991, when she was setting up her first independent laboratory at the University of Cambridge, UK. During her postdoctoral fellowship, she had done a little research on the devastating illness, in which patients' physical and mental faculties inexorably waste away. Then, at a meeting, Morton met a woman whose husband and father-in-law had died of Huntington's and whose two children also inherited the disease. \u201cI thought it was not only an academically interesting subject, but a terrible disease, so I decided to work on it,\u201d Morton recalls. Morton applied for money from the Medical Research Council \u2014 the primary public funder of basic biomedical research in the United Kingdom \u2014 but was turned down. Like other rare conditions, Huntington's disease simply doesn't have the same public priority as illnesses that claim many more victims, such as cancer. And because the potential market for treatments is so small, it is unattractive to drug and biotechnology companies. These factors are frustrating for patients who have these rare conditions, and for the researchers who want to study them. So, in 2002, a group of private donors set up the High Q Foundation. The donors had one goal: to convert the fundamental knowledge about Huntington's disease gained over the past half-century into a cure. \n               Sniffing out success \n             To achieve this goal, High Q and its sister group the CHDI \u2014 the successor to the Cure Huntington's Disease Initiative \u2014 now function more or less as a biotechnology company. High Q looks for disease targets \u2014 biological molecules that behave aberrantly in Huntington's. The CHDI looks for drugs that can correct the behaviour of those targets. For much of this work, the groups fund efforts in the biotechnology industry. But High Q also relies on hand-picked projects by academic researchers such as Morton to aid its quest for targets. \u201cNinety-five per cent of science works on the principle that the best thing to do is to let good scientists follow their noses,\u201d says Allan Tobin, a senior scientific adviser to High Q. \u201cBut this is a different attitude. We think we can direct the science.\u201d Morton received her first funding from the Hereditary Disease Foundation in 2002, and now receives around half of her costs from High Q. The money supports studies on mice with characteristic features of Hungtington's disease. Morton also collaborates with doctors, who help her to design her animal experiments so that they are relevant to finding a cure for patients. Morton sees many benefits to drawing much of her funding from a private foundation, rather than from a public source. For one, the organization is more willing than the government to take risks on new ideas. Five years ago, for instance, she and two other scientists brainstormed an idea for a computerized 'touch-screen' for mice. They hoped it would solve a major problem in the field by allowing them to evaluate the brain functions of animals who were physically incapable of running through a maze but could still touch a screen with their noses. Some colleagues sneered at Morton's idea because they thought it would be impossible to teach mice with movement disorders to use such a device. Thinking that their derision would be reflected in grant reviews from government agencies, she asked High Q for the money instead, and the foundation gave her a $350,000 contract for three years to make the touch-screen prototype. Last year, Morton and her colleagues showed that they could teach mice to touch the screen and earn food pellets (A. J. Morton, T. J. Bussey & L. M. Saksida  Nature Methods   3 , 767; 2006). Although foundations provide valuable support for new ideas, working with these organizations can also be challenging \u2014 and most scientists still prefer to work with at least some government grants. For Morton, an important concern is the maintenance of her mice, which costs about \u00a380,000 (US$160,000) a year. Breeding for experiments has to be planned months or even years in advance, so Morton needs to know that money will be available for those future studies and to keep staff on contracts to do them. This can be done with a typical grant from the Medical Research Council, which runs from three to five years, but High Q's awards are typically only for one year. \u201cShort-term funding when you are trying to keep a long-term project going is very difficult,\u201d Morton says. \n               Straight and narrow \n             With their new focus on delivering cures, today's foundations also insist that researchers deliver results or, in the case of high-risk projects, at least show that they tried. Some ask for detailed reports every six months or a year, and make further funding contingent on meeting strict milestones. High Q awards contracts rather than grants to ensure that scientists actually do the work they say they will do, rather than pursuing serendipitous tangents \u2014 a luxury that is allowed or even encouraged with government grants. The foundation also insists that researchers share the results of their work as quickly as possible, and discuss unpublished findings openly with colleagues at meetings. Some researchers find this type of oversight too onerous or controlling. They resent the loss of intellectual freedom and tend to stay away from groups such as High Q. Robi Blumenstein, a former businessman who now manages the operations of High Q and the CHDI, acknowledges that their business-like procedures can chafe some academics, but he makes no apologies. \u201cWe want people to have great ideas, but we need to get them done,\u201d he says. \u201cWhen we switched to this more rigorous model we acknowledged that we weren't going to get everybody to work with us.\u201d Because of the constraints, researchers who do work closely with foundations like these are usually driven by something more than pure intellectual curiosity. Many scientists relish the opportunity to participate in directed, translational research that could benefit patients. Foundations may also allow them to belong to a larger research community of scientists dedicated to the same pressing problem. In 2003, funding from High Q created the European Huntington's Disease Network, which was the first organization to unite Huntington's disease researchers across Europe and the thousands of patients that they study. \u201cIn Europe, there is no network of collaborative research like this in any other neurodegenerative disease,\u201d says Sarah Tabrizi, a doctor and researcher at University College London. This network is helping Tabrizi to find biomarkers that may predict the onset of the disease and launch clinical trials to test them. These benefits, she says, far outweigh the inconvenience of complying with short grant cycles and stringent milestones. But for some, the expectations and sacrifices demanded by foundations are too high. For these researchers, the traditional security of government money offers a more productive union. Tobin acknowledges that researchers need to make difficult calculations before plunging into a relationship with High Q. \u201cWe realize this is an experiment, and it involves a bunch of trade-offs,\u201d he says. \u201cThe key is that we respect each other.\u201d And that, of course, is good advice for any solid partnership \u2014 in science, as in other parts of life. \n                     Giving it away \n                   \n                     Biomedical philanthropy, Silicon Valley style \n                   \n                     High Q Foundation \n                   \n                     Disease Foundation \n                   Reprints and Permissions"},
{"file_id": "447256a", "url": "https://www.nature.com/articles/447256a", "year": 2007, "authors": [{"name": "Rex Dalton"}], "parsed_as_year": "2006_or_before", "body": "A controversial new idea suggests that a big space rock exploded on or above North America at the end of the last ice age. Rex Dalton reports. Around 13,000 years ago, North America was a busy place. Millennia of ice sheets had melted away, and humans crossed from Siberia to Alaska, spreading from the Canadian woods to the lush Carolina coastline. But after just two centuries of hunting mammoth, bison and horse, this 'Clovis' culture suddenly disappeared 1 , posing one of the great anthropological questions of the peopling of the Americas: why did the New World's most sophisticated hunters of the time suddenly vanish? Now, a team of researchers is invoking an out-of-this-world cause. On 24 May, at the American Geophysical Union's meeting in Acapulco, Mexico, some two dozen scientists will present multiple studies arguing that a comet or asteroid exploded above or on the northern ice cap almost 13,000 years ago \u2014 showering debris across the continent and causing temperatures to plunge for the next millennium. The history of geology is peppered with such notions \u2014 from the 'cosmic serpent' theory that the outer planets nudge comets onto a collision path with Earth, to the idea that an impact could have caused the collapse of Bronze-Age civilizations. Most of these theories have never become widely accepted by the scientific community. But the new team argues that its idea explains multiple observations: not only the climate cooling and the disappearance of the Clovis hunters, but also the near-simultaneous extinction of the continent's large mammals. \u201cThe magnitude of this discovery is so important,\u201d says team member James Kennett, a palaeoceanographer at the University of California, Santa Barbara (UCSB). \u201cIt explains three of the highest-debated controversies of recent decades.\u201d Not all will be convinced. Several leading hypotheses already explain each of these three events. A change in ocean circulation is generally thought to have brought about the onset of the millennium-long cooling, which is known as the Younger Dryas. This cooling might, in turn, have caused the Clovis hunters to disappear. And, if they had not previously been killed by disease or hunted to extinction, the big prehistoric beasts may also have been doomed by this change in climate. The new evidence comes in the form of geochemical analysis of sedimentary layers at 25 archaeological sites across North America \u2014 9 of them Clovis. Certain features of the layers, say the team, suggest that they contain debris formed by an extraterrestrial impact. These include spherules of glass and carbon, and amounts of the element iridium said to be too high to have originated on Earth. In addition, the rocks contain black layers of carbonized material, which the team says are the remains of wildfires that swept across the continent after the impact. Other experts are waiting to see how the data pan out. Vance Haynes, an archaeologist at the University of Arizona who has studied the Clovis people for more than 40 years, says the new theory could be viable. \u201cIf their geological analysis can be replicated by another group,\u201d he says, \u201cit would make it believable.\u201d \u201cTheir impact theory shouldn't be dismissed; it deserves further investigation,\u201d says Jeff Severinghaus, a palaeoclimatologist at the Scripps Institution of Oceanography in La Jolla, California, who studies ice cores. But he thinks it \u201cvery, very unlikely that such an event could have led to climate change\u201d, scepticism based on analysis of ice cores from Greenland, which show that some cooling had begun earlier 2 . \n               A rocky road \n             Duncan Steel, an Australian asteroid expert, has seen many groups try to connect impacts with major cultural changes. In this case, he says, \u201cthe researchers make at least a prima facie case for a link.\u201d Team members know they have a lot of scepticism to overcome. Many of them have signed on tentatively, saying they find the idea intriguing if not yet compelling. When Kennett heard of the theory, he says he thought: \u201cThis really makes sense. But it needs to be examined carefully.\u201d Earlier theories about what caused the Younger Dryas held that it was a temporary reversal of the global warming trend that brought the Clovis people to the Americas in the first place, across a land bridge from Siberia. At that time, North America's climate was warming and its land was being revealed by the melting of a massive ice cap that at its extreme, about 22,000 years ago, reached into what is now the heartland of the United States. Meltwater running off the ice cap fed a huge body of freshwater known as Lake Agassiz, which stretched across the upper Midwest. At some point, the idea goes, Lake Agassiz breached a natural dam and began rushing into the North Atlantic. The rapid addition of freshwater effectively slowed down the ocean conveyor system that transports heat north from the mid-latitudes 3 . The climate cooled for about a millennium, then gradually warmed again as the Atlantic returned to more normal conditions. \n               Fire and ice \n             In place of the Lake Agassiz breach, the new theory estimates that an object up to 5 kilometres across hit the northern ice cap, causing melting and the flood into the North Atlantic. No obvious crater remains behind \u2014 perhaps, suggest those proposing the theory, because the space rock exploded in the air, or because the ice cap was thick enough to mask the impact. The theory came together over several years, evolving with a number of twists. In Michigan in the late 1990s, archaeologist William Topping was working on a Clovis campsite known as Gainey. Topping had been exploring a theory that the Younger Dryas was caused by a nearby supernova, or exploding star. In 2001, he and Richard Firestone, a nuclear physicist at Lawrence Berkeley National Laboratory in California, published an article to that effect with data from the sediments at Gainey 4 . Critics immediately questioned the work, and after further analysis Firestone withdrew the claim. But Topping, still wedded to the supernova idea, disagreed; the two parted ways thereafter. Firestone eventually joined forces with Kennett, but both give primary credit to their colleague Allen West, a self-taught geophysicist in Arizona who collected many of the new samples. \u201cWest is the champion,\u201d says Kennett. Today, Kennett groans at the mention of the since-corrected 2001 article, but says he is fully committed to the quality of the new studies. \u201cI'm not going to jump on some project that's not supported by sufficient data,\u201d he says. \u201cAnd this project is data-driven.\u201d Some of the data come from a Clovis site known as Murray Springs, located in southeastern Arizona and long-studied by Haynes. The sediments there include the 'black mat' of carbon layers laid down just before the onset of the Younger Dryas. Beneath that layer, West found samples of carbon spherules, ranging from 0.15\u20132.5 millimetres across, some of which were hollow. Firestone and West argue that similar spherules have been found at a crater in Germany, and could be remnants of an impact. The team also retrieved samples of glassy-looking carbon, with textures they say suggest melting during an impact, as well as layers enriched in iridium, an element not found in abundance on Earth. And team member Luann Becker, of UCSB, who was previously involved in controversial claims that an asteroid caused the Permian-Triassic extinction about 250 million years ago 5 , says in an abstract for the Acapulco meeting that she has found fullerenes in layers at Clovis sites \u2014 possible support for an impact event. Team members say that they have found at least some of these markers in 25 widely scattered sites across the continent. Kennett, for instance, found spherules in 12,900-year-old sediments on Santa Rosa Island, which lies off the Californian coast and is the site of the oldest human bones found so far in North America. Other potential evidence for an impact at the time comes from neighbouring San Miguel Island. \u201cThese gave us the best examples from the West Coast,\u201d says Kennett. In the east, the team scrutinized a series of circular depressions that run from Georgia to Delaware. Known as the Carolina Bays, the way in which they were created has been the subject of much debate; impacts are one idea among many. Firestone thinks the bays may be craters left by debris from a Clovis-era explosion, although others remain to be convinced. \n               Decline and fall \n             Near the bays, in South Carolina, the team has also been looking for evidence at a Clovis site known as Topper. Clovis points are abundant in this region. But Albert Goodyear, an archaeologist at the University of South Carolina at Columbia, will report in Acapulco that there was a sharp decline in Clovis points at Topper during the Younger Dryas. This, the team argues, is evidence that at that time humans went through some sort of population collapse. But other archaeologists say they have no evidence of a similar decline in other Palaeoindian populations; even as the Clovis culture was disappearing, other cultures arose in its place, for reasons not entirely understood In the end, the Acapulco meeting may cause other scientists to re-examine their evidence for what was happening in North America at the end of the last ice age. For instance, Paul Mayewski, an ice-core expert at the University of Maine in Orono, is investigating ice cores from Greenland that show a massive burning event occurred around the start of the Younger Dryas. Others will undoubtedly start digging through whatever records they have of the time. And, at the end of the Acapulco session, the team will host a dinner \u2014 throwing open the doors for critics and supporters alike to begin talking through the theory. It will be interesting to see where it goes. \n                     Who were the first Americans? \n                   \n                     Meteor theory gets rocky ride from dinosaur expert \n                   \n                     Comet impact theory faces repeat analysis \n                   \n                     Hot tempers, hard core \n                   \n                     Archaeology: The coast road \n                   \n                     Center for the Study of the First Americans \n                   Reprints and Permissions"},
{"file_id": "447768a", "url": "https://www.nature.com/articles/447768a", "year": 2007, "authors": [{"name": "Dan Jones"}], "parsed_as_year": "2006_or_before", "body": "Is there wisdom to be found in repugnance? Or is disgust 'the nastiest of all emotions', offering nothing but support to prejudice? Dan Jones looks at the repellant side of human nature. In 1997, Dolly the sheep unleashed bioethical responses of every conceivable flavour, from the ruminatively utilitarian to the emotionally outraged. Leon Kass, a bioethicist at the University of Chicago, Illinois, who chaired President Bush's Council on Bioethics from 2002 to 2005, combined scholarly and visceral responses in a much cited essay entitled 'The Wisdom of Repugnance'. He did not go quite so far as to say that the revulsion reportedly evoked by the prospect of human cloning was in itself an argument against such endeavours. But he got fairly close: \n               \u201cWe are repelled by the prospect of cloning human beings not because of the strangeness or novelty of the undertaking, but because we intuit and feel, immediately and without argument, the violation of things that we rightfully hold dear... Shallow are the souls that have forgotten how to shudder.\u201d \n               1 \n             Kass had aired the same worries at the advent of  in vitro  fertilization treatment in the 1970s, and similar reactions continue to bedevil biomedical developments today, such as in the debate over the creation of human\u2013animal hybrids for stem-cell research. But in recent years, a loose band of interdisciplinary psychologists and neuroscientists has been putting together a new picture of the emotion that underlies such responses to biology: disgust. Drawing on both evolutionary theory and moral philosophy, their work casts doubt on the idea that disgust embodies a deep-seated wisdom. Instead it provides an emerging portrait of an evolutionarily constrained emotion that is a poor guide to ethical action. These scholars see disgust as a basic emotion that, like fear, anger, sadness and joy, is found across all cultures. All around the world pus, maggots, rotting food and scavenging animals such as rats produce the distinctive facial expression of disgust: nose wrinkled, mouth agape, lips raised. When severe, the feeling of revulsion can be accompanied by throat clenching, nausea and vomiting. In evolutionary terms, the adaptive value of such reactions seems to be to prevent people from eating contaminated foodstuffs and to get rid of any they have ingested. Disgust is related to bodily purity and integrity, with things that should be on the outside \u2014 such as faeces \u2014 kept out, and things that should be on the inside \u2014 such as blood \u2014 kept in. Although the experience of disgust feels primal, the emotion does not seem to be widespread in other animals. Many species exhibit distaste in response to the sensory properties of food \u2014 such as sourness and bitterness \u2014 and a monkey, cat or human infant might spit out something disagreeable. But only humans beyond infancy will reject food on the basis of where it might have been and what it might have touched (see  'Gut reactions' ). \u201cDisgust is a much more cognitive and emotional feeling than distaste,\u201d says Paul Rozin, a psychologist at the University of Pennsylvania, Philadelphia, who has pioneered research on the subject. \u201cIt involves understanding what a food is and where it comes from.\u201d Part of its complexity is that disgust carries with it the notion of contamination; otherwise-edible food that has been touched even fleetingly or innocuously by something viewed as disgusting will be avoided. Most people, Rozin has found, won't drink juice that has been whisked with a sterilized cockroach or drink out of a meticulously cleaned bedpan. Rozin suggests that it is the cognitive sophistication of this idea that explains why the emotion is absent in other animals and infants. \n               Basic instinct \n             But what is the link between visceral or 'core' disgust \u2014 the feeling you get when you encounter an unflushed toilet \u2014 and a disgusted reaction to something much more abstract, such as the idea of animal chromosomes in a part-human embryo? A clue is the language of moral indignation itself. \u201cAll cultures and languages that we have studied have at least one word that applies both to core disgust (cockroaches and faeces) and also to some kind of social offence, such as sleazy politicians or hypocrites,\u201d says Jonathan Haidt, a psychologist at the University of Virginia in Charlottesville and a former student of Rozin's. People labelled as disgusting in this way evoke fears of contamination just as rotting food does. When Rozin asked people about the prospect of wearing Hitler's carefully laundered sweater, most didn't feel at all comfortable with the idea. \u201cThe contamination of disgust is generalized to moral issues, and that's a very deep feature of disgust,\u201d he says. \u201cIf it was just metaphorical then Hitler's sweater wouldn't be so offensive.\u201d Paul Bloom, a psychologist at Yale University is sceptical. He agrees that disgust drives some moral judgements, but points out that they are mainly those relating to behaviour that involves bodily fluids or contact \u2014 gay sex, for instance \u2014 rather than more abstract issues. Just as people don't really lust for a car or genuinely thirst after knowledge, suggests Bloom, they don't really feel disgust at more abstract issues. \u201cWhen we say something like 'This tax proposal is disgusting', we're using a metaphor,\u201d he says. \u201cIt's a very powerful metaphor, but it doesn't elicit the same disgust or nausea as primary disgust elicitors such as faeces and body fluids.\u201d But Haidt thinks he has found clues pointing to a physiological reality for moral disgust. Whereas anger pushes the heart rate up, being viscerally disgusted makes it drop. With his student Gary Sherman, Haidt showed people hooked up to a heart monitor video footage of morally negative but not viscerally disgusting behaviour, such as an American neo-Nazi meeting. The participants said that the video triggered disgust and anger, and on average their heart rates fell, not rose. What's more, those who reported increased clenching in their throat had a greater drop in heart rate, making the link with core disgust look stronger. \u201cWe think that this is the first physiological evidence that socio-moral disgust really is disgust and not just metaphor or anger,\u201d says Haidt of the as yet unpublished work. Brain imaging studies might also point to an overlap between core and moral disgust. Jorge Moll, a cognitive neuroscientist at Rede Labs D'Or, Rio de Janeiro, Brazil, used magnetic resonance imaging (MRI) to monitor the flow of blood in the brains of 13 healthy adult volunteers as they mulled over situations evocative of core disgust and those that elicit self-reported moral disgust or indignation 2 . He found that core and moral disgust recruit overlapping brain areas, particularly the lateral and medial orbitofrontal cortex, suggesting that the emotions are related. These regions of the brain are activated by unpleasant sensory stimuli, and they connect with other emotion-related areas, such as the amygdala. As well as showing overlap, Moll's work suggests that core and moral disgust also activate some distinct areas. They produced similar activity in the posterior orbitofrontal cortex, but moral disgust produced greater activity than core disgust in the more evolutionarily recent anterior region, which some think is involved in more abstract emotional associations. \n               Moral foundations \n             Moll's original work can be criticized because some of the 'moral disgust' scenarios also featured elicitors of core disgust, such as rats. But in a paper to be published in  Social Neuroscience 3 , Moll and his colleagues created 'cleaner' scenarios that describe pure moral violations without a visceral element, and arrived at much the same results. And last October, the team presented evidence suggesting that the lateral orbitofrontal cortex was also activated when volunteers made decisions on whether to oppose charitable organizations that had moral agendas different from their own \u2014 on abortion, gun control or the death penalty, for example 4 . It seems that just thinking about some sorts of moral conflict is enough to get parts of the brain implicated in disgust ticking over. Even if moral and visceral disgust are not the same emotion, visceral disgust will sometimes affect ethical judgements. Susan Fiske and Lasana Harris, psychologists at Princeton University in New Jersey, have used MRI to probe the disgust evoked by images of people such as drug addicts or the unkempt poor and homeless 5 . Their findings seem to support an unsurprising but depressing conclusion. Not only did the amygdala and insula fire up (taken to indicate fear and disgust, respectively), the medial prefrontal cortex, which is usually active when thinking about people and social situations, as opposed to thinking about objects, was less active. This can be interpreted as evidence that disgust goes some way to trumping empathy and compassion. \u201cWhen we respond to a homeless person with disgust, we avoid considering the person's mind,\u201d says Fiske. \u201cWe treat the person as equivalent to a pile of garbage.\u201d Also disturbing is the way in which disgust can play a similar role in interactions with people who offer none of the objective correlates of visceral disgust. Its role may be tied to the evolutionary process, though, in that disgust is broadened out from the original purely hygienic concerns to the more general moral role that Rozin, Haidt and their colleagues see it playing today. Visceral disgust is in essence an emotion of distancing \u2014 of avoiding or expelling the contaminant. Marc Hauser, a psychologist at Harvard University who has worked with primates, suggests that this aspect of disgust made it a suitable raw material for evolution to work with in building up instinctive distinctions between the in-group and the out-group. The force with which such distinctions are felt may promote survival, and thus have adaptive value in the face of natural selection. Drawing distinctions between in-group and out-group \u2014 us and them \u2014 \u201cis not something the human line invented\u201d, says Hauser. It is seen in various social animals. But humans are peculiarly preoccupied with these distinctions. Some, such as the football team someone supports, are not widely accepted by outsiders as carrying moral weight. Others, including our political and religious affiliations, are value-laden to the core. \u201cThe moral faculty, which deals with moral problems, is going to have a deep connection with issues of 'in' versus 'out',\u201d claims Hauser. Evolution suggests that the human moral faculty \u2014 the psychological systems that make judgements about right and wrong, what's permissible and what isn't \u2014 was cobbled together from pre-existing brain systems over millions of years of biological and cultural evolution. Along the way, it latched onto disgust as a useful tool. \u201cThe experimental data point to the possibility that our disgust system might have been adapted by evolution to allow us to reject or disapprove of abstract concepts such as ideologies and political views that are deeply influenced by culture, as well social groups associated with 'disgusting' concepts,\u201d says Moll. Some theories of the evolution of human cooperation and altruism suggest that inter-group conflict was a potent force driving cooperation within groups, the most cooperative being the most successful at surviving. In making symbolic distinctions between us and them visceral, disgust could potentially foster greater cohesion within groups by bringing people together in defence against a common out-group. \u201cDisgust works for the group as it does for the individual \u2014 what is in the group is 'me' and what is not is 'not me',\u201d says Haidt. \u201cWhere core disgust is the guardian of the body, moral disgust acts as the guardian of social body \u2014 that's when disgust shows its ugliest side.\u201d \n               Repulsive alliances \n             Propagandists throughout history have been quick to pick up on the possibilities raised by the blurring of visceral disgust into a weapon for the in-group/out-group border patrol. Nazi propaganda depicted Jews as cockroaches and rats; Hutu instigators denigrated Tutsis as cockroaches during the Rwandan genocide. As with the sight and smell of a dispossessed street person, identifying the enemy with an object of disgust throws up strong emotional barriers to empathy. \u201cThat's why I say that disgust is the nastiest of all emotions,\u201d says Hauser. \u201cOur moral disgust/indignation brain network is the source of prejudice, stereotyping and sometimes outward aggression,\u201d says Moll. Fiske agrees, saying the picture of disgust painted by data from psychology and neuroscience should make us think twice about drawing on revulsion as a basis for our personal moral judgements. History seems to bear this out. Women (especially menstruating ones), the mentally and physically disabled, and inter-racial sex have all been viewed with disgust, and are still viewed as such by some. But few people in liberal societies today would defend such attitudes and many have genuinely ceased to feel them. If disgust wasn't a good moral indicator then, why should it be now? Some defenders of disgust accept at least some of the implications of the current research. Kass emphasizes that he does not claim that repugnance is a sufficient guide in moral matters. \u201cIt is at most a pointer, and of course the objects of disgust are to a considerable extent and in many cases culturally malleable,\u201d he says. At the same time, Kass is sceptical of throwing out the baby with the disgusting bathwater. Although he acknowledges that disgust has historically been used and abused to persecute ethnic and religious minorities and to promote the mistreatment of women, he doesn't rule out a role for disgust in morality: \u201cIt does not follow from these examples that repugnance about, say, the eating of human flesh (or excrement) or father\u2013daughter incest is unlinked to the moral/aesthetic truth about these practices.\u201d One way forward is to at least recognize the part played by disgust and to be vigilant in its surveillance. And in the special case of bioethics, it also means thinking carefully about what is actually being proposed, rather than concentrating on outrageous scenarios that elicit emotion even while straining credulity. \u201cIt is almost impossible to consult our moral intuitions on bioethical questions when we have so little experience of their outcomes,\u201d says Haidt. \u201cWe can make up all sorts of sci-fi futures but it's not even worth thinking about them because our intuitions are just too unreliable.\u201d No one is arguing that emotion has no role in moral judgements. Indeed, the whole basis of this new approach is to argue that emotion is inseparable from morality, and that feelings matter deeply. But that does not mean all emotions are created equal. The distinctions that disgust has evolved to police, those between the in-group and out-group, and to some extent the sacred and profane (see  'The basis of belief' ), are much more subjective than the aspects of life dealt with by the other emotions that Haidt and his colleagues see contributing to morality. \u201cDisgust didn't evolve to track things that we would normally consider morally important, unlike empathy, which is triggered by the real pain or suffering of others,\u201d says David Pizarro of Cornell University in Ithaca, New York. Still, disgust is an emotion we are stuck with. The challenge, suggests Hauser, is to make people more reflective about what they say and think. He cites the success that advocates of political correctness have had in lowering the prevalence of casually sexist and racist language. Moll suggests optimistically that cultivating cultural and personal values of tolerance and empathy could function as an antidote to the toxic effects of disgust. This may all sound a little wishy-washy; the implications of this research, as well as the research itself, deserve critical examination from well beyond the confines of the small group of scientists currently involved. But it is hard not to conclude that, by thinking less with our guts, and more with our heads and hearts, we might be able push back the boundaries of our moral world. See Editorial,  \n                     page 753 \n                   . \n                     Well-being research: A measure of happiness \n                   \n                     The Chomsky of morality? \n                   \n                     A search for meaning \n                   \n                     Is repugnance wise? Visceral responses to biotechnology \n                   \n                     Explore your disgust potential on yourmorals.org \n                   \n                     Jonathan Haidt's homepage \n                   \n                     Marc Hauser on edge.org \n                   \n                     Hauser's Online Moral Sense Test \n                   Reprints and Permissions"},
{"file_id": "446971a", "url": "https://www.nature.com/articles/446971a", "year": 2007, "authors": [{"name": "Rex Dalton"}], "parsed_as_year": "2006_or_before", "body": "From Florida to California the story is much the same. Local political leaders, noting that there are too few doctors to serve their state's growing population, are arranging to build new medical schools. These, they say, will perform cutting-edge research and generate successful biotechnology companies, as well as training the much-needed doctors. The projects are planned on the basis that they will do research that attracts funding from the National Institutes of Health (NIH), the main US biomedical-research agency. But the NIH's budget, which was doubled from $13 billion to $27 billion by 2003, has since stagnated \u2014 raising questions about where the funds will come from. Critics also argue that there may not be enough qualified students who will want to enrol at the medical schools. The doubters say that there's little likelihood that the new schools will generate clusters of biotech companies and the associated economic benefits promised by their champions. In short, the critics charge, taxpayers in the cities in question have been hoodwinked into backing expensive white elephants that will weaken the states' existing medical schools, leaving medical education and research worse off than they were before. \u201cThis is a very expensive experiment,\u201d says Joseph Cortright, a consulting economist in Portland, Oregon, and co-author of  Signs of Life , an analysis of biotechnology clusters published by the Brookings Institution in Washington DC. \u201cThere is no way to determine if it will succeed.\u201d Medical-school campuses, some costing several hundred million dollars, are currently planned or being built in Athens, Georgia; Houston, Texas; three different cities in Florida; Phoenix, Arizona; and Riverside, California. Of the various states pursuing this policy, Florida has proved particularly keen. During his time in office, former governor Jeb Bush championed initiatives that will expend more than $1 billion on building and equipping biomedical-research institutes in the state (see  page 1112 ). \n               Class act \n             In the past decade, the Florida legislature has created three new medical schools \u2014 two of them in the past 12 months. Florida International University in Miami and the University of Central Florida in Orlando will each open medical schools that will accept their first classes in 2009. \u201cFlorida is a rapidly growing state, with an increasingly affluent and ageing population that accesses health care frequently,\u201d says neuroscientist Terry Hickey, provost at the University of Central Florida. \u201cWe are convinced there is a physician shortage; by 2020, that situation will be dire.\u201d Officials at established Florida medical schools are not impressed by the plan, however. Abdul Rao, vice-dean for research at the University of South Florida in Tampa, says existing institutions and universities are being undercut. \u201cWe need continued support for existing medical schools,\u201d Rao says. When researchers receive a grant from an agency such as the NIH, the university to which they are affiliated gets additional funds to cover overheads. But for the past three years, the NIH's total budget for these operational costs has remained flat at $5.9 billion. That means the new medical schools that come online are simply increasing competition for fixed resources. Some specialists also take issue with the largest single political selling point for the new schools \u2014 the need for more doctors in states with booming populations. Officials at the University of California, Riverside, for instance, argue that the planned $500-million medical school set to open in 2012 is needed to help reverse a shortage of doctors in the surrounding area. This point of view has some sympathizers. Edward Salsberg, a veteran healthcare planner who now directs the Center for Workforce Studies at the Association of American Medical Colleges in Washington DC, says that the United States needs to increase the number of doctors it trains by 30% by 2020. But David Goodman, a paediatrician at Dartmouth University in Hanover, New Hampshire, who leads a project that assesses US healthcare needs, takes issue with this. He says that the problem is not how many doctors there are, but how they are distributed. \u201cThere has been fantastic spinning of the need for more physicians,\u201d he says. In the United States, a private doctor can practise anywhere and, as a result, more tend to congregate in high-income or desirable communities, leaving some areas underserved. Miami is a prime example, as about three-quarters of its residents live in areas that are medically underserved, according to Goodman's analysis (D. C. Goodman  et al .  Health Affairs   25,  521\u2013531; 2006). But because of its pockets of wealth, he says, Miami actually has far more doctors than some other US cities. It has 40% more per capita than Minneapolis, Minnesota, for example. But on average, people in Miami live shorter lives, are subjected to more treatments and are less happy with their care than patients in Minneapolis, he says. \n               School of thought \n             Goodman claims that training yet more doctors will just add to the excess without addressing the needs of the medically underserved. \u201cThe free market doesn't work for physician distribution,\u201d he says. \u201cYou have to have incentives to have physicians practise in needy communities.\u201d History also suggests that when states build medical schools, the resulting graduates won't necessarily practise in the vicinity. Doctors in the United States typically take speciality residencies that last up to six years, and can be anywhere in the country. In 2005, for example, Florida State University in Tallahassee lost half its first medical class to residencies in other states. And another objective \u2014 that of broadening the diversity of communities from which medical students and researchers are drawn \u2014 is problematic, too. Public-health specialists all agree that recruits need to be drawn from disadvantaged communities in order to provide a better service for these groups. In Florida, the greatest needs are in African-American and Hispanic communities. But among Florida State University's first graduating class of 27, there were only two African-Americans. And among the 50 medical students graduating this spring, only three are African-Americans and three Hispanics. To fill up places, many medical schools in the states in question already draw students from outside the United States and pay their full tuition and stipend costs. In Florida State University's PhD programme, which began in 2004, half the 21 students are undergraduates from overseas universities. \u201cThe number of applicants was very low; kind of pathetic,\u201d says microbiologist Myra Hurt, associate dean of research at Florida State University's medical school. \u201cBut we are committed to developing a very robust research programme.\u201d \n               Joint venture \n             In Phoenix, Arizona, a new satellite campus of the University of Arizona College of Medicine, based about 200 kilometres away in Tucson, is taking shape. City leaders hope that the school, which is now recruiting its first class of 24 students, will help to invigorate downtown Phoenix, strengthen biomedical research at Arizona State University (see  page 968 ) and spawn a biotechnology hub. The Phoenix school will be operated under a partnership between the two state universities. Arizona governments have so far put more than $130 million into the project, which is projected to cost around $470 million. In five years' time, the campus hopes to accommodate a total of 450 medical students, nearly 500 PhD students and almost 300 postdoctoral researchers. \u201cIf you want to crack into the top 50 US medical schools, you have to grow,\u201d says Phoenix lawyer Gary Stuart, a member of the Arizona Board of Regents. \u201cBut the University of Arizona medical school in Tucson couldn't grow, because it is land-, patient- and resource-locked. We wanted to make something bigger, better and faster. The Phoenix campus made sense at every level.\u201d Yet academics at the existing College of Medicine in Tucson view the project with some apprehension. \u201cEverybody has concerns about how they will pay for this huge effort,\u201d says biologist Stuart Williams, who recently resigned his position as chairman of college's bioengineering department to take up a post at the University of Louisville in Kentucky. Raymond Nagle, a pathologist and former deputy director of Arizona's only federally designated cancer-research centre in Tucson, shakes his head when he hears news reports pledging that the new campus will create a biotechnology industry in Phoenix. \u201cI wish them well; I hope they succeed,\u201d he says. \u201cI just haven't seen the evidence they will.\u201d And Joe Panetta, head of the consultancy BIOCOM in San Diego, says it is too late for the areas around the new medical schools to compete with the likes of San Francisco, Boston and Research Triangle in North Carolina. Arizona lacks the venture capital, industrial partners and specialized legal and scientific services. But legislators are optimistic that one day their cities can host all of these services with a brand new medical school at their core. \n                     Dartmouth Atlas of Health Care \n                   \n                     AAMC Center for Workforce Studies \n                   \n                     Center for Health Workforce Studies \n                   Reprints and Permissions"},
{"file_id": "446850a", "url": "https://www.nature.com/articles/446850a", "year": 2007, "authors": [{"name": "Declan Butler"}], "parsed_as_year": "2006_or_before", "body": "You've heard what the presidential candidates think the challenges facing science in France are.  Nature  also canvassed opinion across the French research spectrum: from young researchers to reformers and industrialists. Declan Butler reports. This online version contains additional material not published in print. There is also a guide to the  issues and acronyms .  \n               Pierre Chambon \n             \n               Biologist at the Institute of Genetics and Molecular and Cellular Biology in Illkirch near Strasbourg, of which he was formerly director. \n             The common assertion that French research is doing badly is untrue; it's doing badly in some sectors, with life sciences the main concern. The French research system hasn't been organized in 40 years. There's duplication in biology across the CNRS, INSERM, INRA and the universities that's damaging competitiveness. We need a single life-sciences agency, plus perhaps a second for more applied work, to bring together the best life scientists and give them the means to do top-quality research. Biotech companies and universities compete for the best scientists worldwide. But France's civil-service pay scales mean that a biologist earns the same as a sociologist or anthropologist. This makes it impossible to attract the world's best biologists, but life sciences in France cannot survive on French researchers alone. In France, most scientists start and finish their careers in the same organization, whether this is a research agency or a university. There are two separate corps of scientists, some teaching and researching, some doing only research. This is inefficient, because you want less effective researchers doing more teaching, and the best researchers doing less, according to their success in winning grants. For this to work, we need to have just a single corps of researchers, all attached to universities, with the research agencies transformed into research councils. The efficiency of French research won't improve without more flexibility about who gets what during their career. France wants top research but rejects \u00e9litism; that's not possible. Biology also lacks a strong industrial lobby in France to push for greater public support. When the US National Institutes of Health's budget doubled in [the] five years [to 2003], France's was flat. The French drug industry is weak, comprising mainly Sanofi-Aventis [a multinational with much of its research in the United States] and a few other companies. In Strasbourg, our institute was the first in Europe for molecular geneticists, and in the top 10 worldwide. But soon we will have lost all our top foreign scientists. This is not just because of the salaries; people would prefer to stay in France because of the quality of life. But they feel they are wasting their time having to look right and left for scraps. They can get better working conditions, and salaries, abroad. There's no point in creating more posts unless people are given the means to do top-quality research, and there's no point in increasing funding if we don't reform our structures, otherwise it is like watering sand. Unless we attack the problems of the research agencies, modernizing French research will be mission impossible. We, the researchers, want reform. \n               Edouard Br\u00e9zin \n             \n               Physicist at the Ecole Normale Sup\u00e9rieure in Paris and ex-president of CNRS and the French Academy of Sciences. \n             The low attractiveness of science and engineering careers is a big problem in France. The young are increasingly turning to other careers. Moreover, the best students aim not to go to university or to get a PhD, but to graduate as an engineer from the \u00e9lite Grandes Ecoles, as this offers better career prospects. So although at present French research is strong in many fields, I'm not at all sure that this will still be the case in 10\u201315 years time. The high teaching loads imposed on young scientists starting work at a university prevent them from doing research during the very years when they should be cutting their research teeth. They are recruited on the basis of research talents, and we then annihilate their chances of establishing themselves. New recruits need to be freed up for research. France has a chronic problem building links between public and private research. The concept of wealth creation in public research is perhaps less naturally engrained in our culture than in the United States and Britain. But I'm also struck by the fact that in French industry those in management rarely have any research training, as most are graduates of the Grandes Ecoles. Companies also tend to have a negative view of people with a research background \u2014 an engineer from one of the Grandes Ecoles who also has a PhD might be best advised to hide that fact in job interviews. The relationship of universities to the Grandes Ecoles is one of many important questions facing the university system. There are important issues that need to be tackled to reform university governance, and to give them greater autonomy than they have in today's homogenized national system. But there hasn't been a major reform of French universities for decades, so all of these questions are off the radar. The new ANR has its pros and cons. It's beneficial in that it makes it easier for young research teams to attain funding, but the central government still decides research priorities and nominates management teams. Compare this with how its German counterpart, the DFG, is run; there the management is largely elected by the scientific community on the grounds of their scientific expertise. They are two different worlds. That said, the French scientific community also needs to reform itself. The research agencies and universities elect various committees and evaluation panels, and I've long fought with the community for people to be elected on merit, and not on trade-union criteria. When I receive a voting bulletin, it doesn't give any indication of whether the candidate has even published in the past 5 years, just their trade-union profession of faith. It's incredible, ridiculous: a system from the Middle Ages. \n               Ma\u00efwenn Corrignan \n             \n               President of the Young Researchers' Confederation and a sociologist at the University of Rennes 2 in Brittany. \n             Last year's decree that the PhD is a professional qualification was a real advance for young scientists here. It put an end to the widespread practice \u2014 particularly by research charities \u2014 of paying postgrads a minimal salary without benefits (or no salary at all). This change forces research agencies to provide contracts and pay social security and pension benefits, and also means postgrads on short-term contracts subsequently qualify for unemployment benefit. The financial uncertainty of PhD students and the lack of recognition of the value of a PhD in recruitment was one of the main factors behind the street demonstrations of 2004. We don't want a postdoc system. It won't work in France, which has a culture of full-time employment; recruiters have a negative view of scientists with multiple short-term contracts as perpetual students. We need a change in mentalities. Many scientists would prefer not to stay at one research organization for their entire career, but there are too few bridges to allow greater mobility between research agencies, universities and industry. Better career guidance is also needed, so that the young are aware of opportunities beyond the research agencies and universities, both in industry and outside research. Things are improving; more than 20% of PhDs now go into industry, and that trend is increasing. Companies are also beginning to recruit, and value, those with PhDs. The young aren't disinterested in research; it's the poor working conditions that put them off. \n               Alain Trautmann \n             \n               Cell biologist at the Cochin Institute in Paris and an instigator and former spokesman for the movement 'Save Research' that led protests in 2004. \n             In 2004, the scientific community in France organized an emergency nationwide science summit, and proposed a rash of reforms. But the government ignored them, and pushed ahead with its own. We are not against reform, but want changes to be decided in consultation with the science community. Successive governments have instead tried to impose unworkable reforms. The current government is trying to impose an American system on France, without taking our history into account. They act as if the CNRS and the Grandes Ecoles, the main elements of the French system, don't exist. The fact that the Grandes Ecoles divert the brightest young people away from science is catastrophic. We need to evolve the French system, taking such realities into account. In 2004 we proposed that, to make science more attractive to the young, the government tie its generous tax relief to companies not just on the basis of their degree of investment in research but also according to the number of trained scientists with PhDs they recruited. This could create change, and would not cost the state any extra, but the government rejected it. Ideologically, it was opposed by the lobby of the Grandes Ecoles, whose engineering graduates dominate the state apparatus. The government also wants to make the universities the centrepiece of the research system. But France is currently one of the worst-placed countries in terms of investment in its universities. The universities that perform well are those that have joint laboratories with the CNRS. I'm not for maintaining the CNRS just because it already exists, but it's naive to believe, as some in government do, that one can simply abolish the CNRS overnight. Doing so without first reforming and reinvesting in the universities would just make the system worse. \n               Andr\u00e9 Choulika \n             \n               Chief executive of Cellectis, a biotech spin-off from the Pasteur Institute in Paris. \n               France's competitiveness in research and innovation has emerged as an election issue in the 2007 presidential campaign. Significant investment in the life sciences in 2005 \u2014 \u20ac2.4 billion [US$3.2 billion], or 25.7% of the 2005 civil research budget \u2014 has so far failed to deliver the anticipated socioeconomic benefits. The money has not been spent optimally. The multiplication of publicly supported research organizations and structures has resulted in a failure to concentrate funds behind ambitious projects, and no clear policy for increasing competitiveness. The creativity and talent of French researchers is undisputed, but the quality of the country's research has deteriorated in recent years. The ANR is a step in the right direction for improving research, provided it is given transparent governance and true independence. There is a need for greater translational research, and a culture that recognizes and rewards those whose research is outside the mainstream and ahead of the curve. It seems timely to recall De Gaulle's famous quote: \u201cWe have plenty of researchers who seek, what we're seeking is researchers who find.\u201d The greater the confidence our leaders have in the quality of their researchers, the greater the latter's achievements will be. \n               Philippe Froguel \n             \n               Geneticist at Imperial College London (ICL), a French expat. \n             I quit France for Britain in 2000, because I was frustrated by a system that refuses to recognize and reward success, and in which promotion can depend more on political skills than on scientific merit. Funding is not sufficiently tied to results, and evaluations fail to distinguish excellent from average research. Money is spread thinly among many institutions and fields, with insufficient focus on strengthening the best campuses and teams. As chair of genomic medicine at ICL, I've discovered that a world-leading university can offer a unique range of opportunities to the most entrepreneurial academics. This is almost impossible to imagine in France, where universities and research institutions lack power, and are overly centralized and bureaucratic. Britain's academic world is unashamedly \u00e9litist, with constant evaluation of projects and outcomes. Institutions have long-term visions, and recruit a critical mass of the best scientists in order to achieve them. At ICL, individual success is rewarded in salaries and bonuses. French universities should be given autonomy from the central government, which currently nominates chairs in medicine. Funding should be focused on just a handful of the best medical schools and university hospitals on the basis of a regular research assessment akin to the UK Research Assessment Exercise. And a single French Institute of Health should be created to end the inefficient competition among the many research players in the life sciences. \n               William Rutherford \n             \n               Biophysicist at the Atomic Energy Commission's Photosystem II: Photochemistry, Water Oxidation and Regulation Institute in Saclay, near Paris; a British expat in France. \n             The French national sport is whingeing. They are very good at complaining about what's wrong with the research system. But they overstate the case. I did my PhD in London, and postdocs at the University of Illinois [at Urbana-Champaign] and the RIKEN Institute of Japan, and I think the French system is great in its own way. The system is very different from elsewhere. There are peculiarities, such as the system of the Grandes Ecoles, and also much greater stability, in particular when starting out, because of the rolling grants funding system, and better job security. Coming here after my postdoc wanderings, I found it all very weird. The system certainly has its weaknesses, and the French focus on these. But the truth is that there is much that is positive about the French system. Some things are better in the United States and Britain, but others are much better here. I really do believe that not having the huge pressure to publish before the next grant deadline allows you to do better science. I also often see my US competitors, who are extremely good, go through patches in which they are less so. It's often to do with the loss of a grant or staff; they lose their expertise, and take time rebuilding it. Our lab, probably one of the best in the world for photosynthesis, doesn't have that problem. France is a great place to do what we do. There are downsides. Where should I start? France's legendary red tape is true bureaucracy for bureaucracy's sake. If you don't let it get to you, you can get your job done. Fixed salaries across the board are also a painful point, especially early in one's career. Fixed salaries probably hit recruitment in sectors such as biotech, but are less of an issue in basic research. I could earn much more in Britain or the United States, but here I can do the science I want. Our lab also has no difficulty attracting foreign researchers. People love the lifestyle in France. In reforming research, France must be careful not to throw the baby out with the bathwater. Much as I approve of certain aspects of the new ANR's competitive grant system, I certainly don't think that France should adopt an Anglo-Saxon system; the diversity of international systems is a richness that we should preserve. I would hate to see French research lose its unique character in an ill-advised attempt to copy the United States. \n               The following material is web-only additional text \n             \n               Bertrand Monthubert \n             \n               Mathematician at Paul Sabatier University in Toulouse and current president of \u2018Save Research\u2019. \n             French researchers keep being told they should adopt an Anglo-Saxon system for research funding, yet if you look at Britain\u2019s share of world publications, its drop is, in fact, the same \u2014 13% from 1999 to 2004. That said, we are not opposed to funding research on the basis of competitive projects. But we are opposed to the government\u2019s eagerness to use project funding for short-term strategic research, while at the same time cutting back on the research agencies that offer labs long-term stable funding. INSU [the National Institute for Earth Sci-ences and Astronomy], for example, must plan expensive long-term ocean expeditions, and carry out its own peer review of missions. But current budgetary difficulties mean that it is having problems putting together expeditions for France\u2019s contribution to the International Polar Year. You can\u2019t do this sort of research with 3-year project grants. The comparison of the French ANR to the US National Science Foundation [NSF] is inaccurate; the NSF often provides rolling funding for labs, much as the CNRS already does. France\u2019s centralized history allows us to carry out evaluations of laborato-ries at the national scale. The United States cre-ated its National Institutes of Health to obtain precisely that sort of overall vision, a common evaluation system. With the ANR, we risk giv-ing labs large sums of money for 3 years, and then nothing more. We support funding by project for new research areas in which France is lagging. We also support it as a way of bringing together the various research agencies. There are too many research agencies in France, and simplification is long overdue.  Increasingly, it is the science ministry that decides research orientations. They should leave it to the scientists to decide for themselves what the most important scientific opportuni-ties are. Scientific progress cannot be made by decree. In addition, instead of simplifying the struc-ture of the research system, the government has multiplied new structures. We have hubs of competitiveness, thematic hubs and the-matic networks. The list goes on. A single lab might be attached to four different hubs, all linked to others. Each of these structures has its own boards, meetings, statutes, funds, and requests for grant proposals. Just to get their job done, researchers have to juggle new layers of bureaucracy and all sorts of different calls for proposals. French science does have serious difficulties, and there is worse to come, because the nega-tive impact of recent government policies will take time to show.  \n               Soumitra Dutta \n             \n               Chair of business and technology at the European campus of INSEAD, a top business school in Fontainebleau, near Paris. \n             One should not underestimate France\u2019s strength in research and innovation. Innova-tion is much more complex than scores on this or that indicator. There are many factors that contribute to innovation, from infrastructure to human capacity, the ability to bring research to the market, and the complexity of doing business in a country.  At INSEAD this year we created a Global Innovation Index that combines more than 80 formal indicators on innovation, as well as informal surveys of the opinions of chief executives worldwide. France ranks fifth in the index, with the United States leading, followed by Germany, Britain and Japan. France\u2019s strength lies in large technology-based companies in many sectors. It has been less successful in spawning new high-technol-ogy businesses, such as information technol-ogy. It also suffers from complexity in doing business, and inflexibility in its labour markets makes adapting to change difficult. Its lack of top research universities and the poor quality of tertiary education are weaknesses. Globali-zation means that success in innovation is now highly dependent on attracting top talent, and France isn\u2019t succeeding in this area.  France needs to realize that success in inno-vation also depends on factors outside France. Foreign talent is a critical factor, and no coun-try can succeed without it, but talent is now globally mobile. If a country doesn\u2019t have the best universities it won\u2019t get the best students, and it won\u2019t have the best universities unless it can attract the best researchers. An open attitude towards skilled immigra-tion is also key, and this is going to be a massive problem for France\u2019s future. It ranks 61st in the world in terms of reducing obstacles to skilled foreign labour; Britain is 12th, a factor that largely explains its economic boom. Measures such as automatic work permits for skilled staff are a magnet for talent. Immigration is such a politically charged issue in France that policies relating to it are a mess. France needs to be more open to skilled immigration, and needs to learn how to make foreigners feel at home \u2014 a key factor in the high-tech success of the United States.  \n                 What do you think? Join in the debate on French science at  \n                 \n                     http://blogs.nature.com/news/blog/2007/04/french_election.html \n                   \n               \n                     French election: The candidates respond \n                   \n                     French election: Is French science in decline... \n                   \n                     French science after Chirac \n                   \n                     French research chief quits over reforms \n                   \n                     French government concedes defeat to researchers \n                   \n                     French scientists prepare for mass resignation \n                   \n                     Resignation threats add steel to French revolt \n                   \n                     Institute for Genetics and Cellular and Molecular Biology in Strasbourg \n                   \n                     Global Innovation Index \n                   \n                     Cellectis \n                   \n                     Philippe Froguel \n                   \n                     Bill Rutherford \n                   Reprints and Permissions"},
{"file_id": "446854a", "url": "https://www.nature.com/articles/446854a", "year": 2007, "authors": [{"name": "Declan Butler"}], "parsed_as_year": "2006_or_before", "body": "...or have its failings been greatly exaggerated? Declan Butler finds out. Se regarder le nombril  \u2014 navel gazing \u2014 is a national sport in France, and that gaze has turned to science. The sentiment that French research is in steep decline has become a recurrent theme of political discourse, newspaper editorials and TV talk shows. France's research system has its problems \u2014 from unwieldy bureaucracies to dilapidated universities. As a result, many gaze across the channel, and the Atlantic, to the Anglo-Saxon system as a model for science and innovation. Key science indicators reveal a more complex picture, however. Despite deep funding cuts, basic research in France seems to be stable in terms of overall output of papers, but is losing ground in 'visibility' \u2014 the papers that have the greatest impact. The country does have a chronic weakness in private-sector research, though it is hardly alone in this malaise. The challenge for French research in which politicians can make the biggest difference is the decline in government spending. Recent history reveals an 'accordion' effect in which right-leaning governments squeeze research funding, and socialist ones expand it (see graphic). President Fran\u00e7ois Mitterrand's election in 1981 marked the start of a golden decade for French research as spending grew rapidly under successive socialist governments. But the growth came to an abrupt end in 1993 when conservatives were elected under Prime Minister Edouard Balladur. And spending has fallen further since Jacques Chirac was elected as president in 1995, albeit punctuated by a rise under socialist prime minister Lionel Jospin from 1997 to 2002. But the net result of Chirac's 12-year presidency is that spending on science was lower as a proportion of GDP in 2005 \u2014 2.13% \u2014 than it was in 1985. This decline runs contrary to the goal of raising average science spending from 2% to 3% of GDP by 2010, which EU governments agreed in Lisbon in 2000. Some smaller European nations have already reached this target, although they remain the exceptions. Britain and Germany are the two European countries most directly comparable to France in research power, and the three together account for two-thirds of all research in the European Union. So how does France compare to them? Germany overtook France as Europe's largest research spender in 1997, and its budget reached 2.52% of GDP in 2003. In the decade 1993\u20132003, UK spending fell by 8.7% to 1.88% \u2014 a similar drop to that seen in France. But France has done better than Britain in private sector funding. Between 1993 and 2003, industry funding grew by 8% \u2014 close to Germany's 9%. By contrast, Britain has seen a 15% drop over this period \u2014 by 2003, to just 44% of its total science spending, well below the EU average of 54%. At 51%, France is very much an average European player and still falls short of Japan, where industry accounts for three-quarters of research funding, and the United States and Germany, where it accounts for two-thirds. How does all this spending translate into research outputs? France's share of the world's scientific papers had reached 5.4% by 1999, but by 2004 it had dropped to 4.7%, at which point it was displaced from fifth place in the world league by China. A comparable drop has occurred in most European countries, including Germany and Britain, and in the United States. \u201cThis negative rate is directly related to a positive growth rate in several countries, especially China,\u201d says Henk Moed, an expert in science indicators at Leiden University in the Netherlands. The downward trends in research outputs is most marked in France's share of patents, particularly in the important US market. France's share of US patents dropped by 14% between 1999 and 2004 to 2.5%, whereas the EU's share as a whole fell by just 2% over this period to 17.1%. As with scientific publications, part of this drop is down to emerging economies in Asia. North America's share of US patents fell by 3% to 49.9% during this time; Asia's share rose by 6% to reach 30.1%, and South Korea overtook France and Britain with 2.6% of all US patents. Of all the indicators, few can hurt French pride more than having so few institutions in the top rankings of international universities. But the poor performance of French universities is perhaps also exaggerated. Under France's complex system, most labs belong to one or more research organizations and a university, but automated indicators such as Thomson's Essential Science Indicators count only the first affiliation in an author address, which means that French institutions are often not attributed correctly. To tackle this disparity, Thomson started a project in February with the Paris-based Observatoire des Sciences et des Techniques (OST) to get a truer picture of French science. So the figures, although no cause for complacency, are not as bleak as some of the rhetoric might suggest. \u201cWe shouldn't overdramatize the supposed decline,\u201d says Ghislaine Filliatreau, director of the OST, \u201cFrench basic science is still holding its own, despite poor funding, but risks being outspent and outperformed by other countries.\u201d \n                     Measures for measures \n                   \n                     Plan to rank universities fails to impress \n                   \n                     Automated grading of research performance clearly fails to measure up \n                   \n                     The scientific impact of nations \n                   \n                     Citation analysis: The counting house \n                   \n                     Observatoire des Sciences et des Techniques \n                   \n                     Science and Technology Indicators for the European Research Area \n                   \n                     European Innovation Scoreboard \n                   \n                     Pro Inno Europe \n                   Reprints and Permissions"},
{"file_id": "446968a", "url": "https://www.nature.com/articles/446968a", "year": 2007, "authors": [{"name": "Colin Macilwain"}], "parsed_as_year": "2006_or_before", "body": "A shift in population, money and political influence to America's 'sunbelt states' is helping to reshape its research universities. The first of two features looks at the far-reaching ambitions of Arizona State University. The second asks whether a rush to create extra medical schools could spread the region's resources too thinly. It is a hot February morning in the Arizona desert, and Walter Cronkite, the legendary American newscaster, is straining every muscle in his 90-year-old body to break the hard ground with a golden shovel. Cronkite is in Phoenix to start construction on a new home for America's biggest journalism school, in America's largest university, in what will soon be its third-largest city. He has some generous words for his host, the president of Arizona State University (ASU). Michael Crow, Cronkite tells the crowd gathered for the ground-breaking, is \u201ca true visionary of our time. He entered the city and took the reins of the university, and gave it direction and energy beyond what anyone could have imagined.\u201d Strong words coming from the 'most trusted man in America'. But the energy is palpable across ASU, including its campus here in downtown Phoenix where construction cranes speak to Crow's ambition. In the past four years, since he left Columbia University in New York to take the reins in Arizona, Crow has had one goal in mind. Put simply, he wants to leave behind the Harvard template, and build a new American university for the twenty-first century. The key to Crow's vision is to break away from the department-based model of most universities, and instead build up excellence at problem-focused, interdisciplinary research centres. US research universities, Crow argues, \u201care at a fork in the road: do you replicate what exists, or do you design what you actually need?\u201d By his reckoning, centres that teach students to communicate with the public and to tackle real problems, such as water supply, are more relevant to today's needs than, say, a chemistry department. Crow's ideas for ASU have some powerful supporters. \u201cIt has become a very different and very exciting institution,\u201d says Frank Rhodes, former president of Cornell University in New York and the one-time chair of the US National Science Board. \u201cIt is going to be a prototype for the rest of the country.\u201d Not everyone is convinced. Some think that Crow has over-reached, attempting to turn a public university with a mixed reputation into a research hub of international repute. For instance, critics have attacked plans for a medical school in Phoenix \u2014 supported by Crow, but being built by Arizona State's erstwhile rival, the Tucson-based University of Arizona \u2014 as being extravagant and politically inspired (see  page 971 ). In addition, Crow has been involved in noisy public disputes with ASU's student newspaper over allegations that he tried to censor its content to please Ira Fulton, a Mormon construction magnate who has donated more than US$160 million to various university projects. Before arriving at ASU, Crow had a reputation as a talented but headstrong university leader. A political scientist who specialized in science and technology policy at Iowa State University, he entered full-time university administration as a vice-provost at Columbia University, one of the top private research universities in the United States. There he helped to establish the Earth Institute \u2014 now led by economist Jeffrey Sachs \u2014 to tackle interdisciplinary environmental problems. He also pursued a vainglorious effort to save Biosphere 2, the Earth-sciences experiment-cum-greenhouse built in the Arizona desert and funded by Texan billionaire Ed Bass. It was his sojourns to Biosphere 2 that first drew him to the youngest and, arguably, brashest of the contiguous United States. \u201cI liked the attitude here,\u201d Crow recalls. Talk to any academic who has accepted or rejected a position at ASU recently \u2014 and there are plenty of them \u2014 and this attitude invariably comes to the fore. For your typical American university professor from either coast, the idea of moving to Phoenix is about as appealing as a stint in the nineteenth-century wild-west community portrayed on HBO's series  Deadwood . Yet the size and the sheer energy of the city and the project can overcome initial misgivings. A surprising number of top-flight individuals \u2014 from Nobel-prize-winning economist Edward Prescott to the biologist and former research chief of SmithKline Beecham George Poste \u2014 have taken the plunge.  ASU was already growing its research from a modest base, with an interdisciplinary bent, before Crow turned up. In 2002, the university was involved in setting up the Translational Genomics Research Institute (TGen), a genetics research centre run by Jeffrey Trent, former scientific director of the US National Human Genome Research Institute. Today TGen has about 300 researchers, an annual research income of $60 million, and eight spin-off companies under its belt. Crow's role has been to publicly raise the flag of bold reform, get politicians and philanthropists on board, sign up some star academics and build interdisciplinary centres to house them. Those established under his tenure include the physically spectacular, $150-million Biodesign Institute led by Poste; a School of Earth and Space Exploration (SESE), headed by geologist Kip Hodges; and the Consortium for Science, Policy and Outcomes run by Dan Sarewitz, a former Democrat staff member of the House of Representatives. \n               Stylish approach \n             The Biodesign Institute, whose building won R&D magazine's award last year for the finest new laboratory in the United States, houses 700 staff, including 100 faculty members who are collaborating, drug-industry style, on new approaches to molecular biology and genetics. Biology, computing and engineering in particular, but also law, social sciences and other specialties, are brought into the mix. Last year, the institute attracted about $60 million in public research funding. If all goes to plan, two additional laboratory buildings will be built by 2009, at a further total cost of $300 million. Poste has the air of someone more accustomed to giving orders than following them, but, like many of Crow's recruits, he speaks with an almost-childlike enthusiasm for the project. ASU is \u201csingularly the most radical experiment going on in American higher education\u201d, he says. \u201cThis is the fastest-growing metropolitan area in the United States, and its largest constellation of undergraduates. This isn't just about the research; it is about the future of these young people.\u201d Michael Tracy, deputy director of the institute, admits that he hesitated before coming to Arizona from his previous position at Stanford Research International, a contract research group in California. But he says that he has been impressed by the extent to which Arizona's residents have bought into the university's plans. \u201cLocal people realize that the area needs a high-value proposition,\u201d he says. \u201cThey have really embraced the idea.\u201d Kip Hodges, who came from Massachusetts Institute of Technology (MIT) in Cambridge last July to lead ASU's new School of Earth and Space Exploration, shares this enthusiasm. \u201cIt is a wonderful thing to be part of a place that is becoming, rather than a place that has been,\u201d he says of Phoenix. SESE, which has 36 faculty from many disciplines, hopes to be in a new, purpose-built building by 2009. It aims to pull together expertise in engineering, computation and Earth and space science (ASU is a leader in Mars exploration) to obtain a better understanding of problems on this planet and farther afield. At first, Hodges suspected that ASU's emphasis on serving Phoenix was parochial \u2014 \u201cafter all, what has MIT ever done for Boston?\u201d he asks rhetorically. \u201cBut for SESE, it is a matter of the relationships between the Earth and society.\u201d For example, the centre plans to study how society coevolves with changes in water resources. \u201cIt is obviously important for Phoenix, but it is important for the rest of the world, too,\u201d Hodges says. Addressing such problems requires a huge collection of skills, including archaeology, the physics and chemistry of water, evolution, anthropology, human ecology, climate and palaeoclimatology. The centre aims, for instance, to build a comprehensive model of the entire Colorado river basin. \n               Heat islands \n             Water issues are also to the fore at the Global Institute for Sustainability, an interdisciplinary centre led by Chuck Redman, an anthropologist and long-time ASU faculty member. The institute brings together about 50 faculty, all of whom also have departmental appointments, to study the relationship between people and the environment, especially in urban areas. A focus of interest, Redman says, is to develop building materials and coatings that are suited to 'heat island' cities such as Phoenix, where temperatures can exceed the surrounding area by as much as 8 \u00b0C. Unlike these other centres, the Consortium for Science, Policy and Outcomes is a unit that Crow founded in Washington DC and brought with him to ASU. On Tuesday mornings, he even teaches class there, engaging in a double act with centre director Sarewitz in a three-hour seminar with about 30 undergraduate and graduate students. Crow's a talented teacher. He pulls students into Thomas Kuhn's ideas on how scientific paradigms change by alluding to a friend who died of the immune disease lupus, and illuminates policy questions by referring to people he knows, such as Jack Marburger, the US presidential science adviser, and Jim Collins, an ASU biologist who runs the National Science Foundation's biological-sciences directorate. All of this is part of an overall picture that puts strong precedence on three things: high-quality, interdisciplinary research; access for large numbers of students from Phoenix's racially and socially diverse population; and relevance to the needs of the city and the region. Parts of the vision predate Crow's arrival, but the university has become very much Crow's project. \u201cYou have got to give him credit for attacking on multiple fronts simultaneously,\u201d says Trent. \n               Fast forward \n             Phoenix may be the ideal place to host the vision. \u201cThis is a university that is being built at the same time as the city is being built,\u201d says Crow. On a wall in his office, an aerial view displays the phenomenal growth of a city whose developed area already exceeds that of Los Angeles. The population of Phoenix and its suburbs has grown rapidly to nearly 4 million, and is projected to reach around 8 million within the next quarter century. Yet for Crow and his disciples, this newness is the greatest part of the appeal. The city has no establishment to overthrow; even Los Angeles is an establishment town by comparison. Asked why people come here, Crow's answer is straightforward: \u201cQuality of life,\u201d he says. For most of the city's youthful population that means space to live and drive, not opera or bookshops. That suits Crow, the son of an car mechanic, just fine: 'Someone once said to me, 'some of you blue-collar PhDs are quite smart',\u201d he recalls with glee. The question of what kind of university will best serve this new American university is yet to be answered. The model Crow mentions most frequently is that of the University of London which, although now fraying, includes everything from the \u00e9lite Imperial College to stalwart specialized colleges such as Goldsmiths and Birkbeck, which provide vocational training and adult education. Arizona, with its booming population and slim infrastructure, faces some of the same challenges as Victorian London \u2014 and it sees strong universities as part of the solution. The relationship between public universities and legislature in most American states is traditionally a difficult one. Universities are regarded by many state legislators as expensive and dangerous hotbeds of radicalism and free love (a relationship searingly portrayed by Jane Smiley \u2014 a friend and former Iowa State colleague of Crow's \u2014 in the comical novel  Moo ). But in Arizona, the free love is between the legislature and the state university system, of which ASU is the largest part. Conservative lawmakers and the state's liberal governor, Janet Napolitano, are united in their support for the university's expansion. A state referendum in 2000 allocated $1.5 billion in sales tax revenue over 20 years to research in the state university system. Two years later, the legislature gave $400 million more. And private money is flowing too. In a young city with a thin scholarly tradition, private philanthropy should be a hard sell. But the involvement of people like Fulton, who never attended the university but delivered newspapers near its main Tempe campus, and William Carey, a financier who gave $50 million to ASU's business school in 2003, have shown that this door, too, can be prised open. The programme still has its detractors. Academics who don't agree with the new interdisciplinary paradigm say that they have been trampled underfoot. Take Robert Pettit \u2014 a chemist and long-time director of the Cancer Research Institute at ASU until he lost the position in 2005. Pettit had clashed with Crow over various issues, including the relationship between his lab and new, interdisciplinary centres at the university. Still a tenured professor at ASU, Pettit says that the arrival of the new institutes \u201chas been very destructive to faculty and student morale and has placed ASU in serious financial jeopardy\u201d. Richard Zare, head of chemistry at Stanford University and another former chair of the National Science Board is, like Rhodes, an authority on US research universities. But he thinks that ASU's emphasis on the interdisciplinary may come unstuck. \u201cIt will learn, as others have in the past, that you can't have strong interdisciplinary programmes without strong departments,\u201d Zare says. But to Crow, the hierarchy among established US university departments is too rigid. \u201cIf you are the 25th-best geology department, you are trapped!\u201d he says. \u201cYour chances of getting to be fifth-best are zero.\u201d The interdisciplinary approach, he says, offers more promise. \n               Meaningful measures \n             Measuring achievement for the new ASU falls to the university provost, Betty Capaldi. A typical Crow hire, Capaldi was already co-leader of a project that collates detailed, comparative statistics between US universities. So no one is likely to accuse her of fudging the numbers. Capaldi, however, is rather coy about how the performance of the new ASU should be measured. The centres can \u201cuse anything they find meaningful\u201d, she suggests. \u201cWe are asking every unit to talk to us about how they would measure success. If you are unique \u2014 compare yourself to yourself,\u201d she says. It will take years to determine whether the experiment has been a success. Senior ASU faculty assert that the commitment to interdisciplinary research and broad student access have already developed deep roots at the university. And the enthusiasm that it has rekindled in such diverse and seasoned characters as Walter Cronkite and George Poste is certainly infectious. But the entire project undoubtedly hangs on Crow's unique style of leadership. The approach is unusual, as Rhodes notes, because presidents of US universities often have to content themselves with refining what they have already got. \u201cThe polishing of the status quo is much more comfortable,\u201d he says. At ASU, he says, \u201cthere is a kind of rugged individualism that says \u2014 we will just make this happen.\u201d \n                     Arizona institute names leader \n                   \n                     ASU homepage \n                   \n                     Biodesign Institute \n                   \n                     TGen \n                   \n                     School for Earth and Space Exploration \n                   Reprints and Permissions"},
{"file_id": "446964a", "url": "https://www.nature.com/articles/446964a", "year": 2007, "authors": [{"name": "Erika Check"}], "parsed_as_year": "2006_or_before", "body": "Antibody therapies have had more than their fair share of crashes. But designers are at work on faster, fancier new models, finds Erika Check. You know the scene. The mechanics are in the body shop tinkering with their toys, revving things up. They're reworking big ends, stripping machines down to the basics, welding the back half of one to the front half of another in search of something new. They're after something unexpected, something flashy, something no-one else has but everyone will want. It has to be streamlined; it has to be powerful; it has to be fast. And because these mechanics are customizing the design of medically important antibodies \u2014 not just pimping up their dragsters and low-riders \u2014 it also has to be very reliable and extremely safe. The first therapeutic antibody, a mouse-derived molecule introduced in 1986, succeeded in blocking the normal immune attack on transplanted organs. But this was a jalopy by today's standards, and such molecules have already been through two major redesigns. In the first, scientists developed 'humanized' antibodies: safe, efficient molecules that more closely mimic antibodies made by the human body. In the second, companies worked out how to mass produce them, making antibodies serious contenders in the drug market. Now antibodies are being souped up for a third time, and this time the result is set to be more radical. During the next decade, researchers and drug companies hope to introduce a new convoy of antibody drugs that is safer to patients and more lethal to pathogens or destructive cells. Many members of this new breed are fragments of antibodies that can reach targets that whole antibodies cannot. Some are proving useful for treating diseases once thought to be way beyond antibodies' range. The latest spruce-up is partly motivated by rapid recent progress in biology. Scientists now understand more about how antibodies interact with their target molecules and how they recruit the body's own defences to help eliminate them. This knowledge can be converted into more effective therapies. The redesign is also inspired by some very public disasters that have occurred during the past few years. In 2005, Biogen Idec, based in Cambridge, Massachusetts, and Elan, in Dublin, Ireland, were forced to temporarily remove Tysabri \u2014 an antibody intended to treat multiple sclerosis \u2014 from the US market after it was linked to a rare infection that can be fatal. And last year, six volunteers ended up in intensive care after taking an experimental antibody drug made by the now-insolvent German company TeGenero 1 . These failures have prompted caution in the design, trial and sale of antibody drugs. Biogen Idec and Elan relaunched Tysabri last July under a tightly controlled programme so that it can track and respond to any suspected side effects. And the scientists behind TeGenero's approach are now looking to treat patients' cells outside the body, which they hope will sidestep the problems caused when their antibody was delivered straight into the bloodstream. \u201cThere is definitely a consensus that if you are giving antibodies to patients you should be jolly cautious,\u201d says Richard Begent, a doctor and researcher at Royal Free and University College Medical School in London, UK, who helped investigate what went wrong in the TeGenero trial. \n               Revving up research \n             In the early 1990s, drug companies were dubious about antibodies because some of the earliest candidates didn't work, and because they are large molecules that are expensive and difficult to make. It took the blockbuster success of drugs such as rituximab \u2014 the first antibody approved to treat cancer in 1997, developed by Genentech and IDEC Pharmaceuticals \u2014 to coax major drug companies to take a second look. Antibody therapies have proved successful in part because they work against some diseases for which treatment options are scarce, such as cancer. They also make money. \u201cThere's nothing like success in the marketplace to get pharma to look at something really hard,\u201d says Simon Moroney, co-founder and chief executive of the fifteen-year-old protein-design company MorphoSys, of Martinsreid in Munich, Germany. Today, 21 antibodies are approved for sale, and the market is projected to be worth US$22 billion by the end of this year. Genentech, based in South San Francisco, California, has grown from a small, cash-strapped start-up to a multibillion-dollar biotechnology giant on the strength of antibodies. Most major pharmaceutical firms now have partnerships or in-house discovery units devoted to antibody therapies, and much of the basic research is being done by them and in biotech companies. It may seem audacious for scientists to try to improve on our own antibodies, which are elegantly constructed to recognize and eliminate harmful cells and organisms from the body. Natural circulating antibodies have a structure that resembles a Y (see 'Antibody structure'). The portion of the antibody corresponding to the arms of the Y is the Fab region. These arms feature 'hands' and, just like the myriad shapes and sizes of human hands, the Fab region's hands come in an endless variety of forms that each recognize and dock with one of billions of target molecules, such as structures on invading pathogens. Once docked, the antibody's business end \u2014 the Fc region \u2014 goes to work. This portion corresponds to the tail of the Y. It sends signals to the body's own assassins \u2014 cells and molecules that destroy whatever the antibody has latched on to. \n               Bits and pieces \n             The Fab and Fc regions were both thought to be essential to antibodies' ability to seek out potential threats within the body and mark them for annihilation. So many scientists scoffed when, in 2000, a 28-year-old researcher named Ian Tomlinson and his colleague Gregory Winter, based at the prestigious Medical Research Council's Laboratory of Molecular Biology in Cambridge, UK, founded a company on what seemed like a hopeless premise. (Tomlinson even left the institute to focus on this project.) Winter's team had shown that it could create 'domain antibodies' consisting of only a small stump of the Fab region, and completely lacking an Fc region 2 . Tomlinson and Winter claimed that these 'abbreviated' antibodies would be able to follow paths that are off-limits to whole antibodies, such as passing through the gut into the bloodstream. Whole antibodies are too large to penetrate the gut lining and must be injected directly into the blood. Seven years later, this play looks quite smart. It turns out that the Fc region of an antibody, which marks cells or molecules for destruction, isn't always necessary. In some cases, binding of the Fab region prevents a molecule from performing a detrimental action, making its annihilation unnecessary. (Indeed, the Fc region can even be dangerous; reactions mediated by the Fc portion of the TeGenero antibody are thought to have had a role in its extreme side effects.) Domantis, the company founded by Tomlinson and Winters and based in Cambridge, says it has shown that its domain antibody against a molecule known as tumour necrosis factor alpha is effective in animal models of rheumatoid arthritis and chronic lung disease, and preclinical trials are under way. Last December, Domantis was bought for \u00a3230 million (US$461 million) by drug giant GlaxoSmithKline. The acquisition of Domantis is a prominent endorsement of a burgeoning area of innovation: using pieces, parts or combinations of antibodies. Companies and academic researchers are now testing all sorts of pared-down or pieced-together antibodies in the hope that they will reach targets inaccessible to the whole molecule, or hit already accessible targets more effectively. The varieties are seemingly endless. Some consist of the entire Fab region; six such therapies have been approved by the US Food and Drug Administration (to prevent blood clotting, for example, or treat snake bites) and more are in clinical trials. Others are just a tiny portion of the Fab region, consisting of little more than a targeting domain \u2014 that is, a single hand one of the antibody's arms. Some such fragments have been linked together in doublets or triplets that bind to multiple targets or to different regions on the same target, thus binding it more tightly. Domantis is working on such linked antibodies. Even more fragments have been connected to other molecules such as stabilizing agents or drugs. Scientists in Maryland and California, for example, have glued together small segments of a Fab region that attaches to a receptor on human cancer cells. They linked it to the chemotherapy agent doxorubicin in order to target the drug exclusively to cancer cells 3 . The question now is how effective this and the many other innovative structures will be. Most scientists suspect that this will depend on the disease being treated and the specific structure of the molecule designed to treat it. \n               Back-seat driver \n             Many researchers are focusing on the front portion of the antibody, but there is also increasing interest in the Fc region. The importance of this region was famously shown five years ago when a group of researchers based in France published a study on rituximab. This antibody binds to and wipes out B cells, which multiply in certain types of cancer. Rituximab has been a miracle cure for some patients, but fails to work in 30\u201350% of people with particular lymphomas. The researchers divided patients on rituximab into groups on the basis of genetic variations in particular receptors that bind the Fc region and help to kill whatever the antibody has bound to. One group had receptors that bind the antibody tightly, and another had a form that bind it more loosely. Of those patients with the tighter-binding form, 90% benefited from rituximab, compared with only 50% of those with the looser-binding variant 4 . This was a dramatic demonstration that interactions involving an antibody's back end can be crucial to its effectiveness \u2014 \u201ca pretty big step,\u201d says Greg Lazar of the biotechnology company Xencor, of Monrovia, California. Researchers and companies are now attempting to modify the rear end of antibodies so that they bind their receptors more strongly and boost a patient's immune response. Xencor, for instance, has retooled its entire business model to focus on this idea. The field is rife with competition, and companies are doing everything from changing the structure and surface properties of the Fc region to altering its amino-acid sequence. It's not clear whether these attempts will work. But it won't take long to find out, because clinical trials of antibodies with modified Fc domains have already begun. Until now, many such therapies have been aimed at cancer cells, which have their own unique protein signature an antibody can latch onto. Autoimmune disease has been another focus, because antibodies can interfere with specific overactive proteins in the immune system. \n               New directions \n             But now companies are broadening their sights. One of the most intriguing examples of this began about ten years ago, when scientists at Genentech were trying to treat a condition known as peripheral neuropathy by injecting patients with a form of nerve growth factor, a protein that was lacking in the neurons of their skin. The company stopped developing the drug because it wasn't working well and because of potential side effects. Patients had reported pain at the injection site and throughout their entire bodies. In 2001, Genentech spun out all of its efforts to develop treatments for the nervous system. Arnon Rosenthal, the Genentech executive who left to run the spin-off company, was intrigued by the perplexing side effect of the nerve growth-factor trials. If injecting the protein caused pain, he wondered whether blocking the protein might relieve it. Some thought this idea wildly impractical, because nerve growth factor is thought to be essential for the survival of neurons in the central nervous system. Nonetheless, Ronsenthal's company, Rinat Neuroscience, decided to test the hunch \u2014 after all, the pain market is vast, and no major pain drugs had been developed in almost a decade. The company produced an antibody to block nerve growth factor and tested it in animal models of cancer and arthritis pain, then in human volunteers. Last May, one month after Rinat was acquired by the drug giant Pfizer, company scientists reported to a meeting of the American Pain Society that a single injection of the blocking antibody alleviated arthritis pain in 79 patients for up to two months. Earlier attempts to treat nervous-system disorders with antibodies ran into difficulties. Biogen Idec and Elan's troubled drug Tysabri, for instance, is intended to prevent the immune cells that cause nerve damage in multiple sclerosis from entering nervous tissues. The antibody binds to a protein known as integrin alpha-4 on these cells. But it had been on the market only a few months when the company had to pull the drug after two patients taking it developed a rare brain infection. The companies conducted a thorough \u2014 but inconclusive \u2014 investigation to try to discover what could be causing the infections. The Tysabri episode served as a warning that antibodies for nervous-system conditions can have unpredictable results \u2014 but it hasn't slowed the race to find them. Alzheimer's disease is a particularly attractive target, because the potential drug market is huge. The toxic protein aggregations that cause this brain disease could, in theory, be neutralized or destroyed by an antibody. One complication is that antibodies are large, and so rarely cross the protective blood\u2013brain barrier. Even so, one clinical trial found hints of efficacy. The trial, conducted by Elan in conjunction with Biogen Idec, used a vaccine designed to spur production of patients' own antibodies against the toxic proteins. The trial was halted because it caused dangerous brain infections in some patients 5  \u2014 but autopsy reports suggested that the vaccine did shrink protein clumps in the brain 6 . Moreover, the company says that some treated patients lived more independently than did those in the control group. Other companies have hurried to start work on better antibodies against Alzheimer's disease \u2014 ones that deliver antibodies directly into the body rather than trying to stimulate their production with a vaccine. Elan is farthest ahead, and is running a 240-patient trial in conjunction with Wyeth that is expected to wrap up next year. Other major companies, such as Merck and Eli Lilly, aren't far behind. Should one of these treatments work, it would mark a watershed in Alzheimer's disease treatment, and a major milestone in the expansion of diseases that might be treated by antibodies. Designers are already thrilled by the new mileage promised by their pimped-up antibodies \u2014 and are all too eager to take them for a spin. \n                     Can super-antibody drugs be tamed? \n                   \n                     Battle of the mind \n                   \n                     New test could weed out dangerous drug trials \n                   \n                     Binding activities of a repertoire of single immunoglobulin variable domains secreted from Escherichia coli \n                   \n                     Engineered antibody fragments and the rise of single domains \n                   \n                     http://www.gene.com/gene/index.jsp \n                   \n                     http://www.biogenidec.com/ \n                   \n                     http://www.elan.com/ \n                   \n                     http://www.domantis.com/ \n                   Reprints and Permissions"},
{"file_id": "446250a", "url": "https://www.nature.com/articles/446250a", "year": 2007, "authors": [{"name": "Emma Marris"}], "parsed_as_year": "2006_or_before", "body": "For some, species are simply the things you save; but for taxonomists, the concept is much more complex. Emma Marris asks whether Linnaeus's legacy is cut out for conservation. Conservationists around the world were delighted to hear that the United States was considering adding polar bears to the list of animals enjoying the protection of the Endangered Species Act \u2014 the cornerstone of US environmental law since 1973. But some experts may have greeted the news with a wry smile. Polar bears might well be threatened by the rapidly changing climate of the Arctic \u2014 but whether they actually constitute a species is up for debate. Genetic studies have shown that some brown bears ( Ursus arctos ) are more closely related to polar bears ( U. maritimus ) than they are to some other brown bears 1 . According to some interpretations of the word species, this means that if brown bears are a species, then polar bears are not. This is not, in itself, an insurmountable obstacle to protection under the act, which can be quite flexible on taxonomic matters. But negotiating that flexibility is far from easy: when modern taxonomy comes up against a conservation agenda, things can get very complex. Not everything that humans want to save is covered by an easy definition, whether it is in terms of genes, anatomy or ecological role. Take a less iconic mammal that has been in the act's purview rather longer: Preble's meadow jumping mouse ( Zapus hudsonius preblei ). It's a threatened tea-cup-sized rodent with comically large feet and a counterbalancing tail, and spends its life hopping about the foothills of the Front Range in Colorado and Wyoming. Or that's how some would have it. Others say that the subspecies, named after the naturalist Edward Alexander Preble, is a spurious one, and that the creatures called by that name are just plain old meadow jumping mice ( Zapus hudsonius ), a species with which the United States is crawling. \n               To protect and serve \n             The disagreement is not just a matter of status games between mice. The streamside habitat preferred by the foothill mice is also prime real-estate land for residential and commercial development. The degree of protection accorded to the mice thus has implications for developers. After petitions were filed by a group calling itself the Coloradans for Water Conservation and Development and the State of Wyoming's Office of the Governor in 2005, the US Fish and Wildlife Service announced that it planned to drop Preble's mouse from its list of threatened species and subspecies because of new genetics work by a group led by Rob Roy Ramey when he was at the Denver Museum of Nature and Science 2 . Papers and rhetoric began to fly. A group led by Tim King from the US Geological Survey in Kearneysville, West Virginia, was commissioned by the Fish and Wildlife Service to do further genetic tests. It disagreed with Ramey and his group 3 . In the end, the agency called in an outside group \u2014 the Sustainable Ecosystems Institute of Portland, Oregon, to make the call. In July 2006, the group laid down its verdict: the subspecies exists. The Fish and Wildlife Service is now digesting this ruling at typical government speed. Although it might be tempting just to see a story of evil developers and good conservationists, the mice highlight a more fundamental problem. The act includes in its definition of species 'subspecies' and 'distinct population segments' but offers no definitions for either of these categories. The government agencies that enforce the act do provide guidance but the researchers called on by the government to determine objectively whether various groups of organisms are listable are mostly left to their own devices. Devices that have, over the years, littered the scientific literature with at least two dozen definitions of the concept of a species. \u201cWe have more definitions than I can even remember,\u201d says Scott Steppan, a molecular systematist at Florida State University in Tallahassee, and a member of the deciding panel at the Sustainable Ecosystems Institute. Although most biologists agree that species are real entities \u2014 they exist without humans around to assign them \u2014 the distinctions are not clean-cut. The reason why the relation between this mouse and that mouse can't be nailed down with jurisprudential exactitude is the same reason that there are any mice in the first place: evolution. The problem is that the idea of a distinct species predates Darwin's insights into their origins. Carl Linnaeus thought that species were made separate from one another by God, and that they stayed that way. But Darwin showed us otherwise. As one species splits into two over the millennia, there is no magic generation in which they are clearly separate. \u201cIt is kind of like asking when you are a child and when you are an adult \u2014 where is the boundary?\u201d says Steppan. \n               Splitting hairs \n             Georgina Mace, head of the population biology programme at Imperial College, London, and two of her colleagues have chronicled how the number of species has changed under the rubric of 'taxonomic inflation' 4 . \u201cIn well studied groups, the number of species is increasing very rapidly, and that is in large part from the elevation of subspecies to the species level,\u201d says Mace. According to their analysis, subspecies are becoming species not just at an increasing rate, but also and rather more problematically at varying rates in different parts of the natural world. For example, ant taxonomists have decided that anything that's worth separating should be separated at the species level, and have no truck with subspecies at all. Butterfly taxonomists, however, like the triple-barrelled name approach and dote on subspecies. As a result, the numbers of ant species and butterfly species are not directly comparable. The implications of taxonomic inflation for conservation are wide-ranging, from increasing the number of endemic species in well studied areas and heating up 'hotspots' to making it almost impossible to figure out whether rates of extinction are slowing down or speeding up. Severe inflation can also, as in economics, lead to devaluation: if the smallest distinctions are raised up to the level that defines a species, the idea of a species loses some of its power. What is driving the inflation? According to Mace, much of it can be traced to the adoption of the phylogenetic species concept over the older biological species concept. In the biological species concept, if two fertile creatures cannot produce fertile offspring then they're not the same species, although there is some wiggle room for groups that have rare hybrids but keep their gene pools more or less separate. The phylogenetic species concept rests on the idea of diagnosable differences. If one population has a particular characteristic: a red head, say, or a particular curve of a bill \u2014 and the characteristic can be shown to be inherited, it is taken as evidence of a unique evolutionary history, which would qualify the population as a distinct species. By the phylogenetic species concept, any difference that can be placed on a common limb of a family tree could count as a separate species, but how far up the branch the pruning shears are applied does, in practice, vary between taxonomists. \u201cWe can all agree on the data, but we can't all agree on how to apply the names,\u201d says Jack Dumbacher, a molecular ecologist at the California Academy of Sciences in San Francisco. \n               Lack of definition \n             The phylogenetic approach can lead to a preference for splitting over lumping. It can also call older taxonomy into doubt, as in the case of the polar and brown bears. Although the two types of bear are distinct populations that lead different lives, studies of mitochondrial DNA suggest that brown bears do not share a common ancestor that does not also have polar bears as descendants; if you want to find one point on the tree from which all brown bears branch out, you will have to accept polar-bear branches in the same cluster. By at least one reckoning, the phylogenetic approach comes up with 48% more species than the biological species concept does for the same group of organisms 5 . More species mean smaller groups and smaller ranges, so the groups are more likely to qualify as endangered. If subspecies are ascending to the species level, it may be reasonable to assume that heretofore un-named sub-subspecies \u2014 the sorts of thing that the Endangered Species Act already recognizes as 'distinct population segments' \u2014 are rising to the level of subspecies, and so on up the chain. And that may secure their conservation more attention. The fact that it is now so easy to get gene sequences has contributed to the new ascendancy of phylogenetic over biological concepts. \u201cWe are able to slice the genetic pie thinner and thinner,\u201d says Craig Manson, who was assistant secretary for the Fish and Wildlife Service and the National Park Service from 2002 to 2005. Now teaching law at the University of the Pacific, McGeorge School of Law, Manson sees the Endangered Species Act as a creation of its time \u2014 a time when genetic data were still scarce and no one could foresee the coming orgy of re- and sub-classification. In today's world, he says \u201cthe act is sort of working \u2014 [but] not very well\u201d. The act has no clear thresholds below which a group of organisms is not considered a unit for protection purposes. \u201cI think there needs to be a conference at the national level with the best experts in the scientific community that can be found, and let's hear this issue,\u201d says Manson. \u201cIt was one of the things I intended to do, and I just didn't get it done.\u201d However, one corner of the United States has gone quiet over the definition for the tricky category of 'distinct population segment' \u2014 the smallest taxonomic unit that is listable. In 1991, Robin Waples of the National Marine Fisheries Service in Portland, Oregon, came out with his idea of how the term ought to be defined, and it stuck, at least for his agency 6 . 'National Marine Fish' shares the job of enforcing the Endangered Species Act with the Fish and Wildlife Service, and deals with the anadromous creatures \u2014 those that divide their lifecycle between salt and fresh water \u2014 as well as the truly marine animals. Although this might seem a smallish niche, in terms of the Endangered Species Act it's a big thing; of the ten listed populations in the United States on which most money is spent, eight are anadromous salmon or steelhead (rainbow trout). Anadromous fish nearly always return to their natal stream to reproduce, and so fish from different streams, even though they might brush scales in the wide ocean during their rambling years, are almost completely isolated when it comes to breeding. Waples requirements for a distinct population segment are: \u201cit must be substantially reproductively isolated from other conspecific population units\u201d, and \u201cit must represent an important component in the evolutionary legacy of the species\u201d. So in the case of the chinook salmon ( Oncorhynchus tshawytscha , which, as it happens, is considered by many to be the tastiest), those that spawn in the Columbia River basin are divided up into at least eight distinct population segments on the basis of their specific location and the timing of their runs. Some of the salmon travel to the ocean in spring, others in the summer and autumn. Although there are many more than eight populations in the basin, their listing groups them together into units of a manageable size. \n               Micromanagement \n             As Waples describes it, the distinct population segment combines a biological description of the relationships with a clear value judgement as to whether a population is important. \u201cYou really need something besides science to decide where on that level you are going to focus,\u201d he says. \u201cThere are hundreds of thousands of distinct stocks. It was not the intention of the framers of the Endangered Species Act to micromanage at that level.\u201d Surprisingly, the gambit of defining a unit with a level of value judgement did not result in a rush of litigation, even in the highly fraught conservationist world of the Pacific Northwest. Waples says that the approach has become enough of an institution that it would take \u201ca pretty compelling case\u201d for things to change. As a result, although regulated industries and environmentalists might square off on other matters, such as how listed fish are going to be protected, the distinct population segment does not come up. But although the Fish and Wildlife Service signed on to similar guidance language to that used by National Marine Fish \u2014 its guidelines on 'discreteness' and 'significance' are modelled on Waples' approach \u2014 experts mostly agree that it is less likely to list groups with only shallow genetic distinctness. \u201cOn this issue,\u201d says Manson, \u201cthey have never been in agreement, and they have grown further apart.\u201d Without agreed ways of making judgement calls, personal feelings about conservation in general and about individual organisms in particular, can end up influencing ideas about classification. Ramey, the man whose genetic analysis nearly de-Prebled Preble's meadow jumping mouse, says that his differences with his detractors are \u201cconceptual and philosophical\u201d. Conceptually, he says the fact that when at the museum (he now works independently) he had defined a threshold for what he would consider a subspecies before he did the analysis is proof that he was taking an unbiased look at the mouse. The others, he says, disagreed with his thresholds because they started off wanting the mouse to be a subspecies. But the report by the Sustainable Ecosystems Institute concentrates not on concepts but on data, and rejects much of Ramey's evidence as being based on insufficient and poor quality data and contaminated samples. Ramey replies that his evidence does not hinge on those data points alone. After a slightly sharp series of exchanges on the Ramey work in the pages of  Animal Conservation , experts seem to remain divided over the case. \u201cIf we look at the history of taxonomy, we often see that there have been many species named and then later synonymized,\u201d adds Dumbacher, who also sat on the Preble's mouse panel. \u201cIt may be that in the future, when the right studies are done, Preble's jumping mouse might be synonymized.\u201d Criticisms of his data notwithstanding, Ramey says that his threshold is the right one, and that the level of difference between the Front Range mice and the rest is just too slight, in a world of limited resources, to justify any strenuous efforts to protect them. \u201cWe have to be willing to set priorities and stick to them. Right now, if you look at it, everything is a priority,\u201d he says. Some suggest that the answer to all these problems is to ditch the taxonomic approach and shift to a totally different model of conservation law, such as ecosystem-based conservation. Many conservationists are thinking about concepts such as putting value on ecosystem services, such as water filtration and carbon sequestration. Mace sketches one possible ecosystem conservation model, in which several species' ranges are overlaid until a clear unit of space emerges. \u201cYou may have to deal with extreme specialists and those who like the edges of ecosystems separately,\u201d she adds. But when asked whether she thinks an approach like this will catch on in the near future, she sighs. \u201cNo,\u201d she says. \u201cI think it is going to be very difficult because of the amount of expectation in policy-makers and legislators about the reality of the species concept. They really believe in it.\u201d \n                     Butterflies poke holes in DNA barcodes \n                   \n                     Treasure island: pinning down a model ecosystem \n                   \n                     Biodiversity: A tragedy with many players \n                   \n                     The Endangered Species Act \n                   \n                     Interagency guidance document on \u201cdistinct population segment\u201d \n                   \n                     Fish and Wildlife page on the Preble's Meadow Jumping Mouse \n                   Reprints and Permissions"},
{"file_id": "446255a", "url": "https://www.nature.com/articles/446255a", "year": 2007, "authors": [{"name": "Henry Nicholls"}], "parsed_as_year": "2006_or_before", "body": "Although Linnaeus is best known for his botany and taxonomy, he was also an anatomist \u2014 and a keeper of pets. Henry Nicholls tells the story of Sjupp the raccoon. Not many people would respond to the death of a pet by dissecting it. But Carl Linnaeus was an exceptional man. In 1747, Sjupp, his pet raccoon, clambered over a fence at the botanical garden in Uppsala and met a dog on the other side. The meeting was not a happy one for the raccoon. Unwilling to forego the opportunity to describe the raccoon's anatomy and find out where it sat in his system of nature, Linnaeus laid the mauled body out on a slab and picked up his scalpel. Linnaeus's subsequent account of his raccoon is a perfect illustration of his powers of observation and attention to detail. But it is also a record of tenderness and affection, steeped in the rhythms of daily life at the botanical gardens. It deals with the dead animal's character as well as its anatomy \u2014 and in so doing reveals something of the character of the anatomist himself. Sjupp was a gift from crown prince Adolf Fredrik, known to Swedish schoolchildren as 'the king who ate himself to death' because he keeled over in 1771 after putting away 14 helpings of a traditional pudding. He was also, admittedly less memorably, a keen amateur naturalist as was his wife. The pair acquired thousands of natural-history specimens, according to Anthea Gentry, a research associate at the Natural History Museum in London who has been studying the mammals and birds in the Swedish royal collections. \u201cFor the royal couple, this was an enterprise driven by a desire to establish a collection of the most rare, conspicuous and interesting species,\u201d says Gentry. The queen invited Linnaeus to catalogue and describe what they had acquired. Because they contain the specimens that Linnaeus used to describe various species \u2014 the 'type specimens' \u2014 the collections took on tremendous scientific significance. \u201cThey contain so many type specimens that they are as important as any other collection of similar size,\u201d says Gentry. \n               Sweet tooth \n             After Sjupp made the journey from the small zoo in the Royal Gardens in Stockholm to Uppsala he was kept in the royal manner to which he was accustomed. Although he would eat just about anything, \u201cwhat he liked best were eggs, almonds, raisins, sugared cakes, sugar and fruit of every kind\u201d, Linnaeus observed 1 . \u201cShould there be any cake or sugar on the table or in a cupboard he was on it in a flash, and thoroughly enjoyed himself. If a student came in who happened to have raisins or almonds on him, he at once attacked his pocket and fought until he had captured the spoil. On the other hand, he couldn't bear anything with vinegar on it, or sauerkraut, or raw or boiled fish.\u201d Linnaeus went on from Sjupp's tastes to his temperament. \u201cHe became very friendly with people when he got to know them, letting them pat and play with him (especially if they ingratiated themselves by means of a few raisins).\u201d But the raccoon also had a moody side. \u201cAnyone who had once quarrelled with him found it almost impossible to get back again into his good books.\u201d Linnaeus's head gardener, who had once panicked and flapped when Sjupp bounded up to him and began to search his body for a tid-bit, suffered this disdain. \u201cFrom that moment Sjupp developed an irreconcilable hatred of the man,\u201d he wrote. \u201cEvery time he smelt him he began making a noise like a seagull, the sign that he was extremely angry.\u201d In short, Linnaeus concluded, Sjupp could be \u201cas obstinate as a knife grinder\u201d. His rich account of Sjupp's life reveals the many levels at which Linnaeus engaged with the natural world, says Karen Reeds, historian of science and guest curator of a tercentennary exhibition on Linnaeus and America at the American Swedish Historical Museum in Philadelphia. \u201cNature really captivated him emotionally as well as scientifically,\u201d she says. Testimony to his lasting affection, says Marita Jonsson, author of a book 2  on the power of place in Linnaeus's work, is a watercolour of Sjupp that hung in the study at Hammarby, Linnaeus's summerhouse just outside Uppsala. \n               Change of status \n             This fondness, though, held no squeamishness: the meat of his 1747 paper is an organ-by-organ description of the raccoon's anatomy. He named the raccoon  Ursus cauda elongata , 'the long-tailed bear'. But by the time he published his tenth edition of  Systema Naturae  in 1758, he had revised its name to  Ursus lotor , 'the washing bear', in the light of new evidence and to bring it in line with his binomial nomenclature system. \u201cIt was typical of the way he was constantly tweaking his classification scheme,\u201d says Reeds. \u201cOne of the most impressive things about Linnaeus was his readiness to change his mind as new information reached him.\u201d Today's taxonomists are still struggling to come to an agreement about raccoons, mainly because of an overenthusiastic bout of 'splitting' in the early twentieth century. In a matter of decades, scientists had described more than 20 subspecies of the common Northern raccoon (Linnaeus's  Ursus lotor , and now called  Procyon lotor ). Things got particularly out of hand in the Caribbean, where each island population was designated as a distinct species. A combination of molecular and conventional morphological approaches has now revealed that the Caribbean raccoons are probably recent introductions to the islands from the mainland population, and therefore undeserving of any special taxonomic status, allowing taxonomists to collapse the apparent diversity of raccoons back down to something more meaningful 3 . \u201cIn the Bahamas, they were delighted. They instantly changed the raccoon's status from endangered to invasive species and set up a control programme to eradicate them,\u201d says Don Wilson, a taxonomist at the National Museum of Natural History in Washington DC. But in Guadeloupe, the locals were far from happy, having taken some pride in the distinctiveness of their raccoon. \u201cThey love the little rascals,\u201d says Wilson. Sjupp himself was no Caribbean exotic; he almost certainly came from 'New Sweden', the Swedish colony on the Delaware River founded in the seventeenth century. Pehr Kalm, one of the 'apostles' Linnaeus sent out into the world to gather its riches, went there in 1748 and reported that raccoons, or more precisely their skins, were an important part of the North American economy. \u201cThe hatters purchase their skins, and make hats out of their hair, which are next in goodness to beavers,\u201d Kalm noted in his  Travels into North America 4 . \u201cThe tail is worn around the neck in winter and therefore is likewise valuable.\u201d The village Kalm spent his winters in was actually called Raccoon at the time; today, though, it is known as Swedesboro. Reeds says that Sjupp almost certainly came from this area, as did many furs exported to Sweden and the Netherlands: \u201cIt seems plausible that they would have sent a live specimen along with them.\u201d When Kalm eventually returned to Uppsala, \u201cLinnaeus was tremendously excited about what he found out\u201d, says Reeds. Linnaeus was roused from his sickbed where he'd been suffering from a severe attack of gout and returned to his  Species Plantarum  with renewed enthusiasm, she says. And at some point, he got his hands on a second live raccoon \u2014 there is one listed in the 1769 inventory of Linnaeus's menagerie. Whether it squawked at gardeners and snuffled through students' pockets, though, we do not know. \n                     American Swedish Historical Museum \n                   \n                     The world wide raccoon web \n                   Reprints and Permissions"},
{"file_id": "446253a", "url": "https://www.nature.com/articles/446253a", "year": 2007, "authors": [{"name": "Brendan Borrell"}], "parsed_as_year": "2006_or_before", "body": "Professional taxonomists often bristle at non-professionals who name new species without going through peer review. But are amateur naturalists really bad for science? Brendan Borrell reports. The death adders of Australia are not adders at all. Their closest relatives are cobras and coral snakes, but early naturalists were fooled by the snakes' stout body and triangular head. Even today, their taxonomy is a riddle: no one really knows where one species of death adder ends and the next begins. In the late 1990s, only three species of death adder had been recognized, but herpetologists suspected that there were at least twice as many. Ken Aplin, then a curator at the Western Australian Museum in Perth, had spent years collecting data to back up that hunch. But before his study could be published, Raymond Hoser, a herpetologist not affiliated with an academic institution, described five new species of the snake in a 1998 issue of  Monitor , a hobbyist magazine he edited for the Victorian Herpetological Society. Under the taxonomic code of the International Commission on Zoological Nomenclature (ICZN), Hoser's names \u2014 printed and disseminated to society members \u2014 take priority over any subsequent descriptions of the species. Aplin had been scooped 1 . In the competitive world of taxonomy, countless amateurs have found success by collaborating with academics. Many professionals welcome their contributions; amateur enthusiasts are, in essence, a free workforce at a time when funding for basic taxonomy is waning. But cases such as Hoser's make some scientists wary of such contributions. Hoser, who runs the snake-removal service Snakebusters in Melbourne, paints the picture as a classic case of academic \u00e9litism. \u201cThe description of me as an amateur is complete rubbish,\u201d he says. \u201cThere's no one in history who has spent so much time dealing with, looking at, catching and breeding death adders as myself.\u201d But his critics say it is not Hoser's credentials that they challenge. \u201cA steady drip of shoddy descriptions\u201d is how Wolfgang W\u00fcster, an evolutionary biologist at the University of Wales in Bangor, describes Hoser's work. In a published critique, he and other leading herpetologists argue that Hoser \u201calmost invariably fails to provide adequate information on his species, on their types, or on the material he has examined\u201d, making it difficult to repeat and test the observations 2 . Hoser, for his part, says that his descriptions contain more than adequate information. \n               Seek, locate, describe \n             But the ICZN, the group in charge of setting the ground rules for taxonomy, says it cannot police the quality of every published description. \u201cIt's a very tricky area to work in,\u201d says Andrew Polaszek, executive secretary for the organization. The commission, he says, will arbitrate only on pure nomenclature issues. In such a dispute, it will assess whether a Latin name put forward for a new species is valid under the taxonomic code. The code states that authors need to print the description of their species on paper, designate a type specimen, and list features that distinguish it from others. So just because a description is valid doesn't mean it is good. \u201cThe commission does not like to get involved in subjective taxonomy,\u201d Polaszek says. Some taxonomists have proposed that the ICZN change its rules so that new species can be described only in peer-reviewed journals or through some other formal accreditation process. But others think that might impose too much of a burden on small society journals where many good descriptions are published without peer review. In short, no clear solution is forthcoming. Centuries ago, amateur was the only way to be a naturalist. The linnaean nomenclature system arrived in England in 1760 and soon became \u201ca parlour game for the leisure class\u201d, says retired historian David Allen in Winchester, UK. \u201cPeople armed with guides to the flora took them out on their fashionable wanderings around the countryside.\u201d The surge in popular interest fuelled a rash of publications in club newsletters and self-financed monographs describing species and rearranging classifications. The backlash began almost immediately; even Charles Darwin criticized the vain \u201cspecies-mongers\u201d who perpetuate a \u201c vast  amount of bad work\u201d 3 . He thought the problem stemmed from this notion of priority, which he called \u201cthe greatest curse to natural history\u201d. As a case in point, consider orchids. Two specialists recently published a plea against what they call \u201ctaxonomic exaggeration\u201d 4 . As European botanists continue to subdivide orchid species that are not genetically distinct, they artificially inflate both the diversity and rarity of local flora, which may shift conservation priorities away from remote areas with fewer described species. \n               A question of honour \n             Orchids have long attracted a plethora of amateurs. In 2002, for instance, Michael Kovach smuggled a ladyslipper orchid from Peru and asked that a taxonomist at the Marie Selby Botanical Gardens in Sarasota, Florida, name it after him. Selby's experts reportedly knew that Eric Christenson, an unaffiliated taxonomist also in Florida, had his own description of the species scheduled for a forthcoming issue of  Orchids . Selby rushed a two-page description of  Phragmipedium kovachii  to print as a supplement to its house journal. Kovach eventually pleaded guilty to illegal possession and trade of an endangered species; Selby was fined for its role in the scandal. None of this matters in the eyes of the taxonomic code, which will honour Kovach for ever. Such 'orchid fever' has coloured the work of more than a few budding taxonomists. \u201cThere are lots of orchid amateurs who do not have a taxonomic background and who once in a while feel like they can make taxonomic changes,\u201d says Calaway Dodson, emeritus curator at the Missouri Botanical Garden. \u201cGenerally, it's a disaster.\u201d But some amateurs, he says, do excellent work. In Florida in the 1960s, a retired surgeon called Carlyle Leur began photographing the orchids of the state, and went on to document the entire country. Leur has since published more than 30 painstakingly illustrated monographs on the Pleurothallidinae, a daunting subfamily of orchids that contains more than 3,500 species. Perhaps, then, the energy of amateurs can best be harnessed to fill the gaps in areas where few professionals work. The number of both professional and amateur taxonomists has been declining in Britain since the 1950s, according to a survey done in 2002 (ref.  5 ). But because they are distributed so widely, amateurs can generate better geographical coverage of flora and fauna, and focus on more descriptive taxonomy, leaving professionals free for molecular systematic studies. \n               Ground force \n             One successful venture, launched in 2002, is a partnership between government agency Natural England and the Natural History Museum in London, to encourage amateurs to contribute their data to local recorders. Museum experts have trained fly-fishermen to identify river flies and have enlisted members of the Ramblers' Association to monitor mature elm trees during their walks. The project met resistance from both sides at first, says sociologist Claire Waterton of Lancaster University. Some amateurs were reluctant to submit their data because they didn't understand how the work would be used, whereas others were worried that they would be judged unfavourably by professionals if they made mistakes. In turn, many professionals expressed concern about the quality of the data they might get. But in February, the British Mycological Society uploaded a database of fungal records to the National Biodiversity Network. Compiled by local groups in Britain and Ireland, the database contains 140,000 recorded samples dating back to the eighteenth century. Technology is also an important way to harness amateurs' contributions. Charles Godfray, an evolutionary biologist at the University of Oxford, UK, has worked to set up a peer-reviewed, single repository for all taxonomic information online \u2014 a sort of wiki-taxonomy 6 . That, he says, could help amateurs check the taxonomic designations of species that have been described. \u201cThe single thing that stops amateurs from being better involved in the process of taxonomy is getting at the literature,\u201d says Godfray. He is beginning to see his dream realized with a test website called CATE, for 'creating a taxonomic e-science', which he hopes will hold the taxonomy for aroids (popular house plants) and hawkmoths. Polaszek notes that there is plenty of work for both amateurs and professionals. \u201cWe've got tens of millions of species to be described, and the easier this is, the better it is for everybody,\u201d he says. The ICZN is setting up a new system, called ZooBank, that requires species descriptions to be registered online. Within a year, he says, ZooBank could even be modified to include purely web publications such as CATE. But could such changes sort out the mess over Australian reptiles? Perhaps not. In the 1980s, two amateur herpetologists called Richard Wells and Ross Wellington published more than 550 species descriptions that have since been changed 7 . An attempt to annul the work of the pair was rebuffed by the ICZN and taxonomists still have to sort through this work to determine whether the names chosen by Wells and Wellington have priority over other publications. Hoser, for his part, found inspiration in their example and christened one death adder  Acanthophis wellsei . It may have been a fitting tribute, as the name itself was improperly constructed. In a redescription of the species, Aplin amended the name to  Acanthophis wellsi . \n                     Citizens as amateur scientists \n                   \n                     Family albums highlight climate change \n                   \n                     Plant science: Gardens in full bloom \n                   \n                     Fossil find breaks age record \n                   \n                     The budding amateurs \n                   \n                     English Nature \n                   \n                     CATE Project \n                   \n                     National Biodiversity Network \n                   \n                     ZooBank \n                   Reprints and Permissions"},
{"file_id": "446364a", "url": "https://www.nature.com/articles/446364a", "year": 2007, "authors": [{"name": "Katharine Sanderson"}], "parsed_as_year": "2006_or_before", "body": "The buzz over invisibility cloaks is fun \u2014 while it lasts. But metamaterials are likely to transform optics through more mundane applications. Katharine Sanderson reports. There is a world where objects can be made invisible, where light can be bent the wrong way, and where images of incredibly small objects can be brought into sharp focus by a superlens. That magical world doesn't sound very real, and it isn't. It exists mostly in the minds of theoretical physicists. But under the guise of metamaterials, some of these weird properties are making it into the real world, and beginning to attract commercial interest that goes beyond boyish excitement about 'invisibility cloaks'. In reality, metamaterials applications are likely to be much more mundane, says Michael Wiltshire, a researcher at Imperial College London. Wiltshire predicts that metamaterials will find their first uses in antennas for telecommunication or in biological imaging. It's a far cry from last year's headlines about Harry Potter and magic cloaks but a first step to a world of new optical tricks. Metamaterials are engineered to have features that are about the same size as, and usually smaller than, the wavelength of electromagnetic radiation they are being used to manipulate. The features are often small metallic wires and coils, which together manipulate the electrical and magnetic components of electromagnetic waves in seemingly unnatural ways. It is these tiny features of their design, rather than the atomic structure of the material itself, that determine its properties. The field got going in 2000 when John Pendry at Imperial College suggested that metamaterials could be used to make a 'superlens' 1  that could image objects smaller than the wavelength of the incoming light. Making a lens that could bring such tiny features into focus would require a material with a negative refractive index, one that would actually bend incoming light the wrong way. This was achieved in 2001, although not for visible light, when David Smith at Duke University in Durham, North Carolina, made a two-dimensional metamaterial from interlocking units of fibreglass board patterned on one side with a copper strip and on the other with split-ring resonators 2 . But the public's imagination was really caught in May 2006 when Pendry suggested that, theoretically, metamaterials could be used to make an invisibility cloak. Again, Smith's team followed just five months later with a practical demonstration of invisibility. Smith's cloak used concentric rings of fibreglass imprinted with copper wires (see picture). This contraption, when placed around an object, directed microwaves around that object, much as a stick dipped in a stream interrupts the flow of water. When the microwaves met up behind the object they behaved as if the object hadn't been there 3 . This cloaking prototype only works in two dimensions, and for a narrow band of wavelengths, but it was a proof of principle. With this demonstration came promises of the ultimate camouflage \u2014 in short, applications that would have military agencies salivating. According to Pendry, the first metamaterial was developed by the US military in the 1950s, and its interest in the topic has never died. \u201cThe primary interest for sure is from the military,\u201d says Smith, who received most of his research funding from the military. In 2001, the US Defense Advanced Research Projects Agency (DARPA) set up a metamaterials research programme. In total, it has invested US$40 million over six years \u2014 enough for a young field to get going but small change for an agency whose annual budget is several billion dollars. \u201cMetamaterial developments so far are exciting and promising,\u201d a DARPA spokesperson says, \u201cbut they still require significant development to qualify for 'real world' applications.\u201d Not even Smith thinks that a fully functional invisibility cloak will become a reality. \u201cWe won't be able to cloak a fighter jet,\u201d says Smith. \u201cI don't know how large a structure we could eventually cloak. Right now, we are trying to play around with the parameters to implement a full cloak, but it is much more difficult.\u201d \n               Mind your Ps \n             Pendry was working on radar-absorbing materials for a military contractor when he came across the work of Victor Veselago, a physicist at the Russian Academy of Sciences in Moscow. Metamaterials take advantage of two fundamental properties of materials \u2014 their electric permittivity and magnetic permeability, which are the amounts by which the electric and magnetic parts of a wave interact with a material. Most, but not all, materials have positive values for these two parameters. And in the 1960s, Veselago expanded previous work to study what happens when both the permittivity and the permeability are negative. This leads to a negative refractive index \u2014 the amount by which light is bent as it passes from one medium into another \u2014 and as Veselago showed, to new optical properties. It was Pendry who realized that with metamaterials Veselago's ideas could become reality. DARPA's initial interest was in applications beyond optics, such as magnets for more powerful motors, radar applications, and textured surfaces that could control a material's heat-handling capabilities. DARPA now thinks, and Wiltshire agrees, that the most feasible advances are likely to be ultra-high-frequency antennas for possible radar applications. DARPA won't reveal much about its radar work, but others are pursuing commercial applications of metamaterials for radio-frequency antennas. Conventional antennas need to be half as high as the wavelength of the radio waves, which can be many metres for low radio frequencies. More compact antennas are needed for the next generation of hand-held devices such as mobile phones and laptops. \n               Fast and small \n             To reduce the wavelength (and the size of antennas), one option is to increase the frequency; at 30 gigahertz the wavelength is one centimetre. But these higher-frequency waves are blocked more easily by obstacles, making them impractical for networks in urban areas. In 'normal' materials the wavelength is inversely proportional to the frequency of the wavelength, but in a negative-index antenna, something strange happens \u2014 the apparent wavelength decreases as the frequency decreases. \u201cThis is quite the opposite of what happens in conventional materials,\u201d says George Eleftheriades, an electrical engineer at the University of Toronto in Canada. According to Eleftheriades, by exploiting this property, the height of the antenna could be just one-thirtieth of the wavelength, making compact millimetre-sized devices a real possibility (see above). \u201cNow, the wavelength reduces as the frequency is reduced,\u201d says Eleftheriades, \u201chence short antennas can be constructed even at the low frequencies (1\u20135 gigahertz) at which most wireless telecom systems work.\u201d Developing metamaterials for antennas has allowed Eleftheriades to bag a commercial contract with Nortel Networks in Toronto, and a project funded jointly by Nortel and the Canadian government to the tune of $400,000. He also has a patent on his antenna design. Beyond antennas, \u201cthe optical range is perhaps the most practically important for applications,\u201d says Pendry. Optical communication devices, such as fibre-optic cables, and biological imaging would be enhanced if light at optical wavelengths could be manipulated in new ways. But shrinking a metamaterial's features to match these wavelengths is more challenging: the material would need features that control multiple wavelengths of light simultaneously \u2014 and that would involve some very small and complicated engineering. By making optical devices, physicists hope to beat the 'diffraction limit' and so achieve a superlens. For features smaller than roughly half the wavelength of light, diffraction in a normal lens distorts the image. A negative-index material that could focus visible light from subwavelength objects should produce distortion-free images. This would revolutionize biological imaging, Pendry argues, by bringing the details of cells and cellular processes into sharp focus. In 2005, two teams from the United States and New Zealand showed that a simple superlens could be created for ultraviolet wavelengths from very thin layers of silver 4 , 5 . Silver naturally has a negative electric permittivity (but not permeability) at these wavelengths and is able to function as a superlens as long as the dis-tance between the object and lens is much less than the wavelength used. These experiments achieved only modest gains over the diffraction limit, but such superlenses are already being used to improve near-field infrared microscopy 6 . Smith admits that losses and imperfections in the metamaterial will limit the resolution that can ultimately be obtained, but insists, \u201cThe superlens as a concept is sound.\u201d \u201cIt is a separate question as to whether any of these metamaterials technologies will be useful or commercial \u2014 that will depend on the competition,\u201d Smith says. He argues that metamaterials ideas are already influencing the next generation of engineers: \u201cSo these concepts may seep into the techniques and bags of tricks that engineers employ without necessarily being identified as 'metamaterials'. It could be a very quiet revolution.\u201d For example, the aerospace company Boeing in Seattle, Washington, has explored the use of metamaterials with negative indices as lenses, again funded mainly by DARPA. Lenses are used in satellites to help focus microwave signals onto antennas, enhancing their detection from the ground. Using a negative-index lens in a communications satellite would enhance performance while reducing the weight of the lenses. This is a simple, albeit niche, application of a metamaterial, says Wiltshire. \u201cWhat we're exploiting is nothing sexy in the metamaterial \u2014 it's mundane \u2014 the lens is lighter,\u201d he explains. Nathan Myhrvold, chief executive of Intellectual Ventures in Bellevue, Washington, and formerly Microsoft's first chief technology officer, claims to have more patents for metamaterials than anyone. \u201cDozens of them,\u201d he says. The issue for metamaterial applications is not so much when, but rather where they will be used, he says: \u201cAt the moment metamaterials are an answer searching for a question \u2014 certainly from a commercial standpoint.\u201d Like many in the field, he has faith that the technological breakthroughs will come. \n                     How to drive light round the wrong bend \n                   \n                     Invisibility cloaks are in sight \n                   \n                     Metamaterial bends microwaves into beam \n                   \n                     Negative refraction: A lens less ordinary \n                   \n                     Nature Materials interview with John Pendry \n                   \n                     Nature Materials commentary \n                   \n                     Nature Photonics \n                   \n                     DARPA's metamaterials programme \n                   Reprints and Permissions"},
{"file_id": "446369a", "url": "https://www.nature.com/articles/446369a", "year": 2007, "authors": [{"name": "Emma Marris"}], "parsed_as_year": "2006_or_before", "body": "The US military is getting a lot of flak for the way it treats wounded soldiers returning from Iraq. Emma Marris reports on the advances in medical care that are helping to bring them home. Many numbers can be used to tell the stories of war. Almost 3,500 Americans dead in Iraq and Afghanistan. Some 24,000 injured servicemen and women. Other numbers are subject to their own skirmishes: estimates of the number of civilian Iraqi dead are hotly contested and range from tens of thousands to half a million. Every number tells a story. A little-known number is that since 2001, the wars in Iraq and Afghanistan have had a US military case fatality rate \u2014 the percentage of injured soldiers who die \u2014 of 9.4. That is, of 100 people wounded, 9.4 of them will die, either instantly or later, from their wounds. This compares with case fatality rates of 15.8 in Vietnam and 19.1 in the Second World War 1 . The reasons for the decline in the current conflicts are better armour, logistics and medical tools. Battlefield medicine has often led to advances in trauma care. Florence Nightingale's sanitary reforms in hospitals during the Crimean War are a classic example. In the 1950s Korean War, improved resuscitation of the wounded and rapid air evacuation to Mobile Army Surgical Hospitals \u2014 the famous MASH units \u2014 reduced the US hospital mortality rate by 24% compared with the Second World War 2 . John Holcomb, commander of the US Army Institute of Surgical Research in Fort Sam Huston, Texas, summarizes the advances in the current conflicts as \u201ctraining, protection and devices\u201d. The devices that he and other trauma surgeons point to are better body armour, redesigned tourniquets and advanced clotting aids and transfusion fluids. But all the military personnel interviewed for this story stressed the importance of training and preparation: most surgeons sent to Iraq and Afghanistan are first required to spend time in a civilian trauma centre. Teasing apart the actual contributions of individual advances to the lower body count is a much greater challenge. Many data are classified or unavailable. Perhaps the most scientific of all the wartime advances in Iraq is the creation of the Joint Theater Trauma Registry. For the first time, solid data are being collected for trauma cases from injury through to recovery. \u201cWe are able to get a sense about what kind of improvements can be made in the body armour,\u201d explains Paul Cordts, who runs the office that sets policy at the office of the US Army Surgeon General. \u201cWe can see what kind of body armour the casualty was wearing, what damage there was to the vehicles, and what happened to the casualties.\u201d This data collection and analysis is intended to guide future improvements in medical training and equipment, but also to investigate the effect of procedural changes made during the current conflicts. \u201cFor all these wars \u2014 and we have had a lot of wars \u2014 we have never done any thorough data collection,\u201d says Basil Pruitt, editor of the  Journal of Trauma . \u201cNow they can really identify the outcomes of these patients and figure out what is particularly beneficial.\u201d For Pruitt, the steady improvement in survival continues a historical trend: \u201cThe survival from serious wounds has increased as the time between injury and arrival at a definitive hospital has decreased. That started in the First World War really.\u201d The Korean war took this further by making rapid helicopter transfers to surgical units routine practice. Now, he says, such units are better equipped: \u201cThey have a whole panoply of specialists. If you have a head injury, you do better now than you used to do because you have neurosurgeons.\u201d Military medicine often begins just seconds after a wounding, because all soldiers and marines now carry one-handed tourniquets and clotting agents with them to help slow down blood loss. The main agents used are QuikClot, which comes as a sponge or as powder, and HemCon, which is in bandage form. No one knows exactly how many lives they save: \u201cGetting data from the fog of war isn't easy,\u201d says Raymond Huey, chief executive of Z-Medica in Wallingford, Connecticut, the company that makes QuikClot. \n               Far-forward thinking \n             QuikClot powder is made of porous minerals called zeolites. The story is that inventor Frank Hursey, who was working with zeolites as sieves to separate gases, cut himself shaving and applied it to his face on a whim. How it works is still unclear, although it has been approved for clinical use. \u201cThere is a whole lot of surface chemistry,\u201d says Huey. The product also includes calcium ions, catalysts for the body's clotting process. It is carried by every marine and by members of a number of other forces. HemCon bandages rely on chitin molecules, which are obtained from shrimp shells. The chitin is extremely sticky and glues the bandage to the flesh \u201clike super duct tape\u201d, according to Mike Zoormajian, product manager at HemCon Medical Technologies in Portland, Oregon. Then, the positively charged chitin attracts and clumps together the negatively charged blood cells. Zoormajian says that the product was developed after casualties in Somalia bled to death in the streets because no medics could reach them. Since 2005, every US soldier serving in the Middle East carries at least one of these bandages. Zoormajian thinks that the bandages have saved well over 100 lives. Once a medic gets to a wounded soldier, the solider enters a rapid and organized system. The casualty is stabilized far-forward \u2014 that is, right up in amongst the action \u2014 and then sent to a nearby aid station, where they are patched up further and sent to a Combat Support Hospital. From there, they might be flown to Landstuhl Regional Medical Center in Germany or on to Walter Reed Army Medical Center in Washington DC. All this often happens inside a week. Speed really does save people with battlefield injuries. The alternative can be bleeding to death. \n               Saving lives \n             For Richard Jadick, a surgeon deep in the heart of Falluja, Baghdad, with the 8th Marine regiment in 2004, it was Hespan \u2014 a colloid solution \u2014 that saved lives. During a sustained battle, Jadick put the far-forward concept to its extreme, setting up an aid station in an abandoned prayer room, with stretchers up on cement blocks and bullets ricocheting outside. Colloids such as Hespan contain starch molecules that pull water into the veins from other tissues. \u201cHespan was a huge player,\u201d says Jadick. \u201cIt keeps fluid in the vein. But you are going to see a huge increase in combat survival when you can get blood or something like it on the battlefield.\u201d Surgeons such as Jadick know that the most severely injured casualties \u2014 about 10% \u2014 are not only losing blood but fighting shock, hypothermia and, unhelpfully, lowered blood clotting. The answer is to get the blood volume back up \u2014 but with how much of what fluid on what timeline? Jadick had only Hespan to hand, but medics at units farther back from the action have access to various blood fractions and crystalloids \u2014 solutions of water and electrolytes \u2014 in addition to colloids. \n               Pressure points \n             Civilian doctors generally use crystalloids and colloids initially and transfuse blood or plasma later. But as of 3 January, military policy has been to give the patients something close to real blood as quickly as possible. This might mean warm blood donated by a fellow soldier or an infusion of thawed red blood cells and plasma (in a one-to-one ratio). According to Holcomb, Hespan and other products get the blood pressure back up, but do nothing to restore clotting. The clotting factors in human blood or blood products are what is needed. And because keeping blood on ice is tricky on the battlefield, other technologies will be needed. Cellphire \u2014 a company based in Rockville, Maryland \u2014 is freeze-drying platelets, which help with clotting, to lengthen their storage time. At room temperature, platelets keep for only a few days, but the freeze-dried products can be rehydrated with sterile solution when the need arises. Cellphire hopes to have their product in clinical tests in 18 months. \n               Joint effort \n             Holcomb stresses that no advance can save lives on its own. Speed won't help, if you don't have the right devices. All the blood in the world is no good if your body armour has let you down. The goal of military medicine is moving the right people to the right places at the right time \u2014 what Holcomb calls \u201cthe trauma systems approach\u201d. The Joint Theater Trauma Registry should reveal how well all the parts of this system work. Body protection, for example, has improved enormously since the flak jackets of Vietnam. Materials science has created ever-better protective gear \u2014 such as lighter Kevlar vests that are more resistant to bullets and shrapnel. Ceramic inserts and add-ons are available for the upper arms, neck, groin and other areas. As a result, wounds now tend to be to the arms, legs and head, with far fewer chest injuries. The amount that body armour has contributed to the lower case fatality rate in Iraq and Afghanistan is unknown, as are many other figures that might reveal the truth behind the lower case fatality rate. Despite the lack of data, Holcomb and others argue that battlefield medicine has consistently produced medical advances, some of which transfer to civilian practice. Although US civilians might have wars to thank for helicopter pads on hospitals and procedures for massive trauma, Iraqi citizens are getting the trauma without the advances. US care for wounded Iraqis is often no more than a goodwill gesture. General healthcare is bad. Worse, the poor security situation is leading Iraqi doctors to flee the country: up to 30% of qualified physicians are estimated to have left since 2003. Many doctors are targeted for threats and attacks by unknown parties and for unknown reasons, and a depressingly high number have been murdered or kidnapped. \n               All wounds leave scars \n             Bassim Irheim Mohammed Al Sheibani of the Diwaniyah College of Medicine in Iraq wrote a letter to the  British Medical Journal  last October, explaining that there are no drugs, no trained people and no equipment 3 . No aid or assistance has been forthcoming from abroad, apart from some donations from the occupying force, which Sheibani says are often inappropriate. As for personal security, there isn't any. Sheibani himself has received phone threats. His college is planning to provide training courses in emergency medicine for students and doctors, but they lack funds and support. As for the US military, people are coming home who would have died in previous wars, but some of them are in pretty bad shape (see  'After the battle' ). For one doctor, at least, this can seem a mixed blessing. \u201cPeople do fixate on the body count,\u201d says Jadick, \u201cand there are a lot of people who are coming back alive and \u2014 I don't know, they maybe shouldn't be. You've always got to think \u2014 did we do them any favours? There are some of them that I'm not sure.\u201d \n                     Death toll in Iraq: survey team takes on its critics \n                   \n                     Scientists become targets in Iraq \n                   \n                     Polymer sniffs out explosives \n                   \n                     Cell biology: Just add water \n                   \n                     Army medicine innovations since Desert Storm \n                   \n                     Institute for Soldier Nanotechnology \n                   \n                     GWOT related medical publications \n                   \n                     Rapid Prototyping for Bagdad \n                   Reprints and Permissions"},
{"file_id": "446366a", "url": "https://www.nature.com/articles/446366a", "year": 2007, "authors": [{"name": "Jim Giles"}], "parsed_as_year": "2006_or_before", "body": "Many Iraqi academics have escaped death threats only to find that their qualifications are obsolete and immigration authorities are unsympathetic. Jim Giles hears their stories. Some 20 years ago, Ali Althamir led a comfortable life in Iraq. As head of a university department and an expert in computing, he was part of a well-heeled middle class. He worked at one of the better institutions in the Middle East. And before sanctions crippled the regime of Saddam Hussein, he was one of many researchers who had money for their studies and could sometimes travel to foreign conferences. It is a far cry from the life Althamir lives now. Since arriving in Britain to seek asylum in 2003, his fingerprints have been placed on file at the police station and he reports there once a month. Filling in the time between visits is difficult because he cannot now legally be paid to clean a lecture hall, let alone speak in one. When I thank him for taking time to meet me, he tells me not to worry. \u201cI have nothing to do,\u201d he says. It's a drab January day when I meet Althamir and other Iraqi scientists and clinicians at an office in the London South Bank University. They have travelled across the capital to meet me, from cheap, rented accommodation paid for by social services and charitable hand-outs. With Althamir are an expert in radar technologies, a civil engineer and a former lecturer at Baghdad's most prestigious medical school. Between them, they share well over half a century of research experience, the painful decision to flee their country \u2014 and little idea of what the future holds. The emotions they express are predictable: anger that a nation that invaded their country is now treating them so badly; frustration at the unfathomable British immigration laws; fear for family and colleagues they have left behind. They have fled death threats only to run into an unwelcoming immigration system and an indifferent academic community that often does not recognize their experience. They did not know that Britain's growing immigrant populations have been a source of acute political tension for decades, tension that has led to tight restrictions on who can enter and complex rules to control the country's gateway. Nor did they know that experience such as being a head of department would do little to help them find work. Learning about these obstacles beforehand is not always a priority, or even a possibility. It is a physical and emotional journey that has been shared by many since the invasion of Iraq in 2003. The United Nations estimates that half a million Iraqis left their country last year alone. Some of the academics who want to continue their research head to Britain, which has strong historical links with Iraq and is where many Iraqis studied at the start of their careers. They also go to the United States because of its strong reputation for research and funding. No hard figures are available, but several thousand academics are thought to have left Iraq since the invasion, of which at least a hundred have entered Britain and the United States. Psychiatrist and researcher Ali Omar, who asked that his real name not be used, tells a story that many Iraqi academics will recognize. His life entered a radically new chapter on 9 April 2003, the day American troops famously toppled the statue of Saddam Hussein in central Baghdad. The looting began almost immediately. \u201cEverything went,\u201d Omar recalls, \u201cdoors, air conditioning, lights.\u201d Omar found his clinic destroyed and spent much of the time afterwards building up a network of psychiatrists in Iraq to help those harmed by the war. He later organized a conference that attracted media attention \u2014 it should have been a triumph, but immediately afterwards relatives called. The publicity, they said, would make him a target of the militias and criminal gangs that were kidnapping and killing academics. They were right. \n               Targeted groups \n             Around 300 academics have been killed since the invasion, according to human-rights groups and media reports. No one is sure whether Shia, Sunni, Baath or anti-Baath groups are to blame, or exactly why academics are being targeted. Some researchers blame fundamentalist Islamic groups, who are accused of wanting to destroy Iraq's middle class to establish religious rule. In addition, criminal gangs could be the cause, seeking to make money through kidnapping affluent professors and researchers. In Omar's case, trouble began when he was away at conferences in 2005. Neighbours reported that strangers had been inspecting his family home. In April 2006, he received an anonymous phone call and the warning to \u201cleave or die\u201d. Within an hour, he and his family had fled to nearby relatives \u2014 by July they were in Jordan. Omar arrived in Britain later that month on a six-month visitor visa to attend a conference at London's Royal College of Psychiatrists. Once in the country, he says that he arranged to speak to a barrister through a friend. The advice made him hopeful that he could qualify for asylum within a few months because of the death threats against him, and that, if successful, his family would be able to join him. Immigration lawyers say that the advice from the barrister was flawed. It is now eight months since Omar's claim went to the Home Office and no decision has been made. The delay should have been expected, say lawyers, given the backlog of 6,000 asylum applications that the Home Office says has built up, in part because of underfunding and the complex immigration rules. And because Omar arrived on a visitor visa, he is not allowed to work and is banned from receiving financial benefits. The money he brought with him, generated by selling his possessions in Iraq, has now run out. Yet if he were to leave Britain now he would forfeit his application. \u201cI'm desperate,\u201d he says. \u201cI don't know what to do.\u201d \n               Support structures \n             For academics like Omar, the only means of support is charity, and in his case the Council for Assisting Refugee Academics (CARA) has come to the rescue. The London-based organization, conceived in 1933 to help academics forced out by the Nazis, has provided accommodation and maintenance grants and flown his family to safety in Cairo. Organizations such as CARA are keen to help academic refugees because \u201cthey represent the core of their country's scientific and cultural capital and have a key role to play in the rebuilding of their country,\u201d says Kate Robertson, CARA's deputy executive secretary. \u201cAs educators and independent thinkers, academics are seen to be particularly influential,\u201d she says. Preserving such capital also matters on a global scale. Of the thousands of academics helped by CARA in the 1930s, sixteen went on to become Nobel prize winners. Like Omar, many Iraqi academics are unaware of the minefield of bureaucracy they will face to stay permanently or to work in Britain. Some could apply to the Highly Skilled Migrant scheme, which provides entry for well qualified individuals. But they then need to produce recent payslips and tax returns to show that they have been employed in their field, and these documents are often not issued in Iraq. Another route of entry is to obtain a work permit through a sponsoring institution before entering Britain. The institution must prove that it has tried and failed to find a better qualified candidate from within Europe, which is difficult and often requires an inside champion for the applicant. \n               Political barriers \n             When it comes to gaining asylum in Britain, there could be an additional political impediment, says Chris Randall, an immigration lawyer with Bates Wells and Braithwaite in London, who has advised CARA. Ninety per cent of the Iraqi asylum decisions announced in the past year have been refusals, and Randall thinks that this is partly because British authorities are reluctant to admit that the situation in Iraq is bad enough to warrant asylum. \u201cThe government wants to portray the idea that Iraq is getting better,\u201d he says. Those waiting to hear are therefore understandably nervous. The academics who asked that their names be changed were fearful of reprisals against relatives back home, but were also worried about antagonizing the UK immigration authorities. The Home Office denies any political bias against Iraqi asylum seekers, saying that every case is taken on its merits. Even if Iraqi academics overcome the legal obstacles, they frequently struggle to find a job. Sabrine Gilel, once a dentistry researcher at Baghdad Medical College, left Iraq before the invasion. High-ranking party officials in Saddam Hussein's Baath party would send their children to her college in the expectation that they would be awarded a qualification, even if they hadn't earned one. Gilel refused to wave students through exams and this, together with a family link to a relative who had opposed Saddam Hussein, marked her out. The breaking point for Gilel, who also asked for her name to be changed in this article, occurred in 1994. United Nations weapons inspectors were trying to discover the extent of the country's weapons facilities. During a break one day, Gilel and a colleague spotted men carrying barrels through the school. \u201cWe laughed. We said: these are the chemicals, here they are in our school.\u201d Gilel does not know who overheard, but by the next day her colleague had vanished. A couple of weeks later, after being warned by a friendly security official that she was also being targeted, she fled to Jordan. \n               Transferable skills \n             When she arrived in Britain in 1999, following a stint at a north African university, Gilel wanted to take up her research in dentistry again but found it impossible to get a research position or grants. Initially she did not have full residency status, which was enough to deter some institutions from employing her. And academic research was impossible because the General Medical Council, which oversees British doctors, did not recognize her qualifications and would not allow her to perform studies that involved patients. \u201cAll my experience has gone down the drain,\u201d she says. Even to practise as a dentist in Britain she had to study for two years and requalify, which she finally achieved last November. Even with extensive experience back home, Iraqi academics find it difficult to break into British universities. Many have no connections in Britain, so they try writing fruitlessly to whoever is listed as a contact on a university website. They also have language barriers and are at a disadvantage because they do not understand how the British funding system works \u2014 problems that CARA is tackling by setting up a network of university contacts that can help to advise Iraqi academics. Another harsh reality is that outside Iraq researchers may not have the skills, reputation or publications they need to compete in academia. After Iraq invaded Kuwait in 1990, Iraqi universities were isolated from the rest of the world by sanctions. For many academics, journal articles could be obtained only by sending someone to photocopy papers from libraries in Jordan. So unless they were able to spend time in foreign labs, even the most talented researchers would have fallen behind colleagues abroad. \u201cDuring sanctions the level of knowledge moved on and left quite a lot behind,\u201d says Robertson. Hatem Al-Delaimi is the author of three textbooks and has more than 20 years engineering experience \u2014 but now finds himself desperately trying to update his skills. After fleeing Iraq for Britain in 2002, he sent off countless application forms and never received a single reply, let alone a job offer. But he was able to get unpaid work on a robotics project at Kings College London, and CARA has provided \u00a33,100 to cover around a year of research costs. It's a breakthrough, but perhaps only a temporary one. When the money from CARA runs out, Al-Delaimi will once again need to convince universities to take him on. But by then he hopes to have at least two publications. In the United States, another major destination for fleeing Iraqi researchers, immigrants find the situation slightly easier. They can still struggle to find an opening, says Rob Quinn, director of the Scholars at Risk Network, a New York-based organization that defends the human rights of academics. But neither Quinn nor others working with Iraqis in the United States report the level of difficulty experienced in Britain. One reason is that the US visa system is more flexible than that in Britain. Iraqis often go to the United States to work on temporary visas, for example, and can generally get those visas repeatedly renewed, provided the institution they work for backs their application. (No equivalent visa exists in Britain, says Randall.) The Scholar Rescue Fund, based at the Institute of International Education in New York, is currently funding around 20 Iraqi academics on such visas. \n               More funds \n             Once in the country, those academics must still convince institutions that they are up to scratch and worthy of employment. But this also seems to be easier, partly because a little more funding is available. American universities often have more flexible budgets than their UK counterparts and can find the money for a one-year contract, particularly if part of it is provided by organizations like the Scholar Rescue Fund. The Home Office is talking about changing the country's immigration system and moving towards a system that may look more like that of the United States: one that places more emphasis on admitting those with appropriate skills rather than simply controlling numbers. Lawyers who represent immigrants say they will wait to see details before being convinced. Such changes could take years to materialize \u2014 and offer little solace to Iraqis already in Britain, such as computer scientist Althamir. But in the weeks after meeting Althamir, I learn that he has been offered work with a team working on artificial intelligence at a British university. As an asylum seeker, he cannot be paid \u2014 but he can just about survive on state benefits and the \u00a32,000 that CARA will provide in research funds. It is a step back into research, although it seems a far cry from what a former head of department might hope for. But when I ask Althamir how he feels about going back to work, he corrects me: \u201cI am so happy to join the research team; at least I can catch up with up-to-date research. But it is not going back to work,\u201d he adds. \u201cI understand 'work' as a paid job.\u201d \n                     Iraqi death toll withstands scrutiny \n                   \n                     Scientists become targets in Iraq \n                   \n                     US bungled investigation into weapons research in Iraq \n                   \n                     Iraqi killings prompt calls for US to evacuate weapons scientists \n                   \n                     Council for Assisting Refugee Academics \n                   \n                     Rescue Fund \n                   \n                     Scholars at Risk Network \n                   Reprints and Permissions"},
{"file_id": "446488a", "url": "https://www.nature.com/articles/446488a", "year": 2007, "authors": [{"name": "Jo Marchant"}], "parsed_as_year": "2006_or_before", "body": "How much can geometry and mathematics reveal about paintings? How far should hidden meanings be trusted in art? Jo Marchant investigates the latest, and possibly most controversial, interpretation of a Renaissance masterpiece. The scene is eerily still. In the distance, a half-naked figure is being whipped, watched over by an impassive ruler. In the foreground stand three expressionless men, more richly dressed, apparently unaware of the violence behind them and even of each other. The scene is vivid and lifelike, yet it has a dream-like quality. Nobody speaks. Nobody catches anyone else's eye. No wonder this painting has been described as the pictorial equivalent of silence. This is  The Flagellation , painted by Piero della Francesca in fifteenth-century Italy and reproduced in full on the cover of this issue. An accomplished mathematician, Piero is known for his stunning use of perspective (just look at the tiled floor), which has helped make this painting one of the most famous masterpieces of the Renaissance. But that is not its only fascination. Piero planned his paintings down to the last detail, and this meticulously executed scene contains several mysteries, particularly the identities of the three men on the right. Countless attempts have been made to name them. \u201cThe interpretation of  The Flagellation  is one of those  causes c\u00e9l\u00e8bres , like  The Last Supper ,\u201d says Martin Kemp, an art historian at Oxford University, UK, who has written on Piero's paintings. \u201cIt's in that league as a magnet for theories.\u201d A leading historian of science now believes he has historical evidence that can identify all the mysterious figures at a stroke. David King, director of the Institute for the History of Science in Frankfurt, Germany, says that his interpretation reveals new mathematical features that push our understanding of Piero's geometrical vision beyond anything thought to have been achieved at the time. The consensus on the painting is that it is Christ being flagellated, perhaps along with the Church, or the Byzantine Empire, which fell to the Ottoman Turks in 1453. The ruler sitting on the throne of Pontius Pilate and watching over the punishment seems to be the fifteenth-century Byzantine emperor Ioannis VIII, identified by his characteristic red hat. The man in the turban with his back to us has been identified as either King Herod or the Ottoman sultan, Mehmet II. But the identity and purpose of the figures on the right has been the subject of wild speculation. There is no documentary evidence on who commissioned the painting, why, or even when (though historians think it was late in Piero's career, around 1460). The painter left just one tantalizing clue: according to a report in 1839, an integral frame, now lost, bore the painting's original title,  Convenerunt in unum , or \u201cThey came together in one.\u201d Controversially, King's smoking gun is not a manuscript but a scientific instrument, and its meaning is buried deep within a mind-boggling code. If he is right, the painting can be seen as one of the most ingenious hidden messages of all time. Although some scholars cautiously welcome his ideas, art historians are notably less impressed. King pushes his theory to its limits, and his bullish claims have angered some. King thinks the reluctance to accept his work results from a culture clash between mathematical expertise and art history. Kemp, who has a scientific background, acknowledges that when it comes to studying painters such as Piero, art historians could do with a better understanding of mathematics. \u201cThe maths isn't that hard,\u201d he says. \u201cIf you can do Euclid you can do Piero.\u201d Art historians counter that King has been too quick to disregard criteria they have developed to guide legitimate enquiry into such enigmatic artworks. \u201cIt's not a case of keeping outsiders out,\u201d Kemp agrees. \u201cDavid King is a major historian of scientific instruments. It is a question of how far his huge expertise is transferable without a good deal of caution.\u201d \n               Material evidence \n             King's interest in this story began with a fifteenth-century astrolabe made in Vienna. Now superseded by modern instruments, astrolabes were used by Renaissance astronomers for time-telling, navigation and predicting the movements of the heavens. This particular astrolabe is modest, just 11.5 centimetres across, but it has an unusual inscription on the back (see below). Roughly translated, the inscription says \u201cUnder the protection of the divine Bessarion, said to come from the axis, I arise as the work of Johannes in Rome in 1462.\u201d (The inscription is reproduced at larger size in the graphic.) The couplet suggests that the astrolabe was a gift from Johannes Regiomontanus, a young astronomer and instrument-maker, to his mentor Bessarion. The gift is dated to 1462, roughly the time that Piero painted  The Flagellation . \n               Master and pupil \n             Johannes (Ioannis) Bessarion was a Greek cardinal who famously switched churches to became a cardinal in Rome, following failed attempts to unify the Eastern and Western Churches in 1439. In Rome, Bessarion campaigned for a crusade to fight off the advancing Ottoman Turks, and when Constantinople, the centre of the Eastern Church, fell to the Turks in 1453, the cardinal was devastated. A learned scholar, Bessarion brought hundreds of Greek manuscripts to Italy to be translated into Latin, helping to kick-start the Renaissance there. Bessarion met Regiomontanus in Vienna in 1460, and persuaded the youth to come to Rome as his student. King believes that Bessarion at the time had in his possession a famous Byzantine astrolabe made in 1062 with a Greek inscription, which King has also studied, and that Bessarion showed it to Regiomontanus while in Vienna. Before leaving for Rome, Regiomontanus made Bessarion another astrolabe as a gift, inscribed with a personal message. Flaws in the inscribed couplet have intrigued King for years. The metre is almost \u2014 but not quite \u2014 perfect. The letters are oddly spaced, with some squeezed together and others stretched apart. And the name 'Ioannis' is split over two lines. Regiomontanus was a renowned poet and instrument-maker \u2014 so why would he have let pass such imperfections in a gift to his beloved mentor? These and other loose ends have caused some scholars to brand the astrolabe a fake. King has since described 22 other astrolabes from fifteenth-century Vienna, which he believes validate this one (although not all his critics are convinced). None of these other astrolabes has an inscription, however. Early in 2005, King asked a mature student, Berthold Holzschuh, to study the inscription for a seminar at the Frankfurt institute. A wood-construction engineer, Holzschuh is fluent in Greek and Latin, and has a long-standing interest in astrolabes. He turned up at the seminar beaming. He had spotted names and meanings hidden within the inscription, which explained why the couplet was so oddly arranged. A date referring to the original Byzantine astrolabe, 1062, can be found in the lower right of the inscription, using the IO from IOANNIS, perhaps explaining why Regiomontanus split that word. Reading down the left-hand side, Holzschuh noticed the phrase SVB CD ANNIS \u2014 'at 400 years' \u2014 which would refer to the anniversary of the Byzantine astrolabe. And reordering the words in the inscription produced the message: \u201cUnder the protection of Bessarion, I arise in Rome in 1462 as a work of Johannes explaining the rotation of the universe,\u201d which is reminiscent of the Greek inscription on the Byzantine astrolabe. It is the sort of puzzle that Bessarion would have appreciated, says King, and that the brilliant Regiomontanus was perfectly placed to devise. The mystery of the imperfect couplet thus apparently solved, King and Holzschuh wrote a paper on the topic. But when King went to Holzschuh with the finished manuscript, the engineer had another surprise for him. In researching Bessarion, Holzschuh discovered that the cardinal had been proposed as the bearded figure in  The Flagellation , and he found that several more names identified in the inscription also appear in interpretations of the picture. He made an enlarged photocopy of the inscription, the same width as the painting, and laid it underneath (see graphic). Names in the poem started to line up with the appropriate figures. Could it provide a key to the painting? King has since extended Holzschuh's findings into a 300-page book intended for publication this year. He looked for clusters of letters suggesting names that would have been important to Bessarion, and found they fell into eight vertical groupings across the inscription: \u201cWe tried other combinations but they didn't work.\u201d King concluded that the eight vertical groupings correspond to the eight figures in the painting. \n               Cryptic crossword \n             For the figures on the left, King finds clusters of letters suggesting Latin names that roughly agree with previously proposed identities. For the man on the throne, King picks out references to Ioannis VIII and Herod, as well as the word SEDES, meaning throne, and suggesting Pontius Pilate. For the man being whipped, he identifies references to Christ and the Church, and for the turbanned figure, names suggesting the Ottoman sultan and Herod again. For the two figures whipping Christ, King associates the man on the right with Herod, Pontius Pilate and Caesar. But for the man on the left, King sees the letters IVDAEUS \u2014 suggesting Judas. This identity has not been taken seriously before, but King says it is supported by the painting itself, which has the figure reaching out and touching Christ: \u201cJudas Iscariot was the disciple close enough to touch Jesus,\u201d he says. Another name that occurs is George of Trebizond, with whom Bessarion had a long-running scholarly feud. So which identity is correct for each figure? This is where King's interpretation departs fundamentally from previous attempts. He argues that there is not just one identity intended for each man, but several, explaining why it has been so difficult to assign the figures to any one person. Like the poem, the painting conceals multiple meanings, depending on how it is read. Once King was satisfied that the poem matched up with the flagellation scene, he turned his attention to the men on the right. For the bearded figure, King sees the name Bessarion. For the angelic youth, he sees Regiomontanus, and also the names of three talented young men who had been close to Bessarion but were felled by disease \u2014 his dead godson Buonconte and two of his friends' sons, Bernardino Ubaldini and Vangelista Gonzaga. Bessarion transferred all his lost hopes for these young scholars to the promising Regiomontanus, says King. The man on the right is associated with three people: the nobleman Giovanni Bacci, previously suggested as a possible sponsor for the painting, Bernardino's father and Vangelista's adoptive father. \n               Multiple identities \n             King believes that subtle clues in the painting confirm his interpretation. For example, the angelic face of the young man in red is explained by the dead youths. And the gown of the figure on the right is embroidered with thistles, perhaps hinting at dalla Carda, the name of Bernadino's father (the Italian for thistle is  cardo ), and holds purse strings, hinting at the sponsor, Bacci. According to King, many times, events and people are overlaid in one frozen scene. The painting represents not just the flagellation of Christ, but what Bessarion saw as the betrayal of the Eastern Church. It also serves as a consolation for the lost young men, and celebrates the arrival of Regiomontanus. And it demonstrates just how far it is possible to go with the concept described in the painting's enigmatic original title: \u201cThey came together in one.\u201d King suggests that when Regiomontanus conceived the inscription, he added a few concealed meanings, mostly relating to the older astrolabe, which he thought would amuse his teacher. But as Bessarion studied the letters, he may have seen other chance combinations \u2014 you can see anything in this kind of puzzle if you look hard enough \u2014 that meant something to him, and so the idea for a painting may have emerged. Transferring the hidden identities into a painting would have been the ultimate representation of Bessarion's feelings about his life and the Church. There is no direct evidence that Bessarion commissioned the painting from Piero, but they were known to each other (Bessarion is featured in Piero's most famous work, the wall frescoes at Arezzo). And there are several occasions at around the right time when they may have been in the same town. So how do we know that King is not just seeing what he wants to see, as he believes Bessarion once did? Beyond the hidden characters he has identified in the painting, King says his evidence connecting the astrolabe and the painting is mathematical. According to King, the epigram and the painting both hint at a ratio called the 'divine proportion' or 'golden ratio', which is aesthetically pleasing. It describes a line divided such that the ratio of the lengths of the two sections (A:B) is the same as the ratio between the whole line and the larger section (A+B:A). The current view among art historians is that although the ratio was known during the Italian Renaissance, there is no evidence that Piero, or anyone else, used it in their art. Regiomontanus would have been familiar with the divine proportion from his studies of the geometry of Euclid, and King believes that the dates of the two astrolabes (1062 and 1462), which cut their centuries roughly in the divine proportion, may have reminded him of it. In King's interpretation, the eight vertical groupings in the poem match the geometry of the painting by lining up with the eight figures. King identifies two vertical lines at 3/8 and 5/8 of the inscription's width, drawn through the B and the I of BESSARION (that is, his initials), which each divide the inscription in something close to the divine proportion. These match vertical lines drawn between the eyes of Christ and of the bearded figure, two important figures in the painting (see graphic). King believes that contrary to the standard view, Piero may have used the golden ratio in his painting. The vertical line between Christ's eyes divides the flagellation scene (its edges defined by the bordering columns) almost exactly in the divine proportion. To get this effect, Piero would have had to position the observer's viewpoint very precisely, thus fixing the position of the 'vanishing point' of his carefully worked out perspective. King is careful to describe his ideas as a \u201chypothesis\u201d. But he argues that his interpretation fits with what is known, and solves many of the mysteries surrounding the painting. \u201cIt's one of the most spectacular discoveries in the history of science,\u201d he says. Many disagree. Some art historians have dismissed the idea entirely, describing it to  Nature  as \u201cnonsensical\u201d, \u201cembarrassing\u201d and \u201cutter rubbish\u201d. Unlike the most widely accepted interpretation of Piero's painting, by Princeton art historian Marilyn Lavin (see  'The Establishment view' ), they feel King's hypothesis stretches credulity too far: if you layer enough subjective assumptions on top of each other, they argue, you can find anything you like. Kemp is one of the more moderate voices from that community. But even he is highly sceptical about any link between the astrolabe and the painting. \u201cIt requires a substantial leap of faith,\u201d he says. \u201cWhat is the concrete reason for making the connection?\u201d There is more enthusiasm for King's ideas outside the art-history community. Jo\u00e3o Pedro Xavier, an architect and geometrician in Porto, Portugal, who has studied Piero, finds King's measurements persuasive. Piero wrote extensively about the divine proportion in his mathematical work on regular polyhedrons, Xavier notes, and it follows naturally from his work with perspective. \u201cHe knew these relations, he could do it almost exactly.\u201d And if Piero was going to use the divine proportion anywhere in his art, \u201cthe person to put in such a position is Christ\u201d. Of course there is no evidence that Piero intended this golden ratio, but for a painting that has been so intensively studied, \u201cwhy did nobody notice this before?\u201d Xavier says. \u201cIt is strange.\u201d \n               Divided opinion \n             Neil Graves, a professor of English at the University of Tennessee in Knoxville and an expert in hidden meanings in literary texts, says that in the absence of direct evidence of what an author intended, it is necessary to consider factors such as whether a practice was common at the time, and how likely it is to have occurred by chance. He is taken with King's and Holzschuh's reading of the astrolabe inscription, and agrees that the odd spacing was probably intentional, to allow extra meanings. Word-play and hidden readings were particularly popular during mediaeval and Renaissance times. Signatures were commonly written as clusters of letters: Christopher Columbus adopted a four-line acrostic that has never been fully decoded. But both Xavier and Graves baulk at King's attempts to link his inscription geometrically with the painting. Both feel that it seems arbitrary to divide the painting vertically at the eyes of Christ and of the bearded figure, and to split the poem at the B and the I. Graves says that King deserves a hearing, however. \u201cI don't think he proves his argument, but he makes an interesting and sensible case.\u201d King is undaunted by the criticism he has received, and believes that some art historians will dismiss his work because they can't understand it. \u201cThe epigram and painting are mathematical in nature,\u201d he says. \u201cNo art historian has ever looked at the basic geometry of the painting\u201d. But even an expert with mathematical training, such as Kemp, says that drawing any conclusions from measurements alone is fraught with problems. Part of the difficulty is deciding what to measure and where to measure it from and to, especially on a complicated painting like  The Flagellation . \u201cYou are likely to hit something,\u201d Kemp says. \u201cI want to see direct evidence.\u201d Such evidence might be lines drawn underneath the paint. Like other art historians approached by  Nature , Ellen Handy of the City College of New York worries that King may be jumping too quickly to conclusions, but she acknowledges that art history often ignores mathematics. \u201cIronically, many of those who consider themselves as art historians don't have the training that the artists of the time did,\u201d she says. Many Renaissance artists, such as Piero, were skilled geometricians. \u201cWe are not. We can learn from those who have that mathematical training now.\u201d Architect James Bradburne, also a cultural historian and director general of the Palazzo Strozzi in Florence, acknowledges that proof may never be found, but supports King's ideas nevertheless: \u201cIf this is accepted even as a plausible hypothesis, then it says that scientific objects can legitimately be treated as historical documents, in the same way as paintings themselves have been. Scientific objects can be considered part of the puzzle.\u201d \n                 For more information on King's theory\u00a0\u2192  \n                 \n                     http://web.uni-frankfurt.de/fb13/ign/Code.htm \n                   \n               \n                     Fractals and art: In the hands of a master \n                   \n                     Computers confront the art experts \n                   \n                     Virtual art: Art that draws you in \n                   \n                     Moon dates Van Gogh \n                   \n                     Artists on science: scientists on art \n                   \n                     David King's hypothesis \n                   \n                     Wikipedia page on Piero's  The Flagellation \n                   \n                     Marilyn Lavin's web page \n                   Reprints and Permissions"},
{"file_id": "446608a", "url": "https://www.nature.com/articles/446608a", "year": 2007, "authors": [{"name": "David Cyranoski"}], "parsed_as_year": "2006_or_before", "body": "Can a vast monoculture plantation be at the forefront of biodiversity protection? David Cyranoski meets conservation biologists who hope to save species by making peace with the enemy. The forest in Borneo is usually a lush affair. But in the province of Sarawak, some forest regions have a striking monotony. Here, the skinny trees are all the same variety of acacia, some 20 metres high with short, jutting branches. The trees are spaced in regular rows, three metres apart. Acacia mangium , a fast-growing species that is good for pulp and paper, is not native here. But at these nurseries on the Malaysian part of Borneo, 70,000 hectares of forest have been razed and replaced with a monoculture of acacias, grown at a rate of 3 million seedlings a month. Grand Perfect, a consortium owned jointly by three local timber companies, eventually plans to cover a few hundreds of thousands of hectares like this. The acacias will grow for just seven years before being harvested. Eventually, 30-tonne trucks will come every few minutes to carry away 3.5 million tonnes of wood a year. This factory-like image is not what an environmentalist might call conservation. But many scientists are calling it the future of biodiversity protection. It is Grand Perfect's Planted Forests Project, a 490,000-hectare experiment in which just under half the land is set aside for logging, around a third for conservation and the rest for use by indigenous groups. In this unusual mosaic, customary foes \u2014 timber companies, conservation biologists and local tribes \u2014 are working together to create a forest-management scheme that can meet all their respective needs. Each group has much to gain, or lose. For conservationists, the project's land is a precious piece of southeast Asia's famously biodiverse tropical forest, an important 'hotspot'. Globally, tropical rainforests are disappearing at a rate of 12 million hectares a year. The Sarawak government, which commissioned and funds the project, and the timber companies that it has contracted, stand to gain tens of millions of dollars, if not more. Those living in the forest want to preserve their right to roam, hunt, fish and farm, and are being promised infrastructure and employment. Rob Stuebing, a herpetologist who is the head of Grand Perfect's conservation division, is in the uncomfortable position of trying to reconcile these demands. It requires careful diplomacy. He argues with the loggers and pleads with the locals. He also deals with conservationists and fellow scientists, some of whom share his excitement about this new approach. Others charge that the effort is just an attempt to put an environmentally friendly gloss on a typical logging project. Stuebing's greatest challenge will be to prove the sceptics wrong. He needs to show that biodiversity can be preserved within an area devoted mostly to a non-native monoculture, something that has never been proved before (see  'Tree swap' ). As a conservation biologist taking up a position with a timber company, is Stuebing shaking hands with the devil? \u201cI had misgivings at first,\u201d says Stuebing. \u201cBut you can't change a whole system by working against it. The only way to do it is from the inside.\u201d Stuebing is a 30-year veteran of Malaysia's natural history who also has experience in the timber industry. He is tall, lean and seems more comfortable dressed for the field than in the suit and tie that his current job sometimes demands. He also has little patience for the most idealistic of his critics, who say that true conservation would be to enforce a hands-off policy. Since he joined Grand Perfect in October 2004, Stuebing has moved full-speed ahead on his mission: an inventory of the wildlife in the massive planted forest zone and coordination with the timber companies on a foresting strategy that protects, however imperfectly, what he finds. The plantations were started in 1996 without any biodiversity project in place, and some critics have said that the inventory is starting tens of thousands of hectares too late. In January, Stuebing held a conference called Biodiversity Conservation in Tropical Planted Forest in Southeast Asia. Held in Bintulu, a coastal town 45 kilometres from the inland planted forest zone, the meeting was the first opportunity for all interested parties to discuss the future directions of his project and work out how to assess its progress. Some 70 naturalists, taxonomists and conservation biologists as well as representatives from the timber companies and the government attended the conference. The Sarawak chief minister, Pehin Sri Haji Abdul Taib Mahmud, called it a \u201cdream come true for Sarawak's forest and biodiversity conservation\u201d and, in grand ceremonial fashion, planted the plantation's 88 millionth tree. The local authorities love the scheme because they can make money from the land and still feel good about conservation. \n               Chop and change \n             Like many lush tropical forests, much of the area allocated to the planted forest zone is already 'disturbed'. For centuries, indigenous groups have chopped up parts of the forest to make room for farmland. Large-scale industrial logging, starting some 40 years ago, took a greater toll. Original 'old-growth' forest occupies at most 20% of the area; what has grown back on the disturbed land is called secondary forest. The idea that these forests are already degraded and will continue to change is an important, and controversial, one. Some experts say that trying to return biodiversity to some ideal past era is unrealistic. Jianguo Wu, a specialist on sustainability at Arizona State University in Tempe, opened the meeting with an appeal to consider forests as something to be used for sustainable development rather than as a static entity. Others vehemently disagree, and say that in the past five or ten years the degradation has become a frequently used excuse to convert forests to plantations in Malaysia and Indonesia. \u201cAll the forests need is some rest,\u201d said one silviculturist who did not want to be named. Everyone agrees that there are limits to what can be called biodiverse. On the other side of the highway from one of the acacia plantations lies a wasteland of terraced, orangeish soil \u2014 the scar of an oil-palm plantation. Malaysia is the number one producer of oil palm, but this type of forestry is the worst-case scenario for the conservationists. While Hollywood stars talk up the environmental friendliness of biofuels such as palm oil, here, the plants that produce them are demonized because they leave little room for anything else to live. If nothing else, everyone hopes that the acacia plantations will be more forgiving to their environment. But what is at stake? Borneo is home to some 1,300 known mammals, birds, frogs, snakes and lizards, about two-thirds of which are thought to be in and around the planted forest zone. Stuebing estimates that around 20% of the mammals and perhaps 60% of the amphibians are endemic, existing here and nowhere else. But these are approximations. The area has never been explored thoroughly, and the project's first priority is to take an inventory. As the loggers move in, there is a sense of urgency because many species could be lost before they are ever discovered. At the conference, Stuebing called on all scientists to help identify the forest's wealth of organisms. Many, keen to expand their collections, have already taken up the call and their initial forays have turned up plenty. Jaap Vermeulen of the National Herbarium of the Netherlands at Leiden University has found 83 species of snail in a region called Bukit Sarang, part of the area devoted to conservation. \u201cThirty-one of these are new to science. It is exceptional, a really hot piece of real estate,\u201d he says. Stuebing's challenge is to use the conservation zones, which are marbled into the plantations, to simulate a more extensive forest that has room for large mammals to roam. When local populations go extinct, larger forests also support 'metapopulations' that can repopulate them. Stuebing works with the loggers to designate corridors \u2014 such as 50 to 500-metre-wide swathes of land alongside rivers \u2014 that are crucial for mobility of many species. Maintaining the survival of rare, symbolic flagship species, such as hornbills, tufted ground squirrels and gibbons, would be a huge success. But Stuebing is not sure that his corridors will be long or wide enough. \n               Support structures \n             The biggest question mark, and one that could be crucial to the success of the entire venture, is whether the acacias will support wildlife. Frogs, rats, squirrels, ungulates and carnivores seem to do well in acacias because they like the food and breeding sites. Understory birds, snakes, lizards and bats, however, are less happy. Bill McShea, an ecologist at the Conservation and Research Center of the Smithsonian National Zoological Park in Front Royal, Virginia, uses camera trapping to follow large mammals. He found that about two-thirds of the 39 species he had identified in the local 'disturbed' forest were also living in the acacias. \u201cSo does [this result] mean they are running for their lives? Or are they living there stably? It is impossible to tell right now,\u201d he says. And even if the animals are settled, no one knows what will happen when the acacias are harvested after seven years. Stuebing admits that, with only 2% of Grand Perfect's total budget, the conservation programme is a relatively small concern for the timber companies, so working to influence them is crucial. But the companies are coming under increasing pressure from their buyers to certify that pulp and paper exports come from rain forests that are managed sustainably. Such concerns give Stuebing more leverage to fight for wider corridors. Grand Perfect says that it is also trying to change its logging practices by, for example, using an elevated cable system to extract logs rather than dragging them on the ground and eroding the soil. \n               Lap of luxury \n             Grand Perfect's efforts to integrate scientists \u2014 and its construction of relatively luxurious field stations \u2014 impressed the conservation biologists who attended the Bintulu meeting. \u201cNever before has a commercial enterprise spent so much time ensuring that there is a biodiversity programme in place,\u201d says the Earl of Cranbrook, chairman of the International Trust for Zoological Nomenclature and a 50-year veteran of the southeast Asian natural-history scene. Because the acacia plantations are very productive, some argue that they will also relieve the pressure to cut into other, more pristine forest. Warren Ellis, who represented Grand Perfect's logging division, told the meeting that a hectare of acacia plantation produced more wood than ten hectares of forest that had been logged and had then regrown naturally. Hans ter Steege, a plant ecologist at the National Herbarium of the Netherlands in Utrecht, is also optimistic. \u201cI would approach the project with a positive attitude. All the right ingredients \u2014 social, economic and ecological \u2014 have been addressed.\u201d But not everyone is happy. Some scientists have accused the project of failing to take into account a key factor \u2014 the incredible diversity of the indigenous trees. Peter Ashton, a retired expert on the forests in Sarawak, points out that the Sarawak lowlands contain some 3,000 species of tree and that the trees support the insects and microorganisms that account for the vast majority of the forest's biodiversity. \n               Token gestures \n             Conservationists have also become cynical of nods to their cause by loggers. A 2006 report by the Center for International Forestry Research in Indonesia found that some companies are recruiting billions of dollars from investors by saying that they have sustainable sources to feed their pulp mills when, in reality, they are relying on natural forests in Indonesia for most of the supply. Gustavo Fonseca, the chief conservation and science officer at Conservation International in Arlington, Virginia, questions whether the proportion of land given over to native forest in the project is high enough, and says that road-building and increased fire-hazards \u201cmay create impacts far beyond [those of the] plantation\u201d. Fonseca also notes that  A. mangium , although not thought to be invasive, has shown signs of spreading spontaneously in the Caribbean. As yet, there is no pulp mill, and the logging will probably not start in earnest for several years. By then, Stuebing and his colleagues will have a better idea of whether their patchwork experiment will work. If it does not, the acacia harvesting will still proceed. The data obtained could still be of use, because they will probably be the best record yet of how acacia plantations affect biodiversity, something that is important for the many other plantations in Sarawak and elsewhere in Borneo. And Stuebing looks on the bright side: \u201cAt the very least we have done some good science.\u201d The conservationists may not share his rosy outlook, but they \u2014 and the loggers they traditionally eyed with aversion \u2014 may also gain a new perspective. \u201cSome romantics, like myself, thought that all was lost with the cutting of the rainforests,\u201d says Robert Inger of the Field Museum in Chicago, who has worked in Borneo for decades. Like most at the conference, Inger now recognizes that demand for timber means the inevitable end of many species. He has watched frogs try to familiarize themselves with their new environment in the acacias and is hopeful that they have set up shop there. \u201cWe may have to look at the forests again with less romantic eyes,\u201d he says. See Editorial,  \n                     page 583 \n                   . \n                     Environmental activism: In the name of nature \n                   \n                     Only 5&#x25; of tropical forests managed sustainably \n                   \n                     Calls to conserve biodiversity hotspots \n                   \n                     Brazilian Amazon being cut down twice as fast \n                   \n                     2007 regional conference on biodiversity conservation in tropical planted forests \n                   \n                     World Forestry Centre \n                   \n                     Department for International Development and the EU presidency \n                   \n                     The IUCN/WWF Forest Conservation Newsletter \n                   Reprints and Permissions"},
{"file_id": "446600a", "url": "https://www.nature.com/articles/446600a", "year": 2007, "authors": [{"name": "Jeff Kanipe"}], "parsed_as_year": "2006_or_before", "body": "Vast stellar nurseries, clouds that dwarf the Solar System and lurking swarms of black holes. Jeff Kanipe probes the unfolding mysteries at the heart of the Milky Way. Before it was seen, it was heard. In the early 1930s, a Bell Labs engineer named Karl Jansky was given the job of sorting out where the static interference in radio transmissions came from. With an ungainly but ingenious steerable antenna he tracked a number of sources. Most were thunderstorms, but one wasn't. As Jansky tracked it across the sky from day to day he realized that it was far beyond Earth's atmosphere, and indeed beyond the Solar System \u2014 an abiding hiss from somewhere in the constellation of Sagittarius 1 . As constellations go, Sagittarius is modest both in size and in brightness. What sets it apart, on dark, moonless nights, are its background contrasts: brilliant, billowy clouds of stars that are punctuated by dusky rifts and voids. No other place in the sky looks this compelling. By the time of Jansky's discovery, the behaviour of other objects in the sky had already provided good evidence that something special lay within those beguiling clouds. In 1918, a study of star clusters by Harlow Shapley, an astronomer at the Mount Wilson observatory above Los Angeles, showed that 'open' star clusters were spread throughout the plain of the Milky Way, whereas globular clusters were concentrated in the direction of Sagittarius 2  \u2014 some above the Milky Way, and some below it, drawn by some unseen immensity. The globular clusters were like moths batting about a lamp hidden in Sagittarius's dense folds of dust. \n               Hidden behind the clouds \n             Jansky's observations provided the first hint of what lay behind those shrouds, but it took decades for further details to become clear. It was not until 1968 that the radio source at the centre of the Galaxy, now called Sgr A *  (or 'Sagittarius A-star'), was detected in the infrared, showing that it was 1,000 times brighter than the radio emission had led astronomers to suspect 3 . At shorter infrared wavelengths \u2014 which like their longer brethren pass through dust much more easily than visible light \u2014 it was even brighter. By this time, astronomers gazing elsewhere in the sky had discovered quasars, bodies so bright and yet so small that it seemed possible they were powered by vast black holes sucking up dust and gas at an incredible rate. In 1969, the British astronomer Donald Lynden-Bell suggested that our Galaxy and its neighbours could all have 'dead quasars' at their hearts 4 . By this line of reasoning, Sgr A *  had to be a vast black hole. Even with a compelling theory, seeing the details was hard. Telescopes using visible light \u2014 even the mighty Hubble \u2014 could not see through the clouds of dust. But in the past ten years, powerful radio arrays, new infrared and X-ray telescopes, detectors in orbit and adaptive-optics systems on Earth have revealed strange new structures in and around the Galaxy's central engine: magnetic arcs and filaments, giant clumps of massive stars and whorls of gas. Analysis of the motions and masses of the stars within the central two light years of Sgr A *  have shrunk the known heart of our Galaxy down to a region of space no larger than Earth's distance from the Sun, and probably much smaller, containing the mass of four million Suns. For all this insight, the heart of the Galaxy still has mysteries for astronomers to explore \u2014 from echoes of outbursts to the silhouette of the event horizon itself. One problem with observing the very centre of the Galaxy, though, is that it simply isn't very bright \u2014 a firefly to the searchlight of a full-blown quasar. The obvious explanation for its lack of luminosity, says Andrea Ghez, principal investigator of the Galactic Center Group at the University of California, Los Angeles, is that even though Sgr A *  lies at the heart of a galaxy of hundreds of billions of stars, it may be a bit isolated. The radiation from black holes comes not from the holes themselves, but from matter falling onto the accretion disks that swirl around them. There may just not be much matter around to fall onto the accretion disk at the centre of the Galaxy. Or the disk may be generating a wind of radiation strong enough to stop any more gas and dust flowing into it. \n               Unexpected echo \n             Nevertheless, the flow sometimes increases and the central engine heats up. For a couple of years in the 1950s, for example, Sgr A *  looked perhaps a hundred thousand times brighter in the X-ray spectrum than it is today, probably because it swallowed a planet's worth of gas in a gulp. Unfortunately, humankind had no X-ray telescopes in the 1950s \u2014 the devices only work in space \u2014 which might seem a serious impediment to learning from the event. But it is not, it turns out, an insurmountable one. At the January 2007 meeting of the American Astronomical Society in Seattle, Washington, Michael Muno of the California Institute of Technology (Caltech) in Pasadena announced that his team had managed to see part of the 1950s outburst reflected off clouds on the far side of the Galactic Centre 5 . X-rays that had started off heading away from Earth had bounced back to us, and because the clouds were a few tens of light years away from the centre, the reflected X-rays took half a century longer to get to Earth than did those that had taken the direct route. Half a century isn't much in terms of a journey across the 26,000 light years that separate Earth from the Galactic Centre, but it's enough to make the difference between astronomers stuck underneath Earth's X-ray-absorbing atmosphere and astronomers who, like Muno's team, can use NASA's Chandra X-ray telescope to monitor the centre of the Galaxy. \u201cThis is the first X-ray echo that we have seen propagating through space after an event that we had not originally seen,\u201d says Muno. The observations allowed his team to say that the burst must have been 1,000 times brighter and 1,000 times longer than the contemporary ones seen with the Chandra telescope or the Japanese Advanced Satellite for Cosmology and Astrophysics. The intermittency of such events could imply that the disk of material swirling about the black hole is both meagre and unstable, only occasionally dropping a gobbet of matter into the black hole's maw. But if the black hole's neighbourhood is by and large empty, how can we account for the family of bright young stars that swarms about it? This apparent paradox received prominent attention at the Galactic Center Workshop held in Bad Honnef, Germany, in 2006. The black hole's inactivity suggests that the central few light years doesn't contain enough raw material to make stars. And the enormous gravitational tidal forces around the black hole would seem to prohibit stars from forming even if the material were there: it's hard for a cloud of gas to contract into a star under its own gravity when something that weighs as much as four million stars is sitting next door. Nevertheless, says Ghez, at the Galaxy's core is a swarm of about 40 massive young stars; they are called 'S stars' because they belong to the Sgr A *  cluster. One, called S0-2, has a mass that is some 15 times that of the Sun and orbits Sgr A *  with a period of just over 15 years. At its closest, it comes within 17 light hours of the supermassive black hole 6  \u2014 as close as the edge of our Solar System is to Earth. After 12 years of monitoring the motions of these stars using the infrared capabilities and adaptive optics of the W. M. Keck Observatory on Mauna Kea, Hawaii, Ghez's Galactic Center Group, has almost seen them make complete circuits of the centre and return to where they started: \u201cWe should see S0-2 close [its orbit] in 2010,\u201d she says. The orbits of the central stars of the Galaxy can be used to further refine the mass of the central black hole and to constrain the distribution of mass in the neighbourhood. And their motions might also reveal something about how they got there in the first place. There are two explanations for the stars' presence. One theory is that the stars formed more or less where they are today, near the black hole. In principle, this could have occurred if the density of the gases in the centre of the Galaxy was much higher in the past. Higher density would allow clumps in the clouds to collapse to form stars, even in the presence of a strong gravitational field. The alternative explanation is that the stars formed outside the adverse con-ditions of the central region and migrated there later on as a single massive cluster. However, for this to work the core of the original cluster would have needed a mass that was ten million times greater than that of the Sun, packed into a volume of no greater than 3 light years, which is more compact than any cluster known. At the moment, most astronomers seem to favour the first scenario. But although young stars may not be migrating into the central zone, very old ones probably are. Theorists at the Galactic Center Workshop described recent simulations that bolster a striking prediction first made by Mark Morris of the University of California, Los Angeles, in 1993. Morris postulated that the inner three light years of the Galaxy's centre might contain as many as 20,000 star-sized black holes. These are the remnants of previous generations of bright young stars, which have sunk slowly in towards the central and much larger black hole over billions of years. The presence of a close-knit cluster of dead stars is supported by Chandra's discovery of four bright but variable X-ray sources \u2014 within 3 light years of Sgr A *  (ref.  7 ). The sources' variability is a characteristic of systems in which matter from a normal star is sucked onto a black hole or an ultradense neutron star. Four fairly easily discerned X-ray sources of this type in such a confined region, say astronomers, provide strong circumstantial evidence that tens of thousands of black holes and neutron stars have settled in and around Sgr A * . \n               Smouldering stars \n             To see some real clusters, we have to step back a little. A hundred light years from Sgr A *  lie the Arches and Quintuplet clusters \u2014 two of the most massive young clusters and unlike either the open or globular clusters seen elsewhere. The stars in the Arches cluster are 50 times closer together than are those in the neighourhood of our Solar System. At the density seen in the Arches cluster, the space between the Sun and its nearest neighbour would contain 100,000 stars. The other cluster, the Quintuplet, is a bit older and more dispersed, but it has one of the biggest and potentially most volatile stars known, called the pistol star (so named for the pistol-shaped nebula in which it lies). The source of these mammoth star clusters are giant molecular clouds \u2014 cool, dense complexes of dust and hydrogen gas up to 130 light years in breadth and containing the mass of between 10,000 and 500,000 Suns. Instruments such as those on NASA's Spitzer space telescope can see the parts that contain young stars, which glow brightly in the infrared. Astronomers working at longer wavelengths can see even earlier stages of the star-birth process. New maps of the Galactic Centre made at the Caltech Submillimeter Observatory (CSO) on Mauna Kea, Hawaii, reveal objects so early on in their development that they can't yet be called stars. \u201cWhat we see are usually cores of clouds that have not necessarily begun to form stars yet, or are in the early stages of doing so,\u201d says Elisabeth Mills, who is a member of CSO's Bolocam Galactic Plane Survey. \u201cWith millimetre-wave data you get a more unbiased census of where star formation begins to occur. You see all of these nurseries, and whether or not they have a 'baby' in them yet.\u201d A mosaic of these maps reveals a different side to the Galactic Centre. The bright clusters seen in Spitzer images are diminished; the cores of cooler clouds blossom with light, indicating that they have yet to collapse into massive protostars. These regions, which form a ridge-like structure in images taken at millimetre wavelengths, might one day form a chain of clusters like those in the Arches or Quintuplet, unless gravitational forces from the black hole disrupt their formation. Such structures, says John Bally, the principal investigator of the survey, \u201care unique to the Galactic Centre region.\u201d \n               The Galaxy's dark heart \n             The one structure that is absolutely unique, though, is the supermassive black hole itself. Given its apparent size and proximity to Earth, says Geoffrey Bower of the University of California, Berkeley, it affords astronomers their best chance to image the black hole's event horizon \u2014 the boundary beyond which no light can escape. At the moment, the best observations of the black hole and its accretion disk are those made by a technique that links radio telescopes around the world, called very long baseline interferometry. At a distance of 26,000 light years, an interferometer working at radio wavelengths with a baseline the size of a planet should be able to resolve details as small as the orbit of the Earth. Unfortunately, the radio waves from Sgr A *  pass through intervening regions of highly ionized gas, which scatter its radio emissions. \u201cThese random distortions blur the image of Sgr A * , much like frosted glass blurs an image,\u201d says Bower. Bower and his peers hope that new generations of interferometers working at millimetre and submilli-metre wavelengths, which are less subject to intervening distortion, might in the long run actually reveal the black hole's event horizon 8 . How would that look to outside observers? Depending on its orientation, astronomers think that the relativistic effects of the black hole's intense gravitational field would make the event horizon appear as a large shadow or a silhouette cast on a background of bright plasma, in which the shadow is the boundary where light passes into the throat of the black hole itself. \u201cThis image can be made with a network of millimetre- and submillimetre-wavelength telescopes distributed around the Earth,\u201d Bower says. Some of these telescopes already exist. Others, including the largest, the Atacama Large Millimeter Array, are under construction. Lashing them together into an ad-hoc interferometer the size of Earth, though, is a daunting technological challenge. That, says Bower, \u201cis part of the thrill of the chase\u201d. Such an observation would, of course, be a milestone \u2014 the first direct proof that an event horizon, and therefore a black hole, exists. And observing 'hot spots' orbiting the black hole would allow astronomers a qualitative way to test the effects of relativity in a strong gravitational field, an endeavour that has so far yielded ambiguous results 9 . The Galactic Centre may no longer be the mystery it was in Shapley or Jansky's day, but the better known it is, the more remarkable it looks \u2014 and it promises to become even more remarkable before too long. \n                     MIRLIN GC image \n                   \n                     Images and Animations: COSMUS, Open Source Science Outreach \n                   \n                     UCLA Galactic Center Group \n                   \n                     Simulations of Black Hole silhouette \n                   Reprints and Permissions"},
{"file_id": "446605a", "url": "https://www.nature.com/articles/446605a", "year": 2007, "authors": [{"name": "Rex Dalton"}], "parsed_as_year": "2006_or_before", "body": "Philadelphia's venerable natural history museum is teetering on the brink of financial disaster. A new president recently took the helm, but can he save one of America's great institutions? Rex Dalton reports. The Academy of Natural Sciences in Philadelphia is best known for its vast natural history collections from some of America's founding pioneers: Benjamin Franklin, Thomas Jefferson, Meriwether Lewis and William Clark. But a visitor today might meet some decidedly less august figures coming through the door: express packages full of snails and slugs, sent to the museum regularly from pest inspectors at US borders. When North America's first natural history museum was founded in 1812, no one envisaged that it would one day end up fighting invasive species. But the centuries have shown the huge importance of the academy's collection of 17 million specimens. Academy scientists rely on its vast holdings and library to identify animals and plants that have crept into new areas, and their expeditions have extended into far-flung corners of the world. Last year, for instance, a researcher from the academy reported finding the Tiktaalik fossil \u2014 a veritable missing link in the evolutionary journey from water to land some 375 million years ago \u2014 on a near inaccessible island off Greenland 1 . Yet despite the academy's rich history and modern discoveries, it is struggling to survive. Last year, financial problems prompted it to sell 18,200 specimens from its historic mineral and gem collection \u2014 a decision that became public at the same time as the city hosted Geological Society of America's annual meeting. The move infuriated scientists, many of whom saw it as selling off a precious piece of history. Now, though, the sale seems to have marked the academy's low point. A new president, William Brown, arrived at the helm in January, bringing with him a strong background of having righted a previously troubled museum. For the past five years Brown, an ornithologist with a law degree from Harvard, served as president of the Bishop Museum in Honolulu, the official state museum of Hawaii. During his term, Brown extricated the museum from its financial and public-relations challenges. \u201cHe really left the museum in a better place,\u201d says Haunani Apoliona, a Bishop board member who is also an elected trustee of the state's Office of Hawaiian Affairs. \u201cEveryone is sorry to see him move on.\u201d Brown seems to have his work cut out for him in Philadelphia. \u201cThe academy has been waiting for someone like Bill for a long time,\u201d says museum trustee Thomas Lovejoy, a biologist and president of the Heinz Center, a progressive advocacy institute in Washington DC. \u201cThere is plenty for him to do, particularly attending to a bruised staff that has been through some rough years.\u201d Before Brown arrived, administrators had on occasion threatened to fire anyone who spoke out publicly about the museum's problems. Financial woes are the first of many problems. At about US$55 million, some say that the academy's endowment is lower than it needs to be to support a research institution of its size. For the past decade, academy deficits have averaged some $700,000 a year, although it did end up $2 million in the black for 2005 on its $17-million operating budget. The debts have taken their toll on staff; over the past ten years, curators have been laid off in waves, with three of the ten remaining being laid off in 2005. The mineral sale netted the academy more than $1 million, which will be used to bolster its renowned library. But with Brown now in place, a proposed sale of another 7,300 specimens has been halted. This collection of North American minerals was donated by the Vaux family nearly 125 years ago, who stipulated that they must never be sold. That kept the collection out of the October sale, and now Brown and academy scientists are drawing up plans to exhibit the minerals for the first time in decades. Many academy scientists are relieved; selling the Vaux collection on top of the other minerals, says palaeontologist Ted Daeschler, would have been \u201ctoo horrible to swallow\u201d. In private, many academy scientists lay part of the blame for the museum's desperate financial straits on its board of trustees \u2014 a typical mix of local business executives, community leaders and a few scientists. Some academy staff say that board members were well-meaning but sometimes uninformed, which hindered their ability to engage fully in the museum's problems. Academy staff have also weathered turnover with the museum's presidents. Brown's predecessor, former US oceans chief James Baker, left in April 2006 after serving for four years. Administrator Ian Davison then stepped in to act as chief executive temporarily, overseeing the mineral sale among other changes. \n               Troubled times \n             The academy is not the only museum facing tough times (see  page 594 ). In recent years, natural history museums in the United States have struggled to survive in the face of diminishing government support, competition from the entertainment industry, and lack-lustre philanthropy. Some have taken what scientists deridingly call the theme-park approach \u2014 glitzy exhibits to pull in paying crowds, while the scientific mission is reduced if not all but eliminated. Museums have also used varying approaches to pay back bonds issued for refurbishment, such as increasing their revenue with community dinners or party events, and bringing in star-quality exhibits to boost attendance. For instance, the Houston Museum of Natural Science is taking the controversial step of creating a touring exhibit that includes the real skeleton of Lucy, the famed 3.2-million-year-old hominid from Ethiopia 2 . Brown comes to Philadelphia with a reputation as a savvy financial manager. At the Bishop Museum, he accelerated programmes to rent out museum locations for parties. \u201cWe went from $50,000 to a $1 million a year in revenue,\u201d he says. And although Philadelphia isn't exactly sunny Honolulu, the downtown part of the city is undergoing a major redevelopment, offering new revenue opportunities from businesses and visitors. Brown also comes with a track record of smoothing over museum disputes. When he took on the position at Bishop, the museum was embroiled in its own volatile controversy \u2014 about 80 priceless Hawaiian artefacts had been illegally spirited away and hidden in an isolated cave by indigenous traditionalists. By the time Brown departed, he had managed to help secure the return of the specimens, after a lengthy legal battle. Brown's predecessor at the academy had sanctioned numerous employees with letters in their personnel files for speaking out publicly about the dispute. \u201cI went through all the files, pulled out the letters and did a blanket pardon,\u201d says Brown. Those involved appreciated his openness. \u201cWhen he walked in, the artefact dispute was really bubbling over,\u201d says Apoliona. \u201cHis efforts were really honourable. Everything was transparent.\u201d In Philadelphia, Brown will face the challenge of maintaining the remaining staff. Some of those who haven't been fired have left of their own accord, such as renowned botanist Lucinda McDade. She left the academy last year to be research director at the Rancho Santa Ana Botanic Garden in Claremont, California. \u201cI really missed the teaching,\u201d she says. In Philadelphia, almost no graduate-education programmes are associated with the academy, even though it has plenty of top universities nearby, such as the University of Pennsylvania. Brown says that he is interested in exploring the possibility of bolstering graduate programmes. \u201cThe academy has many positive aspects,\u201d he adds. \u201cIt is clear we need to get all the cylinders firing effectively.\u201d He knows the academy's legacy well; he studied there briefly as a young scientist, was on the board between 1987 and 1993, and remained an adviser in recent years. His experience with government funding agencies will also come in handy \u2014 he was science adviser to Bruce Babbitt, secretary of the US Department of the Interior under President Bill Clinton. Brown will have a portfolio of top scientists to handle. In addition to Daeschler, who discovered the Tiktaalik, academy ichthyologist John Lundberg is completing a family tree of the catfish, a creature that can be found on every continent 3 . He, together with the academy's Mark Sabaj and their colleagues, regularly plies the waters of the Americas, Africa and Asia, confirming previous sightings and discovering new species. The team expects to identify 1,750 new species, which will push the total number of catfish species up to 4,600. \n               Illegal immigrants \n             Elsewhere at the academy, the malacology collection \u2014 of snails, slugs, clams and various other molluscs \u2014 includes 10 million specimens, some of which date to the 1700s. These specimens, along with the library, help malacologist David Robinson to guard ports of entry for the US Department of Agriculture. It is his express packages that arrive at the academy, at a rate of up to 60 a day, full of invasive species. The crawlers and sliders try to slip in across the border in shipments of marble, tiles, stones, manufactured goods, produce, plants, and even military vehicles \u2014 arriving from all continents. When the border inspectors can't identify the species, they send them to specialists such as Robinson for help. Because he spends so much of his time identifying unknown critters, Robinson rarely publishes on the rare and unusual species he discovers \u2014 like one in 1999. That express package contained dried plants collected by scientists from the Missouri Botanical Garden in St Louis and flown from Bali to the port of entry near San Francisco. Buried in the leafage was a lone dead snail. Robinson was stumped: \u201cI didn't have a clue what it was for months.\u201d Then, leafing through a publication 4  one day, he spotted the Bali native ( Anaglyphula whitteni vermeulen ) and was finally able to identify his specimen. And Robinson's work on that snail is but one example of how a long-standing academy can help scientists in the new century to fight invading pests. \n                     The fish that crawled out of the water \n                   \n                     Ethiopian plan for Lucy tour splits museums \n                   \n                     Academy of Natural Sciences  \n                   \n                     Bishop Museum \n                   \n                     Archaeology feature on Bishop bone dispute \n                   Reprints and Permissions"},
{"file_id": "446716a", "url": "https://www.nature.com/articles/446716a", "year": 2007, "authors": [{"name": "Jim Giles"}], "parsed_as_year": "2006_or_before", "body": "African communities have been adapting to climate change for millennia. Jim Giles reports on the strategies that seem most effective. Four decades ago, drought arrived in El Fasher, an impoverished state capital in western Sudan. Rural communities there had relied on weak rains to raise crops in sandy soils. But rainfall has been below average ever since, displacing a million people in the area and forever altering the lives of many more. Yet the people of El Fasher managed to adapt and survive. They built low earth embankments, known as  trus , around their villages to hold water and irrigate crops of sorghum and vegetables. They developed new planting methods, digging through the layers of sand to the fertile ground beneath. And they introduced a wider range of crops, from citrus fruit to tobacco, both to broaden their food sources and to sell. Today, El Fasher is facing a vastly different challenge, as it is at the heart of the bloody Darfur conflict. But elsewhere in the world, El Fasher's experience during drought should prove useful. Humans have adapted to changing environments for millennia, and many communities contain a wealth of knowledge about how to beat climatic odds and survive when rains fail or floods sweep away crops. So far, there have been few systematic studies of these survival strategies; but by tapping into communal knowledge, researchers are beginning to tease apart the actions that determine whether or not a community will make it in the face of change. The process is formally known as 'adaptation', a term that is becoming a political buzzword even among environmentalists, who once saw it as a distraction from the business of cutting carbon dioxide emissions. Now, with climate change expected to have unprecedented effects on people worldwide (see  page 706 ), adaptation can no longer be shunned (R. Pielke  et al .  Nature   445 , 597\u2013598; 2007). \u201cPeople talk about adaptation as if it's a new invention,\u201d says Guy Jobbins, a senior programme officer at Climate Change Adaptation in Africa, a research organization based in Cairo. \u201cIt's not. People in Africa have been adapting for thousands of years.\u201d It's not only Africa that will have to adapt, but the continent is often singled out because many of its residents live in precarious circumstances. In some countries, poor soils and a lack of government support already make many vulnerable to drought or extreme weather, so they have little to fall back on should climate change make things worse. Yet that is exactly what climate models predict is about to happen. \u201cDrought and other climate disturbances exact an unacceptably high and reducible toll on the people of Africa, a toll that is likely to grow with climate change,\u201d says Balgis Osman-Elasha, a climate-policy expert at the Higher Council for Environment and Natural Resources in Khartoum, Sudan. Africa's climate is poorly understood, in part because of patchy historical weather data. But the results that are available give cause for concern. The Sahara, for example, is expected to warm at rates greater than the global average of about 0.2 \u00b0C per decade during the early part of this century. Computer models also predict that eastern Africa will get wetter and see an increase in extreme rainfall events, and that parts of southern Africa will become drier. Elsewhere on the continent, things are less certain. Richard Washington, an expert in African climate at the University of Oxford, UK, says that predictions are vague for El Fasher and the rest of the Sahel \u2014 the east\u2013west band of semi-arid land between the Sahara and the greener landscape of central Africa. This is partly because the thunderstorms of the monsoon are difficult to simulate in climate models. Capturing the effect of dust blown from the deserts of North Africa is another challenge the models struggle with. Consequently, says Washington, \u201cyou can get any result you like\u201d with a climate simulation. If all models are averaged, a wetter future is predicted; if the selection is limited to those that closely simulate the current climate, the future looks drier than today. \n               Changeable weather \n             Data, and hence predictions, are better in South Africa, where some regions are already experiencing changes similar to those predicted by some of the better models. A team led by David Thomas, of the University of Oxford, has been studying the village of eMcitsheni, in the eastern part of the country, where rains have become more uncertain during the past 50 years. More heavy rainfalls have been arriving early in the wet season, before Christmas, and fewer at the tail end, around March. Researchers can't say whether such changes are due to natural variability or rising carbon dioxide levels, but that matters little to the communities experiencing new weather patterns. Nor does it matter to those interested in understanding adaptation. As similar climatic changes are expected to occur in the future, villages in the region form a kind of natural laboratory in which to study the ongoing effects of climate change. Several adaptation strategies are used at eMcitsheni, a rural area accessible only by dirt road that is home to about 300 households who depend on agriculture and livestock. Crops are planted farther apart so that more moisture is available for each row, increasing the likelihood that they will survive a period of drought. Corn (maize) varieties that mature faster have been brought in, again limiting the threat of dry spells. Local people have also set up a commercial cooperative: if the village produces excess maize in a certain year, the group works together to transport the crop to markets, earning money that can be used to buy food when yields are lower. But would such strategies work in other places? Many development experts are cautious about extrapolating from small studies to broader rules about how communities in general should adapt. Climate is far from the only factor changing people's lives, and given the complexity associated with even simple changes, it is dangerous to think about a magic-bullet solution to adaptation, says Jobbins. Yet some general principles are starting to emerge. To that end, the Oxford team has studied three other rural communities in South Africa and Mozambique. All of these villages rely on crops and livestock, and all have seen substantial climate change during the past decade. In the Limpopo province, near the South African border with Zimbabwe, the wet season has started later over the past 50 years and included more dry spells. Further west, in the region near the town of Mafikeng, farmers have seen more rainy days early in the wet season, but more overall variability in rainfall between successive seasons. In general, communities seem to have adapted best when working as a collective rather than as individuals. Overall, the team found four main strategies of adaptation: changes to agricultural practices; the formation of social networks; commercial projects, such as investing in livestock; and seeking work in distant areas. The first three of these strategies rely on people working together to better their community. In eMcitsheni, for example, people developed communal horticultural projects. Local women created jointly run irrigated gardens that, because the risk was shared among the collective, allowed them to diversify into crops they might not have attempted to grow on their own, such as potatoes. When heavy rain damaged traditional corn plots, these gardens could help to compensate for what was lost. Surplus crops were sold and the proceeds invested in pumps. Such sharing seems to work for many groups, says Henny Osbahr, a geographer at the University of Oxford. \u201cIt's complex,\u201d she says, \u201cbut we did see generic characteristics: strong informal institutions and networks of reciprocity.\u201d Of course, many groups' ability to adapt is limited by factors beyond their control, such as lack of skills or money. Osman-Elasha has studied communities in El Fasher and other parts of Sudan, and says that various 'essential resources', such as access to spare machine parts, were cited repeatedly as problems by local people. \n               Joining forces \n             When communities work together they are better able to interact with outside organizations such as government agricultural officials and donor agencies. And several larger development agencies are making adaptation a formal part of what they do. The US Agency for International Development (USAID) in Washington DC, for example, has begun funding the introduction of rice varieties in southern Mali that are better suited to shorter rainy seasons, because farmers there have reported experiencing hotter and drier conditions in recent decades. USAID will release an adaptation handbook for all its programme officers this May; the World Bank is also developing adaptation information for its staff. The agencies, along with local and national governments, face an enormous struggle. With wars, poverty and the AIDS epidemic, Africa is perhaps the region least well equipped to cope with climatic disturbances. The focus on building on local knowledge doesn't offer all the answers, but to adaptation experts it at least offers a way forwards. \u201cPeople felt that too much time spent on adaptation was taking attention away from doing something about the problem,\u201d says Emma Archer, a climate researcher at the University of the Witwatersrand in Johannesburg, South Africa. \u201cThat it was letting people off the hook. But now we know that we're committed to change. We're at an exciting stage.\u201d \n                     Britain introduces sweeping climate-change bill \n                   \n                     Special Report: From words to action \n                   \n                     Climate report released \n                   \n                     Developing countries get climate adaptation boost \n                   \n                     Economic review counts costs of climate change \n                   \n                     Climate change in focus \n                   \n                     Regional climate change special \n                   \n                     Climate Change Adaptation in Africa \n                   \n                     Dave Thomas & Henny Osbahr's case studies \n                   \n                     Blagis Osman-Elasha's research in Sudan \n                   \n                     www.defra.gov.uk/africa-climate.pdf \n                   \n                     www.tyndall.ac.uk/t2_31.pdf \n                   \n                     www.aiaccproject.org/AIACC_WP42_Osman.pdf \n                   Reprints and Permissions"},
{"file_id": "446723a", "url": "https://www.nature.com/articles/446723a", "year": 2007, "authors": [{"name": "Amanda Haag"}], "parsed_as_year": "2006_or_before", "body": "Members of the public are taking to the streets to spread Al Gore's message of climate crisis. Amanda Haag meets the foot soldiers of global warming. It's after hours at Monarch High School in Louisville, Colorado, and the hallways have fallen silent. In one classroom, a handful of high-school science teachers sit at desks usually occupied by biology students. Mark McCaffrey is giving a talk on how to teach global warming, and he points to a PowerPoint projection of the full-Earth shot snapped by Apollo 17 astronauts: the iconic 'blue marble'. \u201cEverything that's ever happened in human history has happened on this fragile little spaceship Earth,\u201d he says, almost reverentially. If this seems familiar, it could be because it sounds like Al Gore. McCaffrey is one of roughly 1,000 volunteers who, since last September, have been through a two-and-a-half-day training session with former US vice-president Gore and his staff in his home town of Nashville, Tennessee. The point is to spread the message of Gore's Oscar-winning documentary on climate change,  An Inconvenient Truth . Trainees learn how to manoeuvre through the science of Gore's 300-plus slide presentation and to discuss weighty topics such as rising carbon-dioxide concentrations, the physics of hurricane intensity and the mechanics of sea-ice retreat. They then return to their communities hoping to reach as many others as possible, in high schools, churches, city council meetings, businesses and retirement homes. McCaffrey, whose day job involves science education and outreach for a joint institute at the University of Colorado, Boulder, says that one of his motivations was to improve his understanding of climate science so that he could respond to naysayers. \u201cThe sceptics would derail me and I'd get flustered and not know how to respond,\u201d he says. \u201cSometimes they're scientists from outside the field of climate, but they know enough about how to throw up uncertainty and plant doubts in people's minds.\u201d Gore calls the trainees his \u201ccavalry\u201d, but a more apt name might be missionaries, given the fervour with which they approach their roles. One volunteer, from Hackett, Arkansas, signs off his e-mails as \u201cRobert McAfee, Climate Change Messenger\u201d. Gary Dunham, an independent voter from Sugar Land, Texas, says he had a near-religious conversion while watching  An Inconvenient Truth . \u201cI went to see the movie intrigued by what it was about but certainly not believing in the global-warming message,\u201d he recalls. \u201cWithin 15 minutes I completely changed my viewpoint. I don't think I've heard a political speech that really motivated me to get up and do something since John Kennedy's day.\u201d And volunteer Reggie Allen of Keller, Texas, says he sees the need to disseminate the \u201ctruth\u201d about global warming as a mission akin to the civil-rights movement, for which his parents used to march after church on Sundays. The volunteers were chosen from several thousand applicants, and include a middle-school student, priests, mayors, nuclear engineers, right-wing conservatives, Wal-Mart employees, Miss Rhode Island and Cameron Diaz. No matter what their walk of life, their motivation is the same: to tell their friends, families and neighbours that human activities are altering global climate and that each person can do something about it. The mission begins in Nashville where, on the first full day, Gore himself leads 90% of the training, walking volunteers through the science slide by slide. He takes questions, and a scientist is always present to help answer them. Gore's team includes a rotation of four scientists, including Michael MacCracken, chief scientist for climate change at the Climate Institute in Washington DC and a longtime contributor to the Intergovernmental Panel on Climate Change, and glaciologist Richard Alley of Pennsylvania State University. On the second day, the trainees break into smaller groups to practise giving the presentations themselves. \u201cThey blow you out of the water because they're speaking from their hearts,\u201d says Carey Stanton, senior director for education at the National Wildlife Federation, and one of Gore's staff. \u201cWhen they're backed up by really knowing the science, they're very good.\u201d Trainees are encouraged to tailor their talks to individual audiences while still preserving the framework of Gore's presentation. For instance, the inspirational images of Earth from space are expected to bookend the presentations, as they do the film. In his presentation, McCaffrey shows about a third of the available slides. \n               Made to measure \n             McCaffrey veers away from the original presentation mainly to use examples he sees as effective for spreading the message to young people. \u201cIf you just have a laundry list it's going to go in one ear and out the other. At least that's my experience with a teenager in the house,\u201d he tells his audience. He also mentions opportunities for students to take the lead in being part of the solution, such as by initiating carbon-neutral school programmes. The Gore campaign has garnered a near cult-like following. In December, the left-wing advocacy group MoveOn.org campaigned to have the documentary aired on the same day and time across the United States. Training sessions have taken place in Australia and the United Kingdom. And last week, Gore supporters flooded the website of Step it Up, a group planning a 'National Day of Action on Climate Change' on 14 April, and nearly shut the site down. But the movement hasn't had a warm reception in all quarters. In November, the producers of the film sought to have it mass-distributed to high-school classrooms, offering to deliver 50,000 free DVDs to the National Science Teachers Association. The association baulked, saying that it doesn't send out unsolicited material to its members and that doing so would \u201cconstitute an endorsement\u201d that might trigger other \u201cspecial interests\u201d to ask it to distribute material. Laurie David, one of the film's producers, publicly criticized the association's unwillingness to distribute the DVDs \u2014 even though the group offered to post links to the film on its website and to make it available to anyone who requested it. And in January, the school board of Federal Way, a district near Seattle, Washington, made national news when it placed a temporary moratorium on showing  An Inconvenient Truth . A parent had complained that the film presents only one side of the global-warming debate, and district policy states that teachers who choose to show material containing \u201cbias\u201d must also present a \u201ccredible, legitimate opposing view\u201d and that the principal and superintendent must grant permission. The moratorium has since been lifted. Even in school districts as left-leaning as Boulder, a quieter undercurrent of dissent brews about how \u2014 and how much \u2014 global warming should be taught in the classroom. Some Boulder Valley teachers showed  An Inconvenient Truth  in class and met with resistance from parents and other teachers afterwards. \u201cTeachers tend to shy away because we don't have the political support and backing for controversial issues,\u201d says Kristin Donley, who coordinates science curricula for the Boulder Valley School District. She is working to develop a unit on climate change for her classes. Some of the basic concepts, such as the carbon cycle and the greenhouse effect, are taught by default in physical sciences, Donley says. But the curricula offer a lot of leeway, and teachers can choose whether or not to broach the topic of global warming. \n               Lesson plan \n             McCaffrey came to the school after hearing that teachers in the district were interested in including climate change in their lessons. As he flips through his final slides, he charges the teachers to embrace the opportunity to reach today's youth. \u201cWe have a huge challenge in front of us,\u201d he says. \u201cYou as educators have a particularly important job in communicating the basics of climate and the context around it.\u201d Afterwards, one teacher brings up the fact that the film uses the word 'truth' in its title. But isn't science only supposed to deal with theories, he asks, and how does one explain this to students? Another teacher points out that his students didn't understand the meaning of peer-reviewed science as discussed in the film. Isn't it likely that this point is missed by most of the American public, too, he asks. McCaffrey suggests that a role-playing activity on peer review might help drive the point home. After all, not all teenagers are as attentive to the science of climate change as 14-year-old Alex Budd, of Boulder, who believes that doing something about climate change is a moral imperative. Alex heard about the programme from an aunt who lives in Tennessee, and was the youngest volunteer trained under Gore's tutelage. He and McCaffrey, along with another Colorado-based volunteer, Steve Wilton, have given the presentation together in the Boulder area. \u201cWe're destroying our planet,\u201d Alex says matter-of-factly. \u201cThat's not an issue of politics or economics. That's just morally wrong.\u201d He brings up what he learned from the Gore training over lunch at school \u201cor anywhere that I can just say a word or two\u201d. The biggest difference can be made by the small things people can do, he says, rattling off a list that includes compact fluorescent light bulbs and improving home insulation. \u201cI really wanted to make sure people at least know what's happening,\u201d Alex says. \u201cIt's not going to be easy. That's why they call it an inconvenient truth. It's true but it's not something that fits right into your schedule.\u201d \n                     Al Gore: Eco matinee idol? \n                   \n                     Grizzlies, dodos and Gore put science on film \n                   \n                     Climate Change In Focus \n                   \n                     The Climate Project \n                   \n                     An Inconvenient Truth \n                   \n                     National Science Teachers Association \n                   Reprints and Permissions"},
{"file_id": "446847a", "url": "https://www.nature.com/articles/446847a", "year": 2007, "authors": [], "parsed_as_year": "2006_or_before", "body": "France's presidential elections are taking place at a time of deep debate over the French research community's standing and prospects. To further the debate,  Nature 's Declan Butler submitted a list of questions on research issues to the three leading candidates. Their full responses, in both French and English, are on our website. Here we present extracts: Full versions of the questions and answers are available as PDFs in  French  and   English . There is also a  guide to the candidates , and to the  issues and acronyms .  \n               Is French science in decline and, if so, why? How much will you invest in science, when and on what priorities? \n               Nicolas Sarkozy:  It's true that we've had various warning signs over the past few years that the relative position of French science in the world is being eroded. France nonetheless maintains expertise of the highest international level in many disciplines, in particular in mathematics, physics and engineering. I note too that France exports its scientific expertise abroad, even if I regret the fact that many of our young scientists increasingly choose to leave the country because they no longer feel they can succeed at home. Research and higher education will be at the forefront of my priorities. Although this will be realized in the shape of more resources \u2014 \u20ac4 billion [US$5.4 billion] extra for research, and \u20ac5 billion for higher education \u2014 it will also involve deep reforms in the way the system works. Reform without resources would be as fruitless as resources without reform. I want to favour powerful and autonomous universities, which will be reinstated at the core of our research effort, and to reinforce a culture of scientific evaluation by promoting competitive grants. I would also note that the reshaping and restructuring of the Saclay plateau will be a major presidential initiative. This site is unique in Europe in its enormous concentration of centres of education and research, but there is scope for it to be exploited yet further. S\u00e9gol\u00e8ne Royal:  I don't think it's fair to speak of a decline, because France still has great assets and renowned university researchers, for example in mathematics. But for years, France has not really made research or the universities a top priority. The recruitment drive launched by Lionel Jospin was stopped when the conservatives returned to power; budgets have stagnated, PhD students and young scientists have been neglected, and the research system has been made more complex. France has fallen from 5th to 11th place among OECD countries in terms of science spending. I want to turn that situation around by making higher education, research and innovation top spending priorities, with a 10% budget increase each year for five years. Fran\u00e7ois Bayrou:  It's not so much that French science is in decline, but rather that other countries, including the United States, Japan, the nations of northern Europe and, more recently, China, have constantly increased investment in research over the years. For too long, research budgets in France have stagnated. Despite this situation, French scientists still rank among the best in many disciplines, in particular in mathematics, and in certain fields in physics, chemistry, biology, social sciences and the environment. It is now urgent to invest in emerging disciplines. I propose that France reinvest in research, with a 5% budget increase in real terms every year for ten years. \n               Is French science beyond reform? \n             Mme Royal:  The real question is: which reforms does French science need? It's our misfortune that successive right-wing governments have failed to make research a priority. It was opposition to the impoverishment of the university and its decline that brought students out on to the streets in 1986 and 1995, and scientists in 2004. M. Bayrou:  That's not fair to French research. I am very conscious of its willingness to get beyond the deadlock in which it finds itself. Research must first be reinstated as a priority on the political agenda; leading French politicians must make that their personal duty. M. Sarkozy:  No, French science is not unreformable! I think it has never been more ripe for reform. Growing awareness of the state of research is spurring demand for change. Lecturers and researchers are faced with globalization and international competition, just as everyone else is. Travelling a lot in their jobs, they are able to make comparisons. \n               The leadership of France's industries and government is dominated by graduates not from universities, but from the Grandes Ecoles. How would you reconcile the parts of this dual system so as to recognize the value of PhD scientists? \n             M. Bayrou:  Rather than pitting the different educational routes against each other, I prefer policies that diminish the differences between them. For example, we might extend the existing bridges between universities and the Grandes Ecoles, such as reorientation classes, to deserving students in all courses at the master's or PhD level; or it might be possible for the Grandes Ecoles and universities to set up common curricula. M. Sarkozy:  Research students will come into their own when French universities finally have available the finances and the autonomy they need to be centres of excellence \u2014 something that is already the case for courses in law, medicine and economics. Research needs to play a bigger role at the Grandes Ecoles, and the best university students should be able to switch to those courses. Universities and Grandes Ecoles that are close to each other should have joint campuses with shared services. Universities could benefit from the Grandes Ecoles' business know-how, and the access their students enjoy to highly responsible jobs. I want PhD students to be able to access opportunities beyond the areas of research and education. Mme Royal:  I have pledged to have the PhD recognized both in the civil service and in the private sector. It is also necessary to bring the various parts of the higher-education system closer together, and to establish routes for moving between the Grandes Ecoles and the universities. That will be a role for the PRES, which will stimulate interaction and synergy between universities, Grandes Ecoles and research organizations. \n               How would you modernize France's universities? \n             M. Sarkozy:  As of the day after the elections, I will be ready to launch a major reform of French universities designed to give them much more autonomy. This will include powers to recruit, to fix salaries, to decide how they organize themselves, to build endowments and to diversify their funding sources. I will also rebuild the way that they are governed, restructuring their executive boards and the ways they choose their presidents. M. Bayrou:  After a massive increase in student numbers over the past two decades, enrolment has now stabilized, and this makes it possible to envisage a new phase of long-term development. The universities suffer three ills: the absence of recognition of the PhD, lack of funding and a poorly adapted governance structure. We need to reach spending-per-student levels equal to or more than the average of OECD countries, continue the rapprochement with the Grandes Ecoles that has now begun, and make changes in the ways the universities are run. Mme Royal:  I favour a rational, optimal use of resources based on evaluation; this means we must provide favourable working conditions for all researchers. If we supported only a small proportion of researchers it would mean that we would be paying the others without benefiting from their potential. That would be absurd. \n               State planning created France's powerful aerospace, nuclear and transport industries, but is 'innovation by decree' possible for fast-moving sectors such as biotechnology and information technology? \n               M. Bayrou:  Research is not in itself innovation, but it can and should contribute to it. There are several ways of promoting this. Researchers contribute to our growth and our competitivity by taking interest in their work's potential for wealth creation. To attract talent, I propose that for researchers living in France, royalties from patents should be exempt from income tax. I want to reinforce technology-transfer departments in our research institutes \u2014 a few hundred million euros could have a substantial impact \u2014 to help the birth and development of innovative enterprises. Mme Royal:  Innovation has stagnated because fundamental research has not been supported. The proactive policies of the 1960s led to the development of the space, aeronautics and nuclear-power industries. The 'breakdown' in biotechnology \u2014 despite the fact that French labs were at the forefront of sequencing the human genome in the early 1990s \u2014 stems from a glaring lack of support for the life sciences a decade ago. M. Sarkozy:  The competitiveness clusters that I established in my different positions in government are still very young. But an initial appraisal is rather encouraging. I would emphasize that they represent a relatively innovative approach, at least by French standards. As you say in Great Britain it is a bottom-up, and not top-down, approach. It is, above all, the quality of research and the dynamism of the ways in which it is disseminated that determine our potential for innovation \u2014 not just financial incentives. But let's be pragmatic; everywhere in the world, from emerging economies to developed and ostensibly free-market countries, the state intervenes to encourage innovation, and to build and reinforce the industrial and technological sectors of the future. \n               What cuts in greenhouse-gas emissions would you commit France to and how would you attain them? What should be agreed post-Kyoto? \n             Mme Royal:  I am committed to a 75% reduction in our greenhouse-gas emissions by 2050. I'm setting out an ambitious policy because I'm convinced that climate change is the major challenge of the twenty-first century. So I advocate saving energy in the building and transport sectors, developing renewable energy (solar, wind, biomass and geothermal) and promoting research into carbon capture and storage, electricity storage, hydrogen, intelligent electricity grids and new fuels created through hydrolysis of biomass. M. Sarkozy:  I am proud to be able to say that France made a visionary choice in committing itself several decades ago to developing its nuclear-power programme. Just think, the carbon emissions saved by France using nuclear-power stations rather than fossil fuels are equivalent to those of all Europe's cars. Of course, we have also made scientific and technological priorities of research on renewable energy and more energy-efficient means of production and transport. As a market of 500 million people, Europe should make greater efforts to encourage its large commercial partners \u2014 in particular the United States, China and Canada \u2014 to play according to the planet's own rules. Countries that behave like stowaways hitching a free ride, making no effort to reduce their emissions, should not continue to benefit from the competitive industrial advantage this gives them. To compensate for this we must tax products from countries that make no effort to reduce emissions after 2012, even if this means modifying World Trade Organization rules. M. Bayrou:  I will fulfil the European commitment to a target of generating 20% of our energy from renewable sources. But we need to do even more, through saving energy and new technologies \u2014 for example, the construction sector already has the know-how to erect zero-emission buildings. That will create local jobs. \n               Would you maintain nuclear power's current 75% share of French electricity generation? And how will you tackle France's accumulating nuclear waste? \n             M. Sarkozy:  The nuclear sector is of absolute strategic importance, as well as of industrial and technological excellence. France is one of the rare countries to have mastered the nuclear-fuel cycle in its entirety. France will continue to nurture its comparative advantage here by modernizing its nuclear fleet and know-how. That's why we have committed to a series of third-generation reactors, the EPR, and a research programme into fourth-generation reactors. Mme Royal:  I will pay particular attention to guaranteeing that the storage of nuclear waste is reversible. Parliament will decide in ten years time which options should be retained for long-term management of nuclear waste. I think that the current government took the decision to go ahead with the EPR without adequate analysis or debate. No impact assessment was presented to parliament, and no effort was made to create real diversification in our energy mix. We cannot set our country's energy future in stone without an in-depth debate, not just on the EPR but on the entire issue. M. Bayrou:  The EPR project will be maintained. But we need a scientific assessment of it. The renewal of our existing fleet of reactors hinges on this project, and a decision this important for our energy policy cannot be taken on the sly. There must be as wide and democratic a debate as possible. A demonstration project should be launched rapidly to prove that after temporary storage, the volume and radioactivity of waste can be reduced to low levels. This is necessary to reassure the many men and women in France, and worldwide, who have doubts about this form of energy because of the risks still associated with it. \n               What are your priorities for space? \n             Mme Royal:  France is a major space power, having developed, with its European partners, the high-quality Ariane launchers. I think that the high costs of manned space flight mean that such ventures should be carried out in international programmes. The launch of Earth-observation satellites is, of course, a priority. M. Bayrou:  The emphasis should also be on fundamental research and exploration of the Universe, a field in which France and Europe are proficient, and then on GMES and on the Galileo navigation system. For Europe to carry sufficient weight, should it aim for the Moon or even farther, perhaps going it alone? Should it support the International Space Station? This all demands reflection. M. Sarkozy:  The Galileo project has become bogged down in national quarrels that are petty compared with the stakes on the table, and breaking the current deadlock is a matter of urgency. I'm keen on greater European cooperation in space. France should be ready to make the extra effort to lead the way, if needs be, as it has done in the past. The main goal must be to maintain and reinforce our basic civil, military and scientific skills. If, after that, we can together develop more ambitious manned flight and planetary-exploration missions, then why not? \n               What will be your policy on genetically modified crops? Would you change existing laws on embryonic stem-cell research? \n             M. Bayrou:  What has happened with GMOs is symptomatic of the lack of democratic consultation on major topics in France. Such a debate would have helped to bring out the citizens' expectations, and to provide directions for research, so as to allow the downstream use of the technology in a way that was regulated and acceptable. We must therefore organize this now, with input from independent scientists. In the meantime, I am in favour of an immediate moratorium on GMOs. M. Sarkozy:  I want research to continue on GMOs as, among other things, that is the only way to improve our knowledge of the potential risks to human health and biodiversity. Unless we can be highly certain that they are harmless, I am less enthusiastic about their industrialization and marketing. As for research on embryonic stem cells, with the creation of the Agency of Biomedicine in 2005, researchers can now submit research projects for review. I'm delighted that projects that have been favourably reviewed can thus develop in our country within a clear and evolving framework. I think it's indispensable that research also continues to develop in this area. Mme Royal:  I am in favour of a moratorium on open-field cultivation of GMOs and of having a public debate on this question, which is of interest to all citizens. Research on human stem cells should be permitted provided that they are obtained after informed consent, they come from embryos that are no longer part of any fertility treatments and the proposed protocol has been rigorously examined. We must revise the legal framework to reconcile ethical principles and scientific progress. For fuller answers, download PDFs of the full Q&A in  \n                   English \n                  and  \n                    French \n                 . \n                     French election: Let science speak for itself \n                   \n                     French election: Is French science in decline... \n                   \n                     French science after Chirac \n                   \n                     French research chief quits over reforms \n                   \n                     French government concedes defeat to researchers \n                   \n                     French scientists prepare for mass resignation \n                   \n                     Resignation threats add steel to French revolt \n                   \n                     Fran\u00e7ois Bayrou \n                   \n                     S\u00e9gol\u00e8ne Royal \n                   \n                     Nicolas Sarkozy \n                   Reprints and Permissions"},
{"file_id": "446485a", "url": "https://www.nature.com/articles/446485a", "year": 2007, "authors": [], "parsed_as_year": "2006_or_before", "body": "Researchers have found certain stem-cell studies notoriously difficult to replicate. Erika Check finds out why, and whether it is slowing down the field. Eight years ago, a team of scientists based in Canada and Italy published an astonishing paper in  Science . They started with mouse neural stem cells \u2014 cells destined to give rise to brain tissue. At the time, scientific dogma held that cells such as these, which are already moving towards one destiny, cannot switch paths and generate other types of cell. The  Science  paper challenged that view. The team transplanted the stem cells into mice whose bone marrow had been wiped out. Lo and behold, a miraculous transformation seemed to occur: the stem cells changed their fate and gave rise to the haematopoietic cells that normally reside in bone marrow and generate blood and immune cells. The paper was provocatively titled 'Turning brain into blood' 1 . It was part of a wave of studies reporting a fate-switching phenomenon that some termed 'transdifferentiation', and that offered a possible way to grow replacement tissues without destroying embryos for stem cells. It took a separate group of scientists more than two and a half years of painstaking work to show that the  Science  paper wasn't as exciting as it had first seemed. This group tried to replicate the findings by transplanting neural stem cells into each of 128 mice whose bone marrow had also been killed. But after exhaustive analysis, they never saw these stem cells generate haematopoietic cells. They concluded that the cells may rarely switch fates, but that this occurs only because of genetic changes that accumulate after long periods growing in a Petri dish 2 . The authors of the  Science  paper disputed this conclusion, and there is still no consensus on whether this phenomenon can ever be useful in medical practice. \n               Reputation at stake \n             The brain-to-blood debate highlights a more pervasive problem in the adult stem-cell field: that many teams struggle to reproduce others' seemingly promising results. Replication is a crucial and arduous part of the scientific process 3 , and it is one that has been especially problematic in this area because of restrictions surrounding the use of stem cells, inexperienced researchers and technical difficulties. Many scientists feel that these problems have slowed progress and harmed the field's reputation. \u201cOverall damage to the field has been enormous,\u201d says stem-cell biologist Naohiro Terada of the University of Florida in Gainesville. \u201cIf we keep producing hype, no one will trust stem-cell researchers.\u201d The issue of replication was most recently thrust into the spotlight because of renewed attention on a 2002  Nature  paper 4  by Catherine Verfaillie of the University of Minnesota, Minneapolis, and her colleagues. After culturing mouse bone-marrow cells for months, the researchers reported that they could extract an exceptional population of cells they termed multipotent adult progenitor cells, or MAPCs. When they injected these cells into developing mouse embryos, and studied the chimaeric baby mice that were born from them, the MAPCs seemed to contribute to all the major cell types of the body, including brain, heart, bone marrow, skin, blood and lung. This and other experiments raised the possibility that MAPCs could be used therapeutically in organ regeneration and repair. The finding created a buzz, because no other adult stem cell had been found to generate such a variety of different cell types \u2014 a range almost equivalent to that produced by embryonic stem cells. Verfaillie's work has since proven exceedingly difficult to replicate, although some groups have reproduced certain parts. Earlier this year, after spending several years learning to work with MAPCs, Scott Dylla, a former postdoctoral fellow in the lab of Stanford University biologist Irving Weissman, was able to use MAPCs to make blood-forming haematopoietic stem cells in mice 5 . But although many groups have tried, none has managed to repeat the key aspect of Verfaillie's paper \u2014 injecting MAPCs into an embryo to create all the major cell types of the body. \u201cI have not seen any convincing data showing that anyone has repeated the chimaera experiment, so I don't think this part of it is true,\u201d says Rudolf Jaenisch of the Whitehead Institute in Cambridge, Massachusetts, whose lab tried and failed to reproduce the work. \n               Restricted access \n             Stuart Orkin of Harvard Medical School in Boston, Massachusetts, requested MAPCs from Verfaillie's lab a few months after her paper was published. He found clauses in the material transfer agreement \u2014 the contract that governs use of shared research material \u2014 saying that users could not disclose information about the cells to others not working on the project. \u201cWe couldn't accept that,\u201d says Orkin. More broadly, researchers sometimes have difficulty laying their hands on stem-cell lines and methods because of competition between labs and because these cells could prove commercially valuable and so access to them can be restricted. Those labs that could negotiate licences for MAPCs ran up against another problem: the cells themselves are very tricky to work with. Two labs told  Nature  that they couldn't keep the MAPCs alive long enough to study them, and so abandoned attempts to replicate Verfaillie's work. Both labs followed Verfaillie's published and unpublished instructions closely. A large part of the problem is that methodology on how to handle the cells keeps evolving, says Paul Simmons at the University of Texas Health Science Center in Houston and one of those who tried in vain to repeat the work. \u201cThere's been a series of moving goal-posts as far as the conditions for replication are concerned,\u201d he says. For instance, it has now become clear from Verfaillie's later work that MAPCs grow better in low-oxygen conditions, and that they are best taken from very young mice. \u201cMethodology is everything, and the devil is in the details,\u201d says Simmons, who is also president of the International Society for Stem Cell Research. \u201cThere's a real need to have methodologies out there that include all the protocol details.\u201d But scientists say they feel forced to skimp on their methods sections so they can cram as much data as possible into high-profile journals with severe page limits. And Verfaillie says that the improvements in culture conditions were discussed at meetings and published. The circumstances surrounding Verfaillie's paper became even murkier last month when questions about duplicated figures used in her  Nature  paper and another published in  Experimental Hematology 6  prompted the University of Minnesota to convene an inquiry into her study. The inquiry found that some of her procedures were flawed. Verfaillie has said that the duplicated figures were an honest mistake. And other scientists have not suggested that she committed fraud \u2014 on the contrary, Verfaillie, now at the Catholic University of Leuven, Belgium, enjoys a reputation as a meticulous researcher. Verfaillie says that the  Nature  paper discussed several possible explanations for the chimaera results and that its conclusions still stand. But she admits that the duplication errors in her published work may have contributed to confusion in the field: \u201cWe have made mistakes for which I take responsibility, and we have done everything possible to alert the scientific community regarding these mistakes,\u201d Verfaillie wrote in an e-mail to  Nature . The confusion looks set to continue: last week, new questions were raised about duplicated images in other work from Verfaillie's lab. \n               Lack of experience \n             Technical problems have scuppered many attempts to replicate stem-cell experiments. The best way to isolate and handle fiddly stem-cell lines is only just being worked out, and by their very nature, stem cells constantly divide and change, making it demanding to extract or maintain precisely the same population of cells in culture. In 2001, a team led by biologist Diane Krause of Yale University in New Haven, Connecticut, published a high-profile paper in  Cell  claiming that a single bone-marrow cell could give rise to multiple cell types, from gut to lung to skin 7 . But the following year, a second group disagreed, saying they could not obtain such a range of cell types from a bone-marrow cell 8 . Krause says that although some groups have replicated aspects of her work, others have not, and that it is important to her to understand why. Probably the main reason is that different labs have used different experimental techniques, she says, so they may have isolated slightly different starting populations of bone-marrow stem cells. The same year, two groups showed that results such as Krause's might be explained by cell fusion, in which stem cells that seem to adopt new fates were actually merging with other cells 9 , 10 . The fusion experiments helped to discredit the idea of transdifferentiation, which has now become a taboo word among most serious stem-cell researchers. Krause says she is performing experiments to clarify how much cell fusion contributed to her results. Inexperience is another problem in the stem-cell field, which attracts new researchers with its white-hot reputation. This has been a particular issue with the transdifferentiation studies, says cell biologist Sean Morrison of the University of Michigan, Ann Arbor. Some may make honest mistakes because they are new to the studies. Others may be sloppy. \u201cIn some cases people have a get-rich-quick mentality and are more interested in publishing high-profile papers than in getting the answers right,\u201d he says. Does any of this really matter? Similar problems crop up in many young fields, in which scientists can struggle to reproduce initially exciting data. Inevitably, some results fall by the wayside if they cannot be repeated. But the adult stem-cell field is under particular scrutiny because of its medical promise and its potential to bypass embryonic stem-cell research. It takes tremendous time and resources to see whether an experiment can be repeated. The difficulties with replication might partly explain why there is no consensus on the properties of most adult stem-cell lines, or which lines are most medically promising. \u201cIf your research depends critically on the veracity of an observation, it's incumbent upon you to reproduce it \u2014 if it's not a solid observation, it becomes a roadblock,\u201d Simmons says. Add up enough of these roadblocks, and it is easy to see how a field can get bogged down chasing spurious leads, instead of forging ahead in new directions. \n                     Stem-cell powers challenged \n                   \n                     True stem cell found? \n                   \n                     Stem cells in focus \n                   \n                     Web focus on stem cells \n                   \n                     Catherine Verfaillie curriculum vitae \n                   Reprints and Permissions"},
{"file_id": "446718a", "url": "https://www.nature.com/articles/446718a", "year": 2007, "authors": [{"name": "Gabrielle Walker"}], "parsed_as_year": "2006_or_before", "body": "Despite years of speculation, little can be said for sure about the future of the Arctic's permafrost. But that's no grounds for complacency, reports Gabrielle Walker. Some of Phil Camill's trees are drunk. Once, the black spruce trees on the plots of woodland that he monitors in northern Manitoba stood as straight and honest as pilgrims. Now an ever-increasing number of them loll about leaning like lager louts. The decline is not in the moral standards of Canadian vegetation, but in the shifting ground beneath their roots. Once it was all hard, solid permafrost. Now much of it has thawed into a soggy sponge that no longer provides a steady footing for the trees. Some contrive to grow at screwball angles; others have drowned and been replaced by floating mats of mosses or sedges. \u201cIt is really easy to tell when the permafrost has gone,\u201d says Camill. \u201cThe vegetation changes right before your eyes.\u201d Camill, an ecologist from Carleton College in Northfield, Minnesota, has used those changes to trace the rate at which the permafrost is disappearing. In their desperate attempts to buttress themselves upright, his leaning spruces put on extra wood on the downslope side of their trunks. Counting the asymmetrical tree rings that result and measuring the distance of each tree from the current boundary of the permafrost gives a measure of the pace of change. The results are shocking. An average warming across his sites of 1.3 \u00b0C since 1970 has brought with it a trebling of the thaw rate. In some places the permafrost's perimeter is retreating by 30 centimetres a year 1 . If this trend continues, Camill estimates that no permafrost will be left in any of his five sites by the end of the century. Thawed-out permafrost has already undermined buildings, highways and other infrastructure from Alaska to Siberia. The damage is one of the most visible effects of warming temperatures on human activities. But the effects on natural systems are to some extent more worrying. Buildings can be rebuilt, asphalt relaid and agricultural practices changed through adaptation, given the right policies and priorities (see  page 716 ). But changes in the vegetation and, crucially, in the soils of the frozen northern landscapes might not be so easy to cope with. The soils of the Arctic are crammed with organic matter \u2014 a frozen reservoir of beautifully preserved roots, leaves and other raw material that may contain as much carbon as the whole atmosphere. They are quite unlike soils from more temperate regions, which are mostly made up of the parts that the bacteria cannot digest. \u201cWe are unplugging the refrigerator in the far north,\u201d says Camill. \u201cEverything that is preserved there is going to start to rot.\u201d For decades environmentalists have worried about the possibility of this great putrefaction. It has become perhaps the most cited example of the biogeochemical feedback that could drastically worsen the effects of anthropogenic climate change. The idea is that humans increase levels of carbon dioxide in the atmosphere, warming the permafrost, which in turn releases yet more carbon, warming the world \u2014 and the permafrost \u2014 further still in an ever-escalating positive-feedback loop. However, although such feedback has been discussed for almost as long as the threat of global warming has been taken seriously by scientists, the lack of firm data on the subject is striking. \u201cThere is a lot that we don't know at this point,\u201d says Walter Oechel from San Diego State University in California. \u201cPeople haven't quite pulled the whole picture together yet \u2014 but what we do know is that the potential amounts are huge and very, very scary.\u201d \n               The big picture \n             There is no doubt that the Arctic is heating up. Vladimir Romanovsky from the University of Alaska Fairbanks has collated borehole- and air-temperature data from throughout the Arctic 2 . He found that only one region in the Arctic had not warmed over the past 30 years \u2014 and in the 1990s even that region joined the trend. Some places are warming at more than twice the global average rate. Romanovsky recently received a US$1 million grant to take this monitoring work further with a network of stations in North America and Russia. The effect that this warming will have on the permafrost and its stored carbon will vary from region to region. Not surprisingly, the most dramatic signs of thaw have come from the fringing, southerly regions of discontinuous permafrost \u2014 such as Camill's research sites \u2014 where the frozen layer is only a few metres thick and average temperatures are already within a whisker of the melting point. In the colder Arctic the permafrost can be hundreds of metres thick and it is harder to know what to expect. In principle, the thawing might be quite slow, with the warmth at the surface being transmitted gradually into the colder depths. In practice, things are probably more complex, and in places more precipitous.\n \n               Patchy progress \n             Permafrost is defined as ground in which the temperature is less than 0 \u00b0C for at least two successive years. But the ground in question does not have to be at the surface. In most places the top part of the soil thaws during the summer, providing plants and microbes with an 'active layer' above the permafrost in which they can flourish and decompose the defrosted organic matter. The probability that the active layer will deepen \u2014 putting a larger stash of carbon up for grabs \u2014 or that the permafrost will thaw completely depends on the type of vegetation and soil. Thus, thawing can accelerate rapidly if a fire passes through a dried-out forest in the uplands, or if the soil contains enough ice that thawing causes it to collapse, creating a crater-scarred 'thermokarst' landscape. Researchers expect to see the first signs of thaw in a deepening of the active layer. But complications from terrain, vegetation and other local conditions mean that data need to be collected continuously over several decades to pick up such a trend. Unfortunately, the relevant measurements so far have been patchy and sporadic. In 1998, various organizations involved in permafrost research banded together to receive funding as the Circumpolar Active Layer Monitoring (CALM) programme, hosted at the University of Delaware in Newark, to tackle the problem. The network now includes 125 active sites and has participants from 15 countries. Still, it is unlikely to bear fruit in the form of spotting unequivocal trends for some time. If measuring the damage done so far is hard, predicting its future course is even harder. In 2005, two researchers from Boulder, Colorado \u2014 David Lawrence from the National Center for Atmospheric Research and Andrew Slater from the Cooperative Institute for Research in Environmental Sciences \u2014 published the results of the first attempt to project the fate of the permafrost through the twenty-first century using the climate predictions of a general circulation model (GCM). Their results made dramatic headlines. Using figures from one of the 'high emissions' scenarios developed by the Intergovernmental Panel on Climate Change (IPCC) in the model resulted in 90% of the northern permafrost disappearing by 2100. Of 10.5 million square kilometres of permafrost around today, only about 1 million made it through the century. Even more worryingly, running the model with a 'low emissions' scenario still wiped out 60% of the permafrost, suggesting that severe losses are inevitable no matter which policies are followed. And the model did not take into account any further warming from carbon given off in the thaw 3 . \n               Thick and thin \n             The model has since attracted some criticism, most notably because its permafrost is a mere 3.4 metres thick throughout the Arctic, which is far from the hundreds of metres present in some regions. However, Lawrence says that he has since re-run the model for a mid-way emission scenario and dealt with some of the other criticisms at the same time, and the results remained more or less the same. He says that the model captures many important aspects of the Arctic system \u2014 including the hydrology and physical properties of the soil \u2014 and that the point is not so much the actual percentage of loss but the overall principle that a frighteningly large amount of permafrost could be vulnerable to quite small changes in climate. \u201cIn this field you have to accept that we won't have a perfect knowledge of what's happening up there,\u201d he says. \u201cBut we should be able to capture the fundamental properties. And so far the model shows that a major change is going to happen to the Arctic.\u201d Perhaps the most intriguing of the complications that Lawrence's original work did not address is the suspicion in the minds of some researchers that the permafrost itself is putting up a defence against the thaw \u2014 a set of negative feedbacks. For instance, Oechel points out that warmer temperatures lead to a thicker layer of moss on the surface of his research sites in the Alaskan tundra. Because moss is a superb insulator, especially when dried out by surface warming, the thickened vegetation helps to shield the frozen soil beneath from the warmth above. In one of the north\u2013south transects he studies on Alaska's North Slope, the warmest, southernmost section has both the thickest layer of moss on the surface and the shallowest active layer. Frederick Nelson at the University of Delaware points to another self-preserving feature of permafrost. The base of the active layer, he says, can become especially icy because water draining down there will pool above the impermeable layers below. The richer this layer is in ice, the more difficult it is to thaw, preventing the active layer from deepening further 4 . Even snow cover does its part. Like moss, snow is an effective insulator. But because it falls in the winter, it works in the opposite direction, shielding the soil from cooling further in air temperatures that can be as low as \u221240 \u00b0C. Camill noticed that the snow at some of his sites has thinned in the past few decades, which may cause the soil to grow colder in winter than it used to and thus store up protection against the heat of the following summer. Increased forest cover can have a similar effect, causing snow to be thinner beneath the trees than it would have been on open ground. \u201cAt face value you would expect the permafrost to start thawing really rapidly as temperature rises, but these feedbacks can keep it around longer than you would expect,\u201d says Camill. \n               Pushing the boundaries \n             But these effects can't last forever \u2014 and although they might forestall thawing, the sudden change in conditions when one or more of them fails might lead to quicker thawing thereafter. A few really hot summers could break through the ice barrier. And Oechel is already worried about his mosses. He has noticed that they are highly sensitive to direct sunlight, and now that the Arctic has fewer cloudy days the mosses could well begin to suffer. Loss of any or all of these protections would allow any thaw to accelerate. \u201cIt is difficult to push permafrost over the threshold of thawing,\u201d says Romanovsky. \u201cBut after tipping it will go by itself\u201d. Uncertainties about thawing obviously complicate the question of how much greenhouse gas the permafrost will emit \u2014 but they are not the only complications. Oechel has been tracking the carbon balance of his Alaskan tundra for several decades. After a serious bout of warming in the late 1970s, he saw the region emit a mighty pulse of carbon dioxide \u2014 a pulse that is now showing signs of tailing off. He suggests that one possible reason for the decline is that the initial orgy of decomposition spurred by the warming released nitrogen-containing nutrients into the soil. The plants have now responded to the additional nutrients by growing more and taking up carbon dioxide in the process. Oechel thinks that in the summer some of his sites now take up more carbon than they emit, although when winter emissions are added in they are still sources, not sinks, overall 5 . Another factor is the northern march of the treeline. As the region warms, the growth of new forests over what was once tundra could also help to reduce the net release of carbon. However, even if the surface ecosystem starts to take up more carbon, the old, dead material frozen in the soil still needs to be considered. What's more, the overall amount of carbon emitted is not necessarily the whole story. Torben Christensen at Lund University in Sweden also has data on the carbon balance that span several decades, in his case from a low-lying mire in the patchy permafrost of northern Sweden 6 . As the permafrost has steadily thawed, the ground has grown soggier; the previous hummock vegetation has been replaced by sedges, which are better at tolerating wet roots. The water seals the underlying soil from the air, which means that decomposition has to proceed without the benefit of oxygen. This slows things down, and has reduced the region's carbon emissions by 13% since 1970. However, the carbon that is released by this oxygen-free decomposition comes out in the form of methane, rather than carbon dioxide, and methane packs a far greater warming punch than its oxidized sibling. So the overall greenhouse effect of the mire has actually gone up by a disturbing 47%. \n               Back to biology \n             These are the sorts of issues that future attempts to model the process will need to take into account. Lawrence says that the next phase of this work will be to treat the biology more carefully, incorporating carbon and nitrogen cycles into the models, allowing the vegetation to respond to the changes in climate, and modelling sources of methane. According to Nelson at least half a dozen groups around the world are planning to develop their GCMs to address at least some of these aspects of the permafrost thaw, and the plan is to have a much more extensive set of predictions in the next IPCC report, in five years time. Knowing more about the thaw, though, will allow useful predictions of carbon emissions only if researchers can also quantify the amount of organic matter in the soils. A series of workshops under the auspices of the Global Carbon Project and the International Polar Year is aiming to answer those questions. Current estimates of the amount of carbon that might be in play range from 350 gigatonnes to more than 900 Gt; by way of comparison, the atmosphere contains 750 Gt or so. The estimates at the high end of the scale are based on the discovery of a new, vast pool of buried carbon \u2014 a type of wind-blown soil called yedoma, which was laid down over large tracts of northern Siberia in the ice ages. \n               Rich sources \n             Stuart Chapin, also from the University of Alaska Fairbanks, says that the yedoma soil is extraordinarily rich in carbon. Last year, he and his colleagues estimated the range and possible thickness of this layer and calculated that it alone could contain 450 Gt of carbon, compared to the estimated 350\u2013450 Gt in the rest of the Arctic 7 . This number, says Chapin, is probably \u201conly good to within a factor of two\u201d; but even half of such a huge amount would be significant, whereas twice as much hardly bears thinking about. Camill points out that humans release around 9 Gt of carbon per year from fossil fuels and deforestation. \u201cIf just 1% of [the possible 900 Gt in the yedoma] is decomposed in a warmer world it would be as if we doubled our current rate of emissions. That's what is alarming.\u201d Ominously, the first signs that parts of the continuous permafrost might now be thawing have come from lakes that overlie this carbon-rich yedoma. Katey Walter from the University of Alaska Fairbanks and colleagues have been tracking the methane that bubbles out of thaw lakes in northern Siberia. These lakes don't necessarily arise from global warming. Any local disturbance can trigger a temporary thaw in the permafrost. As the ice melts, the ground sinks and fills with water. The lakes then tend to migrate across the landscape, eroding away their margins, and can last as long as a thousand years; their sideways motion allows them to eat through permafrost much more quickly than would a steady heating pulse heading down from the surface. And decomposition in their oxygen-free depths and the thawed sediment beneath will produce methane, not carbon dioxide. How much methane nobody realized until recently \u2014 mainly because it bubbles out from random parts of the lake and disappears unnoticed into the air. Walter and her colleagues managed to catch this methane in the act, by noting where the bubbles emerged when the lake froze over in the winter, and then leaving instruments to catch the emissions throughout the following year 8 . They calculate that bubbling lakes from northern Siberia are already responsible for nearly four million tonnes of methane a year, and that the amount is on the rise. Warmer temperatures mean that the lake area in Walter's study region has increased greatly in the past few decades, leading to a rise in methane emissions of nearly 60%. And that methane seems to be coming from the depths of the permafrost: its lack of carbon-14, an isotope continually made in the atmosphere that takes thousands of years to decay, suggests that the organic matter beneath the lakes has been stored away for a very long time. This and the Arctic's other warning signs make it increasingly urgent that researchers resolve their remaining questions about the fate of the permafrost. And those answers won't come a moment too soon. \u201cWe have been asleep at the switch,\u201d says Oechel. \u201cIf you look at the things that were said in the 1970s about the Arctic's response to increasing CO 2 , the place we were off is not that we overstated or were overly pessimistic, but that we were not aggressive enough about the predictions. To me, the precariousness of the situation is now clear. We are in a world of hurt.\u201d \n                     Climate Change 2007: How to survive a warming world \n                   \n                     CALM permafrost monitoring \n                   Reprints and Permissions"},
{"file_id": "446126a", "url": "https://www.nature.com/articles/446126a", "year": 2007, "authors": [{"name": "Nicola Jones"}], "parsed_as_year": "2006_or_before", "body": "Air bubbles trapped in the Antarctic ice sheet could yield precious information about Earth's climate more than a million years ago. But to access this record, scientists first have to climb one of the coldest peaks on Earth. Nicola Jones reports. Dome Argus in Antarctica is a silent and lonely place. Snow stretches to the horizon in all directions, unbroken by any sign of life. The mountain is one of the least accessible parts of the frozen continent, and quite likely the coldest place on Earth \u2014 although no one has been there long enough to catch a record-breaking low. People first stepped onto the summit just two years ago, after a Chinese team made an arduous month-long, 1,200-kilometre trek from the coastal station of Zhongshan. If you think that buried treasure should be hidden somewhere as remote as possible, then Dome Argus, or 'Dome A', would be an ideal spot. But if you think that it should be gold and shiny, you will be disappointed. Dome A's bounty, if it exists, is tiny pockets of gas trapped in its depths. Antarctica's great ice cap is crowned by several flat snowy summits known as Domes A, C and F (see map). Reaching the top of Dome A, the tallest and the last to be conquered, was hailed as a triumph of exploration. The scientific importance, though, is not the height itself (although astronomers are excited by the clear air that comes with it) but the ice below. From more than 3,000 metres below Dome A's 4,093-metre peak, researchers hope to extract the oldest ice core in the world, and with it a treasury of climate information. As part of the International Polar Year, which launches this month, a Chinese team plans to return to Dome A this austral summer to set up a camp. Next year, a larger, international group intends to storm the dome in aircraft to map the ice below. And the Chinese will return too, this time lugging a giant drill. Dome A might not be so lonely or so quiet for long. An ice core from Dome A would join a formidable list of samples taken from previous Antarctic expeditions. A core from the Russian station Vostok eventually reached 3,600 metres deep and yielded measurable ice some 420,000 years old 1 . A core from Dome C reached only 3,200 metres down 2  but, thanks to a better preserved bottom section, pushed 800,000 years back in time. And a 3,000-metre core being drilled at Dome F caused fleeting excitement last year when the Japanese team spearheading the project said that the ice might be even older. But more recent tests have shown that the usable ice from close to the bottom of the dome was some 720,000 years old, and the team now suspects that the last few chunks will hold only a few thousand years more. \n               A wealth of information \n             The reason for this old-ice treasure hunt is simple. Earth used to experience periods of glaciation once every 41,000 years or so. Today, glacial cycles come every 100,000 years. Evidence from sediment cores suggest that the key transition between these states took place over a period of several hundred thousand years, about a million years ago. No one knows why it happened. One idea is that levels of carbon dioxide in the atmosphere plummeted and cooled Earth enough for a substantial extra layer of ice to form. This massive burden of ice would have made it hard for the planet to respond so nimbly to the orbital drivers of climate change, thus shifting it into a more stately pace of glaciations. The only way to confirm the idea is to find air bubbles that date back 1.5 million years or so, and track how the carbon dioxide levels changed over time. Ice is the only place to find such bubbles. And for old ice, Antarctica is the place to go. Ice cores from Greenland, where the ice flows more dynamically, reach back only 100,000 years or so. But Antarctica has been covered in ice for an estimated 30 million years, and models of glacial flow suggest that there could be an uninterrupted record of ice that stretches back a few million years \u2014 probably beneath Dome A. The International Partnership in Ice Core Science (IPICS), a 19-nation group co-chaired by Eric Wolff, an ice-core specialist with the British Antarctic Survey in Cambridge, UK, and Ed Brook of Oregon State University in Corvallis, has made finding that ice one of its main goals. Dome A not only has plenty of ice to drill, it also gets very little snow \u2014 just 1.25 to 1.5 centimetres of its equivalent in water per year, compared with the 3 centimetres at Domes C and F or the 50 centimetres that dumps on the coastal station of Halley. That means that the ice on Dome A contains snow from a very long time period. But there are complications. The Antarctic ice cap acts as an insulator, lying like a blanket across the continent and trapping geothermal heat below it. The thicker the ice, the greater the insulation, and so when the ice gets really thick its base will frequently become warm enough to melt, shortening the record. Also, the topography of the rock beneath the ice is complex, to say the least. A mountain range lies down there (see  'The hidden mountains' ). If the ice at the bottom of the ice sheet has been forced to move up and over rocky ridges, it will be folded, muddled and mixed, making it impossible to date it or to extract clean information from it. The bottom 70 metres of the Dome C core were like this, making its oldest ice unusable. Models suggest that the flow of ice away from the base of Dome A is small, so older ice should still be preserved at the bottom. A map of ice ages by modeller Philippe Huybrechts of the Dutch-speaking Free University Brussels in Belgium ( see map ), confirms that an area near the peak of Dome A \u2014 a vast swath about the size of Britain \u2014 is suitable for an old-ice hunt. Apart from Dome A, other candidate sites for the oldest ice do exist. The Aurora basin, near Dome C but closer to the coast, for example, is about 4,500 metres deep and could potentially hold very old ice. Australia plans to drill a 400-metre test core there in the 2008\u201309 season, says Vin Morgan of the Australian Government Antarctic Division near Hobart, Tasmania. But this area is lower and warmer than Dome A, increasing the chances that its bottom ice has melted substantially. The nearby Astrolabe basin has an even deeper 4,700 metres of ice, but covers a small area in which the surrounding rock may have distorted the ice at the bottom. Farther afield, there are other areas in Antarctica likely to hold very old ice (see map), but they are trapped in mountain ranges where the ice record is much thinner and more squashed. \n               Disturbing the peace \n             Dome A thus remains the prime candidate for drilling. But too little is known for researchers to draw an X in the snow and plant their drill. So starting next year, if funding comes through, the sky over Dome A will be filled with the rare noise of low-flying planes, burdened with radar and equipment to measure gravity and magnetic fields. Particularly helpful will be the radar surveys, which can pick up changes in density, crystal structure or dust content in the ice. By flying from Dome C to Dome A, the IPICS team hopes to be able to track ancient layers along the 1,000-kilometre flight path, thus revealing the depth of correlated layers in the ice at Dome A. Given the remoteness of Dome A, the planes will need local bases from which to refuel. Ideally, these will be placed around the dome at slightly lower altitudes, as the height of the dome makes the air so thin that propeller planes have trouble taking off and pilots' functioning can be impaired. \u201cTechnically, the pilots should be using supplementary oxygen,\u201d says Wolff. And in similar circumstances, loaded Twin Otter planes have had to use jets strapped to their wings to gain enough lift. That same season, the Chinese researchers plan to return, this time carrying French drilling equipment capable of bagging a 500-metre core \u2014 an upgrade on the 110 metres they pulled from the ice in 2005. Their work will be dangerous; last time, a team member fell into a crevasse while working at the summit camp. \u201cHe was lucky \u2014 it wasn't so deep,\u201d says expedition member Shugui Hou, of the Chinese Academy of Sciences' Cold and Arid Regions Environmental and Engineering Institute in Lanzhou. The good news is that summer temperatures are only \u221235 \u00b0C. \u201cWhen the weather was good we were wearing only one sweater, perhaps because it's so dry,\u201d says Hou. \u201cIt's quite comfy.\u201d \n               Pooling resources \n             Funding a serious drilling effort will be a bigger project than any country can tackle alone. \u201cNo one nation has the resources. We're going to have to do it internationally,\u201d says Alan Rodger, head of science programmes at the British Antarctic Survey. With the help of many countries, Wolff estimates, a site could be selected and a drilling operation under way by 2012. \u201cWe want to drill two cores, to give ourselves two chances,\u201d says Wolff. One idea would be to blast through the top part of the first hole with a heat drill, not bothering to pull up usable cores until reaching a predetermined depth. Meanwhile, a second site, tens or hundreds of kilometres away, could be drilled in detail from the surface. This strategy of extracting two cores would be a new one; most other sites poured all their resources into a single drill hole (although jammed drills have often meant that teams had to start a new core). The technology, at least, exists. Drilling deep into the Antarctic is tricky but doable. The difficulties lie in such things as getting the drill fluid right. The cold, dense liquid that drillers insert into the hole to stop it from closing up on itself must not dissolve the snow around it or get clogged with ice chips that can form a sludge; kerosene with chemical additives is often used. And when the drill gets near the bottom, where the ice can be near the melting point, the problem is to stop ice that melts and refreezes from jamming the drill bit, by using some antifreeze. Once the core has been extracted, researchers will use all the techniques they can to date the ice and pull information from it. Old layers of ash from known volcanic eruptions act as date markers, and the top part of the core can be matched up to previous ones, already dated, to pin down the age. Another dating trick for the older ice will be to search for the higher amounts of beryllium produced by the increased flux of cosmic rays when Earth's magnetic field reverses itself 3  \u2014 as happened 780,000 and 900,000 years ago. Analysing the amount of nitrogen and oxygen in the ice can also help; the ratio of the two changes in concert with a 23,000-year cycle of alterations in the amount of solar radiation reaching the Antarctic 4 . Technicians will then crush the ice in a vacuum to release the air, and measure the amounts of greenhouse gases such as carbon dioxide and methane. The ratio of oxygen isotopes can also be used to estimate past temperature. But the big prize will be the carbon dioxide \u2014 the amount of the gas in each bubble could confirm the idea that a drop in carbon dioxide caused the change in glaciation cycles a million years ago. As always, there is still the chance that the ice won't hold the expected treasure. If so, the answer to this million-year-old puzzle may lie elsewhere: perhaps, some speculate, the vast plateau of the Canadian shield was at some point scrubbed clean of lubricating mud by all the glaciers, and it was this, rather than a drop in carbon dioxide, that allowed ice sheets to build up enough to slow the planet's cycle of glaciations. Finding the oldest ice could go a long way to answering these questions. If not the bounty of plummeting carbon dioxide, then a core from Dome A would still guarantee the gem of an ancient climate record. Xiao Cunde, another member of the Chinese expedition, puts the chances of finding million-year-old ice under Dome A at 95%, and 80% for ice older than 1.5 million years. With odds like that, most would be happy to go on a treasure hunt. See Editorial,  \n                     page 110 \n                   . \n                     Climate warming 'seesaws' between the poles \n                   \n                     Ice core shows its age \n                   \n                     Greenhouse-gas levels highest for 650,000 years \n                   \n                     Palaeoclimate: Frozen time \n                   \n                     Climate change in focus \n                   \n                     Polar research in focus \n                   \n                     Ice cores Web focus \n                   \n                     EPICA adventure \n                   \n                     IPICS white papers \n                   \n                     IPY Chinese proposal for exploration of Dome A \n                   \n                     IPY Chinese proposal for traverse to Dome A \n                   \n                     IPY Australian proposal for astronomy at Dome A \n                   Reprints and Permissions"},
{"file_id": "446133a", "url": "https://www.nature.com/articles/446133a", "year": 2007, "authors": [{"name": "Quirin Schiermeier"}], "parsed_as_year": "2006_or_before", "body": "Every summer the Arctic Ocean loses more ice \u2014 and it could all be gone within decades. Quirin Schiermeier looks at how the vanishing summer ice affects those living in the north. It was on Christmas day that Duane Smith first noticed that something weird was happening. When he and his family went to church, they did so in the rain. That was in 1983. \u201cWe'd never ever seen anything like it,\u201d remembers Smith \u2014 then a little boy in Inuvik, a town of 3,500 people just north of the Arctic Circle, and now president for the Inuit Circumpolar Council (Canada), an indigenous people's organization. \u201cAround Christmas it was supposed to be some 30 degrees below zero. None of our elders had any memory of such mild weather in winter.\u201d It could just have been weird weather. In fact, it was a harbinger of things to come. Nowhere else on the planet is the current warming trend more pronounced than in the Arctic, and nowhere else does it seem to leave a deeper mark. The Arctic was a favoured site for early-warning systems during the cold war. Today, it is the early-warming system for climate change. In recent years, researchers have started to pin down the details of what might happen to the Arctic as the planet warms. Although many of the specifics remain speculative (see  'How the Arctic might change' ), everyone is certain that change is coming \u2014 and fast. \u201cThe Arctic is changing extremely abruptly on a geological time scale,\u201d says David Barber, a climatologist and sea-ice specialist at the University of Manitoba in Winnipeg, Canada. \u201cThere is no good historical analogue that could tell us what might happen.\u201d Blame it on the sea ice. Unlike Antarctica \u2014 a continent surrounded by oceans \u2014 the Arctic is for the most part an ice-covered ocean, making it particularly vulnerable to climate change. Within several decades, the entire Arctic Ocean, including Hudson Bay and the countless channels between Canada's Arctic islands, could be free of ice during the summer months 1 . Palaeoclimatic evidence suggests that this has not been the case for at least the past 1 million years. Less ice during the Arctic summer might not necessarily be all bad. New shipping channels and oil and gas regions could open up, for instance, and local hunters could get around by boat more easily (see  'Life in a warming world' ). But the rate and magnitude of the changes are unprecedented, and the consequences are difficult to predict 2 . The amount of sea ice in the Arctic usually reaches its maximum \u2014 more than 14 million square kilometres in recent years \u2014 around the end of March. The slowly moving pack ice is separated from the immobile ice attached to the coastlines by the perennial 'circumpolar flaw lead' \u2014 a narrow corridor of open water that is rich in biological productivity and crucial for the heat exchange between the ocean and the atmosphere. This lead will be the focus of a multinational expedition led by Canada during this International Polar Year. During the summer, ice melts and thins, reaching its minimum in September. The minimum extent of sea ice in the Arctic has decreased from a long-term average of more than 7 million square kilometres since 1979, to less than 6 million square kilometres in 2002 (ref.  3 ). Every year since, it has continued to drop or stay at near-record low levels. In September 2005, the Arctic was covered by just 5.32 million square kilometres of ice \u2014 the lowest yet. As more and more ice disappears, a vicious cycle sets in. Ice reflects away a large fraction of incoming sunlight, whereas the darker open ocean absorbs more radiation. This 'albedo' effect is the main reason why the influence of warming is significantly more pronounced at high northern latitudes than in temperate or tropical regions. A reduced albedo in the Arctic affects the entire planet's energy balance, causing yet more energy to be absorbed in the darker waters. \n               A sizeable problem \n             The ice is not only shrinking in its area, but also in its depth, as recorded by submarines 4  and radar images from satellites. And it seems to be declining even in the winter 5 . If the ice continues to disappear at its current rate of nearly 9% per decade, the Arctic Ocean will be ice-free in September by 2060. But if, as some scientists suspect, the shrinking were to accelerate, this date will come forward by 20 years to 2040. This dire scenario is just one of seven computer simulations published in December by a team led by Marika Holland of the National Center for Atmospheric Research in Boulder, Colorado 1 . That particular simulation suggests that the summer Arctic sea ice will decrease from 6 million square kilometres to 2 million square kilometres in the course of a decade. The ice that remains would be tucked along the coasts of Canada and Greenland, leaving the central Arctic basically free of ice by the end of the melting season, although the region would refreeze during the winter. The receding ice cover could also affect large-scale patterns of ocean circulation. Ice, for instance, seems to be moving at an increasing rate out of the Arctic Ocean through the Fram Strait to the east of Greenland and through the Canadian Arctic archipelago to Greenland's west. On average, the amount of ice moving out is around 10%, but during the winter of 2005 to 2006, a strong counterclockwise rotation pattern in the Arctic Ocean pushed about 40% of the pack ice into the warmer Atlantic waters. The events over the past two years, says Barber, are the first sign that the rates of ice export can change dramatically. If more storms start to enter the Arctic, as expected with the rising temperatures around the world, the pack ice will be broken up and potentially carried away more often. \n               Flow-on effects \n             Reduced sea-ice cover might also increase the influx of warm Pacific waters through the Bering Strait between Russia and Alaska. Koji Shimada, a physical oceanographer at the Institute of Observational Research for Global Change in Yokosuka, Japan, has suggested that because sea ice is starting to form later each year, the warmer Pacific waters will be able to flow into the Arctic 6 . Such ocean changes could dramatically affect the lives of Arctic organisms, from algae to fishes to mammals. \u201cAll indigenous animals are somehow tied to ice,\u201d says Jacqueline Grebmeier, a biological oceanographer at the University of Tennessee in Knoxville. \u201cSo the early-season retreat of ice will no doubt trigger cascading effects.\u201d For instance, the thinning and retreat of sea ice allows more sunlight to penetrate the ocean earlier in the season, making surface waters richer and more biologically productive. Already, scientists have seen the dominant zooplankton in the Arctic \u2014 the small copepods  Calanus hyperboreus  and  C. glacialis  \u2014 replaced by their warmer-water, Atlantic cousins  C. finmarchicus . Their move is thought to be related to the fact that the surface waters in the Arctic are now warmer and so more algal food can be found there. \n               Mix and match \n             This 'Atlantification' of the Arctic comes at the expense of species composition and biodiversity, says Louis Fortier, a biological oceanographer at the University Laval in Quebec. As warmer water enters the Arctic basin, generalist species from temperature latitudes may move in and out-compete the Arctic specialists. In Canada's Hudson Bay, for instance, black guillemots ( Cepphus grylle ) used to feed exclusively on Arctic cod ( Arctogadus glacialis ). In recent years, scientists have found that up to half of the birds' stomach contents consisted of small capelin fish ( Mallotus villosus ), which are common around Iceland. Meanwhile, Arctic cod are increasingly being replaced by the larger Atlantic cod ( Gadus morhua ) as the dominant species in some parts of the Arctic Ocean. Seals, whales and polar bears all feed on the smaller, easier-to-catch Arctic cod. The shrinking of the sea-ice habitat and the shifting ecosystem have already affected the behaviour of the most emblematic Arctic species \u2014 polar bears, walruses and ringed seals. The Pacific walrus ( Odobensus rosmanus divergens ), for instance, is being forced to feed in deeper waters as the sea ice retreats off the coasts of continents, and mother walruses seem to be separated from their calves more often as sea ice continues to fragment 7 . And polar bears are being considered for listing in the United States as a threatened species because of their disappearing sea-ice habitat. Humans, too, are preparing for the change. Subsistence hunters are learning to follow mammal and fish populations into new areas as the ecosystem shifts 8 . And Inuit leaders such as Smith are realizing how they will have to adapt \u2014 and are even looking forward to it. \u201cOur people have understood ice conditions for ages,\u201d he says. \u201cNow we're keen to learn how global warming will change our world.\u201d \n                     Arctic clear for summer sailing by 2040 \n                   \n                     Arctic ecology: On thin ice \n                   \n                     Climate change: The tipping point of the iceberg \n                   \n                     Climate change: A sea change \n                   \n                     Climate Change in focus \n                   \n                     Polar research in focus \n                   \n                     International Polar Year 2007/2008 \n                   \n                     Arctic Monitoring and Assessment Programme \n                   \n                     ArcticNet \n                   \n                     Western Arctic Shelf-Basin Interactions Project \n                   Reprints and Permissions"},
{"file_id": "445474a", "url": "https://www.nature.com/articles/445474a", "year": 2007, "authors": [{"name": "Geoff Brumfiel"}], "parsed_as_year": "2006_or_before", "body": "Three years ago, President George W. Bush told NASA to return American astronauts to the Moon. Geoff Brumfiel reports on how far they have got. It's a drizzly afternoon in the middle of January, and a trickle of tourists is visiting the Smithsonian's Air and Space Museum in downtown Washington DC. Ambling around the museum's glass-and-steel atrium, the sightseers gravitate slowly towards the vintage space capsule at its centre. Nine-year-old John Kalman and his great-aunt Janet peer into the open door of the Apollo 11 command module  Columbia , the craft in which the first men to walk on the Moon made their epic journey there and back. \u201cThat's awesome,\u201d exclaims the tow-headed boy as he gazes at panels from another age, jammed with hundreds of switches and indicator lights. \u201cLook at how small it is,\u201d his aunt tells him. \u201cIt's not like the ones we use today, that's for sure.\u201d True: today's astronauts fly in shuttles that were designed in the 1970s, rather than capsules from the 1960s. But for tomorrow's astronauts,  Columbia  is a glimpse of the future as well as the past. Under the agency's new Vision for Space Exploration, a somewhat larger craft, very similar to the Apollo capsule \u2014 'Apollo on steroids', as it has been dubbed \u2014 will have replaced the shuttle in the role of carrying astronauts into orbit as early as 2014. By around 2020, the agency plans to use these new capsules, along with other spacecraft, to return to the Moon and establish an outpost there. That sustained operation, officials hope, will provide a technical basis for a future mission to Mars. 'The vision', as it is often referred to within the agency, was first outlined by President George W. Bush on 14 January 2004. It marks a radical new direction for America's human spaceflight programme. For the past two decades, NASA has been preoccupied with shuttling people to and from a low Earth orbit, mostly to visit the International Space Station. But the vision \u201cis fundamentally different\u201d, says Shana Dale, second in command at NASA. \u201cIt's about extending human presence on another world.\u201d This extension, however, can't be built on the cheap; NASA's early estimates put the cost of the programme through to 2018 at around $104 billion. To meet this bill, the agency is committed to grounding the space shuttle fleet in 2010 and cutting back its spending on the space station, which should be completed by then (see chart). It is also delaying and cancelling space-based science missions in astronomy, planetary science and Earth observation, as well as aeronautics programmes. Louis Friedman, executive director of the Planetary Society, a Pasadena-based educational organization in California that regularly criticizes this reallocation of resources, complains: \u201cThis is attacking exploration to supposedly pay for exploration.\u201d NASA is thus under pressure from many fronts to make the vision more affordable. At the same time, it needs to be exciting enough to enthuse a population half of which was not born when a man last walked on the Moon. Yet the budgetary constraints mean that things need to be done in a way that seems less than fresh, using spacecraft that look like throwbacks, and making progress in seemingly undramatic, incremental steps. The vision requires NASA to provide an inspiring future frontier while keeping things sustainable, sensible and safe. \n               On a wing and a prayer \n             To understand the vision, you have to understand its origin. Business-as-usual at NASA did not come to a close with President Bush's speech in January 2004. It ended a year before. At 08:54 on 1 February 2003, engineers at mission control in Houston, Texas, were guiding another spacecraft called  Columbia  home from a routine, 16-day trip into low Earth orbit. Moving at 24 times the speed of sound, the shuttle had just begun a pre-programmed braking manoeuvre high over California, its wings sheathed in air heated to 1,300 \u00b0C. Suddenly, four temperature sensors on the left wing cut out. \u201cYou're telling me you lost them all at exactly the same time?\u201d flight director LeRoy Cain asked his team. \u201cNo, not exactly,\u201d came the reply. \u201cThey were within probably four or five seconds of each other.\u201d The wing had been damaged, and now the superheated air had found its way inside; within minutes,  Columbia  and its crew of seven were a trail of debris falling out of the morning sky. The damage was down to a flaw fundamental to the shuttle's design. During the launch, the leading edge of the wings was below the external fuel tank, so anything \u2014 in this case, insulating foam \u2014 that came off the tank stood a chance of damaging the surfaces most exposed during re-entry. The resultant accident investigation board chided NASA for not coming to grips with the problem earlier. But the possibility of such engineering lapses was not the only problem with the shuttle programme, according to John Logsdon, a space-policy expert at George Washington University in Washington DC, who sat on the investigation board. Just as disturbing was the banality for which the astronauts had died.  Columbia  had been on a mission to conduct some small experiments in microgravity, including a promotional test for a fragrance company. Until it ended in tragedy, the mission barely made local headlines in Houston. \u201cWe believed this was not an adequate vision to justify the risk of putting astronauts into space,\u201d Logsdon says. The board recommended a re-examination of NASA's entire rationale for human spaceflight. In the late summer of 2003, NASA and the White House began to speak in earnest about the future. The administrator at the time, Sean O'Keefe, was promoting an ambitious programme that would have kept the space shuttle flying until they were ready to go to the Moon, according to Glen Asner, a historian at NASA. But John Marburger, the president's science adviser, and the White House Office of Management and Budget wanted a more conservative programme that focused on robotic missions to the Moon and Mars and on human missions to the Moon. The debate ended in compromise: an aspirational nod towards Mars in the unspecified future, a near-term focus on the Moon in the 2010s, no more shuttle by the end of this decade. \u201cI don't think anybody got everything they were looking for,\u201d Asner says. \n               A giant leap \n             When finally given voice, though, the vision's call to \u201cextend human presence across the Solar System\u201d certainly sounded satisfactorily sweeping in scope. \u201cIn the past 30 years, no human being has set foot on another world,\u201d Bush told a crowd at NASA's headquarters in Washington DC. \u201cIt is time for America to take the next steps.\u201d The crowd and almost everyone else familiar with NASA greeted the announcement enthusiastically, if only because the status quo had grown increasingly intolerable. \u201cNASA has been lacking a compelling vision for decades,\u201d says Keith Cowing, a former employee of NASA and co-author of  New Moon Rising , a book about NASA's renewed plans for exploration. Before the vision, he says, \u201cYou could ask ten people what the goal of the agency was and you would get twelve answers\u201d. Now, NASA once again had a single purpose. So far, so Apollo. But Bush made it clear that the vision was not going to involve an Apollo-era surge in NASA's budgets. The billions needed to complete the vision, it is claimed, will be the billions 'saved' by not flying the shuttle and not spending on the space station. The budget for the vision will thus increase slowly, in synch with the decline of the existing programmes. The agency hopes to have its new 'crew exploration vehicle', the capsule called Orion, and the Ares-I rocket on which it will ride to orbit ready by 2014. By 2020, it hopes to have built a lunar lander, and another, larger rocket, allowing missions to the surface of the Moon. \n               Keeping up appearances \n             The slow pace of the programme was, at the time, reflected in the president's budget, which initially recommended a modest 7% increase over five years for the US$15-billion agency, but which didn't envisage an actual landing for 15 years or so. By contrast, Project Apollo went from a standing start to the Moon's Sea of Tranquillity in under a decade, but ramped up the NASA budget by 300% in the process. This modest approach is the new way to do business at NASA. \u201cThe key word in the current vision has to be sustainability,\u201d says Jeffrey Hoffman, professor of engineering at Massachusetts Institute of Technology (MIT) in Cambridge, and a former astronaut. Unlike Apollo, which spent enormous sums on a short-term programme to reach the Moon, the vision is supposed to be a sustained effort that will guide NASA well into the twenty-first century while not breaking the bank. Perhaps no one element of the vision illustrates NASA's change in strategy better than its replacement for the space shuttle. In the 1980s and 1990s, the agency toyed with various ideas for follow-on crafts that might fly into orbit unaided by booster rockets. Billions were spent, only to demonstrate that many more billions would be needed to build something that actually worked: a single-stage-to-orbit spacecraft is a very difficult proposition. This time, says Scott Horowitz, who is in charge of developing Orion, \u201cWe're not depending on any technological miracles. It is a much more reasonable programme.\u201d The Orion capsule eschews wings and advanced propulsion, and depends entirely on disposable booster rockets. The result is not just operationally similar to Apollo's conical capsule on a cylindrical service module: it is essentially an enlarged carbon-copy that uses specific similarities to cut down the need for expensive novelty. The Ares-I rocket on which it will sit uses a solid-rocket booster just like the shuttle's as its first stage and an Apollo-era J-2 engine to power its second. The crew-escape system, which is also based on an Apollo-era system, can rapidly lift the capsule from the rocket if something goes wrong, making this a safer vehicle than the shuttle, at least during the launch. Prudence is also guiding the agency's strategic planning, which now operates under what Dale describes as a \u201cgo-as-you-can-afford-to-pay approach\u201d, taking exploration forward in modest increments. After developing the Orion, which will cut its teeth taking crews to and from the space station, the programme will construct a lunar lander and a new heavy rocket \u2014 called Ares V \u2014 which will combine solid rockets and J-2 engines to deliver 130 tonnes of payload to Earth orbit, as opposed to Ares I's 25 tonnes. That's enough to launch both the lunar lander and the engine needed to take Orion and its accompanying lander from orbit round the Earth to orbit round the Moon. The difference from the 1960's approach is that Apollo launched its lander and command modules on a single Saturn V rocket; by launching Orion on one rocket and the lunar lander and transfer stage on a second, then having them dock in Earth orbit, the vision architecture allows larger crews and payloads. (see  'Along for the ride?' ) \n               Short breaks \n             At first, astronauts will use the lander/Orion combo for week-long trips to the Moon of the sort undertaken by the later Apollo missions. Gradually, the agency will build up resources for a permanently occupied base at one of the Moon's poles that will be visited by mission after mission. It is hoped that the outpost will give engineers experience in supporting humans on other worlds, eventually paving the way for a much more ambitious expedition to the surface of Mars. Rather than setting hard dates for completing each stage of the vision, NASA will adjust its plans to match each year's budget. The vision's fiscal conservatism helped it win the imprimatur of Congress, which passed legislation to support it in 2005. But not all of its money-saving strategies have been popular with lawmakers. In August, NASA administrator Michael Griffin, who replaced O'Keefe in 2005, awarded the crew capsule contract to Lockheed Martin in Bethesda, Maryland, before the design had been reviewed fully. The move, says Horowitz, helped to cut costs by allowing NASA to fund one development team, rather than working with a pair of competing designs. But awarding a contract before the final design and cost had been fully assessed drew criticism. \u201cCongress has to keep a keen and constant eye on the project,\u201d warned Sherwood Boehlert, the former Republican chair of the House of Representatives Science and Technology Committee in a hearing last September. \u201cNeither the agency nor the nation can afford another space station \u2014 a project that, for all its technical magnificence, has seen its costs balloon while its capabilities shrank to near the vanishing point.\u201d Already the agency is facing some tough financial choices. The 7% over five years that President Bush spoke of did not end up in his budgets; in the White House's budget request for fiscal year 2007, produced last January, NASA's budget was kept essentially flat. \u201cI would give Bush an 'A' for vision and a 'C\u2212' for follow-through,\u201d Cowing says. And Congress, which has the final say, has allowed the agency less than the president asked for every year since the vision was first proposed. Adding to its woes have been the unexpected hundreds of millions of dollars spent on returning the shuttle to flight and repairs to facilities damaged by 2005's Hurricane Katrina. NASA also has a service mission to the Hubble Space Telescope back on the books, with all its attendant costs. And this year, a breakdown in the congressional budget process is likely to leave the agency about half-a-billion dollars short of where it wanted to be (see  Nature 445, 130; 2007 ). \n               Chopping and changing \n             To offset a lack of overall growth in NASA's funding, the president's budget for 2007 called for its astrobiology programmes to be slashed by 50% and for an 18% cut in its $854-million aeronautics budget. Several Earth-observing satellites, notably the Global Precipitation Mission, have been delayed. Longed-for science flagships such as missions to obtain samples from the surface of Mars and to study Earth-like planets around nearby stars have been deferred indefinitely. Friedman argues that similarly inspiring science missions, such as the Hubble telescope and the Mars Exploration Rovers, have been the bedrock of public support for NASA. Cancelling their natural heirs, he says, will erode enthusiasm for space: \u201cYou can't maintain public support on just a rocket programme.\u201d Horowitz counters that the exploration programme will open up new areas of research to scientists, especially on the surface of the Moon. Plans call for astronauts to be able to access to any part of the Moon that scientists might want to visit, and NASA is holding workshops to find out the sorts of projects they might want to do there. The bottom line, he says, is that the exploration will fund science \u201ceven though you don't see it in dollars in the science budget line\u201d. And some scientists are enthused about the prospect of getting to work on the lunar surface. \u201cThere's a lot that we didn't learn about the Moon from the Apollo missions,\u201d says Paul Spudis, a geologist at Johns Hopkins University's Applied Physics Laboratory in Laurel, Maryland. Although models show that the Moon was probably formed by Earth's collision with another body the size of Mars, many details about this originating cataclysm remain unknown. Studying materials near the base of the Moon and comparing that information with remote sensing data from all around it will teach scientists a great deal about the specific circumstances surrounding the Moon's creation and evolution. But most researchers, including some who are enthused by lunar exploration, are deeply sceptical of claims that it offers a worthwhile scientific return. They think that science is being used to lend more legitimacy to a vision that has little chance of producing scientific breakthroughs. These fears are based in part on past experience with the space station, according to Roger Blandford, director of the Kavli Institute for Particle Astrophysics and Cosmology at Stanford University in California. During the planning of the station, scientists were asked to suggest a research agenda. Even if it had been carried out, that agenda could hardly have justified the $25.7-billion programme (a sum that doesn't take into account the related shuttle flights). As it turned out, only a fraction of that science got done. Now many wonder whether signing on to a mission to the Moon will produce similar results. \u201cThere are obviously things one can do,\u201d says Blandford, referring to plans for Moon-based telescopes and other instruments, \u201cBut as a practical matter, an awful lot of space science is better off not on the Moon.\u201d So far, the vision has been relatively impervious to the scepticism of scientists. A contract for the first stage of Ares I is expected to be awarded in February, following on from the $3.9-billion contract to design and build Orion signed with Lockheed Martin last August. And the agency is also eyeing up contracts for a heavy-lift vehicle and a next-generation lunar lander. \u201cI think things are going great,\u201d Dale says. It is possible, though, that the new Democratic Congress will be less sanguine than NASA, or its Republican predecessor. The vision should not \u201csimply shuffle money from NASA's other core missions\u201d, says Tennessee Democrat Bart Gordon, who chairs the House Committee on Science and Technology. He and his colleagues are also sceptical of the pay-as-you-go philosophy, which avoids making long-term budget projections of the total cost of the future missions. \u201cI don't think Congress is going to be very tolerant of avoidable overruns,\u201d Gordon says. Congress and observers elsewhere agree that, in the short term, there is no real alternative to the vision, inasmuch as grounding the shuttle for good means that another system has to be developed that is relatively cheap to put people into space. \u201cI don't think this country will ever turn its back on having a human spaceflight capability,\u201d says Dale. But the need to develop Orion to keep America aloft will not necessarily translate into the ambitious Moon-base strategy, let alone a renewed push to Mars. As Gordon says, \u201cIf it's not a priority \u2014 and the president has made very few statements about it since he announced it three years ago \u2014 it's going to be difficult to convince future administrations and congresses to sustain it in its current form.\u201d \n               Public eye \n             For the vision to become a priority \u2014 rather than just a new and safer way of keeping NASA and its contractors ticking over \u2014 it needs to excite people. And it is here that some observers feel that NASA has erred too far on the side of practicality, producing a near-pedestrian plan that looks no further than the already-explored Moon. \u201cIn order to sustain the vision, you have to think big-picture, long-haul sorts of things,\u201d he says, his mind's eye clearly on Mars. \u201cNASA isn't doing that, and until they do, they run the risk of having the vision taken away from them.\u201d Cowing sees a real possibility that the vision could be truncated to simply create a space capsule that can travel to and from the space station. Dale, however, thinks that the vision is powerful enough to capture the public imagination. NASA is now working out a strategy to communicate it more effectively to the public. \u201cI think when we start to talk about it a little more with the American public and Congress, they will feel the same kind of excitement about it that we feel,\u201d she says. Why the agency has seen fit to wait three years before starting to talk the public into excitement is not clear; maybe they are reluctant to drum up support when everything is still on paper. Standing beneath the original Apollo 11 capsule, it seems as if Dale might be right. When John Kalman learns of NASA's new plans, his nine-year-old face lights up. \u201cI want to be an astronaut!\u201d he declares excitedly. But Aunt Janet, who was just a year older than John when Neil Armstrong first set foot on the Moon, seems more perplexed than impressed. \u201cSo they're recreating that whole mission?\u201d she asks while staring thoughtfully at the old command module. Then she shakes her head. \u201cWe need to explore other places,\u201d she says. \u201cWe've already done that.\u201d See Editorial,  \n                     page 459 \n                   . \n                     Replacing the Space shuttle: On wings and a prayer \n                   \n                     NASA draws up blueprint for craft to reach Moon and Mars \n                   \n                     US astronomy: Is the next big thing too big? \n                   \n                     Earth science loses autonomy as NASA switches focus to the Moon \n                   \n                     Return to Flight Special \n                   \n                     NASA's Vision \n                   \n                     Columbia Accident Investigation Board \n                   \n                     Bush's 2004 Announcement \n                   Reprints and Permissions"},
{"file_id": "446129a", "url": "https://www.nature.com/articles/446129a", "year": 2007, "authors": [{"name": "Alexandra Witze"}], "parsed_as_year": "2006_or_before", "body": "The rocks of Antarctica are obscured literally, and sometimes scientifically, by its ice. But drilling efforts are now showing what we can learn from the hard stuff. Alexandra Witze reports. When it comes to Antarctica's history, ice cores get all the glory. Large-scale ice-drilling efforts, such as Europe's EPICA and Russia's Vostok cores, capture headlines and the lion's share of people and funding. After all, these cores contain air bubbles that are hundreds of thousands of years old, a frozen time capsule from Earth's icy past. But buried beneath the thick layers of ice, the rocks of Antarctica have far older stories to tell. Trapped within layers of mud and sand are geological records stretching back millions of years. As Antarctica's ice teams continue to hunt for the oldest ice their drills will reach (see  page 126 ), a smaller band of rockhounds is on a similar quest to plug the gaps in the geological record. The team now has a core that promises fresh insight into how Antarctica's ice waxed and waned over the past few million years. On 26 December, a US$30-million international project called ANDRILL pulled up the final piece of a core from beneath the Ross ice shelf ( see map ). Previous coring efforts have offered peeks into Antarctica's deep history \u2014 back as far as 34 million years when the continent was first covered in ice. But the new core fills a gap in the ice shelf's history, and sets a new Antarctic record for drilling depth. The period covered by the core \u2014 from the present to more than 5 million years ago \u2014 seems to be quite active. Preliminary analysis has revealed thick layers of a greenish rock interspersed throughout the core. This is an indication of open-water conditions, suggesting that the Ross shelf retreated and then advanced at least 50 times within the past 5 million years. With this nearly unbroken record, scientists can explore the history of the shelf in unprecedented detail. \u201cIt's going to be a benchmark that we hope we can refer to for years to come,\u201d says Ross Powell, a geologist at Northern Illinois University in DeKalb. \u201cIt may be the geological equivalent of the Vostok ice-core record.\u201d As project co-leader, Powell is understandably enthusiastic about the core, but then so are other geologists. The Ross ice shelf, the largest in the world, is a floating extension of the even more massive West Antarctic Ice Sheet. That part of Antarctica is regarded as the most unstable and potentially prone to collapse in a globally warmed world. If the west Antarctic ice sheet melted entirely, it would raise the global sea level by about 5 metres. The Ross shelf is a tiny but important fraction of that. Its behaviour over past millennia could help researchers improve their understanding of how it might respond as temperatures rise in the future. Conditions on Earth are returning to a state that hasn't existed for millions of years, says David Harwood, a geologist and ANDRILL scientist at the University of Nebraska in Lincoln. \u201cWe have to go back to previous times in Antarctica when things were very warm, when carbon dioxide levels were higher, for an analogue of where we're heading,\u201d he says. The history of Antarctica's ice starts about 35 million years ago, when atmospheric carbon dioxide levels and temperatures began to drop. As the globe cooled, the great ice sheet in east Antarctica began to form. Some time afterwards \u2014 perhaps as early as 30 million years ago, or as late as 5 million years ago \u2014 west Antarctica gained its ice as well. ANDRILL scientists hope to refine these timing estimates. Until ANDRILL, Antarctic rock cores have only offered relatively short glimpses of the continent's history. Inland, researchers drilled shallow cores during the 1970s for the Dry Valleys Drilling Project \u2014 the deepest reached 300 metres. Offshore, ocean-drilling ships have collected sediment records that were scraped off the continent by the flowing ice; such debris provides clues to what Antarctica's bedrock looked like long ago. And in the broken-up sea ice that fringes the continent, icebreakers have occasionally been able to grab a quick core of similar sediments (see  'Quick-hit drilling' ). Technologically, it is much harder to drill through the thin layer of sea ice that surrounds the continent, or beneath the thicker ice shelves such as the Ross. In the late 1990s, the international Cape Roberts Project tried this for the first time, drilling through sea ice into the sea floor. A severe storm cut one of the seasons short, but the team managed to collect short cores dated to between 34 million and 17 million years ago (T. R. Naish  et al .  Nature   413 , 719\u2013723; 2001). That took scientists back to the formation of the east Antarctic ice sheet, but couldn't tell them what had happened more recently. \n               Shelf help \n             ANDRILL aims to fill in that gap. \u201cNo one has ever tried to recover a long sedimentary record from under an ice shelf,\u201d says the project's other co-leader, Tim Naish of the Institute of Geological and Nuclear Sciences in Lower Hutt, New Zealand. The 200-plus consortium of researchers, run by the United States, New Zealand, Germany and Italy, has allocated two field seasons for drilling two separate cores: the one just completed through shelf ice, and another to be drilled through thinner sea ice. For this season's work, which began in November near the New Zealand Scott base, the drilling company had to devise a hot-water system to get through the 85 metres of ice shelf, and keep it from refreezing around the pipe. The solution was a giant metal 'doughnut' filled with hot water that continuously ran up and down the drill shaft, keeping the surrounding ice melted enough for operations. Drilling was tricky at first. Tides cause the ice shelf to flex up and down by about a metre per cycle, and sideways by about half a metre per day. And stronger-than-expected ocean currents bent the pipe surrounding the drill as it stretched more than 900 metres through the ice shelf and the water below, before finally entering the sea floor. But after two months of non-stop drilling, the team recovered 1,285 metres of rock. The core, now stored in the freezers of the Antarctic geologic repository at Florida State University in Tallahassee, has yet to reveal all its secrets. The first challenge, says Powell, is pinning down its age. The top half of the core seems to cover the past 5 million years, and that's what contains the 50-plus cycles of ice-shelf collapse. The sediments show repeated layers of ground-up rock debris scraped off the continent by glaciers, interspersed with the open-water greenish ooze rich in the marine organisms known as diatoms. The transitions between the glacial sediments and the open-water ooze seem to be quite sharp, says Powell \u2014 suggesting that they took place relatively quickly. \n               Debris and ooze \n             Peter Barrett, a geologist at the Victoria University of Wellington in New Zealand, who was chief scientist for the Cape Roberts Project, says he was struck by the number of transitions over such a short period. \u201cI think it's spectacular,\u201d he says of the core. \u201cWe'd talked about what they might find beforehand of course. But I was surprised at the striking differences that the core brought out.\u201d Another major question is where the ground-up glacial deposits in the ANDRILL core came from. Studying the sediments in the core could help the team trace the material \u2014 and therefore the ice \u2014 back to its source. They might turn out to have come from somewhere else in west Antarctica, says Powell, or perhaps even as far as east Antarctica. Knowing the path the ice took could help researchers better understand how ice flows across the continent, thus aiding future models of ice flow as Antarctica warms. Other insights could come from comparing the ANDRILL core with ice cores such as EPICA. There isn't much overlap; only the upper 80 metres or so of the ANDRILL rock represent the past 1 million years, and the whole of EPICA covers only the past 800,000 years. But comparing rock and ice records could help palaeoclimatologists correlate increases and decreases in carbon dioxide levels with what the ocean and the Ross shelf were doing at the time. \u201cIt will be very exciting to see how the records of ice-sheet changes they have are related to the changes in Antarctic temperature we have,\u201d says Eric Wolff, an ice-core specialist with the British Antarctic Survey in Cambridge, UK, and a member of the EPICA team. The next chapter in the story will start in October, when the second leg of ANDRILL gets under way. For that, the researchers will drill through the 7-metre-thick sea ice in the southern part of McMurdo Sound, through the ocean and again into the sea floor. There, they are expected to pick up where the current core left off \u2014 probably around 7 million years ago \u2014 and extend the record to around 17 million years ago. With that, geologists hope, they will finally have a complete history of the Ross shelf. This is key because it seems likely that Antarctica will undergo some serious changes in the future. Both atmospheric carbon dioxide levels and temperatures are projected to increase beyond historical highs over the next several centuries. \u201cJust how long will it be before the temperature increase catches up and we watch the Ross ice shelf go away?\u201d asks Barrett. With the Ross gone, the scenario goes, melting on the main part of the west ice sheet could accelerate. That's something neither the ice experts nor the rockhounds want to contemplate. \n                     Drillers get into Antarctic seabed \n                   \n                     Does a giant crater lie beneath the Antarctic ice? \n                   \n                     Ice core shows its age \n                   \n                     Antarctic ice puts climate predictions to the test \n                   \n                     Climate Change In Focus \n                   \n                     Polar Research In Focus \n                   \n                     Ice cores web focus \n                   \n                     ANDRILL \n                   \n                     SHALDRIL \n                   \n                     Antarctic Research Facility \n                   \n                     Cape Roberts Project \n                   Reprints and Permissions"},
{"file_id": "445481a", "url": "https://www.nature.com/articles/445481a", "year": 2007, "authors": [{"name": "Thomas Hayden"}], "parsed_as_year": "2006_or_before", "body": "Is parachuting into the Amazonian jungle any way to save an ecosystem? One team of biologists thinks so. Thomas Hayden joined them on a trip to Peru to find out what they do. It's dark on the forest floor, the air humid and still. Various birds startle at an intruder's passing \u2014 plump brown tinamous and large bush turkeys or curassows, squawking in protest. Further along the trail, prehistoric-looking hoatzins flap and yodel above an oxbow lake, the blue patches on their cheeks bright in the late-afternoon sun. Suddenly, the air is alive with activity \u2014 a small band of Goeldi's marmosets, among the rarest of the New World primates, is alarmed by approaching visitors and unsure which way to flee. The tiny, jet-black monkeys leap from tree to tree, almost buzzing with fluffy, comical charm as they swarm to safety. \u201cPerfect,\u201d Peruvian ornithologist Christian Albujar exhales into the silence they leave behind. The upper Amazonian region of Sierra del Divisor \u2014 almost 1.5 million hectares of highland forest along Peru's border with Brazil \u2014 is full of similar delights. But decades of aggressive economic development throughout the Amazon basin have made such vast stretches of contiguous forest increasingly rare \u2014 and increasingly threatened. Although Divisor also faces pressure from development, it is still substantially pristine, protected thus far by its remoteness and elevation. Albujar is here to help survey the area's flora and fauna in an expedition organized by the Field Museum of Natural History in Chicago, Illinois. It would take years of meticulous fieldwork to fully document the region's rich biodiversity, but the group of 13 Peruvian, Brazilian and American scientists will spend just three weeks on the ground. This is a rapid biological inventory (RBI), the Field Museum's version of an increasingly popular tool in conservation science \u2014 a quick, intensive taxonomic expedition designed to identify areas of particular biological, geological and cultural significance before development and exploitation take hold. \n               Fast and furious \n             The idea is simple. With funds, expertise and time too limited to conduct thorough species surveys of every unknown region, experts instead target the most promising areas and quickly assess whether they are worthy of conservation. Proponents admit that the resulting data are incomplete, but say the compromise is justified because rough estimates of biodiversity can help inform preservation decisions. And even a quick survey by scientists with decades of broad experience in the field and in museum collections can lead to qualitative estimates of an area's relative conservation value. \u201cWe would all love to spend more time,\u201d says Debra Moskovits, a tropical biologist at the museum and founder of its RBI initiative. \u201cBut the time pressure is intense. If we can protect some of these areas, then maybe people will be able to do the more extensive studies the areas deserve.\u201d The protected areas on a list maintained by the United Nations have grown to some 11.5% of the planet's land surface in recent years 1 , but 'gap analysis' studies show that large numbers of species are not represented in the existing network 2 . Conservationists have argued for decades about how best to prioritize areas for future conservation: whether by geographical location and political expediency, or through some measure of species numbers, degrees of species interconnectedness, or the presence of rare, threatened or keystone organisms. The Field Museum team has focused its attention on the high-biodiversity upper Amazon and Andean foothills regions of Ecuador, Peru and Bolivia. Working with local, regional and international scientists and conservation groups, members of the core group of five museum biologists have conducted 12 rapid inventories in these regions since 1999, representing 9.2 million hectares of surveyed land \u2014 an area about the size of the US state of Maine. Preliminary results are shared immediately with local communities, organizations and political leaders; formal reports are usually published in a matter of months. \u201cPeople are making land-use decisions all the time, and they can't protect places if they don't know anything about them,\u201d says Corine Vriesendorp, a conservation ecologist and the Field Museum's director for rapid inventories. \u201cIf you bring in a crack team of biologists, you can very quickly tell decision makers whether a place is special or not.\u201d And it can be done relatively inexpensively \u2014 a typical, foundation-funded RBI costs about US$300,000 from initial planning through to the final published report. Six new protected areas have already been established, comprising about half of the territory the team has surveyed. Most of the remaining land is also on the road to legal protection. \n               Preservation order \n             The survey information can also help local communities and governments prioritize specific zones within a surveyed area, identifying those that most urgently need full protection, and those that might be allocated for uses such as tourism and sustainable harvesting. \u201cPeople overlook the impact that these kinds of survey can have on building local capacity for conservation,\u201d notes tropical land use specialist Arturo Sanchez-Azofeifa of the University of Alberta in Canada, who is not involved in the programme. \u201cYou can get local participation and a broader perspective, and that really helps build expertise and interest in the local communities.\u201d For the Field Museum team, that means working with local scientists and graduate students to compile the inventories, and forming links with indigenous groups, regional and national governments, and local conservation organizations. The biological work is paralleled by 'social inventories' \u2014 surveys of the organizational structure and natural resource use of local villages. That social aspect, says Vriesendorp, sets RBIs apart from other rapid conservation assessments and is crucial to their success. \u201cThe dream is to protect these areas indefinitely,\u201d she says, \u201cand that can only happen if the local people and their leaders are fully engaged.\u201d The core of the RBI approach is the rapid inventory itself. The surveys have their roots in a 1987 birding trip, when physicist and avid birder Murray Gell-Mann and ornithologist Ted Parker first started talking about using quick-time biological surveys to spur conservation. By 1990, environmental group Conservation International, based in Washington DC, had put their ideas into practise; Moskovits and other Field Museum team members, including botanist Robin Foster and ornithologists Doug Stotz and Tom Schulenberg, were all early participants. The idea caught on with other groups, and by 1995 the Field Museum had initiated its own rapid inventory programme, spearheaded by Moskovits. Each inventory starts with satellite images, geological maps and, if possible, video footage taken from the air. Experience has taught that the taxonomists' time is better spent walking trails than wielding machetes to create them. The RBI scientists identify areas they think will give them a broad cross-section of the available habitats, and advance teams of local workers prepare bush camps and cut trails to their specifications. In Sierra del Divisor, that means three separate campsites: two at higher elevations, one along a lowland riverbank, and each with a network of trails stretching for dozens of kilometres along streams, through swamps and up mountains. The museum biologists are joined by nine scientists and students from Peru and Brazil. \u201cWe try to balance the teams to get broad experience across the region, as well as local expertise,\u201d Vriesendorp says. The inventory focuses on limited taxa \u2014 trees, shrubs, mammals, birds, fish, reptiles and amphibians \u2014 to keep the workload manageable, and seeks to determine \u201cwhat's common, what's rare, what's dominant and what's really weird\u201d, says Vriesendorp. The scientists tabulate individual species of plant and animal, and draw on experience to gauge the relative health, uniqueness and diversity of each ecosystem. The social inventory team, meanwhile, conducts its own surveys to map out the location and structure of local human populations, including the potential for and threats to long-term conservation. \n               Jungle manoeuvres \n             The survey site is spectacular. Sierra del Divisor contains the only mountain range in the Peruvian Amazon, a series of low sandstone ridges and weathered volcanic cones with peaks of up to 800 metres high. The resulting series of 'elevation islands' supports a stunted forest ecosystem and, combined with a cool wind from the south, provides the unexpected experience of shivering from cold in the middle of the Amazon. Even close to the Equator, seasonal variations in temperature and rainfall affect the forest's plants and animals, presenting challenges to conducting a survey in a single season. The dry August conditions are ideal for the fish team \u2014 lakes and wetlands have shrunk, leaving their quarry restricted to small ponds. But many trees are without the flowers and fruit that botanists often need to make species-level identifications. Seasonal variations make work even harder for the group studying amphibians. \u201cIn the wet season, the trees would be screaming with frogs,\u201d says Mois\u00e9s Barbosa da Souza of the Federal University of Acre, in Brazil. It's the middle of the night, and the former jungle warfare specialist is halfway up a tree, working hard to retrieve an Amazon milk frog from a high-up bromeliad. But, he adds, \u201cat this time of year just look what I have to do.\u201d The Sierra del Divisor RBI results 3 , published in December, include up to three dozen new species of plant, fish and amphibian, and many more species that are endemic to the area. The forests are also home to many endangered plants and animals, including valuable lumber trees and 20 threatened species of mammal, several of which are hunted for bush meat. Schulenberg lured the Acre antshrike \u2014 a species of bird previously known from a single ridge on the Brazilian side of the border \u2014 out from the stunted ridge-top forests by playing its call on an iPod. And, on one occasion, Moskovits returned to the riverside camp energized, despite hours tramping through an aguajal palm swamp, by the sight of some 30 red uakaris. These monkeys are large, with bald red faces and orange fur, and extremely rare. A total of 12 species of monkey and marmoset was observed at the camp, one of the highest primate diversities documented at a single site in the New World. Sierra del Divisor has been teetering between protection and development for most of the past decade, but the August 2005 RBI survey and follow-up meetings may have been enough to tip the balance. In April 2006, the Peruvian government declared the entire 1.48-million-hectare area a reserved zone, a preliminary classification that halts all development until the permanent protection status can be determined. However, petroleum exploration leases are still being granted within the protected area, leaving the ultimate fate of the forest and its inhabitants in question. The hope, says Moskovits, is for an area co-managed by conservation groups and indigenous organizations, with strict protection for unique biological and geological resources and for the indigenous groups living in voluntary isolation within its borders. The RBI team remains deeply involved with the planning process, Moskovits says, but it will be up to Peruvians to hammer out the final compromises. Despite its obvious impact on conservation decisions, there has been debate about the scientific value of rapid assessment data. \u201cWe do get criticized \u2014 some people say our work isn't scientific because it's done so fast,\u201d says Foster, the team's most experienced member. \u201cPeople feel that you have to do a quadrant plot and measure each tree to within a millimetre. But for this purpose it's much better science to sample quick and dirty over several larger areas.\u201d A one-hectare plot, for example, may contain just a few mature trees, Foster says, \u201cbut those canopy trees account for most of the production, most of the insects, most of the fruits that are good for birds, monkeys or peccaries. If you just study your quadrant, you'll miss the forest for the trees.\u201d Yet with any short-term study \u201cyou'll get true presences but you can't know whether the absences are definitive\u201d, says ecologist Michael Willig of the University of Connecticut. \u201cSo you'll almost certainly be underestimating biodiversity.\u201d Given the realities of limited funding and expertise, however, Willig says that rapid assessments are a valuable tool not just for conserving biodiversity, but for understanding it, too. Ultimately, he says, \u201cyou just have to respect the limitations of the data.\u201d It doesn't take much to see that, even in an area as remote as Sierra del Divisor, the threats are manifold. Although large-scale exploitation has not yet reached Divisor, gas, mining and logging leases overlap with much of the new reserved zone, and unregulated settlers push closer to the area every day. Seen from a helicopter, the intact forest canopy is breathtaking, with brightly coloured parrots and toucans darting between giant trees. But gaps soon appear in the undulating surface where giant mahogany and tornillo trees have been felled for lumber, and wood smoke billows up from the proliferating slash and burn fields below. In Pucallpa, a sprawling regional capital along the Ucayali River less than 100 kilometres from Sierra del Divisor, thick smoke often closes the municipal airport for days. The town's only green space \u2014 a strip down the centre of a main road \u2014 boasts a sign whose translation reads, \u201cjungle, lungs of the planet.\u201d \n               Trading standards \n             Down by the river, settlers pour in from around the region in low-slung wooden boats laden with fish, skins and produce, some of it harvested from near the reserved zone's boundaries. Downtown, traders display hundreds of peccary and deer hides. One proprietor refuses photographs; another proudly directs a browser to the real prizes \u2014 contraband skins of ocelots and other wild cats, barely concealed behind his front door. At the main market, several hundred stalls sell everything from wild palm fruits and desiccated bushmeat to medicinal saps. There are species that have already disappeared from more heavily populated regions of the Amazon, including various fishes as well as tortoises and river turtles. Some of these have already been butchered; others are propped up and immobile on their backs, and sold alongside plastic buckets of their pickled eggs. There are wire cages packed tight with bright green parakeets and majestic ruff-necked macaws, but the most wretched pets for sale are the squirrel monkeys and saddle-back tamarins. They can be bought for less than US$8 \u2014 inexpensive, but evidence of the high cost to local fauna when humans intrude. \u201cThe forest still looks so large, people just don't realize that this is going to be the end of these guys,\u201d says Moskovits. \u201cIt's repeated over and over, every day and in hundreds of places throughout the Amazon. It really has a huge impact, and it makes you worry, are we moving fast enough?\u201d \n                     Conservation areas in Brazil set to grow \n                   \n                     Calls to conserve biodiversity hotspots \n                   \n                     Brazilian Amazon being cut down twice as fast \n                   \n                     Conservation in Brazil: The forgotten ecosystem \n                   \n                     Field Museum's rapid biological inventories \n                   \n                     Field Museum's tropical plant guides \n                   \n                     United Nations List of Protected Areas \n                   Reprints and Permissions"},
{"file_id": "446247a", "url": "https://www.nature.com/articles/446247a", "year": 2007, "authors": [{"name": "John Whitfield"}], "parsed_as_year": "2006_or_before", "body": "Updating the tree of life needs both the skills of evolutionary biologists and the data from genome-crunchers \u2014 the two ignore each other at their peril. John Whitfield reports. On 1 July 1858, in the Linnean Society of London's imposing neoclassical building on Piccadilly, biology changed for ever. That evening John Bennett, the society's secretary, read out papers by two biologists, Alfred Russel Wallace and Charles Darwin. From that point on, linnaean science was about more than describing and classifying living things. It was about using their traits to reveal their evolutionary relationships and assign them a position on life's family tree. Famously, such a tree was the only illustration in Darwin's  On the Origin of Species , published the following year. In the past 30 years, DNA sequencing has revolutionized this project. Comparing gene sequences has revealed not only a new domain of life, the archaea, but has affirmed that chimpanzees rather than gorillas are our closest relatives. And what was a trickle has become a flood \u2014 the major sequencing centres alone generate some 5,000 bases of DNA a second. About 1,000 species have now had their genomes sequenced, with more being published each month. This torrent is set to reshape systematics \u2014 as the study of evolutionary relationships is known \u2014 once more. \u201cGenomes are giving a much better view of the tree of life on Earth,\u201d says evolutionary biologist Mark Blaxter of the University of Edinburgh, UK. \u201cThe revolution is just starting \u2014 every new genome is causing rethinking.\u201d Blaxter is one of several researchers who has rallied under the banner of phylogenomics \u2014 the use of large quantities of genetic data (not just entire genomes) to build evolutionary trees, or phylogenies. What has also become clear is that many problems cannot simply be battered into submission with more data. \u201cQuestions that are not resolved by a kilobase of sequence are seldom resolved by a megabase,\u201d says Jeffrey Boore of the Joint Genome Institute in Walnut Creek, California. At its best, phylogenomics is a two-way street \u2014 genome researchers need evolutionary biologists to help them work out the function of genes, and to avoid embarrassing mistakes. Perhaps the most high-profile gaffe was the declaration by the Human Genome Project in 2001 that 100\u2013200 genes in humans had come directly from bacteria. Analysis had revealed genes found in both humans and bacteria, but not in any species more closely related to humans. Project scientists concluded that such genes probably got into humans by lateral gene transfer from bacteria, a kind of inter-species sex where chunks of DNA cross from one cell to another. The result was heralded as one of the project's major revelations. Jonathan Eisen first heard this finding while watching the press conference on TV at his workplace, the Institute for Genomic Research in Rockville, Maryland. \u201cI felt sick to my stomach,\u201d he says. \u201cThey were talking about genes involved in brain development having come directly from bacteria. If you just think about that for a minute, it sounds so implausible.\u201d Sure enough, Eisen, and other similarly flabbergasted evolutionary biologists, rapidly shot the claim down, showing that the genes in question were more likely to have been present in the common ancestor of humans and bacteria but then lost in other lineages 1 . This story is not the only example of the mistakes that can happen when genomics ignores evolution. \u201cMolecular biologists are prone to not treating evolution as a particularly rigorous science,\u201d says Eisen, who now works at the University of California, Davis. \u201cBut you can't do good genome analysis without evolutionary analysis.\u201d Phylogenomics is about integrating the two, he says. \n               Form and function \n             Despite the shaky start, genome-sequencing centres now recognize the importance of evolution, and Eisen is one of several evolutionary biologists working at the heart of genomics. It's not just about keeping egg off faces. The techniques of evolutionary biology can also enhance other aspects of genome analysis \u2014 finding genes, and working out what is done by the proteins they encode. Presented with a gene of unknown function, scientists can search for similar genes, of known function, in other sequences. A more rigorous approach is to use the sequence data to build a phylogeny of the different versions of that gene. This lets researchers track the gene's evolution, see how its function might have changed, and identify which other gene is most closely related to the target \u2014 which is not necessarily the one with the most similar sequence. Software packages can automate this process, and will predict protein function more accurately than doing a simple sequence comparison. Tree-building is still the trickiest part. Systematists work out the relationships between genes, species or higher groups by searching for characters, be they morphological, biochemical or genetic, that arose in their last common ancestor and are shared by their descendants \u2014 lactation in mammals, for example. But even for just 10 species, there are more than 34 million possible evolutionary trees. Phylogenetic researchers have developed complex algorithms to search among these possibilities, to find the tree most likely to reflect reality. Picking the right tree is challenging. \u201cFor 15 years, people hoped for some software that could solve these problems at the push of a button, but sadly that's not come to pass. It's a very, very difficult problem algorithmically,\u201d says Boore. It is especially hard to say whether traits were inherited from a common ancestor, whether they look similar but evolved independently in unrelated groups \u2014 such as the wings of birds and bats \u2014 or whether they might have evolved but were then lost in later descendants, such as eyes in cave-dwelling fish. Often several different trees are equally good explanations of the data. And different genes from the same set of organisms often predict different trees. \n               Branching out \n             It is in the latter case that genome-sized datasets should be helpful. Analysing many genes at once allows the overall pattern of evolution to emerge, swamping the effect of quirky genes and confusing traits. One early success was in working out the relationships between groups of mammals, such as rodents, bats, carnivores and ungulates. These seem to have evolved very rapidly after the extinction of the dinosaurs, so their various common ancestors had little time to evolve unique characters that would mark their descendants. But multiple analyses, combining a few dozen genes from a cell's nucleus with the complete genomes of mitochondria, a cellular energy generator with its own small chromosome, have resulted in a robust consensus on who is related to whom. Not everything is so tidy. Take the nematodes. For the past decade, a debate has swung back and forth about where this group of worms belongs. Anatomically simple organisms such as nematodes are a particular headache for systematists, as they offer few clues to their ancestral state. Based on morphology, nematodes were placed close to the root of the tree of animals, because they lack a body cavity called a coelom, which is found in molluscs, insects and vertebrates. But in the late 1990s, molecular analyses suggested that nematodes had actually lost their coelom, and belonged in a group with the insects that was named the Ecdysozoa, because its members grow by moulting their outer layers 2 . Where to put nematodes, and how they relate to insects, matters to genomics more broadly because the worm  Caenorhabditis elegans  and the fruitfly  Drosophila melanogaster  are two of the most important model animals. If the Ecdysozoa is the true group, both are equally distant from humans. If not, the fly is more like us than the worm. Humans,  C. elegans  and  D. melanogaster  were among the first animals to have their genomes sequenced. These complete genomes seemed to tell a different story from the earlier genetic analysis that put forward the Ecdysozoa. Or rather, they retold the old story. A tree built from the human, fly and worm genomes 3  makes the nematodes the outsiders. \n               Express yourself \n             One way to choose between contradictory trees is to add more species to the analysis. With animals, one soon runs out of sequenced genomes to add \u2014 but there is a third way between having some data for lots of species and lots of data for a few species. This is to use expressed sequence tags (ESTs), which are DNA sequences made from the genes that are active in cells. EST libraries can provide data on several hundred genes for a species relatively quickly and cheaply, allowing more species to be analysed. Using ESTs representing nearly 150 genes, Herv\u00e9 Philippe's team at the University of Montreal, Canada, built a tree containing 35 species that supported the existence of the Ecdysozoa 4 . Most phylogenomics researchers are now inclined to believe the Ecdysozoa data. But enough analyses suggest the contrary for the question to remain open, says Antonis Rokas, a genomics researcher at the Massachusetts Institute of Technology in Cambridge. \u201cI would be agnostic,\u201d he says. \u201cJust looking at the phylogenetic evidence, I'm not convinced that either side has it right.\u201d This seesawing has become common, as accumulating data tip an argument first one way, then the other. Last year, Philippe's team suggested that the closest relatives of vertebrates were the sea squirts, which resemble bags of jelly, and not, as previously thought, the fish-like cephalochordates 5 . Philippe's work grouped the cephalochordates with the echinoderms (sea urchins and the like) with the troubling implication that either fish-like creatures evolved twice, or that the common ancestor of vertebrates and sea urchins looked like a fish. In November, evolutionary biologist Max Telford and his colleagues at University College London, added a dataset of 35,000 amino acids from other groups, including starfish. The new tree put the cephalochordates back where they started 6 . \u201cThere was a big sigh of relief,\u201d says Telford. \u201cNot even Philippe had much faith in the first result.\u201d In general, relations between the animal phyla \u2014 large groups such as molluscs or arthropods \u2014 remain a mystery. \u201cThe evolution of the animals has plagued us for years,\u201d says Rokas. \u201cWe're hitting a wall.\u201d The animal phyla all seem to have evolved very rapidly some 600 million years ago and the signal in DNA data has eroded. Rokas has compared evolutionary trees of fungi and animals built using the same genes 7 . Those from fungi, whose genes are thought to have changed at a steady pace, are well resolved. The animal trees are rickety. Will more sequences help? \u201cThere are some problems that you could throw a whole genome at and will still probably be unresolved,\u201d says Telford. He gives the example of predatory marine worms called the chaetognaths, or arrow worms, which refused to reveal their evolutionary allegiance even when more than 70 genes were analysed 8 . \u201cWe had more data than you can shake a stick at, and we still weren't able to place them,\u201d he says. \n               Location, location, location \n             Where sequence fails, other features of the genome might help. The positions of genes, for example, are also inherited, and changes in gene order can mark evolutionary splits. Boore pioneered this approach, using the order of genes on mitochondria to show that insects and crustaceans are closer to each other than either is to spiders or centipedes 9 . Similarly, the points at which the 'jumping genes' called transposons insert themselves into sequences have been used to reconstruct the history of the mammals 10 , and the positions of introns \u2014 gene pieces that are cut out before DNA is turned into protein \u2014 have been used to support the existence of the Ecdysozoa 11 . When a rare genetic event is shared among a group, it's a sure sign that they had a common ancestor, says Boore. The usefulness of such approaches is not yet universally accepted. \u201cI have found many genome features don't work well for evolutionary reconstruction \u2014 they're too prone to convergent evolution,\u201d says Eisen, explaining that the same gene order probably often arises independently in different groups. Philippe believes that sequence data are still the most reliable, but that other approaches will mature in time. Part of the problem may be that most of the genomes sequenced so far, besides humans, have been lab models, diseases or economically important organisms. None is ideal for understanding evolution \u2014 model species and crops have quick generation times, so they are among the fastest evolving species. \u201cThe organisms chosen as models have got biological properties that mean, genomically, they're odd,\u201d says Blaxter. But evolutionary biologists are hoping that genome sequencers will soon switch focus. \u201cA lot of genome projects are finishing and the centres are looking for something to do,\u201d says Tim Littlewood of the Natural History Museum in London. One target might be the eukaryote tree. The eukaryote domain covers organisms whose cells are divided into compartments, including such familiar kingdoms as animals, plants and fungi, but also a plethora of single-celled organisms called protists. No one has any idea which of these groups is the closest ancestor to the protist that made the leap to multicellularity and gave rise to the animals. \u201cOne of the most interesting questions that phylogenomics can address is of the major lines of eukaryotic evolution,\u201d says Philippe. But evolutionary puzzles are not going to drive sequencing agendas on their own, says Boore. One fear is that once all the crops and pathogens are sequenced, the cash will dry up. \u201cSo far, the appetite for comparative genomics has been surprising,\u201d Boore says, \u201cbut it's hard to predict how things are going to go.\u201d When selecting organisms for sequencing, the potential for illuminating human biology and disease are strong arguments. Fussing over the lineage of obscure marine species is less compelling. \u201cGenomics people recognize the need for systematics,\u201d argues Littlewood. If systematists \u2014 whose discipline labours under a fusty, arcane image \u2014 show that voguish molecular research depends on their work, it should help reverse the decline in jobs and funding for the discipline, he says. \u201cIn many ways, genomics stands to learn more from systematics than vice versa \u2014 the onus is on systematists to point out that we can help.\u201d \n                     Phylogenomics and the reconstruction of the tree of life \n                   \n                     Microbial phylogenomics: Branching out \n                   \n                     Evolution: Affinity for arrow worms \n                   \n                     Family differences divide insect world \n                   \n                     Creepy-crawlies keep secrets \n                   \n                     Shaking the family tree \n                   \n                     A jumping off point \n                   \n                     Tree of Life \n                   \n                     TIGR's tree of bacterial life \n                   \n                     Linnean Society of London \n                   \n                     Jonathan Eisen's blog \n                   Reprints and Permissions"},
{"file_id": "446020a", "url": "https://www.nature.com/articles/446020a", "year": 2007, "authors": [{"name": "Helen Pearson"}], "parsed_as_year": "2006_or_before", "body": "From a New Jersey beauty parlour to cutting-edge genetics by way of her own alopecia, Angela Christiano's life has all been tied up with hair. Helen Pearson meets a woman whose head is full of the stuff that covers it. No one forgets their first encounter with Angela Christiano. For Jorge Frank, it was in an office at New York's Columbia University in 1995. \u201cIn walks this blizzard of a woman with the bright smile and the big hair and these long fingernails,\u201d recalls Frank, a molecular dermatologist at University Hospital Maastricht in the Netherlands. \u201cI thought: 'This is a rock star not a scientist'.\u201d Christiano certainly cuts an arresting figure. Deep-brown and bronze tresses erupt from her head in a gravity-defying explosion and surge over her shoulders. Her perfect make-up and carefully manicured nails complete an image that is the antithesis of a stereotypical scientist. But for Christiano, hair is not just an adornment. It is a filament that binds together her appearance, her family, her personal life and her work. Descended from two generations of hairdressers, she came to appreciate hair's true importance when her own locks began to fall out in an episode of alopecia areata. Working at Columbia, she immediately shifted the focus of her research from skin disease to hunting down the genes that underlie human hair disorders, such as an atavistic condition in which people sprout thick hair all over their faces. Her scientific work may even end up with a cosmetic use, saving men and women with normal but nevertheless unwanted hair from shaving, waxing and depilation. \n               Grooming talent \n             Christiano grew up in New Jersey with an unusual appreciation for the importance of personal grooming. Hair and nails \u2014 \u201cectodermal appendages\u201d, as she now calls them \u2014 were her family's bread and butter. Her grandparents had set up a barber's shop after immigrating from Italy; her mother worked as a hairdresser and beautician. Christiano spent after-school hours sweeping up hair in her mother's shop. \u201cI didn't realize it at the time,\u201d she says, \u201cbut I was becoming a keen observer.\u201d Many of her schoolmates went on to become hairdressers and beauticians, and Christiano's family expected her to stay at home until she got married. But by that time, school had already kindled in her a love for genetics, and she was the first of her family to attend college and earn a PhD, at Rutgers University in New Jersey. She didn't abandon her roots though. Having watched so many hours of beautification, she found herself drawn to dermatology. \u201cIt is one of the only areas in medicine where visual things provide clues,\u201d she says. \u201cOnce I was exposed to a skin disease it was like instant love.\u201d The subject is rarely the focus of such enthusiasm; indeed, dermatology is sometimes given short shrift in medicine. Very few dermatological conditions are life threatening, and a concern with unsightly skin or hair can be dismissed as vanity. As a result, those working in the field take great pains to spell out the difficulties faced by those with alopecia and other disorders. \u201cEvery day I have someone in my clinic crying and saying they want to commit suicide,\u201d says Abraham Zlotogorski, a specialist in hair disorders at Hadassah University Medical Center in Jerusalem, Israel, and a collaborator of Christiano's. Christiano established herself in the science of skin during her postdoctoral position at Thomas Jefferson University in Philadelphia, Pennsylvania. While there, she studied the genes underlying epidermolysis bullosa \u2014 a condition in which the skin is incredibly fragile, blisters and falls off, and one of very few life-threatening skin diseases. \n               First signs \n             When she was 30, Christiano had just started a faculty position at Columbia, and felt under pressure to establish her own group in dermatology. Then one day, her hairdresser in New Jersey asked her whether she'd had a biopsy: \u201cYou have a little spot.\u201d The next day, a colleague who worked across the hall took a closer look and let out \u201cthe most blood-curdling scream I've ever heard\u201d, Christiano recalls. The spot was the size of an orange. In her new apartment, the blocked shower that she had attributed to bad Manhattan drains turned out to contain a clump of her hair. Christiano was diagnosed with alopecia areata, a disease in which the immune system attacks hair follicles, causing hair to fall out in patches, or sometimes completely. Because the stem cells in the follicles escape the attack, the hair sometimes grows back, and the condition can come and go throughout life. Christiano had known that hair problems ran in her family: her mother and grandmother both developed female pattern baldness and wore wigs. Now she learned that a distant cousin had a more severe form of alopecia areata and did not have any hair at all. As soon as she was diagnosed, she decided to refocus her work on alopecia. Alopecia areata is known to involve several genes and is hard to trace through families. But Christiano found a Pakistani family in the scientific literature who had a simple, inherited form of alopecia called papular atrichia. The gene involved causes a specific phenotype in which babies lose their first hair from the front of the head to the back and then remain bald. By looking for genetic landmarks inherited alongside the condition, Christiano's team narrowed its search for the responsible gene to a large region of chromosome 8. But the group was not able to narrow it down any further until Christiano attended a meeting of the Society for Investigative Dermatology and heard a talk about a mutant mouse called hairless that also loses its hair in a wave from head to tail. The human equivalent to the gene turned out to sit on that suspicious section of chromosome 8, and when she sequenced it in her Pakistani patients, they all had mutations that seem to prevent hair growth and destroy the hair follicle 1 . \n               From mice to men \n             Since then, Christiano has tracked down genes in two other hair disorders in a similar way, using human families alongside mouse models. In one disorder, she showed that a type of cadherin, a protein that holds cells together in the hair follicle, is mutated in people whose hair breaks off near the head like razor stubble 2 . With time, the gene hunting has become easier, thanks to human sequence data and high-resolution genome maps. Last year, she and her colleagues took only six weeks to show that some families who had no nails on any of their fingers or toes had a mutation in a gene normally active in the embryo's developing nails 3 . Christiano is sensitive to the accusation that she is merely 'stamp collecting' \u2014 bagging genes for the sake of it \u2014 and tries to delve into what each gene does and how they drive skin and hair development. But she admits that the hunt has an addictive rush: \u201cIt's one of the most exciting things we do. When we have a new gene, it's like having a new baby.\u201d And it's not as if all gene hunting is now easy. One of Christiano's most intractable puzzles is in her \u201cMexican hair people\u201d, who have thick, 'terminal' hair all over their faces rather than the finer 'vellus' hair, which is normal. Christiano and her team have spent more than five years studying one of the only reported cases of a family with this disorder \u2014 called hypertrichosis 4 . But although they sequenced 82 genes in the relevant region of the X chromosome and every snippet of microRNA, they could not find a causative mutation. Some kind of genetic trickery could be afoot: perhaps a mutated RNA outside the protein-coding genes is failing to regulate a gene on another chromosome. To her obvious delight, last year Christiano made headway in the genetics of her own affliction. Various researchers have identified families in which several members have the disease, and Christiano and her collaborators have been able to pinpoint four locations in the genome that are strongly associated with the condition in these families 5 . If they find the actual genes, they might be able to unravel the immune system's antipathy to hair follicles, and even suggest ways that drugs could abate it. Christiano had hoped that a company called Sirna Therapeutics, which is based in San Francisco, would find a use for her discoveries. Sirna had licensed some of the patents from her work and, before the company was acquired by Merck in late 2006, researchers with the company were trying to find a way to deliver inhibitory RNA to the skin to curb hair growth, with an eye to the cosmetic market. Despite her background and impeccable grooming, Christiano says that she is not interested in developing cosmetic applications from her work. She wants the company to find a way to deliver the inhibitory RNA to the skin so that she and other researchers can use it to hinder genes involved in conditions such as alopecia. Her own case didn't need such interventions. Over two years she lost ten large patches of hair and became obsessed with her tresses, checking them constantly and carefully styling and dyeing them to hide the nude spots. But her hair came back, although with an odd, wiry texture. \u201cNow every day I have hair, it's like an accomplishment.\u201d The flamboyance of her accomplishment with her ectodermal appendages can lead to some teasing. When she goes to conferences that have hairdryer-free accommodation: \u201cI have to bring my entire arsenal, and people make fun of my luggage.\u201d The teasing may, on occasion, give way to unfair criticism, says skin researcher Elaine Fuchs of Rockefeller University in New York: \u201cSome scientists tend to judge people by their scientific pedigree and their appearance, and Christiano doesn't fit the mould.\u201d \u201cWhat I so admire about Christiano [is that] she proves these people wrong by her accomplishments.\u201d And Fuchs isn't talking about that hair. \u201cI've learned that it's a phenotype really,\u201d Christiano says. \u201cEveryone has a phenotype and this just happens to be mine.\u201d \n                     Drug discovery: In the eye of the beholder \n                   \n                     Hair-raising stem cells confirmed in mouse skin \n                   \n                     National Alopecia Areata Foundation \n                   Reprints and Permissions"},
{"file_id": "446127a", "url": "https://www.nature.com/articles/446127a", "year": 2007, "authors": [{"name": "Lucy Odling-Smee"}], "parsed_as_year": "2006_or_before", "body": "The fourth International Polar Year (IPY) is a bit of a misnomer \u2014 it's actually two years long. From March 2007 to March 2009, a host of scientists will head out to both the Arctic and Antarctic for targeted research, from marine biology to anthropology. Here's a look at some of the projects being planned \u2014 although funding for some is still pending. \n               Whale movement \n             Several hundred beluga whales ( Delphinapterus leucas ) in the Arctic will be tagged with satellite transmitters and, in some cases, oceanographic data collectors. Understanding the timing and pattern of beluga movements in relation to ice and ocean conditions may help efforts to protect the whales in the face of climate change. \n               Ocean microbes \n             A Norwegian-led effort to document the biodiversity of microorganisms in the polar seas could provide a basis for understanding how these creatures help to regulate the ecosphere. \n               Polar astronomy \n             Because of their extremely cold, dry, stable air, the polar plateaus provide the best sites on Earth for a range of astronomical observations. An Australian-led team will assess just how good the conditions are for astronomy at sites including Dome A on the Antarctic plateau. \n               Spider survey \n             German researchers hope to lead a survey of spider biodiversity across the Arctic. Because spiders adjust their lifecycles to microclimatic conditions, studying them can help scientists track the effects of rising temperatures on terrestrial habitats. \n               Carbon pools \n             An international effort, led by Sweden, plans to assess the quantity and quality of organic matter in high-latitude soils. The work may prove crucial to predicting what could happen to the enormous stock of carbon trapped there if the soils thaw. \n               Martian mimics \n             The Phoenix spacecraft will land on Mars during the IPY. Scientists hope to compare data from the planet's northern polar region with soil measurements from an analogous 'extreme environment': the Antarctic Dry Valleys. \n               Antarctic aliens \n             More than 40,000 people visit the Antarctic each year \u2014 and they probably bring with them the seeds and spores of non-native species. The IPY will see the first full assessment of the environmental impact of these visitors. \n               Past warming \n             One way to predict what might happen in a warmer climate is to look back in time. The WARMPAST project will use Arctic sediment cores to reconstruct ocean temperatures for the past tens of thousands of years. \n               Icy lakes \n             Scientists from Canada and Russia will pore over historical data to see how the timing of ice freezing and breaking up on Arctic lakes has changed over the past 50 years. \n               Changes in sea level \n             UK and other researchers plan to add gauges to monitor the sea level and tides in the waters around Antarctica. \n               Arctic greening \n             Climate change is likely to alter the distribution and type of plants at high latitudes. Assessing satellite data, and doing field studies to produce new vegetation maps for Russia, Alaska and Canada, will help scientists predict such changes. \n               Reindeer herders \n             The knowledge and experience of nomadic reindeer herders, accumulated over generations, will be documented to establish how herders living across Norway, Russia and Alaska can sustain their way of life. \n               Warm vents \n             Studying hydrothermal vents on the Arctic mid-ocean ridge is a challenge as major portions of the ridge lie more than 4,000 metres under pack ice. In July and August, robotic vehicles will dive to the Gakkel Ridge to search for vents. \n               Satellite shots \n             The photographic power of Earth-observing satellites is being pooled to yield a wide range of snapshots of the world's polar regions in the highest resolution possible. \n               Solar activity \n             Scientists will take measurements from the polar regions to assess whether variation in the Sun's activity affects Earth's weather and climate by influencing a global electrical circuit in the atmosphere. \n               Particle physics \n             Physicists will use the IceCube observatory being built at the South Pole to search for subatomic particles called neutrinos. During the IPY, glaciologists are being invited to use the detectors to study ice flow. \n               Polar bears \n             A Danish-led team will examine contaminants in the muscles and bones of polar bears killed by Inuit hunters. Chemical analyses of the bears' body tissues could also shed light on how much climate change is stressing the animals. \n               Open leads \n             Some 200 researchers from 15 countries will study the circumpolar flaw lead \u2014 an area of open water that forms each autumn when the main Arctic pack ice pulls away from coast. With thin ice becoming more common in the Arctic seas, this region offers a glimpse of how changes to the ice affect ocean life. \n                     Climate Change In Focus \n                   \n                     Polar Research In Focus \n                   \n                     Ice cores web focus \n                   \n                     International Polar Year \n                   Reprints and Permissions"},
{"file_id": "445706a", "url": "https://www.nature.com/articles/445706a", "year": 2007, "authors": [{"name": "Apoorva Mandavilli"}], "parsed_as_year": "2006_or_before", "body": "How often does independent research change laws as well as minds? A lobby group in Delhi is forcing the Indian government into new regulations. Apoorva Mandavilli meets its leader. A decade ago the city of Delhi was choking. Fumes from the growing traffic rendered the air thick and foul with toxic chemicals, earning India's capital city the dubious distinction of being the fourth most polluted city in the world. Levels of fine particles in the air were nearly 17 times higher than the permissible maximum. You could almost feel them as you breathed. Visit Delhi today, and the difference is palpable. Green-striped buses and auto rickshaws rush past powered by compressed natural gas. Levels of sulphur in diesel have been brought down from 2,500 parts per million to 500 parts per million. Concentrations of particles in the air are still three times the national standard, but more bearable \u2014 the air feels unmistakably cleaner. The improvement is largely due to the efforts of one small non-governmental organization, the Centre for Science and Environment (CSE). Founded by the science journalist Anil Agarwal in 1980, the Delhi-based group launched a relentless campaign in 1996 to replace diesel in Delhi's public transport with a cleaner fuel: compressed natural gas. Its headline-grabbing tactics were what you might expect from a group founded by a science journalist: at one point it hired a booth at a Delhi car show and offered attendees lung tests. In April 2002, after years of legal battles, India's Supreme Court forced Delhi's public vehicles to switch to compressed natural gas. \u201cIt's undoubtedly one of the most influential organizations in the country,\u201d says Mahesh Rangarajan, a former Rhodes scholar and commentator on Indian politics based in Delhi. So how did a small band of campaigning journalists evolve into a respected environmental pressure-group powerful enough to change laws and send multinational companies running for cover? The CSE is that rare entity, an activist group that is prepared to back up its campaigns with its own research. India has countless problems, but the CSE picks its fights wisely. For the past 20 years, it has focused on five main areas: air pollution, climate change, water management, pesticides and poverty eradication. Today the CSE has a charismatic woman at its head in Sunita Narain, who took over the reins of the organization in 2002 after the death of Agarwal. \u201cWe are essentially playing the role of a watchdog, pushing for policy and being a public advocate,\u201d she says. \u201cWe have the arrogance to believe we are as powerful as government.\u201d Most recently, the CSE has taken on the soft-drinks giants Coca-Cola and PepsiCo. In August 2006, the CSE released a technical analysis of 12 popular soft drinks made by these companies and sold in India, claiming that they contained toxic pesticides, including lindane, DDT, malathion and chlorpyrifos, at up to 36 times the European standards for bottled water. In a testament to the organization's credibility, four Indian states promptly banned sales of soft drinks made by Coca-Cola and PepsiCo at schools, government-owned offices and hospitals, with one state imposing a total ban on sales and production. Sales of Coca-Cola products nationwide fell by about 18%. As in other CSE campaigns, the battle was waged on two fronts. Articles reporting the results appeared in the CSE's fortnightly magazine  Down to Earth , while the technical data generated by three staff scientists were posted on the CSE's website. \u201cTo my mind, that's what makes them powerful,\u201d says David Dickson, director of UK-based SciDev.net, an online science information resource for the developing world. \u201cI'm not saying all their science is perfect, but I think they realize the importance of having science behind their argument. That differentiates them from many other environmental groups.\u201d Agarwal played the research card from the start. A mechanical engineer by training, Agarwal's career as a journalist stretched from Indian newspapers to UK publications including  Nature  and  New Scientist . Returning to India in 1980, Agarwal launched the CSE with two fellow journalists. They began by researching environmental issues, looking at poverty and resource management at the village level and putting together the first of the ambitious series of 'Citizen's Reports' published under the general title  State of India's Environment . At that time, environmental issues were discussed only in developed countries, or as Narain puts it: \u201cFirst you get rich, then you pollute, then you get dirty, then you start cleaning up.\u201d But Agarwal argued that India was too poor not to care about the environment. \u201cHe was putting environmental issues on the development agenda and he was doing it at a time when most people weren't,\u201d says Dickson. \n               Hard graft \n             The material for those first reports was gathered painstakingly. Narain, who joined the CSE in 1981 as a volunteer, and her colleagues collected newspaper articles, literature from non-governmental organizations and scientific papers. They also wrote to scientists for more information. \u201cThe task was to take all that, assimilate it, analyse it and find a trend. It was massive,\u201d recalls Narain. Right from the first report in 1982 it was apparent that the CSE had found a winning formula. The report gained accolades from the international press, stirred Tunisia to launch its first environmental ministry and inspired Lester Brown of the Worldwatch Institute in Washington DC to launch his annual  State of the World  reports. A 1991 report by the CSE linking monsoon flooding with deforestation also helped transform the environmental debate in India. Although the CSE wasn't the first to make the link, its report consolidated research and traditional knowledge and presented it in accessible language, says P. V. Unnikrishnan, humanitarian coordinator for ActionAid in Asia. \u201cIn 1991 for such a report to come out, touching on such a controversial issue, was very good. It helped kick-start an informed debate.\u201d Although generating its own research gets the CSE attention, it also means that its research can become the focus of criticism. For example, its reports on pesticides in soft drinks were attacked on technical grounds from the start by both the multinationals involved and the Indian government. This high-profile fight has earned Narain the moniker of 'Coke lady' and has gained the CSE fame beyond Delhi. \n               Standard bearer \n             Narain is used to public criticism, but she admits the soft-drinks fight has been much nastier than she expected \u2014 and the CSE got into it almost by accident. The organization became interested in general water-safety standards in 2002, when it tested ten brands of bottled water and found pesticides in them. Puzzled by the vehemence with which Coca-Cola and PepsiCo, which had some of the purest bottled water, were fighting demands for water standards, the CSE decided to look at pesticide levels in soft drinks. \u201cWe just tested out of curiosity, but now that we're in it, we can't back off,\u201d Narain says. Nevertheless, Narain says her main gripe isn't with Coca-Cola or PepsiCo, but with the government, which has dragged its feet in setting safety limits for pesticide residues in soft drinks. \u201cEven now, our only demand from the government is standards,\u201d says Narain. \u201cWe found something that's unclean. Clean it up. We want standards. Simple.\u201d Most countries, including India, have standards for pesticide levels in the water that goes into carbonated drinks but not for the product itself. That may be acceptable in countries where the raw material is relatively clean, but not in India, Narain argues.The solution, she says, is for India to implement testing of the final products. The CSE released its initial findings on pesticides in soft drinks in 2003. After initially disparaging the group's methods, the government set up a Joint Parliamentary Committee \u2014 only the fourth in India's history \u2014 to investigate. The committee upheld the CSE's methods and findings and asked the Bureau of Indian Standards to review soft-drinks standards. Two years later, the bureau set new pesticide limits, which the CSE welcomes. But it has yet to make them law \u2014 thanks, Narain alleges, to interference from the soft-drinks industry. \n               Imperfect harmony \n             Coca-Cola and PepsiCo are both standing by their products, saying that their soft drinks made in India are of the same quality as anywhere else. The companies maintain that setting standards is complicated because there are no reliable tests to assess pesticide levels in carbonated beverages. \u201cMeasuring for the pesticides [in a final product] is very difficult to do,\u201d says Kari Bjorhus, a spokeswoman for Coca-Cola in Atlanta, Georgia. The CSE data have not been peer reviewed but scientists contacted by  Nature  say that the organization followed methods accepted by bodies such as the US Environmental Protection Agency. \u201cThe data are believable and the levels they find [of pesticides] are believable,\u201d says Laura McConnell, a research chemist for the US Department of Agriculture's research service. \u201cThese should be values that would stand up in court as valid.\u201d McConnell also questions the soft-drinks companies' claim that testing for pesticides is complicated. \u201cI don't think it's that complex,\u201d she says. After the 2003 report, Coca-Cola approached the UK government's Central Science Laboratory (CSL), which tested 180 samples and reported that pesticide levels in the soft drinks were below \u201cacceptable\u201d limits for drinking water. But, Narain says, the lab never released the detailed reports or chromatograms for scrutiny. Bjorhus confirms that the CSL did not release its full data to the public, but she argues that the lab didn't find any pesticides, \u201cso there is nothing to review in terms of findings\u201d. She notes that the CSL is \u201cone of the most respected laboratories in the world and their integrity is unquestioned\u201d. The UK laboratory's report has not yet persuaded Indian state governments that the drinks are safe, and they remain banned in many schools and state-run institutions. The pesticide battle has also fuelled a wider resentment in India against multinational companies, with people smashing up drinks bottles. In response, Coca-Cola has launched an expensive media campaign, featuring top movie stars such as Aamir Khan vouching for the drinks' safety. But Narain says, \u201ctheir sales are still down, Aamir Khan or no Aamir Khan\u201d. \n               Close focus \n             Away from the media battle, the technical arguments over safety are now focused on the validity of different tests. After the CSE released its 2006 report, assessing 25 new soft-drink samples for pesticides, PepsiCo's president Indra Nooyi visited India in December, saying that the two multinationals would work together to find \u201ca breakthrough science-based method to reliably and consistently measure the levels in finished products\u201d. Coca-Cola and PepsiCo have commissioned AOAC International, an independent not-for-profit American scientific association, to review about 17 available analytical methods and to make recommendations to the Indian government. The report is expected in February. Narain maintains that she would be willing to accept evidence showing the CSE's science to be wrong, but that the companies or the government have to provide it first. \u201cWe are not asking for closure of Coke and Pepsi, we are not interested in them as companies,\u201d says Narain. \u201cWe just want standards.\u201d Today the CSE employs 125 journalists and researchers, about half of whom hold degrees in science. But it wasn't until 1992 that Agarwal launched the magazine  Down to Earth , backed by the belief that journalism is one of the most powerful ways to change things in India. Today, the magazine has a popular website and 20,000 paying subscribers. Like Agarwal, Narain sees herself as a journalist first, and is not afraid to attack those scientists who, she says, stay on the sidelines or sell their expertise to companies. She once wrote an editorial criticizing the scientific community for its \u201carrogance and ignorance, both completely unacceptable\u201d. Scientists have in turn criticized the CSE. In 2005, Narain was asked to head the Tiger Task Force, which looked into the controversies inherent in human and tiger coexistence and advised on tiger conservation. The final report left a lot to be desired, says biologist Ullas Karanth of the Wildlife Conservation Society, India, but he was impressed with Narain's hard work and willingness to change her mind when confronted with new evidence. \u201cShe's a person who in the end accepts science.\u201d Soon after the launch of  Down to Earth , the CSE embarked on its anti-pollution campaign in Delhi and on one of its other top priorities \u2014 water. The organization helped communities use traditional know-how to collect and store rainwater. \u201cWhen we began advocating this solution in the mid-1990s, people laughed at us, basically said we were idiots, that this was not a solution for the scale of India,\u201d says Narain. \u201cToday, nobody will argue that with you.\u201d Passionate about every aspect of her work, Narain is harder to pin down on personal questions. On the sunny day I visit the CSE offices, Narain is rushing about barefoot in her office, laughingly deflecting the conversation from her personal life. \u201cI'm the world's most boring person. I come to work, I go to sleep, I know nothing else,\u201d she says. In addition to answering my questions, Narain juggles other visitors, including local student journalists, frequent updates on the lawsuits the CSE is embroiled in, a television interview, and a domestic crisis precipitated by a fallen tree in front of her house. Narain comes from one of Delhi's illustrious families: her grandfather was an eminent journalist and his brother a gandhian freedom fighter. Her father, also a young freedom fighter, died when she was eight, leaving her mother to bring up Narain and her three sisters. Narain joined the CSE straight after school and pitched into whatever tasks were most urgent. She says Agarwal was a tough boss \u2014 she is reputed to be one herself \u2014 tearing up copy until the writers got it just right. Agarwal was known for his intelligence and integrity, but many say privately that Narain can be more open-minded and has given the CSE a friendlier profile. Some even suggest she is too soft on the government \u2014 and too cosy with government insiders. Among Narain's friends, for instance, is prime minister Manmohan Singh's daughter. Such personal attacks offend Narain: \u201cWho cares if I am well connected? I haven't gone to Manmohan Singh to ask for a job.\u201d Narain may flinch at personal attacks but remains undeterred in her work. When I met her, she had just returned from Ahmedabad, the second city where the CSE has successfully pushed for a switch to compressed natural gas. \u201cWe've been fighting for two years to clean Ahmedabad's air,\u201d she says. \u201cYesterday, the air in Ahmedabad was clean. It's worth it.\u201d \n                 See Editorial,  \n                 \n                     page 683 \n                   \n               \n                     The Centre for Science and Environment \n                   Reprints and Permissions"},
{"file_id": "445586a", "url": "https://www.nature.com/articles/445586a", "year": 2007, "authors": [{"name": "Declan Butler"}], "parsed_as_year": "2006_or_before", "body": "Energy efficiency is one of the least flashy but most promising ways to cut carbon dioxide emissions. In the first of two features, Declan Butler explores the energy-saving possibilities of an intelligent electrical grid. In the  second , Zo\u00eb Corbyn looks at how labs can cut their energy use. This article is part of Nature's Climate Change 2007 special. \u201cSwitch off the washing machine or dryer for the next 3 minutes, and let me buy, at 10 cents per kilowatt hour, 20% of your solar energy output.\u201d Such instant electronic transactions between electricity distributors and smart electric meters in millions of homes and businesses are set to add some badly needed intelligence to the electricity grid, bringing greater efficiency and reliability. Ways that humanity can adapt their use of energy to the realities of global warming tend to focus either on supply \u2014 renewable technologies such as solar power \u2014 or demand, personal abstinence of various sorts. But there is untapped potential in the space between supply and demand \u2014 the mundane world of energy distribution. Making the electrical grid more efficient would offer benefits on the demand side, by helping users to consume less energy, and in terms of supply, by providing better ways to handle the intermittent and distributed nature of alternative energy sources. The humble electric meter might seem an unlikely place to start a revolution. But today's centralized grid, which has electricity fanning out from a few large generators across transmission lines to end users, is mostly dumb. Utility companies have detailed data on events at the power plant or on their transmission line, but once the electricity radiates out to the community they have no idea where it goes, let alone how to manage it. Smart meters change that by providing data on what is happening in every corner of the grid at any instant. Utilities in Europe and the United States are rolling out millions of meters that send real-time data on the electricity use of individual homes and businesses via the Internet, or along the electricity supply. In turn, these meters can receive real-time data on grid conditions, load and pricing. The meters are a stepping stone to smart grids, which would provide a modern, distributed network of computing and telecommunications \u2014 a dynamic 'energy Internet'. And just as the Internet triggered an explosion of innovative technologies and services, not least the Web, new and more efficient electricity services will flow from smart grids. \u201cSmart meters are the harbinger of the future,\u201d says Steven Hauser, vice-president of strategy at GridPoint, the first company to sell such energy management systems direct to consumers. \u201cThey are the breakthrough that has helped utilities understand that you can see what's happening on the network right down to the customer level,\u201d he says. Enel, an Italian electricity company based in Rome, is the world's biggest user of smart meters. It has installed more than 30 million meters since 2001, mostly to improve billing. In the United States, the world's largest consumer of electricity, California is one of many states following suit. By 2011, Pacific Gas & Electric, which serves northern and central California, will have supplied its 9.3 million customers with smart meters, and Southern California Edison is rolling out 5 million. The latest meters are built to meet smart grid standards designed to ensure that all networks and devices speak the same language \u2014 the grid equivalents of the data-transfer protocols that made the Internet possible. The standards are being driven by a swarm of research efforts, including the US Department of Energy's (DoE) GridWise programme, and the European Union's SmartGrid initiative. On the supply side of the efficiency ledger, smarter devices and richer data will facilitate the development of sophisticated energy-management software. A smart grid can accommodate a greater diversity of fuel supplies and, in particular, intermittent energy sources, such as wind and the Sun, says Kevin Kolevar, director of the DoE's Office of Electricity and Energy Assurance. A centralized grid is inefficient and costly. Only a third of the fuel energy burnt in power plants ends up as electricity, with half lost as waste heat, and a further 8% lost along long-distance transmission lines. Moreover, 20% of generating capacity exists only to meet peak demand, so it runs just 5% of the time and provides just 1% of supply. The grid is often congested because it relies on a few high-traffic arteries. The congestion amplifies the inefficiency because if the utility cannot redirect power from efficient sources, they have to turn to costlier, dirtier and more inefficient sources to meet peak demand. A more distributed grid, by its very architecture, can improve efficiency by matching local supply with demand. With multiple decentralized energy sources, electricity can be generated close to the point of use, avoiding the losses and congestion that result from long-distance transmission. Some of the most efficient energy sources are small turbines powered by natural gas, or biogas, which use waste heat to provide heat and hot water to the local area, and convert energy with 70\u201385% efficiency. The world leader in decentralized power, Denmark, now generates half its electricity through decentralized grids, with combined heat and power accounting for 80% of local-area heating, and wind power about 20% of all electricity. As a result, its carbon dioxide emissions have tumbled from 937 grams per kilowatt hour in 1990, to 517 grams per kilowatt hour in 2005. Denmark began its push towards decentralization two decades ago, before smart-grid technology was available, relying mostly on tough regulations to force the change. The key to Denmark's success was standing up to the utility companies, says Henrik Lund, an expert in energy systems at Aalborg University. For instance, the government required that energy companies buy back electricity from consumers at 85% of the price. Lund recalls being told by the utilities in the 1980s that it was not technically possible for more than 10% of electricity to be provided by wind power. The country wouldn't be where it is now, he says, if they had taken statements from the utility companies as fact. This is an important lesson for the smart-grid movement. Although the infrastructure for intelligent grids is falling into place, overcoming institutional and market barriers remains a major issue, says Kolevar. \u201cMany utilities have a disincentive to push distributed generation, as generally it is the customers who are the biggest beneficiaries, with the utilities seeing lower sales. Resistance to distributed generation can be seen in the development of 'net-metering' laws, which oblige US utilities to allow consumers to sell energy back to the grid. Forty states have implemented such laws, but by 2004, just 15,200 US customers were taking advantage of the rules, 13,000 of whom were in California, according to the Network for New Energy Choices, a New York-based advocacy group. In a survey the group published in November, the fingerprints of utility companies were found in nearly all the state laws. Indiana forbids commercial and industrial companies from taking part, for example, whereas Arkansas pays such poor rates for consumer-generated electricity that only three customers have participated since its launch in 2001. California emerged among the best, but even here the state has capped solar-energy generation at 2.5% of utilities' peak demand, arguing that it needs to assess its impact on the grid. But the regulatory environment continues to change. The 2005 Energy Act requires that all federal buildings be equipped with two-way metering and energy-management systems by 2012, creating a huge potential market. Eventually, all states will be expected to offer net-metering and time-based pricing. And with the traditional grid straining under peak load, utility companies are waking up to distributed generation and storage as being vital to meet future demand, says Kolevar. Companies are also emerging to exploit the new regulations. GridPoint's device, for example, can manage a residential solar panel or windmill, store generated energy in a battery that holds 12 kilowatt hours of electricity, and then buy and sell to the grid depending on price. The battery also provides 8\u201312 hours of backup supply for a typical home. The units contain no one technological breakthrough, admits Hauser, but instead pull existing devices together into a single box and add clever software. The climate is now ripe for such units, he says, owing to increasingly friendly legislation. Gridpoint units are not cheap, ranging from $6,000 to $16,000, but buyers will benefit from a 30% income tax credit on the purchase price through the 2005 Energy Act (with a $2,000 cap for residential customers), and could shave 10\u201315% off their electricity bills. With appropriate regulation, real-time pricing should increase the competitiveness of renewable energies and of storage because local producers could sell power when prices are highest during peak demand and purchase it at the lower rates during off-peak periods. The overall effect would be to flatten out the load and price curve, and shave demand at peak hours, says Kolevar. The extra capacity offered by a distributed market should also mean that fewer of the inefficient central generators would need to be built. Capacity demands can also be buffered by energy storage systems. A suite of distributed storage technologies are available, including fuel cells, flywheels, superconducting magnetic energy storage and advanced batteries, but they have never been used or tested on a large scale. Smart grids should change the economics in their favour. Electricity storage is another area in which GridPoint is trying to turn theory into practice. It has signed a deal with the power company Cogentrix, headquartered in North Carolina, and is in discussion with others to test thousands of its units as a backup supply to ease demand on the grid at peak periods. Even the idea of using plug-in hybrid vehicles as distributed energy storage is now being taken seriously, says Kolevar (see  'Plug in, turn on...sell out' ). It was previously considered a research area, but Kolevar sees real interest in the idea, adding that what is now needed is a \u201clarge and definitive\u201d demonstration project. Another hoped-for result of real-time pricing is that homeowners and businesses will reduce their energy consumption. Typical wholesale electricity prices vary wildly, even over the course of a day, but consumers are usually charged a flat rate calculated as an average estimate over months. Smart meters can, however, make decisions that are based on real-time pricing information and the user's preset choices, such as to turn down the air conditioning if the price goes above a certain level. Market research says that introducing such price information will encourage consumers to use less electricity overall, not just at peak periods, although these ideas still need to be tested in the marketplace. The Pacific Northwest National Laboratory in Richland, Washington, is testing a GridWise system to send real-time pricing via the Internet every 5 minutes to 200 homes on Washington's Olympic Peninsula. Heating and use of other appliances is adjusted automatically according to the pricing and usage preferences set by the users. Although the results are not yet in, preliminary data show that customers have responded well to the system and are highly responsive to the price, with consumption falling by more than 10% at peak times, says Donald Hammerstrom, a project manager for GridWise at the Pacific Northwest National Laboratory. Experiments like this would have been unthinkable a few years ago, when an infrastructure for grid data and communications did not exist, says Hauser. With intelligent grids now emerging, the stage is set for a wave of innovation that will generate, store and manage electricity more efficiently, resulting in energy production and consumption that are less polluting and more cost-effective. \n                     Solar energy: A new day dawning?: Silicon Valley sunrise \n                   \n                     Make your own energy at home, Britons urged \n                   \n                     Energy in California: Power struggle \n                   \n                     US energy bill pushes research but fails to cut consumption \n                   \n                     www.nature.com/nature/focus/energy/index.html \n                   \n                     European Technology Platform for Electricity Networks of the Future \n                   \n                     GridWise \n                   \n                     IntelliGrid Consortium \n                   \n                     California Energy Commission \n                   Reprints and Permissions"},
{"file_id": "445703a", "url": "https://www.nature.com/articles/445703a", "year": 2007, "authors": [{"name": "Mark Schrope"}], "parsed_as_year": "2006_or_before", "body": "Could a change in the dining habits of orcas crash an ecosystem? Mark Schrope reports on a mystery that reveals how little we know of the oceans. When you're up to nine metres in length, weigh more than an African elephant and can swim at over 50 kilometres an hour, you can expect to be admired for your sheer brawn. Striking black-and-white markings over a sleek, streamlined torso will earn you points for beauty. But orcas, also called killer whales ( Orcinus orca ), can lay claim to brains too. These magnificent creatures have devised cunning methods to earn a top-carnivore's living from the sea. Some force sharks to the surface and club them with their flukes; others hunt the sharks down in underwater gangs. In Norwegian fjords, the orcas herd up herring, while on the shores of Patagonia they all but beach themselves to pick off seals. These differences in hunting practices between the orcas, along with differences in their looks and songs, have led some experts to suspect that the creatures actually belong to several separate species (see  'Species apart' ). An abiding mystery, though, is whether a change from one feeding habit to another could profoundly alter the balance of marine ecosystems. In 1998, a team led by marine ecologist Jim Estes at the University of California, Santa Cruz, proposed that just such a shift might explain an enigmatic and precipitous decline in Western Alaska's population of sea otters ( Enhydra lutris ) 1 . The decline was of particular interest because the lack of predatory sea otters caused a boom in the sea-urchin population; the flourishing sea urchins, in turn, laid waste to large areas of kelp forest, thus changing the balance of a whole ecosystem. Five years later, Alan Springer, a marine ecologist at the University of Alaska in Fairbanks, and colleagues, including Estes, took the hypothesis several steps further. They suggested that the sea otters' demise might be the last stage of a grander collapse in which orcas had shifted repeatedly to new prey as old prey ran low 2 . The cause of this change, they argue, was commercial whaling, which deprived some orca populations of the great whales \u2014 such as humpbacks ( Megaptera novaeangliae ) and blues ( Balaenoptera musculus ) \u2014 on which they used to feed. The orcas turned instead to smaller sea mammals such as harbour seals ( Phoca vitulina ) and sea lions ( Eumetopias jubatus ), eventually working their way down to the otters and triggering the destruction of the kelp forests. The idea that whaling could have changed the orca's diets had been suggested before by French researchers 3 , but this was the first time that it had been linked to wholesale ecological change. \u201cI'll be the first to admit that it isn't even close to being definitive,\u201d says Estes, who came to the story through his studies of sea otters \u201cIt was intended to be provocative.\u201d Mission accomplished, judging by the two rebuttals contained in an upcoming issue of  Marine Mammal Science 4 , 5 . \u201cSuperficially attractive\u201d is pretty much the nicest term used \u2014 \u201csimplistic and highly selective\u201d and \u201cpoorly supported\u201d also feature. \u201cIt's a beautiful idea. I wish it were true,\u201d says Lance Barrett-Lennard, a biologist studying orcas at the Vancouver Aquarium in British Columbia, Canada. \u201cBut it's wrong.\u201d Meanwhile, Springer and his colleagues are working on their own rebuttals. The detractors challenge nearly every link in the chain of evidence that implicates the orcas, starting with whether they ever actually ate many great whales. There is little doubt that they ate whales: Springer, Estes and their colleagues point to historical records from whalers recounting orca attacks. Television viewers around the world have seen the harrowing sequence of a grey-whale calf being eaten by orcas that featured in the documentary series  Blue Planet  \u2014 not an uncommon occurrence. In fact, the very name 'killer whale' may be a confused translation of the old Spanish term  asesina ballenas , or whale killer. \n               Biting attacks \n             The problem is that in some of the places where whale populations have rebounded since commercial whaling stopped, biologists have yet to see any orca attacks on whales. \u201cWe can be out there on the water day after day with literally tens if not hundreds of humpbacks, and we just never see attacks,\u201d says Paul Wade of the National Marine Mammal Laboratory in Seattle, Washington. But whales are sometimes found with scars from orca bites. \u201cIf they aren't eating them, why are they biting into them?\u201d asks Estes. Although no current population of orcas has been seen hassling humpbacks, he says, that doesn't mean that they aren't doing so \u2014 or did not in the past. There is also a possibility, raised by Hal Whitehead of Dalhousie University in Nova Scotia, Canada, that whaling helped the orcas, rather than cheating them of their food. Harpooned whales that were left floating on the surface were often partially eaten by orcas, he and his colleague Randall Reeves note in their paper on the subject 6 . In the heyday of whaling, harpoon shots may have sounded like dinner bells to orcas' ears, announcing a big fresh meal that required no hunting. In this version of the hypothesis, the end of whaling changed the orca's diet not because the number of humpbacks hit an all-time low, but because whalers stopped providing orcas with ready-to-eat meals. The next step in the cascade is no less controversial. Wade points out that although populations of harbour seals crashed around the Aleutian Islands off Alaska in the 1970s and 1980s, they were stable in parts of the Bering Sea where commercial whaling had been just as heavy. And questions remain as to whether the seals and sea lions declined species by species or all at once. At least as far as the North Pacific and Bering Sea are concerned, Estes agrees that the issue of sequentiality is important. Sequential collapse would point very specifically to the orcas, whereas simultaneous collapse might reflect human exploitation of the fish that all the mammals prey on. But simultaneous collapse does not rule out variants of the hypothesis. At the other end of the world, Terrie Williams, also of the University of California, Santa Cruz, and her colleagues have proposed that the simultaneous collapse of the southern elephant seal ( Mirounga leonina ) and southern sea lion ( Otaria flavescens ) populations in the Southern Ocean might have been caused by orcas finding new prey after the end of whaling 7 . The 'fewer sea otters more sea urchins' link in the chain is not quite as controversial. Orcas have been seen to eat sea otters, in one case sweeping a group off the ice it was lying on to catch the otters in open water. But the evidence that enough of this sort of thing goes on is only circumstantial. Proponents point to observations that populations of sea otters have declined in areas of open water that contain orcas, but not in nearby orca-free lagoons, for instance. \u201cThere's nothing wrong with circumstantial evidence,\u201d says Wade. \u201cYou just have to be clear that it is circumstantial evidence, not direct evidence.\u201d That said, you don't need many orcas eating otters for the effects to be felt. As big, fast, warm-blooded creatures, orcas need a phenomenal amount of energy. Williams calculates that relying on sea otters for even a fraction of that energy would mean eating a great many of them. But orcas eating otters would not prove that the cascade effect suggested by Springer and Estes exists.Wade thinks that a turn to sea otters as food would not require a collapse in the stocks of other possible prey. Hunting practices are learned, so if, for example, an injured mother decided to eat sea otters because they were easier to catch, that behaviour could spread in a population irrespective of what other food was available. This might also reconcile the possibility that orcas used to feed on humpbacks with the claim that they no longer do. Williams, like some other participants, has been surprised at how long the debate has gone on. \u201cIt basically just highlights how difficult it is to study these animals,\u201d she says. Clearly, one of the reasons that the arguments have been so heated is that the conservation stakes are high. The domino effect from whales through seals to sea otters as a potential explanation for the declines in marine mammals could take some of the pressure off commercial fishing. It might also be seen as a strong argument to continue the whaling moratorium \u2014 although some fisherman have argued that the orcas should be culled. The debate is likely to continue for years, meaning that orcas will remain a symbol of how much remains unknown about the oceans. \u201cYou have to have the debate and you have to have people thinking creatively, and then, ultimately, you'll come down to the truth,\u201d says Williams. \u201cIt will happen \u2014 we just don't have the information now to know what the truth is.\u201d \n                     Oceanography: The real sea change \n                   \n                     Whaling blamed for seal and otter slumps \n                   \n                     A stunning performance \n                   \n                     Orca Research \n                   \n                     North Gulf Oceanic Society \n                   \n                     National Marine Mammal Laboratory \n                   \n                     Orca Relief Citizens' Alliance \n                   Reprints and Permissions"},
{"file_id": "445590a", "url": "https://www.nature.com/articles/445590a", "year": 2007, "authors": [{"name": "Zo\u00eb Corbyn"}], "parsed_as_year": "2006_or_before", "body": "This article is part of Nature's Climate Change 2007 special. In 1971, no one really worried about energy efficiency \u2014 certainly not at Fermilab, the US Department of Energy's (DoE's) particle-physics laboratory in Batavia, Illinois. A new superconducting ring for the lab's accelerator, designed to push particles closer than ever to the speed of light, was enthusiastically talked up as the 'energy doubler'. A few years on, though, as the lab prepared its funding bid against the backdrop of the oil crisis, the system started being referred to as the 'energy saver', shifting the emphasis from increased performance to reduced power requirements. When you spend US$1 million a month on electricity, as Fermilab does, such care in presentation is important. Under most circumstances, though, scientists give scant thought to totalling up the wasted power and unnecessary carbon emissions that their work generates. Geoffrey Bell, who works on reducing the energy consumption of Lawrence Berkeley National Laboratory in California, is one of the exceptions. He's eager to point out that a traditional fume cupboard, for example, uses as much energy in a year as three US households. \u201cIf you have a laboratory with 50 of those, you've made a town in one building!\u201d Laboratories consume between five and ten times more energy than office buildings \u2014 but they are also rarer and more diverse in design, making neat, generalized solutions to profligacy hard to find. Add that to concerns about safety and a lack of transparency in costs (few scientists know or care what their lab's electricity bill is), and you get a 'that's just the way they are' mentality. That's the mindset that the Labs21 programme, an initiative started by the DoE and the US Environmental Protection Agency (EPA), exists to challenge. According to the EPA's Dan Amon, who oversees Labs21, the big beast for energy experts to tame is ventilation: some 60\u201370% of the energy a lab uses goes on moving, heating and cooling the air, with the rest split about two to one between appliances and lighting. Labs21 concentrates its efforts on 'wet labs' \u2014 chemical or biological labs fitted with fume cupboards, which need to exchange the lab's air with outside air at a high rate. Bell, a member of Labs21, says that most of these facilities have been designed according to \u201cwhat is in the drawer\u201d, with fume cupboards making many more air changes, and at higher velocities, than is necessary. In the United States and Britain, safety standards require that air is exchanged 6 to 12 times an hour. Yet rates as high as 15 to 25 per hour are not unusual. Even a small reduction from 12 to 10 air changes per hour can reduce the amount of fan power by more than 40%, says Bell. A key to progress here is replacing fume cupboards that replace air constantly with technology that has a variable air volume. A variable-air-volume fume cupboard adjusts the speed of the fan, and therefore the energy use, to the position of the cupboard's sash opening. As the requirements for ventilation change \u2014 when researchers open and close the sashes \u2014 the building's exhaust and supply fans adjust accordingly. Bell says that some US labs have used a carrot-and-stick approach to encourage careless researchers to make the most of this technology by using the sashes on their cupboards sensibly. \u201cBeware the sash police,\u201d he jokes, \u201cthey might make you walk around with your apron on backwards as a punishment.\u201d The new Science and Technology facility at the DoE's National Renewable Energy Laboratory (NREL), completed in July last year, is designed on Labs21 principles, with an airflow rate of at least 0.3 cubic metres per minute per square metre (see  'Look, no carbon!' ). With ceiling heights of three to four metres, this means about six air changes an hour. In addition to variable-air-volume technology, the lab has systems to reuse the heat from exhaust air and from the water used to cool the equipment. It also makes extensive use of natural light and groups activities that require high ventilation rates together so they can be dealt with differently from the rest of the space. \u201cOverall, if you were to compare this facility with a more conventional lab, the savings are about 41% of the overall energy cost,\u201d estimates NREL's Pete Sheldon, who coordinated the lab's design. \u201cThis is about $96,000 per year.\u201d He adds that it's the quality of the light that the researchers comment on most frequently. \n               Sustainable inklings \n             According to Phil Wirdzek, who had Amon's job at the EPA in the early 1990s, the Labs21 programme started more or less by accident. When reporting their annual energy consumptions to the US Congress, the EPA and the DoE went beyond what was required and included the energy bills of their various facilities in calculations. Wirdzek was responsible for his agency's report. \u201cIt was like, 'holy mackerel did I get us into trouble!' But it started Labs21 because we began to say: well, how do you fix these things?\u201d Now administered in part by Wirdzek's not-for-profit organization the International Institute for Sustainable Laboratories, Labs21 runs an annual conference and trade show, offers design assistance and training to labs, which become 'partners', and has recruited 4,000 engineers, architects and manufacturers to its green agenda. Designing an energy-efficient lab requires a different way of thinking, according to Amon. \u201cIf you don't tie into the community [of people thinking differently], then you are going to do what you have always done,\u201d he says. Bell emphasizes that when designing or refitting a lab, the key is to get all concerned \u2014 lab managers, contractors, scientists, union representatives, cleaning staff \u2014 to buy into the process from the beginning, and to ask why things are the way they are. Why are there so many fume cupboards and refrigerators? Why is the temperature tolerance so tight? Why are air-flow rates not reduced at night? Although he doesn't put it all down to Labs21, Amon guesses that about a quarter of the labs in the United States now use energy-efficient design principles. Thanks to interest from the US Green Building Council in Washington DC, labs will soon be eligible for the prestigious Leadership in Energy and Environmental Design (LEED) awards that are already in place for office buildings, schools and shops. Peter James, a professor in environmental management at Bradford University in the United Kingdom and coordinator of a public-sector initiative to raise universities' environmental performance, is importing the Labs21 approach to Britain. According to James, who took part in an analysis of data from around 50 campuses in Britain a few years ago, \u201ca really horrifying thing we found was that, by and large, the more modern the laboratory, the worse its energy consumption\u201d. He points to more stringent health and safety requirements in the past ten years and to the wasteful margins being built into ventilation rates; like Bell and Amon, he is eager to assure people that the waste can be cut without compromising safety. \n               Two-way systems \n             Mike Dockery, a UK-based consultant who designs laboratory systems, argues that Britain can teach the United States a thing or two \u2014 specifically on 'FlexiLab' methodology. First developed by the drug giant GlaxoSmithKline for its UK facilities, the FlexiLab system is now being rolled out at the company's sites in the United States. Portable variable-air-volume fume cupboards connect into prefabricated, standardized ducting and service 'spines'. It makes it possible to change the type of science a lab is doing over the weekend, says Dockery, adding that reusability also means less wasted equipment when the lab's function changes. James says that in Britain, public labs are playing a game of catch-up with private companies. But are scientists really ready to embrace sustainability in their own backyards? A recent online survey by the UK Department for Environment, Food and Rural Affairs had telling results. Of 400 scientists across a wide range of disciplines questioned in August 2006, 95% agreed that science and technology were important if sustainable solutions were to be developed for the future, but only 40% said that they always or often considered the effect their work would have on the environment when planning their research. Of those who didn't think about the environmental impact of their research, 53% said it was because they felt it wasn't relevant to their area of science. The results don't surprise Bell. \u201cScientists tend to be in their own little world most of the time. They are not necessarily belligerent or disrespectful, they are just very focused people...and they really don't understand how they might be knocking things over in the process.\u201d \u201cBut,\u201d he adds, \u201cat some point in time you have to pull them out of that blinders-on attitude to say, 'Look \u2014 you did great science over here and you made this new wizard medicine \u2014 but look what you did over here.'\u201d This week, Geoffrey Bell will be answering questions about this subject on the  Nature  newsblog ( http://tinyurl.com/3a3d4f ), where you can tell us what your lab is \u2014 or isn't \u2014 doing about energy efficiency. \n                     Energy for a cool planet:  \n                   \n                     Labs21 (US) \n                   \n                     Labs21 (UK) \n                   \n                     US Green Building Council \n                   \n                     National Renewable Energy Laboratory \n                   Reprints and Permissions"},
{"file_id": "445479a", "url": "https://www.nature.com/articles/445479a", "year": 2007, "authors": [{"name": "Bruce Goldman"}], "parsed_as_year": "2006_or_before", "body": "Extracting a cell from a budding human embryo can expose genetic defects, but does it actually help generate more healthy babies? Bruce Goldman investigates. Suppose that one-eighth of you suddenly disappeared \u2014 but that within a few hours, your missing parts grew back and life carried on with no obvious sign of harm. Apart from wondering why it happened, you might worry whether your new body parts are proper replacements for the old. Is there something indefinable missing? This year, more than 1,000 human embryos will begin their lives with just such a loss \u2014 and exactly the same nagging doubts will surround them. During a procedure called preimplantation genetic diagnosis (PGD), one cell is teased from the eight or so that comprise a three-day-old embryo. It is analysed for genetic abnormalities as an adjunct to  in vitro  fertilization (IVF). The results are meant to help doctors decide which embryos are free of genetic defects and should be placed in the mother's uterus. PGD was first reported in 1990 for identifying and screening out male embryos that might carry a sex-linked disorder 1 . Since then, its popularity has risen on the coat-tails of IVF. An informal reporting system in Europe, which represents some two-thirds of all PGD activity in the continent, shows a jump from PGD tests in 131 IVF cycles in 1999 to 2,984 in 2003 (ref.  2 ). The goal of PGD is to foster life not interfere with it \u2014 but a number of researchers and clinicians find its soaring popularity unsettling. Many are concerned that a lot of PGD is being done for little benefit, because it may not boost the number of healthy babies. And some question whether removing a single cell \u2014 once assumed to be a harmless procedure \u2014 might subtly impair an embryo or the long-term health of the resulting children. \n               Health check \n             For prospective parents who could pass on a genetic condition such as cystic fibrosis, PGD has proved priceless. The only alternative to PGD would be to test several weeks into a pregnancy and, if positive, make the difficult choice of whether or not to abort the fetus. But PGD has another, more controversial use: screening embryos for abnormal chromosome counts, or aneuploidy. The number of a woman's eggs carrying such defects rises rapidly with advancing age, and embryos with these faulty chromosomes are thought to be more likely to miscarry or produce babies with birth defects. Using aneuploidy screening in IVF should reduce the miscarriage rate and enable doctors to transfer fewer and healthier embryos into the uterus, improving pregnancy rates and cutting multiple births. Roughly two-thirds of more than several thousand PGD procedures in the United States are for aneuploidy screening 3 , as are well over half in Europe 2 . There is good evidence that such screening can reduce the proportion of pregnancies lost in women who suffer from recurrent miscarriage \u2014 one study 4  showed a drop from 87% to 17% using the technique. But there is now intense debate about whether it helps increase the number of IVF attempts that go on to produce a 'take-home' baby. One of the strongest advocates for routine aneuploidy screening is Yury Verlinsky, director of the Reproductive Genetics Institute in Chicago. \u201cIn the near future I hope all IVF will be done with PGD,\u201d he says. Verlinsky points to one of his own studies, in which his team compared what happened to hundreds of IVF recipients before and after undergoing aneuploidy screening. He found that with PGD the miscarriage rate fell from 68% to 28%, and the number who ended up with babies increased 5 . But other experts are critical of this and similar studies because the women were not randomly assigned to screening. And as there is no ongoing system for collecting PGD outcomes across the United States, \u201call the reporting is by individual laboratories publishing their own results\u201d, says Susannah Baruch, director of reproductive genetics at the Genetics and Public Policy Center (GPPC) in Washington DC. \u201cThey're really picking and choosing who they're looking at and what they're writing about.\u201d \n               Babes in arms \n             In one Belgian study done with rigorous controls, women aged 37 or older were randomly assigned to receive aneuploidy screening during IVF. The women who had PGD gave birth to slightly fewer babies than the control group 6 , a similar finding to many other studies. \u201cThere's a lot of controversy in the field as to how PGD should be appropriately applied,\u201d says Catherine Racowsky, director of the IVF lab at Brigham and Women's Hospital, Boston, Massachusetts. \u201cIt doesn't seem to be very helpful in improving overall pregnancy rates.\u201d One reason is that older women produce only a limited number of eggs during IVF. Many of these generate the chromosomally faulty embryos that are identified and discarded after PGD, so there are few embryos left for use in each IVF cycle. The embryos that remain after PGD may be more likely to implant and produce a pregnancy, but this does not seem to compensate for the reduced number of embryos transferred. Another problem is that some perfectly good embryos may be getting thrown out. In the past few years, embryologists have realized that as many as half of all human embryos exhibit 'mosaicism', in which the embryo is a patchwork of chromosomally normal and abnormal cells. The cell selected for PGD may have a different chromosome tally from the rest. Recent studies have also suggested that these mosaic embryos tend to self-correct, or eliminate cells with aberrant chromosomes, notes Richard Scott of Reproductive Medicine Associates in West Orange, New Jersey. When Scott's lab took embryos that were abnormal on the third day of development and tested them again on day five, the researchers could no longer identify the abnormality in one-third of the embryos. So many embryos identified and eliminated using PGD would, quite possibly, develop as normal, another reason why PGD does not dramatically boost pregnancy rates. \u201cWe're probably overcalling the number of faulty embryos,\u201d says Robert Jansen, managing and medical director of Sydney IVF, which does close to half of all the PGD in Australia. \u201cThe risk, in an older woman, is that so many of her embryos are likely to screen as abnormal that all will be rejected, when one or more of them, if transferred, might have produced a healthy baby.\u201d Jansen and the directors of other fertility clinics have been cutting back on aneuploidy screening, and both the American Society for Reproductive Medicine and the European Society of Human Reproduction and Embryology (ESHRE) are debating the technique's validity. There is another concern clouding the use of PGD \u2014 the question of whether it is truly harmless. The traditional view among embryologists is that the first few cells of mammalian embryos are essentially equivalent: remove one or two, and the rest can ably fill the gap. But some recent studies have challenged that idea, and many now believe that even these early cells are predisposed to contribute in different ways to future tissues. Earlier this year, Magdalena Zernicka-Goetz of the University of Cambridge, UK, and her colleagues established how four genetically identical embryonic cells acquire their own character 7 . Working with mouse embryos, she showed that the DNA packaging proteins called histones, which help control gene expression, have different levels of specific chemical tags in each cell \u2014 and that changing these tags changes the type of embryonic tissues to which that cell contributes. Studies such as this raise questions about whether removing a cell during PGD leaves a molecular mark on the embryo that somehow affects its ability to develop, or affects the child's long-term health. \u201cI would be very, very surprised if implantation rates overall were not compromised by the removal of a cell from the embryo,\u201d Racowsky says, perhaps because the removal has a metabolic cost that compromises its development. \n               A question of balance \n             And there the matter sits. Thousands of PGD babies have already been born without any obvious problems. But the oldest of these babies are just teenagers \u2014 and rare or subtle effects of the procedure, if they exist, may not become apparent until researchers collect together many more cases and until these children reach middle age or beyond. Many PGD experts acknowledge the need for a large effort to collect data on the number and health of children born after PGD. In 2005, the GPPC convened a working group of reproductive endocrinologists, IVF doctors and PGD providers in the United States to discuss a voluntary data-collection system. \u201cWe're optimistic that it will be up and running within the next year or so,\u201d says Baruch. ESHRE already collects information on babies born after PGD and is in the process of setting up a study to follow these babies in more detail. Meanwhile, patients are left in a quandary about whether the small, unproven risks are worth taking. To avoid having a child with a severe genetic disease, it is probably well worth undergoing PGD; but for aneuploidy screening the decision is less clear. At the very least, experts say, practitioners should spell out the risks and benefits of the procedure to would-be parents (and many already do just that). Some researchers are seeking ways round the problem by trying to identify genetically abnormal embryos in less invasive ways, such as detecting secreted molecules or testing cells destined to become part of the placenta. \u201cMy view is that the less one perturbs an embryo, the better,\u201d says Racowsky. \u201cWe really do not know yet whether we're doing any harm in removing cells from early embryos.\u201d \n                     Safer embryo tests could boost IVF pregnancy rates \n                   \n                     Making the most of a little DNA \n                   \n                     UK to extend embryo testing \n                   \n                     Biologists forced to reassess embryo test \n                   \n                     Age is no barrier... \n                   \n                     Fertility supplement \n                   \n                     Fertility outlook \n                   \n                     ASRM \n                   \n                     ESHRE \n                   Reprints and Permissions"},
{"file_id": "448526a", "url": "https://www.nature.com/articles/448526a", "year": 2007, "authors": [{"name": "Ned Stafford"}], "parsed_as_year": "2006_or_before", "body": "Rising carbon dioxide levels should increase crop yields. But what if their effect on the nutritional value of our food is less benign, asks Ned Stafford. When you step into a commercial greenhouse, the chances are you are stepping into the future. To plants, carbon dioxide is food, and greenhouse operators, knowing this, use it to fatten them up. While today's atmosphere contains about 380 parts per million of carbon dioxide, commercial greenhouses often contain carbon dioxide concentrations of twice that or more \u2014 the sort of concentration that we might expect in the open air at the end of the century. The fact that plants thrive in environments with high levels of CO 2  \u2014 all other things being equal \u2014 seems to offer a silver lining to the otherwise dark clouds of climate change. Many crop scientists believe that this carbon dioxide fertilization effect will go at least some way towards offsetting the losses in yield to be expected as a result of the higher temperatures, flooding, drought and rising sea levels that the CO 2  greenhouse effect will bring. The fact that many horticulturalists already choose higher CO 2  environments for their work underlines this assumption. But some are not so sure. They are sounding alarm bells about potential negative impacts; bigger yields, they say, are not always better. Their worry is that the nutritional value of crops could suffer regardless of overall abundance. These researchers point to the known negative effects that increased carbon dioxide concentrations have on the protein content of crops. They also worry about subtler effects that might be felt in everything from the micronutrient properties of soya beans to the ability of wheat to be baked into bread. They do not all share the same level of concern, but they do all agree on one thing. Compared to the amount that is being spent on climate research, the amount being spent on understanding the agricultural effects of higher CO 2  levels is woefully inadequate. Steven Adams, head of strategic and applied research on the physiology of protected crops at the University of Warwick's Horticulture Research International in the UK, reflects the relatively relaxed attitude of most crop scientists. He acknowledges that hardly any research has been done on the effect of high\u2013CO 2  in greenhouses on food quality and concedes that a drop in nutritional value is possible. \u201cIf it affects photosynthesis and yields, it could do,\u201d he says. \u201cBut I would have thought the impact is relatively small.\u201d A more complex view comes from Bruce Kimball, a soil physicist with the United States Department of Agricultural in Maricopa, Arizona \u2014 a pioneer in high-CO 2  plant research. When he started out in the 1970s, available technology for high-CO 2  research was limited mainly to greenhouses and open-top chambers, both of which offer questionable results. Plants grown in such systems are not exposed to environmental variables such as wind and normal variations in temperature and humidity. In the mid 1980s, Kimball was a driving force in development of a system dubbed FACE (free air carbon dioxide enrichment), which simulates natural field conditions. A FACE system includes a ring of equipment up to 30 meters in diameter encircling the plants that are the subject of the research. The vertical pipes in this ring emit CO 2 ; sensors measure wind speed, wind direction and carbon dioxide levels. A computer uses this data to keep the CO 2  within the ring at the target level by releasing it from specific upwind pipes as required. It's not a perfect simulacrum of a high-CO 2  world\u2014for one thing, the gas is pumped in only while there's daylight for the plants to photosynthesize in. But it's the best so far. After his decades of work Kimball agrees that higher CO 2  concentrations can lead to lower nutritional quality of crops. But he sees nothing in his FACE studies \u2014 or those of others \u2014 to cause alarm. Based on current knowledge, he says, the net effect of increasing CO 2  is a good one: \u201cAs far as crops go, I think higher CO 2  is a definite benefit. Yes, a little less nutrition than before, but we get more food.\u201d The plants almost always deliver higher yields than controls, with more sugar and starch in their leaves. They also take up less nitrogen from the soil, because they are making less protein. A lot of the protein in leaves is involved in assimilating CO 2  into sugars. At higher CO 2  levels that becomes easier; less protein is needed, and so less protein is made. The major exception is in the 'C4 plants', which are better at photosynthesis in less favourable conditions and so are less susceptible to the effects of changing CO 2  levels. C4 crops include maize, sorghum and sugar cane. But while Kimball thinks that, in general, the gains in yield are the most important thing, he is not blind to the drop in protein levels. Talking of data from a cotton-leaf experiment, he finds himself struck by the size of the effect: \u201cThat is a big change,\u201d he says, wondering aloud what that might mean for lettuce, other plant leaves and grasslands. \u201cThink of all the livestock that only eat leaves.\u201d Grass-grazing livestock in the 550 parts per million CO 2  world that is likely, though not inevitable, by 2050 might be getting significantly less protein from their forage. And it's not only livestock that eat plants \u2014 there's the rest of the ecosystem too. Kimball conducted side experiments in the 1990s when growing crops in his FACE systems. He describes the effects of a diet of high\u2013CO 2  cotton leaves on beet armyworms. \u201cWhat we found is that their growth and reproductive capability was reduced,\u201d he says. But Kimball believes that the protein levels can be lifted by increasing the amount of nitrogen supplied to the plants. \u201cThe farmer in the future would have to be sure and apply ample fertilizers to keep protein quality up,\u201d he says. And he thinks crop scientists and plant breeders will be able to rectify most other potential problems arising from high-CO 2  levels. But he believes more money is needed to fund high\u2013CO 2  food research. Operating FACE systems is expensive \u2014 according to Keith Lewin of Brookhaven National Laboratory, US, a forest-based FACE system costs about $1.5 million a year to run even before you include the costs of research \u2014 and Kimball's last major field experiments were in 1999. \u201cAfter that, funding dried up,\u201d he says. \u201cI think we need to do more experiments at much higher CO 2  concentrations,\u201d he goes on, noting that nearly all CO 2  plant research up to the present has been done at 550 parts per million, not at any of the higher levels that are conceivable in the second half of the century. But while Kimball is confident that increasing nitrogen levels could help retain higher protein levels in plants, others are not so sure. Arnold Bloom, a plant biologist at the University of California, Davis, believes the reduction in nitrogen content seen in plants in a high CO 2  environment is not just due to a lowered need for protein in leaves. He thinks it is because of a decreased ability to take-up nitrates from the soil. If this is the case, then adding nitrogen fertilizer would be less effective in future, which could have implications both for protein content and for overall yields (see  'Diminishing returns' ). The question of whether adding nitrogen can offset the effects of increased carbon dioxide remains open, according to Hans-Joachim Weigel, director of the Institute of Agroecology, part of the Federal Agricultural Research Centre in Braunschweig, Germany. For some crops you have to add \u201cenormous amounts of nitrogen\u201d, amounts that would be unfeasible in terms of cost and unacceptable in terms of environmental damage. Weigel's FACE research on barley, wheat and sugar beet in the past six years leaves no doubt in his mind that higher CO 2  levels in coming decades will have a significant impact on crop quality. \u201cWe should be concerned about it, but not in a panic about it,\u201d he says. Herbert Wieser, head of cereal proteins at the German Research Centre for Food Chemistry in Garching, has provided some of the grounds for that concern by milling Weigel's winter wheat into white bread flour and analyzing the flour for protein content. Wieser found that high-CO 2  wheat, grown with a normal amount of nitrogen fertilizer, produced white bread flour with 7.8 grams of total protein per 100 grams of flour, 14% less than the 9.1 grams of protein in flour from wheat grown at normal CO 2  levels. When the wheat was grown with half the usual fertilizer, the protein level was 6.7 grams in the control, and 6.1 grams in the high-CO 2  group. The total protein content is not the only thing that changes \u2014 so does the type of protein. Gluten proteins are used as a nitrogen store in wheat, and in high-CO 2  conditions this store was lowered more than the overall protein level, dropping by 18%. The high-molecular-weight subunits which are particularly important for dough and bread quality fell by 23%. Wieser concludes that high-CO 2  growing conditions cause \u201ca serious impairment of wheat baking quality\u201d. This goes some way towards vindicating Andreas Fangmeier, professor of plant ecology and ecotoxicology at the University of Hohenheim in Germany, who was quoted in a university-issued press release earlier this year warning that by 2050, CO 2  concentrations could make French fries poisonous, beer foamless and wheat flour unbakeable. Unfortunately, Fangmeier did not actually bake bread, brew beer or fry chips from crops harvested from his high-CO 2  test fields. Chemical analysis of his crops merely indicates \u201cthe potential\u201d for the problems foreseen \u2014 the press release, he admits, was \u201can exaggeration\u201d. Despite his lapse into hype, though, Fangmeier refuses to back down from the underlying sentiment that higher atmospheric CO 2  levels will damage crop quality in a number of ways. For example, he notes that in one of his high-CO 2  field tests, Vitamin C in potato tubers dropped by 50\u201360%. He can't explain the drop, but says it is just one of many indications that high-CO 2  levels will have a much larger impact on food quality than many currently believe. Weigel says the database for speculating about such things is currently too small. Another researcher convinced that protein levels are not the whole story is Irakli Loladze, now a mathematician at the University of Nebraska in Lincoln. In 2002, while doing postdoctoral work in mathematical biology at Princeton University in New Jersey, Loladze published an opinion article (I. Loladze  Trends Ecol. Evol.   17,  457\u2013461; 2002) highlighting the risks of changed plant composition in a high-CO 2  world and focusing on micronutrients such as iron, iodine, copper and manganese. Scouring the \u201csurprisingly scant\u201d literature he found that, on average, the concentrations of all the micronutrients he looked at decreased. The plants put more effort into storing up carbohydrates, and the resultant \u201ccarbohydrate dilution\u201d reduced the proportion of metabolically important trace elements such as chromium, selenium and zinc. Loladze believes that with current atmospheric CO 2  levels a third higher than pre-industrial levels, plants have already changed in this way, and that they will inevitably change more. He's concerned that this could put hundreds of millions at risk of the \u201chidden hunger\u201d of micronutrient malnutrition. However, he has been unable to attract significant funding for further research into the area. Peter Curtis, professor of ecology at Ohio State University in Columbus, Ohio, says Loladze's 2002 paper identified \u201ca clear gap in our understanding of plant responses to elevated CO 2 : how this global change in plant nutrition will affect plant tissue micronutrient status\u201d. Even subtle changes in micronutrient status could affect both human health and the wider ecosystem. Some research has bolstered Loladze's argument, at least in part. Stephen Long, a crop scientist at the University of Illinois at Urbana-Champaign, says that soya beans grown in his FACE fields have shown drops in calcium and zinc levels of 10\u201320%, sugars and starches have been up 50%. The drop in calcium might be particularly noteworthy, as soya beans are used to make substitute dairy products. His team also found effects passing up the food chain. For example, western corn rootworms feeding on high-CO 2  soya bean leaves live longer and produce twice as many young as those feeding on normal soya bean leaves. \u201cThey lay eggs in the soya bean fields in late summer so that they are ready to infect the corn crop that will be planted in the next year,\u201d Long says, referring to the standard practice in the region of rotating corn and soya beans annually. They have also measured changes in the microbial community within the FACE rings. \u201cWe don't know whether this is good or bad,\u201d he admits. \u201cBut if we see this change in just five years, what will happen in 50 years?\u201d \n                     Hikes in surface ozone could suffocate crops \n                   \n                     Climate change could boost cash crops \n                   \n                     Climate change In Focus \n                   \n                     German Research Center for Food Chemistry \n                   \n                     What is FACE? \n                   \n                     Bruce Kimball \n                   \n                     Soybean Free Air Concentration Enrichment facility \n                   Reprints and Permissions"},
{"file_id": "445700a", "url": "https://www.nature.com/articles/445700a", "year": 2007, "authors": [{"name": "Jennifer Ouellette"}], "parsed_as_year": "2006_or_before", "body": "One half of a physics couple that met online, Jennifer Ouellette seeks some advice from married scientists on how to handle both long-distance and up-close relationships, while juggling career and family. Can love survive? Last October, I became engaged to Caltech cosmologist Sean Carroll, capping six months of a long-distance romance that began via our respective physics blogs. Our his-and-hers blog announcements garnered the proverbial 15 minutes in the online scientific community, and it didn't take long before someone asked: \u201cSo, will you be relocating to California?\u201d Of course I will move to Los Angeles from Washington DC. Like any romantic, I would move mountains to be with my beloved; a cross-country trek, yowling cat in tow, is trivial in comparison. Sean is well worth that and more. But then I'm a self-employed science writer. You can give me a mobile phone, a laptop and a high-speed Internet connection, and I can do my job from almost anywhere. Alas, scientists who marry scientists can't always get it together quite so easily. There is a daunting obstacle to be overcome: they must find jobs not just for themselves, but for their spouses. This is the 'two-body' problem: a reference to the challenge of calculating the paths of two objects interacting with each other. Mathematics solved the two-body problem long ago, but married scientists still struggle with it. What little hard data are available show that they are in good company. According to several surveys of European scientists at least half of all scientists questioned have partners who are also working in science (H. L. Ackers  Gender, Mobility and Career Progression in the European Union: Final Project Report ; European Commission, Brussels, 2005). The problem is most acute in the natural sciences, says Londa Schiebinger, an expert on gender in science at Stanford University, who is heading up a US-wide survey of dual-career academic couples, building on a pilot programme at Stanford. Such a study is badly needed as there are very few hard US statistics on the matter \u2014 and those figures that do exist tend to be out of date. Schiebinger's group will survey more than 30,000 faculty members from the top US research universities, and conduct follow-up interviews and focus-group discussions. One question the team hopes to answer is are such marriages tougher for female scientists? Besides being a minority in their field, female physicists struggle with the two-body problem more often than their male counterparts. A 1998 survey by the American Physical Society found that although only about 6% of its members are women, 43% of these are married to other physicists. In contrast, only 6% of married male physicists have a physicist spouse. Other studies have found that almost twice as many women chemists are married to or partnered with another chemist as compared to their male colleagues, and 80% of women mathematicians are married to other scientists. For most female scientists, the benefits of an intellectual connection with their partner probably outweigh any hardship. I certainly appreciate finding someone with whom I can discuss ideas, who continually challenges my assumptions and helps me view things from a different perspective; how much more true this must be for couples pursuing similar scientific careers. According to Schiebinger, nirvana for married scientists in academia is two faculty (tenured or tenure-track) positions at the same institution or in the same area. Less desirable options include shared positions at the same institution, or one partner getting a tenure-track position while the other makes do with a lower-level lectureship or part-time position. \n               The course of true love \n             Back in 1976, physicist Ruth Howes didn't have that many options when she followed her husband Bob, a professor of dentistry, to Oklahoma. She took a temporary position, against the advice of her thesis adviser, and soon found herself unemployed. She worked part-time and focused on raising their children, but grew frustrated. \u201cNobody would hire me in Oklahoma,\u201d she says. The nadir came when a small private college refused to hire her because she insisted on teaching stellar evolution in her astronomy courses. \u201cThey didn't want any form of evolution taught,\u201d she says. So when Ball State University in Muncie, Indiana, offered her a full-time position, she accepted, even though it meant living in different states while her children were quite young. \u201cIn those days, if you told people you were going to have a commuter marriage, they assumed you were getting a divorce,\u201d she says. \u201cSo we were a little ahead of the curve.\u201d But the Howeses made it work for 25 years by following two rules: \u201cTalk every day, no matter what, and have a home for both partners on both ends. Both places should be home,\u201d says Howes, who now chairs the physics department of Marquette University in Milwaukee, Wisconsin. Combining a family with a commuter marriage adds yet more complications, Howes acknowledges. As in physics, the many-body problem becomes much harder to solve. For a year, Howes and her husband tried splitting their two children between the two households. From then on, the children lived with Howes while her husband did the commuting. She discovered that her children were very resilient. \u201cThey took it in their stride,\u201d says Howes, and became adept at packing. Although she worried about the potential psychological damage to her offspring, they didn't think it was so bad: \u201cEvery other weekend, we would basically freeze time. It was family time, and very special.\u201d A few years ago, Bob Howes retired and joined his wife in Muncie. \u201cThat's something hardly anyone talks about: putting it all back together again,\u201d she says, admitting that initially there was conflict as the couple readjusted after so long apart. \u201cThe two-body problem is rough no matter how you look at it.\u201d They ended up buying a bigger house. Not only did it give each of them more space, it was \u201cneutral ground\u201d. They also bought a second home in Santa Fe, New Mexico; remodelling that home in anticipation of Ruth Howes' retirement is a shared project. \n               Long-distance romance \n             Thirty years on, many scientific couples still opt for commuter marriages, at least at the beginning of their careers, rather than sacrifice one partner's dreams to the other's. A physicist friend of mine, Diandra Leslie-Pelecky, says: \u201cIf you both want to be high-powered researchers, you are limited in your choice of jobs, because there may not be many places with strong programmes in both areas.\u201d Now at the University of Nebraska in Lincoln, Leslie-Pelecky spent the first nine years of her marriage to fellow physicist Robert Hilborn commuting between Nebraska and the University of Massachusetts in Amherst. (Hilborn got a job at the University of Nebraska late last year.) Do they have any advice for newlyweds, or soon-to-be-married scientific couples? \u201cBoth partners should win Nobel prizes,\u201d jokes Hilborn, thus giving them their pick of academic appointments. For those of us whose last name isn't Curie, one or both partners must inevitably make concessions, and it might take longer than they would like to achieve their professional goals. Unlike Howes, Leslie-Pelecky opted not to have a family, a decision she is happy with. But she cautions that although both partners should be willing to make sacrifices, \u201cif you compromise too much, you can limit your choices for future positions. The last thing you want is to have one partner feel that he or she got the raw end of the deal.\u201d Chemist Julia Fulghum, of the University of New Mexico, Albuquerque, agrees. \u201cWe have tried hard to find positions that are a compromise for both of us, rather than ideal for one person and a bad fit for the other,\u201d she says. After spending a year apart from her husband and fellow chemist Stephen Cabaniss while he was a postdoc, they decided they didn't want a commuter marriage, especially as they knew they wanted children. Initially, their new positions were more limiting than they might have liked, but over time, each established successful research and teaching programmes. They now both have tenure. In fact, they've pulled this trick off twice. They found dual tenure-track positions at Kent State University in Ohio, before moving to their current jobs. But they had to make some trade-offs along the way. Both applied to a swathe of different academic departments, and they didn't put any geographical restrictions on their dual job search. That proved to be a key factor in their success, even though finding that first position took two years. In the interim, they both turned down attractive jobs at other schools, rather than live apart. \u201cEvery couple has to figure out the issues that are most important to them,\u201d says Fulghum. \u201cYou have to be honest with each other about what is and isn't acceptable.\u201d So perhaps the Valentine cards have it right, love is communication. \u201cWe know couples that have made every possible combination work, and others who are miserable,\u201d says Fulgham. The flip side of the commuter marriage is the danger of too much togetherness, particularly for scientists who marry their bench partner. What if your careers mean you end up sharing office space or writing papers together? \n               The things we do for love \n             Edmund and Laura Gerstein have tested their togetherness to the extreme. They are married scientific collaborators at Florida Atlantic University in Boca Raton, specializing in animal acoustics. Not only do they work at the same institution, they once spent three-and-a-half years living in a small trailer behind a zoo \u2014 the better to study the acoustic behaviour of manatees. The couple endured extreme close quarters, no private bathroom, bizarre hours, and the occasional rampaging elephant knocking into their trailer. \u201cIt got to the point where we didn't really have to talk, we could just kind of grunt at each other to communicate,\u201d says Edmund. They sometimes joke that it's a miracle they are still together, but during the experience they figured out an efficient division of labour that Edmund says helped their research and relieved the inevitable tensions produced by constant togetherness. For instance, Laura handled the computational aspects of the project while Edmund worked with the animals, putting some much-needed distance between them, at least during work time. Does having similar research areas help or hurt a job hunt? Fulghum believes too-similar fields can be more of a disadvantage for younger, less established scientists. \u201cI've frequently observed a sometimes conscious, sometimes subconscious, tendency for faculty evaluating two junior people to assume that only one of them can be 'good,' or that they have to figure out which one is 'best,'\u201d she says, adding that this is less of a problem at the senior level because you are judged more on accomplishments. For Fulghum and Cabaniss, the overlap proved advantageous, because Kent State was interviewing for two positions, and their research areas were sufficiently different: she works on materials characterization, and he specializes in environmental geochemistry. The University of New Mexico's policy encourages hiring spouses if one member of the couple is being actively recruited, and the relevant departmental heads worked to bring about the dual positions. Yet the couple also interviewed at less progressive universities that \u201cmade it very clear they were not interested in having a couple in the department\u201d, says Fulghum. One rarely discussed aspect of the two-body problem is divorce rates. Certainly there have been scientific couples, some quite prominent, for whom the challenges proved too great. But the frequency of such breakdowns is unknown. If all this anecdotal evidence proves anything, it's the need for the comprehensive Stanford survey currently under way. A report on the findings will be released later this year. The hope is that it will provide data to back up personal experiences, so that universities can formulate the best policies for their married faculty members. In the meantime, I've gleaned some useful titbits of advice and encouragement (see  'Tips for newlyweds' ) for my own foray into marriage. No doubt there will be a few bumps in the road ahead as Sean and I adjust to life together, but we're ready to take the next step. Fortunately, we're both good communicators, as our blogging activities and six-month bicoastal love affair show. And we won't have the two-body problem. That gives me confidence in our shared future; the rest \u2014 well, it's mostly logistics. \n                 Share your valentine stories on  \n                 Nature \n                 's newsblog at  \n                 \n                     http://blogs.nature.com \n                   \n               \n                     The familial balancing act \n                   \n                     Cosmic Variance blog (where Sean Carroll blogs) \n                   \n                     Cocktail Party Physics (Jennifer's blog) \n                   \n                     Stanford study website \n                   Reprints and Permissions"},
{"file_id": "445362a", "url": "https://www.nature.com/articles/445362a", "year": 2007, "authors": [{"name": "Philip Ball"}], "parsed_as_year": "2006_or_before", "body": "By 2020 the semiconductor industry wants a memory device that can store a trillion bits of information in an area the size of a postage stamp. As companies race towards this goal, chemists are coming up with an unusual approach. Philip Ball reports. It's a new year. But in the labs of Jim Heath and Fraser Stoddart in California, that year is not 2007, it's 2020. They have leapt into the future with a memory device that potentially matches the needs of the semiconductor industry 13 years from now. The array is no bigger than a single white blood cell, yet it contains 160,000 memory elements, each with an area of just 30 nanometres square \u2014 some 40 times smaller than those in existing devices. It's not the first time someone has made a prototype ultrasmall memory that is years ahead of its time. But what distinguishes the device made by Heath, Stoddart and their colleagues is that it stores the zeroes and ones of binary information in the switchable states of organic molecules. The system, described in detail on  page 414  (ref.  1 ), brings the idea of molecular memories a step closer to reality. Although it may have a very high 'bit density' (the number of memory elements per square centimetre), this super-memory isn't going to appear in a laptop any time soon. The researchers are frank about its current shortcomings, not least of which is that the memory cells stop working after being switched just ten times. \u201cIt wouldn't surprise me if we really did have to wait until 2020 to see molecular devices with this bit density actually being used,\u201d admits Heath, who is based at the California Institute of Technology in Pasadena. Computer memories have been reaching higher bit densities for decades \u2014 but it's becoming ever tougher to keep up with the industry's long-term trends. Today's lithographic techniques for carving silicon into circuit patterns are unlikely to deliver the 2020 target of memory cells just 30 nm or so apart. That's one reason why researchers have begun to think seriously about building memory devices from the bottom up using individual molecules. Heath and Stoddart's work is a proof-of-concept, showing that molecular memory cells can be made with a bit density of 10 11  per cm 2 . And it's worth taking seriously, because in terms of miniaturization, the computer companies know that this density is what they want to achieve by 2020 \u2014 although they're still unsure of the best way to do it. There are several possible alternatives on the menu, such as storing data in magnetic or ferroelectric cells or by reversibly altering the atomic structure of thin solid films. Some of these approaches are already well advanced, and the route Heath and Stoddart are offering \u2014 a curious hybrid of silicon-based microfabrication and organic chemistry \u2014 is widely seen as an outside contender. But Heath says that their silicon circuit has in itself been enough to catch the eye of the semiconductor industry, even though the molecular switches aren't robust, or fast. Stoddart, an organic chemist at the University of California, Los Angeles, who has been exploring switchable molecules since the 1980s, confirms that the interest from industry is real. The work has emerged from the collaborative California NanoSystems Institute, where Stoddart is the director and Heath was the founding director. The institute received four years of initial funding from many IT companies, including $7.8 million from Hewlett-Packard, says Stoddart, and now \u201cIntel has bought into it, to the tune of some $30 million\u201d. But a better memory isn't just about higher bit density. \u201cChemists tend to focus on one aspect of the problem: size,\u201d says Phaedon Avouris of IBM's T. J. Watson Research Center in Yorktown Heights, New York. Although he is also working on molecular devices, made from carbon nanotubes, Avouris stresses that in the end companies such as IBM want something they can make dependably and economically. \u201cThe bottom line for devices is always reliability, performance and ease of fabrication. One does not usually hear any justification for molecular electronics on those grounds. It is always just size.\u201d \n               Easy to forget \n             Today's dynamic random-access memories (DRAMs), the working memory of most electronic devices, leak charge and must be refreshed thousands of times a second, making them power-hungry and draining batteries. What's more, DRAM loses all information when the power is switched off. So power failures can cause loss of data, and a computer's operating system has to be copied afresh from the hard drive during start-up, resulting in long boot-up times. Ideally a RAM would be non-volatile, meaning that the data don't evaporate the moment power is cut. Flash memory, used in mobile phones and digital cameras, is a form of non-volatile RAM, but it has drawbacks: writing data is very slow, its switching lifetime is limited to around 100,000 cycles, and the data do leak away eventually. That means flash memory isn't a viable option for computers. The molecular memory made by Heath and Stoddart isn't truly non-volatile yet \u2014 the molecules begin to switch states spontaneously after an hour or so. That may be improved by tinkering with the molecular structure to make the two states more stable. But if they hope to replace DRAM or flash they have some way to go to match other non-volatile memories currently being developed. Magnetic RAM (MRAM) holds data magnetically, so no power is needed to sustain it. The electrical resistance of MRAM cells changes when their magnetic orientation is switched. Last July, the company Freescale Semiconductor in Austin, Texas, released the first commercial MRAM non-volatile memory chip, with a capacity of 4 million bits and a switching time of 35 nanoseconds. The most mature approach is ferroelectric RAM (FeRAM), where the switching involves altering the polarization state of a ferroelectric material 2 . Commercialization of this technology is fairly advanced: Samsung markets a 64-million-bit FeRAM for low-tech applications such as smart cards. There are already prototype memory arrays with cell spacings of 45 nm \u2014 the semiconductor industry's 2010 target for DRAM. Moreover, FeRAMs switch very quickly; commercial devices take just a few nanoseconds. The barriers facing wider adoption of FeRAM are now more economic than technical, says Jim Scott, a specialist in ferroelectrics at the University of Cambridge, UK. \n               The next phase \n             Another option for non-volatile RAM is to record data in a 'phase-change material', whose atomic structure can be reversibly altered within a small volume. For example, heating caused by an electric current can partially melt the material, switching it between crystalline and amorphous phases with different electrical conductivity. Commercial 'PRAMs' are already being built by companies such as Samsung and Intel but the bit densities are not much higher than those of today's flash memory. At the end of last year, IBM researchers unveiled a PRAM device 3  based on a single cell measuring just 3 by 20 nanometres and switching in 2\u201320 nanoseconds. The team is now working on making large arrays of these elements, something that Spike Narayan, at IBM's Almaden Research Center in San Jose, calls \u201cvery feasible\u201d. He adds, however, that devices this small can't yet be made with the patterning technologies used for commercial chips. So Heath and his colleagues face some stiff competition. What do they have to offer? Their array is made up of two sets of tiny wires \u2014 one of silicon, the other of titanium \u2014 arranged in parallel. Each of these wires is just a few tens of nanometres wide \u2014 smaller than the circuit components on today's silicon chips, which are more than 100 nm across. The researchers use a method they devised in 2003 that relies on etching ultrathin layered films rather than lithography to produce the sets of wires. To create the memory array, a set of titanium wires is placed at right angles on top of the silicon wires to form a grid with multiple junctions. It's at these junctions that the switchable molecules, known as rotaxanes, are anchored. The rotaxanes consist of a linear chain-like molecule threaded through a molecular hoop. The hoop 'docks' at either of two sites along the rotaxane chain, and bulky groups of atoms act as 'stoppers' at the ends. One of these stopper groups is designed to anchor the molecules to silicon, so that they will readily attach themselves to the nanowires. A few hundred rotaxanes at the junction of two wires can be switched by applying voltages to the wires, changing the electrical conductivity of the junction as the molecules become oxidized or reduced and the hoop jumps between the docking sites. \n               Wired up \n             Heath, Stoddart and their colleagues first demonstrated that these memory cells worked in 2002, using an 8 \u00d7 8 array. Scaling this up to 400 \u00d7 400 nanowires was no easy matter, but their latest memory array has 160,000 junctions, each housing about 100 rotaxane molecules. Wiring up the whole array is challenging, so they have tested a subset of 128 junctions. They found that only half were switchable, and only about half of those gave a sufficiently reliable signal for read-out. In other words, just one in four of the memory elements actually works. That's not great, but it's not a fatal flaw. Heath and his collaborators have shown that robust memories can be made from defective arrays by using software that finds the 'good' bits and routes around the 'bad' ones 4 . \u201cThat isn't so different from magnetic hard-disk memory,\u201d says Heath, in which bad sectors are identified so that those bits aren't used. Stoddart suggests the problem is not that the molecules are failing to switch, but that there are limitations to the nanofabrication \u2014 especially etching \u2014 which he anticipates will improve. Harry Atwater of the California Institute of Technology, who is working on new types of flash memory, says that \u201calthough the rotaxane switch is a triumph for chemical synthetic technique, as a memory element it is orders of magnitude slower than a DRAM cell\u201d. Perhaps even more troubling is that ten switching cycles is enough to damage the molecules irreparably. The researchers haven't yet tried to improve the lifetimes \u2014 Heath says that automating the fabrication process is their first priority. But he argues that merely making silicon wiring at this density is impressive, and may be useful for miniaturizing standard silicon circuitry beyond memory devices. For now, high-density PRAM or FeRAM arrays look more likely to supplant DRAMs. Narayan calls the new molecular memory a very fine piece of work, but adds that its application is likely to be different from other arrays. \u201cMolecular memory and PRAM will ultimately cater to very different markets, and are unlikely to compete directly with each other,\u201d he says. Rather than being a replacement for conventional DRAM, rotaxane-based arrays might find applications as cheap, disposable memories \u2014 which could still be a huge market. For his part, Heath is confident that applications will be found for a device that \u201conly a few years ago people were calling flat-out ridiculous dreams of science fiction\u201d. \n                     Atoms never forget \n                   \n                     Cylinders make circuits spontaneously \n                   \n                     Striped nanowires shrink electronics \n                   \n                     Molecular maths \n                   \n                     Chemistry meets computing \n                   \n                     Heath lab \n                   \n                     Stoddart lab \n                   \n                     California NanoSystems Institute \n                   \n                     Freescale MRAM \n                   \n                     www.eetimes.com \n                   \n                     IBM PRAM prototype \n                   Reprints and Permissions"},
{"file_id": "445812a", "url": "https://www.nature.com/articles/445812a", "year": 2007, "authors": [{"name": "David Cyranoski"}], "parsed_as_year": "2006_or_before", "body": "How did a mud volcano come to destroy an Indonesian town? David Cyranoski reports from Sidoarjo. It started on 29 May 2006: a small spurt of mud in the middle of a rice paddy. Now the cauldron of hot, bubbling mud is some 50 metres in diameter and rises 16 metres above that long, submerged paddy. A dented horseshoe of a levee \u2014 the tallest of a series of often-failed efforts to protect homes and factories \u2014 tries to channel the muddy outflow towards a river. On the disaster-management maps that show those 13 kilometres of levee and 450 hectares of muddy devastation, the source at the centre is labelled simply 'Big Hole'. The Big Hole, more commonly called Lusi \u2014 a contraction of  lumpur  for mud and Sidoarjo, the place in East Java where the mud is erupting, is the opening of what's called a mud volcano. It's not, perhaps, the most apt of names: the mud is more like thick water, and the volcano's only structure is that imposed on it by the artificial embankments. But if it gets across the idea of something bizarre and disastrous, then the name is doing its job. In the Big Hole, below the thick, billowing plumes of vapour, black waves ripple the unfathomable surface. \u201cIt's like the sea,\u201d says my guide \u2014 but a sea that is pockmarked with bubbles of gas from a source about which little is known, even after nine months. A smooth black lake stretches for more than a kilometre in most directions, past treetops, past street lights, and over barely exposed roofs. The flow, which started at just 5,000 cubic metres a day, has now topped 130,000, and Lusi has already displaced 24,000 people. To the southwest, within an area closed off by levees, the mud has crusted over into chunky, grey blocks that come up to the eaves of a series of factories. In an earthwork-protected area that allows access to the Big Hole, bustling workers dismantle a drilling rig that tried and failed to intercept the mud surge on its way to the surface. On 22 November, the weight of the mud destroyed a gas pipeline, killing 13 disaster workers; two other workers have since died in accidents with heavy equipment. And no one knows when the mud flood will stop: Richard Davies, a geologist at Durham University in the United Kingdom with an interest in mud volcanoes, points to some that have been spewing forth for months or even, as in the case of the Koturdag mud volcano in Azerbaijan, decades. \u201cI expect Lusi to be bubbling for years to come,\u201d he says. Lusi's devastation has so far been a defeat not only for civil engineering but also for scientific understanding. Efforts to figure out how it started have turned into a hotly contested whodunit with two suspects: a drilling project by an Indonesian oil-drilling company named PT Lapindo Brantas, and a nearby earthquake. Now Earth scientists are starting to weigh in. Geologist Adriano Mazzini at the University of Oslo in Norway sees it as a golden opportunity to learn about the evolution of these strange phenomena, which are generally studied only long after the structure has set. \u201cWe can see this one from day one,\u201d says Mazzini. But such studies are hindered by the fact that Lapindo has been keeping much of the drilling data under wraps. Meanwhile, the national team charged with sorting things out has focused its efforts on penning in the mud. It has had little time or effort to spare for obtaining the data on temperature, viscosity and flow rate that scientists might be interested in. \u201cWe are trying to save homes. This is not a science experiment,\u201d says Basuki Hadimuljono, the head of the team. But with no signs that the mud will let up, and the possibility that a greater understanding of what happened could improve the countermeasures, the need to learn more is growing urgent. \n               What's in a name? \n             An estimated couple of thousand mud volcanoes have been seen around the world, including several in East Java. Mostly they are a naturally occurring process in which sediment that has been buried deep in the Earth is liquefied and squirted back to the surface, along with water and gas. But little is known about the high-pressure systems that drive such outbursts and the circumstances that start them off. Some \u2014 like the Koturdag in Azerbaijan \u2014 live up to the promise of their name, with thick mud flowing from a raised, central crater. And many are genuinely, viscously muddy. \u201cSome of them come out a couple of centimetres per day, like toothpaste,\u201d says Davies. Lusi is exceptional in its sheer volume. It is also an outlier at the dilute, watery end of the volcanoes' viscosity spectrum. Its mud is about 70% water, according to Bambang Istadi, the exploration manager at PT Energi Mega Persada (EMP), Lapindo's parent company. After examining fossils within the mud, Istadi says that the particles in the mud come from a layer of shale and mudstone at a depth of somewhere between 1,220 and 1,830 metres. Davies says this mudstone continues down, mixed with sandbeds, to nearly three kilometres. Below that is the Kujung limestone formation, into which Lapindo was planning to drill in search of gas. The first published analysis of the mud volcano, from a group led by Davies (R. J. Davies  et al .  GSA Today   17,  4\u20139; 2007), conjectures that the water driving the mud volcano comes from that Kujung limestone, and suggests that the escape could have been caused by Lapindo's drilling at a site called Banjar Panji-1. The Banjar Panji-1 well is an exploratory well, started when little was known about the underlying geology. In his paper, Davies argues that the drill at Banjar Panji-1 punctured the Kujung, allowing high-pressure water and gas to escape into the borehole. The fluids forced their way into the surrounding rock and fractured it and the high-pressure water passing through these fractures liquefied the surrounding shale before new cracks gave it access to the surface. The cracks have been growing ever since. As evidence, Davies cites abnormally high pressure readings for the Kujung formation taken some five kilometres away from Banjar Panji-1 and for a shallower layer right at the drilling site. Davies notes that he cannot say how he acquired these data, or present more data to back up his analysis because his source must be kept confidential. Istadi, who oversaw the drilling of the well, describes Davies' paper as \u201cpreliminary conclusions, [with] interpretation based on an incomplete data set\u201d, and says that Davies did not contact his company to confirm the data. Istadi's main gripe is that drilling records show that the borehole never penetrated the Kujung formation. When the team hit an instability at 2,834 metres \u2014 around the depth that they had expected to find the Kujung \u2014 they too assumed that they had reached the limestone. \u201cAt first we thought so, but it can't possibly be,\u201d says Istadi. As evidence, he offers drilling cores that show no evidence of the 12-metre-thick layer of hardened clay that has been found draped over the Kujung elsewhere, and says that chloride levels in the water are twice as high as would be expected from the Kujung. Davies has since been contacted by Lapindo representatives who corrected a few details in his analysis, but says that nothing they told him changed his overall interpretation. He does, however, agree that the available evidence does not necessarily show that the drill entered the limestone. \n               Shaken, not stirred \n             Istadi offers another explanation for the volcano's origin. The day before the volcano erupted, a magnitude-6.3 earthquake struck Yogyakarta, 280 kilometres to the southwest of Sidoarjo. Istadi says that seven hours after the earthquake, drilling fluid, which is circulated up and down the borehole to keep the pressure higher than that of the fluids in the surrounding rock, leaked out. He thinks this drilling loss, or 'loss of circulation', as it is known, was caused by shock waves from the earthquake \u2014 and that the same shocks might have triggered the mud volcano: \u201cFaults became open, lost their sealing capacity, and became permeable.\u201d Those faults, he says, \u201cserved as the conduit where the mud flows out\u201d. This explanation squares well with the views of one of the country's most powerful and richest men, Aburizal Bakrie, the Coordinating Minister for People's Welfare. Bakrie, whose family owns part of Lapindo, has long been arguing that Lusi is just another \u201cnatural disaster\u201d \u2014 no more the fault of an individual company than the earthquake itself, or the floods that have hit Jakarta in the past months. But others have their doubts. Davies thinks that had the eruption, or the drilling loss, been triggered by the earthquake, it would have started much more promptly. Michael Manga, a geophysicist at the University of California, Berkeley, says a causal link between the earthquake and the mud volcano would be extremely surprising. Manga has collected data on the distances over which earthquakes have had similar impacts on Earth's plumbing, triggering mud volcanoes and other 'liquefaction events'. In 343 such events, Manga found a clear lower limit below which earthquakes won't do the job (see graph). \u201cIf the Yogyakarta earthquake caused that mud volcano, it would have been way out of range,\u201d says Manga. Manga also found some recent earthquakes larger than 6.3 magnitude closer to the Big Hole that did not cause a mud volcano, suggesting that such weak seismic stress could not account for Lusi. \n               A tragedy of errors \n             Moreover, the drilling loss might have started a series of events that could implicate Lapindo further, suggests Andang Bachtiar, a consultant at Jakarta-based Exploration Think Tank Indonesia. The drill team reacted with a special drilling fluid to seal the fractures responsible for the loss of fluid. The next step was to pull the drill out and add cement to the unstable area at the bottom to secure it before they continued drilling. But at 8 a.m. on 28 May, when they were pulling the drill out, they got some 'kick' 1,293 metres down. Kick is basically the opposite of loss: instead of drilling fluids leaking out of the borehole, highly pressurized liquids or gas suddenly rush in. \u201cThe monster from below caught up with them,\u201d says Bachtiar. An executive at EMP says that kick and loss are common, often anticipated and usually easily managed occurrences that have no connection with the rare disaster of the volcano. He did not wish to be named because he is currently under criminal investigation, along with a dozen other employees of EMP, Lapindo and subcontractors involved in the operation of the drilling project. \u201cKick and loss are like falling off a bike. It happens all the time. You just get back on. What you want to avoid is getting hit by a truck. That's what happened the next morning,\u201d he says. But Davies and Bachtiar think the 'truck' came up through the borehole. \u201cThey drilled something overpressured, something that was driving the system,\u201d Davies says. Bachtiar, one of the first people to suggest that the drilling might have been connected with the mud volcano, was a witness in a police investigation into the possible link and thus studied Lapindo's drilling report closely. He suggests several ways in which what went on at the drilling site could have led to the eruption. It is possible, he suggests, that the workers at the well withdrew their drill too quickly, losing control of the pressure in the hole. Istadi admits that they might have pulled out too fast, and that the effect could have been to suck in fluids from pressurized pockets in the rock, leading to the kick. But he insists that the kick was killed. \n               Soft spots \n             To 'kill' a kick, drillers circulate drilling fluid heavy enough to fight back the incoming liquid and gas. But this is a delicate process. The heavy fluid itself can open cracks in the surrounding rock, which the upcoming fluids can then enter. Lower strata are less vulnerable to cracking because the weight of overlying rock holds them together. In the shallower regions, steel casing is cemented to shore things up. The most vulnerable point is just below this casing \u2014 about a kilometre in the case of Banjar Panji-1. If the pressure caused by the heavy drilling fluid exceeds the pressure holding the rock together here, \u201cit will be a disaster\u201d, says Bachtiar. Rudi Rubiandini, a petroleum engineer at the Bandung Institute of Technology in Indonesia who led an investigation into the mud flow last June, believes that evidence in the drilling report shows that this is what happened. \u201cIt was very clear that the [rock] formation cracked,\u201d claims Rubiandini. The timing and the geographical location also suggest a connection between Lapindo's kick and the eruption. According to Istadi, the kick was resolved within four hours. Moreover, he claims that, at 2 p.m. on 29 May, long after Lusi had started, Lapindo did a test that showed that the borehole was not fractured, at least not at 1,091 metres, the weakest point. Below that, he says, the borehole would have been strong enough to withstand the force of the kick. Bachtiar says, however, that such tests are open to interpretation; other experts contacted by  Nature  also wondered how such a test could have worked. Mark Tingay, a geologist at the University of Adelaide in Australia, says the Sidoarjo volcano has a striking similarity to drilling-induced eruptions offshore from Brunei in 1974 and 1979 (M. R. P. Tingay  et al .  J. Geol. Soc.   162,  39\u201349; 2005). There, deeply buried fluids under high pressure rose to a shallower rock formation that they then fractured, thus eventually reaching the surface. The event also showed the pattern of loss, kick and then eruptions seen in Lusi, some of which were kilometres from the drilling site. In the Brunei case, Shell, the company responsible for the drilling, has documented the expulsion and its efforts to alleviate the situation. The flow took more than 20 years and more than 20 relief wells to quell, says Tingay. \u201cThe similarities all suggest a man-made cause for Lusi,\u201d he says. \n               Money matters \n             Indonesia's president, Susilo Bambang Yudhoyono, has already forced Lapindo to pay 3.8 trillion rupiah (US$420 million) to help deal with the disaster, 2.5 trillion to provide relief and buy the despoiled land from its owners and 1.3 trillion to try to stem the flow. For Lapindo, the downside could get even worse if more villages flood or if it loses a lawsuit brought by Medco, Indonesia's largest oil producer and one of Lapindo's two partners in the joint venture responsible for the drilling. Medco alleges that Lapindo should have put casing down to 2,591 metres to stabilize the hole, which might have made it possible to prevent or control any damage done by the kick. Some experts argue that the borehole should be cased for at least two-thirds of its depth and that Lapindo cut corners to save time and money. Lapindo, in a legal document sent to Medco on 2 February, countered the allegations by saying that the operating agreement requires Medco to cover its part of the liability. The letter ends: \u201cMedco's ongoing failure to pay is jeopardizing the success of the Sidoarjo relief effort.\u201d The Australian company Santos, which holds 18% of the joint venture and has several other drilling operations in Indonesia, had paid US$15 million to the relief effort as of 8 December. People in Indonesia have already made up their minds. If \u201cmud volcano\u201d is mentioned, they shoot back, \u201cAh, lumpur Lapindo\u201d \u2014 the Lapindo mud. And the locals do not think \u201cLapindo relief\u201d is coming fast enough, although the company says that it has already spent extensively on medical assistance, food, emergency and temporary housing and cash allowances for the people of the villages. Despite the damage, the area has become a spectacle as well as a wasteland. Locals have become self-designated toll masters trying to recoup from visitors what the Big Hole took from them. Making left turns, right turns and U-turns on the roads costs 1,000 rupiah (about 10 cents), as do roadside parking spots that had previously been mere dusty lots. Some bare-chested musicians on the side of one highway, who call themselves 'Victims of the Hot Mud', play guitar and dance to drums. A motorcycle guide at the site is happy to take me along one of the earthworks. At one point, he kneels down and tosses a rock some 20 metres. It plops. \u201cThat is where my house is,\u201d he says. Looking in the opposite direction from the same embankment, he points to a partially submerged factory where he used to make plastic and rattan. Lapindo, as part of a government order, has agreed to buy the land and property affected \u2014 but it is not clear when any payment will be made. In the meantime, my guide makes a living by guiding tourists to see the destruction that waylaid his home and job and selling them CD-ROMS of photography \u2014 although mine didn't work. As a motorcycle guide he makes 30,000 rupiah a day, compared with 700,000 rupiah a month in the old days. \u201cNot bad,\u201d I say. \u201cI have no house,\u201d he reminds me. Meanwhile, there are some attempts to deal with the problem at the source. Rubiandini says that last year he and his team planned a series of relief wells to intercept the conduit bringing the mud up and staunch the flow by sending heavy mud back down. Two wells were started; both failed before reaching a third of the required 2,500\u20132,800 metres. \u201cThe drilling teams were constantly off and on because the money wasn't there. People are not serious about killing this,\u201d he says, apparently referring to Lapindo. Istadi retorts, \u201cWe gave him $80 million. We gave him everything. Even if he had had more time it would not have worked. Anyway, the whole project assumes that the borehole is to blame. And we don't know that.\u201d \n               Energy zapper \n             The government has also undertaken a project to push the mud out into a nearby river, so far with little success. The latest hopes rest on an untested scheme to drop concrete balls down the Big Hole to soak up the energy of the upcoming flow (see  'A unique plan to stem the flow' ). But for now, it seems that the mud will continue to flow, and the ground will continue to sink under its weight. The hopes for the 200,000 houses under threat depend on the effective use of levees. Basuki says they have already brought in 2.5 million cubic metres of dirt. \u201cWe moved a mountain,\u201d he says. But more science is needed too, since things might not merely continue, but get worse. For example, says Davies, the central vent could collapse as more and more mud gets removed from the subsurface and spreads its burden over the land. Other mud volcanoes have sunk 500 metres at the centre and forced land to sag for five kilometres around. The Big Hole could yet get bigger still. \n                     Java mud volcano seems unstoppable \n                   \n                     Mud volcano floods Java \n                   \n                     Volcano gets choke chains to slow mud \n                   \n                     NGO/WALHI \n                   \n                     Santos: disaster updates and press releases \n                   \n                     Energi Mega Persada \n                   \n                     UNEP report on mud volcano \n                   Reprints and Permissions"},
{"file_id": "445816a", "url": "https://www.nature.com/articles/445816a", "year": 2007, "authors": [{"name": "Heidi Ledford"}], "parsed_as_year": "2006_or_before", "body": "Hundreds of orchid species lure their pollinators with the promise of sex, only to send them away unfulfilled. Heidi Ledford looks at how dishonesty gives them the evolutionary edge. He's a loner and, truth be told, he's never been with a lady before. She's an experienced siren with an irresistible scent. Lightning quick, he zigzags close, strokes her hair and mounts her. But it is only as he engages in a few confused attempts at copulation that the truth dawns: his chosen is not a lady after all. She's a flower. So goes the tale of the male solitary bee ( Andrena nigroaenea ) and his seducer, the green and dainty early spider orchid ( Ophrys sphegodes ). These orchids bloom in the spring, when female bees are still in their nests and the males are young, inexperienced, and likely to encounter an orchid before they find a real mate. During his attempts to mate with the elaborate flower, the bee may pick up a large packet of sticky pollen on his head, ready to deliver to another plant. That's a boon to the orchid; the bee, on the other hand, gets nothing but hot and bothered. Some 30,000 species strong, orchids are one of the largest groups of flowering plants; collectors covet their elaborate and sometimes rude-looking flowers. But the plants are also unusual in their dishonesty. Although most insect-pollinated plants pay their pollinators in energy-rich nectar, about a third of orchids offer no rewards. Roughly 10,000 species deceive their pollinators by mimicking plants that do provide nectar. And, unique among plants, another 400, including the early spider orchid, mimic females and promise their pollinators sex. Researchers have recently dissected the scents that some deceptive orchids use to lure insects. Because each species manufactures a unique cocktail, and typically attracts only one species of pollinator, the odours provide a way for biologists to identify species in this class of orchid and to study how they have diversified. These studies may eventually provide evidence for sympatric speciation, the concept that a new species can evolve without being geographically isolated. Indeed, orchids may be one of the most promising ways in which to find evidence for this controversial idea. At first glance, dishonesty seems to be a bad policy for orchids. Non-rewarding flowers have low pollination rates, and produce only half as much fruit as rewarding flowers 1 . Add nectar to a non-rewarding plant and pollination rates shoot up. If reproductive success was measured by seed production alone, then deceptive orchids would have lost the evolutionary race long ago. But deception may be worthwhile because it prevents inbreeding and so boosts the quality, rather than quantity, of seeds. Deceived pollinators are more likely to go off in a huff after realizing they've been tricked: they'll take their package of pollen and fly to a more distant patch of flowers, reducing the chance of landing on a close relative of the original plant 2 . \u201cTo be deceptive means that the orchids have less sex, but the sex is better because it's not with a close relative,\u201d says Salvatore Cozzolino, an evolutionary biologist at the University of Naples in Italy. A few years ago, Florian Schiestl, a biologist at the Swiss Federal Institute of Technology in Zurich, set out to determine just what makes orchids masquerading as females so attractive to insects. Although the flowers do look and feel a bit like a female \u2014 the hairs on  O. sphegodes  flowers, for example, resemble those on a female bee \u2014 Schiestl knew the key to long-distance attraction was to smell like a female, because bees respond more to distant odours than to visual cues. But plants produce hundreds of volatile compounds to repel predators and microbes, so the precise perfume was difficult to determine. \u201cYou don't know which are important for the pollinators and which are not,\u201d Schiestl says. \n               Sexy smell \n             Schiestl and his colleague Wittko Francke of the University of Hamburg in Germany reasoned that the way to intoxicate a bee must be through its antennae. Bee antennae are carpeted with hundreds of sensors that carry receptors to detect odours and dispatch electrical signals to the brain. To figure out which floral compounds were attracting male pollinators, the researchers extracted compounds from the labellum of the flower, where insects land, and then separated the chemicals using gas chromatography. They plucked a bee antenna, placed it between two electrodes and exposed it to each component of the mix. When a compound bound a receptor on the antenna, a tiny current passed through the apparatus. Using this method, Schiestl and Francke were able to show that the complex perfume that arouses male  A. nigroaenea  bees is made up of 14 different compounds that are also common components of the waxy cuticle that protects the surface of many plants 3 . They showed that the same combination of compounds is present in the volatile sex pheromone that a female bee uses to attract a mate, and that a blend of these chemicals could make bees mate with dummy flowers. The finding also revealed how sexual deception could have evolved in this species by gradual modification of systems the plant was already using to make its own compounds. Each tweak in the ratio of compounds that increased pollinator visitation would have given the orchid a reproductive advantage. The discovery also suggested one way in which orchids could evolve new species. Most insects are very picky about the scent they respond to and often pollinate only one orchid species. So if a plant changed its aromatic bouquet enough it would attract a different pollinator and rebuff the old one. Over time, that would \u2014 theoretically at least \u2014 result in genetic isolation and the creation of a new orchid species. \u201cWe think that speciation can occur fairly quickly in that system,\u201d says Schiestl. \u201cThe plants need only to change their odour bouquet to attract a new pollinator.\u201d The team found evidence to back this idea in the orchid blooms of Australia. They repeated the experiment on the orchid  Chiloglottis trapeziformis , which tricks the male thynnine wasp ( Neozeleboria cryptoides ). Analysis of  C. trapeziformis  scent revealed a surprise \u2014 rather than adapting existing mechanisms, the orchid was producing an entirely different chemical compound they named chiloglottone, which is also a pheromone made by female wasps 4 . They also found that another  Ophrys  species,  O. speculum , concocts a different wasp pheromone by developing several novel compounds 5 . In this case, the orchid mimic worked so well that, when offered a choice between a female or an orchid, male wasps courted the orchid. Rod Peakall, an evolutionary biologist at Australian National University in Canberra and his colleagues have now surveyed 10 of the 30 known species of  Chiloglottis  orchids across Australia. In some regions, the team found orchids growing together that were identical in appearance, location and flowering time yet were, based on genetic markers, not interbreeding and that belonged to different species. Invariably, flowers of the two species produced slightly different pheromones: one, for example, produced chiloglottone alone whereas the other mixed chiloglottone and a slightly modified version. The two pheromones attract different species of wasp, and targeting different pollinators seemed to have defined species boundaries by preventing the flow of genetic information between the two sets of orchids. \n               Growing apart \n             If Peakall can prove that is the case, he may have evidence for sympatric speciation. This has been notoriously difficult to confirm because, in many systems, it is tough to prove that two similar species growing together were not spatially separated in the past and then reunited. The few documented examples have been in geographically remote areas, such as isolated islands or crater lakes, where the influx of species from other regions is unlikely. \u201cHaving solid evidence of sympatric speciation in a continental setting would be a big deal,\u201d says Daniel Bolnick, an evolutionary biologist at the University of Texas, Austin, \u201cbut it would have to be very strong evidence.\u201d Peakall plans to seek such evidence by using small regions of highly variable genome sequences to track the evolutionary history of his orchids. The aim is to show that the two species growing together are more closely related to one another than either is to orchids located farther away. To do this, he must piece together a precise orchid family tree and rule out the possibility that two similar species actually evolved because they were once geographically separated and later came to grow side by side again. Cozzolino warns that this will be tough because the DNA sequences of these orchids do not vary by much and it may be difficult to distinguish between very closely related species. Nevertheless, Cozzolino agrees that sympatric speciation may be under way, and is looking for evidence of it among European sexually deceptive orchids as well. And what of the insects in this web of deception? Both orchid and insect enter into the encounter in the hope of better sex \u2014 but only the flower comes away with it. This apparent inequality is such that Charles Darwin refused to believe that pollinators would waste their efforts on orchids that offer no reward. He expected the insects to figure out the ruse and save energy by avoiding the flowers and forcing the orchid into extinction. But evolutionary biologists now know that young insects gain more by enthusiastic but indiscriminate mating than they would by being more choosy. Females are often in short supply and there is intense competition among males to find a mate \u2014 so it is better for males to try and fail than not to try at all. \u201cThe males aren't too picky,\u201d Peakall says, \u201cTheir strategy is 'Hey, I will go for anything that looks like a female because I can't afford not to'.\u201d \n                     Inbreeding is bad for plants too \n                   \n                     One place, one parent, two species \n                   \n                     Bees say it with flowers \n                   \n                     Florian Schiestl's webpage \n                   \n                     Rod Peakall's sexually deceptive orchid page \n                   \n                     Pacific Orchid Exposition \n                   Reprints and Permissions"},
{"file_id": "445811a", "url": "https://www.nature.com/articles/445811a", "year": 2007, "authors": [{"name": "Declan Butler"}], "parsed_as_year": "2006_or_before", "body": "An Italian scientist revived the hunt for the mafia's boss of bosses. Declan Butler reports. Bernardo Provenzano, Italy's most wanted man, was on the run for more than 40 years. Moving from one safe house to another, he became the godfather of the Sicilian Mafia in 1993, running the huge organization without ever using a telephone, communicating instead through tiny typed notes,  pizzini , passed back and forth through a labyrinth of secret relays. Over time, many outside the Mafia had begun to believe the rumours, and his lawyers' assertions, that he was dead. That was until an Italian scientist used DNA to confirm that Provenzano was still alive, relaunching the manhunt that led to his capture on 11 April last year. Provenzano had risen up the ranks of the Mafia by mowing people down, earning him the nickname 'the tractor'. He was a member of the Corleone clan, which took control of the Sicilian Mafia after a power struggle in the early 1980s that left hundreds dead. Provenzano was the right-hand man, and natural heir, of Salvatore Riina, who was the Corleone boss and Sicilian godfather until his arrest in 1993. \n               The invisible enemy \n             The prosecutor who was most responsible for Riina's arrest, Giovanni Falcone, was murdered in 1992, and Provenzano was implicated in the crime. Pietro Grasso, who is now Italy's national prosecutor against organized crime, vowed to find Falcone's killers. But, as Grasso acknowledges, all the police had to go on was a photograph of Provenzano taken in 1959, and a computer-generated 'identikit' of what he might look like now \u2014 his invisibility led to another nickname, the ghost of Corleone. With few leads, there followed a decade of fruitless searching. But Grasso never gave up. In 2005, he asked Guiseppe Novelli, head of the medical genetics lab at Tor Vergata University in Rome, to help identify Provenzano. \u201cI was taken aback at first,\u201d says Novelli, a specialist in genetic fingerprinting. \u201cI mean, how can you identify someone who you have no information about?\u201d But Grasso had a lead from a Mafia informant, or  pentito , arrested in 2005. In 2002, the source said, Provenzano had gone to Marseille in France under a false name to be treated for a prostate tumour. A police raid at the clinic found a case history for a man named Gaspare Troia, who had never been admitted to the hospital. The patient's description matched Provenzano. Novelli extracted DNA from hospital tissue samples and ran a DNA profile of both mitochondrial and Y-chromosome DNA. He then compared the DNA with that taken from blood samples of Provenzano's brother, stored in a Palermo hospital where he had been admitted for surgery. The results came in November that year. \u201cIt showed that they shared the same mother and father \u2014 they were brothers,\u201d says Novelli. \u201cThe lead was the right one, they knew they had the right man.\u201d \u201cNovelli's group did a terrific job,\u201d says Grasso. \u201cYou cannot even imagine my enthusiasm when I had the proof of the complete match between the profiles.\u201d The result gave prosecutors and police the strength to carry on looking for Provenzano, adds Paola Di Simone, head of the Italian Scientific Police's Forensic DNA Unit in Palermo. Knowing that Provenzano was alive, the investigators focused their attention on busting the ring of people who had provided the logistics of Provenzano's trip and hideout in Marseille. Last April, by tracking a packet of laundry sent by his wife, police captured Provenzano in an isolated farmhouse just a mile from his birthplace in Corleone. At the hideout, Di Simone collected DNA from false teeth, glasses and an electric razor to confirm Provenzano's identity. She also hoped to find DNA in the house pointing to the henchmen, but the only DNA was his. \u201cBeing in the den just after his capture was very emotional,\u201d she says. \n               Code of silence \n             Novelli told no one what he was working on, except his lab collaborator Ruggiero Mango, who also worked on the case. Grasso advised secrecy to protect the scientists and their families. Novelli admits to having worried for his safety. \u201cI spent one year in a dangerous situation,\u201d he says, recalling flying once to Palermo to meet Grasso and being whisked away from the steps of the aircraft in a car. \u201cI didn't even tell my wife,\u201d says Novelli. \u201cShe learnt it from the TV after Provenzano had been captured.\u201d But he now speaks openly about the work and says that he is no longer concerned for his safety. \u201cIt's finished, he is in prison. I think it is over \u2014 I hope so.\u201d Provenzano was convicted  in absentia  in 1993 to life imprisonment for his involvement in the bombings that killed Falcone and another prosecutor, Paolo Borsellino, in 1992. He now faces further trials on other charges of murder and blackmail. His capture also marked the end of an obsession for Grasso, who says he is overjoyed \u201cto have fulfilled the promise I made on the grave of Falcone, that for the rest of my life I would pursue a sole objective: capturing all those responsible for his death\u201d. \n                     DNA tests put death penalty under fire  \n                   \n                     Japan's ethnic crime database sparks fears over human rights \n                   \n                     Crime busters \n                   \n                     Think-tank calls for an end to DNA deception \n                   \n                     Individual-specific \u2018fingerprints\u2019 of human DNA \n                   \n                     Laboratorio di Genetica Medica, Rome \n                   \n                     Italian forensic science police \n                   \n                     CODIS DNA fingerprint system used \n                   \n                     Software used in investigation \n                   Reprints and Permissions"},
{"file_id": "445144a", "url": "https://www.nature.com/articles/445144a", "year": 2007, "authors": [{"name": "Navroz Patel"}], "parsed_as_year": "2006_or_before", "body": "Life is full of events that are basically games, from paying for a meal to bidding in an auction. Can incorporating a quantum strategy into the rule book increase your chances of winning? Navroz Patel reports. The year is 2015. Representatives from the world's leading defence firms are gathered in a high-security room at the Pentagon. Each is seated facing a screened-off console. Together, they are bidding in an auction that will allocate the various contracts for a $100-billion project to develop a new breed of fighter aircraft. But this is no ordinary auction \u2014 it is designed to ensure that the companies bidding will collectively offer the US government the lowest price for the whole project. In an ordinary auction, one firm might be prepared to offer a low price for a particular contract if it could be sure that another company, with which it has collaborated, will win the contract for another component. If this situation could be encouraged, then the government could shave billions off the cost of the project. But such conditional bids are unlikely as none of the firms wants to reveal its bidding strategy to the others, or even to the auctioneer. Which is where the new strategy comes in \u2014 the contracts for this project will be decided by a 'quantum auction'. The participants use their consoles to manipulate qubits (the bits inside their quantum computers) to produce quantum states that correspond to each of their desired bids. Because the bids are encoded in fragile quantum states, no one, not even the auctioneer, can read the information in the qubits without destroying them. The success of the auction depends on repeated exchange of information between the bidders, through a quantum algorithm that operates on the qubits, until an optimal outcome is reached. In the process of reading this outcome, the individual bids are destroyed. This guarantees that losing bids will never be revealed and so dramatically increases trust between the participants. The ability to express conditional bids in a secure way is what leads to the more ideal outcome. Think this all sounds too futuristic to take seriously? Think again. A group of researchers at HP Labs, the research arm of Hewlett-Packard in Palo Alto, California, is working to create the quantum protocols that could allow such an auction to take place within a decade. And their work is helping to bridge the gap between theory and practice in both game theory and quantum information processing. The field of quantum game theory (QGT) is still in its infancy: just tens of economists and quantum physicists have so far published a total of 200 or so papers on the topic. Its roots stretch back to a paper published in 1999 by mathematician David Meyer of the University of California, San Diego 1 . In it, Meyer showed how a quantum approach always beats a classical strategy in a simple game where two players flip a coin. This is because the laws of quantum mechanics allow the coin to exist in a state that is a combination of heads-up and tails-up at the same time, so the person playing by classical rules will always be outmanoeuvred. That's all fine in theory, but subsequent work on QGT has struggled to be taken seriously. \u201cMuch of the claimed superiority of quantum games to their classical counterparts has been the result of incorrect comparison,\u201d Meyer acknowledges. Some of the scepticism arises because both game theory and quantum information processing have unresolved issues. Like QGT itself, the field of quantum information processing is mostly theoretical and, because computers that can handle hundreds of qubits are several decades away, critics argue that much of the research is simply impractical. Game theory \u2014 which is used to analyse strategic situations and the behaviour of participants seeking to maximize their success in such games \u2014 also has problems, not least of which is the fact that some of its predictions contradict what happens in the real world. But by linking these two problematic fields into QGT, researchers hope to find practical applications that can resolve issues in both fields. For example, the prediction that rational players in many kinds of games will pursue their own selfish interests \u2014 referred to as defecting \u2014 is not borne out in real-life: people are often altruistic. In the game known as the prisoners' dilemma, if two players both cooperate, they earn more than if they both defect; but an individual who defects always gets a higher pay-off, regardless of what the other player does. Game theory predicts that rational players will all defect 2 , but in practice they often cooperate. \n               Jail-break \n             At HP Labs, a team including experimental economist Kay-Yut Chen and quantum physicists Tad Hogg and Raymond Beausoleil is making theoretical and experimental advances in QGT that may address some of these criticisms. One advantage of studying QGT over other applications of quantum technology is that fewer qubits should be needed to play quantum games, so it might be possible to implement them fairly soon. HP Labs hopes that Chen and Hogg's work on quantum auctions will ultimately lead to a new business model for selling digital content on the Internet that might help to discourage illegal downloading. In terms of game theory, how digital content is sold and distributed on the Internet can be seen as a 'public-goods game'. Much as in the prisoners' dilemma, which is a two-player version of this type of game, selfless choices by members of a group reap greater benefits for the group as a whole, although selfish behaviour produces the greatest personal gain. So how do content providers prevent illegal downloading of software, music and videos, when others have already paid for the privilege of exclusive access? The problem is similar to that faced by a table of diners in a steakhouse when it comes to settling the bill. Experience shows that when people agree to split the cost of a meal between them, if the group is small \u2014 just a few people \u2014 the diners tend to order more modestly priced items from the menu, and may even offer to pay more than their share of the bill. But as the size of the group increases, people feel a greater sense of anonymity and may conclude that their individual decisions will have a smaller impact on the overall bill. The end result is defection: some people will order the most expensive steak rather than a cheap cheeseburger and, to add insult to injury, might not even stump up their share of the bill. Game theorists refer to this kind of player behaviour, which is detrimental to the wider good, as free-riding. \n               Game on \n             In their quantum approach to the public-goods problem 3 , Hogg and his colleagues focused on multiple games of prisoners' dilemma played between pairs of people within a larger group. In a three-player group, for example, each player would play two games, one with each of the other two. Unlike classical game theory, the quantum version of the prisoners' dilemma predicts that players will cooperate in 50% of the games played. And, unlike in real-life classical situations, Hogg and Chen predicted that the greater the number of players involved, the less of a problem free-riding should be. In a classical prisoners' dilemma, defecting is always the best strategy for rational players. In the quantum version of the game, which intrinsically links the two players' decisions through quantum mechanics, the outcome is often probabilistic in that the players can't be sure their pay-off will be the same each time, even when all players make the same choices. This uncertainty alters the structure of the game to one in which no single choice is best. \u201cThe quantum version of the game is more like scissors, paper, stone,\u201d says Hogg. In such games, there is no single move that will win each time, so a mixed strategy \u2014 choosing moves randomly or based on guesswork \u2014 is the best option for rational players. This analysis worried the researchers as people don't usually play mixed-strategy games very well. As with all game-theory research, one question loomed large: will the outcomes predicted by Hogg and Chen occur in reality? \u201cWe can safely assume that 99% of the population could not accurately be described as quantum physicists,\u201d says Chen. \u201cSo we needed to understand how players would behave in this kind of game.\u201d In 2004, Chen and Hogg set out to show that it is possible for people to play primitive quantum public-goods games 4 , using a dozen or so students from Stanford University. The experiments, which mimicked both classical and quantum versions of the games, did not involve actual quantum computers or qubits, instead the students interacted with computers that simulated the players' operations on quantum states. Indeed, the students were completely unaware that any feature of the game aped the laws of quantum mechanics. \n               Place your bets \n             In both the classical and quantum games, the students were each given $100 and randomly grouped into pairs to play a version of the prisoners' dilemma in which the choice was either to keep the $100 or to contribute it to a common fund. The pair were better off if they both cooperated and contributed to the fund, but individuals got higher pay-offs by defecting and not contributing no matter what the other player decided. In the quantum version of the game, the decision of whether or not to contribute was determined by a more complex process intended to simulate quantum entanglement between qubits assigned to each player. Entanglement means that the quantum states of two or more disparate objects are intrinsically linked in some way \u2014 so changing the state of one object automatically affects the state of the others. Players were asked to pick three numbers that controlled the probabilities of the various outcomes of the entanglement; before and during the game, they were given software tools to help them to understand the consequences of their choices and those of their opponents. But because their opponents' choices remained unknown, and because the probability of an outcome was rarely 100%, each player could only guess at the best numbers to choose. Overall, the students cooperated roughly 50% of the time in the quantum games, as opposed to just 33% of the time in the classical version. \u201cWe were surprised. Our lab experiments showed that players without any training in quantum mechanics exhibited behaviour indistinguishable from that predicted by our theory,\u201d Hogg says. This optimal behaviour seemed to emerge because players tried to second-guess what their opponent's strategy was, resulting in a mixed strategy even though that was not what they intended. \n               Joint effort \n             Although these experiments involved two-player games, and so simulated two-way entanglement between players' qubits, Hogg and Chen also studied larger groups of three of four players, in which each permutation of pairs played a game of prisoners' dilemma. The results suggested that free-riding decreases as game size increases to three or four players. If confirmed with larger groups, this effect would be highly desirable in the context of Internet piracy, where the number of players, that is downloaders, can run into tens of millions. \u201cThere are many complexities in this context, but it falls within the scope of what quantum public-goods games research can address,\u201d Hogg says. Hogg and Chen believe that their experiments can mimic more complex entanglement within larger groups of players, using relatively few qubits. \u201cThe way we are creating these protocols \u2014 with the small number of qubits required and pair-wise entanglement \u2014 is, we believe, physically doable within five to ten years,\u201d says Chen. Such experiments might allow them to study economic problems in which the arguments for players using a quantum approach are compelling. Chen says they hope that their work on quantum auctions will soon produce a protocol that will enable collaborative auctions without needing to rely on trusted third parties (the auctioneer) or prior agreements to reach a preferred outcome. Knowing what your competitors will bid is a major uncertainty at every auction, but knowing what a potential collaborator will bid and factoring that into your strategy is a more subtle uncertainty, referred to by economists as 'allocative externality'. In the fighter-jet example, this might translate to a company that is bidding to build the undercarriage wanting to bid less if another specific firm wins the contract for the landing gear. \u201cAllocative externality is a prevalent problem in the awarding of large government contracts, and is something that all non-quantum auctions fail to address,\u201d says Chen. Last summer, Chen and Hogg began to investigate how people behave in a quantum auction \u2014 although they first chose to study a simpler highest-bid-wins auction without collaborative bids. In this, the bidders decided how much they wanted to bid, and then deployed a quantum protocol similar to that used in the public-goods game to encode their bid and keep it private. To process the multiple bids, the researchers simulated the action of a quantum algorithm designed to find the maximum value of a quantum state \u2014 in this case, the maximum revenue received by the auctioneer. In an actual implementation, this search would be performed during the repeated exchange of qubits between the auctioneer and participants until an optimal answer was converged upon. At that point the auctioneer would perform a measurement of the qubits to decide the outcome of the auction, and all the losing bids would be destroyed. Initial three-player auctions revealed flaws in the quantum search algorithm. \u201cDeficiencies in the algorithm enabled players to potentially misrepresent bids so that ultimately they could win with a lower bid,\u201d says Hogg. He adds that the team is currently redesigning the algorithm to avoid such problems before tackling more complex collaborative auctions. Chen and Hogg hope to introduce allocative externality into their quantum auctions in the next year or so. Although it will be some years before they begin to experiment with actual qubits, Meyer is encouraged by the team's experiments: \u201cI'm fairly optimistic about the kind of research they are doing.\u201d Given the obvious security advantages of quantum auctions, Meyer believes that there is now a greater chance that QGT will prove to be genuinely useful. Hogg and Chen's studies have also been welcomed by physicist Paul Ellsmore, co-author of a recent UK report on the commercial prospects for quantum information processing 5  and chief executive of semiconductor firm Nanion in Oxford. Conceptually, quantum games have some attractive features, Ellsmore says, but the creation of robust algorithms to encode quantum information has been lacking. \u201cCommercialization had been considered a dirty word in this field,\u201d he says. \u201cIt's encouraging that researchers are now developing algorithms that could be implemented with technology that will be viable in the near future.\u201d Navroz Patel is a freelance writer based in New York City. \n                     Quantum search algorithm demonstrations \n                   \n                     Game theory \n                   \n                     Quantum information processing \n                   Reprints and Permissions"},
{"file_id": "446016a", "url": "https://www.nature.com/articles/446016a", "year": 2007, "authors": [{"name": "Ed Gerstner"}], "parsed_as_year": "2006_or_before", "body": "Physicists are planning lasers powerful enough to rip apart the fabric of space and time. Ed Gerstner is impressed. Almost 100 million times more powerful than its earliest ancestor, the Large Hadron Collider (LHC) is the latest triumph in a century of astounding physics. Officially inaugurated later this year, the \u20ac3.7-billion (US$4.9-billion) accelerator at CERN, Europe's particle-physics laboratory, will next year be fully up to speed in its search for exotic phenomena, such as the Higgs boson, that can only be discovered at the extreme energies it makes available. Yet even before it is turned on, the particle-physics community is already looking beyond it, with plans for an even more powerful machine, the International Linear Collider (ILC), which will cost some $6.7 billion. At about the time that the LHC hopes to be homing in on the Higgs, construction of a much less feted multibillion-dollar physics research facility, the National Ignition Facility (NIF) at Lawrence Livermore National Laboratory in California, will be reaching completion. At a cost of about US$4 billion, NIF is an assembly of 192 lasers that, for a billionth of a second at a time, can pump out energy at more than 50 times the rate that it is generated in all of Earth's power stations put together. Its aim is to ignite a fusion reaction that turns a tiny pellet of hydrogen at the lasers' focus into helium. Science at NIF will bring astrophysics into the laboratory by aping stars in microcosm, and it could conceivably provide the basis for future energy generation. But the main rationale for NIF, and the reason it has been able to command the budget that it has, is to help the United States assure the operability and safety of its nuclear arsenal. Similar motivations lie behind the French Laser MegaJoule (LMJ) facility, which is expected to achieve ignition a few years after NIF. The lasers of the LMJ and NIF don't come close to an accelerator such as the LHC in terms of generating excitement among physicists. At least, not yet. But laser beams even more intense than NIF's \u2014 and far cheaper to generate \u2014 might in the next decades begin to take over from particle accelerators in exploring the outermost frontiers of the physical world. The world may never see conventional particle accelerators much more powerful than the LHC and ILC. But lasers a million times more intense than NIF are already to be found in the presentations of physicists looking for funding. \n               Let there be light \n             The guiding inspiration for extreme lasers lies in the way light interacts with the vacuum. Quantum field theory sees the vacuum as a strange sea of possibilities, where pairs of 'virtual' particles and antiparticles ceaselessly pop in and out of existence. When light is bright enough, the electromagnetic fields it is composed of begin to interact with that sea in unusual ways. The vacuum no longer behaves as a simple, predictable medium \u2014 it becomes something altogether more exotic, unpredictable and nonlinear. Pump in enough energy, and the paired virtual particles become real, separated at birth by the extraordinarily strong fields involved. This energy level, currently thought to require fields of a little more than 8\u00d710 18  volts per metre, is known as the Schwinger limit, and it is the point at which the vacuum sea begins to boil. \u201cWhen I give talks to general audiences, that's the thing that they really seem to get drawn in by,\u201d says Tom Katsouleas, an extreme-laser enthusiast at the University of Southern California in Los Angeles. The problem is that the Schwinger limit is a long way away. For fields of 8\u00d710 18  V m \u22121  you need a laser with an intensity of more than 10 30  W cm \u22122  \u2014 a thousand trillion times more intense than NIF. Given NIF's multibillion-dollar pricetag, that seems an overly ambitious target. But a team led by G\u00e9rard Mourou, director of the Laboratory of Applied Optics near Paris, believes it can meet this target for a relatively moderate price. And the researchers predict all manner of wonders on the way to their eventual goal. Throughout their history, lasers have excited physicists by opening up new possibilities with light. In the 1960s, the fact that early lasers were powerful enough to change the refractive index of the medium through which they travelled opened up fresh vistas in 'nonlinear' optics. Today the frontier buzzword is 'relativistic optics' \u2014 systems in which the fields associated with the laser light can accelerate every electron in the medium the light is passing through close to the speed of light. \n               Need for speed \n             Mourou's proposal \u2014 the Extreme Light Infrastructure (ELI) \u2014 would up the ante further, moving into 'ultrarelativistic systems' in which not only electrons but also the ions from which they have been stripped move close to the speed of light. Mourou notes that the nonlinear effects of lasers revealed in the 1960s far exceeded expectations. \u201cOnly the tip of the iceberg was predicted,\u201d he says. He is similarly optimistic about what the new energies available at the ELI could deliver. On 15 February, the French government announced that it had bought into the vision enough to pay for a new laser beamline at the Laboratory of Applied Optics to show that the ELI could work. If it goes ahead, the full facility would provide unprecedented opportunities for scientists to pursue fundamental, curiosity-driven science, says Mourou. Perhaps more importantly, it would be relatively cheap, costing \u20ac138 million to build and \u20ac6 million per year to run \u2014 considerably less than the \u20ac380-million (US$746-million) Diamond synchrotron X-ray source recently opened in Britain, for example. High-power lasers achieve their awesome intensities by squeezing moderate amounts of energy into very short-lived bursts, thus driving up the power \u2014 which is energy divided by time. So although the power of NIF's lasers sounds incredible, they actually use relatively small amounts of energy. A single pulse contains just 2,000 kilojoules \u2014 roughly half a kilowatt hour. It's just that all that energy is delivered in a few billionths of a second. \n               Power trip \n             The ELI takes the same principle further. By generating pulses a million times shorter than those of NIF \u2014 five femtoseconds \u2014 the ELI should fairly quickly be able to generate peak powers of more than a petawatt (10 15  watts) from just a few joules of energy. This radically reduced need for energy makes things much easier than they are at NIF. By shortening the pulse lengths by a factor of a hundred more, down to tens of attoseconds (10 \u221218  seconds), the ELI's proponents hope to reach peak intensities of more than 100 petawatts. The ELI's extraordinarily short pulses will be made possible by a technique called chirped-pulse amplification (CPA), which Mourou developed at the University of Rochester, New York, in the mid-1980s. CPA works by decomposing the light in a laser pulse using a diffraction grating known as a stretcher, which acts like a prism. Having been stretched, the pulse's components, now spread out in space and time, are fed individually through an optical amplifier, before a similar grating designed for the opposite effect \u2014 a 'compressor' \u2014 reunites them into a pulse far shorter and more intense than the original. These stretchers and compressors are currently used on almost all of the world's most powerful lasers except those, such as NIF and the LMJ, that need pulses that are relatively long (of the order of nanoseconds). Osaka University and the Central Laser Facility at Rutherford Appleton Laboratory in Didcot, UK, both have CPA lasers that can generate a petawatt, and the University of Rochester is also building one, as are other institutions. Mourou and others feel that the technique has, as yet, no obvious limitations; pulses can go on getting shorter and shorter. In 2006, the ELI was one of 35 projects short-listed for consideration under the European Roadmap for Research Infrastructures, a programme that will provide money to help develop proposals for international projects. Also on the shortlist is another, costlier project that plans to use a CPA-enabled laser. A consortium led by the Central Laser Facility wants to build HiPER \u2014 the High Power Laser Energy Research facility \u2014 as a civilian equivalent to NIF and the LMJ, but pursuing a subtly different path to fusion. Whereas NIF and the LMJ use megajoule beams to crush their targets into fusion, the \u20ac855-million HiPER would compress the target comparatively gently and then ignite it with a much shorter high-power pulse. One advantage of this approach is that the laser could be fired far more frequently than NIF. With more pulses, and freed from the demands of weapons research, HiPER should offer physicists far greater scope for non-fusion research \u2014 NIF's non-fusion work is effectively limited to about one pulse a week. And the power would be greater. With further developments of CPA, Mike Dunne, director of the Central Laser Facility, predicts that amalgamating HiPER's beams could push the machine's limits beyond petawatts to exawatts (10 18  watts). \u201cIf \u2014 and it's a big if \u2014 such beamlines could be coherently combined,\u201d Dunne says, \u201cthen we believe that a beam of up to 2 exawatts is feasible. Although horribly difficult in practice.\u201d \n               Curiouser and curiouser \n             In fundamental physics terms, energies at the exawatt level would offer some intriguing possibilities for producing strangeness from the vacuum, and so could allow physicists to study phenomena unreachable any other way, such as those found at the edges of black holes. In the 1970s, Stephen Hawking predicted that when a virtual particle\u2013antiparticle pair is created just 'outside' a black hole, the warping of the local vacuum by the hole's gravity will be strong enough to tear the pair asunder, with one disappearing into the black hole and the other surviving as matter outside it. The effect is analogous to the production of electron\u2013positron pairs by electric fields at the Schwinger limit. \u201cThe vacuum really doesn't care if it's an electric field, a magnetic field, a gravitational field, or even if it's a weak nuclear field or a strong nuclear field,\u201d says Bob Bingham of the Rutherford Appleton Laboratory. \u201cIf you can pack enough energy in, you can excite particles out of the vacuum.\u201d The attraction of Hawking radiation is that its dependence on a gravitational field means that its subtleties will depend on interactions between quantum field theory and general relativity, the sort of thing that could throw light on quantum theories of gravity. The laser-builders don't want to create black holes with which to look for Hawking radiation \u2014 but they don't have to. General relativity's equivalence principle means that to something experiencing one of them, a gravitational field and an acceleration are indistinguishable. So, as theorist William Unruh pointed out in the 1970s, when a particle is accelerated at a sufficient rate, it should see and be affected by Hawking-like radiation in its own frame of reference \u2014 if, that is, certain assumptions about the curvature and structure of space-time are correct. The accelerations involved in Unruh radiation are far too extreme for a traditional particle accelerator, but perhaps not for lasers on the scale of the ELI. \u201cNothing generates fields even close to those produced by an ultra-high-intensity laser \u2014 except perhaps a black hole,\u201d says Bingham. \u201cSome of the lasers at the Rutherford Appleton Laboratory will produce a field of about 3 trillion volts per centimetre. Nothing else comes close to that!\u201d Using their lasers to accelerate electrons fast enough to feel Unruh effects could be a tantalizing problem for the ELI, HiPER or other extreme lasers to tackle. \u201cIt seems to me to be a very hard experiment \u2014 creating such strong and short intense pulses,\u201d says Unruh, now at the University of British Columbia in Vancouver. \u201cBut I am always astonished at what experimentalists can actually do if they put their minds to it.\u201d This is not the only way in which lasers could beat accelerators at their own game \u2014 or help them to greater heights. By definition, relativistic optics requires that electrons be accelerated close to the speed of light. Get them very close and researchers might be able to do particle physics beyond the reach of the LHC and ILC. One approach to this would build on the idea of a 'wakefield' accelerator, in which electrons hitch a ride on the wake left behind by an intense pulse of laser light blasting through a plasma. \u201cIf you look at the progress that has gone on in laser wakefield accelerators,\u201d says Katsouleas, \u201cand extrapolate that to the kinds of powers that people are talking about for the ELI, then it becomes possible to think about accelerating particles to [ILC energies] in just a short section of plasma.\u201d But even if the vast engineering challenges of such an accelerator could be met, it would hardly be a replacement for what the current and planned accelerators do, because the average power would be far lower: big accelerators store large amounts of energy in their beams. \u201cTo get the luminosity one needs for doing high-energy physics,\u201d says Katsouleas, \u201cand for events to be detectable on a reasonable timescale, the average power of the laser isn't sufficient. It's about three orders of magnitude away. But there are already many ways you could think about doing this.\u201d \n               Follow the light \n             Barry Barish, director of the global design effort for the ILC, is intrigued by the possibility of using lasers to accelerate particles, and agrees that it might be one of the future technologies that keeps the field going. \u201cIt's certainly a very promising avenue to pursue, because up front there doesn't seem to be any obvious limitation to it, and it could be that it will help in the long term,\u201d he says. But he doesn't see it as the only game in town, or as a sure thing. \u201cIt's just hard to know where the show-stoppers will be. But at the level where it won't require enormous resources, this is the sort of thing that needs to be pursued.\u201d Mourou, on this as on much else, is bullish. \u201cProbably within 20 years!\u201d he says of laser-driven follow-ons to the ILC, before back-pedalling, at least a little. \u201cThe time constant to design it and to build it is ten years. And you always build these things with the previous technology. So I have to be very careful when I say 20 years. But in 10 to 20 years we will have this technology, and then it would take another 10 years to build.\u201d Beyond that, he and his colleagues say, there are even more remarkable technologies: gamma\u2013gamma colliders that can reach energies millions of times higher than today's accelerators; and 'relativistic mirrors' that can push light to the Schwinger limit and beyond. \u201cWe're going to change the index of refraction of the vacuum,\u201d enthuses Mourou, evoking the ultimate fulfilment of the laser's original promise. \u201cAnd we're going to produce new particles \u2014 the vacuum is the mother of all particles. And I'm sure we're going to discover more.\u201d \n                     Plasma physics: On the crest of a wake \n                   \n                     A high-power laser fusion facility for Europe \n                   \n                     Laser gears up for star role \n                   \n                     US sends Livermore laser target chamber to France on loan \n                   \n                     ELI website \n                   \n                     HiPER website \n                   \n                     LHC website \n                   \n                     NIF website \n                   Reprints and Permissions"},
{"file_id": "445250a", "url": "https://www.nature.com/articles/445250a", "year": 2007, "authors": [{"name": "Alison Abbott"}], "parsed_as_year": "2006_or_before", "body": "Programming a robot to think like an insect is tough, finds Alison Abbott, but it could help breed machines as manoeuvrable as flies. Tarry II looks like a robot, sounds like a robot, and walks like an insect. He sprouts a tangle of wires, and the mechanical joints on his six legs emit a metallic creak with every step. But he strides determinedly across the lab with the steady gait of a fly marching towards rotting fruit. The strutting machine is the work of Roland Strauss at the University of W\u00fcrzburg and robotics colleagues elsewhere in Germany. Strauss acquired a zeal for mechanics as a schoolboy, when he won a national prize for designing electronics that could imitate a crab's visual system. But his real passion was always biology, and his goal today is to understand how behaviour involving movement is controlled in insects. He has recruited Tarry II and a small army of other biorobots to help. Although our encounters with flies often leave an impression of aimless and irritating meandering, these tiny creatures' decisions are just as purposeful as those of other animals. A fly scans its environment with eyes and antennae, processes this information in its brain and then makes a decision, perhaps to turn away from potential danger or hurry towards food. Strauss aims to tease apart the complex of brain circuits that coordinates such movements. He hopes to identify broad principles about how the brain directs these behaviours, which might also apply to other animals including, perhaps, ourselves. To help, Strauss and a handful of other insect biologists have turned to robotics experts. By programming simple robots to react to stimuli and move in particular ways, they can test biological hypotheses about which neural networks an insect uses to navigate. And the biologists hope to return the favour. The algorithms they use to direct their biorobots could in the future help design smarter and more agile robots, capable of overcoming many barriers without direction from humans. (Videos of both insect-like robots and insects whose movement behaviour has been experimentally manipulated can be seen in this feature on the  Nature  website.) If only the Mars rovers had been more like cockroaches, sigh insect biologists, they might have been able to extricate themselves from the sand dunes and rocks on which they have occasionally come a cropper and had to be carefully steered to safety by their human controllers. \u201cWe are very happy if what we learn from nature can be put to use to make better robots,\u201d Strauss says. A large amount of insect movement occurs without guidance from the brain. It is instead directed by circuits of nerves in the nerve cord, which extends from the insect's head into its thorax and is equivalent to the vertebrate spinal cord. If you decapitate a fly it will stop moving because the brain no longer provides the 'go' signal. But if you drop a neurotransmitter such as octopamine directly onto its thoracic nerve cord, then it will start to walk around like \u2014 well, like a headless chicken 1 . The fly can even be induced to carry out more complex and ghoulish behaviours, such as grooming eyes that are no longer there. These basic movement programmes are well studied and have been transferred to robots. Tarry II's prototype, designed by biorobotics pioneer Holk Cruse at the University of Bielefeld, has been walking with the confident coordination of a decapitated stick insect for more than a decade ( see video 1 ). \n               Executive decision \n             But for the cleverer stuff \u2014 deciding when to move, at what speed and in which direction \u2014 the insect recruits its brain. Just as we do. And just as robots should, but cannot, because current algorithms do not to provide them with this level of sophisticated and autonomous decision making. Like other animals, an insect's brain contains specialized circuits responsible for controlling particular aspects of behaviour, spanning defined anatomical areas. So biologists have begun to decipher which particular region is involved in directing each type of movement by destroying that area and watching what happens. Strauss likes to work with the fruitfly  Drosophila melanogaster  because genetic tools are available to selectively damage very specific brain areas. Researchers can, for example, introduce random mutations into the fly's genome and then screen for flies that walk or move abnormally. In many of these, a mutation will have interfered with the structure of a particular brain region. Using such methods, Strauss has accumulated a collection of several hundred mutants with aberrant movement behaviours. A large proportion have damage to a brain structure called the central complex, a collection of components sitting between the two hemispheres of an insect's brain known to be critical for executive control of movement. Mutants with a damaged central complex might walk more slowly, or spiral their way towards an object of desire instead of approaching it in a straight line. Over the past ten years, Strauss has dissected out exactly how different regions of this complex cause specific movement or orientation problems, and he is now starting to see if he can translate these functions successfully into algorithms for Tarry II as well as for a newer, wheeled machine called Dro-o-boT. Already these robots have supported his hypothesis that flies use parallax motion to gauge distances \u2014 the motion familiar to train passengers, who see close objects rushing past while distant ones seem to move slowly. The ability to gauge distance is critical for flies; they approach stationary objects in the hope that they may turn out to be safe havens or sources of food, and to save energy they always approach the closest first. To monitor how flies move when they see objects, Strauss built a virtual-reality environment which looks like a a nightclub for flies. The setup consists of a small cylinder with a glass floor, and walls packed with light-emitting diodes that can be programmed to create geometric patterns of light and dark. The movements of a fly placed in the centre of the floor are videoed from above, and each footfall is recorded automatically from below. Test flies have their wings clipped so they cannot fly, and a tiny moat keeps them in the central arena because they are repelled by water. \n               Parallax view \n             Flies, it turns out, will move towards dark shapes created by unlit diodes as if they were real objects. To test his parallax-motion hypothesis, Strauss programmed his virtual-reality environment to automatically move the shapes according to the flies' movements. The flies did, as predicted, use parallax motion to identify the nearest 'object', which they approached 2 . But Strauss next wanted to test whether insects use parallax motion alone to judge distance or whether they also recognize what the objects actually are. For this, he turned to Dro-o-boT. The researchers placed Dro-o-boT in a scaled-up, two-metre-square model of the virtual-reality environment. Dro-o-boT has an integrated 360\u00b0 camera for eyes and was programmed to integrate visual information on the size of 'objects' with its own movements, so that it could recognize and wheel itself towards the dark spot that appeared closest ( see video 2 ). Movement traces of Dro-o-boT confronted with four landmarks were virtually identical to those of a fly confronted with an equivalent four landmarks, showing that parallax motion is indeed sufficient to direct this type of movement. In the past couple of years, Strauss's group has similarly programmed Tarry II so that it can imitate flies even more closely than the wheeled Dro-o-boT, by using parallax motion to identify and approach the closest object on its six legs. The robot can even follow the movements of its owner relative to marks on the more distant wall and change direction to move towards him ( see video 3 ). Having explored how a fly reacts to an object, Strauss is now starting to examine other movement behaviours, including walking speed. Such is the simplicity of a fly's brain that if two black squares appear directly opposite each other on the cylinder walls, it will walk towards one, then turn at the moat and pace back towards the other, lacking the intellectual wherewithal to choose a destination. It thus continues a seemingly pointless march until fatigue sets in ( see video 4 ). This is known as the Buridan paradigm, after 'Buridan's ass', a thought experiment in which a donkey placed exactly between two identical piles of hay is destined to starve to death because it cannot decide which pile to choose. In another variation, the virtual-reality environment is set up with vertical lines and rotated, making the insect circle at the same speed as it continually tries to walk towards the nearest object. The fly's predictable behaviour in these paradigms is useful for experimentalists because it allows them to find mutants with brain damage that interferes with their turning, walking speed and resoluteness (the time before they simply give up the march). Many of the mutants that performed badly in these two experiments turned out to have damage to a structure within the central complex called the protocerebral bridge, a string of nerves which Strauss has shown changes the swing speed of the leg and hence controls step length. In one of his mutants, appropriately called 'no bridge', the bridge is cut right through. Another, called 'tay bridge', after a Scottish bridge that collapsed disastrously in 1879, has a decided kink. Normal flies take longer strides when their stride rate increases, to gather speed efficiently. But in Buridan's paradigm, tay bridge mutants did not increase stride length, walked at half the speed of normal flies, and gave up earlier. In the rotation paradigm, neither the no bridge nor the tay bridge mutants could keep up with the moving stripes. \n               Mind the gap \n             Strauss used Tarry II to understand in more detail why these mutant flies are so sluggish. Graduate student Simon Pick, now at the University of Ulm, Germany, programmed Tarry II to mimic the mutants so that the robot no longer increased stride length with stepping frequency. In doing so, he uncovered what robotics experts call an emergent property. He found the tay bridge way of walking was not just slow, but also inefficient \u2014 it consumed 8% more energy for a given speed than normal walking. \u201cWe didn't expect it, but the robot simulation made us realize that energy efficiency is an important evolutionary advantage for walking insects,\u201d says Strauss. The evolutionarily ancient brain of the stick insect, he notes, is not so sophisticated: the insect can vary only step frequency, not step size. Strauss is now turning his attention to more complex movements: his current preoccupation is how flies climb across gaps. By filming flies as they confront gaps of varying widths, he has described distinct units of behaviour as the flies try first of all to assess the width of the gap, then initiate and complete the crossing or, alternatively, topple into the abyss. When a fly stumbles upon a chasm, for example, it typically raises its front legs high above its head and performs far-reaching climbing movements as if it were feeling for a way across. If it fails to find a way over, a normal fly tries again and will eventually throw its body across to grasp the far edge. But Strauss has identified several mutant lines which consistently foul-up gap-crossing, and seem unable to improve. One fails to reach the other side correctly because it initiates leg-over-head movements too early. Another fails to even start the crossing, choosing instead to turn tail and walk away 3  ( see video 5 ). Strauss wants to use his robots to test how flies control their reaching when they negotiate gaps \u2014 but these complicated behaviours are difficult to model in machines. He is planning to fit Tarry II with attachments that would allow him to climb in the same way as flies need to when they cross a gap. \u201cOne might think about Velcro attachments and an environment with carpeted walls, or claw attachments with Styrofoam walls,\u201d he says. Mutant fruitflies are a treasure trove for movement-behaviour researchers, but their minuscule brains do impose some experimental limitations. The cockroach brain, on the other hand, is 50 times larger, so researchers can inflict brain damage using surgery. Other advantages are a matter of taste. \u201cCockroaches can actually be quite pretty, I think,\u201d says neuroscientist Roy Ritzmann from Case Western Reserve University in Cleveland, Ohio, \u201cand my sort [ Blaberus discoidalis ] don't really smell so bad.\u201d \n               About turn \n             In one series of experiments, Ritzmann removed the cockroaches' wings and severed the circumoesophageal connective, the part of the nervous system that separates the lower and upper parts of the brain. Following this procedure, cockroaches will walk until they drop, but cannot negotiate obstructions. When climbing an incline of 40\u201345\u00b0, for example, their legs might slip and they sometimes fall backwards in slapstick fashion ( see video 6 ). This is because some of the brain-damaged insects are unable to control the height of their bodies and tend to raise their centre of mass too high, so adhesive pads on their legs no longer stick 4 . Like Strauss, Ritzmann wants to unravel the neural mechanisms his creepy-crawlies use when confronted with obstacles, behaviour which is coordinated by the central body complex. His students carefully exposed the brains of cockroaches and used tin foil to make cuts through the complex. They then identified ones that no longer turned correctly by videoing them in a transparent chamber and later dissected out the brains to identify exactly which part had been damaged. Intact cockroaches turn to the left when their right antenna touches the wall, and vice versa. But few of Ritzmann's lesioned cockroaches could turn properly. Depending on where the cut was made, some animals could turn only in one direction, some circled continuously and others were disinclined to turn at all, crashing into the walls 5  ( see video 7 ). These disabled roaches have helped Ritzmann develop a hypothesis that the insect brain goes through three distinct mental steps when it confronts an obstacle: it must detect the block, decide that it would be appropriate to turn, and then decide which way to turn. Ritzmann now wants to test this hypothesis by programming these three steps into a robot; he is collaborating with colleague Roger Quinn in the university's biorobotics laboratory. Quinn's six-legged Robot III is modelled on the  Blaberus  cockroach and it already possesses the astonishing stability of a headless insect \u2014 give it a strong shove, and it will find its feet again almost instantly ( see video 8 ). But this work is just beginning and as yet Robot III can't walk very well, let alone negotiate turns. Using a particularly manoeuvrable breed of robots called Whegs, Quinn has tested another of Ritzmann's hypotheses about a behaviour at which cockroaches seem adroit \u2014 climbing over or slipping under a shelf. \n               On the shelf \n             Whegs, so called because they are propelled by a cross between wheels and legs, have the tripod gait of all six-legged insects (see  'Why robots need legs' ) and can flex their bodies thanks to a special body joint. Quinn used his robots to test the idea that a cockroach's decision to clamber over or crawl under the shelf depends on whether its antenna first brushes the shelf's top or its underside. This is hard to prove definitively in the insects because many types of information or neural circuits might be involved in the decision. But a robot, which can be programmed with a set of algorithms so that it reacts only to information from the antenna, can reveal whether this sensory input is sufficient to direct its behaviour. Quinn's student William Lewinger added suitably programmed antennae to the Whegs, allowing the robot to detect a shelf's top or bottom and trigger it to climb or crouch respectively. As predicted, the robot behaved just like the real roach ( see video 9 ). Insect biologists are eager to model ever more intricate types of insect behaviour in their robots, such as walking uphill or climbing, and some robots with this hardware capability already skulk in other labs. One six-legged machine called RiSE, built by a team at Carnegie Mellon University, Pittsburgh, can even climb trees. \u201cRobots like these are going to help us work out increasingly complex movement behaviours in insects,\u201d says Robert Full at the University of California, Berkeley, whose studies on the mechanics of insects and other arthropods have inspired RiSE and other machines. But until these robots can be programmed with more sophisticated and autonomous software \u2014 precisely the directions that biologists are extracting from insect's brains \u2014 they cannot pass for true robotic insects ( see video 10 ). Insect biologists aren't the only ones anxious to tap into flies' intellect. Just a few of an insect's effortless navigational skills would be a boon for many of today's applied robots, which can negotiate obstacles only via human intervention and remote control. For this reason, space agencies such as NASA and the European Space Agency are watching developments in these insect labs with keen interest \u2014 and many robotics groups already include neurobiologists on their teams. \u201cWe think we may find it easier to go to higher control levels with neurobiological input,\u201d says Dirk Spenneberg of the University of Bremen, Germany, who works on autonomous robots including six- and eight-legged machines. Quinn, for example, says he is working with NASA to determine whether Whegs could work well on the lunar surface, and has other collaborations exploring their use for surveillance, and search and rescue. And nothing would make insect biologists happier than seeing future generations of Tarry lookalikes confidently striding the canyons of Mars. \n                     Robot sensors go touchy-feely \n                   \n                     Spirit finishes mountain marathon \n                   \n                     Animal behaviour: When robots go wild \n                   \n                     Scorpion robot could conquer worlds \n                   \n                     High speed biomechanics: Caught on camera \n                   \n                     Mars special \n                   \n                     Mission to Mars in focus \n                   \n                     Biologically Inspired Robotics Laboratory \n                   \n                     Poly-PEDAL Lab, UC Berkeley \n                   \n                     NASA Mars Exploration Rover Mission \n                   Reprints and Permissions"},
{"file_id": "445254a", "url": "https://www.nature.com/articles/445254a", "year": 2007, "authors": [{"name": "Mark Buchanan"}], "parsed_as_year": "2006_or_before", "body": "Statistics have the power to trip everyone up \u2014 including judges and juries. Even when extra care is taken to get the numbers right in court, confusion often reigns. Mark Buchanan reports. In March 2003, when nurse Lucia de Berk faced trial in a Dutch court for charges of murder and attempted murder, the statistical evidence against her seemed compelling. Investigators had identified a number of 'suspicious' deaths and near deaths in hospital wards in which de Berk had worked from 1999 to 2001, and records showed that she had been present when many of those events took place. The statistical expert testifying in the case, Henk Elffers, reported that the chance that her presence was mere coincidence was only 1 in 342 million. On the basis of this number and on limited forensic evidence \u2014 traces of toxic substances found in two of the exhumed bodies \u2014 the court found de Berk guilty, and sentenced her to life in prison. But some Dutch mathematicians now say that the figure cited was incorrect, and that the case is a classic example of how statistical reasoning can go horribly wrong. \u201cThe magical power of the big number led everyone at an early stage to be totally convinced of Lucia's guilt,\u201d says mathematician Richard Gill of the University of Leiden in the Netherlands. \u201cThen they went to work to confirm their theories.\u201d A court of appeal later upheld de Berk's conviction, but an advisory judicial committee has now been appointed by the central office of public prosecutors to reassess the case. The committee's decision is expected later this year, and it could recommend that the case be reopened. But whatever the result, the case illustrates the ongoing difficulty of ensuring that courts use statistical reasoning properly. \u201cThis is a serious problem,\u201d says statistics professor Philip Dawid at University College London, \u201cbut there is no easy solution.\u201d \n               Learning from experience \n             The case also raises questions over whether courts have learned anything from past misuse of statistics. In a high-profile British case in 1999, Sally Clark was convicted of murdering her two small children based, at least in part, on faulty statistical reasoning by an expert witness \u2014 the paediatrician Roy Meadow. Dawid was invited to submit written evidence on the statistical arguments during Clark's first appeal. He was not, however, allowed to present oral testimony. \u201cThe lawyers and the judge argued that it was not rocket science, so a statistical expert was not needed.\u201d The appeals court confirmed Clark's original conviction, but Dawid says that the written judgement showed that the jury had failed to grasp the relevant statistical issues. A second appeal in 2003 freed Clark, concluding that the jury might have been misled in part by the statistics presented in the original trial. Without a change in legal attitudes and procedures, mathematicians worry that statistical arguments will continue to be misinterpreted by the courts. In de Berk's case, Gill and another Dutch mathematician, Peter Gr\u00fcnwald of the National Research Institute for Mathematics and Computer Science in Amsterdam, have submitted letters to the judicial committee arguing that the original figure of 1 in 342 million was incorrect \u2014 or, at best, irrelevant to the proceedings. De Berk first became a suspect when working at the Juliana Children's Hospital in The Hague. But because observations from this ward were the source of the initial suspicion, Gill and Gr\u00fcnwald say, those observations should not have been used in a calculation that tested the validity of the suspicion. Elffers, of the Netherlands Institute for the Study of Crime and Law Enforcement in Leiden, combined the Juliana data with data from another hospital in which de Berk had previously worked to get his figure. Gill and Gr\u00fcnwald insist that the analysis was misleading. \u201cIt makes little sense to do formal hypothesis testing when the data themselves have suggested the hypothesis,\u201d says Gill. \u201cThe only safe thing is to go out and collect new independent data.\u201d Gill's own calculation estimates that the probability that the correlation arose by chance is not 1 in 342 million, but a much smaller 1 in 48, or even as low as 1 in 5 \u2014 figures that are unlikely to meet the 'beyond reasonable doubt' needed for a criminal conviction. But Elffers defends his original calculations, arguing that he applied a factor that corrected for his using some of the data twice. \u201cEveryone is aware that I applied a correction,\u201d he says. \n               Fact or fallacy? \n             Aside from this debate, equally important is how the court interpreted the number. Philosopher of science Ton Derksen of the University of Nijmegen, who has written a book that criticizes de Berk's conviction, argues that the court made an elementary statistical error known as the prosecutor's fallacy. The court needs to weigh up two different explanations: murder or coincidence. The argument that the deaths were unlikely to have occurred by chance (whether 1 in 48 or 1 in 342 million) is not that meaningful on its own \u2014 for instance, the probability that ten murders would occur in the same hospital might be even more unlikely. What matters is the relative likelihood of the two explanations. However, the court was given an estimate for only the first scenario. Without additional information, says Derksen, Elffer's number is meaningless \u2014 and could easily be misinterpreted as a very small chance that de Berk is innocent. To raise further doubt, other important statistics were neglected by the court. When de Berk worked at Juliana between 1999 and 2001, there were six unexplained deaths in her unit. The same unit, in a similar period before de Berk started working there, had seven unexplained deaths. \u201cIt seems very strange,\u201d says Gr\u00fcnwald, \u201cthat fewer people die when there is a serial killer around.\u201d Derksen says that the statistics comparing deaths before and after de Berk started work at the hospital were mentioned by her defence lawyers, but were not sufficiently emphasized to have any influence on the court. \n               Due process \n             This neglect illustrates a difference between legal and scientific processes. Although science aims to bring together all relevant evidence, this is not necessarily true with the law. David Kaye, an expert in statistics and the law at Arizona State University in Tempe, notes that lawyers have an incentive, and even a duty, to select the evidence that makes their case stronger. \u201cWhat the judge ends up hearing often comes from the two extreme ends of the distribution,\u201d he says. Procedures to correct such distortions are also lacking, even after a trial has reached a verdict. In the United States, written statistical arguments are often protected by court orders, and so are not available for review or correction. \u201cThe data pertaining to an individual deserve some protection,\u201d says statistical expert Joseph Gastwirth of George Washington University in Washington DC, but a summary of the expert reports should be made publicly available, he suggests. Independent scientific comment of this kind occurred during the Clark case, but to unknown effect. In the 1999 Clark trial, Meadow testified that the chance of two infants from the same mother dying of Sudden Infant Death Syndrome (SIDS) was only 1 in 73 million. Two years later, after the first appeal, the Royal Statistical Society in London, condemned both this figure and its interpretation. The figure would be valid only if SIDS cases arise independently within families, the statement said, whereas there may be unknown genetic or environmental factors that predispose families to SIDS. After the Clark case, the society established a working group, chaired by statistician Colin Aitken of the University of Edinburgh, to examine how courtroom use of statistics might be improved. In the immediate future, the society hopes to help provide continuing education to practising lawyers, judges and other legal practitioners, so that they can at least recognize the potential hazards in statistical reasoning. Ultimately, Aitken suggests, including statistics in the core curricula of law degrees will be more effective. \u201cThings will not improve overnight,\u201d says Aitken, \u201cbut we are in it for the long haul.\u201d \n               A matter of opinion \n             In the United States, the Federal Judicial Center, an organization created by Congress to improve federal courts, has published a reference manual on the use of scientific evidence, with one chapter devoted to statistics. \u201cBut education is only palliative,\u201d says Kaye, who helped to write the statistics chapter. \u201cI don't think there is any single way to ensure that statistics and other scientific evidence gets used accurately.\u201d Although courts expect one simple answer, statisticians know that the result depends on how questions are framed and on assumptions tucked into the analysis, giving tremendous room for legal argument. Kaye recalls giving lawyers in one case what he thought was a crystal-clear explanation of a statistical argument. \u201cTheir response was 'Let's just put him on the stand, he will confuse everyone'.\u201d Indeed, the biggest practical challenge, some argue, lies in the unusually subtle nature of statistical reasoning, which research shows confuses experienced professionals, such as physicians, just as easily as the general public. In de Berk's case, even Elffers now suggests that with so much uncertainty swirling around the statistics, they should play no further role in the considerations. \u201cThe review committee should concentrate on non-statistical arguments,\u201d he says. Those arguments are also a matter of dispute. Until the committee decides, de Berk languishes in prison, quite possibly because of mathematical errors. \u201cI am so convinced that the statistics are wrong,\u201d says Gill, \u201cthat I am more inclined to believe in the incompetence of the entire process than in the existence of a serial killer.\u201d \n                     Quashed convictions reignite row over British cot deaths \n                   \n                     Statistics don't support cot-death murder theory \n                   \n                     Richard Gill's argument \n                   \n                     Sally Clark's campaign \n                   \n                     News feature on Sally Clark's case \n                   \n                     Royal Statistical Society \n                   \n                     Evidence \n                   \n                     Science and Law blog \n                   \n                     Federal Judicial Center \n                   Reprints and Permissions"},
{"file_id": "445014a", "url": "https://www.nature.com/articles/445014a", "year": 2007, "authors": [{"name": "Alexandra Witze"}], "parsed_as_year": "2006_or_before", "body": "Optimists see oil gushing for decades; pessimists see the planet's energy future already drying up. Alexandra Witze reports. Don't say they didn't warn us. The poster for the meeting of the Association for the Study of Peak Oil and Gas in Boston this October featured American revolutionary Paul Revere on his midnight ride, bringing news of imminent calamity. Only this time it is not the British who are coming, but the end of the oil era, and with it much of western civilization. Many attendees at the meeting were people who could tell you how to stock a bunker to survive the inevitable collapse of civilization, and then opine at length about the extent and characteristics of the great tar-sand deposits of Canada. Some of them conduct a thriving mini-business in preparing for the coming apocalypse \u2014 \u201cdeal with reality or reality will deal with you\u201d, as one website claims \u2014 while scrutinizing table after table of data on world oil production. But this is not an easily dismissed fringe. Respected geologists with lifetimes of experience are genuinely concerned that the world is about to see an unprecedented crisis \u2014 a reduction in the supply of a primary fuel before an alternative is available. When we moved from wood to coal, it was not for a shortage of forests; when we moved in large part from coal to oil and gas, it wasn't because the pits were empty. But many people are convinced that the flow of oil is destined to start falling, and soon. Matthew Simmons, an energy investment banker in Houston, Texas, and self-described \u201cpetro-pessimist\u201d, argues that the world's great oilfields are moving quickly towards the end of their production, or have already passed into rapid decline. The North Sea, for instance, is the only place that a significant new discovery has been made outside of nations in the Organization of the Petroleum Exporting Countries (OPEC), Russia and Alaska in the past four decades. It is now in eclipse \u2014 production in the region peaked in 1999, which is earlier, Simmons says, than expected. The United Kingdom no longer exports oil, he notes, and production in Norway \u2014 the North Sea's long-term stalwart \u2014 is also declining. And no new giant oilfields are taking the place of those that have already passed their peaks, says Simmons. Some people think that the declines we are seeing are indicators that the world is on the verge of, or has already passed, the maximum amount of oil that can physically be produced. In their view, oil production follows a bell-shaped trajectory, with the peak occurring when half of the total reserves have been consumed. Therefore, it should, in theory, be easy to determine whether the peak has already occurred or whether it is yet to come. Total up the world's oil reserves, estimate the rate at which countries have produced oil, and you'll know where you are in the trajectory. If the reserves are more or less equal to the amount already pumped, then production is at its peak. If you accept this principle, then the issue of when the peak comes depends mainly on the amount of reserves that remain untapped, and that in itself gives room for disagreement. But some don't accept these premises. To them, these arguments are simplistic geological determinism that does not take into account the role of oil prices. To the dissenters, reserves are not a geological given but a function of the current price and the extraction technology that price can buy. New reserves will be developed as the market demands. \n               Opposites attract \n             Both sides issue regular, well-referenced reports that come to opposite conclusions on whether the world is running out of oil. \u201cThere's just no middle ground,\u201d says Kenneth Deffeyes, a geologist who has retired from Princeton University in New Jersey, and is a leading supporter of the peak-oil theory. His personal belief is that we are already a year past the peak. If he's wrong, though, he's sure that it will prove not to be by much: the peak is imminent, and unavoidable. Meanwhile, a study from energy analysts Cambridge Energy Research Associates (CERA) in Massachusetts sees no sign of any peak in production occurring before 2030. And, crucially, CERA doesn't see a peak with a steep downside \u2014 rather a crest followed by an undulating plateau (see chart), which would be much less apocalyptic even if it happened today. It's not that CERA thinks that oil production has no constraints, or that the geological resource can't be depleted. Even oil company executives say publicly that they see a problem. T. Boone Pickens, the maverick Texas oil magnate, has said that he thinks oil production may already have reached its maximum. And in October, during an address to the National Press Club in Washington DC, Shell Oil president John Hofmeister acknowledged that \u201cthe easy stuff is running out\u201d. \u201cWe may argue about when the peak is, but it doesn't change the argument that it's coming,\u201d says Robert Kaufmann, an energy economist at Boston University in Massachusetts. \u201cI don't know if we are all Hubbertists now, but we are all recognizing that there is a finite quantity of oil.\u201d Hubbert, in this context, is M. King Hubbert, the geophysicist who first predicted that oil production would peak quite suddenly \u2014 and that when it did, it would slump sharply thereafter. In 1956, while working in Shell Oil's research laboratory in Houston, Texas, Hubbert predicted 1  that oil production in the contiguous 48 states of the United States would peak in the early 1970s. Hubbert's calculations produce a bell curve to describe the rate of oil production, with a sharp rise on one side of the peak and a symmetrical drop-off on the other (see chart). At the time Hubbert made these calculations less than half of this two-sided curve had been seen. Oil exploration and discovery were booming, and Hubbert's prediction looked implausibly pessimistic. But he turned out to be right; production in the contiguous United States reached its peak in 1970, and almost overnight Hubbert gained his own personal fan club. Those in favour of the peak-oil theory argue that Hubbert's methods for analyzing US oil output can also be used to analyze the global production peak. Deffeyes, known to many as the charismatic protagonist of  Basin and Range , the first of John McPhee's great popular accounts of modern geology, has emerged as a particularly prominent Hubbertist. In  Hubbert's Peak: The Impending Oil Shortage 2  and  Beyond Oil: The View from Hubbert's Peak 3  he used the same mathematics as Hubbert to calculate the total oil reserves worldwide that remain to be produced. With a flourish of exactitude, Deffeyes estimated that the world reached peak-oil production on 24 November 2005. He later pushed this back \u2014 but only to 16 December of that year. \u201cI'm not declaring victory just yet,\u201d he says hastily. He's saving the victory announcement until he sees world production numbers drop for three years in a row. \u201cIf I'm right, it will be very transparent in five years.\u201d \n               Changing with the times \n             But not everyone buys Deffeyes' interpretations. \u201cThe technique that Hubbert used in the 1950s is simply not applicable on a global basis in 2006,\u201d argues Peter Jackson, an oil and gas analyst who works for CERA near London, UK. Jackson authored CERA's background briefing paper \u201cWhy The Peak Oil Theory Falls Down\u201d in November 2006. Jackson notes that proponents of the peak-oil theory have changed their dates several times before; as production numbers come in year after year, for instance, leading peak-oil theorist Colin Campbell has postponed his estimates for the peak in all hydrocarbon production from around the turn of the millennium to 2010. Others point out that predictions of an unavoidable slump are almost as old as the oil business; John Strong Newberry, chief geologist of the state of Ohio, was predicting that America's oil would soon be tapped out back in 1875. Jackson, Deffeyes and everyone else in the debate agree that nearly 1.1 trillion barrels (175 trillion litres) of oil have been produced worldwide. A key difference is that supporters of the peak-oil theory argue that roughly that same amount remains to be pumped out of the earth's reserves, whereas CERA's report estimates those reserves at 3.7 trillion barrels, a number that would place the world well on this side of any peak in production. One reason for the difference is that CERA is much more optimistic about the amount of oil that can be recovered from operating oilfields through the use of new technologies. Jackson points out that the expected recovery estimates for operating oilfields often grow with time. The total estimate of recoverable oil from the North Slope of Alaska, for instance, used to be 9.6 billion barrels; today it is 13.7 billion barrels. \u201cIf I were to say we are not finding enough oil every year through exploration to replace what we are producing, you would be alarmed,\u201d says Jackson. \u201cAnd that is correct. But peak-oil supporters don't talk about field reserve upgrades \u2014 in a lot of the producing fields around the world, companies are constantly updating estimates of reservoir reserves.\u201d Reserves change in size even after the initial geological mapping of an oilfield because the amount of oil that's recoverable \u2014 and thus deserves to be counted as reserves \u2014 depends on the skill of the oil companies and the effort they are willing to put in. Globally, about 35% of the oil present in established fields is actually produced. Nearly every oilfield matures through the same sequence: first, the easily recovered oil is extracted through traditional drilling. When that runs low, engineers begin a process of secondary extraction, using techniques such as injecting water or carbon dioxide to drive more oil out of the rock. Many fields also undergo tertiary extraction to squeeze out yet more oil, usually by injecting steam to lower the viscosity of the oil. Oil wells are abandoned once the cost of extraction is no longer worth it. \u201cWe will still have oil in 100 million years,\u201d says David Hughes, a geologist with the Geological Survey of Canada in Calgary. \u201cIt just won't be recoverable at an energy profit.\u201d But the fluctuating price of oil means that fields abandoned after secondary production can be re-opened for tertiary production when demand calls. The current high price of oil \u2014 just over US$60 a barrel \u2014 provides an incentive for companies to start tapping into their reserves and pushing into more areas for discovery. In general, those against the peak-oil theory claim that the Hubbert-curve approach underestimates changes in extraction technology brought about by both natural developments and changes in the price of oil, which can turn fields that are too costly to pump from into valid reserves. As the great oilfields of the world age \u2014 most of them are now undergoing secondary, if not tertiary, extraction \u2014 other discoveries could help to plug the supply gap, argues Jackson. In 2000, an analysis by the US Geological Survey of petroleum reserves estimated that there were 1 trillion more barrels of oil worldwide than previously thought. The survey estimated that any worldwide peak in oil production wouldn't happen until 2030 at the earliest. In part as a result of the high prices of the past few years, roughly 500 oil-development projects are slated to start producing oil in the next five years, says Jackson, and these will range in size from an estimated million barrels per day down to 10,000 barrels per day. \n               In the deep \n             Energy optimists point to recent discoveries such as the seven new oilfields uncovered in the deep waters of the Gulf of Mexico last year \u2014 reserves that are only accessible because of new technology. The biggest oilfield found there to date, the Thunder Horse field, is estimated to contain 300 million barrels of oil. Such exploration may also soon be helped by the US Congress, which voted on its last day of session in 2006 to approve expanded drilling into previously off-limits areas in the Gulf of Mexico. \u201cThere's a lot of new capacity coming to the market,\u201d says Jackson. \u201cThat's one of the reasons I'm not too concerned about a peak.\u201d Another reason for doubt is that Hubbert's model was not perfect even when applied just to the contiguous 48 states of the United States \u2014 its great claim to fame. Although the date predicted for the peak was roughly correct, the model predicted an amount of production at the peak that was 20% less than reality. And, in part because of unforeseen discoveries in the Gulf of Mexico, the amount of oil produced in the United States after the peak was much greater than Hubbert had predicted. But the peak-oil theorists are not convinced. \u201cThe problem is, if you go and talk to people whose job it is to actually go and find this stuff, they have no clue as to where these trillion barrels of reserves actually are,\u201d says Michael Rodgers of the energy analysis firm PFC Energy in Washington DC. \n               Who to trust? \n             Estimates of reserves that are published by oil companies, national governments and researchers such as the US Geological Survey are not to be trusted, according to the peak-oil supporters. Jeremy Gilbert, former chief petroleum engineer for BP, says that not all oil companies work to the same standards. The US Securities and Exchange Commission sets rules for how to report reservoir estimates, but only US and major international companies generally abide by those standards \u2014 and they don't always do so reliably. \u201cThe standards for other national companies are unknown,\u201d Gilbert says. \u201cIf someone tells you the reserves in Kuwait are 75 billion barrels, he has no idea how that 75 billion was calculated.\u201d Peak-oil supporters are eager to point out that after a sharp drop in the oil price in the mid-1980s the estimated reserves of various OPEC countries \u2014 including Iran and Iraq, which had a mutual war to finance at the time \u2014 were jacked up by their governments. Partly as a result of such manoeuvres, Simmons is particularly pessimistic about whether OPEC nations can continue to slake the world's thirst for oil. Currently, OPEC countries provide about 40% of the world's oil. Non-OPEC countries have been consistently producing more oil than they've been finding since the late 1980s, Rodgers told the meeting in Boston. He thinks that production from non-OPEC countries will peak between 2010 and 2015; after that, no amount of OPEC production can make up the gap between supply and the world's growing demand. The fact that such predictions have been made before does not necessarily mean that they are wrong now. Most of the attention on OPEC focuses on Saudi Arabia, by far the biggest producer and the driver of oil prices worldwide. The country gets most of its oil from seven giant maturing oilfields. The three biggest fields have been producing oil for more than 50 years, and the oil industry constantly swirls with rumours that the biggest of all \u2014 the Ghawar field \u2014 has been increasingly cut with water to drive its production even higher. Simmons, after studying technical reports published by the Society of Petroleum Engineers, has argued that the state-run oil company Saudi Aramco routinely overestimates the country's oil reserves. Saudi Arabia, he argues, is closer to running out of oil than most people think. The 1970s oil crisis, when OPEC slapped an embargo on countries that had supported Israel, was only a taste of things to come, says Rodgers. At the time, the United States was the only country that had already peaked in oil production. Now, many more countries \u2014 including Peru, Argentina, Norway, Congo and Mexico \u2014 have also passed their peak. Countries such as Canada, China, Brunei and Malaysia are currently undulating around a plateau of oil production and could soon decline, he predicts. New discoveries, such as in the Mexican section of the Gulf of Mexico or in Angola or Brazil, could change the date of an imminent oil peak, says Rodgers, but only slightly. \u201cAll you can do is take the cliff facing us in the next few years, and push it farther out over time. This has already happened.\u201d Fields of the size now being discovered, though, will not be large enough to push the peak back far. The 300 million barrels at Thunder Horse provide less than a week's consumption at the world's current rate of use. Another way Russia helped out in the 1990s, Simmons points out, was by producing less oil than had been expected. That apparently helpful drop, though, was an exception. \u201c[Misstated reserves] wouldn't be so bad if demand remained steady,\u201d says Simmons, \u201cbut instead it soared.\u201d The world produced 16% more oil in 2005 than it did in 1990 \u2014 and none of that production, Simmons argues, came from any major new discovery. Instead, it came from a compilation of much more incremental discoveries and increased production from a number of countries. Meanwhile, demand is expected to continue to grow as more and more families buy cars worldwide. \n               A dipstick for the future \n             Some of the oil production to meet this future demand may come from alternative approaches \u2014 extracting oil from oil shale, for example, or liquefying natural gas or coal for fuel (see  Nature 444, 677\u2013678; 2006 ). These 'unconventional' sources of oil are some of the reasons that the CERA outlook is so optimistic. But many of the other sources, say peak-oil supporters, are not good alternatives \u2014 certainly not the sort of thing that can easily be used in vast quantities as a replacement for sweet Saudi crude, a high-quality oil with a low sulphur content, even if the price is right. In Alberta, Canada, geologists have focused on extracting oil from tar sands which could, in theory, help supply the world's needs for decades. But the process is expensive, both financially and environmentally; three barrels of water, for instance, are needed to produce each barrel of oil from the sands, and the production releases large amounts of greenhouse gases. Thinking along those lines raises a parallel question: can we afford, in environmental terms, to put the peak off, and to keep turning oil into atmospheric carbon dioxide at an ever-increasing rate? From an environmental point of view, a peak might almost be welcome. If the subsequent rapid drop in production crashed the world economy, though \u2014 in the way that peak-oil supporters fear \u2014 those benefits might be hard to appreciate. What's more, the resources needed to develop the alternatives on which economic recovery would depend might not be available. Those problems might be lessened if the peak could somehow be predicted. But Kaufmann, the economist, says not to expect any financial or other indicators. Oil prices didn't rise sharply or otherwise indicate an imminent depletion of US oil resources just before the peak in 1970, he says, mainly because the cost of production was staying stable. To predict peak oil in advance, \u201cyou need some kind of nice price signal,\u201d he says, \u201cand we don't see any of those signs yet.\u201d One place to look for such signals might conceivably be in the prices for which oil is bought and sold on the futures market. And at the moment, the New York mercantile exchange is settling on prices around US$67 a barrel. It's a price high enough to make alternative fuels interesting, but in real terms not remarkably high compared with long-term averages. If oil production does start to collapse, peak-oil supporters who want to stock their bunkers with luxury goods have the opportunity to make a killing, by buying tomorrow's oil comparatively cheap and selling it, when the time comes, much more dearly. If, that is, the time does actually come. Alexandra Witze is Nature's Chief of Correspondents, America. \n                     Liquid fuel synthesis: Making it up as you go along \n                   \n                     Energy: China's burning ambition \n                   \n                     Oil exploration: Every last drop \n                   \n                     Energy for a cool planet Web Focus \n                   \n                     US Geological Survey's World Petroleum Assessment from 2000 \n                   \n                     National Academy of Sciences report on peak oil \n                   \n                     Simmons International \n                   \n                     Ken Deffeyes \n                   \n                     Association for the Study of Peak Oil \n                   \n                     Cambridge Energy Research Associates \n                   Reprints and Permissions"},
{"file_id": "445018a", "url": "https://www.nature.com/articles/445018a", "year": 2007, "authors": [{"name": "Jim Giles"}], "parsed_as_year": "2006_or_before", "body": "Manipulating society has traditionally been the preserve of politicians and the gods. Does the current boom in virtual worlds give social scientists and economists an opportunity to join them? Jim Giles investigates. Is a ruthless dictatorship a better way of running a country than a well-oiled democracy? Would people be happier if all their property was confiscated? Might our economies be healthier if inflation ran at 100%? We're sure we know the answers. Kim Jong-il's reign in North Korea is a brutal one. Hyperinflation is wrecking Zimbabwe. History tells us that stealing land prompts wars, not happiness. By comparing these and other examples, we can be confident that each proposal is, well, foolish. But how would an experimental scientist view this kind of knowledge? Many physicists answer questions by running experiments, tweaking conditions and recording the results. They do so repeatedly, until the role of every variable is understood. Case studies are the beginning of the process, not the end. And physical laws are not limited to observations of what approach seems to work best; they can produce complex models that allow predictions of the future. The difference in approach is no reflection on the abilities of social scientists. It's just that societies are not so amenable to experimentation. Government economists would not be popular if they repeatedly tweaked monetary policy just to see what happened. Social scientists learn from history. They run surveys. They even conduct small experiments with a handful of subjects in idealized conditions. What they can't do is manipulate the system they are studying. Enter Edward Castronova, an economist at Indiana University in Bloomington, occasional writer of fiction and an expert on multiplayer online games. His vision is nothing if not ambitious: to create societies with the intention of experimenting on them. The societies will exist only inside computers, with real people living some of their lives through characters (avatars) in these virtual worlds. But Castronova does not intend to merely simulate real life. He and other researchers want to tweak the rules of those worlds so they can study everything from democracy to monetary policy. Such tools, says Castronova, would be the \u201csupercolliders\u201d for his field. They would usher in an era of \u201ccomputational social science\u201d. \n               Parallel worlds \n             If that sounds unlikely, visit Azeroth. You'll have plenty of company: more than seven million people play World of Warcraft, the online role-playing game that is based in this virtual realm. The game, run by Blizzard Entertainment of Irvine, California, is pure axe-wielding tolkienian fantasy: players go on quests and slay mythical beasts, amassing virtual wealth and power in the process. Nevertheless, Azeroth society shares many features with our own. Players produce goods and trade them; they cooperate to achieve goals but also conduct personal vendettas; some are unhelpful and rude, others \u2014 admittedly not many \u2014 display altruism. Elsewhere in cyberspace, economies are springing up that, superficially at least, seem to mirror the real world. Cyber-capitalism is most apparent in the malls of Second Life, an online world that is visited by around 10,000 people a day. The world's designers, Linden Lab of San Francisco, have eschewed fantasy quests and given users the tools and virtual land they need to build their own online experience. Since Second Life's launch in 2003, universities have leased space to teach classes, and high-street retailers have set up outlets where avatars can get their hair cut or buy virtual roller-skates. The in-world currency now floats against the dollar on dedicated exchanges, and one user claimed last November that her Second Life property business has made her a real-world millionaire. Virtual worlds also show similarities to real life at the level of one-on-one social interactions (despite many players choosing outlandish avatars such as giant mice). In a paper in press with  CyberPsychology & Behavior , Nick Yee at Stanford University, California, shows that some of the unwritten rules of real-life socializing appear to cross over into Second Life, even though communication is purely text based. Male avatars stand further apart than females when talking, for instance, and tend to make less eye contact. If these cyber-societies follow some of the rules of real life, what can social scientists do with them? Many want to treat them like a biologist treats cells in a Petri dish. Tinker with monetary policy. Rewrite the rules of democracy. Force players to work together, or try and drive them apart. But conduct each intervention in a systematic way, holding all other variables constant and rigorously monitoring the outcomes. Thanks to the huge audiences that online games attract, social scientists will then be manipulating society itself. Although a full \u201csupercollider\u201d experiment has not yet been done, tentative steps have been taken, and the results suggest that the idea has merit. \n               Location, location \n             First up was a study of a seemingly trivial question: why are certain businesses based in specific places? The answer is also apparently trivial: they are there because we say they are. It makes sense for actors and film producers to agree that movies are made in Hollywood and Mumbai. Anyone who wants to be in the film business then knows where to go. There is nothing that compels movie stars to behave in this way, except that everyone involved agrees on the location \u2014 what social scientists call a 'coordination effect'. At least that's one theory. One can also argue that there is something special about Hollywood or Mumbai that means that movies get made there, for example, and that coordination effects are less important. It has been hard to settle the argument either way, which undermines attempts by economists to explain how societies reach such agreements. That then hampers our understanding of bigger questions, such as how cooperative behaviour evolved to become such a prominent feature of everyday life. This uncertainty persists in part because social scientists have only limited ways of studying coordination effects in real life. Distinguishing between rival explanations is tricky because we can't re-run history and, for example, probe the factors that drew movie moguls to California in the first place. Except that now we can, because some virtual worlds regularly recreate themselves to cope with overcrowding. Take Norrath, the land in the fantasy game EverQuest. After launching in 1999, EverQuest's blend of trolls and spells proved so popular that Sony Online Entertainment, which runs the game, created additional copies of Norrath on multiple servers. There are currently 26 servers running. And each time Sony creates another world, a new society evolves under almost exactly identical starting conditions. This cloning of worlds gave Castronova his first chance to do computational social science. Using a survey of EverQuest players (E. Castronova  Games and Culture   1,  163\u2013186; 2006), he showed that on each server just one region has become established as a market. Crucially, that region differs between servers, although mountain ranges and cities have identical locations in all the worlds. So there does not seem to be a single prime location for the market; instead, some chance event seeds its creation, and coordination effects then lock it into place. \u201cWith no small amount of trepidation,\u201d Castronova writes in a footnote to the paper, \u201cI would venture to claim that this is the first time in human history that a distinct macro-social phenomenon has actually been verified experimentally.\u201d For Castronova, this result hints at what could be achieved. But other social scientists are more sceptical. When  Nature  contacted authors of some highly cited papers on coordination effects, they expressed interest in Castronova's approach, but none had yet read his paper. After doing so, they highlighted various shortcomings they saw in the study. Some took issue with the paper's more grandiose statements. Castronova's \u201ctrepidation\u201d is well justified, says Andrew Colman, a game theorist at the University of Leicester, UK. Colman's laboratory studies of coordination effects, along with other work, had indicated their importance before Castronova's study. And Colman's studies involve more than 80 students \u2014 around the same number of players from each server that responded to Castronova's survey. Other researchers questioned whether the worlds that Castronova studied are truly independent, as it would be possible for players to communicate outside the game. Castronova acknowledges that lab work has already shown that coordination effects are real, and says that although his study needed just a few survey respondents to identify the market location, the coordination effect resulted from the interaction of hundreds of thousands of people. \u201cIt's a dramatic difference in terms of scale and complexity.\u201d Researchers might, for example, be interested in how coordination effects are influenced by information campaigns run by governments \u2014 an issue that is beyond the scope of lab work. But the biggest criticism of this approach is one that all virtual-world experiments will have to face: \u201cThe number-one challenge is generalizability to the real world,\u201d says Dmitri Williams, a social scientist at the University of Illinois at Urbana-Champaign, who is credited along with Castronova for developing the idea of using virtual worlds to study real society. To illustrate the problem, Williams points to a 'plague' that broke out in World of Warcraft in summer 2005. At first, says Williams, researchers proposed that the spread of the disease could be used as a model for real-world epidemiology. But it soon became clear that some players were deliberately infecting others, because, unlike in real life, they suffered little penalty for doing so. Players are also much more willing to risk death online than they are in the real world. \u201cThe risks don't match up,\u201d says Williams, \u201cso we have to be very careful about generalizing our conclusions.\u201d Nevertheless, Williams is keen to use virtual worlds to examine theories about how communities form. Like many of those working on virtual worlds, he feels that data from these experiments complement rather than replace those from traditional social-science studies. Castronova agrees, adding that watching mice in mazes does not reveal everything about how the human brain works, but it does give researchers another way of studying the issue. For now, the only 'mice' that Castronova and others can study are those playing games such as EverQuest. The designers have often welcomed researchers who want to study cyber-societies, but this partnership can go only so far. Game companies cannot be expected to mess with their profitable products to please social scientists, so to realize his goal, Castronova needs to build his own world to rule over. That world, or at least a prototype, should be here soon. Using a one-year grant of $250,000 from the John D. and Catherine T. MacArthur Foundation, based in Chicago, Illinois, Castronova and his colleagues are building the Shakespeare-themed world of Arden. Fifteenth-century maps of the English county of Somerset will define a landscape over which blacksmiths, tavern-keepers and bards will roam. A test version should be up and running by March, and its results might enable Castronova to attract the funding needed to build a fully functional version. Ultimately, he wants around 500 people to play for 100 hours per month each \u2014 enough to provide a functioning economy. The players will be there because it's fun. To run economic experiments of interest to Castronova, Arden will need to develop an economy that features aspects of the real world, such as inflation. By building different monetary conditions into different versions of Arden, Castronova will be able to test aspects of economic theory such as supply and demand. With two versions of Arden with different prices for a particular good, theory says that demand should be higher in the world where the good costs less. This is just an example, as Castronova will not reveal exactly what experiment he is planning for fear of invalidating the study. Arden could in principle be used to test any idea that interests social scientists. One might examine how laws influence individual behaviour, suggests law researcher Dan Hunter of the University of Pennsylvania in Philadelphia. Others want to test new forms of democratic participation or assumptions in marketing theory. \n               Reality check \n             Enthusiasm is no promise of success, however. Perhaps the most immediate problem facing Castronova's team, if they secure longer-term funding, will be game design. Sony's multimillion-dollar budget buys teams of experienced game designers. Castronova has around 30 committed but less qualified members of his university. He must also balance the need to design a world that he can control for his research with the need to create an experience that is enjoyable enough to get people coming back. No university team has built a large-scale online game for research purposes before, and there is no guarantee of success. Castronova will also have to grapple with the sometimes unanticipated ethical problems that arise in online research. In one study of the playing habits of online gamers, a group of US-based social scientists collected anonymized data from a game company. But researchers who were regular game players soon realized they had enough information to link avatars to people they knew. Other problems may emerge as games become more popular. There is real money at stake in many games, and players have been known to contact the police over thefts of cyber-weapons. Greg Lastowka, at Rutgers School of Law in Camden, New Jersey, says that players may increasingly take real-world legal action to remedy online crimes, undermining the chances of testing new legal systems in the worlds themselves. Computational social science is yet to prove itself as an established research method. Right now, admits Castronova, many social scientists dismiss such studies because they feature fantastical avatars in mythical quests. And to prove them wrong, solid results are needed. But as befits a man on a mission, Castronova is confident that he can provide them: \u201cThey'll believe us when we start showing it's possible\u201d. Jim Giles is a reporter for Nature based in London. \n                     Concept of 'personal space' survives in virtual reality \n                   \n                     Is it small or just far away? \n                   \n                     Archaeology: Digital digs \n                   \n                     Virtual-reality mummy \n                   \n                     Second Life \n                   \n                     EverQuest \n                   \n                     World of Warcraft \n                   \n                     Castronova's EverQuest survey \n                   \n                     Castronova's homepage \n                   Reprints and Permissions"},
{"file_id": "445356a", "url": "https://www.nature.com/articles/445356a", "year": 2007, "authors": [{"name": "Michael Cherry"}], "parsed_as_year": "2006_or_before", "body": "Next week, African leaders will come together to talk about science and technology at a summit in Ethiopia. This presents an opportunity to allot some foreign aid and, if they get it right, to launch projects that will draw further donations from abroad, says Michael Cherry. If asked to name Africa's highest priorities, most people would cite poverty, disease and conflict, not science and technology. But at next week's summit of African leaders in the ancient Ethiopian capital of Addis Ababa, science and technology research for African development is top of the agenda. \u201cThe challenge,\u201d says South African director-general of science and technology Phil Mjwara, \u201cis to impress upon the heads of state that science and technology is of such critical importance to the continent's development that it should be a policy priority for every African nation.\u201d For many, this meeting of the five-year-old African Union (AU) offers a historic opportunity. David King, chief scientific adviser to the British government, argues that, apart from boosting the economy and creating employment opportunities, \u201cthe important point about investing in science and technology is that it raises the level of aspiration throughout the educational system.\u201d Indeed, some now view science and technology as a vehicle for spending foreign aid wisely. Eyes in the developed world as well as in Africa will be watching the summit keenly, especially because the new United Nations secretary-general, Ban Ki-moon, is scheduled to put in an appearance. Africa's leaders could use this summit as an opportunity both to convey their own financial commitment to the development of science and technology, and to begin work on detailed plans that foreign donors can support. During the past 18 months the world's rich nations have expressed renewed interest in addressing Africa's fate. In July 2005, the Gleneagles summit committed the G8 countries both to eliminating debt and to providing a significant injection of aid. This renewed interest in Africa is accompanied by the realization that past aid was often squandered, either on corrupt officials in developing countries or on foreign consultants. The latter problem can be addressed by removing restrictions on where aid is spent; spending new money wisely is a challenge that Africa must meet, and the hope is that science and technology offers one route to doing so. The AU has been gearing up to this summit over the past two years, charging its ministerial council on science and technology (AMCOST) to come up with a wish list of appropriate research and development for the continent \u2014 the Consolidated Plan of Action. The list of research clusters covers 12 flagship projects, and with the exception of space science, all are targeted at fairly obvious African concerns (see  'Africa's 12-point plan' ). Surprisingly, biomedical science is omitted, but according to Botlhale Tema, the AU's director of human resources, science and technology, this is not because the AU is unconcerned about research on human disease, but because it falls under a different department (social affairs) of the AU. Mjwara is positive about the plan: \u201cAll of the proposed projects are in fields in which there is already existing momentum based on at least some local expertise.\u201d The more difficult issue of implementation is largely unresolved. A small meeting of African scientists in Alexandria, Egypt, last October produced some very vague recommendations, and a further meeting of science ministers in Cairo in November made some suggestions for implementing the Consolidated Plan of Action, embodied in the Cairo declaration. \u201cMore important than the actual recommendations that came out of either meeting is the fact that they took place at all,\u201d says Andy Cherry, science and technology adviser at the Association of Commonwealth Universities in London, who attended both meetings as an observer. The Cairo declaration, which is likely to be ratified by the leaders at the Addis summit, contains recommendations on potential funding mechanisms and 'centres of excellence' for moving the plan forward, as well as possible agreements on biotechnology and biosafety. Biotechnology is one area on which the summit could conceivably reach agreement, because a fairly detailed report on both priorities for biotechnology research, and biosafety measures for genetically modified (GM) organisms, will be on the table. \n               Increasing integration \n             Kenyan Calestous Juma of Harvard University, a member of the panel that compiled the report, hopes that biotechnology development and regulation can proceed hand-in-hand. He believes the initial focus should be on making effective use of products that are relevant to local needs and are ready for commercialization, including techniques for disease control, pest tolerance and weed management. \u201cI will consider the summit a success if a handful of leaders return home emboldened to champion the role of technological innovation in developing their country and region,\u201d he says. The report also encourages African nations to abandon their individual policies on GM crops in favour of a common, or at least regional, stance. South Africa and Kenya, for example, have promoted the introduction of GM crops such as corn (maize), cotton and soya, whereas Zambia has banned all such crops. Until 2004, the European Union had a five-year moratorium on GM imports, and getting approval for new products is still difficult and time-consuming. As Europe is the main importer of African agricultural products, and because most African nations lack facilities for separating GM and normal crops, many have been reluctant to pursue the technology. Because Africa is much less integrated than the European Union, with economic integration existing at a regional, but not a continental, level, Juma thinks regional consensus on biotechnology and biosafety is most likely. \u201cIf biotechnology development and regulation are to go hand in hand, then the AU needs to develop a strategy broad enough to empower regional economic communities to take differing positions,\u201d he says. \n               Fund management \n             On the thorny issue of funding mechanisms, the Cairo declaration backed the establishment of an African Science and Innovation fund to take forward the research themes proposed in the Consolidated Plan of Action. As an intergovernmental entity, under the auspices of the AU, the fund would support five of the recommended 12 flagship research projects in its first two years of operation, and eventually have 12 running at any given time. But the Cairo meeting was unable, despite intense debate, to make recommendations about either the governance or the administration of the facility, apart from deciding that it should be based on existing organizations rather than an entirely new one. One option would be to contract fund management to the African Development Bank. But it seems unlikely that such details will be decided and agreed in Addis. The most that can be expected at the summit would be pledges towards initial funding from individual African governments. An initial AU estimate for funding the Consolidated Plan of Action over the next 5 years asks for US$158 million. One AU-backed proposal suggests that funding sources should include African governments, foreign donors \u2014 including bilateral and international aid agencies and foundations \u2014 and private-sector contributions from Africa and elsewhere. Khotso Mokhele, former president of the South African National Research Foundation, the continent's largest funding agency, argues that African states need to start by making a financial commitment themselves. \u201cIf an agency or fund is established simply as a vehicle for handouts by the West, it is doomed to failure,\u201d he says. Similar pledges have been made before by African science ministers, but they need to be supported by heads of state. In 2003, at the launch of AMCOST, African states pledged to work towards a spending target of 1% of gross domestic product on science and technology, compared with a global average of 2.36%. At the time, Egypt was the only African state reaching this target, with Algeria and Uganda coming close. South Africa announced in December that anticipated increases should allow it to reach the 1% target by 2008. But most other nations are still some way off. \n               High hopes \n             Another element of the Cairo declaration requiring additional funding is the establishment of centres of excellence to take forward some of the research in the Consolidated Plan of Action. Proponents of the concept point to India's four large science and technology institutes, which arguably provided the foundation for the country's sound base in science and technology. Britain's Commission for Africa proposed last year that the international community provide up to $3 billion over the next ten years to develop such centres. The problem with establishing centres of excellence is that, by definition, funding becomes localized in certain areas in preference to others. This makes any decisions about which institutes to build and fund very political. In certain scientific fields Africa will have to create new institutes, whereas in others existing ones can be expanded. In the area of biotechnology (which has been broadly defined), for example, AMCOST has recommended building on existing institutions, or regional networks of institutes. Not surprisingly, these reflect regional strengths and priorities: pharmaceutical biotechnology would be the focus in north Africa, with other healthcare biotechnology (including medical diagnostic testing kits, and stem-cell research) in southern Africa. In east Africa, biotechnology research on breeding and feed for livestock would build on expertise at the International Institute for Livestock Research (ILRI) in Nairobi, Kenya, and, in west Africa, biotechnology for crop improvement would extend the work of the West Africa Rice Development Association. Finally, central Africa would target biotechnology for forest conservation and development \u2014 one possibility being the use of DNA barcoding to conduct taxonomic studies in forests. On the ground, increased funding would allow these institutions to extend their activities. For example, John McDermott, director of research at ILRI, says it would allow his institute to make its state-of-the-art facilities in genomics and immunology available to other researchers in the region. McDermott wants ILRI to be a regional platform for biotechnology. \u201cThis is important because capacity in science and technology is directly linked to capacity for innovation, which the African economy so urgently requires,\u201d he says. \n               The big picture \n             But DNA barcoding in forestry research seems far removed from the daily concerns of most ordinary Africans. One criticism African leaders face whenever they focus on science and technology is that it is an \u00e9litist concern, distant from the realities of poverty, disease and infrastructure in Africa. Complaints about low representation at the pre-summit meetings in Alexandria and Cairo only strengthen such concerns. The Alexandria meeting in particular was billed as an opportunity for African scientists to provide input, yet it was attended by only 110 African delegates, of whom less than one fifth were active researchers. Similar charges of \u00e9litism have been levelled at the African Institutes of Science and Technology (AIST) initiative, a possible model for African centres of excellence. First proposed by former World Bank president James Wolfensohn, the initiative hopes to establish four \u00e9lite universities in science and technology in sub-Saharan Africa: one each in the north, west, east and south. Its proponents argue that it addresses crucial manpower needs by offering first degrees in science and engineering (including some business modules), as well as postgraduate degrees. The institutes will be overseen by the Nelson Mandela Institute, a non-profit company registered in Delaware, the board of which includes former presidents Nelson Mandela of South Africa and Joachim Chissano of Mozambique. The first institute will be founded later this year in Abuja, Nigeria, although a president has yet to be appointed. It has substantial support from the Nigerian government, and plans to enrol its first students in September next year. At full capacity, it is expected to have 5,500 students, 40% of whom will be in graduate programmes. Further institutes are planned in Burkina Faso and Tanzania. Wole Soboyejo, a Nigerian materials scientist at Princeton University who chairs AIST's African Science Committee, rejects the idea that the concept is \u00e9litist. He argues that enhancing manpower capability is critical: sub-Saharan Africa, for example, produces only about 83 engineers per million of the population annually, whereas, on average, developed countries produce 1,000. He emphasizes that \u201cthe institutes will not be islands of excellence, but will be linked to existing research centres and universities in each region, for example, providing them with access to electronic journals\u201d. Soboyejo suggests that closer cooperation with the AU would also be desirable. Mjwara is concerned that the new institutes \u201cmight not be sustainable as they do not seem to be based on local capacity\u201d. He would prefer the AU's proposed centres of excellence to follow the example of the Pretoria-based African Laser Centre, with its network of six national facilities in Tunisia, Senegal, Ghana, Algeria, Egypt and South Africa. Soboyejo, however, defends the model proposed for AIST by comparing it to another South African success story, the African Institute for Mathematical Sciences (AIMS) in Muizenberg. Like AIMS, the new institutes would combine local expertise with that of foreign academics teaching short courses. \u201cAt present, there is huge goodwill towards Africa worldwide, and the challenge is to channel this into returning lasting benefits for the continent,\u201d he says. This call is echoed by Juma, who will deliver a keynote address at the summit. \u201cThe debate urgently needs to shift from calls for funding to thinking about creative ways of using existing scientific and technical knowledge to solve local problems,\u201d he says. And in so doing, Juma believes the linkages between institutions of higher learning and research will need to be re-examined. \u201cMore attention should be paid to rebuilding these institutions as vehicles of community development,\u201d he explains. Many African universities operate like those in the West did 40 years ago, with few links to either industry or local communities. King is optimistic about the summits' chances of reaching agreement on the infrastructure required to go forward: \u201cAs soon as we have a realistic set of proposals on the table, we will be in a position to discuss how to fund them.\u201d Concrete policy proposals, backed by foreign investment, would surely be welcomed by African scientists. \u201cWe've had a lot of rhetoric so far about the potential importance of science and technology in African development,\u201d says McDermott, \u201cbut not much action.\u201d See Editorial,  \n                     page 339 \n                   . \n                     Science in Africa: Conscious of change \n                   \n                     Ministers agree to act on warnings of soaring temperatures in Africa \n                   \n                     African states pledge increased spending on research \n                   \n                     Gene-bank expansion plan launched at Earth summit \n                   \n                     UK aid for African maths institute \n                   \n                     African Union web site \n                   \n                     SciDev web forum on African Union summit \n                   \n                     Commisison for Africa \n                   Reprints and Permissions"},
{"file_id": "445140a", "url": "https://www.nature.com/articles/445140a", "year": 2007, "authors": [{"name": "Colin Macilwain"}], "parsed_as_year": "2006_or_before", "body": "Containing a potential HIV explosion in the strife-torn Niger delta is a tough job \u2014 but circumstances are forcing the oil and gas industries to confront it. Colin Macilwain reports. Bonny Island's strategic location, at the mouth of the Niger River in west Africa, has always been a blessing and a curse. An important trading post for at least seven centuries, the island shipped hundreds of thousands of slaves from inland tribes to the Americas. Its Ibani people have retained good relations with foreign businesses ever since, even as the island's main export changed from people to oil and gas. The traditional rulers of Bonny Island have helped to maintain a relative stability here in this oil-rich but desperately poor region. That is one reason why Bonny Island hosts the largest-ever industrial investment in Africa: a US$15-billion gas-liquefaction plant that compresses and exports millions of tonnes of natural gas \u2014 enough to feed half the gas needs of France. The plant, run by a consortium of oil and gas companies known as Nigerian Liquefied Natural Gas (NLNG), expanded spectacularly after Nigeria's government decided that the gas that had previously been flared off should be compressed and exported. The plant now draws thousands of skilled workers from all over the world, as well as impoverished workers from the surrounding regions. Although accurate figures are hard to come by, the island's population has grown from perhaps 30,000 a decade ago to 100,000 or more today. That influx has set the stage for a new menace. A combination of migration, fast money and rampant prostitution may be placing the island on the brink of an AIDS explosion. And so NLNG, with help from its sister oil companies, has chosen Bonny Island for an experiment that could change the face of public health in the entire Niger Delta: a partnership between the local community, government agencies and oil and gas companies that strives to study, test for and eventually control AIDS on the island. The project is unusual in that it seeks to pull together industry and government money to support an initiative that will be built from the ground up, within the community of Bonny Island. It places great emphasis on self-help groups and other mechanisms that will involve all of the relevant groups in the community, from schoolteachers to sex workers, in the nitty-gritty of AIDS education and prevention. \u201cWe are in Bonny Island because it is isolated and relatively secure,\u201d says Donald de Korte, a veteran AIDS physician and consultant to the US drug giant Merck who is overseeing the project. Relatively is a key word here; in late December, the oil company Shell evacuated family members of expatriate staff from Bonny Island after car bombings and armed attacks elsewhere in the delta. \n               Money for nothing \n             The project, called Ibani-se after the local people and their word for 'energy', is the first of its kind in Nigeria. The country has an HIV prevalence of about 4% \u2014 much lower than that in southern or eastern Africa, perhaps because of western Africa's retained tradition of male circumcision. Still, an estimated 3 million adults in Nigeria are HIV-positive \u2014 more than anywhere else except South Africa and India. With the exception of a few big city hospitals, little counselling and treatment are available; only about 5% of Nigerians who need antiretroviral treatment are getting it, says de Korte. Nigeria is awash with oil and cash, but not much of that makes it into basic health services for the country. Oil money has helped the Nigerian government to clear its $32 billion debt since 2004. But the economic activity generated by the production of oil and gas has inflated prices in the Niger Delta, fuelling social division. The Ibani-se initiative is being supported initially by industry sponsors including NLNG, Shell and ExxonMobil, who have done other outreach programmes on the island. There is a discernable dichotomy here. On the one hand, the initiative hopes to achieve a sustainable partnership that is anchored in the community. But others have a more jaundiced view \u2014 grounded in past experience in the region \u2014 that nothing will function on Bonny Island unless NLNG does it, and pays for it. Still, the project has a determined leader in de Korte, a Dutch physician who used to run Merck's business in Africa and then, from 2000 to 2004, led one of the continent's largest and most comprehensive AIDS treatment programmes, in Botswana. De Korte was tapped by officials from Shell and NLNG to chair the panel overseeing the initiative, after they heard him talk about his views on how AIDS should be fought. Since then de Korte has been working to constitute Ibani-se as a non-governmental organization (NGO), which would make it eligible for financial support from international bodies such as the Global Fund to fight AIDS, Tuberculosis and Malaria, or the US President's Emergency Plan For AIDS Relief (PEPFAR). The key to the project is not to pour in drugs, money or equipment, which would in all likelihood be stolen, but rather to build something that is sustainable. To that end, de Korte and his team have painstakingly consulted a wide range of people on the island, including businessmen, sex workers, fishermen, the Navy, drivers, market traders, churches and mosques, and the twelve 'houses' of the Ibani people. \n               Reaching high \n             The initial goals of Ibani-se are, at first glance, quite modest. The project plans to have 50 patients on antiretroviral treatment by the end of the year, for example, and to have 800 patients \u2014 a third of those who could benefit \u2014 on treatment by the end of 2009. But given current conditions \u2014 with minimal public-health infrastructure, few people being tested or counselled, and practically no one getting drugs \u2014 these goals are much more ambitious than they initially appear. The medical focal point of the initiative will be Bonny Island's dilapidated, 60-bed general hospital, where a team of 26 medical staff deal with 450 patients a week. The hospital is supposed to be being renovated; last month, the patients were crowded into just one of the four wards while they waited for the other wards to be fixed. \u201cA lot needs be done\u201d to upgrade the hospital, admits Douglas Pepple, the senior physician at the hospital, \u201cbut you have to start somewhere.\u201d At present, the only people who can get AIDS drugs on Bonny Island are pregnant women, who are entitled to treatment until their baby is born. Everyone else has to go to a hospital in Port Harcourt \u2014 the sprawling capital of the delta, 50 kilometres up the pirate-ridden river. No one knows anyone on the island who has managed to complete the trip. Ibani-se may help to change that situation. It was launched on 1 December 2006 \u2014 the last World AIDS Day. The day has a high, if ambiguous, profile in Nigeria. In the morning, local radio stations discuss the issue of AIDS, and O Network, MTV's popular African affiliate, broadcasts a question-and-answer session in which South African schoolchildren interrogate ex-US president Bill Clinton about AIDS. It also shows videos of ditties such as  Put You On the Game , in which Los Angeles rapper The Game threatens women with a life of prostitution. Then, King Edward XI of Bonny Island and Peter Odili, the governor of Rivers State, arrive in the town of Akiama to officially launch Ibani-se. After a slow start the ceremony soon gathers steam, as about 500 schoolchildren and other islanders crowd into the community hall. The king \u2014 who lives for most of the year in Nigeria's capital Abuja, where he chairs a confederation of Nigeria's traditional rulers \u2014 tells the crowd about an initial baseline study for the project. The survey, based on a representative sample of Bonny Island's adult population, found that an estimated 9,600 people on the island are now HIV-positive. \u201cThere's an urgent need to commence an HIV treatment programme,\u201d the king says. \u201cWe need to rise up against the stigma and discrimination that surrounds AIDS.\u201d \n               Empty gestures \n             For the event, Shell had shipped in a $40,000 machine to test CD4 counts \u2014 a measure of immune function \u2014 and dropped it, incongruously, on the floor of the hall. The gesture does not exactly thrill Ibani-se's organisers; nobody on the island knows how to use the machine. Thanking Shell for the gift, the king takes an adroit swipe at the donor: \u201c$20,000 [sic] is nothing compared with what we will do have to do in the future to mount a project that must be sustainable,\u201d he tells the community. The budget of the programme has been modest so far, with about $600,000 donated by NLNG last year and $1.4 million set to be forthcoming in 2007. Merck has contributed the time of de Korte and other outside consultants, and plans to eventually supply drugs to the programme at a reduced rate. The initiative's small group of permanent staff are Nigerian healthcare workers, hired mainly from other NGOs. \u201cThe community is yet to come to terms with the fact that AIDS is a big issue here,\u201d says one of the team, Samson Sunday. \u201cThey know it exists, but they see it as a blight from the outside that only affects a small number of people. The first promising sign is that community leaders are accepting that they are sitting on a time bomb if they don't do something soon.\u201d The project's start-up has been much tougher than anticipated. Ibani-se still hasn't attained the legal status that it seeks as an NGO, despite trying for the past year and a half. Basic logistics are immensely problematic. Project managers didn't want new vehicles at first, as ostentatious transportation is the first thing that tends to divide community projects in Africa from the communities they purport to serve. But as a result, the project has become too dependent on NLNG for logistical support. Ibani-se's staff have found, to their frustration, that while they wait for transport and phone lines, staff at the gas company have other priorities. The headquarters of the project is also still located on NLNG's fortified residential compound, whose manicured lawns and smart buildings look more like a retirement complex in Florida than the 'real' island outside the gates. Ibani-se is planning to move to one of the only concrete buildings in Bonny Town, the island's largest village. The building was constructed 20 years ago by Shell as a community library, but it has never been used and has stood derelict ever since. It is palpable that local participants in the project are expecting some material gain. \u201cThere is hardly any volunteerism on the island,\u201d says de Korte, who wants as many people as possible on Bonny Island to devote some of their own time to the project. But so far, \u201ccommunity engagement has only scratched the surface\u201d. The whole exercise has been much more difficult than expected, he admits. \u201cI underestimated the Nigerian environment.\u201d \n               Platforms for success \n             On the other hand, de Korte is proud of the baseline survey done last summer by a Nigerian NGO, the Society for Family Health. The survey forms the basis for the project's action plan and provides the first detailed examination of sexual practices and HIV prevalence on Bonny Island. It shows that the prevalence of HIV is now 7.8% \u2014 double what it was five years ago. The study is not yet complete or cleared for release, but it is already identifying areas that Ibani-se could focus on, such as the sexual behaviour of the local sex workers and the 17,000 staff and contractors at the NLNG plant. Not everyone on the island was surveyed, however. An extension to the liquefied gas plant, being built by an international consortium led by the US corporation Halliburton, employs another 1,700 workers. The consortium declined to participate in the survey, citing security concerns. Unlike the gas liquefaction plant itself \u2014 which is increasingly staffed by Nigerians \u2014 almost all the workers in the extension are white expatriates. Brian Buckley, general manager for production at NLNG, says that the Ibani-se project is just one component of the gas company's plan to tackle social issues on the island. He says that he is hopeful that Halliburton workers will participate in the project later on. NLNG staff have already done so, he points out, although not many of the expatriates volunteered for testing. From the company's point of view, Buckley says, the Ibani-se programme is vital to the long-term future of the plant. \u201cWe don't want to end up with our workforce depleted, like the mines in South Africa,\u201d he says. \u201cThe kids growing up on this island are the future of this community.\u201d World AIDS Day may have provided a glimpse of that future. Before and after the king's ceremony, long lines of people queued patiently to be counselled individually in a small interview cubicle, then give blood and be tested for AIDS. The four Ibani-se counsellors worked for hours and tested 100 people, but eventually ran out of time. They took down details of 121 more people for subsequent testing \u2014 taking the project some way towards its initial goal of testing 3,000 people by the end of the year. If Ibani-se takes off as planned, its cost will rise significantly: thousands of islanders could end up on antiretroviral drugs priced at perhaps $60 per month. De Korte is confident that the Global Fund and other sources will help to foot the bill. But a spokesman for the fund, Jon Liden, says that public\u2013private partnerships are \u201cnot as easy in practice as we had thought\u201d. Efforts to fight AIDS in Nigeria, he says, have varied tremendously in quality; on several occasions, the fund has not renewed grants to Nigeria, \u201cdue to poor performance\u201d. \n               Addressing the stigma \n             World AIDS Day saw the project off to an auspicious start though, and the king's words provided some assurance that the initiative has buy-in at the right levels. \u201cWe are pleased that the king has given his support,\u201d says Ibiba Chidi, a project manager for Ibani-se. For now, one of the project's focal points will be education. Stigma continues to surround AIDS on Bonny Island: health workers have uncovered very few people who admit to being HIV-positive. At the ceremony, a woman who was billed as speaking for people who are HIV-positive said she was, in fact, speaking out for a friend who is. The other focal point, again reflecting data from the baseline survey, will be the activities of sex workers on the island. Project officials estimate that some 500 prostitutes work in Monkey village, on the main drag between the NLNG residential compound and the gas plant, where men gather at night to meet girls. 25-year old Joy Odudu runs a makeshift wooden bar in the ramshackle village. \u201cI came to the island to work, but was looking for a job for a year and none was coming,\u201d she says. She claims that site managers wanted a bribe of 40,000 naira (more than US$300) from women seeking employment there. Odudu, whose arm was shot by river bandits, says that about half the men who frequent her bar after dark are expatriates, and that some of them will pay girls extra not to use condoms. She says she doesn't know how often this happens \u2014 \u201cwe can't follow them into their rooms\u201d \u2014 but that arguments between clients and girls, and between girls, have exposed the practice. One Australian worker, she asserts, was paying girls $300 or $400 for nights of unprotected sex. A nightly curfew, introduced after three oil workers were kidnapped on Bonny Island last August, is hurting business, Odudu says. But the expatriates are still coming, curfew or no curfew, says Peters Kunamon, headman of a section of Monkey village, Odudu's protector and a member of Ibani-se's community interface committee. Asked about the project itself, both Odudu and Kunamon say it will win their support if it provides them with some material benefit. \u201cIt is supposed to be voluntary \u2014 but we need to eat,\u201d says Kunamon, who also runs a pharmacy store in the village that will sell condoms. \u201cThe project is good if it gives us something.\u201d Kunamon mentions something else that others are not keen to point out: women with day jobs on the site, he alleges, are also 'working' there at night. Lucy, a girl at the bar, is just as direct. As she realizes that visitors are here for a chat, not for sex, her mood swings between charm and irritation. \u201cYou want jiggy-jiggy? Condoms or no condoms \u2014 what does it matter?\u201d she asks sarcastically, voice rising. \u201cWe have to pay 100 naira here for water, to wash in the morning. We need to eat! We have nothing!\u201d \n               Setting the scene \n             Back inside the compound at the close of World AIDS Day, the Sun goes down soon after six, as it does all year round so near the equator. Soon after, as the curfew takes effect, engineers converge on the open-air cocktail bar, where local musicians provide some outdoor entertainment. It's an attractive setting, with elegant log cabins and decks and an outdoor pool whose racing section is 49 metres long. If it were an olympian 50 metres, apparently, the law would require NLNG to open it to the public; in this intensely fortified zone, that wouldn't do at all. Buckley is there, on what happens to be his last day as production manager. Next week, he is off to run an even larger (and more peaceable) liquefied-gas facility in Oman. He relaxes with a beer and a group of friends, most of whom sport polo shirts and chinos, the international uniform of the professional man at rest. But all around the outer reaches of the extensive bar pavilion, smartly dressed Nigerian girls who don't look at all like petrochemical engineers are hanging out. Their teeth and eyes sparkle in the evening light, they have the great posture you only get from learning to balance ten litres of water on your head at the age of four, and they're keen to strike up conversation with strangers. \u201cFive thousand naira,\u201d one of them whispers, by way of introduction. \u201cAll night long!\u201d Colin Macilwain writes for Nature from Edinburgh. \n                     AIDS and the private sector \n                   \n                     Ibani-se \n                   \n                     NLNG \n                   \n                     Global Fund \n                   \n                     US Presidents' fund \n                   Reprints and Permissions"},
{"file_id": "445359a", "url": "https://www.nature.com/articles/445359a", "year": 2007, "authors": [{"name": "Tom Siegfried"}], "parsed_as_year": "2006_or_before", "body": "Taking hormones to replace those lost during menopause helps many women with their symptoms, yet it may also cause cognitive decline. Could the age at which hormones are taken determine whether they will be beneficial or harmful? Tom Siegfried reports. Ageing female brains are perplexing, and not just to ageing husbands. Male and female neuroscientists are confused about the role that sex hormones play in mental decline after menopause. Fuzzy memory and other mental maladies often accompany menopause, when natural production of steroidal sex hormones \u2014 mostly oestrogens \u2014 diminishes. Until a few years ago, evidence suggested that replacement of the lost hormones could alleviate memory problems and even protect against dementia later in life. But in 2003, a large clinical trial performed in the United States reported that a popular type of hormone replacement therapy (HRT) did not improve cognition \u2014 and actually raised the risk of dementia 1 , 2 . That result baffled many scientists, as it contradicted both animal experiments and preliminary human studies showing that HRT benefited the brain. But a better understanding of the biology that governs the brain's response to HRT might help to explain these discrepancies. The latest research is revealing a neurochemical soap opera about the brain's relationship with the sex organs, and how that relationship turns turbulent as the body ages 3 . The latest twist in the plot suggests that HRT can indeed aid the brain, especially if the timing is right. In fact, when given to a relatively young and healthy brain, HRT could well protect it against later damage. But when given to a brain that is already in decline, it can make things worse. \u201cTiming is everything,\u201d says Andrea Gore, a neuroendocrinologist at the University of Texas at Austin. After all, the landmark study that indicated possible mental declines under HRT \u2014 part of the massive Women's Health Initiative \u2014 tested women aged 65 to 79. That is long past the average age of menopause in the Western world, at about 51 years old. Women who start HRT around the time of menopause, however, might instead reduce their risk of dementia, says Peter Schmidt of the National Institute of Mental Health in Bethesda, Maryland, who studies the brain's response to sex hormones. \u201cThere may be a critical window within which, if oestrogen is started, it will have a beneficial effect on the brain,\u201d he said in October in Atlanta at a meeting of the Society for Neuroscience. \u201cThe time and age at which a person takes hormone therapy may predict the clinical outcome.\u201d Nobody knows for sure whether such a window exists, or precisely when it would be open. And even if the benefits to the brain are confirmed, the Women's Health Initiative study identified other hazards from HRT, such as an elevated risk of breast cancer. The findings drove a dramatic drop in the number of postmenopausal women using HRT. In 2001, by one estimate, about 15 million women in the United States received HRT; that number fell to about 10 million after the trial reported on its dangers 4 . Women are now generally advised to take HRT for as short a time as possible (if at all) and only for treatment of menopausal symptoms. So, for HRT to help the brain, new hormone replacement molecules will probably be needed that confer the benefits without the risks. To help in the hunt for new therapeutic molecules, researchers have realized that a deeper understanding is needed of the brain's role in menopause. For decades, research into menopause focused on the ovaries, which start to lose their hormone-producing follicles at age 40\u201350. \u201cIt really puzzled me that there was very little interest in whether the brain may also play a primary role in the control of reproductive ageing,\u201d says Gore. But research in Gore's and several other laboratories has shown that menopause does not simply result from the ovaries' decline. It also encompasses complex cross-talk between these organs and the brain. Menopause occurs in stages, and hormone secretion fluctuates considerably during the 'menopause transition' \u2014 the time from the first changes in the activity of the ovaries to the year after the final menstrual period. That hormone variability reflects the shifting action in a three-character play starring the hypothalamus, a small structure at the base of the brain (see graphic). Supporting actors include the pituitary gland and the gonads (in women, the ovaries); collectively, the three make up the hypothalamic\u2013pituitary\u2013gonadal (HPG) axis. The activity along the HPG axis is thought to be driven by the hypothalamus. Neurons there produce a peptide hormone known as gonadotropin-releasing hormone, or GnRH, which travels to the nearby pituitary gland and stimulates it to secrete other hormones. In turn, these hormones circulate to the ovaries, which respond to this chemical chain-mail by releasing their own hormones that travel back to the brain. In younger adults, the characters in the HPG axis play their parts robustly, communicating clearly to maintain the body's reproductive ability. Reproductive ability declines once the hypothalamus starts to falter in how it responds to feedback from the ovaries \u2014 especially to signals sent by the main type of oestrogen, known as oestradiol. Several actors conspire to twist the plot at this point. Oestradiol messages cannot signal the GnRH neurons directly, but instead stimulate a type of receptor on neighbouring neurons in order to relay the signal. But that nearby receptor starts to exit the stage as the body ages. \u201cThe expression of that receptor changes between young and middle-aged animals,\u201d Gore notes. \n               Early retirement \n             The GnRH neurons themselves remain on-stage, but no longer respond as effectively to the signals that do get relayed from their neighbours. The problem, Gore says, seems to be related to subtle alterations in receptors for glutamate \u2014 the courier for the signals \u2014 on the ageing GnRH neurons. Other players in the drama also diverge from the original script. Chemical cues sent to the hypothalamus from the ovaries change with age. In fact, all three levels of the HPG axis are changed in ageing rats and monkeys. \u201cThe responsiveness of this hypothalamic network to ovarian input decreases with ageing,\u201d says Gore. \u201cHere is where timing is everything.\u201d For some reason, she says, the hypothalamus gets old while the rest of the brain remains middle-aged. That hypothalamic senescence can trigger menopause-related mental decline years before deficits in learning and memory would normally start to become evident. When the hypothalamus loses interest in the messages from the ovaries, hormone production diminishes. But the loss of hormones can't be the only thing that causes mental deficits after menopause. Young rhesus monkeys whose ovaries have been removed, depriving them of oestrogen, can still perform well on mental tests, says neuroscientist John Morrison of the Mount Sinai School of Medicine in New York City. \u201cSo they could counter the absence of oestradiol, whereas the aged animals could not.\u201d Older monkeys can, however, regain robust mental function when given oestradiol supplements. Oestradiol apparently enhances memory skills by boosting the numbers of dendritic spines, which are sites on neurons where synapses, or connections, form. Morrison's research shows that oestradiol increases the density of synapses in the prefrontal cortex \u2014 the region of the brain responsible for the highest level of mental function. To produce those benefits for ageing monkeys, Morrison and collaborators found, timing is doubly important. Besides administering oestradiol to monkeys near the time of menopause, researchers found the best results when injecting it at 21-day intervals, which mimicked the monkeys' natural hormonal cycle. \n               On a whim \n             Timing is clearly at the heart of the debate over HRT in women as well. The large clinical trial \u2014 called WHIMS, for the memory-study part of the Women's Health Initiative \u2014 documented problems that stemmed from the commonly prescribed combination of oestrogen and progestogen. (Oestrogen alone is typically prescribed only for women who have had their uterus removed, as oestrogen alone raises the risk of uterine cancer.) An analysis of more than 4,000 women found that the rate of dementia was approximately doubled in elderly women on combination HRT compared with those on a placebo. That surprising result led researchers to ponder whether HRT could be beneficial if administered to younger women, around the time of menopause. And studies examining oestrogen's actions within cells seem to support that suggestion. Young, healthy cells are protected by oestrogen, says molecular neuropharmacologist Roberta Brinton of the University of Southern California in Los Angeles. But in cells already affected by disease (say, in the early stages of Alzheimer's), adding oestrogen accelerates the damage, her studies show 5 , 6 . That could explain why the earlier studies of younger women suggested that HRT helped the brain. Within a neuron, Brinton says, oestrogen regulates the viability of mitochondria, the cell's energy-production centres. \u201cIf you damage mitochondria and you damage enough of them and damage them severely enough, the mitochondria release messengers that basically turn on the 'die' message to the cell,\u201d she says. Stimulating oestradiol receptors on mitochondrial membranes triggers a cascade of reactions that causes calcium to flood into a neuron. For a healthy neuron, that's not a problem, and in fact, the calcium influx initiates other chemical cascades that prevent mitochondria from signalling the cell to die. Older neurons, though, are not able to regulate calcium levels effectively, and therefore suffer even further if given oestrogen. \n               Defensive driving \n             \u201cIf you take a healthy cell and expose it to oestrogen, you create what I call a proactive defence survival state,\u201d Brinton says. Experiments show that such 'protected' cells can resist attacks from toxic substances such as free radicals or amyloid. \u201cThese cells have about a 25\u201350% survival advantage,\u201d she says. On the other hand, for a damaged cell, oestrogen turns from a friend into an enemy. When experimenters expose a cell to toxic insults first, addition of oestrogen later leads to further degeneration 5 . Brinton believes that women in the WHIMS study who developed Alzheimer's disease while taking oestrogen\u2013progestogen supplements were probably in the early stages of Alzheimer's already. Replacement hormones then accelerated the disease. \u201cThat's our hypothesis,\u201d she says. \u201cYou have to treat these cells in the brain at a time when they are healthy, when they have healthy calcium function, and that allows them to activate these mechanisms that lead to survival.\u201d So far, her results are based on studies of cultured neurons, but similar experiments have now started in a transgenic mouse model of Alzheimer's disease. Further studies in Brinton's lab focus on the precise formulation of HRT, which typically consists of a complex mix of equine oestrogens, some of which are not made by human ovaries. Some of those oestrogens, Brinton and co-workers have found, are more effective than others at protecting neurons. Research is under way to identify the key properties of the most effective molecules and to determine the structure of the specific cellular receptor sites at which those molecules exert their influence. Such knowledge should aid efforts to find, or design, 'selective oestrogen receptor modulators' for the brain \u2014 known as neuroSERMs \u2014 that would offer oestrogen's positive neurological benefits without the negative effects of standard HRT. Effective neuroSERMs would penetrate the blood\u2013brain barrier and activate particular oestrogen receptors in the brain, without stimulating receptors responsible for deleterious effects in other tissues, such as in the breast or ovaries. Numerous neuroSERM candidate molecules are being developed and tested 6 . Still, for all the advances in mapping out the interplay of brains and hormones, many issues remain unresolved. For instance, more work is needed to determine whether a window of opportunity exists during which people would benefit from HRT, and if so, when it would be best to start, and end, an HRT regimen. \u201cI think nobody really knows the answer,\u201d says Schmidt. \u201cIt will be important to nail down what we mean by this critical window, and whether in fact it would suffice to take hormone therapy during that time.\u201d Some researchers also think that a similar window of opportunity might exist to evade some of HRT's other possible dangers, particularly heart disease. Oestrogen's effect on atherosclerosis, for instance, will be examined by the new Kronos Early Estrogen Prevention Study, a five-year randomized trial to test oestrogen supplements in women who are within three years of their last menstrual period 7 . The results from the Women's Health Initiative study indicated a raised risk of heart disease for users of HRT, but for much older women. Ultimately, researchers expect that some women will benefit from HRT whereas others will not. The trick is to identify who belongs to which group. So a major priority of future research will be finding biomolecules that signal whether an individual is likely to benefit (or suffer) from hormone supplements. \u201cBiomarkers are a critically important issue in all of ageing research,\u201d says Morrison. \u201cWe sorely need biomarkers to give us a reflection of brain changes for any treatment of age-related disorders.\u201d \n               A complicated affair \n             Investigations aimed at identifying such biomarkers will have to sort out many poorly understood biochemical complications. Oestradiol, for instance, operates on at least two receptors \u2014 alpha and beta \u2014 found at different sites in the brain. Add in the realization that the brain can produce its own oestrogen, as well as respond to oestrogen from the hormonal system, and ample avenues open up for new subplots in the hormone soap opera. \u201cYou have to take into consideration the complexity of where the oestrogen receptor is,\u201d says Brinton. Oestrogen receptors are found not only on mitochondria, but also in a neuron's outer membrane and in its nucleus. And as Gore points out, the body has oestrogen receptors in other tissues too. \u201cThere are different tissues and different targets,\u201d she says, \u201cand as we age our bodies may express a different assortment of these oestrogen receptors, even within a specific tissue.\u201d And although oestradiol is the most important of the steroidal sex hormones in women, it doesn't work alone. Male sex hormones \u2014 the androgens \u2014 also circulate, and could affect how much oestradiol ends up acting within the brain, says Gore. As for men, all the attention given to menopause may have misled them into thinking that they have nothing to worry about with respect to oestrogen and the ageing brain. But hormone and ageing issues may affect them as well. The male brain contains oestrogen receptors too, says Gore. \u201cSo there may be similar types of effects in men.\u201d \n                     News Feature: Hormone in the hot seat \n                   \n                     Oral contraceptive may cut risk of heart disease \n                   \n                     Hormone therapy: A dangerous elixir? \n                   \n                     Long-term hormone replacement linked to breast cancer \n                   \n                     Robert Brinton's lab page \n                   \n                     Women's Health Initiative reports \n                   Reprints and Permissions"}
]