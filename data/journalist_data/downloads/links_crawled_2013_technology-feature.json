[
{"file_id": "498255a", "url": "https://www.nature.com/articles/498255a", "year": 2013, "authors": [{"name": "Vivien Marx"}], "parsed_as_year": "2006_or_before", "body": "As they grapple with increasingly large data sets, biologists and computer scientists uncork new bottlenecks. Biologists are joining the big-data club. With the advent of high-throughput genomics, life scientists are starting to grapple with massive data sets, encountering challenges with handling, processing and moving information that were once the domain of astronomers and high-energy physicists 1 . With every passing year, they turn more often to big data to probe everything from the regulation of genes and the evolution of genomes to why coastal algae bloom, what microbes dwell where in human body cavities and how the genetic make-up of different cancers influences how cancer patients fare 2 . The European Bioinformatics Institute (EBI) in Hinxton, UK, part of the European Molecular Biology Laboratory and one of the world's largest biology-data repositories, currently stores 20 petabytes (1 petabyte is 10 15  bytes) of data and back-ups about genes, proteins and small molecules. Genomic data account for 2 petabytes of that, a number that more than doubles every year 3  (see 'Data explosion'). This data pile is just one-tenth the size of the data store at CERN, Europe's particle-physics laboratory near Geneva, Switzerland. Every year, particle-collision events in CERN's Large Hadron Collider generate around 15 petabytes of data \u2014 the equivalent of about 4 million high-definition feature-length films. But the EBI and institutes like it face similar data-wrangling challenges to those at CERN, says Ewan Birney, associate director of the EBI. He and his colleagues now regularly meet with organizations such as CERN and the European Space Agency (ESA) in Paris to swap lessons about data storage, analysis and sharing. All labs need to manipulate data to yield research answers. As prices drop for high-throughput instruments such as automated genome sequencers, small biology labs can become big-data generators. And even labs without such instruments can become big-data users by accessing terabytes (10 12  bytes) of data from public repositories at the EBI or the US National Center for Biotechnology Information in Bethesda, Maryland. Each day last year, the EBI received about 9 million online requests to query its data, a 60% increase over 2011. Biology data mining has challenges all of its own, says Birney. Biological data are much more heterogeneous than those in physics. They stem from a wide range of experiments that spit out many types of information, such as genetic sequences, interactions of proteins or findings in medical records. The complexity is daunting, says Lawrence Hunter, a computational biologist at the University of Colorado Denver. \u201cGetting the most from the data requires interpreting them in light of all the relevant prior knowledge,\u201d he says. That means scientists have to store large data sets, and analyse, compare and share them \u2014 not simple tasks. Even a single sequenced human genome is around 140 gigabytes in size. Comparing human genomes takes more than a personal computer and online file-sharing applications such as DropBox. In an ongoing study, Arend Sidow, a computational biologist at Stanford University in California, and his team are looking at specific changes in the genome sequences of tumours from people with breast cancer. They wanted to compare their data with the thousands of other published breast-cancer genomes and look for similar patterns in the scores of different cancer types. But that is a tall order: downloading the data is time-consuming, and researchers must be sure that their computational infrastructure and software tools are up to the task. \u201cIf I could, I would routinely look at all sequenced cancer genomes,\u201d says Sidow. \u201cWith the current infrastructure, that's impossible.\u201d In 2009, Sidow co-founded a company called DNAnexus in Mountain View, California, to help with large-scale genetic analyses. Numerous other commercial and academic efforts also address the infrastructure needs of big-data biology. With the new types of data traffic jam honking for attention, \u201cwe now have non-trivial engineering problems\u201d, says Birney, \n               Life of the data-rich \n             Storing and interpreting big data takes both real and virtual bricks and mortar. On the EBI campus, for example, construction is under way to house the technical command centre of ELIXIR, a project to help scientists across Europe safeguard and share their data, and to support existing resources such as databases and computing facilities in individual countries. Whereas CERN has one supercollider producing data in one location, biological research generating high volumes of data is distributed across many labs \u2014 highlighting the need to share resources. If I could, I would routinely look at all sequenced cancer genomes. With the current infrastructure, that's impossible. Much of the construction in big-data biology is virtual, focused on cloud computing \u2014 in which data and software are situated in huge, off-site centres that users can access on demand, so that they do not need to buy their own hardware and maintain it on site. Labs that do have their own hardware can supplement it with the cloud and use both as needed. They can create virtual spaces for data, software and results that anyone can access, or they can lock the spaces up behind a firewall so that only a select group of collaborators can get to them. Working with the CSC \u2014 IT Center for Science in Espoo, Finland, a government-run high-performance computing centre, the EBI is developing Embassy Cloud, a cloud-computing component for ELIXIR that offers secure data-analysis environments and is currently in its pilot phase. External organizations can, for example, run data-driven experiments in the EBI's computational environment, close to the data they need. They can also download data to compare with their own. The idea is to broaden access to computing power, says Birney. A researcher in the Czech Republic, for example, might have an idea about how to reprocess cancer data to help the hunt for cancer drugs. If he or she lacks the computational equipment to develop it, he or she might not even try. But access to a high-powered cloud allows \u201cideas to come from any place\u201d, says Birney. Even at the EBI, many scientists access databases and software tools on the Web and through clouds. \u201cPeople rarely work on straight hardware anymore,\u201d says Birney. One heavily used resource is the Ensembl Genome Browser, run jointly by the EBI and the Wellcome Trust Sanger Institute in Hinxton. Life scientists use it to search through, download and analyse genomes from armadillo to zebrafish. The main Ensembl site is based on hardware in the United Kingdom, but when users in the United States and Japan had difficulty accessing the data quickly, the EBI resolved the bottleneck by hosting mirror sites at three of the many remote data centres that are part of Amazon Web Services' Elastic Compute Cloud (EC2). Amazon's data centres are geographically closer to the users than the EBI base, giving researchers quicker access to the information they need. More clouds are coming. Together with CERN and ESA, the EBI is building a cloud-based infrastructure called Helix Nebula \u2014 The Science Cloud. Also involved are information-technology companies such as Atos in Bezons, France; CGI in Montreal, Canada; SixSq in Geneva; and T-Systems in Frankfurt, Germany. Cloud computing is particularly attractive in an era of reduced research funding, says Hunter, because cloud users do not need to finance or maintain hardware. In addition to academic cloud projects, scientists can choose from many commercial providers, such as Rackspace, headquartered in San Antonio, Texas, or VMware in Palo Alto, California, as well as larger companies including Amazon, headquartered in Seattle, Washington, IBM in Armonk, New York, or Microsoft in Redmond, Washington. \n               Big-data parking \n             Clouds are a solution, but they also throw up fresh challenges. Ironically, their proliferation can cause a bottleneck if data end up parked on several clouds and thus still need to be moved to be shared. And using clouds means entrusting valuable data to a distant service provider who may be subject to power outages or other disruptions. \u201cI use cloud services for many things, but always keep a local copy of scientifically important data and software,\u201d says Hunter. Scientists experiment with different constellations to suit their needs and trust levels. Most researchers tend to download remote data to local hardware for analysis. But this method is \u201cbackward\u201d, says Andreas Sundquist, chief technology officer of DNAnexus. \u201cThe data are so much larger than the tools, it makes no sense to be doing that.\u201d The alternative is to use the cloud for both data storage and computing. If the data are on a cloud, researchers can harness both the computing power and the tools that they need online, without the need to move data and software (see 'Head in the clouds'). \u201cThere's no reason to move data outside the cloud. You can do analysis right there,\u201d says Sundquist. Everything required is available \u201cto the clever people with the clever ideas\u201d, regardless of their local computing resources, says Birney. Various academic and commercial ventures are engineering ways to bring data and analysis tools together \u2014 and as they build, they have to address the continued data growth. Xing Xu, director of cloud computing at BGI (formerly the Beijing Genomics Institute) in Shenzen, China, knows that challenge well. BGI is one of the largest producers of genomic data in the world, with 157 genome sequencing instruments working around the clock on samples from people, plants, animals and microbes. Each day, it generates 6 terabytes of genomic data. Every instrument can decode one human genome per week, an effort that used to take months or years and many staff. \n               Data highway \n             Once a genome sequencer has cranked out its snippets of genomic information, or 'reads', they must be assembled into a continuous stretch of DNA using computing and software. Xu and his team try to automate as much of this process as possible to enable scientists to get to analyses quickly. There's no reason to move data outside the cloud. You can do analysis right there. Next, either the reads or the analysis, or both, have to travel to scientists. Generally, researchers share biological data with their peers through public repositories, such as the EBI or ones run by the US National Center for Biotechnology Information in Bethesda, Maryland. Given the size of the data, this travel often means physically delivering hard drives \u2014 and risks data getting lost, stolen or damaged. Instead, BGI wants to use either its own clouds or others of the customer's choosing for electronic delivery. But that presents a problem, because big-data travel often means big traffic jams. Currently, BGI can transfer about 1 terabyte per day to its customers. \u201cIf you transfer one genome at a time, it's OK,\u201d says Xu. \u201cIf you sequence 50, it's not so practical for us to transfer that through the Internet. That takes about 20 days.\u201d BGI is exploring a variety of technologies to accelerate electronic data transfer, among them  fasp , software developed by Aspera in Emeryville, California, which helps to deliver data for film-production studios and the oil and gas industry as well as the life sciences. In an experiment last year, BGI tested a  fasp -enabled data transfer between China and the University of California, San Diego (UCSD). It took 30 seconds to move a 24-gigabyte file. \u201cThat's really fast,\u201d says Xu. Data transfer with  fasp  is hundreds of times quicker than methods using the normal Internet protocol, says software engineer Michelle Munson, chief executive and co-founder of Aspera. However, all transfer protocols share challenges associated with transferring large, unstructured data sets. The test transfer between BGI and UCSD was encouraging because Internet connections between China and the United States are \u201criddled with challenges\u201d such as variations in signal strength that interrupt data transfer, says Munson. The protocol has to handle such road bumps and ensure speedy transfer, data integrity and privacy. Data transfer often slows when the passage is bumpy, but with  fasp  it does not. Transfers can fail when a file is partially sent; with ordinary Internet connections, this relaunches the entire transfer. By contrast,  fasp  restarts where the previous transfer stopped. Data that are already on their way do not get resent, but continue on their travels. Xu says that he liked the experiment with  fasp , but the software does not solve the data-transfer problem. \u201cThe main problem is not technical, it is economical,\u201d he says. BGI would need to maintain a large Internet connection bandwidth for data transfer, which would be prohibitively expensive, especially given that Xu and his team do not send out big data in a continuous flow. \u201cIf we only transfer periodically, it doesn't make any economic sense for us to have this infrastructure, especially if the user wants that for free,\u201d he says. Data-sharing among many collaborators also remains a challenge. When BGI uses  fasp  to share data with customers or collaborators, it must have a software licence, which allows customers to download or upload the data for free. But customers who want to share data with each other using this transfer protocol will need their own software licences. Putting the data on the cloud and not moving them would bypass this problem; teams would go to the large data sets, rather than the other way around. Xu and his team are exploring this approach, alongside the use of Globus Online, a free Web-based file-transfer service from the Computation Institute at the University of Chicago and the Argonne National Laboratory in Illinois. In April, the Computation Institute team launched a genome-sequencing-analysis service called Globus Genomics on the Amazon cloud. Munson says that Aspera has set up a pay-as-you-go system on the Amazon cloud to address the issue of data-sharing. Later this year, the company will begin selling an updated version of its software that can be embedded on the desktop of any kind of computer and will let users browse large data sets much like a file-sharing application. Files can be dragged and dropped from one location to another, even if those locations are commercial or academic clouds. The cost of producing, acquiring and disseminating data is decreasing, says James Taylor, a computational biologist at Emory University in Atlanta, Georgia, who thinks that \u201ceveryone should have access to the skills and tools\u201d needed to make sense of all the information. Taylor is a co-founder of an academic platform called Galaxy, which lets scientists analyse their data and share software tools and workflows for free. Through Web-based access to computing facilities at Pennsylvania State University (PSU) in University Park, scientists can download Galaxy's platform of tools to their local hardware, or use it on the Galaxy cloud. They can then plug in their own data, perform analyses and save the steps in them, or try out workflows set up by their colleagues. Spearheaded by Taylor and Anton Nekrutenko, a molecular biologist at PSU, the Galaxy project draws on a community of around 100 software developers. One feature is Tool Shed, a virtual area with more than 2,700 software tools that users can upload, try out and rate. Xu says that he likes the collection and its ratings, because without them, scientists must always check if a software tool actually runs before they can use it. \n               Knowledge is power \n             Galaxy is a good fit for scientists with some computing know-how, says Alla Lapidus, a computational biologist in the algorithmic biology lab at St Petersburg Academic University of the Russian Academy of Sciences, which is led by Pavel Pevzner, a computer scientist at UCSD. But, she says, the platform might not be the best choice for less tech-savvy researchers. When Lapidus wanted to disseminate the software tools that she developed, she chose to put them on DNAnexus's newly launched second-generation commercial cloud-based analysis platform. That platform is also designed to cater to non-specialist users, says Sundquist. It is possible for a computer scientist to build his or her own biological data-analysis suite with software tools on the Amazon cloud, but DNAnexus uses its own engineering to help researchers without the necessary computer skills to get to the analysis steps. Catering for non-specialists is important when developing tools, as well as platforms. The Biomedical Information Science and Technology Initiative (BISTI) run by the US National Institutes of Health (NIH) in Bethesda, Maryland, supports development of new computational tools and the maintenance of existing ones. \u201cWe want a deployable tool,\u201d says Vivien Bonazzi, programme director in computational biology and bioinformatics at the National Human Genome Research Institute, who is involved with BISTI. Scientists who are not heavy-duty informatics types need to be able to set up these tools and use them successfully, she says. And it must be possible to scale up tools and update them as data volume grows. Bonazzi says that although many life scientists have significant computational skills, others do not understand computer lingo enough to know that in the tech world, Python is not a snake and Perl is not a gem (they are programming languages). But even if biologists can't develop or adapt the software, says Bonazzi, they have a place in big-data science. Apart from anything else, they can offer valuable feedback to their computationally fluent colleagues because of different needs and approaches to the science, she says. Increasingly, big genomic data sets are being used in biotechnology companies, drug firms and medical centres, which also have specific needs. Robert Mulroy, president of Merrimack Pharmaceuticals in Cambridge, Massachusetts, says that his teams handle mountains of data that hide drug candidates. \u201cOur view is that biology functions through systems dynamics,\u201d he says. Merrimack researchers focus on interrogating molecular signalling networks in the healthy body and in tumours, hoping to find new ways to corner cancer cells. They generate and use large amounts of information from the genome and other factors that drive a cell to become cancerous, says Mulroy. The company stores its data and conducts analysis on its own computing infrastructure, rather than a cloud, to keep the data private and protected. Drug developers have been hesitant about cloud computing. But, says Sundquist, that fear is subsiding in some quarters: some companies that have previously avoided clouds because of security problems are now exploring them. To assuage these users' concerns, Sundquist has engineered the DNAnexus cloud to be compliant with US and European regulatory guidelines. Its security features include encryption for biomedical information, and logs to allow users to address potential queries from auditors such as regulatory agencies, all of which is important in drug development. \n               Challenges and opportunities \n             Harnessing powerful computers and numerous tools for data analysis is crucial in drug discovery and other areas of big-data biology. But that is only part of the problem. Data and tools need to be more than close \u2014 they must talk to one another. Lapidus says that results produced by one tool are not always in a format that can be used by the next tool in a workflow. And if software tools are not easily installed, computer specialists will have to intervene on behalf of those biologists without computer skills. Even computationally savvy researchers can get tangled up when wrestling with software and big data. \u201cMany of us are getting so busy analysing huge data sets that we don't have time to do much else,\u201d says Steven Salzberg, a computational biologist at Johns Hopkins University in Baltimore, Maryland. \u201cWe have to spend some of our time figuring out ways to make the analysis faster, rather than just using the tools we have.\u201d Yet other big-data pressures come from the need to engineer tools for stability and longevity. Too many software tools crash too often. \u201cEveryone in the field runs into similar problems,\u201d says Hunter. In addition, research teams may not be able to acquire the resources they need, he says, especially in countries such as the United States, where an academic does not gain as much recognition for software engineering as for publishing a paper. With its dedicated focus on data and software infrastructure designed to serve scientists, the EBI offers an \u201cinteresting contrast to the US model\u201d, says Hunter. US funding agencies are not entirely ignoring software engineering, however. In addition to BISTI, the NIH is developing Big Data to Knowledge (BD2K), an initiative focused on managing large data sets in biomedicine, with elements such as data handling and standards, informatics training and software sharing. And as the cloud emerges as a popular place to do research, the agency is also reviewing data-use policies. An approved study usually lays out specific data uses, which may not include placing genomic data on a cloud, says Bonazzi. When a person consents to have his or her data used in one way, researchers cannot suddenly change that use, she says. In a big-data age that uses the cloud in addition to local hardware, new technologies in encryption and secure transmission will need to address such privacy concerns. Big data takes large numbers of people. BGI employs more than 600 engineers and software developers to manage its information-technology infrastructure, handle data and develop software tools and workflows. Scores of informaticians look for biologically relevant messages in the data, usually tailored to requests from researchers and commercial customers, says Xu. And apart from its stream of research collaborations, BGI offers a sequencing and analysis service to customers. Early last year, the institute expanded its offerings with a cloud-based genome-analysis platform called EasyGenomics. In late 2012, it also bought the faltering US company Complete Genomics (CG), which offered human genome sequencing and analysis for customers in academia or drug discovery. Although the sale dashed hopes for earnings among CG's investors, it doesn't seem to have dimmed their view of the prospects for sequencing and analysis services. \u201cIt is now just a matter of time before sequencing data are used with regularity in clinical practice,\u201d says one investor, who did not wish to be identified. But the sale shows how difficult it can be to transition ideas into a competitive marketplace, the investor says. When tackling data mountains, BGI uses not only its own data-analysis tools, but also some developed in the academic community. To ramp up analysis speed and capacity as data sets grow, BGI assembled a cloud-based series of analysis steps into a workflow called Gaea, which uses the Hadoop open-source software framework. Hadoop was written by volunteer developers from companies and universities, and can be deployed on various types of computing infrastructure. BGI programmers built on this framework to instruct software tools to perform large-scale data analysis across many computers at the same time. If 50 genomes are to be analysed and the results compared, hundreds of computational steps are involved. The steps can run either sequentially or in parallel; with Gaea, they run in parallel across hundreds of cloud-based computers, reducing analysis time rather like many people working on a single large puzzle at once. The data are on the BGI cloud, as are the tools. \u201cIf you perform analysis in a non-parallel way, you will maybe need two weeks to fully process those data,\u201d says Xu. Gaea takes around 15 hours for the same number of data. To leverage Hadoop's muscle, Xu and his team needed to rewrite software tools. But the investment is worth it because the Hadoop framework allows analysis to continue as the data mountains grow, he says. They are still ironing out some issues with Gaea, comparing its performance on the cloud with its performance on local infrastructure. Once testing is complete, BGI plans to mount Gaea on a cloud such as Amazon for use by the wider scientific community. Other groups are also trying to speed up analysis to cater to scientists who want to use big data. For example, Bina Technologies in Redwood City, California, a spin-out from Stanford University and the University of California, Berkeley, has developed high-performance computing components for its genome-analysis services. Customers can buy the hardware, called the Bina Box, with software, or use Bina's analysis platform on the cloud. \n               From vivo to silico \n             Data mountains and analysis are altering the way science progresses, and breeding biologists who get neither their feet nor their hands wet. \u201cI am one of a small original group who made the first leap from the wet world to the  in silico  world to do biology,\u201d says Marcie McClure, a computational biologist at Montana State University in Bozeman. \u201cI never looked back,\u201d During her graduate training, McClure analysed a class of viruses known as retroviruses in fish, doing the work of a \u201cwet-worlder\u201d, as she calls it. Since then, she and her team have discovered 11 fish retroviruses without touching water in lake or lab, by analysing genomes computationally and in ways that others had not. She has also developed software tools to find such viruses in the genomes of other species, including humans. Her work generates terabytes of data, which she shares with other researchers. The cultural baggage of biology that privileges data generation over all other forms of science is holding us back. Given that big-data analysis in biology is incredibly difficult, Hunter says, open science is becoming increasingly important. As he explains, researchers need to make their data available to the scientific community in a useful form, for others to mine. New science can emerge from the analysis of existing data sets: McClure generates some of her findings from other people's data. But not everyone recognizes that kind of biology as an equal. \u201cThe cultural baggage of biology that privileges data generation over all other forms of science is holding us back,\u201d says Hunter. A number of McClure's graduate students are microbial ecologists, and she teaches them how to rethink their findings in the face of so many new data. \u201cBefore taking my class, none of these students would have imagined that they could produce new, meaningful knowledge, and new hypotheses, from existing data, not their own,\u201d she says. Big data in biology add to the possibilities for scientists, she says, because data sit \u201cunder-analysed in databases all over the world\u201d. \n                     Computing: A vision for data science \n                   \n                     Big data: Welcome to the petacentre \n                   \n                     The making of ENCODE: Lessons for big-data projects \n                   \n                     European Bioinformatics Institute \n                   \n                     US National Center for Biotechnology Information \n                   \n                     ELIXIR \n                   \n                     Helix Nebula \u2014 The Science Cloud \n                   Reprints and Permissions"},
{"file_id": "494131a", "url": "https://www.nature.com/articles/494131a", "year": 2013, "authors": [{"name": "Vivien Marx"}], "parsed_as_year": "2006_or_before", "body": "Physics- and engineering-based approaches are helping researchers stop the spread of cancer by anticipating tumour cells' moves and habits.  In the 1880s, British surgeon Stephen Paget examined autopsy reports from more than 900 patients with advanced breast cancer. He was trying to understand metastasis \u2014 the process by which tumours spread through the body. Paget reasoned that if all the organs were equally susceptible, with the location of tumour fragments governed by chance alone, then all the organs should be equally affected. Instead, he found that the breast cancers had spread mainly to the uterus and the bones. He suggested that when tumours shed cells that move through the bloodstream, the cells are like seeds that grow only where they find congenial soil. To understand metastasis, it might be useful, he wrote, to study not only the seeds, but the soil too 1 . Paget's 'seed and soil' hypothesis \u201cstill holds forth today\u201d, writes Isaiah Fidler of the University of Texas MD Anderson Cancer Center in Houston 2 . It informs present-day efforts to understand every detail of metastasis \u2014 including the genetic pathways that direct a cancer's growth, the factors that set some tumour cells free to travel through the bloodstream, and their interactions with the environments at the organs in which they settle 3 . Only a few of those cells manage to seed new tumours. Metastasis is a \u201clow-probability event\u201d, says Robert Gillies of the H. Lee Moffitt Cancer Center in Tampa, Florida. Even so, more than 90% of cancer deaths are the result of metastasis. Understanding the processes involved is therefore something of a priority. Studying metastasis in patients would require unethical experimentation, so scientists are creating artificial conditions similar to those in the body. They are developing approaches based on physics, chemistry and engineering, and are increasingly using three-dimensional assays for experiments that are usually two-dimensional. For example, they have created mimics of the extracellular matrix (ECM) \u2014 the body's tissue scaffolding \u2014 in devices that are hooked up to imaging systems. This set-up allows them to track tumour cells over extended periods of time to see how they detach from one spot and attach at another, and how they creep through tissue that should be too dense to let them pass. Other approaches involve scooping migrating tumour cells out of the blood and locking them in highly engineered cages to discover how they seed secondary tumours.  Scientists are making rapid progress thanks to an array of approaches that consider networks of interacting genes and proteins, and breakthroughs in imaging and animal modelling are yielding increasingly comprehensive data (see 'Technology timeline in metastasis research'). Such technological innovations are galvanizing efforts to overcome gaps in the scientific understanding of metastasis, as Yibin Kang, a cancer biologist at Princeton University in New Jersey, and Nilay Sethi, a former member of Kang's lab now at the University of California, San Francisco, have pointed out 4 . \n               Ready to spread \n             Metastasis is thought to begin with the epithelial\u2013mesenchymal transition (EMT), a cascade of events in which tumour cells lose many of their 'epithelial' characteristics and become more like mesenchymal cells with the ability to spread and invade tissue 5 . This process occurs naturally in healthy cells but is poorly understood in tumours. Researchers are not even sure whether it happens early in tumour development or only in advanced tumours. But they know that the transformation involves a chatter of signalling, both within a tumour cell and between cells. For example, both normal cells and tumour cells shed small vesicles, known as exosomes, into the bloodstream. Previously considered a cell's way of putting out the rubbish, exosomes are now known to contain proteins and signalling molecules \u2014 like messages in a bottle \u2014 that prepare the target tissues to accept metastatic cells. We believe that components of the microenvironment matter in the adaptive changes in tumour cells. Even before cells migrate, tumours create their own microhabitat that promotes metastasis. \u201cWe believe there are components of the microenvironment that matter in the adaptive changes in tumour cells,\u201d Gillies says. Clinicians can identify genetic signatures in patients' cancers and prescribe drugs to attack the specific tumour type, but this treatment often fails because tumours activate alternative genetic pathways instead, he says. If clinicians can understand the adaptive landscape of the tumour and its environment, they will have more lines of attack and may even be able to create a habitat that inhibits metastasis. Researchers have found that as tumours grow they create a unique microclimate with highly acidic areas that are low in oxygen \u2014 conditions that seem to favour metastasis. Combining imaging with technology that characterizes cancer metabolism in this microhabitat helps to reveal how tumour cells shift to a metastatic state, Gillies says. Rapid cancer growth is fuelled by a metabolic process called glycolysis. Gillies and his team are able to detect this \u2014 and thereby identify cancer cells \u2014 using an instrument from Seahorse Bioscience in Billerica, Massachusetts, that allows them to analyse both tumour metabolism and the microenvironment in an automated fashion. \u201cSince we got the device, it has been running non-stop,\u201d he says. The instrument, an XF Analyzer, measures the acid produced by glycolysis in cells, thereby identifying tumour cells, which have a high glycolysis rate, says Min Wu, director of biology at Seahorse Bioscience. A sensor cartridge positioned above the cells creates a temporary microchamber and measures oxygen and pH levels. This arrangement lets scientists detect cell metabolism in real time, typically within 1\u20132 hours, says Wu. Buffers that neutralize the tumour's acidity have been shown to make chemotherapies more effective and inhibit metastasis. To determine the right buffer mix, Gillies uses the analyser to type cells according to their metabolic profile. Several research groups are also exploring how drugs can be tailored to release a warhead loaded with anticancer drugs into tissue areas with a profile characteristic of cancer. \n               On the move \n             The next step in metastasis is for cells to break away from the tumour so that they can enter the bloodstream. Several relatively low-tech but tried-and-tested assays can help scientists watch this migration. Companies such as EMD Millipore of Billerica, Massachusetts, and Cell Biolabs in San Diego, California, offer two-dimensional assays that help researchers track and quantify migration and chemotaxis (the movement of cells towards a chemical signal). Scientists sometimes build these assays themselves, but the companies say that standardized products result in more comparable experimental results. One technique developed in the 1960s to study cell migration is the Boyden chamber, which is essentially a small cup containing cells placed inside a larger cup containing growth medium or chemoattractant to attract cancer cells. The two chambers are separated by a porous membrane through which cells can migrate. In this way, scientists can track chemotaxis and see which cells migrate through the membrane. In a similar test known as a scratch assay, tumour cells are spread in a layer across growth medium in a plate. When the surface is scratched \u2014 by a researcher running a pipette tip across it, for example \u2014 cells move together to fill in the artificial gap, much as they would when closing a wound in the body. Richard Klemke, a cell biologist at the University of California, San Diego, and a consultant at EMD Millipore, developed a comb for the scratch assay, putting \u201ca new spin on a very old technique\u201d, says Jun Ma, a product manager at EMD Millipore. The comb makes uniform, high-density scratches that induce large numbers of cells to migrate synchronously for scientists to image, says Klemke.  A German company, Ibidi, based in Munich, offers what it believes is an improved migration assay. Most labs use the Boyden chamber to study chemotaxis, but Ibidi's president, Roman Zantl, is aware of its limitations. \u201cI doubt that it gives valid results in case of adherent cancer and other cell types,\u201d he says. The synthetic membrane between the two cups is unlike the material through which cells travel in the body and might alter the cells' behaviour. And although the assays work well enough for fast-migrating cells, such as leukocytes, they are less suited to the slower movement of tumour cells, which might not be counted if they take too long to complete their journey across the membrane from one cup to another. To address these issues, Ibidi developed a migration assay based on the Zigmond chamber, an approach developed in the 1970s that uses two reservoirs, one containing cells and a second containing a chemoattractant. In Ibidi's version, cells move along a chemoattractant gradient across a small area that can be viewed under a microscope. Diffusion of the chemoattractant is slow, allowing researchers to use time-lapse microscopy to watch cells over long periods of time. These two-dimensional migration assays are \u201cgreat\u201d, says Mingming Wu, a bioengineer at Cornell University in Ithaca, New York. Wu says she uses them because \u201ccancer metastasis is a game of going somewhere\u201d. But she hopes to be able to model migration in three dimensions. \n               Seeing 3D \n             \u201cWhat you see in two dimensions has very little apparent correlation with what you see in three dimensions,\u201d says Wu. In a two-dimensional assay, tumour cells use adhesion molecules to inch along. \u201cIf you disable the adhesion molecules, the cells cannot move,\u201d she says. But in a three-dimensional assay, cancer cells switch to a different type of movement when their adhesion molecules are disabled. \u201cA cancer cell is very smart in some ways,\u201d Wu says. It's a physics problem in some ways, it's an integrated problem. Imaging experiments by several researchers, including John Condeelis at Albert Einstein College of Medicine in New York, have provided a visual, three-dimensional view of metastasis in mice, says Wu. Now she wants to bring this observational power to an engineered environment that realistically mimics human physiology. \u201cIt's a physics problem in some ways, it's an integrated problem.\u201d Wu has developed a device in which tumour cells move through a synthetic version of an ECM. As they move, they generate force and migrate along the ECM fibrils to reach the bloodstream. \u201cSome of them climb the ropes. They extend and cling onto fibres and pull themselves, which makes them more elongated, but some are more rounded,\u201d she says. Where the matrix is too dense, tumour cells secrete chemicals to create a passageway. The device has three tiny channels patterned into the surface of a growth medium, and Wu places tumour cells embedded in ECM in the middle channel. Chemoattractants known to affect cancer cells are added to another channel. Wu uses time-lapse microscopy to observe what happens. To explore why breast cancer cells first metastasize to lymph nodes, the researchers use chemoattractants that correspond to molecules present in lymph nodes. The device makes it possible to observe tumour cells interacting as naturally as possible with one another, with healthy tissue cells and with the body's immune cells. Once again, commercial alternatives are available for scientists who do not want to build their own microfluidic platform. Last year, for example, EMD Millipore began selling one after acquiring CellASIC, which launched the platform in 2007. The company's programmable perfusion system is hooked up to a microscope and lets scientists use software-based commands instead of manual ones to make cells or molecules flow in or out of a microenvironment. They can investigate factors that might promote or thwart a metastasis-like state in cells and \u201cget real-time movies of these cells as they're behaving\u201d, says bioengineer Alex Mok, who was in Luke Lee's lab at the University of California, Berkeley, where the device was developed, and is now a product manager at EMD Millipore. Mok describes the device as \u201ca semiconductor chip for biologists\u201d that should allow more researchers to do this kind of experiment. \n               Scooping up the seeds \n             A cancer patient's blood may harbour only one metastatic cell for every 1 billion to 5 billion other cells. Once the malignant cells enter the bloodstream they are known as circulating tumour cells (CTCs). Clinicians can gauge how a breast, prostate or colon cancer patient is faring from the number of CTCs in the blood and from their molecular characteristics. However, CTCs are extremely difficult to isolate. Test instruments typically sample 5\u201310 millilitres of blood, a tiny fraction of the body's total of 5 litres. And a cancer patient's blood may contain only one metastatic cell for every 1 billion to 5 billion other cells, says Mehmet Toner, a bioengineer at Massachusetts General Hospital in Boston. \u201cIt's a statistical nightmare.\u201d One device that can help to improve the odds of finding CTCs is the CellSearch, made by Veridex, a subsidiary of Johnson & Johnson based in Raritan, New Jersey. Approved by the US Food and Drug Administration in 2004, it tags tumour cells with magnetic beads coated with antibodies that adhere to proteins found on the surface of tumour cells. The CTCs are then separated using a magnet and are then stained and imaged. Toner and other scientists want devices that capture CTCs to be more sensitive and to keep the cells viable for further analysis, which CellSearch does not. The technical challenge and its commercial promise have led to a flurry of device-building. \u201cThere are lots of good ideas out there,\u201d Toner says. His own team has been developing different types of bank-card-sized devices coated with antibodies that trap CTCs by attaching to proteins on their cell surface. In his most recent device, the herringbone-chip (HB-chip), blood flows in a see-through channel that lets scientists image the captured cells. The herringbone pattern of grooves on the inside of the channel's roof creates a gentle vortex in the blood, increasing the number of tumour cells that stick to the antibody-coated surface, he says. \n               A liquid biopsy \n             Toner thinks that CTC chips such as this will help cancer researchers understand the separate steps in the transformation from tumour cells to the metastatic seeds of a new tumour. The devices make it possible to use extremely high-definition imaging, which can enhance the work of pathologists, he says. In a test using blood from patients with metastatic prostate cancer, Toner's team was able to distinguish between circulating tumour cells before and after medical treatment, an ability that could help to guide therapy. In a separate test, the researchers found higher levels of a signalling molecule called WNT2 in circulating pancreatic tumour cells than in primary tumour cells 6 . This difference is a potential biomarker that could help drug developers find ways to inhibit metastasis. The test also revealed metastatic cells' survival strategies. Cells need to be anchored if they are to survive \u2014 once they are in the bloodstream, even tumour cells should die, Toner says. But turning on the  Wnt2  gene helps them avoid cell death. \u201cCells that express  Wnt2  can survive in a suspended state,\u201d he says.  In contrast to CellSearch, Toner's chip does not need blood samples to be preprocessed. It also keeps cells viable for further profiling. In 2011, his team landed a five-year, US$30-million collaboration agreement with Johnson & Johnson to refine CellSearch technology with a new approach to characterize tumour cells. Toner says he cannot comment on the progress yet, but he challenges views that metastatic cells are too scarce to capture in sufficient numbers or that microfluidics cannot be used for large-scale blood analysis. Using such a tiny device has engineering efficiencies. \u201cMicrofluidics gives us multiplexing, building redundancy into the system, and it gives us very uniform conditions,\u201d he says. Many other labs are also working on CTC chips, including the National University of Singapore, Stanford University in California, Louisiana State University and the University of California, Los Angeles (UCLA). Hsian-Rong Tseng, a chemist at UCLA, says CellSearch has raised awareness among scientists, physicians and patients that CTCs allow \u201ca biopsy directly from the blood\u201d. Tseng's team has developed a microfluidic 'Nano-Velcro' assay, in collaboration with researchers at the RIKEN Advanced Science Institute in Saitama, Japan, and several institutions in China, including Wuhan University. Tseng's group has now founded a company called CytoLumina Technologies to commercialize the instrument. Tseng's device can capture and release live CTCs from a blood sample. When the team used it to analyse CTCs from patients with metastatic melanoma, they sequenced the cells' DNA and identified clinically relevant mutations. To ramp up the scale at which it can be used with genetic testing, Tseng teamed up with the Beijing Genomics Institute in China. The device contains polymer-coated silicon nanowire brush hairs studded with antibodies that match proteins on the surface of metastatic cells. As the blood flows through the device, also aided by a herringbone pattern, the tumour cells stick to the surface \u2014 hence the Nano-Velcro name. \u201cWe can release them simply by changing the temperature,\u201d he says. Lowering the temperature changes the polymer's configuration, pulling the antibodies inwards, allowing the tumour cells to detach and flow out of the channel \u2014 viable, intact and ready for further study. \n               Lessons from the heart \n             The final stage of metastasis is when the CTCs find fertile 'soil' in the body, to use Paget's term, and begin to grow. It's not clear what makes particular environments hospitable to certain tumour cells, but one place to look for answers is the heart \u2014 \u201cthe one organ in the body that has actually beaten cancer\u201d, according to cardiology researcher Jay Schneider at the University of Texas Southwestern Medical Center in Dallas. Some cancers such as melanoma can metastasize to the heart, but it is \u201cvery, very rare\u201d, he says. Attempts to achieve metastasis to the heart experimentally have worked only with one cancer cell line, he adds. The heart's defence mechanism remains unknown, but it seems to be shielded by either mechanical or physical barriers that make the microenvironment hostile to CTCs. It's likely that some component of the heart's microenvironment \u2014 maybe its tissue, cells or the scaffolding around the cells \u2014 is protecting it. One way to understand the mechanism is to identify what aspect of the 'soil' makes healthy tissue vulnerable to metastatic seeds. Biomedical engineer Sangeeta Bhatia and her graduate student Nathan Reticker-Flynn at the Massachusetts Institute of Technology (MIT) in Cambridge have built a platform to help them study the interactions between tumour cells and various synthetic models of the ECM (the dense weave of fibrils that make up the scaffolding connecting cells in tissues). The pair want to learn which components of the ECM are most hospitable to metastatic seeds and favour cell migration and adhesion. Far from being mere stabilizing mortar, the ECM has \u201cits own universe of biology; it is a signalling hub\u201d, she says. \u201cAnd the cells are constantly modifying it, so it's a dynamic and biologically active glue.\u201d In a primary tumour, cancer cells stick to one another. To spread, metastatic cells must first detach from their mooring, then move within the ECM to the bloodstream, travel in the blood until they reach a suitable destination, and then attach to the ECM and grow into a tumour there. \u201cThe attachment at that metastatic site is what this experiment was about,\u201d she says.  Bhatia's team used robots to print arrays of ECM dots, each with a different composition. The researchers scoured catalogues to find all the synthetic ECM molecules on the market and developed 800 unique combinations. They coated glass slides with polyacrylamide, which swells to trap the ECM molecules in one spot, and then seeded metastatic lung cancer cells on the spots. \u201cWe developed a platform to query these all at once,\u201d Bhatia says, and it has attracted so much interest that she plans to distribute it commercially. The researchers imaged the different ways the cells adhered to the ECM spots and compared the adhesion profiles. \u201cWe found they have gained the ability to stick to different things than in the primary tumour,\u201d she says. As the cells grew over a period of time, the scientists used a clustering algorithm to find patterns in the data. They hope that understanding how tumour cells adhere to the ECM could open the way for a potential therapeutic to interfere with this ability. \n               The gaze of physicists \n             Metastasis research is benefiting from a multidisciplinary approach that includes biologists, physicists, chemists and engineers. In 2009, for example, the US National Cancer Institute (NCI) started funding a network of 12 Physical Science\u2013Oncology Centers (PSOCs). In 2010 it launched a Provocative Questions initiative requesting research proposals that address \u201cperplexing\u201d problems in cancer, such as devising engineering approaches to improve the study of metastasis. Another NCI scheme, the Innovative Molecular Analysis Technologies (IMAT) programme, launched in 1998, also fosters these partnerships and aims to develop tools that could accelerate cancer research, says IMAT director Tony Dickherber. \u201cOur understanding of how metastasis works, and the importance and complexity of the microenvironment, is significantly influenced by what tools we have to tell us about either of those things,\u201d he says. Several IMAT-funded projects have become widespread research tools as well as commercial products. \u201cI really appreciate the community they have brought together,\u201d says Tseng, who received a grant from IMAT. \u201cWe don't prescribe what kinds of innovations we're interested in, we want investigators to come to us and surprise us with their innovative ideas,\u201d Dickherber says. The programme puts together interdisciplinary review panels to score prospective research tools according to their potential impact. Tyler Jacks, who directs the MIT's David H. Koch Institute for Integrative Cancer Research, says his institute is \u201cexpressly about bringing biologists and engineers together under one roof\u201d. Collaborations are encouraged by ensuring that everyone circulates in the same sections. \u201cWe're neighbours now and we interact much more extensively,\u201d he says. Physicist Jean-Fran\u00e7ois Joanny at the Curie Institute in Paris is also using his expertise to benefit cancer biology. Building on work by the late Malcolm Steinberg, a Princeton University biologist, Joanny's team has looked at how pressure affects the growth of clusters of cancer cells known as spheroids. Perhaps a physical property is what distinguishes cancer cells. \u201cCould you characterize the degree of invasiveness by looking at properties like this?\u201d he says. \u201cThat's the idea at the back of our minds.\u201d One cancer biologist proposed a scientific challenge for Joanny's group. He asked the physicists to make pressure measurements in a mouse with multiple tumours and determine, from those measurements alone, which cancer is most likely to metastasize. As the biologist knows the correct answer, his team will use biology to validate the physics-based results. The Curie Institute has a tradition of cross-disciplinary collaboration, with cancer biologists approaching suggestions from physicists with open minds, Joanny says. \u201cWe are used to the idea that they might consider us crazy.\u201d But if wild ideas from physicists can boost the fight against cancer \u2014 and their work to help biologists understand the various processes of metastasis suggests that they can \u2014 then maybe they're not quite so crazy after all. \n                     Evolutionary dynamics of carcinogenesis and why targeted therapy does not work \n                   \n                     Unravelling the complexity of metastasis \u2014 molecular understanding and targeted therapies \n                   \n                     Why don't we get more cancer? A proposed role of the microenvironment in restraining cancer progression \n                   \n                     US National Cancer Institute, Physical Sciences Oncology Centers \n                   \n                     NCI Provocative Questions \n                   \n                     Innovative Molecular Analysis Technologies \n                   Reprints and Permissions"},
{"file_id": "503147a", "url": "https://www.nature.com/articles/503147a", "year": 2013, "authors": [{"name": "Vivien Marx"}], "parsed_as_year": "2006_or_before", "body": "Tools that make it possible to chart every neuron and its connections are helping neuroscientists to realize their dream of whole-brain maps. Researchers wanting to understand the workings of the brain need maps on many different scales \u2014 to link structure to function, to parse the complexities of memory loss or learning disability, or to find out what is different about the brains of people with neurodegenerative diseases. For instance, programmes such as the Human Connectome Project, funded by the US National Institutes of Health (NIH) in Bethesda, Maryland, and the Consortium of Neuroimagers for the Non-invasive Exploration of Brain Connectivity and Tracts (CONNECT), funded by the European Commission in Brussels, are producing magnetic resonance imaging (MRI) maps that follow neuronal connections across the entire brain, on a scale of tens of centimetres. Some research teams are embarking on even more ambitious projects to create maps that reveal structure and connections at the scale of individual neurons. The slender processes of neurons \u2014 the axons and dendrites \u2014 are 20 micrometres or less in diameter, but can extend for several millimetres. Mapping the thicket of 86 billion neurons and their connections in the human brain at this scale is still a distant dream, but the goal of mapping the 75 million neurons in the mouse brain could be a little closer. And the second might pave the way for the first. To achieve this, however, researchers need new tools, starting with refinements to electron microscopy. In conventional electron-microscopy approaches, scientists have to make ultrathin slices of brain tissue and laboriously image them slice by slice to build up a three-dimensional (3D) picture. In 1986, the complete nervous system of one model organism was mapped using this approach \u2014 the 302 neurons of the nematode  Caenorhabditis elegans 1  \u2014 but the process is too slow and cumbersome to scale up. Now, electron microscopes at many universities are \u201cbeing mothballed\u201d, says neurobiologist Jeff Lichtman at Harvard University in Cambridge, Massachusetts, because they are deemed a throwback to an era when \u201call you could do was stain and look\u201d. Nevertheless, high-resolution, 3D neuronal mapping by electron microscopy is gaining momentum. \u201cIt's not just the old neuroanatomy gussied up with new machines,\u201d says Lichtman, who studies the neuronal architecture of the mouse brain. \u201cIt's giving us three-dimensional information about structure at the super-resolution of nanometres \u2014 and that is invaluable.\u201d These capabilities are spurring the development of new ways to prepare, image and analyse brain tissue from model organisms such as mice, and from people who have donated their brains to science on their death. Until now, when neuroscientists published cell-level images and analysis, they were looking at a tiny region of brain; for example, a few hundred neurons in the mouse or fruitfly 2 , 3 . Efforts to map all the neurons and circuits in the mouse brain are just beginning in several labs. Creating structural maps is a high priority of the NIH-funded Brain Research Through Advancing Innovative Neurotechnologies (BRAIN) Initiative, which US President Barack Obama launched in April 4 . Another large-scale effort is the Human Brain Project, funded by the European Union, which is focused on a different kind of map \u2014 a computational model of the brain. Neurobiologist and microscopist Winfried Denk at the Max Planck Institute for Medical Research in Heidelberg, Germany, thinks that the technology to image an entire mouse brain at cellular resolution is within grasp. Late next year, he and Lichtman are each scheduled to receive a new multi-beam scanning electron microscope (SEM) for their labs, developed by Carl Zeiss Microscopy in Oberkochen, Germany. Now in the prototype stage, this instrument will enable the imaging of brain slices to be accelerated hugely \u2014 possibly by as much as 60 times. Such gains could make it feasible to map all the neurons in a mammalian brain. Other microscope manufacturers, such as FEI in Eindhoven, the Netherlands, are also developing tools to enable detailed brain mapping. \n               Preparing to see \n             Before brain tissue can be imaged using electron microscopy, samples must be prepared by slicing and staining. These steps are important for the subsequent process of using the two-dimensional images generated by the electron microscope to reconstruct the three dimensions of the brain and its neuronal connections. Scientists are exploring new ways to prepare samples so that they can obtain higher contrast for electron microscopy and view larger pieces of brain tissue. When brain tissue is well stained with chemicals, thinly sliced and viewed using electron microscopy, researchers see slices that look like thin plates covered with tiny soap bubbles. Some bubbles represent the cross-section of a neuron, and the soap-bubble boundary shows where one neuron ends and another begins. Contrast is crucial, because scientists need to discern neurons from a wealth of other cell types and organelles in the brain tissue. They can then trace each neuron in each of the imaged slices, by hand and by eye. But such imaging has its problems: staining often does not show enough structure, the slices might be warped and images can be blurry. Denk and his team previously developed an approach called serial block-face electron microscopy (SBEM), which obviates the need to prepare slices before imaging them. Instead, successive images are obtained by scanning the face of an unsliced block of tissue placed in the electron microscope, then cutting off an ultrathin slice using an automated microtome within the instrument. The newly exposed surface of the sliced block is rescanned, and so on until a stack of images has been obtained. The advantage of imaging the unsliced tissue, Denk says, is that it does not matter if the slice itself crumples. Until now, Denk and his team have cut only from pieces of tissue that measured much less than 1 mm across. But Denk's ultimate aim is to image and cut slices from a whole mouse brain, which will mean dealing with tissue blocks that are some 10 mm across. Denk is now building a whole-brain microtome to incorporate into the microscope. This will require a diamond knife that is 8\u201310 mm wide instead of the 1-mm knife used at present, he says. To make these knives, he is collaborating with diamond-knife manufacturer Diatome in Biel, Switzerland. The longer blade width is a challenge, says Diatome engineer Helmut Gnaegi. At 10 mm, it is around ten times wider than a typical diamond knife, and the blade must be made from a large diamond that is free of crystalline imperfections (see 'Sharp tools'). The entire 10-mm cutting edge must be polished to perfection, Gnaegi says, because even a single knife mark would interfere with the smooth sample surface that Denk requires. After polishing, the cutting edge must have a maximum thickness of only 4 nm. This is a \u201cformidable challenge\u201d that the company hopes to overcome by the end of this year, says Gnaegi. He and his team have designed special polishing equipment for the task. Gnaegi's team is also addressing the build-up of electrostatic charge that occurs when the knife tries to cut through tissue that has been embedded in non-conducting resin. The charge tends to make the sections stick to the sample surface, which makes imaging difficult. \u201cAn electrically conductive knife surface with optimized gliding properties leads to a sample surface free of cutting debris,\u201d says Gnaegi. The SBEM approach helps with precise rendering of tissue in three dimensions, but it is hard to make it high-throughput. Researchers would have to set up many microscopes in a dedicated facility. The alternative is an instrument that is \u201cintrinsically a parallel imaging machine\u201d, Denk says. That is why his lab is so interested in the Zeiss multi-beam machine that is due next year. Lichtman and his team take a different approach to sample preparation. He has developed an automatic tape-collecting ultra-microtome (ATUM), which slices tissue, places each slice on a tape and delivers the slices to the microscope in a conveyor-belt fashion. Still other labs are working on different tissue-slicing approaches. \u201cOne of them will eventually emerge as the way to do things, probably,\u201d says Denk. Many labs are working on ways to improve the conventional tissue-staining methods used for electron microscopy, which rely on heavy metals such as lead and osmium. For example, Shawn Mikula, a postdoc in Denk's team, has been experimenting with immersing whole, unsliced mouse brains in various staining chemicals for extended periods to enhance the spread of osmium throughout the tissue and to achieve adequate contrast in the electron microscope. This procedure worked well for imaging axons, Denk says, but was less successful for imaging the finer processes 5 . Given time, he says, \u201cwe're pretty confident that we'll figure it out.\u201d \n               Beam bonanza \n             As part of their collaboration with Zeiss, both Lichtman and Denk deliver test samples to the company, which are imaged using the prototype microscope. At present, the number of scientist-testers has to be limited, and the instrument is not available for pre-order. Looking at images generated by the device has been \u201cvery exciting, because you realize how much faster those images were taken\u201d, says Lichtman. When he and his team run experiments, the lab generates around 1 terabyte of image data a day. With the new microscope, he says, the rate will be more like 3 terabytes per hour. Using his tape-collecting microtome with the new instrument, Lichtman plans to slice a mouse brain into many tens of thousands of sections, which can be put into the machine in large groups. \u201cIt images all of them and then you put the next several hundred or thousand in,\u201d he says. The Zeiss multiple-beam SEM resembles a standard SEM in many ways, says Dirk Zeidler, the physicist who led its design. It has an electron source, lenses and a scanning unit that moves the beam across the sample surface horizontally, in the same way as a reader's eye moves over lines of text. It is built to be compatible with the various ways in which neuroscientists prepare tissue for imaging. But the differences from a conventional SEM are crucial. The machine has been engineered for a wide view of the tissue and for speed, says Gregor Dellemann, who is the business-development manager for the new microscope. The most obvious difference is that the instrument has 61 electron beams scanning across the sample instead of the one beam in a conventional SEM, and an array of 61 secondary electron detectors (see 'Speed reading'). It also dispenses with features such as energy-dispersive X-ray spectroscopy detectors or back-scattered electron detectors, both of which are used in standard electron microscopes to map variations in a sample's chemical composition. Dellemann explains that including several detector types would mean incorporating three arrays of 61 detectors each, and would require a mechanism to guide the right signal to each one. Although possible, such a configuration would add cost and complexity to the instrument and slow it down. Some scientists had voiced doubts about the multiple-beam SEM when they first heard about it, says Zeidler, who notes that they even questioned whether it might violate the principles of optics. But they recanted when shown a working instrument as it generated images, he says, adding \u201cthat was quite fun\u201d. The new instrument addresses the speed limits and blind spots of a standard SEM. When the beam of electrons hits the sample, it emits secondary electrons that give information about, for example, the sample's surface shape and staining. Detectors record the intensities of the secondary electrons and the instrument analyses these data to give an image of the sample. To speed up this process, the beam would need to move more quickly and detect secondary electrons in less time. But when detection times fall below about 50 nanoseconds, the images get noisy. Furthermore, in the brief moment when electrons hit the detector, \u201cit goes blind\u201d, Dellemann says. No data can be collected until this 'dead time' is over and the detector 'sees' again. Enhancing the abilities of an SEM using multiple electron beams and detectors for wider and quicker imaging requires attention to be paid to basic physics. Image quality suffers if the multiple beams are too close to each other because the electrons repel one another. \u201cIt will get blurry, which means you do not see the features you want to see,\u201d says Zeidler. The solution was to spread the charge over a larger area, minimizing the forces between electrons. More beams cover a larger sample area, and more detectors mean that the temporary blind spots are less of a hindrance. But the team still had to optimize the number of beams and their spacing, Dellemann says. Adding electron beams in a hexagonal configuration turned out to be the best geometry, and 61 beams with 61 detectors turned out to be the optimal number. Most SEMs detect electrons for intervals of around 50 nanoseconds, such that the observer can image 20 million pixels per second before any physical limits are reached, Dellemann says. In a multi-beam instrument, each beam has a dedicated detector in the array and the 50-nanosecond detection speed stays the same. So with 61 beams, the images can get bigger. In 1 second, Dellemann says, the instrument can take images of up to 1.2 billion pixels. Zeiss started developing the instrument in 2000, when the company began exploring multi-beam SEM for the semiconductor industry in partnership with Applied Materials of Santa Clara, California. The aim was to develop technology that could inspect semiconductors using partially automated, parallel imaging. A few years later, Zeiss realized the potential of this technology for the life sciences, with the most interest coming from neuroscientists who wanted high-resolution brain mapping. \n               Energy boost \n             At microscope company FEI, engineers are souping up SEMs in a different way, using a single electron beam. The prototype of a new multi-energy deconvolution SEM is currently undergoing testing at the company, says Ben Lich, who handles business development for the life sciences at FEI. This instrument, too, is intended to speed up neuronal mapping. Lich and his team have long worked with neuroscientists to optimize the standard components of SEMs. The US BRAIN Initiative and other ventures are encouraging the company to \u201cdo a bit more\u201d for neuroscience, Lich says, so that researchers in the field can extract more information from their samples. Current technology has a hard time imaging the more slender processes. To see cell bodies and axons, tissue sections need to be 30\u201340 nm thick, but to see the fine processes, they can be only 10\u201320 nm thick. Missing thinner branches in the imaging increases the risk of inaccuracies when reconstructing 3D circuits of connected neurons. To address this challenge, FEI's new microscope performs 'sub-surface imaging'. It takes multiple images of the same area of a sample and is able to image below the surface. It can help scientists to track down finer processes that they may have missed at a different resolution, Lich says. The instrument sends sequential electron beams into the sample at differing speeds \u2014 starting at 1 kilovolt and ramping up to around 2.5 kV. This way, it obtains more information from the sample than a conventional single-scan microscope can, Lich says. After it acquires images, the instrument applies a mathematical technique called deconvolution, which separates the information from the different depths in the sample and produces virtual sections. The instrument lets researchers scan their samples with varying resolution. A first image at low magnification might reveal cell bodies, which are important for neurobiology but not for understanding neuronal connectivity. An area that is densely forested with finer processes might merit a high-resolution scan. Microscope manufacturers say that their experience in other fields helps with the challenge of whole-brain mapping, which will involve imaging many samples for months at a time. FEI, for example, has microscope customers who are checking the quality of small structures on semiconductors or determining the association of minerals and precious metals in mining ores. Their instruments run 24 hours a day, seven days a week for many months, says Lich, and they hold up. The engineering of microscopes for neuroscience in particular also means helping to achieve better contrast from all samples, including those that might not be well stained. Lich says that FEI has designed its electron-beam column to optimize detection sensitivity. In juggling such factors as data quality, scanning speed and robustness, FEI's team designed the multi-energy SEM so that its detectors for back-scattered electrons are engineered for speed, particularly at low energies. Using back-scatter signals is a more stable way to create images, Lich says. Secondary electron detectors are more prone to showing sample-charging problems, which is when the interaction between the electron beam and the sample leads to distorted images. In areas where the sample is charged, the acquired data can lead to ambiguous interpretations of neuronal connectivity. Back-scatter images are not as sensitive to this problem, he says. For neuron mappers, FEI's method for SEM imaging offers a way to slice a sample virtually before physically cutting it, Lich says. The microscope's electron beam hits an area multiple times, generating a virtual image stack. Rather than needing to physically slice tissue to much less than 40 nm thickness, the sample can be imaged ten times at 4-nm resolution to create a virtual slice of the desired width. The technique could reduce the need for ultrathin slicing and the consequent risk of sample damage and distortion. Physical slices could remain relatively thicker, and intermediate layers in the slice would still be resolved, says Lich. And for a scientist using Denk's approach of imaging followed by slicing, the tissue can be virtually imaged several times to different depths before it is physically shaved off. The new SEM technology quickly creates issues of scalability for those who want to image large brain sections, Lich says. If scientists want data at 5-nm resolution, every voxel \u2014 a 3D pixel \u2014 must be 5 nm on each side. At this resolution, the FEI microscope can obtain images, but imaging a cube of brain tissue measuring 1 mm on each side would yield 8 petavoxels of data \u2014 and those could take years to analyse, says Lich. In his view, the neuroscience community is pushing microscope development with its structural mapping projects. Some, he says, might call the plan to image the whole mouse brain outrageous. \u201cIn principle, nothing is impossible \u2014 but it is really an extraordinary challenge,\u201d he says. \n               Data analysis \n             As neuroscience ramps up its mapping efforts, the challenge of big data becomes more apparent. Mapping the human brain is as difficult as creating a 3D map of a city 10,000 times the size of Tokyo, London or New York, and locating every inhabitant in every building, street, stairwell, lift and subway. To establish the same connectivity that neuroscientists are hoping to achieve, the mapping would then have to unearth all lines of communication \u2014 personal interactions as well as those by phone, post and e-mail \u2014 between all inhabitants of these 10,000 cities, explains Moritz Helmstaedter at the Max Planck Institute of Neurobiology in Munich, Germany. Denk estimates that capturing images of a whole mouse brain with the new Zeiss SEM could deliver 60 petabytes of data. And Lichtman notes that such large amounts of data are going to be even more challenging to share than they are now. At present, he sends his microscopy data sets to the Open Connectome project at Johns Hopkins University in Baltimore, Maryland. The project's website is an open repository for data accrued from different labs working on the brains of model organisms such as mice and nematodes. It enables anyone to download information from MRI, electron-microscopy and brain-wiring studies. Lichtman has one data set that is around 100 terabytes in size. \u201cHow do you give that to somebody?\u201d he asks. As data sets get even larger, the community will have to explore ways to make data available to scientists for perusal without download, he says. The next-generation microscopes can deliver the high-resolution images, but new approaches, including computational tools, are needed to build 3D wiring diagrams of neuronal circuits based on those images. Denk and colleagues in the United States and Germany have recently used SBEM to map a small section of mouse retina 2 . Even with computer software, tracing the neuronal pathways in the images would have taken vastly more time and more people than are available in a typical lab. The team turned to crowd-sourcing, enlisting the help of a host of non-scientists sitting at their home computers. Software to help researchers with these tasks is being developed, and computer scientists plan to keep pace with the advances in microscope technology, Lichtman says. Image-analysis software under development includes tools with built-in machine learning and other types of algorithm to automate or semi-automate the tasks. Denk says that he used to think that researchers had to solve data-analysis problems before imaging larger brain volumes and generating huge data sets. But his view has changed. \u201cMy current thinking is that we'll just produce a large volume,\u201d Denk says. \u201cThen, that will embarrass the analysis people into coming up with something.\u201d Neuroscientist Sebastian Seung at the Massachusetts Institute of Technology in Cambridge, who collaborated on the mouse retina mapping study 2 , leads a group that launched the online game EyeWire. More than 80,000 volunteers worldwide have helped to trace the complete 3D contours of neurons running through the game's electron-microscopy images. He is undaunted by the large data sets that brain mappers will be sending his way for analysis. \u201cWhy would we be frightened?\u201d he says. \u201cIt is just an opportunity.\u201d Size in image data sets is a plus when imaging the mammalian nervous system. Imaging too small a section risks missing neuronal connections, he says. Still, the challenge in mammals is formidable. When scientists image fruitfly brain tissue, a small imaging volume can capture much of a neuronal circuit, because many neurons do not run and branch over long distances, says Seung. In mammals, many researchers trace connections found in the retina, because much of the circuit can be captured with a small tissue volume, he says. But that is already a huge challenge \u2014 and it gets worse when tracing connections in the brain, because neuronal circuits are spread out over much larger volumes in mammals than in flies. \u201cYou can't really map out connections and circuits unless the volume you are imaging is reasonably large,\u201d he says. Lichtman believes that the neurobiology community will ultimately find a way to garner the required knowledge because researchers stand to learn so much about the brain. \u201cWithout it, there are lots of mysteries,\u201d he says. \u201cWith it, mysteries become just facts.\u201d \n                     NIH serves up wide menu for US brain-mapping initiative \n                   \n                     Neuroscience: Solving the brain \n                   \n                     US brain project puts focus on ethics \n                   \n                     The benefits of brain mapping \n                   \n                     Nature Methods Focus on Mapping the Brain \n                   \n                     US government BRAIN Initiative \n                   \n                     Human Connectome Project \n                   \n                     CONNECT \n                   Reprints and Permissions"},
{"file_id": "501261a", "url": "https://www.nature.com/articles/501261a", "year": 2013, "authors": [{"name": "Vivien Marx"}], "parsed_as_year": "2006_or_before", "body": "Advances in high-throughput sequencing are accelerating genomics research, but crucial gaps in data remain. To understand why high-throughput gene-sequencing technology often produces frustrating results, says Titus Brown, imagine that 1,000 copies of Charles Dickens' novel  A Tale of Two Cities  have been shredded in a woodchipper. \u201cYour job is to put them back together into a single book,\u201d he says. That task is relatively easy if the volumes are identical and the shreds are large, says Brown, a microbiologist and bioinformatician at Michigan State University in East Lansing. It is harder with smaller shreds, he says, \u201cbecause if the sentence fragments are too small, then you can't uniquely place them in the book\u201d. There are too many ways they might fit together. \u201cAnd it's harder still if the original pile of books includes multiple editions,\u201d he says. Researchers in genetic sequencing today face a similar task. An organism's DNA \u2014 made up of four basic building blocks, or bases, denoted by the letters A, T, C and G \u2014 is chopped into short snippets, sequenced to determine the order of its bases and reassembled into what researchers hope is a good approximation of the organism's actual genome. Today's high-throughput sequencing technology is remarkably powerful and has led to an explosion of sequencing projects in laboratories around the world, says Jay Shendure, a molecular biologist who develops sequencing methods at the University of Washington School of Medicine in Seattle. Thousands of patient tumours and more than 10,000 vertebrate species have been or are being sequenced. High-throughput sequencing is now an essential tool for basic and clinical research, with applications ranging from detection of microbial 'bio-threats' to finding better biofuels 1 . But some types of genomic DNA cannot be sequenced by high-throughput methods, leaving many frustrating gaps in data (see  'What makes a tough genome?' ). For example, a genome might contain long stretches in which the sequence simply repeats \u2014 as if Dickens had filled whole pages with a word or sentence written over and over \u2014 making that passage hard, if not impossible, to reconstruct by the usual technologies. And the widespread adoption of next-generation sequencing has meant that the quality of genome assemblies has declined significantly over the past six years, says Evan Eichler, a molecular biologist also at the University of Washington. Although \u201cwe can generate much, much more sequence, the short sequence-read data translate into more gaps, missing data and more incomplete references,\u201d he says. Incomplete genomes make it harder for researchers to identify and interpret sequence variations. \u201cInstead,\u201d Eichler says, \u201cwe focus only on the accessible portions, creating a biased view,\u201d which in turn hinders efforts to study the genetic basis of disease or how species have evolved. For example, the human-genome sequence, used as a reference by scientists around the world, has more than 350 gaps, says Deanna Church, a genomicist at the US National Center for Biotechnology Information. An updated reference genome is filling in much of the missing data, but \u201ceven with the release of the new assembly, there will still be gaps and regions that aren't well represented,\u201d she says. \u201cIt is definitely a work in progress.\u201d More than 900 human genes are in regions where there is much repetition. About half of these genes are in areas so poorly understood that they are often excluded from biomedical study, says Eichler. Certain regions of chromosomes, notably those near centromeres (where the two halves of a chromosome connect) and telomeres (the ends of chromosomes) are especially incomplete in the reference genome. This lack of information can have medical consequences. For example, researchers have known for more than a decade that medullary cystic kidney disease \u2014 a rare disorder that occurs in mid-life \u2014 can be caused by mutations in a gene hidden somewhere along a 2-million-base-pair stretch of chromosome 1. Early detection of the mutation is the first step towards preventative therapies, but would require a DNA test. The gene, however, lies within a region rich in sequence repeats as well as in the bases guanine (G) and cytosine (C). Such 'GC-rich' regions, like repetitions, are difficult to sequence. Only by reverting to Sanger sequencing \u2014 a classic but more laborious approach \u2014 and combining it with special assembly methods were researchers able to decipher the DNA region involved in the disease. The results, which were published in February 2 , mean that a test to screen younger members of families affected by the disorder is now a possibility. Sanger sequencing is a painstaking process in which each type of DNA base is labelled with a different compound. The labelled DNA is then separated and the sequence is read. For the Human Genome Project, researchers combined Sanger sequencing with techniques to establish markers that locate where the sequences fit. The approach, which has been in use for decades, delivers a read accuracy and contiguity of sequence that are unmatched by current technology, Shendure says. \u201cI couldn't do anything remotely approaching the quality of what resulted from the project.\u201d But the art of Sanger sequencing and its associated methods cannot be scaled up for the high-throughput sequencing projects done today. \u201cWe need to think about how to 'next-generation-ify' all of this,\u201d he says. Research to do just that is well under way, with a variety of methodologies that address problems such as repetitive sequences and GC-rich regions, as well as the knotty task of assembling complete genomes for organisms that have four or even eight copies of each chromosome, for example, as opposed to humans' two. Some of the technologies on the horizon promise to deliver longer reads and, possibly, fewer headaches for researchers trying to assemble them. But until those instruments are on bench tops, scientists are combining new and old approaches to refine sequencing. \n               Rich is poor \n             Some of the newer approaches aim to tackle GC-rich regions. For high-throughput sequencing, DNA is often first chopped into short fragments, which are then amplified by polymerase chain reaction (PCR). But the enzyme used in PCR \u201chas trouble getting through\u201d GC-rich regions, says Shendure. As a result, GC-rich stretches can end up poorly represented in the DNA sample delivered to the sequencer, thus skewing the data. Some sequencing technologies, such as those made by Illumina, based in San Diego, California, use amplification before and during the sequencing process, causing further bias against GC regions. A number of sample-preparation approaches reduce this GC bias. The amplification step is cut out completely in platforms made by Pacific Biosciences, based in Menlo Park, California, and in a method being developed at Oxford Nanopore Technologies in Oxford, UK. And although DNA read lengths differ among platforms, the most widely used bench top sequencers \u2014 which are made by Illumina \u2014 generate short reads, of around 150 base pairs. \u201cThe killer with short reads is that they're very sensitive to repeated content,\u201d says Brown. If the read length is shorter than a repeat \u2014 or, to draw on the book analogy, if the shreds of the novel are only a fraction as long as a repeated paragraph \u2014 it is hard or even impossible to uniquely place. \u201cThat's where things like long reads or other technologies can be helpful,\u201d says Shendure. Long DNA fragments can bridge repetitive regions and thus help to map them. As another way to ease assembly, researchers in Shendure's group and elsewhere are exploring different methods to tag and group DNA fragments before sequencing. \u201cThere are more on the horizon,\u201d says Shendure, but he prefers to divulge the details in research publications. The terms 'short' and 'long' are in a state of flux in this fast-moving industry. The first generation of Illumina machines generated reads of around 25 base pairs in length; the latest ones have upped that to around 150 base pairs (see 'Extended sequence'). But it is still hard to assemble a complete genome from reads of this length. Geoff Smith, who directs technology development at Illumina in Cambridge, UK, acknowledges the drawbacks of short-read technology for sequencing repetitive regions and various types of genomic rearrangements. He says that the company aims to address issues that crop up as researchers compare genomes they sequence to reference genomes, or sequence organisms from scratch without references. Illumina has launched a service to allow longer reads with its current short-read technology. Last year the firm bought Moleculo, a company based in San Francisco, California, which has developed a process to create long reads by stitching together short ones through a proprietary sample-preparation and computational process. In July Illumina began offering Moleculo's process as a service for customers. The Moleculo process first creates DNA fragments about 10,000 bases (10 kilobases) in length. The fragments are sheared and amplified, then grouped and tagged with a unique barcode that helps to identify which larger fragment they originated from and aids in assembly. At present, sample preparation for the Moleculo process takes around two days. Smith says that he and his team are refining the process and that by the end of the year Illumina will launch Moleculo as a stand-alone sample-preparation kit. He says that company scientists are now evaluating the kit's performance by sequencing a well-known genome, but he prefers not to say which one. \u201cWe suspect you will be able to uncover a lot more of the genome with 10-kilobase reads versus the [150-base-pair] read length that we currently have,\u201d says Smith. He adds that the company plans to increase the fragment length to 20 kilobases. He and his team hope to \u201cdevelop better molecular-biology tools to allow us to reach into these difficult-to-sequence parts of the genome but also use those tools on well-characterized genomes,\u201d he says. The team is also tuning the Illumina software to better distinguish between false and correct reads. The company's initiative comes at a time of intense commercial and academic activity around long-read sequencing technology and new assembly methods. Finished genomes have taken a back seat, leaving many highly fragmented assemblies that need completing, says Jonas Korlach, chief scientific officer of sequencing manufacturer Pacific Biosciences in Menlo Park, California, whose sequencer generates read lengths of around 5 kilobases. Korlach agrees that long reads will help to sequence repetitive regions, such as those that characterize many plant genomes, for example. They will also help with the challenge of distinguishing between copies of chromosomes, important in identifying the tiny variants that can affect biological function. Humans are diploid, meaning they have two copies of each chromosome, but \u201cmany organisms, especially plants, have even more copies, which makes resolving all the different chromosomes so much harder\u201d, Korlach says. \n               Tough nuts \n             Plant sequencing, in particular, will benefit from improvements 3 . The spruce genome is a \u201creal nightmare\u201d, says Stefan Jansson, a plant biologist at the Ume\u00e5 Plant Science Centre in Sweden. Jansson led a study that generated a draft assembly of the Norway spruce genome ( Picea abies ) 4 . In addition to being the largest genome yet sequenced, it also contains many repeats, and the differences between its chromosomes are larger than in the human genome. \u201cSequencing diploid spruce is like mixing human and chimpanzee DNA and then trying to assemble them simultaneously,\u201d Jansson says. Many plants have more than two copies of chromosomes. Bread wheat ( Triticum aestivum ), for example, is hexaploid, and sequencing and assembling the six sets of chromosomes to completion has proven extremely difficult. And although some strawberry species are diploid, the commercial strawberry ( Fragaria  \u00d7  ananassa ) is octoploid: it has eight sets of seven chromosomes, four sets from each parent, says Thomas Davis, a plant biologist at the University of New Hampshire in Durham. \u201cGood thing Mendel didn't use octoploid strawberries to try to understand heredity,\u201d he says. Davis and his colleagues have published a draft genome of the diploid woodland strawberry ( Fragaria vesca ), and now want to apply their experience to the octoploid strawberry 5 . Assembling this tough-nut genome will require high-quality reads longer than 500 base pairs, Davis says. He believes he can succeed, although he does not want to share his methodology just yet. \u201cIf anyone cracks that nut, he'll do it,\u201d says Kevin Folta, a molecular biologist at the University of Florida in Gainesville, who led the woodland-strawberry project. The plant world has many other challenging genomes to offer. The onion genome is massive, Folta says, and sugarcane has 12 copies of each chromosome. \u201cThose will take special techniques,\u201d he says. Every platform has benefits and drawbacks, and scientists must weigh the costs, sample-preparation time and sequencing-error rates for each. To sequence the woodland strawberry, for example, the scientists used a combination of three platforms. But for polyploid genomes, short-read sequencing is almost a waste of time, says Clive Brown, chief technology officer at Oxford Nanopore. \u201cYou don't know where your short read comes from, which chromosome it is from,\u201d he says. \u201cIt's very hard to piece that together.\u201d He believes that the problem will be helped by instruments, including those in development in his company, that can generate long reads without the need for special sample preparation or complex assembly. The longer the reads, the easier the assembly, because the overlapping sequences will help researchers to determine which sequence belongs to which chromosome. Fresh approaches were needed to crack the genome of the oil palm ( Elaeis guineensis ), reported last month in  Nature 6 . The effort was more than a decade in the making. Oil palm is an important source of food, fuel and jobs in southeast Asia, and the industry is under pressure to produce it sustainably and avoid increased rainforest logging, says study co-leader Ravigadevi Sambanthamurthi, director of the advanced biotechnology and breeding centre at the Malaysian Palm Oil Board in Kajang, which works with the country's oil-palm industry. With millions of repeats distributed throughout the plant's genome, short reads could fit in many possible spots in the assembled DNA sequence. \u201cIt is as if you were assembling a jigsaw puzzle in which most of the pieces are identical,\u201d says Robert Martienssen, a geneticist at Cold Spring Harbor Laboratory in New York, who co-led the project with Sambanthamurthi. Classic sequencing methods were too laborious and expensive for the oil-palm project. So Martienssen suggested applying a technique based on a finding he had made in 1998 \u2014 that repeats in plant genomes can be distinguished from genes because the cytosine bases in the repeats usually carry methyl groups. Before fragments are sent to the sequencer, they are treated with enzymes that digest methylated DNA and thereby remove the repeats from the samples. To complete the oil-palm project, the scientists applied this methylation-filtration technique and then sequenced the DNA regions housing genes. The technique has now been commercialized through Orion Genomics, a company based in St Louis, Missouri, which Martienssen co-founded. The researchers used a high-throughput sequencer made by 454 Life Sciences, a company owned by Roche and based in Branford, Connecticut, that generates short reads from longer, filtered fragments. In preparing the samples, the researchers used bacteria to amplify DNA in large chunks on bacterial artificial chromosomes \u2014 an approach also used in the Human Genome Project \u2014 to pin down hard-to-map regions by retaining them next to genes with known positions to act as signposts. Assembly of the oil-palm genome called for extensive computational resources, which crashed multiple times, the researchers say. But now, with the genome in hand, they have located a gene that encodes the shell of the palm fruit, knowledge they hope to harness to increase the plant's yield. Sambanthamurthi says that when the researchers finally pinned down the shell gene, they popped a bottle of champagne, then celebrated with a traditional Malaysian meal served on a banana leaf. \n               The long and the short \n             Bacterial genomes are smaller and less complex than those of plants and other multicellular organisms, but they, too, have regions that are tough to sequence. For example,  Bordetella pertussis , which causes whooping cough, has hundreds of insertion sequence elements \u2014 stretches of mobile DNA inserted into various locations in the genome \u2014 each more than 1 kilobase long. Proponents of long-read technology say that spanning these regions with long reads will deliver sequencing efficiency gains. Korlach points out that it took a team of more than 50 scientists to solve the bacterium's complete genome 7 . But long-read technology can make assembly of highly repetitive genomes faster and easier, he says. He says that he and scientists in the Netherlands were able to assemble nine whooping-cough bacterial strains in one month. Whether a read is classified as 'long' or 'short' is in great flux. Two years ago, scientists might have said that a long read was 1 kilobase, Korlach says. \u201cNow [Pacific Biosciences] customers are generating an average of 5,000 bases, with some reads longer than 20,000 bases \u2014 and we are working to deliver even more than that.\u201d Ultimately, a 'long read' will be as long as is needed to sequence a given genome, he says. Korlach knows that some scientists say his company's sequencers are pricey, but he says that the newer versions have seen a significant drop in price and an increase in throughput. He says that the question of price is often raised \u201cin the context of pure cost per sequenced base\u201d. And, he adds, if a certain sequencing technology is the only one that will work to solve a medically important question, \u201cthen there is no price tag that can be put on this medically relevant information\u201d. Last year, researchers collaborating with Pacific Biosciences used the company's sequencer to distinguish the repetitive genomic region involved in fragile X syndrome, a developmental disorder that is caused by repeats in a particular region on the X chromosome, and that worsens in severity with higher numbers of repeats 8 . As technology developers get closer to instruments that produce longer reads, scientists will need longer DNA fragments at the beginning of their sequencing experiments. Several companies focus on helping researchers to prepare DNA fragments for sequencing. For example, Sage Science, based in Beverly, Massachusetts, has a platform that uses pulsed-field electrophoresis to select and sort DNA fragments of sizes ranging from 50 base pairs to 50,000 base pairs. In May, the company began marketing its instrument to accompany the Pacific Biosciences sequencing platform. Steve Siembieda, who is responsible for business development at Advanced Analytical Technologies in Ames, Iowa, says that his company sees the trend towards longer reads as writing on the wall. The company has licensed patents from Iowa State University, also in Ames, to develop an instrument to assess the integrity, fragment length and concentration of DNA samples. With this instrument, an electric field is applied to a tiny amount of DNA so that it is pulled into a long, hair-thin capillary tube containing a gel with a fluorescent dye that binds to DNA molecules. As the DNA fragments move through the gel, they separate according to size. \u201cSmall molecules move fast, big molecules move slowly,\u201d Siembieda says. As the molecules pass by a window in the capillary, a flash of light excites the dye and a camera records the DNA fragment length (see 'Bits and pieces'). The instrument's readout tells scientists whether the size distribution of the DNA fragments is in the range needed for a given sequencing platform and whether the DNA has the right concentration. Siembieda says that skipping these measurements can be the wrong experimental shortcut \u2014 if the concentration or fragment size is off, \u201ca sequencer may run for nine days, it will cost them thousands of dollars, plus all the time wasted to not make sure they have the appropriate material\u201d. The instrument will possibly be used in developing the Moleculo process, but negotiations between the two companies are still under way. Technology development at Advanced Analytical is focusing increasingly on long DNA fragments, which are challenging to resolve, Siembieda says. One solution is to customize gels for different applications. At present, the company's instrument can resolve lengths of up to 20 kilobases and the company is working on resolving longer fragments, he says. \n               Assembly required \n             Scientists are applying many methods and tricks to create longer fragments. \u201cUnfortunately, these technology tricks create erroneous data at points, so now you're stuck with some data that may be wrong,\u201d says Michigan State's Titus Brown. He was part of an effort, published in April 9 , to sequence the lamprey ( Petromyzon marinus ) genome, one-third of which is covered by long repeats. Obtaining an assembly even with Sanger sequencing, which generates 1-kilobase reads, was difficult, he says. In addition, the lamprey genome has many GC-rich regions. The team used several types of software to assemble the complete DNA sequence. In July, scientists published a comparison of software programs used to assemble sequence reads 10 . The researchers found that different assemblers give different results \u2014 even when fed the same sequence reads. Brown says that biologists should never forget that assemblies are not certainties. Every new sequencing technology \u2014 from how the DNA sample is prepared to the sequencing chemistry \u2014 has the potential for error and bias. \u201cIf you have short reads, or bad biology, you're going to have a very hard time getting a good assembly, even in theory,\u201d he says. Ideally, a genome assembly should deliver end-to-end chromosomal sequences, says Shendure. What worries him more than the discordance among assemblers in the comparison study is that all of the assemblies were very fragmented. \u201cThat's not a fault of the assemblers, that's a fault of the data that we're putting into the assemblers and the fact that we're not capturing contiguity at these longer scales,\u201d he says. \u201cThe algorithms can only make do with the ingredients that they are provided by the technologies.\u201d Brown is hopeful about the potential impact of longer-read technology. If Pacific Biosciences or Oxford Nanopore \u201cdeliver on many inexpensive long reads \u2014 more than 10 kilobases, I'd say \u2014 regardless of accuracy, you would end up revolutionizing the genome-assembly field, because it would give you so much more information to work with\u201d, he says. However, he adds, assembly software has to be compatible with each sequencing method. \u201cSo we're continually playing catch up, where new sequencing technologies lead to new sequence-analysis approaches a year or three later.\u201d Eichler agrees that sequencing and assembly must continue to improve. Read lengths longer than 200 kilobases and with 99.9% accuracy rates will be needed to unpick repeats and other complications, he says. He says that the Pacific Biosciences instrument and what he knows of Moleculo \u201cfall short of this, but are on the right track\u201d. All read-length requirements depend on the genome and complexity, he adds. For many bacterial genomes, current read lengths and accuracy are already sufficient, he says. \n               The next telescope \n             Oxford Nanopore plans to launch its new sequencing technology in the near future, but no date has been given. The technology expands on findings by researchers at Harvard University in Cambridge, Massachusetts, the University of Oxford, UK, and the University of California in Santa Cruz to harness the abilities of pore-forming proteins for DNA-sequencing devices. One of the weaknesses of current high-throughput sequencing technology is amplification chemistry, says Oxford Nanopore's Clive Brown. Although DNA is made up of four bases, it is possible that more than those canonical four \u2014 such as bases that are methylated \u2014 should be detected, he says. And in some sections of genomes, bases are naturally missing. But current sequencers do not capture such variations \u2014 instead, says Brown, they produce the equivalent of a four-colour photocopy of a picture with many more colours. \u201cA lot of the detail is lost immediately, as soon as you make a four-colour copy,\u201d he says. Ideally, \u201cyou take a chromosome and run it through the sequencer. You can't quite do that yet.\u201d He, too, says that the next crucial phase of sequencing technology will be about long reads. Brown says that to his mind, sequencers are just opening the door to characterizing the genome. People can get \u201cvery cosy about what they can see\u201d, with scientific instruments, he says. He likens today's sequencers to the first telescopes, which offered a view of the Moon's features and exploration of the visible spectrum. \u201cIt gets you a long way, you can count the stars, see the planets,\u201d he says. But the telescope does not show other celestial phenomena \u2014 such as dark matter or galactic movement. Like astronomers with their telescopes, genome researchers will get a clearer picture of the genome as the sequencing technologies improve, he says. And, inspired by that picture, they will strive to see even more. \n                     The search for genome 'dark matter' moves closer \n                   \n                     Human genome: Genomes by the thousand \n                   \n                     Human genetics: Genomes on prescription \n                   \n                     Genomics: Decoding our daily bread \n                   \n                     Pacific Biosciences \n                   \n                     Illumina \n                   \n                     Roche/454 \n                   \n                     Oxford Nanopore \n                   Reprints and Permissions"},
{"file_id": "499505a", "url": "https://www.nature.com/articles/499505a", "year": 2013, "authors": [{"name": "Caitlin Smith"}], "parsed_as_year": "2006_or_before", "body": "Tumours are made up of disparate cell populations that often resist treatment \u2014 but understanding this heterogeneity could provide ways to improve chemotherapy. Cells come in all shapes and sizes \u2014 boxy epithelial cells, discoid red blood cells, delicate, threadlike neurons and the behemoth human egg that is just visible to the naked eye. Even among cells of the same basic type, no two are identical. And the same is true of cells within a cancerous tumour, where differences in size and shape can have profound implications for the progression of a patient's disease. As a result, researchers are keen to get to grips with cell heterogeneity. Developing the tools and techniques to rationalize this cellular chaos has been a slow process, but the latest methods for imaging, modelling and sorting cells are at last coaxing them to relinquish their secrets. For cancer, this may help to explain why a tumour that has been shrunk by chemotherapy suddenly kicks back into life and starts growing again. The plasticity of individual tumour cells lets them modify their behaviour in response to external cues, says Nicholas Saunders, a cancer biologist at the University of Queensland in Brisbane, Australia. One such cue is chemotherapy, and although the heterogeneity of tumour cells makes it harder to predict how each will respond to treatment, \u201cwe now have tools that allow us to interrogate this issue in a relatively definitive way,\u201d he says. Recent techniques for sequencing the DNA of single cells from tumours, for instance, has fired up this area as scientists explore ways to use the technology, says Saunders. \n               Single life \n             To investigate how cancer cells survive chemotherapy, researchers are moving into the challenging realm of single-cell analysis. At this small scale, it becomes hard to separate true variations between cells from technical errors in measurement, says Nicholas Navin, a molecular geneticist from the University of Texas MD Anderson Cancer Center in Houston. When differences between cells are detected, scientists can question whether the observed variations are important. Researchers are particularly interested in the individual cells shed by tumours into a patient's bloodstream. Carried around the body, these 'metastatic' cells can initiate fresh tumours, allowing the disease to progress. But capturing these roaming cells for study is tricky, because they are mixed in with multiple cell types in the bloodstream. One system Navin is using to isolate single tumour cells from blood is DEPArray, an instrument made by Silicon Biosystems of Bologna, Italy. This can isolate, move and image one tumour cell from a mixture of 100,000 cells. Metastatic cells in the blood sample are first tagged with a chemical marker that emits light under a fluorescence microscope. In the DEPArray system, the individual cancer cells are then imprisoned in 'cages' created using an electric field. Viewed on a monitor, these cages can be manipulated to move a single cell into a collection vessel, ready for further study. The lack of physical contact helps the cells to stay alive during the manipulation. Nevertheless, sorting takes time, says Navin. Initially, the DEPArray system took around an hour to isolate one tumour cell, but improvements to the technology and software mean that it can now move multiple tumour cells simultaneously from the mixture to the collection vessel. \u201cThe current system can route 13 cells in about 4 hours,\u201d says Navin. Navin is also working with a system made by Fluidigm in South San Francisco, California. This captures 96 cells in one run, says Ken Livak, a researcher at the company. However, unlike DEPArray, it does not image cells to help with visual sorting, so it is best for isolating previously sorted cells, he says. Fluidigm's system features a device about the size of a postage stamp that contains tiny channels, valves and chambers. Minute amounts of fluid, along with cells, are driven through channels across the chip by opening and closing the valves. The channels contain a series of alcove-like capture sites. An unoccupied site will trap and hold an individual cell, but if the site is already occupied, the cells bypass it and move to the next one, until all 96 sites hold cells. \n               Shape-shifters \n             Chris Bakal, a cancer biologist at the Institute of Cancer Research in London, is hunting for patterns in the diversity of cancer-cell shapes. He and his team study metastatic melanoma cells, which are notorious for making drastic changes to their shape so that they can infiltrate far-flung reaches of the body. The team's work centres on analysing images derived from spinning-disk confocal microscopy. In this technique, a laser illuminates the cells, and the microscope scans the light bouncing off them at many points simultaneously, gathered through pinholes in a spinning disk. The method is more sensitive than conventional confocal microscopy, which detects only one point of light at a time. Bakal and his group have seen the diverse shapes of some cancer cells 1 , and are now using statistical and computational analysis to try to identify which shapes are important. \u201cWe think of these cells as extreme shape-shifters that can do anything,\u201d says Bakal. But generating and maintaining more diversity in cell shape than is needed may simply squander energy and drain the population of the cell shapes that are most useful, he says. Bakal and his team have found that, in fact, metastatic melanoma cells generally assume one of two shapes: rounded or spindle-shaped, each with its own advantages. \u201cIf you're a metastatic cell, you want those two shapes because a rounded shape migrates through soft tissues like the brain or the circulatory system,\u201d he says, \u201cwhereas the spindle shape is good for bone and hard tissues.\u201d Bakal thinks that looking at various aspects of heterogeneity in single cells will prove useful. \u201cYou might see genetic heterogeneity in this experiment, and you might see shape heterogeneity in another experiment\u201d, he says, but notes that it may not always be clear whether the two observations are connected. To determine if there is a link, he plans to sequence the DNA from individual cells after imaging them to see if he can find mutations that correlate with one shape or the other. \n               Order out of chaos \n             Despite their heterogeneity, tumours cannot be totally chaotic, says Garry Nolan, a cell-signalling researcher at Stanford University in California. He thinks that there must be organization somewhere within the diversity, so his approach to studying individual cells focuses on differences in the patterns of the myriad proteins that cells express. He believes that the complement of proteins alters as a normal cell becomes cancerous. As a result, the different protein complements seen in a sample of cancer cells could be related to the past history of those cells. So far, Nolan's group has tracked more than 100 proteins simultaneously in individual cells, using a technique called mass cytometry. This is similar to flow cytometry, which separates cells according to fluorescently labelled proteins of interest. However, Nolan and his team wanted to look at many more proteins than is possible with flow cytometry, which is limited to the analysis of only a handful of proteins by the number of fluorescent tags that can be used. To solve this problem, the researchers developed mass cytometry so that they could identify tens or hundreds of proteins at the same time 2 . In mass cytometry, instead of proteins of interest being labelled with fluorescent markers, they are tagged with small metal particles that differ in mass. Once tagged, each cell is ionized and sent to a mass spectrometer, which separates the metal-tagged labels by mass. Unlike the fluorescent signals of flow cytometry, the mass measurements are relatively easy to distinguish from one another. Another benefit of this method is that it can measure proteins within the cell, because the cell is essentially vaporized during the process. Nolan and his team are now developing their mass-cytometry technique to measure hundreds of proteins per cell, enabling them to piece together the puzzle of how cells become cancerous. The team has discovered a group of heterogeneous cancer cells that each have \u201ctheir own little time-stamp signature on them\u201d, Nolan says. The varying complement of proteins on each cell indicates how far it has passed along the path to becoming cancerous, he adds. By arranging the cancer cells according to these time-stamp proteins, the researchers created a timeline for a cell's physiology. Nolan believes that what seems to be a heterogeneous group of cancer cells is actually a snapshot of cells that represent different stages on a pathway leading to fully fledged cancer cells 3 . Viewed one by one, the mix may look wildly variable. But when viewed as a time-stamped group, \u201cthere is order there, waiting to be understood\u201d, Nolan says. \n               Personal space \n             As well as presenting deviant shapes, cancer cells have a tendency to disregard the normal rules that other cells use for spacing themselves in three dimensions. \u201cBasically they don't sit in these nice structures within the tissue like most cell types do,\u201d says Navin of breast-cancer cells. \u201cThey don't respect their neighbours.\u201d The three-dimensional position of cancer cells within the tissue, as well as the areas immediately surrounding them, influence tumour formation and growth. Researchers hope that studying cellular heterogeneity in three dimensions \u2014 which more closely resembles real tissues \u2014 will deliver insights that could help to fight cancer. Lucas Pelkmans, a researcher at the Institute of Molecular Life Sciences at the University of Zurich in Switzerland, studies how cells are affected by their surroundings. He and his team use automated, high-resolution imaging of millions of cells to monitor hundreds of parameters, including a cell's shape, distance from neighbouring cells and position within a tissue. Pelkmans and his team then correlate these parameters with other measures of cellular activity, such as the molecular composition of cell membranes and the abundance of messenger RNA (mRNA) molecules, which are transcribed from DNA to serve as templates for the production of proteins in cells. The team developed a technique that attaches fluorescent labels to single mRNA molecules of interest within individual cells, and then massively amplifies the fluorescent signal. \u201cWith that, you get a bright spot inside single cells,\u201d says Pelkmans. \u201cBy counting the number of spots, you basically get a read-out of the number of mRNAs in one cell.\u201d This can reveal whether a particular cell has different levels of gene expression compared to another; if it does, this might suggest that the two cells will go on to have different roles. The varying types of correlation between the measured parameters create a tell-tale cell signature. \u201cThese signatures clearly can be different for different genes, but there are strong signatures,\u201d says Pelkmans. The signatures indicate a kind of tumour geography and hint at the functional role of a cancer cell at a given position. Cells can grow together as a community, but those on the periphery can show different signatures to those in the interior. Interpreting these signatures can help researchers to understand how signals exchanged between cells influence tumour growth. The movement of cells within tumours has piqued the interest of Kornelia Polyak at the Dana-Farber Cancer Institute and Harvard Medical School in Boston, Massachusetts. She is studying the spatial changes that occur during cancer treatments. Using measurements from real tumour cells obtained from cancer patients, she and her team have built a computer model that simulates tumour growth. The model allows researchers to take virtual samples of the simulated tumour at different times and places, Polyak says. They can even subject the simulated tumour to a course of cancer treatment. Although it is not yet ready for clinical use, Polyak hopes that the model will ultimately act as a surrogate patient, allowing clinicians to try out different simulated therapies and assess predicted outcomes before they treat patients. \u201cWe could actually use this for designing the best treatment strategy,\u201d she says. \u201cBut the treatment itself changes the tumour, so you have to know how the tumour changes.\u201d Models might help physicians to get one step ahead of the tumour, Polyak suggests, allowing them to anticipate the survival of a small population of drug-resistant cells and so quickly fight back against cancer recurrence 4 . Key to the issue of resistance is knowing how many tumour cells already have the genetic mutations that make them resistant to drugs, and to what extent chemotherapy itself induces such mutations. Studying individual cells may provide the answer. \u201cThe more genetically diverse a tumour is, the more likely it is to be resistant to certain therapies, so that's one potentially useful parameter that you could get for patients,\u201d says Navin. Understanding the degree of heterogeneity within a tumour is important in assessing the severity of the cancer. Individuals with diverse tumours might be more likely to harbour metastatic cells or be more resistant to therapy, compared with patients whose tumours are more homogeneous, says Navin. \u201cIf we can measure the extent of heterogeneity of a tumour-cell population, then we may be able to use this index to predict which patients will have invasive or metastatic tumours, and which will respond to chemotherapy or show poor survival,\u201d he says. Cell heterogeneity gives normal cells the power to react to the environment, but it also underlies the ability of some tumour cells to emerge unscathed from even the strongest chemotherapy. If researchers can uncover how cancer cells adapt to cancer treatments, cell heterogeneity might ultimately be turned to the patients' advantage. \n                     Mutational heterogeneity in cancer and the search for new cancer-associated genes \n                   \n                     viSNE enables visualization of high dimensional single-cell data and reveals phenotypic heterogeneity of leukemia \n                   \n                     Single-cell dissection of transcriptional heterogeneity in human colon tumors \n                   \n                     Tumour heterogeneity: That's the theory \n                   \n                     Tumour evolution inferred by single-cell sequencing \n                   \n                     Heterogeneity of cancer cells \n                   \n                     Cellular heterogeneity and molecular evolution in cancer \n                   \n                     Tumor cell heterogeneity in mouse model of lung cancer \n                   \n                     Drug resistance, epigenetics, and tumor cell heterogeneity \n                   \n                     Cell sorting to reduce cancer cell heterogeneity \n                   \n                     Do the differences between cells matter? \n                   Reprints and Permissions"},
{"file_id": "496253a", "url": "https://www.nature.com/articles/496253a", "year": 2013, "authors": [{"name": "Vivien Marx"}], "parsed_as_year": "2006_or_before", "body": "Advances in cell culture media mean that scientists increasingly know what has gone into the mix, and cells are enjoying a more natural environment \u2014 even in the lab. Cells that thrive in the lab make for happy researchers. And vice versa: biology experiments can grind to a halt if investigators fail to get their cell cultures growing in the right nutrient medium. That is why the market for such culture media is a lively one, with scores of commercial and home-brewed mixes available to help biologists to deal with all the different cell types that their experiments might require. But although the field of cell culturing can draw on generations of experience, making the right choice of medium is still more of an art than a science. Even slight differences in media can have a large impact on cells \u2014 often for no clear reason. Many scientists mix their own culture media, but that can hamper the reproducibility of scientific findings. John Masters, an experimental pathologist at University College London and editor of numerous books on animal and human cell culture, says that the recipe for such 'home-brews' can be difficult to follow owing to the sheer number of ingredients, as well as variations in purity and content between suppliers, variations between batches from a single supplier, and the difficulties of making relatively small quantities of a labile mixture of chemicals consistently. However, as scientists come to terms with the importance of knowing exactly what their cells are thriving on, the field is becoming more rigorous. Some researchers, for example, are trying to eliminate culture-media components that originate from animals, because of fears that they could contaminate or infect potential human therapeutics down the line. Other investigators are trying to make growth media reproduce a natural environment more realistically \u2014 for example by creating three-dimensional (3D) tissue structures. \n               Some cells are hard to please \n             A prime example of the importance of good culture media is in the burgeoning field of induced pluripotent stem cells (iPSCs) \u2014 adult cells that have had their molecular clocks turned back to regain the any-fate-is-possible state of their infancy 1 . These cells can be redirected to become many cell types, offering prospects for regenerative medicine 2  using lab-grown tissues to replace or renew aged, injured or diseased tissue in patients. At the RIKEN Center for Developmental Biology in Kobe, Japan, for example, opthalmologist Masayo Takahashi is hoping to gain approval soon for the first clinical trial of an iPSC-based treatment, for age-related macular degeneration, in which portions of the retina begin to die. Takahashi's goal is to replace the diseased parts of the retina with reprogrammed skin cells. Meanwhile, basic researchers are exploring 'transdifferentiation': a genetic approach that converts one type of cell into a completely different one, skipping reversion to the stem-cell phase entirely. An example is the work of Rudolf Jaenisch and Yosef Buganim at the Whitehead Institute for Biomedical Research in Cambridge, Massachusetts. Using a process based on cell culture, the researchers have shown that connective-tissue cells can be transformed into cells that express markers specific to Sertoli cells, which are normally found in the testis 3 . The results may help researchers to solve the puzzle of male infertility, and may pave the way for techniques for growing cell types that are currently difficult or even impossible to culture. Buganim says the \u201cspecific culture medium used is crucial for the particular fate the cells assume\u201d. Transdifferentiated cells, iPSCs or cultured neurons each need their own culture media tailored to the needs of the cells and cell type. The media might have or lack certain growth factors, for example, or create high or low oxygen levels, all of which allow cells to retain their normal characteristics and properties, says Buganim. More generally, stem-cell researchers in academic or industry labs need media and substrates to maintain and grow their cells, and to coax them down a series of developmental pathways, says Bradley Garcia, who directs technology and business development at Primorigen Biosciences in Madison, Wisconsin, which develops and sells such products. Media tuned to cells' requirements can also keep differentiated stem cells, whether liver or heart cells or neurons, in culture for days, months or even more than a year. Cells can be unpredictable, growing more readily in one medium than another for no apparent reason. And stem cells, according to Scott Monsma, senior director of research and development at Primorigen, are \u201cbalanced on a razor-edge\u201d, and will differentiate in response, for example, to rough handling, overcrowding and stress. These factors make stem-cell-media development challenging, but at the same time, the medical potential of stem cells raises demand for such media, companies say. \n               Less can be more \n             Some scientists use feeder-based systems to get their stem cells growing. In these systems, a layer of supporting cells, such as mouse embryonic fibroblasts, supply the medium with growth factors. But these systems can be prone to error, warns Erik Hadley, senior scientist in research and development at Stemcell Technologies, a spin-off from the British Columbia Cancer Agency that is based in Vancouver, Canada, and sells stem-cell media. Not only is each batch of feeder cells different, but it is also hard to control the amount and timing of excreted growth factors, making it difficult for researchers to know which ingredients make cells respond in what way. To combat these issues, Stemcell Technologies sells a feeder-free stem-cell maintenance medium called mTeSR1. A follow-on product, TeSR2, is completely free of animal proteins, and another, TeSR-E8, which was released in January, contains a set of eight components with a formulation based on work by James Thomson, a stem-cell researcher at the University of Wisconsin\u2013Madison. The media are sold by Stemcell Technologies under patent licences from the university. Life Technologies in Carlsbad, California, also sells a version, called Essential 8 Medium. Defining exactly what has gone into a culture medium takes the field beyond alchemy and helps scientists to reproduce findings from colleagues, as well as to approach clinical applications, says Mikhail Kolonin, a stem-cell researcher at the University of Texas Health Science Center in Houston. \n               No more animal pharma \n             In addition to problems with feeder systems, another stumbling block to reproducibility is that the growth factors, proteins and other nutrients in stem-cell media typically come from fetal bovine serum, which can comprise up to one-fifth of a medium's volume, says Monsma. Each batch of serum \u2014 which is part of the blood \u2014 comes from a different animal and contains different amounts of components. \u201cThe point is, we don't know what's inside,\u201d says Kolonin. That is one reason why stem-cell scientists eyeing clinical applications are becoming wary of using products that contain serum. Another reason is that cells grown in animal products for use in tissue transplantation can \u201cpotentially cause an immune response in patients\u201d, says Kolonin. Contamination can have even more serious consequences, as the experience with mad cow disease showed, says Nathan Allen, a product marketing manager in the cell-culture and bioprocessing business at Thermo Fisher Scientific, headquartered in Waltham, Massachusetts. In the mad-cow episode, a UK outbreak of variant Creutzfeldt\u2013Jakob disease was caused by contamination of food with the infectious agent of bovine spongiform encephalopath. The US Food and Drug Administration has asked manufacturers to avoid animal-derived components in therapeutics. This preference affects preclinical research, because ideally, technology choices in the early stages of development should set the pattern for manufacturing further down the road, says Roberta Morris, business director at Thermo Fisher Scientific. For that reason, new commercial media are increasingly serum-free, says Allen. His company offers a number of serum-free media and defined media free of all components originating from animals. But banishing animal products is not easy, if only because converting media containing serum or with undefined supplements into a more chemically defined version means massively reworking a proprietary recipe, which affects manufacturing. All of this can make media expensive. Last autumn, Sigma-Aldrich in St Louis, Missouri, launched a stem-cell maintenance medium as part of its Stemline series. The medium is not completely free of animal products, but is composed of defined components and does not contain the types of crude protein preparation found in many formulations, such as serum or pituitary extracts, says Dan Allison, principal research scientist at Sigma-Aldrich. It was designed to cater for labs that are working towards industrial applications for stem cells and that will need high volumes of media, says the company. Monsma says that creating media without animal components, using only chemical compounds and supplements of non-animal origin \u2014 such as human serum albumin or recombinant growth factors \u2014 means that the proteins must be expressed in human cells or bacteria, then purified and tested. His company and others are setting up capabilities to manufacture such media. For example, Primorigen is collaborating with several university labs to convert a differentiation medium into one that is animal-component-free. For stem-cell researchers, shifting away from animal products means abandoning some lab staples, such as mouse feeder cells. Furthermore, some substances traditionally used to coat cell culture dishes are not animal-free. Matrigel \u2014 a product that was previously made by Becton, Dickinson of Franklin Lakes, New Jersey, but was sold to Corning Inc. in Corning, New York, last autumn \u2014 is a gel used to coat dishes, and is derived from a type of mouse tumour. Researchers at the University of Michigan in Ann Arbor have noted that, although Matrigel has helped scientists to define what iPSCs need, its animal origins and variability are problematic if cells are being cultured as eventual patient therapeutics 4 . \n               Engineering in the mix \n             Some scientists will be satisfied only by mixing their own cell culture media. They \u201ctend to know what they are doing and be highly experienced\u201d, says Masters. But most, he adds, \u201care generally not interested in the basics of how to do it properly, just the end product\u201d. They want to be able to buy media off the shelf. Companies have begun catering for scientists who want more defined media. Firms interviewed by  Nature  say that their products are superior to home-brew because they can exert more quality control over the way they source, store, mix and evaluate ingredients, and can manufacture media under controlled conditions. Engineered media can make a difference. For example, stem cells are deep frozen until their use in the lab, with scientists using a variety of cryopreservation media, including home-brews. But an ongoing challenge in the field is that most cells do not survive the thaw, says Hadley. Towards the end of last year, Thermo Fisher Scientific began to sell a serum-free, animal-origin-free cryopreservation medium called HyCryo for standard cell lines, and a separate one, HyCryo-STEM, for stem cells. HyCryo-STEM is engineered to improve the recovery rate of stem cells after thawing. Scientists working with neural stem cells can usually recover only 10\u201320% of cells, and increasing that proportion is not easy with the typical home-brewed freezing media used in labs, says Cindy Neeley, a cell-culture specialist at Thermo Fisher. In tests, the company's new medium is as good as home-brew, and for neural stem cells the recovery increased to 50\u201360%, she says. Improving cell-culture environments also means improving containers. Taking an engineering approach, Po Ki Yuen, a bioengineer at Corning, has built a 96-well plate that nurtures growing cells while removing waste \u2014 which is toxic \u2014 and replenishing media, without an external pump 5 . Not only does the plate require less than the usual amount of daily media exchange, says Yuen \u2014 thus minimizing the need for human intervention and lowering the risk of contamination \u2014 but it also has fluid movement that is a bit more like that of the body than that of a classic lab vessel. The idea for the plate, which emerged during a product-development session with two colleagues, he says, is to take advantage of pressure differences between wells that contain different amounts of fluid. Narrow strips of filter paper or a cellulose membrane connect the wells, so that fluid is forced to flow in a controllable way into the adjacent connected well until the liquid heights reach the same level in both. \u201cThe flow rate in our perfusion plate can be controlled by liquid height difference between connected wells, and the dimensions and pore size of the strip of cellulose membrane or filter paper,\u201d says Yuen. The 96-well version is not yet on the market but a 6-well version is, says Brian Douglass, a business-development manager at Corning. Cells in the 6-well version can last for at least 72 hours without media exchange, says Yuen. And, says Douglass, less-frequent media exchange means that \u201cresearchers get their weekends back\u201d. \n               Spherical thinking \n             Scientists and companies are also exploring 3D environments in which to foster tissue-like growth of cell clusters, keeping cells close and in constant communication. In this kind of architecture, stem cells can grow into rounded aggregates called embryoid bodies, which is part of the differentiation process. This means that cells must not attach to the surface of their container, because if they do, they will grow in a spread-out single layer, says Neeley. Thermo Fisher Scientific has developed a series of dishes and multiple-well plates with a polystyrene surface that offers low-adhesion properties. Scaffolds can be used to shape cell clusters as they grow, but the three-dimensionality collapses once the scaffold is removed, like a tent without its supporting poles. They can also block a researcher's view through a microscope. To cater for researchers seeking viable 3D cell-culture options, Thermo Fisher Scientific has developed a culture plate called Nunclon Sphera. \u201cThe cells, instead of sticking to the surface, aggregate with themselves and form a three-dimensional sphere in the culture environment,\u201d says Neeley. When cultured in this plate, cells grow into spheres that the scientists can transfer from one vessel to another using a pipette, without disrupting the form, she says. Customers are currently beta-testing the product. Other plate-focused efforts rely on more radical architectural changes. Three-dimensional cell culture dates back more than 100 years, says Ross Harrison, a biologist at Johns Hopkins University in Baltimore, Maryland. He cultured neural tissue in a hanging drop of frog lymph and was able to observe live nerve cells sprouting axons, the long extensions through which neurons send messages to other neurons. Now, a Swiss company, InSphero in Schlieren, is using the hanging-drop technique as the basis for a multiple-well plate made of the conventional polystyrene but with redesigned wells. After building a prototype, the Swiss Federal Institute of Technology (ETH) in Zurich set up the fledgling firm in the institute's technology park, says Jens Kelm, a biotechnologist formerly at the University of Zurich who founded InSphero four years ago along with University of Zurich colleague Wolfgang Moritz and ETH engineer Jan Lichtenberg. The firm is just moving into its own facilities. Unlike typical round-bottomed wells, InSphero's wells have a V-shaped lower part, similar in appearance to a champagne flute. At its very bottom, the well is flat. In a hanging drop of medium, cells settle and grow as spheroids in a way that enables microscopy, says Kelm. Getting the cells into the well also meant changing the well openings, which are shaped like a very narrow flower vase so that they fit tightly around a pipette tip. The researchers found that air-tight contact between the well opening and the pipette tip allowed them to deposit near-identical amounts into each well, which is important for making sure that the results are comparable between wells. To arrive at the design, says Kelm, \u201cwe started experimenting with pipette tips, cut them off and put in drops from the top and looked at how they came out at the bottom\u201d. In 2011, InSphero began a partnership with PerkinElmer in Waltham, Massachusetts, allowing the plates to be incorporated into PerkinElmer's automated screening instruments, which are used by drug companies. What began as a marketing deal has morphed into the companies collaborating on assay development; for example, they create plates that hold spheroids of liver microtissue ready for drug toxicity tests. Kelm sees a broad international market for his technology. European laws that prohibit the use of animals in cosmetics testing have left the industry clamouring for  in vitro  models, such as his microtissues. Drug developers and chemical companies also want cell-based assays to test toxicity. And hanging-drop technology can be used to culture stem cells, an area that could expand as these cells move towards medical applications. Nadia Benkirane-Jessel, a biologist at the French National Institute of Health and Medical Research (INSERM) in Strasbourg, uses InSphero's technology to investigate ways to shorten recovery times for people undergoing bone-repair procedures and, potentially, bone regeneration. To position bone cells correctly for growth, Benkirane-Jessel seeds cells that have grown into spherical microtissues onto 3D nanofibres developed in her lab for use in mice. She also plans to use InSphero's technology for a product developed by her spin-off company, Artios Nanomed in Strasbourg, in the field of bone and cartilage regeneration. \n               Cell levitation \n             Another academic spin-off that is developing 3D cell culture is n3D Biosciences in Houston. As chief scientific officer and company co-founder Glauco Souza explains, the technology seeds tissue by levitating cells and bringing them together 6 . The first step is to decorate cells with NanoShuttle, the company's magnetic nanoparticle assembly of gold and iron oxide crosslinked with polylysine, he says. Next, the cell culture dish is exposed to a magnetic field. \u201cWhen the magnetic field is applied, it brings the cells together while levitating them,\u201d he says. What keeps cells growing, Souza explains, is the cell\u2013cell interaction that the levitation process promotes, which is more like the body's environment than conventional cell culture. The technology also makes media exchange easier, because a magnet can hold the tissue in place, he says. Souza, a physical chemist formerly at the MD Anderson Cancer Center in Houston, says that research with this technology at the company and in collaborating academic labs shows that the resultant microtissues have  in vivo -like morphology and protein production, enabling them to be used in  in vitro  drug-testing models. n3D Biosciences has customers in university labs and pharmaceutical companies, and is focusing on high-throughput toxicity testing and drug development. \n               Overnight success \n             Kolonin uses the technology to study fat tissue, and also considers it a possible environment for growing stem cells into organs. Recreating an organ in a dish requires all the organ's cell types to be present and to make connections. In a flat dish, however, one cell type usually takes over because it happens to respond best to the media or to the plastic, and other cells are quickly lost, he says. That situation is different with the n3D technology. \u201cYou plate cells out, throw particles at them ... overnight, and put them into the magnetic field, and the next day you already have the spheroids,\u201d he says. The spheroids include all the cell types. \u201cIt literally takes one day.\u201d Magnetic levitation has been a good way to model adipose tissue, Kolonin adds, and to culture stem cells while retaining their ability to differentiate 7 . The magnetic particles may cause adverse consequences for the cells that contain them, but, these are a minority of the cells in culture. The microtissues stay together and the cells tend to spit out the particles, which then remain in the matrix outside the cells. \u201cThere has been a boom of late in 3D formats, and I think the field is rapidly adopting and critically evaluating these technologies,\u201d says Jeffrey Morgan, a bioengineer at Brown University in Providence, Rhode Island. He thinks that when cells contact, interact with and communicate with other cells rather than with artificial scaffolds, the cell culture more closely replicates the  in vivo  environment, especially that in solid organs such as the heart and liver, where cell density is high. Morgan invented what he calls the 3D Petri dish, and in 2009 he founded a company: Microtissues, based in Providence. In a deal that went through last year, Sigma-Aldrich is distributing the dish. Morgan's customers are academic biomedical researchers, pharmaceutical firms doing toxicity testing and cell-therapy companies exploring how to prepare cell clusters for possible transplantation. The 3D Petri dish came about when, to guide the growth of tissue-like multicellular spheroids, Morgan and his graduate student Anthony Napolitano began making moulds in the lab. They wanted a material that was non-adhesive for cells, which is the \u201cdirect opposite\u201d of the classic plastic Petri dish, says Morgan. At the same time, the researchers needed a material that would not interfere with the small cell\u2013cell adhesive forces that drive cell clustering. The material they chose was agarose, which forms a commonly used hydrogel and is made of 98% water, which is why Morgan says that his approach is \u201clike sculpting water\u201d. A user casts molten agarose in the micro-moulds, allows it to gel, then removes the micro-moulded agarose and places it in a standard multiple-well dish. Cell media and cells are pipetted into the dish, and cells then settle by gravity into each of the micro-wells and self-assemble into a multicellular spheroid at the bottom of each moulded well. The micro-moulds can be autoclaved and reused to cast more gels. The mould makes spheroids that are uniform in shape, says Morgan. Their size can be varied with the number of cells that are seeded. The cells are easier to harvest than in scaffold-based methods. They spill out when the gel is inverted, allowing further tests. \n               Hopes flying high \n             Although stem-cell research and advances in cell culture are quickly advancing, viable cell therapies are years away from the market, says Chuck Oehler, chief executive of Primorigen. But companies such as his regularly get calls from people seeking stem-cell-based cures. Neither are stem-cell scientists immune to hope. A researcher who did not wish to be identified is diabetic, and has been dependent on insulin for more than 30 years. A few years ago, he had a transplant of insulin-producing islet cells from cadavers, which allowed him to go for nearly one year without insulin injections and also lessened some of his symptoms, such as numbness in his fingers and toes. \u201cSo the potential seems to be there, if the work we and others are doing to ensure production of cells with adequate, lasting function can be produced,\u201d says the researcher. As a practitioner, he knows the scientific reality. \u201cBut given the impact that regenerative medicine can have on my quality of life and on my loved ones, it is easy to understand how those less familiar with the science and industry can be frustrated or impatient with the rate of progress.\u201d \n                     Stem-cell ruling riles researchers \n                   \n                     Three-dimensional tissue culture based on magnetic cell levitation \n                   \n                     Changing medium and passaging cell lines \n                   \n                     Web focus on iPS cells \n                   Reprints and Permissions"}
]