[
{"file_id": "539346a", "url": "https://www.nature.com/articles/539346a", "year": 2016, "authors": [{"name": "Jeff Tollefson"}], "parsed_as_year": "2006_or_before", "body": "The waters of the Southern Ocean have absorbed much of the excess heat and carbon generated by humanity. Joellen Russell wasn\u2019t prepared for the 10-metre waves that pounded her research vessel during an expedition south of New Zealand. \u201cIt felt like the ship would be crushed each time we rolled into a mountain of water,\u201d recalls Russell, an ocean modeller at the University of Arizona in Tucson. At one point, she was nearly carried overboard by a rogue wave. But what really startled her was the stream of data from sensors analysing the seawater. As the ship pitched and groaned, she realized that the ocean surface was low in oxygen, high in carbon and extremely acidic \u2014 surprising signs that nutrient-rich water typically found in the deep sea had reached the surface. As it turned out, Russell was riding waves of ancient water that had not been exposed to the atmosphere for centuries. Although controversial when she encountered it back in 1994, this powerful upwelling is now recognized as a hallmark of the Southern Ocean, a mysterious beast that swirls around Antarctica, driven by the world\u2019s strongest sustained winds. The Southern Ocean absorbs copious amounts of carbon dioxide and heat from the atmosphere, which has slowed the rate of global warming. And its powerful currents drive much of the global ocean circulation. The hostile conditions have kept oceanographers at bay for decades, but a new era of science is now under way. Researchers from around the world are converging on the region with floats, moorings, ships, gliders, satellites, computer models and even seals fitted with sensors. The goal is to plug enormous data gaps and bolster understanding of how the Southern Ocean \u2014 and the global climate \u2014 functions. Doing so could be key to improving predictions of how quickly the world will warm, how long the Antarctic ice sheet will survive and how fast sea levels will rise. \u201cIt\u2019s been amazing to see this explosion of information,\u201d says Arnold Gordon, an oceanographer at Lamont-Doherty Earth Observatory in Palisades, New York, who led some of the early Southern Ocean surveys in the 1960s. \u201cNew technologies are allowing us access to these remote areas, and we are far less dependent on driving a ship through the sea ice.\u201d Already, initial data from an array of ocean floats suggest that upwelling waters could be limiting how much CO 2  the Southern Ocean absorbs each year. This raises new questions about how effective these waters will be as a brake on global warming in decades to come. \u201cThe Southern Ocean is doing us a big climate favour at the moment, but it\u2019s not necessarily the case that it will continue doing so in the future,\u201d says Michael Meredith, an oceanographer with the British Antarctic Survey in Cambridge, UK. Meredith is heading a series of expeditions over the next five years to help document the uptake of heat and carbon. \u201cIt really is the key place for studying these things.\u201d \n               Tracking carbon \n             The mysteries of the Southern Ocean have beckoned explorers for centuries, but the unique geography of the region makes it a perilous place for ships. There are no landmasses to tame the winds and waves that race around the planet at 60\u00b0 S. And the ice surrounding Antarctica is notorious for engulfing wayward vessels, including Ernest Shackleton's  Endurance  in 1915. Scientists only started to realize how important the region is for controlling global climate in the 1980s, when several groups were trying to explain what had caused atmospheric CO 2  concentrations to drop by about one-third during the last ice age and then later rise. Oceanographer Jorge Sarmiento at Princeton University in New Jersey realized that changes in circulation and biology in the Southern Ocean could help to cool and warm the planet 1 . Three decades later, Sarmiento is leading an effort to gather the first real-time data on the chemical and biological processes that govern carbon in the Southern Ocean. The US$21-million Southern Ocean Carbon and Climate Observations and Modeling Project (SOCCOM) has already deployed 51 of a  planned 200 robotic floats  that bob up and down in the upper 2,000 metres of the Southern Ocean. Building on the global Argo array, which consists of more than 3,700 floats collecting temperature and salinity data, the SOCCOM floats also measure oxygen, carbon and nutrients. With the new data, Sarmiento and his team can test their models and refine estimates of how CO 2  moves between the seas and the sky. Indirect evidence suggests that the Southern Ocean is a net carbon sink and has absorbed as much as 15% of the carbon emissions emitted by humanity since the industrial revolution. But at some times of year and in specific places in this region, carbon-rich surface waters release CO 2  into the atmosphere. Now, researchers are getting some of their first glimpses in near-real time of what happens in the Southern Ocean, particularly in winter. \u201cRight off the bat, we are seeing CO 2  fluxes into the atmosphere that are much greater than we had estimated before,\u201d Sarmiento says. \u201cIt\u2019s just revolutionary.\u201d The unpublished analysis is based on just 13 floats that have been in the water for at least a year, so the question now is whether the higher CO 2  emissions during winter represent larger trends across the entire Southern Ocean. \u201cIt\u2019s pretty tantalizing,\u201d says Alison Gray, a postdoctoral researcher at Princeton who is leading the study. \u201cIt would imply that potentially there is a much weaker carbon sink in the Southern Ocean than has been estimated.\u201d Hints of something similar have been seen before. In 2007, a team led by Corinne Le Qu\u00e9r\u00e9, now director of the Tyndall Centre for Climate Change Research in Norwich, UK, published a study in  Science 2  indicating that the rate of carbon uptake by the Southern Ocean decreased between 1981 and 2004. The authors blamed the changes on the winds that encircle the Antarctic continent. The speed of those winds had increased during that time, probably as a result of the hole in the stratospheric ozone layer over Antarctica and possibly because of global warming. Stronger winds are better able to pull up deep, ancient water, which releases CO 2  when it reaches the surface. That would have caused a net weakening of the carbon sink. If that trend were to continue, atmospheric CO 2  levels would rise even faster in the future. However, a study in  Science 3  last year found that the carbon sink started to strengthen in the early 2000s (see \u2018The unreliable sink\u2019). Le Qu\u00e9r\u00e9 says it\u2019s unclear whether that rise in CO 2  absorption is a return to normal or a deviation from the long-term weakening of the sink. Regardless, she says, it\u2019s now clear that the Southern Ocean might be much more fickle than scientists thought. SOCCOM floats will probably help researchers to answer these questions, but it could be years before they can say anything concrete about trends. Nor is Le Qu\u00e9r\u00e9 convinced that the new network of floats will provide enough detail. In a paper published in July 4 , she found that models of carbon uptake by the Southern Ocean depend strongly on assumptions about the structure of the food web there. She says that climate scientists need to improve their understanding of the type and timing of phytoplankton and zooplankton blooms if they are going to get their climate projections right. \u201cIn my view, that's the next frontier,\u201d she says. \n               Warming waters \n             Carbon is only part of the story in the Southern Ocean. Scientists are also beginning to pin down what happens to all the heat that gets absorbed there. The Southern Ocean is the starting point for a network of currents that carry water, heat and nutrients throughout the ocean basins. Near Antarctica, surface waters normally grow cold and dense enough to sink to the bottom of the ocean, forming abyssal currents that hug the sea floor as they flow north into the Pacific, Atlantic and Indian oceans. Much of what scientists know about these currents comes from ship surveys conducted every decade or so since the early 1990s. In 2010, when researchers analysed data from the surveys, they found a pronounced warming trend in abyssal waters, which were somehow absorbing about 10% of the excess heat arising from global warming 5 . The level of warming in the deep ocean came as a surprise, and researchers have proposed several explanations that centre on the Southern Ocean. One factor could be that surface waters around Antarctica have become less salty, in part because of an increase in summer rainfall over the ocean. Fresher surface water is less dense, so that change would choke the supply of cold water sinking to the sea floor to feed the bottom currents. \u201cThe deep water warms up because it\u2019s not getting as much cold-water replenishment,\u201d says Gregory Johnson, an oceanographer with the National Oceanic and Atmospheric Administration (NOAA) in Seattle, Washington, who co-authored the 2010 analysis. An as-yet-unpublished analysis, based on initial data from the third round of ship surveys, finds similar trends, but researchers have longed for more frequent measurements to provide a fuller picture. That could happen if a proposed international project moves forward. Called Deep Argo, this would be an array of floats that regularly dive all the way to the bottom of the ocean. Johnson is involved in a US consortium that is testing 13 floats in a basin off the coast of New Zealand, and another nine south of Australia. Others are using moorings to monitor deep water flows. Since 1999, Gordon has maintained an array of moorings in the Weddell Sea, one of the main areas where cold surface waters sink to form ocean bottom currents. He has seen the deep water growing less salty in some areas, but the long-term trends are not clear 6 . \u201cWe are really only scratching the surface of how bottom waters are changing, and how that is impacting the large-scale global ocean circulation,\u201d he says. \n               Along the edge \n             In January 2015, oceanographers aboard the Australian icebreaker  Aurora Australis  were cruising off the coast of Antarctica when they were presented with a unique opportunity. Following a crack in the sea ice, they were able to reach the edge of the Totten Glacier, one of the biggest drainage points for the East Antarctica ice sheet. No other expedition had reached within 50 kilometres of the glacier. The team deployed floats and gliders into the waters around and underneath the glacier, which is 200 metres thick at its front edge. What they found came as a shock. The water at the front of the glacier was 3 \u00b0C warmer than the freezing point at the base of the glacier. \u201cWe always thought Totten was too far away from warm water to be susceptible, but we found warm water all over the shelf there,\u201d says Steve Rintoul, an oceanographer at the Antarctic Climate and Ecosystems Cooperative Research Centre in Hobart, Australia. Scientists had already shown 7 , 8  that warm-water currents are undercutting the West Antarctic ice sheet in many areas along the peninsula where the glaciers extend into the ocean. But Rintoul says that this expedition provided some of the first hard evidence that these same processes are affecting East Antarctica, raising new questions about the longevity of the mammoth ice sheets that blanket the continent. There is no clear answer yet for what is driving the warming of these near-surface currents. Some explanations invoke changes in the winds over the Southern Ocean and the upwelling of warm waters. Others focus on fresher surface waters and an expansion of sea ice in some areas. The combination of extra sea ice and fresher surface waters could create a kind of cap on the ocean that funnels some of the warmer upwelling water towards the coast. \u201cEvery scientist, including me, has their favourite explanation,\u201d Gordon says. \u201cBut that\u2019s how science works: the more you observe, the more complicated it gets.\u201d Finding the answers may require recruiting some of Antarctica\u2019s permanent residents. Meredith\u2019s team at the British Antarctic Survey plans to equip Weddell seals with sensors so that the animals can collect water measurements as they forage below the sea ice along the continental shelf. This zone has particular importance because it is precisely where cold water begins its descent into the abyss. \u201cThe processes that happen in that shelf region are very important on a global scale, but measuring them is very difficult,\u201d Meredith says. \u201cThe seals sort of transcend that barrier.\u201d The Weddell seals are just one component of the expedition\u2019s arsenal. The team will also send autonomous gliders under the sea ice on preprogrammed routes to collect temperature and salinity data down to depths of 1,000 metres. Measurements taken from ships will help fill in the picture of what happens in this crucial region around Antarctica \u2014 and how it relates to the rest of the global ocean circulation. Getting the data is only half the challenge. Ultimately, scientists need to improve their models of how currents transport heat, CO 2  and nutrients around the globe. Even armed with better measurements, results suggest that modellers have a way to go. An analysis of data from the ship surveys suggests that upwelling ocean water does not rise in a simple pattern near Antarctica. Rather, it swirls around the continent one and a half times before reaching the surface. And Sarmiento\u2019s team at Princeton found that only the highest-resolution models could accurately capture that behaviour. Sarmiento says that it could be a while before the models can simulate what really happens in this region, but he is confident that day will eventually arrive. For Russell, it\u2019s as if scientists are at last lifting the veil on the Southern Ocean. After she returned from her maiden voyage in 1994, she turned to modelling because there wasn\u2019t enough data at the time to quantify the effects of the upwelling she encountered. Today she has it both ways. Russell is heading the modelling component of the SOCCOM project, and she is getting more data than she ever dreamt of. \u201cIt\u2019s just a wonderful time to be an oceanographer,\u201d she says, \u201ceven as we are carrying out this really scary geophysical experiment on our planet.\u201d \n                 Tweet \n                 Follow @NatureNews \n                 Follow @jefftollef \n               \n                     World\u2019s largest marine reserve hailed as diplomatic breakthrough 2016-Oct-28 \n                   \n                     Antarctic model raises prospect of unstoppable ice collapse 2016-Mar-30 \n                   \n                     Massive network of robotic ocean probes gets smart upgrade 2016-Mar-22 \n                   \n                     Antarctic coast meltdown could trigger ice-sheet collapse 2015-Nov-02 \n                   \n                     Southern Ocean sucks up more carbon dioxide than was thought 2015-Sep-11 \n                   \n                     Southern Ocean Carbon and Climate Observations and Modeling (SOCCOM) \n                   \n                     British Antarctic Survey \n                   \n                     Global Ocean Ship-Based Hydrographic Investigations Program (GO-SHIP) \n                   Reprints and Permissions"},
{"file_id": "540330a", "url": "https://www.nature.com/articles/540330a", "year": 2016, "authors": [{"name": "Amy Maxmen"}], "parsed_as_year": "2006_or_before", "body": "He has influenced leaders from Melinda Gates to Fidel Castro. Now, he is on a mission to save people from their preconceived ideas. Hans Rosling knew never to flee from men wielding machetes. \u201cThe risk is higher if you run than if you face them,\u201d he says. So, in 1989, when an angry mob confronted him at the field laboratory he had set up in what is now the Democratic Republic of the Congo, Rosling tried to appear calm. \u201cI thought, \u2018I need to use the resources I have, and I am good at talking\u2019.\u201d Rosling, a physician and epidemiologist, pulled from his knapsack a handful of photographs of people from different parts of Africa who had been crippled by konzo, an incurable disease that was affecting many in this community, too. Through an interpreter, he explained that he believed he knew the cause, and he wanted to test local people\u2019s blood to be sure. A few minutes into his demonstration, an old woman stepped forward and addressed the crowd in support of the research. After the more aggressive members of the mob stopped waving their machetes, she rolled up her sleeve. Most followed her lead. \u201cYou can do anything as long as you talk with people and listen to people and talk with the intelligentsia of the community,\u201d says Rosling. He is still trying to arm influential people with facts. He has become a trusted counsellor and speaker of plain truth to United Nations leaders, billionaire executives such as Facebook\u2019s Mark Zuckerberg and politicians including Al Gore. Even Fidel Castro called on the slim, bespectacled Swede for advice. Rosling\u2019s video lectures on global health and economics have elevated him to viral celebrity status, and he has been listed among the 100 most influential people in the world by the magazines  Time  and  Foreign Policy . Melinda Gates of the Bill & Melinda Gates Foundation says, \u201cTo have Hans Rosling as a teacher is one of the biggest honours in the world.\u201d But among his fellow scientists, Rosling is less popular. His accolades do not include conventional academic milestones, such as massive grants or a stream of publications in top-tier journals. And rather than generating data, Rosling has spent the past two decades communicating data gathered by others. He relays facts that he thinks many academics have been too slow to appreciate and argues that researchers are ignorant about the state of health and wealth around the world. That\u2019s dangerous. \u201cCampuses are full of siloed people who do advocacy about things they don\u2019t understand,\u201d he says. So now, in the sunset of his career, Rosling is writing a book with his son Ola and his daughter-in-law Anna Rosling R\u00f6nnlund to dispel outdated beliefs. It has the working title  Factfulness , and they hope it will inform everyone from schoolchildren to esteemed experts about how the world has changed: how the number of births per woman worldwide has dropped over the past few decades, for example, and how average life expectancy (71 years) is now closer to that of the country with the highest (Japan, 84) than the lowest (Swaziland, 49). He reasons that experts cannot solve major challenges if they do not operate on facts. \u201cBut first you need to erase preconceived ideas,\u201d he says, \u201cand that is the difficult thing.\u201d \n               Life on the brink \n             Rosling\u2019s ambitions were born from curiosity. As a young boy in Uppsala, he listened intently as his father, a coffee-factory employee, described the hardships of the East African labourers who picked the beans. Rosling and his girlfriend, Agneta Thordeman, joined student protests against South African apartheid and the US war in Vietnam. The couple studied medicine \u2014 she as a nurse and he as a doctor \u2014 and travelled through India and southeast Asia on a shoestring budget. In 1972, they were married and seven years later they moved to Mozambique with their two small children. Rosling wanted to fulfil a promise he had made many years earlier to the founder of the Mozambican Liberation Front, Eduardo Mondlane. Mondlane had explained that Mozambique\u2019s future would be challenging after the country gained independence from Portugal, because the nation was so poor and education levels low. Rosling recalls, \u201cHe shook my hand and looked me in the eyes and said: \u2018Promise you will work with us\u2019.\u201d Mondlane was killed by a letter bomb soon afterwards \u2014 he did not live to see independence, which came in 1975 \u2014 but Rosling kept his word. The Mozambican government assigned Rosling to a northern part of the country, where he would be the only doctor serving 300,000 people. Because of the scarcity of health care, patients were often in excruciating pain by the time he saw them. Rosling recalls performing emergency surgery to extract dead fetuses from women on the verge of death. He watched helplessly as children perished from diseases that should have been simple to prevent. \u201cThose years became a sort of trauma,\u201d he says. In 1981, he received a letter from an Italian nun working as a nurse at a remote health post. \u201cPlease come,\u201d she wrote. People in the surrounding villages had been stricken with sudden paralysis of both legs. Separating from his family, Rosling embedded himself in the crisis. He was assigned to lead a survey of 500,000 people and found that populations with the highest rate of the disease survived entirely on bitter cassava, the only crop that could grow when drought struck the region. The plant turned out to contain cyanogenic glucoside, a precursor to cyanide. Typically, soaking cassava roots in water for several days removed the toxin. But with streams running dry and families starving, women who prepared cassava had skipped this step \u2014 to their detriment. Dietary amino acids can also detoxify the poison, but people had no access to meat or beans that provide them. At the end of 1981, owing to a number of circumstances including the death of their third child, Rosling and his family returned to Sweden. Rosling became a lecturer on health care in low-income countries at Uppsala University but spent time in Tanzania and the Congo region as well, studying the paralysing disease he had first observed in Mozambique. He noticed that no matter what country he was in, the towns afflicted looked similarly tragic. Skeleton-thin people hobbled down dirt paths on makeshift crutches, or crawled with their legs twisted and dangling behind them like anchors. One Congolese community called the malady konzo, derived from a word referring to an antelope tethered at its knees. This is the name that Rosling would use in 1990, when he and his colleagues formally defined the disease and laid out the evidence for what causes it ( W.\u00a0P.\u00a0Howlett  et\u00a0al. Brain   113,  223\u2013235; 1990 ). As Rosling travelled, he trained African graduate students who specialized in konzo, and together they found that proper cassava processing was the most realistic method of short-term prevention. However, the message often fell on deaf ears because of hunger and conflict. Rosling became convinced that the real root of konzo resided not in cassava, but in economic devastation. \u201cExtreme poverty produces diseases. Evil forces hide there,\u201d he says. \u201cIt is where Ebola starts. It\u2019s where Boko Haram hides girls. It\u2019s where konzo occurs.\u201d \n               The true picture of poverty \n             The World Bank defines extreme poverty as a state in which people survive on less than US$1.90 per day. Rosling can recognize it in other ways. He has seen it in people who must walk for hours without shoes to find water or to farm eroded soil. He sees it in those who remain short because of malnourishment, whose babies are born dangerously underweight and who are trapped with no options in life. Ultimately, he says that eliminating extreme poverty is the only way to cure konzo and prevent other maladies \u2014 both social and infectious. Money, politics and culture underlie disease in many circumstances, he argues.  Extreme poverty produces diseases. Evil forces hide there.  Take an outbreak in Cuba that Rosling investigated in 1992. The Cuban embassy in Sweden had asked him to find out whether toxic cassava could have caused roughly 40,000 people to experience visual blurring and severe numbness in their legs. On his first morning in Havana, Rosling met local epidemiologists in a conference room. \u201cThen, two men walk in with guns, and in comes Fidel Castro,\u201d he recalls. \u201cMy first surprise was that he was so kind, like Father Christmas. He didn\u2019t have the attitude you might expect from a dictator.\u201d With Castro\u2019s approval, Rosling travelled to the heart of the outbreak, in the western province of Pinar del R\u00edo. It turned out that there was no link with cassava. Rather, adults stricken with the disorder all suffered from protein deficiency. The government was rationing meat, and adults had sacrificed their portion to nourish children, pregnant women and the elderly. Reporting back to Castro, Rosling couched his conclusions carefully: \u201cI know your neighbours want to force their economic system on you, which I don\u2019t like, but the system needs to change because this planned economy has brought this disease to people.\u201d After his presentation, Rosling went to the toilet. A Cuban epidemiologist approached him to thank him. He and his colleagues had come to the same conclusion several months earlier, but they were removed from the investigation for criticizing communism. Corroboration of their work from Rosling and other independent researchers supported the policy changes that stemmed the outbreak. \n               Ignorance about ignorance \n             Back in Sweden, Rosling continued to teach global health, moving to the Karolinska Institute in Stockholm in 1996. But he came to realize that neither his students nor his colleagues grasped extreme poverty. They pictured the poor as almost everyone in the \u2018developing world\u2019: an arbitrarily defined territory that includes nations as economically diverse as Sierra Leone, Argentina, China and Afghanistan. They thought it was all large family sizes and low life expectancies: only the poorest and most conflict-ridden countries served as their reference point. \u201cThey just make it about us and them; the West and the rest,\u201d Rosling says. How could anyone hope to solve problems if they didn\u2019t understand the different challenges faced, for example, by Congolese subsistence farmers far from paved roads and Brazilian street vendors in urban  favelas ? \u201cScientists want to do good, but the problem is that they don\u2019t understand the world,\u201d Rosling says. Ola, his son, offered to help explain the world with graphics, and built his father software that animated data compiled by the UN and the World Bank. Visual aids in hand, the elder Rosling began to script the provocative presentations that have made him famous. In one, a graph shows the distribution of incomes in 1975 \u2014 a camel\u2019s back, with rich countries and poor countries forming two humps. Then he presses \u2018go\u2019 and China, India, Latin America and the Middle East drift forward over time. Africa moves ahead too, but not nearly as much as the others. Rosling says, \u201cThe camel dies and we have a dromedary world with one hump only!\u201d He adds, \u201cThe per cent in poverty has decreased \u2014 still it\u2019s appalling that so many remain in extreme poverty.\u201d Rosling\u2019s online presentations grew popular, and the investment bank Goldman Sachs invited him to speak at client events. His message seemed to support advice from the firm\u2019s chief economist, Jim O\u2019Neill. In 2001, O\u2019Neill had coined the acronym BRIC for the emerging economies of Brazil, Russia, India and China, often considered part of the developing world. He warned that financial experts ignored these rising powers at their peril. \u201cI used to tease my colleagues who thought in a traditional framework,\u201d O\u2019Neill says. \u201cWhy are we talking about China as the developing world? Based on the rate of economic growth, China creates another Greece every three months; another UK every two years.\u201d Rosling welcomed the new audience. \u201cThey request my lectures because they want to know the world as it is,\u201d he says. The private sector needs to understand the economic and political conditions of current and potential markets. \u201cTo me it was horrific to realize that business leaders had a more fact-based world view than activists and university professors.\u201d O\u2019Neill left Goldman Sachs in 2013, and went on to lead a committee on global antibiotic resistance. He looked to Rosling for a big-picture view. \u201cI wish there were more people like him,\u201d says O\u2019Neill. \u201cHe genuinely thinks about the future of all seven-plus-billion of us, rather than so many who claim they do but actually come at it with a narrow and national perspective.\u201d Rising wealth pleases Rosling because he wants extreme poverty to disappear. To help get there, he celebrates improvements. He calls the UN\u2019s push to eradicate extreme poverty by 2030 an entirely reasonable goal because the proportion of people living in extreme poverty has declined by more than half in the past quarter of a century, and the strategies needed to help the remainder are known. His attitude aligns him with Steven Pinker of Harvard University in Cambridge, Massachusetts, who wrote  The Better Angels of our Nature  (Viking, 2011). In the book, Pinker argues that global rates of violence are much lower than they were in the past. The two met at a TED conference in 2007, when Pinker took the stage after Rosling ended his talk by swallowing a sword (whatever grabs attention). Pinker says that Rosling made him think that \u201cthe decline in violence might be a part of an even bigger story about humans gradually making progress against other scourges of the human condition\u201d. Both have been criticized as being Pollyannaish about the global situation in the face of tragedies such as the conflict in Syria. \u201cPeople think that if you emphasize how things have gone well it is the same as saying no problems remain. That\u2019s not true,\u201d Pinker counters. \u201cIn fact, I strongly suspect that people are more motivated to reduce problems like poverty and violence if they think there is a good chance they can succeed.\u201d And as a cognitive scientist, Pinker admires the animations that Rosling uses. One, which depicts countries as bubbles that migrate over time according to wealth, life span or family size, allows viewers to grasp multiple variables simultaneously. \u201cIt\u2019s a stroke of genius,\u201d Pinker says. \u201cHe gets our puny human brain to appreciate five dimensions.\u201d In 2005, Rosling, Ola and Anna founded the non-profit Gapminder Foundation in Stockholm to develop the \u2018moving-bubble\u2019 software, Trendalyzer, and to spread access to information and animated graphs depicting world trends. Google acquired Trendalyzer in 2007, and Gapminder has successfully pressured the World Bank to make its data free to the public. \n               How to dismantle the population bomb \n             Rosling\u2019s charm appeals to those frustrated by the persistence of myths about the world. Looming large is an idea popularized by Paul Ehrlich, an entomologist at Stanford University in California, who warned in 1968 that the world was heading towards mass starvation owing to overpopulation. Melinda Gates says that after a drink or two, people often tell her that they think the Gates Foundation may be contributing to overpopulation and environmental collapse by saving children\u2019s lives with interventions such as vaccines. She is thrilled when Rosling smoothly uses data to show how the reverse is true: as rates of child survival have increased over time, family size has shrunk. She has joined him as a speaker at several high-level events. \u201cI\u2019ve watched people have this \u2018aha\u2019 moment when Hans speaks,\u201d she says. \u201cHe breaks these myths in such a gentle way. I adore him.\u201d The appreciation extends to the World Health Organization: director-general Margaret Chan says that Rosling provides facts for decision-makers to consider. \u201cHe makes the case that as people grow in wealth, they grow in health,\u201d she says. And his talks help her to convince governments that data collection can help them to track whether they are getting returns on their investments in global health. The past few years have brought new challenges. In 2014, Ebola was spreading in West Africa, and Rosling\u2019s liver was failing. A hepatitis C infection that he had mysteriously acquired in his youth was becoming lethal. He travelled to Japan to receive the newest treatment, not yet approved in Sweden. By October, he found himself fretting, from afar, over discrepancies in official reports on the number of suspected and confirmed Ebola cases. \u201cI realized my skills were needed,\u201d he says. As soon as the drugs cured him, Rosling flew to West Africa to join the Liberian government\u2019s epidemiological-surveillance team. The team wanted to consolidate data, but struggled with the disparate ways in which international agencies collected information. \u201cWe were losing ourselves in details,\u201d says Rosling. \u201cI saw this was a war situation: all we needed to know is, are the number of cases rising, falling or levelling off?\u201d After a few months, it became clear that the rate of new cases had diminished. Rosling was rewarded with a traditional chieftainship by the Liberian government. Now, at the age of 68, Rosling has retreated to his red wooden house in Uppsala with Agneta. He continues to work and plugs away at his \u201cfactfulness book on megamisconceptions\u201d. Every now and again, he stirs the pot. In October, he published a piece in  The Lancet  identifying a misleading statistic in a widely cited report from an advocacy organization launched by the UN ( H.\u00a0Nordenstedt and H.\u00a0Rosling  Lancet   388,  1864\u20131865; 2016 ). The group claimed that 60% of maternal deaths occur in settings of conflict, displacement and natural disaster. Rosling checked the numbers and calculated that the true amount was no more than 17%. A UN spokesperson explains that part of the discrepancy derives from the fact that in the original figure, women who gave birth in nations affected by crises were included \u2014 even if their region had not been directly impacted.  Global health seems to have entered into a post-fact era.  Rosling blames the popularity of the dramatic-sounding statistic on the desire to raise funds at a time when refugee crises garner financial support. \u201cGlobal health seems to have entered into a post-fact era, where the labelling of numerators is incorrectly tweaked for advocacy purposes,\u201d he wrote in the  Lancet  article with Helena Nordenstedt, a colleague at the Karolinska Institute. The majority of maternal deaths occur among the extremely poor, they added. Those remote populations are hidden even from the aid community. Rosling prods academics when he can (see \u2018Test your world knowledge\u2019). For instance, at a Nobel-laureate meeting in Lindau, Germany, in 2014, he quizzed the audience of leading scientists on the average life expectancy in the world today. Out of three choices, just over one-quarter of the crowd picked the correct answer of 70. That\u2019s less than would be expected by chance. The quiz spurred laughter in Lindau, but scientists are generally not his audience. Rosling is rarely invited to give keynote lectures or departmental seminars because he doesn\u2019t push a single field forward; he has not made fundamental discoveries since his konzo days. Researchers agree that he is a good communicator \u2014 but not the kind to teach scientists. \u201cPeople like Hans Rosling face the criticism of being too superficial,\u201d explains Peter Hotez, a tropical-disease scientist at Baylor College of Medicine in Houston, Texas. \u201cIt\u2019s the dilemma of the public intellectual,\u201d he says, describing academics who bridge several disciplines rather than excel at one. Rosling says he never cared much about his academic reputation. He was lucky to receive steady support from the former head of the Karolinska Institute, Hans Wigzell, who encouraged him to seek outside funding so that he could pursue whatever he deemed most important. After Rosling decided that that meant teaching broadly, he walked away from research entirely. He also differs from global-health experts who have stepped outside academia to change policies. He hasn\u2019t worked to expand access to HIV medication, for example. He has not \u2014 like Hotez \u2014 put neglected tropical diseases on the world health agenda. And konzo still exists. But Rosling has had success; it\u2019s just that the impact becomes harder to measure the broader his goals become. Now that he has decided that the public at large must buy into ending extreme poverty and creating a sustainable world, he has dedicated the last chapter of his career to education. With the right facts, he hopes, people will make the right decisions \u2014 he just needs to face down the misconceptions. Who is better suited to the task than a man able to stave off machetes with the power of a few pictures and his words? \n                 Tweet \n                 Follow @NatureNews \n               \n                     The mental-health crisis among migrants 2016-Oct-10 \n                   \n                     Where to put the next billion people 2016-Sep-28 \n                   \n                     Development: Slow down population growth 2016-Feb-24 \n                   \n                     The science myths that will not die 2015-Dec-16 \n                   \n                     China's birth rate won't be dramatically affected by end of one-child policy 2015-Oct-29 \n                   \n                     World population unlikely to stop growing this century 2014-Sep-18 \n                   \n                     Seven billion and counting 2011-Oct-19 \n                   \n                     Gapminder \n                   Reprints and Permissions"},
{"file_id": "540507a", "url": "https://www.nature.com/articles/540507a", "year": 2016, "authors": [], "parsed_as_year": "2006_or_before", "body": "Ten people who mattered this year. Gabriela Gonzalez: Gravity spy | Demis Hassabis: Mind crafter | Terry Hughes: Reef sentinel | Guus Velders: Cooling agent | Celina M. Turchi: Zika detective | Alexandra Elbakyan: Paper pirate | John Zhang: Fertility rebel | Kevin Esvelt: CRISPR cautionary | Guillem Anglada-Escud\u00e9: Planet hunter | Elena Long: Diversity trailblazer | Ones to watch \n               GABRIELA GONZALEZ: Gravity spy \n             \n               A physicist helped to catch the first direct signs of long-sought gravitational waves. \n               By Davide Castelvecchi \n             A year ago, Gabriela Gonzalez was struggling to contain the biggest secret of her life. Two giant detectors in the United States had picked up  signs of gravitational waves  \u2014 wrinkles in space-time imagined by Albert Einstein but never before directly witnessed. It was Gonzalez\u2019s job to help lead more than 1,000 scientists in their careful efforts to verify the discovery before announcing it to the public. News like that doesn\u2019t stay under wraps for long, but the discovery was so momentous that the research team took nearly five months to  analyse data from the two Laser Interferometer Gravitational-Wave Observatory (LIGO) detectors  in Washington state and Louisiana. As spokesperson for the LIGO Scientific Collaboration, Gonzalez was one of the key people coordinating the analysis by groups scattered around the world, including researchers at the Virgo interferometer near Pisa, Italy, which pools its data with LIGO. The role of shepherding this massive effort made use of Gonzalez\u2019s multidimensional talents. Most physicists know early on whether they will be a theorist or an experimentalist. But Gonzalez started her graduate studies as a theoretical physicist and only later switched to experimental work, when she showed uncommon aptitude. \u201cIt was the thing that set her up as a first-class scientist,\u201d says Rainer Weiss, a physicist at the Massachusetts Institute of Technology in Cambridge and one of the founders of LIGO. Throughout her career, Gonzalez has done \u201ca bit of everything\u201d at LIGO, she says. For a while, she took on the crucial task of diagnosing the performance of the interferometers to make sure that they achieved unparalleled sensitivity \u2014 which is now enough to detect length changes in the 4-kilometre-long arms of the interferometers to within one part in 10 21 , roughly equivalent to the width of DNA compared with the orbit of Saturn. She has helped to lead the teams that analyse the data. And she nudged gravitational-wave researchers and dozens of their colleagues in conventional astronomy into signing pacts of cooperation. Together, they will look for phenomena that emit both gravitational and electromagnetic waves, in what has been called the coming age of multimessenger astronomy. In the hectic months before announcing the LIGO discovery, Gonzalez and her colleagues struggled to make sure that they had iron-clad evidence. They knew that history had not been kind to those who had previously reported gravitational waves. Most recently, in early 2015, an  international collaboration had to retract its claims  that a tele\u00adscope at the South Pole had discovered indirect signs of the long-sought vibrations. To add to the pressure on the LIGO team, rumours of a discovery began to leak within a week of the initial finding, and reporters started to call. Throughout the long analysis period, Gonzalez says, she never made an important decision without consulting colleagues. But others laud her leadership. \u201cWhat Gaby did is, she managed to get us through this period,\u201d Weiss says. Gonzalez is based at Louisiana State University in Baton Rouge, close to the LIGO interferometer in Livingston. In 2008, she became the first woman to receive a full professorship in her department. She says that she has never experienced outright sexual harassment or discrimination during her career, but \u201cI had to prove myself perhaps more than other people\u201d. Gonzalez has said that after her current term as LIGO spokesperson ends in March 2017, she will not run again. She plans to go back to full-time research. The field of science she helped to create \u2014 gravitational-wave astronomy \u2014 has just seen its dawn. \u201cIt has always been a fun ride. And now it\u2019s even better.\u201d \n               DEMIS HASSABIS: Mind crafter \n             \n               An AI developer beat one of the best at Go. Next up, solve global problems. \n               By Elizabeth Gibney \n             For veteran gamer Demis Hassabis, March brought the toughest match of his life\u00a0\u2014\u00a0and he wasn\u2019t even playing. Hassabis had to watch from the sidelines as his team\u2019s creation, the computer program AlphaGo, took on Lee Sedol, a top-ranked champion in the strategy game Go. The computer won, marking a huge victory for the field of artificial intelligence (AI) and another in a series of triumphs for Hassabis. As co-founder of DeepMind, the London-based firm that developed AlphaGo, Hassabis was both elated and relieved. \u201cIt felt like our moonshot, and it was successful,\u201d he says. But the win was about much more than Go. Hassabis wanted to show the world the power of machine-learning techniques, which he hopes to someday harness in a human-like, general AI capable of solving complex global problems. Hassabis had sketched this vision out as a precocious youth. A chess prodigy, he began designing innovative, multimillion-selling video games while in his teens and started his own company in his early 20s. After completing a PhD in cognitive neuroscience, he founded DeepMind in 2010. Google bought the company 4\u00a0years later for a reported \u00a3400\u00a0million (more than US$650\u00a0million at the time). At the firm, researchers apply inspiration from neuro\u00adscience to eye-catching AI tasks, from synthesizing speech to  navigating the London Underground . Each algorithm builds complexity on to the last, says Hassabis, and weaves in capabilities that have historically been developed separately in AI. DeepMind AIs have gone from learning how to see, and acting on that vision, to using it to plan and reason. In terms of real-world problem-solving, the team used machine learning to cut power usage in Google\u2019s data centres by 15%, something that Hassabis hopes to apply on a much grander scale. Although the company\u2019s researchers do publish, their work-in-progress is kept under wraps, which irks some academics. And some data-privacy advocates have concerns over Google DeepMind\u2019s plans to collaborate with the UK National Health Service. Scientists, however, have been  flocking to work at the company . In person, Hassabis is unassuming but eager. He has a knack for swaying others to his passion, says Eleanor Maguire, his former PhD supervisor at University College London. \u201cOnce he gets talking about something he\u2019s interested in, it\u2019s infectious,\u201d she says. Fitting research alongside running the company now means saving science for the small hours of the morning, something Hassabis says he doesn\u2019t mind. \u201cIt\u2019s a very important mission that we\u2019re on, and I think it\u2019s worth the sacrifice.\u201d \n               TERRY HUGHES: Reef sentinel \n             \n               A coral researcher sounded the alarm over massive bleaching at the Great Barrier Reef. \n               By Daniel Cressey \n             When Terry Hughes flew over the Great Barrier Reef in March, his heart sank at the sight of telltale pale patches just below the surface, where corals were dead or dying. Hughes, director of the Australian Research Council\u2019s (ARC\u2019s) Centre of Excellence for Coral Reef Studies in Townsville, says that he and his students wept after looking at the aerial surveys of the damage. The bleaching hit nearly all of the reef, with initial surveys showing 81% of the northern section suffering severely. It was the  most devastating bleaching ever documented on the Great Barrier Reef  \u2014 and part of a wider event that was harming corals across the Pacific. The trigger for this year\u2019s coral troubles in the Pacific was a  strong El\u00a0Ni\u00f1o warming pattern  in the tropical part of that ocean. Abnormally high water temperatures prompt corals to expel the symbiotic zooxanthellae algae that provide them with much of their food \u2014 and\u00a0their colour. Some corals can recover after bleaching, but others die. Follow-up studies in October and November found that 67% of\u00a0\u00adshallow-water corals in the 700-kilometre northern section of the Great Barrier Reef had died. When the massive El Ni\u00f1o reared up in the Pacific in 2015, Australian researchers feared that the country\u2019s reefs could be in danger. So Hughes, one of the world\u2019s leading coral researchers, assembled a task force ready to survey the reef if bleaching occurred. The group eventually expanded to 300 scientists. \u201cWe put together a very detailed research plan, hoping of course that it wouldn\u2019t happen,\u201d he says. Hughes is based close to the central portion of the Great Barrier Reef. After leading the initial surveys, he became the de facto spokesperson on the catastrophe. At the height of media interest in the bleaching, Hughes did 35 interviews in one day. \u201cIn Australia, even people who have never been to the Great Barrier Reef and might never go there regard it as an icon,\u201d says Bob Pressey, a fellow researcher at the ARC centre. The crisis on the reef defied some rules. Conventional thinking on bleaching events, says Hughes, is that corals die slowly from starvation after their zooxanthellae leave. But this year, water temperatures were so high that \u201cwe saw a lot of corals die before the starvation kicked in. They actually cooked.\u201d Corals throughout the world have struggled in the past couple of years, as global temperatures have repeatedly hit record highs. In October 2015, the US National Oceanic and Atmospheric Administration declared that a global bleaching event was happening as coral reefs in Hawaii, Papua New Guinea and the Maldives began to succumb. This year, the bleaching spread to Australia, Japan and other parts of the Pacific. Researchers say that, as climate change drives up baseline temperatures, bleaching will afflict reefs more frequently. Under some scenarios, this could happen so often that most corals can no longer survive. Hughes is not ready to give up on the Great Barrier Reef just yet. But the recent bleaching has left corals in a weakened state, prone to attacks from pathogens and predators. Another bleaching event in the near future could bring further damage. \u201cThe message to people,\u201d he says, \u201cshould be we\u2019ve got a closing window of opportunity to deal with climate change.\u201d \n               GUUS VELDERS: Cooling agent \n             \n               An atmospheric chemist laid the foundation for an international climate agreement. \n               By Jeff Tollefson \n             It isn\u2019t often that atmospheric chemists get to help save the world, but Guus Velders had his chance in October. He was attending inter\u00adnational negotiations in Kigali, Rwanda, that were seeking to phase out production and use of hydrofluorocarbons (HFCs), extremely potent greenhouse gases commonly used in air conditioners. Most nations had agreed on an aggressive timetable to begin eliminating the compounds, but India and a handful of other countries wanted an extra four years. After plugging the numbers into a model on his laptop computer, Velders informed negotiators that this particular concession would have little impact on the planet. That and his earlier work helped to smooth the way for a  widely hailed global accord , which was signed on 15 October. Velders, a soft-spoken researcher at the National Institute for Public Health and the Environment in Bilthoven, the Netherlands, is proud of the part he played. \u201cI\u2019ve never been involved in a process that leads to a global agreement on climate before,\u201d he says. It was no coincidence, however. Colleagues say that Velders has become the world\u2019s expert on HFC emissions, and that nobody else could have provided such rapid analysis in Kigali. He is part of a community of scientists that has helped to refashion the 1987 Montreal Protocol \u2014 an international agreement designed to protect the stratospheric ozone layer \u2014 into a  tool with which to fight global warming . The refrigerants that fall within the scope of the protocol are also powerful greenhouse gases, and Velders\u2019 team showed that the Montreal agreement actually did more to control global temperatures than did the 1997 Kyoto Protocol climate treaty. More recently, the team projected how much warming HFCs were likely to cause over the twenty-first century. That helped to set the stage for the agreement on HFCs, which was reached as an amendment to the Montreal Protocol. \u201cThe Velders team always answered the right questions at the right time,\u201d says Durwood Zaelke, president of the Institute for Governance & Sustainable Development, an advocacy group in Washington DC. \u201cIt\u2019s safe to say that we wouldn\u2019t have this agreement without them.\u201d Now it\u2019s back to the drawing board for Velders\u2019 team. Their scenario about how HFC emissions would grow over time was rendered obsolete by the new agreement to ban them. That\u2019s the kind of intellectual setback that Velders heartily accepts. \n               CELINA M. TURCHI: Zika detective \n             \n               A physician raced to make sense of a medical mystery in northeast Brazil. \n               By Declan Butler \n             Fears about the Zika virus spread across the globe in 2016, and the \u00adepicentre of concern was Brazil, where the epidemic first appeared in the Americas. Some researchers even called for postponing the Olympic Games scheduled for Rio de Janeiro in August that year. But away from the media frenzy, Celina Maria Turchi Martelli battled on the front lines in northeast Brazil to make sense of the medical mystery there. Turchi, a physician and infectious-disease expert, has had her life turned upside down by Zika since September 2015. That\u2019s when the ministry of health asked her to investigate a sharp rise in  reports of babies born with abnormally small heads and brains , a condition known as microcephaly, in her home state of Pernambuco. She quickly became convinced that the country was facing a public-health emergency. \u201cNot even in my worst nightmare as an epidemiologist had I imagined a microcephaly neonate epidemic,\u201d she says. Turchi, who is based at the Aggeu Magalh\u00e3es Research Center in Recife, immediately contacted scientists across the globe for help. She formed a networked task force of epidemiologists, infectious-diseases experts, paediatricians, neurologists and reproductive biologists. The challenges were formidable, says Turchi: there were no reliable lab tests for Zika, and there was  no consensus on a case definition of microcephaly . But the intense networking paid off, and Turchi and her colleagues eventually generated enough evidence to demonstrate a link between the condition and infection with Zika in the first trimester of pregnancy. Still, the mysteries are far from solved, says Turchi. Although Zika has spread across the Americas, the expected explosion in the number of microcephaly cases outside northeast Brazil has not materialized. Turchi and her task force are now trying to work out why. When she started going into the hospitals of Recife to investigate the outbreak, Turchi says, she had to innovate. \u201cThere was no book to follow.\u201d Now, she and her colleagues are writing that book. \n               ALEXANDRA ELBAKYAN: Paper pirate \n             \n               The founder of an illegal hub for paywalled papers has attracted litigation and acclaim. \n               By Richard Van Noorden \n             It took Alexandra Elbakyan just a few years to go from information-technology student to famous fugitive. In 2009, when she was a graduate student working on her final-year research project in Almaty, Kazakhstan, Elbakyan became frustrated at being unable to read many scholarly papers because she couldn\u2019t afford them. So she learnt how to circumvent publishers\u2019 paywalls. Her skills were soon in demand. Elbakyan saw scientists on web forums asking for papers they couldn\u2019t access \u2014 and she was happy to oblige. \u201cI got thanked many times for sending paywalled papers,\u201d she says. In 2011, she decided to automate the process and founded Sci-Hub, a pirate website that grabs copies of research papers from behind paywalls and serves them up to anyone who asks. This year, interest in Sci-Hub exploded as mainstream media cottoned on to it and usage soared. According to Elbakyan\u2019s figures, the site now hosts around 60\u00a0million papers and is likely to serve up more than 75\u00a0million downloads in 2016 \u2014 up from 42\u00a0million last year and, by one estimate, encompassing around 3% of all downloads from science publishers worldwide. It is copyright-breaking on a grand scale \u2014 and has brought Elbakyan praise, criticism and a lawsuit. Few people support the fact that she acted illegally, but many see Sci-Hub as advancing the cause of the open-access movement, which holds that papers should be made (legally) free to read and reuse. \u201cWhat she did is nothing short of awesome,\u201d says Michael Eisen, a biologist and open-access supporter at the University of California, Berkeley. \u201cLack of access to the scientific literature is a massive injustice, and she fixed it with one fell swoop.\u201d For the first few years of its existence, the site flew under the radar \u2014 but eventually it grew too big for subscription publishers to ignore. In 2015, the Dutch company Elsevier, supported by the wider publishing industry, brought a  US lawsuit against Elbakyan  on the basis of copyright infringement and hacking. If Elbakyan loses, she risks having to pay many millions of dollars in damages, and potentially spending time in jail. (For that reason, Elbakyan does not disclose her current location and she was interviewed for this article by encrypted e-mail and messaging.) In 2015, a US judge ordered Sci-Hub to be shut down, but the site popped up on other domains. It\u2019s most popular in China, India and Iran, she says, but a good 5% or so of its users come from the United States. Elbakyan has found her name splashed across newspapers, and says she typically gets a hundred supportive messages a week, some with financial donations. She says she feels a moral responsibility to keep her website afloat because of the users who need it to continue their work. \u201cIs there anything wrong or shameful in running a research-access website such as Sci-Hub? I think no, therefore I can be open about my activities,\u201d she says. Critics and supporters alike think that the site will have a lasting impact, even if it does not last. \u201cThe future is universal open access,\u201d says Heather Piwowar, a co-founder of Impactstory, a non-profit firm incorporated in Carrboro, North Carolina, which helps scientists track the impact of their online output. \u201cBut we suspect and hope that Sci-Hub is currently filling toll-access publishers with roaring, existential panic. Because in many cases that\u2019s the only thing that\u2019s going to make them actually do the right thing and move to open-access models.\u201d Whether or not that\u2019s true, Elbakyan says she will keep building Sci-Hub \u2014 in particular, to expand its corpus of older manuscripts \u2014 while studying for a master\u2019s degree in the history of science. \u201cI maintain the website myself, but if I\u2019m prevented, somebody else can take over the job,\u201d she says. \n               JOHN ZHANG: Fertility rebel \n             \n               A physician jump-started debate over a controversial IVF procedure. \n               By Sara Reardon \n             Shock, anger, scepticism and congratulations. Those were some of the reactions that fertility specialist John Zhang triggered in the scientific community in September, when he announced that a  controversial technique that mixes DNA from three people  had been used to produce a healthy baby boy. This kind of technique is intended to prevent children from inheriting disorders involving mitochondria \u2014 the cellular structures that produce energy. But ethical and safety concerns have prompted the United States to ban such procedures without a permit. Zhang, who works at New Hope Fertility Center in New York City, performed the technique at the company\u2019s clinic in Mexico. Critics saw this as an attempt to evade regulation, and complained that he had announced the work at a conference rather than in a publication. But Zhang brushes aside those objections. \u201cThe most important is to have a live-birth baby, not to tell the whole world,\u201d he says. Zhang has a habit of pushing scientific and ethical boundaries. In the 1990s, he worked with reproductive endocrinologist Jamie Grifo at the New York University Langone Medical Center to develop a version of the technique that Zhang used this year. The approach was designed to help older women to become pregnant by replacing their ageing mitochondria with those from younger eggs. No successful pregnancies resulted. When US regulators began restricting this technique in 2001, Zhang and his collaborators in China took over the work. In 2003, Zhang\u2019s team created and implanted multiple embryos into a woman. After all the fetuses were miscarried, China banned the technique as well. Grifo and some others applaud Zhang\u2019s latest work. \u201cI think it\u2019s a great thing it was finally done,\u201d says Grifo. But  others have criticized the New Hope team . \u201cA lot of things they did were completely unsafe,\u201d such as infusing the donor\u2019s egg with a drug that could cause chromosomal abnormalities, says Shoukhrat Mitalipov, a stem-cell scientist at Oregon Health & Science University in Portland. Zhang is undeterred. He says that plenty of other families at risk of mitochondrial disease have expressed interest in his procedure, and he hopes to perform it in other countries. \u201cFive to ten years from today, people will look at it and say, \u2018Why were we all so stupid, why were we against it?\u2019\u201d he says. \u201cI think you have to show the benefit to mankind.\u201d \n               KEVIN ESVELT: CRISPR cautionary \n             \n               A budding biologist put gene-drive ethics before experiments. \n               By Heidi Ledford \n             It was a trip to the Galapagos Islands at the age of ten that first whetted Kevin Esvelt\u2019s appetite for tinkering with evolution. As he stood marvelling at the iguanas, birds and sheer diversity of the place that had inspired Charles Darwin, Esvelt vowed to understand evolution \u2014 and improve on it. \u201cI wanted to learn more about how these creatures came to be,\u201d he says. \u201cAnd, frankly, I wanted to make more of my own.\u201d Today, Esvelt is still a precocious biologist. Less than a year after launching his lab at the Massachusetts Institute of Technology Media Lab in Cambridge, he has already made a name for himself as one of the pioneers of a controversial technique called a gene drive. His method harnesses  CRISPR\u2013Cas9 gene editing  to circumvent evolution, forcing a gene to spread rapidly through a population. It could be used to  wipe out mosquito-borne diseases such as malaria  or eradicate invasive species. But it could also  set off unintended ecological chain reactions , or be used to create a biological weapon. The idea of CRISPR gene drives hit Esvelt when he was tinkering with the Cas9 enzyme in 2013. \u201cI had one day of absolute, ecstatic glee: this is what\u2019s going to let us get rid of malaria,\u201d says Esvelt. \u201cAnd then I thought, \u2018Wait a minute.\u2019\u201d Following that thought, Esvelt has worked to ensure that ethics comes before experiments. He first sounded the alarm in 2014, calling for public discussion about gene drives even before he had demonstrated that a CRISPR\u2013Cas9 gene drive could work ( K. A. Oye  et\u00a0al .  Science   345,  626\u2013628 (2014) ;  K. M. Esvelt  et al .  eLife   3,  e03401; 2014 ). Since then, he and his colleagues have shown  how gene drives might be made safer , and how they could be reversed ( J. E. DiCarlo  et al .  Nature Biotechnol.   33,  1250\u20131255; 2015 ). This year, his advocacy has begun to bear fruit. Researchers and policymakers worldwide have been discussing the technology, and a report from the US National Academies of Sciences, Engineering, and Medicine urged that  gene-drive research proceed, but cautiously . Omar Akbari, who studies gene drives at the University of California, Riverside, believes Esvelt\u2019s outreach has focused public attention \u2014 and attracted funding \u2014 for a nascent technology at just the right time. \u201cI attribute that to Kevin,\u201d says Akbari. \u201cIt\u2019s difficult for a scientist to do what he\u2019s done.\u201d \n               GUILLEM ANGLADA-ESCUD\u00c9: Planet hunter \n             \n               An astronomer detected the nearest known planet outside the Solar System. \n               By Alexandra Witze \n             Guillem Anglada-Escud\u00e9 wasn\u2019t surprised early this year when evidence of an alien world rippled across his computer screen. He had been almost certain that an Earth-sized planet orbited Proxima Centauri, the star nearest the Sun at just 1.3 parsecs (4.2 light years) away. To Anglada, an astronomer at Queen Mary University of London, the discovery came as more of a relief than a shock. He and his colleagues had been working feverishly to stake their claim in the competitive world of planet hunting, and the Proxima find confirmed that they were on the right path. \u201cWe made it,\u201d he says. To the rest of the world, the  discovery of the closest known exoplanet  to Earth stoked the public imagination. It raised questions about whether life might exist in our cosmic backyard, and whether astronomers might be able to detect it. These are the kinds of question that got Anglada into planet hunting in the first place. A science-fiction fan while growing up near Barcelona, Spain, he got his astronomical start doing data simulations for Gaia, a European Space Agency mission to map 1 billion stars. Later, he turned his data-crunching skills to exoplanets. He developed a method for extracting faint planetary signals from data gathered by the world\u2019s premier ground-based planet-hunting instrument, the High Accuracy Radial velocity Planet Searcher (HARPS) at the European Southern Observatory in La Silla, Chile. \u201cGuillem has a natural talent of seeing the big picture where others see details,\u201d says Mikko Tuomi, an astronomer at the University of Hertfordshire in Hatfield, UK, and a collaborator of Anglada\u2019s. But Anglada soon ran straight into high academic drama, tussling with other researchers over who deserved credit for discovering a planet bigger than Earth and smaller than Neptune orbiting the star Gliese\u00a0667C. \u201cI could have left the field and done something else,\u201d he says. \u201cBut I took the decision of following it very aggressively.\u201d He dived into HARPS data, publishing paper after paper on the planetary signals he discovered amid the background noise in the data. And then, as if to push back on all the secrecy and competition, Anglada launched a very public hunt for a planet orbiting Proxima. He put together a team and got observing time on HARPS, as well as other telescopes that could double-check whether any promising evidence that they found was caused by stellar activity, which can mimic the signs of a planet (a problem that plagues many exoplanet claims). The researchers put nearly all their details on an  outreach website  and social-media accounts. Being so transparent \u201cdidn\u2019t seem dangerous at all\u201d, Anglada says. \u201cWe had a feeling nobody else would do this.\u201d Within days, they confirmed that the planet was there; within weeks, they submitted a manuscript detailing their discovery. The planet, called Proxima b, is at least 1.3 times the mass of Earth and orbits Proxima every 11.2 days. Although it is close to its star, the world is within the \u2018habitable zone\u2019, where liquid water could exist on its surface. That makes it not only the closest known exoplanet of the 3,500-plus confirmed so far, but also a place where otherworldly life could thrive \u2014 a double bonus for researchers and science-fiction fans alike. Just before the paper was published in  Nature  in August ( G. Anglada-Escud\u00e9  et al .  Nature   536,  437\u2013440; 2016 ), Anglada e-mailed British sci-fi writer Stephen Baxter, author of the novel  Proxima  (Gollancz, 2013). They corresponded about what life might be like on a world with one hemisphere permanently facing a flaring star, as happens at Proxima. People could eventually get a close-up look at Proxima b. The Breakthrough Starshot initiative aims to send fleets of tiny laser-propelled spacecraft to a nearby star, and it may target Proxima as its closest and best option. Anglada\u2019s next step is to see whether Proxima b transits, or passes across the face of its star as seen from Earth. The chances are low, but if it does, then much more science can be gleaned when Proxima\u2019s light passes through the planet\u2019s atmosphere, if it has one. And if the transit does not happen? Then Anglada may be off, to tease out some other signal of another world. \n               ELENA LONG: Diversity trailblazer \n             \n               A transgender physicist paved the way for greater acceptance of minority groups. \n               By Elizabeth Gibney \n             Physicists can be open to seeing the world in new ways, but they need to see the data first. This posed a problem for Elena Long, a nuclear physicist who has fought for her field to be more inclusive of people from sexual and gender minorities. \u201cWe didn\u2019t have any data, because people considered it too offensive to ask if we exist. It was a catch-22.\u201d Long was one of the architects of a  first-of-its-kind survey  run by the American Physical Society (APS), charting the experiences of physicists who are lesbian, gay, bisexual, transgender or from another sexual or gender minority (LGBT). The findings, presented to a packed room at the APS March meeting this year, were stark. Of the 324 scientists who responded, more than one in five reported having been excluded, intimidated or harassed at work in the previous year. Transgender physicists reported the highest incidence of discrimination. Long, who is transgender herself, was unsurprised. In 2009, she began work for her PhD at the Thomas Jefferson National Accelerator Facility in Newport News, Virginia, which lacked trans-inclusive employment protections and health-care benefits. She felt isolated without LGBT support networks. \u201cI loved the work I was doing, and I loved the research. But it was rough,\u201d she says. So she founded the  LGBT+ Physicists  support group and began pushing for greater recognition at the APS, which eventually created a committee to collect data on LGBT discrimination. Many physicists, she says, could not even understand the need for such a study. Thanks to Long and her colleagues, physics is emerging as exemplary in its approach to these issues, says Samuel Brinton, a board member of the society  Out in Science, Technology, Engineering and Mathematics . \u201cWe are literally using their work to start changes for the better in multiple fields,\u201d he says. The APS accepted the recommendations made in the March report. And in August, a major APS division  voted to move its 2018 meeting out of Charlotte, North Carolina , in response to a state law that forces people to use public toilets that match the gender they were assigned at birth. Long has meanwhile won two young-scientist awards offered by her lab and become a co-leader on two new accelerator experiments. \u201cI\u2019ve known a lot of postdocs who\u2019ve done voluntary work, and usually it compromises their science,\u201d says Karl Slifer, Long\u2019s postdoctoral supervisor at the University of New Hampshire in Durham. \u201cI\u2019ve never seen that in Elena.\u201d (Long attributes her strict time management to a computer program she designed that charts every hour of her day.) Now Long is helping to set up an APS membership group focusing on diversity and inclusion, which she hopes will make it easier for scientists in other minority groups to flourish. \u201cI\u2019m sure there are other people facing problems in the field I never thought about,\u201d she says. \u201cI don\u2019t want them to wait seven years to get to a place where they can have a voice.\u201d \n               Ones to watch in 2017 \n             \n               Cori Bargmann, Science president, Chan Zuckerberg Initiative \n             Bargmann is steering the research operations of a US$3-billion effort by the philanthropic organization to cure, prevent or manage all disease by 2100. \n               Robert Feidenhans\u2019l, Chairman, European XFEL \n             As the new head of the world\u2019s most powerful X-ray free-electron laser, Feidenhans\u2019l will guide the \u20ac1.2-billion (US$1.3-billion) facility during its ramp up to becoming fully operational by mid-year. \n               Jef Boeke, Co-leader, Human Genome Project\u2013Write \n             Boeke is a director of an ambitious effort that is seeking to synthesize the human genome. He and others are already close to making a yeast genome. \n               Wu Weiren, Chief Designer, China Lunar Programme \n             China\u2019s plans call for launching the Chang\u2019e-5 mission in the latter half of 2017 to collect the first lunar rock samples to be brought back to Earth since the 1970s. \n               Marcia McNutt, President, National Academy of Sciences \n             With her experience in President Barack Obama\u2019s cabinet, McNutt will have a central role in representing US science during Donald Trump\u2019s presidency. \n                 Tweet \n                 Follow @NatureNews \n               \n                     2016 in news: The science events that shaped the year 2016-Dec-16 \n                   \n                     2016 in pictures: The best science images of the year 2016-Dec-16 \n                   \n                     Nations agree to ban refrigerants that worsen climate change 2016-Oct-15 \n                   \n                     \u2018Three-parent baby\u2019 claim raises hopes \u2014 and ethical concerns 2016-Sep-28 \n                   \n                     Earth-sized planet around nearby star is astronomy dream come true 2016-Aug-24 \n                   \n                     Brazil asks whether Zika acts alone to cause birth defects 2016-Jul-25 \n                   \n                     Gene editing can drive science to openness 2016-Jun-08 \n                   \n                     Paper piracy sparks online debate 2016-May-02 \n                   \n                     Coral crisis: Great Barrier Reef bleaching is \u201cthe worst we\u2019ve ever seen\u201d 2016-Apr-13 \n                   \n                     Gravitational waves: How LIGO forged the path to victory 2016-Feb-16 \n                   \n                     Google AI algorithm masters ancient game of Go 2016-Jan-27 \n                   \n                     Nature  special: The year in review \n                   \n                     LIGO \n                   \n                     ARC Centre of Excellence for Coral Reef Studies \n                   \n                     \n                         Gene Drives on the Horizon  \n                       \n                   \n                     LGBT+ Physicists \n                   Reprints and Permissions"},
{"file_id": "537471a", "url": "https://www.nature.com/articles/537471a", "year": 2016, "authors": [{"name": "Corie Lok"}], "parsed_as_year": "2006_or_before", "body": "Wages for top scientists are shooting skywards while others are being left behind. For a portrait of income inequality in science, look no further than the labs of the University of California. Twenty-nine medical researchers there earned more than US$1 million in 2015 and at least ten non-clinical researchers took home more than $400,000 each. Meanwhile, thousands of postdocs at those universities received less than $50,000. Young professors did better, but many still collected less than one-quarter of the earnings of top researchers. The University of California is far from unique. At universities across several countries, the salary gap between elite scientists and those toiling in the trenches has been expanding over the past few decades, according to labour economists. The inequality mirrors the trend across the rest of society, where stagnating middle-class wages and soaring incomes for the wealthy have created a widening gap between top and bottom earners. The super-rich '1%' is a hot political issue in many countries. Researchers who study the science workforce say that there is a  dearth of data on the salaries of scientists , which makes it difficult to know the full extent and causes of income inequality. But the gulf in wages has reached a point at which it could be driving talented young people away from careers in academic science, says Richard Freeman, an economist at Harvard University in Cambridge, Massachusetts. The  results of Nature's 2016 salary survey  support that concern. More than half of 3,600 respondents say that they have sacrificed a good salary by going into science, and nearly 20% would not recommend that students pursue a career in scientific research. The problem shows no sign of disappearing. \u201cWith more competition and fewer rewards, it makes income inequality much more intense,\u201d says Freeman. \n               Inequality index \n             When labour economists measure disparities in salaries, one of the metrics they use is the Gini coefficient, named after an Italian statistician who developed the measure in the early twentieth century. A coefficient of 0 means that everyone earns the same. A value of 1 indicates maximum inequality \u2014 everyone earns nothing except for one person. In her 2012 book  How Economics Shapes Science  (Harvard University Press), economist Paula Stephan at Georgia State University in Atlanta calculated the Gini coefficient for science and engineering faculty members at US doctorate-granting institutions, using salary data from the US National Science Foundation's  Survey of Doctorate Recipients . She found that the Gini coefficient more than doubled between 1973 and 2006 in most fields and faculty ranks, with the biggest increases in the life sciences (see 'Money matters'). By contrast, it grew by just 35% for full-time male earners in the United States and by only 18% for US households. The faster increase among researchers probably stems from the fact that wages in science have historically been more equal than for other sectors of the economy and are now spreading out, says Stephan. In 2006, the Gini coefficient for science professors ranged from 0.14 to 0.25, depending on discipline and rank; for households in the United States, it was 0.47. To find out how wage gaps in science have changed in recent years,  Nature  worked with Freeman and Zhuge Liqun, a research assistant at the National Bureau of Economic Research in Cambridge, Massachusetts, to calculate Gini coefficients from the National Science Foundation's survey data. The analysis suggests that inequality decreased slightly in 2008. Since then, however, the gaps seem to have remained relatively large and stable, with academia showing wider spreads than industry and government. One factor that could have fuelled part of the rise over recent decades in the US biomedical sciences is the doubling of the National Institutes of Health's budget during the late 1990s and early 2000s, says Stephan. As new research buildings sprang up across the country, institutions needed to fill them with productive scientists who could bring in grants. This created competition among institutions for a small pool of top-ranked, grant-winning scientists, and that probably drove up their salaries, she suggests. \u201cScience is risky and expensive,\u201d says Donna Ginther, a labour economist at the University of Kansas in Lawrence. \u201cOne way for universities to minimize risk is to pick someone who is a demonstrated winner.\u201d In the United Kingdom, too, the salaries of top-earning professors have been pulling away from the pack since the late 1990s, says Ben Martin, a science-policy researcher at the University of Sussex near Brighton, UK. Back then, the highest salaries were roughly twice that of the nationally agreed minimum value set by unions and employers, he estimates. Now that factor has ballooned to more than seven, according to 2013 data from the UK Higher Education Statistical Agency. Growing competition for good researchers from around the world has helped to boost faculty salaries in the United Kingdom, says Martin. \u201cIt's become a much more global market in which universities operate.\u201d The regular evaluation of research quality at UK universities, the Research Excellence Framework (REF), could also be bumping up salaries, adds Martin. The assessment, done by UK funding agencies roughly every five years, determines the amount of research money that universities receive from the government. Universities are evaluated in part on the quality of their researchers' publications up to a certain 'census' date. Faculty members who were hired just months or even days before this date are included in the assessment, and so are their publications from the past six years. This gives universities an incentive to recruit researchers with strong publication records as the deadline for the assessment approaches, as a way to  boost their REF scores , says Martin.  With more competition and fewer rewards, it makes income inequality much more intense.  An analysis of UK full-professor salaries posted online in July suggests that some universities \u2014 particularly lower-ranked ones that want to improve their REF performance \u2014 are using high salaries to recruit researchers with high-quality papers to boost their scores (see  go.nature.com/2cwnyjj ). Gianni De Fraja and his colleagues at the University of Nottingham, UK, showed that departments with higher average pay for full professors in 2013 got better REF funding scores in the 2014 assessment. \u201cOur data suggest that universities are buying CVs in the run-up to the REF,\u201d De Fraja says. The competition to lure \u2014 or keep \u2014 star scientists is raising wages in other countries, too. In China, various government-sponsored initiatives to boost research excellence have been using high salaries to recruit leading researchers, according to Qi Wang at Boston College in Massachusetts, who studies research universities. In Germany, faculty salaries are more regulated than those in the United States or United Kingdom, but some of those restrictions have been loosened in the past decade or so, says Ulrich Teichler, a higher-education researcher at the University of Kassel in Germany. Many professors can now use good performance to negotiate larger pay rises than before, he says. In the United States, wage gaps are easy to spot in databases for some public universities, such as the University of California. But top professors can earn considerably more at the elite private universities, which are not required to report their wage data to the public. In response to a question from  Nature  about how the salaries of its highest-paid professors compare with those elsewhere, the University of California said that \u201cfaculty at the University of California are, by and large, paid less than their peers at comparable institutions\u201d. The state has faced major budget pressures over the past decade. \n               Bottom bracket \n             At the other end of the salary spectrum, there is little pressure to boost pay. With grants getting harder to win, labs rely on a large, low-cost workforce to maximize research output, says Gary McDowell, a former postdoc and now the executive director of the Future of Research, a non-profit organization in San Francisco, California, that  advocates for young scientists . This labour environment benefits from the willingness of postdocs to sacrifice income for a chance at an academic research career, he says. Even those lucky enough to land offers for tenure-track junior faculty positions find that starting salaries are not very negotiable, say researchers. Instead, they focus on trying to get bigger start-up packages for their new labs. The culture among researchers helps to keep salaries low because scientists tend to value discovery much more than financial gain, says McDowell. \u201cPeople don't go into this for money,\u201d he says. \u201cWe want a rewarding job that uses our passion for science. But that gets taken advantage of.\u201d The growing ranks of non-tenure-track faculty members \u2014 such as adjunct professors hired to teach students \u2014 may also be contributing to low wages, says Stephan. They are paid much less than assistant professors or even postdocs. McDowell says he is seeing a growth in these kinds of positions in science, which are often taken by people who want to hang on in academia in the face of dwindling career prospects there. Income inequality can be good and bad, says Julia Lane, an economist at New York University who studies the research workforce. High salaries at the top can attract productive workers, but low pay at the bottom can signal that there may not be a good future in this career. More than 60% of respondents to  Nature 's salary survey said that their job prospects were worse than those of previous generations, with that pessimism running at around 70% in North America and Europe. And just over half of respondents around the world reported that they had received a salary increase in the past year. Income inequality seems to have little effect on overall research performance, says De Fraja. In his analysis of UK faculty salaries, he found no correlation between the spread of professors' salaries in departments and their REF performance. Egalitarian and elitist departments seem to do just as well. But too much inequality could cut many academic careers short, says Ginther. If big rewards become concentrated in the hands of a smaller number of people in a highly competitive area, then many others who could still have been productive scientists end up losing a disproportionate amount in terms of earnings and career prospects. That could keep promising people from further pursuing a research career, says Ginther. \u201cYou're discouraging a lot of potential scientific discoveries.\u201d Freeman says that how much more people should be paid for better performance is an open question. How steep should that incentive curve be in science? \u201cIt's by no means clear that we have the right steepness,\u201d he says. \u201cThe danger today is that it's steeper than it should be, too steep to be efficient.\u201d One key problem is that  not enough is known about where scientists with PhDs end up  in their careers and what their salaries are, says Lane. Without this kind of information, young scientists often find themselves making important career decisions on the basis of erroneous assumptions about pay, says Keith Micoli, director of the New York University School of Medicine Postdoctoral Affairs. He says that some postdocs are surprised and disappointed when they find that a starting salary for a long-sought job may not be as high as they had hoped. So Micoli advises postdocs to do the research themselves: find people who are doing the jobs they are interested in and ask them what they earn. That's what Harvard postdoc Rebeccah Lijek did when she entered the job market for a faculty position last year. She says that she benefited from her network of contacts, getting details about what she should expect as a fair salary and what size of start-up package she would need to set up her immunology lab. Someone without the kind of contacts and mentorship she had could be at a disadvantage when trying to negotiate compensation, says Lijek. \u201cIt's really about who has the privilege to have this information.\u201d \n                 Tweet \n                 Facebook \n                 LinkedIn \n                 Weibo \n                 Wechat \n               \n                     Is science only for the rich? 2016-Sep-21 \n                   \n                     Income inequality is cyclical 2016-Sep-21 \n                   \n                     Socio-economic inequality in science is on the rise 2016-Sep-21 \n                   \n                     End class wars 2016-Sep-21 \n                   \n                     Salaries: Reality check 2016-Sep-21 \n                   \n                     How to build a better PhD 2015-Dec-02 \n                   \n                     The future of the postdoc 2015-Apr-07 \n                   \n                     Life outside the lab: The ones who got away 2014-Sep-03 \n                   \n                     Nature  special: Inequality in science \n                   \n                     Nature  special: The future of the PhD \n                   \n                     Nature  special: Diversity in science \n                   \n                     University of California Employee Pay \n                   \n                     Future of Research \n                   \n                     FASEB: Education and Employment of Biological Medical Scientists 2016 \n                   Reprints and Permissions"},
{"file_id": "540184a", "url": "https://www.nature.com/articles/540184a", "year": 2016, "authors": [{"name": "Ewen Callaway"}], "parsed_as_year": "2006_or_before", "body": "Investigation of a 150-year-old burial site is helping to unlock the mysteries of one of humanity\u2019s darkest chapters. In 1849, Anglican bishop Robert Gray described a slave ship being unloaded on the island of St Helena. \u201cI never beheld a more piteous sight,\u201d he observed of the people on board. Some were dead; many more were close to it. \u201cThey had a worn look and wasted appearance, and were moved into the boats like bales of goods, apparently without any will of their own.\u201d These men and women were refugees of the British Navy's campaign against the slave trade. The United Kingdom had outlawed the trade in 1807, and anti-slaver patrols were intercepting boats along the Middle Passage \u2014 the trade route from Africa to the Americas \u2014 even venturing into the harbour of Rio de Janeiro. Situated in the middle of the Atlantic Ocean, St Helena became a favoured drop-off point for people freed from the ships. From 1840 to the late 1860s, scores of ships carrying some 27,000 slaves were captured and brought to the island. The slaves who survived were granted freedom and most were eventually relocated, but for nearly 10,000, many of them children, a rocky valley on St Helena became their final resting place. A burial ground on a remote island sheds light on the horrors of nineteenth-century slavery. These people's lives had been all but forgotten until a decade ago, when a construction project unearthed a couple of skeletons. An archaeological excavation found hundreds more. Now, a team of researchers is studying the remains and sequencing DNA that spent more than a century underground to find out as much as it can about the freed slaves of St Helena: from their birthplaces in Africa to their cultural backgrounds and how they died. The site's unique place in history could provide important missing information about the experiences and identities of those who were kidnapped but never reached the Americas, says Fatimah Jackson, a biological anthropologist at Howard University in Washington DC, who is sequencing ancient DNA from other slave burial sites. The work has special significance for Jackson, who traces her own heritage to Africa. \u201cWe are descendants of people who came through the Middle Passage, American slavery and institutionalized segregation, and in the process lost a lot of details of our identity and our heritage,\u201d she says. \u201cAncient DNA gives us a glimmer into a past we've longed for, but not had direct access to.\u201d \n               Valley of lost souls \n             Rupert's Valley is a seaside gorge on the remote isle where Napoleon Bonaparte spent his final days in exile. Driving through it, you pass warehouses, a small diesel power station and a fish-processing plant, as well as a few houses and a lone church. The valley is also dotted with crumbling reminders of its past, such as a metre-wide defensive wall built by the British East India Company and a late-nineteenth-century desalination plant that supported a prison camp during the Second Boer War. But aside from a nondescript stone building, there is little visible reminder of the thousands of former slaves who once resided here. In November 2006, geologist David Shilston came to St Helena by boat from Cape Town, South Africa \u2014 a five-day journey and one of the only ways to reach the island. His employer, the engineering firm Atkins, had been hired to plan an access road for the construction of the island's first airport. Shilston's job was to explore the geology beneath it. The digging had just begun when a worker called him over, pointing out a broken human leg bone sticking out of the ground. \u201cI thought it might have been a child. It looked pretty small,\u201d Shilston says. Historical records illustrate the dire condition in which many newly liberated slaves arrived. \u201cTheir arms and legs were worn down to about the size of a walking-stick,\u201d wrote one witness of an 1861 slave-ship landing. \u201cMany died as they passed from the ship to the boat \u2026 there was no time to separate the dead from the living.\u201d Diseases such as smallpox and dysentery killed scores more soon after they landed, and there are oblique references to suicides. Between 1840 and 1849, one-third of the freed slaves, nearly 5,000 people, died. Many of those who survived the camp's miserable conditions were sent to Jamaica, Trinidad and other British colonies, where they served as indentured workers on sugar plantations. After Shilston found the first remains of St Helena's Liberated African Graveyard, as the site is now known, he learned that the burials hadn't been so much forgotten as ignored by many of St Helena's residents. His discovery made the front page of a local newspaper. \u201cThere is no doubt that numerous remains of human bodies will be found,\u201d the story read. \u201cMany believe that the souls of the dead slaves still are haunting Rupert's.\u201d The remains were stored in small caskets at a local church, and later reburied in a cemetery in the island's capital, Jamestown. The UK government's Department for International Development was footing the \u00a3250-million (US$310-million) bill for the airport (St Helena is a British Overseas Territory). The department wanted the site professionally examined, so in 2007 it sent over Andrew Pearson, a commercial archaeologist based in Cardiff, UK. He identified two distinct burial locations, smack in the middle of the path for the planned road. \u201cI was the bearer of the bad news that they had a large and internationally unique burial right where they wanted to build,\u201d he says. Given instructions to excavate only the skeletons beneath the planned road, Pearson and his colleague Ben Jeffs returned to St Helena the following year with a team of 5 people; it quickly swelled to 15 when the size of the task became clear. The first graves that the group excavated were densely packed together, and some contained as many as six skeletons. Pearson's team spent 4 months exhuming 325 individuals from a 1,800-square-metre plot and boxing them up for later examination in Jamestown. More than half looked as if they were under 18, and the largest demographic group was children aged 12 and younger. Pearson estimates that a total of 8,000 people were buried across the entire valley. North and South America and the Caribbean islands are full of burial sites related to the transatlantic slave trade, but these tend to hold multiple generations of slaves: some born in Africa and others in the Americas. Rupert's Valley, by contrast, contains physical evidence of the individuals who were sold into slavery and then freed. It therefore has the potential to indicate not only where slaves came from and their condition aboard the ships, but also how the slave trade might have reshaped their identities. \u201cIt's literally people who are kidnapped in Africa, weeks before,\u201d says Pearson. \u201cSo you actually have a snapshot of the Middle Passage, which in other ways is completely intangible.\u201d \n               Hidden identity \n             Hannes Schroeder, an ancient-DNA researcher at the University of Copenhagen, first encountered archaeological remnants of the slave trade during an excavation in the Virgin Islands as an undergraduate. The site he was working on dated to before the arrival of Europeans in the Americas, but it happened to abut an overgrown eighteenth-century plantation. He and a friend spent their weekends investigating the plantation, and identified the remnants of the great house, slave quarters and other landmarks. \u201cThere was this site, forgotten, but it was important in terms of the history of the island,\u201d he says. \u201cThat had a very profound effect on me.\u201d In 2009, as he was starting a postdoc in Copenhagen, Schroeder started to think about determining where slaves had actually come from by looking at their genomes. The history of the roughly 12 million Africans kidnapped as part of the slave trade is based largely on maritime records \u2014 including a database of nearly 36,000 voyages. These are largely transactional and generally mention only a slave's port of departure from Africa, not their ethnicity or geographic origins, which could lie hundreds of kilometres from where they were sold. But DNA collected from remains and matched to a genome database of modern Africans could, Schroeder realized, link victims of the slave trade to their homelands. It was a challenging proposition for two reasons. First, although scientists have recovered readable bits of  DNA dating back hundreds of thousands of years , most of the success has come from sites in northern Europe, Siberia or North America. No one has had much luck sequencing ancient DNA from tropical locations, because heat and humidity hasten the breakdown of biomolecules. Second, the genetics community has historically failed to fully represent the genomic diversity of modern Africa in databases. Even if high-quality sequences could be obtained from slave burial sites, it would be difficult to link them to contemporary populations with much precision. But Schroeder persevered. In 2011, he and his colleagues  won a \u20ac4-million (US$5.8-million) grant from the European Commission  to apply scientific approaches to the study of the transatlantic slave trade, and he and others are now sequencing remains from half a dozen slave burial sites, including those on St Helena. The first bones that the team analysed were from three individuals discovered by construction workers in the Dutch territory of Saint Martin in the Caribbean. Carbon dating suggested that the two men and one woman died between 1660 and 1688, when records showed at least one slave ship arriving at the island. All three had their incisors filed or chipped to points, hinting that they were born in Africa, where tooth modification was a common rite of passage among many ethnic groups. Using molecular techniques that 'fish out' bona fide ancient human DNA from the microbial genetic material that invades bones after death, Schroeder's team managed to recover partial genome sequences for each person. Comparisons with DNA from 11 contemporary African groups suggested that one of the St Martin individuals was most closely related to members of a Bantu-speaking ethnic group in northern Cameroon called the Bamoun, whereas the other two had DNA in common with ethnic groups that now live in Nigeria and Ghana, including the Igbo and Brong. Schroeder is hesitant to conclude that they belonged to these groups, but the findings, published last year ( H. Schroeder  et al .  Proc. Natl Acad. Sci. USA   112,  3669\u20133673; 2015 ), gave him hope that ancient DNA could offer genuine insights into the genetic ancestry of slaves, as well as how the Middle Passage reshaped their identities. \u201cThese three individuals \u2014 despite the fact that they were found buried together and may have arrived on the same vessel \u2014 had different ethnic backgrounds\u201d and probably spoke distinct languages, he says. \u201cThat makes you think: how did they communicate with each other and what does this mean for the formation of new identities in the Americas?\u201d Rupert's Valley was probably even more polyglot. Shipping records suggest that slave ships that landed in St Helena had embarked from ports across Central and West Africa, including present-day Angola and the Congo region (see 'The route to Rupert's Valley'). There are indications that some individuals came from much further away, including Mozambique and even Madagascar. One 1840s observer noted a group of 40 \u201cnatives of the interior\u201d, who had travelled for several months to reach the Atlantic coast. \u201cThey are of so many different tribes and districts that it would be curious, if one knew the languages, to trace them out,\u201d said another visitor to Rupert's Valley. The former slaves' DNA backs up the observation. Schroeder and Marcela Sandoval Velasco, a palaeogeneticist at the University of Copenhagen, collected DNA from the teeth of 63 individuals and sequenced partial genomes from 20 of the best-preserved samples. Comparisons with contemporary African populations suggested that the liberated slaves came from diverse African backgrounds. A few individuals shared ancestry with contemporary West and Central African ethnic groups such as the Bamoun and Kongo, but for most, none of the African groups that the team compared them with was an especially close match. Schroeder attributes that to a lack of genomic data from places such as Angola and Mozambique. \n               Alternative evidence \n             Schroeder is frustrated that his team cannot yet pinpoint where the liberated slaves of St Helena were from, nor link them closely to any contemporary population. But he is confident that, eventually, their genomes will help to narrow down that search. Several biomedical projects are sequencing the genomes of lots of people from across sub-Saharan Africa. \u201cIt won't take long,\u201d he says. Until then, he and his colleagues are following other leads to trace the origins of the 325 liberated slaves and \u2014 equally important, they say \u2014 to fill in details about their lives. Levels of certain chemical isotopes, including strontium, in teeth vary depending on local geochemistry, offering clues to where individuals lived as children, when their adult teeth were formed. Judy Watson, a PhD student at the University of Bristol, UK, who is collaborating with Schroeder's group, has found that groups of individuals in Rupert's Valley had similar isotope levels, as if they had grown up in the same area.\u201cThat could be evidence for people being picked up, not just as individuals, but as groups of people from the same region,\u201d says Kate Robson-Brown, a biological anthropologist who leads the Bristol group. And if genome analysis can link the liberated slaves to broad modern populations, isotope levels \u2014 which can vary enormously between nearby areas \u2014 could narrow down their geographical origins. With Pearson's help, Watson has even drawn up a list of potential slave ships and ports of departure, based on coins and other archaeological evidence that suggest the individuals recovered from Rupert's Valley died in the 1840s. In the quest to learn as much as possible about the lives of the former slaves, the team is also looking at other lines of evidence. \u201cIsotopes and genetics are not going to tell us about the social identity of that person,\u201d says Robson-Brown. For instance, a graduate student in her lab, Erna Johannesdottir, is examining the variety of cultural tooth modifications seen in the remains. Specific modifications could hint at the bearers' ethnic affiliations, especially when compared with the genome data. Some of the modifications seem to have been performed not long before the individuals died, Robson-Brown notes, possibly even aboard the slave ship. \u201cIt's not impossible to imagine that these young people are developing kinship groupings among themselves during this process of capture, enslavement and shipping and all these awful things. That, to me, is such a strong message of the human spirit and resilience.\u201d Scholars of the transatlantic slave trade are eager to weave these insights into their research, says David Richardson, an economic historian at the University of Hull, UK, who is working with the team. \u201cHistorians are fascinated by issues of identity and also by issues of cultural adaptation and cultural legacies. This is all part of the transatlantic slave trade.\u201d \n               Ancestral links \n             DNA research into the slave trade may also reshape how people think about the trade's legacy. \u201cIt's a very personal thing for us,\u201d says Jackson \u201cIt's not just ancient DNA. It's ancient DNA of potential ancestors.\u201d Even if it never becomes possible to link former slaves to their living descendants through DNA, the mere presence of genetic material holds power, Jackson says. \u201cAncient DNA research allows us to connect with history, both the good and bad, and reconcile it with our own identity.\u201d Sociologist Alondra Nelson at Columbia University in New York City says that  this kind of genetic root-seeking  started around 1991, with the discovery of a slave burial site in lower Manhattan thought to hold between 10,000 and 20,000 individuals. Mitochondrial DNA analysis (some of it by Jackson) could not say much about the individuals recovered from the site, except that they carried maternal markers common in Africa. But even years before this tentative genetic link was made, people claiming that they had connections to the buried individuals had come forward. A group called Descendants of the African Burial Ground criticized politicians and researchers for mishandling the remains. Pearson worried that the excavations in Rupert's Valley would draw a similar response from the island's residents (known locally as Saints), many of whom are descended from earlier slave populations, the Chinese indentured workers who replaced them, and British settlers. \u201cI thought we'd have terrible trouble \u2014 descendants coming up asking, 'How are you doing this to my people?',\u201d he says. But no one ever came forward. \u201cI think people probably chose not to emphasize their black heritage, and it's just been lost.\u201d Although St Helena's airport has been completed, its wind-blasted runway has yet to receive regular commercial flights \u2014 and it may never do so. The fate of the remains of the liberated slaves is similarly in question. In 2015, St Helena's government conducted a survey of residents, asking what to do with the skeletons. Options ranged from maintaining them as a research resource to repatriating them to a yet-to-be-determined location in Africa (a proposition that, even if their origins could be pinpointed, would be filled with political and logistical hurdles, Pearson notes). The vast majority wanted them put back to rest in Rupert's Valley \u2014 and many indicated that it should be done as soon as possible. As much as researchers could still learn from the bones, Pearson agrees with the will of the Saints, and hopes to return to St Helena to help with reburial. \u201cThese people have had such an awful end-of-life experience. What final indignity at the hands of white people to be stuck in a lab for perpetuity.\u201d In 1864, Rupert's Valley received its last six liberated slaves from a ship so damaged that the Royal Navy left it to sink. The refugee camp was disbanded in 1867. Historians are ambivalent about the campaign to intercept and prosecute slave traders. It probably hastened the end of the transatlantic slave trade in Brazil and Cuba, the last holdouts of a dark chapter in human history. But the people liberated by the British Navy who eventually left St Helena may have considered their fates \u2014 working in foreign lands or fighting other people's wars \u2014 merely an extension of their enslavement. Then there were those who remained. Several hundred liberated Africans eventually integrated into St Helena's population. An 1881 census recorded 77 people whose birthplace was listed as \u201cWest Coast of Africa\u201d. Their lives are not easy to imagine \u2014 ripped from their homelands, saved from enslavement but left on a remote island. Free but exiled. One turn-of-the-century photograph shows five of them: three seated women and two men, all residents of the poorhouse. \u201cThe men, although over seventy, are still able to earn a little, but the women are helpless, and almost blind,\u201d reads a caption from the time. They had arrived on the  Cyclops , a naval vessel that last visited St Helena half a century earlier. It is the final picture ever taken of St Helena's liberated slaves, but DNA may soon offer one more. \n                 Tweet \n                 Follow @NatureNews \n               \n                     Source material 2016-May-25 \n                   \n                     Oldest ancient-human DNA details dawn of Neanderthals 2016-Mar-14 \n                   \n                     Teeth from China reveal early human trek out of Africa 2015-Oct-14 \n                   \n                     Underwater archaeology: Hunt for the ancient mariner 2012-Jan-25 \n                   \n                     Filling in the gaps in the slave trade 2011-Dec-01 \n                   \n                     Blog post: Researchers highlight the impact of slavery on health and disease \n                   Reprints and Permissions"},
{"file_id": "540022a", "url": "https://www.nature.com/articles/540022a", "year": 2016, "authors": [{"name": "Josie Glausiusz"}], "parsed_as_year": "2006_or_before", "body": "Some of the most toxic refuse from modern society ends up in poor communities. Researchers are helping one area in the Middle East clean up its electronic-waste problem. Bidoo Cave in the hills west of Hebron opens with towering arches that lead through subterranean chambers carved perhaps thousands of years ago. Cut into the walls of one cavern are hundreds of small, rectangular niches where ancient residents once raised pigeons for meat, eggs and ritual sacrifice. The cave could be an archaeological treasure, but soot coats the walls and the floor is littered with rubbish, including burnt tyres and wiring. Deep inside the cavern, beside a giant rock, is a lone upside-down computer monitor. Bidoo is used by local children to burn 'e-waste' \u2014 mostly leftover foams and plastics from computers and televisions. Electronics are dismantled in nearby villages as part of a massive recycling industry outside Hebron in the Palestinian territories. The scale of this industry is enormous: roughly half of all the e-waste generated in Israel finds its way to a cluster of four villages in the area. About 80% of households there \u2014 including both adults and children \u2014 are involved either directly or indirectly in processing e-waste to extract copper and other valuable metals. The informal, unregulated trade takes a heavy toll. Hundreds of e-waste burn sites are scattered about the region and have polluted the soil with lead, as well as dioxins and other toxic compounds. \u201cThe landscape is saturated with these contaminants,\u201d says Yaakov Garb, an environmental scientist at Ben-Gurion University of the Negev in Sde Boker, Israel, who has spent the past five years mapping the burn sites and assessing their effects on the health of people who live nearby. \u201cMost houses are within a stone's throw of a current site or former site.\u201d Locals say that the burning sickens entire villages with respiratory illnesses, and water that runs off the contaminated hillsides kills vegetation. Although medical information there is spotty, preliminary studies have found very high lead levels in children. And studies of exposure to similar  e-waste sites in China  have documented a range of health effects, including increases in spontaneous abortions, still births, DNA damage and breathing difficulties 1 . Now, an innovative plan is taking shape to clean up the electronics recycling industry in the Hebron Hills. Garb is working with local leaders, government agencies and nongovernmental organizations to remediate the toxic sites and replace burning with non-polluting recycling methods that still allow residents to earn a living. The Swedish International Development Cooperation Agency (SIDA) plans to provide US$2.7 million to support the project, which is awaiting approval by the Palestinian Authority. And in a rare example of cooperation, the Israeli and Palestinian governments are nearing agreements to put the plan into action, says Garb. \u201cWhat he's been able to pull together is nothing short of miraculous; getting both sides to agree like that at all levels, it's remarkable,\u201d says Richard Fuller, chief executive of the non-governmental agency Pure Earth in New York City, which will be overseeing the project. The Palestinian clean-up scheme, he says, could serve as a model project for similar toxic e-waste sites in poor communities around the world, where millions of tonnes of electrical equipment gets dumped each year. \n               Border crossings \n             Garb has a long history of working across borders \u2014 both international and academic. Born in Johannesburg in 1960, he came to Israel as a 13 year old with his parents, who saw no future in apartheid-era South Africa. He has studied irrigation in sub-Saharan Africa, former toxic waste sites in the Czech Republic and deforestation in Guatemala. E-waste caught his attention in 2008, when he noticed something odd while conducting a freight survey in the southern West Bank. Each morning, 70 to 80 trucks left the Hebron area and passed into Israel. They returned to the West Bank each night, laden with washing machines, refrigerators, toaster ovens, LCD screens, computers and furniture. After publishing his traffic survey, Garb set the data aside for a few years, until he did a study analysing the water sources in hundreds of West Bank towns and villages. Residents of the town of Beit Awwa told a student working with Garb: \u201cWe used to collect the rainwater, but we don't because of the 'black rain'.\u201d Laundry, hung out to dry, came back soot-coloured. The villagers blamed the black rain on the local recyclers, who burn electrical cables and wiring to extract copper.  What he's been able to pull together is nothing short of miraculous; getting both sides to agree like that at all levels, it's remarkable.  That made Garb even more curious. He and another graduate student, John-Michael Davis of Memorial University of Newfoundland in St John's, Canada, analysed the local e-waste economy from top to bottom by conducting hundreds of interviews and doing randomized surveys. Davis even moved to Beit Awwa for more than a year to immerse himself in the community. Their study 2  is the first to chart the entire e-waste recycling economy anywhere, says Garb. They found that the items in the trucks came from a variety of sources in Israel: technology companies or government ministries upgrading computers or routers and disposing of the old equipment; repair labs that discard or sell old televisions or other hard-to-fix items; and households getting rid of old appliances. Some of it ends up in Beit Awwa, says Davis, where there is a big market for discarded furniture and appliances. Although many items are refurbished and resold, large quantities of electronic waste are dismantled and burnt in an industry spread across Beit Awwa and three adjacent villages: Idhna, Al-Kum and Deir Sammit. He and Garb calculated that some 60,000 tonnes of e-waste \u2014 about half of what is produced in Israel \u2014 were processed at these four villages in 2014. Some residents break down or repair the equipment in informal facilities, and others burn components at more than 500 sites in and around the villages. About 70 of these sites have, at some point in the past decade, burned at least a tonne of waste per day over the course of a year, says Garb. The local industry took off in 2004 after the price of copper jumped and Israel's construction of a security barrier made it difficult for Palestinian men in this area to cross into Israel for work. \n               Toxic imports \n             This kind of e-waste traffic from prosperous to poorer communities happens around the world. According to a report 3   compiled by the United Nations , an estimated 41.8 million tonnes of waste electrical and electronic equipment was generated in 2014 (see 'Mountains of e-waste'). Europe and Asia are the largest producers, and African and Asian countries \u2014 including Ghana, Nigeria, China, Pakistan, India and Vietnam \u2014 are key destinations for shipments of hazardous e-waste for dumping. In the Palestinian territories and much of the developing world, recyclers rely on inexpensive methods \u2014 using hammers and axes to dismantle equipment and burning cables to extract the copper. These techniques are also among the most polluting. So when Garb became aware of the problem in what is practically his backyard, he felt compelled to try clean it up, he says. In combination with Palestinian and Israeli officials, Garb and his colleagues are attempting to transform the illegal, unregulated enterprise into a formal recycling trade \u2014 with facilities that allow for safe extraction of valuable components. Instead of stamping out the industry, which he says would drive it elsewhere in the West Bank, Garb hopes to build a partnership that will benefit all parties. A tour of Beit Awwa and surrounding villages reveals how much is at stake. An acrid smell fills the air as Garb walks past ramshackle workshops and blackened hills. One facility is stacked high with old lamps, refrigerators, sinks, cables, metal cabinets, motherboards, keyboards and a pile of burned metal scrap. Nailed to a pole outside, an old Hebrew sign warns, \u201cPlease keep [area] clean.\u201d A boy, about ten years old, saunters about in flip-flops over the greasy black ground. According to a small survey by Garb, the average age at which burners start in the industry is 15. Many residents worry about the recycling industry. At a burn site close to a quarry east of Idhna, two men driving past in their truck stop to talk to Garb. They complain that rainwater run-off from the fires has rendered nearby fields so contaminated that crops cannot grow there. During burning, chickens die or lay eggs without shells or yolks, say the workers. In an unpublished study, Garb and Davis found hints that one type of cancer might be more common near burn sites. Garb has worked with a local group, Al Yassaria Women's Association, as well as Noam Weisbrod, a contaminant hydrologist from Ben-Gurion University, to submit a proposal to the US Agency for International Development to study household exposure to e-waste burning and its health effects, including birth defects and cancer. Preliminary results from other studies suggest that children in the area are getting high doses of heavy-metal pollution. Sulaiman Swaitti, a Palestinian nurse from Beit Awwa who is now a master's student in public health and environmental studies at Ben-Gurion University, analysed levels of lead in the blood of 22 children from Deir Sammit. Twelve children had concentrations above 5 micrograms per decilitre, the point at which the US Centers for Disease Control and Prevention recommends public-health action; the highest value was 18.7 micrograms per decilitre. For comparison, a separate study 4  of children in the West Bank found an average lead level of 4.2 micrograms per decilitre. Swaitti plans to do a more formal study by measuring lead in blood samples from 40 children in each of the other villages, plus a control group. He will also test dust in their households for heavy metals, and correlate the results with school grades. \u201cWe have been suffering from this e-waste for a very long time \u2014 from the emissions from the burning of plastics, copper and aluminium,\u201d says Muhamad Sweity, administrator of the Al Yassaria municipality, which covers Deir Sammit, Beit Awwa and Al-Kum. \u201cWe suffer from many, many problems: contamination of water, agriculture, farm animals, nature.\u201d Children who live close to the burn sites have breathing problems, he says, and olive-tree yields have declined year after year. \u201cWe tried several times to solve the problem with the aid of the police, the [Israeli] Civil Administration, the government \u2014 nothing helped us,\u201d Sweity says. Although burning e-waste is illegal, oversight in the area is complicated because local Palestinian police have to coordinate with the Israeli Civil Administration \u2014 which has military and administrative control of the area \u2014 to enter the region where much of the burning takes place. By the time police get there, the people responsible are typically gone. And many families earn their livelihood from recycling, so they are reluctant to give it up. What is really needed, Sweity says, is financial support for clean recycling companies, as well as better police enforcement and inspection schemes. \n               Two worlds \n             That's where Garb comes in. With his calm and easy-going manner, he gets a warm welcome in the Palestinian villages and also works effectively with Israeli authorities and other parties. \u201cSomehow I seem to be put together in a way that allows me to move between the Arab and Jewish worlds, and between the social worlds of consulates and scrap yards, ministries and smugglers,\u201d says Garb. That sensibility has helped to give Garb a unique role in this area, says Johan Schaar, head of development cooperation at the Consulate General of Sweden in Jerusalem. Sweden has been active in promoting development in the Palestinian territories, and Schaar first met Garb three years ago. \u201cWhat Yaakov has done, all this mapping that he has been able to do in these villages, that he has gained the confidence of all the people involved in this, is quite extraordinary,\u201d he says. Garb's efforts stand out, say others in the region, because there is a high degree of tension there, with little progress towards peace after nearly 50 years of Israeli occupation of the West Bank. In December 2015, SIDA gave Garb and Pure Earth a $180,000 grant that funded a trial remediation of two sites in Beit Awwa. Workers used a tractor to scrape off the black surface goop, and then dug out the remaining toxic grime with picks and shovels. They transferred the material to a double-sealed plastic storage deposit at the same site. (Garb is now negotiating to move the contaminated soils to a certified facility for hazardous-waste disposal in Israel.) That was just the first phase of a much broader project. In January this year, Garb and his team drew up an ambitious programme consisting of three components: clean up the hazardous-waste sites, create a sustainable Palestinian recycling sector and prevent further contamination of sites. The plan to build a sustainable recycling industry was key to convincing the Israeli Civil Administration to come on board, Garb says. Schaar says that SIDA expects to provide a $2.7-million grant, channelled through Pure Earth, to implement the broader plan. One part will be used to clean up 100 toxic waste sites. Another portion will fund an interim programme of free, legal copper recycling for residents of these villages. And a third will pay for a small rapid-response unit to shut down illegal burning. Fuller says the project yields multiple benefits by \u201cgiving jobs to the poorest, helping to build that economy, and that helps the whole peace process\u201d. The Israeli government and Palestinian Environmental Quality Authority are now in the final stages of negotiating an agreement to put the plans into place. \n               Back to the grind \n             The cleaner future that Garb envisions is slowly taking shape in a small warehouse in Idhna. At the Safa Recycling and Material Processing company, workers shovel wiring from electronics and electrical cables into a $220,000 grinding machine that shreds copper cables and separates them from their plastic sleeving. Safa's owner, Ismail Suleiman, says that when he bought the machine, \u201ceveryone was happy and clapping\u201d, but it has been difficult to turn that into a profitable business. Grinding the cables costs much more than burning them. And he had trouble obtaining the proper permits to import the cables legally from Israel, he says. \u201cThere is no enforcement of laws by the Palestinian Authority against burning,\u201d he says. \u201cAny way you look at it, it's a mess.\u201d Money from the Swedish pilot project has helped. In February, the SIDA grant provided funds for locals to bring in cables for free grinding. Recyclers were so eager that they processed 15 tonnes in just 3 days, demonstrating that they might embrace a cleaner alternative to burning if it is economical. A portion of the new Swedish grant would provide an economic incentive to continue this legal recycling.  We suffer from many, many problems: contamination of water, agriculture, farm animals, nature.  The Israeli government has also become involved in the clean-up efforts, because smoke from the four villages affects Israeli residents as well. Benny Elbaz, the Israeli Ministry of the Environment official who heads the West Bank environmental division of the Israeli Civil Administration in Beit El, is upbeat about the upcoming agreement. \u201cWe're doing everything we can to ensure the success of this project,\u201d he says. Elbaz plans to expand the initial pilot recycling project at Safa to other materials and other companies. Garb, too, has high hopes for extending the scope of the industry. He foresees \u201ccompetitive boutique recycling\u201d, in which Palestinians develop their own micro-niches for hand-dismantling devices and extracting rare-earth metals such as neodymium from the powerful magnets found in microphones and hard drives. Pure Earth sees the Palestinian\u2013Israeli initiative as a model for how to clean up burn sites in urban centres of many other poor places around the world, including those in Africa. Fuller says that Garb's project is one of the best designs he has seen, because it is politically viable. \u201cI hope that we're going to do it in many other places,\u201d he says. But making the shift from burning to clean recycling is challenging, says Kees Bald\u00e9, an associate programme officer at the United Nations University in Bonn, Germany, and co-author of the 2014 report 3  on global e-waste. The higher costs of clean recycling and local corruption in many areas often conspire to doom such efforts. If the Palestinian\u2013Israeli agreement succeeds, Bald\u00e9 says, \u201cI think that the societal impact is going to be big\u201d. With proper facilities, e-waste recyclers will earn more money, and be protected from the toxins released by burning, he says. Even if it works well, the project in the Hebron Hills will not completely solve the problem there, says Garb, although it will put a big dent in it. The blackened slopes around Beit Awwa and nearby villages show just how far he and his colleagues have to go in cleaning up the pollution. But here and there, a delicate purple wildflower or a green shoot pokes up amid the scattered phone cases and burned rocks \u2014 a sign of what could be. And the agreement over e-waste is a rare case of Israelis and Palestinians working together for a better future, says Garb, \u201cto see beyond the politics of the moment to the long-term human and environmental significance\u201d.\n \n                 Tweet \n                 Follow @NatureNews \n               Reprints and Permissions"},
{"file_id": "539482a", "url": "https://www.nature.com/articles/539482a", "year": 2016, "authors": [{"name": "Carrie Arnold"}], "parsed_as_year": "2006_or_before", "body": "Elaina Tuttle spent her life trying to understand the bizarre chromosome evolution of a common bird \u2014 until tragedy struck. There was one sound that biologist Rusty Gonser always heard at Cranberry Lake \u2014 and there was one sound that he would never hear again. Every summer for more than 25 years, Gonser and his wife, Elaina Tuttle, had made the trip to this field station in the Adirondack Mountains \u2014 a 45-minute boat ride from the nearest road. Now, as he moored his boat to the shaky wooden dock, he heard a familiar and short song that sounded like 'oh-sweet-Canada'. The whistle was from a white-throated sparrow calling hopefully for a mate. What he didn't hear was the voice or laughter of his wife. For the first time, Gonser was at Cranberry Lake alone. Just a few weeks earlier, Tuttle had died of breast cancer. Her entire career, and most of Gonser's, had been devoted to understanding every aspect of the biology of the white-throated sparrow ( Zonotrichia albicollis ). Less than six months before she died this year at the age of 52, the couple and their team published a paper 1  that was the culmination of that work. It explained how a chance genetic mutation had put the species on an extraordinary evolutionary path. The mutation had flipped a large section of chromosome 2, leaving it unable to pair up with a partner and exchange genetic information. The more than 1,100 genes in the inversion were inherited together as part of a massive 'supergene' and eventually drove the evolution of two different 'morphs' \u2014 subtypes of the bird that are coloured differently, behave differently and mate only with the opposite morph. Tuttle and Gonser's leap was to show that this process is nearly identical to the early evolution of certain sex chromosomes, including the human X and Y. The researchers realized that they were effectively watching the bird evolve two sex chromosomes, on top of the two it already had. \u201cThis bird acts like it has four sexes,\u201d says Christopher Balakrishnan, an evolutionary biologist at East Carolina University in Greenville, North Carolina, who worked with Tuttle and Gonser. \u201cOne individual can only mate with one-quarter of the population. There are very few sexual systems with more than two sexes.\u201d The work helps to explain a long-standing puzzle for biologists. It shows how two identical chromosomes can evolve into distinct subtypes that can define the sexes of a species and their different behaviours. \u201cThese birds are an amazing system,\u201d says Catherine Peichel, an evolutionary ecologist at the University of Berne. \u201cThe process of sex-chromosome evolution tends to erase much of the evidence of how it happened, so being able to watch the process in action is a huge benefit.\u201d What makes Tuttle and Gonser's project even more unusual is the accumulation of almost 30 years of data, \u201csomething that is almost unheard of in biology now\u201d, says Melissa Wilson Sayres, a computational biologist at Arizona State University in Tempe. \u201cMost people tend to jump from project to project.\u201d Gonser is determined to carry on the project. He returned to the field station this summer to continue Tuttle's legacy and use this drab little garden bird to understand how sex chromosomes may have evolved. \u201cWho knows \u2014 there might be many more species that have weird sex chromosomes and we've just never bothered to look,\u201d he says. \n               A flock of ideas \n             Gonser's mind moves at a mile a minute, his thoughts racing ahead of his grey Honda Civic. Dressed in basketball shorts and a T-shirt, and sporting days-old stubble, he speeds west on the I-70 interstate towards Indiana State University in Terre Haute, where he and Tuttle shared an office. As ideas tumble out \u2014 about new research projects, people to meet, restaurants to avoid \u2014 he pauses briefly and apologizes. \u201cSorry if I'm repeating myself. My brain hasn't been working right since \u2026\u201d he pauses briefly, and clears his throat, \u201csince Elaina passed.\u201d Tuttle and Gonser first met in 1991 at the State University of New York at Albany, where both were pursuing PhDs in ecology. Gonser was investigating the Puerto Rican frog  Eleutherodactylus coqui  and Tuttle was studying fish ecology at New York's Finger Lakes. Long months in the field suited her love for the natural world, and it was there that she got to know the curious lifestyle of the white-throated sparrow. The bird is relatively common in gardens across the eastern half of North America \u2014 and on first glance it is rather plain. All the birds have mostly tan and grey plumage, except for a patch of white under the chin and a bright pop of yellow between the eyes and beak. But closer scrutiny reveals the two types: some have white stripes on their heads, and some have tan stripes. And, as bird spotters and naturalists have long known, the two morphs behave in different ways. The tan-striped birds are poor at singing, monogamous and fiercely protect their hatchlings from predators such as raccoons and snakes. The white-striped ones are aggressive, promiscuous, more cavalier about their offspring, and tuneful: Gonser says that they produce a more operatic refrain of oh-sweet-Canada. White-striped birds seem to mate only with tan-striped ones \u2014 a relatively unusual phenomenon called disassortative mating (see 'Opposites attract'). Tuttle became interested; why do the two morphs behave in this way? The literature already contained a huge clue. In 1966, ornithologist H. B. Thorneycroft published a paper in  Science 2  pointing out the bird's unusual chromosome pair. Tan birds carry two identical copies of chromosome 2, but in white birds, one copy contains an inversion. It's as if someone had taken a pair of scissors, snipped out most of the chromosome and placed the DNA back in reverse. A chromosomal inversion this large was rare in vertebrates, Thorneycraft noted. It looked like disassortative mating maintained the two morphs in equal proportions in the population, because Mendelian inheritance ensures that, on average, half the offspring of a white\u2013tan pair will inherit the inverted version of chromosome 2. But it would take more work to prove that this was true. To Tuttle, it was a fascinating puzzle \u2014 a way to shed light on chromosome evolution, as well as the genes underlying social behaviour. In the early 1990s, however, it was too expensive and laborious to find answers by sequencing the bird's genome. So Tuttle initially focused on collecting more detail about their behaviour, such as how they selected mates and where they built nests. The goal was to understand what might affect offspring survival. She caught and tagged birds, drew blood samples and perfected the art of collecting semen. \u201cElaina was the best bird masturbator I ever met,\u201d Gonser says. Gonser soon became drawn into the work. (The couple married in 1994.) After the birth of their son Caleb in 2000, the family began spending summers at Cranberry Lake, and they slowly learnt more about the sparrow. For a 2003 paper 3 , Tuttle used genetic analysis of nestlings to quantify the different reproductive strategies of the two morphs. She showed that nearly one-third of the offspring fathered by white-striped males were not born to females with whom they shared a nest. Tan males, by contrast, spent less of their energy finding extra partners and more time guarding their own, so they were more likely to have fathered the chicks they were raising. Although the two morphs went about reproduction in very different ways, both achieved equal reproductive success. Six years later, the team secured a grant from the US National Institutes of Health to start genetic analyses. They mapped chromosome 2 in detail 4  and found that the changes were not a single inversion as Thorneycroft had indicated, but actually a series of inversions within inversions that scrambled the order of genes. They identified several genes that seemed to be associated with feather colour and behaviour, and that might explain the differences between the two morphs. \u201cIt's really, really rare to find such a direct relationship between a set of genes and behaviours. That's what makes these birds so interesting to study,\u201d says Donna Maney, a neuroendocrinologist at Emory University in Atlanta, Georgia, who uses the white-throated sparrow as a model to understand how hormones affect the brain. But then, in 2011, just as Tuttle and Gonser were ramping up their genome sequencing, a routine mammogram revealed a lump in one of Tuttle's breasts. A biopsy confirmed that it was invasive cancer. The couple was shocked \u2014 their son was only 11 \u2014 but a mastectomy and the drug tamoxifen seemed to contain the disease. Determined not to let the cancer control her life, Tuttle pressed on with her work. \n               Doubly divided \n             By this point, the researchers had come up with the concept that the birds were evolving a second set of sex chromosomes. \u201cIt's a bizarre idea, but it just makes sense from the data,\u201d said Alan Bergland, a geneticist who studies the molecular basis of evolutionary adaptation at the University of Virginia in Charlottesville. No other species was known to have one set of fully operational sex chromosomes and another pair that subdivided the species again on another aspect of mate choice. Evolution has led to many weird and wonderful sexes and means of determining them. Some animals, such as reptiles, have two sexes and no sex chromosomes, whereas the freshwater protozoan  Tetrahymena thermophila   has seven sexes , each of which can mate with any type except its own. Two sexes with one set of sex chromosomes is the most common arrangement, and has evolved independently many times. But there's no reason a species can't have more sex chromosomes, Wilson Sayres says. \u201cIf you've got two genes linked together that can't cross over during meiosis, and one of them plays a role in sex determination, suddenly you can have a new sex chromosome.\u201d Researchers believe that the X and Y chromosomes in many mammals, and the W and Z in birds, emerged from a major inversion on a normal pair of chromosomes that prevented them from swapping genetic material, or 'crossing over'. The suite of genes in the inversion was cemented together as a supergene that was inherited in one large chunk. On both the Y and Z chromosomes, the inversion shifted the location of a gene that determined male or female sexual development, respectively (male birds are ZZ and females are ZW). Over time, the Y and Z chromosomes accumulated mutations, because  crossing over wasn't weeding them out . But, because all this happened in the distant evolutionary past, scientists had struggled to identify the precise steps involved. The sparrows offered such a chance. The inversion on chromosome 2 doesn't include genes that determine sexual development \u2014 but it does contain some that affect the birds' reproductive behaviour. Over time, those genes diverged and drove the different characteristics of the two morphs. \u201cWhatever genes control these behavioural differences will ultimately be traced back to this inversion,\u201d says Maney. But to truly show that chromosome 2 was evolving like a sex chromosome, Tuttle and Gonser would need to demonstrate that the genes in the inversion were acquiring mutations much faster than those on other chromosomes. This would prove that the white and tan morphs were using true disassortative mating, so that all pairs were white\u2013tan. Even a tiny percentage of white\u2013white matings would allow the inverted chromosome 2 to undergo crossing over that would help to cleanse it of mutations, and undermine the idea that it was acting like a sex chromosome. Tuttle and Gonser would need to sequence the genomes of many birds, to show parentage and to compare mutation rates in the inversion with the rest of the genome. Finding DNA to sequence was easy. By now, Tuttle had several freezers full of blood samples collected over the years from thousands of birds. But other biology was getting in the way. In autumn 2013, a chronic cough sent Tuttle back to the doctor with what she thought was bronchitis. She learned that her cancer had returned and spread to her lungs. Genome sequencing of her tumour revealed that her original cancer was a mosaic of cell types; the tamoxifen had tamed the hormone-sensitive cancer cells, but the insensitive ones had survived. She underwent more chemotherapy, which held the cancer in check. \n               The last lake trip \n             By the summer of 2015, the team had gathered both the ecological and the genetic data that they needed and were putting the finishing touches to their big paper \u2014 when a routine scan revealed that Tuttle's tumours were growing again. Tuttle made the 13-hour drive from Terre Haute to Cranberry Lake between chemo treatments. \u201cWe knew she was sick, but we didn't realize just how sick she really was,\u201d says graduate student Lindsay Forrette. Soon, Tuttle was getting increasingly grim predictions about how much time she had left. \u201cWe were all thinking she was going to beat the cancer, and when we finally understood she wasn't \u2026\u201d Gonser's voice trails off. When Tuttle realized she would never return to Cranberry Lake, she broke down and cried. In January 2016, when the paper was published in  Current Biology , it showed unequivocally that chromosome 2 was evolving like a sex chromosome. White\u2013white and tan\u2013tan matings were exceedingly rare. Using the whole-genome sequences of 50 birds, the team demonstrated that the genes in the inversion were acquiring mutations much more quickly than elsewhere in the genome, a pattern that echoed the evolution of sex chromosomes in humans and birds. In a press release, Tuttle said: \u201cThis is probably my pinnacle paper.\u201d It would also be her last. By spring, her health had worsened. Just five days before she died, Tuttle busied herself from her hospital bed, writing more papers and helping graduate students to analyse data. She died on 15 June. When Gonser announced the news, he received condolences from around the world. \u201cEveryone who knew her, loved her. She was just that kind of person,\u201d Gonser says. Tuttle leaves a rich research legacy that raises further questions, including whether this chromosomal system is ultimately destined to disappear. Balakrishnan says it is unsustainable. \u201cThat we never see systems with four sexes says that they're evolutionarily unstable and one of these alleles will ultimately go extinct.\u201d That's because in a four-sex system, each bird must work that much harder to find a mate \u2014 a white female is not just looking for a male, she needs a tan one \u2014 so selection will favour a more advantageous two-sex system instead. Balakrishnan intends to use the white-throated sparrow to further tease apart the genetic and environmental factors that drove its evolution in the first place. Gonser will keep going back to Cranberry Lake. He and his group want to better understand which genes control the sparrow's behaviour \u2014 from mate selection to parenting \u2014 and how those characteristics were affected by the inversion. They are using digital maps and satellite data to chart nesting sites, track tagged birds and build a richer set of behavioural data. \u201cThere's a lot more information left in these birds,\u201d Gonser says. \u201cAnd I think Elaina would like that we're trying to uncover their secrets.\u201d \n                 Tweet \n                 Follow @NatureNews \n               \n                     A battle of the sexes is waged in the genes 2015-Jun-26 \n                   \n                     Sex redefined 2015-Feb-18 \n                   \n                     Reprieve for men: Y chromosome is not vanishing 2014-Apr-24 \n                   \n                     How a microbe chooses among seven sexes 2013-Mar-27 \n                   \n                     White-throated sparrow project \n                   Reprints and Permissions"},
{"file_id": "537465a", "url": "https://www.nature.com/articles/537465a", "year": 2016, "authors": [], "parsed_as_year": "2006_or_before", "body": "A special issue explores the study of inequality, and how socio-economic divides affect the science workforce. In every society on Earth, at least some fraction of the citizens find their talents being sacrificed to poverty, prejudice, poor schooling and lack of opportunity. Science is comparatively open: many top-rank inventors and researchers have risen from humble beginnings through a combination of brilliance and luck. Even so, the field is losing out on millions of bright but underprivileged students. And now that researchers have begun to grapple with ways to increase gender and ethnic diversity in science, many are calling for socio-economic status to be the next big topic of debate. This  issue of  Nature  contributes to that conversation with a look at what research can say about inequality as a phenomenon, as well as an examination of how it plays out in science. Economist Branko Milanovic  uses historical data to follow the ebb and flow of inequality  on a timescale of centuries. He concludes that the striking global increase in inequality since the early 1980s will turn around at some point \u2014 driven either by comparatively benign historical forces such as technological change, or by malign events such as war or plague. Economist Mike Savage takes on the  surprisingly fraught question of what 'class' actually means . Definitions have historically centred on people's income or occupation, but, more recently, sociologists have argued for incorporating an individual's cultural and social capital \u2014 the connections that he or she can parlay into better education and jobs. The difference could influence how governments shape programmes to address inequality. Two News Features look at inequality within research itself. One examines how  class and inequality are affecting science and scientists in eight countries  around the world, and finds that even the richest countries still struggle with the issue. The other  examines salary data for scientists  in several countries, and finds that there is a growing gap between top earners and the rest. These data back up the findings from  Nature's annual salary survey , which reveals that many scientists think they have made an economic sacrifice for their career. A notable proportion say that their job prospects are worse than those of past generations, and many would not recommend a career in research to young people. On the subject of inequality, it seems, science still has a lot of work to do. \n                 Tweet \n                 Follow @NatureNews \n               \n                     Nature  special: Inequality in science \n                   Reprints and Permissions"},
{"file_id": "538158a", "url": "https://www.nature.com/articles/538158a", "year": 2016, "authors": [{"name": "Alison Abbott"}], "parsed_as_year": "2006_or_before", "body": "The refugees and migrants surging into Europe are suffering very high levels of psychiatric disorders. Researchers are struggling to help. On an ice-cold day in January, clinical psychologist Emily Holmes picked up a stack of empty diaries and went down to Stockholm\u2019s central train station in search of refugees. She didn\u2019t have to look hard. Crowds of lost-looking young people were milling around the concourse, in clothes too flimsy for the freezing air. \u201cIt struck me hard to see how thin some of the young men were,\u201d she says. Holmes, who works at Stockholm\u2019s Karolinska Institute, was seeking help with her research\u00a0\u2014\u00a0a pilot project on post-traumatic stress disorder (PTSD), which is all too common in refugees. She wanted to see whether they would be willing to spend a week noting down any flashbacks\u00a0\u2014\u00a0fragmented memories of a trauma that rush unbidden into the mind and torment those with PTSD. She easily found volunteers. And when they returned the diaries, Holmes was shocked to see that they reported an average of two a day\u00a0\u2014\u00a0many more than the PTSD sufferers she routinely dealt with. \u201cMy heart went out to them,\u201d she says. \u201cThey managed to travel thousands of kilometres to find their way to safety with this level of symptoms.\u201d Europe is experiencing the largest movement of people since the Second World War. Last year, more than 1.2 million people applied for asylum in the European Union\u00a0\u2014\u00a0and those numbers underestimate the scale of the problem. Germany, which has taken in the lion\u2019s share of people, reckons that it received more than a million refugees in 2015, tens of thousands of whom have yet to officially apply for asylum. Most came from Syria, Afghanistan and Iraq. Many have experienced war, shock, upheaval and terrible journeys, and they often have poor physical health. The crisis has attracted global attention and sparked political tension as countries struggle to accommodate and integrate the influx. What hasn\u2019t been widely discussed is the enormous burden of mental-health disorders in migrants and refugees. Clinical psychologist Thomas Elbert from the University of Konstanz in Germany is conducting a local survey of refugees that suggests \u201cmore than half of those who arrived in Germany in the last few years show signs of mental disorder, and a quarter of them have a PTSD, anxiety or depression that won\u2019t get better without help\u201d. Previous research shows that refugees and migrants are also at a slightly increased risk of developing schizophrenia. \u201cIt is a public-health tragedy \u2014\u00a0and it\u2019s a scandal that it is not recognized as such, as a physical epidemic would be,\u201d says epidemiologist James Kirkbride of University College London. Doctors and researchers are starting to take action. Holmes and other psychologists and psychiatrists are working with refugees to develop practical, cheap and effective therapies for trauma-related disorders\u00a0\u2014\u00a0therapies that could be quickly deployed on this group. Other scientists want to work with local refugees to understand more about how the different types of stresses they suffer play out in their brains, and to learn more about the basic biology of psychiatric disorders. Scientists hope that their studies will help them to deal with other displaced populations, and help policymakers to accommodate the current influx. Politicians have been too slow to consider mental health when they call for refugees to integrate quickly, Elbert says. \u201cIt is illusory to think that people can learn a new language and find work when they can\u2019t function properly mentally. If we want quick integration, we need an immediate plan for mental health.\u201d \n               Making a new life \n             Amira is a clinical psychologist and a refugee from Syria. When the war there started, she worked in camps for Syrian refugees in Jordan. She saw people who had been physically attacked, women who had been raped and children who had been neglected. The symptoms of PTSD were clear, and she knows that many refugees have depression and anxiety, too. She asked that her real name not be used. She arrived in Sweden at the end of December 2015, and wanted to help other refugees but was not allowed to work at first. She tried to make contacts in Stockholm and joined a language course for refugees; she felt very alone but carried on. Now, she has a 6-month position. \u201cI met many children who have experienced war,\u201d she says. \u201cWe feel sad, [about] how our children think and how they feel. I have a child and I try to protect him.\u201d Researchers already have a wealth of evidence about the mental health of migrant and refugee populations around the world. (The United Nations defines refugees as people fleeing armed conflict or persecution and migrants as people who choose to move to improve their lives. Asylum seekers are those seeking official refugee status; but sometimes different definitions are used.) A 2005 meta-analysis of studies performed mostly in northern Europe showed that first- and second-generation migrants were at much greater risk of schizophrenia than non-migrants \u2014 and that those from developing countries were more at risk than those from developed ones 1 . A large cohort study published in March looked at 1.3 million people who had arrived in Sweden before 2011 (see \u2018Migrant crisis\u2019). Refugees had a threefold higher incidence of schizophrenia and other psychotic disorders than native-born Swedes, and a 66% higher incidence than migrants who were not refugees 2 . (The overall risk for refugees and migrants still remains comparatively low, at perhaps 2\u20133%.) Kirkbride, an author on the study, says that his team\u2019s more recent analysis of UK migration data suggests that the level of increased risk of psychotic disorders may depend on how old people were when they migrated\u00a0\u2014\u00a0with children potentially at greater risk. Those who stand out most seem to be particularly vulnerable. The 2005 meta-analysis showed that black migrants in a mostly white population had an almost fivefold increased risk of psychotic disorders 1 . And the risk is higher for migrants living in neighbourhoods with a low proportion of residents from their own ethnic group compared with those surrounded by many of their own ethnicity 3 . Psychiatrist Andreas Meyer-Lindenberg from the Central Institute for Mental Health in Mannheim, Germany, is one of those trying to understand the brain mechanisms involved. He has already studied other populations with an above-average risk of psychosis, such as city dwellers 4  and ethnic minorities 5 . The work suggested that the brains of these people are  overly sensitive to social stress , such as a stream of disapproving feedback. Thanks to a grant awarded last month from the state government of Baden-W\u00fcrttemberg, Meyer-Lindenberg plans to extend his studies by recruiting 200 refugees and 200 people from the local community. The refugees will use smartphones to note their state of mind\u00a0\u2014\u00a0such as feelings of suspiciousness\u00a0\u2014\u00a0and they will later receive brain scans. The eventual aim is to find patterns in the data that indicate people with abnormal processing of social stress, who may therefore be at  increased risk of mental illness . Jean-Paul Selten, a psychiatrist at Maastricht University in the Netherlands, is also exploring the poisonous nature of social stress. He proposes that pressures such as social exclusion raise the risk of psychosis by changing the brain\u2019s sensitivity to the neurotransmitter dopamine 6 . Germany\u2019s integration plans, consolidated in a law that came into force in August, involve distributing refugees across the country to avoid the creation of large, isolated ethnic communities. That could be problematic if it increases people\u2019s isolation, but Meyer-Lindenberg says it\u2019s \u201cactually a good policy\u201d\u00a0\u2014\u00a0because other people in the community get to know refugees, and this usually reduces xenophobia, another major source of social stress. Politicians consider integration to be essential for security, among other things. A handful of terrorist attacks in Europe over the past two years have been carried out by refugees or others with a migrant background who had been known to have a history of psychiatric problems. But doctors and researchers are extremely wary about making a link between refugees or migrants and terrorist acts, pointing out that very few of those with mental-health problems become violent, regardless of their origins. The security concern simply accentuates the need to help all those with mental-health problems across the population, they say. \n               The stress of upheaval \n             Psychologists recognize three windows of extreme stress for refugees: the often violent traumas in their home countries that led to their flight; the journey itself; and the arrival, when people are thrust into a foreign country. \u201cThe latter \u2018post-migration\u2019 phase is becoming increasingly important,\u201d says psychiatrist Malek Bajbouj from the Charit\u00e9 university hospital in Berlin. \u201cSuddenly they realize they have lost everything, have no control over aspects of their lives and no social standing.\u201d In February, Bajbouj\u00a0\u2014\u00a0who is of Syrian descent and speaks Arabic\u00a0\u2014 and two colleagues from other departments opened a clearing centre for refugees with mental-health problems, the first of its kind in Germany. It\u2019s a quiet building, a former hospital in the centre of Berlin\u00a0\u2014\u00a0but already 1,500 troubled people have passed through its doors. \u201cRefugees may arrive in Germany with great hope, but then find themselves stuck for months in camps with no apparent prospects,\u201d he says. \u201cWhen we ask them what their greatest stressors are, they typically refer not to their traumatic memories, but to their current frustrations.\u201d The biggest challenge for Bajbouj and others is the sheer volume of people in need of help. They must be assisted quickly and cheaply, in ways that take the pressure off overstretched health professionals. At the clearing centre, three psychiatrists assess visitors rapidly, categorizing them into those who require low-level or more-intensive psychiatric help and those who can be aided by social workers. A lot of effort goes into teaching about stress management and the science behind mental health. \u201cSome people from rural areas hold the Djinn responsible for their moods,\u201d says Bajbouj, \u201cand we teach them that symptoms like sleeplessness and depression are biologically based and can be treated.\u201d Elbert wants to see similar triage systems put in place across Germany. In a paper to be published next month, he and a group of colleagues call for a three-tiered approach. Refugees would initially be helped by bilingual laypeople\u00a0\u2014\u00a0ideally migrants or refugees themselves\u00a0\u2014\u00a0who are trained to guide people through the German health system (tier one) or to offer trauma counselling (tier two). Those in most need would progress to tier three: qualified psychologists or psychiatrists. \n               Peace of mind \n             Training laypeople seems to work in emergency situations. Elbert, together with Sarah Ayoughi, a clinical psychologist from the psycho\u00adsocial-care organization Ipso, carried out a randomized controlled study of people with mental-health conditions in north Afghanistan who received psychosocial counselling\u00a0\u2014\u00a0a type of talking therapy\u00a0\u2014\u00a0conducted by local physicians who had no previous education in psychology or psychiatry, but who were specially trained for the trial. Just 5\u20138 sessions improved symptoms of depression and anxiety for up to 3 months 7 . And several studies, including a 2011 randomized controlled trial of former child soldiers in northern Uganda 8 , show that an approach called narration exposure therapy (NET), carried out by trained lay counsellors, can reduce the severity of PTSD symptoms. Elbert started developing NET with his wife Maggie Schauer, also a clinical psychologist at the University of Konstanz, when they were working with refugees in Kosovo in the late 1990s. It exploits new understanding of how memories are linked with fear circuitry in the brain. A traumatized person works with a therapist or counsellor to construct a narrative of their lives and anchor their traumatic experiences in the correct time and place. Pragmatic as the three-tiered approach may sound, it won\u2019t be simple to introduce in Germany. Professional associations are resistant to allowing people without formal qualifications to help out with psychotherapies, and various regulations could get in the way. But while the federal government ponders what to do, some programmes are starting up with regional government support. Schauer has received \u20ac100,000 (US$112,000) to test whether NET works as well on refugees in Germany as it has in war-torn countries. And Ayoughi is organizing the training of refugees in Erfurt in Germany, with additional support from the Google Foundation. Bajbouj thinks that the political desire to get refugees into the workforce fast may end up easing the way for more relaxed rules about psychotherapy. And there is another way to deliver inexpensive mental-health care:  through the Internet and apps . He is developing an Arabic-language version of the smartphone app PTSD Coach, which provides education, a personalized emergency plan, self-assessment and 25 different techniques to regulate stress. He is testing it in the Arab Outpatient Centre he opened at the Charit\u00e9 in 2008. In Stockholm, Holmes also hopes that technology can help. The aim of her work is to test whether it\u2019s possible to subdue PTSD-linked emotional flashbacks if a person immediately plays a video game on their phone that competes for cognitive space in the brain\u00a0\u2014\u00a0a technique that she has seen work in laboratory tests 9 . \u201cThe important thing now is to develop simple new approaches to therapy that can be scaled up, and to prove that they help,\u201d she says. Sweden, which has taken in a relatively large number of refugees, is also starting mental-health programmes. Early this year, local authorities rolled out a plan to make it easier for refugees to access support: health checks will include more questions about states of mind, and those recognized as being in need will be channelled towards psychological or psychiatric support. The flow of refugees and migrants has eased this year, in part because Turkey has agreed to take back those who illegally entered EU countries from there. But people keep coming. This August, more than 18,000 refugees entered Germany to seek asylum. And even if the current crisis eases, conflict, poverty, natural catastrophe and climate change will inevitably drive fresh waves of migration around the world. \u201cWe\u2019ve learnt lessons about mental health from crises in war-torn countries,\u201d says Ayoughi, \u201cand we can apply these in the refugee crisis in Europe now if we get the support.\u201d Then, perhaps, lessons learnt in Europe could feed back to war zones. Bajbouj has been calling for a \u2018migration think-tank\u2019, a permanent institution in Germany where scientists of different disciplines can come together to work out what needs to be done. \u201cThe challenges are not just about mental health, but about education, integration into the work force and much more,\u201d he says. \u201cBut mental health impacts everything.\u201d See Editorial  page 139 \n                 Tweet \n                 Facebook \n                 LinkedIn \n                 Weibo \n                 Wechat \n                 Follow @alison_c_abbott \n               Reprints and Permissions"},
{"file_id": "538154a", "url": "https://www.nature.com/articles/538154a", "year": 2016, "authors": [{"name": "Erika Check Hayden"}], "parsed_as_year": "2006_or_before", "body": "Why many \u2018deadly\u2019 gene mutations are turning out to be harmless. Lurking in the genes of the average person are about 54 mutations that look as if they should sicken or even kill their bearer. But they don't. Sonia Vallabh hoped that D178N was one such mutation. In 2010, Vallabh had watched her mother die from a mysterious illness called fatal familial insomnia, in which misfolded prion proteins cluster together and destroy the brain. The following year, Sonia was tested and found that she had a copy of the prion-protein gene,  PRNP , with the same genetic glitch \u2014 D178N \u2014 that had probably caused her mother's illness. It was a veritable death sentence: the average age of onset is 50, and the disease progresses quickly. But it was not a sentence that Vallabh, then 26, was going to accept without a fight. So she and her husband, Eric Minikel, quit their respective careers in law and transportation consulting to become graduate students in biology. They aimed to learn everything they could about fatal familial insomnia and what, if anything, might be done to stop it. One of the most important tasks was to determine whether or not the D178N mutation definitively caused the disease. Few would have thought to ask such a question in years past, but medical genetics has been going through a bit of soul-searching. The fast pace of genomic research since the start of the twenty-first century has packed the literature with thousands of gene mutations associated with disease and disability. Many such associations are solid, but scores of mutations once suggested to be dangerous or even lethal are turning out to be innocuous. These sheep in wolves' clothing are being unmasked thanks to one of the largest genetics studies ever conducted: the Exome Aggregation Consortium, or ExAC. ExAC is a simple idea. It combines sequences for the protein-coding region of the genome \u2014 the exome \u2014 from more than 60,000 people into one database, allowing scientists to compare them and understand how variable they are. But the resource is having tremendous impacts in biomedical research. As well as helping scientists to toss out spurious disease\u2013gene links, it is generating new discoveries. By looking more closely at the frequency of mutations in different populations, researchers can gain insight into what many genes do and how their protein products function. ExAC has turned human genetics upside down, says geneticist David Goldstein of Columbia University in New York City. Instead of starting with a disease or trait and working backwards to find its genetic underpinnings, researchers can start with mutations that look like they should have an interesting effect and investigate what might be happening in the people who harbour them. \u201cThis really is a new way of working,\u201d he says. ExAC is also providing  better information for families facing genetic diagnoses . D178N, for example, was strongly suspected of causing prion disease because it had been seen in several people with the condition and seldom elsewhere. But before ExAC, no one really had the power to see just how rare it was. If it shows up in people more frequently than prion disease does, that would mean Vallabh's risk of getting the disease is much lower than predicted. \u201cWe needed to find out if this mutation had ever been seen in a healthy population,\u201d Minikel says. \n               Data gathering \n             ExAC was born of frustration. In 2012, geneticist Daniel MacArthur was starting his first laboratory, at Massachusetts General Hospital (MGH) in Boston. He wanted to find genetic mutations that caused rare muscle diseases, and needed two things: genome sequences from people with these disorders, and genome sequences from people without them. If a mutation was more common in people with a disorder than in healthy controls, it stood to reason that the mutation was a likely cause. The problem was that MacArthur couldn't find enough sequences from unaffected people. He needed lots of exomes, and although researchers had been sequencing them by the thousands, existing data sets weren't large enough. No one had pulled enough together into one combined, standardized resource. So MacArthur started asking his colleagues to share their data with him. He was well suited to the task: an early adopter of social media, his lively blog posts and acerbic Twitter feed had made him unusually popular and authoritative for a young scientist. He also had a position with the Broad Institute in Cambridge, Massachusetts, a genome-sequencing powerhouse. MacArthur convinced researchers to share data from tens of thousands of exomes with him; most were in some way connected to the Broad. All that remained was to analyse the data, but that was no trivial task. Although the genes had been sequenced, the raw data had been analysed using different types of software \u2014 including some that were out of date. If one individual in the collection showed a rare mutation, it could be real \u2014 or it could be an artefact of how different programs 'called' the bases within, judging whether they were As, Cs, Ts or Gs. MacArthur needed something that would standardize this gigantic data set. The Broad had developed genome-calling software, but it wasn't up to the task of churning through the tremendous amount of data included in ExAC. So MacArthur's team worked closely with the Broad programmers to test the software and scale up its abilities. \u201cThat was a pretty horrific 18 months,\u201d MacArthur recalls. \u201cWe ran into every obstacle imaginable and had nothing to show for it.\u201d \n               Personal stake \n             While this was going on, in April 2013, Vallabh was learning how to work with stem cells at MGH while  Minikel studied bioinformatics . Minikel met MacArthur for lunch and explained his and Vallabh's curiosity about whether D178N existed in healthy people. He admits to being a bit star-struck by MacArthur's reputation. \u201cI thought if I could get him to think about my problem for half an hour, that would probably be the most important thing that happened in my whole month,\u201d Minikel says. The pair went upstairs to MacArthur's lab, where bioinformatician Monkol Lek ran a search on the ExAC data that had been analysed so far \u2014 about 20,000 exomes. They didn't see Vallabh's mutation. That wasn't good news, but, optimistic about exploring the data further, Minikel joined MacArthur's lab. By June 2014, MacArthur's team and its collaborators had a data set that they were confident in \u2014 exomes from 60,706 individuals representing various ethnic groups, who met certain thresholds for health and consent. They released ExAC that October at the annual meeting of the American Society of Human Genetics (ASHG), in San Diego, California. Immediately, researchers and physicians recognized that the data could help to recast their understanding of genetic risks. Many disease-association studies, particularly in recent years, have identified mutations as pathogenic simply because scientists performing analyses on a group of people with a disorder found mutations that looked like the culprit, but didn't see them in healthy people. But it's possible that  they weren't looking hard enough , or in the right populations. Baseline 'healthy' genetic data has tended to come mainly from people of European descent, which can skew results. In August this year, MacArthur's group published 1  its analysis of ExAC data in  Nature , revealing that many mutations thought to be harmful are probably not. In one analysis, the group identified 192 variants that had previously been thought to be pathogenic, but turned out to be relatively common. The scientists reviewed papers about these variants, looking for plausible evidence that they actually caused disease, but could find solid evidence for only nine of them. Most are actually benign, according to standards set by the American College of Medical Genetics and Genomics, and many have now been reclassified as such. Similar work promises to have direct impacts on medical practice. In a companion paper 2 , geneticist Hugh Watkins of the University of Oxford, UK, looked at genes associated with certain types of cardiomyopathy that cause gradual weakening of the heart muscle. Undetected, they can lead to sudden death, and it has become fairly common to check relatives of people with the conditions for genetic mutations associated with them. Those found to have a genetic risk are sometimes counselled to get an implanted defibrillator, which delivers electrical shocks to the heart if it seems to be beating abnormally. Watkins checked the ExAC database for information on genes that have been associated with these heart conditions, and found that many mutations are much too common among healthy people to be pathogenic. About 60 genes had been implicated as harbouring pathogenic mutations that cause one form of the disease; Watkins' analysis revealed that 40 of these probably bear no link. This was troubling. \u201cIf you have a genetic risk that you believe is predicting disease but isn't, you can end up doing drastic things that can harm someone,\u201d says Watkins. Even some of the mutations that seem to be reliably linked to disease aren't a sure bet \u2014 such as those in  PRNP . There are definitely mutations in the gene that cause the disease, but some variants might not be pathogenic or might elevate the risk only slightly (see 'The deadly mutations that weren't'). To find out the status of D178N, Vallabh and Minikel gathered genetic data from more than 16,000 people who had been diagnosed with prion diseases, and compared them with data from almost 600,000 others, including the ExAC participants 3 . The pair found that 52 people in ExAC had  PRNP  mutations that have been linked to prion diseases, but based on the prevalence of the disease, they would have expected to see maybe two. Minikel calculated that some of these supposedly lethal mutations elevated a person's risk of prion disease slightly; some seemed not to be linked to prion disease at all. This work provided insight for people such as Alice Uflacker. In 2011, Uflacker's father, Renan, died from Creutzfeldt\u2013Jakob disease, a prion illness that causes rapid mental and physical deterioration. He was 62. Alice found out that she carried a mutation in  PRNP  called V210I, which had been linked to her father's disease in previous studies. Three years later, she learned from Minikel that the mutation confers, at most, a small risk of disease. The information was helpful, and the result made sense; her grandmother had lived to 93 despite having the same mutation. Vallabh and Minikel would find no such relief, however. D178N was absent from the other genomes they looked at, and is still highly likely to cause prion disease. Minikel and Vallabh had already begun to suspect as much, as Minikel dug into the data. \u201cAll along the way was gradual confirmation of what we were assuming anyway,\u201d Minikel says. \u201cThere wasn't any moment where we said, 'Ah, this is the worst news.' We'd already gotten the worst news.\u201d \n               Human knockouts \n             ExAC is revealing a lot about genes through the frequency of mutations. MacArthur and his team found 1  3,200 genes that are almost never severely mutated in any of the ExAC genomes \u2014 a signal that these genes are important. And yet 72% of them have never before been linked to disease. Researchers are eager to study whether some of these genes play unappreciated parts in illness. Conversely, the group has found nearly 180,000 instances of mutations so severe that they should render their protein products completely inactive. Scientists have long studied genes by knocking them out in animals such as mice, so that they don't work. By looking at the symptoms that develop, they can study what the genes do. But that has never been possible in humans. Now, researchers are eager to study these natural human knockouts to understand what they can reveal about how diseases develop or may be cured. MacArthur and other researchers are gearing up to prioritize which human knockout genes to study and how best to contact the people carrying them for further study. But it will have to wait until he completes the second phase of ExAC. Due to be unveiled at the ASHG meeting in Vancouver, Canada, this month, it will double the data set's size to 135,000 exomes and include some 15,000 whole-genome sequences, which should allow researchers to explore mutations in regulatory regions of the genome that are not captured by exome sequencing. ExAC is quietly becoming a standard tool in medical genetics. Clinical labs around the world now check it before telling a patient that a particular glitch in their genome might be making them ill. If the mutation is common in ExAC, it's unlikely to be harmful. Geneticist Leslie Biesecker at the US National Human Genome Research Institute in Bethesda, Maryland, says that his lab uses ExAC daily in patient care. \u201cIt's a critical factor that we take into consideration for every variant,\u201d he says. He and other geneticists are now embarking on a  painstaking reckoning with the genetics literature  that will probably take years. ExAC has also driven home a point that Goldstein and other researchers have made repeatedly: that failing to include people from Asian, African, Latino and other non-European ancestries is holding back understanding of how genes influence disease by  limiting the view of human genetic diversity . There is now a fresh impetus to include under-represented groups in planned studies linking genetics and health information on large numbers of people, such as the  US Precision Medicine Initiative . For Vallabh and Minikel, ExAC provided a disheartening confirmation, but also some promising insight. Minikel's studies have identified 3  three people in ExAC with mutations that should silence one of the two copies of the prion protein gene. If they can live with a limited amount of functioning protein, perhaps a drug could be made that would silence the defective protein in Vallabh, preventing prion aggregation and disease progression without dangerous side effects. Minikel got in touch with one of the individuals, a man in Sweden, who agreed to donate some cells for research. Minikel and Vallabh have now joined the lab of biochemist Stuart Schreiber at the Broad Institute, where they are working full-time to find candidate drugs to treat prion disease. The couple exemplifies the challenge of translating ExAC data into real medical benefits. \u201cWe can't go back from this,\u201d Vallabh says. \u201cWe have to go through it.\u201d Their situation couldn't be more illustrative of what is at stake: Vallabh is now 32 \u2014 just 20 years younger than her mother was when she died. She has no time to waste. \n                 Tweet \n                 Follow @NatureNews \n                 Follow @Erika_Check \n               See Editorial  page 140 Reprints and Permissions"},
{"file_id": "538308a", "url": "https://www.nature.com/articles/538308a", "year": 2016, "authors": [{"name": "Adam Mann"}], "parsed_as_year": "2006_or_before", "body": "Scientists are beginning to understand why these \u2018mini Wall Streets\u2019 work so well at forecasting election results \u2014 and how they sometimes fail. It was a great way to mix science with gambling, says Anna Dreber. The year was 2012, and an international group of psychologists had just launched the \u2018Reproducibility Project\u2019 \u2014 an effort to repeat dozens of psychology experiments to see which held up 1 . \u201cSo we thought it would be fantastic to bet on the outcome,\u201d says Dreber, who leads a team of behavioural economists at the Stockholm School of Economics. In particular, her team wanted to see whether scientists could make good use of prediction markets: mini Wall Streets in which participants buy and sell \u2018shares\u2019 in a future event at a price that reflects their collective wisdom about the chance of the event happening. As a control, Dreber and her colleagues first asked a group of psychologists to estimate the odds of replication for each study on the project\u2019s list. Then the researchers set up a prediction market for each study, and gave the same psychologists US$100 apiece to invest. When the Reproducibility Project revealed last year that it had been able to replicate fewer than half of the studies examined 2 , Dreber found that her experts hadn\u2019t done much better than chance with their individual predictions. But working collectively through the markets, they had correctly guessed the outcome 71% of the time 3 . Experiments such as this are a testament to the power of prediction markets to turn individuals\u2019 guesses into forecasts of sometimes startling accuracy. That uncanny ability ensures that  during every US presidential election, voters avidly follow the standings for their favoured candidates  on exchanges such as Betfair and the Iowa Electronic Markets (IEM). But prediction markets are increasingly being used to make forecasts of all kinds, on everything from the outcomes of sporting events to the results of business decisions. Advocates maintain that they allow people to aggregate information without the biases that plague traditional forecasting methods,  such as polls or expert analysis .  In science, applications might include giving agencies impartial guidance on the proposals that are most worth funding, helping panels to find a consensus in climate science and other fields or, as Dreber showed, giving researchers a fast and low-cost way to identify the studies that might face problems with replication. But sceptics point out that prediction markets are far from infallible. \u201cThere is a viewpoint among some people that once you set up a market this magic will happen and you\u2019ll get a great prediction no matter what,\u201d says economist Eric Zitzewitz at Dartmouth College in Hanover, New Hampshire. That is not the case: determining the best designs for prediction markets, as well as their limitations, is an area of active research. Reporter Adam Levy finds out how to predict an election, and why the recipe isn\u2019t always easy to follow. Nevertheless, prediction-market supporters argue that even imperfect forecasts can be helpful. \u201cHearing there\u2019s an 80 or 90% chance of rain will make me take an umbrella,\u201d says Anthony Aguirre, a physicist at the University of California, Santa Cruz. \u201cI think there\u2019s a big space between being able to time travel and physically see what will happen, and then throwing up your hands and saying it\u2019s totally unpredictable.\u201d \n               The magic of gambling \n             People have been betting on future events for as long as they have played sports and raced horses. But in the latter half of the nineteenth century, US efforts to set betting odds through marketplace supply and demand became centralized on Wall Street, where wealthy New York City businessmen and entertainers were using informal markets to bet on US elections as far back as 1868. These political betting pools lasted into the 1930s, when they fell victim to factors such as stricter gambling laws and the rise of professional polling. But while they lasted they had an impressive success rate, correctly picking the winners of 11 out of 15 presidential races, and correctly identifying that the remaining 4\u00a0contests would have extremely tight margins. The prediction-market idea was revived by the spread of the Internet, which dramatically lowered the entry barriers for creating and participating in prediction markets. In 1988, the University of Iowa\u2019s Tippie College of Business launched the not-for-profit IEM as a network-based teaching and research tool; ahead of the 8\u00a0November presidential election that year, they set up a market to predict the fraction of votes that would go to each of the presidential candidates (see \u2018How a market predicts\u2019). The fractions changed daily as traders interpreted fresh information about polls, the economy and other issues. On the eve of the election, the market predicted that the Republican nominee, George H. W. Bush, would be victorious with 53.2% of the vote \u2014 which is exactly what he got. And in 2008, a study found that the IEM\u2019s predictions across five presidential elections were more accurate than the polls 74% of the time 4 . The success of the IEM helped to inspire the creation of dozens of other prediction markets. In 1996, for example, the Hollywood Stock Exchange was launched to forecast opening-weekend box-office take and other movie-related outcomes; its markets correctly predicted that  Hamlet  would be a flop that year and that  Jerry Maguire  would be a hit. In the early 2000s, employees of information-technology company Hewlett-Packard participated in prediction markets that beat the firm\u2019s official projections of quarterly printer sales 75% of the time. And in September 2002, six months beforethe US-led invasion of Iraq, the Dublin-based betting site TradeSports.com gained international notoriety when it ran a prediction market on when Iraqi dictator Saddam Hussein would be ousted. By the time the war began in March 2003, betters were 90% certain Hussein would be out by April and 95% sure he\u2019d be gone by May or June. He was deposed in April. \n               Market research \n             Prediction markets have also had some high-profile misfires, however \u2014 such as giving the odds of a Brexit \u2018stay\u2019 vote as 85% on the day of the referendum, 23\u00a0June. (UK citizens in fact narrowly voted to leave the European Union.) And prediction markets lagged well behind conventional polls in predicting that Donald Trump would become the 2016 Republican nominee for US president. Such examples have inspired academics to probe prediction markets. Why do they work as well as they do? What are their limits, and why do their predictions sometimes fail? Perhaps the most fundamental answer to the first question was provided in 1945 by Austrian economist Friedrich Hayek. He argued that markets in general could be viewed as mechanisms for collecting vast amounts of information held by individuals and synthesizing it into a useful data point \u2014 namely the price that people are willing to pay for goods or services. Economists theorize that prediction markets do this information gathering in two ways. The first is through \u2018the wisdom of crowds\u2019 \u2014 a phrase popularized by business journalist James Surowiecki in his book of that name (Doubleday, 2004). The idea is that a group of people with a sufficiently broad range of opinions can collectively be cleverer than any individual. An often-cited case is a game in which participants are asked to estimate the number of jelly beans in a jar. Although individual guesses are unlikely to be right, the accumulated estimates tend to form a bell curve that peaks close to the actual answer. When investor Jack Treynor ran this experiment on 56 students in 1987, their mean estimate for the number of beans\u00a0\u2014\u00a0871\u00a0\u2014\u00a0was closer to the correct answer of 850 than all but one of their guesses 5 . As Surowiecki and others have emphasized, however, crowds are wise only if they harbour a sufficient diversity of opinion. When they don\u2019t \u2014 when people\u2019s independent judgements are skewed by peer pressure, panic or even a charismatic speaker \u2014 the wisdom of crowds can easily fall prey to collective breakdowns. The housing bubble of the mid-2000s, which was a major contributor to the 2007\u201308 financial crash, was one such breakdown of judgement. But this is where the second market mechanism comes in. Sometimes called the marginal-trader hypothesis, it describes how \u2014 in theory \u2014 there will always be individuals seeking out places where the crowd is wrong. In the process, these traders will identify undervalued contracts to buy and overvalued contracts to sell, which tends to push prices back towards a sensible value. An example can be seen in the 2015 film  The Big Short , which dramatizes the true story of a hedge fund that bet against the irrational exuberance of the US housing market and gained substantially from the crash. Laboratory experiments have been used to test many aspects of this theoretical framework, including how well prediction markets aggregate information under different conditions. In a 2009 experiment 6  that was designed to mimic scientific research and publishing, researchers set up three prediction markets in which participants tried to predict which hypothesis about a fictitious biochemical pathway would end up being true. \n               Field-testing the future \n             In one market, key pieces of information about the pathway were available to all participants; the traders quickly converged on the correct answer. In another, analogous to proprietary corporate research, information was privately held by individuals; the traders often failed to reach a consensus. And in the third, analogous to results being discovered in different labs and then published in journals, information was initially kept private and then made public. The market was able to find the right answer \u2014 but the individuals who discovered useful information first could use their private knowledge to anticipate the markets and extract a small profit. One of the first prediction markets devoted exclusively to scientific questions grew out of a project started in 2011 by economist Robin Hanson of George Mason University in Fairfax, Virginia. Eventually known as SciCast, the project included a website where participants could wager on questions such as, \u201cWill there be a lab-confirmed case of the coronavirus Middle East Respiratory Syndrome (MERS or MERS-COV) identified in the United States by 1 June 2014?\u201d. (There was.) SciCast\u2019s assessments were more accurate than an uninformed prediction model 85% of the time (see  go.nature.com/2dm6Ilp ). SciCast was discontinued in 2015, when its funding ended. But it helped to inspire Metaculus, a market launched in November 2015 by Aguirre and his colleague Greg Laughlin, an astrophysicist now at Yale University in New Haven, Connecticut. The site grew out of Aguirre\u2019s interest in finding \u2018superpredictors\u2019 \u2014 people whose forecasting skills are far above average. Metaculus asks participants to estimate the probabilities of such things as, \u201cWill a clinical trial begin by the end of 2017 using CRISPR to genetically modify a living human?\u201d or \u201cWill the National Ignition Facility announce a shot at break-even fusion by the start of 2017?\u201d. As in SciCast, Metaculus participants do not use actual money: players instead move a slider representing their belief in the likelihood of an answer and accrue a track record for being correct. The lack of cash bets is partly a matter of practicality, says Aguirre. \u201cWhen it\u2019s \u2018Will Hillary win?\u2019, zillions of people will buy on that. But if it\u2019s \u2018Will this new paper on arXiv get more than ten citations?\u2019, you\u2019re not going to find enough people with real money to make an accurate prediction.\u201d But it\u2019s also the case that real money isn\u2019t strictly necessary for a successful prediction market: several studies 7 , 8  have shown that traders can be equally well motivated by the prestige of being right. Metaculus currently has around 2,000 active users, although its creators hope to accrue 10,000 or more. Already, the site has produced evidence that successful prediction is a skill that can be learned. The best players work out the optimal time to adjust their guesses up or down, and their performance gradually improves. Laughlin and Aguirre suggest that Metaculus could be useful to journalists and other members of the public who want to know which questions most interest scientists. Funding agencies might similarly be attracted to its results. \u201cHaving prediction markets that are getting an even-handed assessment is potentially a way of aiding the decision for what projects are most worth funding,\u201d says Laughlin. But scientific prediction markets have yet to gain much traction with researchers or the public. One important reason is that most political and business questions get clear-cut answers in relatively short time periods, and this is where prediction markets excel. But few would-be traders have the patience to endure the decades of effort, ambiguity and experimentation that are often required to answer questions in science. This problem is hardly unique to prediction markets, however: \u201cIt is in general easier to make short-term than long-term predictions,\u201d says Aguirre. As long as prediction markets offer a way to update guesses in light of new information, proponents argue, they will do as well or better than other forecasting methods. Scientific prediction markets also suffer more from ambiguity issues than do political or economic ones. In an election, one person is eventually declared the winner, whereas in science, resolutions are rarely so neat. But prediction-market advocates don\u2019t think that this is necessarily a cause for concern. \u201cWhen someone starts to suggest a bet, people immediately start to clarify what they mean,\u201d says Hanson. Aguirre says that he and Laughlin take great pains on Metaculus to ensure that predictions are well-defined and easy to understand. Whether prediction markets can work for science remains an open question. When Dreber\u2019s team repeated 18 economics experiments as part of a follow-up to her psychology investigation, both the prediction markets and surveys of individuals overestimated the odds of each study\u2019s reproducibility 9 . Dreber isn\u2019t sure why this happened. She points out that the psychologists in the first study were all already interested in replication \u2014 whereas the economists in the second were not involved in the reproducibility project \u2014 so they might have been better at collectively estimating reproducibility. Prediction markets in general still need to deal with challenges such as how to limit manipulation and overcome biases. Yet conventional representative polling, which once relied on answers from phonecalls to randomly sampled landlines, is being jeopardized by the movement to mobile phones and online messaging. Because the accuracy of prediction markets is at least on par with, if not better than, polls, economist David Rothschild of Microsoft Research in New York City thinks that prediction markets are well placed to take over if polling goes into decline. \u201cI can create a poll that can mimic everything about a prediction market,\u201d he says. \u201cExcept markets have a way of incentivizing you to come back at 2 a.m. and update your answer.\u201d \n                     Psychologists' betting market hints at most reliable research findings 2015-Nov-09 \n                   \n                     Over half of psychology studies fail reproducibility test 2015-Aug-27 \n                   \n                     Counting Google searches predicts market movements 2013-Apr-26 \n                   \n                     Replication studies: Bad copy 2012-May-16 \n                   \n                     The Reproducibility Project \n                   \n                     The Science Prediction Market Project \n                   \n                     The Iowa Electronic Markets \n                   \n                     Metaculus \n                   \n                     SciCast \n                   Reprints and Permissions"},
{"file_id": "537600a", "url": "https://www.nature.com/articles/537600a", "year": 2016, "authors": [{"name": "Sara Reardon"}], "parsed_as_year": "2006_or_before", "body": "Tensions between Cuba and the United States are easing. But researchers still struggle to join the scientific world. The western edge of Havana hides a side of Cuban society that tourists rarely see. High fences and thick vegetation wall off the grand estates and embassies where the elites congregate. And amid these enclaves of privilege lies a cluster of concrete buildings belonging to the Polo Cient\u00edfico del Oeste \u2014 the \u2018scientific pole\u2019 of Cuba\u2019s capital city. Here, a cluster of biotechnology research institutions are protected from the chaos and poverty of a city in transition. For a country whose entire gross domestic product (GDP) is just half of what the US government spends on research, Cuba punches above its weight in some areas of science. Fuelled by relatively generous government support, biomedical researchers have managed to excel at creating low-cost vaccines, developing cancer treatments and screening infants for disorders. Other areas of science get more meagre funding, but Cuba still boasts some bright spots. As the largest and most populous island in the Caribbean, it is a key node in international networks monitoring hurricanes and infectious-disease outbreaks. And because there is so little trade and tourism, the country has nearly pristine coral reefs and mangroves, which attract attention from researchers worldwide. The productivity and quality of some research in Cuba surprises those from other countries. \u201cWe had the same thought about Cuban science as everyone else did: that it was stuck back in  I Love Lucy  days,\u201d says Kelvin Lee, invoking the 1950s TV show. Lee, an immunologist at Roswell Park Cancer Institute in Buffalo, New York, is organizing the first US clinical trial of a Cuban vaccine. Yet the success stories don\u2019t outweigh the profound challenges facing scientists in Cuba. Research jobs pay poorly, and the number of students getting science doctorates has not risen in the past decade. Internet access is scarce, and those who have it find the service so sluggish that it can be next-to-impossible to e-mail a scientific paper. An energy shortage this summer forced government buildings to shut off their electricity for large portions of the week. During a temporary ban on air conditioning, scientists at the University of Havana sweltered over their laptops in 35 \u00b0C temperatures. Another problem looms above all others: the US trade embargo. For the past half-century, the embargo has severely restricted the ability of Cuban researchers to buy scientific equipment, win international grants and travel in the United States. But in December 2014, US President Barack Obama announced his intention to restore full relations between the two countries and began lifting travel restrictions. On 31 August 2016, a JetBlue Airways plane flew directly from Florida to Cuba \u2014 the first commercially scheduled flight between the two countries in five decades. This softening of relations has led to an era of evolution: it has opened up opportunities for researchers, such as easier travel to international meetings, and raises the prospect of many future benefits through collaborations and purchases. Yet the pace of progress has been much slower than many had hoped, and the future of US\u2013Cuba relations remains uncertain. A decision to lift the embargo entirely requires action from a hostile Congress and lies in the hands of the next US president. And in the meantime, Cuban researchers are stuck with many of the same problems as their counterparts in other developing nations: an exodus of young scientists, difficulty finding collaborators and an inability to afford increasingly specialized scientific equipment. This sets Cuba back years from where it could be, says Sergio Jorge Pastrana, executive director of the Academy of Sciences of Cuba. \u201cThe changes are coming but they change too slowly.\u201d \n               Old school \n             In the heart of Havana\u2019s Old Town, the academy is a cool, marble respite from the humidity. It is in the midst of remodelling: librarians sort century-old books of its proceedings, and Pastrana says that he plans to install solar panels. Outside, people pick their way along streets strewn with construction rubble while shops hawk Che Guevara shirts, cheap cigars and mass-produced paintings of cars. Like the colourful, iron-railed buildings that surround it, the science academy is a grand institution, the first of its kind established outside Europe. In its 155 years, it has hosted greats such as Albert Einstein and one of Cuba\u2019s most famous scientists, epidemiologist Carlos Finlay, who discovered that mosquitoes transmit yellow fever in the late 1800s. Until the revolution, the Cuban academy shared close ties with the US National Academy of Sciences and with its European counterparts. Even under the embargo restrictions, the organization has forged ties with US scientists at institutions such as the American Museum of Natural History and the American Association for the Advancement of Science (AAAS). Pastrana says that science got lucky when Fidel Castro took over (see \u2018 How Cuban science stacks up\u2019 ). Cuba could have ended up with \u201ca very bad leader for science\u201d, he says. Instead, one of Castro\u2019s first acts was to create and enforce a universal-literacy requirement, and he prioritized knowledge building and discovery. \u201cThe future of our country has to be necessarily a future of men of science,\u201d Castro said in a 1960 speech. The now-famous quote is engraved in Spanish on the wall of the science academy\u2019s lecture hall. The communist government also made health a top priority. Castro was unwilling to be left behind as other countries underwent a biotechnology revolution, especially as it became clear that the US embargo would cut the Cuban people off from modern treatments. In 1986, Cuba opened its Center for Genetic Engineering and Biotechnology (CIGB) in Havana\u2019s scientific pole. The CIGB now employs 1,600 people and has commercialized 21 products internationally, including cancer immunotherapies, a hepatitis B vaccine, pesticides and therapies for macular degeneration. \u201cI think they have accomplishments they can be quite proud of when adjusted to GDP and the size of the country,\u201d says Michael Clegg, a geneticist at the University of California, Irvine, and former foreign secretary of the US National Academy of Sciences. The investments in public health have paid off, as Cuba has logged impressive gains in recent decades. Cubans now live longer than Americans, on average, and infant mortality rates are comparable to those of the European Union and United States. Castro\u2019s dedication to science was tested in 1991 following the break-up of the Soviet Union, which had heavily subsidized its communist ally. Scientists in some sectors suffered during the 'special period of economic depression\u2019 over the next decade. Yet biotechnology continued to thrive through continued government support. In 1994, Cuba opened the Center of Molecular Immunology (CIM) in Havana to develop vaccines and other biologicals such as cancer immunotherapies. \u201cIn the middle of a big crunch of the national budget, money for science was respected,\u201d Pastrana says. Unlike many of its Latin American peers, Cuba has had a stable government for decades, which has allowed the country to carry out long-term plans. And the country\u2019s isolation from foreign aid spurred innovation in ways that few other low-income nations can claim. An often-heard verb is  resolver  \u2014 to fix problems, often on your own. \u201cMost Cuban scientists now didn't know Cuba before the blockade, and the fact you cannot get any device is so familiar,\u201d says Luis Montero Cabrera, a computational chemist at the University of Havana. There is something of a perverse pride: people who have spent their lives overcoming obstacles, he says, make the best scientists. \n               boxed-text \n             And they have had to scrape by with relatively little. In the past 6 years, Cuba has spent between 0.3% and 0.6% of its GDP on research and development annually \u2014 one of the highest rates in Latin America, but far less than Brazil (1.2%) and the United States (2.7%). And the government has yet to honour repeated promises to create a competitive grant-funding agency akin to the US National Science Foundation. The funding that is available can be strictly budgeted. Stem-cell biologist Porfirio Hern\u00e1ndez Ram\u00edrez at the Institute of Hematology and Immunology in Havana has ready access to patients and to state-paid clinicians who can serve as researchers. Yet he says he has no grant money to publish papers in open-access journals that charge thousands of dollars, so he mostly publishes in small domestic journals. This can make it difficult for outside researchers to evaluate or replicate his work, and to form collaborations \u2014 a common problem for Cuban scientists. \n               Tight controls \n             Like their counterparts in biomedicine, physicists at the University of Havana yoke their research to Cuba\u2019s national interests \u2014 namely, energy and biotechnology. Several work with BioCubaFarma, the state-run biotechnology agency based in Havana, to develop computational models for new drugs and biologicals. Osvaldo de Melo Pereira has built his own furnace for growing cadmium sulfide particles into nanowires that could serve as semiconductors in solar panels. He shows off images of the tangle of tubes and wires \u2014 images made by collaborators in France who have access to more-powerful microscopes. The research is still basic, de Melo Pereira admits, but he says that some of the particles show potential. The long-term goal of creating cheaper solar cells would be a boon for Cuba, which depends on its politically unstable Latin American trading partners for fossil fuels. University of Havana physicist Alejandro Lage-Castellanos says the focus on national priorities makes sense for researchers. \u201cIt\u2019s rational to join those industries that already have some success.\u201d The overall lack of funding makes it difficult for young scientists who want to pursue other lines of research. \u201cYou have to survive or you have to emigrate,\u201d he says. The government enforces such narrow pursuits. Graduate students, for instance, must defend their thesis not only to their department but also to a national tribunal that ensures the project will serve Cuba \u2014 a student would probably not be allowed to write a thesis on icebergs, for instance. To maximize scientific output, the tribunals also ensure that no two students in the country are working on the same topic. The government\u2019s tight control on science has actually provided some freedom in the biotech sector, where companies are run by the state and researchers don\u2019t worry about profitability. \u201cInstead of focusing on the market, we can focus on problems,\u201d says Eulogio Pimentel V\u00e1zquez, director-general of the CIGB. These can include particular genetic disorders that are common in Cuba, or problems associated with ageing \u2014 a growing concern, because 18% of the population is over the age of 60. The neurosciences centre CNEURO, for instance, is developing cognitive and biomarker tests that would allow earlier screening for Alzheimer\u2019s disease. And Cuban researchers say that the top-down approach allows for a more efficient process. At the CIGB, the structure of the suburban campus mirrors the research and development process, with basic research taking place on the top floor, research and engineering to scale up the operations downstairs, and production in nearby buildings. And the research process there is relatively inexpensive: labour on the island is cheap, and Cuban scientists, who are accustomed to frugality in their daily lives, carefully choose experiments and recycle items such as pipette tips that a wealthier laboratory would discard. \n               boxed-text \n             Cuban researchers take pride in their creative approaches. In 1970, for example, scientists at CNEURO decided that they wanted a primate research lab but had no money to buy the animals. So director Mitchell Vald\u00e9s-Sosa says he joined the crew of a cement steamer as the ship\u2019s doctor to get a free ride to St Kitts, where he picked up 25 vervet monkeys \u2014 which are regarded as an agricultural pest on the island. One monkey escaped when the ship docked near Santiago, and Vald\u00e9s-Sosa jumped overboard to rescue it. Now CNEURO has a colony of 50 monkeys that its scientists use for cognitive research. Vald\u00e9s-Sosa runs the centre along with his twin brother Pedro, who serves as vice-director. At CNEURO, they have made cheap translation of basic research a priority. Pedro is working on ways to obtain brain mapping from quantitative electroencephalography (qEEG) \u2014 a non-invasive measurement of brain activity that is much cheaper than magnetic resonance imaging (MRI) and many other scanning techniques. The centre has also developed a hearing aid for children that costs just US$2, a fraction of the cost in the United States or Europe. Physicians send scans of children\u2019s ears to CNEURO, where technicians create a structure for the implant using a 3D printer. The device can be inexpensively reprinted as the child\u2019s ear grows. \n               A tested country \n             Cuba is an enthusiastic user of medical testing, especially for newborns. In 2015, the World Health Organization declared Cuba the first country in the world to eliminate mother-to-child transmission of HIV \u2014 an achievement reached through the use of intensive screening and drugs for HIV-positive mothers. The Center for Immunoassays (CIE) manufactures much of the medical equipment used in the country, some of which is also used for research. Miguel Angel Garcia, director of science policy at the CIE, says the centre produces a total of 57 million tests per year for 19 different diseases, including HIV and Chagas disease. The facility saves money by doing everything in-house. Upstairs, researchers working in old fume hoods are developing better fluorescent markers for tests for Chagas disease. Next door, a machine shop grinds out metal sheets for spectrophotometers that will read the tests and spit out results. \u201cThe circumstances force us to integrate all that technology,\u201d Garcia says. The system also lowers the price: a glucose-monitoring system that costs only $0.40 may sell for 100 times more in the United States. \n               boxed-text \n             Although medical care is free for Cubans, the immunoassays centre and other branches of BioCubaFarma have been able to make a profit through exports. According to the Cuban government, international biotech sales netted the country $2.5 billion between 2008 and 2013 \u2014 a figure that the agency expects to double by 2018. Cuban scientists blame much of their difficulty in breaking into global science on the US economic sanctions. They are quick to correct foreigners who mention the embargo. \u201cYou say embargo, we say blockade,\u201d says Ileana Morales, director of science and technology at Cuba\u2019s public-health ministry. \u201cYou might say it\u2019s a play on words but it\u2019s a major impact.\u201d Cuba, in her opinion, is under a debilitating siege. The sanctions are legally complex, and include a prohibition on selling products to Cuba without the appropriate licence if more than 10% of the components are made in the United States \u2014 a cut-off raised to 25% in 2015. But the rules are so complex that many companies with US branches tend to play it safe by not selling to Cuba. The embargo creates constant problems for researchers, who complain about how long it takes to obtain reagents. Getting an enzyme from Europe, for instance, can take weeks. And some products aren\u2019t available at all. Researchers at BioCubaFarma want to buy a line of mice that are genetically engineered to lack three genes involved in Alzheimer\u2019s disease, but the animals are sold only by a US firm. International companies have reason to worry about the embargo. In 2009, the United States slapped a $130,000 fine on the domestic branch of Philips Electronics for selling medical equipment to Cuba, including half of the MRI machines at CNEURO. That caused the company to stop servicing its machines in Cuba, which meant the centre could not use those scanners. The problem didn\u2019t affect CNEURO\u2019s other machines, which came from Siemens, based in Munich, Germany. The US government says that the situation has since been resolved. \n               The big thaw \n             With the improved relations between the United States and Cuba, there are signs that some issues are getting easier for scientists. Later this year, researchers in the United States plan to begin the first clinical trials in the country of a Cuban therapy: a cancer vaccine called CimaVax that the CIM has been developing for two decades. Lee, the New York immunologist collaborating on the project, says he was initially surprised that the CIM had a lung-cancer vaccine, especially one that seems so effective \u2014 a 400-person study suggests that the vaccine can increase survival in people with lung cancer by a year (P. C. Rodriguez  et al .  Clin. Cancer Res.   22 , 3782\u20133790; 2016 ). CimaVax is approved in four Latin American countries, and approval is pending in several others, so Lee hopes that the US government approval process will move quickly. So far, it has been arduous. \u201cWe\u2019re sailing in uncharted waters,\u201d he says. But he has been surprised at how open the US government has been to the idea of working with Cuba. A few scientific fields have been doing this for some time. US and Cuban government agencies have been collaborating on hurricane forecasting since the 1950s. Because of the intense risk to the island, the government has provided strong support for weather radar systems, says Juan Carlos Antu\u00f1a-Marrero, an atmospheric scientist at the Meteorological Center of Camag\u00fcey. Antu\u00f1a-Marrero works with the University of Valladolid in Spain, which provides equipment for his group to measure atmospheric aerosols. And in 2014, US researchers were able to donate and install a Global Positioning System instrument in Camag\u00fcey as part of a Caribbean earthquake-monitoring network called CoCoNet. In addition to geodetic information, the device records meteorological data such as water vapour. \u201cOur team\u2019s research philosophy is to get as much as possible from the instrument,\u201d Antu\u00f1a-Marrero says. International researchers are flocking to Cuba to study its coral reefs and mangroves before hordes of US tourists arrive. These ecosystems are among the best preserved in the world, and the Cuban government has been proactive about creating protected areas, says Luis Sol\u00f3rzano, executive director of the Nature Conservancy\u2019s Caribbean programme, based in Coral Gables, Florida. But the influx of tourists could threaten coastal regions. Foreign researchers who visit the island to study those ecosystems find that many Cuban scientists are happy to show off the areas they\u2019ve been studying for so long. One of those looking forward to more international contacts is Alieny Gonz\u00e1lez Alfonzo, a graduate student working in an ornithology lab at the University of Havana. On the last day of classes before the summer break, she is one of the few people in her building as she finishes up some work. After nine years in the lab, she is getting ready to leave for the United States on an exchange programme later this year. But Gonz\u00e1lez Alfonzo plans to return to Cuba to finish her degree. Once she graduates, she will be one of only seven ornithologists in the country with a PhD. \u201cI want to contribute here,\u201d she says. \u201cThis is the best job in Cuba with birds.\u201d Gonz\u00e1lez Alfonzo is in a minority \u2014 more and more students seem to leave each year, says Montero Cabrera. According to data from the Network for Science and Technology Indicators \u2014 Ibero-American and Inter-American (RICYT), the number of science PhDs awarded annually has generally remained flat for the past decade. Montero Cabrera says that, in 2015 alone, 22 of the approximately 70 faculty members with PhDs in the University of Havana\u2019s chemistry department left the country for jobs overseas. Cuba is simply not an attractive market. With the exception of BioCubaFarma, science jobs at government institutions pay a fraction of labourers\u2019 and engineers\u2019 salaries. Few, if any, jobs in Cuba\u2019s small private sector make use of a science degree. Several US science associations, including the AAAS and the American Physical Society, have recently set up exchange programmes that will bring Cuban students to the United States for training. Europe already has many such programmes. And the relaxed US travel restrictions are already making it easier for Cuban scientists to attend international meetings. But unless Cuba\u2019s economy picks up, thawed relations may not be enough to boost Cuban science to world recognition, however good its scientists. \u201cNow young PhDs from Cuba have the opportunity to change everything here,\u201d Montero Cabrera says. \u201cBut they must find the resources.\u201d \n                     Cuban crocodiles pose conservation conundrum 2016-Sep-28 \n                   \n                     Mosquito guns and heavy fines: how Cuba kept Zika at bay for so long 2016-Aug-17 \n                   \n                     Cuba forges links with United States to save sharks 2015-Oct-21 \n                   \n                     A network to track Caribbean hazards 2013-May-22 \n                   \n                     Cuba's biotech boom 2009-Jan-07 \n                   \n                     Cuban science: \u00bfVive la revoluci\u00f3n? 2005-Jul-20 \n                   Reprints and Permissions"},
{"file_id": "538304a", "url": "https://www.nature.com/articles/538304a", "year": 2016, "authors": [{"name": "Ramin Skibba"}], "parsed_as_year": "2006_or_before", "body": "This year\u2019s US presidential election is the toughest test yet for political polls as experts struggle to keep up with changing demographics and technology. Hillary Clinton is heading for a landslide victory over Donald Trump. But wait. Trump is pulling ahead and could take the White House. No, Clinton has a clear lead and is gaining ground. Nearly every day, a new poll comes out touting a different result, leaving voters wondering what to believe. The results of recent elections give even more reason for scepticism. In 2013, the Liberal Party of Canada confounded expectations when it won the provincial elections in British Columbia. The following year, polls overestimated support for Democrats in the US congressional elections. And this year, some pollsters underestimated Britons\u2019 support for leaving the European Union in the  Brexit referendum . These blunders have led some political commentators to say that polls are headed for the graveyard. \u201cIt\u2019s harder and harder to find people willing to pay for any polls, given their poor performance this year and last year. They\u2019re heavily discredited in the UK,\u201d says Stephen Fisher, a political sociologist at the University of Oxford. As the US presidential election approaches, pollsters are scrambling to improve their methods and avoid another embarrassing mistake. Their job is getting harder. Until as recently as ten years ago, polling organizations were able to tap into public opinion simply by calling people at home. But large segments of the population in developed countries have given up their landlines for mobile phones. That is making them more difficult for pollsters to reach because people will often not answer calls from unfamiliar numbers. So the pollsters are fighting back. They are fine-tuning their efforts in reaching mobile phones, using statistical tools to correct for biases and turning to online surveys. The increasing number of online polls has prompted the formation of polling aggregates, such as FiveThirtyEight, RealClearPolitics and Huffington Post, which combine and average the results to develop more nuanced forecasts. \u201cPolling\u2019s going through a series of transitions. It\u2019s more difficult to do now,\u201d says Cliff Zukin, a political scientist at Rutgers University in New Brunswick, New Jersey. \u201cThe paradigm we\u2019ve used since the 1960s has broken down and we\u2019re evolving a new one to replace it \u2014 but we\u2019re not there yet.\u201d \n               Changing times \n             The ingredients of an accurate poll are fairly simple, but they can be hard to find, and everyone uses a different recipe to pull them all together. Start by recruiting a large group of people \u2014 preferably more than 1,000. The sample should be split evenly between women and men. And it should reflect the population\u2019s mix in terms of race, education, income and geographical distribution, to represent these groups\u2019 different views and voting behaviours. Once the data are in hand, pollsters analyse the gaps in their sample and weight the results to account for groups that are under-represented. \u201cPolling is an art, but it\u2019s largely a scientific endeavour,\u201d says Michael Link, president and chief executive of Abt SRBI polling firm in New York City and former president of the American Association for Public Opinion Research. Reporter Adam Levy finds out how to predict an election, and why the recipe isn\u2019t always easy to follow. It\u2019s also a process that is conducted behind closed doors. Polls are run by a mix of companies and academic groups, but they are generally commissioned by news organizations and political groups. As a result, pollsters rarely share the details of their techniques. \u201cThere\u2019s a lot of people who make a living doing this, and whose reputations are set on it,\u201d says Jill Darling, survey director at the University of Southern California\u2019s Center for Economic and Social Research in Los Angeles. The data-gathering part of polling used to be relatively easy in developed countries. Pollsters simply called people at home \u2014 at first, by hand, and later with automatic diallers in the United States. But landlines are quickly going the way of the telegraph (see \u2018The line on voters\u2019). In 2008, more than eight in every ten US households had landlines; by 2015, that number had dropped to five and it continues to decline. In the United Kingdom, more people have landlines but the fraction is dropping. As of this year, 53% of them claim that they never or rarely use them. The mobile revolution has hit pollsters hard in the United States because federal regulations require that mobile phones be called manually. And people often do not answer calls to their mobiles when an unfamiliar number pops up. In 1997, pollsters could get a response rate of 36% but that has dropped to just 10% or less now. As a result, pollsters are struggling to reach as many people, and costs are going up: each mobile-phone interview costs about twice as much as a landline one. There is also a \u2018non-response bias\u2019, because people who respond to pollsters\u2019 calls sometimes do not reflect a representative sample, says Frederick Conrad, head of the Program in Survey Methodology at the University of Michigan in Ann Arbor. Despite the expense and difficulty of calling people, this method still produces the most accurate results, says Courtney Kennedy, director of survey research at the Pew Research Center in Washington DC. US pollsters now call mobile phones for more than half of their samples, and that fraction will probably rise as more and more people ditch their landlines. Pollsters are also grappling with another major problem \u2014 predicting who will vote. That is likely to be unusually difficult in the United States this year because many voters aren\u2019t enamoured of  the leading candidates , who have historically low approval ratings. US national elections typically have turnouts of 40\u201355%, lower than most other developed countries, according to the Organisation for Economic Co-operation and Development. In the United Kingdom, by contrast, 60\u201370% of the eligible population usually votes. Richer, older, better-educated people, and those who voted in the previous election, are more likely to vote, but this varies with each election. Pollsters typically base their estimates of turnout on their own proprietary mix of factors such as respondents' voting history, whether they\u2019re registered with a political party, their engagement with politics, whether they say they're planning to vote, as well as demographics and socioeconomic factors. \u201c\u2018Likely voter\u2019 modelling is notoriously the secret-sauce aspect of polling,\u201d says Kennedy. It\u2019s also one of the most difficult parts of accurate polling. In the 2014 mid-term US election, most pollsters failed in their forecasts of Democratic voting. Turnout was just 36% \u2014 a record low in the past 70 years \u2014 which disproportionately depressed votes for Democratic candidates. In the  2015 UK general election , most major pollsters, including ICM Unlimited and YouGov, underestimated the turnout of older, Conservative Party voters, according to an inquiry published in March by the British Polling Council and Market Research Society 1 . The inquiry also found that pollsters have systematic biases in their samples. They tend to have too many Labour supporters at the expense of Conservative ones. They had applied weighting and adjustment procedures to the raw data, but this has not mitigated the bias problem. Another source of error identified in the report is \u201cherding\u201d \u2014 when pollsters consciously or unconsciously adjust their polls so that their results seem similar to those released earlier, causing the polls to converge. The bias in favour of left-leaning parties is not unique to the United Kingdom. The inquiry analysed more than 30,000 polls from 45 countries and found a similar, although smaller, bias. The report did not give an explanation for why, but some pollsters in the United States and Britain attribute the trend to inaccurate predictions of who will turn up to vote. In the case of the United Kingdom, the panel recommended that pollsters work to obtain more representative samples and to investigate better ways to weight them. Pollsters are also trying to improve their accuracy by changing how they model likely voters. In the past, they treated their sample in a binary fashion: determining how many would turn out on election day and how many would stay home. Now they tend to assign a probability to whether someone will vote. More transparency could help. Pollsters in the United Kingdom share their methodologies with the British Polling Council, which aided the recent investigation and has led to fruitful debates about ways to improve accuracy, says Fisher, who participated in the inquiry. \n               In data we trust \n             Even if polling organizations manage to collect a representative sample, they can\u2019t always trust the responses that people give them. One of the starkest examples in the United States came in the 1982 election for California\u2019s governor. Los Angeles Mayor Tom Bradley, an African American, was consistently leading in the polls but lost the election by a narrow margin. Afterwards, pollsters suggested that the discrepancy arose because some voters might not have wanted to admit that they would not support an African American candidate. This is now known as the \u2018Bradley effect\u2019. A variation on this is the \u2018shy Tory effect\u2019, named after Conservative-leaning voters in the United Kingdom who hide their views or misreport their intentions to pollsters. That makes some experts wonder whether a shy Trump effect might come into play in the forthcoming US election \u2014 in which a fraction of voters are embarrassed about or reluctant to admit their support for Trump or opposition to Clinton. But most major pollsters doubt that this will be a major factor because polls before the Republican primary elections gauged support for Trump accurately and he has performed similarly in online polls and in ones that use live interviews. Advanced technology may allow pollsters to get a better read on voters\u2019 true feelings. Online polls, for instance, allow people to respond at their convenience and state their intentions without fear of judgement from a live interviewer. They also make it easy to collect thousands of responses in a short time and at a lower cost: about US$30,000 for a 12-minute survey as opposed to more than US$70,000 for a similar telephone one, says Chris Jackson, vice-president at Ipsos Public Affairs, a global market-research and polling firm in Washington DC. But online polls have challenges, too. They typically recruit by advertising on popular websites, so people choose whether to participate, and that means that there might be a built-in bias in their samples. Pollsters don\u2019t exactly know who is missing from the poll, and it\u2019s harder to estimate the reliability of the final poll numbers. Some pollsters have begun experimenting with polls conducted through text messages. As with online polls, people can choose to respond whenever they want and avoid talking to a person. Michael Schober, a psychologist at The New School for Social Research in New York City, and his colleagues tested the differences between live and text interviews 2 . \u201cThe lack of time pressure and social pressure of texting leads people to disclose more information and be more honest,\u201d he says. Another approach is to assemble a panel of people to survey repeatedly. The most prominent is a University of Southern California Dornsife/ Los Angeles Times  Presidential Election tracking poll that launched in July. These pollsters randomly selected people on the basis of information from the US Postal Service and contacted them by mail, recruiting 3,000 people to participate each week in their online surveys. Unlike other polls, they need not continually recruit new respondents, and their response rate is at least 15% \u2014 higher than for telephone polls. The pollsters have enough data to know the demographics of their sample very well and can have confidence in their trends, says Darling, who leads the survey. However, if their sample turns out to be biased, then all polls for the duration of the sample will be biased. This may be the case with this year\u2019s poll, which leans slightly towards Trump, according to the aggregator FiveThirtyEight. To reduce the risk of bias, researchers are experimenting with a new type of poll. Andrew Gelman, a statistician and political scientist at Columbia University in New York City, and his colleagues have collected a very large set of people and divided them up into tens of thousands of demographic categories. The researchers tested this extreme categorization method on polling data from the 2012 US presidential election, showing that it produced accurate forecasts of state-level results by using highly tuned weights to correct for the non-representative sample 3 . However, this sophisticated method takes much more time and requires more detailed data than are usually gathered. It could be a glimpse of the future, however. \u2018Big data\u2019 are where more accurate results will come from, says Joe Twyman, head of political and social research for Europe, Middle East and Africa at YouGov. \u201cIt will be about linking a respondent\u2019s voting data with Internet usage, other survey data, and demographic information, creating a much richer picture of that person, which will allow for more accurate granulations of predictions,\u201d he says. Pollsters would use this information to assess who is likely to vote and to analyse the survey results \u2014 for example, by determining which issues most concern different voters. The low cost of Internet polling has triggered a surge in the number of polls of varying quality, making it hard for journalists, policymakers and others to separate the wheat from the chaff. Poll aggregators attempt to weight polls on the basis of the past reliability, but that doesn\u2019t guarantee future success, especially if low-quality and short-lived polling outfits are included in the mix. Contrary to bold claims of the death of polls, practitioners say that they are merely going through a transition. But pollsters do recognize that some of the barriers are insurmountable. As election seasons lengthen and people find more reasons to survey public opinion, the number of polls will continue to rise. Pollsters recognize that they can only ask so much of people, says Gelman. \u201cThere\u2019s a non-renewable resource of public trust.\u201d \n                     Polls apart 2015-May-13 \n                   \n                     Why the polls got the UK election wrong 2015-May-08 \n                   \n                     Statistics win in US election 2012-Nov-14 \n                   \n                     British polling inquiry report \n                   \n                     US AAPOR public opinion resources \n                   \n                     Pew election surveys \n                   \n                     FiveThirtyEight poll ratings \n                   Reprints and Permissions"},
{"file_id": "can-we-open-the-black-box-of-ai-1.20731", "url": "https://www.nature.com/news/can-we-open-the-black-box-of-ai-1.20731", "year": 2016, "authors": [], "parsed_as_year": "2011_2015", "body": "Dean Pomerleau can still remember his first tussle with the black-box problem. The year was 1991, and he was making a pioneering attempt to do something that has now become commonplace in autonomous-vehicle research: teach a computer how to drive. This meant taking the wheel of a specially equipped Humvee military vehicle and guiding it through city streets, says Pomerleau, who was then a robotics graduate student at Carnegie Mellon University in Pittsburgh, Pennsylvania. With him in the Humvee was a computer that he had programmed to peer through a camera, interpret what was happening out on the road and memorize every move that he made in response. Eventually, Pomerleau hoped, the machine would make enough associations to steer on its own. On each trip, Pomerleau would train the system for a few minutes, then turn it loose to drive itself. Everything seemed to go well \u2014 until one day the Humvee approached a bridge and suddenly swerved to one side. He avoided a crash only by quickly grabbing the wheel and retaking control. Back in the lab, Pomerleau tried to understand where the computer had gone wrong. \u201cPart of my thesis was to open up the black box and figure out what it was thinking,\u201d he explains. But how? He had programmed the computer to act as a 'neural network' \u2014 a type of artificial intelligence (AI) that is modelled on the brain, and that promised to be better than standard algorithms at dealing with complex real-world situations. Unfortunately, such networks are also as opaque as the brain. Instead of storing what they have learned in a neat block of digital memory, they diffuse the information in a way that is exceedingly difficult to decipher. Only after extensively testing his software's responses to various visual stimuli did Pomerleau discover the problem: the network had been using grassy roadsides as a guide to the direction of the road, so the appearance of the bridge confused it. Twenty-five years later, deciphering the black box has become exponentially harder and more urgent. The technology itself has exploded in complexity and application. Pomerleau, who now teaches robotics part-time at Carnegie Mellon, describes his little van-mounted system as \u201ca poor man's version\u201d of the  huge neural networks  being implemented on today's machines. And the  technique of deep learning , in which the networks are trained on vast archives of big data, is finding commercial applications that range from  self-driving cars  to websites that recommend products on the basis of a user's browsing history. It promises to become ubiquitous in science, too. Future radio-astronomy observatories will need deep learning to find worthwhile signals in their otherwise  unmanageable amounts of data ; gravitational-wave detectors will use it to understand and eliminate the tiniest sources of noise; and publishers will use it to scour and tag millions of research papers and books. Eventually, some researchers believe, computers equipped with deep learning may even display imagination and creativity. \u201cYou would just throw data at this machine, and it would come back with the laws of nature,\u201d says Jean-Roch Vlimant, a physicist at the California Institute of Technology in Pasadena. But such advances would make the black-box problem all the more acute. Exactly how is the machine finding those worthwhile signals, for example? And how can anyone be sure that it's right? How far should people be willing to trust deep learning? \u201cI think we are definitely losing ground to these algorithms,\u201d says roboticist Hod Lipson at Columbia University in New York City. He compares the situation to meeting an intelligent alien species whose eyes have receptors not just for the primary colours red, green and blue, but also for a fourth colour. It would be very difficult for humans to understand how the alien sees the world, and for the alien to explain it to us, he says. Computers will have similar difficulties explaining things to us, he says. \u201cAt some point, it's like explaining Shakespeare to a dog.\u201d Faced with such challenges, AI researchers are responding just as Pomerleau did \u2014 by opening up the black box and doing the equivalent of neuroscience to understand the networks inside. Answers are not insight, says Vincenzo Innocente, a physicist at CERN, the European particle-physics laboratory near Geneva, Switzerland who has pioneered the application of AI to the field. \u201cAs a scientist,\u201d he says, \u201cI am not satisfied with just distinguishing cats from dogs. A scientist wants to be able to say: 'the difference is such and such'.\u201d The first artificial neural networks were created in the early 1950s, almost as soon as there were computers capable of executing the algorithms. The idea is to simulate small computational units \u2014 the 'neurons' \u2014 that are arranged in layers connected by a multitude of digital 'synapses' (see  'Do AIs dream of electric sheep?' ) Each unit in the bottom layer takes in external data, such as pixels in an image, then distributes that information up to some or all of the units in the next layer. Each unit in that second layer then integrates its inputs from the first layer, using a simple mathematical rule, and passes the result further up. Eventually, the top layer yields an answer \u2014 by, say, classifying the original picture as a 'cat' or a 'dog'. The power of such networks stems from their ability to learn. Given a training set of data accompanied by the right answers, they can progressively improve their performance by tweaking the strength of each connection until their top-level outputs are also correct. This process, which simulates how the brain learns by strengthening or weakening synapses, eventually produces a network that can successfully classify new data that were not part of its training set. That ability to learn was a major attraction for CERN physicists back in the 1990s, when they were among the first to routinely use large-scale neural networks for science: the networks would prove to be an enormous help in reconstructing the trajectories of subatomic shrapnel coming out of particle collisions at CERN's  Large Hadron Collider . But this form of learning is also why information is so diffuse in the network: just as in the brain, memory is encoded in the strength of multiple connections, rather than stored at specific locations, as in a conventional database. \u201cWhere is the first digit of your phone number stored in your brain? Probably in a bunch of synapses, probably not too far from the other digits,\u201d says Pierre Baldi, a machine-learning researcher at the University of California, Irvine. But there is no well-defined sequence of bits that encodes the number. As a result, says computer scientist Jeff Clune at the University of Wyoming in Laramie, \u201ceven though we make these networks, we are no closer to understanding them than we are a human brain\u201d. To scientists who have to deal with big data in their respective disciplines, this makes deep learning a tool to be used with caution. To see why, says Andrea Vedaldi, a computer scientist at the University of Oxford, UK, imagine that in the near future, a deep-learning neural network is trained using old mammograms that have been labelled according to which women went on to develop breast cancer. After this training, says Vedaldi, the tissue of an apparently healthy woman could already 'look' cancerous to the machine. \u201cThe neural network could have implicitly learned to recognize markers \u2014 features that we don't know about, but that are predictive of cancer,\u201d he says. But if the machine could not explain how it knew, says Vedaldi, it would present physicians and their patients with serious dilemmas. It's difficult enough for a woman to choose a preventive mastectomy because she has a  genetic variant known to substantially up the risk of cancer . But it could be even harder to make that choice without even knowing what the risk factor is \u2014 even if the machine making the recommendation happened to be very accurate in its predictions. \u201cThe problem is that the knowledge gets baked into the network, rather than into us,\u201d says Michael Tyka, a biophysicist and programmer at Google in Seattle, Washington. \u201cHave we really understood anything? Not really \u2014 the network has.\u201d Several groups began to look into this black-box problem in 2012. A team led by Geoffrey Hinton, a machine-learning specialist at the University of Toronto in Canada, entered a computer-vision competition and showed for the first time that deep learning's ability to classify photographs from a database of 1.2 million images far surpassed that of any other AI approach 1 . Digging deeper into how this was possible, Vedaldi's group took algorithms that Hinton had developed to improve neural-network training, and essentially ran them in reverse. Rather than teaching a network to give the correct interpretation of an image, the team started with pretrained networks and tried to reconstruct the images that produced them 2 . This helped the researchers to identify how the machine was representing various features \u2014 as if they were asking a hypothetical cancer-spotting neural network, 'What part of this mammogram have you decided is a marker of cancer risk?\u201d Last year, Tyka and fellow Google researchers followed a similar approach to its ultimate conclusion. Their algorithm, which they called Deep Dream, starts from an image \u2014 say a flower, or a beach \u2014 and modifies it to enhance the response of a particular top-level neuron. If the neuron likes to tag images as birds, for example, the modified picture will start showing birds everywhere. The resulting images evoke LSD trips, with birds emerging from faces, buildings and much more. \u201cI think it's much more like a hallucination\u201d than a dream, says Tyka, who is also an artist. When he and the team saw the potential for others to use the algorithm for creative purposes, they made it available to anyone to download. Within days, Deep Dream was a viral sensation online. Using techniques that could maximize the response of any neuron, not just the top-level ones, Clune's team discovered in 2014 that the black-box problem might be worse than expected: neural networks are surprisingly easy to fool with images that to people look like random noise, or abstract geometric patterns. For instance, a network might see wiggly lines and classify them as a starfish, or mistake black-and-yellow stripes for a school bus. Moreover, the patterns elicited the same responses in networks that had been trained on different data sets 3 . Researchers have proposed a number of approaches to solving this 'fooling' problem, but so far no general solution has emerged. And that could be dangerous in the real world. An especially frightening scenario, Clune says, is that ill-intentioned hackers could learn to exploit these weaknesses. They could then send a self-driving car veering into a billboard that it thinks is a road, or trick a retina scanner into giving an intruder access to the White House, thinking that the person is Barack Obama. \u201cWe have to roll our sleeves up and do hard science, to make machine learning more robust and more intelligent,\u201d concludes Clune. Issues such as these have led some computer scientists to think that deep learning with neural networks should not be the only game in town. Zoubin Ghahramani, a machine-learning researcher at the University of Cambridge, UK, says that if AI is to give answers that humans can easily interpret, \u201cthere's a world of problems for which deep learning is just not the answer\u201d. One relatively transparent approach with an ability to do science was debuted in 2009 by Lipson and computational biologist Michael Schmidt, then at Cornell University in Ithaca, New York. Their algorithm, called Eureqa, demonstrated that it could  rediscover the laws of Newtonian physics  simply by watching a relatively simple mechanical object \u2014 a system of pendulums \u2014 in motion 4 . Starting from a random combination of mathematical building blocks such as +, \u2212, sine and cosine, Eureqa follows a trial-and-error method inspired by Darwinian evolution to modify the terms until it arrives at the formulae that best describe the data. It then proposes experiments to test its models. One of its advantages is simplicity, says Lipson. \u201cA model produced by Eureqa usually has a dozen parameters. A neural network has millions.\u201d Last year, Ghahramani published an algorithm that automates the job of a data scientist, from looking at raw data all the way to writing a paper 5 . His software, called Automatic Statistician, spots trends and anomalies in data sets and presents its conclusion, including a detailed explanation of its reasoning. That transparency, Ghahramani says, is \u201cabsolutely critical\u201d for applications in science, but it is also important for  many commercial applications . For example, he says, in many countries, banks that deny a loan have a legal obligation to say why \u2014 something a deep-learning algorithm might not be able to do 5 . Similar concerns apply to a wide range of institutions, points out Ellie Dobson, director of data science at the big-data firm Arundo Analytics in Oslo. If something were to go wrong as a result of setting the UK interest rates, she says, \u201cthe Bank of England can't say, 'the black box made me do it'\u201d. Despite these fears, computer scientists contend that efforts at creating transparent AI should be seen as complementary to deep learning, not as a replacement. Some of the transparent techniques may work well on problems that are already described as a set of abstract facts, they say, but are not as good at perception \u2014 the process of extracting facts from raw data. Ultimately, these researchers argue, the complex answers given by machine learning have to be part of science's toolkit because the real world is complex: for phenomena such as the weather or the stock market, a reductionist, synthetic description might not even exist. \u201cThere are things we cannot verbalize,\u201d says St\u00e9phane Mallat, an applied mathematician at the \u00c9cole Polytechnique in Paris. \u201cWhen you ask a medical doctor why he diagnosed this or this, he's going to give you some reasons,\u201d he says. \u201cBut how come it takes 20 years to make a good doctor? Because the information is just not in books.\u201d To Baldi, scientists should embrace deep learning without being \u201ctoo anal\u201d about the black box. After all, they all carry a black box in their heads. \u201cYou use your brain all the time; you trust your brain all the time; and you have no idea how your brain works.\u201d"},
{"file_id": "warning-to-forest-destroyers-this-scientist-will-catch-you-1.20730", "url": "https://www.nature.com/news/warning-to-forest-destroyers-this-scientist-will-catch-you-1.20730", "year": 2016, "authors": [], "parsed_as_year": "2011_2015", "body": "Ask Matthew Hansen to show off his data and he hunches over his computer like a possessed video gamer. With a few mouse clicks, he flies over the globe and zooms in on a forest in Indonesia. The area is designated as a preserve \u2014 supposedly protected from deforestation \u2014 but Hansen's data reveal a different reality. Bird's-eye images of the trees taken every eight days flash by on the screen. At first, a few red spots perforate the green canopy around the preserve's edge. Then they spread, like bloodstains. \u201cThat's got to be illegal fires,\u201d he says. \u201cThe forest is getting chewed up.\u201d Hansen is among the world's foremost forest sentries. In 2013, he and his colleagues used satellite data to produce the first global, high-resolution maps of where trees are growing and disappearing 1 . Those images revealed some large-scale patterns for the first time, such as that Indonesia had nearly equalled Brazil as the country with the world's highest rate of tropical deforestation. Since then, his team has refined its methods and can now reveal the loss of trees within days. Just as important is what Hansen does with the underlying data. Unlike some scientists, he makes them freely available online, giving activists, companies and others the ability to monitor activities such as illegal logging and mining, which have destroyed millions of hectares of forest per year over the past few decades. The data have enabled non-governmental organizations (NGOs) and officials in Peru, Congo and other nations to see deforestation as it happens. And they let countries monitor each other's trees \u2014 potentially a crucial step in enforcing the international climate agreement signed in Paris last December. But some have argued that the maps do not always work as advertised. For instance, they lump together destruction of natural forests and the harvesting of managed ones, which critics say leads to inflated estimates of deforestation. And others question whether satellites can monitor forest loss and growth accurately enough to determine how well  countries are complying with their commitments  on climate change and deforestation, including the Paris deal. One thing no one disputes is that Hansen is showing the world how mapping from the sky can have an impact on the ground. \u201cIf you want to know what's up, you look at what Matt's doing,\u201d says Martin Herold, a remote-sensing expert at Wageningen University in the Netherlands. \u201cNobody's even close.\u201d Hansen instantly disarms people with his down-to-earth nature. On an unseasonably warm day earlier this year, he was wearing shorts and a short-sleeved shirt when his assistant reminded him that he was due at a meeting. \u201cI'm not dressed for that at all,\u201d he laughed as he set off across the campus of the University of Maryland in College Park. His informality helps when working with both African farmers and Hollywood actors, with whom he mingles as easily as with other scientists and policy wonks. But beneath the casual exterior is an intensity that has made Hansen one of the world's most sought-after experts on forests. Growing up in Indiana surrounded by farm fields, Hansen did not spend a lot of time among trees. But he was struck by trips to the state's few remaining patches of original hardwood forest, which reminded him of Lothl\u00f3rien, the sylvan kingdom of the elves in  The Lord of the Rings . He studied electrical engineering at university and then was accepted into law school, but neither stoked his passion. What did excite him was adventure, and he got plenty of it when he headed to what was then Zaire (now the Democratic Republic of the Congo) to volunteer with the Peace Corps. But when he returned, he still had no clear career direction. \u201cI came back and I thought, what do I like? I like maps,\u201d he says. So he went to the University of North Carolina in Charlotte for master's degrees in geography and civil engineering. He took a job at the University of Maryland in 1994 and has been mapping land-cover change using satellite data ever since, picking up a PhD in 2002. Hansen has pursued a single goal: to map global land cover with the highest possible resolution using cheap or free data, to better visualize the human footprint on the planet. He has specialized in writing programs to identify diverse types of vegetation \u2014 from boreal conifers to palm plantations \u2014 using the handful of light frequencies that satellite sensors collect. \u201cHe's an exceptionally good geographer,\u201d says long-time colleague Thomas Loveland of the US Geological Survey in Sioux Falls, South Dakota. \u201cHe really has an understanding of what this planet's made of.\u201d Hansen and his colleagues also meticulously 'ground-truth' their maps by picking random samples of GPS points and getting to them by any means necessary. \u201cIt's his favourite type of vacation, to throw random points on ground and go visit them,\u201d says his postdoc Alexandra Tyukavina. In the mid-1990s, when Hansen was starting, the best information about tree cover came from country-level ground-based assessments, in which crews measured individual trees in representative plots and then extrapolated across large regions. Such measurements were \u2014 and still are \u2014 used alongside remote-sensing data by the Food and Agriculture Organization of the United Nations (FAO) in its periodic global forest assessments. But many countries lack the resources to conduct regular surveys, and others publish statistics that seem unreliable. So Hansen set his sights on producing what he calls a \u201cglobally consistent, locally relevant product\u201d from data available to everyone in the world. But first he had to wait for technology \u2014 sensors in space and computer processing power on the ground \u2014 to catch up. The first global land-cover map from the University of Maryland came out 2  in 1994, using data from the Advanced Very High Resolution Radiometer (one of a series of orbiting imagers operated by the US National Oceanic and Atmospheric Administration). It had enormous pixels of one degree latitude by one degree longitude, much too coarse to make out details of forests. A big step forward came when NASA launched its two Moderate Resolution Imaging Spectroradiometer (MODIS) instruments, which gather data at a resolution of up to 250 metres. In 2008, Hansen and his colleagues produced a map 3  that started to reveal large-scale trends in the tropics, such as that nearly half of widespread humid tropical-forest loss between 2000 and 2005 occurred in Brazil. Around that time, scientists working for both the Brazilian government and local NGOs used MODIS and other data sources to develop their own maps and issue alerts when large clearings appeared. This helped officials to use financial pressure, law enforcement and other means to  dramatically reduce deforestation in the Amazon , the world's largest and most carbon-rich tropical-forest region. That success inspired Hansen. But in many other tropical countries, rising consumer demand for commodities such as cattle, soya beans and palm oil has created powerful incentives to  clear tropical forests . And in poorer countries, where heavy tree-felling equipment is rare and clearings tend to be small, MODIS's blocky images have proved less useful. Hansen knew that he needed to make his maps sharp enough to show roads snaking their way into previously untouched forests \u2014 an almost universal harbinger of larger clear-cutting. \u201cWe had to push the spatial resolution because we're interested in humans,\u201d he says. In fact, the data that he needed already existed. Since 1972, Landsat satellites had been collecting images of Earth's surface, starting at a resolution of 80 by 80 metres per pixel and improving to 30 metres in 1982 \u2014 roughly the size of two basketball courts side-by-side. But those images had to be bought individually, at costs from hundreds to thousands of dollars each \u2014 much too expensive for a global study. That changed in 2008, when the US government made all Landsat images free, including 3.6 million archived ones. Hansen immediately began making 30-metre-resolution maps showing how tree cover was changing in regions of interest, such as Indonesia and parts of Russia. But making a global map still required processing power out of reach of any university computer cluster. A solution appeared when Hansen met Google engineer Rebecca Moore at a conference in Brazil. Moore was looking for scientists to try out her Earth Engine, a platform to analyse remote-sensing data using Google's cloud-computing capabilities. Hansen and Moore's teams processed the Landsat archive back to 2000 and translated it into annually updated maps that anybody with a computer and an Internet connection could view. \u201cMatt was the first scientist who really leapt onto the platform with a global-scale analysis,\u201d Moore says. In 2013, Hansen, Moore, Loveland and others published 1  their results in  Science , showing where trees had appeared or disappeared every year from 2000 to 2012. The maps lit up the research community, which for the first time could see the world's forests shift in one consistent picture (see  'Better eyesight in space' ). The fact that Hansen put his raw data on the web for others to scrutinize and use has also drawn admiration. But it didn't take long for the critics to chime in. Many have objected to Hansen's use of 'forest', which he defines to include oil-palm plantations and agroforestry, categories not included in FAO data sets. That made his deforestation estimates higher than many previous ones, such as the FAO's. The widespread publicity has further stoked concerns that non-experts are ill-equipped to interpret the data. \u201cI personally think the data set was in some sense oversold,\u201d says Herold. Hansen's visibility added to the scientific scrutiny. On the day that his  Science  paper was published, for example, he was in California showing his maps to actor Harrison Ford in a scene filmed for the 2014 US television series 'Years of Living Dangerously'. Ford later confronted Indonesia's forestry minister with some of the findings. Other concerns have emerged. Some drier forests, such as those in parts of Africa and South America, have relatively sparse tree cover and might never reach the threshold that Hansen uses to define forest, which is that 30% of a pixel is occupied by vegetation at least 5 metres tall. So when those areas are cleared, the change might not register as forest loss, says Peter Holmgren, director of the Center for International Forestry Research in Bogor, Indonesia. Satellites struggle even more to capture forest gain, he adds, because the signal from growing trees is subtler than that of trees falling. For these and other reasons, he has warned against using Hansen's data to assess progress towards international climate and deforestation commitments, arguing that nations should instead invest in on-the-ground monitoring systems. Hansen acknowledges that his maps do not supply everything. \u201cYou can't fit everybody's needs,\u201d he says. But his team is working to add data and make improvements that will show what activities are causing forests to change, and will differentiate plantations from natural forests. \u201cThat's what we have to do next, to make it more valuable.\u201d Some of the objections have been more political. Hansen's map was particularly embarrassing for Indonesia because it came out during the 2013 UN climate talks, and revealed that deforestation rates in the country had spiked after a 2011 moratorium on new logging permits was announced. Indonesia's forestry ministry countered that Hansen and his colleagues were including large areas that the government had designated as plantation, unfairly overstating the deforestation. Hansen's group responded the following year with a more sophisticated analysis 4 , which confirmed that, in 2012, more primary tropical forest had fallen in Indonesia than in any other country. For Hansen, the country's refusal to come clean about its forests is frustrating. But increasing transparency will take time, says Belinda Margono, a scientist with the Indonesian Ministry of Forestry who earned her PhD with Hansen and led the follow-up study by his group. She says that the maps have already helped to set that shift in motion, by promoting a culture of data sharing and openness, and by creating pressure to respond. \u201cSometimes the government has more courage to release the data after they see what's reported by the global system.\u201d Larger forces are also at work. Nations and corporations are under increasing pressure to show that they are conserving forest to meet commitments under the Paris agreement or in sustainability-certification programmes for products such as palm oil. Since his 2013 paper, Hansen has become a globe-trotting door-to-door salesman of sorts, hawking his maps to forest ministers, corporate accountability officers, NGOs and others who need to keep an eye on forests. As almost 200 nations were hammering out the climate deal in Paris last December, Hansen was nearby, receiving a glowing introduction before he spoke at an environmental conference. \u201cMatt and his team ushered in really a new era of measuring deforestation,\u201d said Frances Seymour, a forest-policy researcher at the Center for Global Development in Washington DC. \u201cHe's now immortalized because everybody talks about the Matt Hansen data on tree-cover change.\u201d Hansen is now working to push his technique even further. Inspired by Brazil's alerts, he has begun processing and displaying data on tree loss as it happens in Peru, Congo, parts of Indonesia and Brazil. In the few months since the alerts went public, Peruvian environmental ministry personnel have used them to expose and shut down an illegal gold-mining operation. The alerts' very existence can have an impact, says remote-sensing scientist Fred Stolle of the World Resources Institute in Washington DC, which is releasing them weekly on its Global Forest Watch online platform. \u201cPeople know now that they can be seen from space.\u201d Hansen hopes  to expand his alerts  to the whole tropics by the end of the year, and later to cover the globe. The European Space Agency's Sentinel-2 satellites, which will collect data starting next year with a resolution of up to 10 metres, will enable him to update even more frequently. Between the travel and the research, Hansen keeps a hectic schedule. But on a rare quiet afternoon, he can explore the world's forests from his desk on the edge of the Maryland campus. As he pans over Peru, a sea of green gives way to a rectangular island of pink that has grown during the past two years. \u201cSomeone went out there and clear-cut that,\u201d he says. The view that Hansen has opened up, of trees falling all over the world, does not always reflect the best in people. \u201cIt's fucking alarming,\u201d he says. \u201cThe human footprint is amazing. We are a rapacious species.\u201d But making that view available to everyone, he says, could help to rein our species in. \u201cI hope it will bring some order to the chaos.\u201d"},
{"file_id": "538443a", "url": "https://www.nature.com/articles/538443a", "year": 2016, "authors": [], "parsed_as_year": "2006_or_before", "body": "A special issue explores how the research enterprise keeps early-career scientists from pursuing the most important work, and what can be done to help. Young researchers aspire to explain the world and fix its problems \u2014 but they are increasingly finding their ambitions thwarted. The challenge comes from the scientific enterprise itself. Grants and permanent positions are scarce, and administrative burdens can be crushing. Scientists, when judged on steady funding or publication streams, pursue projects that are certain to produce a \u2018publishable unit\u2019 even if they do not address big questions. Ultimately, this is bad for science and scientists alike. Two researchers \u2013 separated by a couple of decades \u2013 describe their experiences of beginning a scientific career. This issue attempts to give young scientists a voice. An  infographic  helps to explain the pressures: funding for basic science has been level or is declining in most countries, but the number of scientists seeking funds has grown. This means that scientists are putting time into preparing grant applications with a lower chance of pay-off. That burden is felt most acutely by young scientists, who often lack administrative experience and resources. A  News Feature  tells the stories of three researchers as they have fought to establish their labs, highlighting the relentless struggle to secure grants and publish more papers. What\u2019s to be done? In a  Comment , four researchers recognized for their innovation propose ways of enabling scientists to pursue promising ideas. They recommend accommodating shifts in research focus, and accepting the temporary interruption in research outputs that such pivots entail. And two sets of academic administrators  describe efforts to wean promotion committees  away from an undue focus on paper counts. The Careers section  presents voices from three erstwhile researchers who have each left the bench to fix the scientific enterprise. But as  an Editorial  argues, it is up to everyone \u2014 young and old \u2014 to ensure that the next generation of scientists is not lost. \n                     http://www.nature.com/nature/careers/index.html \n                   Reprints and Permissions"},
{"file_id": "539156a", "url": "https://www.nature.com/articles/539156a", "year": 2016, "authors": [{"name": "Elie Dolgin"}], "parsed_as_year": "2006_or_before", "body": "Three things are needed to turn the tide on the costliest crisis in health care. There are not a lot of things that could bring together people as far apart on the US political spectrum as Republican Newt Gingrich and Democrat Bob Kerrey. But in 2007, after leading a three-year commission that looked into the costs of care for elderly people, the political rivals came to full agreement on a common enemy: dementia. At the time, there were fewer than 30\u00a0million people worldwide diagnosed with the condition, but it was clear that the numbers were set to explode. By 2050, current predictions suggest, it could reach more than 130\u00a0million, at which point the cost to US health care alone from diseases such as Alzheimer\u2019s will probably hit US$1\u00a0trillion per year in today\u2019s dollars. \u201cWe looked at each other and said, \u2018You know, if we don\u2019t get a grip on Alzheimer\u2019s, we can\u2019t get anything done because it\u2019s going to drown the system,\u2019\u201d recalls Gingrich, the former speaker of the US House of Representatives. He still feels that sense of urgency, and for good reason. Funding has not kept pace with the scale of the problem; targets for treatments are thin on the ground and poorly understood; and more than 200 clinical trials for Alzheimer\u2019s therapies have been terminated because the treatments were ineffective. Of the few treatments available, none addresses the underlying disease process. \u201cWe\u2019re faced with a tsunami and we\u2019re trying to deal with it with a bucket,\u201d says Gingrich. But this message has begun to reverberate around the world, which gives hope to the clinicians and scientists. Experts say that the coming wave can be calmed with the help of just three things: more money for research, better diagnostics and drugs, and a victory \u2014 however small \u2014 that would boost morale. \u201cWhat we really need is a success,\u201d says Ronald Petersen, a neurologist at Mayo Clinic in Rochester, Minnesota. After so many failures, one clinical win \u201cwould galvanize people\u2019s interest that this isn\u2019t a hopeless disorder\u201d. \n               Cost calculations \n             Dementia is the fifth-biggest cause of death in high-income countries, but it is the most expensive disease to manage because patients require constant, costly care for years. And yet, research funding for dementia pales in comparison with that for many other diseases. At the US National Institutes of Health (NIH), for example, annual funding for dementia in 2015 was only around $700\u00a0million, compared with some $2\u00a0billion for cardiovascular disease and more than $5\u00a0billion for cancer. One problem is visibility. Other disease communities \u2014 most notably, people affected by breast cancer and HIV/AIDS \u2014 have successfully advocated for large pots of dedicated research funding. But \u201cthere simply wasn\u2019t any comparable upswell of attention to Alzheimer\u2019s\u201d, says George Vradenburg, chair and co-founder of UsAgainstAlzheimer\u2019s, a non-profit organization in Chevy Chase, Maryland. The biggest reason, he says, is that \u201cthe victims of the disease hide out\u201d. Dementia mostly affects elderly people and is often misconstrued as a normal part of ageing; there is a stigma attached to the condition, and family care-givers are often overworked and exhausted. Few are motivated enough to speak up. However, social and political awareness has increased in the past five years. \u201cWe all started to work together a lot more, and that helps,\u201d says Susan Peschin, chief executive at the Alliance for Aging Research in Washington DC, one of more than 50 non-profit groups in the Accelerate Cure/Treatments for Alzheimer\u2019s Disease coalition. The impact can be seen in government investments. France took action first, creating a national plan for Alzheimer\u2019s in 2008 that included \u20ac200 million (US$220\u00a0million) over five years for research. In 2009, the German Centre for Neurodegenerative Diseases in Bonn was created with a \u20ac66-million annual budget. And UK spending on dementia research more than doubled between 2010 and 2015, to \u00a366\u00a0million (US$82\u00a0million). The European Union has been dishing out tens of millions of euros each year for dementia studies through the Innovative Medicines Initiative and the Joint Programming process, and Australia is now about halfway through doling out its Aus$200-million (US$150-million), five-year dementia-research fund. \u201cThis is a global challenge, and no one country will be able to solve the problem,\u201d says Philippe Amouyel, a neurologist and geneticist at the University Hospital of Lille in France. Yet it\u2019s the United States that has been the biggest backer by far, thanks in part to efforts by Gingrich and Kerrey. The NIH\u2019s annual budget for Alzheimer\u2019s and other dementias jumped in the past year to around $1\u00a0billion, and there is support for a target to double that figure in the next few years \u2014 even in the fractious US political landscape. \u201cAlzheimer\u2019s doesn\u2019t care what political party you\u2019re in,\u201d says Kerrey. Two billion dollars is \u201ca reasonable number\u201d, says Petersen, who chairs the federal advisory board that  came up with the target in 2012 . Now, he adds, the research community just needs to work out \u201cwhat are we going to do with it if in fact we get it?\u201d. The answer could depend in large part on the fate of a drug called solanezumab, developed by Eli Lilly of Indianapolis, Indiana. This antibody-based treatment removes the protein amyloid-\u03b2, which clumps together to form sticky plaques in the brains of people with Alzheimer\u2019s. By the end of this year, Lilly is expected to announce the results of a 2,100-person clinical trial testing whether the drug can slow cognitive decline in people with mild Alzheimer\u2019s. It showed preliminary signs of cognitive benefit in this patient population in earlier trials ( R. S. Doody  et al .  N. Engl. J. Med.    370,  311\u2013321; 2014 ), but the benefits could disappear in this final stage of testing, as has happened for practically every other promising compound. No one is expecting a cure. If solanezumab does delay brain degradation, at best it might help people to perform 30\u201340% better on cognitive tests than those on a placebo. But even such a marginal gain would be a triumph. It would show scientists and the drug industry that a disease-modifying therapy is at least possible. By contrast, another setback could bring recent momentum in therapeutic development to a halt. \u201cThis is a fork in the road,\u201d says John Hardy, a neurogeneticist at University College London. \u201cThis is going to be a very important outcome, way beyond the importance for Lilly and this particular drug.\u201d On a scientific level, success for solanezumab could lend credence to the much-debated amyloid hypothesis, which posits that the build-up of amyloid-\u03b2 in the brain is one of the triggers of Alzheimer\u2019s disease. The previous failure of amyloid-clearing agents led many to conclude that plaques were a consequence of a process in the disease, rather than the cause of it. But those in favour of the amyloid hypothesis say that the failed drugs were given too late, or to people with no amyloid build-up \u2014 possibly those with a different form of dementia. For its latest solanezumab trial, Lilly sought out participants with mild cognitive impairment, and used brain scans and spinal-fluid analyses to confirm the presence of amyloid-\u03b2 in their brains. Another company, Biogen in Cambridge, Massachusetts, took the same approach to screening participants in a trial of its amyloid-targeting drug aducanumab. Earlier this year, a  165-person study reported  early signs that successfully clearing amyloid-\u03b2 with the Biogen therapy correlated with slower cognitive decline ( J.\u00a0Sevigny  et al .  Nature   537,  50\u201356; 2016 ). If those results hold up to further scrutiny, \u201cthat will at least tell us that amyloid is sufficiently upstream in the cascade that it deserves being targeted and tackled pharmacologically\u201d, says Giovanni Frisoni, a clinical neuroscientist at the University of Geneva in Switzerland who is involved in the drug\u2019s testing. \n               To defeat, delay \n             Although debate over the amyloid hypothesis continues, interest is growing in earlier intervention with drugs that clear the protein. Reisa Sperling, a neurologist at Brigham and Women\u2019s Hospital in Boston, Massachusetts, worries that even mild dementia is a sign of irreparable brain-cell death. \u201cYou can suck all the amyloid out of the brain or stop it from further accumulating, but you\u2019re not going to grow those neurons back.\u201d That is why she is leading Anti-Amyloid Treatment in Asymptomatic Alzheimer\u2019s, or A4, a $140-million, placebo-controlled solanezumab study that aims to treat people with elevated amyloid levels before they show any signs of cognitive impairment. And A4 is not her only trial. In March, she and neurologist Paul Aisen of the University of Southern California\u2019s Alzheimer\u2019s Therapeutic Research Institute in San Diego launched a trial in 1,650 asymptomatic people with early signs of amyloid-\u03b2 build-up. It will test a pill from Johnson & Johnson that blocks \u03b2-secretase, an enzyme responsible for producing the toxic protein. These interventions are known as secondary prevention because they target people who are already developing amyloid plaques. Sperling and Aisen also plan to test what\u2019s called primary prevention. In August, they received NIH funding to start treating people who have normal brain levels of amyloid-\u03b2 and no signs of cognitive decline, but who have a high risk of developing Alzheimer\u2019s \u2014 because of  a combination of factors such as age and genetics . \u201cThe biggest impact we can have is in delaying the onset of the diseases,\u201d says David Holtzman, a neurologist at Washington University School of Medicine in St. Louis, Missouri, and an investigator in the Dominantly Inherited Alzheimer Network, which is testing the benefits of giving either solanezumab or another anti-amyloid therapy to people who inherit gene mutations that predispose them to develop Alzheimer\u2019s at an early age. Secondary prevention could eventually mean screening everyone past middle age for signs of amyloid-\u03b2, although the current testing methods are either expensive ($3,000 brain scans) or invasive (spinal taps). Researchers have flagged a dozen possible blood-based biomarkers, but none has yet panned out, says Dennis Selkoe, a Brigham and Women\u2019s Hospital neurologist. Yet a cheap and easy diagnostic test for amyloid-\u03b2 could ultimately prove unnecessary. In the same way that some have suggested giving cholesterol-lowering drugs to anyone at risk of heart disease, clinicians might eventually give anti-amyloid drugs to a broad set of people prone to Alzheimer\u2019s \u2014 even if they are not already amyloid positive, says Sperling. \n               Target practice \n             Just as cholesterol is not the sole cause of heart disease, amyloid-\u03b2 is not the only driver of Alzheimer\u2019s. There\u2019s also tau, a protein that causes tangles in the brains of most people with Alzheimer\u2019s. Several pharmaceutical companies are targeting tau, but few large drug-makers have clinical candidates directed at other types of target. \u201cThey know how to modulate a specific target and keep looking under that lamp post, rather than venturing away from their comfort zones,\u201d says Bernard Munos, an industry consultant and former Eli Lilly executive. That\u2019s a problem, says Howard Fillit, chief science officer of the Alzheimer\u2019s Drug Discovery Foundation in New York City. \u201cWe really need to increase the diversity of targets we\u2019re tackling.\u201d After amyloid and tau, the only target receiving much attention from researchers is neuro\u00adinflammation \u2014 the \u201cthird leg of the stool\u201d in treating Alzheimer\u2019s, according to neuro\u00adgeneticist Rudy Tanzi at Massachusetts General Hospital in Boston. He likens Alzheimer\u2019s disease to a wildfire in the brain. Plaques and tangles provide the initial brush fires, but it\u2019s the accompanying neuro\u00adinflammation that fans the flames. Once the blaze is raging, Tanzi says, \u201cputting out those brush fires that got you there isn\u2019t good enough\u201d. This could explain why anti-amyloid drugs failed when given to people with full-blown dementia. For these individuals, perhaps reducing the inflammatory activity of brain immune cells called microglia could help. Drug researchers are now focusing on two genes,  CD33  and  TREM2 , that are involved in microglial function. But, says Tanzi, \u201cthere are two dozen other genes that deserve attention. Who knows if one of these new genes that no one is working on might lead to drug clues?\u201d \n               Alternative avenues \n             Many Alzheimer\u2019s experts emphasize the need to develop better low-cost interventions that don\u2019t require drug research. At the University of New South Wales in Sydney, Australia, for example, geriatric psychiatrist Henry Brodaty is testing whether an Internet coaching tool that focuses on diet, exercise, cognitive training and mood can postpone disease development. \u201cWe know that two-thirds of the world\u2019s dementia is going to be in developing countries,\u201d he says (see \u2018The approaching wave\u2019). Lifestyle interventions, he argues, could be more broadly scalable than expensive drugs. Researchers also need to look beyond Alzheimer\u2019s, to the many other types of dementia. Injuries to the vessels that supply blood to the brain cause a form called vascular dementia. Clumps of a protein called \u03b1-synuclein underlie cognitive problems in people with Parkinson\u2019s disease and also what\u2019s called Lewy body dementia. Tau deposits are often behind the nerve-cell loss responsible for frontotemporal dementia. And there are many other, equally devastating, drivers of serious mental decline. \u201cWe should not be ignoring these other diseases,\u201d says Nick Fox, a neurologist at University College London, especially given that many types of dementia share biological mechanisms. Tackling one disease could help inform treatment strategies for another. But perhaps the biggest hindrance to drug development today is more logistical than scientific, with clinical trials for dementia taking years to complete as investigators struggle to recruit sufficient numbers of study participants. \u201cWe need to get answers more quickly,\u201d says Marilyn Albert, director of the Johns Hopkins Alzheimer\u2019s Disease Research Center in Baltimore, Maryland. One solution is trial-ready registries. By enrolling people who are interested in taking part in a study before it actually exists, investigators can start a trial as soon as a drug comes along for testing. \u201cWe have to register humanity in the task of defeating this disease,\u201d says Aisen. The 1,600-person COMPASS-ND registry is being funded through the Canadian Consortium on Neurodegeneration in Aging. Member Serge Gauthier, a neurologist at McGill University in Montreal, says that finding participants can be challenging. But he adds that around one-third of the people who come to memory clinics such as his have what\u2019s known as subjective cognitive impairment \u2014 they might forget names or suffer from other \u2018senior moments\u2019, but they do not meet the clinical definition of dementia. They are perfect for trial-ready registries, says Gauthier: they are at an elevated risk of the disease, and they\u2019ve demonstrated concern. Gauthier wants to find more people like them. He fits the profile himself, so he joined the Brain Health Registry, which has more than 40,000 participants so far and is led by researchers at the University of California, San\u00a0Francisco. He takes regular cognitive tests, and could be asked to do more once potential diagnostic tools or therapies are ready for testing. \u201cIt\u2019s a fun thing to do,\u201d he says. Voluntarily or not, people will need to face up to dementia, because in just a few short decades, pretty much everyone is going to have a friend or loved one affected by the disease. It\u2019s an alarming idea, and it should spur action, says Robert Egge, chief public policy officer of the Alzheimer\u2019s Association in Chicago, Illinois. \u201cWe know where we\u2019re heading,\u201d he says. \u201cThe question is: are we going to get in front of it or not?\u201d \n                 Tweet \n                 Follow @NatureNews \n               See  Books & Arts . \n                     Neuroscience: Tide of forgetting 2016-Nov-09 \n                   \n                     The red-hot debate about transmissible Alzheimer's 2016-Mar-16 \n                   \n                     Alzheimer\u2019s research takes a leaf from the prion notebook 2015-May-29 \n                   \n                     Medical research: Treat ageing 2014-Jul-23 \n                   \n                     Alzheimer's disease: The forgetting gene 2014-Jun-04 \n                   \n                     Diseases: Study neuron networks to tackle Alzheimer's 2013-Nov-06 \n                   \n                     Neuroscience: My life with Parkinson's 2013-Nov-06 \n                   \n                     Nature  Outlook: Alzheimer\u2019s disease \n                   \n                     Nature  Insight: Neurodegenerative diseases \n                   Reprints and Permissions"},
{"file_id": "538444a", "url": "https://www.nature.com/articles/538444a", "year": 2016, "authors": [{"name": "Brendan Maher"}, {"name": "Miquel  Sureda Anfres"}], "parsed_as_year": "2006_or_before", "body": "Young researchers are having to fight harder than past generations for a smaller share of the academic pie. Scientists and policymakers around the world increasingly worry about the plight of young researchers in academia, and for good reason. Competition for tenure-track positions has surged, and some early-career researchers face tough odds in the quest for funding. As a result, many see lower pay-offs for their efforts in preparing and writing grant applications. Although everyone is under pressure, those just starting out seem to feel the impacts more acutely. Two researchers \u2013 separated by a couple of decades \u2013 describe their experiences of beginning a scientific career. \n                     Young, talented and fed-up: scientists tell their stories 2016-Oct-26 \n                   \n                     Let researchers try new paths 2016-Oct-26 \n                   \n                     Fewer numbers, better science 2016-Oct-26 \n                   \n                     How to build a better PhD 2015-Dec-02 \n                   \n                     The future of the postdoc 2015-Apr-07 \n                   \n                     Life outside the lab: The ones who got away 2014-Sep-03 \n                   \n                     The missing piece to changing the university culture 2013-Oct-08 \n                   \n                     Nature  Special: Young scientists \n                   \n                     OECD Stat \n                   \n                     NIH Extramural Nexus \n                   \n                     RCUK success rates \n                   \n                     DFG success rates \n                   \n                     ERC statistics \n                   Reprints and Permissions"},
{"file_id": "538446a", "url": "https://www.nature.com/articles/538446a", "year": 2016, "authors": [{"name": "Kendall Powell"}], "parsed_as_year": "2006_or_before", "body": "Scientists starting labs say that they are under historically high pressure to publish, secure funding and earn permanent positions \u2014 leaving precious little time for actual research. Martin Tingley was coming undone. It was late autumn 2014, just over a year into his assistant-professor job at Pennsylvania State University in State College, and he was on an eight-hour drive home after visiting his wife in Boston. He was stressed, exhausted and close to tears. As the traffic zipped past in the dark hours of the early morning, the headlights gave him the surreal feeling that he was inside a video game. Usually, Tingley thought of himself as a \u201cpretty stoic guy\u201d \u2014 and on paper, his career was going well. He\u2019d completed a master\u2019s degree in statistics and a PhD in Earth science, both at Harvard University. With these, and four years of postdoctoral experience, he had landed a rare tenure-track faculty position. He thought he would soon be successfully combining statistics and climate science to produce the type of interdisciplinary research that funding agencies say they want. In fact, scientific life was proving tough. He found himself working 60\u201380 hours per week doing teaching and research. His start-up funding had run out, he had yet to secure a major grant and, according to a practice common in US academia, he would not be paid by his university for three summer months. His wife had not been able to move with him, so he was making tiring weekend commutes. It seemed that the pressures had reached unsustainable levels. Something had to give. Tingley is one of many young scientists who are deeply frustrated with life in research. In September,  Nature  put a post on Facebook asking scientists who were starting their first independent position to tell us about the challenges that they faced. What followed was a major outpouring of grief. Within a week, nearly 300 scientists from around the world had responded with a candid catalogue of concerns. \u201cI see many colleagues divorcing, getting burnt out, moving out of science, and I am so tired now,\u201d wrote one biomedical researcher from Belgium (see \u2018Suffering in science\u2019).  Nature  selected three young investigators who voiced the most common frustrations; here, we tell their stories. \n               boxed-text \n             But are young scientists whining \u2014 or drowning? Our interviewees acknowledge that they are extremely fortunate to have an opportunity to direct their own creative, stimulating careers, and they are hardly the only professionals who are expected to work hard. It\u2019s easy for each generation to imagine that things are more difficult for them than they were in the past. But some data and anecdotal evidence suggest that scientists  do face more hurdles in starting research groups now  than did many of their senior colleagues 20\u201330 years ago. Chief among those challenges is the unprecedented number competing for funding pools that have remained stagnant or shrunk in the past decade. \u201cThe number of people is at an all-time high, but the number of awards hasn\u2019t changed,\u201d says Jon Lorsch, director of the US National Institute of General Medical Sciences (NIGMS) in Bethesda, Maryland. \u201cA lot of people with influence on the system recognize this is a serious problem and are trying to fix it.\u201d Two researchers \u2013 separated by a couple of decades \u2013 describe their experiences of beginning a scientific career. Young scientists and senior scientists alike feel an acute pressure to publish and are weighed down by a growing bureaucratic burden, with little administrative support. They are largely judged on their record of publishing and of winning grants \u2014 but without clear targets, they find themselves endlessly churning out paper after paper. The crucial question is whether this is harming science and scientists. Bruce Alberts, a prominent biochemist at the University of California, San Francisco, and former president of the US National Academy of Sciences, says that it is. The current hyper-competitive atmosphere is stifling creativity and pushing scientists \u201cto do mediocre science\u201d, he says \u2014 work that is safe and uninteresting. \u201cWe\u2019ve got to reward people who do something differently.\u201d Our informal survey suggests that the situation is already making research an unwelcoming career. \u201cFrankly, the job of being a principal investigator and running a lab just looks horrible,\u201d wrote one neuroscientist from the United States. Tingley wouldn\u2019t disagree. \n               Funding fight \n             Tingley has always had broad interests. At university in Canada, he switched from art history to physics. For his graduate studies, he was drawn to the vibrant research environment at Harvard, in Cambridge, Massachusetts, where he built statistical methods that helped to make sense of data on past climate gathered from sources such as tree rings and ice cores. By the time he was searching for academic positions, he was already working 60-hour weeks, he says: he would be at work by 8\u2009a.m., go home for dinner, and then pull out his laptop again at night. But by 2013, his research was hitting a high: he had published a statistical analysis in  Nature 1  and, after applying for jobs worldwide, was offered a joint appointment in meteorology and statistics at Penn State. By this point, his wife, Gabrielle, ran the communications programme for Harvard\u2019s Research Computing centre in Cambridge. Positions offered to her at Penn State fell far short of her qualifications, and she opted to stay where she was. They were facing the \u2018two-body problem\u2019 \u2014 a long-standing stress point for scientists. Like many first-year assistant professors, Tingley immediately felt pressure to publish in top journals, attract funding and students, and innovate in the classroom. He also knew that his roughly US$200,000 in start-up funding from the university \u2014 to cover his summer salary, computing access and more \u2014 wouldn\u2019t last long, and he applied to the US National Science Foundation for grants. That process was \u201cheartbreaking\u201d, he says. In one instance, he put in a proposal with his collaborator, organic geochemist Jessica Tierney at the University of Arizona in Tucson, for work on proxies for past sea surface temperatures. On the first round of review, the application got two scores of \u201cexcellent\u201d and two of \u201cvery good\u201d, yet it still fell short of being funded. The two were encouraged to resubmit, which they did. On the next round, the proposal scored worse. \u201cPart of it is on me, I was unsuccessful,\u201d Tingley says \u2014 but the anecdote shows the frustration that young scientists face when trying to get a research programme off the ground. \u201cThe funding cycle is brutal.\u201d In the meantime, the pair published the initial stages of the work 2  in an article that has been cited 40 times. The views of scientists who responded to  Nature  revealed a generational divide: many feel that today\u2019s senior investigators experienced a more comfortable trajectory in science and now have a competitive advantage. The \u2018baby boom\u2019 scientists, who have longer track records and well-established labs, are in a stronger position to win funds. (In September,  Nature  asked on Twitter: \u201cWhat are the challenges facing young scientists?\u201d \u201cOld scientists,\u201d one respondent shot right back.) In December 2014, shortly after his low point in the car, Tingley and his wife took a month-long trip to Australia and Indonesia for some much-needed time together. The next month, Tingley returned to the winter chill at State College and walked across campus feeling as if his head was scraping against the low-hanging clouds. He knew that much of his time was about to be sucked up teaching two advanced courses, leaving little time for research, and he would be back to the tiring commute to see his wife at the weekends. If he didn\u2019t get a grant soon, he would have no summer salary. \u201cMy wife and I knew this wasn\u2019t a sustainable way for us to live our lives.\u201d Tingley started googling around late at night, and in March, he spied the perfect job posting. Insurance Australia Group in Sydney was looking for someone with experience in meteorology, statistics and climate. He started there two months later, and his wife easily found a position in communications with the University of New South Wales. Now a senior research analyst, Tingley models and quantifies risks from bush fires, cyclones and other storms. The transcontinental move was not without its difficulties, of course \u2014 and as a young researcher moving to the private sector, he\u2019s had to prove himself all over again. Tingley now advises others to recognize that there are various paths to a successful career. \u201cIt\u2019s perfectly legitimate to use your training and skill set in the private sector.\u201d He isn\u2019t missing the stress and high expectations placed on young investigators\u2019 shoulders, he says. On a sunny spring Saturday in September, he and his wife head out for a walk on their neighbourhood beach. \u201cIt turns out that weekends are fantastic,\u201d he says. \n               Internal pressure \n             Sometimes, pressures come not from chasing funding or tenure, but from chasing an ideal of what makes a good scientist. Young researchers from all disciplines told  Nature  that they wrestle with the lack of clear expectations for success \u2014 and materials scientist Eddie L\u00f3pez-Honorato is one. He grew up in Mexico City and studied chemistry there, at the National Autonomous University of Mexico, but for his PhD, he struck out for the University of Manchester, UK. He worked at night and at weekends to complete his experiments, he says, which became more difficult after his son was born. He found it stressful, but his time at Manchester gave him high working standards that he now tries to emulate. Next, he did a postdoctoral fellowship at the Institute for Transuranium Elements in Karlsruhe, Germany, where he worked on developing safer coatings for nuclear fuels used in reactors. At the end of his postdoc, he had the opportunity to return to the United Kingdom as a lecturer at the University of Sheffield, but he and his wife, Paola, yearned to go back to Mexico. They weighed up the pros and cons. L\u00f3pez-Honorato knew that he would need to build up his professional reputation in Mexico and that the science infrastructure there was less developed than in Europe. But he thought that working in the United Kingdom would be harder for his family, because they faced constant changes in language and culture. The family chose Mexico. In March 2012, L\u00f3pez-Honorato started at the Center for Research and Advanced Studies of the National Polytechnic Institute (CINVESTAV) in Ramos Arizpe. He felt an amazing sense of independence and potential on standing in front of his brand new empty lab space. \u201cYou know that you have to get some students and money fast, really fast, and that\u2019s when the urge to work kicks in,\u201d he says. Although the government paid his and his students\u2019 salaries, he still needed to secure funds to support his research. He sent out a flurry of grant proposals for government funding, without success. L\u00f3pez-Honorato spent 2012 travelling around Mexico and the United States to build collaborations. He cold e-mailed other scientists to explain his work. The grants started trickling in. By 2014, he had secured enough to cover most of his research expenses and had established a second arm to his lab\u2019s work: developing adsorptive materials to remove arsenic from drinking water, a problem that affected nearly half of all wells in certain parts of Mexico 3 . Since starting at CINVESTAV, he has published 20 research papers and has built up a lab group of 15 people. Like many of those interviewed, he says that the work to sustain funding is as tough as winning the first grants. Even though his position is secure, he feels the pressure of maintaining his research projects and launching the careers of younger scientists. \u201cIt\u2019s stressful when you don\u2019t have money, and stressful when you do have money, because then you have to deliver. It\u2019s my fault if anything goes wrong.\u201d He points to a recent eight-month bureaucratic delay in purchasing a coating machine that is essential to his nuclear-fuel work; it put the project a year behind schedule, and he feels that he is to blame. Many scientists, like other professionals, say that there aren\u2019t enough hours in the day. (\u201cMy cohort, we feel exhausted,\u201d said one Generation X scientist, who asked to remain anonymous to protect his career.) In the past two months, L\u00f3pez-Honorato says, he has averaged four hours of sleep per night. He and other early-career researchers are \u201cin a stage where our kids and partners need us the most at home\u201d, he says. His second son is now eight months old. He wrestles with whether he has valid reasons to complain, and knows the pressures are largely self-generated. \u201cIt\u2019s a problem of saying, \u2018That\u2019s enough\u2019,\u201d he says. It\u2019s an issue that many young investigators struggle with \u2014 when you\u2019re the one setting the goals, when do you have enough money, students or publications? Philip Guo, a cognitive scientist at the University of California, San Diego, described in a 2014 blogpost how academics often feel as if they are on an accelerating treadmill. In his previous work as a software engineer at Google, Guo said, he had \u201ctremendous clarity about what and how much I was expected to do\u201d. Academics, however, have obligations to teach, advise, do research, write grants and support departments, universities and the academic community \u2014 and \u201cnone of these sources of work know of or care about one another\u201d. Alberts highlights the young investigators who need two major grants, one to supply their salary and one for their research programme. \u201cIt\u2019s horrible pressure on young people. How are they going to be excellent at anything? The incentives are all wrong.\u201d This year, L\u00f3pez-Honorato is trying to lower his own expectations, applying for only one industry grant \u2014 compared with the seven he applied for in 2012 \u2014 in the hope that he\u2019ll get home in time to play with his boys. But that internal pressure is hardest to quell. \u201cWe want to be the best \u2014 that\u2019s how we got to the job we have right now. It\u2019s a personal pressure. But that\u2019s even more difficult to get rid of.\u201d \n               No time to think \n             Computing always attracted Felienne Hermans, who taught herself programming at age 10. She specialized in computer science at university and pursued a PhD at Delft University of Technology in the Netherlands. There, she applied methods of software engineering to spreadsheets, so that end users such as accountants or biologists would have better ways of maintaining and annotating their data 4 . The creative work won her top conference papers, which are key for advancement in this field. When a tenure-track position opened up in her research group of four professors, she asked whether she could apply. She beat internal and external candidates and started as an independent professor in March 2013, at the age of just 28. Two years into the position, Hermans was feeling overwhelmed. She was grappling with the responsibilities of managing her two graduate students and one postdoc, prepping for teaching courses, and what felt like endless \u2018service\u2019 requests to review papers for journals and colleagues. The spreadsheet work had in some ways run its course, and she wanted to pivot to a more stimulating research area. But the pressure to publish continuously and copiously dogged her. Her job is formally split between 40% teaching, 40% research and 20% academic service, but the message is that research should trump everything else. \u201cFour papers are better than three. And five are better than four,\u201d she says. Like Alberts, she says the idea that research output is now synonymous with publication quashes all creativity. \u201cPapers are just one form of communicating ideas and experiments.\u201d She yearns \u201cfor an afternoon of looking out the window and thinking, \u2018What will I do next?\u2019\u201d. Another barrier has been constant throughout her career: being a woman in an overwhelmingly male-dominated field. In 2014, she attended the Code Generation hands-on programming conference in Cambridge, UK, and found herself 1 of only 2 women among roughly 100 attendees. She spent the three days speaking to colleagues about this sad statistic, rather than about her programming, as she would have preferred. \u201cIt drags you down and drains your energies,\u201d she says. In the survey,  Nature  received roughly a dozen comments from young scientists who indicated that sexism, gender bias or lack of support for women held back their careers. Hermans eventually developed a fresh research focus through her Saturday volunteer work at a community centre, where she taught programming to inner-city kids. She and a colleague began thinking about how best to teach the children. Rather than just explaining how to make a robot move forward, say, they wanted to communicate how to maintain code quality through properly naming program features and avoiding \u2018code smells\u2019, or poorly designed program sections. The pivot wasn\u2019t totally smooth \u2014 her first conference paper about a generic theory for code smells was rejected for not having enough supporting evidence, but now she is hitting her stride. Looking back, Hermans says that she probably should have ignored the pressure to publish, and ruminated more. \u201cBut I was new in the tenure track and super scared about not being able to pay my mortgage in two years.\u201d Now, she keeps more careful track of her time. If a colleague knocks on her door for help with a student\u2019s paper, she can turn them down: \u201cI\u2019ve already done my 20% to service.\u201d She\u2019s rearranged her week, cramming teaching, grant writing and service into Monday to Thursday so that she can spend Fridays with her lab group, which now comprises six people. There are more-organized moves to help young investigators \u2014 to win grants, for example. Alberts says that \u201cthere has to be a shift of resources to the younger people\u201d. He points to the European Research Council grant programme that divides applicants into three career stages \u2014 Starter (2\u20137 years post-PhD), Consolidator (7\u201312 years post-PhD) and Advanced (more than 12 years post-PhD) \u2014 so that applicants from each career stage compete with their peers. In the same vein, this year the NIGMS piloted a grant called Maximizing Investigators\u2019 Research Award, which separates early-stage investigators from established ones, and offers five years of guaranteed funding. That\u2019s an innovation in the US funding system, says Lorsch, because it means no longer \u201ccomparing apples and oranges\u201d. And Lorsch says that older investigators should be encouraged to move into alternative stages of their career \u2014 working in teaching, mentoring and science advocacy \u2014 that don\u2019t require research funds. This could help younger researchers to break in. Other scientists vehemently oppose such ideas. And Alberts, like many senior scientists, doesn\u2019t see the problem as solely based on age. \u201cIt\u2019s not about fairness. It\u2019s about how to get the best science for the dollar. We\u2019ll get much better science by funding young or old people to do innovative things.\u201d Hermans is acutely aware that the grumbles of young scientists can be brushed away. \u201cIf people are complaining about an injustice, it\u2019s easy to say they are just moaning,\u201d she says. \u201cBut these are not imaginary problems.\u201d She feels it\u2019s her duty to be vocal about the challenges facing young investigators. \u201cExperienced researchers should be observing if a young scientist is failing and asking, \u2018Are you overwhelmed? Why aren\u2019t you inspired?\u2019\u201d Lorsch says that he knows first-hand that Generation X scientists are not whiners: \u201cI do not hear complaining from the people who are trying to get their first grant or renew their first grant, the people trying to get a lab running,\u201d he says. \u201cIt\u2019s the really well-funded people who\u2019ve lost one of their grants \u2014 that\u2019s who call me and scream.\u201d \n                 Tweet \n                 Facebook \n                 LinkedIn \n                 Weibo \n                 Wechat \n               \n                     Let researchers try new paths 2016-Oct-26 \n                   \n                     Fewer numbers, better science 2016-Oct-26 \n                   \n                     Young scientists under pressure: what the data show 2016-Oct-26 \n                   \n                     How to build a better PhD 2015-Dec-02 \n                   \n                     The retirement debate: Stay at the bench, or make way for the next generation 2015-May-06 \n                   \n                     The future of the postdoc 2015-Apr-07 \n                   \n                     Life outside the lab: The ones who got away 2014-Sep-03 \n                   \n                     'Extreme' workloads plague scientists at the start of their careers 2014-Jan-22 \n                   \n                     Nature Careers \n                   Reprints and Permissions"},
{"file_id": "539020a", "url": "https://www.nature.com/articles/539020a", "year": 2016, "authors": [{"name": "Erica Gies"}], "parsed_as_year": "2006_or_before", "body": "With prices for renewables dropping, many countries in Africa might leap past dirty forms of energy towards a cleaner future. At the threshold of the Sahara Desert near Ouarzazate, Morocco, some 500,000 parabolic mirrors run in neat rows across a valley, moving slowly in unison as the Sun sweeps overhead. This US$660-million solar-energy facility opened in February and will soon have company. Morocco has committed to generating 42% of its electricity from renewable sources by 2020. Across Africa, several nations are moving aggressively to develop their solar and wind capacity. The momentum has some experts wondering whether large parts of the continent can vault into a clean future, bypassing some of the environmentally destructive practices that have plagued the United States, Europe and China, among other places. \u201cAfrican nations do not have to lock into developing high-carbon old technologies,\u201d wrote Kofi Annan, former secretary-general of the United Nations, in a report last year 1 . \u201cWe can expand our power generation and achieve universal access to energy by leapfrogging into new technologies that are transforming energy systems across the world.\u201d That's an intoxicating message, not just for Africans but for the entire world, because electricity demand on the continent is exploding.  Africa's population is booming  faster than anywhere in the world: it is expected to almost quadruple by 2100. More than half of the 1.2 billion people living there today lack electricity, but may get it soon. If much of that power were to come from coal, oil and natural gas, it could kill  international efforts to slow the pace of global warming . But a greener path is possible because many African nations are just starting to build up much of their energy infrastructure and have not yet committed to dirtier technology. Several factors are fuelling the push for renewables in Africa. More than one-third of the continent's nations get the bulk of their power from hydroelectric plants, and droughts in the past few years have made that supply unreliable. Countries that rely primarily on fossil fuels have been troubled by price volatility and increasing regulations. At the same time, the cost of renewable technology has been dropping dramatically. And researchers are finding that there is more potential solar and wind power on the continent than previously thought \u2014 as much as 3,700 times the current total consumption of electricity. This has all led to a surging interest in green power. Researchers are mapping the best places for renewable-energy projects. Forward-looking companies are investing in solar and wind farms. And governments are teaming up with international-development agencies to make the arena more attractive to private firms. Yet this may not be enough to propel Africa to a clean, electrified future. Planners need more data to find the best sites for renewable-energy projects. Developers are wary about pouring money into many countries, especially those with a history of corruption and governmental problems. And nations will need tens of billions of dollars to strengthen the energy infrastructure. Still, green ambitions in Africa are higher now than ever before. Eddie O'Connor, chief executive of developer Mainstream Renewable Power in Dublin, sees great potential for renewable energy in Africa. His company is building solar- and wind-energy facilities there and he calls it \u201can unparalleled business opportunity for entrepreneurs\u201d. \n               Power problems \n             Power outages are a common problem in many African nations, but Zambia has suffered more than most in the past year. It endured a string of frequent and long-lasting blackouts that crippled the economy. Pumps could not supply clean water to the capital, Lusaka, and industries had to slash production, leading to massive job lay-offs. The source of Zambia's energy woes is the worst drought in southern Africa in 35 years. The nation gets nearly 100% of its electricity from hydropower, mostly from three large dams, where water levels have plummeted. Nearby Zimbabwe, South Africa and Botswana have also had to curtail electricity production. And water shortages might get worse. Projections suggest that the warming climate could reduce rainfall in southern Africa even further in the second half of the twenty-first century. Renewable energy could help to fill the gap, because wind and solar projects can be built much more quickly than hydropower, nuclear or fossil-fuel plants. And green-power installations can be expanded piecemeal as demand increases. Egypt, Ethiopia, Kenya, Morocco and South Africa are leading the charge to build up renewable power, but one of the biggest barriers is insufficient data. Most existing maps of wind and solar resources in Africa do not contain enough detailed information to allow companies to select sites for projects, says Grace Wu, an energy researcher at the University of California, Berkeley. She co-authored a report 2  on  planning renewable-energy zones  in 21 African countries, a joint project by the Lawrence Berkeley National Laboratory (LBNL) in California and the International Renewable Energy Agency (IRENA) in Abu Dhabi. The study is the most comprehensive mapping effort so far for most of those countries, says Wu. It weighs the amount of solar and wind energy in the nations, along with factors such as whether power projects would be close to transmission infrastructure and customers, and whether they would cause social or environmental harm. \u201cThe IRENA\u2013LBNL study is the only one that has applied a consistent methodology across a large region of Africa,\u201d says Wu. High-resolution measurements of wind and solar resources have typically been done by government researchers or companies, which kept tight control of their data. The Berkeley team used a combination of satellite and ground measurements purchased from Vaisala, an environmental monitoring company based in Finland that has since made those data publicly available through IRENA's  Global Atlas for Renewable Energy . The team also incorporated geospatial data \u2014 the locations of roads, towns, existing power lines and other factors \u2014 that could influence decisions about where to put energy projects. \u201cIf there's a forest, you don't want to cut it down and put a solar plant there,\u201d says co-author Ranjit Deshmukh, also an energy researcher at Berkeley. The  amount of green energy  that could be harvested in Africa is absolutely massive, according to another IRENA report 3 , which synthesized 6 regional studies and found potential for 300 million megawatts of solar photovoltaic power and more than 250 million megawatts of wind (see 'Power aplenty'). By contrast, the total installed generating capacity \u2014 the amount of electricity the entire continent could produce if all power plants were running at full tilt \u2014 was just 150,000 megawatts at the end of 2015. Solar and wind power accounted for only 3.6% of that. The estimate of wind resources came as a surprise, says Oliver Knight, a senior energy specialist for the World Bank's Energy Sector Management Assistance Program in Washington DC. Although people have long been aware of Africa's solar potential, he says, as of about a decade ago, few local decision-makers recognized the strength of the wind. \u201cPeople would have told you there isn't any wind in regions such as East Africa.\u201d The World Bank is doing its own studies, which will  assess wind speeds and solar radiation  at least every 10 minutes at selected sites across target countries. It will ask governments to add their own geospatial data, and will combine all the information into a user-friendly format that is freely available and doesn't require advanced technical knowledge, says Knight.\u201cIt should be possible for a mid-level civil servant in a developing country to get online and actually start playing with this.\u201d \n               South Africa leads \n             In the semi-arid Karoo region of South Africa, a constellation of bright white wind turbines rises 150 metres above the rolling grassland. Mainstream Renewable Power brought this project online in July, 17 months after starting construction. The 35 turbines add 80 megawatts to South Africa's supply, enough to power about 70,000 homes there. The Noupoort Wind Farm is just one of about 100 wind and solar projects that South Africa has developed in the past 4 years, as prices fell below that of coal and construction lagged on two new massive coal plants. South Africa is primed to move quickly to expand renewable energy, in part thanks to its investment in data. Environmental scientist Lydia Cape works for the Council for Scientific and Industrial Research, a national lab in Stellenbosch. She and her team have created planning maps for large-scale wind and solar development and grid expansion. Starting with data on the energy resources, they assessed possible development sites for many types of socio-economic and environmental impact, including proximity to electricity demand, economic benefits and effects on biodiversity. The South African government accepted the team's recommendations and designated eight Renewable Energy Development Zones that are close to consumers and to transmission infrastructure \u2014 and where power projects will cause the least harm to people and ecosystems. They total \u201cabout 80,000 square kilometres, the size of Ireland or Scotland, roughly\u201d, says Cape. The areas have been given streamlined environmental authorization for renewable projects and transmission corridors, she says.  A decade ago, people would have told you there isn't any wind in regions such as East Africa.  But for African nations to go green in a big way, they will need a huge influx of cash. Meeting sub-Saharan Africa's power needs will cost US$40.8 billion a year, equivalent to 6.35% of Africa's gross domestic product, according to the World Bank. Existing public funding falls far short, so attracting private investors is crucial. Yet many investors perceive African countries as risky, in part because agreements there require long and complex negotiations and capital costs are high. \u201cIt's a real challenge,\u201d says Daniel Kammen, a special envoy for energy for the US Department of State and an energy researcher at the University of California, Berkeley. \u201cMany of these countries have not had the best credit ratings.\u201d Elham Ibrahim, the African Union's commissioner for infrastructure and energy, advises countries to take steps to reassure private investors. Clear legislation supporting renewable energy is key, she says, along with a track record of enforcing commercial laws. South Africa is setting a good example. In 2011, it established a transparent process for project bidding called the Renewable Energy Independent Power Producer Procurement Programme (REIPPPP). The programme has generated private investments of more than $14 billion to develop 6,327 megawatts of wind and solar. Mainstream Renewable Power has won contracts for six wind farms and two solar photovoltaic plants through REIPPPP. \u201cThis programme is purer than the driven snow,\u201d says O'Connor. \u201cThey publish their results. They give state guarantees. They don't delay you too much.\u201d Although the country's main electricity supplier has wavered in its support for renewables, the central government remains committed to the programme, he says. \u201cI would describe the risks in South Africa as far less than the risks in England in investing in renewables.\u201d For countries less immediately attractive to investors, the World Bank Group launched the Scaling Solar project in January 2015. This reduces risk to investors with a suite of guarantees, says Yasser Charafi, principal investment officer for African infrastructure with the International Finance Corporation (IFC) in Dakar, which is part of the World Bank Group. Through the Scaling Solar programme, the IFC offers low-priced loans; the World Bank guarantees that governments will buy the power generated by the projects; and the group's Multilateral Investment Guarantee Agency offers political insurance in case of a war or civil unrest. Zambia, the first country to have access to Scaling Solar, has won two solar projects that will together provide 73 megawatts. Senegal and Madagascar were next, with agreements to produce 200 and 40 megawatts, respectively. Ethiopia has just joined, and the IFC will give two further countries access to the programme soon; its target is to develop 1,000 megawatts in the first 5 years. \n               Making it flow \n             That power won't be useful if it can't get to users. One of the big barriers to a clean-energy future in Africa is that the continent lacks robust electricity grids and transmission lines to move large amounts of power within countries and across regions. But that gap also provides some opportunities. Without a lot of existing infrastructure and entrenched interests, countries there might be able to scale up renewable projects and manage electricity more nimbly than developed nations. That's what happened with the telephone industry: in the absence of much existing land-line infrastructure, African nations rapidly embraced mobile phones. The future could look very different from today's electricity industry. Experts say that Africa is likely to have a blend of power-delivery options. Some consumers will get electricity from a grid, whereas people in rural areas and urban slums \u2014 where it is too remote or too expensive to connect to the grid \u2014 might end up with  small-scale solar and wind installations and minigrids . Still, grid-connected power is crucial for many city dwellers and for industrial development, says Ibrahim. And for renewables to become an important component of the energy landscape, the grid will need to be upgraded to handle fluctuations in solar and wind production. African nations can look to countries such as Germany and Denmark, which have pioneered ways to deal with the intermittent nature of renewable energy. One option is generating power with existing dams when solar and wind lag, and cutting hydropower when they are plentiful. Another technique shuttles electricity around the grid: for example, if solar drops off in one place, power generated by wind elsewhere can pick up the slack. A third strategy, called demand response, reduces electricity delivery to multiple customers by imperceptible amounts when demand is peaking. These cutting-edge approaches require a smart grid and infrastructure that connects smaller grids in different regions so that they can share electricity. Africa has some of these 'regional interconnections', but they are incomplete. Four planned major transmission corridors will need at least 16,500 kilometres of new transmission lines, costing more than $18 billion, says Ibrahim. Likewise, many countries' internal power grids are struggling to keep up. That's part of what makes working in energy in Africa challenging. Prosper Amuquandoh is an inspector for the Ghana Energy Commission and the chief executive of Smart and Green Energy Group, an energy-management firm in Accra. In Ghana, he says, \u201cthere's a lot of generation coming online\u201d. The country plans to trade electricity with its neighbours in a West African Power Pool, Amuquandoh says, but the current grid cannot handle large amounts of intermittent power. Despite the challenges, he brims with enthusiasm when he talks about the future: \u201cThe prospects are huge.\u201d With prices of renewables falling, that kind of optimism is spreading across Africa. Electrifying the continent is a moral imperative for everyone, says Charafi. \u201cWe cannot just accept in the twenty-first century that hundreds of millions of people are left out.\u201d \n                 Tweet \n                 Follow @NatureNews \n               \n                     Renewables: Share data on wind energy 2016-Jan-04 \n                   \n                     Renewable energy: Back the renewables boom 2014-Mar-19 \n                   \n                     Energy: Islands of light 2014-Mar-11 \n                   \n                     Renewable power: Germany\u2019s energy gamble 2013-Apr-10 \n                   \n                     Smart grids: The energy storage problem 2010-Jan-06 \n                   \n                     Nature  special: Outlook for Earth \n                   \n                     Nature  special: 2015 Paris climate talks \n                   \n                     World Bank Renewable Energy Resource Mapping Initiative \n                   Reprints and Permissions"},
{"file_id": "537466a", "url": "https://www.nature.com/articles/537466a", "year": 2016, "authors": [], "parsed_as_year": "2006_or_before", "body": "Around the world, poverty and social background remain huge barriers in scientific careers. Last year, Christina Quasney was close to giving up. A biochemistry major at the University of Maryland, Baltimore County, Quasney's background was anything but privileged. Her father runs a small car-repair shop in the tiny community of Millersville, Maryland, and she was the first person in her immediate family to attend university. At the age of 25, she had already spent years struggling to make time both for her classes and the jobs she took to pay for them, yet was still far from finishing her degree. \u201cI started to feel like it was time to stop fighting this losing battle and move on with my life,\u201d she says. Quasney's frustrations will sound familiar to millions of students around the world. Researchers like to think that nothing matters in science except the quality of people's work. But the reality is that wealth and background matter a lot. Too few students from disadvantaged backgrounds make it into science, and those who do often find that they are ill-prepared owing to low-quality early education. Few countries collect detailed data on socioeconomic status, but the available numbers consistently show that nations are wasting the talents of underprivileged youth who might otherwise be tackling challenges in health, energy, pollution, climate change and a host of other societal issues. And it's clear that the universal issue of class is far from universal in the way it plays out. Here,  Nature  looks at eight countries around the world, and their efforts to battle the many problems of class in science. United States:  How the classroom reflects class divide  China:  Low pay powers brain drain  United Kingdom:  The paths not taken  Japan:  Deepening divisions  Brazil:  Progressive policy pays off  India:  Barriers of language and caste  Kenya:  Easy access but limited prospects  Russia:  Positive policy, poor productivity \n               UNITED STATES: How the classroom reflects class divide \n             By Jane J. Lee  Quasney is lucky by global standards. She lives in an exceedingly rich country that is brimming with educational opportunities and jobs. Yet for students who share her struggles to make ends meet, the US higher-education system can pose one obstacle after another. \u201cIt starts in high school,\u201d says Andrew Campbell, dean of the graduate school at Brown University in Providence, Rhode Island. Government-supported early education is funded mainly at the state and local level, he notes, and because science courses are the most expensive per student, few schools in the relatively poor districts can afford to offer many of them. Students from these districts therefore end up being less prepared for university-level science than are their wealthier peers, many of whom attended well-appointed private schools. That also puts the students at a disadvantage in the fiercely competitive applications process: only about 40% of high-school graduates in the lowest-income bracket enrolled in a university in 2013, versus about 68% of those born to families with the highest incomes. The students who do get in then have to find a way to pay the increasingly steep cost of university. Between 2003 and 2013, undergraduate tuition, fees, room and board rose by an average of 34% at state-supported institutions, and by 25% at private institutions, after adjusting for inflation. The bill at a top university can easily surpass US$60,000 per year. Many students are at least partly supported by their parents, and can also take advantage of scholarships, grants and federal financial aid. Many, like Quasney, work part time. Nonetheless, some 61% of US students earning bachelor's degrees graduate with some debt \u2014 US$26,900, on average. For those who go on to graduate programmes, tuition is usually paid for by a combination of grants and teaching positions. But if graduate students have to worry about repaying student loans, that can dissuade them from continuing with their scientific training. Several initiatives are under way around the country to ease the way for science students from disadvantaged backgrounds, among them is the $14-million INCLUDES programme announced earlier this year by the US National Science Foundation. But for students such as Quasney, staying in science can still be a matter of luck. One evening last year, she says, Michael Summers, a structural biologist at the university, happened to have dinner at the restaurant where she was hosting and waiting tables. That chance encounter led Quasney to join Summers' laboratory in January, and it was a revelation. Before, she had felt that some of her professors had forgotten what it was like to be a struggling student. Summers' lab is the exact opposite, she says. \u201cThere's no judgements and he doesn't discriminate.\u201d Her experiences have helped her to understand what she can expect when she applies to graduate school and pursues a career in research. \u201cI'm gonna go for it,\u201d she says. \u201cGo big or go home.\u201d \n               CHINA: Low pay powers brain drain \n             By David Cyranoski  It is no accident that  China  currently produces more science PhDs than any country in the world. To combat large-scale poverty, especially in the interior provinces, the communist government in Beijing is trying to make education equally available to everyone. To help the poor, for example, Beijing sets tuition fees low and forbids raising them. Just 5,000 yuan (US$750) per year is enough for entry into premier institutions such as Tsinghua University in Beijing. And for those unable to come up with that sum, the country has national scholarship programmes, including tax-free loans and free admission. Meanwhile, to help integrate China's 55 ethnic minorities, which are also often poor, most provinces give bonus points to minority students who take the Gaokao: a university entrance examination that is the most important threshold to pass on the way to an academic career. A quota system ensures that students from remote regions such as Xinjiang and Tibet are represented at elite schools. China even has 12 universities that are dedicated to minorities. Beneath the surface, however, the reality of Chinese science often falls short of its egalitarian ideals. Children of senior government leaders and private business owners account for a disproportionate share of enrolment in the top universities. And students hesitate to take on the work-intensive career of a scientist when easier, and usually more lucrative, careers await them in business. According to Hepeng Jia, a journalist who writes about science-policy issues in China, this is especially true for good students from rich families.  Chinese science often falls short of its egalitarian ideals.  As a result, says Jia, scientists usually come from poorer families, get less support from home and work under a heavier financial burden. The situation is exacerbated by the low salaries, he says. The average across all scientific ranks is just 6,000 yuan per month, or about one-fifth of the salary of a newly hired US faculty member. Things are especially tough for postdoctoral researchers or junior-level researchers \u201cwho can hardly feed their families if working in bigger cities\u201d, says Jia. This leads many scientists to use part of their grants for personal expenses. That forces them to make ends meet by applying for more grants, which requires them to get involved in many different projects and publish numerous papers, which in turn makes it hard to maintain the quality of their work. Many Chinese researchers escape that trap by seeking positions overseas. Thousands of postdoctoral researchers will go abroad in 2016 with funding from the China Scholarship Council, and many more will find sponsors abroad to fund them. But China has also been able to lure some of the most prominent of these researchers back home. Cao Kai, a researcher at the Science and Technology Talent Center of the science ministry in Beijing, released a survey in April that found one such returning scientist was rewarded with a stunningly high annual salary of 800,000 yuan. But that is not the norm, Kai says. It was just one extreme case he and his colleagues raised to convince \u201cthe government to raise the salary of professors at public universities\u201d. That, he says, would go a long way to attracting and retaining talent in science, regardless of social background. \n               UNITED KINGDOM: The paths not taken \n             By Elizabeth Gibney  For the most part, science in the United Kingdom is egalitarian \u2014 for those who have already made it their career. A 2016 study found that, unlike in law or finance, researchers from lower-income backgrounds are paid no less than their more advantaged peers ( D. Laurison and S. Friedman  Am. Soc. Rev.   81,  668\u2013695; 2016 ). But getting into science is different. The same study found that only 15% of scientists come from working-class households, which comprise 35% of the general population (see 'Elite careers'). Another found that, over the past 25 years, 44% of UK-born Nobel-prizewinning scientists had gone to fee-paying schools, which educate 7% of the UK population (P. Kirby  Leading People 2016  The Sutton Trust, 2016). \u201cThere's a class barrier to the professions,\u201d says Katherine Mathieson, chief executive of the British Science Association, \u201cbut it's more extreme for science.\u201d One hurdle is aspirational. In an ongoing, 10-year study, a group from King's College London found that most English 10\u201314 year olds find science interesting. But those from working-class backgrounds rarely saw it as a career \u2014 perhaps because they seldom encountered people in science-related jobs ( ASPIRES: Young People's Science and Career Aspirations, Age 10\u201314  King's College London, 2013). To tackle this, the King's team is working with London schools on a pilot programme to show children aged 11 to 15 how science fits into everyday life \u2014 by examining the chemicals in food, for example \u2014 and how science skills are relevant in a range of jobs. Early results are promising, and the team plans to expand the programme next year.  There's a class barrier to the professions, but it's more extreme for science.  Another barrier could be that UK students who are interested in a science career often need to abandon other subjects at the age of 16. \u201cPeople from lower-income backgrounds who are unaware of the range of possible science careers might see it as a high-risk gamble,\u201d says Mathieson. A third issue is the effect of a sudden trebling of annual university fees to \u00a39,000 (US$12,000) in 2012. \u201cI suspect that fees could be a massive deterrent to those who grow up in families that have to worry about the basic level of income,\u201d says Mathieson. The danger, she adds, is that a failure to represent all backgrounds will not only squander talent, but increasingly isolate science from society. That disconnect was apparent in the  Brexit referendum in June , when more than half of the public voted to leave the European Union, compared with around  one in ten researchers . \u201cThat diverging world view is a real problem,\u201d says Mathieson, \u201cboth for the quality of research and for scientists' place in society.\u201d \n               JAPAN: Deepening divisions \n             By David Cyranoski  In Japan, inequalities in wealth and status do not reach the extremes found in China and India. Nonetheless, graduate education and academic research have become  less attractive options over the past decade , especially for the underprivileged. Some warn that this could make research a preserve of the wealthy \u2014 with grave social costs.\u201cIt is an emerging issue in Japan,\u201d says Yuko Ito, who researches science policy at the Japan Science and Technology Agency in Tokyo, a major science funder. A big part of the problem is the rise in tuition fees: even at the relatively inexpensive national universities, the \u00a586,000 (US$840) in entrance and first-year tuition fees students paid in 1975 would make little dent in the \u00a5817,800 they've been paying since 2005. In addition, thanks to Japan's long economic contraction, parents are chipping in 19% less for living costs on average than they did a decade ago. This leaves students increasingly dependent on 'scholarships' \u2014 which in Japan are mainly loans that need to be paid back. Half of all graduate students have taken out loans, and one-quarter owe more than \u00a55 million. \u201cMany students just can't come up with the tuition and living costs to become researchers,\u201d says Koichi Sumikura, a professor of science policy at the National Graduate Institute for Policy Studies in Tokyo. Even for those who make it through university on loans, jobs that would make the debt worthwhile are far from guaranteed. In their prime years, between the ages of 30 and 60, one-third of university graduates earns less than \u00a53 million per year. \u201cIn these conditions,\u201d says Ito, \u201cone would hesitate to follow an academic career.\u201d The social divide in higher education already shows. A crucial step to becoming a researcher is to enter a powerful institution such as the University of Tokyo, where the average income of a student's family is twice the national average. \u201cIf this situation continues,\u201d Ito says, \u201cscience will become something that only the rich will hold an interest in, and research will grow distant from solving current social problems.\u201d The government has taken stock of the issue. A government plan for 'investment in the future', announced on 2 August, promises to increase funding for scholarships that need not be repaid as well as to boost the availability of tax-free student loans. But the government has yet to take up a more specific examination of the relationship between success as a researcher and economic factors, says Sumikura. \u201cThat will be an important topic in the future,\u201d he says. \n               BRAZIL: Progressive policy pays off \n             By Jeff Tollefson  In  Brazil , inequalities in wealth are extreme by almost every measure \u2014 including education. The government-run schools are so bad that they are avoided by all but the poorest families. As recently as 2014, just 57% of the country's 19-year-olds had completed high school. And yet there are  signs of progress , especially in science, technology, engineering and medicine. In 2011, for example, Brazil created Science Without Borders, a programme to send tens of thousands of high-achieving university and graduate students to study abroad. Because students from wealthier families have by far the best primary and secondary education, they might have been expected to dominate the selection process. But by the end of the first phase this year, more than half of the 73,353 participants had come from low-income families. \u201cThese statistics really caught us all by surprise,\u201d says Carlos Nobre, a climate scientist who formerly headed of one of the public foundations that fund Science Without Borders. In S\u00e3o Paulo, meanwhile, the medical school at the prestigious University of Campinas (UNICAMP) gives preference to admitting gifted students from government-run schools. The programme started in 2004 after research suggested that out of those with similar test scores prior to admittance, predominantly poor government-school students tended to perform better at UNICAMP than did their counterparts from private schools. The former comprised 68% of this year's entering class. Carlos Henrique de Brito Cruz, who launched the UNICAMP initiative when he was the university rector, suspects that part of the answer is quite simple. \u201cThese students had more obstacles to overcome,\u201d he says. \u201cAnd when you put them in an environment where the obstacles are more or less the same, they tend to realize more of their potential.\u201d Brazil may also be seeing the fruits of the government's effort to improve scientific literacy and push more students into science careers, which gained momentum after the inauguration of Luiz In\u00e1cio Lula da Silva as president in 2003. A division at the federal Ministry of Science, Technology and Innovation focuses entirely on 'social inclusion', with programmes to improve public schools and promote research in fields that affect local communities, such as nutrition and sustainability. The poor quality of secondary education remains a substantial problem that could take a generation or more to address, experts say. Nonetheless, existing initiatives could be boosting the quality of government schools enough for ambitious students to excel, says Nobre. The next question, he says, is whether these students will be able to bolster innovation in Brazilian science. \u201cNow that they are coming into the market, we will have to start evaluating very quickly what happened to these students.\u201d \n               INDIA: Barriers of language and caste \n             By T. V. Padma  Despite the  renown of technology hubs  such as Bangalore and universities such as the multicampus Indian Institute of Technology, vast numbers of talented students in India never get to realize their full potential owing to poor rural schools, language barriers and the caste system. Especially outside the cities, higher education \u2014 including science \u2014 largely remains a privilege of the rich, the politically powerful and the upper castes. India's national census does not collect data on caste, rural or gender representation in science, nor do the country's science departments. Nonetheless, says Gautam Desiraju, a chemist at the Indian Institute of Science in Bangalore, it is clear that rural Indian students are hampered by a lack of good science teachers and lab facilities, and are unaware of opportunities to enter mainstream science (see  www.nature.com/indiascience ). The barriers are even higher for rural girls, who are discouraged from pursuing higher studies or jobs, and for girls from poor urban families, who are expected to take jobs to contribute to their dowries. Many rural students are also hampered by their poor English, the language that schools often use to explain science. \u201cTeachers from elite colleges and interview and selection committees are often biased against such students,\u201d says immunologist Indira Nath, at the Indian National Science Academy in New Delhi. Caste \u2014 the hereditary class system of Hindu society \u2014 is officially not an issue. India's constitution and courts have mandated that up to half of the places in education and employment must be reserved for people from historically discriminated-against classes. However, a clause excludes several of India's top science centres from this requirement. And in reality there is an \u201cunintentional, subtle or hidden discrimination against students from reserved categories, right from high school to college levels\u201d, says Shri Krishna Joshi, a scientist emeritus at the National Physical Laboratory in New Delhi. Teachers do not encourage them as much as they do students from upper castes. As a result, he says, \u201cpoor students from reserved categories in turn often have psychological barriers and believe they cannot compete with the others\u201d. Still, says Desiraju, there are signs of progress. For a long time, Indian officials assumed that all they had to do was set up centres of scientific excellence and the effects on education would simply trickle down to the masses. \u201cBut now,\u201d he says, \u201cagencies are beginning to adopt a more bottom-up approach\u201d that seeks to find talented people at the lowest economic levels. At the University of Delhi South Campus, geneticist Tapasya Srivastava sees the effects of that shift. \u201cCompetitiveness for higher science education is increasing across all caste-based categories and gaps are dissolving,\u201d she says. \u201cTalented young researchers are getting admissions based on their merit alone and not because of the constitutional provision,\u201d agrees Desiraju. But there is much still to be done, he says. \u201cFinding the right talented girl or boy in a small town or village in India is often like finding a needle in a haystack.\u201d \n               KENYA: Easy access but poor prospects \n             \n               By Linda Nordling \n             In Kenya, where around 40% of the population lives on less than US$1.25 a day, class matters surprisingly little for who makes it into science. As one of  Africa's fast-growing 'lion' economies , the country has seen university enrolment more than double since 2011, reaching more than 500,000 last year. The government subsidizes tuition fees for poor secondary-school students who get good grades in science, and there are loans available to help them with living expenses. At the postgraduate level, however, the lack of opportunities in Kenya means that many science hopefuls have to do part of their training abroad. \u201cThe problem for me wasn't getting into science, it was staying in,\u201d says Anne Makena, a Kenyan from a lower-class background with an undergraduate degree in biochemistry from Moi University in Eldoret. She now has a Rhodes scholarship to finish her PhD in chemical biology at the University of Oxford, UK.  Class matters surprisingly little for who makes it into science.  For those staying at home, the surest path to a research career is to get a job with foreign-funded organizations such as the International Centre of Insect Physiology and Ecology (ICIPE) in Nairobi, or the partnership between the Kenya Medical Research Institute (KEMRI) and the UK Wellcome Trust. But competition is fierce, and it can take years to get accepted. This is when graduates from a poorer background are more likely to give up, says Makena. They are drawn by lucrative private-sector salaries and mindful of the need to contribute financially to their families, whereas wealthier students can afford to wait. Another source of uncertainty is Kenyan universities' struggle to secure enough operating funds from the government. The shortfall has led vice-chancellors in the country's public universities to propose up to a five-fold rise in tuition fees for resource-intensive courses, including science. If this happens and government subsidies do not keep pace, poorer students might forego science courses for cheaper degrees. That would be a pity, says Baldwyn Torto, head of behavioural and chemical ecology at ICIPE, because Kenyan students from modest backgrounds make excellent scientists in his experience. \u201cYou find kids from poorer families performing equally well, if not better, than kids from wealthier families,\u201d he says. \n               RUSSIA: Positive policy, poor productivity \n             By Quirin Schiermeier  Following the Soviet Union's collapse in 1991, Russia was quickly given over to untamed capitalism and increasing inequity. Yet the country retained its socialist ideals in education: even now, Russia produces a large share of its science students and researchers from low- and middle-income backgrounds. \u201cThere is a national consensus in Russia regarding the value of equal opportunities in education for the modernization of our country,\u201d says Dmitry Peskov, who directs the young professionals division of the Moscow-based Agency for Strategic Initiatives, which promotes economic innovation in Russia. The country hosts some 3,000 universities and higher learning institutes, and about half of its secondary-school graduates go on to attend them. The average among all Organisation for Economic Co-operation and Development countries is about 35%. In peripheral regions such as the Urals or Siberia, where local governments are keen to develop scientific and engineering capacity, teachers identify talented students as early as ages 4 to 6. If they continue to show promise, they are encouraged to enrol at local universities, whose tuition-free programmes may focus on local needs such as agricultural technology. Children who demonstrate exceptional skills in science, art, sports or even chess may earn admission to the Sirius educational centre in Sochi on the Black Sea. This centre, backed by Russian president Vladimir Putin, was set up after the 2014 Winter Olympics to help Russia's most gifted youths develop their talent with support from leading scientists and professionals. Since December 2015, prospective students who succeed in local or national science competitions and maths Olympiads can also hope to secure a presidential grant worth 20,000 roubles (US$307) per month. These grants allow hundreds of students from lower social backgrounds to study at the nation's top universities on the sole condition that they will stay in Russia for at least five years after graduation. But despite such efforts, Russia's science output remains relatively low. One reason, Peskov says, is the Russian science community's isolation. For all their skills and social diversity, Russian researchers tend to speak poor English and are underrepresented in international meetings and collaborations.  Uncertainty over the Russian government's future support of science  adds to the problem. \u201cLucrative jobs in finance, business administration or industry are much more popular among well-trained young Russians than is a risky academic career,\u201d he notes. \n                 Tweet \n                 Facebook \n                 LinkedIn \n                 Weibo \n                 Wechat \n               \n                     Income inequality is cyclical 2016-Sep-21 \n                   \n                     Science\u2019s 1%: How income inequality is getting worse in research 2016-Sep-21 \n                   \n                     Socio-economic inequality in science is on the rise 2016-Sep-21 \n                   \n                     End class wars 2016-Sep-21 \n                   \n                     UK scientists in limbo after Brexit shock 2016-Jun-28 \n                   \n                     Brazilian science paralysed by economic slump 2015-Sep-30 \n                   \n                     Online learning: Campus 2.0 2013-Mar-13 \n                   \n                     Nature  special: Inequality in science \n                   \n                     Nature  special: Science in India \n                   \n                     Nature  special: Science in China \n                   \n                     Nature  special: South American science \n                   \n                     Nature  special: Africa \n                   \n                     Nature  special: Brexit and science \n                   \n                     OECD Centre for Opportunity and Equality \n                   \n                     NSF INCLUDES \n                   \n                     Science without Borders \n                   \n                     King's College London ASPIRES \n                   Reprints and Permissions"},
{"file_id": "535342a", "url": "https://www.nature.com/articles/535342a", "year": 2016, "authors": [{"name": "Heidi Ledford"}], "parsed_as_year": "2006_or_before", "body": "The soaring popularity of gene editing has made celebrities of the principal investigators who pioneered the field \u2014 but their graduate students and postdocs are often overlooked. When Blake Wiedenheft started studying microbes, his work was both remote and obscure. He spent his PhD sampling hot springs in Yellowstone National Park, then created artificial versions in the laboratory to study the microorganisms that lived in the inhospitable water. \u201cWe wanted to understand how life could survive in boiling acid,\u201d he says. Over time, Wiedenheft became more interested in how microbes fend off viruses. He read around, and came across a peculiar bacterial immune system called CRISPR. In 2007, he approached Jennifer Doudna, a molecular biologist at the University of California, Berkeley, and found that she shared his interest. Join the lab, she said \u2014 and he did. Over the next five years, he studied the structure and biochemistry of CRISPR systems, landing a first-author publication in  Nature 1 . Today, CRISPR is a household name for molecular biologists around the world. Researchers have eagerly co-opted the system to  insert or delete DNA sequences in genomes across all kingdoms of life . CRISPR is being used to generate a  new breed of genetically modified crops  and may one day  treat human genetic diseases . Doudna and other principal investigators involved in the seminal work have become scientific celebrities: they are profiled in major newspapers, star in documentaries and are rumoured to be contenders for a Nobel prize. \u201cWhen I came to the lab, I was the only person studying CRISPR,\u201d Wiedenheft says. \u201cWhen I left the lab, almost everyone was studying it.\u201d Wiedenheft, however, has hardly achieved the same fame as his mentor \u2014 and nor have the other students and postdocs who toiled at the bench to make CRISPR genome editing a reality. They certainly reap benefits from their work: support and reflected glory from their supervisors, as well as expertise in a coveted technique. But some also face a difficult transition to becoming independent scientists as they try to establish themselves in a hypercompetitive field. For Wiedenheft, the key to survival has been seclusion. When he struck out from Doudna's lab, he opted for a return to Montana State University in Bozeman, where he did his PhD, over an offer from a larger, better-known institution. \u201cAt the end of the day, the opportunities for solitude and being outdoors make me more creative and a better scientist,\u201d he says. But like other young scientists who graduate from powerhouse labs, he can't help but wonder how different life might have been if accolades in biomedical science were given to the first authors on a paper, rather than the last. Now and then, he admits, he doesn't feel quite appreciated enough. \u201cSome days it matters, some days it doesn't.\u201d \n               An edited history \n             The history of CRISPR\u2013Cas9 gene editing has become a subject of fierce debate and a bitter, high-stakes patent battle. Researchers and institutes have been jostling aggressively to make sure that they are credited for their share of the work in everything from academic papers to news stories. \u201cI get a lot of phone calls from lawyers about what I did and when,\u201d Wiedenheft says. In January, Eric Lander, president of the Broad Institute of MIT and Harvard in Cambridge, Massachusetts, tossed into this minefield a historical portrait called 'The Heroes of CRISPR' 2 . It was instantly controversial. Some said that it marginalized the contributions of certain researchers, and they questioned the decision to publish the article without a conflict-of-interest statement noting that the Broad Institute is embroiled in a patent dispute that hinges on determining  who invented CRISPR\u2013Cas9 gene editing . But for George Church, a geneticist at Harvard Medical School in Boston, Massachusetts, who is also a pioneer in the field, it was particularly painful to see statements attributing key discoveries to him rather than his postdocs and graduate students. \u201cEric said my name too many times,\u201d Church says. Lander says that there was no intended slight in the 'Heroes' story. He was mindful that there were dozens of other co-authors on the key papers, \u201cBut I couldn't figure out how to collect and tell their stories within a nine-page article.\u201d If anything, he adds, the article widened the CRISPR spotlight: most discussion up to that point had focused on 3 major contributors to the field, whereas his piece featured 17 major players and acknowledged that there were many others. Any lack of attention to CRISPR's junior discoverers comes despite fervent advocacy on the part of their advisers. Junior investigators in the Church lab praise their leader's unwavering support, along with the unique intellectual environment he has fostered in the lab. Doudna is a fierce champion of the scientists she has mentored. \u201cIt's really important for junior investigators to get the credit they deserve,\u201d she says. \u201cThey really drive the scientific enterprise.\u201d What is more, academic papers often set out each author's contribution to the work. But those details often get lost simply because, broadly speaking, credit in science goes to the leader of the lab, as do any prizes that follow. \u201cThat's just how the system works, and I accept my role in this system,\u201d says Martin Jinek, another Doudna lab alumnus. \u201cBut yeah, it's something you can't help but think about.\u201d Sometimes people may take note of the first author, but not in a meaningful way, says Rachel Haurwitz, a former Doudna graduate student and now president of Caribou Biosciences in Berkeley, California. \u201cThey'll say 'the 2012 Jinek paper' but most people have no idea who Martin Jinek is,\u201d she says. Jinek was co-first author on a seminal paper 3  showing that the enzyme Cas9 can be programmed to target specific sequences of DNA using only a short strand of RNA \u2014 and he found that his life became defined by CRISPR. When he entered the job market, he couldn't even discuss the work in interviews because the patent had not been filed. Even so, he got an attractive offer from the University of Zurich in Switzerland, and has since built a lab there that focuses on the basic biology of CRISPR more than its applications. As interest in CRISPR\u2013Cas9 gene editing grew, his schedule became packed: he now travels to talks two to three times a month. Although he appreciates the professional boost that the CRISPR frenzy has given him, he also struggles to find a balance between running his lab and other obligations. Haurwitz has faced obstacles, too. She spent her PhD characterizing the CRISPR-based microbial immune system and the structure of a CRISPR-associated enzyme called Cys4 (ref.  4 ). In 2011, she co-founded Caribou along with Doudna and others to commercialize research tools based on CRISPR. The early days were tough, but Caribou has since formed partnerships with major industry players, and the company announced in May that its latest round of fundraising had brought in US$30 million. Yet as the firm has grown, some investors have pushed to replace Haurwitz with a more seasoned leader. Doudna has quashed the idea. \u201cThere's no reason to replace her,\u201d says Doudna. \u201cShe keeps showing that she has the talent to be successful.\u201d \n               Riding the wave \n             For many early career scientists, working in such a hot field has clear advantages. As a postdoc, bioengineer Prashant Mali helped to launch the CRISPR project in Church's lab. He was a co-first author on the lab's 2013 paper 5  demonstrating that CRISPR\u2013Cas9 could be used to edit the genome in human induced pluripotent stem cells. The discovery sent CRISPR excitement to fever pitch \u2014 a wave of enthusiasm that Mali rode into the job market later that year. \u201cI definitely got a lot of endorsements,\u201d he says. (There is, however, no mention of him in 'Heroes of CRISPR' \u2014 a sore point with Church.) Eventually settling at the University of California, San Diego, Mali continues to study stem-cell development and develop CRISPR-based tools. He accepts the intensity of the field as a small price to pay. His lab is just 18 months old \u2014 too young to have been scooped yet, he says \u2014 but competition is inevitable. \u201cThere will obviously be a lot of overlap of good ideas.\u201d CRISPR threw open doors for Luhan Yang, the other first author on the 2013  Science  paper from Church's lab. Soon after the paper was published, the lab was contacted by several researchers who study organ transplantation. They wanted to know whether genome editing could now be used to engineer pig organs so that they would be less likely to provoke an immune response in humans. Yang seized the idea with gusto, says Church. The pig genome is home to retroviral DNA, and concern that those retroviruses might become reactivated in a human host led  many researchers to flee the field in the late 1990s . Yang reasoned that the retroviral sequences are so similar to one another that a single CRISPR\u2013Cas9 experiment might knock out many of them at once. She and her three co-first authors now hold the world record 6  for the  largest number of sequences targeted in a single CRISPR\u2013Cas9 experiment : 62. And Yang is raising money to launch a company with Church called eGenesis, to further the work. \u201cGeorge always gave me the opportunity to establish my leadership,\u201d she says. Across the Charles River from the Church lab, graduate student Le Cong worked side-by-side, late into the night with his mentor, bioengineer Feng Zhang at the Broad Institute, to develop CRISPR gene editing in mammalian cells. Zhang was himself a young investigator just launching a lab when Cong joined; Cong remembers opening the box containing the lab's first centrifuge and sitting with Zhang at a computer googling 'DNA-binding protein' to look for new ways to edit genomes. The two became a tightly knit team. When they embarked on the CRISPR project, it seemed like a long shot as Cong screened enzymes and reaction conditions, trying to find those that would work in human cells. But Cong was willing to take the risk. He and Zhang had previously pioneered  the use of a different gene-editing system , called TALENs, in mammalian cells, and he reasoned that this early success would be enough to allow him to graduate if the CRISPR project failed. He never had to test that hypothesis: in 2013, Cong and his fellow graduate student Fei Ann Ran co-first authored a  Science  paper 7  showing that the system works in mammalian cells \u2014 the paper was published simultaneously with that of Mali, Church and their team. At that point, Cong was advised that he could skip the postdoc and go straight to a faculty position. But he worried that doing so would limit him: he could be pigeonholed as 'the CRISPR guy'. \u201cI felt uncomfortable about that,\u201d he says. \u201cI was not only looking to develop technology.\u201d Instead, Cong opted for another postdoc; he is now embarking on a faculty job search, and plans to use his lab to study allergies and autoimmune disorders. Cong says that he feels no resentment at being largely excluded from the CRISPR media frenzy and attention centred on Zhang. \u201cI do think I've been recognized,\u201d he says; Zhang has been generous in giving him credit within the scientific community, and has encouraged Cong to give talks in his stead. And Cong, like others interviewed for this story, is himself insistent about giving credit to others in the field. He sprinkles in references to work done in other labs, including some of the earliest microbiology work characterizing the CRISPR system. Wiedenheft says that's characteristic of the CRISPR community. \u201cIt's competitive, but it's friendly.\u201d Outside that community, however, the accolades continue to be heaped on senior investigators. \u201cWe need to invent ways to expand the medals podium,\u201d says Lander. \u201cThe idea that scientific discovery involves just one, two or three people is so nineteenth-century.\u201d There are many more unsung heroes of CRISPR than this article could do justice to. One often overlooked group is headed by Virginijus Siksnys at Vilnius University in Lithuania \u2014 where Giedrius Gasiunas began his PhD in 2007. He plugged away for years, tackling the biochemistry of CRISPR\u2013Cas9 and, like Jinek, eventually came to the conclusion that the Cas9 enzyme could be programmed to cut isolated DNA at specific sites. In 2012, the lab sent a paper to  Cell , where it was rejected without review. Gasiunas then submitted the paper to the  Proceedings of the National Academy of Sciences  and waited. A few months later, while his paper was still under review, the now-legendary Jinek paper appeared in  Science . The two papers had key differences, but reached similar conclusions. Gasiunas had been scooped 8 . Seeing other scientists collect awards for CRISPR gene editing sometimes irks Gasiunas, now a postdoc in Siksnys's lab. But the experience has not entirely soured him on the subject. Although he has since been scooped again, he finds it no longer stings as much as it did. \u201cIt's a risky field,\u201d he says. \u201cBut I think if you want to achieve something great, you need to take risks.\u201d See Editorial  page 323 \n                 Tweet \n                 Facebook \n                 LinkedIn \n                 Weibo \n                 Wechat \n               \n                     Reward the forgotten foot soldiers of science 2016-Jul-20 \n                   \n                     First CRISPR clinical trial gets green light from US panel 2016-Jun-22 \n                   \n                     Fast-spreading genetic mutations pose ecological risk 2016-Jun-08 \n                   \n                     CRISPR gene-editing system unleashed on RNA 2016-Jun-03 \n                   \n                     Gene-editing hack yields pinpoint precision 2016-Apr-20 \n                   \n                     Second Chinese team reports gene editing in human embryos 2016-Apr-08 \n                   \n                     CRISPR: gene editing is just the beginning 2016-Mar-07 \n                   \n                     Nature  special: CRISPR \n                   \n                     AddGene: CRISPR\u2013Cas9 guide \n                   Reprints and Permissions"},
{"file_id": "535338a", "url": "https://www.nature.com/articles/535338a", "year": 2016, "authors": [{"name": "Brendan Borrell"}], "parsed_as_year": "2006_or_before", "body": "Jim Papadopoulos has spent a lifetime pondering the maths of bikes in motion. Now his work has found fresh momentum. Seven bikes lean against the wall of Jim Papadopoulos's basement in Boston, Massachusetts. Their paint is scratched, their tyres flat. The handmade frame that he got as a wedding present is coated in fine dust. \u201cI got rid of most of my research bikes when I moved,\u201d he says. The bicycles that he kept are those that mean something to him. \u201cThese are the ones I rode.\u201d Papadopoulos, who is 62, has spent much of his life fascinated by bikes, often to the exclusion of everything else. He competed in amateur races while a teenager and at university, but his obsession ran deeper. He could never ride a bike without pondering the mathematical mysteries that it contained. Chief among them: What unseen forces allow a rider to balance while pedalling? Why must one initially steer right in order to lean and turn left? And how does a bike stabilize itself when propelled without a rider? He studied these questions intensely as a young engineer at Cornell University in Ithaca, New York. But he failed to publish most of his ideas \u2014 and eventually drifted out of academia. By the late 1990s, he was working for a company that makes the machines that manufacture toilet paper. \u201cIn the end, if no one ever finds your work, then it was pointless,\u201d he says. But then someone did find his work. In 2003, his old friend and collaborator from Cornell, engineer Andy Ruina, called him up. A scientist from the Netherlands, Arend Schwab, had come to his lab to resurrect the team's research on bicycle stability. \u201cJim, you need to be a part of this,\u201d Ruina told him. \n               Two wheels good \n             Together, the researchers went on to crack a century-old debate about what allows a bicycle without a rider to balance itself, publishing in  Proceedings of the Royal Society 1  and  Science 2 . They have sought to inject a new level of science into the US$50-billion global cycling industry, one that has relied more on intuition and experience than on hard mathematics. Their findings could spur some much needed innovation \u2014 perhaps helping designers to create a new generation of pedal and electric bikes that are more stable and safer to ride. Insights from bicycles also have the potential to transfer to other fields, such as prosthetics and robotics. \u201cEverybody knows how to ride a bike, but nobody knows how we ride bikes,\u201d says Mont Hubbard, an engineer who studies sports mechanics at the University of California, Davis. \u201cThe study of bicycles is interesting from a purely intellectual point of view, but it also has practical implications because of their ability to get people around.\u201d For a mechanician \u2014 that fusty breed of engineer whose subject is defined by Newton's three laws of motion \u2014 the conundrums of the bicycle hold a special allure. \u201cWe are all stuck in the nineteenth century, when there wasn't such a difference between math and physics and engineering,\u201d says Ruina. Bicycles, he says, are \u201ca math problem that happens to relate to something you can see\u201d. The first patents for the velocipede, a two-wheeled precursor to the bike, date to 1818. Bikes evolved by trial and error, and by the early twentieth century they looked much as they do today. But very few people had thought about how \u2014 and why \u2014 they work. William Rankine, a Scottish engineer who had analysed the steam engine, was the first to remark, in 1869, on the phenomenon of 'countersteering', whereby the rider can steer to the left only by first briefly torquing the handlebars to the right, allowing the bike to fall into a leftward lean. The link between leaning and steering gives rise to the bicycle's most curious feature: the way that it can balance while coasting on its own. Give a riderless bike a shove and it may wend and wobble, but it will usually recover its forward trajectory. In 1899, English mathematician Francis Whipple derived one of the earliest and most enduring mathematical models of a bicycle, which could be used to explore this self-stability. Whipple modelled the bicycle as four rigid objects \u2014 two wheels, a frame with the rider and the front fork with handlebars \u2014 all connected by two axles and a hinge that are acted upon by gravity. Plugging the measurements of a particular bicycle into the model revealed its path during motion, like a frame-by-frame animation. An engineer could then use a technique called eigenvalue analysis to investigate the stability of the bicycle as one might do with an aeroplane design. In 1910, relying on such an analysis, the mathematicians Felix Klein and Fritz Noether along with the theoretical physicist Arnold Sommerfeld focused on the contribution of the gyroscopic effect \u2014 the tendency of a spinning wheel to resist tilting. Push a bicycle over to the left and the rapidly spinning front wheel will turn left, potentially keeping the bicycle upright. In April 1970, chemist and popular-science writer David Jones demolished this theory in an article for  Physics Today 3  in which he described riding a series of theoretically unrideable bikes. One bike that Jones built had a counter-rotating wheel on its front end that would effectively cancel out the gyroscopic effect. But he had little problem riding it hands-free. This discovery sent him hunting for another force that could be at play. He compared a bike's front wheel to the casters on a shopping trolley, which turn to follow the direction of motion. A bicycle's front wheel can act as a caster because the point at which the wheel contacts the ground typically sits anywhere from 5 centimetres to 10 centimetres behind the steering axis (see 'What keeps a riderless bike upright?'). This distance is known as the trail. Jones discovered that a bike with too much trail was so stable that it was awkward to ride, whereas one with negative trail was a death trap and would send you tumbling the moment you released the handlebars. When a bicycle starts to topple, he concluded, the caster effect steers the front end back under the falling weight, keeping the bicycle upright. To Jones, the caster trail was the sole explanation for a bike's self-stability. In his memoir, published 40 years later, he counted the observation as one of his great accomplishments. \u201cI am now hailed as the father of modern bicycle theory,\u201d he declared. \n               Gearing up \n             That article would make an impression on Jim Papadopoulos, then a teenager in Corvallis, Oregon, with a gift for numbers and a home life in tatters. In 1967, his father Michael, an applied mathematician from England, started a job at Oregon State University. But Michael Papadopoulos was denied tenure after protesting against the Vietnam War, setting off a decade-long legal battle with the university that left him out of a job and the family scouring rubbish bins for scraps. Jim's mother killed herself in the early 1970s. \u201cJust as I was opening my eyes to the world and deciding who I was,\u201d Papadopoulos says, \u201cmy family was falling apart. He found solace in bikes. He pedalled his Peugeot AO8 around town and grew his hair to his shoulders. He stopped going to classes, and his grades took a tumble. At 17, he dropped out of school and left home. But before he abandoned his studies, a teacher gave him the Jones article. Papadopoulos found it captivating but confusing. \u201cI've got to learn this stuff,\u201d he thought. He spent the summer bumming around Berkeley, California, reading George Arfken's textbook  Mathematical Methods for Physicists  in his spare time. Then, he worked at a plywood mill in Eugene, Oregon, earning enough money to buy the legendary Schwinn Paramount that he raced every weekend. In 1973, he worked for the frame builder Harry Quinn in Liverpool, UK, but he was terrible at it and Quinn asked him to leave. Papadopoulos returned to Oregon in 1975, spent a year at the state university and then started undergraduate studies in mechanical engineering at the Massachusetts Institute of Technology (MIT) in Cambridge. He did well. Oil company Exxon later supported him as he studied for a PhD in fracture mechanics. Papadopoulos's adviser, Michael Cleary, was optimistic about his prospects as an academic. \u201cI think Jim will become a university professor \u2014 and we certainly hope it's going to be here at MIT,\u201d he told a writer from Exxon's in-house magazine. Papadopoulos had other ideas. He had been studying Whipple's model and Jones's article, and one summer, an internship took him to the US Geological Survey in Menlo Park, California, where he met Andy Ruina. The two became fast friends. When Ruina got a job at Cornell, he hired Papadopoulos as a postdoc. \u201cWe talked about bikes all the time, but I didn't realize he wanted to make a serious thing about it,\u201d Ruina says. Papadopoulos convinced Ruina that bicycle companies \u2014 like oil companies \u2014 might be interested in supporting academic research. So he started fund-raising, reaching out to bike makers. For $5,000, they could be benefactors of the Cornell Bicycle Research Project, an ambitious effort that would investigate everything from the strength of wheels to brake failure in the rain. Papadopoulos's first goal was to finally understand what makes one bicycle more stable than another. He sat in his office and scrutinized 30 published attempts at writing the equations of motion for a bicycle. He was appalled by the \u201cbad science\u201d, he says. The equations were the first step towards connecting the geometry of a bicycle frame with how it handled, but each new model made little or no reference to earlier work, many were riddled with errors and they were difficult to compare. He needed to start from scratch. After a year of work, he had what he believed to be the definitive set of equations in hand. Now it was time for them to talk back to him. \u201cI was sitting for hours at a time, staring at the equations and trying to figure out what they implied,\u201d he says. He first rewrote the bicycle equations in terms of the caster trail, the crucial variable that Jones had championed. He expected to find that if the trail was negative, the bicycle would be unstable, but his calculations suggested otherwise. In a report that he prepared at the time, he sketched a bizarre bicycle with a weight jutting out in front of the handlebars. \u201cA sufficiently forward [centre of mass] can compensate for a slightly negative trail,\u201d he wrote. No single variable, it seemed, could account for self-stability. This discovery meant that there was no simple rule-of-thumb that could guarantee that a bike is easy to ride. Trail could be useful. Gyroscopic effects could be useful. Centre of mass could be useful. For Papadopoulos, this was revelatory. The earliest frame builders had simply stumbled on a design that felt OK, and had been riding around in circles in that nook of the bicycle universe. There were untested geometries out there that could transform bike design. \n               The crash \n             After two years, Ruina could no longer support Papadopoulos. Apart from the bike manufacturer Murray, the only industry donations the two ever got were from Dahon and Moulton, makers of small-wheeled bicycles \u2014 perhaps because the bikes' unconventional designs could make them tricky to ride. Ruina joked that he should change the name to the \u201cFolding Bicycle Research Project\u201d. It was gallows humour. And although Papadopoulos was making progress in the mathematics of bikes, he only published one paper related to the topic as a first author 4 . \u201cI find much more joy discovering the new and working out the details and, of course, it's boring to write it up,\u201d he says. Without money or publications, his time in bicycle research wound down. In 1989, he put his bikes into a moving van and drove west to Illinois, where his then-wife had a job. He endured a succession of teaching and industry jobs that he hated. In his spare time, he founded and moderated the Hardcore Bicycle Science e-mail list for bicycle-science nerds and helped to build a car that fitted into a few suitcases for the reality television show  Junkyard Wars . In 2001, David Wilson, an MIT engineer and inventor of one of the first modern recumbent bicycles, invited Papadopoulos to co-author the third edition of the book  Bicycling Science . Papadopoulos was overwhelmed by monetary debts and responsibilities. He failed to send Wilson the first chapter, and then stopped responding to e-mails altogether. Wilson felt betrayed. \u201cHe is a rather brilliant guy,\u201d Wilson says, but \u201che always had problems finishing anything\u201d. Papadopoulos says that he did complete the work, but that it took two years longer than it should have, partly because of a stressful divorce. \n               Back to the bike \n             At Cornell, Ruina moved on. He applied the team's insights about bicycles to a new arena: robots. If bicycles could demonstrate such elegant stability without a control system, he reasoned, it might be possible to design a stripped-down walking machine that achieves the same thing. In 1998, he worked with Martijn Wisse, a graduate student of Schwab's at the Delft University of Technology in the Netherlands, to build a bipedal machine that could walk down a slight incline with no motor at all,  storing energy in its swinging arms . Adding a few electronic motors generated an energy-efficient robot that could walk on level ground. In 2002, Schwab decided to spend his sabbatical with Ruina, and they started discussing the old bicycle work. It was then that Ruina called Papadopoulos and paid for him to visit. \u201cThat was the first time I met the genius,\u201d says Schwab. With more bicycles on the road than ever before, Schwab found it inconceivable that no one had published the correct set of bike equations, or applied it to bicycle design challenges. Within a year, he and Jaap Meijaard, an engineer now at the University of Twente in the Netherlands, independently derived their own equations and found complete concordance with Papadopoulos's. They presented the definitive bicycle equations at an engineering conference in South Korea, and the four collaborators published them jointly 1 . The challenge now was to prove that it was more than just a mathematical finding. Schwab and a student spent a year building a self-stable bike with a very small negative trail. Looking like the offspring of a razor scooter and a see-saw, it had a weight angled out in front of the front wheel and a counter-turning wheel to cancel out gyroscopic effects. In a video of it coasting, you can see it lean and veer to the right, but then recover on its own 2 . The experiment proved that Papadopoulos had been right about the complex interplay of factors that make a bicycle stable or unstable. Yet, after waiting for three decades for his discoveries to reach a wider audience, Papadopoulos can't help but feel deflated. \u201cIt did not change everything in the way that we imagined,\u201d he says. This year's bike frames look much like last year's. \u201cEveryone is still in the box,\u201d he says. Nevertheless, other researchers have since been pulled into the group's orbit, creating enough momentum to launch a Bicycle and Motorcycle Dynamics conference in 2010. It gathers together tinkerers from all over the world, some of whom have also built weird experimental bicycles to test design principles. One of the organizers of this year's conference, engineer Jason Moore of the University of California, Davis, has sought to probe the link between a bicycle frame's geometry and an objective measure of handling \u2014 its ease of control 5 . The work was inspired by extensive military research on aircraft pilots. Moore created a model of human control by performing various manoeuvres on bikes kitted out with sensors to monitor his steering, lean and speed. To force himself to balance and ride using steering movements alone (rather than shifting his weight), he had to don a rigid upper-body harness that bound him to the bike. The research confirmed the long-standing assumption that more stable bikes handle better, and potentially gives frame builders a tool to optimize their designs. It also introduced a puzzle: the steering torque required was two or three times that predicted by the Whipple bicycle model 6 . This might have been caused by friction and flexing of the tyres, which are not part of the model, but no one is certain. For further tests, Moore and his colleagues have built a robotic bike that can balance itself. \u201cOnce you have a robot bicycle, you can do a lot of crazy experiments without having to put a human in danger,\u201d he says. (One of his earlier handling experiments had him regaining his balance after a sideways blow from a wooden stick.) Unlike many other riderless-bike robots, it does not use internal gyroscopes to stay upright, but depends on steering alone. Moore has shipped it to Schwab for further study. Today, Schwab has the kind of laboratory that Papadopoulos always dreamed of, and Papadopoulos is grateful to be able to collaborate. \u201cIt's the most beautiful thing you can imagine,\u201d he says. Schwab's other projects include a 'steer by wire' bike, which allows him to separate steering movements from balancing ones, and a 'steer assist' bicycle, which stabilizes itself at slow speeds. He has also identified a rear-steered recumbent bike that shows self-stability, in part owing to an enlarged front wheel that boosts gyroscopic effects. The chief advantage of a rear-steered recumbent is that it would have a shorter chain than standard recumbents, which should lead to better energy transfer. \u201cPeople have tried to build them before, but they were unrideable,\u201d Schwab says. Papadopoulos, who now has a teaching position at Northeastern University in Boston, is trying to get comfortable with academia once again. He's establishing collaborations, and testing out long-dormant ideas about why some bicycles wobble at high speed 7 . He believes he can eliminate speed wobble with a damper to soak up vibrations in the seat post. With his new colleagues and students, he is branching out into other types of question, not all them bike-related. Down in his basement, Papadopoulos opens the drawer of a tan filing cabinet and starts flipping through crinkled manila folders marked with labels such as 'tire pressure', 'biomechanics' and 'Cornell'. He pulls out a textbook. \u201cExercise physiology? I never really got into that one,\u201d he says, tossing it aside. In the back of the drawer, he finds a thick folder of bicycle research ideas, marked 'Unfinished'. Papadopoulos thinks for a second and then offers a correction: \u201cMostly unfinished.\u201d See Editorial  page 324 \n                 Tweet \n                 Facebook \n                 LinkedIn \n                 Weibo \n                 Wechat \n               \n                     Time for physics to make its mark on cycling 2016-Jul-20 \n                   \n                     Sustainable mobility: Six research routes to steer transport policy 2015-Jul-01 \n                   \n                     Science at the Olympics: Team science 2012-Jul-18 \n                   \n                     Racing just to keep up 2011-Jul-15 \n                   \n                     Theoretical walker struts its energy-efficient stuff 2011-Jan-21 \n                   \n                     Injured robots learn to limp 2006-Nov-16 \n                   \n                     When is an ant like a bicycle? 2001-Oct-30 \n                   Reprints and Permissions"},
{"file_id": "535484a", "url": "https://www.nature.com/articles/535484a", "year": 2016, "authors": [{"name": "Ewen Callaway"}], "parsed_as_year": "2006_or_before", "body": "A Belgian lab aims to turn the brewing world on its head with new strains of yeast. Kevin Verstrepen\u2019s lab meetings can be pretty boozy affairs. Twice a week, several members of his group at Belgium\u2019s University of Leuven and the Flanders Institute for Biotechnology gather around a table loaded with black, tulip-shaped beer glasses, together with spit buckets and crackers. Verstrepen holds a glass and takes a whiff of its contents. \u201cFor me, this was an ethyl acetate bomb,\u201d he pronounces, referring to a chemical found in pear-flavoured sweets that, at high concentrations, reeks of nail polish. Brigida Gallone, a graduate student in the lab, detects a second aroma: \u201cethyl acetate and 4-VG\u201d, she says. That\u2019s 4-vinyl-guaiacol, which smells of smoke, cloves and \u2014 according to a tasting sheet in front of her \u2014 a dentist\u2019s surgery. \u201cI like 4-VG and this was too much for me.\u201d Another student, Stijn Mertens, catches a smell of wet cardboard, which is common to stale beers. \u201cI got some  trans -2-nonenal,\u201d he says. With that, the group finishes its analysis of this brew and moves onto the ninth and final glass. It is not even 11 a.m.. \u201cThere\u2019s only so many you can do before you lose focus,\u201d says postdoc Miguel Roncoroni, who has been hosting these tastings for more than 4 months. They are part of a project to characterize some 200 commercially produced Belgian beers. Their assessments, alongside precise measurements of the dozens of chemicals that produce the flavours and aromas, could help consumers to identify new beers to try, by comparing the lab\u2019s profiles to ones they like. But Verstrepen has loftier ambitions than helping beer lovers to select their next bottle. He wants to build the perfect yeast. His lab is deploying what it is learning about the chemical and genetic basis of beer flavour to breed yeast strains that generate unique flavours and other qualities coveted by brewers and drinkers. The beer geeks in his lab straddle the worlds of curiosity-driven science and industrial brewing. They study evolution, biochemistry, and even neuroscience through yeast. But they also have contracts with beer makers worldwide, from multinational conglomerates to small trend-setting craft breweries. In an upcoming  Cell  paper, the lab will report the genomes of some 150 yeast strains used to make beer, sake and other fermented products, a project done in collaboration with a leading supplier of yeast to brewers and a synthetic-biology firm. Ewen Callaway visits the beer lab and joins the researchers for a tasting session. For a US$500-billion industry whose products depend on complex interactions between chemistry and microbiology, sophisticated yeast strains are hot commodities. \u201cYou always want to know what\u2019s new in Kevin\u2019s lab,\u201d says Peter Bouckaert, the brewmaster at New Belgium, a leading craft brewery in Fort Collins, Colorado. \u201cPeople are watching what he does.\u201d Beer gets its flavour from just a few ingredients (see \u2018Better brewing through biology\u2019). Grains \u2014 mainly malted barley \u2014 provide sugar and body, but can also imbue flavour, such as the chocolate notes common to dark stouts. Hop flowers bring bitterness as well as the tropical fruit notes in some craft beers. Dissolved minerals in the water influence the flavours that come through from grain and hops. And brewer\u2019s yeast,  Saccharomyces cerevisiae , provides alcohol, bubbles and hundreds of aroma compounds. Fermentation produces everything from isoamyl acetate, which makes German Hefeweizens taste of banana, to the clove notes of 4-VG. Yeast science was spearheaded by beer makers. Denmark\u2019s Carlsberg brewery established one of the world\u2019s first yeast-biology labs in 1875, and it was there that Emil Christian Hansen isolated the first pure culture of a brewing yeast in 1883. In the 1930s and 40s, another Carlsberg scientist, \u00d8jvind Winge, discovered that yeast reproduces both sexually and asexually and used this insight to breed new strains with useful brewing traits. Winge\u2019s work moved yeast from the brewery to biology labs, and many scientists now use brewer\u2019s yeast as a model to probe the inner workings of complex cells. But despite a long and fruitful marriage between yeast and bioscience, Verstrepen argues that many brewers are still stuck in the nineteenth century when it comes to the yeast they use. \u201cBrewers, especially traditional brewers, are often not using the optimal yeast.\u201d Most use just one strain \u2014 isolated from their brewery or borrowed from another decades ago \u2014 in all their beers. Verstrepen wants to change that. He started out working in a South African wine yeast lab, then joined a University of Leuven beer lab for his PhD in 1999. But he was disappointed to find that most of the research involved trouble-shooting brewers\u2019 problems. \u201cNobody was doing any biology, really,\u201d he says. Disillusioned, he moved to the Whitehead Institute for Biomedical Research in Cambridge, Massachusetts, to pursue a postdoc with Gerald Fink, who pioneered genetic engineering in yeast in the 1970s. But although the scientists there liked yeast, nobody was interested in beer \u2014 at least not by day \u2014 Verstrepen says. So, his work centred on the proteins that pathogenic yeast deploy to stick to human tissue. Verstrepen discovered that the level of stickiness is dictated by the number of repeating DNA sequences in a certain gene (K. J. Verstrepen  et al .  Nature Genet.   37 , 986\u2013990; 2005 ). \u201cIt\u2019s like having longer Velcro loops that stick more easily,\u201d he explains. The proteins are also responsible for flocculation, the process by which yeast cells clump together in a beer and fall out of solution. Flocculation varies between brewing strains and influences the flavour, clarity and alcohol content of a beer. In 2005, Verstrepen moved across town to open a lab at Harvard University, focusing on the roles of various repetitive DNA sequences in generating diversity. He taught Harvard undergraduates biology in a course that incorporated brewing \u2014 \u201cIt was a pretty tough course,\u201d he says \u2014 but beer didn\u2019t feature in his research until he returned to Leuven in 2009. Verstrepen hoped to combine the kind of research he was doing with his interests in beer and wine. But it was a phone call from a Swiss chocolate company that jump-started his work with industry. Zurich-based Barry Callebaut, one of the world\u2019s largest cocoa makers, needed help transforming bitter cacao beans into cocoa powder (which is traditionally done by yeasts present in the environment). \u201cMy response was \u2018Is chocolate fermented?\u2019 That wasn\u2019t the smartest thing to say,\u201d says Verstrepen. Nevertheless, the company became the first client of Verstrepen\u2019s lab consultancy, which now earns around \u20ac500,000 ($555,000) per year. Half of the 25 or so scientists in his lab do applied research on beer, biofuels and other fermented products; the rest pursue epigenetics, molecular evolution and other basic research. \n               Lager lab \n             At first glance, the lab looks like any other. Centrifuges, petri dishes and pipettes cover benches. There is also an incubator full of small glass bottles. It would look at home in any microbiology lab, were the bottles not filled with a strong broth of malted barley, sugar and hops. The cold room offers more clues. \u201cThere\u2019s not too much beer in here now,\u201d Verstrepen says, pointing to a few full crates. And a stockroom contains hundreds of the black glasses used for blind tastings; they are opaque so tasters cannot see what they are sipping. His lab\u2019s freezers house about 30,000 types of yeast, including 1,000 strains used by brewers, bakers and others worldwide, and another 1,000 wild isolates from fruit, flowers, insects and even people. Many of these have been characterized for the genes known to influence taste and other traits that brewers care about. The lab is collaborating with yeast supplier White Labs in San Diego, California, and with Craig Venter\u2019s Synthetic Genomics in nearby La Jolla to build a family tree of industrial yeast. The balance in the freezers is made up by the lab\u2019s creations \u2014 completely new strains that have unique combinations of traits. The team makes them by mating different strains and then screening the offspring for the aromas they produce and, more recently, for genes that underlie these traits. This latter approach, known as genetic-marker-assisted breeding, is common in agriculture, and Verstrepen thinks that it will transform brewing. A Canadian brewery commissioned one of these custom strains when it wanted a full-flavoured Belgian \u2018tripel\u2019 that has less alcohol than is typical for the style. Another brewery has asked for yeast that makes chocolatey aromas, a request that has stumped the lab so far. \n               Mass mating \n             Using a robot that can accomplish hundreds of matings in a day, the lab generates many more strains than it can analyse or use in its taste tests. To cope with the glut, the researchers are developing microfluidic chips that can brew with 2,000 different yeasts at a time in 20-picolitre batches, each of which contains a single yeast cell. They can automatically test the resulting \u2018picobrews\u2019 for alcohol content and hope eventually to be able to measure the aroma compounds produced, too. At the other end of the spectrum, the lab will soon take delivery of a kit to make 500-litre batches of beer to better appreciate the challenges of industrial-scale brewing. Verstrepen\u2019s archive makes the lab a one-stop shop for brewers looking for unique flavours. For example, when Bouckaert wanted \u201cfunky\u201d aromas, such as the smell of barnyards or horse blankets, but without the usual accompanying fruity notes, he tried four of the lab\u2019s strains that showed some promise. None was going to work for a New Belgium beer \u2014 at least not yet. \u201cKevin\u2019s research is a little bit out there and on the edges of brewer\u2019s applications,\u201d Bouckaert says. \u201cBut that doesn\u2019t mean it will not translate to something that could be huge in the future.\u201d Natural variation in brewer\u2019s yeast offers much leeway to dial up and down flavours and other traits, but the approach can only go so far, Verstrepen says. Genetic-modification tools could improve on that. \u201cWith breeding we can increase flavour 10-fold, with genetic modification we can increase it 100-fold or 1,000-fold, and we\u2019ve done this,\u201d Verstrepen says. \u201cThe beers you make are more like banana milkshakes. Is this something we really want to do?\u201d Brewers are excited about the work, but the stigma that surrounds genetically modified (GM) foods means that the lab always uses more conventional techniques such as breeding and directed evolution to make strains destined for industry. And gene-editing techniques such as CRISPR could be used to introduce naturally occurring variants linked to flavour production into yeast strains that perform well but don\u2019t impart much flavour. This would accomplish the same goals as conventional breeding much more quickly. Food regulators are already grappling with the question of whether plants and livestock made this way count as GM foods, so it\u2019s not inconceivable that CRISPR beers could force the issue. A few craft breweries have asked the lab for GM yeast (they were turned down), but most of the brewing industry has little appetite for it, says Bouckaert. \u201cAmerican craft is pushing the boundaries, but on genetic modification, it\u2019s a no-no,\u201d he says. Multinational brewers are even more skittish about being linked to GM beer. Leuven-based AB InBev, the world\u2019s biggest brewing conglomerate and the maker of Budweiser and Stella Artois, confirmed that it works with Verstrepen. Philippe Malcorps, a senior scientist in the company\u2019s yeast and fermentation division, says that AB InBev is interested in tapping the diversity of wild yeasts to get flavours and other traits that would be impossible to find in classic brewing strains. Much of Verstrepen\u2019s work with such companies is covered by strict non-disclosure agreements. \u201cSome of our best applied research, we cannot publish, we cannot talk about,\u201d he says, \u201cThat is frustrating as a scientist, because you want to share.\u201d But smaller breweries tend to be more liberal. In fact, Orval, one of the six Belgian breweries run by Trappist monks, declined to sign a non-disclosure agreement or even a contract when it asked Verstrepen to sequence its house strain. \u201cIf you cannot trust monks any more, what are you going to do?\u201d says Verstrepen (who keeps his personal refrigerator stocked with Orval, one of his favourite beers). Bouckaert says that he was offered exclusive access to the yeast strains his company was trying, but he balked. \u201cI said, \u2018I\u2019m a craft brewer. I don\u2019t want exclusivity. I want you to send them to more people!\u2019\u201d The applied work has led to scientific insights. Verstrepen\u2019s team is still grappling with a project that originated from trouble-shooting a problem it heard about from brewers. Often, brewers reuse yeast to cut costs. But periodically, they start new cultures, growing them first on glucose (yeast\u2019s preferred food), then adding them to unfermented beer, which contains mainly maltose. Yeast sometimes responds sluggishly to the new food source, and it can take days before the beer is fermenting at full clip (and that leaves it prey to contaminants). \u201cYeast remember not only what they were eating, but also what their grand-grand-grandmothers were eating, up to five or six generations,\u201d Verstrepen says. Working out the mechanism, which seems to involve the inheritance of epigenetic modifications to DNA or proteins, is now a focus of the lab. Often, the lab has found, good beer and good science go hand in hand. For his PhD project, Mertens was tasked with creating new strains of the yeast used to make lagers. Lager is brewed at cooler temperatures than other beers, using a yeast that emerged several centuries ago when  S. cerevisiae  hybridized with a related cold-tolerant species called  Saccharomyces eubayanus . Although responsible for the vast majority of beer sales, lagers tend to have a limited flavour range. To make yeasts that might expand this palette, Mertens coaxed various strains of the two species into mating. Some beers made with these yeasts tasted of onion, and many fermented poorly, but one combined the bracing crispness of a good pilsner (a pale lager) with hints of fruity aromas that are untypical of the style (S. Mertens  et al .  Appl. Environ. Microbiol.   81 , 8202\u20138214; 2015 ). The beer was so popular among lab members that bottles vanished from the cold room. Several breweries are now testing that strain, known as H29. Mertens would love to see a commercial beer made from his creation, but he also hopes to sequence the genomes of the other yeasts he has created to work out how species hybridize \u2014 and maybe even unpick the conditions that spawned the first lager yeast. \u201cWe have new yeast and the brewers love it,\u201d he says. \u201cBut we\u2019re looking at the fundamentals of how these hybrids work. That goes beyond beer science.\u201d By the end of the morning\u2019s beer tasting, the spit buckets are filling up. Verstrepen has a meeting with a DNA-sequencing company, and Mertens and the other students have their research projects to get to. The lab might be a magnet for beer geeks, but it is no keg party. \u201cYes, you\u2019re working with a very fun product, but at the end of the day, it\u2019s genetics work,\u201d Verstrepen says. \u201cWe\u2019re not drinking for fun.\u201d At least, not until after work. \n                 Tweet \n                 Facebook \n                 LinkedIn \n                 Weibo \n                 Wechat \n               \n                     Cheesy, metallic, sweet: 170-year-old champagne is clue to winemaking\u2019s past 2015-Apr-20 \n                   \n                     Why fruitflies know their beer 2013-Dec-09 \n                   \n                     Beer gets fresh approach 2008-Jun-02 \n                   \n                     Intragenic tandem repeats generate functional variability 2005-Aug-07 \n                   \n                     The Verstrepen Lab \n                   Reprints and Permissions"},
{"file_id": "536139a", "url": "https://www.nature.com/articles/536139a", "year": 2016, "authors": [{"name": "Jeff Hecht"}], "parsed_as_year": "2006_or_before", "body": "Researchers are scrambling to repair and expand data pipes worldwide \u2014 and to keep the information revolution from grinding to a halt. On 19 June, several hundred thousand US fans of the television drama  Game of Thrones  went online to watch an eagerly awaited episode \u2014 and triggered a partial failure in the channel's streaming service. Some 15,000 customers were left to rage at blank screens for more than an hour. The channel, HBO, apologized and promised to avoid a repeat. But the incident was just one particularly public example of an increasingly urgent problem: with global Internet traffic growing by an estimated 22% per year, the demand for bandwidth is fast outstripping providers' best efforts to supply it. Although huge progress has been made since the 1990s, when early web users had to use dial-up modems and endure 'the world wide wait', the Internet is still a global patchwork built on top of a century-old telephone system. The copper lines that originally formed the system's core have been replaced by fibre-optic cables carrying trillions of bits per second between massive data centres. But service levels are much lower on local links, and at the user end it can seem like the electronic equivalent of driving on dirt roads. The resulting digital traffic jams threaten to throttle the information-technology revolution. Consumers can already feel those constraints when mobile-phone calls become garbled at busy times, data connections slow to a crawl in crowded convention centres and video streams stall during peak viewing hours. Internet companies are painfully aware that today's network is far from ready for the much-promised future of mobile high-definition video,  autonomous vehicles , remote surgery, telepresence and interactive 3D virtual-reality gaming. That is why they are spending billions of dollars to clear the traffic jams and rebuild the Internet on the fly \u2014 an effort that is widely considered to be as crucial for the digital revolution as  the expansion of computer power . Google has partnered with 5 Asian telecommunication companies to lay an 11,600-kilometre, US$300-million fibre-optic cable between Oregon, Japan and Taiwan that started service in June. Microsoft and Facebook are laying another cable across the Atlantic, to start service next year. \u201cThose companies are making that fundamental investment to support their businesses,\u201d says Erik Kreifeldt, a submarine-cable expert at telecommunications market-research firm TeleGeography in Washington DC. These firms can't afford bottlenecks. Laying new high-speed cable is just one improvement. Researchers and engineers are also trying several other fixes, from speeding up mobile networks to turbo-charging the servers that relay data around the world. \n               The fifth generation \n             For the time being, at least, one part of the expansion problem is comparatively easy to solve. Many areas in Europe and North America are already full of 'dark fibre': networks of optical fibres that were laid down by over-optimistic investors during the Internet bubble that finally burst in 2000, and never used. Today, providers can often meet rising demand simply by starting to use some of this dark fibre. But such hard-wired connections don't help with the  host of mobile phones, fitness trackers, virtual-reality headsets  and other gadgets now coming online. Data traffic from mobile devices is increasing by an estimated 53% per year \u2014 most of which will end up going through mobile-phone towers, or 'base stations', whose coverage is already spotty, and whose bandwidth has to be shared by thousands of users. The quality is spotty, as well. First-generation mobile-phone networks, introduced in the 1980s, used analogue signals and are long gone. But second-generation (2G) networks, which added digital services such as texting in the early 1990s, still account for 75% of mobile subscriptions in Africa and the Middle East, and are only now being phased out elsewhere. As of last year, the majority of mobile-phone users in Western Europe were on 3G networks, which were launched in the late 1990s to allow for more sophisticated digital services such as Internet access. The most advanced commercial networks are now on 4G, which was introduced in the late 2000s to provide smartphones with broadband speeds of up to 100 megabits per second, and is now spreading fast. But to meet demand expected by the 2020s, say industry experts, wireless providers will have to start deploying fifth-generation (5G) technology that is at least 100 times faster, with top speeds measured in tens of billions of bits per second. The 5G signals will also need to be shared much more widely than is currently feasible, says Rahim Tafazolli, head of the Institute for Communication Systems at the University of Surrey in Guildford, UK. \u201cThe target is how can we support a million devices per square kilometre,\u201d he says \u2014 enough to accommodate a burgeoning 'Internet of Things' that will range from networked household appliances to energy-control and medical-monitoring systems, and autonomous vehicles (see 'Bottleneck engineering'). The transition to 5G, like those to 3G and 4G before it, is being coordinated by an industry consortium that has retained the name Third Generation Partnership Project (3GPP). Tafazolli is working with this consortium to test a technique known as multiple-input, multiple-output (MIMO) \u2014 basically, a way to make each radio frequency carry many streams of data at once without letting them mix into gibberish. The idea is to put multiple antennas on both transmitter and receiver, creating many ways for signals to leave one and arrive at the other. Sophisticated signal processing can distinguish between the various paths, and extract independent data streams from each. MIMO is already used in Wi-Fi and 4G networks. But the small size of smartphones currently limits them to no more than four antennas each, and the same number on base stations. So a key goal of 5G research is to squeeze more antennas onto both. Big wireless companies have demonstrated MIMO with very high antenna counts in the lab and at trade shows. At the Mobile World Congress in Barcelona, Spain, in February, equipment-maker Ericsson ran live indoor demonstrations of a multiuser massive MIMO system, using a 512-element antenna to transmit 25 gigabits per second between a pair of terminals, one stationary and the other moving on rails. The system is one-quarter of the way to the 100-gigabit 5G target, and it transmits at 15 gigahertz, part of the high-frequency band planned for 5G. Japanese wireless operator NTT DoCoMo is working with Ericsson to test the equipment outdoors, and Korea Telecom is planning to demonstrate 5G services when South Korea hosts the next Winter Olympics, in 2018. Another approach is to make the devices much more adaptive. Instead of operating on a single, hard-wired set of frequencies, a mobile device could use what is sometimes called cognitive radio: a device that uses software to switch its wireless links to whatever radio channel happens to be open at that moment. That would not only keep data automatically moving through the fastest channels, says Tafazolli, but also improve network resilience by finding ways to route around failure points. And, he says, it's much easier to upgrade performance by replacing software than by replacing hardware. Meanwhile, a crucial policy challenge for the 5G transition is finding a  radio spectrum that offers adequate bandwidth and coverage . International agreements have already allocated almost every accessible frequency to a specific use, such as television broadcasting, maritime navigation or even radio astronomy. So final changes will have to wait for the 2019 World Radiocommunication Conference. But the US Federal Communications Commission (FCC) is trying to get a head start by auctioning off frequencies below 1 gigahertz to telecommunications companies. Once reserved for broadcast television because they are better than higher frequencies at penetrating walls and other obstructions \u2014 but no longer needed after television's shift to digital \u2014 these low frequencies are particularly attractive for serving sparsely populated areas, says Tafazolli: only a few base stations would be required to provide broadband service to households and driving data to autonomous cars on motorways. Other bands in the 1\u20136-gigahertz range could be opened up for 5G use as 2G and 3G technologies are phased out. But the best hope for dense urban areas is to exploit frequencies above 6 gigahertz, which are currently little-used because they have a very short range. That would require 5G base stations up to every 200 metres in dense urban areas, one-fifth the spacing typical of urban 4G networks. But the FCC considers the idea promising enough that on 14 July, it formally approved opening these frequencies for high-speed, fast-response services. Ofcom, the UK regulatory body, is considering similar steps. Companies are particularly interested in these higher frequencies as a way to extend 5G technology for other uses. In the United States, wireless carrier Verizon and a consortium of equipment-makers including Ericsson, Cisco, Intel, Nokia and Samsung have tested 28-gigahertz transmission at sites in New Jersey, Massachusetts and Texas. The system uses 5G technology to deliver data at 1 gigabit per second, and Verizon is adapting it for use in fixed wireless connections to homes, which it plans to test next year. The company has been pushing fixed wireless as an alternative to wired connections, because connection costs are much lower. \n               Bigger pipes \n             \u201cWhen I take out my cell phone, everyone thinks of it as a wireless communications device,\u201d says Neal Bergano, chief technology officer of TE SubCom, a submarine-cable manufacturer based in Eatontown, New Jersey. Yet that is only part of the story, he says: \u201cUsers are mobile, but the network isn't mobile.\u201d When someone uses their phone, its radio signal is converted at the nearest base station to an optical signal that then has to travel to its destination through fixed fibre optics. These flexible glass data channels have been the  backbone of the global telecommunications network  for more than a quarter of a century. Nothing can match their bandwidth: today, a single hair-thin fibre can transmit 10 terabits (trillion bits) per second across the Atlantic. That is the equivalent of 25 double-layer Blu-ray Discs per second, and is 30,000 times the capacity of the first transatlantic fibre cable, laid in 1988. Much of that increase came when engineers learned how to send 100 separate signals through a single fibre, each at its own wavelength. But as traffic continues to increase over heavily used routes, such as New York to London, that approach is coming up against some hard limits: distortion and noise that inevitably build up as light passes along thousands of kilometres of glass have made it effectively impossible to send more than 100 gigabits per second on a single wavelength. To overcome that limit, manufacturers have developed a new type of fibre. Whereas standard fibres send the light through a 9-micrometre-wide core of ultrapure glass running down the middle, the newer design spreads the light over a larger core area at lower intensity, reducing noise. The trade-off is that the new fibres are more sensitive to bending and stretching, which can introduce errors. But they work very well in submarine cables, because the deep sea provides a benign, stable environment that puts little strain on the fibre. Last year, networking-systems firm Infinera in Sunnyvale, California, sent single-wavelength signals at 150 gigabits per second through a large-area fibre for 7,400 kilometres \u2014 more than 3 times the distance possible with a standard fibre, and easily enough to cross the Atlantic. They also transmitted 200-gigabit-per-second signals a shorter distance. The highest-capacity commercial submarine cable now in service is the 60-terabit-per-second FASTER system that opened in June between Oregon and Japan. It sends 100-gigabit-per-second signals on 100 wavelengths in each of 6 pairs of large-core fibres. But in late May, Microsoft and Facebook jointly announced plans to beat it with MAREA: a large-area fibre cable spanning the 6,600 kilometres between Virginia and Spain. When completed in October 2017, the cable will link the two companies' data centres on opposite sides of the Atlantic at 160 terabits per second. Another approach to reducing performance-limiting noise was demonstrated last year by a group at the University of California, San Diego. Fibre-optic systems normally use separate lasers for each wavelength, but tiny, random variations can generate noise. Instead, the group used a technique known as a frequency comb to generate a series of uniformly spaced wavelengths from a single laser (  E. Temprana  et al .  Science   348 , 1445\u20131448; 2015). \u201cIt worked like a charm\u201d to reduce noise, says group member Nikola Alic, an electrical engineer. With further development, he says, the approach could double the data rate of fibre-optic systems. \n               Time of flight \n             Impressive bandwidth is useful, but promptness also matters. Human speech is so sensitive to interruption that a delay of one-quarter of a second can disturb a phone or video conversation. Video requires a fixed frame rate, so streaming video stalls when its input queue runs dry. To overcome such problems, FCC rules allow special codes that give priority passage for packets of data carrying voice calls or video frames, so that they flow quickly and uniformly through the Internet. New and emerging services including telerobotics, remote surgery, cloud computing and interactive gaming are also sensitive to network responsiveness. The time it takes for a signal to make a round trip between two terminals, often called latency, depends largely on distance \u2014 a reality that shapes the geography of the Internet. Even though data travel through fibre-optic cable at 200,000 kilometres per second, two-thirds the velocity of light in the open air, a person tapping a key in London would still need 86 milliseconds to get a response from a data centre in San Francisco, 8,600 kilometres away \u2014 a delay that would make cloud computing crawl. Emerging mobile applications require both broad bandwidth and low latency. Autonomous cars, for example, need real-time data on their environment to warn them about hazards, from potholes to accidents ahead. Conventional cars are becoming wireless nerve centres, needing low latency for 'hands-free' voice-control systems. A potentially huge challenge is the emergence of 3D virtual-reality systems. Interactive 3D gaming requires data to travel at 1 gigabit per second \u2014 20 times the speed of a typical video feed from a Blu-Ray Disc. But most crucially, the image must be rewritten at least 90 times per second to keep up with users turning their heads to watch the action, says computer scientist David Whittinghill of Purdue University in West Lafayette, Indiana. If the data stream slips behind, the user gets motion sickness. To keep that from happening, Whittinghill has installed a special 10-gigabit-per-second fibre line to his virtual-reality lab. To speed up responses, big Internet companies such as Google, Microsoft, Facebook and Amazon store replicas of their data in multiple server farms around the world, and route queries to the closest. Video cached at a local data centre is what allows viewers to fast-forward as if the file was stored on a home device, says Geoff Bennett, director of solutions and technology for Infinera. But the proliferation of these data centres is also one of the biggest drivers of bandwidth demand, he says: vendors' efforts to synchronize private data centres around the world now consume more bandwidth than public Internet traffic. The Microsoft\u2013Facebook cable is being built expressly for this purpose (see 'The submarine web'). So far, most data centres are where the customers and cables are: in North America, Europe and east Asia. \u201cMany parts of the world still rely on remote access to content that is not stored locally,\u201d says Kreifeldt. South America has few data centres, he says, so much of the content comes from well-wired Miami, Florida: traffic between Chile and Brazil might be routed through Miami to save money, but at a cost in latency. The same problem plagues the Middle East, where 85% of international traffic must travel to centres in Europe. That is changing, says Kreifeldt, but progress is slow. Amazon Web Services launched its first cloud data centre in India this year, in Mumbai; it has had a similar centre in S\u00e3o Paulo, Brazil, since 2011. \n               Internal communications \n             Bandwidth is also crucial on the very smallest scale: on and between the chips in the banks of servers in a data centre. Expanding the flow here can help information to move more quickly within the data centres and get out to users faster. Chip clock speeds \u2014 how fast the chip runs \u2014 flat-lined at a few gigahertz several years ago, because of heating problems. The most practical way to speed up processors significantly is to divide the operations that they perform between multiple 'cores': separate microprocessors operating in parallel on the same chip. That requires high-speed connections within the chip \u2014 and one way to make them is with light, which can move data faster than electrons can. The biggest obstacle has been integrating microscale optics with silicon electronics. After years of research on 'silicon photonics', engineers have yet to find a way to efficiently generate light from silicon, a key step in optical information processing. The  best semiconductor light sources, such as indium phosphide , can be bonded to silicon chips, but are very difficult to grow directly on silicon, because their atoms are spaced differently. Optical and electronic components have been integrated on indium phosphide, but so far only on a small scale. In an effort to scale up photonic integration to a commercial level, the United States last year launched the American Institute for Manufacturing Integrated Photonics in Rochester, New York, which is supported by $110 million from federal agencies and $502 million from industry and other sources. Its target is to develop an efficient technology to make integrated photonics for high-speed applications, including optical communications and computing. Separately, a Canadian-funded team earlier this year demonstrated a photonic integrated circuit with 21 active components that could be programmed to perform 3 different logic functions (  W. Liu  et al .  Nature Photon.   10 , 190\u2013195; 2016). That's an important step for photonic microprocessors, comparable in complexity to the first programmable electronic chips that opened the door to microcomputers. \u201cCompared to current electronics, it's simple, but compared to photonic integrated circuits it is quite complicated,\u201d says study co-author Jianping Yao, an electrical engineer at the University of Ottawa in Canada. Further development could lead to varied applications. For example, Yao says that after the chip is optimized for manufacture, it could convert a 5G smartphone signal received at a base station into an analogue optical signal, which could be transmitted by fibre optics to a central facility, and then digitized. The quest for faster chips, like other parts of the Internet problem, is a daunting challenge. But researchers such as Bergano see a lot of potential for improvements. After 35 years of working on fibre optics, he says, \u201cI remain a complete optimist when I think about the future.\u201d \n                 Tweet \n                 Facebook \n                 LinkedIn \n                 Weibo \n                 Wechat \n               Reprints and Permissions"},
{"file_id": "536266a", "url": "https://www.nature.com/articles/536266a", "year": 2016, "authors": [{"name": "Mark Peplow"}], "parsed_as_year": "2006_or_before", "body": "Polymers have infiltrated almost every aspect of modern life. Now researchers are working on next-generation forms. Hermann Staudinger was a pacifist, but this was one fight he was determined to win. In 1920, the German chemist proposed that polymers \u2014 a broad class of compounds that included rubber and cellulose \u2014 were made of long chains of identical small molecules linked by strong chemical bonds 1 . Most of his colleagues thought this was arrant nonsense, and argued that polymers were merely looser aggregations of small molecules. Staudinger refused to back down, sparking feuds that spanned a decade. Eventually, laboratory data proved that he was right. He won the 1953 Nobel Prize in Chemistry for his work, and synthetic polymers are now ubiquitous: last year, the world produced about 300 million tonnes of them. The molecular chains that Staudinger hypothesized have entered almost every aspect of modern life, from clothes, paint and packaging to drug delivery, 3D printing and self-healing materials. Polymer-based composites even make up half the weight of Boeing\u2019s most recent passenger aeroplane, the 787\u00a0Dreamliner. So where will polymers go next? Some answers will come this week, when a once-per-decade workshop organized by the US National Science Foundation attempts to survey which new areas are emerging. \u201cThe general trend \u2014 still continuing \u2014 is the expansion of polymers into applications that have not been traditionally theirs,\u201d says Tim Lodge, a polymer chemist at the University of Minnesota in Minneapolis and editor of the journal  Macromolecules . That expansion has been driven by advances in every aspect of polymer science, he says. Researchers have developed new methods to synthesize and analyse molecules, improved theoretical models and created mimics of polymers found in nature. At the same time, says Lodge, attitudes to the science have changed. No longer do universities dismiss polymer science as too dirty, practical and industrial for academia. \u201cJust about every chemistry department has someone doing polymer stuff now,\u201d he says, and frontier work on polymers is increasingly interdisciplinary. It will need to be. Researchers have a growing toolbox of techniques with which to craft the chemical architecture of polymer strands, but they are often unable to predict whether the resulting compound will have the particular properties needed for, say, a membrane or a drug-delivery system. Meeting that challenge will demand a much deeper understanding of how the chemical structure of a polymer determines its physical properties, at every scale from nanometres to metres. \n               Polymers forever \n             Polymers are everywhere \u2014 and therein lies the problem. \u201cMost polymers we use in everyday life are from petroleum-based products, and although they\u2019re durable in use, they\u2019re also durable in waste,\u201d says Marc Hillmyer, director of the Center for Sustainable Polymers (CSP) at the University of Minnesota. An estimated 86% of all plastic packaging is used only once before it is discarded 2 , producing  a stream of waste that persists in waterways and landfill , releases pollutants and  harms wildlife . That is why the past decade has seen an explosion of interest in polymers that are  made from renewable resources and biodegrade easily and harmlessly . Polymers based on natural starch are already on the market; so too is synthetic polylactide (PLA), which is made from lactide or lactic acid derived from biological sources, and which is found in products from tea bags to medical implants. But sustainable polymers still make up less than 10% of the total plastics market, says Hillmyer. One hurdle is that they cost too much. Another is that the monomer building blocks of natural polymers tend to contain more oxygen atoms than are found in the fossil hydrocarbons of petroleum. This affects the polymers\u2019 properties \u2014 stiffening the materials, for example \u2014 which can make it difficult for them to directly replace cheap and flexible plastics such as polyethylene and poly\u00adpropylene. Turning natural polymers into exact molecular matches for conventional ones takes some sophisticated chemistry. One alternative approach is to beef up sustainable polymers such as PLA by blending them with conventional polymers. This route typically has downsides, such as rendering some plastics less transparent. But CSP researchers have got around that problem by adding just 5% by weight of a low-cost, petroleum-derived polymer that contains some sections that are hydrophobic \u2014 water-insoluble \u2014 and others that are hydrophilic, or water-soluble 3 . These additives cluster together to create spherical structures, which render PLA substantially tougher without reducing its transparency. Hillmyer\u2019s team has also made 4  a partially recyclable form of polyurethane foam, which is found in a host of products, including insulation, seat cushions and gaskets. The recipe for this polyurethane includes a low-cost poly\u00admer called poly(\u03b2-methyl-\u03b4-valerolactone) (PMVL), based on monomers made by modified bacteria. Heating the foam to above 200\u2009\u00b0C breaks down the polyurethane so that the monomers can be extracted and used again. It remains to be seen whether these sustainable polymers can be commercialized. \u201cOften the biggest challenge is to do it at scale, which requires favourable economics,\u201d says Hillmyer. He thinks the field needs to establish general design rules that predict how a monomer\u2019s chemical structure affects the rate, temperature and yield of polymerization reactions, and how the resulting polymers will interact with other materials. His team has developed such guidelines for PMVL\u2019s constituents 5 , and last year formed a spin-off company at the CSP called Valerian Materials to exploit these principles. Some researchers are pursuing another trick: rather than stringing together bioderived monomers, they are learning to use natural polymers directly. Cellulose, for example, consists of glucose molecules strung together into chains, which in turn line up to form strong fibres, or fibrils, that make up the stiff cell walls of plants. In many places, the cellulose chains form crystalline chunks that are up to 20\u2009nanometres wide and hundreds of nano\u00admetres long, and that can be chemically extracted from cellulose pulp. Proponents say that these crystals could be used for applications such as strengthening composites, forming insulating foams, delivering drugs and providing a scaffold for tissue repair 6 . Cellulose nanocrystals and longer nano\u00adfibrils are now produced on a commercial scale, but the commercial applications do not yet go much beyond stiffening paper or thickening fluids. Christoph Weder, director of the Adolphe Merkle Institute for nanoscience at the University of Fribourg in Switzerland, says that it will take a lot more work to reduce costs and demonstrate unique advantages for sustainable polymers. \u201cWe really need a road map for biobased polymers,\u201d he says. \n               Skin in the game \n             In a mixed-up world, polymers can restore some order. Polymer membranes already serve as  molecular sieves for separating gases, de\u00adsalinating seawater and keeping molecules apart inside fuel cells . But they could have a much bigger impact in the future, says Lodge. \u201cThere are so many problems that could be solved by better membranes.\u201d Separating mixtures with membranes  takes a lot less energy than does distillation, in which a liquid is heated to evaporate its components at different temperatures. It also requires much less space than using scrubbers, devices in which pollutants are trapped by chemical reactions. Membranes made from polymers are not only cheap to make at large scale, but can cover large areas without acquiring structural defects that let the wrong molecules pass through. Gas-separation membranes are already used industrially to tease hydrogen and carbon dioxide from natural gas. But improved membranes could tackle harder tasks, such as distinguishing between the very similar hydrocarbons propane and propene. Tougher, chemically robust membranes could operate at higher temperatures to remove carbon dioxide from hot flue gases. Membrane chemist Benny Freeman of the University of Texas at Austin is hoping to improve the treatment of waste water from gas fracking operations, in which water is forced into rock to split it open and release natural gas. After use, the water is so dirty that standard filtration membranes quickly get clogged, so the water must be put under high pressure to push it through, and the membranes must be cleaned with chemicals that shorten their lifespan. But Freeman has found a way to sidestep that problem by giving the membranes a gossamer-thin coating of polydopamine, which mimics the waterproof glue used by mussels to cling onto rocks. Piloted at a fracking water-treatment facility near Fort Worth, Texas, the polydopamine coating halved the pressure needed to push water through the membrane, which could result in smaller, more efficient treatment systems 7 . The team has already used these membranes to build units for the US Navy, so that ships can purify oily bilge water before dumping it. In December 2015, the US presidential administration launched a \u2018moonshot for water\u2019 to boost water sustainability, and as part of that effort the US Department of Energy plans to establish a desalination-research hub in 2017. Polymer membranes will have a big role in that effort, says Freeman. \u201cWe\u2019re slated to see a huge increase in efforts to expand the use of polymers in that space.\u201d To design better desalination membranes, researchers will need to be able to predict how factors such as the distribution of charged chemical groups in a polymer affects its permeability to ions. Earlier this year, Freeman and his colleagues published 8  what he believes is the first model to do just that, which could enable chemists to build particular properties into a membrane by tailoring its chemical substituents and cross-linking the molecules. \u201cI\u2019m on a mission to get people to ask these kinds of questions about structure\u2013property relations, which could really guide synthesis,\u201d he says. The ultimate separation membrane could be just one molecule thick. These 2D polymers are surfing the  wave of enthusiasm for single-layer materials  that followed the isolation of graphene just over a decade ago. The flat polymers are not just very thin films of ordinary, linear polymers. Instead, they have an intrinsically 2D chemical structure that looks like a fishing net, with a regular, repeating mesh full of molecule-size openings. They can also carry a wide variety of chemical decorations on their surfaces, so that each opening can be precisely engineered to allow certain molecules through and bar others. But creating 2D polymers is tough. If just one of the holes in the growing mesh closes up in the wrong way, the membrane could buckle into a 3D mess. Polymer chemist Dieter Schl\u00fcter of the Swiss Federal Institute of Technology in Zurich worked on this problem for more than a decade before achieving success in 2014. His approach relies on coaxing carefully designed monomers to form a crystal. A blast of blue light then triggers a chemical reaction between monomers in the same plane, creating a new crystal made up of stacked polymer layers. These can be peeled off to give individual 2D sheets just one monomer thick (see \u2018Chemical peel\u2019). Using the same approach, Schl\u00fcter and Benjamin King, head of the chemistry department at the University of Nevada, Reno, have independently produced different types of 2D\u00a0polymer 9 , 10 . Now collaborators, the two researchers hope that they will soon be able to make these sheets in kilogram batches, easily enough to distribute samples to research groups around the world. Schl\u00fcter admits that he has faced scepticism about whether 2D polymers will flourish. \u201cBut that\u2019s healthy,\u201d he says. \u201cAnd I\u2019m very stubborn\u00a0\u2014 I will not give up, I\u2019m convinced of the great potential this development has.\u201d \n               Boutique polymers \n             Widely used polymers such as polystyrene and polyethylene are spectacularly boring in one sense: they repeat the same monomer over and over again. Their one-note tune is especially monotonous when compared with the quadraphonic symphony of DNA, which encodes an entire genome with 4 monomers; or the baroque masterpiece of a protein, drawing from 23 amino acids to build a complex 3D structure. One of the most challenging frontiers of polymer research is to tailor synthetic polymers with the same precision, so that chemists can fine-tune the electronic and physical properties of their products. \u201cIt\u2019s become very fashionable in the past five years,\u201d says Jean-Fran\u00e7ois Lutz, a macromolecular chemist at the University of Strasbourg in France. Sequence-controlled polymers would contain monomers in a predetermined order, forming strands of a very specific length. Last year, a team led by Jeremiah Johnson, a chemist at the Massachusetts Institute of Technology in Cambridge, showed 11  that they could achieve that kind of control through iterative exponential growth \u2014 first uniting two different monomers to make a dimer, then connecting two dimers to make a tetramer, and so on. Modifying each monomer\u2019s chemical side-chains between cycles adds complexity, and a semi-automated system can make the process less laborious 12 . Johnson is now studying how his sequence-controlled polymers might be used in drug delivery. A dozen drugs approved by the US Food and Drug Administration use a polymer called polyethylene glycol to shield them from the body\u2019s immune system, improve their solubility or prolong their time in the body. Johnson says that a sequence-controlled polymer could provide a more predictable biological effect, because every strand would be the same length and shape, and its chemistry could be carefully designed to assist its drug cargo in the most useful way. Sequence-controlled polymers could also store data in a more compact and inexpensive form than can conventional semiconductor technology, with each monomer representing a single bit of information. Last year, Lutz demonstrated 13  a key step towards that goal. He used two types of monomer to represent digital 1s or 0s, and a third to act as a spacer between them. The monomers contained chemical groups that allowed them to connect only to the growing polymer, rather than reacting with each other randomly. The string of 1s and 0s could be read by watching how the polymer broke apart inside a mass spectrometer. Earlier this month, Lutz showed that a library of different polymer strands could encode a 32-bit message 14 . That pales by comparison with the 1.6\u2009gigabits that have been  stored in artificial DNA molecules  (see  go.nature.com/2b2ve0u ). But momentum is growing for polymer data storage. In April, the Intelligence Advanced Research Projects Activity (IARPA), a US agency that funds high-risk research for the intelligence community, drew representatives from the biotechnology, semiconductor and software industries to a workshop on the subject. \u201cThere\u2019s a vibrant and growing community of researchers working on this,\u201d says David Markowitz, a technical adviser at IARPA who helped to organize the workshop. But the approach still faces enormous technical challenges: current synthetic techniques are much too slow and expensive. The key to cracking the data-storage problem \u2014 and many other problems at the polymer frontier \u2014 will be to develop better ways to predict the properties of polymers and fine-tune their production. That will require a concerted effort. \u201cWe need to establish collaborations with physicists, materials scientists, theoretical chemists,\u201d says Lutz. \u201cWe need to build a new field.\u201d\n \n                 Tweet \n                 Follow @NatureNews \n               \n                     Seven chemical separations to change the world 2016-Apr-26 \n                   \n                     Hacked molecular machine could pump out custom proteins 2015-Jul-29 \n                   \n                     Plastic fantastic 2013-Jul-24 \n                   \n                     A fresh chapter for organic data storage 2012-Aug-19 \n                   \n                     New desalination technique yields more drinkable water 2012-May-23 \n                   \n                     Puzzle persists for 'degradeable' plastics 2011-Apr-21 \n                   \n                     CelluForce \n                   \n                     White House \u2018moonshot for water\u2019 \n                   \n                     IARPA workshop on polymer-based data storage \n                   Reprints and Permissions"},
{"file_id": "536263a", "url": "https://www.nature.com/articles/536263a", "year": 2016, "authors": [{"name": "Daniel Cressey"}], "parsed_as_year": "2006_or_before", "body": "Scientists know that there is a colossal amount of plastic in the oceans. But they don\u2019t know where it all is, what it looks like or what damage it does. Kamilo beach, on the tip of Hawaii\u2019s Big Island, is a remote tropical shore. It has white sand, powerful waves and cannot be reached by road. It has, in fact, much that an idyllic tropical beach should have. But there is one inescapable issue: it is regularly carpeted with plastic. Bottles, fishing nets, ropes, shoes and toothbrushes are among the tons of waste washed up here, thanks to a combination of ocean currents and local eddies. A study in 2011 reported that the top sand layer could be up to 30% plastic by weight 1 . It has been called the dirtiest beach in the world, and is a startling and visible demonstration of how much plastic detritus humanity has dumped into the world\u2019s oceans. From Arctic to Antarctic, from surface to sediment, in every marine environment where scientists have looked, they have found plastic. Other human-generated debris rots or rusts away, but plastics can persist for years, killing animals, polluting the environment and blighting coastlines. By some estimates, plastics comprise 50\u201380% of the litter in the oceans. \u201cThere are places where you don\u2019t find plastic,\u201d says Kara Lavender Law, an oceanographer at the Sea Education Association in Woods Hole, Massachusetts. \u201cBut in terms of the different marine reservoirs, we\u2019ve found plastic in all of them. We know it\u2019s pervasive.\u201d Newspapers tell stories of the \u2018Great Pacific garbage patch\u2019, a region of the central Pacific where plastic particles accumulate, and volunteers participate in beach clean-ups across the globe. But in many ways, research lags behind public concern. Scientists are still struggling to answer the most basic questions: how much plastic is in the oceans, where, in what form and what harm it\u2019s doing. That\u2019s because science at sea is hard, expensive and time-consuming. It is difficult to comprehensively survey vast oceans for small\u00a0\u2014\u00a0sometimes microscopic\u00a0\u2014\u00a0plastic fragments, and few researchers have made this their line of work. But now interest is picking up. \u201cThere have been more publications in the last four years than the previous four decades,\u201d says Marcus Eriksen, director of research and co-founder of the 5\u00a0Gyres Institute in Santa Monica, California, which works to fight plastic pollution. Scientists and environmentalists know that there is a lot to do. Last May, the United Nations Environment Programme (UNEP) passed a resolution at its Nairobi meeting, stating that \u201cthe presence of plastic litter and microplastics in the marine environment is a rapidly increasing serious issue of global concern that needs an urgent global response\u201d. \n               Where does it come from? \n             In 2014, a team at the US marine park Papah\u0101naumoku\u0101kea, off the northwest coast of Hawaii, removed a fishing net from the reserve that weighed 11.5 tonnes\u00a0\u2014\u00a0roughly equivalent to a London bus. Nets and other fishing equipment that have been lost or discarded at sea are thought to make up a large fraction of marine plastic. An estimate 2  from UNEP suggests that this \u2018ghost\u2019 fishing gear makes up 10% of all marine litter, or around 640,000 tonnes. There is much more than that. Global production of plastics rises every year\u00a0\u2014\u00a0it is now up to around 300 million tonnes\u00a0\u2014\u00a0and much of it eventually ends up in the ocean. Plastic litter is left on beaches, and plastic bags blow into the sea. The vast quantities of plastics dumped as landfill can, if sites are not properly managed, easily wash or blow away. Some sources are less obvious: as tyres wear down, they leave tiny fragments on roads that leach into drains and on into the ocean. In a 2014 paper, Eriksen and his team analysed data on the items found in a series of expeditions across the world\u2019s oceans and estimated that 87% by weight of floating plastic was greater than 4.75 millimetres in size 3 . The list included buoys, lines, nets, buckets, bottles and bags (see \u2018A sea of plastic\u2019). But when the pieces were counted instead of weighed, large plastics made up just 7% of the total. Many plastic items break down under the onslaught of sunlight and waves until they eventually reach microscopic sizes, and other plastics are small from the start, such as the \u2018microbeads\u2019 that are added to face scrubs and other cosmetic products, and that go down the drain. Concern about these microplastics has been growing ever since 2004, when Richard Thompson, who researches ocean plastic at Plymouth University in the United Kingdom, coined the term. (It is now often used to refer to pieces less than 5\u00a0millimetres across.) His team found microplastics in most of the samples it took from 18 British beaches, as well as in plankton samples collected from the North Sea as far back as the 1960s 4 . Since then, the number of papers using the term has rocketed, and researchers are attempting to answer questions ranging from how toxic the materials are, to how they are distributed around the world. \n               How much is out there? \n             If surveying the ocean for plastic is expensive and difficult at the surface, it\u2019s even harder below it: researchers lack samples from enormous areas of the deep sea that have never been explored. And even if they could survey all these regions, the concentration is typically so dilute that they would have to test huge volumes of water to get reliable results. Instead, they are forced to estimate and extrapolate. In a paper published last year, a team led by Jenna Jambeck, who researches waste management at the University of Georgia in Athens, estimated how much waste coastal countries and territories generate, and how much of that could be plastic that ends up in the ocean 5 . The group reached a figure of 4.8 million to 12.7\u00a0million tonnes every year\u00a0\u2014\u00a0very roughly equivalent to 500 billion plastic drinks bottles. But her estimate excluded the plastic that gets lost or dumped at sea, and all the plastic that is already there. To get a handle on this, some researchers have gone trawling, using fine-meshed nets to see what plastic they can catch. Last year, oceano\u00adgrapher Erik van Sebille of Imperial College London and his colleagues published one of the largest collections of such data 6 . They combined information from 11,854 individual trawls, from every ocean except the Arctic, to produce a \u2018global inventory\u2019 of small plastic pieces floating at or near the surface. They estimated that, in 2014, there were between 15 trillion and 51\u00a0trillion pieces of microplastic floating in the oceans, with a total weight of 93,000 to 236,000 tonnes. But these numbers present scientists with a problem. This estimate of total surface plastic is just a small fraction of what Jambeck estimated entered the ocean every year. So where is all the rest? \u201cThat\u2019s the big question,\u201d says Jambeck. \u201cThat\u2019s a tough one.\u201d Researchers are trying to find answers. Jambeck is now working with a mobile-phone app called the Marine Debris Tracker, which offers a way to crowdsource vast amounts of data as users send in information about rubbish they encounter. She is also working on a project for UNEP to build a global database of marine-litter projects. \n               Where is it? \n             The mismatch between the estimated amount of plastic entering the oceans and the amount actually observed has come to be known as the \u2018missing plastic\u2019 problem. Adding to the puzzle, data from some locations do not show a clear increase in plastic concentrations over recent years, even though global production of the materials is soaring. Public attention has focused on the Great Pacific garbage patch, where plastics collect thanks to an ocean current called a gyre. The name is something of a misnomer\u00a0\u2014\u00a0visitors to the patch would not find piles of seaborne rubbish. A study from 2001 reported 334,271\u00a0pieces of plastic per square kilometre in the gyre 7 . This is the largest tally recorded in the Pacific Ocean, but still works out as roughly one small fragment for every three square metres. Modelling by van Sebille and his colleagues suggest that concentrations could be several orders of magnitude higher in the Pacific garbage patch, and an equivalent zone in the North Atlantic, than elsewhere. But the plastic here is accounted for in surveys, whereas the missing plastic is, by definition, missing and therefore somewhere else. Some of it is probably on the sea floor. Certain types of plastic sink, and even ones that start out floating can eventually become covered with marine organisms and be pulled down. Work from Thompson has shown microplastics in deep-ocean sediment\u00a0\u2014\u00a0an under-studied zone that could be hiding some of the missing millions of tonnes 8 . Remotely operated vehicles also regularly find large plastic items among the litter that has sunk into the deepest ocean trenches. A substantial portion of ocean plastic may simply end up on shorelines, and other plastic \u2018sinks\u2019 are uncovered all the time. In 2014, Thompson co-authored a paper showing that microplastics had accumulated in Arctic sea ice at concentrations several orders of magnitude greater than that found even in highly contaminated surface waters 9 . \u201cWe have a lot of educated guesses\u201d about where the missing plastic is, says Law. \u201cIn my mind, we don\u2019t have the answer to that.\u201d Thompson and others are now looking beyond microplastics to nanoplastics\u00a0\u2014\u00a0ones less than 100 nanometres in size. \u201cNano-sized particles of plastic are being manufactured,\u201d says Thompson. \u201cSo it\u2019s highly likely that some will escape into the environment. There\u2019s also the fragmentation of larger items.\u201d But nanoplastics are proving hard to study. Researchers commonly use a type of spectroscopy to confirm whether fragments recovered from the sea are made of plastic, but the method does not work well on pieces below about 10\u00a0micrometres, Thompson says. He hopes to learn more as part of a UK-government-funded project called RealRiskNano, which will look at sources and pathways to the environment for these tiny fragments. \u201cIt wouldn\u2019t surprise me to find they do exist. But at the moment it\u2019s below the level of detection from an environmental sample.\u201d \n               What harm does it do? \n             Researchers know that marine plastic can harm animals. Ghost fishing gear has trapped and killed hundreds of animal species, from turtles to seals to birds. Many organisms also swallow pieces of plastic, which can accumulate in their digestive system. According to one often-quoted figure, around 90% of seabirds called fulmars washed ashore dead in the North Sea had plastic in their guts. What\u2019s less clear is whether this pollution has major impacts on populations. Lab studies have demonstrated the toxicity of microplastics, but these often use concentrations that are much higher than those found in the oceans. In February this year, though, Arnaud Huvet, who studies invertebrates at France\u2019s national marine research agency (Ifremer) in Plouzan\u00e9, published work in which he exposed Pacific oysters to microplastics at concentrations similar to those found in the sediment where the creatures live. Animals in the plastic-laced water had poorer-quality eggs and sperm and produced 41% fewer larvae than did those in a control group 10 . It was one of the first studies to show a direct link between plastic and fertility problems. \u201cThat made an impact,\u201d van Sebille says. So did a study in June from fish ecologists Oona L\u00f6nnstedt and Peter Ekl\u00f6v, in which they exposed perch larvae to \u2018environmentally relevant\u2019 concentrations of microplastics. The larvae ate the plastics\u00a0\u2014\u00a0they even seemed to prefer them to actual food\u00a0\u2014\u00a0which made them grow more slowly and fail to respond to the odour of predators. After 24 hours in a tank with a predator, 34% of plastic-dosed larvae survived, compared with 46% of those raised in clean water 11 . L\u00f6nnstedt, at Uppsala University in Sweden, was disturbed by photos of the transparent larvae clearly showing the small plastic spheres in their guts. \u201cIt\u2019s awful, so of course I feel strongly about it,\u201d she says. \u201cPeople who say plastics won\u2019t be an issue in the oceans need to take a look at the evidence again.\u201d But some scientists question the implications of the work. Alastair Grant, an ecologist at the University of East Anglia in Norwich, UK, says that the levels of plastic that gave adverse effects in L\u00f6nnstedt\u2019s paper\u00a0\u2014\u00a010\u201380 particles per litre\u00a0\u2014\u00a0are still orders of magnitude higher than the vast majority of field measurements. Most reports are less than 1 particle per litre, he says. \u201cThe evidence I can see at the moment suggests microplastics are probably within safe environmental limits in most places.\u201d \n               What should we do? \n             Despite the lack of comprehensive data about ocean plastics, there is a broad consensus among researchers that humanity should not wait for more evidence before taking action. Then the question becomes, how? One controversial project has been devised by The Ocean Cleanup, a non-profit group that by 2020 hopes to deploy a 100-kilometre-long floating barrier in the Great Pacific garbage patch. The group claims that the barrier will remove half of the surface plastic there. But the project has met with scepticism from researchers. They say that plastic in the gyre is so dilute that it will be tough to scoop up, and they worry that the barrier will disturb fish populations and plankton. Boyan Slat, chief executive of The Ocean Cleanup, welcomes the criticism, but says that the barrier project is still in an early phase, with a prototype currently deployed off the Dutch coast. \u201cWe\u2019re using this test as a platform to investigate whether there\u2019s any negative consequences. The only way to find out is to go out and do it,\u201d he says. In a paper published earlier this year 12 , van Sebille and his colleague Peter Sherman showed that it would be much more effective to place clean-up equipment near the coasts of China and Indonesia, where much of the plastic pollution originates. \u201cThe closer to the plastic economy loop you intervene the better it is,\u201d van Sebille says. \u201cWe\u2019ve got to stop it in the treatment plants, in the landfills. That is the point to intervene.\u201d Eriksen likens the situation to addressing air pollution, where people have long realized that filtering the air is not a long-term solution. Filtering the oceans seems similarly implausible, he says. \u201cWhat we\u2019ve seen worldwide is you go to the source.\u201d That means reducing the use of plastic, improving waste management and recycling the materials to stop them from reaching the water at all. That\u2019s a lot to ask, considering how ubiquitous plastics are. But some scientists allow themselves to imagine a world where plastics have been brought under control. According to research by Law and Jan van Franeker, some types of floating plastic might disappear in just a few years 13 . Perhaps even Kamilo beach would eventually return to its unpolluted form. But plastic will have left its mark, as layers of tiny particles embedded in sediment on the ocean floor. Over time, this plastic will become cemented into Earth\u00a0\u2014\u00a0a legacy of the plastic era. \u201cThere will be this layer of rock around the world that is going to be plastic,\u201d Eriksen says. \n                 Tweet \n                 Follow @NatureNews \n                 Follow @DPCressey \n               \n                     Man-made pollutants found in Earth's deepest ocean trenches 2016-Jun-20 \n                   \n                     Plastic waste taints the ocean floors 2014-Dec-17 \n                   \n                     Fate of ocean plastic remains a mystery 2014-Dec-10 \n                   Reprints and Permissions"},
{"file_id": "535480a", "url": "https://www.nature.com/articles/535480a", "year": 2016, "authors": [{"name": "Quirin Schiermeier"}], "parsed_as_year": "2006_or_before", "body": "To tell whether the island\u2019s glacial cap will melt away any time soon, researchers are poring over old pictures and drawings for clues to its past behaviour. Anders Bj\u00f8rk could have stepped out from a painting of a nineteenth-century Arctic expedition. Tall, athletic and burning for outdoor action, he has been attracted to the wild fjords and glaciers of Greenland since his university days, when he took summer tourists on guided tours there. He still ventures to the island every summer to measure its waning ice sheet \u2014 and he doesn\u2019t always travel by air. In 2014, he retraced the route of early Danish explorers by sailing to Greenland in a three-masted wooden schooner built in the 1930s. But on a pleasant spring day in Copenhagen, Bj\u00f8rk is carrying out a much safer reconnaissance. In the belly of Denmark\u2019s national archive, he sifts through yellowed files filled with rows of daily weather and ocean-temperature measurements that Danish clergymen and village chiefs made decades ago along the Greenland coast. \u201cI get a kick out of sitting over old maps and documents,\u201d he says. \u201cI somehow find that kind of stuff just as fascinating as doing field work.\u201d Bj\u00f8rk has alternated expeditions to the archives and to distant glaciers for ten years, ever since he first came across tens of thousands of aerial photographs of Greenland from the 1930s, lying all but forgotten in a seventeenth-century fortress outside Copenhagen. Bj\u00f8rk and his colleagues at the Natural History Museum of Denmark in Copenhagen are ice historians. They are combing the old records to document how Greenland\u2019s ice sheet and glaciers have behaved since the nineteenth century \u2014 a crucial set of information for climate scientists trying to predict how it might change in the future. With  Arctic temperatures rising faster  than anywhere else on Earth, Greenland is now losing about 200 billion tonnes of ice per year and raising ocean levels around the globe. Projections suggest that melting from the island might swell sea levels by 30 centimetres by the end of this century. If all Greenland\u2019s ice melted \u2014 a possibility over the next few centuries \u2212 it would push up sea level by more than 6 metres, enough to flood coastal megacities such as New York and Miami. But the projections carry large uncertainties, in part because researchers lack basic information about Greenland\u2019s past. Satellite data only go back 40 years, which is why Bj\u00f8rk and his colleagues are poring over 180,000 photographs and other data that record how glaciers have advanced and retreated during cold and warm spells in the recent past. Their first sets of findings suggest that Greenland ice has responded more strongly to past climate changes than was previously realized. Now, the researchers are trying to unravel what factors within the oceans, atmosphere and inside glaciers control their behaviour. What sets Bj\u00f8rk apart, say other scientists, is that he combines the heart of a seafarer with a strong sense of detail and creativity in research. The studies by him and his fellow ice historians are making key contributions to glacial science, says Beata Csatho, a glaciologist at the University at Buffalo in New York. \u201cCollecting historical data sets is probably more important at this point than having yet another satellite do more of the same stuff,\u201d she says. \u201cThis is not incremental progress \u2014 it is a big advance in science.\u201d \n               Epic adventures \n             Bj\u00f8rk\u2019s fascination with Arctic exploration reaches back to his childhood. Like many Danes, he learned to sail at a young age and over the years he embarked on a series of increasingly epic voyages that blended science and adventure. In Bj\u00f8rk\u2019s most extreme expedition, he and a small crew of like-minded scientists sailed through the storm-tossed south Atlantic for two months in 2011. Svante Bj\u00f6rck, a quaternary geologist at the University of Lund in Sweden, had chartered a two-masted ship to sail from the Falkland Islands to Tristan da Cunha, the most remote inhabited island in the world. During the 13-day trip, Bj\u00f8rk spent several nights awake on deck, tied to the mast during gales, helping the captain steer the ship through huge waves. \u201cIt was scary. I\u2019ve never seen anything like this,\u201d he says. The return journey to Uruguay took 33 days against relentlessly unfavourable trade winds. Yet the rich yield in climate data and the unforgettable adventure was well worth the ordeal, he says. The son of two school teachers, Bj\u00f8rk grew up in Svendborg, a small harbour town in southern Denmark with a long seafaring tradition. Eager to explore the world, he studied geography and became enthralled with Greenland, which has been  part of the kingdom of Denmark  since the early 1800s. While serving as a student assistant to Quaternary geologist Kurt Kj\u00e6r of the Natural History Museum of Denmark, Bj\u00f8rk learned to mix traditional geology and expedition-era geography with modern high-tech data. His interest in historic records was piqued ten years ago when he first heard tales of a cache somewhere in Copenhagen filled with aerial images of Greenland taken in the 1930s during mapping expeditions. He eventually discovered that the formerly classified images had been locked away in a ramshackle citadel outside the city. Bj\u00f8rk sensed a unique research opportunity, but it took him years to sift through rows and rows of boxes filled with thousands of light-sensitive negatives and to work out which might be useful. As he extended his search, he came across many more images taken during Danish\u2013US Greenland aerial surveys from the 1940s to the 1970s. Then came pictures from Danish mapping expeditions carried out from 1978 to 1987. Bj\u00f8rk has now amassed a collection of 180,000 aerial photographs (see \u2018Flight lines\u2019). But he has paid particular attention to the first images he acquired, from the 1930s, because they chronicle a warm period that shares some similarities with the present. The mapping expeditions, led by the explorer Knud Rasmussen, were partly motivated by a dispute with Norway about which country had sovereignty over Greenland \u2014 a legal battle that the international court in The Hague settled in 1933 in favour of Denmark. But the crews were aware of the scientific potential of their work. \u201cThe instantaneous record contained in the air and terrestrial photographs of the present state of the glaciers of the entire coast is in itself of great value for future researchers,\u201d the Danish captain Carl Gabel-J\u00f8rgensen wrote in a 1935 report. \u201cThese men were professionals,\u201d says Eric Rignot, a glaciologist at the University of California, Irvine, whose group Bj\u00f8rk will join in the autumn as a postdoc. \u201cWithout GPS or other precision tools they still managed to produce glacier data of amazing quality.\u201d The image collection from the 1930s includes 10,000 overlapping aerial photographs captured with a rotating camera flown in an open hydroplane at 4,000 metres altitude. Bj\u00f8rk and his colleagues have used these images \u2014 alongside more recent aerial images and satellite observations \u2014 to produce an 80-year record of how 132 glaciers in southeast Greenland have waxed and waned 1 . They found that the glaciers retreated strongly during both warm periods but many have lost more ice during the record temperatures in recent decades. Not all glaciers have responded to warming in the same way. It has hit low-elevation glaciers harder than ones higher up, and glaciers that terminate in the sea seem to be more vulnerable to the present rise in temperatures than those that end on land. Researchers are particularly concerned about how quickly these marine glaciers might shrink as they are attacked by the rapidly warming ocean. That may be a key factor in determining how quickly Greenland loses ice. The signs are not good. Hundreds of outlet glaciers that drain the ice sheet are losing substantially more mass and are thinning more than they are gaining through snow accumulation. Jakobshavn, one of Greenland\u2019s largest glaciers, is moving faster than all the others and is the poster child for the fragile ice sheet, which some fear might approach a fatal tipping point as air temperatures continue to rise and the glaciers thin even more 2 . The findings from the historical studies released so far are a reminder that the current glacier retreat has lasted far longer and is more pronounced than the typical fluctuations over the past century. \u201cIt\u2019s that kind of long-term perspective that helps us understand that the rapid changes we are seeing now are definitely more than just noise,\u201d says Csatho, who has looked at satellite and aerial data to reconstruct how the surface elevation of the Greenland ice sheet changed at nearly 100,000 locations from 1993 to 2012 (ref.  3 ). \n               Recovered treasure \n             Bj\u00f8rk\u2019s work at the Arctic Institute in Copenhagen is more sedate than his trips to Greenland, but no less rewarding. The centre\u2019s archives hold treasured documents and memorabilia of Denmark\u2019s Greenland expeditions, which are part of the Nordic kingdom\u2019s cultural DNA. Bj\u00f8rk gleams with the delight of an art lover as he unrolls maps drawn by Danish explorer Hinrich Rink in 1851 of the mighty Jakobshavn. Carefully opening stitched albums, Bj\u00f8rk passes by snapshots of Inuit villagers with sealskin boats, and heads for the pages that most interest him \u2014 the sketches and images depicting the shapes of glaciers and their thickness. \u201cRink and later explorers documented very carefully where they were, and we can trust that what they painted is exactly what they saw,\u201d he says. \u201cTheir zeal and accuracy is a gift for us.\u201d Bj\u00f8rk has done much of his work with fellow geographer Kristian Kjedlsen, with whom he shares an office in the Natural History Museum plastered with Greenland maps and Arctic photography. In a second study, they and their colleagues carefully picked out geographical features on old photographs, such as moraines of rock or vegetation lines that mark the greatest extent of the glaciers during the Little Ice Age at the end of the nineteenth century. They used these data, along with more recent aerial surveys, to determine how the height of the glaciers has shifted, which provides a way to track changes in the ice mass. Their results indicate that the mean annual rate of ice loss between 2003 and 2010 was more than double the average for the twentieth century 4 . Moreover, striking spatial differences seem to confirm that the ice sheet\u2019s response to changing climates is governed by the topography of the underlying bedrock and by the geometry of the fjords through which outlet glaciers are flowing towards the sea. \n               Unruly ice \n             Greenland\u2019s glaciers have been fickle over the past century, advancing in some places and retreating in others. By discovering when those changes happened and what kind of conditions prevailed at the time, Bj\u00f8rk and his co-workers hope to shed light on the ice sheet\u2019s complicated mechanics \u2014 a missing piece in attempts to model its waxing and waning accurately. But nailing the ice sheet\u2019s behaviour requires looking at as many individual glaciers as possible. This is just what Bj\u00f8rk and his group are doing now. Using all available historic records from Greenland, including pictures of glaciers in the least-explored high north, alongside modern satellite imagery, he aims to reconstruct the history of the island\u2019s 309 biggest glaciers in unprecedented detail. The work is in full swing. On most days, two of his students toil away in the basement of the museum in front of computers, clicking on boulders, cliffs and other recognizable features that can be spotted in photographs taken decades apart. These are control points, which allow the researchers to match pictures captured at different altitudes and angles. Most aerial pictures were not taken looking straight down and hence lack a single scale. All of these oblique pictures need to be converted, one by one, to vertical views so that they can be transferred onto a common coordinate system. Without doing this geo-rectification, the researchers could not accurately measure the glaciers\u2019 advances and retreats. Photos are not the only source of information. In the Arctic Institute on a sunny afternoon, Bj\u00f8rk looks at sketches that the German geologist Alfred Wegener produced during his last expedition, shortly before he died in November 1930 near an ice camp in central Greenland. Other albums hold illustrations of glaciers that scientists made during field trips in a cold spell in the late nineteenth century. So far, he has discovered about 600 sketches and paintings that might help to tell the story of Greenland\u2019s glaciers before aerial photographs first became available. For some glaciers, official aerial pictures don\u2019t exist. To fill the gaps, he is also consulting privately held images that geologists have taken over the years. \u201cNew information is coming up from all sides,\u201d he says. In 2014, the daughters of two pilots involved in the 1930s surveys offered Bj\u00f8rk their large collections of photos and even an 8-millimetre film their fathers had taken during the campaigns. The emerging story is that ice disappeared very fast in the early twentieth century in the warming that followed the end of the Little Ice Age. Then the subsequent cool spell brought widespread glacial advances. The profound switch between shrinking and surging suggests that the glaciers are more sensitive to warmings and coolings than researchers had previously thought, says Bj\u00f8rk. But why some glaciers advanced forcefully at given periods and temperatures whereas others did not is still a puzzle. NASA has launched the  Oceans Melting Greenland (OMG) project , led by Rignot, to provide glaciologists and ice-sheet modellers with unprecedented base maps of fjord bathymetry and other information needed to determine how glaciers interact with the sea. That is where one of Bj\u00f8rk\u2019s latest discoveries could prove useful. The historic records of sea-surface temperature that he unearthed can be combined with the individual histories of different glaciers to see how those that end in the ocean responded to changing marine conditions. The past behaviour of the ice, Rignot says, \u201cmatters a great lot when it comes to projecting its fate\u201d. After all, the ultimate goal of this historical research is to look forward. Along with palaeoclimatic data from hundreds and thousands of years ago, the findings of Bj\u00f8rk and his colleagues from the recent past promise to increase confidence in the projections of ice-sheet models, says Richard Alley, a glaciologist at Pennsylvania State University in University Park. \u201cWe need history as well as modern observations to build and test predictive models.\u201d For Bj\u00f8rk, the historical research goes well beyond science. It also connects him with the pioneering scientists and explorers he grew up admiring. He is grateful that their legacy is finally being dug out from the crypt. \u201cIt\u2019s part of Nordic history,\u201d he says, \u201cand a real gift to modern science.\u201d\n \n                     Antarctic model raises prospect of unstoppable ice collapse 2016-Mar-30 \n                   \n                     NASA launches mission to Greenland 2015-Jul-28 \n                   \n                     Climate science: Rising tide 2013-Sep-18 \n                   \n                     Greenland defied ancient warming 2013-Jan-23 \n                   \n                     Climate change: Losing Greenland 2008-Apr-16 \n                   \n                     Natural History Museum of Denmark - Anthropocene-Quaternary Research Group \n                   \n                     Danish Arctic Institute \n                   \n                     NASA Oceans Melting Greenland \n                   Reprints and Permissions"},
{"file_id": "536020a", "url": "https://www.nature.com/articles/536020a", "year": 2016, "authors": [{"name": "Sara Reardon"}], "parsed_as_year": "2006_or_before", "body": "The Cybathlon aims to help disabled people navigate the most difficult course of all: the everyday world. Vance Bergeron was once an amateur cyclist who rode 7,000 kilometres per year \u2014 much of it on steep climbs in the Alps. But in February 2013, as the 50-year-old chemical engineer was biking to work at the \u00c9cole Normale Sup\u00e9rieure in Lyons, France, he was hit by a car. The impact sent him flying through the air and onto his head, breaking his neck. When he woke, he learnt that he would never again move his legs on his own, and would have only limited use of his arms. Confined to bed for months while his body did what healing it could, Bergeron began to look for a way back to cycling. He started to study neuroscience, with an emphasis on research into robotic prostheses that could turn people like him into 'cyborgs': combinations of human and machine. He learnt that some of these prostheses used a technique known as functional electrical stimulation (FES) to deliver electrical signals to atrophied limbs or the stumps of missing ones, causing the muscles to contract and restoring some function. As soon as Bergeron had recovered enough to use a wheelchair, he took that idea back to the lab, where he switched his research focus to neuroscience. Using himself as a guinea pig, he and his team worked out how to stimulate the nerves in his legs so that his muscles would flex and pedal a bike. \u201cI have become my own research project and it's a win\u2013win,\u201d he says. Even with regular exercise sessions to build muscle, Bergeron's artificially stimulated legs have produced at most 20 watts of power, barely one-tenth of the 150\u2013200 watts produced by an average cyclist. But he and his team are building the FES controller and electrodes into a carbon-fibre recumbent tricycle that he hopes will help him to do better \u2014 and perhaps win a medal on 8 October, when he takes his machine to Zurich, Switzerland, to race against other FES cyclists in the Cybathlon: the first cyborg Olympics. \n               Machine learning \n             Around the world, nearly 80 research groups in 25 countries are honing their technologies for the \u20ac5-million (US$5.5-million) event. They range from small, ad hoc teams to the world's largest manufacturers of advanced prostheses, and comprise about 300 scientists, engineers, support staff and competitors: disabled people who will each compete in one of six events that will challenge their ability to tackle the chores of daily life. A race for prosthetic-arm users will be won by the first cyborg to complete tasks including preparing a meal and hanging clothes on a line. A powered-wheelchair race will test how well participants can navigate everyday obstacles such as bumps and stairs. The venue \u2014 Zurich's 7,600-spectator ice-hockey stadium \u2014 should combine with the presence of television cameras and team jerseys to give the Cybathlon a sporting vibe similar to that of the Paralympics, in which disabled athletes compete using wheelchairs, running blades and other assistive technologies. The difference is that the Paralympics celebrates exclusively human performance: athletes must use commercially available devices that run on muscle power alone. But the Cybathlon honours technology and innovation. Its champions will use powered prostheses, often straight out of the lab, and are called pilots rather than athletes. The hope is that devices trialled in the games will accelerate technology development and eventually be used by people around the world. The everyday tasks being tested in the Cybathlon are much more difficult than they seem, says Robert Riener, a biomedical engineer at the Swiss Federal Institute of Technology in Zurich and creator of the Cybathlon. \u201cI think that people are spoiled by the Internet and Hollywood movies,\u201d he says. \u201cWe want to show people there are still challenges.\u201d Riener traces the origins of the Cybathlon back to news accounts of a charity event: in November 2012, a man called Zac Vawter, who had lost a leg in a motorcycle accident, used an experimental, motorized prosthetic leg to climb the 103 storeys of Willis Tower in Chicago, Illinois, in just 45 minutes. The feat impressed the media \u2014 and Riener. But it also frustrated him: Vawter's device, along with similarly impressive prostheses from Riener's lab and others around the world, were not reaching people who needed them. \u201cWe're doing great work but not selling it well,\u201d he thought. So why not take inspiration from the Willis Tower stunt, and draw attention to the technology through a competition open to everyone in the prosthetics-research community? Riener's 30-person lab team was excited about designing such an event. And before long, word had spread to colleagues around the world. At first, Riener had considered hosting showy events such as climbing a mountain on prosthetic limbs. But he changed his mind in 2013, after talking to an acquaintance who had lost an arm to cancer and wore a prosthesis. The device ended in a hook that was moved by cables when the man flexed certain muscles in his stump. It worked well enough for large movements, but was hopeless for fine control. Once, the man told Riener, he had been buying cinema tickets, and could feel the people queuing behind him staring and growing impatient as he struggled to draw out his wallet and grasp the pieces of paper. These mundane challenges, Riener realized,  were greater and more meaningful  than the need to design, say, a spring-like leg that simply helps someone to run fast. So he decided that most of the Cybathlon competitions would be distinctly non-Olympian. \n               Brain power \n             Easily the strangest will be the  brain\u2013computer interface (BCI)  race, which will feature 15 pilots sitting still for 4 minutes while large screens in the arena show what is going on in their heads. Each will attempt to guide an on-screen character through an obstacle course using specific patterns of brain activity, translated by an electrode cap into three commands: accelerate, jump over spikes or roll under laser rays. In principle, the patterns can be anything. At the University of Essex in Colchester, UK, for instance, a team of current and former students led by postdoc Ana Matran-Fernandez has designed an algorithm that associates the three motions with a pilot thinking of his or her hand or foot, or working through a maths equation. The electrical signals are weak, and each individual is different, so it can be difficult to distinguish between the commands \u2014 especially when a pilot is distracted, for example by cheering and adrenaline in the competition. Constantly thinking about tasks is mentally exhausting, says neuroscientist Jos\u00e9 del R. Mill\u00e1n of the Swiss Federal Institute of Technology in Lausanne, whose team is working on ways to predict thought patterns to make the association more natural and let the pilot relax. BCIs will probably never be used for real jumping and running, because detecting electrical activity in muscles is much easier. But if such devices could be made cheap and accurate enough, they could help disabled people to guide wheelchairs, cursors or even Skype-enabled robots that would let them participate virtually in an event. \u201cThe fact that you can develop this in the lab and bring it out and see it works means there's a future,\u201d says Matran-Fernandez. Other Cybathlon events will highlight the great strides being made with more-conventional devices. In the  prosthetic-leg race , competitors must get past obstacles such as stairs, randomly placed stones, tilted pavements and doors \u2014 not to mention sitting down in a chair and standing up again. Several participants will be using state-of-the-art smart knees and ankles that can detect force and acceleration as they walk, and correct their motion if they start to fall. But even the most advanced engineering pales beside what the intact body does naturally. When a person picks up a pen with a flesh and blood arm, their brain and peripheral nervous system coordinate how far to reach, how to bend each joint in each finger into a precise shape, and how hard to grasp \u2014 all without conscious effort. Standard movable prostheses, such as the type with the hook and cables, require the user to do all of this consciously. This takes a great effort, which is one reason many amputees choose not to wear them. To get around that, researchers have to create computer algorithms that  decode signals from muscles and nerves  and predict what a wearer is trying to do. In Burnaby, Canada, a Cybathlon team called MASS Impact is working with pilot Danny Letain, a former Canadian Paralympic skier who lost his left arm in a 1980 railway accident. The team has built an arm with a panel of flat buttons that sits on Letain's arm stump. Using his memory of a hand, Letain imagines making one of 11 gestures, such as pointing. The muscles in his stump then compress the buttons and tell his artificial hand to do what he intends. Letain was pleased to find that the brain circuitry that once controlled his fingers is still in working order, long after he stopped feeling any 'phantom pain' in his lost arm. \u201cI'm using something I haven't used in 35 years,\u201d he says. Some arms are even more advanced. A team led by biomedical engineer Max Ortiz Catalan at Chalmers University of Technology in Gothenburg, Sweden, has developed a  two-way prosthetic hand that can feel as well as move  (  M. Ortiz-Catalan  et al .  Sci. Transl. Med.   6 , 257re6; 2014). The arm is permanently implanted in the wearer's bone, and uses up to nine electrodes to convey motor commands from the remaining muscles to the prosthesis, and to send signals from sensors in the fingers back to the arm's sensory nerves. Cybathlon pilot Magnus Niska is the only person in the world who wears such a prosthesis outside the lab. Ortiz Catalan hopes that the ability to feel objects will give Niska a competitive advantage. A team led by Ronald Triolo, a biomedical engineer at Case Western Reserve University in Cleveland, Ohio, has a similar strategy for the FES cycling event, in which contestants with spinal-cord injuries will pedal for 750 metres around a circular track. Many of the competitors, including Bergeron, use electrodes on the skin to stimulate the leg muscles. But the Cleveland system \u2014 originally designed to allow people with lower-limb paralysis to walk with the help of crutches \u2014 features electrodes surgically implanted in the leg muscles. Using an external device, the wearer chooses a menu option, such as 'sit'. An implanted pulse generator activates the electrodes that cause the muscles to contract in the correct order. After Triolo heard about the Cybathlon, he realized that he could add cycling to his volunteers' exercise regimes. His team has equipped a recumbent tricycle with sensors that detect the angle of the cyclist's leg as he or she pedals, and automatically change the stimulation patterns so that one leg pushes while the other pulls. Triolo says that all 27 of the people implanted with his electrodes want to try cycling. After putting them through qualifying trials, he is down to a few finalists for the Cybathlon. \u201cWe want to go to Switzerland and win this thing,\u201d he says. \u201cThen I'd like to use that as a springboard to build an exercise programme here.\u201d \n               Eyes on the prize \n             This competitiveness is a far cry from Triolo's initial reaction to the Cybathlon, which was that the competition was a foolish idea. \u201cWe should find a way to collaborate internationally on these problems rather than compete,\u201d he recalls saying \u2014 something that he and Riener say they still hear from some in the prosthetics field. Triolo eventually came around: he decided that the Cybathlon would at least be a good learning experience. Riener himself hopes that bringing the competition into the open will spur creativity better than the conventional academic process, which is hampered by researchers' concerns about their intellectual property and competitiveness for grants. Karim Lakhani, an economist who studies innovation at the Harvard Business School in Boston, Massachusetts, notes that competitions also force researchers to finish their work quickly and eliminate doubts about feasibility that prevent them from starting in the first place. He points to the self-driving car, which languished in development for decades until 2005, when the US Defense Advanced Research Projects Agency held a race with a $2-million prize. The contest eventually drew interest from Google, which is now testing such cars on the roads. \u201cThis contest will serve the same way,\u201d says Lakhani. The Cybathlon will not award monetary prizes, just medals. But his research suggests that the recognition enjoyed by the winners could be just as motivating (  K. J. Boudreau  et al .  RAND J. Econ.   47 , 140\u2013165; 2016). Perhaps the greatest advantage of prizes is that they give unknown contestants an opportunity to compete alongside big, well-known players, says Lakhani. The Cybathlon has drawn plenty of both. Otto Bock HealthCare, a multibillion-euro company based in Duderstadt, Germany, and the world's largest manufacturer of prosthetic limbs, has entered three Cybathlon events. One is the powered-exoskeleton race, in which  contestants with spinal injuries will use an external support system  to navigate obstacles similar to those in the powered-leg race. Otto Bock's pilot, Lucia Kurs, lost the use of her legs to spinal tumours. Now in her 60s, she can walk 12 kilometres using the firm's commercial leg brace, which has sensors, electronics and motors to guide the knees and ankles through a normal leg swing. \u201cWe're showing off, and checking out other manufacturers\u201d at the Cybathlon, says Christof K\u00fcspert, a product manager at Otto Bock. But he says the company is also interested in learning about innovative prototypes from dark-horse developers at universities. One smaller developer is Jes\u00fas Tamez-Duque, managing director of the start-up INDI Engineering and Design in Monterrey, Mexico, who is entering a prototype for an exoskeleton much cheaper than Otto Bock's US$75,000 model. The device's joints are moved by windscreen-wiper motors, and much of the body is 3D printed. INDI's competitor uses a joystick attached to his crutches to choose from several programmed movements, such as climbing stairs or sitting. Tamez-Duque hopes that the Cybathlon will attract collaborators and prove that Mexico can be a player in the field. \u201cThe way we see it, the Cybathlon is a competition that concentrates the top-notch robotics labs in the world,\u201d he says. \u201cWe're still working on getting that representation so that other people believe we can actually add value to them.\u201d The Cybathlon will be back in 2020, as a seven-day event in Tokyo, coinciding with the Olympics. It will have new events for competitors with visual impairments, and will conduct some races outside the stadium. But for competitors in the first Cybathlon, being at the cutting edge is already a thrill. \u201cThis is  Iron Man , this is  Avatar ,\u201d says Bergeron. \u201cIt's a combination of BCI and exoskeletons all over the place.\u201d \n               \n                 Tweet \n                 Facebook \n                 LinkedIn \n                 Weibo \n                 Wechat \n               See Editorial  page 5 \n                     Scholarly Olympics: How the games have shaped research 2016-Aug-04 \n                   \n                     Step aside, Olympics: here\u2019s the Cybathlon 2016-Aug-03 \n                   \n                     The Pentagon\u2019s gamble on brain implants, bionic limbs and combat exoskeletons 2015-Jun-10 \n                   \n                     Artificial arms get closer to the real thing 2014-Oct-08 \n                   \n                     Toy helicopter guided by power of thought 2013-Jun-05 \n                   \n                     Neuroprosthetics: Once more, with feeling 2013-May-08 \n                   \n                     The Cybathlon \n                   \n                     Vance Bergeron \n                   \n                     Ronald Triolo \n                   \n                     INDI \n                   \n                     Otto Brock C-Brace \n                   Reprints and Permissions"},
{"file_id": "535026a", "url": "https://www.nature.com/articles/535026a", "year": 2016, "authors": [{"name": "Julie Gould"}], "parsed_as_year": "2006_or_before", "body": "Doctoral courses are slowly being modernized. Now the thesis and viva need to catch up. On the morning of Tom Marshall's PhD defence, he put on the suit he had bought for the occasion and climbed onto the stage in front of a 50-strong audience, including his parents and 6 examiners. He gave a 15-minute-long presentation, then faced an hour of cross-examination about his past 5 years of neuroscience research at the Donders Institute for Brain, Cognition and Behaviour in Nijmegen, the Netherlands. A lot was at stake: this oral examination would determine whether he passed or failed. \u201cAt the one-hour mark someone came in, banged a stick on the floor and said ' hora est ',\u201d says Marshall \u2014 the ceremonial call that his time was up. \u201cBut I couldn't. I had enjoyed the whole experience far too much, and ended up talking for a few extra minutes.\u201d Marshall's elaborate, public PhD assessment is very different from that faced by Kelsie Long, an Earth-sciences PhD candidate at the Australian National University (ANU) in Canberra. Her PhD will be assessed solely on her written thesis, which will be mailed off to examiners and returned with comments. She will do a public presentation of her work later this year, but it won't affect her final result. \u201cIt almost feels like a rite of passage,\u201d she says. PhDs are assessed in very different ways around the world. Almost all involve a written thesis, but those come in many forms. In the United Kingdom, they are usually monographs, long explanations of a student's work; in Scandinavia, science students typically top-and-tail a series of their publications. The accompanying oral examination \u2014 also called a viva voce or defence \u2014 can be a public lecture, a private discussion or not happen at all. There is wide variation across disciplines and from one institution to the next. \u201cIt is a complicated world in doctoral education. One format does not fit all,\u201d says Maresi Nerad, founding director of the Center for Innovation and Research in Graduate Education at the University of Washington in Seattle. This isn't necessarily a problem in itself, but some researchers worry that the decades-old doctoral assessment system is showing strain. Time-pressured examiners sometimes lack training and preparation for PhD assessments, which can lead to lack of rigour. \u201cTwo or three examiners come together to go through the thesis in a perfunctory way. They tick the boxes, everyone is happy, and then a PhD walks away,\u201d says Jeremy Farrar, director of the biomedical research charity the Wellcome Trust in London. Farrar, like other scientists, suspects that the  PhD assessment is not keeping up with the times . Single-author tomes seem outdated when much of research has become a multidisciplinary, team endeavour. Research is becoming more open, but PhD assessments can lack transparency: vivas are sometimes held behind closed doors. Some PhD theses languish, little-used, on office shelves or in archives. \u201cWe're seeing some students who are still submitting paper theses to us \u2014 they don't have electronic theses yet,\u201d says Austin McLean, director of scholarly communication and dissertation publishing at ProQuest in Ann Arbor, Michigan, which has the largest database of PhD theses in the world. What's more, little attention is given in the PhD assessment to soft skills such as management, entrepreneurship and teamwork, even though these are an essential part of life beyond the PhD, and  students are increasingly leading that life outside academia . \u201cThe assessment of the PhD hasn't been updated to fit the modern definition of a PhD,\u201d Farrar says. \u201cThere are a lot of pressures to make changes to the thesis,\u201d says Suzanne Ortega, president of the Council of Graduate Schools in Washington DC, one of a number of groups discussing the issue. The council organized a workshop in January this year called Future of the PhD Dissertation, and in March, the Australian Council of Learned Academies (ACOLA) in Melbourne examined changes to the thesis as part of a review on researcher training. Some scientists and education experts welcome the attention. \u201cI don't think the current model for thesis examination is ideal, but there are positive movements towards changing it,\u201d says Inger Mewburn, director of research training at ANU and editor of the blog The Thesis Whisperer, which is dedicated to those completing a thesis. \n               Passing the test \n             Academics agree about one thing regarding the PhD assessment \u2014 its aim. The traditional goal is to demonstrate the candidate's ability to conduct independent research on a novel concept and to communicate the results in an accessible way. Where the academics differ is on how best to achieve that goal. Shirley Tilghman, a molecular biologist and former president of Princeton University in New Jersey, sees merit in the monograph form of the thesis. It demonstrates scholarly ability by requiring students to \u201cframe the historical context of a problem, describe in detail the purpose and execution and then come to a credible conclusion\u201d, she says. But should the thesis include academic publications, too? That's the norm at the Karolinska Institute in Stockholm, Sweden, where most theses are a compilation of the student's original papers, along with a relatively short discussion, perhaps 50 pages long. The rationale is that publishing should be part of training because it better equips students for academic life and securing jobs. Some students who complete a monograph end up wishing that they had spent more time on writing papers. James Lewis successfully defended his physics PhD at Imperial College London in October 2015, but he thinks that his one published paper landed him his postdoc at NASA's Goddard Space Flight Center in Greenbelt, Maryland. \u201cThe job market for postdoc positions is very competitive,\u201d he says, \u201cso if you can get a paper published during your PhD then you're helping yourself.\u201d While he waits to start, Lewis is spending his days  writing papers based on his research . \u201cI'm wondering: would it not have been better to write these instead of the thesis, which took me five months to write?\u201d But others argue that the pressure to publish could rob PhD students of valuable parts of their studies, such as the time to  shape their research path and to think creatively and independently . \u201cThe PhD might become driven by papers only,\u201d says Farrar. \u201cStudents might end up spending their time focusing only on what papers they can produce, then staple them together with a summary and they're done \u2014 adding to the sense that the whole scientific enterprise is a paper factory rather than an exploration.\u201d Long is working at ANU towards a thesis-by-publication: she's written and submitted one paper and has started on a second. But she's struggling. \u201cI am finding this one much harder to write, mostly because it isn't as new or exciting as the previous one,\u201d she says. What's more, her strategy depends on things at least partly outside her control \u2014 on her PhD generating enough complete studies for publication and on a reasonably timely peer-review process. Completed PhD theses are typically stored in university libraries \u2014 but that doesn't mean that they are read or used. Some 60% of submissions to the ProQuest database fall under the category of science, technology or mathematics, but they are the ones that are accessed least. \u201cWe think this is because the communication is more journal-focused,\u201d says McLean. Scientists do tend to keep a copy of their theses in their office or lab for use by students and colleagues. Neil Curson, a physicist at the London Centre for Nanotechnology, says that his PhD, written more than 20 years ago, is still consulted by his students when they come into his lab. Many theses, however, end up collecting dust. \n               Viva la viva \n             Whatever form the thesis takes, it has to be assessed \u2014 in most countries, by a panel of experts, and often involving an oral exam. But the viva \u201cdoesn't have the same level of consistency as the written form of examination\u201d, says Allyson Holbrook, an education researcher at Australia's University of Newcastle. In Israel, the viva is optional and very few students choose to have one; in the Netherlands, it is formal and ceremonial; in the United Kingdom, it's typically a private affair with two or three examiners; and in Australia, it's hardly performed at all. \u201cOne hundred per cent of the doctoral examination is about the thesis here,\u201d says Holbrook. That's largely because, historically, there weren't enough experts in the country to examine the work in person and it was costly to fly them in, she says. Holbrook and her research team published a study last year that compared the assessment methods used in Australia with those in New Zealand and the United Kingdom (T. Lovat  et al. Higher Ed. Rev.   47,  5\u201323; 2015). They concluded that doing an oral defence rarely changed the result, and that the thesis itself was the \u201cdeterminative step\u201d of passing. The review on Australian research training published in March didn't support adding a viva either, but it did recommend a move towards more continuous assessment of a student, rather than waiting until the end of the training. Some researchers see problems with the viva. It's not uncommon for nerves to get the better of a student, and for them to freeze in front of their audience, however small it is. Examiners could worsen the situation by asking very difficult questions, says David Bogle, a chemical engineer at University College London. \u201cThere are cases where undue pressure is placed on the candidate by the examiners. This shouldn't be allowed.\u201d \n               Trial by error \n             Most researchers don't support a global standard for the PhD assessment. A one-size-fits-all approach would be impossible to implement, they say, and the type of assessment \u2014 be it continuous appraisal, written thesis or oral exam \u2014 should depend on discipline, project, student, supervisor and institution. \u201cIf you take away the variability in assessment and form of the thesis then you lose all creativity and innovation from the PhD,\u201d Nerad says. But many feel that the system could be improved \u2014 by making the thesis shorter, for one. Data from ProQuest, which stores 4 million theses, show that the average length of biology, chemistry and physics PhDs soared to nearly 200 pages between 1945 and 1990. That could be because students are analysing more complex questions, performing longer literature reviews and using increasingly complicated methods that require lengthier explanations (see 'The expanding thesis'). \u201cIt's unnecessary to have such a long thesis,\u201d says Farrar, who recently assessed one such tome. \u201c'The thicker my PhD, the better' has become a myth in the PhD community, and is taking it down the wrong direction.\u201d  'The thicker my PhD, the better' has become a myth in the PhD community.  Farrar says that a slimmed-down document would be more appropriate. That could follow the concise format of a research paper, and include a review of the field, then short chapters on methods, analysis and discussion. \u201cIt would be more succinct and focused. And the examiners will probably read it all.\u201d That isn't necessarily the case now. Examiners have to find time to review theses in between research, teaching, grant-writing and many other demands. \u201cSomething has to give, and what gives is the amount of time spent on any of those individual tasks,\u201d says Farrar. That means an examiner might skim through years of a PhD student's work in just a couple of hours. \u201cI think we owe it to the students to examine them properly and help prepare them for their future careers,\u201d he says. \n               The modern thesis \n             One way to better reflect the team-based nature of science would be to write a joint thesis, an approach that has been used in arts and humanities graduate education in the past. However, this can make it difficult to assign credit. \u201cIf you have worked on a collaborative dissertation, a potential employer might struggle to see whether you really are an independent thinker or could you read a lead a research project,\u201d says Ortega. There is another matter to wrestle with \u2014 the fact that half of science PhD graduates in the United States are  choosing careers outside of academia , according to the National Science Foundation's 2014 Survey of Earned Doctorates. \u201cUnder those conditions, the standard assessment should include the skills in what they'll need when going on to future careers,\u201d says Michael Teitelbaum, labour economist at Harvard Law School. Increasingly, institutions offer courses to PhD students in skills such as teamwork, management and research ethics, but these skills aren't usually assessed formally. The viva would be one opportunity to do so, perhaps by seeing how students react to various scenarios. Alternatively, as the ACOLA review suggested, PhD candidates could accrue credits in transferable skills through professional-development activities that are recorded in a portfolio. \u201cYou can't just assume that if you throw them into an environment they will meaningfully learn from that environment,\u201d says psychologist Michael Mumford, a director of the Center for Applied Social Research at the University of Oklahoma in Norman. \u201cWe need exams that ask students to deal with both real-world problems as well as ambiguous academic problems.\u201d Farrar thinks that a change in emphasis could help. Rather than thinking of the thesis and viva as an exam, it should be viewed as the culmination of a long project. \u201cYou need to look at the PhD in the context of those four years of research, not just as revision for one big test.\u201d Mewburn stresses that whatever form the assessment takes, it should focus more on the individual than on their work. \u201cMy preference is to assess the researcher,\u201d she says, \u201cbut we haven't developed the tools and curriculum to do that.\u201d \n               Few failures \n             It's difficult to find figures on how many students fail their PhD if they get to the point of submitting a thesis but, anecdotally, scientists say that few flunk it outright. More often, students are sent away with minor or major corrections that have to be completed before the PhD is awarded. There are theories that few students fail because universities want to keep their number of graduates high for the rankings. But most researchers dispute this, and point to other reasons. One is that weak students are likely to have dropped out before they reach the final assessment. Furthermore, supervisors and the supporting institutions typically work hard \u2014 through regular reviews and assessments \u2014 to make sure that a candidate and project are of a sufficient standard before the thesis is submitted. \u201cYou haven't done your due diligence as a university if a student is getting to a stage where they are sending out theses that are going to fail,\u201d says Simon Hay, a global-health researcher at the University of Washington. Nerad sees no need to reform the final PhD assessment. For her, the problem lies with the variability of graduate education as a whole. \u201cNow that research is becoming more globalized, the PhD needs to be too.\u201d That process is under way, Nerad says: the pressures of economic globalization, international policies and national drives to house world-class universities have led to a more standardized PhD experience across the world. During her tenure as Princeton's president, Tilghman was often asked if there was a perfect way to assess a PhD course. Not many liked her answer \u2014 that she could only really evaluate a student at the 25-year reunion. \u201cIn the end, the only way you can assess it is whether the graduates of the programme become successful scientists. If they do, you've done a good job. If they haven't, you haven't.\u201d See Editorial  page 7 \n                 Tweet \n                 Facebook \n                 LinkedIn \n                 Weibo \n                 Wechat \n               \n                     Back to the thesis 2016-Jul-06 \n                   \n                     The past, present and future of the PhD thesis 2016-Jul-06 \n                   \n                     The future of the postdoc 2015-Apr-07 \n                   \n                     How not to deal with the PhD glut 2014-Oct-22 \n                   \n                     Education: The PhD factory 2011-Apr-20 \n                   \n                     Seven ages of the PhD 2011-Apr-20 \n                   \n                     Nature  special: Future of the PhD \n                   \n                     \n                         Naturejobs  \n                       \n                   \n                     The Thesis Whisperer \n                   \n                     ProQuest Dissertations and Theses \n                   \n                     Centre for Innovation & Research in Graduate Education \n                   \n                     Council of Graduate Schools \n                   \n                     Australian Council of Learned Academies: Review of Australia's Research Training System \n                   Reprints and Permissions"},
{"file_id": "537156a", "url": "https://www.nature.com/articles/537156a", "year": 2016, "authors": [{"name": "XiaoZhi Lim"}], "parsed_as_year": "2006_or_before", "body": "Advances in catalyst research could create a superhighway to clean energy sources and a more-sustainable chemical industry. In her 1794 book,  An Essay on Combustion , Scottish chemist Elizabeth Fulhame noted a peculiar fact: substances such as coal and charcoal burned better when they were damp. After many experiments to understand why, she concluded that the water briefly split into hydrogen and oxygen, which interacted with the other compounds in a way that made the combustion go faster. Yet at the end, Fulhame wrote, the process \u201cforms a new quantity of water equal to that decomposed\u201d. Many historians consider this to be the first scientific account of a catalyst: a material that speeds reactions by making or breaking chemical bonds, without being consumed. It was hardly the last: modern chemistry would be almost inconceivable without catalysts. \u201cThey not only make transformations accessible, but also direct them in new ways,\u201d says Susannah Scott, a chemist at the University of California, Santa Barbara. \u201cThat's very powerful.\u201d Catalysts are used in some 90% of processes in the chemical industry, and are essential for the production of fuels, plastics, drugs and fertilizers. At least 15 Nobel prizes have been awarded for work on catalysis. And thousands of chemists around the world are continually improving the catalysts they have and striving to invent new ones. That work is partly driven by an interest in sustainability. The aim of catalysis is to direct reactions along precisely defined pathways so that chemists can skip reaction steps, reduce waste, minimize energy use and do more with less. And with growing concerns about climate change and the environment, sustainability has become increasingly important. Catalysis is a key principle of ' green chemistry ': an industry-wide effort to prevent pollution before it happens. Catalysts are also seen as the key to unlocking energy sources that are much more inert and difficult to use than coal, oil or gas, but much cleaner. Catalysis can make it more economically feasible to split water into oxygen and hydrogen fuel, or can open up new ways to use raw materials such as biomass  or carbon dioxide . \u201cThese are feedstocks that are ripe for advances in catalysis,\u201d says Melanie Sanford, a chemist at the University of Michigan in Ann Arbor. These challenges have led to an explosion in catalyst innovation, with the annual number of publications on the subject tripling in the past decade. Many groups are coming up with new small-molecule complexes or are chemically tailoring biological enzymes in search of radically new catalytic activity. Others are pursuing advances in nanotechnology, which allow them to  engineer the action of solid catalysts at the atomic scale . Still others are experimenting with catalysts that are activated by light, or that incorporate the DNA double helix. And everyone in the field is trying to streamline the search for better catalysts with modern computational modelling tools. The pace of innovation is such that even the experts are struggling to keep up, says Scott, who leads the US Department of Energy's efforts to develop benchmarks for the new catalysts' performance 1 . \u201cWe need to make sure we are advancing the science that's most efficient,\u201d she says. And the scope of catalysis is increasing rapidly. \u201cTwenty years ago,\u201d says John Hartwig, a chemist at the University of California, Berkeley, \u201ccatalysis to make molecules that were complex did not exist.\u201d Anyone who wanted to modify a large complicated structure would have to tear it down and build it back up, says Sanford. But now, chemists can often edit parts of a molecule precisely. \u201cIt's incredibly enabling,\u201d she says. \n               Cut-price catalysts \n             Using a catalyst is like bulldozing a shortcut between reactants A and product B, bypassing convoluted chemical pathways that might otherwise take forever. Using a really good catalyst is like building a multilane superhighway. And some of the best are the 'homogeneous' catalysts: free-floating molecules that are mixed in with the reactants. Industrial catalysts in this category most often consist of a metal ion that does the hard work of making or breaking chemical bonds, surrounded by 'ligands': connected groups, often carbon-based, that control the reactants' access to the ion. Much of the research in this field comes down to tailoring these ligands to produce a catalyst that performs only a desired reaction. Unfortunately, many of the successes so far have come through the use of scarce and expensive metals such as palladium, platinum, ruthenium and iridium. Today, chemists are increasingly striving to build catalysts around cheaper, 'Earth-abundant' elements such as iron, nickel or copper \u2014 or to do without metals altogether. Nickel is a particularly attractive candidate for mimicking the chemistry of palladium and platinum because it sits directly above them in the periodic table, and therefore has similar properties. At the Swiss Federal Institute of Technology in Lausanne, for example, synthetic chemist Xile Hu and his group are working with a remarkably versatile nickel complex 2  that they first reported in 2008. The complex consists of a nickel ion surrounded by a single, large ligand that binds to it in three places, leaving a fourth binding spot available for catalysing reactions. A similar ligand is already used in certain palladium catalysts. But the radius of a nickel ion is almost 20% smaller than that of a palladium ion, so Hu had to shrink the ligand to fit it more closely around the nickel. To do so, he replaced phosphorus atoms in the ligand with smaller nitrogen ones. The result is a rigid ligand that stabilizes the nickel ion as it performs a wide array of reactions 3 , 4 , 5 . The original nickel catalyst is already available commercially, and Hu is systematically modifying the ligand to make a whole family of catalysts. In 2008, chemists discovered that certain standard catalysts could be made more powerful by combining them with a technique known as photoredox catalysis. When photoredox catalysts absorb light, an electron leaps from the metal ion to the ligand and becomes stuck there, leaving the molecule in an unstable state. \u201cThe catalyst becomes desperate to fill the hole in the metal and get rid of the electron in the ligand,\u201d explains David MacMillan, a chemist at Princeton University in New Jersey who first reported 6  the idea in collaboration with chemist David Nicewicz from the University of North Carolina at Chapel Hill. But the only way the photoredox system can accomplish this is to trade electrons with the standard catalyst, supercharging it and triggering chemical transformations that were previously impossible. As a bonus, the photoredox catalysis drives the process with energy that it absorbed from light, reducing the heat required to keep the reaction going. Nicewicz and MacMillan have independently used photoredox catalysis to make major improvements to the Buchwald\u2013Hartwig reaction, which is frequently used to bond carbon with nitrogen when making drugs. Typically, the reaction requires the use of palladium salts, expensive, phosphorus-based ligands and difficult-to-make reactants. But in 2015, Nicewicz's group announced 7  that it had not only made a carbon\u2013nitrogen bond using a completely metal-free catalyst, but had done so starting from cheaper and more accessible reactants; it is already being used by pharmaceutical companies, says Nicewicz. In June, MacMillan's group and its collaborators at Merck Research Laboratories in Rahway, New Jersey, reported 8  making the Buchwald\u2013Hartwig reaction work with minute amounts of an iridium light absorber and a nickel salt, eliminating the need for ligands. A specific challenge for many researchers is to find better ways of creating the carbon\u2013fluorine bonds at the heart of fluorinated compounds that are widely used in pharmaceuticals, agrochemicals and medical imaging. Currently, the bonds are made using expensive specialized reagents or the highly corrosive gas hydrogen fluoride. In 2013, a team of researchers led by Sanford showed 9  how to make such bonds with a safer potassium fluoride salt using a copper catalyst. First, the catalyst is exposed to a compound that strips away three of its electrons. This leaves the catalyst so hungry for electrons that it can pull some from a nearby fluoride ion, which holds them in a notoriously tight grip. The fluoride is then so desperate for a replacement electron that it will readily bind with a carbon atom to get it. \n               Pebbles in a stream \n             Despite their versatility, many homogeneous catalysts are fragile. Their internal bonds weaken after prolonged exposure to heat and collisions with reactant molecules, and their ligands start disintegrating. \u201cThey die after a while,\u201d says Sanford. That is a big reason why large-scale industry tends to use 'heterogeneous' catalysts: solid materials that are fixed in place while the reactants stream past. A classic example is the mix of powdered  platinum and other metals found in the catalytic converters  that clean vehicle exhaust gases. In the past, chemists had a tough time designing heterogeneous catalysts with atomic precision because it was difficult to make and study the active sites, where catalysis occurs, in a solid material. Mostly they had to optimize the catalysts through trial and error. But what's changing, says Scott, \u201cis the synthetic control that we can exert over the materials\u201d. In particular, rapid advances in nanotechnology are allowing chemists to work towards systems with the robustness of solid catalysts and the high performance of homogeneous ones. At the Chinese Academy of Sciences' State Key Laboratory of Catalysis in Dalian, director Can Li has used platinum and cobalt oxide nanoparticles to create a  catalyst for splitting water  with sunlight 10  (see 'Light splitter'). He starts by sticking the nanoparticles to crystals of a semiconductor called bismuth vanadium oxide, with each type of particle carefully isolated on a specific face of each crystal. Then, when he immerses the crystals in water and exposes them to light, photons strike the semiconductor and loosen electrons. The result is a flow of current that the nanoparticles use to break water molecules into hydrogen and oxygen. Oxygen gas comes bubbling off the cobalt oxide sites, while positively charged hydrogen ions migrate to the platinum particles. \u201cWe separated the active sites to block the reverse reaction,\u201d says Li \u2014 that is, a dangerously explosive conversion of hydrogen and oxygen back into water. (To simplify the experimental set-up, the hydrogen ions are currently captured by a separate compound rather than turned into gas.) The process is not yet efficient enough to be economically viable, says Li. But his team is testing combinations of semiconductors and metal catalysts to refine the design. Audrey Moores, a chemist at McGill University in Montreal, Canada, is tackling a bothersome issue in the pharmaceutical, cosmetics and food industries, which often use heavy-metal-ion catalysts. Ions of palladium, ruthenium and platinum are toxic, so products made with them cannot be sold until they have been through a series of meticulous and expensive cleansing steps. Moores is working on alternative catalysts based on iron, which is much safer. In 2014, her research group prepared a series of hollow, magnetic iron oxide nanoparticles for making benzaldehyde 11 : a molecule that smells like almonds and is widely used in flavourings. It is typically manufactured by reacting certain electron-hungry compounds with styrene: a sweet-smelling but hazardous liquid that is better known as a raw material for plastics. The process tends to generate a relatively small amount of benzaldehyde mixed with other molecules. But Moores' iron nanoparticles catalyse a more controllable reaction between styrene and oxygen, yielding almost pure benzaldehyde. And as an added advantage, iron is magnetic, so at the end of the reaction the iron nanoparticles can be extracted for reuse with a magnet. \n               Even-handedness \n             When making large, complex molecules such as steroids, antibiotics or hormones, a major challenge involves  chirality , or the 'handedness' of a carbon atom. Such an atom carrying four different groups can have two configurations that are mirror images of each other, like human hands. A complex molecule may contain many such carbon atoms \u2014 and if even one of them has the wrong configuration, the compound can end up interacting badly with the human body. One notorious example is thalidomide, a drug developed in the 1950s for treating morning sickness in pregnant women. One chiral configuration seems to have been effective and safe for that purpose. But many chemists believe that its mirror image, which was present in the over-the-counter drug, is what caused babies to be born with severe limb deformations. Molecules from biomass feedstocks contain a wide variety of chiral carbon atoms in a chain, and it is almost impossible to distinguish one from another. \u201cA small-molecule catalyst wouldn't recognize it,\u201d says Hartwig. Instead, chemists are turning to biological enzymes, which can be large enough to recognize the overall shape of the target molecule and home in on the bond where the reaction should occur. Enzymes also have the advantage of using water as a solvent and working at body temperatures, which makes them more environmentally friendly than processes that require toxic solvents and large amounts of heat. Naturally occurring enzymes don't always catalyse the reactions that chemists want, however \u2014 which is why one frontier of catalysis research is to rework these proteins so that they do. Hartwig has been looking at the haem enzyme, which is similar to the compounds that carry oxygen in red blood cells, and has developed 12  an artificial enzyme that substitutes an iridium complex for the haem's iron centre. Although this runs contrary to the goal of replacing precious metals with Earth-abundant ones, says Hartwig, iridium can work with strong bonds such as those between carbon and hydrogen, which iron cannot. His team is using crystallographic data to study the enzyme's structures near the iridium site and is systematically modifying them so that they can precisely transform a carbon\u2013hydrogen bond into a carbon\u2013carbon bond with the desired chiral configuration \u2014 a formidable challenge. The chemists can prepare hundreds or even thousands of new enzymes in this way, limited only by the time it takes to test them and analyse their activity. Still, enzymes are very specific to their target, and although they yield a product with a single chiral configuration, it is often the configuration that isn't wanted. \u201cIf you're interested in the other, you're in trouble,\u201d says Stellios Arseniyadis, a synthetic chemist at Queen Mary University of London. To address that problem, Arseniyadis is collaborating with Michael Smietana of the University of Montpellier in France to make catalysts from DNA. Although most natural DNA spirals in only one direction, it is possible to make an artificial version that twists in the opposite direction. The two researchers and their teams make their catalysts by choosing a natural or non-natural helix of DNA and then attaching a metal ion inside it. The spiral grooves align the reactants so that they fuse with the desired chiral configuration. In 2015, Arseniyadis and Smietana reported a recyclable DNA\u2013copper catalyst 13  that created the correct chiral products as reactants flowed past. With endless combinations of base pairs and metal ions, \u201cthere's a plethora of parameters that you can fine-tune\u201d, says Arseniyadis. Chemists are continuing to push the boundaries of catalysis research. Li, for example, is experimenting with housing enzymes inside nanoparticles 14  to help them last longer. Others are synthesizing completely artificial enzymes 15  using techniques from synthetic biology. And earlier this year, an international team of researchers reported 16  using an electric field to catalyse the formation of ring-shaped carbon compounds. These ideas are starting to constitute entire new research fields in which conventionally distinct disciplines overlap \u2014 for example, combining chemical synthesis and DNA. That, says Arseniyadis, leaves \u201ca lot of room for serendipity\u201d. \n                 Tweet \n                 Facebook \n                 LinkedIn \n                 Weibo \n                 Wechat \n               \n                     How to make the most of carbon dioxide 2015-Oct-28 \n                   \n                     Materials science: The hole story 2015-Apr-08 \n                   \n                     Chemistry: It's not easy being green 2011-Jan-05 \n                   \n                     Chemistry Nobel won by ... chemists 2010-Oct-13 \n                   \n                     What is green chemistry? \n                   Reprints and Permissions"},
{"file_id": "535022a", "url": "https://www.nature.com/articles/535022a", "year": 2016, "authors": [{"name": "Kerri Smith"}, {"name": "Noah Baker"}], "parsed_as_year": "2006_or_before", "body": "Late nights, typos, self-doubt and despair. Francis Collins, Sara Seager and Uta Frith dust off their theses, and reflect on what the PhD was like for them. Francis Collins shakes his head in bewilderment as he flicks through the pages of his thesis. \u201cAt this point it looks very much like another language,\u201d he says, looking with puzzlement at page 71, which contains far more equations than text. The PhD was on theoretical quantum chemistry, and had \u201cabsolutely no practical application\u201d, Collins says. Looking at it now, \u201cit does feel a little bit like this was another person\u201d. Collins was in his early 20s when he was studying for his doctorate at Yale University in New Haven, Connecticut, modelling how small groups of atoms interact. \u201cA lot of what I did was pencil on paper, trying to solve really complicated calculus equations. It was a little lonely at times,\u201d he says. Then, about halfway through his studies, he decided to quit his PhD and transfer to medical school. He ended up finishing the thesis in his spare time. \u201cI spent many nights and many weekends trying to get this written out,\u201d he says, with something of a grimace. \u201cI made myself a schedule and tried to stick to it, with my little electric typewriter, banging away.\u201d The writing machines have changed, but the slog is the same.  Completing a thesis is a huge undertaking  for PhD students, and many struggle to get that far: only around 70% of UK students who embark on doctoral studies actually emerge with a PhD, and the rate is just 50% in the United States. Many of those who do finish  move on to careers outside academia ; even those who stay sometimes wish they'd spent more time writing papers \u2014  the currency of career progression  \u2014 instead. So what value does the thesis retain, and what lessons does completing one impart? To find out,  Nature  asked three prominent scientists to dig out their theses, thumb through the pages and reflect on what they \u2014 and the world \u2014 gained from them. What did they learn that could be of value to students who are writing up today? Their reflections, sometimes surprising, are recorded in three short films that accompany this article. Collins's PhD was the start of a stellar career: he famously moved into biological research, identified the  gene that causes cystic fibrosis , led the Human Genome Project to completion and now, more than 40 years after writing his thesis,  directs the US National Institutes of Health . But that doesn't mean that his PhD changed the world. \u201cDid it really add significantly to the knowledge the Universe contains?\u201d he says. \u201cWell, it would be a rather small contribution, to be sure.\u201d But like others who went 'back to the thesis' for  Nature , he thinks that what matters was not so much the subject or results, but what he learnt about the process of research along the way. \u201cI think the greatest beneficiary of my PhD was not the Universe,\u201d Collins says. \u201cIt was probably me.\u201d \n               Francis Collins \n             \n               Semiclassical theory of vibrationally inelastic scattering, with application to H \n               + \n                and H \n               \n                 2 \n               \n                (1974) \n             Collins keeps his PhD on a low shelf in his office, next to those from his past students. He pulls it out and places a paternal hand on the thick, leather-bound book. \u201cI think it turned out pretty well. It's quite a hefty document,\u201d he says. The road to that document started back in 1970, when Collins arrived in the lab of Jim Cross, a theoretical chemist at Yale. Cross remembers Collins as \u201ca quiet, unassuming man, not particularly sophisticated culturally\u201d. But, he says, \u201cI quickly realized that he was one of the brightest and most broadly based students that I have ever met\u201d. Collins was tasked with developing theoretical models to explain what happens when a proton is fired at a hydrogen molecule: how does the energy of the two bodies dissipate, and could the hydrogen be coaxed into another state? Day after day, he sat at his basement desk, tackling calculus equations and writing corresponding computer programs in Fortran. He used a machine in the university computer centre to punch the programs onto cards, then waited until after 1 a.m., when electricity was cheaper, to feed the cards into the mainframe computer. \u201cIt did make me begin to wonder, OK, is this the right path for me?\u201d It wasn't \u2014 something Collins came to realize during an all-nighter about halfway through his studies. He was talking to a fellow graduate student, Jay Gralla, who was examining how molecules of RNA fold up into secondary structures. The broader aim was to understand the rules by which genetic information in RNA and DNA is used to build biological systems. Collins was blown away. \u201cI was astounded that I had missed this whole thing about biology \u2014 that it was digital, it was an information system, it did have principles,\u201d he says. \u201cIt was a revelation.\u201d Shortly afterwards, Collins decided to switch to medical school. \u201cThat was a wrenching time,\u201d he says. He was drawn to explore biology and medicine, but he also had a growing family, financial strains and \u201call kinds of self-doubts\u201d. He also didn't know if he'd actually done enough work to complete his PhD \u2014 but Cross told him to write it up anyway. Collins stayed behind in New Haven to write, while his wife and young daughter left for the family's new home in North Carolina. He still couldn't get it done before his medical studies started. By the time of his graduation ceremony, in May 1974, he was finishing his first year of medical school and expecting a second child. He didn't attend. Several years later, his medical training complete, Collins returned to Yale to work in a molecular-biology lab, and never looked back. The exactitude instilled in him by his PhD stayed with him. He had learned to assess a complex system, strip it down to its component parts and glean insights from it. \u201cThat's something that I do now in my lab,\u201d he says. His thesis work isn't in much demand. The model of colliding particles that Collins developed was a good match with others' experimental findings, and made some useful approximations, but the work has been convincingly superseded by advances in processing power. \u201cThese days, a theoretical chemist wouldn't dream of limiting themselves by doing these approximations,\u201d he says. Reflecting on it now, Collins is glad that he took the risk of switching fields, and would encourage today's PhD students to take chances too. Transitions in a career are \u201cwhen you grow the fastest; they're when you're really alive\u201d. And think big, he urges. \u201cIf you're going to study something, study something important. It might be risky, it might be hard, it might not work, but there are too many people spending their time on obvious next steps.\u201d \n               Sara Seager \n             \n               Extrasolar giant planets under strong stellar irradiation (1999) \n             A flicker of embarrassment crosses Sara Seager's face when she is asked whether there are any mistakes in her thesis. \u201cI definitely have at least one typo. I know where it is, unfortunately. I hate to talk about it.\u201d She thinks for a moment, her thesis unopened on the desk before her. \u201cNow that you mention it, I should probably go back and correct it with a pen.\u201d There is little else for Seager to regret about her thesis. She is now a planetary scientist at the Massachusetts Institute of Technology in Cambridge, and, unusually, her PhD helped to found a field. \u201cIt might have been one of the first \u2014 if not the first \u2014 PhD theses on exoplanets,\u201d she says. In 1996, when Seager started her postgraduate studies at Harvard University in Cambridge, just half a dozen planets had been spotted orbiting distant suns. They could be detected only indirectly, mostly by capturing  the 'wobble' that an orbiting planet caused in the movement of a star . And the signals were noisy \u2014 some astronomers didn't believe that exoplanets were real. Seager was encouraged to enter the field by her supervisor at Harvard, Dimitar Sasselov, who was keen to take a different approach. Sasselov encouraged Seager to study the atmospheres of exoplanets to find ones that  might harbour interesting chemistry or indicate life . This seemed unlikely to work when the planets themselves were so difficult to detect. \u201cIt was a big risk at the time: a non-tenured professor and a grad student. Despite the advice otherwise of colleagues in the department, we went ahead,\u201d Sasselov says. Seager built a theoretical model suggesting that it should be possible to see starlight bouncing off a planet that was orbiting its star closely, and that analysing that light would reveal a fingerprint of the planet's chemical constituents, temperature and pressure 1 . Shortly afterwards, during her postdoc, she predicted that it should be possible to spot clouds in the atmosphere, and that one of the easiest elements to detect would be sodium 2 . It was tough going. She derived equations to represent the components of a planet's atmosphere and then, after teaching herself to code, plugged them into the computer models she was building. Her hours were long and isolated, and she would often hit programming bugs that threatened to derail her work. Meanwhile, ex-students from her department were calling from Silicon Valley: their companies were seeking people like her. \u201cI was far from committed to a career in science. I often thought of leaving,\u201d she says. Yet Seager \u201calways expressed a certainty about what she was working on\u201d, recalls David Charbonneau, a contemporary of hers at Harvard who now leads an astronomy group there, and was using Seager's theoretical predictions to explain his observational results. He describes her as a fierce intellectual and recalls how annoying she found any imperfection. \u201cShe would get frustrated if the data weren't as unambiguous as she would have liked.\u201d Seager says that the day she got her computer code to work \u201cwas one of the defining moments of my entire life\u201d. And once her work was finished, she didn't have to wait long for her predictions to be tested: in 2002, astronomers including Charbonneau detected the first exo-atmosphere 3 , and found that it contained the sodium signature, albeit at a slightly lower level than Seager had predicted. Since then, the field has flourished: 3,285 exoplanets have now been confirmed, and the study of their atmospheres has bloomed. The material in Seager's PhD has been used by astronomers to request time on the Hubble Space Telescope, Keck observatory and other instruments. And although Seager can't erase the sole typo from her thesis, she points out that the papers she published from it are free of mistakes. Did Seager enjoy her PhD? \u201cUnfortunately, I think the answer might be no.\u201d But she does have fond memories of the time she spent writing up her research. \u201cI remember when I was finishing it, I didn't go to any other talks, I didn't really read the news, it was just put the blinkers on and get the job done.\u201d She found great satisfaction in devoting herself to a single task, and relished the clarity of thinking that afforded her. \u201cThe world goes away\u201d, she says. \u201cAnd so when you're in that zone, actually you're happy.\u201d Seager now tries to make sure that those in her lab have the space to think too. \u201cI do let the students spin their wheels. They have to, or they won't find their own way.\u201d And if she could give advice to her younger self, it would be simply: \u201cHang in there.\u201d As for the thesis itself \u2014 a slim, red volume with gold lettering \u2014 it's not something she feels sentimental about. \u201cI've met people who, they cry when they give away their kids' baby clothes, but I was never one of those \u2014 and I think I felt the same way about the thesis.\u201d She's more inclined to look forward. \u201cIn exoplanets, the best planet, the best discovery, is the next discovery.\u201d \n               Uta Frith \n             \n               Pattern detection in normal and autistic children (1968) \n             \u201cI have not looked at this in decades,\u201d declares Uta Frith as she retrieves her thesis from a study in her Victorian house in suburban London. The book, bound in sky-blue cloth, nestles on a low shelf, right next to a science-fiction encyclopaedia. She dusts it off with a cloth and opens it to the typewritten title page. \u201cIt looks very charming and childish. That's really my immediate impression. I did want a short and an interesting title.\u201d The title is as brief as Frith's PhD was: she had only two years of funding, starting in September 1966, and at the end of 1968 she duly turned in the thesis: 205 pages, typed up by a secretary from her handwritten manuscript. The bibliography is concise, just 10 generously spaced pages. \u201cSo little was known about autism at the time that this was the extent of the references I found,\u201d she says. Today, the developmental disorder is the  subject of several thousand publications each year . Frith came to London from her native Germany in 1964 to attend a course in abnormal psychology at the Institute of Psychiatry. There, for the first time, she met children with autism, and was \u201ccompletely fascinated. I still am,\u201d she says. She also met her future supervisors, psychologists Beate Hermelin and Neil O'Connor. At that time, autism spectrum disorders were poorly understood and carried a stigma. Those diagnosed were usually only the severe cases, children with profound intellectual and linguistic difficulties. The mainstream view in psychiatry was that autism was a product of a child's upbringing and environment and that distant, unloving parents \u2014 particularly mothers \u2014 were to blame. Frith refused to subscribe to that view. \u201cI was always struck, when I met the parents of these children, how little they corresponded to what was told about them in the literature,\u201d she says. The question that interested Frith was whether the children might process information differently from other kids. To investigate this, she showed children a simple box containing green and yellow counters that were arranged in a specific pattern. She then covered up the box and asked the child to build the sequence from memory. She often travelled to hospitals to test children with autism, as well as to nurseries and schools to assess children in her control group. She plugged the data into mechanical calculators that were \u201cvery, very noisy\u201d and then took the better part of a day to perform the statistical analysis. Frith turns to the later pages of her thesis to remind herself of what she found, well aware of how dated \u2014 even naive \u2014 it might sound. \u201cI'm a little bit afraid of this now. What nonsense can it be?\u201d She showed that children with and without autism both made errors in about 25% of the trials, but that they made different mistakes. Children in the control group tended to follow the pattern too strongly \u2014 perhaps placing three green counters together instead of two. Those with autism, however, placed the counters in their own simple pattern, such as green, yellow, green, yellow. Frith proposed that these children impose very strict patterns on the outside world, too, and this idea seemed to correlate with the behaviours that clinicians at the time considered characteristic of the condition \u2014 obsessions with particular objects, for example, or disliking change. Frith saw logic in the children's responses, and felt that they were not necessarily inferior to those of others. \u201cIt is presumptuous to think that those patterns imposed by autistic children are any worse than the patterns I have imposed on the data,\u201d the concluding paragraph of her thesis reads. \u201cWell, that's quite philosophical,\u201d she says, in modest delight. Frith is aware that she was studying at a golden time. Psychology was thriving in the United Kingdom; she had the undivided attention of two supervisors; and, just as she was coming to the end of her PhD, she was offered a full-time job at a Medical Research Council (MRC) unit where one of her supervisors had just been appointed director. \u201cI was just so fortunate,\u201d she says. The post led to a 50-year career with the MRC and University College London, during which Frith showed that children with autism have deficits in their 'theory of mind', the cognitive capacity to understand that others have their own beliefs and ideas. This was an important concept that was \u201cjust emerging in primate work\u201d and that she adapted to studies of autism, says Ami Klin, who directs the Marcus Autism Centre at Emory University in Atlanta, Georgia, and whose 1998 PhD was co-supervised by Frith. \u201cShe was always extraordinarily open-minded, patient, supportive,\u201d he says. Frith knows that today's PhD students have a much tougher time: funding is tight and academic jobs scarce. But she remains a fan of the PhD as an apprenticeship in research. She learned from scratch how to formulate hypotheses, design experiments and analyse data. \u201cIt does mean doing what we might call slave labour for some of the time, but you learn through that, and you can see what it feels like to be a scientist,\u201d she says. She admits that her thesis is a product of a different era \u2014 \u201cI'm quite sure it would not meet the requirements now\u201d \u2014 and she is willing to bet that there are mistakes in the text. \u201cBut who knows? I haven't read it. Why should I? There's so much else more interesting to read.\u201d See Editorial  page 7 \n                 Tweet \n                 Facebook \n                 LinkedIn \n                 Weibo \n                 Wechat \n               \n                     What\u2019s the point of the PhD thesis? 2016-Jul-06 \n                   \n                     The past, present and future of the PhD thesis 2016-Jul-06 \n                   \n                     The truth about exoplanets 2016-Feb-17 \n                   \n                     The exoplanet files 2015-Nov-18 \n                   \n                     The future of the postdoc 2015-Apr-07 \n                   \n                     How not to deal with the PhD glut 2014-Oct-22 \n                   \n                     Life outside the lab: The ones who got away 2014-Sep-03 \n                   \n                     Education: The PhD factory 2011-Apr-20 \n                   \n                     Seven ages of the PhD 2011-Apr-20 \n                   \n                     Francis Collins: One year at the helm 2010-Aug-11 \n                   \n                     Nature  special: Future of the PhD \n                   \n                     Nature  special: Exoplanets \n                   \n                     Nature  special: Autism \n                   \n                     \n                         Naturejobs  \n                       \n                   \n                     NIH \n                   \n                     Francis Collins group \n                   \n                     Sara Seager \n                   \n                     Uta Frith \n                   Reprints and Permissions"},
{"file_id": "535218a", "url": "https://www.nature.com/articles/535218a", "year": 2016, "authors": [{"name": "Jane Qiu"}], "parsed_as_year": "2006_or_before", "body": "Fossil finds in China are challenging ideas about the evolution of modern humans and our closest relatives. On the outskirts of Beijing, a small limestone mountain named Dragon Bone Hill rises above the surrounding sprawl. Along the northern side, a path leads up to some fenced-off caves that draw 150,000 visitors each year, from schoolchildren to grey-haired pensioners. It was here, in 1929, that researchers discovered a nearly complete ancient skull that they determined was roughly half a million years old. Dubbed Peking Man, it was among the earliest human remains ever uncovered, and it helped to convince many researchers that humanity first evolved in Asia. Since then, the central importance of Peking Man has faded. Although modern dating methods put the fossil even earlier \u2014 at up to 780,000 years old \u2014 the specimen has been eclipsed by discoveries in Africa that have yielded much older remains of ancient human relatives. Such finds have  cemented Africa's status as the cradle of humanity  \u2014 the place from which modern humans and their predecessors spread around the globe \u2014 and relegated Asia to a kind of evolutionary cul-de-sac. But the tale of Peking Man has haunted generations of Chinese researchers, who have struggled to understand its relationship to modern humans. \u201cIt's a story without an ending,\u201d says Wu Xinzhi, a palaeontologist at the Chinese Academy of Sciences' Institute of Vertebrate Paleontology and Paleoanthropology (IVPP) in Beijing. They wonder whether the descendants of Peking Man and fellow members of the species  Homo erectus  died out or evolved into a more modern species, and whether they contributed to the gene pool of China today. Keen to get to the bottom of its people's ancestry, China has in the past decade stepped up its efforts to uncover evidence of early humans across the country. It is reanalysing old fossil finds and pouring tens of millions of dollars a year into excavations. And the government is setting up a US$1.1-million laboratory at the IVPP  to extract and sequence ancient DNA . The investment comes at a time when palaeoanthropologists across the globe are starting to pay more attention to Asian fossils and how they relate to other early hominins \u2014 creatures that are more closely related to humans than to chimps. Finds in China and other parts of Asia have made it clear that a dazzling variety of  Homo  species once roamed the continent. And they are challenging conventional ideas about the evolutionary history of humanity. \u201cMany Western scientists tend to see Asian fossils and artefacts through the prism of what was happening in Africa and Europe,\u201d says Wu. Those other continents have historically drawn more attention in studies of human evolution because of the antiquity of fossil finds there, and because they are closer to major palaeoanthropology research institutions, he says. \u201cBut it's increasingly clear that many Asian materials cannot fit into the traditional narrative of human evolution.\u201d Chris Stringer, a palaeoanthropologist at the Natural History Museum in London, agrees. \u201cAsia has been a forgotten continent,\u201d he says. \u201cIts role in human evolution may have been largely under-appreciated.\u201d \n               Evolving story \n             In its typical form, the story of  Homo sapiens  starts in Africa. The exact details vary from one telling to another, but the key characters and events generally remain the same. And the title is always 'Out of Africa'. In this standard view of human evolution,  H. erectus  first evolved there more than 2 million years ago (see 'Two routes for human evolution'). Then, some time before 600,000 years ago, it gave rise to a new species:  Homo heidelbergensis , the oldest remains of which have been found in Ethiopia. About 400,000 years ago, some members of  H. heidelbergensis  left Africa and split into two branches: one ventured into the Middle East and Europe, where it evolved into Neanderthals; the other went east, where members became Denisovans \u2014  a group first discovered in Siberia in 2010 . The remaining population of  H. heidelbergensis  in Africa eventually evolved into our own species,  H. sapiens , about 200,000 years ago. Then these early humans expanded their range to Eurasia 60,000 years ago, where they replaced local hominins with a minuscule amount of  interbreeding . A hallmark of  H. heidelbergensis  \u2014 the potential common ancestor of Neanderthals, Denisovans and modern humans \u2014 is that individuals have a mixture of primitive and modern features. Like more archaic lineages,  H. heidelbergensis  has a massive brow ridge and no chin. But it also resembles  H. sapiens , with its smaller teeth and bigger braincase. Most researchers have viewed  H. heidelbergensis  \u2014 or something similar \u2014 as a transitional form between  H. erectus  and  H. sapiens . Unfortunately, fossil evidence from this period, the dawn of the human race, is scarce and often ambiguous. It is the least understood episode in human evolution, says Russell Ciochon, a palaeoanthropologist at the University of Iowa in Iowa City. \u201cBut it's central to our understanding of humanity's ultimate origin.\u201d The tale is further muddled by Chinese fossils analysed over the past four decades, which cast doubt over the linear progression from African  H. erectus  to modern humans. They show that, between roughly 900,000 and 125,000 years ago, east Asia was teeming with hominins endowed with features that would place them somewhere between  H. erectus  and  H. sapiens , says Wu (see \u2018Ancient human sites\u2019). \u201cThose fossils are a big mystery,\u201d says Ciochon. \u201cThey clearly represent more advanced species than  H. erectus , but nobody knows what they are because they don't seem to fit into any categories we know.\u201d The fossils' transitional characteristics have prompted researchers such as Stringer to lump them with  H. heidelbergensis . Because the oldest of these forms, two skulls uncovered in Yunxian in Hubei province, date back 900,000 years 1 , 2 , Stringer even suggests that  H. heidelbergensis  might have originated in Asia and then spread to other continents. But many researchers, including most Chinese palaeontologists, contend that the materials from China are different from European and African  H. heidelbergensis  fossils, despite some apparent similarities. One nearly complete skull unearthed at Dali in Shaanxi province and dated to 250,000 years ago, has a bigger braincase, a shorter face and a lower cheekbone than most  H. heidelbergensis  specimens 3 , suggesting that the species was more advanced. Such transitional forms persisted for hundreds of thousands of years in China, until species appeared with such modern traits that some researchers have classified them as  H. sapiens . One of the most recent of these is represented by two teeth and a lower jawbone, dating to about 100,000 years ago, unearthed in 2007 by IVPP palaeoanthropologist Liu Wu and his colleagues 4 . Discovered in Zhirendong, a cave in Guangxi province, the jaw has a classic modern-human appearance, but retains some archaic features of Peking Man, such as a more robust build and a less-protruding chin. Most Chinese palaeontologists \u2014 and a few ardent supporters from the West \u2014 think that the transitional fossils are evidence that Peking Man was an ancestor of modern Asian people. In this model, known as multiregionalism or continuity with hybridization, hominins descended from  H. erectus  in Asia interbred with incoming groups from Africa and other parts of Eurasia, and their progeny gave rise to the ancestors of modern east Asians, says Wu. Support for this idea also comes from artefacts in China. In Europe and Africa, stone tools changed markedly over time, but hominins in China used the same type of simple stone instruments from about 1.7 million years ago to 10,000 years ago. According to Gao Xing, an archaeologist at the IVPP, this suggests that local hominins evolved continuously, with little influence from outside populations. \n               Politics at play? \n             Some Western researchers suggest that there is a hint of nationalism in Chinese palaeontologists' support for continuity. \u201cThe Chinese \u2014 they do not accept the idea that  H. sapiens  evolved in Africa,\u201d says one researcher. \u201cThey want everything to come from China.\u201d Chinese researchers reject such allegations. \u201cThis has nothing to do with nationalism,\u201d says Wu. It's all about the evidence \u2014 the transitional fossils and archaeological artefacts, he says. \u201cEverything points to continuous evolution in China from  H. erectus  to modern human.\u201d But the continuity-with-hybridization model is countered by overwhelming genetic data that point to Africa as the wellspring of modern humans. Studies of Chinese populations show that 97.4% of their genetic make-up is from ancestral modern humans from Africa, with the rest coming from extinct forms such as Neanderthals and Denisovans 5 . \u201cIf there had been significant contributions from Chinese  H. erectus , they would show up in the genetic data,\u201d says Li Hui, a population geneticist at Fudan University in Shanghai. Wu counters that the genetic contribution from archaic hominins in China could have been missed because no DNA has yet been recovered from them. Many researchers say that there are ways to explain the existing Asian fossils without resorting to continuity with hybridization. The Zhirendong hominins, for instance, could represent an  exodus of early modern humans from Africa  between 120,000 and 80,000 years ago. Instead of remaining in the Levant in the Middle East, as was thought previously, these people could have expanded into east Asia, says Michael Petraglia, an archaeologist at the University of Oxford, UK. Other evidence backs up this hypothesis: excavations at a cave in Daoxian in China's Hunan province have yielded  47 fossil teeth so modern-looking  that they could have come from the mouths of people today. But the fossils are at least 80,000 years old, and perhaps 120,000 years old, Liu and his colleagues reported last year 6 . \u201cThose early migrants may have interbred with archaic populations along the way or in Asia, which could explain Zhirendong people's primitive traits,\u201d says Petraglia. Another possibility is that some of the Chinese fossils, including the Dali skull, represent the mysterious Denisovans, a species identified from Siberian fossils that are more than 40,000 years old. Palaeontologists don't know what the Denisovans looked like, but  studies of DNA  recovered from their teeth and bones indicate that this ancient population  contributed to the genomes of modern humans , especially Australian Aborigines, Papua New Guineans and Polynesians \u2014 suggesting that Denisovans might have roamed Asia. Mar\u00eda Martin\u00f3n-Torres, a palaeoanthropologist at University College London, is among those who proposed that some of the Chinese hominins were Denisovans. She worked with IVPP researchers on an analysis 7 , published last year, of a fossil assemblage uncovered at Xujiayao in Hebei province \u2014 including partial jaws and nine teeth dated to 125,000\u2013100,000 years ago. The molar teeth are massive, with very robust roots and complex grooves, reminiscent of those from Denisovans, she says. A third idea is even more radical. It emerged when Martin\u00f3n-Torres and her colleagues compared more than 5,000 fossil teeth from around the world: the team found that Eurasian specimens are more similar to each other than to African ones 8 . That work and more recent interpretations of fossil skulls suggest that Eurasian hominins evolved separately from African ones for a long stretch of time. The researchers propose that the first hominins that left Africa 1.8 million years ago were the eventual source of modern humans. Their descendants mostly settled in the Middle East, where the climate was favourable, and then produced waves of transitional hominins that spread elsewhere. One Eurasian group went to Indonesia, another gave rise to Neanderthals and Denisovans, and a third ventured back into Africa and evolved into  H. sapiens , which later spread throughout the world. In this model, modern humans evolved in Africa, but their immediate ancestor originated in the Middle East. Not everybody is convinced. \u201cFossil interpretations are notoriously problematic,\u201d says Svante P\u00e4\u00e4bo, a palaeogeneticist at the Max Planck Institute for Evolutionary Anthropology in Leipzig, Germany. But DNA from Eurasian fossils dating to the start of the human race could help to reveal which story \u2014 or combination \u2014 is correct. China is now making a push in that direction. Qiaomei Fu, a palaeogeneticist who did her PhD with P\u00e4\u00e4bo, returned home last year to establish a lab to extract and sequence ancient DNA at the IVPP. One of her immediate goals is to see whether some of the Chinese fossils belong to the mysterious Denisovan group. The prominent molar teeth from Xujiayao will be an early target. \u201cI think we have a prime suspect here,\u201d she says. \n               Fuzzy picture \n             Despite the different interpretations of the Chinese fossil record, everybody agrees that the evolutionary tale in Asia is much more interesting than people appreciated before. But the details remain fuzzy, because so few researchers have excavated in Asia. When they have, the results have been startling. In 2003, a dig on Flores island in Indonesia turned up a diminutive hominin 9 , which researchers  named  Homo floresiensis  and dubbed the hobbit . With its odd assortment of features, the creature still provokes debate about whether it is a dwarfed form of  H. erectus  or some more primitive lineage that made it all the way from Africa to southeast Asia and lived until as recently as 60,000 years ago. Last month, more surprises emerged from Flores, where researchers found the remains of a  hobbit-like hominin  in rocks about 700,000 years old 10 . Recovering more fossils from all parts of Asia will clearly help to fill in the gaps. Many palaeoanthropologists also call for better access to existing materials. Most Chinese fossils \u2014 including some of the finest specimens, such as the Yunxian and Dali skulls \u2014 are accessible only to a handful of Chinese palaeontologists and their collaborators. \u201cTo make them available for general studies, with replicas or CT scans, would be fantastic,\u201d says Stringer. Moreover, fossil sites should be dated much more rigorously, preferably by multiple methods, researchers say. But all agree that Asia \u2014 the largest continent on Earth \u2014 has a lot more to offer in terms of unravelling the human story. \u201cThe centre of gravity,\u201d says Petraglia, \u201cis shifting eastward.\u201d \n                 Tweet \n                 Facebook \n                 LinkedIn \n                 Weibo \n                 Wechat \n               \n                     \u2018Hobbit\u2019 relatives found after ten-year hunt 2016-Jun-08 \n                   \n                     Oldest ancient-human DNA details dawn of Neanderthals 2016-Mar-14 \n                   \n                     Teeth from China reveal early human trek out of Africa 2015-Oct-14 \n                   \n                     Neanderthals gain human neighbour 2015-Jan-28 \n                   \n                     Hominin DNA baffles experts 2013-Dec-04 \n                   \n                     Mystery humans spiced up ancients\u2019 sex lives 2013-Nov-19 \n                   \n                     New DNA analysis shows ancient humans interbred with Denisovans 2012-Aug-31 \n                   \n                     Human migrations: Eastern odyssey 2012-May-02 \n                   \n                     Chinese Academy of Sciences Institute of Vertebrate Paleontology and Paleoanthropology \n                   \n                     Peking Man site at Zhoukoudian \n                   Reprints and Permissions"},
{"file_id": "535214a", "url": "https://www.nature.com/articles/535214a", "year": 2016, "authors": [{"name": "Linda Nordling"}], "parsed_as_year": "2006_or_before", "body": "The country has developed the biggest programme of antiretroviral therapy in the world. Now scientists are exploring the long-term consequences of the drugs. Sixteen years ago, a sickly eleven-year-old became the human face of the AIDS epidemic that was sweeping South Africa. Standing up to speak in front of thousands at the International AIDS Conference in Durban, the diminutive Nkosi Johnson pleaded with the South African government to start giving the drug azidothymidine (AZT) to pregnant women with HIV so that they would not transmit the virus to their babies. \u201cDon't be afraid of us \u2014 we are all the same,\u201d he told the tearful audience. Johnson, himself HIV-positive from birth, lent a moment of high emotion to a tense week that had been dominated by clashes between scientists, activists and AIDS denialists. The president of South Africa, Thabo Mbeki, had sparked international condemnation when he opened the meeting with a speech that failed to acknowledge HIV as the cause of AIDS. That week in Durban was a watershed moment for the global AIDS response. As the first international AIDS conference ever held in a developing country, the meeting turned the spotlight on the epidemic in Africa, where the disease was raging worse than anywhere else. In developed countries, antiretroviral drugs (ARVs) had given hope to those living with HIV, but in poor nations, AIDS was still a death sentence for anyone unable to afford the astronomical cost of the medication: about US$10,000 per person, per year. In the week of the conference alone, an estimated 2,500 South Africans died from AIDS \u2014 one-quarter of them children. Johnson died little under a year after giving his speech. Next week, the International AIDS Conference returns to Durban \u2014 but to a radically changed outlook. The government's AIDS denialists have quietened, and international funding has poured in. Today, around half of the country's 7 million people with HIV are on ARVs \u2014 the biggest such programme in the world. Expanded access to the drugs is largely responsible for a leap in South Africa's average life expectancy at birth, from 53.4 years in 2004 to 62.5 in 2015 (see 'HIV in South Africa'). Mother-to-child transmission has fallen from a high of 30% in the early 2000s to just 1.5%. \u201cIt's a miraculous achievement that shows the world what can be done,\u201d says Steffanie Strathdee, associate dean of Global Health Sciences at the University of California, San Diego, and one of the headline speakers at this month's conference. But enormous challenges remain. South Africa still has the largest HIV epidemic in the world, and the rate of new infections remains depressingly high, especially among young women. At the epidemic's epicentre, the province of KwaZulu-Natal, a 15-year-old girl in some communities has an 80% risk of getting HIV in her lifetime. Epidemiologists struggle to fully understand the situation here for the simple reason that those infected are difficult to track. The massive ARV programme is putting immense pressure on the stretched public-health system \u2014 and yet the country has just signed up to a major expansion that could double the number of people taking the drugs. For doctors and medical researchers, the age of ARVs has thrown up new puzzles. They are just beginning to understand how long-term exposure to the drugs and to HIV itself could affect health. Some are exploring what happens to people who reach middle or old age after decades on the treatments. Others are trying to work out whether children who are exposed to HIV and ARVs in the womb might face health problems even if they do not contract the virus. These results could prove important for the rest of the world, where  ARVs could soon be rolled out on an unprecedented scale . The global AIDS-strategy body, UNAIDS, has set a target known as 90-90-90: by 2020, 90% of people living with HIV should know their status; 90% of those diagnosed should be on ARVs; and 90% of those on ARVs should have undetectable levels of the virus. South Africa has been a testing ground \u2014 to see whether ARV programmes can be ramped up in developing countries, and to see what happens to the population when they are. \u201cSouth Africa's success in defeating AIDS is key to the global effort to end AIDS,\u201d says Salim Abdool Karim, director of the Durban-based Centre for the AIDS Program of Research in South Africa. \n               Life after HIV \n             The face of HIV in South Africa today is a young woman called Thembisa Mbhobho. A 15-metre-tall mural of her greets motorists as they turn off the N2 highway to enter Cape Town's Khayelitsha township; it bears the slogan 'There is life beyond HIV'. Mbhobho knows all about that, being one of the millions of South Africans taking daily ARVs to keep the virus at bay. \u201cYou can live for more than 50 years if you take your medication correctly,\u201d she says. Mbhobho volunteered to be on the mural, which was painted by local artists for World AIDS Day last year, with backing from M\u00e9decins sans Fronti\u00e8res (MSF, also known as Doctors without Borders). \u201cI was diagnosed in 2008. I started taking ARVs in 2014,\u201d says the 26-year-old, whose first name means 'promise' in her mother tongue, Xhosa. Today, the virus is so well suppressed in her body that it doesn't show up in blood tests, reducing her likelihood of passing it on. She enjoys it when people recognize her from the mural, or from the television advert that she featured in to encourage people to find out their HIV status. \u201cThey motivate me, they say I must keep it up.\u201d Her five-year-old son is uninfected, thanks to the drugs Mbhobho was given while pregnant and in labour. She hopes that one day he'll be a pilot, or a lawyer. South Africa's road to widespread ARVs has been bumpy. Its first community treatment programme started in 2001 in Khayelitsha, but because of the HIV-denialist ideas subscribed to by Mbeki and other leading politicians, the programme \u2014 along with many others in the early days \u2014 was labelled as a 'feasibility study'. Some people in the international medical community doubted that ARVs could be effectively administered in Africa. In 2001, the head of the US Agency for International Development, Andrew Natsios, gained notoriety for saying that treatment would not work there because many people in Africa \u201chave not seen a clock or a watch their entire lives\u201d, and so would be unable to take their pills on time. He was wrong. The early studies showed that adherence and treatment outcomes were, in fact, better in African cohorts than in the United States 1 . In 2004, when South Africa started to offer free access to ARVs through the public-health system, just under 50,000 citizens received treatment. By 2007, the number was more than 380,000, and today it's well over 3 million. But the programmes have had different outcomes in South Africa than in high-resource settings in Europe and North America. \u201cIn Europe, when ARVs came along, the hospital wards emptied of people who were severely ill,\u201d says Gilles van Cutsem, medical coordinator for MSF in South Africa. \u201cWhen we started our HIV programme in Khayelitsha, the waiting room was full of sick people in wheelbarrows. There is less of that now, but people are still coming in very sick.\u201d Despite the information campaigns, and free drugs, many people still wait too long to get tested and treated, he says. Keeping track of the ARV roll-out has been challenging. The data centre of the Southern Africa International Epidemiological Database to Evaluate AIDS at the University of Cape Town has tracked treatment enrolment and retention since the initiative was established in 2006. But even those simple data have been difficult to collect and interpret, says Morna Cornell, a senior researcher and project manager at the centre. Sometimes the same person turns up in several different records \u2014 having moved, perhaps, and started going to a different clinic. Many others are enrolled, but are 'lost to follow-up'. For a long time, the centre's HIV-clinic data showed a low number of AIDS deaths and a high loss to follow-up. But when the data were integrated with the country's death register in the late 2000s, it emerged that more than 30% of those reported as lost were actually dead. Monitoring should get easier with the planned introduction of unique patient identifiers, says Cornell. She thinks that better data will help South Africa to improve its HIV programmes and assist in future research. Many scientists would like to see much more detailed clinical data collected, so that they can find out what happens when large populations take ARVs in the long term. There is a wealth of research on the health effects of the drugs, but much of it comes from Europe or North America, where patient populations are much smaller, and the medical resources are vastly bigger. \n               Complicating factors \n             Drug resistance is a looming issue. In rich countries, resistance testing is routinely done on every person diagnosed with HIV to ensure that they are given drugs that will work. But in South Africa, only a few of the available ARV drugs are provided free through the country's health system, and resistance tends to be poorly managed. \u201cPeople think drug resistance isn't a problem any more. But the place where resistance is going to emerge again is here. You can't throw so much drug out there and not experience it,\u201d says Deenan Pillay, a virologist and director of the Wellcome-Trust-funded Africa Centre for Population Health near Durban. The Africa Centre holds more than 15 years' worth of longitudinal data, gathered from a community of about 100,000 households in the rural district of uMkhanyakhude, about 2 hours' drive north of Durban, in KwaZulu-Natal. Sure enough, the data show that resistance is a burgeoning problem. Between 2010 and 2012, the proportion of new HIV cases that were already drug resistant rose by 7% \u2014 and Pillay suspects that it will grow further. \u201cPeople sit on failing drug combinations without getting help. That's a big risk,\u201d he says. South Africa's ageing population of people on ARVs is another focus of research. The fraction of HIV-infected people who are older than 50 is predicted to triple in the next 30 years 2 . Some studies have found that certain cancers are more common in people taking ARVs, and long-term use of the drugs has been linked to increased risk of hypertension, diabetes and obesity 3  \u2014 although it's difficult to establish whether this is caused by the drugs or by HIV itself. To find out, researchers are now looking at the possible effects of ARVs on metabolism. The life-saving benefits of the drugs outweigh these potential risks, but public-health researchers are nevertheless anxious to know what the future holds for a colossal ARV-taking population. In Europe and the United States, people ageing with HIV tend unsurprisingly to fare worse health-wise than do those who do not have the virus. But there was a surprise in store for Janet Seeley, a social scientist based at the London School of Hygiene and Tropical Medicine, when she and her colleagues took a close look at data for several hundred people over the age of 50 in Uganda and South Africa. They found that people with HIV who were taking ARVs had better quality of life and were more able to perform daily activities than were their HIV-negative peers 4 . \u201cMy first thought was 'this must be wrong', but it's logical really,\u201d she says. Care for people with HIV isn't always great, but at least they see a health practitioner on a regular basis. In South Africa, older people who received HIV therapy were more likely to be on treatment for other chronic conditions than were people in the comparison group who were HIV-negative. Researchers are also exploring what happens to children who are born to mothers with the virus but are not infected themselves. Clinicians have found that these 'uninfected exposed' children tend to have worse health \u2014 such as lower birth weight and bone density \u2014 than do those born to mothers without the virus. But the evidence is conflicting: a systematic review published earlier this year 5  found some studies indicating that uninfected exposed children are at increased risk of death, hospitalization and under-nutrition, and others that saw no such evidence. Even if the effects are marginal, the fact that around 30% of children born in South Africa fall into this category is cause to take the issue very seriously. Mark Cotton, head of infectious diseases at Tygerberg Children's Hospital in Cape Town, is convinced from what he has observed in the clinic that these children fare worse than others. The challenge lies in working out why. \u201cThere are theories that HIV-positive women might be sicker, or poorer, or that there might be more TB in these homes. But we think it could go beyond that,\u201d he says. Clive Gray, an immunologist at the University of Cape Town, is investigating the possible effects of  in utero  HIV and ARV exposure on the developing fetus and on an uninfected child's long-term health. With funding from the Canadian Institutes of Health Research, he and some colleagues are investigating 500 mother\u2013baby pairs \u2014 including mothers both with and without the virus \u2014 in Nigeria and South Africa. The study is not complete, but Gray and his colleagues have already spotted that HIV-negative children born to infected mothers have impaired immunity in their first year of life. As to whether HIV or the ARVs are the cause, Gray says, that is difficult to tease out. And he's not sure it's the most important question to answer. \u201cThese children are sick, and whether that is because they are exposed to HIV or to the drug, or to the combination, it doesn't matter from a public-health point of view.\u201d Ultimately, the work might point to alternative ways to handle HIV in pregnant women. And, Gray says, the research might also help to explain why women with the virus often go into preterm labour \u2014 another confounding factor, because it is known to negatively affect child development. \n               Break the cycle \n             But perhaps the most pressing question is how South Africa's massive rollout of ARVs will affect infection rates. People who have a suppressed viral load are less likely to transmit the virus to others \u2014 a fact that led the World Health Organization (WHO) to recommend last year that all people who test positive for HIV  immediately go on ARVs , rather than waiting until their CD4 count \u2014 an indicator of disease progression \u2014 fell below a certain level. In May, South Africa's health minister, Aaron Motsoaledi, announced that the country would adopt these guidelines from September, a strategy that could double the number who receive ARVs. Motsoaledi also said that his department would supply prophylactic treatment to sex workers, who have a very high risk of contracting HIV. Even patchy ARV coverage can slash infection rates. A study by the Africa Centre found that someone living in a community where 30\u201340% of HIV-infected people are on treatment is 38% less likely to get infected than is someone living in a community where fewer than 10% are on treatment 6 . In practice, however, ARV therapy is  unlikely to ramp up quickly . A major difficulty lies in getting people tested in the first place, and another is getting those who test positive to accept treatment \u2014 because many of them will not feel sick \u2014 and to stay on it. To reach those people, the country has to rethink its HIV programmes, van Cutsem says. Research by MSF has found that allowing people who manage their HIV well to attend fewer clinics helps to take the strain off the facilities and health workers. He also supports the introduction of self-testing kits. These can already by bought over the counter in South African pharmacies, but are not promoted in the public-health sector. \u201cThere was an initial concern in South Africa that it would lead to self-harm or domestic violence. But frankly, there is no evidence saying the risk would be bigger than if you test with a counsellor,\u201d van Cutsem says. There is no shortage of scientists to study the evolving epidemic. Since South Africa hosted the International AIDS Conference in 2000, many world-class research centres have emerged to develop vaccines and therapies that might, one day, supersede ARVs. And later this year, the country will start a large trial of an HIV vaccine candidate that has shown promising results in Thailand 7 . It would be the first major vaccine trial in years and one that, if successful, could make a big difference, especially to women with the highest risk of infection. For many of the researchers returning to Durban next week, the trip will remind them of how far they've come. \u201cBefore ARVs, HIV was the same around the world, in that everybody died who got it,\u201d says Carlos del Rio, a global-health researcher at Emory University School of Medicine in Atlanta, Georgia, and one of the keynote speakers in Durban this year. \u201cBut after 1996, people stopped dying in the developed world. It was that meeting in Durban that made us believe that the impossible would be possible. Now people live in South Africa with HIV who would otherwise have died.\u201d But the conference also makes del Rio reflect on how far there is to go. A huge number of African people are reaching reproductive age, and encouraging them to understand and manage their HIV risk will be crucial to sustaining the continent's momentum. Some would not have seen the epidemic at its chilling peak, and could become complacent about the need to protect themselves and others. \u201cWe run a massive risk that if we fail with that generation, the epidemic could get even worse.\u201d \u201cAre we at the end of AIDS?\u201d says del Rio. \u201cI don't think so. I think we're at the beginning of the end of AIDS. But not keeping up investment, not keeping our eyes on the ball, could be potentially devastating.\u201d \n                 Tweet \n                 Follow @NatureNews \n               \n                     The truth about fetal tissue research 2015-Dec-07 \n                   \n                     World Health Organization to recommend early treatment for everyone with HIV 2015-Jul-20 \n                   \n                     The HIV epidemic can be stopped 2015-Jul-07 \n                   \n                     How to beat HIV 2015-Jul-07 \n                   \n                     AIDS prevention: Africa's circumcision challenge 2013-Nov-13 \n                   \n                     Nature  Collection: HIV\u00a0 \n                   \n                     UNAIDS \n                   \n                     AIDS 2016 meeting \n                   Reprints and Permissions"},
{"file_id": "536388a", "url": "https://www.nature.com/articles/536388a", "year": 2016, "authors": [{"name": "Amy Maxmen"}], "parsed_as_year": "2006_or_before", "body": "A non-profit organization is proving that new drugs don't have to cost a fortune. Can its model work more broadly? First, there was the pitching and rolling in an old Jeep for eight hours. Next came the river crossing in a slender canoe. When Nathalie Strub Wourgaft finally reached her destination, a clinic in the heart of the Democratic Republic of the Congo, she was exhausted. But the real work, she discovered, had just begun. It was July 2010 and the clinic was soon to launch trials of a treatment for sleeping sickness, a deadly tropical disease. Yet it was woefully unprepared. Refrigerators, computers, generators and fuel would all have to be shipped in. Local health workers would have to be trained to collect data using unfamiliar instruments. And contingency plans would be needed in case armed conflict scattered study participants \u2014 a very real possibility in this war-weary region. Adam Levy finds out how a non-profit group is developing drugs for the poorest of the poor This was a far cry from Wourgaft's former life as a top executive in the pharmaceutical industry, where the hospitals that she commissioned for trials were pristine, well-resourced and easy to reach. But Wourgaft, now medical director for the innovative Drugs for Neglected Diseases initiative (DNDi), was confident that the clinic could handle the work. She was right. With data from this site and others, the DNDi will next year seek approval for a sleeping-sickness tablet, fexinidazole. It would be a massive improvement on existing treatment options: an arduous regimen of intravenous injections, or a 65-year-old arsenic-based drug that can be deadly. The DNDi is an unlikely success story in the expensive, challenging field of drug development. In just over a decade, the group has earned approval for six treatments, tackling sleeping sickness, malaria, Chagas' disease and a form of leishmaniasis called kala-azar. And it has put another 26 drugs into development. It has done this with US$290 million \u2014 about one-quarter of what a typical pharmaceutical company would spend to develop just one drug. The model for its success is the  product development partnership  (PDP), a style of non-profit organization that became popular in the early 2000s. PDPs keep costs down through collaboration \u2014 with universities, governments and the pharmaceutical industry. And because the diseases they target typically affect the world's poorest people, and so are neglected by for-profit companies, the DNDi and groups like it face little competitive pressure. They also have lower hurdles to prove that their drugs vastly improve lives. Now, policymakers are beginning to wonder whether their methods might work more broadly. \u201cFor a long time, people thought about R&D as so complicated that it could only be done by the biggest for-profit firms in the world,\u201d says Suerie Moon, a global-health researcher at the Harvard T.H. Chan School of Public Health in Cambridge, Massachusetts, who studied PDPs and joined the DNDi's board of directors in 2011. \u201cI think we are at a point today where we can begin to take lessons from their experience and begin to apply to them non-neglected disease,\u201d she says. In that vein, the DNDi has started research on alternatives to pricey drugs for hepatitis C, and is spearheading an effort to create antibiotics for drug-resistant infections, a problem that pharmaceutical companies have been slow to contend with. If successful, the work could challenge standard assumptions about drug development, and potentially rein in the runaway price of medications. \u201cWe can't match our financial figures one to one,\u201d says executive director Bernard P\u00e9coul. \u201cBut we believe that DNDi can demonstrate that a different model is possible for R&D.\u201d \n               The pipeline \n             When medical charity M\u00e9decins Sans Fronti\u00e8res (MSF; also known as Doctors without Borders) won the Nobel Peace Prize in 1999, its members decried the lack of lifesaving drugs for diseases of the poor, and used the Nobel prize money to kick-start the DNDi. P\u00e9coul, a soft-spoken Frenchman who had been with MSF for 20 years, took the helm when the initiative launched in Geneva, Switzerland, in 2003. Pharmaceutical executives were sceptical. Drug development is an expensive, complex, decade-long endeavour. \u201cIn the early days, we saw DNDi as a bit amateurish,\u201d recalls Fran\u00e7ois Bompart, a medical director at the Paris-based drug company Sanofi. \u201cWe thought, they cannot be serious.\u201d P\u00e9coul and his team started with a safe project. In 2001, the World Health Organization had called for malaria drugs that combined ingredients to slow the spread of resistance to the single best available agent, artemisinin. But the poverty of most people who need malaria drugs meant that the private sector had little incentive to create and test such combination therapies. P\u00e9coul contacted Sanofi, which owned two malaria treatments: one based on artemisinin, and the other on the slower-acting amodiaquine. He proposed a deal in which the DNDi would pay for and run clinical trials on a pill that combined the two drugs. In return, Sanofi would not patent the pill and would sell an adult course of treatment for no more than $1, half that for children. \u201cTo me it sounded very aggressive and not reasonable, since the two drugs separately were two to three times that,\u201d says Bompart. But P\u00e9coul convinced Sanofi that the move would be good for the company's public image. He also compromised, allowing Sanofi to stipulate that it could reach the low price gradually. As it turned out, by the time the pills were approved in 2007, manufacturing costs had come down far enough for the company to meet the target price right out of the gate. Hundreds of millions of pills have since been distributed in Africa. All told, the project cost the DNDi about $14 million, a tiny sum in the world of drug development. It has since replicated the process to develop other combination therapies (see 'Discount drugs'). Although they improve on existing therapies, some of these combinations remain inadequate. The DNDi's sleeping-sickness therapy NECT, for example, reduces a standard treatment from 56 intravenous infusions to 14. That is still problematic in affected countries: clean needles can be hard to come by, and long hospital stays are often impossible. People need a pill. Drug development from scratch is arduous and expensive. It begins with experiments on hundreds of thousands of chemicals in the lab, looking for one that kills a pathogen without harming the host. The DNDi does not have a laboratory, so it does this through collaborations. It searches for promising leads in compound libraries generated by biotechnology and pharmaceutical companies. Many firms are willing to share access to these precious libraries because the diseases that the DNDi targets will not result in blockbuster drugs, so it is not infringing on their turf. The DNDi then contracts high-throughput screening centres, such as those at the Institut Pasteur Korea in Seongnam and the University of Dundee, UK, to test them out. \u201cWe use the same technique that pharma does,\u201d says Rob Don, director of discovery and preclinical research at the DNDi, \u201cbut we do it for less.\u201d In 2007, such efforts identified fexinidazole, a compound that had shown promise against single-celled parasites but was pulled from development before reaching clinical trials. The DNDi turned it into a tablet, and passed it to its clinical-development team two years later. The DNDi approached Sanofi again and promised to take care of trials if the company could file for regulatory approval. Sanofi warned that human trials would not be easy, because sleeping sickness is not common and people who get it tend to live in remote, unstable regions. But with the existing therapies being so dreadful, Wourgaft argued that any improvements from fexinidazole would be clear. \u201cThe delta between what we bring and what exists is huge. You don't need a magnifying glass on thousands of patients to see it.\u201d She set up multiple small trial sites in the Democratic Republic of the Congo and the Central African Republic and pooled their data. \n               Clinical challenge \n             Wourgaft says that the studies were the hardest she has ever run. In addition to logistical challenges, civil war erupted in the Central African Republic shortly after the study launched, and rebel groups repeatedly robbed a clinic there and threatened the Congolese surgeon leading the trial. \u201cI squeeze all my energy into each project,\u201d Wourgaft says. \u201cIt's as if I'm using forceps to deliver a baby \u2014 and the baby is an elephant.\u201d The final trials on fexinidazole conclude this year, and Wourgaft is hopeful that the data will earn regulators' stamp of approval. The project has so far cost the DNDi about $45 million \u2014 and it stands to help 21 million people at risk of the disease in Africa. In a few months, Wourgaft will launch another trial, on a completely new oral drug \u2014 SCYX-7158 \u2014 that may cure people with sleeping sickness in a few days. The DNDi estimates that its development up to approval will cost around $50 million. \n               Breaking billions \n             For more than three decades, economists at the Tufts Center for the Study of Drug Development in Boston, Massachusetts, have collected proprietary data from pharmaceutical companies, and used it to calculate the average cost of developing a new drug. The most recent estimate is $1.4 billion. This is used to justify exorbitant drug prices \u2014 companies must recoup their investments. But many don't think it has to cost that much. Even the chief executive of London-based pharmaceutical giant GlaxoSmithKline, Andrew Witty, has called billion-dollar estimates \u201cone of the great myths of the industry\u201d. He attributed the huge sums to spending too much time on failures. Drug candidates can be killed as a result of safety concerns, poor efficacy or profitability worries, and he argued that companies could save money by dropping bad leads sooner. Others say that the figure is inflated by large and excessive trials done to prove that a new drug works just slightly better than an existing one. By averaging the cost of projects in its portfolio, the DNDi says that it can develop a new drug for between $110 million and $170 million. Like the Tufts estimate, these prices include a theoretical cost of failed projects. The DNDi admits to enjoying perks that pharma does not have. It keeps overhead costs low because its organization is virtual. The research organizations that it contracts probably charge the group less than they would a for-profit company. The DNDi also relies on scientific consultants who work for low pay because they relish the chance to make lifesaving drugs without considering competitors, investors and marketing. \u201cDNDi gets a lot for free,\u201d says Richard Bergstr\u00f6m, director-general of the European Federation of Pharmaceutical Industries and Associations in Brussels. \u201cMy companies do a lot of pro bono work, and so do universities.\u201d Still, the organization reckons that such in-kind contributions account for just 10\u201320% of its expenditure. It saves much more through efficient collaboration (avoiding duplicated effort by screening pooled libraries, for example) and a focus on desperately needed drugs. Clinical trials can be smaller, faster and cheaper when the people who run them don't have to struggle to show barely perceptible improvements. And the DNDi kills candidate compounds only if they fail on safety or efficacy \u2014 it doesn't have to worry about marketability. By contrast, a few for-profit companies froze candidate drugs for hepatitis C after Gilead Sciences of Foster City, California, brought powerful drugs to the market. \u201cA lot of R&D failures in pharma are commercial rather than scientific,\u201d says Don. \u201cWe keep going until it gets to market or scientifically fails.\u201d The DNDi has earned respect from the industry, even though its founding organization has been antagonistic to big pharma. \u201cAlthough DNDi came out of MSF, they don't let ideological viewpoints get in the way of making progress,\u201d says Jon Pender, vice-president of government affairs at GlaxoSmithKline. He and others praise P\u00e9coul's skills at negotiation, and the DNDi's pragmatic approach to development challenges. Policymakers have taken notice, too. Last year, the World Health Organization asked the DNDi to consider antibiotics for drug-resistant infections in the developing world; in May, the initiative announced that it would start the GARD (Global Antibiotic Research and Development) partnership with $2.2 million in seed funding. GARD will start by repurposing and combining existing antibiotics to treat a few diseases, including gonorrhoea and infections in newborn babies. Marja Esveld, a research adviser at the Netherlands ministry of health, is watching it closely. \u201cWe are worried about the rising costs of pharmaceuticals,\u201d she says, \u201cand so for us, GARD is also a kind of experiment to see if the DNDi model can work for the development of drugs in the Western world.\u201d Not everyone is convinced. Economist Ramanan Laxminarayan, director of the Center for Disease Dynamics, Economics and Policy in Washington DC, says that pharmaceutical companies have an incentive to make antibiotics for multidrug-resistant infections because patients in the United States and Europe will pay to get them \u2014 and non-profit organizations cannot hope to compete. Once the drugs exist, he says, subsidies could ensure that they are affordable. P\u00e9coul disagrees: he doesn't think that subsidies, donations or tiered pricing can ensure accessibility. \u201cWe need appropriate products and a sustainable market for those products,\u201d he says. That environment has not materialized for other conditions: Gilead's hepatitis C drugs, for example, are listed at more than $74,000 for a course. And their potency against some strains of the virus is questionable, says P\u00e9coul. When he and his team learned about other hepatitis drug candidates being frozen, he launched a project to turn them into treatments that more people could use and afford. They're also attempting to combine existing drugs. If the group succeeds with this and with antibiotics, it will have shown that its model can be applied to diseases that affect developed countries. \u201cI hope we provide lessons that can be used by others,\u201d says P\u00e9coul. But companies won't simply adopt the DNDi's methods, because they do not generate profit. The investors who keep firms alive are concerned with the bottom line. P\u00e9coul says that a transformation would require government involvement and a reorganization of the development process. It would need a system to prioritize what treatments are needed and which companies and organizations could collaborate; and it would require forethought about how the final products would reach those in need. It means shifting away from profit-based incentives to things such as prizes and government funding. Today's profit-driven approach is not only expensive, P\u00e9coul says, it fails huge swathes of the population. When Wourgaft reflects on the differences between her career in pharma and her work at the DNDi, she thinks not about the cost of research and development, but about the value of a human life. She recalls one trip to a Congolese sleeping-sickness trial site. She sat on a cot beside a woman in the middle of a psychotic episode, and spoke to her desperate husband. Later, she learned that the woman survived because of the DNDi's treatment. \u201cWhen you see that, you know the value of what you're doing,\u201d she tells me. \u201cWe are trying to fix diseases that are lethal \u2014 this is really serious medicine.\u201d \n                 Tweet \n                 Facebook \n                 LinkedIn \n                 Weibo \n                 Wechat \n               \n                     How to beat the next Ebola 2015-Aug-05 \n                   \n                     How to beat HIV 2015-Jul-07 \n                   \n                     Projects set to tackle neglected diseases 2014-Jan-07 \n                   \n                     Neglected diseases fund touted 2010-May-18 \n                   \n                     Drug patent plan gets mixed reviews 2009-Feb-23 \n                   \n                     The DNDi \n                   Reprints and Permissions"},
{"file_id": "537026a", "url": "https://www.nature.com/articles/537026a", "year": 2016, "authors": [{"name": "Carrie Arnold"}], "parsed_as_year": "2006_or_before", "body": "Snakes kill tens of thousands of people each year. But experts can't agree on how best to overcome a desperate shortage of antivenom. Abdulsalam Nasidi's phone rang shortly after midnight: Nigeria's health minister was on the line. Nasidi, who worked at the country's Federal Ministry of Health, learnt that he was needed urgently in the Benue valley to investigate a cluster of dying patients. People were bleeding out of their noses, their mouths, their eyes. Names of spine-chilling viruses such as Ebola, Lassa and Marburg raced through Nasidi's mind. When he arrived in Benue, he found people splayed on the ground and tents serving as makeshift hospital wards and morgues. But Nasidi quickly realized that the cause of the mystery illness was millions of times larger than any virus. The onset of the rainy season had brought the start of spring planting for farmers in the valley, and flooding had disturbed the resident carpet vipers ( Echis ocellatus ). Many farmers were simply too poor to buy boots \u2014 and their exposed feet became targets for the highly venomous snakes. Nasidi wanted to help, but he found himself with limited tools. He had only a small amount of antivenom with which to neutralize the toxin \u2014 and it quickly ran out. Once the hospital exhausted its supply, people stopped coming. No one knows how many people were killed. In an average year, hundreds of Nigerians die from snakebite, and that rainy season, which started in 2012, was far from average. Snakebites are a growing public-health crisis. According to the World Health Organization, around 5 million people worldwide are bitten by snakes each year; more than 100,000 of them die and as many as 400,000 endure amputations and permanent disfigurement. Some estimates point to a higher toll: one systematic survey concluded that in India alone, more than 45,000 people died in 2005 from snakebite 1  \u2014 around one-quarter the number that died from HIV/AIDS (see 'The toll of snakebite'). \u201cIt's the most neglected of the world's neglected tropical diseases,\u201d says David Williams, a toxinologist and herpetologist at the University of Melbourne, Australia, and chief executive of the non-profit organization Global Snakebite Initiative in Herston. Many of those bites are treatable with existing antivenoms, but there are not enough to go around. This long-standing problem became international news in September 2015, when M\u00e9decins Sans Fronti\u00e8res (MSF, also known as Doctors Without Borders) announced that the last remaining vials of the antivenom Fav-Afrique, used to treat bites from several of Africa's deadliest snakes,  were about to expire . The French pharma giant Sanofi Pasteur in Lyons had decided to cease production in 2014. MSF estimates that this could cause an extra 10,000 deaths in Africa each year \u2014 an \u201cEbola-scale disaster\u201d, according to Julien Potet, a policy adviser for MSF in Paris. Yet, because most of those affected by snakebites are in the poorest regions of the world, the issue has been largely ignored. \n               Spotlight on snakes \n             In May, however, the crisis was discussed for the first time at the annual World Heath Assembly meeting in Geneva, Switzerland. The world's handful of snakebite specialists gathered in a small conference room in the Palais des Nations \u2014 although they shared concern over the problem, they were split about how to solve it. Many want to use synthetic biology and other high-tech tools to develop a new generation of broad-spectrum antivenoms. Others argue that existing antivenoms are safe, effective and low cost, and that the focus should be on improving their production, price and use. \u201cFrom the physician perspective, patient care and public health comes before anything new,\u201d says Leslie Boyer, who directs an institute dedicated to antivenom study at the University of Arizona, Tucson. The debate mirrors those around many other developing-world challenges, from improving agriculture to providing clean drinking water. Do people need high-tech solutions, or can cheaper, lower-tech remedies do the job? The answer is simple to Jean-Philippe Chippaux, a physician working on snakebite for the French Institute of Research for Development in Cotonou, Benin. \u201cWe have the ability to fix this problem now. We just lack the will to do it,\u201d he says. Every December, Williams sees snakebite victims flood into the Port Moresby General Hospital in Papua New Guinea. Nearly all of them were bitten by the taipan ( Oxyuranus scutellatus ), one of the world's deadliest snakes, which emerges at the start of the rainy season. The venom stops a victim's blood from clotting, paralyses muscles and leads to a slow, agonizing death. It seems a far cry from Australia, where Williams is based. \u201cThere's this incredible suffering just 90 minutes away from the modern world,\u201d he says. Yet Williams knows that these people are the lucky ones. The hospital ward, which might be treating as many as eight taipan victims at any time, is often the only place in the country with antivenom drugs. Without them, some 10\u201315% of all snakebite victims die; with them, just 0.5% do. The situation is reflected around the world. \u201cMany countries don't want to admit that they have such a primeval-sounding problem,\u201d Chippaux says. The method used to make antivenom has changed little since French physician Albert Calmette developed it in the 1890s. Researchers inject minuscule amounts of venom, milked from snakes, into animals such as horses or sheep to stimulate the production of antibodies that bind to the toxins and neutralize them. They gradually increase doses of venom until the animal is pumping out huge amounts of neutralizing antibodies, which are purified from the blood and administered to snakebite victims. Across much of Latin America, government-funded labs typically produce antivenoms and distribute them free of charge. But in other areas, especially sub-Saharan Africa, these life-saving medications are too often out of reach. Many governments lack the infrastructure or political will to purchase and distribute antivenom. Bribery and corruption often jack up the price of an otherwise inexpensive drug from a typical wholesale cost of US$18 to $200 per vial to a retail cost between $40 and $24,000 for a complete treatment, according to a 2012 analysis 2 . Not all hospitals and clinics can afford the antivenom, and some won't risk buying it because their patients either can't pay for it or won't, because they doubt that it really works. With no reliable market for the medicines, some pharmaceutical companies have halted production. Sanofi Pasteur stopped making Fav-Afrique because, at an average retail price of around $120 per vial, it just couldn't sell enough to make production worthwhile. A total of 35 government or commercial manufacturers produce antivenom for distribution around the world, but only 5 now make the drugs for sub-Saharan Africa. In the absence of medicines, snakebite victims have been known to drink petrol, electrocute themselves or apply a poultice of cow dung and water to the bite, says Tim Reed, executive director of Health Action International in Amsterdam. But there are also problems with the drugs themselves, says Robert Harrison, head of the Alistair Reid Venom Research Unit at the Liverpool School of Tropical Medicine, UK. They often have a limited shelf life and require continuous refrigeration, which is a problem in remote areas without electricity. And many are effective against just one species of snake, so clinics need an array of medicines constantly on hand. (A few, such as Fav-Afrique, combine antibodies to create a broad-spectrum product.) Venoms from spiders and scorpions typically have only one or two toxic proteins; snake venoms can have more than ten times that amount. They are a \u201cpandemonium of molecules\u201d, says Alejandro Alag\u00f3n, a toxinologist at the National Autonomous University of Mexico in Mexico City. Researchers do not always know which proteins in this toxic soup are the damaging ones \u2014 which is why some think that smarter biology could help. \n               Old problem, new solution \n             Ten years ago, teams led by Harrison and Jos\u00e9 Mar\u00eda Guti\u00e9rrez, a toxinologist at the University of Costa Rica in San Jos\u00e9, began parallel efforts to create a universal antivenom for sub-Saharan Africa using 'venomics' and 'antivenomics'. The aim is to identify destructive proteins in venoms using an array of techniques, ranging from genome sequencing to mass spectrometry, and then find the specific parts, known as epitopes, that provoke an immunological response and are neutralized by the antibodies in antivenom drugs. The ultimate goal is to use the epitopes to produce antibodies synthetically, using cells rather than animals, and develop antivenoms that are effective against a wide range of snake species in one part of the world. The scientists have made  slow but steady progress . Last year, Guti\u00e9rrez and his colleagues separated and identified the most toxic proteins from a family of venomous snakes known as elapids (Elapidae). By combining information about the abundance of each protein and how lethal it is to mice, the team created a toxicity score to indicate how important it was to neutralize a protein with antivenom, a first step towards making the treatment 3 . In March this year, a Brazilian team reported that they had gone further, designing short pieces of DNA that encode key toxic epitopes in the venom of the coral snake ( Micrurus corallinus ), a member of the elapid family 4 . Mice were injected with the DNA using a technique that enabled some to generate antibodies against coral-snake venom, and the group enhanced the mice's immune responses by injecting them with synthetic antibodies manufactured in bacterial cells. These and other advances led Harrison to estimate that the first trials of new antivenoms in humans could be just three or four years away. But with so few researchers working on the problem, a paucity of funding and the biological complexity of snake venoms, he and others admit that this is an optimistic prediction. Despite the growing literature on antivenomics, Alag\u00f3n and Chippaux aren't convinced that the approach will help. Alag\u00f3n estimates that newly developed antivenoms would need to be priced at tens of thousands of dollars per dose to be financially viable to produce, and that no biotech or pharma company would manufacture one without substantial government subsidies. Compare that, he says, to the rock-bottom price of many existing antivenoms. \u201cYou can't get cheaper than that,\u201d he says. \u201cWe can make an entire lot of antivenoms in one day using technology that's been available for 80 years.\u201d Finding someone to produce new medications might be a greater challenge than actually developing them, Williams acknowledges: governments or non-governmental organizations (NGOs) will almost certainly have to step in to help to defray the development costs. But he argues that now is the time to research alternative approaches. These could \u201crevolutionize the treatment of snakebite envenoming in the next 10\u201315 years\u201d, Williams says. \n               The room where it happened \n             All these tensions, brewing for nearly a decade, came to a head at the Geneva meeting in May. Around 75 scientists, public-health experts and health-assembly delegates crowded around three long tables in a third-floor conference room at the United Nations Headquarters. Spring rain pelted the tall windows. Lights were dimmed, and then the screams of a toddler filled the room. A short documentary co-produced by the Global Snakebite Initiative told the story of a girl bitten by a cobra whose parents carried her for days over rocky roads in Africa to find antivenom. They arrived in time \u2014 the girl survived \u2014 but she lost the use of her arm. Her sister had already died after a bite from the same snake. Convincing attendees of the scale of the problem was the meeting's primary goal; how to solve it came next. For 90 minutes, scientists and NGOs made short, impassioned speeches laying out the scope of the issue and the variety of problems that they faced. At the centre of each presentation was the same message: we need more antivenom. But the meeting was strained. Chippaux and representatives of the African Society of Venomology were disappointed and angry that so few Africans had been invited to speak, even though the continent is where antivenom shortages are most acute. \u201cOur voice, our issues, were completely overlooked,\u201d Chippaux says. Seated at the front of the room, group members whispered and gestured frantically to each other, and Chippaux barely managed to keep them from storming out. They argue that the current antivenom shortage stems from Africa's reliance on foreign companies and governments for its drugs, and that the only solution lies in building up infrastructure in Africa to produce its own high-quality antivenom. Alag\u00f3n views antivenomics as a dangerous diversion. \u201cIt's distracting many brilliant minds and resources from improving antivenoms using existing technology,\u201d he says. \u201cPerhaps by 2050 this will be the standard technique, but the problem is now.\u201d Williams and Guti\u00e9rrez take a middle ground. They feel that the problem requires attacks on all fronts. As well as innovation, Guti\u00e9rrez calls for existing manufacturers to step up the production of current drugs. There are signs of this happening already. Latin America has a long history of producing antivenoms both for its own needs and for those of countries around the world, and even before Sanofi Pasteur announced that it would cease production of Fav-Afrique, Costa Rica, Brazil and Mexico were testing antivenoms for different parts of Africa. One product, EchiTAb-Plus-ICB, is produced by Costa Rica and effective against a range of African viper species; it completed clinical trials in 2014 and is now available for use. Several other antivenoms are expected to be ready in the next two years. The drugs should be affordable: government labs in Costa Rica have already indicated that they will not seek to make money from the antivenoms, just recoup their expenditures. But beyond that, the way forward remains murky. Williams knows that the World Heath Assembly meeting was just a start. Inevitably, more meetings will be needed to produce a concrete action plan. But the discussion still gave him and some others a renewed sense of hope that the international community is beginning to take snakebite seriously \u2014 momentum they hope to build on by banging away at the topic at conferences and in the media. Boyer says that whatever solution the snakebite field decides on, the most important thing is to \u201cbreak the cycle of antivenom failure in Africa\u201d. Doing that requires building trust from governments, health-care workers and the public that the drugs are safe and effective, that clinics will have antivenom on hand, and that people will be able to afford treatment. \u201cWithout that, you've got nothing,\u201d Boyer says. Educating local clinics on how to care for snakebite victims and administer treatments in a timely manner would also go a long way towards preventing deaths. Speaking of the devastation he saw in Benue, Nasidi says that something as simple as providing boots for poor farmers would have helped to prevent much of the suffering and death that he witnessed. It's perhaps the ultimate in low-tech methods in snakebite protection: shielding vulnerable human skin.\u201d \n                 Tweet \n                 Facebook \n                 LinkedIn \n                 Weibo \n                 Wechat \n               \n                     Synthetic biology tackles global antivenom shortage 2016-Apr-15 \n                   \n                     Africa braced for snakebite crisis 2015-Sep-16 \n                   \n                     Four-legged fossil snake is a world first 2015-Jul-23 \n                   \n                     Projects set to tackle neglected diseases 2014-Jan-07 \n                   \n                     World Health Organization snakebites \n                   \n                     World Health Organization snake antivenoms \n                   Reprints and Permissions"},
{"file_id": "537022a", "url": "https://www.nature.com/articles/537022a", "year": 2016, "authors": [{"name": "Andy Extance"}], "parsed_as_year": "2006_or_before", "body": "Modern archiving technology cannot keep up with the growing tsunami of bits. But nature may hold an answer to that problem already. For Nick Goldman, the idea of encoding data in DNA started out as a joke. It was Wednesday 16 February 2011, and Goldman was at a hotel in Hamburg, Germany, talking with some of his fellow bioinformaticists about how they could afford to store the  reams of genome sequences and other data  the world was throwing at them. He remembers the scientists getting so frustrated by the expense and limitations of conventional computing technology that they started kidding about sci-fi alternatives. \u201cWe thought, 'What's to stop us using DNA to store information?'\u201d Then the laughter stopped. \u201cIt was a lightbulb moment,\u201d says Goldman, a group leader at the European Bioinformatics Institute (EBI) in Hinxton, UK. True, DNA storage would be pathetically slow compared with the microsecond timescales for reading or writing bits in a silicon memory chip. It would take hours to encode data by synthesizing DNA strings with a specific pattern of bases, and still more hours to recover that information using a sequencing machine. But with DNA, a whole human genome fits into a cell that is invisible to the naked eye. For sheer density of information storage, DNA could be orders of magnitude beyond silicon \u2014 perfect for long-term archiving. \u201cWe sat down in the bar with napkins and biros,\u201d says Goldman, and started scribbling ideas: \u201cWhat would you have to do to make that work?\u201d The researchers' biggest worry was that DNA synthesis and sequencing made mistakes as often as 1 in every 100 nucleotides. This would render large-scale data storage hopelessly unreliable \u2014 unless they could find a workable error-correction scheme. Could they encode bits into base pairs in a way that would allow them to detect and undo the mistakes? \u201cWithin the course of an evening,\u201d says Goldman, \u201cwe knew that you could.\u201d He and his EBI colleague Ewan Birney took the idea back to their labs, and two years later announced that they had  successfully used DNA to encode five files , including Shakespeare's sonnets and a snippet of Martin Luther King's 'I have a dream' speech 1 . By then, biologist George Church and his team at Harvard University in Cambridge, Massachusetts, had unveiled  an independent demonstration  of DNA encoding 2 . But at 739 kilobytes (kB), the EBI files comprised the largest DNA archive ever produced \u2014 until July 2016, when researchers from Microsoft and the University of Washington claimed a leap to 200 megabytes (MB). The latest experiment signals that interest in using DNA as a storage medium is surging far beyond genomics: the whole world is facing a data crunch. Counting everything from astronomical images and journal articles to YouTube videos, the global digital archive will hit an estimated 44 trillion gigabytes (GB) by 2020, a tenfold increase over 2013. By 2040, if everything were stored for instant access in, say, the flash memory chips used in memory sticks, the archive would consume 10\u2013100 times the expected supply of microchip-grade silicon 3 . That is one reason why permanent archives of rarely accessed data currently rely on old-fashioned magnetic tapes. This medium packs in information much more densely than silicon can, but is much slower to read. Yet even that approach is becoming unsustainable, says David Markowitz, a computational neuroscientist at the US Intelligence Advanced Research Projects Activity (IARPA) in Washington DC. It is possible to imagine a data centre holding an exabyte (one billion gigabytes) on tape drives, he says. But such a centre would require US$1 billion over 10 years to build and maintain, as well as hundreds of megawatts of power. \u201cMolecular data storage has the potential to reduce all of those requirements by up to three orders of magnitude,\u201d says Markowitz. If information could be packaged as densely as it is in the genes of the bacterium  Escherichia coli , the world's storage needs could be met by about a kilogram of DNA (see 'Storage limits'). Achieving that potential won't be easy. Before DNA can become a viable competitor to conventional storage technologies, researchers will have to surmount a host of challenges, from reliably encoding information in DNA and retrieving only the information a user needs, to making nucleotide strings cheaply and quickly enough. But efforts to meet those challenges are picking up. The Semiconductor Research Corporation (SRC), a foundation in Durham, North Carolina, that is supported by a consortium of chip-making firms, is backing DNA storage work. Goldman and Birney have UK government funding to experiment with next-generation approaches to DNA storage and are planning to set up a company to build on their research. And in April, IARPA and the SRC hosted a workshop for academics and industry researchers, including from companies such as IBM, to direct research in the field. \u201cFor ten years we've been looking beyond silicon\u201d for data archiving, says SRC director and chief scientist Victor Zhirnov. \u201cIt is very difficult to replace,\u201d he says. But DNA, one of the strongest candidates yet, \u201clooks like it may happen.\u201d \n               Long-term memory \n             The first person to map the ones and zeroes of digital data onto the four base pairs of DNA was artist Joe Davis, in a 1988 collaboration with researchers from Harvard. The DNA sequence, which they inserted into  E. coli , encoded just 35 bits. When organized into a 5 \u00d7 7 matrix, with ones corresponding to dark pixels and zeroes corresponding to light pixels, they formed a picture of an ancient Germanic rune representing life and the female Earth. Today, Davis is affiliated with Church's lab, which began to explore DNA data storage in 2011. The Harvard team hoped the application might help to reduce the high cost of synthesizing DNA, much as genomics had reduced the cost of sequencing. Church carried out the proof-of-concept experiments in November 2011 along with Sri Kosuri, now at the University of California, Los Angeles, and genomics expert Yuan Gao at Johns Hopkins University in Baltimore, Maryland. The team used many short DNA strings to encode a 659-kB version of a book Church had co-authored. Part of each string was an address that specified how the pieces should be ordered after sequencing, with the remainder containing the data. A binary zero could be encoded by the bases adenine or cytosine, and a binary one could be represented by guanine or thymine. That flexibility helped the group to design sequences that avoided reading problems, which can occur with regions containing lots of guanine and cytosine, repeated sections, or stretches that bind to one another and make the strings fold up. They didn't have error correction in the strict sense, instead relying on the redundancy provided by having many copies of each individual string. Consequently, after sequencing the strings, Kosuri, Church and Gao found 22 errors \u2014 far too many for reliable data storage. At the EBI, meanwhile, Goldman, Birney and their colleagues were also using many strings of DNA to encode their 739-kB data store, which included an image, ASCII text, audio files and a PDF version of Watson and Crick's iconic paper on DNA's double-helix structure. To avoid repeating bases and other sources of error, the EBI-led team used a more complex scheme (see 'Making memories'). One aspect involved encoding the data not as binary ones and zeroes, but in base three \u2014 the equivalent of zero, one and two. They then continuously rotated which DNA base represented each number, so as to avoid sequences that might cause problems during reading. By using overlapping, 100-base-long strings that progressively shifted by 25 bases, the EBI scientists also ensured that there would be four versions of each 25-base segment for error-checking and comparison against each other. They still lost 2 of the 25-base sequences \u2014 ironically, part of the Watson and Crick file. Nevertheless, these results convinced Goldman that DNA had potential as a cheap, long-term data repository that would require little energy to store. As a measure of just how long-term, he points to the 2013 announcement of a horse genome decoded from a bone trapped in permafrost for 700,000 years 4 . \u201cIn data centres, no one trusts a hard disk after three years,\u201d he says. \u201cNo one trusts a tape after at most ten years. Where you want a copy safe for more than that, once we can get those written on DNA, you can stick it in a cave and forget about it until you want to read it.\u201d \n               A burgeoning field \n             That possibility has captured the imaginations of computer scientists Luis Ceze, from the University of Washington, and Karin Strauss, from Microsoft Research in Redmond, Washington, ever since they heard Goldman discuss the EBI work when they visited the United Kingdom in 2013. \u201cDNA's density, stability and maturity have made us excited about it,\u201d says Strauss. And on their return to Washington state, says Strauss, she and Ceze started investigations with their University of Washington collaborator Georg Seelig. One of their chief concerns has been another major drawback that goes well beyond DNA's vulnerability to errors. Using standard sequencing methods, there was no way to retrieve any one piece of data without retrieving all the data: every DNA string had to be read. That would be vastly more cumbersome than conventional computer memory, which allows for random access: the ability to read just the data that a user needs. The team outlined its solution in early April at a conference in Atlanta, Georgia. The researchers start by withdrawing tiny samples from their DNA archive. They then use the polymerase chain reaction (PCR) to pinpoint and make more copies of the strings encoding the data they want to extract 5 . The proliferation of copies makes the sequencing faster, cheaper and more accurate than previous approaches. The team has also devised an alternative error-correction scheme that the group says allows for data encoding twice as dense as the EBI's, but just as reliable. As a demonstration, the Microsoft\u2013University of Washington researchers stored 151 kB of images, some encoded using the EBI method and some using their new approach, in a single pool of strings. They extracted three \u2014 a cat, the Sydney opera house and a cartoon monkey \u2014 using the EBI-like method, getting one read error that they had to correct manually. They also read the Sydney Opera House image using their new method, without any mistakes. \n               Economics versus chemistry \n             At the University of Illinois at Urbana\u2013Champaign, computer scientist Olgica Milenkovic and her colleagues have developed a random-access approach that also enables them to rewrite the encoded data 6 . Their method stores data as long strings of DNA that have address sequences at both ends. The researchers then use these addresses to select, amplify and rewrite the strings using either PCR or the gene-editing technique CRISPR\u2013Cas9. The addresses have to avoid sequences that would hamper reading while also being different enough from each other to stop them being mixed up in the presence of errors. Doing this \u2014 and avoiding problems such as molecules folding up because their sequences contain stretches that recognize and bind to each other \u2014 took intense calculations. \u201cAt the beginning, we used computer search because it was really difficult to come up with something that had all these properties,\u201d Milenkovic says. Her team has now replaced this labour-intensive process with mathematical formulae that allow them to devise an encoding scheme much more quickly. Other challenges for DNA data storage are scale and speed of synthesizing the molecules, says Kosuri, who admits that he has not been very bullish about the idea for that reason. During the early experiments at Harvard, he recalls, \u201cwe had 700 kB. Even a 1,000-fold increase on that is 700 MB, which is a CD\u201d. Truly making a difference to the worldwide data archiving problem would mean storing information by the petabyte at least. \u201cIt's not impossible,\u201d says Kosuri, \u201cbut people have to realize the scale is on the order of million-fold improvements.\u201d That will not be easy, agrees Markowitz. \u201cThe dominant production method is an almost 30-year-old chemical process that takes upwards of 400 seconds to add each base,\u201d he says. If this were to remain the approach used, he adds, billions of different strings would have to be made in parallel for writing to be fast enough. The current maximum for simultaneous production is tens of thousands of strings. A closely related factor is the cost of synthesizing DNA. It accounted for 98% of the expense of the $12,660 EBI experiment. Sequencing accounted for only 2%, thanks to a two-millionfold cost reduction since the completion of the Human Genome Project in 2003. Despite this precedent, Kosuri isn't convinced that economics can drive the same kind of progress in DNA synthesis. \u201cYou can easily imagine markets to sequence 7 billion people, but there's no case for building 7 billion people's genomes,\u201d he says. He concedes that some improvement in costs might result from Human Genome Project-Write (HGP-write), a project proposed in June by Church and others. If funded, the programme would aim to synthesize an entire human genome: 23 chromosome pairs containing 3.2 billion nucleotides. But even if HGP-write succeeds, says Kosuri, a human genome contains just 0.75 GB of information and would be dwarfed by the challenge of synthesizing practical data stores. Zhirnov, however, is optimistic that the cost of synthesis can be orders of magnitude below today's levels. \u201cThere are no fundamental reasons why it's high,\u201d he says. In April, Microsoft Research made an early move that may help create the necessary demand, ordering 10 million strings from Twist Bioscience, a DNA synthesis start-up company in San Francisco, California. Strauss and her colleagues say they have been using the strings to push their random-access storage approach to 0.2 GB. The details remain unpublished, but the archive reportedly includes the Universal Declaration of Human Rights in more than 100 languages, the top 100 books of Project Guttenberg and a seed database. Although this is much less of a synthesis challenge than the HGP-write faces, Strauss stresses the significance of the 250-fold jump in storage capacity. \u201cIt was time to exercise our muscle handling larger volumes of DNA to push it to a larger scale and see where the process breaks,\u201d she says. \u201cIt actually breaks in multiple places \u2014 and we're learning a great deal out of it.\u201d Goldman is confident that this is just a taste of things to come. \u201cOur estimate is that we need 100,000-fold improvements to make the technology sing, and we think that's very credible,\u201d he says. \u201cWhile past performance is no guarantee, there are new reading technologies coming onstream every year or two. Six orders of magnitude is no big deal in genomics. You just wait a bit.\u201d \n                 Tweet \n                 Facebook \n                 LinkedIn \n                 Weibo \n                 Wechat \n               \n                     The tiniest Lego: a tale of nanoscale motors, rotors, switches and pumps 2015-Sep-02 \n                   \n                     Genome researchers raise alarm over big data 2015-Jul-07 \n                   \n                     Synthetic double-helix faithfully stores Shakespeare's sonnets 2013-Jan-23 \n                   \n                     A fresh chapter for organic data storage 2012-Aug-19 \n                   \n                     Rewritable memory encoded into DNA 2012-May-21 \n                   \n                     European Bioinformatics Institute \n                   Reprints and Permissions"},
{"file_id": "537152a", "url": "https://www.nature.com/articles/537152a", "year": 2016, "authors": [{"name": "Tom Clynes"}], "parsed_as_year": "2006_or_before", "body": "A long-running investigation of exceptional children reveals what it takes to produce the scientists who will lead the twenty-first century. On a summer day in 1968, professor Julian Stanley met a brilliant but bored 12-year-old named Joseph Bates. The Baltimore student was so far ahead of his classmates in mathematics that his parents had arranged for him to take a computer-science course at Johns Hopkins University, where Stanley taught. Even that wasn't enough. Having leapfrogged ahead of the adults in the class, the child kept himself busy by teaching the FORTRAN programming language to graduate students. Unsure of what to do with Bates, his computer instructor introduced him to Stanley, a researcher well known for his work in psychometrics \u2014 the study of cognitive performance. To discover more about the young prodigy's talent, Stanley gave Bates a battery of tests that included the SAT college-admissions exam, normally taken by university-bound 16- to 18-year-olds in the United States. Bates's score was well above the threshold for admission to Johns Hopkins, and prompted Stanley to search for a local high school that would let the child take advanced mathematics and science classes. When that plan failed, Stanley convinced a dean at Johns Hopkins to let Bates, then 13, enrol as an undergraduate. Stanley would affectionately refer to Bates as \u201cstudent zero\u201d of his  Study of Mathematically Precocious Youth (SMPY) , which would transform how gifted children are identified and supported by the US education system. As the longest-running current longitudinal survey of intellectually talented children, SMPY has for 45 years tracked the careers and accomplishments of some 5,000 individuals, many of whom have gone on to become high-achieving scientists. The study's ever-growing data set has generated more than 400 papers and several books, and provided key insights into how to spot and develop talent in science, technology, engineering, mathematics (STEM) and beyond. \u201cWhat Julian wanted to know was, how do you find the kids with the highest potential for excellence in what we now call STEM, and how do you boost the chance that they'll reach that potential,\u201d says Camilla Benbow, a prot\u00e9g\u00e9 of Stanley's who is now dean of education and human development at Vanderbilt University in Nashville, Tennessee. But Stanley wasn't interested in just studying bright children; he wanted to nurture their intellect and enhance the odds that they would change the world. His motto, he told his graduate students, was \u201cno more dry bones methodology\u201d. With the first SMPY recruits now at the peak of their careers 1 , what has become clear is how much the precociously gifted outweigh the rest of society in their influence. Many of the innovators who are advancing science, technology and culture are those whose unique cognitive abilities were identified and supported in their early years through enrichment programmes such as Johns Hopkins University's  Center for Talented Youth  \u2014 which Stanley began in the 1980s as an adjunct to SMPY. At the start, both the study and the centre were open to young adolescents who scored in the top 1% on university entrance exams.  Pioneering mathematicians Terence Tao  and Lenhard Ng were one-percenters, as were Facebook's Mark Zuckerberg, Google co-founder Sergey Brin and musician Stefani Germanotta (Lady Gaga), who all passed through the Hopkins centre. \u201cWhether we like it or not, these people really do control our society,\u201d says Jonathan Wai, a psychologist at the Duke University Talent Identification Program in Durham, North Carolina, which collaborates with the Hopkins centre. Wai combined data from 11 prospective and retrospective longitudinal studies 2 , including SMPY, to demonstrate the correlation between early cognitive ability and adult achievement. \u201cThe kids who test in the top 1% tend to become our eminent scientists and academics, our Fortune 500 CEOs and federal judges, senators and billionaires,\u201d he says.  Such results contradict long-established ideas suggesting that expert performance is built mainly through practice \u2014 that anyone can get to the top with enough focused effort of the right kind. SMPY, by contrast, suggests that early cognitive ability has more effect on achievement than either deliberate practice or environmental factors such as socio-economic status. The research emphasizes the importance of nurturing precocious children, at a time when the prevailing focus in the United States and other countries is on improving the performance of struggling students (see \u2018Nurturing a talented child\u2019). At the same time, the work to identify and support academically talented students has raised troubling questions about the risks of labelling children, and the shortfalls of talent searches and standardized tests as a means of identifying high-potential students, especially in poor and rural districts. \u201cWith so much emphasis on predicting who will rise to the top, we run the risk of selling short the many kids who are missed by these tests,\u201d says Dona Matthews, a developmental psychologist in Toronto, Canada, who co-founded the Center for Gifted Studies and Education at Hunter College in New York City. \u201cFor those children who are tested, it does them no favours to call them 'gifted' or 'ungifted'. Either way, it can really undermine a child's motivation to learn.\u201d \n               Start of a study \n             On a muggy August day, Benbow and her husband, psychologist David Lubinski, describe the origins of SMPY as they walk across the quadrangle at Vanderbilt University. Benbow was a graduate student at Johns Hopkins when she met Stanley in a class he taught in 1976. Benbow and Lubinski, who have co-directed the study since Stanley's retirement, brought it to Vanderbilt in 1998. \u201cIn a sense, that brought Julian's research full circle, since this is where he started his career as a professor,\u201d Benbow says as she nears the university's psychology laboratory, the first US building dedicated to the study of the field. Built in 1915, it houses a small collection of antique calculators \u2014 the tools of quantitative psychology in the early 1950s, when Stanley began his academic work in psychometrics and statistics. His interest in developing scientific talent had been piqued by one of the most famous longitudinal studies in psychology,  Lewis Terman's Genetic Studies of Genius 3 , 4 . Beginning in 1921, Terman selected teenage subjects on the basis of high IQ scores, then tracked and encouraged their careers. But to Terman's chagrin, his cohort produced only a few esteemed scientists. Among those rejected because their IQ of 129 was too low to make the cut was William Shockley, the Nobel-prizewinning co-inventor of the transistor. Physicist Luis Alvarez, another Nobel winner, was also rejected. Stanley suspected that Terman wouldn't have missed Shockley and Alvarez if he'd had a reliable way to test them specifically on quantitative reasoning ability. So Stanley decided to try the Scholastic Aptitude Test (now simply the SAT). Although the test is intended for older students, Stanley hypothesized that it would be well suited to measuring the analytical reasoning abilities of elite younger students.  \n               boxed-text \n             In March 1972, Stanley rounded up 450 bright 12- to 14-year-olds from the Baltimore area and gave them the mathematics portion of the SAT. It was the first standardized academic 'talent search'. (Later, researchers included the verbal portion and other assessments.) \u201cThe first big surprise was how many adolescents could figure out math problems that they hadn't encountered in their course work,\u201d says developmental psychologist Daniel Keating, then a PhD student at Johns Hopkins University. \u201cThe second surprise was how many of these young kids scored well above the admissions cut-off for many elite universities.\u201d Stanley hadn't envisioned SMPY as a multi-decade longitudinal study. But after the first follow-up survey, five years later, Benbow proposed extending the study to track subjects through their lives, adding cohorts and including assessments of interests, preferences, and occupational and other life accomplishments. The study's first four cohorts range from the top 3% to the top 0.01% in their SAT scores. The SMPY team added a fifth cohort of the leading mathematics and science graduate students in 1992 to test the generalizability of the talent-search model for identifying scientific potential. \u201cI don't know of any other study in the world that has given us such a comprehensive look at exactly how and why STEM talent develops,\u201d says Christoph Perleth, a psychologist at the University of Rostock in Germany who studies intelligence and talent development. \n               Spatial skills \n             As the data flowed in, it quickly became apparent that a one-size-fits-all approach to gifted education, and education in general, was inadequate. \u201cSMPY gave us the first large-sample basis for the field to move away from general intelligence toward assessments of specific cognitive abilities, interests and other factors,\u201d says Rena Subotnik, who directs the Center for Gifted Education Policy at the American Psychological Association in Washington DC. In 1976, Stanley started to test his second cohort (a sample of 563 13-year-olds who scored in the top 0.5% on the SAT) on spatial ability \u2014 the capacity to understand and remember spatial relationships between objects 5 . Tests for spatial ability might include matching objects that are seen from different perspectives, determining which cross-section will result when an object is cut in certain ways, or estimating water levels on tilted bottles of various shapes. Stanley was curious about whether spatial ability might better predict educational and occupational outcomes than could measures of quantitative and verbal reasoning on their own. Follow-up surveys \u2014 at ages 18, 23, 33 and 48 \u2014 backed up his hunch. A 2013 analysis 5  found a correlation between the number of patents and peer-refereed publications that people had produced and their earlier scores on SATs and spatial-ability tests. The SAT tests jointly accounted for about 11% of the variance; spatial ability accounted for an additional 7.6%. The findings, which dovetail with those of other recent studies, suggest that spatial ability plays a major part in creativity and technical innovation. \u201cI think it may be the largest known untapped source of human potential,\u201d says Lubinski, who adds that students who are only marginally impressive in mathematics or verbal ability but high in spatial ability often make exceptional engineers, architects and surgeons. \u201cAnd yet, no admissions directors I know of are looking at this, and it's generally overlooked in school-based assessments.\u201d Although studies such as SMPY have given educators the ability to identify and support gifted youngsters, worldwide interest in this population is uneven. In the Middle East and east Asia, high-performing STEM students have received significant attention over the past decade. South Korea, Hong Kong and Singapore screen children for giftedness and steer high performers into innovative programmes. In 2010, China launched a ten-year National Talent Development Plan to support and guide top students into science, technology and other high-demand fields. In Europe, support for research and educational programmes for gifted children has ebbed, as the focus has moved more towards inclusion. England decided in 2010 to scrap the National Academy for Gifted and Talented Youth, and redirected funds towards an effort to get more poor students into leading universities. \n               On the fast track \n             When Stanley began his work, the choices for bright children in the United States were limited, so he sought out environments in which early talent could blossom. \u201cIt was clear to Julian that it's not enough to identify potential; it has to be developed in appropriate ways if you're going to keep that flame well lit,\u201d says Linda Brody, who studied with Stanley and now runs a programme at Johns Hopkins focused on counselling profoundly gifted children. At first, the efforts were on a case-by-case basis. Parents of other bright children began to approach Stanley after hearing about his work with Bates, who thrived after entering university. By 17, he had earned bachelor's and master's degrees in computer science and was pursuing a doctorate at Cornell University in Ithaca, New York. Later, as a professor at Carnegie Mellon University in Pittsburgh, Pennsylvania, he would become a pioneer in artificial intelligence. \u201cI was shy and the social pressures of high school wouldn't have made it a good fit for me,\u201d says Bates, now 60. \u201cBut at college, with the other science and math nerds, I fit right in, even though I was much younger. I could grow up on the social side at my own rate and also on the intellectual side, because the faster pace kept me interested in the content.\u201d The SMPY data supported the idea of accelerating fast learners by allowing them to skip school grades. In a comparison of children who bypassed a grade with a control group of similarly smart children who didn't, the grade-skippers were 60% more likely to earn doctorates or patents and more than twice as likely to get a PhD in a STEM field 6 . Acceleration is common in SMPY's elite 1-in-10,000 cohort, whose intellectual diversity and rapid pace of learning make them among the most challenging to educate. Advancing these students costs little or nothing, and in some cases may save schools money, says Lubinski. \u201cThese kids often don't need anything innovative or novel,\u201d he says, \u201cthey just need earlier access to what's already available to older kids.\u201d Many educators and parents continue to believe that acceleration is bad for children \u2014 that it will hurt them socially, push them out of childhood or create knowledge gaps. But education researchers generally agree that acceleration benefits the vast majority of gifted children socially and emotionally, as well as academically and professionally 7 . Skipping grades is not the only option. SMPY researchers say that even modest interventions \u2014 for example, access to challenging material such as college-level Advanced Placement courses \u2014 have a demonstrable effect. Among students with high ability, those who were given a richer density of advanced precollegiate educational opportunities in STEM went on to publish more academic papers, earn more patents and pursue higher-level careers than their equally smart peers who didn't have these opportunities 8 . Despite SMPY's many insights, researchers still have an incomplete picture of giftedness and achievement. \u201cWe don't know why, even at the high end, some people will do well and others won't,\u201d says Douglas Detterman, a psychologist who studies cognitive ability at Case Western Reserve University in Cleveland, Ohio. \u201cIntelligence won't account for all the differences between people; motivation, personality factors, how hard you work and other things are important.\u201d Some insights have come from German studies 9 , 10 , 11  that have a methodology similar to SMPY's. The Munich Longitudinal Study of Giftedness, which started tracking 26,000 gifted students in the mid-1980s, found that cognitive factors were the most predictive, but that some personal traits \u2014 such as motivation, curiosity and ability to cope with stress \u2014 had a limited influence on performance. Environmental factors, such as family, school and peers, also had an impact. The data from such intellectual-talent searches also contribute to knowledge of how people develop expertise in subjects. Some researchers and writers, notably psychologist Anders Ericsson at Florida State University in Tallahassee and author Malcolm Gladwell, have popularized the idea of an ability threshold. This holds that for individuals beyond a certain IQ barrier (120 is often cited), concentrated practice time is much more important than additional intellectual abilities in acquiring expertise. But data from SMPY and the Duke talent programme dispute that hypothesis (see 'Top of the charts'). A study published this year 12  compared the outcomes of students in the top 1% of childhood intellectual ability with those in the top 0.01%. Whereas the first group gain advanced degrees at about 25 times the rate of the general population, the more elite students earn PhDs at about 50 times the base rate. But some of the work is controversial. In North America and Europe, some child-development experts lament that much of the research on talent development is driven by the urge to predict who will rise to the top, and educators have expressed considerable unease about the concept of identifying and labelling a group of pupils as gifted or talented 13 . \u201cA high test score tells you only that a person has high ability and is a good match for that particular test at that point in time,\u201d says Matthews. \u201cA low test score tells you practically nothing,\u201d she says, because many factors can depress students' performance, including their cultural backgrounds and how comfortable they are with taking high-stakes tests. Matthews contends that when children who are near the high and low extremes of early achievement feel assessed in terms of future success, it can damage their motivation to learn and can contribute to what Stanford University psychologist Carol Dweck calls a fixed mindset. It's far better, Dweck says, to  encourage a growth mindset , in which children believe that brains and talent are merely a starting point, and that abilities can be developed through hard work and continued intellectual risk-taking. \u201cStudents focus on improvement instead of worrying about how smart they are and hungering for approval,\u201d says Dweck. \u201cThey work hard to learn more and get smarter.\u201d Research by Dweck and her colleagues shows that students who learn with this mindset show greater motivation at school, get better marks and have higher test scores 14 . Benbow agrees that standardized tests should not be used to limit students' options, but rather to develop learning and teaching strategies appropriate to children's abilities, which allow students at every level to reach their potential. Next year, Benbow and Lubinski plan to launch a mid-life survey of the profoundly gifted cohort (the 1 in 10,000), with an emphasis on career achievements and life satisfaction, and to re-survey their 1992 sample of graduate students at leading US universities. The forthcoming studies may further erode the enduring misperception that gifted children are bright enough to succeed on their own, without much help. \u201cThe education community is still resistant to this message,\u201d says David Geary, a cognitive developmental psychologist at the University of Missouri in Columbia, who specializes in mathematical learning. \u201cThere's a general belief that kids who have advantages, cognitive or otherwise, shouldn't be given extra encouragement; that we should focus more on lower-performing kids.\u201d Although gifted-education specialists herald the expansion of talent-development options in the United States, the benefits have mostly been limited so far to students who are at the top of both the talent and socio-economic curves. \u201cWe know how to identify these kids, and we know how to help them,\u201d says Lubinski. \u201cAnd yet we're missing a lot of the smartest kids in the country.\u201d As Lubinski and Benbow walk through the quadrangle, the clock strikes noon, releasing packs of enthusiastic adolescents racing towards the dining hall. Many are participants in the Vanderbilt Programs for Talented Youth, summer enrichment courses in which gifted students spend three weeks gorging themselves on a year's worth of mathematics, science or literature. Others are participants in Vanderbilt's sports camps. \u201cThey're just developing different talents,\u201d says Lubinski, a former high-school and college wrestler. \u201cBut our society has been much more encouraging of athletic talents than we are of intellectual talents.\u201d And yet these gifted students, the 'mathletes' of the world, can shape the future. \u201cWhen you look at the issues facing society now \u2014 whether it's health care, climate change, terrorism, energy \u2014 these are the kids who have the most potential to solve these problems,\u201d says Lubinski. \u201cThese are the kids we'd do well to bet on.\u201d \n                 Tweet \n                 Facebook \n                 LinkedIn \n                 Weibo \n                 Wechat \n               \n                     Root of maths genius sought 2013-Oct-29 \n                   \n                     Root of maths genius sought 2013-Oct-29 \n                   \n                     Ethics: Taboo genetics 2013-Oct-02 \n                   \n                     Dangerous work 2013-Oct-02 \n                   \n                     Chinese project probes the genetics of genius 2013-May-14 \n                   \n                     Darwin 200: Should scientists study race and IQ? YES: The scientific truth must be pursued 2009-Feb-11 \n                   \n                     Study of Mathematically Precocious Youth \n                   \n                     Johns Hopkins Center for Talented Youth \n                   Reprints and Permissions"},
{"file_id": "534604a", "url": "https://www.nature.com/articles/534604a", "year": 2016, "authors": [{"name": "Ewen Callaway"}], "parsed_as_year": "2006_or_before", "body": "From incubation in a bra to an afterlife under glass, how a cloned sheep attained celebrity status. \n               Dolly, the first mammal cloned from an adult cell, was born 5 July 1996. But she was created five months earlier, in a small room at the Roslin Institute, outside Edinburgh, UK. \n             Karen Walker, embryologist, PPL Therapeutics:  On the day we made Dolly, we had such a rubbish day. Bill Ritchie, embryologist, Roslin Institute:  It was 8 February 1996. I looked it up. We do know it was a rubbish day: we had various problems with infections and things. Walker:  It's a shame the building has been demolished, otherwise you could see the room in which Dolly was made. I use the word 'room' loosely, because it really was just a big cupboard, which, when Bill and I were in there, you could just get two chairs and the incubator in. Ritchie:  It literally was the cupboard. It was the storage cupboard at the end of the lab. When we got camera crews in later, they couldn't believe it, there was no room to shoot. Reporter Ewen Callaway looks back at Dolly the sheep\u2019s legacy, 20 years after her birth \n               Walker and Ritchie were part of a project at the Roslin Institute and spin-off PPL Therapeutics, aiming to make precise genetic changes to farm animals. The scientific team, led by Roslin embryologist Ian Wilmut, reasoned that the best way to make these changes would be to tweak the genome of a cell in culture and then transfer the nucleus to a new cell. \n             Ritchie:  The simple way of describing nuclear transfer is that you take an oocyte, an unfertilized egg, and you remove the chromosomes. You then take a complete cell which contains both male and female chromosomes \u2014 all of our cells do, apart from the gonads. You take that cell and fuse it to the enucleated egg, activate it \u2014 which starts it growing \u2014 and transfer it to a surrogate mother. Hopefully, with your fingers crossed, you will get a cloned offspring, a copy of the animal you've taken that cell from. Walker:  Tedious is absolutely the word. You're sitting, looking down a microscope and you've got both hands on the micromanipulators. It's kind of like the joysticks kids use nowadays on games. If your elbow slipped, you could wipe the whole dish out. \n               A year earlier, the team had produced twin sheep, named Megan and Morag, by cloning cultured embryonic cells in an effort spearheaded by Roslin developmental biologist Keith Campbell. But on this day in February 1996, problems with the fetal cell lines they had planned to use meant that they would need another nuclear donor. \n             Walker:  My memory is of flapping like a chicken, thinking, 'What are we going to put in?' because the cells we were going to use aren't there. The last thing you want to do is waste those oocytes you've got. We wanted to try something, at least. Angela Scott, cell-culture technician, PPL:  I received word from Karen to say that the cells they were expecting had been contaminated. They asked me if I had any cells that they could use. The cells I had were ovine mammary epithelial cells: we were looking to increase expression of proteins in milk. These were adult cells. Alan Colman, research director, PPL:  I had come from a background of nuclear transfer with John Gurdon [a developmental biologist at the University of Cambridge, UK]. He'd never been able to get an adult frog by using nuclear transfer from an adult cell donor. He'd been able to get tadpoles using adult cells, but he'd never been able to get an adult frog. I didn't think it would work with adult cells at all. But we had no other cell line to go with, so we all agreed that we'd use these mammary-gland cells and just see what happened, gain some experience. These were from a 6-year-old sheep \u2014 middle-aged for a sheep. Ian Wilmut, embryologist, Roslin:  This is something that is got wrong to this day. Dolly is described as the first mammal cloned from an adult cell. She's actually the first adult clone, period. She's often undersold. \n               Although cloned and transgenic cows would be more valuable for industry, the Roslin team worked with sheep for practical reasons. \n             Wilmut:  Cattle are incredibly expensive and have a long generation interval. Sheep are much less expensive and much easier to work with. And we knew the reproductive biology. It was very likely that if we could make something work in sheep, it would work in cows. Sheep are small, cheap cows. John Bracken, farm research assistant, Roslin:  There would be 40\u201360 animals going through surgery [to retrieve oocytes or implant embryos in surrogates] each week during the breeding season. It's a lot of different sheep in the system, and that had to be very accurately monitored so the animals were at the right place at the right time. Walker:  Bill used to keep the embryos and oocytes \u2014 when he was bringing them back up from the farm \u2014 in his top shirt pocket. I didn't have a top shirt pocket, so I used to tuck them inside my bra. It was a way to keep them warm and fetch them back into the lab and get them into a proper controlled environment. I don't think inside my bra was terribly controlled, but neither was Bill's top shirt pocket. Ritchie:  On the day we made Dolly, I would have done the enucleation, and she would have done the fusion. That was our normal way of doing things. Walker:  I did the fusion on the day we made Dolly. Bill and I joke, that he's the mum and I'm the dad because, essentially, I was the mimic to what the sperm would do. \n               They transferred 277 nuclei from the mammary cell line \u2014 from a white-faced breed known as a Finn Dorset \u2014 into eggs from the hardy Scottish blackface breed. Just 29 of the resulting embryos were implanted into surrogate ewes. Expectations were low: it seemed almost impossible that an adult cell nucleus could be reprogrammed to give rise to a live animal. Most cloned embryos aborted, many even before a pregnancy could be determined with ultrasound. \n             Wilmut:  The sheep breeding season begins in October and ends in February, March-ish. By Christmas, we had established pregnancies after transfer from fetal cells, so that was going well. If we hadn't done that, we probably wouldn't have gambled on working with what became Dolly, the mammary cells. Angelika Schnieke, molecular biologist, PPL:  I remember meeting Ian Wilmut in the canteen, and he was very sceptical. He said: \u201cI would be surprised if it works, but PPL is paying for the experiments, so we're doing them.\u201d Bracken:  We scanned all the recipients that had embryos transferred, and we knew they were important sheep. Every day that the scientists knew we were scanning, they would be very keen to know if there were any pregnancies. Walker:  I didn't go down to watch all the scans. But with Dolly \u2014 because we knew that those were cells Bill and I had put in \u2014 I had gone down on that particular day with John. Bracken:  I was just really pleased that it was a pregnancy. I didn't realize the real importance of it because we weren't really told. We just knew it was an important pregnancy. It didn't carry the same weight. We weren't thinking, 'Wow! If this progresses to a live lamb, this is going to be a world beater, or it's going to turn scientific understanding on its head.' Walker:  I'd taken a blank video up with me, so that I could show my colleagues. That video is sitting up in my loft, and to my shame, I have never yet transferred it onto DVD. I should. Schnieke:  I remember the day when we had the first scan. We always asked. And then we saw the picture and the scans. Then you just have to hope that it lasts and goes all the way through. Wilmut:  My memory is they were looking around day 30 or 35, so there's another 120 days [until the birth], where you keep on sighing with relief and hoping. \n               Just a few of the team members got to witness her birth. \n             Bracken:  It happened about 4:30 in the afternoon. As soon as she went into labour, we called the Dick Vet [the Royal School of Veterinary Studies in Edinburgh] to get one of their vets to come out. Even though [farm research assistant] Douglas McGavin and myself probably had 50 years of experience between us, it just would have been unheard of if we'd decided we'd assist the birth and something had gone wrong. Ritchie:  We knew Dolly was about to be born, and I think she was showing signs of getting near lambing, and lo and behold I went through and there were bits of Dolly being born. There was a vet there, so she made sure the animal was okay and pulled the lamb out. Bracken:  It was absolutely normal. No complications whatsoever. She was a very viable lamb. She got on her feet very quickly, probably within the first half hour, which is a really good indication that things are normal. Ritchie:  I think I was jumping up and down when I saw that white face. Scott:  Karen was away at a wedding at the time. Walker:  I had given her the fax number of the hotel. I wish I had kept that fax. It said: \u201cShe has a white face and furry legs.\u201d Scott:  I don't know what they must have thought at the hotel: \u201cWow, that's a really unusual baby.\u201d Wilmut:  I was in the allotment. I had a phone call to say we had a live lamb. I issued an instruction that nobody should be there who didn't have to be there. Lots were curious. I obeyed my own rule because I'd got nothing to contribute. Bracken:  I'm standing next to Douglas McGavin watching the vet assist this birth, and I made an off-the-cuff remark to Douglas. I said, \u201cYou know what we're going to have to call this lamb? We're going to have to call it Dolly\u201d, after Dolly Parton, because the cells are derived from mammary tissue. Wilmut:  Being somewhat puritanical, I might have been a bit worried. With hindsight, without a doubt it was a great name. Bracken:  This is hearsay. I never got told this directly. But I heard they had contacted Dolly Parton and said: \u201cWe've got this cloned sheep that's named after you.\u201d Wilmut:  I don't know how the message came through, but we were told her agent had said: \u201cThere was no such thing as baaad publicity.\u201d I don't know if that's true. Over the next few months, Wilmut's team confirmed that Dolly was a clone of the mammary cell line, and wrote up the results. Her birth was to be kept top secret, until the Nature paper describing the experiment could be published in February 1997 ( I. Wilmut  et al. Nature  385,    810\u2013813; 1997 ). Harry Griffin, scientific director, Roslin:  Two or three months before the publication of the paper, I got to know about it. In terms of preparation, PPL were involved. They saw it as an opportunity to get publicity for themselves. We worked with their PR company, De Facto. We did quite a bit of preparation. Wilmut:  Ron James, who was the chief executive of PPL therapeutics, and I were cited as the primary spokesmen and given a bit of training by ex-BBC people, who first of all came up and fairly aggressively stuck microphones up our noses and asked aggressive questions, and subsequently did it very gently. We weren't approached in anywhere near the aggressive way they tried first, which was quite shocking. I'm sure it was worth having. Griffin:  We had everything organized. The calls would be directed to De Facto and they would try and organize some coherence in our response in terms of who got priority and who didn't. All this would culminate, we hoped, on the Thursday that the paper came out. What was that, 27 February? Clearly, it didn't. Wilmut:  Robin McKie at  The Observer  leaked it. He will deny the charge. Robin McKie, science and technology editor,    The Observer   , London:  I didn't see that stuff in  Nature . I don't blame him for being angry, but I went to great pains to avoid the things that would get me to be accused of that. I had helped a couple of guys who were making a TV programme about genetics, and they said, \u201cOh, by the way, they've cloned a sheep in Edinburgh.\u201d I didn't believe them, but I phoned a few people in the field, and one of them in America confirmed it. But I was very, very worried. I was saying something quite sensational, with absolutely no paper proof of anything that had gone on. I told my deputy editor everything I knew, and he made me write it. Then the shit hit the fan. Griffin:  Ian gave me a call and said he'd just been called up and told that  The Observer  was going to run the story on the Sunday prior to publication in  Nature . Ian and I went into the institute at about 9 a.m. on the Sunday, not knowing whether or not people could get through. The phone rang continuously. We had a bizarre circumstance where a phone started ringing in a cleaning cupboard. When I answered it, it was, I think, the  Daily Mirror , who had somehow got this particular connection. About half past nine at night, we went home. Jim McWhir, stem-cell scientist, Roslin:  I remember coming in on the day after the embargo broke and there were several satellite vans in the carpark. Wilmut:  There were television trucks everywhere. I went and spoke on  Good Morning America . Griffin:  CBS, NBC, ABC, BBC, all there wanting interviews with Ian, wanting to see the sheep. It was chaos. I don't think you can ever appreciate the intensity of the media in full flight unless you've experienced it yourself. McWhir:  It was just pandemonium. Going down to the large-animal unit, it was just a forest of flash bulbs and reporters. It was quite amazing. I just turned around and went back to work. Griffin:  My secretary would put the phone down, and it was ringing immediately. One of the names I heard being mentioned was Harold Shapiro [then chair of the US National Bioethics Commission]. She said, \u201cIan Wilmut can't talk to you now, can you call back later?\u201d Bill Clinton had asked him to report back within 90 days on the ethical implications of cloning. I overheard his name, and said, \u201cNo, we definitely want to talk to him.\u201d Colman:  When you're embedded in a project, you have what you consider to be good scientific reasons for doing it. Everything we did was covered by an ethics committee. We had been through a lot of concerns about animal health. Our concern was more about that kind of reaction. We weren't doing it as a prelude to cloning humans. Griffin:  People in the media pressed this point repeatedly. We were accused of keeping Dolly's birth secret because we were contemplating cloning a human. We had our position clear on that: it was unethical and unsafe. Wilmut:  It goes with the job. You just have to explain this is not the case. Schnieke:  In Europe, it was immediately seen as a negative. \u201cWhat have they done now and what could they do next?\u201d We had police at the institute who explained what you do if there's a bomb scare. Packages were being screened for explosives. Walker:  I do remember Ian Wilmut's personal assistant, Jackie, getting phone calls after it all hit the press. She had lots of phone calls, some of them were a bit crackpot, from people wanting their dogs cloned. The sadder ones were those people who had lost children or who had illnesses themselves, and this was going to be a breakthrough that could cure different diseases. Colman:  Dolly seemed to capture the imagination. It was a furry animal. Having a name that was identifiable helped enormously. Bracken:  If she'd been seen as being an animal that was locked away, that not many people saw, that could have perpetuated more bad publicity. But I think, because of the openness, that people were allowed to go and visit her and be shown around, this did help in the acceptance of the public. Griffin:  She performed well for camera, and everybody could see she was a perfectly normal animal. Because she was accessible and photogenic, she became the most famous sheep in the world. Any marketing manager would have killed for it. In some of the pictures it's as if she's interviewing the media. Walker:  I took a photographer down to see Dolly. This guy produced a kid's party crown, a little gold thing. I said: \u201cI don't think we should.\u201d We were all very keen not to allow Dolly to become humanized. She was a sheep and that was it. Bracken:  Away from the media and the cameras, we tried to treat her just like the other sheep, not as a sort of celebrity, which she obviously became. Walker:  The first time she was shorn, they took the wool \u2014 which I have some of, actually \u2014 to be knitted into a jumper for a cystic-fibrosis charity. Have you seen her in the museum? She's behind a glass case now because people kept pinching bits of wool from her. At least I got my wool while she was still alive. \n               Dolly lived for six and a half years \n                and gave birth to several lambs herself. But in 2003, she began to show signs of illness. \n             Bracken:  It was Valentine's Day. I think it was a Friday. We knew that there was the potential for this lung disease to have developed. Griffin:  She suffered from a disease called jaagsiekte. It's a disease of the lungs and one or two other sheep beforehand had gone down with it. Wilmut:  They thought she should be X-rayed over at the vet school. They were surprised at the size of the tumour in her lungs. We debated, under these circumstances, how hard we should struggle for her to recover. Wouldn't it be kinder to just let her go? So we euthanized her. You are responsible for the welfare of the animals on your project. \n               A decade later, another loss struck the scientific team with the death of Keith Campbell. \n             Colman:  Keith was the driving force. He was the person who did the important experimental work that sowed the seeds of the protocol we all used. Dolly would not have happened without Keith. Ritchie:  Keith was, I suppose, 'unusual' is probably the thing you would say about him. He was quite hippy. He drove a Volkswagen Beetle, smoked roll-ups, had long hair. Colman:  He didn't have a great relationship with Ian. They were very different personalities and often argued. Wilmut:  I don't remember rows. We would have had slightly different priorities sometimes. It's always very difficult to divide recognition up. What was obviously the cause of some annoyance and some criticism is that he didn't get the first authorship on the Dolly paper. He did get absolutely all the others. There was a time when he said the Megan and Morag paper was actually more important than Dolly. He definitely was frustrated that I got an FRS (Fellow of the Royal Society) and ultimately a knighthood. \n               After a domestic dispute, Campbell killed himself on 5 October 2012. \n             Colman:  Keith was a very good friend of mine and we used to go mountain biking in Scotland in the evenings after work. I spoke with him three days before he died. I was very shocked. Walker:  That hit me very hard, harder than I would have imagined. I hadn't seen him in many, many years. We were such a close, tight group at the time. We had to be. Colman:  I went to a meeting in Paris last January, where they had a posthumous award. They took a straw poll of how many people in the audience had been helped by what Keith had done, and a huge number of people put their hands up. \n               The techniques developed in the creation of Dolly were used to copy valuable livestock and make transgenic animals. But in biomedical labs, Dolly hinted at a future in which cells could be reprogrammed to an embryo-like state and used to treat human diseases. \n             Wilmut:  The birth of Dolly turned the rules of development upside down, and made a lot of biologists think differently. Jeanne Loring, stem-cell biologist, the Scripps Research Institute, La Jolla, California:  That was the onset of cattle cloning, which is actually quite popular now. There's a tremendous value in being able to improve cattle, and this gave people another tool. George Seidel, animal reproductive biologist, Colorado State University, Fort Collins:  There are cloned bulls producing semen that's being sold. There's an Angus bull called Final Answer, he's got half a million offspring or something like that. So his clone is called Final Answer II, and you can buy his semen at half the price. My wife and I have a cattle ranch, so we use Final Answer II. Hell, it's the same genetics. But from a theoretical standpoint, the transgenic stuff is really much more important than just making copies. To make our first transgenic cow, we created thousands of embryos. It was a huge effort. A tenth of the money, a tenth the animals is what transgenics plus cloning could do for you. Robert Lanza, chief scientific officer, Astellas Institute for Regenerative Medicine, Marlborough, Massachusetts:  I was excited. Now we could hopefully apply the same technique \u2014 not so much for animals and agriculture \u2014 but for treating a long list of human diseases. What Dolly showed was the enormous power of that technology and the magic of the egg. There were factors in the egg that could take adult cells backwards in time and restore them to an embryonic state. Shinya Yamanaka, stem-cell scientist, Kyoto University, Japan:  My initial response was \u201cWow! It's like science fiction.\u201d But it was not something I was planning to work on. Judging from the paper, the cloning process is very technically challenging. The next year, the first human embryonic-stem-cell paper came out. That's when I re-evaluated Dolly. I thought, at least in theory, we should be able to reprogram somatic cells back into the embryonic state so we can make ES-like stem cells directly from skin or blood cells. McWhir:  A result like Dolly stops people in their tracks, and they say: \u201cWell hang on. If I'd have said that is impossible, what else am I saying is impossible?\u201d Schnieke:  You have some experiments where it brings up your heartbeat. Dolly was one. Ritchie:  It's kind of like having children. I haven't got any myself. Maybe Dolly's that sort of child. Wilmut:  It would be wrong to say my name's known all the way around the world \u2014 but Dolly's is. THE CAST   Karen Walker:  embryologist, PPL Therapeutics, Roslin, UK; now director, KXRegulatory: Linlithgow, UK  Bill Ritchie:  embryologist, Roslin Institute; now at Roslin Embryology  Angela Scott:  cell-culture technician, PPL; now chief operating officer, TC BioPharm, Motherwell, UK  Alan Colman:  research director, PPL; now at Harvard University, Cambridge, Massachusetts  Ian Wilmut:  embryologist, Roslin; now University of Edinburgh, UK  John Bracken:  farm research assistant, Roslin; now retired  Angelika Schnieke:  molecular biologist, PPL; now Technical University of Munich, Germany  Harry Griffin:  scientific director, Roslin; now retired  Jim McWhir:  stem-cell scientist, Roslin; now retired \n                 Tweet \n                 Follow @NatureNews \n                 Follow @ewencallaway \n               \n                     How iPS cells changed the world 2016-Jun-15 \n                   \n                     Reproductive medicine: The power of three 2014-May-21 \n                   \n                     Cloning comeback 2014-Jan-14 \n                   \n                     Cloned human embryo makes working stem cells 2011-Oct-05 \n                   \n                     Obituary: Dolly the sheep 2003-Feb-18 \n                   \n                     Nature  special: Reprogramming \n                   Reprints and Permissions"},
{"file_id": "537298a", "url": "https://www.nature.com/articles/537298a", "year": 2016, "authors": [{"name": "Rachel Cernansky"}], "parsed_as_year": "2006_or_before", "body": "Diana Wall has built a career on overturning assumptions about underground ecosystems. Now she is seeking to protect this endangered world. Early on a cold spring morning, Diana Wall is trying out a tool normally used to make holes on golf courses \u2014 and she can't contain her excitement. Her team has always used more laborious methods to take samples of soil and its resident organisms. \u201cOh, that's a beautiful core,\u201d she says as one student bags a sample filled with tiny roundworms. \u201cHello, nematodes!\u201d Wall, a soil ecologist and environmental scientist at Colorado State University in Fort Collins, has come to this site about an hour east of the campus to collect data for one of her latest experiments. She and her colleagues are creating an artificial drought in a patch of grassland by covering it with temporary shelters. They expect that predatory nematodes will die or enter a type of suspended animation, leaving the parasitic nematodes that prey on plants to dominate the ecosystem. \u201cHow do plants respond below-ground to drought?\u201d she wonders. Wall has been asking \u2014 and answering \u2014 similar questions about soil for decades. She has become one of the most celebrated and outspoken experts on the hidden biodiversity in dirt, having studied soils and their inhabitants in nearly every corner of the world. She has a special fondness for Antarctica, which she has visited almost every year since 1989. It was there that she and a colleague made a landmark discovery, demonstrating that the soil in one of the driest spots on Earth is home to some animal life and not sterile, as many had thought. The same drive to challenge orthodoxy also helped her to advance in a field in which women were once rare. \u201cMany times, I felt like I was hitting the glass ceiling and got discouraged,\u201d she says, before emphasizing how things have improved. \u201cToday, I love seeing so many women in Antarctic and other research.\u201d Alongside her own experiments, Wall has become an ambassador for soil science and conservation \u2014 at a time when  soil ecosystems are being devastated  by forces such as erosion, pollution, pesticides and climate change.  Soil degradation  over the past two centuries or so has released billions of tonnes of stored carbon into the atmosphere, and this discharge could accelerate, speeding up climate change. Beyond that, says Wall, the threats to soil could jeopardize food production, water quality and the health of humans, plants and animals. The current path, she says, \u201cleaves our terrestrial biodiverse world as we know it very uncertain\u201d. The efforts of Wall and other scientists to raise the profile of soils have been making an impact. The United Nations declared 2015 the  International Year of Soils , and in May, Wall travelled to Nairobi to launch the  \n                   Global Soil Biodiversity Atlas \n                  \u2014 a compendium of information developed by a team of more than 100 scientists, which she helped to lead. David Montgomery, a geomorphologist at the University of Washington in Seattle, says that Wall has inspired many other researchers in their science and outreach on topics important to society. \u201cWe need more first-rate scientists willing to speak in those arenas.\u201d \n               Wedded to the ice \n             This month, Wall is busy planning for her next trip to Antarctica, which will come, as usual, just after Christmas. Her colleagues joke that those journeys keep Wall young because she often crosses the International Date Line on her birthday, essentially erasing the day from the calendar. Assuming that she passes her physical \u2014 for which she is swimming and cycling \u2014 this trip will be her 27th to Antarctica. Wall is 72 and has seemingly boundless energy. Tall and thin, she speaks quickly and picks up the pace as she describes the zoo of organisms in soils, from nematodes to the vast array of microbes. She emphasizes how bacteria and other microorganisms provide services that humans take for granted: filtering water, stabilizing soil, improving air quality and recycling nutrients that enable crops to grow. \u201cI like to think of it as this factory underground,\u201d she says. Wall credits her mother, a biology teacher, with helping to spark a lifelong interest in biology. Raised in Lexington, Kentucky, Wall got her PhD in plant pathology from the University of Kentucky in her home town. In 1972, she left for the US west coast to pursue postgraduate research in nematology \u2014 convinced that nematode parasites had a lot to reveal about how life behaves above ground. California was a shock at first. \u201cThat was eye-opening to me, because I had never crossed the Mississippi River, and it was \u2014 oh my god, where are the trees?\u201d she says. But she ended up liking it there, and the University of California, Riverside, remained her home for much of the next two decades. She strung together a series of grants to keep her work going, confident that soil microorganisms were more significant than most researchers realized. \u201cOriginally, I was just convinced these all make a difference and I was waiting to be proven wrong,\u201d she says. Wall focused at first on nematodes in deserts and arid croplands, conducting the bulk of her research in southern California, New Mexico and Michigan. By the late 1980s, she was seeking ways to understand a species' impact on an ecosystem. \u201cIf you want to find out how a plant parasite has an effect on a root or a predator, how do you exclude everything in the soil except that?\u201d She tried chemicals to kill off species, but they also harmed what she wanted to study. Then a colleague suggested that Wall go somewhere without plants, where the food web was simpler. \u201cI tossed around a number of places,\u201d she says. \u201cAnd we ended up in Antarctica.\u201d She and her colleague Ross Virginia from Dartmouth College in Hanover, New Hampshire, decided to collect samples in the McMurdo Dry Valleys, a series of ice-free basins near the US McMurdo research station. The valleys receive no snow or rain, and humidity is so low that researchers have found the  mummified remains of seals  that made their way into the valleys thousands of years ago. Previous researchers had discovered nematodes and other life near glacier-fed streams that trickle during summer, but experts thought that the dry soils making up most of the valleys were barren. On one of Wall and Virginia's first visits to the Dry Valleys, they had just six hours to collect as many samples as possible before the helicopter returned to pick them up. They found nematodes in about 65% of the samples. \u201cI couldn't believe it,\u201d she says. Ultimately, this showed that life can thrive even in the most inhospitable underground environments, revealing that major ecosystems were being overlooked. Wall has returned to the Dry Valleys every field season except 1992, when she didn't receive funding for the trip. To recognize that long-running research, the US Geological Survey named valleys there after Wall and Virginia. Their work in Antarctica dovetailed with discoveries that Wall had previously made about how nematodes cope with extremely dry conditions in the US Southwest. In the Chihuahuan Desert, Wall and her colleagues showed that the worms rely on anhydrobiosis 1 : they shed most of their water and put metabolic activity on hold. Wall says that the nematodes end up looking like Cheerios, the ring-shaped dry cereal. When she went to Antarctica, Wall and her colleagues found that Dry Valley nematodes use the same mechanism 2  to cope with arid conditions there 3 . With one eye focused on tiny nematodes, Wall kept the other on the bigger picture of how these creatures fit into ecosystems. This was all part of her ever-growing desire to understand and highlight the importance of life underground \u2014 something routinely ignored by many researchers until roughly the past decade. Studies that tracked the decomposition of fallen leaves and other organic materials, for example, tended to overlook the role of soil organisms. Wall says she grew tired of that limited perspective. \u201cWe wanted to show that animals are important in these processes.\u201d So in 2001, she started a global, multiyear  project to measure the impact of soil animals . Her team sent mesh bags filled with hay to colleagues at more than 30 sites around the world. Placed in various locations, the bags attracted worms, beetles and other types of soil invertebrate, while control bags excluded them. Wall's team then analysed the carbon content in each bag and compared the rates at which the organic matter decomposed with and without the soil animals. The results supported Wall's point: soil fauna increased decomposition rates significantly in many regions 4 . A follow-up study 5  found that excluding soil fauna reduced decomposition rates by a global average of 35%. Those studies helped to convince researchers to pay more attention to life in soil (see \u2018Soils under siege\u2019). \u201cWe now understand how key these organisms are to many ecosystem processes,\u201d says Amy Austin, an ecologist at the University of Buenos Aires. The litter finding means that there could be big changes in how carbon moves throughout ecosystems as forces such as climate change alter soil communities. Wall and her colleagues have seen some of this up close during their most recent field season in Antarctica. In as-yet-unpublished work, they found that the dominant nematode in the Dry Valleys, an endemic genus named  Scottnema , has been declining in number, whereas a nematode that lives in wetter soils,  Eudorylaimus , has been increasing, thanks to the melting of ice and permafrost. \u201cIt looks like there's going to be a species shift,\u201d she says. \u201cIt's a fight for habitat.\u201d Scottnema  is Wall's favourite nematode. \u201cIt's living in this harshest environment, mostly by itself, and it's just so recognizable,\u201d she says. But that's not the only reason that she has concerns about the species' decline. The two nematodes feed on different carbon sources in the soil, and population changes could alter the rate at which underground carbon escapes into the atmosphere. If so, the carbon-storage potential of the soil in Antarctica \u2014 a crucial region for absorbing carbon dioxide from the atmosphere \u2014 could change. Shifts in soil biota elsewhere on the planet could also affect how much carbon remains locked up, she says. \n               Emissary for soil \n             In August this year, Wall found herself at the White House talking about soils with other experts and policymakers as part of a  national effort to prevent erosion  and promote soil health. It was the latest scene in a role she has increasingly embraced over the past 15 years \u2014 to bring soil health to the global stage. As Wall's research career blossomed, she took on more leadership positions. She served as president of the American Institute of Biological Sciences in 1993 and the Ecological Society of America in 1999. By that point, her involvement in these organizations was making her think bigger. \u201cI'd been pretty concentrated on the Antarctic research,\u201d she says. \u201cI thought I should be doing more.\u201d She began participating in and leading initiatives that were increasingly global in scope \u2014 chairing, for example, the International Biodiversity Observation Year starting in 2001, which funded research projects to highlight the importance of biodiversity around the world. In 2011, Wall became the founding science chair of the Global Soil Biodiversity Initiative, the group behind the soil atlas that was launched in May. Looking forward, Wall wants to integrate data on soil health and biodiversity into global policies for mitigating large-scale environmental challenges. And she's talking to colleagues about launching a big US experiment to unravel the relationships between soils, biodiversity and health. \u201cConservation and protecting species is a very old idea, and so is soil conservation. But only now are these two ideas coming together,\u201d she says. While campaigning for soils, Wall has also been a champion for women in science. When she was starting out, there weren't many role models for women in her field. And when she made her first trips to Antarctica, she made do with men's long underwear and boots, and endured eight-hour flights on military aircraft that lacked sit-down toilets. Wall was initially turned down for a tenure-track position at the University of California, Riverside, in the late 1980s \u2014 a decision that she and others suggest was related to her gender. Jill Baron, director of the North American Nitrogen Center at Colorado State University, says that how Wall recovered from that rejection is emblematic of her character. \u201cShe moved on into this stellar career,\u201d says Baron. \u201cAnd she's been working to make sure that other young women who come in don't have to ever have that again.\u201d That kind of drive makes a big impression on people just entering science. Ashley Shaw, a PhD student studying under Wall, recalls their first meeting. \u201cShe was just so enthusiastic about her science and what she was working on,\u201d says Shaw. \u201cI walked away feeling like I could save the world.\u201d Wall joined Colorado State University in 1993 to become director of the institution's Natural Resource Ecology Laboratory. There, colleagues say, she attracted interdisciplinary, accomplished scientists, which elevated the stature of the lab both on and off campus. She now serves as founding director of the university's School of Global Environmental Sustainability. There, in an office covered in photos and paraphernalia from Antarctica, she talks eagerly about her goals for the future \u2014 and takes offence when people ask her if she plans to retire. \u201cWhether I pass my physical to go to Antarctica or get too old and have to have a wheelchair dropped for me from the sky,\u201d she says, \u201cI want to keep working on the issues.\u201d \n                 Tweet \n                 Follow @NatureNews \n               \n                     Soil biodiversity and human health 2015-Nov-23 \n                   \n                     Agricultural policy: Govern our soils 2015-Nov-23 \n                   \n                     Forensic science: The soil sleuth 2015-Apr-21 \n                   \n                     Down to earth 2015-Jan-20 \n                   \n                     Agriculture: State-of-the-art soil 2015-Jan-14 \n                   \n                     Subterranean worms from hell 2011-Jun-01 \n                   \n                     International Year of Soils \n                   \n                     Global Soil Biodiversity Atlas \n                   Reprints and Permissions"},
{"file_id": "537294a", "url": "https://www.nature.com/articles/537294a", "year": 2016, "authors": [{"name": "Emily Anthes"}], "parsed_as_year": "2006_or_before", "body": "Windows, desks and employees are being wired up in a quest to create healthy, evidence-based environments. In late May, eight employees of Mayo Clinic's medical-records department packed up their belongings, powered down their computers and moved into a brand new office space in the heart of Rochester, Minnesota. There, they made themselves at home \u2014 hanging up Walt Disney World calendars, arranging their framed dog photos and settling back into the daily rhythms of office life. Then, researchers started messing with them. They cranked the thermostat up \u2014 and then down. They changed the colour temperature of the overhead lights and the tint of the large, glass windows. They played irritating office sounds through speakers embedded in the ceilings: a ringing phone, the clack of computer keys, a male voice saying, \u201cmedical records\u201d, as if answering the phone. On a warm morning in June, the recording is playing on a loop. \u201cI've timed it,\u201d says Randy Mouchka, one of the relocated office workers, with exasperation. \u201cIt's 55 seconds.\u201d Today, the air feels stale and stuffy, but the sun is streaming in \u2014 an improvement over last week, Mouchka says, when the researchers kept the window shades pulled all the way down. These people are the first guinea pigs in the Well Living Lab, an immersive, high-tech facility where Big Brother meets big data. The lab \u2014 a collaboration between Mayo Clinic in Rochester and Delos, a design and technology firm based in New York City \u2014 was built to host studies on how the indoor environment influences health, well-being and performance, from stress to sleep quality, physical fitness to productivity. Down the hall, in a glass-walled control centre crammed with computers, scientists are keeping a close eye on Mouchka and his colleagues. \u201cWe have a panoramic view of everything that's happening,\u201d says Alfred Anderson, the lab's director of technology. One monitor features a live video feed; others display light levels, air temperature, humidity and atmospheric pressure from the 100 or so sensors scattered around the office. The workers are wired up, too: a large monitor reveals the readouts from  biometric wristbands  that measure their heart-rate variability and the electrical conductance of their skin, both crude measures of stress. Researchers will monitor all of this as they subject the employees to nine different types of office environment. \u201cWe're in 'Bad Office 2' today,\u201d Anderson says. Experts know that indoor spaces can pose health risks. Excessive noise is thought to contribute to high blood pressure and heart disease. Artificial light can disrupt circadian rhythms and may increase the risk of certain cancers. There is growing evidence that a sedentary lifestyle could damage health, leading to type 2 diabetes, cardiovascular disease, cancer or early death \u2014 a major concern when so many modern jobs demand sitting at a desk all day. And workplace stress is thought to cost hundreds of billions of dollars worldwide each year in sick days, health-care costs and lost productivity. \u201cWe spend 90% of our time indoors,\u201d says Brent Bauer, the Well Living Lab's medical director. \u201cIf we don't optimize that, we're going to have a hard time optimizing wellness as a whole.\u201d Scientists hope that the lab will allow them to add to the growing literature on the  impact of the built environment , and to produce practical, evidence-based recommendations for creating healthier indoor spaces ranging from offices to homes. It's an ambitious mission that will involve integrating and interpreting vast quantities of data. But scientists, companies and organizations \u2014 impressed by the lab's size, scope and approach \u2014 are eager to see what it finds. \u201cEverybody I've talked to who has heard about it is very excited because it is truly unique,\u201d says Gail Brager, associate director of the Center for the Built Environment at the University of California, Berkeley. \n               Living in the lab \n             Decades of research have revealed that indoor spaces can affect how people think, feel and behave. In a landmark 1984 study 1 , Roger Ulrich, a pioneer in health-care design research now at Chalmers University of Technology in Gothenburg, Sweden, found that people recovering from surgery in hospital rooms with views of nature needed shorter stays and fewer doses of strong pain medication than did those in rooms looking onto a brick wall. Others have reported that certain kinds of artificial light can improve sleep and reduce depression and agitation in people with Alzheimer's disease 2 ; that higher air temperatures seem to curb calorie consumption 3 ; that employees take more sick leave when they work in open-plan offices 4 ; and that  children in daylight-drenched classrooms  progress faster in maths and reading than do those in darker ones 5 . In 2012, the accumulating research led Delos \u2014 which aims to create spaces that boost health and wellness \u2014 to start developing evidence-based guidelines for healthier buildings. The WELL Building Standard, first released in 2014, outlines more than 100 best practices, from using paints that release minimal levels of potentially toxic compounds to organizing cafeterias so that they prominently display fruit and vegetables. Buildings that meet enough of the standards can become 'WELL Certified', in much the same way that buildings can earn sustainable, eco-friendly certification. But in developing the standard, Delos noticed gaps in the scientific literature. There were many studies on a single aspect of the indoor environment, such as light or sound, but in the real world, these variables operate in concert. Studies have shown, for example, that as the temperature and humidity of indoor air increases, its perceived quality declines 6 . Programmes to  reduce indoor air pollution  could yield greater benefits if building managers pay attention to these other factors. Other recommended practices might conflict. In June, researchers reported 7  that office workers scored higher on tests of cognitive function when the room was better ventilated, but many studies have found that background noise impairs cognitive performance. What if increasing air flow requires office workers to open a window onto a loud street? If one worker wants quiet, and another wants fresh air, can evidence decide who should win? \u201cThere are some building-science labs out there who try to bring in as many components as possible, but we never thought they got to the point where they really could address all the issues that might come up in a building design standard,\u201d says Dana Pillai, president of Delos's research division and executive director of the Well Living Lab. \u201cSo we thought we'll just do it ourselves.\u201d In 2013, Delos began discussions with Mayo Clinic. Together, the organizations decided to build an adaptable, immersive lab that gave them precise control over many environmental variables and mirrored the real world as closely as possible. They assembled an 18-person team and sketched out a 700-square-metre dream lab. The facility, which cost more than US$5 million to build and occupies the third floor of an office building, is endlessly transmutable. The tint of the windows can be altered with a mobile app; LED lighting can be tuned to different colours and intensities, and the motorized shades can be programmed to rise and fall at specific times of day. \u201cWe can move walls, we can move plumbing, we can move ducts,\u201d says Bauer. Researchers can transform the lab from a large, open-plan office to a cluster of 6 apartments or 12 hotel rooms, where study participants might live for weeks or even months. \u201cIt's imaginative,\u201d says Alexi Marmot, an architect and researcher at University College London. \u201cThis really has potential to allow all sorts of things to be done that we have not been able to do.\u201d The Well Living Lab occupies a scientific sweet spot \u2014 more controlled than the real offices used for field studies and more realistic than many laboratories. \u201cThat they're going to have people there for extended periods of time, I think is really important,\u201d says Brager, who was not involved in the planning or design of the lab, but will serve on its scientific advisory board. \u201cWhile this still isn't quite a real building \u2014 so there's still going to be some question about the ability to generalize to real-world conditions \u2014 it's a lot closer than the conventional labs.\u201d \n               Office space \n             The Well Living Lab's scientists are starting small and simple, drawing on previous findings to create a variety of office environments that they hypothesize will have positive, negative or no effects on workers' comfort and stress. They are monitoring participants' responses to these changing conditions with daily surveys \u2014 which ask for ratings of comfort, satisfaction, productivity and stress \u2014 and the biometric wristbands. This study is a trial run, designed to validate the lab's systems and approach, as well as the basic idea that office conditions influence employees' well-being. Later this year, the team will explore in more detail how light, noise and temperature affect employee performance, as measured by tests of executive function and productivity, surveys of perceived productivity and physiological measures. Crucially, the researchers will also assess how variables interact, which have the greatest impact on individual and group performance, and what the cumulative effects of changing them are. Such studies might eventually show, for example, that an office with plenty of natural light, a thermostat set to 21 \u00b0C and a modest hum of background noise produces the happiest employees, who respond to e-mails quickly or enter database information accurately. \u201cThe world is a multicomponent place, so there's a benefit of doing that \u2014 that's how the real world is,\u201d says Mariana Figueiro, who directs the Light and Health Program at Rensselaer Polytechnic Institute in Troy, New York. But there's a danger, too, she says. \u201cThose are probably going to be very expensive studies, and they might be very noisy\u201d statistically, which may make the data difficult to interpret. Even the relatively simple pilot study is already generating nearly 9 gigabytes of data per week. As the researchers enrol bigger groups and monitor more variables and outcomes, that figure could expand tenfold. The complexity will also grow as the team begins to layer studies on top of one another. Nicholas Clements, a director at Delos Labs, is collecting samples of the office microbiome: bacteria, fungi and more that live in the office's nooks and crannies, and on the surfaces that people touch every day. Scientists think that it may be possible to actively  shape the indoor microbiome to improve human health , but research into this idea is in its infancy. \u201cWe'd like to push that science further and hopefully we can accomplish that here,\u201d says Clements, who plans to test whether certain environmental interventions, such as changing flooring and surface materials or installing a 'green wall' of living plants, can alter the office's microbes \u2014 or the health of its human occupants. (He will also track participants' exposure to indoor air pollutants, such as the volatile organic compounds emitted by paint and furniture.) Other Mayo faculty members are eager to use the facility. Early next year, ergonomist Susan Hallbeck will investigate whether standing desks improve health in workers with and without certain risk factors for disease \u2014 and, if so, what the optimal ratio and schedule of standing and sitting is. Research has shown that using a standing desk can slightly increase the number of calories burnt, but the evidence for broader health benefits is limited. \u201cThis is a dream study,\u201d says Hallbeck. In addition to the office space, the lab currently contains a single studio apartment, which the researchers will use to learn how to design living spaces that improve sleep quantity and quality in night-shift workers, and whether changes in these workers' circadian cycles influence their microbiota. And whenever the scientists get together, they start churning out new ideas and hypotheses. Perhaps they could turn the space into a classroom, study whether lighting can reduce falls among older people or probe whether certain office conditions make it easier for people with traumatic brain injuries to return to work. \u201cWe're taking kind of a kid-in-a-candy-store approach,\u201d Bauer says. \u201cWe've got almost endless opportunities now to start answering these important questions about, 'How do we optimize the indoor environment?'\u201d \n               Complex challenges \n             The lab's leaders still have a long wish list of sensors and technologies that they would like to deploy, and they're eyeing international expansion. They're not alone. A handful of other teams are taking an immersive, multivariable approach to studying human responses to indoor conditions, using flexible facilities \u2014 from the Total Indoor Environmental Quality Lab at Syracuse University in New York, to the SenseLab at the Delft University of Technology in the Netherlands, which should open in December. But big ambitions can be expensive. To help cover costs, Mayo and Delos have been recruiting corporations and other organizations to the Well Living Lab Alliance. Members make contributions ranging from $75,000 to $300,000, and receive several benefits in return, including early access to research findings, attendance at an annual Well Living Lab summit and discounts on sponsored research. So far,  nine organizations  \u2014 in industries including construction, property management, health-care technology, manufacturing and computing \u2014 have signed up. Corporate partnerships aren't unusual in built-environment research, but scientists say that the lab will have to select its members carefully, be transparent about funding sources and work to ensure scientific independence. \u201cIn this field that's normally been neglected, there's now somebody who clearly has very deep pockets,\u201d Marmot says. \u201cI think it's all to the good. But let's make sure that the appropriate scientific review processes are there.\u201d Bauer says that all proposed studies \u2014 including those sponsored by alliance members \u2014 will need approval from the lab's leaders, its joint steering committee and Mayo's institutional review board. \u201cI think we've been very clear with the companies that are participating that membership isn't a carte blanche,\u201d he says. At the Well Living Lab, the workers are now feeling at home. Despite being poked, prodded and observed by the scientists behind the glass, the first test participants love their temporary office. The desks are adjustable, the chairs comfy and the windows big. Even the air, they say, seems cleaner than in their old offices, to which they will eventually return. \u201cI don't want to go back,\u201d says Mouchka. \u201cI'm hoping we're here for a year.\u201d \n                 Tweet \n                 Follow @NatureNews \n               Reprints and Permissions"},
{"file_id": "534310a", "url": "https://www.nature.com/articles/534310a", "year": 2016, "authors": [{"name": "Megan Scudellari"}], "parsed_as_year": "2006_or_before", "body": "Induced pluripotent stem cells were supposed to herald a medical revolution. But ten years after their discovery, they are transforming biological research instead. \u201cWe have colonies.\u201d Shinya Yamanaka looked up in surprise at the postdoc who had spoken. \u201cWe have colonies,\u201d Kazutoshi Takahashi said again. Yamanaka jumped from his desk and followed Takahashi to their tissue-culture room, at Kyoto University in Japan. Under a microscope, they saw tiny clusters of cells \u2014 the culmination of five years of work and an achievement that Yamanaka hadn't even been sure was possible. Two weeks earlier, Takahashi had taken skin cells from adult mice and infected them with a virus designed to introduce 24 carefully chosen genes. Now, the cells had been transformed. They looked and behaved like embryonic stem (ES) cells \u2014 'pluripotent' cells, with the ability to develop into skin, nerve, muscle or practically any other cell type. Yamanaka gazed at the cellular alchemy before him. \u201cAt that moment, I thought, 'This must be some kind of mistake',\u201d he recalls. He asked Takahashi to perform the experiment again \u2014 and again. Each time, it worked. Over the next two months, Takahashi narrowed down the genes to just four that were needed to wind back the developmental clock. In June 2006, Yamanaka presented the results to a stunned room of scientists at the annual meeting of the International Society for Stem Cell Research in Toronto, Canada. He called the cells 'ES-like cells', but would later refer to them as induced pluripotent stem cells, or iPS cells. \u201cMany people just didn't believe it,\u201d says Rudolf Jaenisch, a biologist at the Massachusetts Institute of Technology in Cambridge, who was in the room. But Jaenisch knew and trusted Yamanaka's work, and thought it was \u201cingenious\u201d. The cells promised to be a boon for regenerative medicine: researchers might take a person's skin, blood or other cells, reprogram them into iPS cells, and then use those to grow liver cells, neurons or whatever was needed to treat a disease. This personalized therapy would get around the risk of immune rejection, and sidestep the ethical concerns of using cells derived from embryos. Ten years on, the goals have shifted \u2014 in part because those therapies have proved challenging to develop. The only clinical trial using iPS cells was halted in 2015 after just one person had received a treatment. But iPS cells have made their mark in a different way. They have become an important tool for modelling and investigating human diseases, as well as for screening drugs. Improved ways of making the cells, along with gene-editing technologies, have turned iPS cells into a lab workhorse \u2014 providing an unlimited supply of once-inaccessible human tissues for research. This has been especially valuable in the fields of human development and neurological diseases, says Guo-li Ming, a neuroscientist at Johns Hopkins University in Baltimore, Maryland, who has been using iPS cells since 2006. The field is still experiencing growing pains. As more and more labs adopt iPS cells, researchers struggle with consistency. \u201cThe greatest challenge is to get everyone on the same page with quality control,\u201d says Jeanne Loring, a stem-cell biologist at the Scripps Research Institute in La Jolla, California. \u201cThere are still papers coming out where people have done something remarkable with one cell line, and it turns out nobody else can do it,\u201d she says. \u201cWe've got all the technology. We just need to have people use it right.\u201d \n               From skin to eyes \n             Six weeks after presenting their results, Yamanaka and Takahashi published 1  the identities of the genes responsible for reprogramming adult cells:  Oct3/4 ,  Sox2 ,  Klf4  and  c-Myc . Over the next year, three laboratories, including Yamanaka's, confirmed the results and improved the reprogramming method 2 , 3 , 4 . Within another six months, Yamanaka and James Thomson at the University of Wisconsin\u2013Madison managed to reprogram adult cells from humans 5 , 6 . Labs around the world  rushed to use the technique : by late 2009, some 300 papers on iPS cells had been published. Many labs focused on working out what types of adult cell could be reprogrammed, and what the resulting iPS cells could be transformed into. Others sought to further improve the reprogramming recipe, initially by eliminating 7  the need to use  c-Myc , a gene with the potential to turn some cells cancerous, and later by  delivering the genes without them integrating into the genome , a looming safety concern for iPS-cell-based therapies. Another big question was how similar iPS cells really were to ES cells. Differences started to emerge. Scientists discovered 8  that iPS cells retain an 'epigenetic memory' \u2014 a pattern of chemical marks on their DNA that reflects their original cell type. But experts argue that such changes should not affect the cells' use in therapies. \u201cThere might be some differences from ES cells, but I believe they are really not relevant,\u201d says Jaenisch. By 2012, when  Yamanaka won half of the Nobel Prize in Physiology or Medicine  for the work, the first human trial of an iPS-cell-based therapy was being planned. Masayo Takahashi, an ophthalmologist at the RIKEN Center for Developmental Biology (CDB) in Kobe, Japan, had been developing ES-cell-based treatments for retinal diseases when Yamanaka first published his reprogramming method. She quickly switched to iPS cells, and eventually began to collaborate with Yamanaka. In 2013, her team made iPS cells from the skin cells of two people with age-related macular degeneration, an eye condition that can lead to blindness, and used them to create sheets of retinal pigment epithelium (RPE) cells for a clinical trial. Not long after, CDB researchers working on another cell-reprogramming technique \u2014 stimulus-triggered acquisition of pluripotency, or STAP \u2014  came under investigation for misconduct . Although unconnected to the iPS-cell trial, the furore made it difficult for Takahashi to advance her study: it created a \u201cheadwind in the calm sea\u201d in which she had been working, she says. Yet her team pushed ahead, and on 12 September 2014, doctors implanted the first RPE sheets into the right eye of a woman in her seventies. Takahashi says that the therapy halted the woman's macular degeneration and brightened her vision. But as the lab prepared to treat the second trial participant, Yamanaka's team identified two small genetic changes in both the patient's iPS cells and the RPE cells derived from them. There was no evidence that either mutation was associated with tumour formation, yet \u201cto be on the safe side\u201d Yamanaka advised Takahashi to put the trial on hold. She did. The suspension gave pause to other researchers interested in the field, says Paul Knoepfler, a stem-cell biologist at the University of California, Davis: \u201cThe world is watching to see how it progresses.\u201d But the difficulties iPS cells have faced getting to the clinic aren't that unusual, says David Brindley, who studies stem-cell regulation and manufacturing at the University of Oxford, UK. It generally takes about 20 years to move a scientific discovery to clinical and commercial adoption, so iPS cells \u201care following roughly the same trajectory\u201d, he says. In the United States, the  Astellas Institute for Regenerative Medicine in Marlborough, Massachusetts  (formerly Advanced Cell Technology), has several iPS-cell-based therapies in its pipeline, including ones for macular degeneration and glaucoma, says chief scientific officer Robert Lanza. For any such therapy, it takes years to work out a suitable method for making the right cell types in large enough quantities, and with enough purity. \u201ciPS cells are the most complex and dynamic therapies that have ever been proposed for the clinic,\u201d says Lanza. \u201cI'm the first one who wants to see these cells in the clinic, but an abundance of caution is needed.\u201d The other great challenge is working out what will be required to get such treatments approved. Loring hopes to start an iPS-cell-therapy trial for Parkinson's disease in the next two years. But it won't be easy: the treatment uses cells derived from individual patients, and Loring plans to do a complex series of checks and validations for each cell line to demonstrate its safety to the US Food and Drug Administration. Developing and testing a therapy in even one person has been educational, says Yamanaka: it took one year and US$1 million. He expects future therapies to use donor-derived iPS cells from a cell bank, rather than making them for each patient. Takahashi plans to compare banked iPS cells side-by-side with those derived from patients, to observe any differences in immune reaction. She intends to apply to the Japanese government to resume her macular-degeneration trial \u201cvery soon\u201d, but when asked, would not specify a timeline. \n               Cellular improvements \n             Although cell therapy has suffered setbacks, other areas of research have blossomed.  Methods for making iPS cells  \u201care more refined and elegant then they were even five years ago\u201d, says Knoepfler. But most reprogramming techniques are inefficient: only a small fraction of cells end up fully reprogrammed. And, like all cell lines, iPS cells vary from one strain to another. That has made it hard to establish controls in experiments. Marc Tessier-Lavigne, a neuroscientist at the Rockefeller University in New York City, confronted this challenge with colleagues at the New York Stem Cell Foundation when they began to work with iPS cells made from people with early-onset Alzheimer's disease and frontotemporal dementia. They quickly realized that comparing a patient's iPS cells with those from a healthy control didn't work \u2014 the cells behaved too differently in culture, probably the result of disparities in genetic background or gene expression. \u201cSo we turned to gene editing,\u201d says Tessier-Lavigne. The  CRISPR\u2013Cas9 gene-editing tool , which has gained huge popularity in recent years, has enabled researchers to introduce disease-associated mutations into a sample of iPS cells and then compare them with the original, unedited cell lines. Jaenisch's lab uses CRISPR\u2013Cas9 with iPS cells daily. \u201cWe can do any manipulations we want to do,\u201d he says. New, refined gene-editing methods  are proving even more useful. In April, for example, Dominik Paquet and Dylan Kwart in Tessier-Lavigne's lab demonstrated 9  a technique for introducing specific point mutations into iPS cells using CRISPR, and editing just one copy of a gene, rather than both. This allowed them to generate cells with precise combinations of Alzheimer's-associated mutations, and to study the effects. But because iPS cells resemble embryonic cells, they are not always ideal for studying late-onset diseases such as dementia. So researchers are exploring ways to stress cells or introduce proteins that age them prematurely. \u201cIt's a valid concern that hasn't been resolved, but there are a number of approaches to really try to tackle it,\u201d says Tessier-Lavigne. The fact that iPS cells mimic early human development has proved useful in another field \u2014 the sprint to discover whether and how infection with the Zika virus in pregnant women might lead to microcephaly, a condition in which a baby's head is smaller than expected. Ming and her colleagues have used iPS cells to create brain organoids \u2014 3D bits of tissue that resemble developing organs. When they exposed these to Zika, they found 10  that the pathogen preferentially infects neural stem cells over newly formed neurons, leading to increased death of the neural stem cells and a decrease in the volume of a layer of neurons in the cortex, resembling microcephaly. Other groups have used iPS cells to  create organoids such as mini-guts and mini-livers , and the list of disease-related discoveries using iPS cells is growing. It includes showing how a gene duplication in glaucoma causes the death of nerve-cell clusters 11 , and recapitulating genetic and cellular alterations associated with Huntington's disease 12 . iPS cells have also been used with some success in drug discovery: they provide a plentiful source of patient-derived cells to screen or test experimental drugs. In 2012, for example, neural stem cells made from people with a nerve-cell-development disease were used to screen nearly 7,000 small molecules and identify a potential drug for the condition 13 . And this year, a team reported 14  generating sensory neurons from iPS cells made from people with an inherited pain disorder. The researchers showed that a sodium-blocking compound reduced the excitability of neurons and decreased pain in the patients. It would be great to use iPS cells to predict whether people will respond to a particular drug, says Edward Stevens, a research fellow at the Pfizer Neuroscience and Pain Research Unit in Cambridge, UK, who led the work, but there will need to be much more evidence that such a strategy works. Even after a decade of reprogramming cells (see 'Inducing a revolution'), researchers  don't know in detail how the process actually occurs . For now, the field is focused on systematically verifying cell lines' identity and safety, by checking their genomes, gene-expression patterns and more. One such effort, the European Bank for Induced Pluripotent Stem Cells, centred in Cambridge, UK, publicly launched its catalogue of standardized iPS cells for use in disease modelling this March.  Yamanaka is also involved in banking iPS cells  for future therapies, collecting varieties that would be immunologically compatible across a broad population. The greatest future challenges, he says, are not scientific. Researchers are going to need strong support from the pharmaceutical industry and governments to move forward with cell therapies; for drug discovery and disease modelling, researchers must be persistent and patient. iPS cells can only shorten the discovery process, not skip it, he says. \u201cThere's no magic. With iPS cells or any new technology, it still takes a long time.\u201d \n                 Tweet \n                 Facebook \n                 LinkedIn \n                 Weibo \n                 Wechat \n               \n                     Mutated mitochondria could hold back stem-cell therapies 2016-Apr-14 \n                   \n                     A decade of transcription factor-mediated reprogramming to pluripotency 2016-Feb-17 \n                   \n                     Stem cells: The growing pains of pluripotency 2011-May-18 \n                   \n                     Stem cells: Fast and furious 2009-Apr-22 \n                   \n                     Stem cells: 5 things to know before jumping on the iPS bandwagon 2008-Mar-26 \n                   Reprints and Permissions"},
{"file_id": "534456a", "url": "https://www.nature.com/articles/534456a", "year": 2016, "authors": [], "parsed_as_year": "2006_or_before", "body": "From ancient DNA to neutrinos and neuroscience, top researchers in China are making big impacts \u2014 and raising their country\u2019s standing in science. WU JI: Upward bound | NANCY IP: Making connections | NIENG YAN: Crystal connoisseur | CAIXIA GAO: Crop engineer | CUI WEICHENG: Deep diver | WANG YIFANG: Particle power | QIAOMEI FU: Genome historian | QIN WEIJIA: Polar explorer | CHEN JINING: Pollution patrol | CHAOYANG LU: Quantum wizard \n               WU JI: Upward bound \n             \n               The country\u2019s top space-science official aims high with bold research missions. \n               By Celeste Biever \n             A fleet of model spacecraft decorates Wu Ji\u2019s office, including the Chang\u2019e 3 lander and its Yutu rover that made up China\u2019s first mission to explore the Moon\u2019s surface. That expedition in December 2013 captivated the world and signalled China\u2019s vast ambitions in space. But for Wu, who has been  director-general of China\u2019s National Space Science Center  (NSSC) in Beijing since 2003, a much bigger turning point came almost three years earlier. On 11 January 2011, he learned that his centre, which is a division of the Chinese Academy of Sciences (CAS), had won funding for a flotilla of spacecraft dedicated to scientific discovery. Up to that point, say Wu and others, almost all of China\u2019s space missions had been geared primarily towards advancing national prestige or demonstrating technological prowess. The 2011 announcement marked the culmination of more than a decade of research, persuasion and international collaboration, mainly on the part of Wu\u00a0\u2014\u00a0and the start of a new era in Chinese science. \u201cChina has changed direction, and he has been the most important player,\u201d says Roger-Maurice Bonnet, former director of science at the European Space Agency, who is an adviser to the NSSCand a scientist at the non-profit International Space Science Institute in Bern. Two of the NSSC missions have launched. One of them is Wukong, a space telescope  hunting for signs of dark matter , which is thought to make up 85% of the matter in the Universe. \u201cThe data is coming down every day,\u201d says Wu. The mission\u2019s team may have an announcement by the end of the year that could \u201cbe a mark in science history\u201d, he says. Next up in 2016 will be the world\u2019s first space-based experiment to  probe the phenomenon of quantum entanglement , and the Hard X-ray Modulation Telescope (HXMT), which will survey a broad region of the sky with greater sensitivity at high energies than other wide-field telescopes. The funding for these missions has totalled about 3 billion yuan (US$455 million) since 2011, and Wu succeeded in winning the cash by persuading the top brass at the CAS and China\u2019s central government that his agency\u2019s proposals for basic space-science missions would deliver breakthroughs. That message resonates with the government\u2019s push to invest more in fundamental research. In person, Wu is hyper-focused on making clear that Chinese research must earn acclaim for its intrinsic value, not just because it is a first for the nation. \u201cThere is no Chinese space science,\u201d he says. \u201cOnly science.\u201d Funding for space research remains a concern because it is allocated in five-year cycles, making it difficult for research communities to mature. But he is confident that space science will gain a steadier source of support\u00a0\u2014\u00a0especially if the latest satellites deliver the goods\u00a0\u2014\u00a0because both Chinese politicians and the general public increasingly recognize the importance of scientific discovery. \u201cWe are a big nation,\u201d he says. \u201cFor human civilization, we should make contributions.\u201d \n               NANCY IP:  \n               Making connections  \n             \n               A neuroscientist explores the brain through basic research and translational medicine. \n               By Helen Pearson \n             Nancy Ip has been building bridges for much of her career. Born in Hong Kong, she found her calling in science during a graduate degree studying neurotransmitters at Harvard Medical School in Boston, Massachusetts. Then she crossed into the biotechnology industry, where she explored the neurotrophic factors that support neuron survival and growth. She took all that expertise back to her native land in 1993, joining the Hong Kong University of Science and Technology (HKUST) when it was just two years old. \u201cIt was considered bold\u201d to move to a place not known for its research, she says, but she wanted to contribute to the region. And since then, she\u2019s worked to bolster science and biotechnology therethrough her research and leadership. \u201cI sleep very little,\u201d says Ip, who puts in a 12-hour-plus work day and gives credit to her support team. \u201cTime flies when you are doing things that you enjoy.\u201d A lot of that time is spent with her large research group, which spans basic neural biology and translational science for neurological disorders Ip has witnessed huge transformations since her return: Hong Kong transferred from British to Chinese control in 1997, and she\u2019s seen mainland China\u2019s science scene boom. And Ip is now building bridges to the mainland, where she hopes to further clinical research by accessing large populations of people with conditions such as Alzheimer\u2019s disease; training people with expertise in both clinical medicine and research; and playing a leading part in a  major brain project  being developed in China. \u201cI teach my students, sometimes you don\u2019t know where research will take you.\u201d  \n               NIENG YAN: Crystal connoisseur \n             \n               A structural biologist unlocks some problem proteins. \n               By Erika Check Hayden \n             As a girl, Nieng Yan read a classic sixteenth-century Chinese novel featuring a monkey that can transform into other animals. Yan wondered what it would be like to change herself: \u201cIf you could shrink yourself into the size of a molecule or a protein, that would be a totally different world,\u201d she recalls thinking. Now, as a leading structural biologist, Yan inhabits that world every day, investigating the way proteins work at the level of atoms. \u201cIt was almost destined that I would became a structural biologist,\u201d she says. Yan did graduate and postdoctoral research at Princeton University, New Jersey, then set up her own laboratory at Tsinghua University in Beijing in 2007 when she was 30 years old, becoming one of the youngest-ever female professors in China. She focused on determining the structures of proteins embedded in cells\u2019 plasma membranes, which are notoriously difficult to solve. One of her targets was the human glucose transporter GLUT1 \u2014 a protein that is essential for supplying energy to cells. Many labs had tried to determine its structure, but the protein had defied their efforts, in part because it readily changes its shape. Yan used a series of tricks to restrict its troublesome movements and finally managed to make crystals and solve its structure in 2014. \u201cPeople tried to crystallize GLUT1 for more than 50 years, and all of a sudden, bingo \u2014 she hit it,\u201d says biochemist Ronald Kaback at the University of California, Los Angeles. Yan\u2019s hits have kept on coming, with a series of high-profile structures. She stays up most nights until 2 or 3 a.m. and skips morning meetings to maximize her time in the lab. Yan has also become a high-profile advocate for better conditions for women and young scientists. She is excited about using the latest technologies, such as  cryo-electron microscopy , which for the first time is allowing researchers to study proteins in fine detail in their native environments, rather than as purified crystals. Yan says that one of the benefits of working in China is she never has to worry about funding and sees a bright future for structural biology there. \u201cThe sky\u2019s the limit,\u201d she says. \n               CAIXIA GAO: Crop engineer \n             \n               A gene-editing specialist seeks to make her mark by improving key agricultural plants. \n               By Heidi Ledford \n             Plant biologist Caixia Gao was initially reluctant to take up gene editing using CRISPR\u2013Cas9 \u2014 the technique that is  sweeping through biology laboratories  around the world. Her lab had already made mutations in 82\u00a0genes using an older technology, and the thought of switching to something new was daunting. \u201cAt first I felt some resistance,\u201d Gao says. \u201cAnd then we decided: well anyway, we have to try.\u201d After a year of frenzied work, her lab at the Chinese Academy of Sciences\u2019 Institute of Genetics and Developmental Biology in Beijing became the first to use the revolutionarily simple gene-editing technique in crops, specifically wheat and rice ( Q.\u00a0Shan  et\u00a0al. Nature Biotechnol.    31,  686\u2013688; 2013 ). \u201cIf there\u2019s any lesson we learn in genome engineering, it\u2019s that you have to be very flexible and adapt to technology that changes every day,\u201d says Daniel Voytas, a plant biologist at the University of Minnesota in Saint Paul. \u201cCaixia has that ability to adapt.\u201d She has been doing that for her whole career. Gao went to university planning to go into medicine, but was redirected to agriculture. \u201cNot my interest at all,\u201d she says. \u201cBut my thinking is always: as long as I am in this position, I will do my best.\u201d After a PhD in grassland ecology, Gao switched again by taking up plant genetic engineering at the seed company DLF in Roskilde, Denmark. Gao had to develop methods for inserting foreign genes into grass, which was frustrating work, says Klaus Nielsen, research director at DLF. Many grasses are difficult to engineer, and each species \u2014 or even genetic variants within a species \u2014 may require its own special mix of growth conditions. Gao is famously cheerful, but there were days when Nielsen could tell that she was seething. Even so, she pressed on. \u201cEventually, she could look in the microscope and see things no one else could see,\u201d Nielsen says. \u201cShe was cracking the nut every time.\u201d During Gao\u2019s 12 years at DLF, she cracked that nut again and again \u2014 by genetically altering several traits, including the times when key grass species flower. But European suspicion of genetically engineered crops left her with little hope that her work would leave the lab. \u201cIt was so difficult to bring a crop to the market \u2014 in the end, the work cannot inspire you any more,\u201d she says. That issue, plus a desire to return with her children to her mother language and culture, sent her back to China. In Beijing, Gao tackled genetic engineering in wheat, a crop that is  legendary for its difficultly to work with , in part because many strains have six copies of the genome. Soon she was considered one of the best in the world at engineering wheat, says Voytas. Gao is happy with her decision to return to China, where funding for agricultural research is a higher priority than it is in Europe, she says. The government has approved some crops developed with early genetic-engineering techniques, but such approvals have slowed, and China has yet to decide how it will regulate gene-edited crops. Still, Gao is hopeful that some of her creations will reach the market. Meanwhile, a disease-resistant wheat engineered in her lab is being further developed by a company in the United States. Ever the optimist, Gao refuses to accept public fears about genetically modified organisms (GMOs). \u201cIf I meet some people in the street and I ask, they will say they don\u2019t want GMO at all,\u201d she says. \u201cAnd I stop there and educate them. They are so surprised.\u201d \n               CUI WEICHENG:  \n               Deep diver \n             The developer of China\u2019s record-setting  Jiaolong  submersible is determined to go even deeper.   By Jane Qiu Cui Weicheng will never forget the dive of his life: riding inside China\u2019s  Jiaolong  submersible as it reached a depth of more than 7,000 metres in the Pacific\u2019s Mariana Trench 4 years ago. \u201cIt\u2019s rather desolate down there \u2014 but strangely beautiful,\u201d says Cui, who led the submersible project. Thanks to  Jiaolong , China is now one of only a handful of nations that have the capability to explore the deep sea.  Jiaolong , which is named after a mythical sea dragon, can travel deeper than any other manned research submersible currently in operation \u2014 allowing the country to reach more than 99.8% of the ocean floor. \u201cThis symbolizes China\u2019s increasing ambition \u2014 and leadership \u2014 in deep-sea research,\u201d says Jian Lin, a marine geophysicist at the Woods Hole Oceanographic Institution in Massachusetts. Until recently, China\u2019s ocean research focused largely on coastal and offshore waters. But, driven by a growing desire for resources and a stronger position in international disputes over marine regions, it is stepping up its support for scientific programmes in the deep ocean. Now at Shanghai Ocean University, Cui is aiming to reach the deepest place on Earth \u2014 the Challenger Deep valley at the bottom of the Mariana Trench, 11,000 metres down. To achieve this goal, he is leading an effort to build a more-pressure-resistant three-person submersible called  Rainbow Fish  at a cost of US$61 million. When it is completed in 2020, the vessel will be available for use by scientists around the world, says Cui. \u201cThe oceans belong to humanity rather than individual nations.\u201d \n               WANG YIFANG: Particle power \n             \n               A leading high-energy physicist hopes to smash records with a giant collider. \n               By Elizabeth Gibney \n             Wang Yifang has a plan to catapult China to the forefront of particle-physics research. The director of the Beijing-based Institute of High Energy Physics (IHEP) wants to  build a 50\u2013100-kilometre circular particle collider  to succeed the 27-km-circumference Large Hadron Collider (LHC) at CERN, the particle-physics laboratory near Geneva, Switzerland. The plan is bold, particularly for a country whose biggest existing collider ring is less than 250\u00a0metres long. Wang\u2019s plan entails building two machines: the first would explore the Higgs boson starting in around 2028; its follow-up would occupy the same tunnel and smash particles with up to seven times the energy of the LHC. China will have to compete against CERN, which also wants to host a post-LHC machine. Although China remains the underdog, Wang\u2019s scheme has captured increasing support, says Nima Arkani-Hamed, a theoretical physicist at the Institute for Advanced Study in Princeton, New Jersey, whom Wang brought on board to lead IHEP\u2019s Centre for Future High Energy Physics in 2013. \u201cNow it\u2019s not purely fantasy. It has a chance of really happening,\u201d he says. Wang says that he only dared to pitch the project because of the success of China\u2019s Daya Bay Reactor Neutrino Experiment. He led that multinational collaboration, which beat international rivals in 2012 by  measuring a parameter  that governs transformations in the ghostly particles. At more than 250 times the price of Daya Bay, the Chinese mega-collider will be a harder sell. China\u2019s government has yet to say whether it will foot the facility\u2019s estimated US$6-billion bill. Brian Foster, a physicist at the University of Oxford, UK, says that Wang has proved he can get major projects off the ground and bring in international support. And one of his best attributes is persistence, says Shing-Tung Yau, a mathematician at Harvard University in Cambridge, Massachusetts. \u201cHe usually succeeds.\u201d \n               QIAOMEI FU: Genome historian \n             \n               A geneticist uses ancient human remains to rewrite Asia\u2019s prehistory. \n               By Ewen Callaway \n             Qiaomei Fu says that she was nervous when she arrived at Germany\u2019s Max Planck Institute for Evolutionary Anthropology to pursue a PhD on ancient-human genomics, in 2009. Her master\u2019s research in China had focused on the diets of early farmers, and she had no experience with ancient DNA, or even genetics. But Fu jumped headfirst into her new field and \u201cturned out to be one of the most amazing students we\u2019ve ever had\u201d, says Svante P\u00e4\u00e4bo, a geneticist at the hub for ancient genomics in Leipzig. With a trio of  Nature  papers published in the past 20 months, Fu has helped to  redraft the history  of Europe\u2019s earliest modern humans. She returned to China in January to lead an ancient-DNA lab at the Institute of Vertebrate Paleontology and Paleoanthropology (IVPP) in Beijing, where she is set to bring the same upheaval to Asia\u2019s ancient past. She joined P\u00e4\u00e4bo\u2019s team just as it was putting the finishing touches to a draft Neanderthal genome. \u201cIt was really high pressure. There were a lot of really interesting things, and a lot of scary things for me,\u201d says Fu. \u201cI came there at really the right time.\u201d Fu learned how to harvest the scant DNA in ancient bones and quickly picked up evolutionary genetics, bioinformatics and computer programming to analyse the data that she was generating. Her focus soon turned to the early modern humans who settled Eurasia after leaving Africa, and Fu began collecting and analysing their bones and teeth. She has  sequenced the oldest  Homo sapiens  DNA  on record: from a 45,000-year-old thigh bone from Siberia and a 40,000-year-old jawbone from a man who had a  Neanderthal ancestor  in the previous 4\u20136 generations. Her efforts \u2014 culminating in a study of 51 individuals who lived between 14,000 and 37,000 years ago \u2014 have shown that Ice Age Europe was more tumultuous than many had thought, with waves of migrants moving in and around the continent and  contributing to the ancestry of contemporary Europeans . Asia\u2019s early history may have been even more dramatic than that, because several groups of archaic humans probably coexisted with modern humans, says Mar\u00eda Martin\u00f3n-Torres, a palaeo-anthropologist at University College London who works in China. Fu will turn her attention to the first  Homo sapien s to settle Asia, who might have  arrived more than 100,000 years ago . She also hopes to study Asian history as recent as a few thousand years ago \u2014 the IVPP has a vast collection of ancient human bones that have yet to be sampled for DNA. Fu is often asked why she returned to China instead of staying in the West. \u201cI\u2019m curious what happened in China and east Asia,\u201d she responds, \u201cI think it was time to come back.\u201d \n               QIN WEIJIA:  \n               Polar explorer \n             \n               An Antarctic veteran has helped establish China\u2019s scientific footprint on the frozen continent. \n               By Jane Qiu \n             When Qin Weijia first visited Antarctica in 1989, he fell in love with this  terra incognita . \u201cIt\u2019s a mysterious continent, full of unknowns and extremes,\u201d says Qin, who is the executive deputy director of the Chinese Arctic and Antarctic Administration in Beijing. Since then, he has been to the frozen continent half a dozen times, including as the 1996 leader of China\u2019s first inland traverse towards Dome A \u2014 the highest point and one of the least studied regions in Antarctica. That was the first of a series of expeditions, which culminated in the construction of  China\u2019s Kunlun station  on Dome A in 2009. The country is a relative late comer to polar research, but the Chinese government is investing heavily in both the Arctic and Antarctic, driven by the desire for natural resources and for a bigger say in international discussions about the regions.  Last December, an international team flew ice-penetrating radar and other sensors on China\u2019s first fixed-wing aircraft on the continent as it traversed back and forth across thousands of square kilometres over Princess Elizabeth Land in Eastern Antarctica to map features under the ice. \u201cIt was the first survey of its kind in a part of Antarctica we know very little about,\u201d says Martin Siegert, a glaciologist at Imperial College London. \u201cThe results are spectacular.\u201d The team discovered the longest canyon on Earth and one of the largest areas of melt under the ice sheet, says Qin, who led the 2015\u201316 expedition. Looking forward, he hopes that China will be able to retrieve the oldest ice on the planet from Dome A, which will help to uncover the history of the Antarctic ice sheets and how they have changed. \u201cOnly then,\u201d says Qin, \u201ccan we predict how they will respond to a changing climate\u201d. \n               CHEN JINING: Pollution patrol \n             \n               The top environment official tackles deadly air. \n               By Jeff Tollefson with additional reporting by David Cyranoski  \n             Chen Jining has a tough job. As head of the Ministry of Environmental Protection, he is responsible for cleaning up the pollution that blankets China\u2019s cities, contaminates its drinking water and laces its croplands with toxic compounds. Although he faces formidable odds in one of the most polluted countries in the world, Chen has gained the confidence of many environmentalists and fellow scientists in his first 15 months on the job by stepping up efforts to root out corruption and ensure that local officials and companies are following rules. \u201cLocal officials are being held more strictly accountable on the environment quality,\u201d says Li Yan, who is deputy programme director for Greenpeace East Asia and works in Beijing. And because Chen\u2019s efforts to reduce air pollution often reduce carbon emissions as well, Li says that the benefits of these reforms extend well beyond the affected areas. \u201cThis has massive global implications.\u201d After earning his doctorate at Imperial College London in 1993, Chen worked his way through the ranks of Chinese academia to become president of Tsinghua University in Beijing in 2012. Now it looks as though he could become the most powerful Chinese environment minister in modern times. His appointment as head of the environment ministry coincided with  a new law that expanded  the agency\u2019s regulatory powers. He pushed for additional authority to investigate and prosecute polluters, and in May that request was granted. This makes it easier for Chen to intervene when local officials fail to implement many of the government\u2019s policies on pollution and development. In addition to cracking down on pollution, Chen\u2019s ministry has worked to strengthen environmental assessments and has boosted transparency by posting more environmental monitoring data on its website, including air-quality readings, as well as information about its enforcement activities. Chen has often shunned contact with the media, but fellow -scientists say that he has been willing to\u00a0-listen\u00a0to and collaborate with outside -scientists\u00a0and international experts on issues such as air quality. \u201cHe once said that the history of China\u2019s environmental protection is a history of international cooperation on environment and development,\u201d says Lailai Li, who heads the Beijing office of the World Resources Institute, on whose board Chen once sat. The minister still faces huge challenges, however. Citizens are increasingly demanding that the government clean up the environment, but China\u2019s rapid industrial rise has created a backlog of problems. Cleaning up the air in major cities may be the easiest task facing Chen; regulators are only beginning to grasp the extent of the water and soil contamination. And government authorities continue to approve industrial projects, even when the environmental costs are all too clear, says Dasheng Liu, an environmental engineer and research fellow at the Shandong Institute of Environmental Science in Jinan. \u201cHe has more power than before,\u201d Liu says, but he also faces \u201cmore arduous and heavy responsibilities.\u201d \n               CHAOYANG LU:  \n               Quantum wizard \n             \n               A young physicist works wonders with photons. \n               By M. Mitchell Waldrop \n             When Chaoyang Lu was at school in a tiny village in Zhejiang province, he fell in love with physics. \u201cYou could figure out how everything works by a few simple equations,\u201d he says. Now Lu is a rising star in China\u2019s push to master quantum information technology\u00a0\u2014\u00a0which could eventually lead to powerful new types of computing and secure communications. The 33-year-old, a physicist at the University of Science and Technology of China in Hefei, is noted for his work with  \u2018entanglement\u2019 , in which the quantum states of different particles are linked regardless of how far apart they are. He has entangled eight photons at once \u2014 a world record \u2014 and has submitted work using ten. Those achievements led Anton Zeilinger, a quantum physicist at the Vienna Center for Quantum Science and Technology, to call Lu a \u201cwizard of entangled photons\u201d. He has also done groundbreaking work with his mentor,  Pan Jian-Wei , in the related phenomenon of  quantum teleportation , in which a quantum state is transported from one particle to another. It was Pan who encouraged Lu to do his PhD work at the University of Cambridge, UK, and who convinced him to return to China with the promise that the government is investing heavily in quantum information technologies, and that bright young physicists could focus on research rather than funding. Lu\u2019s goal is to advance quantum entanglement enough to use it for computations. \u201cIt will be exciting to see, for the first time, a task where a quantum machine can do a better job than a classical one can,\u201d says Lu.\u00a0 \n                 Tweet \n                 Facebook \n                 LinkedIn \n                 Weibo \n                 Wechat \n               \n                     The future of Chinese research 2016-Jun-22 \n                   \n                     Bioethics in China: No wild east 2016-Jun-22 \n                   \n                     China\u2019s bid to be a DNA superpower 2016-Jun-22 \n                   \n                     China by the numbers 2016-Jun-22 \n                   \n                     Policy: Boost basic research in China 2016-Jun-22 \n                   \n                     Circular economy: Lessons from China 2016-Mar-23 \n                   \n                     What China\u2019s latest five-year plan means for science 2016-Mar-18 \n                   \n                     China\u2019s quantum space pioneer: We need to explore the unknown 2016-Jan-13 \n                   \n                     China\u2019s dark-matter satellite launches era of space science 2015-Dec-17 \n                   \n                     Nature  special: Science in China \n                   \n                     2008  Nature  special: China \n                   Reprints and Permissions"},
{"file_id": "534451a", "url": "https://www.nature.com/articles/534451a", "year": 2016, "authors": [], "parsed_as_year": "2006_or_before", "body": "A special issue looks at the country's astonishing scientific trajectory as it seeks to secure its spot among the leaders in innovation. Chinese science has been moving at breakneck speed for the past few decades, fuelled by vast infusions of cash and a rapidly growing technical workforce. China now boasts more researchers than the United States, outspends the European Union in research and development and is on track to best all other nations in its yearly production of scientific papers. But there have been bumps along the way. Chinese research has generally had low impact, and there have been persistent concerns about quality, which the country is trying to address. This special issue looks at the state of science in China. As part of a new 5-year plan, Chinese leaders have pledged to boost research funding to 2.5% of the country's gross domestic product by 2020.  An infographic  charts the rapid rise of Chinese science and examines some of its problems.  Profiles of ten of the nation's leading scientists  show the breadth and promise of research in fields ranging from neuroscience to neutrinos. One area in which the country is vying to lead the world is DNA sequencing \u2014 and  an article shows that it wants to dominate precision medicine , too. An Editorial  notes that even with its impressive scientific gains, China still has far to go before it becomes a leader in innovation. Wei Yang, head of the National Natural Science Foundation of China, which is the leading funder of scientists in the country,  says in a Comment  that China needs to improve the quality, integrity and applicability of its basic research. And policy specialist Douglas Sipp and stem-cell biologist Duanqing Pei argue on that, contrary to common perceptions,  China offers lessons for other nations  on how to govern ethically sensitive research in the life sciences. In this and other areas of science, the rest of the world will be watching closely as China races forward. \n                 Tweet \n                 Follow @NatureNews \n               \n                     Circular economy: Lessons from China 2016-Mar-23 \n                   \n                     What China\u2019s latest five-year plan means for science 2016-Mar-18 \n                   \n                     Research gets increasingly international 2016-Jan-19 \n                   \n                     China\u2019s dark-matter satellite launches era of space science 2015-Dec-17 \n                   \n                     Nature  special: Science in China \n                   \n                     2008  Nature  special: China \n                   Reprints and Permissions"},
{"file_id": "534314a", "url": "https://www.nature.com/articles/534314a", "year": 2016, "authors": [{"name": "Nicola Nosengo"}], "parsed_as_year": "2006_or_before", "body": "Faced with skyrocketing costs for developing new drugs, researchers are looking at ways to repurpose older ones \u2014 and even some that failed in initial trials. When a young physician opted to do a short stint in Grant Churchill's pharmacology lab as part of his medical training, he asked for a task that would quickly teach him the tools of the trade. \u201cSo I thought, 'I have a good project for you',\u201d says Churchill. That was in 2010, and Churchill's group at the University of Oxford, UK, was looking for ways to treat bipolar disorder without using lithium \u2014 a drug that often works well, but is plagued with side effects. So Churchill asked the physician, Justyn Thomas, to screen all of the 450 compounds in the US National Institutes of Health (NIH) Clinical Collection, a library of drugs that had passed safety tests in humans but, for various reasons, had never reached the market. \u201cThat stuff is just sitting there, and it doesn't take much effort,\u201d says Churchill, \u201cso you think you just have to try.\u201d Thomas pipetted a few drops of each compound into Petri dishes filled with bacteria that had been genetically engineered to manufacture the human enzyme suppressed by lithium \u2014 and eventually got a hit. A compound originally intended for people who had experienced a stroke also damped production of the enzyme, suggesting that it might give patients the same benefits as lithium 1 . After experiments in mice showed that the drug, ebselen, could get through the chemical barrier that protects the brain \u2014 something only a few compounds can do \u2014 Churchill's group did a small-scale trial and found that ebselen could be used safely in healthy volunteers 2 . The University of Oxford has now teamed up with a pharmaceutical company to run clinical trials of ebselen for bipolar disorder. The researchers are able to skip the phase I safety trials because the drug had already passed them, and are going straight to phase II: testing the drug's efficacy against bipolar disorder. Churchill is well aware that ebselen could fail this trial, or the larger, more stringent ones needed to test whether the drug works better than lithium. But he is already proud of what his team has achieved. \u201cAs an academic group with no company money,\u201d he says, \u201cwe were able to go from identification of the molecule to a human trial with a very limited budget.\u201d Such stories are becoming more and more common: taking drugs that have been developed for one disorder and 'repositioning' them to tackle another is an increasingly important strategy for researchers in industry and academia alike. These efforts take inspiration from some classic success stories. One is sildenafil, an angina medication developed in 1989 that is now marketed as Viagra and used to treat erectile dysfunction. Another is azidothymidine, which failed as a chemotherapy drug but emerged in the 1980s as a therapy for HIV. Increasingly, the serendipity responsible for those earlier discoveries is giving way to systematic searches for candidates. Partly, this is the result of advances in technology. These include big-data analytics that can now uncover molecular similarities between diseases; computational models that can predict which compounds might take advantage of those similarities; and high-throughput screening systems that can quickly test many drugs against different cell lines. But for the pharmaceutical industry, the real impetus is economics. Getting a drug to market currently takes 13\u201315 years and between US$2 billion and $3 billion on average, and the costs are going up \u2014 even though the number of drugs approved every year per dollar spent on development has remained flat or decreased for most of the past decade 3  (see 'Eroom's law'). The 3,000 or so drugs that have been approved by at least one country therefore represent a vast untapped resource if they can be used against another condition \u2014 as do the thousands more that stalled in clinical trials. Many of them, like ebselen, can probably skip the phase I trials and pose a substantially lower risk of producing dramatic side effects in later phases \u2014 thereby slashing those development costs compared with completely new compounds. Some estimates suggest that repositioning a drug costs on average $300 million and takes around 6.5 years. \u201cMy feeling is that the proportion of drugs that in theory could be repositioned is probably around 75%,\u201d says Bernard Munos, a senior fellow at FasterCures, a drug-development advocacy organization in Washington DC, and a member of the advisory council of the  National Center for Advancing Translational Sciences (NCATS)  at the NIH. But the fraction is probably quite a bit smaller in practice, he concedes. Repositioned drugs still have to make it through phase II and III clinical trials for their new purpose \u2014 trials that respectively eliminate 68% and 40% of every compound that gets that far. And many drugs also face economic barriers, such as being off-patent, that could dissuade pharmaceutical companies from getting involved. \u201cCan some repositioning projects work? Sure. Can it work systematically as a profitable business model? That, I don't believe,\u201d says John LaMattina, a former president of research and development at Pfizer, and now a senior partner at the health-care technology research firm PureTech in Boston, Massachusetts. Nonetheless, some 30 articles on cases of drug repositioning are now being published in scientific journals every month \u2014 a sixfold increase since 2011. A dedicated journal,  Drug Repurposing, Rescue and Repositioning , was launched last year. Three or four drug-repositioning companies are created every year. And some estimates 4  suggest that the number of repositioned drugs entering the regulatory-approval pipeline is rising, and could account for about 30% of all drugs approved every year. \u201cWe've gone past the stage where we had to explain to everyone what we were talking about,\u201d says Andreas Persidis, chief executive of Biovista in Charlottesville, Virginia, one of about 40 companies that now specialize in drug repositioning. \u201cNow it's a recognized field, and we're in the typical second stage of scientific trends, when lots of people jump on the bandwagon.\u201d \n               Starting point \n             The easiest target for repositioning is generic drugs. They have been on the market for years, their safety profiles are well known and they are easy and cheap to obtain for clinical trials because their original patents have expired. And, if they involve new formulations or applications to new disorders, they can still be covered by patents or be granted three years of market exclusivity by the US Food and Drug Administration (FDA). So they remain attractive targets for companies. Biovista, for example, starts by automatically scanning through all the publicly available information on generic compounds, from scientific papers and patents to the database of adverse events compiled by the FDA. Then it creates a kind of cellular social network, mapping all the connections that it has found between drugs, molecular pathways, genes and other biologically relevant entities. The thinking is that the more connections that a drug has in common with a disease, the more likely it is to be a good candidate for repositioning. This is how Biovista discovered that pirlindole \u2014 a generic antidepressant that was developed and is used in Russia \u2014 might be a potential treatment for multiple sclerosis. In mouse models 5 , the drug slows down the progression of the disease, and is now about to progress to a proof-of-concept study in humans. The company has secured a new patent on pirlindole, as well as on another candidate treatment for multiple sclerosis, still another for epilepsy and three for cancer. Another source of knowledge is what doctors see in the clinic. \u201cEvery drug that's been around for some years has about 20 off-label uses, two-thirds of which are started by practising physicians,\u201d says Moshe Rogosnitzky, who heads one of the first academic centres for drug repositioning, established last year at Ariel University in Israel. \u201cBut the other doctors don't know about them, because clinicians have a hard time publishing their results.\u201d So Rogosnitzky and his group systematically canvas these practitioners in Israel and 12 other countries, try to work out a mechanism of action for each reported effect and help the physicians to get patent protection and attract money for further trials. They also help more people to get the drug on an off-label basis. Next July, the group will start a phase II trial to reposition a generic angina drug, called dipyridamole, to treat dry eye disease, a frequent complication for people who have undergone bone-marrow transplants and risk losing their sight because their eyes stop producing tears. \n               Failed but not forgotten \n             Another favourite target is the long list of failed drugs. Most of them pass phase I trials, but do not get past phase II because they don't have the same effect in humans that they had in animals. \u201cStill, there are not many compounds that have some biological activity and are safe in humans, so for heaven's sake let's try to do something else with them,\u201d says Gregory Petsko, a neuroscientist at Weill Cornell Medical College in New York City. The problem is that, apart from really old ones like ebselen, they tend to be locked in the industry's drawers. \u201cSometimes, companies make official announcements when they abandon a molecule, but in most cases they don't,\u201d says Hermann Mucke, a biochemist who in 2000 founded the Vienna-based firm HM Pharma Consultancy, which now makes a business from hunting through discontinued compounds. \u201cSo we monitor a number of sources and look for drugs that have quietly disappeared from pipelines, or for clinical trials that were announced and never led to a publication.\u201d When they feel there may be room for repositioning, Mucke and his staff approach the owner of the drug and try to strike a deal that will allow them to do further tests and development \u2014 and share in any profits that result. They are also creating a database of drugs that have been approved but are no longer manufactured, and of drugs that have been abandoned during development. \u201cWe are developing it for our own use,\u201d he says. \u201cBut if we can find investors, we would like to turn it into a public resource.\u201d In the absence of such a public resource, both the UK Medical Research Council (MRC) and NCATS  have struck deals with major pharmaceutical companies , convincing them to pick some abandoned compounds from their pipelines and release enough information for academic groups to work out whether repositioning might be feasible. \u201cThere's a lot of research that could be done but is not happening, simply because academic people are not aware of what pharmaceutical companies are doing,\u201d says Christine Colvis, who heads the NCATS drug-repurposing effort. Although the MRC programme officially aims to help researchers to understand the biology of diseases, many of the groups that it funds end up doing interesting repositioning work, too. At the University of Manchester, UK, for example, physician\u2013scientist Jacky Smith is testing a compound that was originally developed to treat heartburn to see whether it can help people with chronic cough. The NCATS programme has drawn criticism, however. \u201cIt's good that some groups have had access to some drugs, but that leaves out the vast majority of us,\u201d says Petsko. \u201cAnd there's no guarantee that the compounds in those lists were really the most interesting ones.\u201d  NCATS  spent $12.7 million on 9 projects in 2013, and 8 of those have progressed to phase II trials. They include a former psoriasis drug that is being tested as a smoking-cessation therapy, a failed diabetes pill that is getting a second chance as a treatment for alcoholism, and a failed cancer drug that is now a potential therapy for Alzheimer's disease. A year from now, says Colvis, the first results of those studies will be published, and if all goes well, at least some of them will progress further. In the meantime, NCATS invested $2 million last year in another round of projects. \n               Turning the tables \n             In the long run, says Munos, drug repositioning could disrupt big pharma's business model in much the same way that digital music upended big record companies in the 1990s. \u201cWhen current efforts start resulting in a flow of market approval,\u201d he says, \u201cand we see many small companies developing drugs for a few millions of dollars, there will be a lot of interesting competition with traditional companies.\u201d That optimism is not universal, however. \u201cNot all repositioning projects that work on paper are really feasible,\u201d says Tudor Oprea, a bioinformatics researcher at the University of New Mexico in Albuquerque who monitors the field in addition to doing his own repositioning work. For instance, he says, side effects that would be acceptable for a life-threatening disease might not be acceptable for a chronic one. And the standard business case for repositioning \u2014 that costs are slashed because safety tests are already in the bag \u2014 works only if the dose and mode of administration remain similar. If the new disease requires a significantly higher dose, the drug will have to go through phase I trials again. In the end, says Oprea, development costs can be similar to those for a new molecule. LaMattina wonders whether the opportunities are really as plentiful as proponents suggest. When companies test a new molecule, he says, they do a wide array of tests on various targets and cell types because they want to anticipate the effects. So if a drug really has interesting effects beyond the expected one, industry scientists will find out for themselves. \u201cIt's a bit naive to think that companies overlook all these opportunities to do business,\u201d he says. \u201cIt's typically people in academia, who don't know what happens in the industry, who think they can do it.\u201d But Persidis argues that many companies are too specialized to benefit from all the repositioning opportunities they have in-house. They may have expertise and market penetration in neurology, but not in oncology, and moving a drug from one field to the other could be out of their strategy. \u201cPeople like us keep getting business,\u201d he says, \u201cand that's because larger companies do appreciate having an external partner looking at their drugs from a different angle.\u201d In the end, says Atul Butte, a bioinformatician at the University of California, San Francisco, drug repositioning is a complement to the discovery of new molecules, rather than an alternative. \u201cWe just need more of both,\u201d he says. \u201cIn modern medicine, we're becoming better at figuring out that each disease is actually five or ten different ones. There are simply not enough companies out there to develop new drugs to treat them all.\u201d \n                 Tweet \n                 Facebook \n                 LinkedIn \n                 Weibo \n                 Wechat \n               \n                     Personalized medicine: Time for one-person trials 2015-Apr-29 \n                   \n                     Europe bets on drug discovery 2013-Feb-07 \n                   \n                     US translational-science centre gets under way 2012-Jan-10 \n                   \n                     Traditional drug-discovery model ripe for reform 2011-Mar-02 \n                   \n                     National Center for Advancing Translational Sciences \n                   \n                     UK Medical Research Council \n                   \n                     Ariel Center for Drug Repurposing \n                   \n                     Food and Drug Administration \n                   Reprints and Permissions"},
{"file_id": "534462a", "url": "https://www.nature.com/articles/534462a", "year": 2016, "authors": [{"name": "David Cyranoski"}], "parsed_as_year": "2006_or_before", "body": "First China conquered DNA sequencing. Now it wants to dominate precision medicine too. Six years ago, China became the global leader in DNA sequencing \u2014 and it was all down to one company, BGI. The Shenzen-based firm had just purchased 128 of the world's fastest sequencing machines and was said to have more than half the world's capacity for decoding DNA. It was assembling an army of upstart young bioinformaticians, collaborating with leading researchers worldwide and publishing the sequences of creatures ranging from  ancient humans to the giant panda . The firm was quickly gaining a reputation as a brute-force genome factory \u2014 more brawn than brains, said some. Six years later, the scene is quite different. BGI's most famous scientist and visionary leader, Jun Wang, left last July. The machine that had given the company its dominance is outdated, and the firm's attempt to develop its own industrial-scale whole-genome sequencer hit a roadblock last November, forcing it to lay off employees at its US subsidiary. Meanwhile, the competing system \u2014 Illumina's X series \u2014 has been selling briskly, raising the speed and dropping the price of sequencing worldwide. Armed with the latest sequencers, rival companies to BGI have emerged. Most prominent of these is Novogene in Beijing, founded in 2011 by former BGI vice-president Ruiqiang Li. And although BGI might not have the uncontested dominance it once did, it still claims to have the world's largest sequencing capacity as well as major scientific ambitions \u2014 including to sequence the genomes of one million people, one million plants and animals and one million microbial ecosystems. Today, China is being reborn as a sequencing power with a broader base. Fuelling the drive is a multibillion-dollar,  15-year precision-medicine initiative , which China announced in March and which rivals a  similar initiative in the United States . If these efforts fulfil their goals, doctors envision being able to use a person's genome and physiology to pick the best treatments for his or her disease. The goal now for sequencing companies is to turn the bounty of genomic data into medical benefits. To do that, sequence data alone are not enough \u2014 so some Chinese companies are going beyond brute-force sequencing to work out how lifestyle factors such as diet are also important for understanding disease risk and for finding therapies. \u201cThe thing about China is the ambition they have for their precision-medicine programme is orders of magnitude larger than the United States',\u201d says Hannes Sm\u00e1rason, chief operating officer and co-founder of WuXiNextCODE, a genomics company in Cambridge, Massachusetts, that is part of Shanghai-based WuXi AppTec. \u201cThey are dynamic and receptive. There, the idea of integrating of genomics into health care is very real.\u201d \n               Rise of the machine \n             The new energy behind sequencing is largely thanks to one machine: Illumina's HiSeq X Ten, so called because it is generally sold as sets of ten units. When the machine hit the market in 2014, one set was able to sequence a human genome for close to US$1,000, and power through some 18,000 human genomes per year. Companies that wanted to rival BGI saw an opportunity \u2014 and leapt. Novogene was the first. Following a model similar to BGI's, Li has been building up a large staff of bioinformaticians to generate and interpret sequence data as part of collaborative basic-research projects on the snub-nosed monkey ( Rhinopithecus roxellana ) 1 , cotton ( Gossypium hirsutum ) 2  and other plants and animals. Using the same machine, a handful of other companies \u2014 including WuXi PharmaTech and Cloud Health, both in Shanghai \u2014 focus more on offering sequencing as a service to pharmaceutical or personal-genomics companies. The growth is accelerating. Novogene added a second X Ten set in April, and Cloud Health chief executive Jason Gang Jin says that the company will add another two sets this year. By the end of the year, China will probably have at least 70 units. (Illumina says that 300 units were sold worldwide by the end of last year.) BGI has been trying to keep pace. In 2013, it purchased Complete Genomics in Mountain View, California, in a bid to create its own advanced sequencing machines for in-house use and for sale. The firm announced a system called Revolocity, its attempt to match the HiSeq X, last June. But in November, having taken just three orders, it suddenly suspended sales. BGI is now left with its ageing fleet of 128 Illumina HiSeq 2000 machines and a m\u00e9lange of newer sequencers from various companies, including its own. Estimates of China's share of the world's sequencing-capacity range from 20% to 30% \u2014 still lower than when BGI was in its heyday, but expected to increase fast. \u201cSequencing capacity is rising rapidly everywhere, but it's rising more rapidly in China than anywhere else,\u201d says Richard Daly, chief executive of DNAnexus in Mountain View, which supplies cloud platforms for large-scale genomics. BGI has another machine up its sleeve. The BGISEQ-500 is designed as more of a desktop instrument for research labs. It is also based on the Complete Genomics technology and is set to begin shipping this year. Yiwu He, BGI's new global head of research, says that the system can sequence a human genome for $1,000, and by being smaller in scale and more flexible to use, it will meet China's emerging need for clinical sequencing. \u201cThere will be more sequencing done outside of research institutes, in the hospitals,\u201d says He. The company will bring the price of one human genome sequence down to $200 in the next few years, he predicts boldly. \u201cChina is the most exciting place to do biomedical research.\u201d \n               Genomes en masse \n             The announcement of the precision-medicine programme sent a ripple of excitement through China's sequencing giants. The money will be spent on improving technologies, sequencing, and sharing and analysing more than one million human genomes, as well as on developing drugs and diagnostics from the data and using those findings to personalize medical care. Hungry for a share of the cash, hospitals and clinicians are teaming up with sequencing companies to come up with proposals for the work. The one million human genomes will be split among a variety of studies, and will include groups of 50,000 people who each have metabolic disease, breast cancer, gut cancer or another condition. There will also be cohorts that represent northern, central and southern China, \u201cto look at the different genetic backgrounds of subpopulations\u201d, says Li.  Sequencing capacity is rising rapidly everywhere, but it's rising more rapidly in China than anywhere else.  Similar projects exist elsewhere, including one in the United Kingdom that is sequencing 100,000 genomes, and one in the United States that has a budget of $215 million and aims to cover one million genomes. But China will have some advantages, observers say, not least of which is firm backing from the government. Over the next five years, the government has promised to add several precision drugs and molecular-diagnosis products to the national medical-insurance list, ensuring that companies' research costs will be recouped if they lead to such a product. In the United States, biotech companies with new products can struggle to get insurance companies or the government to pay. \u201cThere is greater acceptance of sequencing and willingness to invest in it in China,\u201d says Daly. In September, BGI will open the China National Genebank, a five-hectare facility in Shenzhen that will house millions of samples from people, animals, plants and microbes. Entrusted to BGI by the central government, the bank will make some samples and data available to researchers around the world. And the company is compiling its own database of one million human genomes, which will overlap to some degree with the national project. BGI will hit the target before anyone else in the world, predicts He. \u201cWe can get there faster because of our partnership with the government, hospitals, universities, because we can move faster than large consortia, and particularly because we have our own sequencer. That is a huge advantage,\u201d he says. But making sense of one million human genomes is a major challenge, says Wang, who quit BGI to found a company called iCarbonX in Shenzhen. The firm plans to collect sequencing data for more than a million people \u201cas a start\u201d, as well as other biological information, including changes in levels of proteins and metabolites, brain imaging, biosensors to monitor things such as glucose levels and even the use of smart toilets that will allow real-time monitoring of urine and faeces. Wang calls it a \u201cdigital form of you\u201d. He plans to use artificial intelligence to integrate all the data, with the ultimate aim of providing medical care that is tuned to an individual's genes and physiological state. Less than a year in, Wang has raised more than $100 million, including a big chunk from Shenzen-based Tencent, the company behind the social-media application WeChat, which Wang says will help to build the data-collection platform. China is already exploring how else genomics can benefit health. In March, BGI celebrated its one millionth NIFTY test \u2014 a screen that sequences fetal DNA circulating in the mother's blood to detect chromosomal abnormalities such as Down's syndrome 3 . The country's conversion from a one-child to two-child policy is expected to accelerate the demand for such tests. Cancer genetics is also well on its way. Cloud Health last year fed genomic data from some 15,000 tumour samples to more than 100 genetics companies in China to help with diagnosis and make sure that patients get the right chemotherapy drugs. The market for pricey genomic tests is growing in step with the country's middle class. \u201cChina has 100 million people making more than $50,000 now,\u201d says Daly. For Wang, sequencing on its own is old hat. \u201cGenomics is important, but it's just one piece of the puzzle,\u201d he says. \u201cAll the complex traits. All the neurodegenerative disorders, cancer, diabetes \u2014 it's all more than genetics. If we only talk about genomics, about massive data without clinical info, that's not enough.\u201d \n                 Tweet \n                 Facebook \n                 LinkedIn \n                 Weibo \n                 Wechat \n               \n                     The future of Chinese research 2016-Jun-22 \n                   \n                     Bioethics in China: No wild east 2016-Jun-22 \n                   \n                     China by the numbers 2016-Jun-22 \n                   \n                     Policy: Boost basic research in China 2016-Jun-22 \n                   \n                     Science stars of China 2016-Jun-20 \n                   \n                     Monkey kingdom 2016-Apr-20 \n                   \n                     China embraces precision medicine on a massive scale 2016-Jan-06 \n                   \n                     Obama to seek $215 million for precision-medicine plan 2015-Jan-30 \n                   \n                     Chinese bioscience: The sequence factory 2010-Mar-03 \n                   \n                     Nature  special: Science in China \n                   \n                     Nature  Insight: Precision medicine \n                   \n                     BGI \n                   \n                     Ilumina \n                   \n                     Novogene \n                   \n                     Cloud Health \n                   Reprints and Permissions"},
{"file_id": "534166a", "url": "https://www.nature.com/articles/534166a", "year": 2016, "authors": [{"name": "Meera Subramanian"}], "parsed_as_year": "2006_or_before", "body": "India\u2019s capital city scrambles to tackle its epic pollution problems. On winter nights, New Delhi burns with innumerable fires. Flames flicker along pavements and street corners, where the destitute huddle to stay warm and cook their suppers, while night watchmen stand guard next to their own small blazes outside private homes. The rising plumes of smoke mingle with exhaust and dust stirred up by overloaded trucks that rumble down roads blanketed in fog. The mixture melds into a nearly opaque substance that leaves a metallic taste on the tongue. Overhead, there is not a single star to be seen. With dawn comes a hint of warmth, but the sunlight remains hidden by haze. A hopelessly optimistic sign \u2014 \u201cMake Delhi Pollution-Free\u201d \u2014 is lashed to a metal cage that protects a young sapling, its withered leaves caked with dust. The grime is the most obvious part of the pollution that plagues India's capital region and its 25 million people. Less discernible are the airborne particles smaller than 2.5 micrometres in diameter, known as PM 2.5  \u2014 the most harmful size range. Just a fraction of the diameter of a human hair and astoundingly aerodynamic, PM 2.5  can penetrate deep into the body, reaching the recesses of the lungs. The particles are a nasty amalgam of pollutants both natural and increasingly anthropogenic, generated from sources within the city's boundaries and hundreds of kilometres away. The World Health Organization (WHO) declares that no amount of this pollutant is safe to breathe. Two years ago, Delhi had the highest PM 2.5  levels of 1,600 cities surveyed by the WHO. Last month, in an updated and expanded inventory 1 , Delhi retained its status as the  most polluted of the world's largest cities , with an annual PM 2.5  average of 122 micrograms per cubic metre (\u03bcg m \u22123 ) \u2014 three times the permitted Indian standard and greatly exceeding the WHO standard of 10 \u03bcg m \u22123 . The pollution, which comes mainly from combustion of wood, coal, gas, diesel and crop residue, is worst in the winter, when wood-burning peaks and cold-weather inversions trap pollutants close to the ground and cause spikes in the daily average of above 600 \u03bcg m \u22123 . Late last year, the levels prompted the Delhi High Court to declare the city a \u201cgas chamber\u201d. The observed PM 2.5  amounts are estimated to cause as many as 16,000 premature deaths and 6 million asthma attacks 2  in Delhi annually, shaving around 6 years off the life expectancy of city residents 3 . Although the WHO data have brought attention to Delhi, the problem is global: according to the agency,  particulate pollution affects more people  than any other pollutant on Earth. \n               Air patrol \n             Delhi, like Beijing, Mexico City, London and Los Angeles, is struggling to reduce pollution, even as its population swells. Researchers and the government are trying to construct a detailed breakdown of the different pollution sources, and authorities are experimenting with ways to mitigate the damage, from restricting when people can drive to shutting down power plants. But India faces unique challenges. Its population is concentrated in the north, an area geographically prone to pollution, and its people have aspirations for development. There is a growing middle class hungry to own cars, and one-fifth of the population merely wants access to basic electricity. Those facts threaten to compromise Delhi's efforts to improve environmental quality. \u201cThe reality is that the pollution in Delhi is very complex. There are a lot of sources. It varies from season. It varies by time of day. It varies by neighbourhood,\u201d says Namit Arora, a member of the pollution task force of the Delhi Dialogue Commission, a government initiative in the city. But he insists that the city can make progress. \u201cWe can act, and we need to act, on multiple fronts simultaneously.\u201d Delhi is trying to do just that. Long before it was saddled with the mantle of having some of the world's worst air, the city and the Supreme Court of India took several steps to alleviate pollution. Vehicle emissions came down in the early 2000s, thanks to decisions to remove lead from petrol, improve vehicle-emission standards and pull old commercial vehicles off the roads. Around the same time, the city implemented a monumental conversion of the public-transportation fleet, including buses and the city's zippy three-wheel auto-rickshaws, away from gasoline and diesel engines to ones fuelled by cleaner compressed natural gas. In 2002, the Delhi Metro subway system opened its first line, improving public-transportation options. All but two of the coal-based thermal power plants in the city were converted to natural gas, and many industries, including brick kilns, were moved beyond Delhi's bounds. These efforts reaped big gains, yet they have been offset by the incessant growth of the megacity. Since 2000, Delhi's population has nearly doubled. And the number of vehicles has almost tripled, from 3 million to more than 9 million, according to the government. Determining what creates hazardous PM 2.5  is a crucial step in reducing it, yet past studies have varied markedly because they have used different methods and relied on limited data. Filling some of the gaps is a report released in January by the Indian government and the Indian Institute of Technology (IIT) Kanpur 4 , which took a more comprehensive approach to investigating the causes of Delhi's poor air quality. Some sources contribute throughout the year, such as pollution from vehicles, diesel generators, construction dust, biomass and coal combustion and industries. Others are seasonal: dry summer dust blowing in from nearby deserts, autumn crop burning and Diwali holiday fireworks, and the warming fires that make the city glow come winter (see 'Poison stew'). Vehicle emissions are constant, and with all those belching tailpipes in sight every day, they often capture the attention of pollution-fighting officials. In January, Delhi implemented an odd\u2013even programme that allowed car owners to drive only every other day, as dictated by their vehicle's number plate. When the 15-day experiment came to a close, a few hundred Delhiites took to the streets to praise the initiative and rally for continued efforts. Families, musicians, passionate teenagers, costumed 20-somethings and activists all gathered near Jantar Mantar, a cluster of monuments built in the 1700s to study skies that were then crowded with visible stars. But over the temporary stage set up for the event, an air monitor showed that PM 2.5  levels were hovering around 184 \u03bcg m \u22123 , a level that warrants staying indoors to reduce exposure. Although levels remained well above acceptable during the  odd\u2013even experiment , several researchers declared it a success in both lowering emissions and, perhaps more importantly, raising public awareness. \u201cPeople are willing to display the civic sense we thought didn't exist here,\u201d says Arora, who was at the demonstration wearing a shirt emblazoned with the words \u201cHelp Delhi Breathe\u201d and an image that was half flower bloom, half gas mask. Still, government officials and researchers admit that this approach has limited long-term potential because of how difficult it is to enforce the ban. During January's trial, people were already talking about buying a second car as a workaround. After the odd\u2013even effort was repeated in April, the government estimated that there were half a million more vehicles on the roads than during January's trial, according to local media, which suggests that people were skirting the rules. The odd\u2013even policy gets a lot of attention, \u201cbut that is not the solution\u201d, says Ashwani Kumar, who was secretary of the environment of the Delhi Pollution Control Committee (DPCC) at the time. \u201cA typical democratic society cannot depend on a 'don'ts' approach,\u201d he says. Making it more expensive and inconvenient to drive a car, for example, would naturally spur people to use public transportation. \u201cIt has to be based on incentive and disincentive; otherwise, it's easy to find loopholes and descend into a quagmire of corruption.\u201d \n               Going public \n             Although parts of Delhi's public-transportation system are impressive, others are lacking. The Delhi Metro is an efficient, extensive electric-rail system with more than 200 kilometres of lines, and it continues to expand. But the Indian media has reported that the government's plans to increase the number of buses in Delhi have been plagued by delays, and that a pilot programme for dedicated bus lanes was met with so much public resistance that the lanes are now being dismantled. \u201cIf the public-transportation system is robust, and it's made safe and comfortable and reliable, people will automatically switch,\u201d says Sarath Guttikunda, director of the independent research group UrbanEmissions.info, which is registered in Delhi. Nudging people onto buses and subways will deal with only part of the transportation pollution problem. Although vehicles as a group contribute up to about one-quarter of PM 2.5  in Delhi 5 , the fraction generated by heavy-duty freight trucks is twice that of cars. To ease the impact of the estimated tens of thousands of trucks that move through the city daily, the Supreme Court has implemented new taxes on them, and Delhi is adding bypass highways. The other crucial ingredient is the type of fuel that goes into vehicles. Given that diesel engines produce much more particulate matter than ones that run on petrol, the rising percentage of luxury diesel cars is a troubling trend. To try to stem the sales, the government temporarily banned registration of diesel vehicles with larger engines earlier this year, according to Indian media. Delhi's emissions standards for vehicles are more stringent than those in the rest of India, but they still lag far behind those in Europe. And the discrepancy between city and national standards means that many vehicles operating in Delhi spew emissions from lower standard vehicles and fuel obtained beyond city limits. Yet all this attention on vehicles is somewhat misplaced because they are not the biggest source of particulate pollution. \u201cIf the goal is to reduce PM, we need to go beyond traffic,\u201d says atmospheric scientist Pallavi Pant of the University of Massachusetts\u2013Amherst, who studies Delhi's air quality. Just a few steps away from the busy roads, on a broken stretch of pavement, a woman tends a cooking pot perched on three stones, a wood fire burning below. Across South Asia, more than one-quarter of the outdoor  air pollution comes from these traditional stoves 6 . In urban Delhi, only about one in ten households still relies on smoky stoves that use wood, dung or kerosene, but they still contribute a substantial part of the city's PM 2.5  burden. One study found 7  that the emissions from fires used domestically for cooking and warmth rival those from the electricity sector, from brick kilns and from industry. Delhi's government and the country's Supreme Court have tried to limit some of these sources, but there is one major factor that they cannot control: the city's location, far from the cleansing breezes of the ocean. From the west, smoke from agricultural crop burning and dust storms from the Thar Desert blow into the city. And from the north, cold fronts sweep down from the Himalayas, locking pollutants in with winter weather inversions. When it comes to air pollution, the city is \u201cgeographically disadvantaged\u201d, says M. P. George, an environmental scientist with the DPCC. Although Delhi can't fend off desert dust storms, it can control construction dust generated by its never-ending building spree. Officials are attempting to decrease air pollution from construction by improving and enforcing rules, such as requirements to cover building sites and trucks to stop dust from blowing away. The problems that Delhi faces also afflict other cities in the region, creating a vast brown cloud that in satellite images seems to smother much of South Asia. About one-third of Delhi's particulate pollution comes from sources outside the city, according to the IIT-Kanpur study 4 . \u201cIt's not just a Delhi problem; it's a regional problem,\u201d says Milind Kandlikar, who researches development and environmental issues at the University of British Columbia in Vancouver, Canada. India's northern cities have come to dominate the WHO's list of most-polluted cities, with half of the top 20 all located in the region. Delhi's efforts will falter unless the rest of the country also steps up, says Kandlikar. \u201cEverybody else has to be involved. This is not going to go away easily.\u201d But political conflicts are threatening the chances for broad-scale pollution-control efforts. There is currently a fractious tension between the national government, run by Prime Minister Narendra Modi's Bharatiya Janata Party, and Delhi's ruling Aam Aadmi Party \u2014 a situation that hinders efforts to develop a unified strategy to deal with air pollution. In lieu of any cooperation, the Supreme Court continues to have an integral role in improving air quality by directing the government to restrict emissions from sources such as vehicles and power plants. \u201cMy reading is that the politicians are often grateful to the courts for acting, because many of these measures are unpopular,\u201d says Arora. With so many sources of air pollution, city officials would ideally like to know which programmes will work best \u2014 so the Delhi Dialogue Commission task force recruited IBM Research\u2013India to develop computer models to forecast pollution levels. \u201cThe idea is to create a simulation framework which will allow them to evaluate different policies and see their impact before they actually implement them,\u201d says Ashish Verma, a senior manager with IBM's Smarter Planet team in Delhi. Although the modelling is still in progress, early results reinforce how important the weather is. This suggests that air-pollution controls might be most effective if they take advantage of specific weather conditions, for example, by closing power plants or limiting traffic during the worst pollution-trapping weather inversions. \n               Citizen action \n             As it seeks cleaner air, Delhi has to confront the country's desperate desire for development, which includes providing electricity to the roughly 240 million people who still lack it. India pledged to expand its renewable energy capacity aggressively, as part of the national plan that it submitted during the United Nations climate-treaty negotiations last year. But  the plan also defends India's right to use fossil fuels , stating that \u201cin order to secure reliable, adequate and affordable supply of electricity, coal will continue to dominate power generation in future\u201d. Both the climate plan and government officials such as Kumar insist that India need not follow conventional modes of development that rely on fossil fuels. \u201cThere is no way we can adopt the technology trajectory of the developed countries who continue to be the biggest polluters in terms of per capita,\u201d Kumar says. But it is not clear whether India will be able to leap-frog past the most polluting forms of energy.  As it seeks cleaner air, Delhi has to confront the desperate desire for development.  One encouraging sign in Delhi is that air quality is now part of an active city-wide conversation. Full-page newspaper ads solicit citizen input on odd\u2013even schemes, and the Delhi government has teamed up with the University of Chicago's Delhi-based academic centre to launch a design competition called the Urban Labs Innovation Challenge: Delhi, which aims to crowdsource ideas for improving air and water quality. In March, it received hundreds of submissions, including ideas for promoting rooftop solar panels and creating viable alternatives to burning waste and crops. Prize money of up to US$300,000 will fund design pilots. In Delhi and around the world, citizens, governments and researchers are all demanding  more air-quality data , which indicates an interest in knowing the enemy. Information from government monitors is publicly available, but the interfaces are often clunky. This has given rise to new alternatives such as IndiaSpend, an independent media outlet that also conducts 'sensor journalism', in which readings from its own pollution monitors are made available in a user-friendly format. The sheer increase in global monitoring \u2014 the WHO's database doubled in two years because many more cities had begun to monitor their air \u2014 offers an opportunity for more-comprehensive studies going forward, including in regions that have previously been neglected. Meanwhile, concerns about pollution are hard to escape. In upscale Delhi markets, vendors hawk air masks and purifiers, and parents can purchase nebulizers decorated with cute animals to appeal to children with asthma. Doctors have been known to advise patients with lung ailments to leave the city. \u201cPicking up a life is not so easy,\u201d pleads one mother. \u201cWhat are we supposed to do?\u201d Many Delhiites tend to swing between despair and hope \u2014 just as the skies go through their own cycles. By late March, the weather starts to shift and the winds pick up. It's as if the city's windows are thrown open, allowing fresh breezes to blow through. July brings the monsoon rains and washes much of the danger out of the sky, down towards the Yamuna River, which carries some of the burden away. For a few months, citizens can step into the night, tilt their heads back and once again enjoy the stars overhead. \n                 Tweet \n                 Follow @NatureNews \n               \n                     New Delhi car ban yields trove of pollution data 2016-Feb-17 \n                   \n                     India: The fight to become a science superpower 2015-May-13 \n                   \n                     India\u2019s budget disappoints scientists 2015-Mar-02 \n                   \n                     Global health: Deadly dinners 2014-May-28 \n                   \n                     Nature  Special: Science in India \n                   \n                     Nature  Special: 2015 Paris climate talks \n                   \n                     World Health Organization: Air pollution\u00a0 \n                   Reprints and Permissions"},
{"file_id": "534452a", "url": "https://www.nature.com/articles/534452a", "year": 2016, "authors": [{"name": "Richard Van Noorden"}], "parsed_as_year": "2006_or_before", "body": "Research capacity has grown rapidly, and now quality is on the rise. China\u2019s blazing economic growth has cooled in recent years, but the nation\u2019s scientific ambitions show no signs of fading. In 2000, China spent about as much on research and development (R&D) as France; now it invests more in this area than the European Union does, when adjusted for the purchasing power of its currency. That surge in funding has paid off. China now produces more research articles than any other nation, apart from the United States, and its authors feature on around one-fifth of the world\u2019s most-cited papers. Top Chinese scientific institutions are breaking into lists of the world\u2019s best, and the nation has created some unparalleled facilities. There\u2019s room for improvement within that bright picture. China steers much less of its R&D funding towards basic research than do many science power\u00adhouses, and its international collaboration rates are on the low side. The scholarly impact of its papers is improving rapidly, yet it remains below the world\u2019s average. And although China boasts more than 1.5 million researchers, that\u2019s a small number given its vast population. The country\u2019s leaders recognize some of the weaknesses and have pledged to increase funding for science and technology, aiming particularly to stimulate innovation. \n                 Tweet \n                 Facebook \n                 LinkedIn \n                 Weibo \n                 Wechat \n               \n                     The future of Chinese research 2016-Jun-22 \n                   \n                     Bioethics in China: No wild east 2016-Jun-22 \n                   \n                     Science in China 2016-Jun-22 \n                   \n                     China\u2019s bid to be a DNA superpower 2016-Jun-22 \n                   \n                     Policy: Boost basic research in China 2016-Jun-22 \n                   \n                     Science stars of China 2016-Jun-20 \n                   \n                     Second Chinese team reports gene editing in human embryos 2016-Apr-08 \n                   \n                     What China\u2019s latest five-year plan means for science 2016-Mar-18 \n                   \n                     China\u2019s quantum space pioneer: We need to explore the unknown 2016-Jan-13 \n                   \n                     China embraces precision medicine on a massive scale 2016-Jan-06 \n                   \n                     China\u2019s dark-matter satellite launches era of space science 2015-Dec-17 \n                   \n                     Nature  special: Science in China \n                   Reprints and Permissions"},
{"file_id": "534610a", "url": "https://www.nature.com/articles/534610a", "year": 2016, "authors": [{"name": "Elizabeth Gibney"}], "parsed_as_year": "2006_or_before", "body": "Strange signals are bombarding Earth. But where are they coming from? No astronomer had ever seen anything like it. No theorist had predicted it. Yet there it was \u2014 a 5-millisecond radio burst that had arrived on 24 August 2001 from an unknown source seemingly billions of light years away. \u201cIt was so bright, we couldn't just dismiss it,\u201d says Duncan Lorimer, who co-discovered the signal 1  in 2007 while working on archived data from the Parkes radio telescope in New South Wales, Australia. \u201cBut we didn't really know what to do with it.\u201d Such fleeting radio bursts usually came from pulsars \u2014 furiously rotating neutron stars whose radiation sweeps by Earth with the regularity of a lighthouse beam. But Lorimer, an astrophysicist at West Virginia University in Morgantown, saw this object erupt only once, and with more power than any known pulsar. He began to realize the significance of the discovery 1  only after carefully going over the data with his former adviser, Matthew Bailes, an astrophysicist at Swinburne University of Technology in Melbourne, Australia. If the source really was as far away as it seemed, then for a few milliseconds it had flared with the power of 500 million Suns. \u201cWe became convinced it was something quite remarkable,\u201d he says. But when no more bursts appeared, initial excitement turned to doubt. Radio astronomers have learnt to be sceptical of mysterious spikes in their detectors: the events can all too easily result from mobile-phone signals, stray radar probes, strange weather phenomena and instrumental glitches. Wider acceptance of what is now known as the Lorimer burst came only in the past few years, after observers working at Parkes and other telescopes spotted similar signals. Today, the 2001 event is recognized as the first in a new and exceedingly peculiar class of sources known as fast radio bursts (FRBs) \u2014 one of the most perplexing mysteries in astronomy. Whatever these objects are, recent observations suggest that they are common, with one flashing in the sky as often as every 10 seconds 2 . Yet they still defy explanation. Theorists have proposed sources such as evaporating black holes, colliding neutron stars and enormous magnetic eruptions. But even the best model fails to account for all the observations, says Edo Berger, an astronomer at Harvard University in Cambridge, Massachusetts, who describes the situation as \u201ca lot of swirling confusion\u201d. Clarity may come soon, however. Telescopes around the world are being adapted to look for the mysterious bursts. One of them, the Canadian Hydrogen Intensity Mapping Experiment (CHIME) near Penticton in British Columbia, should see as many as a dozen FRBs per day when it comes online by the end of 2017. \u201cThis area is set to explode,\u201d says Bailes. \n               Curiouser and curiouser \n             Astronomers might have had more confidence in the Lorimer burst initially had it not been for a discovery in 2010 by Sarah Burke-Spolaor, who was then finishing her astrophysics PhD at Swinburne. Burke-Spolaor, now an astronomer at the US National Radio Astronomy Observatory in Socorro, New Mexico, was trawling through old Parkes data in search of more bursts when she turned up 16 signals that shook everyone's confidence in the original 3 . In most ways, these signals looked remarkably similar to the Lorimer event. They, too, showed 'dispersion', meaning that high-frequency waves appeared in the detectors a few hundred milliseconds before the low-frequency ones. This dispersion effect was the most important piece of evidence convincing Lorimer and Bailes that the original burst came from well beyond our Galaxy. Interstellar electrons in clouds of ionized gas are known to interact more with low-frequency waves than with high-frequency ones, which delays the low-frequency waves' arrival at Earth ever so slightly, and stretches the signal (see 'Flight delays'). The delay in the Lorimer burst was so extensive that the wave had to have travelled through a lot of matter \u2014 much more than is in our Galaxy. Unfortunately for Lorimer and Bailes' peace of mind, Burke-Spolaor's signals also showed a crucial difference from the original: they seemed to pour in from everywhere, not just from where the telescope was pointing. Dubbed perytons, after a mythical winged creature that casts a human shadow, these bursts could have been caused by lightning, or some human-made source. But they were not extraterrestrial. Lorimer decided to postpone his research into FRBs for a while. \u201cI didn't yet have tenure,\u201d he says, \u201cso I had to go back and do more mainstream projects, just to keep my research moving.\u201d Bailes and his team kept going, and upgraded the Parkes detector's time and frequency resolution. In 2013, they turned up four new FRB candidates that resembled the Lorimer burst 4 . But some outsiders remained sceptical that the signals were really coming from space \u2014 not least because all the FRBs thus far had been seen by one team using one telescope. \u201cI was desperate for someone else to find them somewhere else,\u201d says Bailes. In 2014, his wish was finally granted. A team led by astronomer Laura Spitler at the Max Planck Institute for Radio Astronomy in Bonn, Germany, published their observations of a burst at the Arecibo Observatory in Puerto Rico 5 . \u201cI was ridiculously overjoyed,\u201d says Bailes. The Arecibo discovery convinced most people that FRBs were the real deal, says Emily Petroff, who is now an astrophysicist at the Netherlands Institute for Radio Astronomy in Dwingeloo. Yet, as long as the Burke-Spolaor signals went unexplained, they cast a shadow of doubt. \u201cAt any talks I would give,\u201d says Petroff, \u201csomeone would say, 'But what about perytons?'\u201d So in 2015, while still a graduate student at Swinburne, she led a hunt to track down the source of perytons once and for all. First, Petroff and her team used the upgraded Parkes detector to pinpoint when the bursts were happening: at lunchtime. \u201cImmediately I thought, 'This isn't weather',\u201d says Petroff. Then came another peryton at a suspiciously familiar radio frequency, which led the team to run experiments in the staff kitchen. Perytons, they discovered, were the result of  scientists opening the microwave oven mid-flow . But the Lorimer event was in the clear: records showed that at the time of the burst, the telescope had been pointed in a direction that would have blocked any microwave signal from the kitchen 6 . \u201cSo then I worried, maybe they've just got a different brand of microwave at Arecibo,\u201d says Bailes, whose team at Parkes had, by then, racked up 14 separate bursts. He did not relax completely until later in 2015, when a burst was spotted at a third facility \u2014 the Green Bank Telescope in West Virginia. That burst had another quality that supported an extraterrestrial origin: its waves were rotated in a spiral pattern \u2014 which results from  passing through a magnetic field  \u2014 and were scattered as if they had emerged from a dense medium. \u201cThere's no way that's a microwave oven,\u201d Bailes told himself. \n               Bursts of inspiration \n             But that still leaves the question of what the FRBs actually are. The extreme brevity of the signal, just 5 milliseconds, implied that the source must be a compact object no more than a few hundred kilometres across \u2014 a stellar-mass black hole, perhaps, or a neutron star, the compact core left over by a supernova. And the fact that Earth-based telescopes can detect the FRBs at all means that this compact source somehow puts out an immense amount of energy. But that still leaves a long list of candidates, from merging black holes to flares on magnetars: rare neutron stars with fields hundreds of millions of billions of times stronger than the Sun's.  There's no way that's a microwave oven.  An important clue arrived earlier this year when Spitler's team reported that at least one FRB source repeats: data from Arecibo revealed a  flurry of bursts over two months , some spaced just minutes apart 7 . That behaviour has been confirmed by the Green Bank telescope, which detects signals in a different frequency band 8 . Until then, each of the observed FRBs had been a one-off event, which hinted at cataclysmic explosions, or collisions in which the sources were destroyed. But a repeating FRB implies the existence of a source that survives the pulse event, says Petroff. And for that reason, she says, \u201cI would assume it would be something to do with a neutron star\u201d \u2014 one of the few known objects that can emit a pulse without self-destructing. Spitler agrees. As an example, she points to the Crab nebula: the result of a supernova explosion that was observed from Earth in 1054 and left behind a rapidly spinning pulsar surrounded by glowing gas. The Crab pulsar occasionally releases extremely bright and narrow radio flares, Spitler says. And if this nebula were in a distant galaxy and hugely boosted in energy, its emissions would look like FRBs. If one source repeats, Spitler says, the simplest interpretation is that they all do, but that other telescopes haven't been sensitive enough \u2014 or lucky enough \u2014 to see the additional signals. Yet others think that perhaps only some are repeating. \u201cI wouldn't be surprised if we end up with two or three populations,\u201d says Petroff. \n               A long way home \n             Another crucial question is how far away the FRBs are. The 20 bursts seen so far seem to be scattered randomly around the sky rather than being concentrated in the plane of the Galaxy, which suggests that their sources lie beyond the borders of the Milky Way. And yet to Avi Loeb, a physicist at Harvard University, such vast distances imply an implausibly large energy output. \u201cIf you want the burst to repeat, you won't be able to destroy the source \u2014 therefore, it cannot release too much energy,\u201d he says. \u201cThat puts a limit on how far away you can put it.\u201d Perhaps, he says, the FRB sources are neutron stars in our own Galaxy, and the dispersion is mostly the result of still unknown electron clouds that blanket them. But others suggest that such a dense cloud in the Galaxy should be visible in other wavelengths. At the California Institute of Technology (Caltech) in Pasadena, astrophysicist Shri Kulkarni has scoured data from several telescopes for a galactic source and turned up nothing 9 . Kulkarni, who directs Caltech's optical observatories, initially argued for galactic FRBs, and even made a US$1,000 bet on it with astronomer Paul Groot of Radboud University Nijmegen in the Netherlands. Now, he finds the evidence for extragalactic FRBs to be overwhelming, and has agreed to settle the bet \u2014 grudgingly. \u201cI think I will pay him in $1 bills,\u201d he says. Still, Kulkarni hasn't ruled out the possibility that the FRB sources lie in galaxies that are perhaps a billion light years away, rather than many billions. Such a distance would still require at least some of the signal dispersion to come from electron clouds in the host galaxy, he says. But closer FRBs would not have to be so energetic. \u201cIt takes them from being amazingly exotic, to just exotic,\u201d he says. The answer could mean a great deal to observers. If the FRB signals have travelled through local plasma clouds, they could give weather reports from neighbouring galaxies. But if they are truly cosmological \u2014 coming from halfway across the visible Universe \u2014 they could solve a long-standing cosmic mystery. For decades, astronomers have known from observations of the early Universe that the cosmos should contain more everyday matter \u2014 the kind made up of electrons, protons and neutrons \u2014 than exists in the visible stars and galaxies. They suspect that it lies in the cold intergalactic medium, where it is effectively invisible. But now, for the first time, the dispersion of the FRB signals could enable them to measure the medium's density in any given direction. \u201cThen, we have essentially a surgical device to do intergalactic tomography,\u201d says Kulkarni. \n               Rapid-fire detection \n             First, however, astronomers have to find a lot more FRBs and pin down their locations. \u201cUntil then, we are stumbling in the dark,\u201d says Berger. One way to accomplish that is to extract the FRBs from radio-telescope data in real time, so that scientists at other observatories can observe the bursts in multiple wavelengths. Since last year, the Parkes team has been doing this by boosting the observatory's in-house computing power, and scientists at Arecibo hope to follow suit this year. In February, the strategy seemed to be paying off when an independent team followed up  within two hours of an FRB's detection at Parkes . The team tentatively pinpointed the burst to a specific galaxy almost 6 billion light years away.  Further observations cast doubt on that interpretation.  But even so, says Lorimer, the method is sound and may pay off in the future. Others observers are putting their hopes in new telescopes. In 2014, astrophysicist Victoria Kaspi at McGill University in Montreal, Canada, submitted a proposal to adapt CHIME, which was originally  designed to map the expansion of the Universe in its early years . \u201cIt became clear very quickly that it would be a fantastic FRB instrument,\u201d says Kaspi. Although dish telescopes such as Arecibo can be highly sensitive, they observe only a single, tiny patch of sky at a time. CHIME, by contrast, consists of four 100-metre-long half-pipes dotted with antennas that can monitor much bigger stretches of sky in long lines. After undergoing testing and debugging, CHIME should see its first FRBs sometime next year, says Kaspi, ultimately finding more than a dozen per day. In Hoskinstown, Australia, meanwhile, Bailes and his colleagues are refurbishing the 1960s-vintage Molonglo Observatory Synthesis Telescope, turning it into an FRB observatory with a single half-pipe 16 times longer than CHIME's, although one-quarter as wide. The team has already found three as-yet-unpublished FRBs with the facility working at only about 20% of its final sensitivity, says Bailes. Another strategy for locating the FRB sources is to work with existing facilities such as the Very Large Array: an 'interferometer' that uses the time difference between signals from 27 radio telescopes spaced across 36 kilometres of grassland near Socorro, New Mexico, to create a single, high-resolution image. Sometime in the next year or so, says Lorimer, the array could detect an FRB and locate its home galaxy. \u201cUltimately, that could settle a lot of arguments and bets,\u201d he says. Kulkarni, meanwhile, is leading two projects. The first uses ten 5-metre-wide dishes in an array that can see and locate only super-bright FRBs, but that makes up for its low sensitivity by peering at a huge swathe of sky. The second takes the principle to the extreme, using 2 antennas spaced at observatories 450 kilometres apart that will see only the very brightest FRBs, but that are able to examine half the sky at once. That would enable it to catch the rare FRBs that presumably exist within our own Galaxy, but whose extreme brightness existing telescopes are not designed to see. \u201cMost facilities would just discount it as interference,\u201d says Kulkarni. If FRBs do turn out to come from cosmological distances, says Loeb, their identification would be a major breakthrough, potentially unravelling a new class of source that could be used to probe the Universe's missing matter. But then, he says, FRBs could also be something that no one has thought of yet: \u201cNature is much more imaginative than we are.\u201d \n                 Tweet \n                 Facebook \n                 LinkedIn \n                 Weibo \n                 Wechat \n               \n                     Fresh confusion over origins of enigmatic radio-wave blasts 2016-Mar-02 \n                   \n                     Mysterious radio burst pinpointed in distant galaxy 2016-Feb-24 \n                   \n                     Mysterious radiowave blast may have come from starquake 2015-Dec-02 \n                   \n                     Microwave oven blamed for radio-telescope signals 2015-May-08 \n                   \n                     Mystery extra-galactic radio bursts could solve cosmic puzzle 2013-Jul-04 \n                   \n                     Blog post: Bizarre radio burst baffles astronomers \n                   Reprints and Permissions"},
{"file_id": "533026a", "url": "https://www.nature.com/articles/533026a", "year": 2016, "authors": [{"name": "Lesley Evans Ogden"}], "parsed_as_year": "2006_or_before", "body": "Canadian scientists are now allowed to speak out about their work \u2014 and the government policy that had restricted communications. Early one Thursday morning last November, Kristi Miller-Saunders was surprised to receive a visit from her manager. Miller-Saunders, a molecular geneticist at the Canadian fisheries agency, had her reasons to worry about attention from above. On numerous occasions over the previous four years, government officials had forbidden her from talking to the press or the public about her work on the genetics of salmon \u2014 part of a  broad policy that muzzled government scientists in Canada  for many years. At one point, a brawny \u2018minder\u2019 had actually accompanied her to a public hearing to make sure that she didn\u2019t break the rules. But the meeting last autumn was different. Miller-Saunders\u2019 manager at Fisheries and Oceans Canada (DFO) in Nanaimo walked in with a smile and gave her advance notice that the newly elected government would be opening up scientific communication: she and other federal researchers would finally be free to speak to the press. \u201cIt was like a weight was being lifted,\u201d she says. Important findings on climate change, depletion of the ozone layer, toxicology and wildlife conservation that had been restricted for so long could now be openly discussed. Canadian scientists celebrated the move  far and wide. Shark researcher Steve Campana danced in his office at the University of Iceland in Reykjavik, where he had relocated after leaving the DFO because of the communications constraints and other limitations. Six months later, the government is loosening its grip on communications but the shift at some agencies has not been as swift and comprehensive as many had hoped. And with the newfound freedom to speak, the full impact of the former restrictions is finally becoming clear. Canadian scientists and government representatives are opening up about what it was like to work under the former policy and the kind of consequences it had. Some of the officials who imposed the rules are talking about how the restrictions affected the morale and careers of researchers. Their stories hint at how governments control communications in even more politically repressive countries such as China, and suggest what might happen in Canada if the political winds reverse. \u201cIt was not a good time for journalists. It was not a good time for scientists. It was not a good time for morale in the federal community, and it was not a good time for Canadian citizens,\u201d says Paul Dufour, a science-policy analyst at the University of Ottawa. \n               Set to silence \n             The crackdown on government scientists in Canada began in 2006, after Stephen Harper of the Conservative Party was elected prime minister. During the nine-year Harper administration, the government placed a priority on boosting the economy, in part by stimulating development and increasing the extraction of resources, such as  petroleum from the oil sands in Alberta . To speed projects along, the administration eased environmental regulations. And when journalists sought out government scientists to ask about the impacts of such changes, or anything to do with environmental or climate science, they ran into roadblocks. For decades before the Harper administration, reporters had been free to call up government researchers directly for interviews. But suddenly, all requests for interviews had to be sent to government communications offices, which then had to get approval from multiple tiers of bureaucrats higher up. \u201cIt was an incredible rigmarole to try and get the most innocuous bit of information to media or the public,\u201d says Diane Lake, who was a communications officer with the DFO at the time. Lake had been a newspaper reporter for a dozen years before joining the department in 1992, so she knew what journalists needed to produce stories. She has fond memories of her time as a communications officer before the Harper years, but after he took office, her job became less about communicating science and more about censoring it. When journalists called her trying to reach scientists, she was required to get approval for scripted answers that researchers could give, but she found the authorization process opaque and arbitrary. \u201cThere were never any written protocols on what would pass muster and what wouldn\u2019t,\u201d she says. \u201cI would always say, \u2018can you write that down?\u2019 to folks in Ottawa.\u201d No one ever did. Because the scripts had to be endorsed by \u201clegions of approvers\u201d in a convoluted process, meeting reporters\u2019 deadlines was \u201ckind of hopeless\u201d, says Lake. The starkest example for her came in 2011, when Miller-Saunders (then Miller) and her colleagues published a paper in  Science  that investigated why  unusual numbers of sockeye salmon ( Onchorhynchus nerka ) were dying  in British Columbia\u2019s Fraser River on their way to spawn ( K. M. Miller  et\u00a0al. Science    331,  214\u2013217; 2011 ). Through genomic analysis, the researchers found evidence that a virus might be to blame. The topic was sensitive in part because some scientists and environmentalists had previously raised concerns that fish farms could transfer diseases to wild salmon. Science  had alerted journalists about the paper days ahead of its publication under an embargo, giving reporters time to conduct interviews and write their stories. Many journalists had contacted Lake with requests to speak with Miller-Saunders, and Lake had been busy setting up interviews during the days before publication. But the permission process dragged on, and Lake and Miller-Saunders had to postpone those interviews repeatedly. Then, on the day of the paper\u2019s publication \u2014 14 January \u2014 Lake got word from Ottawa that Miller-Saunders had been denied permission to talk to reporters at all. \u201cObviously, journalists were very upset, and it sort of snowballed from there,\u201d Lake says. Many reporters wrote stories about the muzzling of a government scientist rather than about the genetics of salmon. Journalists who wanted interviews with Miller-Saunders were told to contact her co-authors outside the government. \u201cThe unfortunate thing was that my co-authors were not genomic scientists,\u201d Miller-Saunders says, so they couldn\u2019t readily address specific questions about the genetic aspects of the study. The \u201cKristi Miller debacle\u201d, as Lake calls it, was just one high-profile example of scientists being silenced. But there were hundreds of others, she says. \u201cIt was like an iron curtain was drawn across communicating research to Canadians.\u201d The federal government maintained that it was inappropriate for Miller-Saunders to speak to reporters because she was part of a judicial enquiry into the management of sockeye salmon, known as the Cohen Commission. At a public enquiry of the commission in 2011, the DFO assigned Miller-Saunders a media officer and a bodyguard, whom Miller-Saunders describes as a \u201cvery nice burly man\u201d. Miller-Saunders was kept in a separate room, away from the media and public, when not testifying. Her husband and daughter were there with her. \u201cIt was all very friendly and meant to keep me from distraction and being a distraction,\u201d she says. Because she was not permitted to speak for herself, a media officer answered all questions on behalf of Miller-Saunders. \u201cIt was all a very surreal experience,\u201d she says. University scientists on the commission, by contrast, could speak to the media freely. The decision to muzzle Miller-Saunders was clearly political, says Calvin Sandborn, legal director of the University of Victoria\u2019s Environmental Law Centre. \u201cThere are all sorts of enquiries where experts talk about their findings outside of the hearing room.\u201d Although the approval \u2018rules\u2019 were unwritten, Lake says it became clear over time what stories were likely to be permitted. Under Harper, government-science stories, \u201ccould only reflect economics, and what you could sell, not what you could save or conserve\u201d, she says. Lake\u2019s work environment became a culture of frustration, low morale and fear, she says. Midway through the Harper years, she attended a meeting called by the DFO\u2019s Pacific-region director-general, Paul Sprout. Lake says that Sprout was \u201cfair, and treated staff with integrity\u201d. But on this occasion, \u201che told staff they were not to speak critically about the Harper government, even on their own time\u201d. That atmosphere eventually wore Lake down. She retired several years early, in 2013, explaining that she found the atmosphere at work \u201cuntenable\u201d. Now, she spends her time writing, volunteering and working in a community garden. She would like to have served in Canada\u2019s new government, she says, in a communications role \u201cwhere public employees can actually do their job\u201d. Sprout, now retired from his 34-year career with the DFO, denies having said that employees had to wait until they left their posts before saying anything critical about the government. He confirms, however, that the DFO\u2019s policy was \u201cunequivocal that any approval for doing media interviews would have to be approved by the director-general of communications\u201d, who was based in Ottawa. Sprout says that it was his responsibility to enforce the policy so that communications employees and scientists in his department would not face any repercussions in their personal careers. \u201cI had to make sure that the policies of the department were respected. That was my job,\u201d he says. When he started out as a fisheries biologist in the late 1970s, there was much more flexibility in communications, even when other Conservative governments were in power, Sprout says. During the Harper era, \u201cthere were a lot of limitations on being able to speak\u201d, says Sprout. \u201cIt was difficult to actually get media interviews, even when we wanted to encourage them.\u201d \n               Toxic environment \n             Not all scientists were willing to comply with Canada\u2019s closely controlled communications practices. One senior scientist who flouted the rules was Robie Macdonald, a biogeochemical oceanographer who was at the DFO\u2019s Institute of Ocean Sciences (IOS) in Sidney. He started his career with the DFO in 1973, and had worked under many federal governments. Early in his career, there was no written media policy, but scientists understood that \u201cthey should comment on science and science issues and shouldn\u2019t comment on policy\u201d, he says. The Harper government, however, \u201cmade the process so cumbersome that most media people would not bother talking to you to start with\u201d. Macdonald\u2019s group studied ocean contaminants, and the researchers ran afoul of the administration because they often identified environmental problems, such as the toxic effects of mercury and persistent organic pollutants on wildlife. Under Harper, contaminants research was removed from the DFO\u2019s mandate and toxicologists were fired or transferred, he says. When Macdonald\u2019s work on contaminants was cancelled, he retired early to continue his research, unpaid. Another federal scientist who retired earlier than he had intended \u2014 in part because of media muzzling \u2014 was Ian Stirling, a prominent biologist with Environment and Climate Change Canada, the federal department that conducts research in areas including air quality, ozone, climate, weather, pollution and wildlife. Stirling began studying polar bears in 1970, but such research attracted scrutiny under the Harper government because scientists had shown that the animals were sensitive to climate change and the loss of sea ice. Stirling says that the policies during the Harper administration reminded him of a another regime that had tight control over the media. During the 1970s, he had gone to meetings in Canada that were also attended by Soviet scientists. The visiting researchers would arrive, he says, \u201cwith a KGB guy, who would stand there with no smiles, a scowl on his face and arms crossed\u201d. Stirling still finds it unbelievable that the Canadian government used similar tactics at conferences. In 2012, for example, the Canadian news outlet CBC reported that media minders had shadowed scientists from Environment Canada at a meeting of the International Polar Year in Montreal. Some officials say that the situation was not as bad as it has been portrayed. One manager within Environment Canada spoke to  Nature  on condition of anonymity. He says that the \u201cmuzzling\u201d label used by the media is an overexaggeration. \u201cI think that\u2019s a bit of a coarse way to articulate it. What was done really was a bit more nuanced than that,\u201d he says. The vetting process required approval from such a high level \u201cthat the probability of getting that within a very tight, and very common, media timeline, wasn\u2019t great\u201d, he says. \u201cSometimes we got approval, and sometimes we didn\u2019t. It wasn\u2019t always clear why,\u201d he says. Sometimes even stories about good news wouldn\u2019t get approved. He attributes this to the sheer volume sent \u201cinto the black box of decision-making\u201d. The most profound effect, he says, was that \u201cpeople on both sides stopped trying\u201d. Now, the manager says, media protocols in his office are \u201cback to more or less the old way of doing it\u201d. If a journalist contacts one of his scientists directly, the researcher can do an interview but is required to inform a manager and communications officer beforehand. That\u2019s progress, but it offers less freedom than the DFO\u2019s new directive that scientists can now talk to media first, and let communications staff know later. \n               Government crackdown \n             Some departments are clearly struggling with the transition, as  Nature  found when it requested current media protocols for scientists from several government departments. Parks Canada provided information that had been published in 2006 and was updated in 2012, during the Harper administration. Canadian journalists continue to report difficulties in setting up media interviews with Parks Canada scientists. Some scientists and communications staff worry that a shift in the political winds could bring back restrictive policies. \u201cIt\u2019s hard to say that it wouldn\u2019t happen again. It happens all over the world in totalitarian governments,\u201d Lake says. A former journalist from China says that scientists there are censored, but that the restrictions are often lighter than those imposed on other sectors because science is considered ideologically free and the state censorship agency may not have the capacity to censor every researcher. But he also says that scientists there are generally reluctant to give interviews. \u201cScientists in China are not accustomed to talking to journalists,\u201d he says. The muzzling of scientists is an ongoing concern even in some of the most open countries. The Union of Concerned Scientists (UCS) in Cambridge, Massachusetts, started tracking the issue in the United States during the administration of President George W.\u00a0Bush, when government scientists complained that their data were being altered or suppressed and that they were unable to talk to the media. When President Barack Obama took office in 2009, he vowed to end such practices and ordered government departments to adopt  scientific-integrity policies ; but journalists and scientists still report problems with some agencies. Gretchen Goldman, the lead analyst with the UCS on this issue, says that one thing Canada might learn from the US experience is that it takes time for a culture of transparency to take root. Even after a more open administration assumes power, many staff members remain from the previous government, and have been trained in the more-restrictive policies. \u201cPractices often lag the policy,\u201d she says. It could take years for Canadian scientists to recover from heavy funding cuts, low morale and tight control over communication. Looking back over what happened, Macdonald remembers something his grandmother once told him. \u201cIt takes ten years to make a good garden, but you can wreck it in six months,\u201d he says. \u201cIt\u2019s like that with science.\u201d \n                 Tweet \n                 Facebook \n                 LinkedIn \n                 Weibo \n                 Wechat \n               \n                     Canada\u2019s top scientist faces tough challenge 2015-Dec-22 \n                   \n                     Canadian election brings hope for science 2015-Oct-20 \n                   \n                     Communication breakdown 2015-Apr-01 \n                   \n                     Canadian government accused of destroying environmental archives 2014-Jan-17 \n                   \n                     Canada must free scientists to talk to journalists 2010-Sep-29 \n                   \n                     Blog post: Canada to investigate muzzling of scientists \n                   \n                     Blog post: Canadian scientists fight back against \u2018muzzling\u2019 \n                   \n                     Blog post: Canadian government muzzling scientists \n                   \n                     Grading Government Transparency: Scientists\u2019 freedom to speak (and Tweet) at Federal Agencies \n                   Reprints and Permissions"},
{"file_id": "533022a", "url": "https://www.nature.com/articles/533022a", "year": 2016, "authors": [{"name": "Nicola Nosengo"}], "parsed_as_year": "2006_or_before", "body": "Some researchers believe that machine-learning techniques can revolutionize how materials science is done. It's a strong contender for the geekiest video ever made: a close-up of a smartphone with line upon line of numbers and symbols scrolling down the screen. But when visitors stop by Nicola Marzari's office, which overlooks Lake Geneva, he can hardly wait to show it off. \u201cIt's from 2010,\u201d he says, \u201cand this is my cellphone calculating the electronic structure of silicon in real time!\u201d Even back then, explains Marzari, a physicist at the Swiss Federal Institute of Technology in Lausanne (EPFL), Switzerland, his now-ancient handset took just 40 seconds to carry out quantum-mechanical calculations that once took many hours on a supercomputer \u2014 a feat that not only shows how far such computational methods have come in the past decade or so, but also demonstrates their potential for transforming the way materials science is done in the future. Instead of continuing to develop new materials the old-fashioned way \u2014 stumbling across them by luck, then painstakingly measuring their properties in the laboratory \u2014 Marzari and like-minded researchers are using computer modelling and machine-learning techniques to generate libraries of candidate materials by the tens of thousands. Even data from failed experiments can provide useful input 1 . Many of these candidates are completely hypothetical, but engineers are already beginning to shortlist those that are worth synthesizing and testing for specific applications by searching through their predicted properties \u2014 for example, how well they will work as a conductor or an insulator, whether they will act as a magnet, and how much heat and pressure they can withstand. The hope is that this approach will provide a huge leap in the speed and efficiency of materials discovery, says Gerbrand Ceder, a materials scientist at the University of California, Berkeley, and a pioneer in this field. \u201cWe probably know about 1% of the properties of existing materials,\u201d he says, pointing to the example of lithium iron phosphate: a compound that was first synthesized 2  in the 1930s, but was not recognized 3  as a promising replacement material for current-generation lithium-ion batteries until 1996. \u201cNo one had bothered to measure its voltage before,\u201d says Ceder. At least three major materials databases already exist around the world, each encompassing tens or hundreds of thousands of compounds. Marzari's Lausanne-based Materials Cloud project is scheduled to launch later this year. And the wider community is beginning to take notice. \u201cWe are now seeing a real convergence of what experimentalists want and what theorists can deliver,\u201d says Neil Alford, a materials scientist who serves as vice-dean for research at Imperial College London, but who has no affiliation with any of the database projects. As even the proponents are quick to point out, however, the journey from computer predictions to real-world technologies is not an easy one. The existing databases are far from including all known materials, let alone all possible ones. The data-driven discovery works well for some materials, but not for others. And even after an interesting material is singled out on a computer, synthesizing it in a laboratory can still take years. \u201cWe often know better what we should be making than how to make it,\u201d says Ceder. Still, researchers in this field are confident that there is a trove of compounds waiting to be discovered, which could kick-start innovations in electronics, energy, robotics, health care and transportation.\u201cOur community is putting together a lot of different parts of the puzzle,\u201d says Giulia Galli, a computational materials scientist at the University of Chicago in Illinois. \u201cAnd when they all click into place, materials prediction will become a reality.\u201d \n               Genetic inspiration \n             The idea for this high-throughput, data-driven approach to materials discovery hit Ceder in the early 2000s, when he was at the Massachusetts Institute of Technology (MIT) in Cambridge and found himself inspired by the nearly completed Human Genome Project. \u201cBy itself, the human genome was not a recipe for new treatments,\u201d he says, \u201cbut it gave medicine amazing amounts of basic, quantitative information to start from.\u201d Could materials scientists learn some lessons from geneticists, he wondered. Could they identify a \u201cmaterials genome\u201d that encodes the properties of various compounds in the same way that biological information is encoded in DNA base pairs? If so, he reasoned, that encoding must lie in the atoms and electrons that make up a given material, and in their crystal structure: the way they are arranged in space. In 2003, Ceder and his team first showed 4  how a database of quantum-mechanics calculations could help to predict the most likely crystal structure of a metal alloy \u2014 a key step for anyone in the business of inventing new materials. In the past, these calculations had been long and difficult, even for supercomputers. The machine had to go through an inordinate amount of trial and error to find the 'ground state': the crystal structure and electron configuration in which the energy was at a minimum and all the forces were in equilibrium. But in their 2003 paper 4 , Ceder's team described a shortcut. The researchers calculated the energies of common crystal structures for a small library of binary alloys \u2014 mixes of two different metals \u2014 and then designed a machine-learning algorithm that could extract patterns from the library and guess the most likely ground state for a new alloy. The algorithm worked well, slashing the computer time required for the calculations (see 'Intelligent search'). \u201cThat paper introduced the idea of a public library of materials properties, and of using data mining to fill the missing parts,\u201d says Stefano Curtarolo, who that same year left Ceder's group to start his own laboratory at Duke University in Durham, North Carolina. The idea then gave birth to two separate projects. In 2006, Ceder started the Materials Genome Project at MIT, using improved versions of the algorithm to predict lithium-based materials for electric-car batteries. By 2010, the project had grown to include around 20,000 predicted compounds. \u201cWe started from existing materials and modified their crystal structure \u2014 changing one element here or another one there and calculating what happens,\u201d says Kristin Persson, a former member of Ceder's team who continued to collaborate on the project after she moved to the Lawrence Berkeley National Laboratory in California in 2008. At Duke, meanwhile, Curtarolo set up the Center for Materials Genomics, which focused on research on metal alloys. Teaming up with researchers from Brigham Young University in Provo, Utah, and Israel's Negev Nuclear Research Center, he gradually expanded the 2003 algorithm and library into AFLOW, a system that can perform calculations on known crystal structures and predict new ones automatically 5 . Researchers from outside the original group were getting interested in high-throughput computations as well. One such researcher was chemical engineer Jens N\u00f8rskov, who started using them to study catalysts for breaking down water into hydrogen and oxygen 6  while he was at the Technical University of Denmark in Lyngby, and later expanded the work as director of the SUNCAT Center for the computational study of catalysis at Stanford University in California. Another was Marzari, who was part of a large team developing Quantum Espresso: a program for quantum-mechanics calculations that was launched 7  in 2009. That is the code running on his mobile phone in the video. \n               Materials genomics \n             Still, computational materials science did not become mainstream until June 2011, when the White House announced the multimillion-dollar Materials Genome Initiative (MGI). \u201cWhen people at the White House became familiar with Ceder's work they got very excited,\u201d says James Warren, a materials scientist at the US National Institute of Standards and Technology and executive secretary of the MGI. \u201cThere was a general awareness that computer simulations had got to the point where they could have a real impact on innovation and manufacturing,\u201d he says \u2014 not to mention the 'genomics' name, \u201cwhich was evocative of something grand.\u201d Since 2011, the initiative has invested more than US$250 million into software tools, standardized methods to collect and report experimental data, centres for computational materials science at major universities and partnerships between universities and the business sector for research on specific applications. But it is unclear how far this largesse has actually advanced the science. \u201cThe initiative brought a lot of good things, but also some re-branding,\u201d says Ceder. \u201cSome groups started calling their research genomics this and genomics that, even though it had little to do with it.\u201d One thing the MGI definitely did do, however, was to help Ceder and others realize their vision of an online database of materials properties. In late 2011, Ceder and Persson relaunched their Materials Genome Project as the Materials Project \u2014 having been asked by the White House to give up the 'genome' label to avoid confusion with the national effort. The following year, Curtarolo posted his own database, called AFLOWlib, based on the software he had developed at Duke 8 . And in 2013, Chris Wolverton, a materials researcher at Northwestern University in Evanston, Illinois, launched the Open Quantum Materials Database (OQMD) 9 . \u201cWe borrowed the general idea from the Materials Project and AFLOWlib,\u201d says Wolverton, \u201cbut our software and data are homegrown.\u201d All three of these databases share a core of around 50,000 known materials taken from a widely used experimental library, the Inorganic Crystal Structure Database. These are solids that have been created at least once in a laboratory and described in a paper, but whose electronic or magnetic properties may have never been fully tested; they are the starting point from which new materials can be derived. Where the three databases differ is in the hypothetical materials they include. The Materials Project has relatively few, starting with some 15,000 computed structures derived from Ceder's and Persson's  research on lithium batteries . \u201cWe only include them in the database if we're confident the calculations are accurate, and if there is a reasonable chance that they can be made,\u201d says Persson, who is now director of the Materials Project and has a joint affiliation with the University of California, Berkeley. Another 130,000 or so entries are structures predicted by the Nanoporous Materials Genome Center at the University of Minnesota in Minneapolis. The latter focuses on zeolites and  metal\u2013organic frameworks : sponge-like materials with regularly repeating holes in their crystal structures that can trap gas molecules and could be used to store methane or carbon dioxide. AFLOWlib is the largest database, featuring more than a million different materials and about 100 million calculated properties. That's because it also includes hundreds of thousands of hypothetical materials, many of which would exist for only a fraction of a second in the real world, says Curtarolo. \u201cBut it pays off when you want to predict how a material can actually be manufactured,\u201d he says. For example, he is using data from AFLOWlib to study why some alloys can form metallic glass \u2014 a peculiar form of metal with a disordered microscopic structure that gives it special electric and magnetic properties. It turns out that the difference between good glass formers and bad ones depends on the number and energies of unstable crystal structures that 'compete' with the ground state while the alloy cools down 10 . Wolverton's OQMD includes around 400,000 hypothetical materials, calculated by taking a list of crystal structures commonly observed in nature and 'decorating' them with elements chosen from almost every part of the periodic table 9 . It has a particularly wide coverage of perovskites \u2014 crystals that often display attractive properties such as superconductivity and that are being developed for use in solar cells as microelectronics. As the name suggests, this project is the most open of the three: users can download the entire database, not just individual search results, onto their computer. All of these databases are works in progress, and their curators still spend a good share of their time adding more compounds and refining the calculations \u2014 which, they admit, are far from perfect. The codes tend to be quite good at predicting whether a crystal is stable or not, but less good at predicting how it absorbs light or conducts electricity \u2014 to the point of sometimes making a semiconductor look like a metal. Marzari notes that even for battery materials, an area in which computational materials science is having its best success stories, standard calculations still have an average error of half a volt, which makes a lot of difference in terms of performance. \u201cThe truth is, some errors come with the theory itself: we may never be able to correct them,\u201d says Curtarolo. Each group is developing its own techniques to adjust the calculations and make up for these systematic errors. But in the meantime they are already doing science with the data \u2014 and so are users from other groups. The Materials Project has identified several promising cathodes that may work better than existing ones in lithium batteries 11 , as well as metal oxides that could improve the efficiency with which solar cells capture sunlight and turn it into energy 12 . And earlier this year, researchers from Trinity College Dublin used the AFLOWlib database to predict 20 Heusler alloys, a class of magnets that can be used for sensors or computer memories, and managed to synthesize two of them, confirming that their magnetic properties are very close to the predictions (see  go.nature.com/v7djio ). \n               European expansion \n             Materials genomics has also crossed over to Europe \u2014 although usually by other names. Switzerland, for example, has created MARVEL, a network of institutes for computational materials science with the EPFL as its lead and Marzari as director. Using a new computational platform 13 , he is creating a database called Materials Cloud that he is using to search for  'two-dimensional' materials, such as graphene , that are made from just a single layer of atoms or molecules. Such materials could be used in applications ranging from nanoscale electronics to biomedical devices. To find good candidates, Marzari is subjecting more than 150,000 known materials to what he calls 'computational peeling': calculating how much energy it would take to separate a single layer from the surface of an ordinary crystal. By the time the database is ready for public release later this year, he expects that preliminary runs will have yielded some 1,500 potential two-dimensional structures that can then be tested in experiments. A few kilometres away in Sion, high in the Swiss Alps, computational chemist Berend Smit has set up another EPFL centre that develops algorithms for predicting hundreds of thousands of nanoporous zeolites and metal\u2013organic frameworks. Other algorithms \u2014 including one that scans for certain pore shapes using techniques derived from facial-recognition software \u2014 then seek out the best candidates for absorbing carbon dioxide from the flues of fossil-fuel power plants 14 . Smit's work also shows that materials genomics can bring bad news. Many researchers had hoped to use nanoporous materials to build car tanks that could store more methane in less space. But after screening more than 650,000 computed materials, Smit's group concluded that most of the best ones have already been made 15 . New ones could bring only minor improvements, and energy targets currently set by US agencies \u2014 which bet on major technological improvements in methane storage \u2014 may be unrealistic. As intriguing as these examples are, there are still many hurdles to overcome before materials genomics can live up to its promises. One of the largest is that computer simulations still give few clues on how an interesting material can be made in a lab \u2014 let alone mass produced. \u201cWe come up with interesting ideas for new compounds all the time,\u201d says Ceder. \u201cSometimes it takes two weeks to make it. Other times we still can't make it after six months, and we don't know whether we haven't done the right thing, or it just can't be made.\u201d Both Ceder and Curtarolo are trying to develop machine-learning algorithms to extract rules from known manufacturing processes to guide the synthesis of compounds. Another limitation is that materials genomics has been hitherto applied almost exclusively to what engineers call functional materials \u2014 compounds that can perform a task such as absorbing light in a solar cell or letting electrical current pass in transistor. But the technique does not lend itself well to studying structural materials, such as steel, that are needed to build, for example, aircraft wings, bridges or engines. This is because mechanical properties such as a material's springiness and hardness depend on how it is processed \u2014 something that quantum-mechanical codes by themselves can not describe. Even in the case of functional materials, current computer codes work well only for perfect crystal structures \u2014 which are only a small part of the materials realm. \u201cThe most interesting materials of the future will probably be assembled at the microscopic level in creative ways,\u201d says Galli. They may be assemblies of nanoparticles, crystals with strategically placed defects in their structures, or heterogenous materials made by intertwining different compounds and phases. To predict such materials, says Galli, \u201cyou need to calculate many properties at once and how the system will evolve in time and at specific temperatures\u201d. There are methods to do that, she says, \u201cbut they are still too computationally expensive to be used in high-throughput studies\u201d. In the short term, more data exchange with experiments can give computations a reality check and help to refine them. To that end, Ceder is working with a group at MIT on software that reads papers in experimental materials science and automatically extracts information on crystal structures in a standard format. \u201cWe plan to begin adding these data to the Materials Project in a few months,\u201d he says. And in the long run, some help will come from  Moore's law : as computational power continues to increase, some techniques that are out still of reach for current computers may soon become viable. \u201cWe've moved away from the artisanal era of computational materials science, and into the industrial phase,\u201d says Marzari. \u201cWe can now create assembly chains of simulations, put them to work, and explore problems in totally new ways.\u201d No computationally predicted material is on the market just yet. \u201cBut let's talk again in ten years,\u201d says Galli, \u201cand I think there will be many.\u201d \n                 Tweet \n                 Follow @NatureNews \n               \n                     The chips are down for Moore\u2019s law 2016-Feb-09 \n                   \n                     The super materials that could trump graphene 2015-Jun-17 \n                   \n                     Materials science: The hole story 2015-Apr-08 \n                   \n                     The top 100 papers 2014-Oct-29 \n                   \n                     The rechargeable revolution: A better battery 2014-Mar-05 \n                   \n                     SUNCAT Center \n                   \n                     AFLOWlib \n                   \n                     The Materials Project \n                   \n                     Open Quantum Materials Database \n                   \n                     MARVEL \n                   Reprints and Permissions"},
{"file_id": "533456a", "url": "https://www.nature.com/articles/533456a", "year": 2016, "authors": [{"name": "Chelsea Wald"}], "parsed_as_year": "2006_or_before", "body": "By scouring the remains of early loos and sewers, archaeologists are finding clues to what life was like in the Roman world and in other civilizations. Some 2,000 years ago, a high-ceilinged room under of one of Rome's most opulent palaces was a busy, smelly space. Inside the damp chamber, a bench, perforated by about 50 holes the size of dinner plates, ran along the walls. It may have supported the bottoms of some of the lowest members of Roman society. Today, the room is shut off to the public, but archaeologists Ann Koloski-Ostrow and Gemma Jansen had a rare chance to study the ancient communal toilet on the Palatine Hill in 2014. They measured the heights of the benches' stone base (a comfortable 43 centimetres), the distances between the holes (an intimate 56 cm), the drop down into the sewer below (a substantial 380 cm at its deepest). They speculated about the mysterious source of the water that would have flushed the sewer (perhaps some nearby baths). Graffiti outside the entryway suggested long queues, in which people had enough time to write or carve their messages before taking a turn on the bench. The underground location, combined with the plain red-and-white colour scheme on the walls, implied a lower class of user, possibly slaves. In 1913, when Italian excavator Giacomo Boni excavated this room, toilets were an unmentionable topic. In his report, he seems to mistake the remains of the holey benches for something much more sensational: part of an elaborate mechanism that, he speculated, would have pumped water and provided power for the palace above. Boni's prudish sensibilities wouldn't let him recognize what was before his very eyes, says Jansen. \u201cHe couldn't imagine it was a toilet.\u201d A century later, toilets are no longer such an unacceptable research topic. Koloski-Ostrow, at Brandeis University in Waltham, Massachusetts, and Jansen, an independent archaeologist based in the Netherlands, are among a growing number of archaeologists, infectious-disease specialists and other experts who are shining light on the  lost loos of history , from ancient Mesopotamia to the Middle Ages, with a particular focus on the Roman world. Their investigations have provided a new way to learn about the diets, diseases and habits of past populations, especially those of the lower classes, which have received scant attention from archaeologists. Researchers have inferred that Roman residents ventured into their toilets with some trepidation, in part because of superstition and also because of very real dangers from rats and other vermin lurking in the sewers. And although ancient Rome is famous for its sophisticated plumbing systems, modern studies of old excrement suggest that its sanitation technologies were not doing much for the residents' health. \u201cToilets have a lot to tell us about \u2014 far more than how and where people went to the bathroom,\u201d says Hendrik Dey, an archaeologist at Hunter College in New York. \n               Queen of latrines \n             Although studies of ancient latrines are no longer off limits, they do take a certain amount of fortitude. \u201cYou have to have a very strong sense of self and of humour to work on this topic because one who works on it is going to get ribbed by friends and enemies,\u201d says Koloski-Ostrow. She got started on the topic nearly a quarter of a century ago, when classicist Nicholas Horsfall called her over in the library at the American Academy in Rome. \u201cLatrines. Roman latrines,\u201d he whispered conspiratorially. \u201cNo one has done them properly.\u201d She took up that challenge, and now, she says, \u201cI am known widely on my campus as 'the queen of latrines'.\u201d The invention of some of the first simple toilets is credited to Mesopotamia in the late fourth millennium  bc 1 . These non-flushing affairs were pits about 4.5 metres deep, lined with a stack of hollow ceramic cylinders about 1 metre in diameter. Users would have sat or squatted over the toilet, and the excrement would have stayed inside the cylinders with the liquids seeping outwards through perforations in the rings. Until recently, scholars had little interest in these toilets, says archaeologist Augusta McMahon at the University of Cambridge, UK. \u201cArchaeologists in Mesopotamia have looked at them like, 'this is a problem: it's a pit that's cut into the stuff I'm really interested in'.\u201d As far as she knows, no one has carefully excavated a Mesopotamian toilet yet \u2014 something she's hoping to do when she finds a good candidate and funding. Mesopotamians themselves also seemed to show little enthusiasm for this revolutionary technology. Although the toilets would have been convenient to use, and cheap and easy to install, they were uncommon, says McMahon, who surveyed the number of latrines in different neighbourhoods for a chapter in a book published last year 1 . \u201cThe number of houses that have toilets is very, very low \u2014 one out of five or two out of five,\u201d she says. Everyone else probably used a chamber pot or simply squatted in the fields. So the health benefits of the technology would have been limited, McMahon says. Although the pit toilets would have successfully separated people from their waste \u2014 the measure of a good sanitation system because it prevents the faecal\u2013oral spread of disease \u2014 studies by the US Agency for International Development say that some 75% of a population must have access before there are widespread improvements in health. About 1,000 years later, the  Minoans on the island of Crete  in the Mediterranean improved the toilet by adding the capacity to flush \u2014 although only for the elite. The first known example 2  was in the palace at Knossos, says Georgios Antoniou, a Greek architect who has studied ancient sanitation in that country. Water was used to wash the waste from the toilet into the sewer system of the palace. From there, toilet technology took off. In the first millennium  bc , ancient Greeks of the Classical period and, especially, the succeeding Hellenistic period developed large-scale public latrines \u2014 basically large rooms with bench seats connected to drainage systems \u2014 and put toilets into ordinary middle-class houses. \u201cThe society had become more prosperous, and they were dealing more with comfort in everyday living,\u201d Antoniou says. The Romans were unprecedented in their adoption of toilets. Around the first century  bc , public latrines became a major feature of Roman infrastructure, much like bathhouses, says Koloski-Ostrow. And nearly all city dwellers had access to private toilets in their residences. Nonetheless, archaeologists know very little about how these toilets worked and what people thought of them, she says. One reason is that in Roman times, few people wrote about toilets, and when they did, they were often satirical, making it hard to interpret their meaning. But Koloski-Ostrow and Jansen show that it is worthwhile taking the topic seriously. For a forthcoming book on toilets in the Roman capital, they and some two dozen other archaeologists have analysed more than 60 toilets scattered throughout the city, most of which had not been described before. That includes toilets for guards in the city wall, and a two-person toilet in an apartment block. \u201cI guess it will be news to a lot of archaeologists who have worked on all kinds of Roman buildings that some of these buildings actually had toilet facilities,\u201d Koloski-Ostrow says. Roman public latrines looked much like their Greek predecessors: rooms lined with stone or wooden bench seats positioned over a sewer. The toilet holes are round on top of the bench, and a narrower slit extends forward and down over the edge in a keyhole shape. These slits probably allowed users to insert a sponge-tipped stick for cleaning. Small gutters often run parallel to the seats along the ground; researchers suspect that people probably washed the sponges in water running through those gutters. There are no signs of barriers between the toilet seats, but people probably had a measure of privacy thanks to their long garments and the limited windows, says Koloski-Ostrow. Private toilets were different, Jansen says. In residences, commodes were often in or near kitchens, which was practical because they were also used to dispose of food scraps. Although people flushed the toilets with buckets of water, the loos were rarely connected to sewers. When the pits filled up, they were probably emptied, either into gardens or fields outside the town, Jansen says. Sewers \u2014 long thought to be a crowning achievement of Roman civilization \u2014 were in fact less widespread than once thought and might not have been very effective, says Koloski-Ostrow. In a book published last year 3 , she considered whether Roman sewers would have adhered to any of the modern principles of sanitation engineering, including regular aeration and features to control the deposition of solid waste, which would reduce the stench as well as improve flow. To a great extent, the sewers didn't meet the standards. Her own recent explorations of the Cloaca Maxima, the great sewer under Rome, revealed that some channels could get completely blocked with silt in less than a year. At the very least, they would have required regular cleaning \u2014 dirty and dangerous work. And Roman toilets also had a number of deficiencies. One major problem was that there were no traps \u2014 or S-shaped bends \u2014 in the pipes beneath toilets to keep out flies. Environmental archaeologists Mark Robinson at the University of Oxford and Erica Rowan, now at the University of Exeter, UK, analysed the well-preserved contents of a closed sewer that was connected to several toilets in an apartment block in Herculaneum, a Roman city destroyed by an eruption of Mount Vesuvius. Among the faecal matter and other rubbish thrown down there, Robinson found lots of fragile mineralized fly pupae. With easy access to human waste, flies could have transferred faecal matter and pathogens to people. To look at the benefits of ancient sanitation systems, palaeopathologist Piers Mitchell at the University of Cambridge analysed published studies of parasites found at archaeological sites from several eras 4 . Contrary to his expectations, the prevalence of intestinal parasites such as roundworm and whipworm \u2014 which cause problems such as malnutrition \u2014 did not decrease from the Bronze and Iron ages to the Roman period; they gradually rose. That might be because the Romans used human waste as fertilizer, which would have transferred the parasite eggs to food. \u201cToilets and sewers and things didn't seem to improve the intestinal health of the Roman population,\u201d he says. \n               Diet details \n             The practice of throwing kitchen rubbish down toilets was unhygienic for the ancient Romans, but the remnants of that refuse are now a rich source of information. Rowan was surprised by the quality and variability of the foods in the Herculaneum sewer, especially because it was connected to an apartment complex that housed a large number of mostly poorer people. \u201cWe always think that anyone non-elite in the ancient world is not eating a very diverse or interesting diet,\u201d she says. But the evidence from Herculaneum shows that people across the class spectrum were eating dozens of different types of food, most commonly figs, eggs, olives, grapes and shellfish. They flavoured their meals with seasonings such as dill, mint, coriander and mustard seeds 5 . \u201cIt would be quite healthy, and they'd be getting all their essential nutrients.\u201d Rowan also used the sewer contents to glean insights into the broader food and energy economy. The large amount of kitchen scraps suggested that the residents cooked more at home than previously thought 5 . From the quantity of fish bones found, she concluded that the regional fish trade was probably much larger than scholars had suspected 6 . Such discoveries are part of a broader trend in Roman archaeology, says Dey. Until recently, most scholars focused on the monumental structures occupied by elite residents. But attention has shifted to lower down the class ranking. \u201cRoman archaeologists started to realize that you can't understand how a society works if you only study the 1%,\u201d he says. \u201cThe study of toilets is part of the broader effort to understand how Roman society worked, which includes \u2014 especially \u2014 studying how the non-glamorous parts of society worked.\u201d For Koloski-Ostrow and Jansen, latrines provide a window onto the beliefs of that society. Romans perceived demons everywhere, and some Roman literature refers to ones that lurked in toilets. \u201cThe demons can cast a spell on you, and when you have this spell you die or you get sick,\u201d Jansen says. The Roman writer Claudius Aelianus tells a story in his  De Natura Animalium  about an octopus that swam up through a drain in a toilet and ate the pickled fish in the pantry night after night. That story is probably apocryphal, but rodents, insects and other creatures could have lurked in toilets and invaded homes. And excrement-filled water could have flowed upwards during flooding. Explosive gases might also have been a problem. \u201cYou might walk in and actually see a flame burst out of one of those holes because of the methanic gases that built up in the sewer underneath the toilet,\u201d Koloski-Ostrow speculates. This pervasive fear of toilets could explain the mystery of why there's less graffiti inside public latrines than in the rest of the Roman world, Jansen says. Nobody wanted to spend more time there than necessary. The same fear could also explain why many latrines have small shrines to the goddess Fortuna. Jansen argues that she was thought to protect toilet-users from illness-causing demons, as well as the other bad things that could happen there 7 . More discoveries about ancient lifestyles will come as researchers expand their toilet studies to other parts of the globe. Rowan is studying a site in Turkey, and Mitchell has recently examined evidence from a 2,000-year-old toilet in China. But progress has been slow and archaeologists are not rushing into toilet studies. Although the topic is no longer considered fringe, funding is hard to come by, and Mitchell says that \u201cno one else seems to be that bothered\u201d to work on it. One reason could be that the lack of written sources and the limited physical evidence make it daunting. But for researchers such as Koloski-Ostrow, the recent work raises all kinds of questions about ancient societies. Did women use public toilets? Were they chatty, social places or silent? What were the foreign influences on Roman toilets, and how did the toilet culture propagate between the capital and the distant states? These questions will be hard to answer, she says, but asking them no longer seems as weird as when she started. Rowan agrees: toilets have finally gone mainstream. \u201cIf somebody finds a latrine now, they know to sample it, to excavate it carefully. They know there's going to be a lot of value in it, as opposed to being, like, oh, it's just a toilet.\u201d \n                 Tweet \n                 Facebook \n                 LinkedIn \n                 Weibo \n                 Wechat \n               \n                     Ancient DNA reveals how wheat came to prehistoric Britain 2015-Feb-26 \n                   \n                     Ancient bones show signs of struggle with coeliac disease 2014-Apr-30 \n                   \n                     Environment: Waste production must peak this century 2013-Oct-30 \n                   \n                     Archaeology: The milk revolution 2013-Jul-31 \n                   \n                     Unearthed: ancient sect's extreme latrine 2006-Nov-13 \n                   \n                     http://www.nature.com/news/ancient-dna-reveals-how-wheat-came-to-prehistoric-britain-1.17010 \n                   \n                     http://www.nature.com/news/the-greatest-vanishing-act-in-prehistoric-america-1.18700 \n                   \n                     http://www.nature.com/news/fishing-for-the-first-americans-1.18334 \n                   \n                     http://www.nature.com/news/underwater-archaeology-hunt-for-the-ancient-mariner-1.9880 \n                   \n                     http://www.nature.com/news/environment-waste-production-must-peak-this-century-1.14032 \n                   \n                     http://www.nature.com/news/archaeology-the-milk-revolution-1.13471 \n                   \n                     http://www.nature.com/news/ancient-bones-show-signs-of-struggle-with-coeliac-disease-1.15128 \n                   Reprints and Permissions"},
{"file_id": "533306a", "url": "https://www.nature.com/articles/533306a", "year": 2016, "authors": [{"name": "XiaoZhi Lim"}], "parsed_as_year": "2006_or_before", "body": "Materials scientists are creating next-generation mixtures with remarkable properties. At first glance, the machine seems to be building a miniature cityscape. A ring of nozzles fires four jets of powdered metal into a downward-pointed laser beam, which fuses the colliding grains in a bright orange glow. The mixed grains then solidify on the growing tip of a small pillar of metal alloy. Once the pillar is 1\u20132 centimetres high, the platform that holds it shifts to the side, and the machine starts to build another one right next door. The result looks like a forest of toy skyscrapers. In reality, these towers, generated at the Ames Laboratory in Iowa, reflect a major shift in how researchers think about alloys. The standard recipe \u2014 used for technologies ranging from ancient swords and arrowheads to modern jet-engine turbines \u2014 is to take a useful metal and mix in a pinch of this or a touch of that to improve its properties. One classic example is the  addition of carbon to iron to make steel . But the machine at Ames is making experimental samples of 'high-entropy' alloys, which consist of four, five or more elements mixed together in roughly equal ratios. This deceptively simple recipe can yield alloys that are lighter and stronger than their conventional counterparts, while being much more resistant to corrosion, radiation or severe wear. Eventually, researchers hope, this approach could even produce alloys that have magnetic or electrical properties never seen before, leading to whole new generations of technology. \u201cWe have almost explored everything for traditional alloys,\u201d says Yong Zhang, a materials scientist with the State Key Laboratory for Advanced Metals and Materials at the University of Science and Technology Beijing. \u201cFor high-entropy alloys, the science is very new,\u201d he says \u2014 so new that no such alloy has yet made the leap from lab to market. But some researchers are working to make that happen, eyeing potential applications that range from high-temperature furnace linings to ultralightweight aerospace materials. And the field has attracted funding from research agencies in China, Europe, the United States and elsewhere. \u201cWe're not talking about a narrow class of materials, but an extremely broad philosophy on how to combine elements,\u201d says Daniel Miracle, a materials scientist at the Air Force Research Laboratory at the Wright-Patterson Air Force Base in Ohio. \u201cThe opportunity to find something new and exciting is very high.\u201d Last year, he and his colleagues estimated 1  that almost 313,560 different alloys can be made by combining exactly equal proportions of 3, 4, 5 or 6 metallic elements from a set of just 26. More possibilities can come from varying the proportions or expanding the choice of elements. But not every combination is a winner, says Easo George, a materials engineer at Ruhr University Bochum in Germany. Scientists are still learning what works and what doesn't. Still, he says, \u201cthe space available for exploration is really huge, and we have only looked at a small portion of the Universe\u201d. The idea for high-entropy alloys first struck metallurgist Jien-Wei Yeh in 1995, while he was driving through the Taiwanese countryside. The physics of conventional alloys was already well understood, says Yeh, who works at National Tsing Hua University in Hsinchu, Taiwan. At the atomic level, pure metals have a regular crystal structure that stacks layer upon layer of identical atoms. Often, these layers slip past each other easily, which makes the metal too soft to be useful. That is why pure gold is rarely used in jewellery: it cannot survive much wear and tear. But if a metalsmith mixes in an element with a different atomic size, the interloper will randomly disrupt the layers and reduce their tendency to slip, which creates a much harder alloy. The correct choice of compounds can enable metallurgists to tailor other properties as well, such as corrosion resistance or melting point. But Yeh was also well aware of the potential complications. If too much of the alloying element were added, for example, its atoms could stop falling randomly among the layers, and instead might start to alternate with the primary metal atoms in a more regular pattern, producing a compound that is weak and brittle. And that gave him an idea, says Yeh: instead of starting with one primary material and mixing in small quantities of one or two elements, why not stir together similar quantities of four or five elements \u2014 or even more? The number of possible ways for the different atoms to arrange themselves would expand dramatically, leading to a tendency towards disorder, or 'high entropy', that would overwhelm any bias favouring a regular crystal-lattice structure. Because each of the randomly mixed elements would be a different size, the atoms would become lodged into place and less able to slide past each other, creating a very hard material (see 'Tough and strong'). It was a strange idea at the time, and Yeh gave it a low priority even in his own laboratory; it wasn't until 2004 that his research group first reported success at mixing five to ten elements together 2 , producing alloys that were considerably harder than stainless steel. Another group independently announced similar findings around the same time 3 . \n               Spoilt for choice \n             The field began to move rapidly after that. In 2009, Zhang described 4  an alloy composed of cobalt, chromium, copper, iron, nickel and aluminium that was more than 14 times stronger than pure aluminium, but had nearly 3 times the ductility \u2014 a measure of a metal's ability to stretch without breaking. In 2011, Yeh devised 5  a cobalt, chromium, iron, nickel, aluminium and titanium alloy that is twice as resistant to damage from friction as conventional wear-resistant steels. And in 2014, George and his team concocted 6  a cobalt, chromium, iron, manganese and nickel alloy that can be cooled below liquid-nitrogen temperatures without becoming brittle. The material could be useful for cryogenic vessels, natural-gas pipelines and other low-temperature applications, such as spacecraft. Often, however, the best thing about high-entropy alloys \u2014 the massive number of possibilities \u2014 can also be researchers' biggest challenge. With more than 80 metallic elements in the periodic table, says Miracle, \u201cthere are way too many alloys to test, and not enough time\u201d. For his own work on high-entropy alloys for aircraft engines and aeroplane frames, he is searching for materials that are lighter, more resistant to corrosion and better able to maintain their strength at high temperatures than anything currently available. To cope with the abundance of choices, Miracle is focusing on elements such as niobium, tantalum and chromium, which have high melting points to begin with. Another strategy is to try to replicate the characteristics of alloys that are known to work well. For example, some steels are not just a random mix of atoms, but instead contain small nodules of compounds that form when the steels cool rapidly. Although such a composite structure is less stable than a random mixture, it confers high ductility to the steels. Cem Tasan, a metallurgist at the Massachusetts Institute of Technology in Cambridge, has used that knowledge to blend iron, manganese, cobalt and chromium into a high-entropy alloy 7  that is simultaneously extremely hard and highly ductile \u2014 traits that once seemed completely incompatible. \u201cIt doesn't make sense to abandon all that we know,\u201d he says. The mini-skyscrapers at Ames represent another, more systematic approach. The machine can build up to 30 pillars in less than an hour with a slightly different mix of raw materials in each, so that researchers can test the properties of many alloys quickly. Matthew Kramer, a materials scientist at Ames, leads a project to find high-entropy alloys that can withstand high temperatures and resist corrosion, which could help power plants to operate at higher temperatures and become more efficient. Aiding his team is Duane Johnson, a theorist at Ames who, in 1995, developed an algorithm to predict the properties of conventional alloys before they are made 8 . In 2015, he expanded the code to work for high-entropy alloys 9 . Johnson's algorithm assesses how much one element is attracted to or repelled by another, and then using that information to predict whether a mixture of elements will form a compound, a solid solution or a mixture of both. That enables Kramer's team to identify which alloys might be worth investigating. The experimental results are then fed back into the algorithm to validate and improve the code. There are a number of obstacles to overcome to move the high-entropy-alloy field forward. So far, the emphasis has been on improving structural properties, such as strength. But there has been much less work on developing alloys with specific 'functional' properties, including conductivity or response to a magnetic field \u2014 a development that would enable applications in areas such as refrigeration and electronics. Nonetheless, there are many possibilities still to be explored, especially as researchers start to extend the concept well beyond its original definition. Zhang, for example, is mixing metals with elements such as carbon, nitrogen and silicon in an effort to develop new high-temperature ceramics for use in solar-power applications. And some, including Tasan and Yeh, have begun to experiment with alloys that contain elements mixed in high, but unequal proportions. Their preliminary findings show that many of these still have all the properties that make high-entropy alloys desirable in the first place. For example, Yeh has prepared a range of hard materials that consist of 50% nitrogen, carbon or oxygen, along with a mix of other elements, such as aluminium, silicon or titanium, that can withstand scratching. They could be used as long-lasting coatings for machine parts and cutting tools. \u201cWe now have this rich, rich field for exploration,\u201d says George. \n                 Tweet \n                 Follow @NatureNews \n               \n                     Can artificial intelligence create the next wonder material? 2016-May-04 \n                   \n                     Blended structure makes steel light yet sturdy 2015-Feb-04 \n                   \n                     Glassy metal set to rival steel 2011-Jan-07 \n                   \n                     Ames Laboratory \n                   Reprints and Permissions"},
{"file_id": "533164a", "url": "https://www.nature.com/articles/533164a", "year": 2016, "authors": [{"name": "M. Mitchell Waldrop"}], "parsed_as_year": "2006_or_before", "body": "As cyberattacks grow ever more sophisticated, those who defend against them are embracing behavioural science and economics to understand both the perpetrators and their victims. Say what you will about cybercriminals, says Angela Sasse, \u201ctheir victims rave about the customer service\u201d. Sasse is talking about ransomware: an extortion scheme in which hackers encrypt the data on a user's computer, then demand money for the digital key to unlock them. Victims get detailed, easy-to-follow instructions for the payment process (all major credit cards accepted), and how to use the key. If they run into technical difficulties, there are 24/7 call centres. Noah Baker talks to the computer scientist on a quest to stop spammers \u201cIt's better support than they get from their own Internet service providers,\u201d says Sasse, a psychologist and computer scientist at University College London who heads the Research Institute in Science of Cyber Security. That, she adds, is today's cybersecurity challenge in a nutshell: \u201cThe attackers are so far ahead of the defenders, it worries me quite a lot.\u201d Long gone are the days when computer hacking was the domain of thrill-seeking teenagers and college students: since the mid-2000s, cyberattacks have become dramatically more sophisticated. Today, shadowy, state-sponsored groups launch exploits such as the 2014 hack of Sony Pictures Entertainment and the 2015 theft of millions of records from the US Office of Personnel Management, allegedly sponsored by North Korea and China, respectively. 'Hacktivist' groups such as Anonymous carry out ideologically driven attacks on high-profile terrorists and celebrities. And a  vast criminal underground  traffics in everything from counterfeit Viagra to corporate espionage. By one estimate, cybercrime costs the global economy between US$375 billion and $575 billion each year 1 . Increasingly, researchers and security experts are realizing that they cannot meet this challenge just by building higher and stronger digital walls around everything. They have to look inside the walls, where  human errors , such as choosing a weak password or clicking on a dodgy e-mail, are implicated in nearly one-quarter of all cybersecurity failures 2 . They also have to look outwards, tracing the underground economy that supports the hackers and finding weak points that are vulnerable to counterattack. \u201cWe've had too many computer scientists looking at cybersecurity, and not enough psychologists, economists and human-factors people,\u201d says Douglas Maughan, head of cybersecurity research at the US Department of Homeland Security. That is changing \u2014 fast. Maughan's agency and other US research funders have been increasing their spending on the human side of cybersecurity for the past five years or so. In February, as part of his fiscal-year 2017 budget request to Congress, US President Barack Obama proposed to spend more than $19 billion on federal cybersecurity funding \u2014 a 35% increase over the previous year \u2014 and included a research and development plan that, for the first time, makes human-factors research an explicit priority. The same sort of thinking is taking root in other countries. In the United Kingdom, Sasse's institute has a multiyear, \u00a33.8-million (US$5.5-million) grant from the UK government to study cybersecurity in businesses, governments and other organizations. Work from the social sciences is providing an unprecedented view of how cybercriminals organize their businesses \u2014 as well as better ways to help users to choose an uncrackable yet memorable password. The fixes are not easy, says Sasse, but they're not impossible. \u201cWe've actually got good science on what does and doesn't work in changing habits,\u201d she says. \u201cApplying those ideas to cybersecurity is the frontier.\u201d \n               Know your audience \n             Imagine that it is the peak of a harried work day, and a legitimate-looking e-mail lands in your inbox: the company's computer team has detected a security breach, it says, and everyone needs to run an immediate background scan for viruses on their machines. \u201cThere's a tendency to just click 'accept' without reading,\u201d says Adam Joinson, a social psychologist who studies online behaviour at the University of Bath, UK. Yet the e-mail is a fake \u2014 and that hasty, exasperated click sends malware coursing through the company network to steal passwords and other data, and to convert everyone's computers into a zombie 'botnet' that fires off more spam. The attackers, it seems, have a much better grasp on user psychology than have the institutions meant to defend them. In the scenario above, the success of the attack relies on people's instinctive deference to authority and their lowered capacity for scepticism when they're busy and distracted. Companies, by contrast, tend to impose security rules that are disastrously out of sync with how people work. Take  the ubiquitous password , by far the simplest and most common way for computer users to prove their identity 3 . One study 4 , released in 2014 by Sasse and others, found that employees of the US National Institute of Standards and Technology (NIST), headquartered in Gaithersburg, Maryland, averaged 23 'authentication events' per day \u2014 including repeated logins to their own computers, which locked them out after 15 minutes of inactivity. Such demands represent a substantial drain on employees' time and mental energy \u2014 especially for those who try to follow the standard password guidelines. These insist that people use a different password for each application; avoid writing passwords down; change them regularly; and always use a hard-to-guess mix of symbols, numbers and uppercase and lowercase letters. So people resort to subversion. In another systematic study of password use in the real world 5 , Sasse and her colleagues documented the ways in which workers at a large multinational organization side-stepped the official security requirements without (they hoped) being totally reckless. The employees' methods \u2014 writing down a list of passwords, for example, or transferring files between computers using unencrypted flash drives \u2014 would be familiar in most offices, but essentially created a system of 'shadow security' that kept the work flowing. \u201cMost people's goal is not to be secure, but to get the job done,\u201d says Ben Laurie, who studies security compliance at Google Research in London. \u201cAnd if they have to jump through too many hoops, they will say, 'To hell with it.'\u201d \n               Top 10 most common passwords 2015 \n               Researchers have uncovered multiple ways to ease this impasse between workers and security managers. Lorrie Cranor directs the CyLab Usable Privacy and Security Laboratory at Carnegie Mellon University in Pittsburgh, Pennsylvania \u2014 one of several groups worldwide that are looking at ways to make password policies more human-compatible. \u201cWe got started on this six or seven years ago, when Carnegie Mellon changed its password policy to something really complicated,\u201d says Cranor, who is currently on leave from the university to serve as chief technologist at the US Federal Trade Commission in Washington DC. The university said that it was trying to conform to standard password guidelines from NIST. But when Cranor investigated, she found that these guidelines were based on educated guesses. There were no data to base them on, because no organization wanted to reveal its users' passwords, she says. \u201cSo we said, 'This is a research challenge.'\u201d Cranor and her colleagues put a wide range of password policies to the test 6  by asking 470 computer users at Carnegie Mellon to generate new passwords based on different requirements for length and special symbols. Then they tested how strong the resulting passwords actually were, how much effort was required to create them, how easy they were to remember \u2014 and how annoyed at the system the participants became. One key finding 7  was that organizations should forget the standard advice that complex gobbledygook words such as 0s7G0*7j%x$a are safest. \u201cIt's easier for users to deal with password length than password complexity,\u201d says Cranor. An example of a secure but user-friendly password might be a concatenation of four common but randomly chosen words \u2014 something like usingwoodensuccessfuloutline. At 28 characters, it is more than twice as long as the gibberish example, but much easier to remember. As long as the system guards against people making stupid choices such as passwordpassword, says Cranor, strings of words are quite hard for attackers to guess, and provide excellent security. \n               Time for a change \n             Another key finding, says Cranor, is that unless there is reason to think that the organization's security has been compromised, the standard practice of forcing users to change their passwords on a 30-, 60- or 90-day schedule ranks somewhere between useless and counterproductive (see  go.nature.com/2vq6r4 ). For one thing, she says, studies show 8  that most people respond to such demands by choosing a weaker password to begin with, so that they can remember it, and then making the smallest change that they can get away with. They might increase a final digit by one, for example, so that password2 becomes password3 and so on. \u201cSo if a hacker guesses your password once,\u201d she says, \u201cit won't take them many tries to guess it again.\u201d Besides, she says, one of the first that things hackers do when they break in is to install a key-logging program or some other bit of malware that allows them to steal the new password and get in whenever they want. So again, says Cranor, \u201cchanging the password doesn't help\u201d. Sasse sees encouraging signs that such critiques are being heard. \u201cFor me, the milestone was last year when GCHQ changed its advice on passwords,\u201d she says, referring to the Government Communications Headquarters, a key UK intelligence agency. GCHQ issued a public document 9 , containing several citations to the research literature, that gave up on long-established practices such as demanding regular password changes, and instead urged managers to be as considerate as possible towards the people who have to live with their policies. \u201cUsers have a whole suite of passwords to manage, not just yours,\u201d goes one bit of advice. \u201cOnly use passwords where they are really needed.\u201d \n               Attack the attackers \n             If research can uncover weak points in user behaviour, perhaps it can also find vulnerabilities among the attackers. In 2010, Stefan Savage, a computer scientist at the University of California, San Diego, and his team set up 10  a cluster of computers to act as what he calls \u201cthe most gullible consumer ever\u201d. The machines went through reams of spam e-mails collected from several major antispam companies, and clicked on every link they could find. The researchers focused on illegal pills, counterfeit watches and handbags, and pirated software \u2014 three of the product lines most frequently advertised in spam \u2014 and bought more than 100 items. Then they used specially designed web-crawling software to track back through the spammers' supply network. If an illicit vendor registered a domain name, made payments to a supplier or used a bank to accept credit-card payments, the researchers could see it. The study exposed, for the first time, the entire business structure of computer criminals \u2014 and revealed how surprisingly sophisticated it was. \u201cIt was the ultimate hothouse of weird new entrepreneurial ideas,\u201d says Savage, \u201cthe purest form of small-business capitalism imaginable \u2014 because there is no regulation.\u201d Yet there was order, even so. \u201cSay you have a criminal activity you want to engage in,\u201d Savage explains \u2014 for example, selling counterfeit drugs. You set up shop by creating the website and the databases, striking a deal with a bank to accept credit-card payments and creating a customer-service arm to deal with complaints \u2014 all the back-end parts of the business (see 'A tangled web'). \u201cYou don't send the spam yourself,\u201d says Savage. \u201cYou open that up to affiliates\u201d \u2014 specialists who know how to send reams of clickable messages that fool people's spam filters. \u201cThey get 30\u201340% of the purchase price for any order they bring to you,\u201d he says. And if the brilliant idea turns out to be a dud \u2014 well, they just go and spam for someone else. This affiliate business model has been confirmed in subsequent studies by Savage and many others 11 , and turns out to apply to a broad range of cybercrimes, from the sale of knock-off handbags to ransomware, credit-card piracy and other forms of cybertheft. All are supported by the same underground economy of affiliate services, of which spam generation is only one. Others range from companies in India where people spend their days typing in characters from CAPTCHA symbol-recognition tests \u2014 thus 'proving' that a malicious program is human \u2014 to an up-and-coming spam alternative known as search-engine poisoning, in which people who click on legitimate-looking search results are redirected to malicious websites. Unfortunately for law-enforcement agencies, tracing the structure of this underground economy rarely helps them to arrest the individuals involved; real-world identities tend to be closely guarded behind online pseudonyms. And in any case, the criminal cyberinfrastructure is remarkably resilient. In October 2013, for example, the FBI managed to shut down Silk Road, an eBay-like website that linked buyers and sellers for illicit commodities including hard drugs. Silk Road 2.0 appeared online a month later. And when the FBI shut down that site in late 2014, still others popped up. However, researchers have uncovered some potentially more effective ways to attack the underground economy. Savage and his colleagues found 12  that by far the weakest links were the banks that processed credit-card payments to the profit centres. They were at the mercy of the credit-card companies, whose contracts generally state that any bank that represents a merchant must guarantee that a sale is legal \u2014 and is liable for paying customers back if they complain. Few banks were willing to take such risks. \u201cIt turned out that 95% of counterfeit spam on the planet went through just three banks,\u201d says Savage: one each in Azerbaijan, Latvia and St Kitts and Nevis. In November 2011, Microsoft worked with Visa to pressure those banks to drop the vendors that were pirating its products. \u201cAnd for 18 months,\u201d says Savage, \u201cthere was no one selling pirated Microsoft software on the Internet.\u201d It was not a permanent solution, however: banking support for the shady software vendors eventually moved to east Asia, where Western companies and law-enforcement agencies have considerably less leverage. Still, the hope is that continuing research will be able to make a big difference in the long run. \u201cFor the first time,\u201d says Nicolas Christin, a computer engineer who studies the human side of cybersecurity at Carnegie Mellon, \u201cwe have vast amounts of data about the underground economy.\u201d Try that in the real world, says Christin: anyone who wanted to understand, say, the street-drug trade in Pittsburgh would have to go undercover and risk getting killed. And even then, they would get only a fragmentary, ad hoc glimpse of the whole picture. But in the online world, every transaction leaves a digital trail, says Christin, who has led research on Silk Road \u2014 especially when payments are made using  digital currencies such as Bitcoin . \u201cAnd for an economist, that's wonderful.\u201d Christin and others in this field have watched criminal systems grow, mature and be taken down \u2014 and others spring up in their place. They have watched coalitions form and dissolve, and tracked how the flow of money between criminals helps them to build trust. \u201cWe're just beginning to scratch the surface on the analysis,\u201d says Christin. But he foresees this flood of data resulting in a new fusion of computer science with social science and conventional law enforcement. \u201cIt may actually be very fruitful ground for refining and testing existing theories of criminal behaviour,\u201d he says. Savage has a similar hope. Whether the focus is inward- or outward-looking, he says, \u201cThere is so much snake oil around security. Very few decisions are based on data.\u201d Continuing research could help people to base more of those decisions on evidence, he says. \u201cBut to do that, you have to look at the people involved \u2014 their motivations and incentives.\u201d \n                 Tweet \n                 Facebook \n                 LinkedIn \n                 Weibo \n                 Wechat \n               Reprints and Permissions"},
{"file_id": "533308a", "url": "https://www.nature.com/articles/533308a", "year": 2016, "authors": [{"name": "Natasha Gilbert"}], "parsed_as_year": "2006_or_before", "body": "Old-fashioned breeding techniques are bearing more fruit than genetic engineering in developing hyper-efficient plants. Jonathan Lynch likes to look beneath the surface. In his quest to breed better crops, the plant physiologist spends a lot of time digging up roots to work out what makes some varieties extremely good at extracting nutrients from the ground. Lynch wants to use this knowledge to develop plants with extra-efficient roots \u2014 crops that grow well in the  nutrient-starved soils of the developing world . These plants could also reduce the use of fertilizers in richer nations. Last year, Lynch's forays into the dirt paid off. He and his team at Pennsylvania State University in University Park reported 1  that they had produced a variety of common bean, or string bean ( Phaseolus vulgaris ), with a combination of root traits that allows it to take up phosphorus from the soil with improved efficiency. In experimental plots, the plants produced three times the bean yield of typical varieties. That result has raised hopes in Africa, where common beans are one of the most important sources of protein for poor people. Researchers in Mozambique are testing how Lynch's beans perform in the country's ecological zones, and they expect to win regulatory approval to bring the crop to market by next year. Lynch's beans are among the first successful attempts in a global race to develop crops that grow well in soils depleted of nutrients. \u201cLow availability of nitrogen, phosphorus and water are the main limitations of plant growth on Earth. We desperately need this technology,\u201d says Lynch. His work stands out because he has taken an old-school approach. He is leading a renaissance in some conventional crop-breeding techniques that rely on laboriously examining plants' physical characteristics and then selecting for desirable traits, such as growth or the length of fine roots. And surprisingly, this approach seems to be outpacing the high-tech route. Big corporations such as DuPont Pioneer in Johnston, Iowa, have spent more than a decade  developing improved crops through genetic engineering , and some companies say that their transgenic varieties look promising in field trials. But there are still no fertilizer-frugal transgenic crops on the market, and several agricultural organizations around the globe are reviewing their biotechnology initiatives in this area. Plant biologist Allen Good of the University of Alberta in Edmonton, Canada, spent years working with companies to develop genetically modified (GM) crops that require little fertilizer, but he says that this approach has not been as fruitful as conventional techniques. The problem is that there are so many genes involved in nutrient uptake and use \u2014 and environmental variations alter how they are expressed. \u201cNutrient efficiency was supposed to be one of those traits with broad applicability that could make companies lots of money. But they haven't developed the way we thought,\u201d says Good. Despite the scientific and breeding challenges, some researchers say that all strategies must be explored to develop crops that are less nutrient needy. With the  global population heading towards 10 billion people  by 2050, frugal crops could be essential to feed the planet. \u201cThere is a huge worldwide potential for these traits to help increase food production and sustainable development,\u201d says Matin Qaim, an agricultural economist at the University of G\u00f6ttingen in Germany. \n               Magic beans \n             Lynch says that he spent years \u201cgroping around in the dark\u201d before he worked out which root characteristics help  P. vulgaris  to grow in low-phosphorus soils. Screening for root traits is a messy and difficult business, so it is unpopular among researchers, he says. That's why most previous research had focused on crop shoots rather than their roots. His team dug up plants to decipher which characteristics were beneficial, taking great care to avoid damaging the fragile root structures. The researchers didn't even know what traits they were looking for in the tangled masses, which could contain thousands of roots per plant. The key to success is having an open mind and just seeing what the root looks like, says Lynch. \u201cI tell my students they should drop acid before they go to the field, and just look at the plants and let them tell you what they are doing,\u201d he jokes. One feature that interested Lynch and his team was root depth, because they knew that phosphorus tends to stick to the upper layers of soil. The researchers reported 1  last year that beans with basal roots \u2014 those close to the base of a plant \u2014 that grow at a shallow angle produced 58% more above-ground biomass than beans with steeper basal roots. The team also discovered that plants with long root hairs took up phosphorus better and improved the bean yield by 89%, relative to the variety with shorter root hairs. When the researchers knitted both the root traits together, they saw much bigger gains. In field trials in Mozambique, beans with shallow roots and long root hairs gave yields of 1,500 kilograms per hectare in soils depleted in phosphorus, compared with local varieties that give just 500 kilograms per hectare. The beans could be a boon for African farmers who struggle with depleted soils. According to the Food and Agriculture Organization of the United Nations, soils in sub-Saharan Africa lose around 26 kilograms of nitrogen and 3 kilograms of phosphorus per hectare each year through farming and erosion. Because many farmers cannot afford to replenish these nutrients with fertilizers, this stunts yields to about one-tenth of those in developed countries. The next big research challenge is to improve the beans'  tolerance to drought  while maintaining their ability to thrive in low-phosphorus soils. This could be tricky because crops under water stress need deep roots \u2014 the opposite of what they need to absorb phosphorus, says Magalhaes Miguel, a plant physiologist at the Agricultural Research Institute of Mozambique in Chimoio who collaborates with Lynch. Miguel is testing the beans in different locations around Mozambique to win approval for seed companies to produce and sell the crops. Lynch's phosphorus discoveries have already made the jump into farmers' fields in China. His colleagues there have shown 2  that some of the same root traits that boost common beans also improve growth in soya beans ( Glycine max ). The regulatory system in China is faster at approving new crop varieties than that in Mozambique, so the improved soya beans are already in use and have been planted on 67,000 hectares so far. Lynch has also had some success in improving crops' uptake of nitrogen, which tends to accumulate deeper in the soil. The ability to grow deep roots is influenced by several factors, including the characteristics of a plant's lateral roots \u2014 those that grow out of the main framework of roots. Working in maize, or corn ( Zea mays ), Lynch found that plants perform better under low-nitrogen conditions 3  and water stress 4  when they have just a few long lateral roots rather than many short ones. This arrangement requires less energy to grow, so plants can invest more resources in above-ground vegetation and developing deep roots that can suck up water and nitrogen. Tests in US fields starved of nitrogen showed 3  that maize with few long lateral roots produced yields more than 30% higher than maize with many short lateral roots. The long-lateral-root variety performed even better in a test of drought-like conditions, where it showed a 144% increase in yield 4 . Other research teams are taking different approaches to breeding fertilizer-frugal crops. Over the past six years, a group at the International Maize and Wheat Improvement Centre (CIMMYT), headquartered in Texcoco, Mexico, has had success using conventional breeding to create maize that grows well in the nitrogen-starved soils of Africa. Field trials show that CIMMYT's maize produces up to 20% higher yields than existing commercial maize varieties in low-nitrogen soils. By the end of 2015, the centre had won regulatory approval for 14 nitrogen-efficient maize varieties across 6 countries, including Malawi and Kenya. The organization has more than 50 other varieties in the works. Biswanath Das, a maize breeder who leads CIMMYT's project in Nairobi, says that the team is still investigating why some varieties are better than others at finding and using nitrogen. \u201cWe hope to have some answers in the next two years,\u201d says Das. \n               Boom and bust \n             As Lynch and CIMMYT make big leaps forward, they are overtaking agricultural-biotechnology companies that have invested years of work in tests with GM crops. Good, who was involved in some of the early experiments, remembers those heady times. In 2001, he walked up and down rows of transgenic and conventional oilseed rape ( Brassica napus , also known as canola) growing side by side in Californian test fields starved of nitrogen. He marvelled at the contrast between the bigger, greener GM plants, flourishing in the stressful conditions, and their unmodified relatives that were struggling to survive. The GM plants' advantage was a transgene: alanine aminotransferase ( AlaAT ), which came from barley. \u201cI thought, 'Oh my god, you can see the gene working in the field',\u201d says Good. A few years earlier, he had discovered that  AlaAT  \u2014 which has an important role in how plants metabolize nitrogen and incorporate the nutrient into grain \u2014 could help crops to thrive in low-nitrogen soils. The gene is still one of the more advanced targets for transgenic approaches to farmers' over-dependence on fertilizer. The 2001 trials were run by Arcadia Biosciences, an agricultural-biotechnology company in Davis, California, for which Good was working at the time as a consultant, and to which he had licensed the  AlaAT  discovery. In a collaboration with biotechnology company Monsanto in St Louis, Missouri, separate field studies 5  showed that the transgenic oilseed rape produced 42% more seeds under low-nitrogen conditions than did conventional controls. They also found that the GM crop needed 40% less nitrogen fertilizer to achieve yields equivalent to those of unmodified crops. Arcadia says that it has successfully put the gene into other crops, including wheat and rice. The company has one of the industry's most robust nitrogen-efficiency programmes, says Roger Salameh, Arcadia's chief operating officer. \u201cWe have significant evidence from multiple years of field trials in the two largest food crops in the world \u2014 wheat and rice \u2014 that we are driving double-digit yield gains,\u201d he says. Despite the positive results, Arcadia has not yet commercialized these or related crops. The company has worked on the nitrogen-efficient wheat with several other companies and governmental agencies, including the Commonwealth Scientific and Industrial Research Organisation (CSIRO), Australia's national science agency, but the CSIRO pulled out of that project. A spokeswoman for the organization says that it \u201cdid not see results that met its objectives so is no longer actively researching this area\u201d. Arcadia was hoping to have a transgenic variety of nitrogen-efficient wheat on the market this year, but it will miss that target. Salameh says that it is uncertain when the plant will be ready, but the company is now working with Vilmorin and Cie, a seed company in Saint-Beauzire, France, to commercialize the wheat. \n               From theory to practice \n             Qaim and some other agricultural analysts are impressed with the results of several transgenic efforts. For example, Arcadia and its research partners say that they have developed a transgenic variety of rice that produced, on average, 30% higher yields than conventional controls during four years of field trials in Colombia under low-nitrogen conditions. However, Qaim says that the crops' performance has yet to be demonstrated in realistic field conditions. Good has heard that the GM oilseed rape is 'tweaky', meaning that it performs well in some environments but not others. He has lost his initial rapture with it, and says that if he had his time again, he would put the lion's share of his efforts into conventional breeding approaches like those of Lynch and CIMMYT. Good and the CSIRO are not the only ones worried that the transgenic crops are not yet delivering as hoped. The CIMMYT effort to improve maize yields in low-nitrogen soils kicked off in 2012 with a project to develop a transgenic variety as well as the conventional maize. The transgenic plants are based on technology donated by DuPont Pioneer. They have produced some increases in yields, but not yet enough to warrant the huge effort required to gain regulatory approval, says Das. The transgenic programme is now under review. \u201cThe yield benefits are just not that exciting for commercialization yet,\u201d he says. DuPont Pioneer has also seen some delays in its transgenic products targeted to US markets. The company told  Nature  in 2010 that it would have a product available by 2022 (see  Nature   466 , 548\u2013551; 2010 ). It now says that the products are still in the proof-of-concept stage, and will not be ready until the middle or late 2020s. David Spielman, an economist at the International Food Policy Research Institute in Washington DC, says that the delays don't necessarily signal disaster: companies forecast precise timelines to satisfy investors, but the science doesn't always deliver according to corporate plans. The pushed-back timelines do not mean that the scientific advances won't come at some point in the future, he says. \u201cIt is more important to consider if the traits are showing improvement and if public and private sectors are investing in the research with a longer-term view of the path to commercialization,\u201d says Spielman. A key challenge for transgenic efforts is that traits such as nutrient-use efficiency are influenced by a complex web of genes that interact with each other and the environment. \u201cIt's very difficult to take one transgene and stick it into a maize variety and expect a big effect,\u201d says Das. DuPont Pioneer and Arcadia are still confident that a single gene will one day deliver the nutrient-efficient goods. \u201cPeople question if we could find a single gene with a big enough effect, but we are having real success,\u201d says Tom Greene, research director at DuPont Pioneer. And Salameh also remains confident about this approach. Meanwhile, researchers are trying to combine old-fashioned breeding with a variety of high-tech methods. Lynch is now using a computer model that simulates natural selection to work out what combination of root characteristics will boost crops' performance under nutrient stress. He is also identifying genetic markers for useful root traits, which should help to indicate whether plants' offspring have the desired genes and should speed up the breeding processes, he says. Government regulation could perhaps be the biggest hurdle to both conventional and transgenic frugal crops. Regulatory hurdles in Europe and Africa keep many transgenic technologies from reaching large-scale field testing and commercialization, says Qaim. These hurdles bear part of the blame for the slower-than-expected progress in transgenic frugal crops, he says. And in some African countries, it can take up to ten years to get new conventional varieties to market because of stringent testing requirements, says Das. Farmers and breeders are also hesitant to take a risk on new varieties. But if CIMMYT can get the seeds into farmers' fields, the impact on food security and the environment in developing countries could be massive, says Das. \u201cThe beauty of it is that the solution is packaged in the seed,\u201d he says, so there is no need for extra fertilizer. \u201cOnce the farmers have the seed, they have the solution.\u201d \n                 Tweet \n                 Facebook \n                 LinkedIn \n                 Weibo \n                 Wechat \n               \n                     Case studies: A hard look at GM crops 2013-May-01 \n                   \n                     Food: The growing problem 2010-Jul-28 \n                   \n                     Food: Inside the hothouses of industry 2010-Jul-28 \n                   \n                     Food: An underground revolution 2010-Jul-28 \n                   \n                     The Roots Lab \n                   \n                     Agricultural Research Institute of Mozambique \n                   \n                     CIMMYT Improved Maize for Africa Soils project \n                   \n                     CIMMYT Drought Tolerant Maize for Africa project \n                   \n                     Arcadia Biosciences \n                   \n                     CSIRO \n                   \n                     Australian Centre for Plant Functional Genomics \n                   \n                     DuPont Pioneer \n                   \n                     Monsanto \n                   Reprints and Permissions"},
{"file_id": "533160a", "url": "https://www.nature.com/articles/533160a", "year": 2016, "authors": [{"name": "Sara Reardon"}], "parsed_as_year": "2006_or_before", "body": "The geneticist built a career studying aspects of sex that make some people uncomfortable. Now things are getting uncomfortable for him. As a medical student in Paris in the 1980s, Eric Vilain found himself pondering the differences between men and women. What causes them to develop differently, and what happens when the process goes awry? At the time, he was encountering babies that defied simple classification as a boy or girl. Born with disorders of sex development (DSDs), many had intermediate genitalia \u2014 an overlarge clitoris, an undersized penis or features of both sexes. Then, as now, the usual practice was to operate. And the decision of whether a child would be left with male or female genitalia was often made not on scientific evidence, says Vilain, but on practicality: an oft-repeated, if insensitive, line has it that \u201cit's easier to dig a hole than build a pole\u201d. Vilain found the approach disturbing. \u201cI was fascinated and shocked by how the medical team was making decisions.\u201d Vilain has spent the better part of his career studying the ambiguities of sex. Now a paediatrician and geneticist at the University of California, Los Angeles (UCLA), he is one of the world's foremost experts on the genetic determinants of DSDs. He has worked closely with intersex advocacy groups that campaign for recognition and better medical treatment \u2014 a movement that has recently gained momentum. And in 2011, he established a major longitudinal study to track the psychological and medical well-being of hundreds of children with DSDs. Vilain says that he doesn't seek out controversy, but his research seems to attract it. His studies on the genetics of sexual orientation \u2014 an area that few others will touch \u2014 have attracted criticism from scientists, gay-rights activists and conservative groups alike. He is also a medical adviser for the International Olympic Committee, which about five years ago set controversial rules by which intersex individuals are allowed to compete in women's categories. But what has brought Vilain the most grief of late has been his stance on sex-assignment surgery for infants with DSDs. Although he generally opposes it, he won't categorically condemn it or the doctors who perform it. As a result, many intersex advocates who object to the practice now see him as a hindrance to their cause. In November, nine bioethicists and activists resigned as advisers to his longitudinal study in protest. \u201cI just lost my patience,\u201d says Alice Dreger, a bioethicist who used to work at Northwestern University in Evanston, Illinois, and who was among the first to leave the study. Although dismayed by their departure, Vilain refuses to take a stance until it is supported by science. \u201cThe thing I don't want to compromise is scientific integrity, even when it clashes with the community narrative.\u201d \n               Breaking binary \n             The idea that there are  only two sexes  is so entrenched in society that the first question many people ask on finding out that a friend is pregnant is: boy or girl? \u201cPeople don't answer 'I'm having a baby',\u201d says Vilain. \u201cThey probably should.\u201d At Necker University Hospital for Sick Children in Paris in the 1980s, he says, doctors presumed that a child would be psychologically damaged if he or she did not have normal-looking genitalia. In Vilain's experience, that belief was so strong that doctors would take genital abnormalities into account when deciding how hard to fight to save a premature baby. \u201cThe unanimous feeling was that boys with a micropenis could never achieve a normal life \u2014 that they were doomed,\u201d he says. (The paediatric-surgery department at Necker refused to answer questions relating to past or current standards of care.) DSDs occur in an estimated 1\u20132% of live births, and hundreds of genital surgeries are performed on infants around the world every year 1 . But there are no estimates as to how often a child's surgically assigned sex ends up different from the gender they come to identify with. What do exist, however, are stories of people who say that they have been harmed: children who struggle to fit in with peers, adolescents who are stressed, harassed or attempt suicide, and adults who are furious that they were not involved in the decision to modify their bodies. Over the past two decades, and especially in the past few years, intersex activists worldwide \u2014 some of whom do not identify as either gender \u2014 have begun to speak out against the practice. Unless a child's life is in danger, they argue, he or she should have the right to decide on surgery when older. Vilain's fascination with the biological complexities of sexual differentiation made him want to study the causes of DSDs. So in 1990, he joined the lab of geneticist Marc Fellous at the Pasteur Institute in Paris. Fellous was studying a newly discovered gene called  SRY , which resides on the Y chromosome and is crucial in triggering the development of male features. Vilain helped to identify the causes of several DSDs, such as XY people who look female because of mutations that disable the  SRY  gene 2 , and people who carry a copy of  SRY  even if they do not have a Y chromosome 3 . Vilain was an unusual student, Fellous says, because his clinical background allowed him to bridge lab work and patient care. Fellous says that it is often difficult to explain to the families of children with DSDs why the research would be helpful. \u201cEric was useful for this,\u201d he says. \u201cHe was a very open mind, really close to families.\u201d In 1995, Vilain left France for a faculty job at UCLA. There, he began tackling questions about sexual development from every possible angle. He created mouse models with mutations in  SRY  or other sex-linked genes to study how their developing brains  respond to hormones  \u2014 research that could lead to better care for people with DSDs. Perhaps most notoriously, he has explored the roots of sexual orientation, work that made even his colleagues uncomfortable. In 2006, he was looking to publish work by his postdoc Sven Bocklandt, who had found links between the way genes are expressed from a mother's X chromosomes and the chances of her having a gay son. When he approached biostatisticians for help, several refused to collaborate, Vilain says, because they were afraid of how the public might respond. Studies on the genetic  underpinnings of homosexuality are controversial . Religious conservatives who believe that being gay is a choice argue that scientists are trying to legitimize it; gay activists worry that the research will lead to misguided attempts to 'cure' gay people. Vilain gets occasional attacks from both groups. But he says that his colleagues' squeamishness around controversial research was unscientific. So, he stormed into the office of the UCLA biostatistics chair, Kenneth Lange, to complain. \u201cEric's not afraid to kick up some dust and stand up for the people in his lab,\u201d Bocklandt says. \u201cI think that's why he's been so successful.\u201d A statistician eventually volunteered to help. Dean Hamer, a retired geneticist formerly at the US National Cancer Institute in Bethesda, Maryland, trained Bocklandt and has studied the genetics of sexual orientation. He says that Vilain is pretty much the only geneticist who still does serious research on the topic. \u201cThat takes a level of courage and belief that ultimately the biology will win out,\u201d he says. \n               Courting advocacy \n             Vilain's research and interest in policy has put him on the front lines of the lesbian, gay, bisexual, transgender and queer (LGBTQ) rights movement and has made his lab a magnet for LGBTQ students. His work also made him a sort of scientist-laureate for the intersex advocacy community, which started gaining prominence in the early 1990s with the formation of the Intersex Society of North America in Rohnert Park, California. The group, founded by activist Bo Laurent, lobbied for recognition of intersex as a human condition rather than an affliction, and opposed infant surgery. Vilain, who met Laurent in 1997, says that she helped to shape his opinions on surgery and other topics that are important to intersex people, such as the stigma they face. Although Laurent and her colleagues were well informed and knowledgeable about the science of DSDs, they struggled to be heard in scientific conversations. \u201cI think the view was that they were zealots,\u201d Vilain recalls. In 2005, several paediatric societies met in Chicago to draft a consensus statement on the management of intersex conditions \u2014 a still-influential document 4  that guides the standard of care. Laurent attended the meeting hoping to see the word hermaphrodite struck from the medical vocabulary. The term was not only offensive \u2014 it labelled a person rather than a disorder \u2014 it was also scientifically inaccurate because it suggested that the person had functioning male and female organs. Rather than being heard, Laurent recalls being sidelined. But Vilain, who headed the genetics working group, met with her in secret throughout the meeting, drafting a case to present to the group. They met stiff opposition from medical doctors, who saw no reason for change, but their language was ultimately adopted in the final statement 4 . Over the years, Vilain continued to build a reputation as an ally to intersex people. In 2011, when he and psychologist David Sandberg at the University of Michigan in Ann Arbor began the ten-institution registry to track children with DSDs, ethicists and activists enthusiastically joined its advisory board. Funded by the US National Institutes of Health, the Disorders of Sex Development Translational Research Network has enrolled more than 300 children, collecting medical records and blood samples and performing interviews to answer a variety of biological and psychological questions. Many of the advocates who joined as advisers had hoped that development of the network would lead to a denouncement of infant genital surgery by revealing the damage that it can cause. \u201cNo one has demonstrated anything but harm,\u201d says Anne Tamar-Mattis, legal director of the intersex advocacy group interACT in San Francisco, California. \u201cResearch that settles that question is useful.\u201d But the study has yet to do what advocates hoped. Sandberg, who heads the network's psychological research, has collected evidence that emotional and social support from the family is the most important contributor to the psychological and mental health of a child with a DSD. He suspects that it has an even greater impact than surgery. \u201cI never question people's experiences,\u201d Sandberg says of the activists who believe that surgery is always harmful. \u201cWhat I do question is whether they're generalizable.\u201d One argument in favour of infant surgery is that a child could be psychologically scarred by growing up with intermediate genitalia, but there is little evidence for or against that. In rare cases, surgery could help to prevent cancer. Complete androgen insensitivity syndrome, for instance, confers an increased risk of testicular cancer that can be lowered through surgery 5 . But Vilain points out that the risk before puberty is very small 6 , suggesting that surgery could wait. Although few surgeons were willing to talk openly about infant genital surgery, some do argue that the fear of harm is overblown or at least outdated. Laurence Baskin, a paediatric urologist at the University of California, San Francisco, says that the days of \u201cassigning gender\u201d are long gone, because scientists no longer believe that a child can be made to be a boy or a girl. Most DSDs can be diagnosed and the outcomes predicted; physicians use the diagnosis to advise parents on which gender the child is likely to identify with, he says. For instance, the most common cause for a DSD is congenital adrenal hyperplasia \u2014 which can result in ambiguous genitalia for XX children. Between 90% and 95% of people with the condition identify as female 7 . When asked about children with this disorder who ultimately do not identify as female, another paediatric urologist \u2014 who wished not to be named \u2014 argues that the process can be reversed. People have sex-change surgery as adults all the time, he says. Such arguments infuriate Tamar-Mattis. \u201cIf one time in 20 you're cutting a little boy's penis off, is that a risk worth taking?\u201d she says. Vilain doesn't think so, and doesn't generally recommend surgery to his patients. He says that in his experience, more parents are now choosing to delay surgery. But he and his collaborators on the longitudinal study are reluctant to condemn surgery outright \u2014 they prefer to approach each case individually and to consider the views of parents who may feel strongly about what is right for their child. This attitude helped to create the rift between the researchers and intersex advocates. At the end of 2015, Dreger, who had served as the bioethicist for the longitudinal study announced her resignation in a blogpost. \u201cI can't continue to help develop 'conversations' around 'shared decision making' that allow decisions to be made that I believe violate the most basic rights of these children,\u201d she wrote. \u201cI am fed up with being asked to be a sort of absolving priest of the medical establishment.\u201d Vilain was blind-sided by the post. \u201cI was very saddened by this,\u201d he says. \u201cShe's a friend.\u201d After her departure, eight other advocates sent the study's leaders a letter of resignation.  You're basically calling doctors torturers when they're doing something considered standard medical practice.  Most would not comment on the record, but say that they were upset that the researchers were making decisions about which questions to pursue without sufficiently consulting them. For example, advocates are also concerned about the psychological impacts on children from having their genitals photographed for the purpose of diagnosis or to plan treatment. Some say that Vilain became hostile in meetings. They accuse him and Sandberg of putting research interests ahead of human suffering. \u201cWe live in a community of people who have experienced the harms of these practices,\u201d says Arlene Baratz, a radiologist who serves as medical adviser to a DSD support group and is one of those who resigned from the study. She and others say that in their decades of work as advocates they have never been contacted by someone who was helped by surgery. Vilain says that he does talk to such patients in his practice, but because they are living happy lives, they have no reason to speak out. Without data on outcomes, says Douglas Diekema, a medical ethicist at the Seattle Children's Research Hospital Institute in Washington, it is impossible to weigh up whether surgery is overall harmful, helpful or neutral for most people. \u201cGood ethics requires good data,\u201d he says. But a legal battle in the United States could change medical practice before those data are in. Tamar-Mattis is one of the lawyers representing the family of a baby who underwent feminizing surgery at 16 months old. The child, now 11 years old, identifies as male, and his lawyers argue that South Carolina's Department of Social Services and the university that performed the surgery violated the child's rights. Intersex advocates are watching the case with great interest, because it could lay the groundwork for future suits that could effectively outlaw the procedures in the United States. In January, the United Nations released a report saying that sex-assignment surgeries on infants \u201clead to severe and life-long physical and mental pain and suffering and can amount to torture and ill-treatment\u201d. Vilain and Sandberg worry that the language could alienate doctors and parents alike. \u201cYou're basically calling doctors torturers when they're doing something considered standard medical practice,\u201d Vilain says. He points out that few medical procedures are governed by law \u2014 physicians tend to operate according to guidelines and principles. \u201cI'm not opposed to guidelines, I'm opposed to things that completely alter medical practice in an irreversible way,\u201d he says. He and Sandberg also worry that legal bans could drive infant surgery underground. \u201cParents are scared. You just don't dictate to them and say get over it,\u201d Sandberg says. \n               Testing patience \n             Vilain's expertise has plunged him into other controversies. One example is his involvement with the International Olympic Committee, which in 2011 revised its policy on athletes who identify as female but who have male sex organs or produce high levels of testosterone. The issue came to the fore in 2009 after 18-year-old South African runner Caster Semenya, who identifies as female, was subjected to humiliating sex testing before being allowed to continue competing in the women's category. To head off future problems, the medical advisory board, under Vilain's leadership, drew a bright line for the 2012 Olympics. People with testosterone levels above 10 nanomoles per litre of blood could not participate in women's events, no matter how they identify. Exceptions are made only if the athletes can prove that they are  resistant to the effects of testosterone . Many activists and ethicists are furious about the policy. \u201cIt bears noting that athletes never begin on a fair playing field; if they were not exceptional in one regard or another, they would not have made it to a prestigious international athletic stage,\u201d wrote bioethicist Katrina Karkazis from Stanford University in California in a 2012 article 8  lambasting the policy. Even Vilain struggles to defend it on scientific grounds. Although women with DSDs that result in high testosterone levels are overrepresented among Olympians, the hormone does not seem to directly impact their performance. \u201cIt is very imperfect,\u201d he admits. \u201cBut if we don't have a dividing line, then there is no point in segregating sexes in sports.\u201d (The policy has been temporarily suspended and is under review.) Some of Vilain's detractors question how he can support a somewhat arbitrary call in this situation while requiring more evidence to condemn infant surgery. But sport, he argues, depends on rules and policies, whereas medicine relies on best-practice guidelines \u2014 and that is what he hopes to develop through research. He and his collaborators plan to continue the longitudinal study. The team has recruited a bioethicist, John Lantos of Children's Mercy Hospital in Kansas City, to replace Dreger, and it still has some patient advocates involved. Vilain says that he is trying not to antagonize anyone \u2014 the next iteration will include research on more questions that the participants say are priorities, such as how to preserve fertility for people with DSDs and identifying cancer risks. Yet Vilain's experiences with patient advocates have hardened him somewhat. \u201cI call the ones who work with us advocates; those against us activists,\u201d he says. He remains driven by questions about sex, even if it kicks up dust. \u201cWe're trying to listen to the community, but by the same token we're committed to producing data and evidence.\u201d \n                 Tweet \n                 Facebook \n                 LinkedIn \n                 Weibo \n                 Wechat \n               \n                     Largest ever study of transgender teenagers set to kick off 2016-Mar-29 \n                   \n                     Sex redefined 2015-Feb-18 \n                   \n                     Diversity: Pride in science 2014-Sep-16 \n                   \n                     Ethics: Taboo genetics 2013-Oct-02 \n                   \n                     Disorders of Sex Development Translational Research Network \n                   Reprints and Permissions"},
{"file_id": "533452a", "url": "https://www.nature.com/articles/533452a", "year": 2016, "authors": [{"name": "Monya Baker"}], "parsed_as_year": "2006_or_before", "body": "Survey sheds light on the \u2018crisis\u2019 rocking research. More than 70% of researchers have tried and failed to reproduce another scientist's experiments, and more than half have failed to reproduce their own experiments. Those are some of the telling figures that emerged from  Nature 's survey of 1,576 researchers who took a brief online questionnaire on reproducibility in research. The data reveal sometimes-contradictory attitudes towards reproducibility. Although 52% of those surveyed agree that there is a significant 'crisis' of reproducibility, less than 31% think that failure to reproduce published results means that the result is probably wrong, and most say that they still trust the published literature. Data on how much of the scientific literature is reproducible are rare and generally bleak. The best-known analyses, from psychology 1  and cancer biology 2 , found rates of around  40%  and  10% , respectively. Our survey respondents were more optimistic: 73% said that they think that at least half of the papers in their field can be trusted, with physicists and chemists generally showing the most confidence. The results capture a confusing snapshot of attitudes around these issues, says Arturo Casadevall, a microbiologist at the Johns Hopkins Bloomberg School of Public Health in Baltimore, Maryland. \u201cAt the current time there is no consensus on what reproducibility is or should be.\u201d But just recognizing that is a step forward, he says. \u201cThe next step may be identifying what is the problem and to get a consensus.\u201d Failing to reproduce results is a rite of passage, says Marcus Munafo, a biological psychologist at the University of Bristol, UK, who has a long-standing interest in scientific reproducibility. When he was a student, he says, \u201cI tried to replicate what looked simple from the literature, and wasn't able to. Then I had a crisis of confidence, and then I learned that my experience wasn't uncommon.\u201d The challenge is not to eliminate problems with reproducibility in published work. Being at the cutting edge of science means that sometimes results will not be robust, says Munafo. \u201cWe want to be discovering new things but not generating too many false leads.\u201d \n               The scale of reproducibility \n             But sorting discoveries from false leads can be discomfiting. Although the vast majority of researchers in our survey had failed to reproduce an experiment, less than 20% of respondents said that they had ever been contacted by another researcher unable to reproduce their work. Our results are strikingly similar to another online survey of nearly 900 members of the American Society for Cell Biology (see  go.nature.com/kbzs2b ). That may be because such conversations are difficult. If experimenters reach out to the original researchers for help, they risk appearing incompetent or accusatory, or revealing too much about their own projects. A minority of respondents reported ever having tried to publish a replication study. When work does not reproduce, researchers often assume there is a perfectly valid (and probably boring) reason. What's more, incentives to publish positive replications are low and journals can be reluctant to publish negative findings. In fact, several respondents who had published a failed replication said that editors and reviewers demanded that they play down comparisons with the original study. Nevertheless, 24% said that they had been able to publish a successful replication and 13% had published a failed replication. Acceptance was more common than persistent rejection: only 12% reported being unable to publish successful attempts to reproduce others' work; 10% reported being unable to publish unsuccessful attempts. Survey respondent Abraham Al-Ahmad at the Texas Tech University Health Sciences Center in Amarillo expected a \u201ccold and dry rejection\u201d when he submitted a manuscript explaining why a stem-cell technique had stopped working in his hands. He was pleasantly surprised when the paper was accepted 3 . The reason, he thinks, is because it offered a workaround for the problem. Others place the ability to publish replication attempts down to a combination of luck, persistence and editors' inclinations. Survey respondent Michael Adams, a drug-development consultant, says that work showing severe flaws in an animal model of diabetes has been rejected six times, in part because it does not reveal a new drug target. By contrast, he says, work refuting the efficacy of a compound to treat Chagas disease was quickly accepted 4 . \n               The corrective measures \n             One-third of respondents said that their labs had taken concrete steps to improve reproducibility within the past five years. Rates ranged from a high of 41% in medicine to a low of 24% in physics and engineering. Free-text responses suggested that redoing the work or asking someone else within a lab to repeat the work is the most common practice. Also common are efforts to beef up the documentation and standardization of experimental methods. Any of these can be a major undertaking. A biochemistry graduate student in the United Kingdom, who asked not to be named, says that efforts to reproduce work for her lab's projects doubles the time and materials used \u2014 in addition to the time taken to troubleshoot when some things invariably don't work. Although replication does boost confidence in results, she says, the costs mean that she performs checks only for innovative projects or unexpected results. Consolidating methods is a project unto itself, says Laura Shankman, a postdoc studying smooth muscle cells at the University of Virginia, Charlottesville. After several postdocs and graduate students left her lab within a short time, remaining members had trouble getting consistent results in their experiments. The lab decided to take some time off from new questions to repeat published work, and this revealed that lab protocols had gradually diverged. She thinks that the lab saved money overall by getting synchronized instead of troubleshooting failed experiments piecemeal, but that it was a long-term investment. Irakli Loladze, a mathematical biologist at Bryan College of Health Sciences in Lincoln, Nebraska, estimates that efforts to ensure reproducibility can increase the time spent on a project by 30%, even for his theoretical work. He checks that all steps from raw data to the final figure can be retraced. But those tasks quickly become just part of the job. \u201cReproducibility is like brushing your teeth,\u201d he says. \u201cIt is good for you, but it takes time and effort. Once you learn it, it becomes a habit.\u201d One of the best-publicized approaches to boosting reproducibility is pre-registration, where scientists submit hypotheses and plans for data analysis to a third party before performing experiments, to prevent cherry-picking statistically significant results later. Fewer than a dozen people mentioned this strategy. One who did was Hanne Watkins, a graduate student studying moral decision-making at the University of Melbourne in Australia. Going back to her original questions after collecting data, she says, kept her from going down a rabbit hole. And the process, although time consuming, was no more arduous than getting ethical approval or formatting survey questions. \u201cIf it's built in right from the start,\u201d she says, \u201cit's just part of the routine of doing a study.\u201d \n               The cause \n             The survey asked scientists what led to problems in reproducibility. More than 60% of respondents said that each of two factors \u2014 pressure to publish and selective reporting \u2014 always or often contributed. More than half pointed to insufficient replication in the lab, poor oversight or low statistical power. A smaller proportion pointed to obstacles such as variability in reagents or the use of specialized techniques that are difficult to repeat. But all these factors are exacerbated by common forces, says Judith Kimble, a developmental biologist at the University of Wisconsin\u2013Madison: competition for grants and positions, and a growing burden of bureaucracy that takes away from time spent doing and designing research. \u201cEveryone is stretched thinner these days,\u201d she says. And the cost extends beyond any particular research project. If graduate students train in labs where senior members have little time for their juniors, they may go on to establish their own labs without having a model of how training and mentoring should work. \u201cThey will go off and make it worse,\u201d Kimble says. \n               What can be done? \n             Respondents were asked to rate 11 different approaches to improving reproducibility in science, and all got ringing endorsements. Nearly 90% \u2014 more than 1,000 people \u2014 ticked \u201cMore robust experimental design\u201d \u201cbetter statistics\u201d and \u201cbetter mentorship\u201d. Those ranked higher than the option of providing incentives (such as funding or credit towards tenure) for reproducibility-enhancing practices. But even the lowest-ranked item \u2014 journal checklists \u2014 won a whopping 69% endorsement. The survey \u2014 which was e-mailed to  Nature  readers and advertised on affiliated websites and social-media outlets as being 'about reproducibility' \u2014 probably selected for respondents who are more receptive to and aware of concerns about reproducibility. Nevertheless, the results suggest that journals, funders and research institutions that advance policies to address the issue would probably find cooperation, says John Ioannidis, who studies scientific robustness at Stanford University in California. \u201cPeople would probably welcome such initiatives.\u201d About 80% of respondents thought that funders and publishers should do more to improve reproducibility. \u201cIt's healthy that people are aware of the issues and open to a range of straightforward ways to improve them,\u201d says Munafo. And given that these ideas are being widely discussed, even in mainstream media, tackling the initiative now may be crucial. \u201cIf we don't act on this, then the moment will pass, and people will get tired of being told that they need to do something.\u201d Download the full  questionnaire  used in the survey and the  raw data  in a spreadsheet (the data are also available as a tab-delimited file at  Figshare ). Dan Penny aided in creation and analysis of the survey. \n                 See Editorial \n                 page 437 \n               \n                 Tweet \n                 Facebook \n                 LinkedIn \n                 Wechat \n                 Weibo \n               \n                     Psychology\u2019s reproducibility problem is exaggerated \u2013 say psychologists 2016-Mar-03 \n                   \n                     How many replication studies are enough? 2016-Feb-26 \n                   \n                     Reproducibility: A tragedy of errors 2016-Feb-03 \n                   \n                     How scientists fool themselves \u2013 and how they can stop 2015-Oct-07 \n                   \n                     Robust research: Institutions must do their part for reproducibility 2015-Sep-01 \n                   \n                     Scientific method: Statistical errors 2014-Feb-12 \n                   \n                     Nature  special: Challenges in irreproducible research \n                   \n                     Cleaned tab-delimited file of raw survey data \n                   Reprints and Permissions"},
{"file_id": "534020a", "url": "https://www.nature.com/articles/534020a", "year": 2016, "authors": [{"name": "Mark Zastrow"}], "parsed_as_year": "2006_or_before", "body": "The Asian nation is spending big in the hope of winning a Nobel prize, but it will need more than cash to realize its ambitions. Behind the doors of a drab brick building in Daejeon, South Korea, a major experiment is slowly taking shape. Much of the first-floor lab space is under construction, and one glass door, taped shut, leads directly to a pit in the ground. But at the end of the hall, in a pristine lab, sits a gleaming cylindrical apparatus of copper and gold. It's a prototype of a device that might one day answer a major mystery about the Universe by detecting a particle called the axion \u2014 a possible component of dark matter. If it succeeds, this apparatus has the potential to rewrite physics and win its designers a Nobel prize. \u201cIt will transform Korea, there's no question about it,\u201d says physicist Yannis Semertzidis, who leads the US$7.6-million-per-year centre at South Korea's premier technical university, KAIST. But there's a catch: no one knows whether axions even exist. It's the kind of high-risk, high-reward project that symbolizes the country's ambition to become a world leader in basic research. South Korea is spending heavily to achieve its goal. In 1999, the country's investment in research and development (R&D) totalled 2.07% of its gross domestic product (GDP), just below the average for nations in the Organisation for Economic Co-operation and Development (OECD). In the latest figures, the country has stretched out a clear lead at the top. The 4.29% (63.7 trillion won, or US$60.5 billion) that South Korea invested in R&D in 2014 outstrips runner-up Israel (at 4.11%), as well as regional competitor Japan and the United States. The biggest chunk of the money goes towards applied research and development in industry, but the government has made major investments in basic science, too. The big hope is that the country can innovate its way out of a looming economic crisis \u2014 and win a Nobel prize in the process. South Korea aims to increase its investment to 5% of GDP by 2017, and last month, President Park Geun-hye's government announced that it would boost annual basic-science funding levels by 36% by 2018, to 1.5 trillion won. \u201cBasic research starts with intellectual curiosity among scientists and technicians, but it could be a source of new technologies and industries,\u201d Park said. Can the country achieve its ambition? That depends who you ask. Some Korean scientists and policymakers doubt that it can sustain its high level of investment, and they worry that cultural barriers and bureaucracy are hindering research. Young scientists are voting with their feet: according to figures released in 2014 by the US National Science Foundation (NSF), nearly 70% of South Koreans who were awarded PhDs in the United States in 2008\u201311 planned to stay there. Reorienting the nation's science focus is no easy task, says Youngah Park, president of the Korea Institute of S&T Evaluation and Planning (KISTEP), a government think tank in Seoul. The country has long been an industry-focused 'fast follower' \u2014 excelling at quickly adopting technologies and products, such as semiconductors and smartphones, and making them better and cheaper. Now, Korea needs a new model, she says. \u201cThat is a very challenging and adventurous scheme for us.\u201d \n               Shock and awe \n             When the artificial-intelligence (AI) program AlphaGo beat Korean grandmaster Lee Sedol at the game Go this March, the impact on the national psyche was profound. The AlphaGo shock, as it came to be known, showed the country that AI was the future: Korea must catch up to the likes of Google DeepMind in London, which invented the Go-playing machine. Within days, President Park announced that the government  would invest 1 trillion won in AI by 2020, and prod the private sector into investing a further 2.5 trillion won . The initiative's cornerstone would be a public\u2013private research institute involving corporations such as Samsung and LG. But many scientists criticized the approach as a knee-jerk reaction that would funnel government money into product development, not into the type of basic research that the country needs. The funding injection was typical of the strategy that has propelled South Korea's economy over the past few decades: the government set goals and then channelled money to corporate partners to carry them out. The formula was devised by Park's father, dictator Park Chung-hee, who seized power in a 1961 coup. During his 18-year reign, he favoured companies that grew into behemoths \u2014 conglomerates, called  chaebol  in Korea, such as Samsung, LG and Hyundai, which remain the backbone of the nation's economy today. Powered by these industries, five decades of economic growth vaulted South Korea from developing-world poverty to membership of the group of 20 (G20) leading industrial nations. As the country moved painfully from dictatorship to democracy, government support for research remained a bipartisan priority \u2014 mainly as a driver for further growth. Korea's corporate giants still dominate the R&D scene. According to KISTEP figures, of the 63.7 trillion won spent on R&D in 2014, 49.2 trillion came from private enterprises. That includes more than half of the 11.2 trillion won spent on basic research. Much industrial research happens behind closed doors, although partnerships with academia are on the rise. Meanwhile, government-funded labs also worked mainly towards developing industrial technologies, and blue-sky, basic research remained an afterthought. \u201cPoliticians don't distinguish between R&D in technology and support in basic science,\u201d says physicist Doochul Kim. Until recently, he says, \u201cthere has been no support in basic science, basically\u201d. Change arrived during the run-up to the 2007 presidential election, when a group of researchers pitched an idea to the nation's leading politicians: that the country  build an Institute for Basic Science (IBS) . The organization would be Korea's answer to Germany's academically elite Max Planck institutes and Japan's RIKEN centres. \u201cIt was the first time that scientists went forward and suggested their own big project for the nation,\u201d says Youngah Park, who was a legislator with the conservative party at the time. The institutes would be part of an even bigger plan to create a research and business megahub called the International Science and Business Belt \u2014 and this became government policy when conservative candidate Lee Myung-bak won the election. Political wrangling subsequently forced the government to scale back some of its plans, but IBS survived, in modified form. Fifty IBS centres, one-third of them in Daejeon, would be funded at an average of 10 billion won a year each for at least 10 years \u2014 a boon for researchers, who would be offered secure support to pursue their ideas. \u201cWe have large funding and you can do whatever you want to,\u201d says Kim, now president of the IBS. Today, 26 of the centres have opened, with the rest hoped to follow by 2021. \n               Axion race \n             The Center for Axion and Precision Physics (CAPP) at KAIST is one of them. Semertzidis became head of the centre in 2013, moving from Brookhaven National Laboratory (BNL) in Upton, New York. In its quest to find the axion, CAPP is chasing a high-profile rival in the United States: the Axion Dark Matter Experiment (ADMX), based at the University of Washington in Seattle. \u201cVery smart people, absolutely,\u201d Semertzidis says, with a disarming grin. \u201cBut we'll win, nonetheless \u2014 absolutely.\u201d If axions are indeed part of dark matter, they should be all around us. CAPP's design \u2014 like that of the ADMX \u2014 uses a cavity that should resonate at the axion's mass, with strong magnets outside it that cause the particles to convert into two photons and pop into sight. But physicists don't know what the axion's mass is, so they have to scan for it, tuning the resonant frequency of the cavity with rods of copper or sapphire. It will take years for a single device to cover the whole range of possible frequencies. CAPP has at least a year of development left, whereas ADMX is already beginning operations, giving it a significant head start. But CAPP plans to build not one, but seven cavities \u2014 all in that hole in the ground, down the hall. And it has more powerful magnets, developed at BNL. \u201cWe'll do it seven times better, because of the sheer power of money,\u201d says Semertzidis, who thinks that his team can leapfrog the ADMX within five years. ADMX leader Leslie Rosenberg suspects that it could, too. \u201cCAPP is by far our most credible competitor,\u201d he says. Whatever the outcome, Rosenberg says that CAPP's progress is a milestone for Korean physics. The country's willingness to spend landed foreign talent and technology. \u201cThese new IBS centres have moved them into the top tier,\u201d he says. The other 25 existing centres are pushing into fields ranging from gene editing to nanomaterials and pure mathematics. Roughly one-third of the IBS's budget is devoted to one flagship effort \u2014 the Rare Isotope Science Project (RISP) in Daejeon, which seeks to build a heavy-ion accelerator for nuclear science and biomedical research. South Korea is also investing in basic-research facilities outside of IBS. The Pohang Accelerator Laboratory is receiving a 400-billion-won upgrade to house an  X-ray free-electron laser  that can image materials on nanometre and femtometre scales. And in 2014, the nation completed construction on a 107-billion-won, state-of-the-art Antarctic research centre in Terra Nova Bay, which quickly became the envy of the polar-research community. \u201cIt was like a spaceship had landed,\u201d said US National Science Foundation polar-science head Kelly Falkner in 2014, months after attending the sleek facility's opening ceremony. \u201cIt's amazing to see what they can do by starting from scratch.\u201d Rosenberg, for one, says that Korea is wise to invest in the IBS centres. \u201cIf they can continue to afford it, I think the pay-off is going to be enormous.\u201d And if they find the axion? \u201cOh my goodness, well, let's say it would instantly be a Nobel prize.\u201d And that is something that this country wants very much indeed. \n               Nobel dreams \n             Last October's Nobel-prize announcements triggered a wave of disappointment \u2014 again. There were no awards for South Korean researchers, but scientists in Japan, the nation's most bitter regional rival, collected shares in two: Satoshi \u014cmura for developing a therapy for roundworm, and Takaaki Kajita for showing that neutrinos have mass. \u201cWhy no Korean Nobel laureates?\u201d asked a headline in  The Korea Times . The question came up again at an oversight hearing of South Korea's parliamentary science committee, held that week. One member of parliament compared the full list of the two countries' Nobel laureates in science to a dismal football result: Japan 21, South Korea 0. \u201cWhen will IBS score a goal?\u201d he asked Kim. In some political quarters, IBS was originally hailed as a way to level the Nobel score, but Kim has pushed back against that, arguing that the 'Nobel complex' leads to shortsighted policies that chase hot topics and demand instant results. \u201cWe are only four years old,\u201d he told the committee. He noted that it took decades to develop the infrastructure at Japan's Kamiokande Observatory near Hida, where the neutrino breakthrough was made. \u201cSo you shouldn't ask that question,\u201d he said. Korea did seem poised for a Nobel just over a decade ago, when stem-cell scientist Woo Suk Hwang claimed to have derived the world's first stem-cell lines from cloned human embryos. But glory quickly turned to shame when Hwang was first found guilty of ethics violations in the way he collected women's eggs for research, and then discovered to have  fabricated some of his work . The scandal left the impression that the country's oversight of research ethics and integrity was lax. Scientists in Korea say that the scandal has brought about positive changes. Slowly, more Korean journals have begun to issue retractions, says Eric di Luccio, a structural biologist at Kyungpook National University in Daegu, and many universities are using the plagiarism-detection site turnitin.com to check papers and theses. More attention is also being paid to bioethics, says Jin-Soo Kim, director of the IBS Center for Genome Engineering at Seoul National University. \u201cBefore the Hwang scandal, in the laboratory, people would just draw blood and do experiments,\u201d he says. \u201cNow it's recognized that you shouldn't do it without approval\u201d from an institutional review board. But Kim says that one 'Hwang-gate' reform is now holding Korea back: in the wake of the scandal, the government enacted a ban on human-embryo research, with only occasional exceptions granted for stem-cell studies. Kim has been at the forefront of developments in  CRISPR\u2013Cas9 gene editing , a technique that is revolutionizing biomedical research, but he has found himself unable to use the technology for research in human embryos, even as teams in China, the United Kingdom and elsewhere  forge ahead with such work . \u201cIt's a pity,\u201d says Kim, who has instead focused his efforts on  engineering pigs  and plants. Researchers also chafe at other regulations. At public universities, tenure and promotion decisions are often based in part on evaluations that count papers by fractional contribution: a four-author paper, for example, would earn a scientist a small fraction of the credit of a single-author one. The system is \u201crather counterproductive\u201d, says di Luccio. It dissuades scientists from taking part in the large international collaborations of modern big-budget science, and encourages them to publish single-author papers in less-prestigious national journals to juice up their evaluation scores. \u201cI did it three times already,\u201d he says. \u201cThis evaluation system is the exact opposite of what it should be to elevate scientific research.\u201d The government says that universities and other organizations are free to implement their own standards of evaluation and that nationally funded programmes use more qualitative measures. (IBS insulates researchers from paper counts.) Some scientists see deeper problems with the academic culture, rooted in Korean society at large. Secondary and undergraduate education focus on test-taking and emphasize deference to teachers \u2014 tendencies that academics bemoan as discouraging the creativity and debate necessary in a lab. \u201cWhen new students come, they are quiet \u2014 that is the Korean culture,\u201d says Jin-Soo Kim, who counters this by requiring his students to ask questions before they can leave group meetings. Korean customs were a turn off for Young-Im Kim, who was doing a physics postdoc at the University of Oxford, UK, in 2014, when a friend sent her a link to a job posting at CAPP. Although she thrilled to the research, she was hesitant to return to her home country because of the hierarchical nature of Korean culture. \u201cThe only reason I applied is because of Yannis,\u201d she says. \u201cIf he were Korean, I wouldn't have.\u201d She is now a research fellow at CAPP. Cultural barriers can have a disproportionate impact on female scientists. One example, says Young-Im Kim, is Korean drinking culture, in which men often stay out late with their male co-workers. Important workplace decisions are often made at such events, effectively excluding women. Such problems could go some way towards explaining why Korea has a wide gender gap in its scientific workforce. According to OECD figures, in 2010 less than 17% of researchers in South Korea were women. In Portugal, the OECD leader, the fraction is 45.5%. \n               Stretched resources \n             Policy analysts warn that research spending may slow in the future, as Korea faces the likely prospect of an economic slowdown and, in the long term, a social-welfare net stretched to support an ageing population with one of the lowest birthrates in the world. And although R&D expenditure continues to grow as a percentage of GDP (see 'Science in South Korea'), it is actually shrinking when viewed as a percentage of government spending, says Youngah Park. \u201cThat is a sign that we have no room to increase this government R&D budget anymore.\u201d Some critics say that spending is now too focused on IBS: that the bold plan is sucking up basic-research funds and reducing the pot of money available through other grants, creating a situation of haves and have-nots, and potentially quenching original \u2014 perhaps Nobel-worthy \u2014 projects. The shrinking grant pool is a legitimate issue, acknowledges Doochul Kim. But he thinks that the government should address it by shifting funds from applied research. It continues to subsidize such research when many say that it should be focusing on long-term basic research that industry wouldn't pursue. \u201cThere is some excellent science done in Korea, but still, in general, the average is not as good as the advanced countries like the US, UK and Germany,\u201d says Jinwoo Cheon, director of the IBS Center for Nanomedicine at Yonsei University in Seoul. To spur investment in basic research, he adds, scientists have to convince the public and government officials of its intangible benefits. \u201cExcellence in basic science is not easy to have, and it has to be rooted in our society \u2014 curiosity-driven research, and knowing different ways of thinking.\u201d Sunchan Jeong, director of RISP, says that if there is such a thing as a recipe for winning a Nobel prize, then IBS has got it. \u201cSelect some competitive fields in the world and concentrate their investment on it. That's a good way.\u201d But there are no guarantees, he cautions: \u201cThe people in Korea should understand that scientific results are not necessarily repaid by some greater prize like the Nobel.\u201d \n                 Tweet \n                 Facebook \n                 LinkedIn \n                 Weibo \n                 Wechat \n               \n                     South Korea trumpets $860-million AI fund after AlphaGo 'shock' 2016-Mar-18 \n                   \n                     South Korea stretches lead in research investment 2016-Feb-08 \n                   \n                     South Korean survey ships open up to science 2015-Jan-06 \n                   \n                     Cloning comeback 2014-Jan-14 \n                   \n                     South Korea makes billion-dollar bet on fusion power 2013-Jan-21 \n                   \n                     South Korean research centre seeks place at the top 2012-May-17 \n                   \n                     Institute for Basic Science \n                   \n                     Korea Institute of S&T Evaluation and Planning (KISTEP) \n                   \n                     Jang Bogo Station, Antarctica \n                   \n                     Pohang Accelerator Laboratory \n                   Reprints and Permissions"},
{"file_id": "534170a", "url": "https://www.nature.com/articles/534170a", "year": 2016, "authors": [{"name": "Paul Tullis"}], "parsed_as_year": "2006_or_before", "body": "Pieter Dorrestein uses mass spectrometry to eavesdrop on the molecular conversations between microbes and their world. Apart from the treadmill desk, Pieter Dorrestein's office at the University of California, San Diego (UCSD), is unremarkable: there is a circular table with chairs around it, bookshelves lined with journals, papers and books, and a couple of plaques honouring him and his work. But Dorrestein likes to offer visitors a closer look. On his computer screen, he pulls up a 3D rendering of the space. Four figures seated around the table \u2014 one of whom is Dorrestein \u2014 look as if they've been splashed with brightly coloured paint. To produce the image, researchers swabbed every surface in the room, including the people, several hundred times, then analysed the swabs with mass spectrometry to identify the chemicals present. The picture reveals a lot about the space, and the people in it. Two of Dorrestein's co-workers are heavy coffee drinkers: caffeine is splotched across their hands and faces (as well as on a sizeable spot on the floor \u2014 a remnant of an old spill). Dorrestein does not drink coffee, but has left traces of himself everywhere, from personal-care products to a common sweetener that he wasn't even aware he'd consumed. He was also surprised to find the insect repellent DEET on many of the surfaces that he had touched; he hadn't used the chemical in at least six months. Then there were signatures of the office's other inhabitants: the microbes that reside on human skin. Dorrestein has been using mass spectrometry to look at the small molecules, or metabolites, produced by these microbes, and to get a clearer picture of how microorganisms form communities and interact \u2014 with other microbes, with their human hosts and with the environments that they all inhabit. He has analysed microbial communities from plants, seawater, remote tribes, diseased human lungs and more, in an effort to listen in on their chemical conversations: how they tell one another of good or bad places to colonize, or fight over territory. The work could identify previously unknown microbes and useful molecules that they make, such as antibiotics. \u201cThe applications are broad,\u201d says Katie Pollard, a comparative genomicist at the Gladstone Institutes at the University of California, San Francisco. Because many microbes cannot be cultured and studied directly, she explains, \u201cthese approaches that assay them  in situ  are totally game-changing\u201d. They also directly address some of the main goals outlined in the  US$521-million National Microbiome Initiative , announced by the White House's Office of Science and Technology Policy last month. Dorrestein was present for the announcement. In this fast-moving field, Dorrestein has set himself apart by building useful tools and productive collaborations. \u201cPieter is genuinely interested and very creative,\u201d says Janet Jansson, division director of biological sciences at Pacific Northwest National Laboratory in Richland, Washington. In April, she visited UCSD, and Dorrestein asked whether he could swab her hand for one of his studies. \u201cI said, 'Oh! I want to do that! I want to be involved in that study!'\u201d Jansson recalls: \u201cIt's interesting and exciting science that people want to participate in.\u201d \n               Rock and roll \n             Dorrestein grew up in the Netherlands, and became obsessed with rock-climbing when he visited family friends in Tucson, Arizona, at the age of 16. Faced with the flatness of his homeland, he applied to Northern Arizona University in Flagstaff, in large part because of its proximity to the many stone towers of the Four Corners region, where Arizona meets New Mexico, Colorado and Utah. He studied geology and chemistry, but intended to pursue his passion for climbing. Shortly after graduating in 1998, however, an experience on the 900-metre-tall face of El Capitan in Yosemite, California, made him think again. He was clinging to the rock about 50 metres above his last anchoring point, and realized that if he were to lose his grip, he would drop 100 metres before his safety line tautened and slammed him into the granite. It wasn't fear, he says, but rather his lack of it that troubled him. \u201cI thought, if I keep doing this, it won't be a good ending,\u201d he recalls. \u201cSo I rappelled down.\u201d He drove home to Flagstaff that day, and started filling out applications to graduate school. He ended up at Cornell University in Ithaca, New York, studying how microbes produce small molecules such as vitamin B1. It was here that he was first introduced to mass spectrometry. Mass spectrometry generally involves breaking complex molecules apart, ionizing them and measuring the mass of the resulting fragments, which can be used to calculate the composition of the starting molecules. Dorrestein uses the analogy of a bar code \u2014 mass spectrometry creates a unique identifier for each chemical in a sample. Spurred by his interest in the technology, he went on to do a postdoc in the lab of Neil Kelleher, a chemical biologist at the University of Illinois at Urbana\u2013Champaign. Kelleher was pioneering efforts to do 'top-down' mass spectrometry, in which intact, rather than digested, proteins are put directly into the mass spec. The approach allows researchers to identify small modifications made to proteins, but the process is slow. Within two months of his arrival in Illinois, Dorrestein had developed a speedier approach that allowed him to examine certain large enzymes systematically 1 . \u201cWe boiled down years of work into days, basically,\u201d Dorrestein says. He ended up co-authoring 17 papers in 2 years. \u201cPieter has that unusual combination of creativity and drive, along with an incredible ability to finish projects,\u201d says Kelleher, who is now at Northwestern University in Evanston, Illinois. Dorrestein joined the faculty at UCSD in 2006 \u2014 but things really kicked off for him when Palmer Taylor, then dean of the university's school of pharmacology, authorized the purchase of a MALDI-TOF mass spectrometer (matrix-assisted laser desorption/ionization time of flight), which would allow Dorrestein to do mass-spectrometry imaging. \u201cThat changed the whole world around,\u201d he says. \n               Space crusaders \n             As well as identifying molecules in a sample, mass-spectrometry imaging provides spatial information. MALDI-TOF uses a laser to heat up and ionize molecules. By scanning that laser across a 2D sample, researchers can capture an 'image' that shows exactly where different molecules in the sample reside. The technique can be used to identify and locate biomarkers in slices of tumours, but with his interest in microbes, Dorrestein wondered whether he could take colonies of bacteria on a Petri dish and scan them directly to see the metabolites they produce. No one had ever tried it. Dorrestein suspects that they were afraid of getting their expensive mass spectrometers dirty \u2014 \u201cand this is as dirty as it comes, putting microbes directly into the instrument\u201d. So he tried a simple experiment, asking an undergraduate student, Sara Weitz, to scan a colony of  Bacillus  bacteria. The images generated \u201cweren't the prettiest\u201d, Dorrestein says, but they indicated that the process worked. He sent them to Paul Straight, a microbiologist who had just joined the faculty at Texas A&M University in College Station. \u201cI'm pretty sure his jaw dropped,\u201d Dorrestein says. Together, the two teams used mass-spectrometry imaging on colonies of  Bacillus subtilis  and  Streptomyces coelicolor  grown next to one another. By exploring the spaces where the colonies interacted, they were able to identify molecules that the microbes use to compete with each other 2 . Actually visualizing this microbial arms race, Dorrestein says, makes him think back to 1928, when Alexander Fleming isolated penicillin from a mould that was killing bacteria on a dish. Mass-spectrometry imaging could quickly reveal the chemistries of such interactions, and perhaps speed up the search for new antibiotics. Dorrestein decided to shift his lab to focus almost exclusively on these methods. He was still an early-career investigator, and almost everybody he knew discouraged him from taking such a big risk. But Taylor pushed him to apply for tenure right away. \u201cPieter's potential to think outside the box in the analytical and computational arenas was immediately evident,\u201d Taylor says. \u201cHis research took off very rapidly.\u201d The problem with looking at dirty samples is that they produce messy data. Scanning microbial landscapes produces thousands of bar codes, but it's largely unknown what they correspond to; they haven't been annotated. \u201cIt's the equivalent of looking under the lamp post,\u201d Dorrestein says: one can only 'see' the molecules that have been identified before, and the vast majority haven't. This is currently a big challenge for the field, says Jansson. \u201cIt's possible to analyse features by mass spec, but still very difficult to identify what those features are.\u201d To help to make sense of the heaps of data, Dorrestein worked with Nuno Bandeira, a computational biologist at UCSD, on an approach that classifies bar codes and the molecules to which they correspond according to their relationships with other annotated molecules 3 . This allows researchers to start predicting, computationally, the structures and functions of thousands of metabolites. But there's still a dearth of annotation: although thousands of people worldwide conduct mass-spectrometry research, most annotate only the few molecules that they're interested in. So, beginning in 2014, Dorrestein and graduate student Mingxun Wang from Bandeira's lab started to develop a way to crowdsource annotation. They launched the  Global Natural Products Social Molecular Networking website , a repository and data-analysis tool that enables researchers to uncover relationships between related molecules, group similar ones together and compare data sets. \u201cThis is something he's brought to the field that has really helped,\u201d says Jansson. \n               Team work \n             One of the keys to Dorrestein's success has been his collaborations.  Rob Knight , a leader in microbiome DNA and RNA sequencing , works just across the quad from Dorrestein's office. They've teamed up to blend sequencing with mass spectrometry. Last year, a postdoc in Dorrestein's lab, Amina Bouslimani, took swabs from one male and one female volunteer, at 400 spots on their bodies \u2014 twice. One swab from each spot went to Knight's lab so that the microbes in it could be sequenced, and the other went for mass spectrometry to identify the chemicals, natural and artificial, that coexist with the microorganisms. The participants had refrained from showering or using cosmetics for three days, but the chemical signatures from the hundreds of different types of microbe in the samples were overwhelmed by chemicals from beauty and hygiene products 4 . Still, the researchers did find correlations between microbe communities and local chemistries: for example, the bacteria found in the vaginal area were correlated with molecules associated with inflammation. Such connections, Dorrestein says, could be used to generate hypotheses about host\u2013microbe interactions. Bouslimani is now analysing samples from volunteers' hands and from personal items such as their mobile phones. The work, which has not yet been published, has shown that people leave persistent chemical signatures on the objects that they touch \u2014 like those in the image of Dorrestein's office. Bouslimani and Dorrestein think that this could have applications in forensic science. A suspect could be swabbed to determine whether the chemical signature of his or her skin matches that at a crime scene. Or in the absence of DNA or fingerprint evidence, the chemicals that a criminal leaves behind could help to provide a lifestyle profile: a composite sketch of the products that they use and the mixture of microbes they carry. \u201cMaybe the chemical signature can help the investigator narrow down who was there,\u201d says Bouslimani. Last year, Dorrestein teamed up with microbiologist Maria Dominguez-Bello of New York University and several others who wanted to see what human skin and its microbial diversity look like when people grow up free of the trappings of the developed world. They collected samples from some remote tribes \u2014 one near Manaus, Brazil, and Tanzania's Hadza people \u2014 and compared them with  swabs from non-tribal people near the collection sites . Using Dorrestein's mass-spectrometry techniques, they've found that people in the tribes have more-diverse microbial communities and skin chemistry than those living a more modern lifestyle. The ongoing work is serving up some surprises too, says Dorrestein. People from one village in Brazil had a range of pharmaceuticals on their skin, indicating that they had more contact with outsiders than previously suspected. Dorrestein has a way of leaning forward and almost standing on his toes in excitement when he talks about the technology and how it might help to assess the health of oceans, or improve efficiency in agriculture, a major contributor to greenhouse-gas emissions. But when asked how he chooses projects to pursue, it's work on human health that he mentions first. \u201cTo us, that's a really obvious, direct application of this \u2014 we want to help patients,\u201d he says. Dorrestein teamed up with Knight, Doug Conrad \u2014 director of UCSD's adult cystic fibrosis clinic \u2014 and others to develop a rapid microbial diagnostic test. Cystic fibrosis causes a build-up of mucus in the lungs, which can periodically become infected with bacteria. These infections require aggressive treatment with antibiotics \u2014 and sometimes the bacteria can develop resistance. Dorrestein and his collaborators have shown 5  how analysing mass-spectrometry data on a phlegm sample from someone with cystic fibrosis can identify microbial communities that standard medical culturing techniques miss. Louis-F\u00e9lix Nothias-Scaglia, a postdoc who joined Dorrestein's lab this year, is mapping the skin of people with psoriasis, a condition thought to be triggered by an overactive immune system. If molecules produced by certain bacteria are present when the condition flares up but not when the skin is healthy, Nothias-Scaglia explains, they might point to drugs that could treat or even prevent the disease. Even being able to use microbial changes to predict when a flare-up is coming would enable patients to reduce their use of immune-suppressing drugs. Turning such data-intensive techniques into standard lab tests will be a challenge. \u201cCynics would say it's too complicated, it's never gonna go anywhere,\u201d says Conrad. \u201cTo a certain extent, I can understand that. But that's a good way to keep going the way things are.\u201d Dorrestein definitely wants to change the way things are, particularly for the blossoming field of microbiome research. He views the discipline as passing through phases: the first has centred on determining the identity of microbes. The second phase is working out what they're doing, using techniques such as mass spectrometry. What drives the establishment of these communities? What metabolic processes are under way, and how do they interact with each other and with a host? \u201cIf you fundamentally understand that,\u201d Dorrestein says, \u201cyou can start to take control of it.\u201d And that's the third phase, he says \u2014 taking control. By monitoring microbial communities, is it possible to add the necessary ingredients to change a person's health, their mood, their athletic performance? Dorrestein thinks that the answers to these questions are right in front of him. He just has to look a little closer. \n                 Tweet \n                 Follow @NatureNews \n               \n                     White House goes big on microbiome research 2016-May-13 \n                   \n                     Microbiology: Create a global microbiome effort 2015-Oct-28 \n                   \n                     The tantalizing links between gut microbes and the brain 2015-Oct-14 \n                   \n                     Microbiome therapy gains market traction 2014-May-13 \n                   \n                     Global Natural Products Social Molecular Networking \n                   Reprints and Permissions"},
{"file_id": "534024a", "url": "https://www.nature.com/articles/534024a", "year": 2016, "authors": [{"name": "Corie Lok"}], "parsed_as_year": "2006_or_before", "body": "Mathematician Lydia Bourouiba uses high-speed video to break down the anatomy of sneezes and coughs \u2014 and to understand infectious disease. So, how do you get your research subjects to sneeze on cue? \u201cThat's a question I get a lot,\u201d says Lydia Bourouiba with an easy smile. The solution turns out to be surprisingly simple: just take a small, rod-shaped device, use it to tickle a subject's nostril for a few seconds, and \u2014 achoo! For Bourouiba, a mathematician and fluid dynamicist, that sneeze is the pay-off. She and her team at the Massachusetts Institute of Technology (MIT) in Cambridge record the explosive aftermath in gross detail using one or sometimes two cameras running at thousands of frames per second. Played back in slow motion, the videos reveal a violent explosion of saliva and mucus spewing out of the mouth in sheets that break up into droplets, all suspended in a turbulent cloud. The videos that Bourouiba has recorded in this way allow her to measure everything from the diameter of the droplets to their speed \u2014 data that help her to learn more about how these particles carry viruses and other pathogens to their next host. She has shown that sneeze and cough particles can travel the length of most rooms and can even move upwards into ventilation shafts \u2014 suggesting that microbes in the droplets could potentially spread farther and over longer periods of time than current theories suggest. Ultimately, says Bourouiba, her goal with this work is to ground epidemiology and public health in physics and mathematics. When trying to keep diseases from running rampant, she says, \u201cwe want to be giving recommendations that are based on science that has been tested in the lab\u201d. In practical terms, such insights could lead to maps showing  the contamination risks in the vicinity of infected people , protective equipment optimized to shield hospital workers from specific kinds of germs, and better predictions of how diseases move through a population. Bourouiba pursues this goal with the same energy and ambition that leads her to fill her leisure time with week-long bike trips, mountain climbing \u2014 she ascended Tanzania's Mount Kilimanjaro in 2011 \u2014 and winter camping at \u221220 \u00b0C. Although she is hardly the first researcher to use high-speed video to study fluid dynamics, she is the first to realize its potential in the respiratory field, says David Ku, a biofluid-mechanics researcher at Georgia Institute of Technology in Atlanta. Bourouiba's approach could be transformative in the field, says Ron Fouchier, a virologist at the Erasmus University Medical Center in Rotterdam, the Netherlands. \u201cThis kind of physics is absolutely needed to understand how transmission works.\u201d \n               A fluid career \n             Bourouiba has been a natural explorer as far back as she can remember. As a child in France, she immersed herself in books about science and nature, including a biography of Albert Einstein. She soon fell in love with mathematics and physics, and made them her major subjects when she earned her undergraduate degree in France and Montreal, Canada. But during her graduate work in fluid mechanics at Montreal's McGill University, as she focused on narrower and narrower theoretical questions about turbulent flows, Bourouiba began to feel the itch for something more. She had spent some of her early years in Algeria during the civil war of the 1990s, and vividly remembered the turmoil and misery that she had witnessed there. \u201cWe know what the worst is, we saw a lot of it,\u201d she says. \u201cBut what can we as a species do to push that boundary of what we can achieve, in terms of making the world a better place?\u201d In her search for an answer, Bourouiba soon homed in on health and epidemiology. This was the mid-2000s, and emerging diseases were all over the news.  Severe acute respiratory syndrome (SARS)  had killed nearly 800 people around the world in 2003, polio was making a comeback and  avian flu was jumping across to humans . Infectious diseases seemed to Bourouiba like the perfect way to combine all of her interests and expertise. She was tentative at first. A career in fluid mechanics promised to be secure and certain, whereas a head-first dive into biology seemed like a huge risk. But one day, about halfway through her PhD, she was mulling this conundrum as she made her way up the wall at a rock-climbing gym. \u201cSo what?\u201d Bourouiba suddenly said to herself as she reached for the next handhold. \u201cYou can't make decisions out of fear.\u201d \n               Violent events \n             Having come so far, Bourouiba saw her fluid-dynamics PhD to completion in 2008. But from there she managed to land a postdoc appointment in mathematical epidemiology at York University in Toronto, Canada, where she started thinking about sneezes and coughs. These 'violent expiratory events' (as one of Bourouiba's papers calls them) were assumed to be one of the main ways that respiratory diseases spread. But how, exactly? Epidemiological studies estimate how a disease is transmitted on the basis of people's movements and activities at the time they got infected. Did they contract the disease by direct, person-to-person contact, such as shaking someone's germ-covered hand, or from contaminated surfaces such as doorknobs? Was it through large droplets that make a short leap from one respiratory tract to another, or through smaller aerosol particles that are suspended in air and can travel farther before being inhaled? Or was the route some combination of these modes? Such studies have helped researchers to work out that measles is typically spread by aerosols and that  Ebola is transmitted mainly through direct contact with infected bodily fluids . But there is still a lot of uncertainty for many pathogens, which hampers the ability of public-health officials to control the spread of disease during outbreaks and to prepare for future ones. SARS, for example, is thought to spread mainly through close contact, yet the 2003 outbreak showed at least some evidence of airborne transmission 1 . And some researchers think that  Ebola viruses might travel through air  to some degree 2 . At York, Bourouiba became convinced that these uncertainties could be reduced by pinning down some key details about the physics of sneezes and coughs that conventional disease-transmission models are missing. In 2010, a postdoc appointment at MIT gave her a chance to start filling those gaps with hard data. Up to that point, she had worked only on theory \u2014 but now she plunged into experimental research, learning through trial and error the subtleties of high-speed video and lighting to capture a sneeze. \u201cMathematicians are often uncomfortable in a lab setting,\u201d says John Bush, a fluid dynamicist who was her mentor at MIT. \u201cLydia really took to it.\u201d \n               Suspended spray \n             One thing that Bourouiba particularly wanted to pin down was the size distribution of the droplets coming out of the mouth, because size affects how many microbes a droplet can carry and how far it can travel through the air. For her first set of experiments, published in 2014, she wanted to look at the entire spray of droplets 3 . Bourouiba posted adverts around the MIT campus to recruit volunteers, and filmed the coughs and sneezes of about ten healthy people. After much tinkering with camera positions, backgrounds and lighting levels \u2014 at one point, the lights made the room uncomfortably hot for participants \u2014 Bourouiba captured videos that showed that the droplets were propelled out of the mouth in a turbulent, buoyant cloud. The cloud grew and slowed down as it pulled in air from the environment, lifting and carrying the droplets away from the sneezer. The video evidence contradicted conventional thinking about sneezes, which held that larger droplets would fall to the ground within 1\u20132 metres, and that only the smaller ones would stay aloft as airborne aerosols. Feeding her video evidence into her mathematical models, Bourouiba concluded that, thanks to the cloud dynamics, many of the larger droplets can travel up to 8 metres for a sneeze and 6 metres for a cough, depending on the environmental conditions, and stay suspended for up to 10 minutes \u2014 far enough and long enough to reach someone at the other end of a large room, not to mention the ceiling ventilation system. That conclusion has implications for health-care workers, says James Hughes, an infectious-disease epidemiologist at Emory University in Atlanta. If a disease is thought to be transmitted within 1\u20132 metres, workers might assume that they are safe beyond that zone. \u201cI think maybe we need to be a little bit more circumspect about that,\u201d he says. For Bourouiba's next set of experiments 4 , she zoomed in closer to the mouth to film a 150-millisecond-long sneeze. Videos taken from the side and top at up to 8,000 frames per second revealed that the fluid breaks up in steps, like a slow-motion explosion produced by Hollywood: the fluid emerges from the mouth in sheets, which are then punctured and form rings as they are stretched by the airflow. The rings fracture, leaving filaments. Little beads of fluid form on the filaments, which elongate and fragment to finally produce droplets. Bourouiba was surprised to find so much happening to the fluid outside the mouth \u2014 it countered the prevailing assumption that droplets exit the mouth fully formed. To Gerardo Chowell, a mathematical epidemiologist at Georgia State University in Atlanta, this is an important finding because it means that droplet formation could be strongly influenced by environmental conditions such as humidity and temperature. And that could help to explain why some diseases, such as flu, tend to occur more frequently at certain times of the year, he adds, perhaps because the ambient conditions favour the spread and survival of certain microbes. Bourouiba's research advances previous work measuring sneeze and cough droplet sizes, says Ku. Fluid particles can travel varying distances depending on a lot of different parameters, he says. \u201cIf I just tell you the size of the particles, I can't tell you where they're going to go. Her work actually shows where they go, with a real sneeze.\u201d \n               The next level \n             A back injury last year has curtailed some of Bourouiba's more ambitious outdoor activities. But at work, she and her team are preparing to move into a newly built lab with a biosafety-level-2+ containment room, which will allow them to study the sneezes and coughs not just of healthy participants, but of people infected with colds and flu. In preparation for those studies, she has hired a microbiologist who can help the team to determine the microbial load in droplets and how long pathogens survive in the air or on surfaces while maintaining their ability to infect. Answering this question will be crucial, says Hughes. \u201cWe need to learn more about the concentration of microbes in droplets of varying sizes and the infectious doses of a lot of these pathogens.\u201d The containment room will also allow Bourouiba to control the airflow, temperature and humidity so that she can explore the behaviour of emitted droplets in environments that mimic hospitals, aeroplanes or the tropics. Bourouiba's ultimate aim is to compile all of her data into a mathematical model that could be used by public-health officials to identify the most likely routes of transmission and how to reduce the risk of disease spread. The model would suggest, for example, whether the biggest risk of contamination is from the air or from surfaces, or how to change the airflow or temperature to minimize the risk in a hospital. It could predict whether a particular person is at high risk of being a 'superspreader' and should be quickly placed in a containment unit. During an emergency situation, when a new disease is spreading but it's not clear how, it might also help officials to identify the most dangerous environments, such as aeroplanes, so that people can avoid them. Then, as the first infected patients are tested and more is learned about the pathogen, those data could be incorporated into the model to refine the risk assessment. Chowell, who models the spread of infectious diseases, hopes that Bourouiba's work could eventually be used to give diseases an 'airborne score'. Knowing that a pathogen is transmitted by airborne aerosols, say, 85% of the time could give public-health officials a better idea of how fast and far an outbreak will grow, compared with one that's just 5% airborne, he says. \u201cModels require data, and I think the efforts of Bourouiba and others will help us better calibrate the design of these models, and this will have an impact on our ability to forecast disease spread in real time.\u201d That may depend on the disease, however. Work from Donald Milton, an environmental-health scientist at the University of Maryland School of Public Health in College Park, suggests that Bourouiba's approach may not have much impact on the study of influenza because people with flu rarely sneeze 5 . Studying people with the common cold might be more fruitful, he says, because they sneeze more often. Milton also cautions that focusing on sneezes and coughs may not capture the whole story of respiratory-disease transmission. Breathing and talking are important to consider as well. He and his team have detected flu viral RNA in particles that were simply exhaled by patients, and they have even cultured viruses from such particles. Bourouiba says that she can study breathing emissions using her methods if they turn out to be a factor, but she first wants to study infected people to see which are the most important emissions to examine. One occupational hazard for Bourouiba is that it's difficult to escape her work: whenever she hears a sneeze on a plane or in the classroom, she can't help thinking about the droplets flying through the air. There is not much she can do about that, but it does remind her why she became so fascinated with fluid mechanics when she was an undergraduate: fluids are everywhere. Her videos might earn her the nickname of 'the sneeze lady', a student once warned her. But she says she doesn't mind. \u201cIf people get interested in the topic because of the humorous aspect, I have no problem with that.\u201d \n                 Tweet \n                 Facebook \n                 LinkedIn \n                 Weibo \n                 Wechat \n               \n                     Flu genomes trace H7N9's evolution and spread in China 2015-Mar-11 \n                   \n                     Models overestimate Ebola cases 2014-Nov-04 \n                   \n                     Progress stalled on coronavirus 2013-Sep-18 \n                   \n                     Science Friday segment on Lydia Bourouiba \n                   Reprints and Permissions"},
{"file_id": "532432a", "url": "https://www.nature.com/articles/532432a", "year": 2016, "authors": [{"name": "Alison Abbott"}], "parsed_as_year": "2006_or_before", "body": "The microbiologist spent years moving labs and relishing solitude. Then her work on gene-editing thrust her into the scientific spotlight. Emmanuelle Charpentier's office is bare, save for her computer. Her pictures, still encased in bubble wrap, are stacked in one corner, and unpacked cardboard boxes stuffed with books and papers are lined up in the adjacent room. But across the corridor, her laboratory is buzzing with activity. When Charpentier moved to Berlin six months ago, she had her science up and running within weeks, but decided that the rest could wait. \u201cWe were all determined to get the research going as fast as possible,\u201d she says, leaning forward from her still-pristine office chair. Charpentier's workspace is a fitting reflection of her scientific life \u2014 one in which she always seems to be moving while keeping science on the go. Now 48, she has climbed her way up the academic ladder by way of 9 different institutes in 5 different countries over the past 20 years. \u201cI always had to build up new labs from scratch, on my own,\u201d she says. Her eureka moments have occurred amid packing boxes and, after years on short-term grants, she was 45 before she was able to employ her own technician. \u201cShe's so resourceful, she could start a lab on a desert island,\u201d says Patrice Courvalin, her PhD supervisor at the Pasteur Institute in Paris. The itinerant lifestyle doesn't seem to have hampered the microbiologist as she has carefully dissected the systems by which bacteria control their genomes. Charpentier is now acknowledged as one of the key inventors of the gene-editing technology known as CRISPR\u2013Cas9, which is  revolutionizing biomedical researchers' ability to manipulate and understand genes . This year, she has already won ten prestigious science prizes, and has officially taken up a cherished appointment as a director of the Max Planck Institute for Infection Biology in Berlin. The gene-therapy company that she co-founded in 2013, CRISPR Therapeutics, has become one of the world's most richly financed preclinical biotech companies, and she is in the middle of a  high-profile patent dispute  over the technology. Last September, Charpentier's phone kept on ringing. Journalists from around the world were trying to reach her, thinking \u2014 prematurely, as it turned out \u2014 that the imminent announcement of the 2015 Nobel prizes might well include her. The academic limelight is not a comfortable place for Charpentier, which is why she remains the least well known member of the small international group tipped for the 'CRISPR Nobel', if it arrives. \u201cJean-Paul Sartre, the French philosopher, warned that winning prizes turned you into an institution \u2014 I am just trying to keep working and keep my feet on the ground,\u201d she says. She seems to be succeeding, this week publishing a paper 1  in  Nature  that reveals the mechanism of a CRISPR system that might prove even more efficient than CRISPR\u2013Cas9. Colleagues who know Charpentier well describe her as intense, modest and driven. \u201cShe's a tiny person, with a very strong will \u2014 and she can be pretty stubborn,\u201d says Rodger Novak, who was a postdoctoral researcher with her in the 1990s and is now chief executive of CRISPR Therapeutics. As Courvalin sees it, \u201cShe is like a dog with a bone \u2014 tenacious.\u201d \n               Medical mission \n             Small and slight, with eyes so dark that they seem black, Charpentier looks as restless as she evidently is. Growing up in a small town near Paris, she had a clear idea from the start of what she wanted in life: to do something to advance medicine. A visit to an aunt, a missionary who was living in an old convent, set her dreaming of being able to do this \u201cin a lovely setting, where you can be a bit alone with yourself\u201d. Her socially engaged parents, she says, supported her ideas without guiding her in any direction. She pursued piano and ballet \u2014 but her leaning towards medicine eventually flowered into studies in life sciences. As an undergraduate at Pierre and Marie Curie University in Paris, she decided to do her PhD at the nearby Pasteur Institute, which was gaining a strong reputation in basic research and had a programme on antibiotic resistance that she wanted to join. Her PhD project involved analysing pieces of bacterial DNA that move around the genome and between cells, allowing drug resistance to be transferred. Her years at the Pasteur Institute were formative. Her department in the historic institution was \u201cyoung and fun\u201d, she says. She loved to study at the old St Genevi\u00e8ve library close to Notre-Dame Cathedral, happily isolated in the triangle of light from the green-topped desk lamps. \u201cI realized I had found my environment,\u201d she says. Her ambition was to lead a lab at the Pasteur, and she decided that this would require a postdoc period abroad to gain expertise. \u201cI was a typical French student of the 1990s \u2014 I imagined that after a short excursion I would work the rest of my life at home.\u201d Charpentier sent out 50 or so exploratory letters to labs in the United States, and got a postbag full of offers in reply. She chose a position with microbiologist Elaine Tuomanen at the Rockefeller University in New York City to work on the pathogen  Streptococcus pneumoniae . This microbe, which is a major cause of pneumonia, meningitis and septicaemia, has a particularly free-wheeling relationship with mobile genetic elements, shifting them about its genome while maintaining its vicious pathogenicity. Tuomanen's lab had priority access to its recently sequenced genome, offering the tantalizing prospect of discovering where these elements were landing and what happened when they did. Charpentier carried out a stream of painstaking experiments to work out how the pathogen monitors and controls such elements, and contributed to a study identifying how the pathogen acquires resistance to vancomycin, an antibiotic of last resort 2 . She had set out for New York with some trepidation but, absorbed in her work, was surprised to find that she wasn't homesick. When Tuomanen moved her lab to Memphis, Tennessee, Charpentier wanted to stay, so she found a home in the lab of skin-cell biologist Pamela Cowin at New York University School of Medicine, where she also had the opportunity to learn about mammalian genes through working on mice. Cowin remembers Charpentier as her first postdoc who did not need looking after. \u201cShe just ran with the programme,\u201d she says. \u201cShe was driven, meticulous, precise and detail-oriented\u201d \u2014 as well as a rather quiet, private person. Charpentier soon discovered that genetically modifying mice was a lot harder than manipulating bacteria. She spent two years on the project and emerged with a paper on the regulation of hair growth, a solid grounding in mammalian genetics and a strong desire to develop better tools for genetic engineering. After another postdoc in New York, Charpentier knew that her next step needed to be complete independence \u2014 and a move back to Europe. Her time in the United States had taught her that she was European rather than solely French, and she chose Vienna. She arrived at the university there in 2002, and spent the next seven years running a small lab that was precariously dependent on short-term grants. \u201cI had to survive on my own,\u201d she says. Nevertheless, \u201cI had in mind to understand how every biochemical pathway in a bacterium was regulated.\u201d It was an exciting time scientifically, with the importance of small RNA molecules in regulating genes being revealed, and she embarked on many different projects on various bacteria \u2014 possibly too many, she admits, but she kept winning the grants. She discovered an RNA that controls the synthesis of a class of molecules that are important for virulence in the bacterium  Streptococcus pyogenes 3 . It was in Vienna that Charpentier first found herself thinking about CRISPR. In the early 2000s, this was a niche area: only a handful of microbiologists were paying attention to the newly discovered, curiously patterned stretch of DNA called CRISPR in the genome of some bacteria, where it serves as part of a defence system against viruses. By copying part of an invading virus' DNA and inserting it into that stretch, bacteria are able to recognize the virus if it invades again, and attack it by cutting its DNA. Different CRISPR systems have different ways of organizing that attack; all of the systems known at the time involved an RNA molecule called CRISPR RNA. Charpentier was interested in identifying sites in the genome of  S. pyogenes  that made regulatory RNAs \u2014 and found that bioinformatics took her only so far. So she forged a collaboration with molecular microbiologist J\u00f6rg Vogel, then a junior group leader at the Max Planck Institute for Infection Biology, who was developing methods for large-scale mapping of RNAs in a genome. He agreed to map  S. pyogenes  \u2014 and by 2008 he had sequences of all of the small RNAs generated by the bacterium. The first thing that the researchers noticed was a super-abundance of a novel small RNA that they called trans-activating CRISPR RNA (tracrRNA). From its sequence and position on the genome \u2014 it was at a location that Charpentier's bioinformatics had predicted as being close to the CRISPR site \u2014 they realized that it was highly likely to be involved in a CRISPR system that had not previously been described. Charpentier and her colleagues began a long series of experiments to explore this system, identifying that it had just three components \u2014 tracrRNA, CRISPR RNA and the Cas9 protein. This was a surprise: \u201cOther CRISPR systems involved just one RNA and many proteins, and no one had really thought that two RNAs might be involved,\u201d says Charpentier. The system was so exceptionally simple that she realized that it might one day be harnessed as a powerful genetic engineering tool. If the components could be controlled, it might provide the long-sought ability to find, cut and potentially alter DNA at a chosen, precise site in a genome. But how exactly was this CRISPR system working? Charpentier suspected that the two RNAs might actually interact with each other to guide Cas9 to a particular DNA sequence in the virus. The concept was radical; that type of teamwork is routine for proteins, but not for RNAs. But Charpentier \u201calways looked for the unexpected rather than the expected in a genome\u201d, says Tuomanen. \u201cShe is a very counter-culture person.\u201d Charpentier remembers that it was hard to persuade any of her young students to follow up her intuition and perform the key experiment to test whether the two RNAs might interact, but eventually a masters student at the University of Vienna, Elitza Deltcheva, volunteered. By then, it was June 2009, and Charpentier was again on the move. She had never felt completely at home in Vienna, where she says the grandiose architecture oppressed her. And she knew that she had to find more security and support. \u201cAt this time in my career, I needed the luxury of being able to focus on finalizing a big, cool story,\u201d she says. She took a position at the newly created, well provisioned Ume\u00e5 Centre for Microbial Research in northern Sweden. The pretty, human-scale architecture of the old town made her feel comfortable, and she even learned to like the long, dark winters, which made her lose the feeling of time, allowing an even greater focus on work. In summer 2009, she was still commuting between Austria and Sweden when Deltcheva called her in Ume\u00e5 at 8 p.m. to tell her that the experiments had worked. \u201cI was very, very happy,\u201d Charpentier says. But she told no one. Vogel says that it was \u201ca very intense time\u201d. He recalls getting a call from Charpentier one night that August when he was driving on a country road outside Berlin. \u201cI stood on the kerbside for ages while we discussed when would be the right time to publish, because by then we had actually got the story.\u201d They both knew that this discovery was going to be a game-changer, but both were afraid of being scooped if word of the system they had stumbled on got out. To make sure that publication would not be drawn out by referees' queries, they worked doggedly and silently for more than a year to cover as many bases as they could think of before submitting to  Nature 4 . Charpentier was unknown in the then-small CRISPR world. She presented the work for the first time in October 2010 at a CRISPR meeting in Wageningen, the Netherlands, a few weeks after submitting it for publication. \u201cIt was a highlight of the meeting \u2014 a beautiful story that was extremely unexpected and came right out of the blue,\u201d says microbiologist John van der Oost of Wageningen University, who organized the meeting. Charpentier didn't mind being an outsider. \u201cI have never really wanted to be part of a cosy scientific community,\u201d she says. And she was already thinking ahead to the next step \u2014 how this neat dual-guide RNA system actually led to cleavage of DNA. At a 2011 American Society for Microbiology conference in San Juan, Puerto Rico, she met structural biologist Jennifer Doudna of the University of California, Berkeley. Doudna was immediately charmed. \u201cI loved her intensity, which was apparent from the moment I met her,\u201d she says. They began a collaboration that swiftly led to the second key discovery showing how Cas9 cleaved DNA 5 . With the mechanism elucidated, researchers went on to show that the system could indeed be adapted to make targeted cuts in a genome and to modify a sequence. The technique has since been  embraced by labs around the world . Charpentier, meanwhile, made two decisions. The first was in deference to her original ambition to do something to advance medicine. She contacted Novak, who was by then working at the pharmaceutical firm Sanofi in Paris, with the intention of co-founding a company to exploit the methodology for human gene therapy. CRISPR Therapeutics, based in Cambridge, Massachusetts, and Basel, Switzerland, was born in November 2013 with a third co-founder, Shaun Foy, and Charpentier remains chair of its scientific advisory board. The second decision was in deference to her ambition to fully dedicate her time to basic research in gene regulation. For this she wanted a permanent post, with more institutional support. In 2013, she moved to Germany to become a professor at the Hanover Medical School and a department chief at the Helmholtz Centre for Infection Research in nearby Braunschweig, where she finally got her own technicians and built up a lab of 16 PhD students and postdocs. Just over two years later, she was recruited by the Max Planck Institute in Berlin. Now she has generous technical and institutional support, and her labs are in the elegant, nineteenth-century campus of the Charit\u00e9 teaching hospital, an environment she can relax in. Maybe in a few years, she says, she'll even find a few moments for reading philosophy. But right now, fame and prizewinning leave little time for that. She values the recognition, engaging fully with the publicity activities that each prize requires \u2014 but notes anxiously that on average, each takes two full days from work. She declines to discuss the high-profile and rather complicated patent dispute between herself \u2014 alongside Doudna and Berkeley \u2014 and the Broad Institute of MIT and Harvard in Cambridge, Massachusetts. She leaves that to the patent lawyers,  who are currently arguing it out . Her focus is still on research, and her latest paper 1  \u2014 an elaboration of a CRISPR system that is even simpler than CRISPR\u2013Cas9 \u2014 was once again finalized in the middle of a lab move. The work shows that a protein called Cpf1 can do the jobs of both tracrRNA and the Cas9 protein \u2014 \u201ca very important contribution\u201d, says van der Oost, and part of a flurry of recent studies on this system 6 , 7 . But Charpentier is keen not to be defined by CRISPR, which is just one of five themes in her lab; others include the mechanisms by which pathogens interact with host immune cells and the molecular complexes that regulate the behaviour of bacterial chromosomes. Reflecting back, she feels that her life has been tougher than it need have been. She notes that now there are more sources of major grants to help young investigators to start their own independent labs. And although her goals to further medicine and improve genetic-engineering tools have been met, her ambitions have not waned. \u201cI haven't changed, and I won't change,\u201d she says. \u201cThe scientist that I am got me here, and that is the scientist that I want to remain.\u201d But some things have changed. Charpentier is not an outsider any more: she is an established member of the rapidly expanding CRISPR community and is inundated with invitations to give talks. Her mischievous ambition, however, is to show up at a CRISPR meeting and report the discovery of something entirely different, but equally important. She has a few things up her sleeve, she says. \n                 Tweet \n                 Facebook \n                 LinkedIn \n                 Weibo \n                 Wechat \n               \n                     How the US CRISPR patent probe will play out 2016-Mar-07 \n                   \n                     CRISPR: gene editing is just the beginning 2016-Mar-07 \n                   \n                     Should you edit your children\u2019s genes? 2016-Feb-23 \n                   \n                     Bitter fight over CRISPR patent heats up 2016-Jan-12 \n                   \n                     CRISPR, the disruptor 2015-Jun-03 \n                   \n                     Nature  special: CRISPR \n                   Reprints and Permissions"},
{"file_id": "531294a", "url": "https://www.nature.com/articles/531294a", "year": 2016, "authors": [{"name": "Alison Abbott"}], "parsed_as_year": "2006_or_before", "body": "A controversial study has suggested that the neurodegenerative disease might be transferred from one person to another. Now scientists are racing to find out whether that is true. In the 25 years that John Collinge has studied neurology, he has seen hundreds of human brains. But the ones he was looking at under the microscope in January 2015 were like nothing he had seen before. He and his team of pathologists were examining the autopsied brains of four people who had once received injections of growth hormone derived from human cadavers. It turned out that some of the preparations were contaminated with a misfolded protein \u2014 a prion \u2014 that causes a rare and deadly condition called Creutzfeldt\u2013Jakob disease (CJD), and all four had died in their 40s or 50s as a result. But for Collinge, the reason that these brains looked extraordinary was not the damage wrought by prion disease; it was that they were scarred in another way. \u201cIt was very clear that something was there beyond what you'd expect,\u201d he says. The brains were spotted with the  whitish plaques typical of people with Alzheimer's disease . They looked, in other words, like young people with an old person's disease. For Collinge, this led to a worrying conclusion: that the plaques might have been transmitted, alongside the prions, in the injections of growth hormone \u2014 the first evidence that Alzheimer's could be transmitted from one person to another. If true, that could have far-reaching implications: the possibility that 'seeds' of the amyloid-\u03b2 protein involved in Alzheimer's could be transferred during other procedures in which fluid or tissues from one person are introduced into another, such as blood transfusions, organ transplants and other common medical procedures. Collinge felt a duty to inform the public quickly. And that's what he did, publishing the study in  Nature  in September 1 , to headlines around the world. \u201cCan you CATCH Alzheimer's?\u201d asked Britain's  Daily Mail , about the \u201cpotentially explosive new study\u201d. Collinge has been careful to temper the alarm. \u201cOur study does not mean that Alzheimer's is actually contagious,\u201d he stresses. Carers won't catch it on the job, nor family members, however close. \u201cBut it raises concern that some medical procedures could be inadvertently transferring amyloid-\u03b2 seeds.\u201d Alison Abbott and researcher John Collinge elucidate the debate around transmissible Alzheimer\u2019s. Since then, the headlines have died away, but the academic work and discussion have taken off. Could seeds of amyloid-\u03b2 proteins really be transmitted and, if so, are they harmless or do they cause disease? And could seeds of other related diseases involving misfolded proteins be transmitted in a similar way? In the past decade or so, evidence has been mounting for a controversial theory that rogue proteins, known collectively as amyloids and associated with diverse neurodegenerative diseases \u2014 from Alzheimer's to Parkinson's and Huntington's \u2014 might share some properties of prions, including their transmissibility. Collinge's data bolstered that theory. Urgent though these questions are, it could take years to find answers. The paper by Collinge and his colleagues has sparked a worldwide hunt for similar amyloid pathology in autopsied brains, and a small study 2  published this January  revealed a handful of related cases . Researchers are also trying to work out what the putative amyloid seeds look like, and whether different 'strains' of amyloids exist that are particularly damaging. Some researchers say that it is much too early to be alarmed. They point out that the number of patients in Collinge's study was tiny, that none had displayed symptoms of Alzheimer's disease before their death and that another toxic protein called tau also seems to be required to cause the condition. \u201cWe have to remember that there is no conclusive evidence that seeds of amyloids can transmit actual disease or that amyloids spread in the brain in a prion-like way,\u201d says Pierluigi Nicotera, scientific director of the German Centre for Neurodegenerative Diseases in Bonn. \u201cThere may be other biological explanations.\u201d Right now, there are few solid answers, but plenty of concerns. The sceptics worry that they might one day find themselves working under tight biosecurity regulations to handle proteins that they view as relatively innocuous. Others feel that the dangers may have been underestimated, and that scientists have a duty to investigate this as quickly as they can. \u201cIn my opinion, all amyloids should be considered dangerous until proven safe,\u201d says prion and amyloid researcher Adriano Aguzzi at the University Hospital Zurich in Switzerland. \n               Dangerous folds \n             A few decades ago, it was almost inconceivable that a protein, which has no genetic material or any other obvious way to self-replicate, could cause infectious disease. But that changed in 1982, when Stanley Prusiner, now at the University of California, San Francisco, introduced evidence for disease-causing prions, coining the term from the words 'proteinacious' and 'infectious' 3 . Prusiner showed that prion proteins (PrP) exist in a normal cellular form, and in a misfolded infectious form. The misfolded form causes the normal protein to also misfold, creating a cascade that overwhelms and kills cells 4 . It cause animal brains to turn into a spongy mess in scrapie, a disease of sheep, and in bovine spongiform encephalopathy (BSE or 'mad cow disease'), as well as in human prion diseases such as CJD. Prusiner and others also investigated how prions could spread. They showed that injecting brain extracts containing infectious prions into healthy animals seeds disease 4 . These prions can be so aggressive that in some cases, simply eating infected brains is sufficient to transmit disease. For example, many cases of variant CJD (vCJD) are now thought to have arisen in the United Kingdom in the 1990s after people ate meat from cattle that were infected with BSE. Since then, scientists have come to appreciate that many proteins associated with neurodegenerative diseases \u2014 including amyloid-\u03b2 and tau in Alzheimer's disease and \u03b1-synuclein in Parkinson's disease \u2014 misfold catastrophically. Structural biologists call the  entire family of misfolded proteins (including PrP) amyloids . Amyloid-\u03b2 clumps into whitish plaques, tau forms ribbons called tangles and \u03b1-synuclein creates fibrous deposits called inclusions. A decade ago, these similarities prompted neuroscientist Mathias Jucker at the University of T\u00fcbingen in Germany to test whether injecting brain extracts containing misfolded amyloid-\u03b2 into mice could seed an abnormal build-up of amyloid in the animals' brains. He found that it could, and that it also worked if he injected amyloids into the muscles 5 . \u201cWe saw no reason not to believe that if amyloid seeds entered the human brain, they would also cause amyloid pathology in the same way,\u201d says Jucker. This didn't cause alarm at the time, because it wasn't clear how an amyloid seed from the brain of someone with Alzheimer's could be transferred into another person's body and find its way to their brain. To investigate that, what was needed was a group of people who had been injected with material from another person, and the opportunity to examine their brains in great detail, preferably when they were still relatively young and before they might have spontaneously developed early signs of Alzheimer's. The CJD brains provided just that opportunity. Between 1958 and 1985, around 30,000 people worldwide received injections of growth hormone derived from the pituitary glands of cadavers to treat growth problems. Some of the preparations were contaminated with the prion that causes CJD. Like all prion diseases, CJD has a very long incubation period, but once it gets going it rages through the brain, destroying all tissue in its wake and typically killing people from their late 40s onwards. According to 2012 statistics 6 , 226 people around the world have died from CJD as a result of prion-contaminated growth-hormone preparations. Collinge had not set out to find a link with Alzheimer's \u2014 it emerged as part of routine work at the National Prion Clinic in London, which he heads, and where around 70% of all people in the United Kingdom who die from prion-related causes are now autopsied. The clinic routinely looks for signs of all amyloid proteins in these brains to distinguish prion disease from other conditions. It was thanks to this routine work that the cluster of unusual cases emerged of people who had clearly died of CJD, but who also had obvious signs of amyloid pathology in their grey matter and cerebral blood vessels. As soon as he saw these brains, Collinge knew that he could get into stormy waters. Keen to strike a balance between warning of a possible public-health risk and causing unwarranted panic, he sketched a carefully worded press release that would go out from the National Prion Centre and set up hotlines for people who had been treated with growth hormone in the past. But no panic occurred: apart from one or two overwrought headlines, the news stories were fairly measured, he says. Only around ten people called the hotlines. For scientists, however, the paper was a red flag. \u201cAs soon as the paper came out we realized the health implications and started collecting slides and paraffin blocks from patients,\u201d says Jiri Safar, director of the National Prion Disease Pathology Surveillance Center at Case Western Reserve University in Cleveland, Ohio. Like other pathologists in countries where people had died of CJD associated with medical procedures, he rushed to check the centre's archives of autopsied brains to see if any of them contained the ominous amyloid deposits. The answers are not yet in. Safar says that it has not proved easy to trace brain samples in the United States, but that he is working to do so with the National Institutes of Health and the Centers for Disease Control and Prevention (CDC) in Atlanta, Georgia. Charles Duyckaerts at the Piti\u00e9-Salp\u00eatri\u00e8re Hospital in Paris, France, has now examined brain tissues from around 24 patients and is likely to report the results later this year. A further 228 cases of CJD were caused by transplantation of prion-contaminated dura mater \u2014 the membrane surrounding the brain and spinal cord \u2014 prepared from cadavers around the world. Dura-mater preparations were regularly used in brain surgery as repair patches until the late 1990s. For the study 2  published in January, Herbert Budka at the National Prion Diseases Reference Center at University Hospital Zurich and his colleagues examined the brains of seven such patients from Switzerland and Austria, and found that  five had amyloid deposits in grey matter and blood vessels . In Japan, dementia researcher Masahito Yamada at Kanazawa University is making his way through a large number of such autopsy specimens and says that the 16 brains he has examined so far show signs of unusually high levels of amyloid deposition in cerebral blood vessels. Yet such case studies can only ever provide circumstantial evidence that seeds of amyloid-\u03b2 were transferred during the treatments. And they cannot entirely rule out the possibility that the treatments themselves \u2014 or the patients' original medical conditions \u2014 caused the amyloid pathology. More-conclusive evidence would come from checking whether the original growth hormone and dura-mater preparations contained infectious amyloid seeds, by injecting them into animals and seeing whether this triggers disease. Most of these preparations, however, have long since disappeared. Collinge has access to some original samples of growth hormone stored by the UK Department of Health, and he is planning to analyse them for the presence of amyloid seeds and then inject them into mice. That work will take a couple of years to complete, he says. \n               Seeds of doubt \n             There is another hitch: no one knows for sure what size and shape the amyloid seeds might be. Jucker is hunting for them in an unusual source of human brain tissue that has nothing to do with CJD. A team in Bonn has collected frozen samples from more than 700 people with epilepsy who were operated on over the past 25 years to remove tissue that was driving their seizures. \u201cIt is the best source of fresh human brain tissue available at the moment,\u201d says Jucker, who plans to scrutinize it carefully under the microscope for anything that might resemble tiny clumps or seeds of amyloid-\u03b2. The team also has records of the patients' cognitive skills, such as language and memory skills, before and at regular intervals after the operations. This should allow Jucker's team to correlate the presence of any amyloid-\u03b2 seeds it finds with changes in the cognitive function of individual patients over time. Scientists have shown that tau and \u03b1-synuclein can also seed pathological features in mice. In two studies 7 , 8  from 2012, scientists injected fibrils of \u03b1-synuclein into the brains of mice already engineered to develop some of the characteristics of Parkinson's disease. This triggered the early onset of some of the signs and symptoms of Parkinson's, and eventually killed the animals. A third study 9  showed that similar injections into normal mice caused some of the neurodegeneration typical of Parkinson's disease and the mice became less agile. In humans, \u03b1-synuclein would not necessarily turn out to be equally aggressive \u2014 mouse models of neurodegenerative diseases do not mimic human disease very closely \u2014 but scientists are taking the possibility seriously. If the transmissibility hypothesis proves true, the implications could be severe. Amyloids stick like glue to metal surgical instruments, and normal sterilization does not remove them, so amyloid seeds might possibly be transferred during surgery. The seeds might sit in the body for years or decades before spreading into plaques, and perhaps enabling the other pathological changes needed to induce Alzheimer's disease. Having amyloid plaques in cerebral blood vessels could be dangerous in another way, because they increase the risk that the vessel walls might break, leading to small strokes. But if common medical procedures really increased the risk of neurodegenerative disorders, then wouldn't that already have been detected? Not necessarily, says epidemiologist Roy Anderson at Imperial College London. \u201cThe proper epidemiological studies have not been done yet,\u201d he says. They require very large and carefully curated databases of people with Alzheimer's disease, which include information about the development of symptoms and autopsy data. He and his team are now studying the handful of reliable databases that exist to tease out a signal that might associate medical procedures with Alzheimer's progression. The number of patients currently available may turn out to be too small to draw conclusions, he says, but a more definitive answer could emerge as the databases grow. Faced with so much uncertainty, some researchers and public-health agencies have adopted a wait-and-see approach. \u201cWe are right at the beginning of this story,\u201d says Nicotera, \u201cand if there is one message to come out right now it is that we need more work to see if this is a relevant mechanism.\u201d The CDC and the European Centre for Disease Prevention and Control in Solna, Sweden, say that they are keeping a cautious eye on the issue. If further research does confirm that common neurodegenerative diseases are transmissible, what then? One immediate priority would be rigorous sterilization procedures for medical and surgical instruments that would destroy amyloids, in the way that extremely high temperatures and harsh chemicals destroy prions. Aguzzi says that funding agencies should put out calls now to researchers to develop cheap and simple sterilization methods. \u201cIt's not very sexy science, but it is urgently needed,\u201d he says. He also worries about the safety of researchers working with amyloids \u2014 particularly \u03b1-synuclein. \u201cI have nightmares that someone in my lab may catch Parkinson's,\u201d he says. \u201cWhile the story is in flux, our first duty is to protect lab workers.\u201d \n               Strain seekers \n             The similarities between prions and other amyloids is throwing open other avenues of research. Prions can exist as distinct strains \u2014 proteins that have the same sequence of amino acids but misfold in different ways and have distinct biological behaviours 10 , much as different strains of a pathogenic virus can be aggressive or weak. The outbreak of vCJD in the United Kingdom in the 1990s was traced to BSE-contaminated meat because the prion strain was the same in both. Over the past few years, research in animals has shown that different strains of amyloid-\u03b2 and \u03b1-synuclein exist 11 , 12 . And a landmark paper 13  in 2013 reported that strains of amyloid-\u03b2 with different 3D structures were associated with different disease progression in two people with Alzheimer's. Structural biologist Robert Tycko, who led the work at the National Institute of Diabetes and Digestive and Kidney Diseases in Bethesda, Maryland, is now looking at many more brain samples from such patients. Knowing the structures of pathological forms of amyloid seeds should help to design small molecules that bind to them and stop them doing damage, says biophysicist Ronald Melki at the Paris-Saclay Institute of Neuroscience, who works on \u03b1-synuclein strains. His lab is designing small peptides that target the seeds and mimic regions of 'chaperone' molecules, which usually bind to proteins and help them to fold correctly. Melki's small peptides mimic these binding regions, sticking to the amyloid proteins to stop them from aggregating further. In the research community, much of the agitation in response to Collinge's paper boils down to semantics. Some scientists do not like to use the word 'prion' in connection with the amyloids associated with common neurodegenerative diseases, or to describe any of their properties as 'prion-like' \u2014 because of its connotation of infectious, deadly disease. \u201cThe public has this perception of the word 'prion',\u201d says Alzheimer's researcher Brad Hyman at Harvard Medical School in Boston, Massachusetts, and this matters, even if their ideas are wrong. \u201cOne of my patients told me that she wasn't getting any hugs any more from her husband who had read about the case in the media \u2014 that made me sad,\u201d he says. Others, however, feel that it is helpful to consider prions and other amyloids as being part of a single spectrum of conditions involving proteins that misfold and misbehave. It means that researchers studying prion diseases and neurodegenerative diseases, who until recently had considered their disciplines to be separate, now find themselves tackling shared questions. Both fields are wary of raising premature alarm, even though they wonder what the future will bring. Jucker, only half-jokingly, says he could imagine a future in which people would go into hospital every ten years or so and get the amyloid seeds cleared out of their brains with antibodies. \u201cYou'd be good then to go for another decade.\u201d\n \n                 Tweet \n                 Facebook \n                 LinkedIn \n                 Weibo \n               \n                     More evidence emerges for 'transmissible Alzheimer's' theory 2016-Jan-26 \n                   \n                     Autopsies reveal signs of Alzheimer\u2019s in growth-hormone patients 2015-Sep-09 \n                   \n                     Alzheimer\u2019s research takes a leaf from the prion notebook 2015-May-29 \n                   \n                     Prions and chaperones: Outside the fold 2012-Feb-15 \n                   \n                     Protein folding: The dark side of proteins 2010-Apr-07 \n                   \n                     National Prion Clinic \n                   Reprints and Permissions"},
{"file_id": "531568a", "url": "https://www.nature.com/articles/531568a", "year": 2016, "authors": [{"name": "Brendan Maher"}], "parsed_as_year": "2006_or_before", "body": "Scientists are helping to stop antisocial behaviour in the world's most popular online game. The next stop could be a kinder Internet. It took less than a minute of playing  League of Legends  for a homophobic slur to pop up on my screen. Actually, I hadn't even started playing. It was my first attempt to join what many agree to be the world's leading online game, and I was slow to pick a character. The messages started to pour in. \u201cPick one, kidd,\u201d one nudged. Then, \u201cChoose FA GO TT.\u201d It was an unusual spelling, and the spaces may have been added to ease the word past the game's default vulgarity filter, but the message was clear. Online gamers have a reputation for hostility. In a largely consequence-free environment inhabited mostly by anonymous and competitive young men, the antics can be downright nasty. Players harass one another for not performing well and can cheat, sabotage games and do any number of things to intentionally ruin the experience for others \u2014 a practice that gamers refer to as griefing. Racist, sexist and homophobic language is rampant; aggressors often threaten violence or urge a player to commit suicide; and from time to time, the vitriol spills beyond the confines of the game. In the notorious 'gamergate' controversy that erupted in late 2014, several women involved in the gaming industry were subjected to a campaign of harassment, including invasions of privacy and threats of death and rape. League of Legends  has 67 million players and grossed an estimated US$1.25 billion in revenue last year. But it also has a reputation for toxic in-game behaviour, which its parent company, Riot Games in Los Angeles, California, sees as an obstacle to attracting and retaining players. So the company has hired a team of researchers to study the social \u2014 and antisocial \u2014 interactions between its users. With so many players, the scientists have been able to gather vast amounts of behavioural data and to conduct experiments on a scale that is rarely achieved in academic settings. Whereas other game companies have similar research teams, Riot's has been remarkably open about its work \u2014 with players, with other companies and with a growing collection of academic collaborators who see multiplayer games as a Petri dish for studying human behaviour. \u201cWhat's most interesting with Riot is not that they're doing it but that they're publicizing it and have an established way of sharing it with academics,\u201d says Nick Yee, a social scientist and co-founder of Quantic Foundry, a video-game-industry consulting firm in Sunnyvale, California. Riot's findings have helped to reveal where the toxic behaviour comes from and how to steer players to be kinder to each other. And some say that the work may translate to digital venues outside the game. \u201cThe work they do is extensible to thinking about big questions,\u201d says Justin Reich, an education researcher at the Massachusetts Institute of Technology in Cambridge, \u201cnot just how do we make online games more civil places, but how do we make the Internet a more civil place?\u201d \n               Big business \n             Jeffrey Lin, the lead designer of social systems at Riot, is the public face of its research programme. He has been playing video games online since he was about 11 years old and had long wondered why so many of his fellow gamers put up with toxic behaviour. \u201cEverybody you talk to thinks of the Internet as this hate-filled place,\u201d he says. \u201cWhy do we think that's a normal part of gaming experiences?\u201d In 2012, Lin was finishing a PhD in cognitive neuroscience at the University of Washington in Seattle and was working for the game company Valve in nearby Bellevue when a friend and fellow gamer introduced him to the co-founders of Riot, Marc Merrill and Brandon Beck. They had recognized that toxic behaviour was a major drag on players' experience, and they wanted to solve the problem with science. So they hired Lin as a game designer, essentially giving him the keys to a juggernaut in the online gaming world. League of Legends , Riot's only game, was released in 2009 and currently attracts 27 million players each day. It is by far the most popular of a growing segment of games referred to as eSports, a world in which elite players form professional teams, win university scholarships and take part in million-dollar tournaments in sporting arenas. The final of  League of Legends 's 2015 world championship in Berlin drew 36 million viewers online and on television, rivalling the audience of the finals of some traditional sports. The game can be intimidating to newcomers. Players control one of more than 120 characters called champions, each of which has specific abilities, weaknesses and roles. Teams are usually made up of five players, who must cooperate to kill monsters and opponents, collect gold to purchase magical items, capture territory and eventually destroy the other team's base. Matches last about half an hour on average, so having a poorly performing player on a team can be aggravating. And the game requires coordination between players, for which it provides an in-game chat function. If someone makes a mistake, he or she will generally hear about it fast. Players can report their teammates for being toxic, and this can result in a temporary or permanent ban from the game. But working out how to distinguish a few frustrated grumbles or good-natured trash talk from the kind of vitriol that is worthy of punishment is a difficult task. To tackle it, Lin needed to make sure that he had a good picture of where such toxicity was coming from. So he got a team to review chat logs from thousands of games each day and to code statements from players as positive, neutral or negative. The resulting map of toxic behaviour was surprising. Common wisdom holds that the bulk of the cruelty on the Internet comes from a sliver of its inhabitants \u2014 the trolls. Indeed, Lin's team found that only about 1% of players were consistently toxic. But it turned out that these trolls produced only about 5% of the toxicity in  League of Legends . \u201cThe vast majority was from the average person just having a bad day,\u201d says Lin. They behaved well for the most part, but lashed out on rare occasions. That meant that even if Riot banned all the most toxic players, it might not have a big impact. To reduce the bad behaviour that most players experienced, the company would have to change how players act. Lin borrowed a concept from classic psychology. In late 2012, he initiated a massive test of priming, the idea that imagery or messages presented just before an activity can nudge behaviours in one direction or another. The Riot team devised 24 in-game messages or tips, including some that encourage good behaviour \u2014 such as \u201cPlayers perform better if you give them constructive feedback after a mistake\u201d \u2014 and some that discourage bad behaviour: \u201cTeammates perform worse if you harass them after a mistake\u201d. They presented the tips in three colours and at different times during the game. All told, there were 216 conditions to test against a control, in which no tips were given. That is a ridiculous number of permutations to test on people in a laboratory, but trivial for a company with the power to perform millions of experiments each day. Some of the tips had a clear impact (see \u2018Civil engineering\u2019). The warning about harassment leading to poor performance reduced negative attitudes by 8.3%, verbal abuse by 6.2% and offensive language by 11% compared with controls. But the tip had a strong influence only when presented in red, a colour commonly associated with error avoidance in Western cultures. A positive message about players' cooperation reduced offensive language by 6.2%, and had smaller benefits in other categories. Riot has released just a few of these analyses, so it is hard to make broad generalizations. From a scientific standpoint, says Lin, the results from the priming experiments were \u201cepic\u201d, and they opened the doors to many more research questions, such as how various tips and colours might influence players from different cultures. But the behavioural improvements were too modest and too fleeting to change the culture of the game. Lin reasoned that if he wanted to make the community more civil, then players would have to have a say in devising the norms. So Riot introduced the Tribunal, which gives players a chance to serve as judge and jury to their peers. In it, volunteers review chat logs from a player who has been reported for bad behaviour, and then vote on whether the offender deserves punishment. The Tribunal, which started in 2011, gave players a greater sense of control over establishing community norms, says Lin. And it revealed some of the things that triggered the most rebukes: homophobic and racial slurs. But players who were banned from the game were often unsure why they had been punished, and continued to act negatively when the bans were lifted. So Lin's team developed 'reform cards' to give feedback to banned players, and the company then monitored their play. When players were informed only of what kind of behaviour had landed them in trouble, 50% did not misbehave in a way that would warrant another punishment over the next three months. When they were sent reform cards that included the judgements from the Tribunal and that detailed the chats and actions that had resulted in the ban, the reform rate went up to 70%. But the process was slow; reform cards might not show up until two weeks to a month after an offence. \u201cIf you look at any classic literature on reinforcement learning, the timing of feedback is super critical,\u201d says Lin. So he and his team used the copious data they were collecting to train a computer to do the work much more quickly. \u201cWe let loose machine learning,\u201d Lin says. The automated system could provide nearly instantaneous feedback; and when abuse reports arrived within 5\u201310 minutes of an offence, the reform rate climbed to 92%. Since that system was switched on, Lin says, verbal toxicity among so-called ranked games, which are the most competitive \u2014 and most vitriolic \u2014 dropped by 40%. Globally, he says, the occurrence of hate speech, sexism, racism, death threats and other types of extreme abuse is down to 2% of all games. \u201cIf the numbers they put out there are correct and true, it seems to be working well,\u201d says Jamie Madigan, an author in St Louis, Missouri, who writes about the psychology of gamers. And that's because the reprimands are specific, timely and easy to understand and act upon, he says. \u201cThat's classic psychology 101.\u201d \n               Open data \n             Riot's research team is constantly experimenting with other ways to improve interactions in the game. Sportsmanlike behaviour can earn players honour points and other rewards. Tinkering with chat features helped, too. And the team is planning to use the Tribunal to train the game's algorithms to detect sarcastic and passive-aggressive language in chats \u2014 a major challenge for machine learning. From the start, Riot has also made much of its data available for others to investigate. Jeremy Blackburn, an avid gamer and computer scientist who works at Telefonica Research and Development in Barcelona, Spain, mined data on 1.46 million Tribunal cases to develop his own machine-learning approach for predicting when player behaviours would be deemed toxic. Together with Haewoon Kwak at the Qatar Computing Research Institute in Doha, he found that the most important factor \u2014 beyond the specific words used in the toxic messages \u2014 was how well the opposing team performed 1 . Blackburn, who is interested in studying cyberbullying, hopes to look more at how different cultures judge behaviour. Some evidence, he says, suggests that it is common for Korean gamers to gang up on and berate the poorest-performing players, for example. League data may bear this out. \u201cWe saw there was a lot more pardon for this verbal-abuse category.\u201d Rachel Kowert in Austin, Texas, is a research psychologist on the board of the Digital Games Research Association. She is impressed by the work and especially by Blackburn and Kwak's unfettered access. \u201cIt's awesome for the researchers. You can't put a price on real data,\u201d she says. Other companies also have data that scientists would like. Blizzard Entertainment in Irvine, California, makes the popular online fantasy game  World of Warcraft , which many regard as a treasure trove for data on complex social interactions. But few people outside the company have been able work with the data, and most of those who do are subject to stiff non-disclosure agreements. (Blizzard did not respond to  Nature 's requests for comment.) By contrast, Riot talks about its data at gaming conferences, and when it collaborates with researchers there are few restrictions on publishing. It also has an outreach programme, visiting universities to establish collaborations. And last May, Lin presented data at the annual meeting of the Association for Psychological Science in New York City to drum up more interest. Even with those efforts, the company's research has yet to achieve broad recognition among behavioural scientists. \u201cHopefully they will come to more conferences where people are studying behaviour,\u201d says Betsy Levy Paluck, a social psychologist at Princeton University in New Jersey. Although she was not familiar with Riot, she says that the company seems to be working out how to do high-powered, big-data research in psychology, which has been a major challenge. Daphn\u00e9 Bavelier, a cognitive neuroscientist at the University of Geneva in Switzerland, met Lin at the conference in New York City. Her research has suggested \u2014 to the joy of many gamers and the agony of their parents \u2014 that some games, particularly fast-paced first-person shooters, can improve a handful of cognitive abilities, such as visual attention, both within and outside the games 2 . She plans to collaborate with Riot to study how players tackle the steep learning curve in  League of Legends . The team-based nature of the game could also be useful to scientists. Young Ji Kim, a social scientist at the Massachusetts Institute of Technology's Center for Collective Intelligence, was able to recruit 279 experienced teams from  League of Legends  to fill out surveys and work together on a battery of online tests that were designed to explore team dynamics and the factors that make teams successful. (By providing an in-game incentive worth about $15, Riot helped her team to get thousands of sign-ups in a couple of hours, she says.) The preliminary results suggested that the teams' rank in the game correlates with their collective intelligence \u2014 a measure that generally tracks with things such as social perceptiveness and taking equal turns in conversation.  It's awesome for the researchers. You can't put a price on real data.  The enthusiasm that players show for participating in experiments such as these may be attributable to Lin, who writes frequently about Riot research and can often be found answering players' questions on Twitter and other social media. Being upfront and public about the efforts is important, says Bavelier. Although most digital companies run experiments on users, they are often less transparent. Facebook, for example, published a study about how behind-the-scenes tinkering with news feeds can manipulate user emotions 3 , and received significant backlash from users. \u201cWe need to learn from some of the mistakes of others to make sure that the users are aware of what we're doing,\u201d says Bavelier. Riot has an internal institutional review board that evaluates the ethics of all its experiments. Although not a conflict-free arrangement, it at least suggests that the research is being reviewed with an eye towards participant protection. Academic collaborators also need to get approval from their local boards. \n               Virtual violence \n             Lin has lofty goals for his teams' research and interventions. \u201cCan we improve online society as a whole? Can we learn about how to teach etiquette?\u201d he asks. \u201cWe're not an edutainment company. We're a games company first, but we're aware of how it could be used to educate.\u201d Parents, lawmakers and some scientists have fretted for decades that video games, particularly violent ones, are warping the minds of children. But James Ivory, a communication scientist at Virginia Polytechnic Institute and State University, in Blacksburg, says that much of the attention on violence has missed the biggest impact that games have. \u201cResearchers are slowly starting to wise to the idea that it may not be as important to think of what it means for someone to pretend to be a soldier than whether they're spending their time spewing racial or homophobic slurs.\u201d By the age of 21, the average young gamer will have logged thousands of hours of playing time. That fact alone makes dichotomies such as 'real world' and 'digital world' ring false \u2014 for many, game-playing is the real world. And, says Ivory, \u201cthe strongest influence these games have on people is how they interact with other people\u201d. Some researchers are cautious about trying to apply lessons from the game to other settings. Dmitri Williams, a social scientist and founder of Ninja Metrics, an analytics company in Manhattan Beach, California, warns that games have very specific incentive structures, which could limit how well these experiences map to the wider world. \u201cPeople behave well in real life because if they offend someone or screw up, they have to deal with the consequences.\u201d So, the manipulations that work to curb bad behaviour in  League  may be meaningless elsewhere. And there are still considerable challenges for Riot. Players continue to complain about toxic behaviour or what they deem to be unwarranted punishments. And a blog called 'League of Sexism' argues that the suggestive portrayal of female characters in the game contributes to a strong current of sexism in the player community. \u201cIt's difficult for players to identify sexist behaviour when sexism is built into the game's very imagery,\u201d says a representative for the blog, who wished to remain anonymous. Although Lin's efforts are \u201cadmirable and likely industry-leading\u201d, the representative says, many games are still \u201cawash with verbal harassment, griefing and overall negative behaviour from teammates and opponents\u201d. Lin says that Riot artists are aware of these concerns and that they have made efforts to portray female characters in a stronger and more-powerful way. Although Riot boasts that serious toxic behaviour infects only 2% of games, somehow I managed to experience it within a minute of playing for the first time. But immediately after \u201cFA GO TT\u201d popped up on my screen, something interesting happened. Another player chimed in with, \u201cCalm down\u201d. Perhaps it was a sign that Lin's efforts to engineer a more civil, self-policing digital space is starting to work. Or maybe it was just a friendly teammate reminding us all that it's just a game. \n                 Tweet \n                 Facebook \n                 LinkedIn \n                 Weibo \n               \n                 See Editorial \n                 page 549 \n               \n                     Play nicely 2016-Mar-30 \n                   \n                     Gaming improves multitasking skills 2013-Sep-04 \n                   \n                     Treating schizophrenia: Game on 2012-Feb-29 \n                   \n                     Video-game studies have serious flaws 2011-Sep-16 \n                   \n                     Video games boost visual skills 2003-May-29 \n                   \n                     Jeffrey Lin talks about Riot Games\u2019 research 1 \n                   \n                     Jeffrey Lin talks about Riot Games\u2019 research 2 \n                   Reprints and Permissions"},
{"file_id": "532020a", "url": "https://www.nature.com/articles/532020a", "year": 2016, "authors": [{"name": "Emily Anthes"}], "parsed_as_year": "2006_or_before", "body": "Smartphone apps claim to help conditions from addiction to schizophrenia, but few have been thoroughly tested. Type 'depression' into the Apple App Store and a list of at least a hundred programs will pop up on the screen. There are apps that diagnose depression (Depression Test), track moods (Optimism) and help people to \u201cthink more positive\u201d (Affirmations!). There's Depression Cure Hypnosis (\u201cThe #1 Depression Cure Hypnosis App in the App Store\u201d), Gratitude Journal (\u201cthe easiest and most effective way to rewire your brain in just five minutes a day\u201d), and dozens more. And that's just for depression. There are apps pitched at people struggling with anxiety, schizophrenia,  post-traumatic stress disorder  (PTSD), eating disorders and addiction. Adam Levy investigates whether we can trust apps that claim to help with mental health This burgeoning industry may meet an important need. Estimates suggest that about 29% of people will experience a mental disorder in their lifetime 1 . Data from the World Health Organization (WHO) show that many of those people \u2014 up to 55% in developed countries and 85% in developing ones \u2014 are not getting the treatment they need. Mobile health apps could help to fill the gap (see 'Mobilizing mental health'). Given the ubiquity of smartphones, apps might serve as a digital lifeline \u2014 particularly in rural and low-income regions \u2014 putting a portable therapist in every pocket. \u201cWe can now reach people that up until recently were completely unreachable to us,\u201d says Dror Ben-Zeev, who directs the mHealth for Mental Health Program at the Dartmouth Psychiatric Research Center in Lebanon, New Hampshire. Public-health organizations have been buying into the concept. In its Mental Health Action Plan 2013\u20132020, the WHO recommended \u201cthe promotion of self-care, for instance, through the use of electronic and mobile health technologies.\u201d And the UK National Health Service (NHS) website NHS Choices carries a short list of online mental-health resources, including a few apps, that it has formally endorsed. But the technology is moving a lot faster than the science. Although there is some evidence that empirically based, well-designed mental-health apps can improve outcomes for patients, the vast majority remain unstudied. They may or may not be effective, and some may even be harmful. Scientists and health officials are now beginning to investigate their potential benefits and pitfalls more thoroughly, but there is still a lot left to learn and little guidance for consumers. \u201cIf you type in 'depression', its hard to know if the apps that you get back are high quality, if they work, if they're even safe to use,\u201d says John Torous, a psychiatrist at Harvard Medical School in Boston, Massachusetts, who chairs the American Psychiatric Association's Smartphone App Evaluation Task Force. \u201cRight now it almost feels like the Wild West of health care.\u201d \n               App happy \n             Electronic interventions are not new to psychology; there is robust literature showing that Internet-based  cognitive behavioural therapy  (CBT), a therapeutic approach that aims to change problematic thoughts and behaviours, can be effective for treating conditions such as depression, anxiety and eating disorders. But many of these online therapeutic programmes are designed to be completed in lengthy sessions in front of a conventional computer screen. Smartphone apps, on the other hand, can be used on the go. \u201cIt's a way of people getting access to treatment that's flexible and fits in with their lifestyle and also deals with the issues around stigma \u2014 if people are not quite ready to maybe go and see their doctor, then it might be a first step to seeking help,\u201d says Jen Martin, the programme manager at MindTech, a national centre funded by the United Kingdom's National Institute for Health Research and devoted to developing and testing new mental-health technologies. One of the best-known publicly available apps was devised to meet that desire for flexibility. In 2010, US government psychologists conducting focus groups with military veterans who had PTSD learned that they wanted a tool they could use whenever their symptoms flared up. \u201cThey wanted something that they could use in the moment when the distress was rising \u2014 so when they were in line at the supermarket,\u201d says Eric Kuhn, a clinical psychologist and the mobile apps lead at the US Department of Veterans Affairs' National Center for PTSD. The department joined up with the US Department of Defense to create PTSD Coach, a free smartphone app released in early 2011. Anyone who has experienced trauma can use the app to learn more about PTSD, track symptoms and set up a support network of friends and family members. The app also provides strategies for coping with overwhelming emotions; it might suggest that users distract themselves by finding a funny video on YouTube or lead users through visualization exercises. In its first three years in app stores, PTSD Coach was downloaded more than 150,000 times in 86 different countries. It has shown promise in several small studies; in a 2014 study of 45 veterans, more than 80% reported that the app helped them to track and manage their symptoms and provided practical solutions to their problems 2 . More results are expected soon. Kuhn and his colleagues recently completed a 120-person randomized trial of the app, and a Dutch team is currently analysing data from a 1,300-patient trial on a similar app called SUPPORT Coach. Smartphone apps can also interact with users proactively, pinging them to ask about their moods, thoughts and overall well-being. Ben-Zeev created one called FOCUS, which is geared towards  patients with schizophrenia . Several times a day, the app prompts users to answer questions such as \u201cHow well did you sleep last night?\u201d or \u201cHow has your mood been today?\u201d If users report that they slept poorly, or have been feeling anxious, the app will suggest strategies for tackling that problem, such as limiting caffeine intake or doing some deep-breathing exercises. Some apps help people to stay connected to health-care professionals, too. ClinTouch, a psychiatric-symptom-assessment app designed by researchers at the University of Manchester, UK, analyses users' responses for signs that they may be experiencing a relapse; it can even notify a clinical-care team. Small feasibility studies \u2014 which are generally designed to determine whether an intervention is practical, but do not necessarily evaluate its efficacy \u2014 have shown that patients use and like both apps, and a 2014 study found that those who used FOCUS for a month experienced a reduction in psychotic symptoms and depression 3 . FOCUS and ClinTouch are both now being evaluated in randomized, controlled trials. Some researchers see opportunities in the data that smartphones collect about their users' movement patterns or communication activity, which could provide a potential window into mental health. \u201cYour smartphone is really this interesting diary of your life,\u201d says Anmol Madan, the co-founder and chief executive of Ginger.io, a digital mental-health company based in San Francisco, California. Studies have now shown that certain patterns of smartphone use can predict changes in mental-health symptoms 4 ; a drop in the frequency of outgoing text messages, for instance, may suggest that a user's depression is worsening. The Ginger.io app, which is still in beta, monitors these sorts of patterns and alerts each user's assigned mental-health coach if it detects a worrying change. \n               Absent evidence \n             The evidence supporting the use of such apps is building 5 , 6 , 7 . But this is a science in its infancy. Much of the research has been limited to pilot studies, and randomized trials tend to be small and unreplicated. Many studies have been conducted by the apps' own developers, rather than by independent researchers. Placebo-controlled trials are rare, raising the possibility that a 'digital placebo effect' may explain some of the positive outcomes that researchers have documented, says Torous. \u201cWe know that people have very strong relationships with their smartphones,\u201d and receiving messages and advice through a familiar, personal device may be enough to make some people feel better, he explains. But the bare fact is that most apps haven't been tested at all. A 2013 review 8  identified more than 1,500 depression-related apps in commercial app stores but just 32 published research papers on the subject. In another study published that year 9 , Australian researchers applied even more stringent criteria, searching the scientific literature for papers that assessed how commercially available apps affected mental-health symptoms or disorders. They found eight papers on five different apps. The same year, the NHS launched a library of \u201csafe and trusted\u201d health apps that included 14 devoted to treating depression or anxiety. But when two researchers took a close look at these apps last year, they found that only 4 of the 14 provided any evidence to support their claims 10 . Simon Leigh, a health economist at Lifecode Solutions in Liverpool, UK, who conducted the analysis, says he wasn't shocked by the finding because efficacy research is costly and may mean that app developers have less to spend on marketing their products. A separate analysis 11  found that 35 of the mobile health apps originally listed by the NHS transmitted identifying information \u2014 such as e-mail addresses, names and birthdates \u2014 about users over the Internet, and two-thirds of these did not encrypt the data. Last year, the NHS took this apps library offline and posted a smaller collection of recommended online mental-health services. The NHS did not respond to e-mailed questions or make an official available for interview, but it did provide this statement: \u201cWe are working to upgrade the Health Apps Library, which was launched as a pilot site in 2013 to review and recommend apps against a defined set of criteria which included data protection.\u201d The regulation of mental-health apps is opaque. Some apps designed to be used in a medical context can be considered medical devices and therefore may be regulated by the UK Medicines and Healthcare Products Regulatory Agency, the US Food and Drug Administration (FDA) or equivalent bodies elsewhere. But the lines are fuzzy. In general, an app that claims to prevent, diagnose or treat a specific disease is likely to be considered a medical device and to attract regulatory scrutiny, whereas one that promises to 'boost mood' or provide 'coaching' might not. The FDA has said that it will regulate only those health apps that present the highest risks to patients if they work improperly; even mental-health apps that qualify as medical devices might not be regulated if the agency deems them to be relatively low risk. But the potential risks are not well understood. \u201cAt the low end, people might waste their money or waste their time,\u201d says Martin, \u201cand at the higher end, especially with mental health, they might be actively harmful or giving dangerous advice or preventing people from going and getting proper treatment.\u201d When a team of Australian researchers reviewed 82 commercially available smartphone apps for people with bipolar disorder 12 , they found that some presented information that was \u201ccritically wrong\u201d. One, called iBipolar, advised people in the middle of a manic episode to drink hard liquor to help them to sleep, and another, called What is Biopolar Disorder, suggested that bipolar disorder could be contagious. Neither app seems to be available any more. Martin says that in Europe, at least, apps tend to come in two varieties, those that are commercially developed and come with little supporting evidence or plans for evaluation, and those with academic or government backing that take a more rigorous approach. The problem is that the former are generally more engaging for users and the latter take so long to make it to the market \u2014 if they even do \u2014 that they look out of date. \u201cThis is a generalization,\u201d Martin says, \u201cbut it's broadly true.\u201d \n               Unintended consequences \n             Even well-intentioned apps can produce unpredictable outcomes. Take Promillekoll, a smartphone app created by Sweden's government-owned liquor retailer, designed to help curb risky drinking. While out at a pub or a party, users enter each drink they consume and the app spits out an approximate blood-alcohol concentration. When Swedish researchers tested the app on college students, they found that men who were randomly assigned to use the app ended up drinking more frequently than before, although their total alcohol consumption did not increase. \u201cWe can only speculate that app users may have felt more confident that they could rely on the app to reduce negative effects of drinking and therefore felt able to drink more often,\u201d the researchers wrote in their 2014 paper 13 . It's also possible, the scientists say, that the app spurred male students to turn drinking into a game. \u201cI think that these apps are kind of playthings,\u201d says Anne Berman, a clinical psychologist at the Karolinska Institute in Stockholm and one of the study's authors. There are other risks too. In early trials of ClinTouch, researchers found that the symptom-monitoring app actually exacerbated symptoms for a small number of patients with psychotic disorders, says John Ainsworth at the University of Manchester, who helped to develop the app. \u201cWe need to very carefully manage the initial phases of somebody using this kind of technology and make sure they're well monitored,\u201d he says. In a pilot trial published earlier this year, ten US veterans with PTSD were randomly assigned to use PTSD Coach on their own for eight weeks, while another ten used the app with the support and guidance of primary-care providers. At the end of the trial, seven of the ten patients using the app with support showed a reduction in PTSD symptoms, compared with just three of the patients who used the app on their own 14 . But if apps require medical supervision, that undermines the idea that they will serve as an easy and low-cost way to provide care to the masses. \u201cPeople think there's an app for everything,\u201d says Helen Christensen, the director of the Black Dog Institute at the University of New South Wales in Sydney, Australia, who has developed and studied mental-health apps. \u201cIt's actually about how we build systems around apps, so that people have health care.\u201d Distributing mental-health apps in the developing world presents further challenges. Although mobile technology is spreading rapidly, there are many people who do not have \u2014 or cannot afford \u2014 smartphones or mobile Internet access. And the content of apps needs to be delivered in local languages and reflect local cultures. \u201cThe notion that you can take an intervention and just plop it down in a region where people might not even use the same terms for mental health as you're using is a little unrealistic,\u201d says Ben-Zeev. \u201cWhat we might call 'hearing voices' in the United States might be something like 'communicating with your elders' in a different region, depending on what label people attach to that experience.\u201d At this point, the notion that apps can deliver quality health care in low-income regions remains largely theoretical. \u201cThis is generally where the mHealth field is,\u201d says Natalie Leon, a scientist at the South African Medical Research Council in Cape Town. \u201cIt's a promise of potential effectiveness.\u201d \n               Good practice \n             To make good on that promise, apps will have to be tested. Between 2013 and 2015, the number of mobile-health trials registered on ClinicalTrials.gov more than doubled, from 135 to 300. And the number of trials specifically focused on mental and behavioural health increased by 32%, according to a report by the IMS Institute for Health Informatics in Parsippany, New Jersey. One digital health company that has earned praise from experts is Big Health, co-founded by Colin Espie, a sleep scientist at the University of Oxford, UK, and entrepreneur Peter Hames. The London-based company's first product is Sleepio, a digital treatment for insomnia that can be accessed online or as a smartphone app. The app teaches users a variety of evidence-based strategies for tackling insomnia, including techniques for managing anxious and intrusive thoughts, boosting relaxation, and establishing a sleep-friendly environment and routine. Before putting Sleepio to the test, Espie insisted on creating a placebo version of the app, which had the same look and feel as the real app, but led users through a set of sham visualization exercises with no known clinical benefits. In a randomized trial, published in 2012, Espie and his colleagues found that insomniacs using Sleepio reported greater gains in sleep efficiency \u2014 the percentage of time someone is asleep, out of the total time he or she spends in bed \u2014 and slightly larger improvements in daytime functioning than those using the placebo app 15 . In a follow-up 2014 paper 16 , they reported that Sleepio also reduced the racing, intrusive thoughts that can often interfere with sleep. The Sleepio team is currently recruiting participants for a large, international trial and has provided vouchers for the app to several groups of independent researchers so that patients who enrol in their studies can access Sleepio for free. \u201cWe think this is the way forward for digital health,\u201d says Espie. Mobile-phone-based treatments, he says, \u201cshould be tested and judged like any other intervention. We shouldn't treat people's health with any less respect because the treatment is coming through an app.\u201d\n See Comment  page 25 \n                 Tweet \n                 Facebook \n                 LinkedIn \n                 Weibo \n               \n                     Mind matters 2016-Apr-06 \n                   \n                     Mobile-phone health apps deliver data bounty 2016-Mar-21 \n                   \n                     Smartphones set to boost large-scale health studies 2015-Mar-10 \n                   \n                     Treating schizophrenia: Game on 2012-Feb-29 \n                   \n                     Nature  special: Mental health \n                   \n                     Nature  Special: Depression \n                   Reprints and Permissions"},
{"file_id": "532166a", "url": "https://www.nature.com/articles/532166a", "year": 2016, "authors": [{"name": "Cassandra Willyard"}], "parsed_as_year": "2006_or_before", "body": "Tumours are subject to the same rules of natural selection as any other living thing. Clinicians are now putting that knowledge to use. About six years ago, Alberto Bardelli fell into a scientific slump. A cancer biologist at the University of Turin in Italy, he had been studying targeted therapies \u2014 drugs tailored to the mutations that drive the growth of a tumour. The strategy seemed promising, and some patients started to make dazzling recoveries. But then, inevitably, their tumours became resistant to the drugs. Time and time again, Bardelli would see them relapse. \u201cI stumbled into a wall,\u201d he says. The problem wasn't the specific mutations, Bardelli realized: it was evolution itself. \u201cUnfortunately, we are facing one of the most powerful forces on this planet,\u201d he says. Researchers have long understood that tumours evolve. As they grow, mutations arise and populations of genetically distinct cells emerge. The cells that are resistant to treatment survive and expand. No matter what medication physicians apply, it seems, the tumour adapts. And it has been difficult for researchers to unpick this process, because cancer evolves inside the body over the course of years. \u201cWe used to say to patients all the time that cancers are evolving in a Darwinian manner, but we didn't have a huge amount of evidence at our disposal to really formally prove that,\u201d says Charles Swanton, a cancer researcher at the Francis Crick Institute in London. That is beginning to change. Thanks to advances in sequencing technology and the development of massive collections of samples and clinical data, scientists are piecing together a more precise picture of how cancer evolves, revealing the roots of resistance and, in some cases, finding out how it might be overcome. With a growing arsenal of treatments, biologists are trying to capitalize on these insights. \u201cCancer is continuously adapting, therefore we have to do so as well,\u201d Bardelli says. In that spirit, last year he shifted the focus of his lab to studying the evolution of cancer. His team has modelled 1  how colorectal cancers respond to targeted therapies that are given in combinations, potentially revealing ways to prevent the tumour cells from becoming resistant. \u201cWe have very exciting data now on the possibility to track and treat evolution,\u201d he says. \n               Tree of life \n             Cancer cells harbour a staggering array of mutations. In 2012, when Swanton and his colleagues sequenced multiple biopsies from two people with kidney cancer, they found that even within a single person, no two samples were the same 2 . The team examined not only the primary tumour, but also the satellite tumours \u2014 called metastases \u2014 that had spread throughout the patients' bodies. In each person, the team found more than 100 mutations in the various tumour samples analysed;  only about one-third of them occurred in all samples . The relationships between the various cancerous cells from a single person can be plotted out in much the same way as evolutionary biologists plot relationships between species: by drawing phylogenetic trees, branching diagrams that trace 'descendants' back to a common ancestor. Mutations that occur in the first malignant cells, those in the trunk of this evolutionary tree, will end up in all the tumour cells; mutations that arise later will be found only in the tree's branches. To eliminate the tumour, Swanton says, one must attack the mutations in the trunk. Therapies that target some of these trunk mutations already exist, and they often produce dramatic responses at first. But then resistance develops, as Bardelli found. \u201cWe're so fixated on 'the smaller the tumour gets, the better', but what one doesn't think about is what one has left behind,\u201d Swanton says. \u201cYou're often leaving resistant clones that you can't treat.\u201d But he thinks that by targeting multiple trunk mutations at the same time, researchers might have a shot at wiping out the cancer. Chances are slim that a single cancer cell would be able to evade a two- or three-pronged attack. One way to do this is to use combinations of targeted therapies. \u201cIn theory, they can work,\u201d says Bert Vogelstein, a cancer geneticist at the Sidney Kimmel Comprehensive Cancer Center at Johns Hopkins University in Baltimore, Maryland. In fact, when he and evolutionary biologist Martin Nowak at Harvard University in Cambridge, Massachusetts, modelled the strategy, they found that two targeted medicines for which no common resistance mechanism exists would be enough to control metastatic cancer 3 . For people with a large number of metastases, the model suggested that three therapies would be needed. Researchers are already beginning to test combinations of targeted therapies in the clinic. However, Swanton points out that there are no targeted drugs for the vast majority of mutations. And combining existing drugs in a way that won't harm the patient has proved tricky. So Swanton is focusing on immunotherapies \u2014 strategies that help  the immune system to recognize and destroy cancer cells . The immune system identifies threats, in part, by surveying the surfaces of cells, looking for molecules called antigens that can signal trouble within. The genetic defects in the DNA of a cancer cell can sometimes encode antigens that will trigger an immune response. But Swanton and his colleagues wondered whether it matters if the immune system responds to an antigen that arises from the cancer's evolutionary trunk or to one that springs from its branches. In a paper published in March 4 , he and his colleagues examined samples from the Cancer Genome Atlas, a collection of genetic and clinical data from thousands of people with cancer. They found that people with lung cancer who had lots of trunk antigens \u2014 and a high proportion of trunk antigens to branch antigens \u2014 survived longer than those who had either few trunk antigens or a higher proportion of branch antigens. What's more, people with many trunk antigens seemed to respond better to immune therapies. That makes sense, Swanton says, because if the immune system targets trunk antigens, it's hitting most of the cancer cells, rather than \u201cnipping off little branches\u201d. The research is still in its infancy, but Swanton is leading a clinical study that could help to confirm his findings. The study, called TRACERx \u2014 Tracking Cancer Evolution through Treatment (Rx) \u2014 will follow 850 people from lung-cancer diagnosis through treatment and, in some cases, until death. It will document genetic changes in their tumours over time to examine how lung cancer evolves, and how treatment influences that process. Once he has the data, Swanton hopes to raise enough money to test treatment strategies based on evolution. One approach would be to identify immune cells in a tumour, grow them in a lab, and then infuse them back into the patient \u2014 a technique called adoptive cell transfer. Similar strategies already in use select immune cells that recognize any cancer antigen, but Swanton's group would select those that are primed to recognize the trunk antigens that occur on all cancer cells. This strategy would not be cheap, but neither is doling out a string of targeted therapies only to watch them all eventually fail. \u201cEach course of therapy costs between US$10,000 and $100,000,\u201d Swanton says. If researchers could develop a therapy that would cure metastatic cancer, \u201cthe whole cost\u2013benefit analysis and the health economic models change dramatically\u201d. \n               Cellular competitors \n             Applying evolutionary principles might help the immune system to vanquish tumours. Robert Gatenby, a molecular oncologist at Moffitt Cancer Center in Tampa, Florida, has a more modest goal: he hopes to help people to live with their disease. Gatenby began thinking about cancer as an evolutionary problem in the early 1990s, when he was working at Fox Chase Cancer Center in Philadelphia, Pennsylvania. He saw so many people relapse that cancer began to seem less like a biological problem and more like witchcraft. \u201cIt's like an evil entity that just keeps recurring and overcoming your best efforts.\u201d But when he began thinking about cancer from an evolutionary perspective, the problem became tractable again, he says. Gatenby began trying to mathematically model the disease to work out how best to tackle it. His models suggest that many oncologists are taking the wrong approach. Typically, physicians will give the maximum dose of chemotherapy that a person can tolerate, to kill as many cancerous cells as possible. The hope is that they can wipe out the cancer before resistance evolves. But studies from recent years suggest that tumours harbour drug-resistant cells long before they encounter therapy 5 , 6 , 7 . The population of resistant cells stays small because resistance comes with a fitness cost. When a patient receives a hefty dose of chemotherapy, however, the resistant cells become much fitter than the susceptible cells. Gatenby likens drug resistance to an umbrella: \u201cIf it's raining, the umbrella is very useful. But if it's not raining, it's a burden.\u201d Gatenby thinks that he can capitalize on the natural competition between susceptible and resistant cells by managing drug dosage or timing more carefully. Recently, he tested the idea in mice with two kinds of breast cancer 8 . When he and his colleagues gave the mice the standard, maximum tolerated dose of the chemotherapy drug paclitaxel, the tumours roared back as soon as the treatment was stopped. The team also tried skipping doses whenever the tumour began to shrink, but that worked no better. A third group of mice received the standard high dose of chemotherapy at first, but once the animals' tumours started to shrink, the researchers dialled back the dose. This strategy resulted in the best survival for the mice and allowed three out of the five mice tested to be weaned off the drug completely. The treatment is meant to adapt to how the tumour responds and maintain a balance between drug-resistant and susceptible cells (see 'Evolving strategies'). \u201cI think it's one of the most exciting advances in cancer biology because it's a relatively easy thing to try,\u201d says Carlo Maley, a biologist at Arizona State University in Tempe who has collaborated with Gatenby. In May 2015, the Moffitt Cancer Center launched a pilot study to test whether this kind of adaptive-therapy approach might help people with prostate cancer. Physicians will monitor the patient's levels of prostate specific antigen (PSA), a marker of disease progression. They will then administer standard treatment or stop it, depending on what they see. Researchers have studied intermittent therapy in the past, but the protocols generally involve rigidly controlled cycles. \u201cWith adaptive therapy, the on\u2013off cycle is determined by the tumour response,\u201d Gatenby says. He also plans to use the wealth of molecular and clinical data from the trial to develop computer models that might guide adaptive therapy in the future. \n               In a bind \n             Physicians have noticed other evolutionary paradigms at work. In January, Jeffrey Engelman, a thoracic oncologist at Massachusetts General Hospital in Boston, and his colleagues detailed the case of a 52-year-old woman with metastatic lung cancer in  The New England Journal of Medicine 9 . The woman's tumour had a genetic rearrangement that produced a misshapen version of the ALK protein, so her doctors first administered the drug crizotinib, which inhibits the action of ALK. She responded well for 18 months, but then relapsed. A second-generation therapy also failed, so physicians moved onto a third-generation therapy that is still in clinical trials. That worked for a while, but after less than a year, the woman's liver began to fail, and she had to be hospitalized. Then her doctors found that the third therapy had prompted a new mutation that made her cancer once again responsive to crizotinib. When they administered the drug, her liver recovered, and she improved so much that she was able to leave the hospital. For Engelman and his colleagues, the woman's resensitization to crizotinib was a happy accident. But researchers may be able to direct cancer down such routes intentionally. Gatenby calls this strategy an evolutionary double bind, and he explains it like this: imagine trying to control a population of rats by introducing predators, such as hawks, that can pick them off from the sky. That type of predation selects for rodents that hide under brush. So one might bring in snakes that also hide under brush. The snakes would select for rats that prefer open spaces, making them vulnerable to hawks, Gatenby says. The same idea could apply in cancer. Use one treatment that makes the cancer more vulnerable to a second one, and then alternate between the two. It's \u201cnot whack-a-mole\u201d, Gatenby says, \u201cbut rather a carefully thought-out method that exploits the evolutionary dynamics\u201d. That strategy is exactly what Ben Solomon, a cancer researcher at the Peter MacCallum Cancer Centre in Melbourne, Australia, plans to test in an upcoming trial. Many people with lung cancer harbour mutations in a gene called  EGFR . Several drugs have been approved to target  EGFR  mutations, but tumours invariably develop resistance to them. In about half of patients, this resistance is caused by a mutation in  EGFR  called T790M. Last year, the US Food and Drug Administration approved a targeted drug called osimertinib, which inhibits the standard  EGFR  mutations as well as T790M, but people who respond to it tend to relapse within a year. Solomon and his colleagues plan to start trial participants on osimertinib and then monitor resistance by  tracking tumour DNA that circulates in their blood . The researchers expect to see a reduction in the T790M mutation. When that happens, they will switch to a first-generation EGFR inhibitor, which doesn't inhibit T790M. When T790M levels rise, the researchers will switch back to osimertinib. \u201cOur hypothesis is that that's going to delay the emergence of resistance to osimertinib, because we're not maintaining that selection pressure,\u201d says Solomon. He hopes to have final approval for the trial soon. There is no guarantee that any of these strategies will work. But even if the trials fail, the results of the tests will help researchers to refine their theories, and will address some of the big unknowns. How do the genetically diverse cells in a tumour interact, for example, and what is the role of the cellular environment that they inhabit? Kornelia Polyak, an oncologist at Harvard Medical School in Boston, says that cancer researchers tend to focus on the mutations inside cells, and fail to consider how those mutated cells might influence the cells around them. \u201cThat's a largely unexplored area,\u201d she says. The dynamics inside a tumour are exceedingly complicated, but Engelman is not discouraged. Clinical analyses will help researchers to understand this complexity. \u201cThese insights are going to bring us closer to having bigger and bigger impacts,\u201d he says. \u201cThe depressing thing is to not know what the hell is going on.\u201d \n                 Tweet \n                 Facebook \n                 LinkedIn \n                 Weibo \n               \n                     The tumour trail left in blood 2016-Apr-13 \n                   \n                     Cocktails for cancer with a measure of immunotherapy 2016-Apr-13 \n                   \n                     How elephants avoid cancer 2015-Oct-08 \n                   \n                     Change the cancer conversation 2015-Apr-01 \n                   \n                     Physicists' model proposes evolutionary role for cancer 2014-Oct-02 \n                   \n                     Cancer treatment: The killer within 2014-Apr-02 \n                   \n                     Natural selection: The evolution of cancer 2008-Aug-27 \n                   Reprints and Permissions"},
{"file_id": "531432a", "url": "https://www.nature.com/articles/531432a", "year": 2016, "authors": [{"name": "Garry Hamilton"}], "parsed_as_year": "2006_or_before", "body": "Long regarded as minor players in ocean ecology, jellyfish are actually important parts of the marine food web. Jennifer Purcell watches intently as the boom of the research ship  Skookum  slowly eases a 3-metre-long plankton net out of Puget Sound near Olympia, Washington. The marine biologist sports a rain suit, which seems odd for a sunny day in August until the bottom of the net is manoeuvred in her direction, its mesh straining from a load of moon jellyfish ( Aurelia aurita ). Slime drips from the bulging net, and long tentacles dangle like a scene from an alien horror film. But it does not bother Purcell, a researcher at Western Washington University's marine centre in Anacortes. Pushing up her sleeves, she plunges in her hands and begins to count and measure the messy haul with an assuredness borne from nearly 40 years studying these animals. Most marine scientists do not share her enthusiasm for the creatures. Purcell has spent much of her career locked in a battle to find funding and to convince ocean researchers that jellyfish deserve attention. But she hasn't had much luck. One problem is the challenges that come with trying to study organisms that are more than 95% water and get ripped apart in the nets typically used to collect other marine animals. On top of that, outside the small community of jellyfish researchers, many biologists regard the creatures as a dead end in the food web \u2014 sacs of salty water that provide almost no nutrients for predators except specialized ones such as leatherback sea turtles ( Dermochelys coriacea ), which are adapted to consume jellies in large quantities. \u201cIt's been very, very hard to convince fisheries scientists that jellies are important,\u201d says Purcell. But that's starting to change. Among the crew today are two fish biologists from the US National Oceanic and Atmospheric Administration (NOAA) whose research had previously focused on the region's rich salmon stocks. A few years ago, they discovered that salmon prey such as herring and smelt tend to congregate in different areas of the sound from jellyfish 1  and they are now trying to understand the ecological factors at work and how they might be affecting stocks of valuable fish species. But first, the researchers need to know how many jellyfish are out there. For this, the team is taking a multipronged approach. They use a seaplane to record the number and location of jellyfish aggregations, or 'smacks', scattered about the sound. And on the research ship, a plankton net has been fitted with an underwater camera to reveal how deep the smacks reach. Correigh Greene, one of the NOAA scientists on board, says that if salmon populations are affected in some way by jellyfish, \u201cthen we need to be tracking them\u201d. From the fjords of Norway to the vast open ocean waters of the South Pacific, researchers are taking advantage of new tools and growing  concern about marine health  to probe more deeply into the roles that jellyfish and other soft-bodied creatures have in the oceans. Initially this was driven by reports of  unusually large jellyfish blooms wreaking havoc  in Asia, Europe and elsewhere, which triggered fears that  jellyfish were taking over the oceans . But mounting evidence is starting to convince some marine ecologists that gelatinous organisms are not as irrelevant as previously presumed. Some studies show that the animals are important consumers of everything from microscopic zooplankton to small fish, others suggest that jellies have value as prey for a wide range of species, including penguins, lobsters and bluefin tuna. There's also evidence that they might enhance the flow of nutrients and energy between the species that live in the sunlit surface waters and those in the impoverished darkness below. \u201cWe're all busy looking up at the top of the food chain,\u201d says Andrew Jeffs, a marine biologist at the University of Auckland in New Zealand. \u201cBut it's the stuff that fills the bucket and looks like jelly snot that is actually really important in terms of the planet and the way food chains operate.\u201d \n               A mass of mush \n             The animals in question are descendants of some of Earth's oldest multicellular life forms. The earliest known jellyfish fossil dates to more than 550 million years ago, but some researchers estimate that they may have been around for 700 million years, appearing long before fish. They're also surprisingly diverse. Some are tiny filter feeders that can prey on the zooplankton that few other animals can exploit. Others are giant predators with bells up to two metres in diameter and tentacles long enough to wrap around a school bus \u2014 three times. Jellyfish belong to the phylum Cnidaria and have stinging cells that are potent enough in some species to kill a human. Some researchers use the term jellyfish, or 'jellies' for short, to refer to all of the squishy forms in the ocean. But others prefer the designation of 'gelatinous zooplankton' because it reflects the amazing diversity among these animals that sit in many different phyla: some species are closer on the tree of life to humans than they are to other jellies. Either way, the common classification exists mainly for one dominant shared feature \u2014 a body plan that is based largely on water. This structure can make gelatinous organisms hard to see. Many are also inaccessible, living far out at sea or deep below the light zone. They often live in scattered aggregations that are prone to dramatic population swings, making them difficult to census. Lacking hard parts, they're extremely fragile. \u201cIt's hard to find jellyfish in the guts of predators,\u201d says Purcell. \u201cThey're digested very fast and they turn to mush soon after they're eaten.\u201d For most marine biologists, running into a mass of jellyfish is nothing but trouble because their collection nets get choked with slime. \u201cIt's not just that we overlooked them,\u201d says Jonathan Houghton at Queen's University Belfast, UK. \u201cWe actively avoided them.\u201d But over the past decade and a half, jellyfish have become increasingly difficult to ignore. Enormous blooms along the Mediterranean coast, a frequent summer occurrence since 2003, have forced beaches to close and left thousands of bathers nursing painful stings. In 2007, venomous jellyfish drifted into a salmon farm in Northern Ireland, killing its entire stock of 100,000 fish. On several occasions, nuclear power plants have temporarily shut down operations owing to jelly-clogged intake pipes. \n               Jellies on the rampage \n             The news spurred scientists to take a closer look at the creatures. Marine biologist Luis Cardona at the University of Barcelona in Spain had been studying mostly sea turtles and sea lions. But around 2006, he shifted some of his attention to jellyfish after large summer blooms of mauve stingers ( Pelagia noctiluca ) had become a recurring problem for Spain's beach-goers. Cardona was particularly concerned by speculation that the jellyfish were on the rampage because overfishing had reduced the number of predators. \u201cThat idea didn't have very good scientific support,\u201d he says. \u201cBut it was what people and politicians were basing their decisions on, so I decided to look into it.\u201d For this he turned to stable-isotope analysis, a technique that uses the chemical fingerprint of carbon and nitrogen in the tissue of animals to tell what they have eaten. When Cardona's team analysed 20 species of predator and 13 potential prey, it was surprised to find that jellies had a major role in the diets of bluefin tuna ( Thunnus thynnus ), little tunny ( Euthynnus alletteratus ) and spearfish ( Tetrapturus belone ) 2 . In the case of juvenile bluefins, jellyfish and other gelatinous animals represented up to 80% of the total food intake. \u201cAccording to our models they are probably one of the most important prey for juvenile bluefin tuna,\u201d says Cardona. Some researchers have challenged the findings, arguing that stable-isotope results can't always distinguish between prey that have similar diets \u2014 jellyfish and krill both eat phytoplankton, for instance. \u201cI'm sure it's not true,\u201d Purcell says of the diet analysis. Fast-moving fish, she says, \u201chave the highest energy requirements of anything that's out there. They need fish to eat \u2014 something high quality, high calorie.\u201d But Cardona stands by the results, pointing out that stomach-content analyses on fish such as tuna have found jellyfish, but not krill. What's more, he conducted a different diet study 3  that used fatty acids as a signature, which supported his earlier results on jellyfish, he says. \u201cThey're probably playing a more relevant role in the pelagic ecosystem of the western Mediterranean than we originally thought.\u201d Researchers are reaching the same conclusion elsewhere in the world. On an expedition to Antarctica in 2010\u201311, molecular ecologist Simon Jarman gathered nearly 400 scat samples to get a better picture of the diet of Ad\u00e9lie penguins ( Pygoscelis adeliae ), a species thought to be threatened by global warming. Jarman, who works at the Australian Antarctic Division in Kingston, reported in 2013 that DNA analysis of the samples revealed that jellyfish are a common part of the penguin's diet 4 . Work that has yet to be published suggests the same is true for other Southern Ocean seabirds. \u201cAlbatrosses, gentoo penguins, king penguins, macaroni and rockhopper penguins \u2014 all of them eat jellyfish to some extent,\u201d says Jarman (see 'Lean cuisine'). \u201cEven though jellyfish may not be the most calorifically important food source in any area, they're everywhere in the ocean and they're contributing something to many top-level predators.\u201d And some parts of jellyfish hold more calories than others. Fish have been observed eating only the gonads of reproductive-stage jellyfish, suggesting a knack for zeroing in on the most energy-rich tissues. Through DNA analyses, researchers are also discovering more about how jellyfish function as refuges in the open ocean. Scientists have long known that small fish, crustaceans and a wide range of other animals latch on to jellyfish to get free rides. But in the past few years, it has become clear that the hitchhikers also dine on their transport. In the deep waters of the South Pacific and Indian oceans, Jeffs has been studying the elusive early life stages of the spiny lobster ( Panulirus cygnus ). During a 2011 plankton-collecting expedition 350 kilometres off the coast of Western Australia, he and his fellow researchers hauled in a large salp ( Thetys vagina ), a common barrel-shaped gelatinous animal. The catch also included dozens of lobster larvae, including six that were embedded in the salp itself. DNA analysis of the lobsters' stomach glands revealed that the larvae had been feeding on their hosts 5 . Jeffs now suspects that these crustaceans, which support a global fishery worth around US$2 billion a year, depend heavily on this relationship. \u201cWhat makes the larvae so successful in the open ocean,\u201d he says, \u201cis that they can cling to what is basically a big piece of floating meat, like a jellyfish or a big salp, and feed on it for a couple of weeks without exerting any energy at all.\u201d \n               Where did they go? \n             Researchers are starting to recognize that jellyfish are important for other reasons, such as transferring nutrients from one part of the ocean to another. Biological oceanographer Andrew Sweetman at the International Research Institute of Stavanger in Norway has seen this in his studies of 'jelly falls', a term coined to describe what happens when blooms crash and a large number of dead jellies sink rapidly to the sea floor. In November 2010, Sweetman began to periodically lower a camera rig 400 metres to the bottom of Lurefjorden in southwestern Norway to track the fate of this fjord's dense population of jellyfish 6 . Previous observations from elsewhere had suggested that dead jellies pile up and rot, lowering oxygen levels and creating toxic conditions. But Sweetman was surprised to find almost no dead jellies on the sea floor. \u201cIt didn't make sense.\u201d He worked out what was happening in 2012, when he returned to the fjord and lowered traps baited with dead jellyfish and rigged with video cameras. The footage from the bottom of the fjord showed scavengers rapidly consuming the jellies. \u201cWe had just assumed that nothing was going to be eating them,\u201d he says. Back on land, Sweetman calculated 7  that jelly falls increased the amount of nitrogen reaching the bottom by as much as 160%. That energy is going back into the food web instead of getting lost through decay, as researchers had thought. He's since found similar results using remotely operated vehicles at much greater depths in remote parts of the Pacific Ocean. \u201cIt's overturning the paradigm that jellyfish are dead ends in the food web,\u201d says Sweetman. Such discoveries have elicited mixed responses. For Richard Brodeur, a NOAA fisheries biologist based in Newport, Oregon, the latest findings do not change the fact that fish and tiny crustaceans such as krill are the main nutrient source for most of the species that are valued by humans. If jellyfish are important, he argues, it is in the impact they can have as competitors and predators when their numbers get out of control. In one of his current studies, he's found that commercially valuable salmon species such as coho ( Oncorhynchus kisutch ) and Chinook ( Oncorhynchus tshawytscha ) that are caught where jellyfish are abundant have less food in their stomachs compared with those taken from where jellies are rare, suggesting that jellyfish may have negative impacts on key fish species. \u201cIf you want fish resources,\u201d he says, \u201chaving a lot of jellyfish is probably not going to help.\u201d But other researchers see the latest findings as reason to temper the growing vilification of jellyfish. In a 2013 book chapter 8 , Houghton and his three co-authors emphasized the positive side of jellies in response to what they saw as \u201cthe flippant manner in which wholesale removal of jellyfish from marine systems is discussed\u201d. As scientists gather more data, they hope to get a better sense of exactly what role jellyfish have in various ocean regions. If jellies turn out to be as important as some data now suggest, the population spikes that have made the headlines in the past decade could have much wider repercussions than previously imagined. Back in Puget Sound, Greene is using a camera installed on a net to gather census data on a jellyfish smack. He watches video from the netcam as it slowly descends through a dense mass of creamy white spheres. At a depth of around 10 metres, the jelly curtain finally begins to thin out. Later, Greene makes a crude estimate. \u201cTwo point five to three million,\u201d he says, before adding after a brief pause, \u201cthat's a lot of jellyfish.\u201d A more careful count will come later. Right now there's plenty of slime to be hosed off the back deck. Once that's taken care of, the ship's engines come to life. The next jellyfish patch awaits. \n                 Tweet \n                 Facebook \n                 LinkedIn \n                 Weibo \n               \n                     Fisheries: Eyes on the ocean 2015-Mar-17 \n                   \n                     Ocean 'calamities' oversold, say researchers 2015-Jan-14 \n                   \n                     Coastal havoc boosts jellies 2014-Oct-29 \n                   \n                     Ocean sciences: Follow the fish 2013-Oct-09 \n                   \n                     Why a jellyfish is the ocean's most efficient swimmer 2013-Oct-07 \n                   \n                     Marine science: Oceanography's billion-dollar baby 2013-Sep-25 \n                   \n                     Taxonomy: The spy who loved frogs 2013-Sep-11 \n                   \n                     Systems ecology: Biology on the high seas 2013-Sep-04 \n                   \n                     Marine ecology: Attack of the blobs 2012-Feb-01 \n                   \n                     Jellyfish help mix the world's oceans 2009-Jul-29 \n                   \n                     Nature  special: Darwin 200 \n                   Reprints and Permissions"},
{"file_id": "531564a", "url": "https://www.nature.com/articles/531564a", "year": 2016, "authors": [{"name": "Richard Van Noorden"}], "parsed_as_year": "2006_or_before", "body": "Behind the scenes at Europe\u2019s massive synchrotron \u2014 where science never sleeps. By half past four in the morning, PhD student Warren Stevenson has been awake for 22 hours. \u201cI'm tired \u2014 but we work now or we don't work at all,\u201d he says, leaning back on his swivel chair and staring resolutely at two computer screens. \u201cWe plan to take a nap at 6 a.m. and wake up at 8. It'll be intense.\u201d Stevenson's intensity is warranted: he has just a few precious hours to collect data crucial for his PhD about the structure of liquid crystals. And he is not the only scientist determined to defy sleep on this freezing November night. Nearby, in rooms positioned around a vast circular chamber, workers are simultaneously conducting 28 separate experiments. This is the  European Synchrotron Radiation Facility (ESRF)  in Grenoble, France \u2014 a giant science factory that, for 24 hours a day and as many days a year as its engineers can safely manage, pulses out the most intense high-energy X-ray beams in the world. The teams who come here work in different fields: some study fossils, others batteries, still others proteins, minerals, artwork or  archaeological treasures . But each visits for the same reason: to examine the structure of material by exposing it to light with the short wavelength that's needed to reveal details at the scale of atoms. Work hours are long: most of the current visitors arrived yesterday and have been granted only a few 8-hour shifts to collect their data. And competition to use the machine is so fierce \u2014 around 45% of requests are granted \u2014 that the facility operates around the clock to fit in as many as possible. Altogether, some 8,000 scientists visit here each year, conducting upwards of 2,000 experiments and producing the raw material for around 1,800 papers. In the past seven years, two  Nobel prizes have been awarded  for discoveries made at the synchrotron. It is a non-stop, international, interdisciplinary laboratory \u2014 which is why  Nature  spent 24 hours walking its tunnels to observe a slice of contemporary science as it unfolds (see 'The synchrotron that never sleeps'). \u201cOur time here is precious. It will be months before our next visit \u2014 or it could be a year if we aren't lucky,\u201d says Xiangbing Zeng, Stevenson's PhD supervisor. Some visitors have teams large enough to rotate in shifts; many others will return home with dark circles under their eyes. But the scientists don't mind. They value the opportunity to bury themselves in data collection while escaping the rest of life. \u201cFor the week after, you feel jet-lagged,\u201d says biochemist Andrea Schmidt. \u201cIt doesn't get any easier. But it's always exciting to come here.\u201d \n               10:00 \n             Schmidt is already looking harried. She and two colleagues flew in yesterday from the Charit\u00e9 medical university in Berlin, bringing with them 120 precious protein crystals frozen in a flask of liquid nitrogen. The German biochemists have just 24 hours to deduce the structure of their crystal \u2014 a bacterial protein that has been engineered to fluoresce when hit by red light and that is used to tag and study structures in living tissue. What's not clear is how such proteins work at the level of atoms and molecules. It would be an \u201cimportant publication\u201d if the team can explain it, says Schmidt. Synchrotrons are the workhorses of structural biology. There are around 50 of them in the world, and the vast majority of the 6,000 or so  atomic-resolution protein structures  reported by scientists last year were solved at these facilities, by examining the diffraction patterns formed when high-power  X-rays are fired at crystallized proteins . At the ESRF, working quarters consist of a small office opening onto a cramped experimental room called a hutch, which is where Schmidt and her colleagues will analyse their crystals \u2014 each less than a hair's width across, and trapped in frozen solvent in the centre of a half-millimetre loop of nylon. As soon as Schmidt's team arrived this morning, the researchers began sticking tape over every visible light they could find. Light could cause molecules in the protein crystal to wiggle, making it impossible to capture an atomic-resolution picture. Schmidt flicks the light switch on and off, checking to see whether the rigged-up room is now completely dark. \u201cIt's MacGyver-like stuff,\u201d grins her PhD student Dennis Kwiatkowski. \n               10:23 \n             Inside the 844-metre central ring of the synchrotron, electrons are circling at nearly the speed of light and spitting out X-rays like water flicking from a spinning car wheel. The X-rays are channelled through thin pipes to as many as 43 stations, called beamlines, around the ESRF ring, with a hutch and office attached to each (see \u2018Circle of science\u2019). Earlier this morning, specialist engineers injected fresh bunches of accelerated particles into the ring because the machine had been off for maintenance the day before. Each beamline has its own dedicated ESRF scientist who calibrates the X-ray beam for that day's experiment, and often sits with the team all day. \u201cThere are a lot of critical components and a lot of possible modes of failure,\u201d says David von Stetten, the support scientist for Schmidt's team. \u201cIt's important to check it every day.\u201d \n               10:46 \n             Stevenson, Zeng and a third member of the team, PhD student Huanjun Lu, have started their day by preparing samples of the liquid crystals that they want to image. The group, from the University of Sheffield, UK, wants to work out how molecules are aligned inside the crystals, which are similar to those found inside LCD televisions. The work might one day lead to improved screens that can flicker on and off at higher rates. The team is one scientist down: a Chinese national couldn't get his visa in time, so it's impossible for the rest to pair up in shifts as they had planned. There won't be any chance to leave the building, says Stevenson, but he shrugs. \u201cIt's just the work ethic. We never see Grenoble when we come here.\u201d \n               12:15 \n             In a hutch 200 metres away, PhD student Annalisa Chieli is using a microtome to painstakingly cut ultra-thin slivers of dry paint from a block. It is her first time at the synchrotron. Yesterday, at home with her parents in Perugia, Italy, she watched a YouTube video about it. Now she is here, she says, \u201cyou can feel the research in the air\u201d. Chieli, from the University of Perugia, is preparing to shine X-rays at samples taken from one of the four versions of  The Scream , a painting by the famously melancholic Norwegian artist Edvard Munch, as well as samples from Jackson Pollock's  Alchemy . Yellowish hues in both artworks have faded over time, and museum curators are not sure why. The synchrotron work could help to explain it, and perhaps protect the art. Today is prep work, explains team leader Letizia Monico, from the Institute of Molecular Science and Technology of Perugia: they are scanning various samples of yellow paint to build up a reference library showing how pigments of different elemental composition absorb X-ray light and how that changes with age. Tomorrow, precious flakes from the real paintings will be scanned and compared with this gallery to decipher how the compounds in them have changed with time. Like most of the researchers here, Monico and Chieli need pay nothing to visit. Twenty-one countries have signed up as partners of the synchrotron, contributing a combined \u20ac110 million (US$124 million) to operate it each year. The cash covers all the food and accommodation expenses for three members of each team visiting from a partner nation. \n               13:17 \n             Because the clientele is international, the two canteens have to be, too. The larger one is buzzing as visitors and staff scientists tuck into a selection of meat, fish, pizza, pasta, desserts, cheese, yogurt \u2014 and a choice of wines (often French). Altogether, the canteen doles out some 1,000 meals a day. The working language of the ESRF is English, but here the multilingual chatter is like a session at the United Nations. Monico's team picks the quieter, upper canteen. \u201cFor me, salad, and lots of vegetables,\u201d says Monico, a small, brown-haired Italian who gesticulates animatedly as she explains the chemistry of the cadmium sulfide yellow pigment that she expects to find in Munch's work. \u201cArt is my passion,\u201d she says. The third member of the group, Frederik Vanmeert, of the University of Antwerp in Belgium, says his passions lie elsewhere. \u201cI'm interested in the techniques,\u201d he says. \n               14:00 \n             Back at her hutch, Schmidt's expression grows \u2014 if possible \u2014 slightly more anxious. The team is behind schedule: half the day has been spent setting up everything for the experiment on the light-sensitive protein. \u201cIf you don't have a good method to overcome frustration, then you are in the wrong field,\u201d she says, staring at a magnified computer-screen image of her frozen sample. Schmidt and her group leader, Patrick Scheerer, want to get a picture with better than 2.8-\u00e5ngstr\u00f6m resolution to distinguish the separate amino-acid side chains of the protein. But Schmidt is on the 9th sample out of 100, and hasn't seen a discernible image yet. \u201cNinety per cent of the experiments go wrong,\u201d she says, as she moves her mouse to decide where to aim the X-ray beam next. She clicks the mouse button \u2014 and throws up her hands in frustration. \u201cAah, 4.85 \u00e5ngstr\u00f6ms. You can only see a sheet or a helix, nothing else. Patrick will kill me if I don't collect the data.\u201d Schmidt knows she could be in for a slog. When US structural biologist Brian Kobilka used the ESRF's tightly focused beamline in 2007 to take the  first picture of a G-protein-coupled receptor  \u2014 a structure that would win him the 2012 Nobel Prize in Chemistry \u2014 his team worked through 1,043 crystals before it hit the jackpot. \n               16:00 \n             Next door, Matthew Bowler is looking relaxed. He has already scanned 80 crystal samples, couriered in from Helsinki the day before, and has successfully determined 3 protein structures without breaking a sweat. Bowler hasn't done this all by himself. Rather, he works on one of the world's only  automated X-ray crystallography beamlines . The robot, which Bowler built with other ESRF staff scientists, loads up crystal samples, scans each frozen blob, decides where to fire the X-rays for best results, collects data sets and then moves on to the next one. \u201cIt does this 24 hours a day, and it doesn't get tired,\u201d Bowler says with pride. The robot has processed 15,000 crystals over the past year. (Unfortunately for Schmidt, the specialized light conditions required by her protein make it impossible for the robot to handle.)  For the week after, you feel jet-lagged. But it's always exciting to come here.  Bowler supervises the machinery and loads crystals into a sample-changer at the beginning of the day, but doesn't need to intervene regularly: researchers who send in samples to be analysed are informed of results by automated e-mails. The best part? It keeps Bowler's hours regular: he works from 8 a.m. until 6 p.m. each day. \u201cI'm going to pick up my daughter from a piano lesson, then I'm done for the day,\u201d he says. \n               18:50 \n             Nearby, Alexey Nikulin and Natalia Lekontseva are filling up a vacuum flask at a liquid-nitrogen dispenser. They flew in from the Institute of Protein Research in Pushchino, near Moscow, on Monday, and depart at the end of the week. \u201cHamburg's synchrotron doesn't work at the moment, and Berlin's is full up,\u201d says Nikulin, as the two trundle the full, heavy vessel back to their hutch. The liquid nitrogen will be used to keep their crystals, of an RNA-binding protein, frozen until they are ready for use. \n               19:10 \n             Out in the corridor, Magnus Larsson walks past, on his way home for the day. He's easily identifiable as a non-researcher from his suit. Larsson is an industrial liaison officer who is visiting from the MAX IV synchrotron in Lund, Sweden, the world's first  'fourth-generation' machine , which promises to produce tighter, brighter beams of X-rays when it opens in June. He has been visiting to get tips on how the ESRF works with industry researchers, who pay their own way but are allowed no more than 10% of the synchrotron's beamtime. After he's gone, the synchrotron's grey corridors are empty. The air conditioning clanks and hums. Although each group works just a minute or two's walk away from its neighbours, the teams rarely interact. As darkness falls outside, researchers begin drifting back towards the synchrotron's canteen for dinner. The superior intensity of the X-rays generated by modern machines \u2014 which collect data in minutes, where older ones took hours \u2014 means that scientists who come to synchrotrons now have less time away from their hutches, says Joanne McCarthy, who's in charge of the ESRF's user programme. \u201cIn the olden days, you prepared your samples for measurement and went off to have dinner in town,\u201d she says. \n               21:00 \n             The synchrotron's electrons are tiring out. The lead-lined pipe is not a perfect vacuum, and occasional collisions with air molecules make the tightly focused bunches of electrons spread out slightly and lose energy, causing the flux of each X-ray beam to drop. Engineers pump replacements into the synchrotron's storage ring to maintain the beams through the night. \n               21:36 \n             In hutch ID22, the computer emits a gong sound \u2014 an alert signifying that the most recent measurement has ended. The team of five scientists, led by Boaz Pokroy from the Technion in Haifa, Israel, applied for time here eight months ago. It was worth the wait, Pokroy says: \u201cThe ESRF is, no question, the best synchrotron on this Earth.\u201d The researchers are here in the name of materials science: they are examining the crystals that form when biological molecules, such as amino acids, are mixed with inorganic minerals such as copper oxide, so that the two crystallize together in a composite. The team is exploring whether this changes the electronic properties of the minerals in predictable ways, Pokroy says, perhaps leading to semiconductors with new properties. Carlotta Giacobbe, their Italian staff-support scientist, points to a closed-circuit-television picture on the computer monitor, which shows a tall yellow robotic arm dipping to remove some tubes of powdered crystalline samples from the X-ray beam. \u201cThere are a lot of sounds,\u201d Giacobbe says: \u201cthere's a train whistle when the measurement starts, a gong when it stops and a scream if there's a problem with data backup.\u201d (There's also a donkey bray and the sound of laughter, which play when the scientists make mistakes.) \n               22:19 \n             The researchers in Monico's team are finishing their paint analyses and are about to retire to the ESRF's guesthouse, which has 173 rooms and is full up tonight. Vanmeert is entering code on the computer, instructing a robot to move a sample of pigments slowly through the X-ray beam while they're gone. \u201cIf you make a single error, you lose the night,\u201d warns ESRF beamline scientist Marine Cotte. Vanmeert is careful, but he is also looking forward to bed. \u201cI will read a book and dream about beamlines,\u201d he says. \n               23:00 \n             At Pokroy's beamline, things aren't going so well. The gong sounds, and Giacobbe looks at the camera, expecting the yellow arm to move towards a fresh array of samples. Nothing. \u201cNo \u2026 shit. Wait, it didn't work,\u201d she sighs. \u201cOK. This means that the night will be longer than expected.\u201d Pokroy leans over. \u201cThe robot is shy? Why didn't it change?\u201d \u201cI don't know,\u201d Giacobbe says, rapidly typing in commands. She and Pokroy go inside the hutch and fish around at cables, while the rest of the team looks concerned. Pokroy finds a possible culprit \u2014 a red cable that appears to have been cut. But no-one is sure of the problem, and it is too late to call in a technician, so data collection will stop. \u201cWe will lose a night,\u201d Giacobbe says, looking downcast. The six scientists put on their coats, pick up their bags, and leave for the guesthouse. Behind them, the robot is still. \n               23:13 \n             The guesthouse is so full that some scientists couldn't get rooms. Georgios Kalantzopoulos and Erik Glesne, two materials scientists from the University of Oslo, set out to walk the 40 minutes into Grenoble for a hotel they have booked there instead. It's raining, and by the time they arrive, they are drenched. \n               00:00 \n             At last, a breakthrough. By slogging through dozens of crystals, Schmidt's team has managed to get a 2.5-\u00e5ngstr\u00f6m-resolution picture of the light-sensitive protein \u2014 enough for the core of a research paper, although they won't know the protein's full shape until the researchers can analyse the data later on. \u201cIf we have exciting results like these, we don't need any coffee to stay up,\u201d says Schmidt's colleague, Bilal Qureshi. Qureshi says he likes the single-minded focus the team can achieve on a synchrotron visit. \u201cWe generate a lot of ideas, and we don't think about much else.\u201d \n               2:00 \n             The synchrotron is emptying \u2014 but where samples can't be changed by robot or need careful supervision, dozens of experimenters remain in their hutches. One of them is Stefan Mebs, a cheery German chemist from the Free University in Berlin; he and his colleague Peer Schrapers have elected to do night shifts for the next two weeks, because their samples must be changed every half-hour by hand. \u201cOn the day shift, you can sleep at normal times \u2014 but the experiments are more stressful,\u201d says Mebs. \u201cAt night, it's more relaxed. It's quite calm.\u201d This group is blasting X-ray pulses at haemoglobin \u2014 the oxygen-carrying protein in red blood cells \u2014 in the hope of working out exactly how oxygen molecules bind to its iron-atom core. The team wants to examine this at room temperature as well as the sub-zero conditions that are usual for working with crystals, to see whether the binding is different at each, says Mebs. \n               3:00 \n             On the far side of the synchrotron, three scientists are still up \u2014 although they have installed a sofa in their office in case they have time to catch a nap, says Julie Villanova, the ESRF staffer on the beamline. \u201cWhen people come here, they are really motivated, and they push us all,\u201d she says. Her two users for the day \u2014 Peter Joergenesen and Salvatore De Angelis of the Technical University of Denmark in Kongens Lyngby \u2014 are taking pictures of tiny pits in electrodes from different samples of solid oxide fuel cells to see how they have degraded over time. Villanova makes an espresso in a kitchen nearby, although she does not drink it herself \u2014 something that impresses the Danish visitors, who can't imagine how else one would stay up. \u201cSometimes people jog around the inside of the tunnel,\u201d Villanova says. \u201cThough there's a nice river not far away.\u201d \n               5:00 \n             Despite their earlier success, Schmidt and her team have come unstuck because of a broken sample holder. After half an hour trying to fix it, they give up for the night. With 30 crystals not yet analysed, they creep wearily back to the guesthouse for 5 hours' sleep before they fly back to Germany. Schmidt will be back in two weeks to do it all again. \n               6:45 \n             Stevenson and Zeng have been up for well over 24 hours, but they have managed to obtain results on two liquid crystal samples. \u201cI'm as pleased as I can be in the present circumstances,\u201d Stevenson says. \u201cI think I'll just pass out, eventually.\u201d \n               8:00 \n             With the night shift ending, cleaners mop and vacuum the tunnels before a new tide of scientists pours into the synchrotron. Giacobbe returns early with two colleagues to try to sort out last night's problem. \u201cWe thought it was the wires, but it was actually a software fault,\u201d she says. \u201cI feel relieved. It's nice when you discover what the problem is and fix it.\u201d The gong sounds, and then the train whistle, and in the hutch beyond, a robotic arm whisks around to pick up a sample. Giacobbe smiles. \u201cIt's OK to lose one shift. They can recover the time.\u201d \n               9:00 \n             Two fresh-faced researchers walk into the office just vacated by Schmidt's group and put down their bags. Theodoras Goulas and Mariana Castrillo Bricenyo have just arrived from the Institute of Molecular Biology in Barcelona, Spain \u2014 and now, like their predecessors, they have 24 hours in the hutch. Goulas is looking forward to the long day ahead. \u201cI'm pretty sure we'll get something,\u201d he says.\n \n                 Tweet \n                 Facebook \n                 LinkedIn \n                 Weibo \n               \n                     Next-generation X-ray source fires up 2015-Aug-26 \n                   \n                     X-ray science: The big guns 2014-Jan-29 \n                   \n                     Protein structures: Structures of desire 2009-May-06 \n                   \n                     Nature  special: crystallography at 100 \n                   \n                     ESRF \n                   \n                     Lightsources of the world \n                   Reprints and Permissions"},
{"file_id": "531428a", "url": "https://www.nature.com/articles/531428a", "year": 2016, "authors": [{"name": "Davide Castelvecchi"}], "parsed_as_year": "2006_or_before", "body": "A momentous signal from space has confirmed decades of theorizing on black holes \u2014 and launched a new era of gravitational-wave astronomy. The event was catastrophic on a cosmic scale \u2014 a merger of black holes that violently shook the surrounding fabric of space and time, and sent a blast of  space-time vibrations known as gravitational waves  rippling across the Universe at the speed of light. But it was the kind of calamity that physicists on Earth had been waiting for. On 14 September, when those ripples swept across the freshly upgraded  Laser Interferometer Gravitational-Wave Observatory  (Advanced LIGO), they showed up as spikes in the readings from its two L-shaped detectors in Louisiana and Washington state. For the first time ever,  scientists had recorded a gravitational-wave signal . \u201cThere it was!\u201d says LIGO team member Daniel Holz, an astrophysicist at the University of Chicago in Illinois. \u201cAnd it was so strong, and so beautiful, in both detectors.\u201d Although the shape of the signal looked familiar from the theory, Holz says, \u201cit's completely different when you see something in the data. It's this transcendent moment\u201d. The signal, formally designated GW150914 after the date of its occurrence and informally known to its discoverers as 'the Event', has justly been hailed as a milestone in physics. It has provided a wealth of evidence for  Albert Einstein's century-old general theory of relativity , which holds that mass and energy can warp space-time, and that gravity is the result of such warping. Stuart Shapiro, a specialist in computer simulations of relativity at the University of Illinois at Urbana\u2013Champaign, calls it \u201cthe most significant confirmation of the general theory of relativity since its inception\u201d. But the Event also marks the start of a long-promised era of gravitational-wave astronomy. Detailed analysis of the signal has already yielded insights into the nature of the black holes that merged, and how they formed. With more events such as these \u2014 the LIGO team is analysing several other candidate events captured during the detectors' four-month run, which ended in January \u2014 researchers will be able to classify and understand the origins of black holes, just as they are doing with stars. Still more events should appear starting in September, when Advanced LIGO is scheduled to begin joint observations with its European counterpart, the Franco\u2013Italian-led Advanced Virgo facility near Pisa, Italy. (The two collaborations already pool data and publish papers together.) This detector will not only contribute crucial details to events, but could also help astronomers to make cosmological-distance measurements more accurately than before. \u201cIt's going to be a really good ride for the next few years,\u201d says Bruce Allen, managing director of the Max Planck Institute for Gravitational Physics in Hanover, Germany. \u201cThe more black holes they see whacking into each other, the more fun it will be,\u201d says Roger Penrose, a theoretical physicist and mathematician at the University of Oxford, UK, whose work in the 1960s helped to lay the foundation for the theory of the objects. \u201cSuddenly, we have a new way of looking at the Universe.\u201d \n               A matter of energy \n             Physicists have known for decades that every pair of orbiting bodies is a source of gravitational waves. With each revolution, according to Einstein's equations, the waves will carry away a tiny fraction of their orbital energy. This will cause the objects to move a bit closer together and orbit a little faster. For familiar pairs, such as the Moon and Earth, such energy loss is imperceptible even on timescales of billions of years. But dense objects in very close orbits can lose energy much more quickly. In 1974, radio astronomers Russell Hulse and Joseph Taylor, then of the University of Massachusetts Amherst,  found just such a system : a pair of dense neutron stars in orbit around each other. As the years went by, the scientists found that this 'binary pulsar' was losing energy and spiralling inwards exactly as predicted by Einstein's theory. The two black holes detected by LIGO had probably been losing energy in this way for millions, if not billions, of years before they reached the end. But LIGO did not register the gravitational waves coming from them until 9:50:45 Coordinated Universal Time on 14 September, when the wave's frequency rose above some 30 cycles per second (hertz) \u2014 corresponding to 15 full black-hole orbits per second \u2014 and was finally high enough for the detectors to distinguish it from background noise. But then, in just 0.2 seconds, LIGO watched the signal surge to 250 hertz and suddenly disappear, as the black holes made their final 5 orbits, reached orbital velocities of half the speed of light and coalesced into a single massive object (see 'What made the wave'). The LIGO and Virgo teams soon went to work  extracting every bit of information possible . At the most fundamental level, the signal gave them an existence proof: the fact that the objects came so close to each other before merging meant that they had to be black holes, because ordinary stars would need to be much bigger. \u201cIt is, I think, the clearest indication that black holes are really there,\u201d says Penrose. The signal also provided researchers with the first empirical test of general relativity beyond regions \u2014 including the space around the binary pulsar \u2014 where there is comparatively little space-time warping. There was no empirical evidence that the theory would keep its validity at the extreme energies of merging black holes, says Shapiro \u2014 but it did. The signal held a trove of more-detailed information as well. By scrutinizing its shape just before the final cataclysm, the scientists found that it closely approximated a simple sine wave with a steadily increasing frequency and amplitude. According to B. S. Sathyaprakash, a theoretical physicist at Cardiff University, UK, and a senior LIGO researcher, this pattern suggests that the orbits of the black holes were nearly circular, and that LIGO probably had a bird's-eye view of the circles, looking almost straight down on them rather than edge-on. In addition, the LIGO and Virgo teams were able to use the frequency of the observed wave, along with its rate of acceleration, to estimate the masses of the two black holes: because heavier objects radiate energy in the form of gravitational waves at a faster rate than do lighter objects, their pitch rises more quickly. By recreating the Event with computer simulations, the scientists calculated that the two black holes weighed about 36 times and 29 times the mass of the Sun, respectively, and that the combined black hole weighed about 62 solar masses 1 . The lost difference, about three Suns' worth, was dispersed as gravitational radiation \u2014 much of it during what physicists call the 'ringdown' phase, when the merged black hole was settling into a spherical shape. (For comparison, the most powerful thermonuclear bomb ever detonated converted only about 2 kilograms of matter into energy \u2014 roughly 10 30  times less.) The teams also suspect that the final black hole was spinning at perhaps 100 revolutions per second, although the margin of error on that estimate is large. The inferred masses of the two black holes are also revealing. Each object was presumably the remnant of a very massive star, with the larger star approaching 100 times the mass of the Sun and the smaller one a little less. Thermonuclear reactions are known to convert hydrogen in the cores of such stars into helium much faster than in lighter stars, which leads them to collapse under their own weight only a few million years after they are born. The energy released by this collapse causes an  explosion called a type II supernova , which leaves behind a residual core that turns into a neutron star or, if it's massive enough, a black hole. Scientists say that type II supernovae should not produce black holes much bigger than about 30 solar masses \u2014 and both black holes were at the high end of that range. This could mean that the system formed from interstellar gas clouds that were richer in hydrogen and helium than the ones typically found in our Galaxy, and that were poorer in heavy elements \u2014 which astronomers call metals. Astrophysicists have calculated that stars formed from such low-metallicity clouds should have an easier time forming massive black holes when they explode, explains Gijs Nelemans, an astronomer at Radboud University Nijmegen in the Netherlands and a member of the Advanced Virgo collaboration. That's because during a supernova explosion, smaller atoms are less likely to be blown away by the blast. Low-metallicity stars thus \u201close less mass, so more of it goes into the black hole, for the same initial mass\u201d, Nelemans says. \n               Two by two \n             But how did these two black holes end up in a binary system? In a paper 2  published at the same time as the one reporting the discovery, the LIGO and Virgo teams described two commonly accepted scenarios. The simplest one is that two massive stars were born as a binary-star system, forming from the same interstellar gas cloud like a double-yolked egg, and orbiting each other ever since. (Such binary stars are common in our Galaxy; singletons such as the Sun are the exception, rather than the rule.) After a few million years, one of the stars would have burned out and gone supernova, soon to be followed by the other. The result would be a binary black hole. The second scenario is that the stars formed independently, but still inside the same dense stellar cluster \u2014 perhaps one similar to the globular clusters that orbit the Milky Way. In such a cluster, massive stars would sink towards the centre and, through complex interactions with lighter stars, form binary systems, possibly long after their transformation into black holes.  It is, I think, the clearest indication that black holes are really there.  Simulations made by Simon Portegies Zwart, an astrophysicist at Leiden University in the Netherlands, show 3  that massive stars are more likely to form in dense clusters, where collisions and mergers are more common. He also finds that once a binary black-hole system forms, the complex dynamics of the cluster's centre would probably kick the pair out at high speed. The binary that Advanced LIGO detected may have wandered away from any galaxy for billions of years before merging, he says. Although the LIGO and Virgo teams were able to learn a lot from the Event, there is much more that gravitational waves could teach them, even in the case of black-hole mergers.The detectors showed that immediately after the black holes merged, the waves quickly died down as the resulting black hole settled into a symmetrical shape. This is consistent with predictions made by theoretical physicist C. V. Vishveshwara in the early 1970s, a time when \u201cgravitational waves and black holes both belonged to the realm of mythology\u201d, he says. \u201cAt that time, I had not imagined that it would ever be verified,\u201d says Vishveshwara, who is director emeritus of the Jawaharlal Nehru Planetarium in Bangalore, India. But LIGO saw only just over one cycle of the Event's ringdown waves before the signal became buried once more in the background noise \u2014 not yet enough data to provide a rigorous test of Vishveshwara's predictions. More-stringent tests will be possible if and when LIGO detects black-hole mergers that are larger than this one, or that occur closer to Earth than the Event's estimated distance of 1.3 billion light years, and thus give 'louder' waves that stay above the noise for longer. Alessandra Buonanno, a LIGO theorist and director of the Max Planck Institute for Gravitational Physics in Potsdam-Golm, Germany, says that a more detailed picture of the ringdown stage could reveal how fast the final black hole rotates, as well as whether its formation gave it a 'natal kick', imparting a high velocity. In addition, says Sathyaprakash, \u201cwe are especially waiting for systems that are much lighter, so they last longer\u201d. Such events could include the mergers of lighter binary black holes, of binary neutron stars or of a black hole with a neutron star. Each type would deliver its own signature chirp, and could produce a signal that stays above LIGO's threshold of sensitivity for several minutes or more. \u201cGW150914 is in some sense a very vanilla system,\u201d says Chad Hanna, a LIGO member at Pennsylvania State University in University Park. \u201cIt's beautiful, of course, but it doesn't have all the crazy things that one might expect.\u201d \n               Space artistry \n             One phenomenon that Sathyaprakash is eager to observe is a 'precession' of the black holes' orbital plane, meaning that their paths trace a kind of 3D rosette. This is a relativistic effect that has no counterpart in Newtonian gravity, and it should produce a characteristic fluctuation in the strength of the gravitational waves. But orbital precession occurs only when two black holes have axes of rotation that point in random directions, and it disappears when the axes are both perpendicular to the orbital plane. The occurrence of a precession could provide clues to how the black holes formed. It's hard to be sure about that possibility because there are many uncertainties in simulating supernovas. But astrophysicists suspect that parallel spins generally signify that the original two stars were born together out of the same whirling gas cloud. Similarly, they think that random spins result from black holes that formed separately and later fell into orbit around each other. Once the observatories find more mergers, they may be able to determine which type of system occurs more frequently. Although detecting more events will help LIGO to do lots of science, its interferometers have intrinsic limitations that make it necessary to work together with a worldwide network of similar detectors that are now coming online. First, LIGO's two interferometers are not enough for scientists to determine precisely where the waves came from. The researchers can get some information by comparing the signal's time of arrival at each detector: the difference enables them to calculate the wave's direction relative to an imaginary line drawn between the two. But in the case of the Event, which recorded a difference of 6.9 milliseconds, their calculations limited the field of possibilities merely to a wide strip of southern sky. Had Virgo been online, the scientists could have narrowed down the direction substantially by comparing the waves' arrival times at three places. With a fourth interferometer (Japan is building an underground one called KAGRA, for Kamioka Gravitational-Wave Detector, and India has its own LIGO in planning), their precision would improve much more. Knowing an event's direction will in turn remove one of the biggest uncertainties in determining its distance from Earth. Waves that approach from a direction exactly perpendicular to the detector \u2014 either from above or from below, through Earth \u2014 will be recorded at their actual amplitude, explains Fulvio Ricci, a physicist at the University of Rome La Sapienza and the spokesperson for Virgo. Waves that come from elsewhere in the sky, however, will hit the detector at an angle and produce a somewhat smaller signal, according to a known formula. There are even some blind spots, where a source cannot be seen by a given detector at all. Determining the direction will therefore reveal the exact amplitude of the waves. By comparing that figure with the waves' amplitude at the source, which the researchers can derive from the shape of the signal, and by knowing how the amplitude decreases with distance, which they get from Einstein's theory, they can then calculate the distance of the source to a much higher precision. This situation is almost unprecedented: conventionally, astronomical distances need to be estimated by looking at the brightness of known objects in locations that range from the Solar System to distant galaxies. But the measured brightness of those 'standard candles' can be dimmed by stuff in between. Gravitational waves have no such limitation. \n               Raising the alarm \n             There is another important reason why scientists are eager to have precise estimates of the waves' provenance. The LIGO and Virgo teams have arranged to give near-real-time alerts of intriguing events to more than 70 teams of conventional astronomers, who will use their optical, radio and space-based telescopes to see whether those events produced any form of electromagnetic radiation. In return, the LIGO and Virgo collaborations will be sifting through data to search for gravitational waves that could have been generated by events, such as supernova explosions, seen by the conventional observatories. Some 20 teams tried to follow up on the Event, mostly to no avail. NASA's Fermi Gamma-ray Space Telescope did see a possible burst of \u03b3-rays about 0.4 seconds later, coming from an equally vague but compatible region of the southern sky 4 . But most observers now consider it to be a coincidence. Such \u03b3-rays could, in principle, have been produced when gas orbiting the binary black hole was heated up during the merger, says Vicky Kalogera, a LIGO astrophysicist at Northwestern University in Evanston, Illinois. But \u201cour astrophysical expectation has been that the gas from stars that formed the binary black hole has long dispersed. There shouldn't be any significant gas around\u201d, she says. Going forward, however, matching gravitational waves with electromagnetic ones could  usher in a new era of astronomy . In particular, mergers of neutron stars are expected to produce short \u03b3-ray bursts. Researchers could then measure how far the light from those bursts is shifted towards the red end of the spectrum, which would tell astronomers how fast the stars' host galaxies are receding owing to the expansion of the Universe. Matching those redshifts to distance measurements calculated from gravitational waves should give estimates of the current rate of cosmic expansion, known as the Hubble constant, that are independent \u2014 and potentially more precise \u2014 than calculations using current methods. \u201cFrom the point of view of measuring the Hubble constant, that's our gold-plated source\u201d, says Holz. The LIGO and Virgo teams estimate that they have a 90% chance of finding more events in the data that LIGO has already collected. They are confident that by the time the next run finishes, the event count will be at least 5, growing to perhaps 35 by the end of a run scheduled to start in 2017. \u201cTo be honest,\u201d says Holz, \u201cI find it really hard to believe that the Universe is really doing this stuff. But it's not science fiction. It really happened.\u201d \n                 Tweet \n                 Facebook \n                 LinkedIn \n                 Weibo \n               \n                 See Editorial \n                 page 413 \n               \n                     Gravitational waves: How LIGO forged the path to victory 2016-Feb-16 \n                   \n                     The hundred-year quest for gravitational waves \u2014\u00a0in pictures 2016-Feb-10 \n                   \n                     Nature  special: Gravitational waves \n                   \n                     Nature  special: General relativity at 100 \n                   \n                     Simulating Extreme Spacetimes \n                   Reprints and Permissions"},
{"file_id": "532296a", "url": "https://www.nature.com/articles/532296a", "year": 2016, "authors": [{"name": "Julia Rosen"}], "parsed_as_year": "2006_or_before", "body": "As it pursues independence, Greenland seeks to develop its economy without ruining one of Earth's last pristine places. The houses of Narsaq gleam in a cheerful riot of blues, reds and yellows. The crayon-coloured town spills across a hill that separates barren mountains from a fjord filled with icebergs. But up close, grimmer details come into focus; the paint on many homes is peeling, and few signs of life stir in the narrow streets. Established as a trading post in 1830, Narsaq long served as a hub of Greenland's fishing industry \u2014 the backbone of its economy. But in the past few decades, modernization has moved much of the fishing offshore, and many jobs in Narsaq have disappeared. The town's 1,500 residents have been struggling to find a way forward. The same could be said of Greenland at large. Part of the kingdom of Denmark since 1814, Greenland has transformed over the past century from a society based on subsistence hunting and fishing to one built around an industrial economy and a Nordic-style welfare system. But that rapid development has stalled, leaving communities such as Narsaq to grapple with economic stagnation and high rates of unemployment. At the same time, Greenland has sought to overcome its economic and political dependence on Denmark. \u201cI don't know any people \u2014 any country \u2014 who don't want self-determination, who don't want independence in the world,\u201d says Hjalmar Dahl, president of the Greenlandic branch of the Inuit Circumpolar Council. Some 80% of Greenland's population is Inuit. Over the past 35 years,  Greenland has gained increasing control  over its internal affairs \u2014 it was granted self rule in 2009 \u2014 but it continues to receive Danish subsidies that account for roughly one-third of its gross domestic product (GDP). To gain true independence, it will have to generate almost US$1 billion in additional revenues \u2014 all from a population of just 56,000 people on an island with only 150 kilometres of roads and an ice sheet about 3 times the size of Texas. But Greenlandic leaders see promise in places like Narsaq. Geological studies of the rugged peaks outside town have identified valuable deposits of rare-earth metals, uranium and zinc; a major mine is approaching the final stages of obtaining a permit. These are just some of many such deposits that have attracted the attention of international mining companies, and which proponents say could usher in a new era of prosperity. Researchers and some residents have challenged the idea that Greenland can mine its way to independence. A bitter debate has erupted over the social and environmental impacts that mining will have on one of the last pristine parts of the planet. Now, leaders are looking for opportunities \u2014 and investors \u2014 to expand other industries such as tourism and agriculture, as well as ways to optimize Greenland's vast fishing sector. The government must juggle these goals while  contending with climate change , which threatens traditional ways of life and potentially bolsters new ones. Whatever route Greenland chooses to follow, researchers say that it needs to start paving the way now. Even if the island forgoes full political independence, Danish subsidies will remain fixed at 2009 levels, adjusted for inflation, and the funds will not help to cover the rising costs of Greenland's ageing population or to sustain small towns like Narsaq. \u201cIt is a very urgent problem because Greenland already runs at a deficit,\u201d says Minik Rosing, a geologist at the University of Copenhagen who is well known in Greenland for his work on the island's future. Unless something changes, he says, \u201ceverything points toward the situation getting worse rather than better\u201d. \n               Rich rocks \n             Narsaq's name means 'plain' in Kalaallisut, the official language of Greenland, probably because the town occupies the flattest piece of land in sight. Mountains rise on all sides, their summits dulled by millions of years of glacial erosion. The  inland ice sheet  lurks just over the horizon, leaving only a thin ribbon of ice-free terrain. But what little exposed land there is happens to be rich in minerals (see 'Mineral futures'). The crust here is ancient \u2014 up to 3.8 billion years old, in places \u2014 and has seen many cycles of volcanism and rifting. These brought metal-rich fluids close to the surface, where they formed deposits. The island also has substantial offshore oil and gas resources that could come into play if fuel prices rise or exploration costs drop. Interest in the minerals has grown over the past decade, thanks to a confluence of forces. Greenland gained the right to manage and profit from its mineral deposits in 2009, just as the  global appetite for many metals  started rising. Politicians quickly pointed to mining as the best, and perhaps only, way to offset Danish subsidies and make statehood possible. At the moment, many have their eye on the Kvanefjeld deposit near Narsaq, a contender to host Greenland's first major mine. The resource there is \u201cpotentially huge\u201d, says Kathryn Goodenough, a geologist with the British Geological Survey in Edinburgh. She works with EURARE, an initiative to develop Europe's rare-earth potential that brings together researchers and mining companies such as Greenland Minerals and Energy (GME), the Australian company behind the Kvanefjeld project. GME has been exploring here since 2007 and has studied core samples from hundreds of holes drilled into the nearby mountains over the years. \u201cIt's like Swiss cheese up there,\u201d says Ib Laursen, a company representative based in Narsaq. GME has estimated that the rocks above the town hold approximately 11 million tonnes of rare-earth oxides and that Kvanefjeld is one of the largest rare-earth deposits outside China. Another company is seeking to develop the Kringlerne deposit across the fjord, which it calls a world-class reserve of rare earths and other metals. Until mining starts, it is not clear whether these deposits will prove as extraordinary as the companies contend, says Rosing. But the geologist, who grew up on a reindeer farm outside the Greenlandic capital, Nuuk, is optimistic about the future of the island's mining industry. \u201cGreenland is exceptional, it is large,\u201d he says. \u201cI think with enough effort, there will be definitely something happening.\u201d Like many Narsaq residents, Mariane Paviasen desperately hopes that the mining boom doesn't start at Kvanefjeld. She works for Air Greenland, greeting the handful of helicopter flights that touch down on Narsaq's blustery landing pad. Her house, at the top of a narrow road on the far side of the town, is bright and inviting on a sunny day in September. Some oppose the mine because it would bring an influx of foreign workers, but Paviasen is most worried about the uranium in the deposit, which GME plans to extract and sell along with the rare-earth elements. It's what first brought Narsaq to the attention of scientists, including Niels Bohr, who visited in 1957 as part of Denmark's investigations into atomic energy. The country later banned all nuclear activity, including uranium mining, and Paviasen wishes that Greenland had upheld the tradition. \u201cI think it is very dangerous stuff \u2014 the most dangerous stuff in the world,\u201d she says. That's why, in late 2014, she helped to found a citizens' group called Urani Naamik, or No to Uranium. GME's current plans call for an open-pit mine on top of the plateau, about 10 kilometres from town. Paviasen's group has highlighted the potential risks from uranium to human and environmental health, through water pollution and dust exposure. \u201cMy husband and my sons and my father \u2014 they like to go out and catch some food,\u201d Paviasen says. But she wouldn't eat it if mining began. Others, including environmental organizations in Denmark, have cited the dangers of the radioactive thorium in the deposits, which currently has little commercial value, and of fluorine-containing minerals that can acidify water. Such concerns have fuelled a heated dispute over how to balance the economic benefits of exploiting Greenland's natural resources with the environmental risks. GME insists that Kvanefjeld can be mined safely. The company says that it is considering ways to contain the thorium, and that it will lock up fluorine by converting it into a marketable mineral. \u201cThat's a part of the demand from the government \u2014 to use best practices,\u201d Laursen says. Studies have found 1  that modern techniques for managing tailings can minimize the contamination risk, at least in the short term. The technical details of GME's plans, however, won't be revealed until the Environmental Impact Assessment report comes out later this year. Economic forces may be the biggest barrier to Greenland's mineral plans: the prices of rare earths and other metals have slumped after reaching all-time highs in 2011. \u201cThe simple reality is, it doesn't look good,\u201d says Tim Boersma, a fellow at the Brookings Institution in Washington DC, who co-authored a 2014 report 2  on Greenland's mining potential.  The dream that mining could be a quick fix \u2014 that's not going to happen.  Greenland would need about 24 major mines operating simultaneously to replace the Danish subsidy, according to a 2014 joint report 3  by the University of Copenhagen and the University of Greenland in Nuuk that assessed how the island's mineral resources might shape its future. Given what is known about the deposits, that would be a tall order even in good economic times, says Rosing, who chaired the committee that wrote the report. \u201cThe dream that mining could be a quick fix for the economy \u2014 that's not going to happen,\u201d he says. \n               Power plans \n             As the results of the report have sunk in, talk of political independence has dwindled. Many Greenlanders realize that the process will take time, and Rosing says that some young people have started to question the benefits of completely severing ties with Denmark. In their view, he says, \u201ca nation of 56,000 people is maybe not the best way of ensuring that individuals in Greenland can shape their own future\u201d. Disappointments in the mining sector have spurred discussion about diversifying the strategy for economic self-sufficiency. Rosing suggests that Greenland should devise other ways to profit from what makes it unique. He is exploring the possibility of marketing rock flour \u2014 the fine powder created by glacial erosion \u2014 as a source of nutrients and neutralizing agents for tropical soils. And he says that Greenland should court industries that would benefit from its cold climate, such as computer-server farms, which use enormous amounts of energy for cooling. Greenland has begun harnessing its torrents of glacial melt water to produce renewable energy. The island has 5 hydropower plants, and government estimates suggest that it has enough untapped potential to produce 800,000 gigawatt hours of energy per year \u2014 more than the total used by the United Kingdom and France combined. The aluminium producer Alcoa, based in New York City, has considered building a smelter to capitalize on the cheap energy and, in 2010, Greenland's national energy company launched a pilot project using hydropower to produce clean-burning hydrogen fuel. For the moment, however, those options are largely prospects for the future. Today, about 40% of Greenland's workers are employed by the public sector and 90% of its export economy revolves around fishing, particularly for northern shrimp ( Pandalus borealis ) and Greenland halibut ( Reinhardtius hippoglossoides ). Although catches remain good, west Greenland's shrimp stocks have declined over the past decade, perhaps influenced by climate change. According to Helle Siegstad, director of fish and shellfish at the Greenland Institute of Natural Resources in Nuuk, the culprit could be Atlantic cod ( Gadus morhua ), a predator that could benefit from warming near Greenland and has started to reappear  after being overfished . Another factor behind the shrimp's decline might be that climate change has caused a mismatch between their hatching time and the blooms of phytoplankton that they eat 4 . But higher water temperatures have also lured new species north, such as Atlantic mackerel, Atlantic herring and even some bluefin tuna 5 , says Brian MacKenzie, a marine ecologist at the Technical University of Denmark in Kongens Lyngby. In recent years, temperatures off the coast of east Greenland have become warm enough for tuna, MacKenzie says. \u201cIt's basically a whole new habitat.\u201d Siegstad says that the fishing fleet has been quick to pounce on these opportunities, and she is optimistic that changes in marine ecosystems will ultimately benefit Greenland's fishing industry. But even so, she worries about the island's overwhelming dependence on this variable, uncontrollable resource. \u201cWe are so sensitive,\u201d she says. \u201cI hope we will have something else.\u201d \n               Growth industry \n             Thirty minutes by boat from Narsaq, Kalista Poulsen and Agathe Devisme share 10 hectares of land with 300 head of sheep. Compared with the surrounding tundra, their grassy farm is lush. Purple wildflowers and  Angelica archangelica  \u2014 a popular medicinal herb \u2014 line their carefully manicured fields. The couple is part of Greenland's budding agricultural industry \u2014 one of several small sectors of the economy that the island's leaders are trying to expand. Agriculture currently accounts for less than 1% of Greenland's GDP, but that figure could grow  thanks to climate change , which has boosted temperatures in the south by almost 2 \u00b0C over the past few decades. Modelling work6 by Jens Christensen and others at the Danish Meteorological Institute in Copenhagen suggests that if the world warms according to some of the most dramatic projections, the length of the growing season in southern Greenland will more than double. But the climate is likely to become more variable, too. Already, a string of dry summers has forced farmers to import extra supplies of hay from abroad, supplementing the feed that they grow to get the animals through the long, brutal winter. This has left them wondering whether climate change will help or hurt them, says Devisme. \u201cFor the moment, it's more, kind of, disturbing.\u201d To supplement their farming income, Devisme also runs a small bed and breakfast, where visitors come to relax or to fish for Arctic char in the stream behind the fields. Many Greenlanders see the island's nascent tourism industry as a welcome alternative to exploitative activities such as the mining at Kvanefjeld, which Devisme says poses a threat to her businesses. In 2013, the government counted roughly 35,000 visitors, who contributed around 3% of GDP. Greenland hopes to ramp up adventure tourism, such as hiking and kayaking, and boost cruise-ship traffic \u2014 a pattern that has succeeded in Iceland. The consulting firm Ramboll, based in Orestad, Denmark, has projected that the tourism industry could more than double by 2025, although this would require strong investment in infrastructure such as hotels and airports, as well as increased marketing and international cooperation. But, if Greenland is to benefit from these industries, its people must have the skills to work in them. Developing the island's human capital may be the key to Greenland's success, according to a 2013 report 7  by the Copenhagen Institute for Futures Studies. Today, although many Greenlanders  possess a wealth of informal knowledge , only 35% of students go beyond compulsory school, which they finish at age 15 or 16. The government aims to boost the number continuing with their training, and the plan starts with strengthening elementary education. The residents of Narsaq are doing their part. Here, late on a Sunday afternoon, workers bustle around a fenced-off construction site in the centre of town. A crane swings overhead, hoisting wooden beams onto a tower of scaffolding, where crews are renovating the red-panelled school. It will soon boast a wall of windows, giving Narsaq's children a grand view of the mineral-rich mountains, the ice-choked fjord and their own small town \u2014 as it lurches forward into Greenland's uncertain future. \n                 Tweet \n                 Facebook \n                 LinkedIn \n                 Weibo \n                 Wechat \n               \n                     US plans upgrade for ageing Greenland research station 2014-Oct-07 \n                   \n                     Greenland defied ancient warming 2013-Jan-23 \n                   \n                     Rediscovered photos reveal Greenland's glacier history 2012-May-28 \n                   \n                     Greenland gambles on warmer, richer climate 2008-Nov-26 \n                   \n                     Climate change: Losing Greenland 2008-Apr-16 \n                   \n                     Nature special: 2015 Paris climate talks \n                   Reprints and Permissions"},
{"file_id": "532300a", "url": "https://www.nature.com/articles/532300a", "year": 2016, "authors": [{"name": "David Cyranoski"}], "parsed_as_year": "2006_or_before", "body": "China is positioning itself as a world leader in primate research. An hour's drive from Kunming in southwestern China, past red clay embankments and sprawling forests, lies an unusual zoo. Inside the gated compound is a quiet, idyllic campus; a series of grey, cement animal houses stack up on the lush hillside, each with a clear plastic roof to let in the light. This is the Yunnan Key Laboratory of Primate Biomedical Research, and its inhabitants are some 1,500 monkeys, all bred for research. The serenity of the facility belies the bustle of activity within. Since it opened in 2011, this place has quickly become a Mecca for cutting-edge primate research, producing valuable disease models and seminal publications that have made its director, Ji Weizhi, a sought-after collaborator. Its campus houses a collection of gene-edited monkeys that serve as models of Duchenne muscular dystrophy,  autism  and Parkinson's disease. Ji plans to double the number of group leaders working there from 10 to 20 in the next 3 years, and to seek more international collaborations \u2014 he already works with scientists in Europe and the United States. \u201cIn terms of a technology platform, Ji is just way ahead,\u201d says one collaborator, cardiologist Kenneth Chien at the Karolinska Institute in Stockholm. Ji is not alone in his ambitions for monkey research. With support from central and local governments, high-tech primate facilities have sprung up in Shenzhen, Hangzhou, Suzhou and Guangzhou over the past decade. Last month, the science ministry approved the launch of a facility at the Kunming Institute of Zoology that is expected to cost millions of dollars to build. These centres can provide scientists with monkeys in large numbers, and offer high-quality animal care and cutting-edge equipment with little red tape. A major brain project, expected to be announced in China soon, will focus much of its efforts on using monkeys to study disease. The enthusiasm stands in stark contrast to the climate in the West, where non-human-primate research is increasingly  stymied by a tangle of regulatory hurdles, financial constraints and bioethical opposition . Between 2008 and 2011, the number of monkeys used in research in Europe declined by 28%, and some researchers have stopped trying to do such work in the West. Many have since sought refuge for their experiments in China by securing collaborators or setting up their own laboratories there. Some of the Chinese centres are even advertising themselves as primate-research hubs where scientists can fly in to take advantage of the latest tools, such as gene editing and advanced imaging. \u201cIt could be like CERN in Switzerland, where they set up a large facility and then people come from all over the world to get data,\u201d says Stefan Treue, a neuroscientist who heads the German Primate Center in G\u00f6ttingen, Germany.  China will become the place where all therapeutic strategies have to be validated.  With China fast becoming a global centre for primate research, some scientists fear that it could hasten the atrophy of such science in the West and lead to a near monopoly, in which researchers become over-reliant on one country for essential disease research and drug testing. \u201cGovernments and politicians don't see this, but we face a huge risk,\u201d says Erwan Bezard, who is director of the Institute of Neurodegenerative Diseases at the University of Bordeaux in France, and has set up his own primate-research company, Motac, in Beijing. Europe and the United States still have the lead in primate research, he says, but this could change as expertise migrates eastwards. \u201cChina will become the place where all therapeutic strategies will have to be validated. Do we want that? Or do we want to stay in control?\u201d \n               Simian similarities \n             For decades, researchers have relied on monkeys to shed light on brain function and brain disease because of their similarity to humans. Growth in neuroscience research has increased demand, and although high costs and long reproductive cycles have limited the use of these animals in the past, new reproductive technologies and genetic-engineering techniques such as  CRISPR\u2013Cas9  are helping researchers to overcome these drawbacks, making monkeys a more efficient experimental tool. China has an abundance of macaques \u2014 the mainstay of non-human-primate scientific research. Although the population of wild rhesus macaques ( Macaca mulatta ) has declined, the number of farmed animals has risen. According to data from the Chinese State Forestry Administration, the number of businesses breeding macaques for laboratory use rose from 10 to 34 between 2004 and 2013, and the quota of animals that those companies could sell in China or overseas jumped from 9,868 to 35,385 over that time. Farm populations of marmosets, another popular research animal, are also on the rise. Most monkeys are shipped to pharmaceutical companies or researchers elsewhere in the world, but the growing appreciation among scientists of monkey models has prompted investment by local governments and private companies in dedicated research colonies. The country's 2011 five-year plan singled out primate disease models as a national goal; the science ministry followed up by pumping 25 million yuan (US$3.9 million) into the endeavour in 2014. Scientists visiting China are generally pleased with the care given to animals in these facilities, most of which have, or are trying to get, the gold-standard recognition of animal care \u2014 accreditation by AAALAC International. Ji's Yunnan Key Laboratory is the most active primate facility, but others are giving it competition. The new monkey facility at the Kunming Institute of Zoology was funded as part of the national development scheme for big science equipment that includes telescopes and supercomputers. The money will help the institute to double its colony of 2,500 cynomolgus monkeys ( Macaca fascicularis ) and rhesus macaques. Zhao Xudong, who runs the primate-research facility, says that the plan is to \u201cset it up like a hospital, with separate departments for surgery, genetics and imaging\u201d, and a conveyer belt to move monkeys between departments. There will be systems for measuring body temperature, heart rate and other physiological data, all to analyse the characteristics, or 'phenotypes', of animals, many of which will have had genes altered. \u201cWe are calling it the 'genotype versus phenotype analyser',\u201d says Zhao. It will take ten years to finish, but he hopes to begin building this year and to start research within three. Other facilities, although smaller, are also expanding and diversifying. The Institute of Neuroscience in Shanghai plans to increase its population of 600 Old World monkeys to 800 next year and expand its 300-strong marmoset colony. \n               A question of cost \n             Outside China, the numbers are heading in the opposite direction. Harvard Medical School closed its affiliated primate facility in May 2015 for 'strategic' reasons. Last December, the US National Institutes of Health decided to phase out non-human-primate experiments at one of its labs and subsequently announced that it would review all non-human-primate research that it funds. In Europe, researchers say, the climate is also growing colder for such research. Costs are a major disincentive. In 2008, Li Xiao-Jiang, a geneticist at Emory University in Atlanta, Georgia, helped to create the world's first transgenic monkey model of Huntington's disease 1  with colleagues at Yerkes National Primate Research Centre. But Li says that it costs $6,000 to buy a monkey in the United States, and $20 per day to keep it, whereas the corresponding figures in China are $1,000 and $5 per day. \u201cBecause the cost is higher, you have to write a bigger grant, and then the bar will be higher when they judge it,\u201d says Li. Funding agencies \u201creally do not encourage large-animal research\u201d. For Li, the solution was simple: go to China. He now has a joint position at the Institute of Genetics and Developmental Biology in Beijing, where he has access to around 3,000 cynomolgus monkeys at a farm in Guangzhou and some 400 rhesus monkeys at the Chinese Academy of Medical Sciences' monkey facility in Beijing. He has churned out a series of publications on monkeys with modified versions of the genes involved in Duchenne muscular dystrophy 2  and Parkinson's disease 3 . Neuroscientist Anna Wang Roe says that red tape drove her to China. Roe's team at Vanderbilt University in Nashville, Tennessee, is attempting to work out how modules in the brain are connected, and she estimates that she and her colleagues have spent 25% of their time and a good deal of cash documenting the dosage and delivery-method for each drug they administered to their monkeys, as required by regulations. \u201cWe record something every 15 minutes,\u201d she says. \u201cIt's not that it's wrong. It's just enormously time-consuming.\u201d In 2013, impressed by the collaborative atmosphere at Zhejiang University in Hangzhou, she proposed that it build a neuroscience institute. The next day the university agreed, and she soon had a $25-million, 5-year budget. \u201cOnce the decision is made, you can start writing cheques,\u201d she says. She is now closing her US laboratory to be the director of the Zhejiang Interdisciplinary Institute of Neuroscience and Technology, where she hopes to open a suite of the latest brain-analysis tools, including a powerful new 7-tesla functional magnetic resonance imaging device that she says will give images of the primate brain at unprecedented resolution.  This place just makes things happen quickly.  Bob Desimone was similarly impressed with the speed at which China moves. As a neuroscientist who heads the McGovern Institute for Brain Research at the Massachusetts Institute of Technology in Cambridge, in January 2014, he had a 'meet and greet' with the mayor of Shenzhen. In March, the mayor donated a building on the Shenzhen Institute of Advanced Technology campus for a monkey-research facility, and the centre's soon-to-be director, Liping Wang, promised that it would be ready by summer. Thinking that impossible, Desimone bet two bottles of China's prized mind-numbing liquor,  maotai , that it wouldn't be done in time. He lost. The group raised most of the $10 million needed from city development grants, along with a small input from McGovern, and soon the first animals were being installed in the Brain Cognition and Brain Disorder Research Institute. \u201cThis place just makes things happen quickly,\u201d Desimone says. But money and monkeys alone are not enough to lead to discovery. Researchers say that China is short on talented scientists to take advantage of the opportunities provided by animal research. That's why the organizers of the country's new primate centres hope to attract an influx of foreigners to permanent posts or as collaborators. So far, many of those moving to China have been Chinese or foreigners with a previous connection to the country, but others are expressing interest, says neuroscientist Guoping Feng, also at the McGovern Institute. Already, the Shenzhen primate centre has recruited from Europe and the United States, and Desimone says that it will be \u201can open technology base. Anyone who wants to work with monkeys can come.\u201d \n               Edited monkeys \n             The  rapid spread of CRISPR\u2013Cas9 and TALEN gene-editing tools  is likely to accelerate demand for monkey research: they are turning the genetic modification of monkeys from a laborious and expensive task into a  relatively quick, straightforward one . Unlike engineered mice, which can be bred and sent around the world, \u201cmonkeys are difficult to send, so it will be easier for the PI or postdoc to go there\u201d, says Treue. Already, competition is fierce as researchers are racing for the low-hanging fruit \u2014 engineering genes with established roles in human disease or development. Almost all reports of gene-edited monkeys produced with these techniques have come from China. Desimone predicts that the pursuit of monkey disease models \u201ccould give China a unique niche to occupy in neuroscience\u201d. The cages of Ji's facility are already full of the products of gene editing. One troop of animals has had a  mutation genetically engineered into the  MECP2  gene , which has been identified as the culprit in humans with Rett's syndrome, an autism spectrum disorder. An animal sits listless and unresponsive, holding tight to the bars of the cage as her normal twin sister crawls all over her. In another cage, a monkey with the mutation pumps its arm, reminiscent of repetitive behaviour seen in the human disorder. Some incessantly suck their thumbs. \u201cI've never seen that in a monkey before \u2014 never so constant,\u201d says Ji. Among the range of other disease models in Ji's menagerie are monkey versions of cardiovascular disease, which he is working on in collaboration with the Karolinska Institute. And last year, Ji made the world's first chimeric monkeys using embryonic stem cells 4 , an advance that could make the production of genetically modified animals even easier. The question now is whether these genetically modified monkeys will propel understanding of human brain function and dysfunction to a higher level. \u201cYou can't just knock out one gene and be sure you'll have human-like disease phenotype,\u201d says Ji. Researchers see an opportunity to understand human evolution as well as disease. Su Bing, a geneticist at the Kunming Institute of Zoology, is working with Ji to engineer monkeys that carry the human version of a gene called  SRGAP2 , which is thought to endow the human brain with processing power by  allowing the growth of connections between neurons . Su also plans to use CRISPR\u2013Cas9 to introduce human versions of  MCPH1 , a gene related to brain size, and the human  FOXP2  gene, which is thought to give humans unique language ability. \u201cI don't think the monkey will all of a sudden start speaking, but will have some behavioural change,\u201d predicts Su. \n               International divide \n             Although the opportunities are great, there are still obstacles for scientists who choose to locate their animal research in China. Trying to keep a foot in two places can be challenging, says Gr\u00e9goire Courtine, a spinal-cord-injury researcher based at the Swiss Federal Institute of Technology in Lausanne, who travels almost monthly to China to pursue his monkey research at Motac. He has even flown to Beijing, done a couple of operations on his experimental monkeys, then returned that night. \u201cI'm 40 years old, I have energy in my body. But you need to really will it,\u201d he says. Another downside, says Li, is that policies can change suddenly in China. \u201cThere is uncertainty. That makes us hesitate to commit,\u201d says Li, who has retained his post at Emory University. And the immunity that China's primate researchers have had to animal-rights activism could start to erode, warns Deborah Cao, who researches law at Griffith University in Brisbane, Australia, and last year published a book on the use of animals in China 5 . People are starting to use Chinese social-media sites to voice outrage at the abuse of animals, Cao says. China has competition in its bid to dominate primate research, too. Japan has launched its own brain project  focused on the marmoset as a model : the animal reaches sexual maturity in a year and a half, less than half the time it takes a macaque. Some research facilities in China are now building marmoset research colonies \u2014 but Japan is considered to be several years ahead. And some researchers want to ensure that such work continues outside Asia. Courtine says that he's \u201cfighting to keep alive\u201d a monkey-research programme he has at Fribourg, Switzerland, because he thinks it's important to have a division of labour. \u201cResearch that requires quantity, I'll do in China. I would like to do sophisticated work in Fribourg,\u201d he says. Back at his primate centre in Yunnan, Ji is sure that such work is already taking place. His dream, he says is \u201cto have an animal like a tool\u201d for biomedical discovery. He knows there is a lot of competition in this field, especially in China. But he feels confident: \u201cThe field is wide, and there are many, many projects we can do.\u201d See  Editorial \n . \n                 Tweet \n                 Facebook \n                 LinkedIn \n                 Weibo \n                 Wechat \n               \n                     Welcome to the CRISPR zoo 2016-Mar-09 \n                   \n                     Monkeys genetically modified to show autism symptoms 2016-Jan-25 \n                   \n                     NIH to retire all research chimpanzees 2015-Nov-18 \n                   \n                     CRISPR, the disruptor 2015-Jun-03 \n                   \n                     Biomedicine: The changing face of primate research 2014-Feb-05 \n                   \n                     First monkeys with customized mutations born 2014-Jan-30 \n                   \n                     Animal rights: Chimpanzee research on trial 2011-Jun-15 \n                   \n                     Nature  Special: CRISPR \n                   \n                     Kunming Institute of Zoology \n                   \n                     Zhejiang Interdisciplinary Institute of Neuroscience and Technology \n                   \n                     Shenzen Brain Cognition and Brain Disorder Research Institute \n                   Reprints and Permissions"},
{"file_id": "532162a", "url": "https://www.nature.com/articles/532162a", "year": 2016, "authors": [{"name": "Heidi Ledford"}], "parsed_as_year": "2006_or_before", "body": "The next frontier in cancer immunotherapy lies in combining it with other treatments. Scientists are trying to get the mix just right. In cancer research, no success is more revered than the huge reduction in deaths from childhood leukaemia. From the 1960s to the 2000s, researchers boosted the number of children who survived acute lymphoblastic leukaemia from roughly 1 in 10 to around 9 in 10. What is sometimes overlooked, however, is that these dramatic gains against the most common form of childhood cancer were made not through the invention of new drugs or technologies, but rather through a reassessment of the tools in hand: a dogged analysis of the relative gains from different medicines and careful strategizing over how best to apply them side by side as combination therapies. \u201cIt wasn't just about pounding drugs together,\u201d says Jedd Wolchok, a medical oncologist at Memorial Sloan Kettering Cancer Center in New York City. \u201cIt was about understanding the mechanism and figuring out what should be given when.\u201d That lesson has particular relevance in cancer research today. A new class of immunotherapies \u2014 which  turn the body's immune system against cancerous cells  \u2014 is elevating hopes about combination therapies again. The drugs, called checkpoint inhibitors, have already generated great excitement in medicine when applied on their own. Now there are scores of trials mixing these immune-boosting drugs with one another, with radiation, with chemotherapies, with cancer-fighting viruses, with cell treatments and more. \u201cThe field is exploding,\u201d says Crystal Mackall, who leads the paediatric cancer immunotherapy programme at Stanford University in California. Fast-moving trends in cancer biology often fail to meet expectations, and little is yet known about how these drugs work together. Some observers warn that the combinations being tested are simply marriages of convenience \u2014 making use of readily available compounds or capitalizing on business alliances. \u201cIn many cases, we're moving forward without a rationale,\u201d says Alfred Zippelius, an oncologist at the University of Basel in Switzerland. \u201cI suspect we'll see some disappointment in the next few years with respect to immunotherapy.\u201d But many clinicians argue that delay is not an option as their patients queue up for the next available clinical trial. \u201cRight now I have more patients that could benefit from combinations than there are combinations being tested,\u201d says Antoni Ribas, an oncologist at the University of California, Los Angeles. \u201cWe're always waiting on the next slot.\u201d \n               Lying in wait \n             Immunotherapies have been more than a century in the making, starting when physicians first noticed mysterious remissions in a few people with cancer who contracted a bacterial infection. The observations led to a hypothesis: perhaps the immune system is able to kill tumours when made hypervigilant by an infection. The concept has vast appeal. What better way  to beat a fast-evolving biological system such as a tumour  than with a fast-evolving biological immune system? But it took decades for researchers to turn that observation into something useful. Part of the trouble, they eventually learned, is that tumours suppress the immune response. T cells, the immune system's weapon of choice against cancer, would sometimes gather at the edge of a tumour and then just stop. It turned out that a class of molecules called inhibitory checkpoint proteins was holding those T cells at bay. These proteins normally protect the human body from unwarranted attack and autoimmunity, but they were also limiting the immune system's ability to detect and fight tumours. In 1996, immunologist James Allison, now at the University of Texas MD Anderson Cancer Center in Houston, showed that switching off a checkpoint protein called CTLA-4 helped mice to fend off tumours 1 . The discovery suggested that there was a way to re-mobilize T cells and beat cancer. In 2011, the US Food and Drug Administration (FDA)  approved the first checkpoint inhibitor  \u2014 a drug, called ipilimumab, that inhibits CTLA-4 \u2014 to treat advanced melanoma. The improvements were modest: about 20% of patients benefited from ipilimumab, and the survival gain was less than four months on average 2 . But a handful of recipients are still alive a decade after starting the therapy \u2014 a stark contrast with most new cancer drugs, which often benefit more patients in the short term, but don't have a durable response (see 'Desperately seeking survival'). Ipilimumab was at the leading edge of a flood of checkpoint inhibitors to enter clinical trials. The drug's developer, Bristol-Myers Squibb of New York, followed up with the approval of  nivolumab, which inhibits the protein PD-1 . And a host of other companies have jumped into the immunotherapy fray, as have academics such as Edward Garon at the University of California, Los Angeles. \u201cOur group gladly shifted into this,\u201d says Garon, who began focusing on checkpoint inhibitors in 2012. \u201cIt was very clear this was going to have a major impact.\u201d But even as the family of checkpoint inhibitors was rapidly expanding, the drugs were running up against the same frustrating wall: only a minority of patients experienced long-lasting remission. And some cancers \u2014 such as prostate and pancreatic \u2014 responded poorly, if at all, to the drugs. Further research revealed a possible explanation: many people who were not responding well to the drugs were starting the  treatment without that phalanx of T cells waiting  at the margins of their tumours. (In the lingo of the field, their tumours were not inflamed.) Researchers reasoned that if they could raise this T-cell response first, and recruit the cells to the edges of the tumour, they might get a better result with the checkpoint inhibitors. That realization fuelled a  rush to test combinations of drugs  (see 'Combinatorial explosion'). Radiation and some chemotherapies kill enough tumour cells to release proteins that the immune system might then recognize as foreign and attack. Vaccines containing these proteins, called antigens, could have a similar effect. \u201cOn some level, one can make an argument for almost any drug combining well with an immunotherapy,\u201d says Garon. \u201cAnd obviously we know not all of them will.\u201d \n               Mixing it up \n             One of the first combinations to be tested was made up of two immunotherapies \u2014 ipilimumab and nivolumab \u2014 at once. Although the targets of these drugs both do the same job, silencing T cells, they do so in different ways: CTLA-4 prevents the activation of T cells; PD-1 blocks the cells once they have infiltrated the tumour and its environment. And treating mice with compounds that block both proteins yielded a more-inflamed tumour as well 3 . \u201cThere was reason to think that if you block both, the T cells will be even more ready to kill the tumours,\u201d says Michael Postow, an oncologist at Memorial Sloan Kettering. Together, ipilimumab and nivolumab boost response rates in people with advanced melanoma from 19% with just ipilimumab to 58% with the combination 4 . The combination also produces more-dangerous side effects than using either drug alone, but physicians are learning how to treat immunotherapy reactions, says Postow. Ipilimumab generally doesn't help people with lung cancer when given on its own, but researchers are now testing it with nivolumab. Normally, they would not have bothered to investigate a combination involving a drug that had failed on its own, Garon says. The new approach is grounded in immunology, but some researchers worry that the effort could be wasted, he adds. Researchers are also testing inhibitors of other checkpoint proteins, including TIM-3 and LAG-3, in combination with those that block PD-1. The combination approach is breathing life into drugs that had been shelved. For example, a protein called CD40 stimulates immune responses and has shown promise against cancer in animals. But in the wake of disappointing early clinical trials, some companies put their CD40 drugs to the side. Years later, mouse studies showed that combining CD40 drugs with a checkpoint inhibitor could boost their effect. Now, at least seven companies are developing them. Cancer immunologists have listed the protein as one of the targets they are most interested in studying, says Mac Cheever, a cancer immunologist at the Fred Hutchinson Cancer Research Center in Seattle, Washington. Cancer vaccines \u2014 long pursued by researchers but burdened by repeated failures in clinical trials \u2014 may also see a renaissance. There are now more than two dozen trials of cancer vaccines that make use of a checkpoint inhibitor. Some promising combinations have been uncovered by serendipitous clinical observations. Researchers at Johns Hopkins University in Baltimore, Maryland, were conducting trials of epigenetic drugs, which alter the chemical tags on chromosomes. They shifted a handful of people with lung cancer who had not responded to the drugs to a clinical trial of nivolumab. Five of them responded \u2014 a much higher proportion than expected. The discovery became the seed for an ongoing clinical trial launched in 2013 to study combinations of epigenetic drugs and immunotherapies. Preclinical work has now provided evidence that epigenetic drugs can affect aspects of the immune response. \n               Riding the wave \n             These chance observations could lead to real advances, says Wolchok. \u201cWe're riding the wave of enthusiasm.\u201d But extracting the most from these combinations will require more well-designed preclinical studies to support the human ones. Just as attention to combinations of chemotherapies fuelled advances in treating paediatric leukaemias, the current combinatorial craze will require careful planning to work out the right pairings and timing of therapies. Another class of drug, known as targeted therapies, could also receive a significant boost from immunotherapy. These drugs, which target proteins bearing specific mutations, generate a high response rate when given to patients with those mutations,  but the tumours often develop resistance to the drugs and come roaring back . Coupling targeted therapies with a checkpoint inhibitor, researchers reason, could yield both high response rates and durable remissions. One of the first targeted therapies for melanoma was an inhibitor that is specific to certain mutations in BRAF proteins that can drive tumour growth. However, an early attempt to combine this drug with ipilimumab was aborted when trial participants showed signs of possible liver damage 5 . No one was injured, but for some it was an important reminder that combinations can yield unanticipated side effects. \u201cIt was a good lesson for us to learn,\u201d says Wolchok. \u201cIt will not be as simple as we imagined.\u201d Paying careful attention to sample collection during clinical trials would help researchers to catch toxicity problems early, says Jennifer Wargo, a cancer researcher at MD Anderson. \u201cWe're making mistakes by looking just at clinical endpoints,\u201d she adds. \u201cWe need to be smarter about how we run these trials.\u201d In one of his latest trials, Wolchok wants to combine immunotherapy with a drug that targets a cellular pathway that some cancer cells use to maintain their rapid division. Cancers with mutations in this pathway, which is regulated by the protein MEK, can be extraordinarily difficult to treat. But the pathway is also important for T-cell development, so Wolchok is working to determine the right timing for the treatment. One approach could be to use a MEK inhibitor to quiet tumours in mice and to release tumour antigens. He would then wait for the T-cell response to rejuvenate before adding the immunotherapy. \u201cYou want to make sure you're not trying to activate the immune system at the same time you're turning off that signalling,\u201d he says. Garon is watching such trials with optimism, but he's aware that there may be a limit to how well combinations will perform. He sees a cautionary tale in a drug from an earlier era that works mainly in people with a mutation in the protein EGFR. Researchers spent a decade trying to find drugs that could turn a non-responding patient into a responder. \u201cIt is now clear that there probably is no such agent,\u201d he says. \u201cI'm hopeful we won't be repeating that same response, but we have to watch our data cautiously.\u201d \n               Data frenzy \n             Researchers are so ravenous for those data that the results are being unveiled at major meetings at an earlier stage than in the past, he adds. \u201cPeople are getting up and presenting response rates when the number treated is five,\u201d Garon says. \u201cWe generally have had a higher threshold than that.\u201d He worries that presenting such early data could prompt community physicians in the audience to start making decisions on treatments before they are appropriately studied. The excitement is also fuelling a frenzy of clinical trials that are often based on speed rather than rationale. \u201cRight now I'm kidding myself if I say I'm picking a combination because I have a scientific reason to pick it,\u201d says Mackall. \u201cIt's likely to just be what was available.\u201d The strategy may still produce some wins. \u201cThere is plenty of opportunity for serendipity now,\u201d says Robert Vonderheide, who studies CD40 at the University of Pennsylvania in Philadelphia. But as the field matures, he says, this could give way to a more-systematic approach, similar to the careful planning and testing of variables used for paediatric leukaemias. Despite his concerns, Garon is excited to be a part of the immunotherapy wave. Last autumn, he and his colleagues held a banquet for the patients who had been enrolled in his first immunotherapy trials three years earlier. These were the lucky survivors \u2014 the few who had shown a dramatic response. As he looked around the table at the guests of honour, he marvelled at their recovery. All had been diagnosed with advanced lung cancer, and many had been too weak to work. Now they were talking about their families, re-embarking on careers and taking up old hobbies such as golf and running. \u201cWe've never been able to hold a banquet like that before,\u201d he says. \u201cI would love to hold many more.\u201d\n \n                 Tweet \n                 Facebook \n                 LinkedIn \n                 Weibo \n               \n                     Cancer therapy: an evolved approach 2016-Apr-13 \n                   \n                     Cancer-fighting viruses win approval 2015-Oct-28 \n                   \n                     Immune cells boost cancer survival from months to years 2014-Dec-10 \n                   \n                     Cancer treatment: The killer within 2014-Apr-02 \n                   \n                     Immunotherapy\u2019s cancer remit widens 2013-May-28 \n                   \n                     Sizing up a slow assault on cancer 2013-Apr-03 \n                   \n                     Nature Outlook: Cancer Immunotherapy \n                   \n                     US National Cancer Institute: Cancer immunotherapy \n                   Reprints and Permissions"},
{"file_id": "530398a", "url": "https://www.nature.com/articles/530398a", "year": 2016, "authors": [{"name": "Declan Butler"}], "parsed_as_year": "2006_or_before", "body": "Technological change is accelerating today at an unprecedented speed and could create a world we can barely begin to imagine. In March 2001, futurist Ray Kurzweil published an essay arguing that humans found it hard to comprehend their own future. It was clear from history, he argued, that technological change is exponential\u00a0\u2014 even though most of us are unable to see it\u00a0\u2014 and that in a few decades, the world would be unrecognizably different. \u201cWe won\u2019t experience 100 years of progress in the 21st century\u00a0\u2014 it will be more like 20,000 years of progress (at today\u2019s rate),\u201d he wrote, in \u2018The Law of Accelerating Returns\u2019. Fifteen years on, Kurzweil is a director of engineering at Google and his essay has acquired a cult following among futurists. Some of its predictions are outlandish or over-hyped\u00a0\u2014 but technology experts say that its basic tenets often hold. The evidence, they say, lies in the exponential advances in a suite of enabling technologies ranging from  computing power  to data storage, to the scale and performance of the Internet (see \u2018Onwards and upwards\u2019). These advances are creating tipping points\u00a0\u2014 moments at which technologies such as robotics, artificial intelligence (AI), biology, nanotechnology and 3D printing cross a threshold and trigger sudden and significant change. \u201cWe live in a mind-blowingly different world than our grandparents,\u201d says Fei-Fei Li, head of the Stanford Artificial Intelligence Laboratory in California, and this will be all the more true for our children and grandchildren (see 'Future focus'). Kurzweil and others have argued that people find this pace of change almost impossible to grasp, because it is human nature to perceive rates of progress as linear, not exponential\u00a0\u2014 much as when one zooms in on a small part of a circle and it appears as an almost straight line. People tend to focus on the past few years, but pulling back reveals a much more dramatic change. Many things that society now takes for granted would have seemed like futuristic nonsense just a few decades ago. We can search across billions of pages, images and videos on the web; mobile phones have become ubiquitous; billions of connected smart sensors monitor in real time everything from the state of the planet to our heartbeats, sleep and steps; and drones and satellites the size of shoeboxes roam the skies. \n               Onwards and upwards \n             Exponential advances in enabling technologies have reached the point at which they could trigger disruptive change in sectors from artificial intelligence to robotics to medicine. As a result, many experts argue that tomorrow\u2019s world will be unrecognizable from that of today. \n               ENABLERS \n             1. Computing power  The exponential growth in supercomputing performance is one indicator of dizzying advances across computing. Supercomputers in 2020 are likely to be 30 times more powerful than those of today. 2. Really big data  The amount of data worldwide is predicted to reach a whopping 44 zettabytes (10 21  bytes) by 2020 \u2014 nearly as many digital bits as there are stars in the Universe. This gives more raw material for artificial-intelligence machines to learn from. 3. Communication speed  Meanwhile, the performance and scale of the Internet improves. Broadband and WiFi speeds are increasing, and Internet data traffic will exceed a zettabyte this year and double by 2019. \n               DRIVERS \n             4. Talking devices  By 2020, the number of connected sensors and devices in buildings, cities and farms \u2014 the \u2018Internet of Things\u2019 \u2014 will be twice that of the human population. 5. Biology booms  Conceptual and technological advances are driving progress in biology. DNA sequencing costs have fallen at an exponential rate and the number of sequences has soared since 1985. Similar advances are happening in neuroscience and biological nanotechnology. 6. Like it, print it  3D printing is becoming cheaper and quicker \u2014 one factor that could disrupt manufacturing and allow once-pricey robotics to be mass produced. 7. Rise of robots  Purchases of robots are set to rocket as their capabilities increase and costs fall, a trend driven by massive investments in artificial intelligence and robotics by the military and by computing giants such as Google. All these factors are now converging to push seemingly futuristic technologies out of the lab, and set them on the same path taken by personal computing and consumer electronics. If the pace of change is exponentially speeding up, all those advances could begin to look trivial within a few years. Take \u2018deep learning\u2019, a form of artificial intelligence that uses powerful microprocessor chips and algorithms to simulate neural networks that  train and learn through experience , using massive data sets. Last month, the Google-owned AI company DeepMind used deep learning to enable a computer to  beat for the first time a human professional  at the game of Go, long considered one of the grand challenges of AI. Researchers told  Nature  that they foresee a future just 20 years from now \u2014 or even sooner \u2014 in which robots with AI are as common as cars or phones and are integrated into families, offices and factories. The \u201cdisruptive exponentials\u201d of technological change will create \u201ca world where everybody can have a robot and robots are pervasively integrated in the fabric of life\u201d, says Daniela Rus, head of the Computer Science and Artificial Intelligence Laboratory at the Massachusetts Institute of Technology in Cambridge. After decades in development, applications of AI are moving into the real world, says Li, with the arrival of  self-driving cars , virtual reality and more. Progress in AI and robotics is likely to accelerate rapidly as deep-pocketed companies such as Google, Apple, Facebook and Microsoft pour billions of dollars into these fields. Gill Pratt, former head of the US Defense Advanced Research Projects Agency\u2019s Robotics Challenge, asked last year whether robotics is about to undergo a \u2018Cambrian explosion\u2019\u00a0\u2014 a period of rapid machine diversification ( G.\u00a0A.\u00a0Pratt  J.\u00a0Econ. Perspect.   29,  51\u201360; 2015 ). Although a single robot cannot yet match the learning ability of a toddler, Pratt pointed out that robots have one huge advantage: humans can communicate with each other at only 10 bits per second\u00a0\u2014 whereas robots can communicate through the Internet at speeds 100\u00a0million times faster. This could, he said, result in multitides of robots building on each other\u2019s learning experiences at lightning speed. Pratt was hired last September to head the Toyota Research Institute, a new US$1-billion AI and robotics research venture headquartered in Palo Alto, California. Many researchers say that it is important to prepare for this new world. \u201cWe need to become much more responsible in terms of designing and operating these robots as they become more powerful,\u201d says Li. In January 2015, a group including Elon Musk, Bill Gates and Stephen Hawking penned  an open letter  calling for extensive research to maximize the benefits of AI and  avoid its potential pitfalls . The letter has now been signed by more than 8,000 people. Yet predicting the future can be a fool\u2019s game\u00a0\u2014 and not everyone is convinced that technological change will hit humanity quite so fast. Ken Goldberg, an engineer at the University of California, Berkeley, questions the idea that technologies advance exponentially across the board, or that those that do will continue indefinitely. \u201cThe danger of overly optimistic exuberance is that it could set unrealistic expectations and trigger the next AI winter,\u201d he says, alluding to periods in AI\u2019s history where hype gave way to disappointment followed by steep cuts in funding. Goldberg says that recent warnings that AI and robots risk surpassing human intelligence are \u201cgreatly exaggerated\u201d. And Stuart Russell, a computer scientist at the University of California, Berkeley, questions the notion that exponential advances in technology necessarily lead to transformative leaps. \u201cIf we had computers a trillion times faster we wouldn\u2019t have human-level AI; half in jest, one might say we\u2019d just get wrong answers a trillion times sooner,\u201d he says. \u201cWhat matters are real conceptual and algorithmic breakthroughs, which are very hard to predict.\u201d Russell did sign the Hawking letter\u00a0\u2014 and says it is important not to ignore the ways that technologies could be taken in potentially harmful directions with profound results. \u201cWe made this mistake with fossil-fuel technologies 100\u00a0years ago\u00a0\u2014 now it\u2019s probably too late.\u201d \n               Future focus \n             Expert predictions \u201cA possible \u2018Cambrian explosion\u2019 in robotics with a rapid period of incredible machine diversification. Robots communicating with each other at speeds that are 100 million times faster than humans might allow swarms of robots to build on each other\u2019s learning experiences at lightning speed.\u201d  Gill Pratt, Head of the Toyota Research Institute, Palo Alto, California \u201cA full brain-activity map and connectome by 2020 and by 2040 it will be routine to read and write data to billions of neurons. By 2040,1 billion people will have their whole genome sequenced and get constant updates of their immunomes and microbiomes.\u201d  George Church, Geneticist at Harvard Medical School, Cambridge, Massachusetts \u201cThe promise for the future is a world where robots are as common as cars and phones, a world where everybody can have a robot and robots are pervasively integrated in the fabric of life.\u201d  Daniela Rus, Head of the Computer Science and Artificial Intelligence Laboratory at the Massachusetts Institute of Technology, Cambridge \u201cIn the next couple of generations, we will seethe first phase of true personal, assistive robots in the home and other human environments. There will be a huge opportunity to better the quality of life, for example by freeing up people from work.\u201d  Fei-Fei Li, Head of the Stanford Artificial Intelligence Laboratory, California \u201cTomorrow\u2019s scientists will have armies of virtual graduate students, doing lab work, statistical analysis, literature search and even paper-writing for them.\u201d  Pedro Domingos, Machine-learning researcher, University of Washington, Seattle \n                 Tweet \n                 Facebook \n                 LinkedIn \n                 Weibo \n               \n                     Economics: Current climate models are grossly misleading 2016-Feb-24 \n                   \n                     Sustainability: Game human nature 2016-Feb-24 \n                   \n                     Future generations 2016-Feb-24 \n                   \n                     Should you edit your children\u2019s genes? 2016-Feb-23 \n                   \n                     Can today\u2019s decisions really be future-proofed? 2016-Feb-22 \n                   \n                     Meet the soft, cuddly robots of the future 2016-Feb-03 \n                   \n                     Google AI algorithm masters ancient game of Go 2016-Jan-27 \n                   \n                     Robotics: Countering singularity sensationalism 2015-Oct-14 \n                   \n                     Exclusive: Genomics pioneer Jun Wang on his new AI venture 2015-Jul-28 \n                   \n                     Machine ethics: The robot\u2019s dilemma 2015-Jul-01 \n                   \n                     The Pentagon\u2019s gamble on brain implants, bionic limbs and combat exoskeletons 2015-Jun-10 \n                   \n                     Computer science: The learning machines 2014-Jan-08 \n                   \n                     Nature Insight : Machine intelligence \n                   \n                     Future of Life Institute \n                   \n                     Association for the Advancement of Artificial Intelligence \n                   \n                     OpenAI \n                   \n                     DARPA Robotics Challenge \n                   Reprints and Permissions"},
{"file_id": "530397a", "url": "https://www.nature.com/articles/530397a", "year": 2016, "authors": [], "parsed_as_year": "2006_or_before", "body": "A special issue examines whether researchers today consider the world of tomorrow \u2014 and why they should. The effects on distant tomorrows of the decisions we make today have never been greater. As we change our planet, ourselves and, potentially, our descendants, in ever more dramatic ways, this issue of  Nature  takes stock: do we have the brains and the tools to understand and account for the future and, if not, what should be done? Technology experts foresee a world just a few decades away that is so radically different from today that it is hard to comprehend. The exponential rate of progress in a suite of enabling technologies, ranging from computer-processing power to communication, could drive drastic changes in artificial intelligence, robotics, molecular biology and more (see  page 398 ). Some think that the people who inhabit this world might also be irrevocably altered, for the first time, by genetic engineering. The arrival of the powerful genome-editing technology CRISPR\u2013Cas9 might prevent children from being born with some deadly disorders or disabilities, and a feature on  page 402  discusses the extent to which this is possible and desirable. Forecasting is hard and fraught with bias. For example, as Nicholas Stern warns on  page 407 , current models of climate economics implicitly assume that lives in the future are less important than those today\u00a0\u2014 a value judgement that is rarely scrutinized and difficult to defend. And, as Celine Kermisch writes on  page 383 , near and remote future generations have very different needs. Hundreds of social-science studies highlight the tensions between our tendencies to care about the well-being of others yet to favour current benefits over future ones. Therefore, on  page 413 , behavioural economists Helga Fehr-Duda and Ernst Fehr call for the design of sustainable-development policies and schemes that exploit these evolved behaviours. Finally, John Bongaarts expresses the view on  page 409  that the best thing we could do now for future generations is to ensure that there are fewer of them, by doubling the aid spent on family planning. The only certainties are that tomorrow\u2019s world is difficult to predict, is heading straight for us, and that billions more people will inhabit it. How we account for future impacts in today\u2019s decisions should preoccupy researchers and policymakers more than it does now. \n                 Tweet \n                 Follow @NatureNews \n               \n                     Can today\u2019s decisions really be future-proofed? 2016-Feb-22 \n                   \n                     A radical reorientation 2004-Jul-21 \n                   \n                     Nature  special: Future generations \n                   Reprints and Permissions"},
{"file_id": "532428a", "url": "https://www.nature.com/articles/532428a", "year": 2016, "authors": [{"name": "Jane Qiu"}], "parsed_as_year": "2006_or_before", "body": "A year after a devastating earthquake triggered killer avalanches and rock falls in Nepal, scientists are wiring up mountainsides to forecast hazards. Kodari is a ghost town on an empty Nepalese highway that cuts through some of the steepest slopes of the Himalayas. One year after the magnitude-7.8 Gorkha earthquake killed nearly 9,000\u00a0people, the once-buzzing trade centre looks like a battlefield where armies of giants once waged war. The road is littered with rusting cars and trucks smashed into bizarre shapes. Massive boulders rest on the wreckage of homes. \u201cIt\u2019s a good example of building a town in the wrong place,\u201d says Kristen Cook, a geologist at the German Research Centre for Geosciences (GFZ) in Potsdam, as she climbs over the rubble from one of the landslides that crushed the town. The Arniko Highway, which runs through Kodari, is no stranger to such calamities, especially in the monsoon season. \u201cIt was in frequent repair and closure even before the earthquake,\u201d says Shanmukesh Amatya, landslide-division chief at Nepal\u2019s Department of Water Induced Disaster Prevention in Kathmandu. \u201cThe problem now is overwhelming.\u201d The highway is not the only thing that keeps Amatya awake at night. The earthquake unleashed more than 10,000 landslides that blocked rivers and damaged houses, roads and other key pieces of infrastructure across the country. And the destruction didn\u2019t stop with the shaking (see \u2018Deadly impact\u2019). The hilly terrain, severely weakened by the quake,  is now more likely to slip  after strong rains and aftershocks\u00a0\u2014\u00a0a legacy that is likely to endure for years. During the most recent monsoon, the area affected by landslides was about ten times greater than usual. \u201cIt\u2019s a real problem for reconstruction,\u201d says Tara Nidhi Bhattarai, a geologist at Tribhuvan University in Kathmandu and chief scientist of Nepal\u2019s National Reconstruction Authority\u00a0\u2014\u00a0an agency established last year to manage the recovery efforts. \u201cWhat are the safe places to rebuild, in a landscape that is evolving?\u201d To answer that, geoscientists are wiring up the mountains in Nepal and other seismically active countries. By monitoring how hillsides evolve, researchers are learning why strong shaking weakens a slope and makes it more prone to give way during aftershocks or rainstorms. The lessons from such studies could help to pinpoint when and where the side of a mountain will collapse. The significance goes beyond quake recovery.  Himalayan nations are facing increasing risks from landslides  because of deforestation, road construction, population growth and other changes that have pushed people to live in hazardous locations. Climate change may exacerbate the problem by melting glaciers and triggering increasingly extreme rainfall. \u201cThere is a pressing need to monitor the risks in the long run,\u201d says Amatya. \u201cA nationwide early-warning system is long overdue.\u201d \n               Bird\u2019s-eye view \n             A crowd eagerly looks on as Cook flies a drone through the skies near Listi, a small village perched on a mountainside above the Arniko Highway. With its four propellers, the little robot zips over landslide scars that run down from the ridge like gigantic frozen waterfalls. A camera and other sensors on the drone provide data that let Cook build a 3D reconstruction of the landscape. She started the work last October and will take measurements every few months over the next few years. By scanning as many landslide-inflicted areas as possible, she says, \u201cwe will be able trace how they change over time and what\u2019s the effect of monsoons\u201d. Such measurements of the surface will complement studies that track what\u2019s happening underground. Not far from Cook is her colleague Christoff Andermann, another GFZ geologist, who is performing maintenance on a broadband seismometer, a device that measures shaking across a wide range of frequencies. Last June, the GFZ team installed a dozen such instruments, along with weather stations and river-flow sensors, across 50\u00a0square kilometres of landslide-riddled terrain. Seismometers are a relatively new addition to landslide studies by the GFZ researchers and their colleagues. They started using the sensors only after an accidental discovery. In 2003, a set of seismic stations installed in Nepal to study deep structures in Earth\u2019s crust picked up high-frequency noise from nearby rivers and shifting slopes. Arnaud Burtin, a seismologist now at the Earth Physics Institute in Paris, noticed a series of peaks in that noise before a debris flow in central Nepal that killed 45 people. He and his colleagues went on to identify 1  46 debris flows from seismograms taken during that monsoon season. By comparing the data with information from weather stations, the team also determined how much rainfall was required to trigger slides. Researchers have typically used satellite imagery or aerial photography to track landscape changes on a large scale, but these methods have relatively poor temporal resolution because images are taken days or months apart. Seismometers take snapshots hundreds of times per second, so they are ideal for monitoring slopes for instability, says Colin Stark, a geologist at the Lamont\u2013Doherty Earth Observatory in Palisades, New York, who studies monster landslides using global seismic networks. When seismometers are placed strategically, he says, it\u2019s also possible to precisely locate the source of seismic signals in a large area. \u201cUntil recently, we had little idea why landslides are more likely to happen after an earthquake or how the slopes recover over time,\u201d says Stark. But work over the past decade has revealed that cracks produced by an earthquake can boost the shaking in future shocks. Unpublished results from seismic stations, for example, show that on fractured slopes, ground motion can be up to 30 times what is measured in neighbouring, undamaged areas, says Jeffrey Moore, a geophysicist at the University of Utah in Salt Lake City. This means that minor aftershocks could trigger unexpected levels of landslides in damaged slopes that did not fail in the main shock, he says. In some cases, the increased sensitivity can last for decades. A study 2  of a magnitude-7.4 earthquake in New Zealand in 1968 found that the quake triggered more landslides than expected in places that had been affected by a magnitude-7.8 shock 21\u00a0kilometres away and nearly 4 decades before. Quake-stricken hills also have an increased sensitivity to rainfall, says Niels Hovius, a GFZ geologist who is leading the Nepal study. He and his colleagues have found 3  that after the magnitude-7.6 ChiChi earthquake that hit Taiwan in 1999, the rate of rainfall-triggered landslides in the affected area jumped by a factor of 22. \u201cThe government cleared up the mess and rebuilt, but the same happened again a couple of years later,\u201d he says. If scientists can develop greater insight into the mechanisms that control slope behaviour after an earthquake, that could help authorities to make better decisions about rebuilding. By analysing records after the ChiChi quake and three others with similar depths and slip mechanisms, Hovius and his colleagues also found 3  that it took up to four years for landslide rates to return to pre-quake levels at those sites. In follow-up work, the team mined data from seismometers installed before ChiChi hit. The instruments were near roads, which made it possible to study subsurface properties by measuring how traffic vibrations travel through the ground. They found that the speed of seismic waves dropped markedly immediately after the quake. The velocities then recovered gradually, following roughly the same trajectory as the decline in landslide rates, says Odin Marc, a geologist at the GFZ, who presented the results last week at a meeting of the European Geosciences Union in Vienna. Over the same period, there were frequent, small surface displacements\u00a0\u2014\u00a0presumably caused by the slow, creeping movement of Earth\u2019s crust after an earthquake, a process known as post-seismic deformation. The researchers suspect that subsurface materials are packed together tightly before the earthquake, like beads in a box. Strong ground-shaking causes the granular mass to expand, opening up holes and cracks that make the ground less dense. \u201cThis is why seismic waves travel at reduced speeds,\u201d says Hovius. Post-seismic deformation causes the openings to fill in and the subsurface sediments to become compact once more. \u201cIt\u2019s an internal healing process of the landscape,\u201d he says. Data collected after the Gorkha earthquake support that. Preliminary results show that seismic-wave velocities close to the surface declined sharply after the shock\u00a0\u2014\u00a0and the volume of water flowing through rivers increased by 50%. That backs up the idea that the quake opened holes and fractures in the subsurface, which then allowed groundwater to leak more freely through the cracks, says Andermann, who has been monitoring river flows and sediment transport in the region for the past decade. Such findings suggest a way to predict landslides. Looking back over their data, the researchers were able to identify peaks of seismic signals in the run-up to a major landslide last July. \u201cThese precursors represent a sequence of processes that culminated in the failure,\u201d says Hovius. \u201cThere was a systematic increase in the rate at which these precursor activities occurred, until the whole topography collapsed.\u201d The GFZ team also found that seismic waves travel through the subsurface more quickly when the slope is drenched and pore spaces are filled with water. \u201cWe can see how quickly the effects of rainfall propagate into and through the subsurface\u201d using seismic sensors, says Hovius.This effectively maps groundwater flow, a key factor in the strength of hillsides. With the seismic data, researchers can model the physics of slope stability and monitor changes in ground properties that might precipitate a landslide. \n               Near-atomic blast \n             In the village of Langtang in northern Nepal, a pile of rubble 60 metres deep provides ample incentive to improve landslide forecasts. During the earthquake last year, a mixture of ice and rock crashed down several kilo-metres onto the valley floor \u2014 landing with an impact that released half as much energy as the Hiroshima atomic bomb 4 .  The slide buried Langtang  and nearby villages, leaving nearly 400 people dead or missing. Research groups have been racing to understand where the avalanche began and whether the area is still at risk. One study 5  found 5\u00a0initiation sites between altitudes of 6,800 and 7,200\u00a0metres, along a 3-kilometre ridge where the earthquake shook up snow and glaciers. These swept down the slope, picking up rocks as they went. Roughly 7\u00a0million cubic metres of debris filled the bottom of the valley, and another 10\u00a0million cubic metres still rest precariously on slopes more than 5,000 metres above sea level. A year after the quake, the sounds of falling rocks and shifting slopes frequently echo through the valley\u00a0\u2014\u00a0a reminder of the remaining hazard.  The Langtang case shares features with increasingly common rock avalanches in high mountains in Alaska and the Alps, says Marten Geertsema, a glaciologist with the British Columbia Ministry of Forests and Range in Prince George, Canada. In all these places, glaciers are quickly retreating, leaving rocky hillsides exposed and prone to failure. And warming at high elevations may cause frozen bedrock to thaw, he says, making it more permeable to melt water and weakening the rocks. \u201cClimate change might have primed the landscape for the devastation.\u201d At high-risk sites in Nepal, researchers are combining seismological and other techniques to watch for signs that mountainsides are growing restless. On the steep slope facing Listi, the earthquake caused the lower part of the ridge to subside, resulting in a 5-metre opening that skirts the mountain for about 2\u00a0kilometres. This gigantic crack and many smaller ones nearby pose a serious threat to downslope settlements, says Amod Dixit, executive director of Nepal\u2019s National Society for Earthquake Technology (NEST) in Kathmandu. \u201cThey must be closely monitored.\u201d Last August, Nick Rosser, a geologist at Durham University, UK, and his colleagues installed a series of instruments at ten locations across the slope \u2014 including strain meters to monitor changes in the cracks, accelerometers to measure ground vibration, and rain gauges. The data are relayed to a server at NEST, letting researchers track in real time whether the cracks are opening or contracting and how they respond to rainfall. Although it is not yet a fully fledged early-warning system, the set-up can identify signs of major deformation that could cause the slope to fail. Thankfully, says Rosser, \u201cthe cracks are not growing at the moment\u201d. Settlements will be alerted to any impending danger, he adds. The researchers are using information from the field and from lab experiments on slope materials to try to determine what kind of ground deformation and rainfall would cause landslides. \u201cThis is crucial for setting the criteria for triggering an alert,\u201d he says. The Durham sensors are within the area covered by the GFZ seismic array, so the teams will pool their field data. Together with satellite imagery and other measurements, this information will provide unprecedented insight into how the mountains are changing and what kind of danger this might pose to communities there, they say. At Listi, Cook is worried about a massive pile of debris that the drone has located high above the valley. The earthquake loosened a huge amount of rock and soil, but most did not make it all the way to the bottom. \u201cThey are just sitting there on the hillside,\u201d says Cook, pointing to a mass on her remote-control screen. The materials could all come down in heavy rain \u2014 as some did during the last monsoon. \u201cThey are time bombs waiting to explode.\u201d \n                 Tweet \n                 Facebook \n                 LinkedIn \n                 Weibo \n                 Wechat \n               \n                     Nepal earthquake caused fewer landslides than feared 2015-Dec-16 \n                   \n                     The 24/7 search for killer quakes 2015-Jul-08 \n                   \n                     Mappers rush to pinpoint landslide risk in Nepal 2015-May-11 \n                   \n                     How scientists are aiding quake recovery in Nepal 2015-May-01 \n                   \n                     Landslide risks rise up agenda 2014-Jul-15 \n                   \n                     Afghan landslide was 'an accident waiting to happen' 2014-May-06 \n                   \n                     Death toll from landslides vastly underestimated 2012-Aug-08 \n                   \n                     German Research Centre for Geosciences \n                   \n                     Durham University Nepal project \n                   \n                     American Geophysical Union Landslide blog \n                   \n                     National Society for Earthquake Technology \n                   \n                     Earthquakes Without Frontiers \n                   \n                     Langtang Memory Project \n                   Reprints and Permissions"},
{"file_id": "531022a", "url": "https://www.nature.com/articles/531022a", "year": 2016, "authors": [{"name": "Mason Inman"}], "parsed_as_year": "2006_or_before", "body": "Several countries hope to unleash vast natural-gas reserves through fracking, but drilling attempts have been disappointing. Large petroleum pumps nodded up and down in the background as British Prime Minister David Cameron donned a blue industrial jumpsuit to promote a controversial drilling technique known as hydraulic fracturing, or fracking. In his 2014 visit to a potential drill site in eastern England, Cameron laid out the benefits of tapping Britain's shale formations to release valuable natural gas. \u201cWe're going all out for shale,\u201d he said. \u201cIt will mean more jobs and opportunities for people, and economic security for our country.\u201d Cameron hopes to replicate  the surge in natural-gas production  that has happened in the United States thanks to fracking \u2014 which involves injecting fluids into shale to liberate locked-up hydrocarbon deposits. The fracking revolution helped to revitalize the US economy, and Cameron's Conservative Party seeks to spark a similar gas boom in the United Kingdom. In August last year, his newly elected government offered drilling licences for shale deposits and it touted estimates that \u201cinvestment in shale could reach \u00a333 billion [US$46 billion] and support 64,000 jobs\u201d. Over the past few years, fracking fever has swept through several European nations, including Denmark, Lithuania, Romania and especially Poland, which has seen more shale exploration than any other nation on the continent. Fracking might help to boost gas production in Europe at a time when it is facing a sharp decline. Older gas fields in the North Sea are running out, as are deposits in Germany, Italy and Romania. The disappointing output has increased Europe's dependence on imported gas, mainly from Russia. European leaders have grown wary of relying on that source, especially after diplomatic relations chilled when Russia invaded Ukraine in 2014. But Europe's appetite for gas could increase  as it tries to cut greenhouse-gas emissions  \u2014 which will probably require reducing coal consumption (see 'Looming gas crunch?'). The European Commission says that \u201cgas will be critical for the transformation of the energy system\u201d. This means that countries such as the United Kingdom have invested an immense amount of hope in shale gas. But a close examination of the industry suggests that any fracking boom in Europe is a long way off \u2014 and some experts say that it may never arrive. Despite several years of exploratory drilling, there are currently no commercial shale-gas wells in Europe. Tests of the region's shale potential have been limited, and the results so far have been generally disappointing, say geologists and energy experts. It remains highly uncertain how much gas would be recoverable with today's technologies, and even more difficult to forecast how much would be profitable to extract. All that leads to big questions about Europe's shale hopes, says Jonathan Stern, a natural-gas expert at the Oxford Institute for Energy Studies in Oxford, UK. \u201cThere has been an enormous amount of ridiculous hype about shale gas in Europe.\u201d \n               Waiting for a revolution \n             A decade ago, the United States was facing a similarly dismal outlook for natural gas. Production from conventional fields was petering out, and geologists did not expect that alternative sources of gas could compensate for the shortfall. But within a few years, the picture suddenly brightened owing to improved drilling and fracking technologies, which tapped previously inaccessible gas reserves and unleashed a boom dubbed the shale revolution. Shale is almost impermeable to oil and gas, so companies must fracture the rock to liberate those hydrocarbons. The idea that a similar wealth of untapped energy could be lurking in the rocks below Europe is economically appealing. But geologists know relatively little about the potential of shale-rock formations in Europe because there has been less onshore drilling than in the United States. European companies have sometimes drilled through shale to reach other rock formations, but they have rarely taken detailed measurements or collected samples of the shale layers. So far, Poland's shale formations have attracted the most attention within the region. The nation depends heavily on coal, and what natural gas it does use comes almost exclusively from Russia. In the mid-2000s, the burgeoning US shale boom prompted Poland's government to offer shale exploration licences that went to local companies as well as major international energy firms, including the US companies ExxonMobil and Chevron, and the French firm Total. Poland's foreign minister, Rados\u0142aw Sikorski, said in 2010 that Poland would become \u201ca second Norway\u201d \u2014 referring to Europe's second-largest natural-gas producer, after Russia. The excitement was bolstered in 2011 by an assessment from Advanced Resources International (ARI), a consultancy in Washington DC that was commissioned by the US Department of Energy to study shale-gas resources worldwide. That study estimated the quantity of shale rock and other parameters such as the total organic content of the rock, which is the source of oil and gas. ARI also estimated parameters to represent the risk that some shale zones, or plays, might not prove promising or that only a portion of them might be amenable to drilling. Given these assumptions, ARI calculated that Poland's shale-gas plays hold about 5,295 billion cubic metres (bcm) of technically recoverable gas, the most shale gas of any nation in Europe. If all of that gas could be extracted, it would be equivalent to 325 years of Poland's current gas consumption 1 . While companies began drilling dozens of test wells in Poland, the Polish Geological Institute (PGI) in Warsaw made its own estimate in March 2012. Taking the considerable uncertainty over the data into account, the PGI calculated that Poland has 346\u2013768 bcm of recoverable shale gas onshore \u2014 about one-tenth of ARI's figure 2 . Then in July 2012, the US Geological Survey (USGS) released another study of Poland's shale-gas resources. The agency assumed that individual wells would yield about half as much gas as the PGI assumed and that the area that is likely to contain recoverable gas is only about one-third of the size. So the USGS wound up with an estimate even smaller than the other two, with a mean result of just 38 bcm of recoverable gas, and a huge range of uncertainty, from 0 to 116 bcm. The mean was about one-tenth that of the PGI's estimate, and about one-hundredth of ARI's 3 . \u201cOne report \u2014 huge potential. A year later \u2014 nothing,\u201d says PGI geologist Hubert Kiersnowski. \u201cThe scale of uncertainty is so big.\u201d Meanwhile, results started coming in from test wells. Of the 72 wells drilled by the end of 2015, 25 were successfully fracked to release gas. However, these wells yielded only about one-third to one-tenth of the flow that would be required to turn a profit, says petroleum geologist Pawe\u0142 Poprawa of AGH University of Science and Technology in Krakow, Poland, and formerly of the PGI. None of the wells has become a commercial producer. At the peak of interest in early 2013, companies held shale-drilling licences covering about one-third of Poland. But throughout 2013 and 2014, the major international energy firms gave up their shale-exploration licences and left the country, often citing disappointing results. The last to leave was Texas-based ConocoPhillips in June 2015 \u2014 now Poland's shale drilling is almost at a standstill. One major hurdle to development is that Poland's shale is expensive to drill because it is buried around 3\u20135 kilometres down, compared with around 1\u20132 kilometres for most successful US plays. Some of Poland's shale also has a high clay content, which makes the rock harder to fracture. And exploratory holes into one of Poland's most promising shale formations \u2014 in the north, near the Baltic Sea \u2014 showed that it held a geological barrier that would limit how much gas could be tapped by individual wells, says Poprawa. The drilling results suggest that ARI \u201coverestimated the acreage, the thickness, and the quality of the shale\u201d, he says. The PGI says that its previous lower estimates are reinforced by its latest, as-yet-unpublished assessment, which draws on recent shale-drilling tests. PGI spokesperson Andrzej Rudnicki calls ARI's much higher estimates \u201centhusiastic, but geologically unrealistic\u201d. \u201cThe results in Poland to date indeed have been disappointing,\u201d concedes geologist Scott Stevens of ARI. He says that the main reason for the unproductive wells was \u201cextremely high\u201d stresses in the rock, which makes fracking less effective. \u201cThere was no way that the exploration companies could know that in advance,\u201d he notes. Nonetheless, he argues, \u201cIt is too soon to dismiss Poland's extensive shale potential.\u201d Given the limited available data, he does not see a reason to revise ARI's estimate. Even the PGI's lower estimates suggest that there is a still a substantial amount of gas trapped in Poland's shale. However, it is uncertain whether any of that gas will be profitable to extract. \u201cI am still hopeful,\u201d Poprawa says. \u201cBut the initial hopes were not realistic.\u201d \n               Dash for gas \n             Although companies raced to grab concessions in Poland, activity in the United Kingdom has been subdued. In 2011, Cuadrilla Resources fracked the United Kingdom's first shale well near Blackpool in northern England, but this triggered two small earthquakes, which led the government to place a year-long moratorium on further fracking. After the moratorium lifted, companies slowly began vying to tap UK shale. According to a 2013 assessment by ARI, UK shale holds 17,600 bcm of gas. Only 728 bcm of this is judged to be technically recoverable: if that could be profitably extracted, it would satisfy the United Kingdom's gas needs for about a decade 4 . The British Geological Survey (BGS) has assessed the shale-gas resources in the United Kingdom's three major plays by constructing a 3D model of the subsurface using drilling records and seismic surveys, which has allowed it to roughly estimate the volume of shale rock. But geologist Ian Andrews of the BGS insists that this estimate is just a first pass based on the seismic information available, \u201cwhich is sparse, and fairly poor\u201d. By testing old rock cores stored by the government, the BGS was also able to measure some of the properties of UK shale, such as the total organic carbon (TOC) content. Successful shale plays in the United States typically have TOC values greater than 2%. Although TOC measurements for the United Kingdom are scant, the available data suggest that there are large volumes of rock above the 2% threshold. But data are lacking for other key parameters, such as the rock porosity, which adds greatly to the uncertainty of these projections. The BGS estimated that the three shale plays it has assessed so far hold around 39,900 bcm of gas, with an uncertainty range of 24,700\u201368,400 bcm (refs  5 , 6 ). This is more than the ARI estimate, but that study only considered the most promising rock. The BGS did not attempt to estimate how much of that gas would be technically recoverable. \u201cHow much we can get out of the ground, I don't think anybody knows yet, because the drilling hasn't happened to test it,\u201d says Andrews. Although the BGS's studies used US shale plays as analogues for crucial parameters, the two nations have different geological histories. The United States has large deposits of shale that are not too thick and have been folded little over time. The shale in the United Kingdom is more complicated, says petroleum geoscientist Andrew Aplin of the University of Durham, UK. \u201cIt's been screwed around with more\u201d, creating more folds and faults. That greater complexity could pose challenges. One risk is that pumping fluid into rock can trigger earthquakes if the wells are near faults or large natural fractures. \u201cIt's better to stay away from them, especially when they're located near densely populated areas,\u201d says natural-gas expert Rene Peters of the Netherlands Organisation for Applied Scientific Research (TNO) in the Hague. But there has been relatively little high-resolution seismic imaging in Europe, he says, so \u201cnot all these fractures are known\u201d. Small faults can pose another challenge. If the fracking fluid leaks into a fault, the pressure on the rock is reduced and the fracking is less effective. Given the geological hurdles and the United Kingdom's dense population, it may prove difficult to find many promising, acceptable places to drill. \n               Far from profitability \n             The United Kingdom's appetite for gas is expected to grow sharply. In November, the government set out the goal of phasing out coal-fired power plants by 2025, unless they have carbon capture and storage systems. The government expects nuclear, wind and solar power to play a part in filling the void left by coal \u2014 but natural gas would be the linchpin because it produces less carbon dioxide and other pollution than does coal, and existing infrastructure can be used to produce electricity from gas. \u201cWe'll only proceed if we're confident that the shift to new gas can be achieved within these timescales,\u201d UK energy secretary Amber Rudd said in a speech announcing the policy shift. \u201cWe currently import around half of our gas needs, but by 2030 that could be as high as 75%. That's why we're encouraging investment in our shale-gas exploration so we can add new sources of home-grown supply.\u201d Other European nations are also  counting on natural gas  to help them to cut their coal use and meet their commitments under the United Nations climate treaty signed in Paris in December. But shale gas may not provide the answer. At the June 2015 World Gas Conference in Paris, industry speakers were pessimistic that Europe would see a fracking boom like that in the United States. Philippe Charlez, manager of unconventional resources development at Total, said that given the current costs for shale wells, \u201cwe are very, very far in Europe from profitability\u201d. Many assessments in the past two years \u2014 including those by the International Energy Agency and oil giants BP and ExxonMobil \u2014 agree that Europe is unlikely to produce much shale gas, and that conventional gas production will continue to decline 7 , 8 , 9 . And if gas imports cannot make up the difference, says Stern, \u201cEurope is going to have even more difficulty reducing carbon emissions\u201d. The most recent signs are not good for shale across the continent. Besides retreating from Poland, major petroleum companies have pulled out of nascent shale drilling efforts in Romania, Lithuania and Denmark, usually citing disappointing yields. Various members of the European Union from Bulgaria to France have instituted moratoria or bans on fracking, as have Scotland, Wales and Northern Ireland, all citing environmental concerns. England is home to some of the few remaining attempts to tap shale gas in Europe. A handful of companies have applied for permission to drill, which could finally reveal whether the United Kingdom's shale deposits will be a jackpot or a dud. But environmentalists have put up a strong fight, and permissions have been slow to emerge. Cuadrilla requested approval in January 2015 to drill beneath the undulating fields of Lancashire, but the county council rejected the request in June over concerns about traffic, noise and the visual impact of drilling. That decision and the broader difficulties that confront fracking in Europe leave the future of natural gas there in limbo. To figure out whether any play has potential, companies must drill as many as 50 to 100 wells. But the public opposition and the poor drilling results so far mean that companies are not eager to sink that kind of effort into fracking in Europe right now, says Stern. \u201cI can't see any country, including the UK, where that will happen anytime soon.\u201d \n                 Tweet \n                 Facebook \n                 LinkedIn \n                 Weibo \n               \n                     Natural gas stands to get a boost from carbon tax 2015-Jun-05 \n                   \n                     Natural gas: The fracking fallacy 2014-Dec-03 \n                   \n                     International Energy Agency \n                   \n                     European Commission 2050 Energy Strategy \n                   Reprints and Permissions"},
{"file_id": "531026a", "url": "https://www.nature.com/articles/531026a", "year": 2016, "authors": [{"name": "XiaoZhi Lim"}], "parsed_as_year": "2006_or_before", "body": "Virus-sized particles that fluoresce in every colour could revolutionize applications from television displays to cancer treatment. At Biopolis, a sprawling research complex in Singapore, Chi Ching Goh leans over an anaesthetized mouse lying on the table in front of her, and carefully injects it with a bright yellow solution. She then gently positions the mouse's ear underneath a microscope, and flips a switch to bathe the ear in ultraviolet light. Seen through the microscope's eyepiece, the illumination makes the blood underneath the skin glow green, tracing the delicate vessels that carry the solution through the creature's body. Ultimately, Goh, a PhD candidate at the National University of Singapore, hopes that the method will help her to find blood vessels that are leaking owing to inflammation, perhaps helping to detect malaria or predict strokes. Crucial to her technique are the virus-sized particles that give the solution its colour. Just a few tens of nanometres across, they are among a growing array of 'nanolights' that researchers are tailoring to specific types of fluorescence: the ability to absorb light at one wavelength and re-emit it at another. Many naturally occurring compounds can do this, from jellyfish proteins to some rare-earth compounds. But nanolights tend to be much more stable, versatile and easier to prepare \u2014 which makes them attractive for users in both industry and academia. The best-established examples are quantum dots: tiny flecks of semiconductor that are prized for their beautiful, crisp colours. Now, however, other types of nanolight are on the rise. Some have a rare ability to absorb lots of low-energy photons and combine the energy into a handful of high-energy photons \u2014 a trick that opens up opportunities such as producing multiple colours at once. Others are made from polymers or small organic molecules. These are less toxic than quantum dots and often outshine them \u2014 much to the amazement of chemists, who are used to carbon-based compounds simply degrading in the presence of ultraviolet light. \u201cI was kind of surprised to find that we can make organic particles much brighter than inorganic particles,\u201d says Bin Liu, a chemical engineer at the National University of Singapore and the designer of the fluorescent nanoparticles that Goh is using. Nanolights have already begun to find application in areas ranging from  flat-screen displays  to biochemical tests. And researchers are working towards even more ambitious uses in fields such as solar energy, DNA mapping, motion sensing and even surgery. \u201cThe research is certainly fast-paced,\u201d says Daniel Chiu, who studies fluorescent nanoparticles at the University of Washington in Seattle. It is also increasingly wide ranging, adds Paul Alivisatos, a chemist at the University of California, Berkeley, and a co-founder of the first quantum-dot technology companies. \u201cIt's so much fun now.\u201d \n               Size matters \n             The nanolight era began with the discovery of quantum dots in 1981. Russian physicists were growing tiny crystals of the semiconductor cuprous chloride in silicate glass and observed that the colour of the glass depended on the size of the particles 1 . The crystals were so small that quantum effects were kicking in and they were behaving somewhat like atoms: they could absorb or emit light only as specific colours, with the exact frequencies depending on the size or shape of the particles (see 'Bridge the gap'). The quantum dots were bright and beautiful, says Yin Thai Chan, who studies them at the National University of Singapore, but \u201cthere were no obvious applications\u201d. By the early 2000s, however, the pure colours had begun to attract television manufacturers, as well as biomedical researchers, who saw their potential for labelling specific proteins and DNA segments. \u201cEverything is good about quantum dots,\u201d says Liu \u2014 except for one thing: their toxicity. The best-performing dots contain cadmium, which can poison cells. This limits their usefulness in biology and in applications such as household electronics, because some countries do not allow use of the element in such devices. To some extent, this problem can be overcome by replacing cadmium with zinc or indium, which are considerably less toxic, or by wrapping cadmium-based quantum dots in polymers that are biocompatible. But the toxicity is still a drawback for researchers who are pursuing ambitious applications such as fluoresence-guided surgery, in which nanoparticles are injected into a tumour, for instance, to make it glow and help surgeons to remove all traces of it. \n               Going organic \n             Partly in response to this challenge, researchers have begun to develop nanoparticles from materials that fluoresce naturally. Because the light-emitting properties of these nanolights come from their composition rather than their size or shape, they are easier to make with specific colours. \u201cPractically, this is useful because of the difficulties to synthesize everything in the same size,\u201d says Chiu. It also frees up nanolight researchers to explore alternative materials, such as semiconducting polymers. Studied for their potential in electronics since the 1950s, these polymers consist of simple compounds linked into a long chain in which electrons are free to move, but only at certain energies determined by the chain's composition. Light is emitted when electrons are kicked up to higher energy levels by some outside source, such as ultraviolet light, then fall back down to lower levels. The polymers can also be decorated with side groups to give them specific properties \u2014 for example, targeting them to cancer cells, or helping them to dissolve in water. And when chains are aggregated into polymer nanoparticles, or 'P-dots', they can be as much as 30 times brighter than a quantum dot of comparable size 2 . Semiconducting polymers do tend to be less stable than the inorganic semiconductors used in quantum dots. But because they are based on carbon, and contain no metals, they are much more likely to be biocompatible. P-dots have been used to stain and image cells, and also as sensors to detect oxygen, enzymes or metal ions such as copper. In 2013, for example, Chiu and his collaborators reported that a P-dot bound to a terbium ion can detect biomolecules produced by bacterial spores 3 . Under an ultraviolet lamp, the P-dots glow dark blue and the terbium ions emit a faint neon green colour. But when passing biomolecules attach themselves to terbium, the ions' light strengthens to a bright green. The P-dots' light remains unchanged, so it serves as an internal standard. Unfortunately, P-dots also have a fundamental problem: the polymer molecules are packed together so closely that they can be affected by 'quenching' \u2014 a phenomenon in which most of the energy coming from the original light source is quickly dissipated and fails to trigger fluorescence. Quenching has a huge impact on efficiency, says Yang-Hsiang Chan, a chemist at National Sun Yat-Sen University in Kaohsiung, Taiwan. One way to tackle it is to add bulky groups onto the polymer backbone to prevent the polymers from getting too close to each other. But this can be self-defeating: the resulting nanoparticles tend to be too fat to get into cells, say, or too dim to be useful. \u201cIt is very hard to get the right balance,\u201d says Chan, who is working to solve the problem by designing new polymers. \n               Together we shine \n             A more fundamental solution was pioneered in 2001, when Ben Zhong Tang at the Hong Kong University of Science and Technology in Clear Water Bay found that a class of small organic molecules would fluoresce only when they aggregate together 4 . These molecules are shaped like propellers or pinwheels, and they fluoresce when packed because they can no longer move and waste their energy. Instead, they release their energy as light \u2014 a phenomenon Tang has named aggregation-induced emission (AIE). He called the molecules AIE-gens. Over the next few years, Tang and his students changed the side groups and introduced elements such as nitrogen or oxygen, and AIE-gens can now glow in the entire spectrum of colours from ultraviolet to near-infrared. \u201cMy students quickly made a lot,\u201d says Tang. \u201cWe can change the colour at will.\u201d In 2011, Tang met Liu through a collaboration at the Institute of Materials Research and Engineering in Singapore, part of the government-backed Agency for Science, Technology and Research (A \u2217 STAR). At that time, AIE-gens were performing well, except that they could not dissolve in water, which made them difficult to use in biological applications. Liu was an expert in making things water-soluble, so Tang gave her some of his best AIE-gens to work with. Liu solved the problem by experimenting with polymers that are oil-loving on one end and water-loving on the other. The AIE-gens crowd within the polymer's oil-loving ends, and its water-loving ends point outwards to form a protective shell, resulting in a water-soluble capsule with a dense core full of AIE-gens. Liu designed a protective shell for the resulting nanoparticles, called AIE-dots, such that it could be decorated with various chemical groups that are tailored to specific applications. The shell can easily accommodate a wide variety of AIE-gens, says Liu, \u201cso that we can screen a lot of molecules very quickly to find out which one is the best.\u201d AIE-dots have been used to stain various tissues, from blood vessels to cancer cells to intracellular organelles such as mitochondria. Last year, Liu, Tang and their colleagues reported an AIE-dot that could be useful in a type of light-activated treatment known as photodynamic therapy 5 . It carries two molecules on its surface: one to get the dot into a cancer cell, and another to make it stick to the mitochondria. Once excited by an external light source, the AIE-dot produces red light that generates oxygen radicals near the mitochondria and kills the cancer cells. The best AIE-dots can be 40 times brighter than quantum dots 6 . \u201cWith AIE, high density in constrained space produces high brightness,\u201d says Guangxue Feng, a research assistant in Liu's lab. That is particularly useful for applications such as visualization of tissues or long-term tracking of cancer cells, which halve the number of nanoparticles per cell every time they divide. But the brightness comes at a cost: AIE-dots produce a much broader, more-muted spectrum than the pure, brilliant colours of quantum dots. But that hasn't kept Liu from starting LuminiCell, a spin-off company in Singapore that produces AIE-dots in three colours and three sizes for research such as Goh's at A \u2217 STAR. Tang is also trying to start a company; both he and Liu are now hoping to gain approval from the US Food and Drug Administration to test AIE-dots for human use in applications such as fluorescence-guided surgery. \n               Into the infrared \n             Another thing that limits the biological use of nanolights is that most of them absorb ultraviolet or visible light, which can penetrate only a few millimetres into tissue. Longer-wavelength near-infrared radiation can penetrate up to three centimetres \u2014 a much better depth for uses such as releasing drugs. But infrared light does not have enough energy to break the bonds that hold drugs on the nanoparticle, so many researchers are turning to a process called upconversion. This involves making material that can absorb multiple low-energy infrared photons, accumulate the energy and then re-emit it as higher-energy ultraviolet or visible photons. The group of heavy-metal elements known as lanthanides are particularly good at this trick. In 2011, Xiaogang Liu at the National University of Singapore reported that his laboratory had created a particularly versatile type of nanoparticle 7  with a Russian doll-like structure. It consists of a series of concentric shells that each contains a different combination of lanthanides. The energy from infrared light is absorbed by the core, then migrates outwards layer by layer, snowballing from lanthanide to lanthanide before finally emerging as high-energy light near the surface. The 15 lanthanides can be combined in numerous different ways to produce nanoparticles that emit in all colours, sometimes even several at once. In one demonstration, a student in Liu's lab shone an infrared laser through a series of beakers containing clear solutions of the nanoparticles: glowing lines of purple and green light appeared in the beakers where the infrared beam passed through. Liu thinks that these upconversion nanoparticles have tremendous potential in photovoltaics, where they could help to capture near-infrared light, which makes up almost half of the Sun's radiation. This is a long way from being practical, however: the brightest available nanoparticles convert just 10% of the light they absorb. Liu's group is working to build a library of these nanoparticles \u2014 no small task considering the number of lanthanides \u2014 to systematically study their properties and work on making them brighter. Last December, Marta Cerruti, a biomaterials scientist at McGill University in Montreal, Canada, reported a proof-of-concept system in which a lanthanide-containing nanoparticle is coated with a gel that contains a 'drug' \u2014 for testing purposes, a compact, stable protein 8 . After absorbing near-infrared light, the nanoparticle emits infrared, visible and ultraviolet light simultaneously. The infrared emission allows the researchers to track the nanoparticle's location, and the ultraviolet light cleaves the protein's bond to the gel and releases it \u2014 or at least, it has in the laboratory. Cerruti's group is now planning tests in animals. At the end of the day, quantum dots are still the nanolights to beat. \u201cThey are the de facto standard,\u201d says Chan. \u201cA lot of the fundamental phenomena concerning light emission are established in quantum dots and it shapes the way others explain what they see.\u201d Quantum dots are also still a research frontier. For example, they are getting a boost from relatively new semiconducting materials such as the  perovskites . Unlike conventional semiconductors, which have a fixed ratio of elements, perovskites can have variable ratios, so researchers can tailor the dots' emission by varying their composition as well as their size. \u201cThey have two degrees of freedom for tunability,\u201d says Edward Sargent, a materials engineer at the University of Toronto, Canada. Last year, Sargent reported a hybrid material in which quantum dots are held within a perovskite 9 , yielding the kind of high brightness and good electron mobility that manufacturers might like for use in flat-screen displays. Other researchers are hoping to combine the best properties of each component by pursuing hybrid nanolights. Bin Liu, for example, is trying to blend AIE-dots with quantum dots to produce narrow emissions. And semiconducting polymers paired with AIE-dots can produce much brighter particles than either alone 10 . Another grand challenge for nanolights is to create versions that emit infrared wavelengths efficiently. That would open up applications in motion sensing, from tiny detectors that tell the screen to turn off when a mobile phone is lifted to the ear to sophisticated devices for self-driving cars and home monitoring for elderly people. \u201cThere's so much more we could do,\u201d says Sargent. \n                 Tweet \n                 Facebook \n                 LinkedIn \n                 Weibo \n               \n                     Light flips transistor switch 2013-Jun-12 \n                   \n                     Quantum dots go on display 2013-Jan-15 \n                   \n                     Connect the quantum dots for a full-colour image 2011-Feb-20 \n                   \n                     Cheaper catalyst cleans diesel-car fumes 2010-Mar-25 \n                   \n                     Nano-antennas could help keep quantum secrets 2010-Mar-14 \n                   \n                     A*STAR \n                   \n                     Nanosys \n                   Reprints and Permissions"},
{"file_id": "531156a", "url": "https://www.nature.com/articles/531156a", "year": 2016, "authors": [{"name": "Heidi Ledford"}], "parsed_as_year": "2006_or_before", "body": "The real power of the biological tool lies in exploring how genomes work. Whenever a paper about CRISPR\u2013Cas9 hits the press, the staff at Addgene quickly find out. The non-profit company is where study authors often deposit molecular tools that they used in their work, and where other scientists immediately turn to get them. It is also where other scientists immediately turn to get their hands on these reagents. \u201cWe get calls within minutes of a hot paper publishing,\u201d says Joanne Kamens, executive director of the company in Cambridge, Massachusetts. Heidi Ledford talks Noah Baker through the cutting edge of the CRISPR technique Addgene's phones have been ringing a lot since early 2013, when researchers first reported 1 , 2 , 3  that they had used the CRISPR\u2013Cas9 system to slice the genome in human cells at sites of their choosing. \u201cIt was all hands on deck,\u201d Kamens says. Since then, molecular biologists have rushed to adopt the technique, which  can be used to alter the genome of almost any organism  with unprecedented ease and finesse. Addgene has sent 60,000 CRISPR-related molecular tools \u2014 about 17% of its total shipments \u2014 to researchers in 83 countries, and the company's CRISPR-related pages were viewed more than one million times in 2015. Much of the conversation about CRISPR\u2013Cas9 has revolved around its  potential for treating disease  or  editing the genes of human embryos , but researchers say that the real revolution right now is in the lab. What CRISPR offers, and biologists desire, is specificity: the ability to target and study particular DNA sequences in the vast expanse of a genome. And editing DNA is just one trick that it can be used for. Scientists are hacking the tools so that they can send proteins to precise DNA targets to toggle genes on or off, and even engineer entire biological circuits \u2014 with the long-term goal of understanding cellular systems and disease. \u201cFor the humble molecular biologist, it's really an extraordinarily powerful way to understand how the genome works,\u201d says Daniel Bauer, a haematologist at the Boston Children's Hospital in Massachusetts. \u201cIt's really opened the number of questions you can address,\u201d adds Peggy Farnham, a molecular biologist at the University of Southern California, Los Angeles. \u201cIt's just so fun.\u201d Here,  Nature  examines five ways in which CRISPR\u2013Cas9 is changing how biologists can tinker with cells. \n               Broken scissors \n             There are two chief ingredients in the CRISPR\u2013Cas9 system: a Cas9 enzyme that snips through DNA like a pair of molecular scissors, and a small RNA molecule that directs the scissors to a specific sequence of DNA to make the cut. The cell's native DNA repair machinery generally mends the cut \u2014 but often makes mistakes. That alone is a boon to scientists who want to disrupt a gene to learn about what it does. The genetic code is merciless: a minor error introduced during repair can completely alter the sequence of the protein it encodes, or halt its production altogether. As a result, scientists can study what happens to cells or organisms when the protein or gene is hobbled. But there is also a different repair pathway that sometimes mends the cut according to a DNA template. If researchers provide the template, they can edit the genome with nearly any sequence they desire at nearly any site of their choosing. In 2012, as laboratories were racing to demonstrate how well these gene-editing tools could cut human DNA, one team decided to take a different approach. \u201cThe first thing we did: we broke the scissors,\u201d says Jonathan Weissman, a systems biologist at the University of California, San Francisco (UCSF). Weissman learned about the approach from Stanley Qi, a synthetic biologist now at Stanford University in California, who mutated the Cas9 enzyme so that it still bound DNA at the site that matched its guide RNA, but no longer sliced it. Instead, the enzyme stalled there and blocked other proteins from transcribing that DNA into RNA. The hacked system allowed them to turn a gene off, but without altering the DNA sequence 4 . The team then took its 'dead' Cas9 and tried something new: the researchers tethered it to part of another protein, one that activates gene expression. With a few other tweaks, they had built a way to turn genes on and off at will 5 . Several labs have since published variations on this method; many more are racing to harness it for their research 6  (see 'Hacking CRISPR'). One popular application is to rapidly generate hundreds of different cell lines, each containing a different guide RNA that targets a particular gene. Martin Kampmann, another systems biologist at UCSF, hopes to screen such cells to learn whether flipping certain genes on or off affects the survival of neurons exposed to toxic protein aggregates \u2014 a mechanism that is thought to underlie several neurodegenerative conditions, including Alzheimer's disease. Kampmann had been carrying out a similar screen with RNA interference (RNAi), a technique that also silences genes and can process lots of molecules at once, but which has its drawbacks. \u201cRNAi is a shotgun with well-known off-target effects,\u201d he says. \u201cCRISPR is the scalpel that allows you to be more specific.\u201d Weissman and his colleagues, including UCSF systems biologist Wendell Lim, further tweaked the method so that it relied on a longer guide RNA, with motifs that bound to different proteins. This allowed them to activate or inhibit genes at three different sites all in one experiment 7 . Lim thinks that the system can handle up to five operations at once. The limit, he says, may be in how many guide RNAs and proteins can be stuffed into a cell. \u201cUltimately, it's about payload.\u201d That combinatorial power has drawn Ron Weiss, a synthetic biologist at the Massachusetts Institute of Technology (MIT) in Cambridge, into the CRISPR\u2013Cas9 frenzy. Weiss and his colleagues have also created multiple gene tweaks in a single experiment 8 , making it faster and easier to build complicated biological circuits that could, for example, convert a cell's metabolic machinery into a biofuel factory. \u201cThe most important goal of synthetic biology is to be able to program complex behaviour via the creation of these sophisticated circuits,\u201d he says. \n               CRISPR epigenetics \n             When geneticist Marianne Rots began her career, she wanted to unearth new medical cures. She studied gene therapy, which targets genes mutated in disease. But after a few years, she decided to change tack. \u201cI reasoned that many more diseases are due to disturbed gene-expression profiles, not so much the single genetic mutations I had been focused on,\u201d says Rots, at the University Medical Center Groningen in the Netherlands. The best way to control gene activity, she thought, was to adjust the epigenome, rather than the genome itself. The  epigenome is the constellation of chemical compounds  tacked onto DNA and the DNA-packaging proteins called histones. These can govern access to DNA, opening it up or closing it off to the proteins needed for gene expression. The marks change over time: they are added and removed as an organism develops and its environment shifts. In the past few years, millions of dollars have been poured into  cataloguing these epigenetic marks in different human cells , and their patterns have been correlated with everything from brain activity to tumour growth. But without the ability to alter the marks at specific sites, researchers are unable to determine whether they cause biological changes. \u201cThe field has met a lot of resistance because we haven't had the kinds of tools that geneticists have had, where they can go in and directly test the function of a gene,\u201d says Jeremy Day, a neuroscientist at the University of Alabama at Birmingham. CRISPR\u2013Cas9 could turn things around. In April 2015, Charles Gersbach, a bioengineer at Duke University in Durham, North Carolina, and his colleagues published 9  a system for adding acetyl groups \u2014 one type of epigenetic mark \u2014 to histones using the broken scissors to carry enzymes to specific spots in the genome. The team found that adding acetyl groups to proteins that associate with DNA was enough to send the expression of targeted genes soaring, confirming that the system worked and that, at this location, the epigenetic marks had an effect. When he published the work, Gersbach deposited his enzyme with Addgene so that other research groups could use it \u2014 and they quickly did. Gersbach predicts that a wave of upcoming papers will show a synergistic effect when multiple epigenetic markers are manipulated at once. The tools need to be refined. Dozens of enzymes can create or erase an epigenetic mark on DNA, and not all of them have been amenable to the broken-scissors approach. \u201cIt turned out to be harder than a lot of people were expecting,\u201d says Gersbach. \u201cYou attach a lot of things to a dead Cas9 and they don't happen to work.\u201d Sometimes it is difficult to work out whether an unexpected result arose because a method did not work well, or because the epigenetic mark simply doesn't matter in that particular cell or environment. Rots has explored the function of epigenetic marks on cancer-related genes using older editing tools called zinc-finger proteins, and is now adopting CRISPR\u2013Cas9. The new tools have democratized the field, she says, and that has already had a broad impact. People used to say that the correlations were coincidental, Rots says \u2014 that if you rewrite the epigenetics it will have no effect on gene expression. \u201cBut now that it's not that difficult to test, a lot of people are joining the field.\u201d \n               CRISPR code cracking \n             Epigenetic marks on DNA are not the only genomic code that is yet to be broken. More than 98% of the human genome does not code for proteins. But researchers think that a fair chunk of this DNA is doing something important, and they are adopting CRISPR\u2013Cas9 to work out what that is. Some of it codes for RNA molecules \u2014 such as microRNAs and long non-coding RNAs \u2014 that are thought to have functions apart from making proteins. Other sequences are 'enhancers' that amplify the expression of the genes under their command. Most of the DNA sequences linked to the risk of common diseases lie in regions of the genome that contain non-coding RNA and enhancers. But before CRISPR\u2013Cas9, it was difficult for researchers to work out what those sequences do. \u201cWe didn't have a good way to functionally annotate the non-coding genome,\u201d says Bauer. \u201cNow our experiments are much more sophisticated.\u201d Farnham and her colleagues are using CRISPR\u2013Cas9 to delete enhancer regions that are found to be mutated in genomic studies of prostate and colon cancer. The results have sometimes surprised her. In one unpublished experiment, her team deleted an enhancer that was thought to be important, yet no gene within one million bases of it changed expression. \u201cHow we normally classify the strength of a regulatory element is not corresponding with what happens when you delete that element,\u201d she says. More surprises may be in store as researchers harness CRISPR\u2013Cas9 to probe large stretches of regulatory DNA. Groups led by geneticists David Gifford at MIT and Richard Sherwood at the Brigham and Women's Hospital in Boston used the technique to create mutations across a 40,000-letter sequence, and then examined whether each change had an effect on the activity of a nearby gene that made a fluorescent protein 10 . The result was a map of DNA sequences that enhanced gene expression, including several that had not been predicted on the basis of gene regulatory features such as chromatin modifications. Delving into this dark matter has its challenges, even with CRISPR\u2013Cas9. The Cas9 enzyme will cut where the guide RNA tells it to, but only if a specific but common DNA sequence is present near the cut site. This poses little difficulty for researchers who want to silence a gene, because the key sequences almost always exist somewhere within it. But for those who want to make very specific changes to short, non-coding RNAs, the options can be limited. \u201cWe cannot take just any sequence,\u201d says Reuven Agami, a researcher at the Netherlands Cancer Institute in Amsterdam. Researchers are scouring the bacterial kingdom for  relatives of the Cas9 enzyme that recognize different sequences . Last year, the lab of Feng Zhang, a bioengineer at the Broad Institute of MIT and Harvard in Cambridge, characterized a family of enzymes called Cpf1 that work similarly to Cas9 and could expand sequence options 11 . But Agami notes that few alternative enzymes found so far work as well as the most popular Cas9. In the future, he hopes to have a whole collection of enzymes that can be targeted to any site in the genome. \u201cWe're not there yet,\u201d he says. \n               CRISPR sees the light \n             Gersbach's lab is using gene-editing tools as part of an effort to understand cell fate and how to manipulate it: the team hopes one day to grow tissues in a dish for drug screening and cell therapies. But CRISPR\u2013Cas9's effects are permanent, and Gersbach's team needed to turn genes on and off transiently, and in very specific locations in the tissue. \u201cPatterning a blood vessel demands a high degree of control,\u201d he says. Gersbach and his colleagues took their broken, modified scissors \u2014 the Cas9 that could now activate genes \u2014 and added proteins that are activated by blue light. The resulting system triggers gene expression when cells are exposed to the light, and stops it when the light is flicked off 12 . A group led by chemical biologist Moritoshi Sato of the University of Tokyo rigged a similar system 13 , and also made an active Cas9 that edited the genome only after it was hit with blue light 14 . Others have achieved similar ends by combining CRISPR with a chemical switch. Lukas Dow, a cancer geneticist at Weill Cornell Medical College in New York City, wanted to mutate cancer-related genes in adult mice, to reproduce mutations that have been identified in human colorectal cancers. His team engineered a CRISPR\u2013Cas9 system in which a dose of the compound doxycycline activates Cas9, allowing it to cut its targets 15 . The tools are another step towards gaining fine control over genome editing. Gersbach's team has not patterned its blood vessels just yet: for now, the researchers are working on making their light-inducible system more efficient. \u201cIt's a first-generation tool,\u201d says Gersbach. \n               Model CRISPR \n             Cancer researcher Wen Xue spent the first years of his postdoc career making a transgenic mouse that bore a mutation found in some human liver cancers. He slogged away, making the tools necessary for gene targeting, injecting them into embryonic stem cells and then trying to derive mice with the mutation. The cost: a year and US$20,000. \u201cIt was the rate-limiting step in studying disease genes,\u201d he says. A few years later, just as he was about to embark on another transgenic-mouse experiment, his mentor suggested that he give CRISPR\u2013Cas9 a try. This time, Xue just ordered the tools, injected them into single-celled mouse embryos and, a few weeks later \u2014  voil\u00e1 . \u201cWe had the mouse in one month,\u201d says Xue. \u201cI wish I had had this technology sooner. My postdoc would have been a lot shorter.\u201d Researchers who study everything from cancer to neurodegeneration are embracing CRISPR-Cas9 to  create animal models of the diseases . It lets them engineer more animals, in more complex ways, and in a wider range of species. Xue, who now runs his own lab at the University of Massachusetts Medical School in Worcester, is systematically sifting through data from tumour genomes, using CRISPR\u2013Cas9 to model the mutations in cells grown in culture and in animals. Researchers are hoping to mix and match the new CRISPR\u2013Cas9 tools to precisely manipulate the genome and epigenome in animal models. \u201cThe real power is going to be the integration of those systems,\u201d says Dow. This may allow scientists to capture and understand some of the complexity of common human diseases. Take tumours, which can bear dozens of mutations that potentially contribute to cancer development. \u201cThey're probably not all important in terms of modelling a tumour,\u201d says Dow. \u201cBut it's very clear that you're going to need two or three or four mutations to really model aggressive disease and get closer to modelling human cancer.\u201d Introducing all of those mutations into a mouse the old-fashioned way would have been costly and time-consuming, he adds. Bioengineer Patrick Hsu started his lab at the Salk Institute for Biological Studies in La Jolla, California, in 2015; he aims to use gene editing to model neurodegenerative conditions such as Alzheimer's disease and Parkinson's disease in cell cultures and marmoset monkeys. That could recapitulate human behaviours and progression of disease more effectively than mouse models, but would have been unthinkably expensive and slow before CRISPR\u2013Cas9. Even as he designs experiments to genetically engineer his first CRISPR\u2013Cas9 marmosets, Hsu is aware that this approach may be only a stepping stone to the next. \u201cTechnologies come and go. You can't get married to one,\u201d he says. \u201cYou need to always think about what biological problems need to be solved.\u201d \n                 Tweet \n                 Facebook \n                 LinkedIn \n                 Weibo \n               \n                     Policy: Reboot the debate on genetic engineering 2016-Mar-09 \n                   \n                     Welcome to the CRISPR zoo 2016-Mar-09 \n                   \n                     CRISPR everywhere 2016-Mar-09 \n                   \n                     Governance: Learn from DIY biologists 2016-Mar-09 \n                   \n                     Gene intelligence 2016-Mar-09 \n                   \n                     Monkeys genetically modified to show autism symptoms 2016-Jan-25 \n                   \n                     Bitter fight over CRISPR patent heats up 2016-Jan-12 \n                   \n                     Enzyme tweak boosts precision of CRISPR genome edits 2016-Jan-06 \n                   \n                     Biologists create more precise molecular scissors for genome editing 2015-Dec-01 \n                   \n                     Alternative CRISPR system could improve genome editing 2015-Sep-25 \n                   \n                     Nature  special: CRISPR \n                   \n                     Addgene CRISPR guide \n                   Reprints and Permissions"},
{"file_id": "531155a", "url": "https://www.nature.com/articles/531155a", "year": 2016, "authors": [], "parsed_as_year": "2006_or_before", "body": "A special issue explores what it means to be living in an age of gene editing. Just under a year ago, a molecular-biology technique was thrust onto the world stage. Researchers in China announced that they had used the nascent gene-editing tool CRISPR\u2013Cas9 to modify the genomes of human embryos, triggering a major ethics debate. Yet while this controversy has been playing out, researchers the world over have rushed to use the tool to tinker with the genomes of human somatic cells, viruses, bacteria, animals and plants, and it's in these contexts that the technique promises to have more immediate impact. This issue of  Nature  examines what's going on at the CRISPR frontiers. Biologists are using CRISPR\u2013Cas9 to better understand genomes \u2014 not just by editing DNA, but by  devising variations on the technique to precisely manipulate the activity of genes . And, armed for the first time with a method that can easily introduce genetic changes to many animals,  researchers have edited a veritable menagerie of beasts  \u2014 from ferrets to elephants to koi carp \u2014 in an attempt to combat disease, improve agriculture and even make designer pets. Such advances in gene editing are creating upheaval for regulatory bodies that are responsible for approving genetically engineered products \u2014 it's a \u201cpowder keg waiting to explode\u201d, writes Jennifer Kuzma, a science-policy researcher at North Carolina State University in Raleigh.  She calls for more openness and honesty  than has characterized past discussions of biotechnology, and for a regulatory system that better factors in societal views as well as science. CRISPR\u2013Cas9 may be democratizing gene editing in the laboratory, but Todd Kuiken, who studies science policy at the Wilson Center, a think tank in Washington DC,  argues that the revolution has not yet swept into home workshops  or citizen-science community spaces. Contrary to reports in the popular media, he says, few CRISPR creations are likely to come from the labs of do-it-yourself biologists any time soon. However, this group is arguably ahead of the scientific establishment when it comes to thinking about how to use the technology safely. For better or for worse, CRISPR\u2013Cas9 is transforming biology. We are now at the dawn of the gene-editing age. \n               boxed-text \n             \n                 Tweet \n                 Facebook \n                 LinkedIn \n                 Weibo \n               \n                     Policy: Reboot the debate on genetic engineering 2016-Mar-09 \n                   \n                     Welcome to the CRISPR zoo 2016-Mar-09 \n                   \n                     Governance: Learn from DIY biologists 2016-Mar-09 \n                   \n                     Gene intelligence 2016-Mar-09 \n                   \n                     CRISPR: gene editing is just the beginning 2016-Mar-07 \n                   \n                     Nature  special: CRISPR \n                   Reprints and Permissions"},
{"file_id": "welcome-to-the-crispr-zoo-1.19537", "url": "https://www.nature.com/news/welcome-to-the-crispr-zoo-1.19537", "year": 2016, "authors": [], "parsed_as_year": "2011_2015", "body": "Timothy Doran's 11-year-old daughter is allergic to eggs. And like about 2% of children worldwide who share the condition, she is unable to receive many routine vaccinations because they are produced using chicken eggs. Doran, a molecular biologist at the Commonwealth Scientific and Industrial Research Organisation (CSIRO) in Geelong, Australia, thinks that he could solve this problem using the powerful gene-editing tool CRISPR\u2013Cas9. Most egg allergies are caused by one of just four proteins in the white, and when Doran's colleagues altered the gene that encodes one of these in bacteria, the resulting protein no longer triggered a reaction in blood serum from people who were known to be allergic to it 1 . Doran thinks that using CRISPR to edit the gene in chickens could result in hypoallergenic eggs. The group expects to hatch its first generation of chicks with gene modifications later this year as a proof of concept. Doran realizes that it could be some time before regulators would approve gene-edited eggs, and he hopes that his daughter will have grown out of her allergy by then. \u201cIf not, I've got someone ready and waiting to try the first egg,\u201d he says. Chickens are just one of a menagerie of animals that could soon have their genomes reimagined. Until now, researchers had the tools to genetically manipulate only a small selection of animals, and the process was often inefficient and laborious. With the arrival of CRISPR, they can alter the genes of a wide range of organisms with relative precision and ease. In the past two years alone, the prospect of gene-edited monkeys, mammoths, mosquitoes and more have made headlines as scientists attempt to put CRISPR to use for applications as varied as agriculture, drug production and bringing back lost species. CRISPR-modified animals are even being marketed for sale as pets. \u201cIt's allowed us to consider a whole raft of projects we couldn't before,\u201d says Bruce Whitelaw, an animal biotechnologist at the Roslin Institute in Edinburgh, UK. \u201cThe whole community has wholeheartedly moved towards genome editing.\u201d But regulators are still working out how to deal with such creatures, particularly those intended for food or for release into the wild. Concerns abound about safety and ecological impacts. Even the US director of national intelligence has weighed in, saying that the easy access, low cost and speedy development of genome editing could increase the risk that someone will engineer harmful biological agents. Eleonore Pauwels, who studies biotechnology regulation at the Wilson Center in Washington DC, says that the burgeoning use of CRISPR in animals offers an opportunity for researchers and policymakers to engage the public in debate. She hopes that such discussions will help in determining which uses of CRISPR will be most helpful to humans, to other species and to science \u2014 and will highlight the limits of the technology. \u201cI think there is a lot of value in humility about how much control we have,\u201d she says. Disease resistance is one of the most popular applications for CRISPR in agriculture, and scientists are tinkering across a wide spectrum of animals. Biotechnology entrepreneur Brian Gillis in San Francisco is hoping that the tool can help to stem the dramatic loss of honeybees around the world, which is being caused by factors such as disease and parasites. Gillis has been studying the genomes of 'hygienic' bees, which obsessively clean their hives and remove sick and infested bee larvae. Their colonies are less likely to succumb to mites, fungi and other pathogens than are those of other strains, and Gillis thinks that if he can identify genes associated with the behaviour, he might be able  to edit them in other breeds to bolster hive health . But the trait could be difficult to engineer. No hygiene-associated genes have been definitively identified, and the roots of the behaviour may prove complex, says BartJan Fernhout, chairman of Arista Bee Research in Boxmeer, the Netherlands, which studies mite resistance. Moreover, if genes are identified, he says, conventional breeding may be sufficient to confer resistance to new populations, and that might be preferable given the widespread opposition to genetic engineering. Such concerns don't seem to have slowed down others studying disease resistance. Whitelaw's group at the Roslin Institute is one of several using CRISPR and other gene-editing systems to create pigs that are resistant to viral diseases that cost the agricultural industry hundreds of millions of dollars each year. Whitelaw's team is using another gene-editing technique to alter immune genes in domestic pigs to match more closely those of warthogs that are naturally resistant to African swine fever, a major agricultural pest 2 . And Randall Prather at the University of Missouri in Columbia has created pigs with a mutated protein on the surface of their cells, which should make them impervious to a deadly respiratory virus 3 . Other researchers are making cattle that are resistant to the trypanosome parasites that are responsible for sleeping sickness. Whitelaw hopes that regulators \u2014 and sceptical consumers \u2014 will be more enthusiastic about animals that have had their genes edited to improve disease resistance than they have been for traits such as growth promotion because of the potential to reduce suffering. And some governments are considering whether CRISPR-modified animals should be regulated in the same way as other genetically modified organisms, because they do not contain DNA from other species. Doran's quest to modify allergens in chicken eggs requires delicate control. The trick is to finely adjust a genetic sequence in a way that will stop the protein from triggering an immune reaction in people, but still allow it to perform its normal role in embryonic development. CRISPR has made such precise edits possible for the first time. \u201cCRISPR has been the saviour for trying to tackle allergens,\u201d says Mark Tizard, a molecular biologist at CSIRO who works with Doran on chickens. Using the technique in birds still presents problems. Mammals can be induced to produce extra eggs, which can then be removed, edited, fertilized and replaced. But in birds, the fertilized egg binds closely to the yolk and removing it would destroy the embryo. And because eggs are difficult to access while still inside the hen, CRISPR components cannot be directly injected into the egg itself. By the time the egg is laid, development has proceeded too far for gene editing to affect the chick's future generations. To get around this, Tizard and Doran looked to primordial germ cells (PGCs) \u2014 immature cells that eventually turn into sperm or eggs. Unlike in many animals, chicken PGCs spend time in the bloodstream during development. Researchers can therefore remove PGCs, edit them in the lab and then return them to the developing bird. The CSIRO team has even developed  a method to insert CRISPR components directly into the bloodstream  so that they can edit PGCs there 4 . The researchers also plan to produce chickens with components required for CRISPR integrated directly into their genomes \u2014 what they call CRISPi chickens. This would make it even easier to edit chicken DNA, which could be a boon for 'farmaceuticals' \u2014 drugs created using domesticated animals. Regulators have shown a willingness to consider such drugs. In 2006, the European Union approved a goat that produces an anticlotting protein in its milk. It was subsequently approved by the US Food and Drug Administration, in 2009. And in 2015, both agencies approved a transgenic chicken whose eggs contain a drug for cholesterol diseases. About 4,000 years ago, hunting by humans helped to drive woolly mammoths ( Mammuthus primigenius ) to extinction. CRISPR pioneer George Church at Harvard Medical School in Boston, Massachusetts, has attracted attention for his ambitious plan to undo the damage by using CRISPR to transform endangered Indian elephants into woolly mammoths \u2014 or at least cold-resistant elephants. The goal, he says, would be to release them into a reserve in Siberia, where they would have space to roam. The plan sounds wild \u2014 but efforts to make mammals more mammoth-like have been going on for a while. Last year, geneticist Vincent Lynch at the University of Chicago in Illinois showed that cells with the mammoth version of a gene for heat-sensing and hair growth could grow in low temperatures 5 , and mice with similar versions prefer the colder parts of a temperature-regulated cage 6 . Church says that he has  edited about 14 such genes in elephant embryos . But editing, birthing and then raising mammoth-like elephants is a huge undertaking. Church says that it would be unethical to implant gene-edited embryos into endangered elephants as part of an experiment. So his lab is looking into ways to build an artificial womb; so far, no such device has ever been shown to work. There are some de-extinction projects that could prove less challenging. Ben Novak at the University of California, Santa Cruz, for example, wants to resurrect the passenger pigeon ( Ectopistes migratorius ), a once-ubiquitous bird that was driven to extinction in the late nineteenth century by overhunting. His group is currently comparing DNA from museum specimens to that of modern pigeons. Using PGC methods similar to Doran's, he plans to edit the modern-pigeon genomes so that the birds more closely resemble their extinct counterparts. Novak says that the technology is not yet advanced enough to modify the hundreds of genes that differ between modern and historic pigeons. Still, he says that CRISPR has given him the best chance yet of realizing his lifelong dream of restoring an extinct species. \u201cI think the project is 100% impossible without CRISPR,\u201d he says. For decades, researchers have explored the idea of genetically modifying mosquitos to prevent the spread of diseases such as dengue or malaria. CRISPR has given them a new way to try. In November, molecular biologist Anthony James of the University of California, Irvine, revealed a line of mosquitoes with a synthetic system called a gene drive that passes a malaria-resistance gene on to the mosquitoes' offspring 7 . Gene drives ensure that almost all the insects' offspring inherit two copies of the edited gene, allowing it to spread rapidly through a population. Another type of gene drive, published last December 8 , propagates a gene that sterilizes all female mosquitoes, which could wipe out a population. The outbreak of mosquito-borne Zika virus in Central and South America has increased interest in the technology, and several research labs have begun building gene drives that could eliminate the Zika-carrying species,  Aedes aegypti . Many scientists are worried about unintended and unknown  ecological consequences of releasing such a mosquito . For this reason, Church and his colleagues have  developed 'reverse gene drives'  \u2014 systems that would propagate through the population to cancel out the original mutations 9 ,  10 . But Jason Rasgon, who works on genetically modified insects at Pennsylvania State University in University Park, says that although ecology should always be a consideration, the extent and deadliness of some human diseases such as malaria may outweigh some costs. Mosquitoes are some of the easiest insects to work with, he says, but researchers are looking at numerous other ways to use gene drives, including making ticks that are  unable to transmit the bacteria that cause Lyme disease . Last year, researchers identified a set of genes that could be modified to prevent aquatic snails ( Biomphalaria glabrata ) from transmitting the parasitic disease schistosomiasis 11 . Last November, after a lengthy review, the US Food and Drug Administration approved the first transgenic animals for human consumption:  fast-growing salmon  made by AquaBounty Technologies of Maynard, Massachusetts. Some still fear that if the salmon escape, they could breed with wild fish and upset the ecological balance. To address such concerns, fish geneticist Rex Dunham of Auburn University in Alabama has been using CRISPR to inactivate genes for three reproductive hormones \u2014 in this case, in catfish, the most intensively farmed fish in the United States. The changes should leave the fish sterile, so any fish that might escape from a farm, whether genetically modified or not, would stand little chance of polluting natural stocks. \u201cIf we're able to achieve 100% sterility, there is no way that they can make a genetic impact,\u201d Dunham says. Administering hormones would allow the fish to reproduce for breeding purposes. And Dunham says that similar methods could be used in other fish species. CRISPR could also reduce the need for farmers to cull animals, an expensive and arguably inhumane practice. Biotechnologist Alison van Eenennaam at the University of California, Davis, is using the technique to ensure that beef cattle produce only male or male-like offspring, because females produce less meat and are often culled. She copies a Y-chromosome gene that is important for male sexual development onto the X chromosome in sperm. Offspring produced with the sperm would be either normal, XY males, or XX females with male traits such as more muscle. In the egg industry, male chicks from elite egg-laying chicken breeds have no use, and farmers generally cull them within a day of hatching. Tizard and his colleagues are adding a gene for green fluorescent protein to the chickens' sex chromosomes so that male embryos will glow under ultraviolet light. Egg producers could remove the male eggs before they hatch and potentially use them for vaccine production. There are other ways that CRISPR could make agriculture more humane. Packing cattle into trailers or other small spaces often causes injuries, especially when the animals have long horns. So cattle farmers generally burn, cut or remove them with chemicals \u2014 a process that can be painful for the animal and dangerous for the handler. There are cattle varieties that do not have horns \u2014 a condition called 'polled' \u2014 but crossing these breeds with 'elite' meat or dairy breeds reduces the quality of the offspring. Molecular geneticist Scott Fahrenkrug, founder of Recombinetics in Saint Paul, Minnesota, is using gene-editing techniques to transfer the gene that eliminates horns into elite breeds 12 . The company has produced only two polled calves so far \u2014 both male \u2014 which are being raised at the University of California, Davis, until they are old enough to breed. Last September, the genomics firm BGI wowed a conference in Shenzhen, China, with micropigs \u2014 animals that grow to only around 15 kilograms, about the size of a standard dachshund. BGI had originally intended to make the pigs for research, but has since decided to capitalize on creation of the animals by selling them as pets for US$1,600. The plan is to eventually  allow buyers to request customized coat patterns . BGI is also using CRISPR to alter the size, colour and patterns of koi carp. Koi breeding is an ancient tradition in China, and Jian Wang, director of gene-editing platforms at BGI, says that even good breeders will usually produce only a few of the most beautifully coloured and proportioned, 'champion quality' fish out of millions of eggs. CRISPR, she says, will let them precisely control the fish's patterns, and could also be used to make the fish more suitable for home aquariums rather than the large pools where they are usually kept. Wang says that the company will begin selling koi in 2017 or 2018 and plans to eventually add other types of pet fish to its repertoire. Claire Wade, a geneticist at the University of Sydney in Australia, says that CRISPR could be used to enhance dogs. Her group has been cataloguing genetic differences between breeds and hopes to identify areas involved in behaviour and traits such as agility that could potentially be edited 13 . Sooam Biotech in Seoul, best-known for a service that will clone a deceased pet for $100,000, is also interested in using CRISPR. Sooam researcher David Kim says that the company wants to enhance the capabilities of working dogs \u2014 guide dogs or herding dogs, for example. Jeantine Lunshof, a bioethicist who works in Church's lab at Harvard, says that engineering animals just to change their appearance, \u201cjust to satisfy our idiosyncratic desires\u201d, borders on frivolous and could harm animal well-being. But she concedes that the practice is not much different from the inbreeding that humans have been performing for centuries to enhance traits in domestic animals and pets. And CRISPR might even help to eliminate some undesirable characteristics: many dog breeds are prone to hip problems, for example. \u201cIf you could use genome editing to reverse the very bad effects we have achieved by this selective inbreeding over decades, then that would be good.\u201d Ferrets have long been a useful model for influenza research because the virus replicates in their respiratory tracts and they sometimes sneeze when infected, allowing studies of virus transmission. But until the arrival of CRISPR, virologists lacked the tools to easily alter ferret genes. Xiaoqun Wang and his colleagues at the Chinese Academy of Sciences in Beijing have used CRISPR to tweak genes involved in ferret brain development 14 , and they are now using it to modify the animals' susceptibility to the flu virus. He says that he will make the model available to infectious-disease researchers. Behavioural researchers are particularly excited about the prospect of genetically manipulating marmosets and monkeys, which are more closely related to humans than are standard rodent models. The work is moving most quickly in China and Japan. In January, for instance, neuroscientist Zilong Qiu and his colleagues at the Chinese Academy of Sciences in Shanghai published a paper 15  describing macaques with a CRISPR-induced mutation in  MECP2 , the gene associated with the neurodevelopmental disorder Rett syndrome. The animals  showed symptoms of autism spectrum disorder , including repetitive behaviours and avoiding social contact. But Anthony Chan, a geneticist at Emory University in Atlanta, Georgia, cautions that researchers must think carefully about the ethics of creating such models and whether more-standard laboratory animals such as mice would suffice. \u201cNot every disease needs a primate model,\u201d he says. Basic neuroscience could also benefit from the availability of new animal models. Neurobiologist Ed Boyden at the Massachusetts Institute of Technology is raising a colony of the world's tiniest mammal \u2014 the Etruscan tree shrew ( Suncus etruscus ). The shrews' brains are so small that the entire organ can be viewed under a microscope at once. Gene edits that cause neurons to flash when they fire, for instance, could allow researchers to study the animal's entire brain in real time. The CRISPR zoo is expanding fast \u2014 the question now is how to navigate the way forward. Pauwels says that the field could face the same kind of public backlash that bedevilled the previous generation of genetically modified plants and animals, and to avoid it, scientists need to communicate the advantages of their work. \u201cIf it's here and can have some benefit,\u201d she says, \u201clet's think of it as something we can digest and we can own.\u201d"},
{"file_id": "531290a", "url": "https://www.nature.com/articles/531290a", "year": 2016, "authors": [{"name": "Alexandra Witze"}], "parsed_as_year": "2006_or_before", "body": "Scientists are searching for an unseen world at the fringes of the solar system. Astronomer Scott Sheppard runs through his checklist as he settles in for a long night of skygazing at the Subaru telescope atop Mauna Kea, Hawaii. The air above the summit: clear. The telescope: working smoothly. His 3-terabyte hard drive: emptied and ready to accept a flood of fresh data in the hours to come. On a wall in the observing room, three clocks track the hours in Hawaii, Tokyo and Coordinated Universal Time. Screens display every tic of the weather above the summit: wind direction, temperature and the dreaded humidity levels that could end this November night of observing if they were to rise. But for now, conditions are nearly perfect, especially when it comes to a characteristic known as seeing \u2014 a measure of how stable the stars above look. \u201cSeeing is point-five-five,\u201d says David Tholen, an astronomer at the University of Hawaii in Manoa. \u201cIt doesn't get much better than that,\u201d replies the third member of this team, Chad Trujillo of the Gemini Observatory in Hilo, Hawaii. Sheppard, the lone mainlander of the group, works at the Carnegie Institution of Science in Washington DC. With the weather looking promising, he pulls out his logbook and begins to outline plans for the next ten hours. Between twilight and dawn, he will methodically direct Subaru's enormous 8.2-metre mirror \u2014 one of the largest in the world \u2014 to stare deeply at one patch of the sky, then another and another. Several hours later, he will look at the same areas for a second time, and after that, a third. By comparing the staggered images, the researchers can hunt for objects that move ever so slightly over the course of a few hours. These would be distant worlds beyond Pluto, in the most extreme reaches of the Solar System. This is the realm of the  long-sought Planet X .  Sheppard already has some idea of where to look. On his list of goals for the night, he wants to capture fresh images of an object that he first spotted one month earlier. At 9:20 p.m., and again at 10:46 p.m., he aims Subaru at a region near the constellation Aries, where he thinks this object will be. The exposures come off the telescope, and Tholen begins to process them. After a few minutes, he beckons Sheppard over. On his grey screen is a light-coloured dot that jumps against the field of background stars when Tholen toggles between the two images. \u201cThere it is,\u201d he says. \u201cYou got it.\u201d \u201cRight where it should be,\u201d Sheppard says. It is the same object that he saw previously, and it earns an entry in his logbook: in an image on computer chip number 104, in field number 776, they have spotted an object 90 astronomical units ( AU ) away, or 90 times the Earth\u2013Sun distance. It's not clear yet how big the object is or whether it is scientifically important \u2014 but it is among the most distant worlds ever seen in the Solar System. Astronomers have  discovered more than 2,000 exoplanets  around other stars, mostly through indirect methods that detect changes in the distant star. Yet the farthest reaches of our Solar System remain largely unexplored: the indirect techniques won't work in our own stellar neighbourhood, and objects in the distant suburbs of the Sun are too faint to be glimpsed by anything but the world's most powerful telescopes. Sheppard and Trujillo have been racing to find the frigid worlds that are thought to populate this distant zone. At stake is what could be the last great discovery in the Solar System \u2014 a planet bigger than Earth that may swing around the Sun far past Pluto. Suggestions of a Planet X have circulated for more than a century, but those hypotheses have always fallen apart after closer scrutiny. In 2014,  Trujillo and Sheppard revived the concept of a putative Planet X , on the basis of the orbits of some extremely distant objects 1 . In January, the idea got a boost when two astronomers from the California Institute of Technology (Caltech) in Pasadena calculated more precisely  where in the Solar System it might lie 2 . They dubbed it 'Planet Nine': a back-handed reference to the demotion of Pluto from planet to dwarf planet in 2006. The hunt is now on to find Planet Nine, or any other unseen super-Earths that may lurk out there on other orbits. The quest is likely to reveal fundamental  insights into how the Solar System formed 4.6 billion years ago , and how it has evolved since then. \u201cIf this big object is out there, it basically changes our perception of the Solar System,\u201d says Sheppard. \u201cIt's true discovery at its core.\u201d \n               Unknown zone \n             When Sheppard and Trujillo began to chase distant worlds, they were hoping to follow in the footsteps of other legendary astronomers. In 1846, Johann Galle of Germany first spotted Neptune, the eighth planet, at a distance of about 30  AU , right where it was expected to be according to calculations of how it would gravitationally perturb Uranus.  In 1930, US astronomer Clyde Tombaugh found Pluto , orbiting at a distance of around 40  AU . And in 1992, astronomers David Jewitt, then at the University of Hawaii, and Jane Luu, then at the University of California, Berkeley, discovered an even more distant object, beginning the exploration of a region of space called the Kuiper belt 3 . Since then, astronomers have found thousands of Kuiper belt objects: small icy worlds similar to Pluto that range in distance from about 30  AU  to 50  AU  from the Sun. Beyond that lies the equivalent of 'here be dragons' on old maps. Scientists sometimes call it the outermost Kuiper belt or the innermost Oort cloud \u2014 the next region of the Solar System, which is thought to extend to at least 100,000  AU . \u201cThere's a whole chunk of the Solar System we don't fully understand,\u201d says Meg Schwamb, a planetary astronomer at the Academia Sinica in Taipei. \u201cIt's one of the last unexplored territories.\u201d Which is why Sheppard and Trujillo are out looking. They met as graduate students at the University of Hawaii, where both had Jewitt as their adviser. They worked together to hunt Kuiper belt objects, and then started a systematic survey to search for still more-distant worlds. They are the only team routinely looking for the most-extreme objects. \u201cThe population could be huge,\u201d says Trujillo. \u201cThat's why we're doing the search.\u201d By 2012, the two were using the biggest light buckets they could get their hands on, with wide-field cameras that would let them view as much of the sky as possible. At the Dark Energy Camera, atop a 4-metre telescope in Chile, they got a hit almost immediately. On their first night of observing, they spotted an object that was moving so slowly that it had to be very distant. Thrilled, they watched it move during the course of a year, which provided enough data to calculate its orbit. They found that the closest it ever gets to the Sun \u2014 a measure known as its perihelion \u2014 is 80  AU , beyond the bulk of the Kuiper belt. This made it the  object with the farthest-known perihelion , just beating the dwarf planet Sedna, whose perihelion is 76  AU . The discovery of this object, called 2012 VP 113 , led to a  Nature  paper 1  \u2014 and a lot more observing time on big telescopes. In 2014, Sheppard and Trujillo spent their first nights at Subaru, a facility run by the National Astronomical Observatory of Japan that carries a huge camera called the Hyper Suprime-Cam. The combination of a big telescope and a wide-field camera makes Subaru the world's best place to scan large sections of the sky for faint objects. Many scientists work with Subaru remotely: they stay at sea level in Hilo, and use videoconferencing to communicate with the telescope's operators. That approach saves researchers from making the 2-hour-long journey to the summit of Mauna Kea at 4,200 metres, where the atmosphere has 40% less oxygen and causes many people to experience dizziness, headaches or sometimes more-serious medical problems. But Sheppard likes to be actively involved in directing observations, so he always makes the trip. As the hours tick by during the night, he stays alert, never once clipping an oxygen sensor onto his finger to see how he is coping with the altitude. His logbook fills up with notations: field number, chip number, exposure time. He reorders targets on the fly, rearranging what he is looking at to improve the time gap between the fields. Subaru's huge mirror gazes at the sky, gathering photons for him. Exposure times count down in big green numbers on a computer monitor. When an exposure finishes, an alert dings like a cuckoo clock, and Sheppard hovers over the shoulder of the telescope operator to tell him where to point the camera next. Each good observing night fills up Sheppard's Macbook with data. To identify potential distant worlds, the researchers use a programme that Trujillo wrote to pick out objects that move between different frames of the same star field. But because the programme flags a number of false positives, each field must also be reviewed manually. Sheppard goes through every exposure, eyeballing the faint dots that the programme circled in orange to decide whether they represent a distant Solar System object or something else \u2014 an asteroid or a cosmic-ray blip, perhaps. Frame by frame, Sheppard zips through thousands of exposures as if he's playing a video game. \u201cIt's exciting to go through,\u201d he says. \u201cEvery image \u2014 you never know what you're going to get. It could be the image with the super-Earth in it.\u201d The key is how slowly the objects move. Asteroids are relatively close to Earth, and their position in the sky can shift by 30 arcseconds, or 0.008 degrees, each hour. Kuiper belt objects, which are much farther away, traverse about 3 arcseconds of sky each hour. Anything slower than that must be beyond the main Kuiper belt and is something that interests the search team. Astronomers must observe an object multiple times over the course of a year to pin down its orbit and determine its perihelion. Just because an object is remote when it is discovered does not mean that it is scientifically important. For instance, the object that Sheppard spotted at 90  AU  in November may have been at its closest approach to the Sun. If so, that would make it a record-breaker, situated beyond Sedna and 2012 VP 113 . Or the object might be travelling on a path that takes it much, much closer to the Sun, perhaps 40  AU . That would make it less exciting, because its perihelion distance would place it squarely within the main Kuiper belt \u2014 meaning that it is just another ordinary Kuiper belt object, as opposed to one of the extreme worlds. The same is true for an object that the scientists found at 103  AU  last November \u2014  the most distant ever observed . They will not know for many months whether that body stays in the outer Solar System, or whether it veers inward at its perihelion. By far the most prized quarry out there is the hypothesized Planet Nine. In their 2014  Nature  paper, Trujillo and Sheppard suggested \u2014 on the basis of the orbits of 2012 VP 113  and Sedna \u2014 that an unseen super-Earth could lurk at roughly 250  AU . This January, Konstantin Batygin and Mike Brown of Caltech took these two bodies, along with four other distant Kuiper belt objects, and compared their orbits to narrow the calculations of where such a planet might lie. All six objects share a common orbital property: when they pass closest to the Sun, they are travelling from north to south relative to the plane of the Solar System. If they had no relation to one another, they should not all share that orientation. A second line of argument is that the six objects are also physically clustered in space (see 'Out there'). \u201cThey all point in the same direction and are all tilted at the same angle,\u201d says Batygin. \u201cThat's odd.\u201d He and Brown argue that an unseen Planet Nine must be shepherding them into those clusters. It would be between 5 and 10 times the mass of Earth, and travel as close as 200  AU  to the Sun and as far away as 1,200  AU . \n               Finding the ninth \n             Critics say that the argument rests on just a handful of weird Kuiper belt objects. \u201cIt's very small statistics,\u201d says David Nesvorn\u00fd, a planetary scientist at the Southwest Research Institute in Boulder, Colorado, who nonetheless finds the concept intriguing. \u201cIt's as science should be \u2014 at the edge of believability.\u201d Many astronomers are now running their own calculations to estimate the chances that Planet Nine exists in this particular orbit, and if not, where it might be. Samantha Lawler, of National Research Council\u2013Herzberg in Victoria, Canada, is working with Nathan Kaib, of the University of Oklahoma in Norman, to explore how the presence of a super-Earth might affect the orbits of many Kuiper belt objects. Their preliminary results suggest that, if a Planet Nine were out there, it should have nudged the orbits of Kuiper belt objects in ways that do not reflect reality. Planet Nine \u201cis a cool idea, and it would be really neat if it was true\u201d, says Lawler. \u201cBut you have to be really careful.\u201d Some answers may come from an ongoing project known as the Outer Solar System Origins Survey (OSSOS), run by a consortium of investigators. It is working to find and study all the observable Kuiper belt objects in a small patch of the sky in extraordinary detail \u2014 by following their orbits, classifying their colours and so on. That work has the potential to rule out the existence of Batygin and Brown's hypothesized Planet Nine \u2014 if OSSOS were to find a distant object in a region that should have been cleared out by the proposed planet. Other astronomers have suggested alternative ways to hunt Planet Nine, such as looking at data from the Cassini spacecraft orbiting Saturn to see whether that planet's orbit is perturbed ever so slightly, or by using cosmological telescopes at the South Pole to detect a planet's faint radiation. As Sheppard and Trujillo continue their methodical survey of the sky, they are paying special attention to areas where Batygin and Brown say the planet could be. And the Caltech pair is chasing it as well, also using Subaru. \u201cI'd be astonished if there isn't some kind of planet there,\u201d says Renu Malhotra, a theorist at the University of Arizona in Tucson. In a paper on the preprint server arXiv, she and her colleagues put forward a new analysis 4  of where a super-Earth might lurk, on a different orbit from Batygin and Brown's Planet Nine. Malhotra's team uses four extreme Kuiper belt objects to suggest that an unseen planet moves around the Sun every 17,000 years. But even if a large planet is out there, it will take some luck to find it with existing technology. For one of the teams to spot the object, it would have to be on the larger end of its estimated size range, or be very reflective or in a relatively close-in orbit. If the planet is too small, dark and far away, it may never be seen from Earth. \u201cIt's worse than looking for a needle in a haystack,\u201d says Malhotra. \u201cIt's like looking for the broken tip of a needle in a haystack.\u201d \n               The story of a planet \n             A more fundamental question is not whether Planet Nine exists, but what distant objects say about planetary evolution more generally. Discoveries such as Sedna and 2012 VP 113  have forced a radical rethinking of the gravitational forces that shape the outer parts of the Solar System. When astronomers first started to find Kuiper belt objects in the 1990s and recognized that Pluto was just another member of that clan, they began to paint a picture of this mysterious realm of space. The Kuiper belt seemed to extend neatly from about 30  AU  to 50  AU , with most objects following stately orbits around the Sun. Those that were a bit odd \u2014 travelling off-kilter to the plane of the Solar System, or occasionally to greater distances \u2014 could be explained by gravitational interactions with Neptune. Sedna and 2012 VP 113  do not fit that simple model because they range too far from the Sun to have ever interacted much with Neptune. Theorists suddenly had to confront the question of how these objects reached their current orbits. All known planets in the Solar System, along with the Kuiper belt objects, are thought to have condensed from a disk of gas and dust that swirled around the newborn Sun 4.6 billion years ago. But Sedna and other objects beyond the main Kuiper belt probably weren't born where they are today, because there simply wasn't enough gas and dust available at those great distances to create sizeable worlds. One idea is that they were tossed there by a gravitational battle with other protoplanets closer to the Sun during the first tens of millions of years of the Solar System's existence. A second theory holds that the gravity of a passing star tugged on the outer bits of the planet-forming disk, pulling nascent planets into elongated orbits, where they remain today. If Planet Nine exists, it could complicate this picture even more. It would mean that the orbits of Sedna and 2012 VP 113  were not fixed early on but are being actively shaped \u2014 even today \u2014 by the gravitational tugs of Planet Nine. That would require theorists to rewrite their ideas about how the Solar System's many worlds have interacted with one another over the past 4.6 billion years. \u201cIt's hard to anticipate what direction our imaginations will go,\u201d says Malhotra. Understanding the distant Kuiper belt could also help astronomers to work out how our Solar System compares with planetary systems around other stars. Brown notes that one of the most common types of exoplanet is one missing from our Solar System, a world more massive than Earth but less massive than Neptune \u2014 that is, around the range of the hypothesized Planet Nine. \u201cMaybe we can see what this most common type of planet might actually look like,\u201d he says. For now, scientists' best shot at answering these questions is to find more extremely distant worlds. And that is why Sheppard and Trujillo keep plugging away in Chile and Hawaii, having covered less than 10% of the sky that they intend to survey. Back on Mauna Kea, Sheppard pushes through the night of observing, clocking one field after another with no break. By 4:45 a.m., the atmosphere above the summit is turning a little more opaque, and he begins to shift the exposure times longer. Finally, at 5:25 a.m., he turns to the videoconferencing unit and calls to his colleague in Hilo. \u201cChad, are you there?\u201d he asks. \u201cAll the fields are in.\u201d The skies above Subaru are beginning to brighten, although Sheppard does not get to enjoy the spectacular view of a Hawaiian sunrise because he does not step outside. He is busy tallying his 33 fields for the night. Any one of them could contain a new extreme Kuiper belt object \u2014 or even a Planet Nine. It is after 7 a.m. when the observing team tumbles into two sports-utility vehicles and drives the steep, rocky road down from Mauna Kea's summit. Sheppard starts to flag only when he sits down for breakfast at the astronomers' dorm, 1,360 metres lower down on the mountain. He and Tholen gulp down their food and retreat to black-curtained dormitories to sleep until noon. Sheppard, aged 40, has asked his doctor about eye strain and whether he will be able to keep looking at star fields forever. He and Trujillo have a self-appointed goal of finding ten inner-Oort-cloud objects, a number that they think will enable them to start testing ideas for how these objects formed and evolved. That means many more long nights at the telescope. \u201cIf it turns into postage-stamp collecting, we'll stop,\u201d Sheppard says. \u201cBut right now, every new discovery is a huge difference-maker in trying to understand what's going on out there.\u201d\n \n                 Tweet \n                 Facebook \n                 LinkedIn \n                 Weibo \n               \n                     Evidence grows for giant planet on fringes of Solar System 2016-Jan-20 \n                   \n                     Astronomers spy most distant Solar System object ever 2015-Nov-10 \n                   \n                     Dwarf planet stretches Solar System's edge 2014-Mar-26 \n                   \n                     Dwarf planet found to be heftier than Pluto 2007-Jun-14 \n                   \n                     Extreme outer solar system objects \n                   \n                     Find Planet Nine \n                   \n                     Subaru Telescope \n                   Reprints and Permissions"},
{"file_id": "530268a", "url": "https://www.nature.com/articles/530268a", "year": 2016, "authors": [{"name": "Douglas Fox"}], "parsed_as_year": "2006_or_before", "body": "An evolutionary burst 540 million years ago filled the seas with an astonishing diversity of animals. The trigger behind that revolution is finally coming into focus. A series of dark, craggy pinnacles rises 80 metres above the grassy plains of Namibia. The peaks call to mind something ancient \u2014 the burial mounds of past civilizations or the tips of vast pyramids buried by the ages. The stone formations are indeed monuments of a faded empire, but not from anything hewn by human hands. They are pinnacle reefs, built by cyanobacteria on the shallow sea floor 543 million years ago, during a time known as the Ediacaran period. The ancient world occupied by these reefs was truly alien. The oceans held so little oxygen that modern fish would quickly founder and die there. A gooey mat of microbes covered the sea floor at the time, and on that blanket lived a variety of enigmatic animals whose bodies resembled thin, quilted pillows. Most were stationary, but a few meandered blindly over the slime, grazing on the microbes. Animal life at this point was simple, and there were no predators. But an evolutionary storm would soon upend this quiet world. Within several million years, this simple ecosystem would disappear, and give way to a world ruled by highly mobile animals that sported modern anatomical features. The Cambrian explosion, as it is called, produced arthropods with legs and compound eyes, worms with feathery gills and swift predators that could crush prey in tooth-rimmed jaws. Biologists have argued for decades over what ignited this evolutionary burst. Some think that a steep rise in oxygen sparked the change, whereas others say that it sprang from the development of some key evolutionary innovation, such as vision. The precise cause has remained elusive, in part because so little is known about the physical and chemical environment at that time. But over the past several years, discoveries have begun to yield some tantalizing clues about the end of the Ediacaran. Evidence gathered from the Namibian reefs and other sites suggests that earlier theories were overly simplistic \u2014 that the Cambrian explosion actually emerged out of a complex interplay between small environmental changes that triggered major evolutionary developments. Some scientists now think that a small, perhaps temporary, increase in oxygen suddenly crossed an ecological threshold, enabling the emergence of predators. The rise of carnivory would have set off an evolutionary arms race that led to the burst of complex body types and behaviours that fill the oceans today. \u201cThis is the most significant event in Earth evolution,\u201d says Guy Narbonne, a palaeobiologist at Queen's University in Kingston, Canada. \u201cThe advent of pervasive carnivory, made possible by oxygenation, is likely to have been a major trigger.\u201d \n               Energy to burn \n             In the modern world, it's easy to forget that complex animals are relative newcomers to Earth. Since life first emerged more than 3 billion years ago, single-celled organisms have dominated the planet for most of its history. Thriving in environments that lacked oxygen, they relied on compounds such as carbon dioxide, sulfur-containing molecules or iron minerals that act as oxidizing agents to break down food. Much of Earth's microbial biosphere still survives on these anaerobic pathways. Animals, however, depend on oxygen \u2014 a much richer way to make a living. The process of metabolizing food in the presence of oxygen releases much more energy than most anaerobic pathways. Animals rely on this potent, controlled combustion to drive such energy-hungry innovations as muscles, nervous systems and the tools of defence and carnivory \u2014 mineralized shells, exoskeletons and teeth. Given the importance of oxygen for animals, researchers suspected that a sudden increase in the gas to near-modern levels in the ocean could have spurred the Cambrian explosion. To test that idea, they have studied ancient ocean sediments laid down during the Ediacaran and Cambrian periods, which together ran from about 635 million to 485 million years ago. In Namibia, China and other spots around the world, researchers have collected rocks that were once ancient seabeds, and analysed the amounts of iron, molybdenum and other metals in them. The metals' solubility depends strongly on the amount of oxygen present, so the amount and type of those metals in ancient sedimentary rocks reflect how much oxygen was in the water long ago, when the sediments formed. These proxies seemed to indicate that oxygen concentrations in the oceans rose in several steps, approaching today's sea-surface concentrations at the start of the Cambrian, around 541 million years ago \u2014 just before more-modern animals suddenly appeared and diversified. This supported the idea of oxygen as a key trigger for the evolutionary explosion. But last year, a major study 1  of ancient sea-floor sediments challenged that view. Erik Sperling, a palaeontologist at Stanford University in California, compiled a database of 4,700 iron measurements taken from rocks around the world, spanning the Ediacaran and Cambrian periods. He and his colleagues did not find a statistically significant increase in the proportion of oxic to anoxic water at the boundary between the Ediacaran and the Cambrian. \u201cAny oxygenation event must have been far, far smaller than what people normally considered,\u201d concludes Sperling. Most people assume \u201cthat the oxygenation event essentially raised oxygen to essentially modern-day levels. And that probably wasn't the case\u201d, he says. The latest results come at a time when scientists are already reconsidering what was happening to ocean oxygen levels during this crucial period. Donald Canfield, a geobiologist at the University of Southern Denmark in Odense, doubts that oxygen was a limiting factor for early animals. In a study published last month 2 , he and his colleagues suggest that oxygen levels were already high enough to support simple animals, such as sponges, hundreds of millions of years before they actually appeared. Cambrian animals would have needed more oxygen than early sponges, concedes Canfield. \u201cBut you don't need an increase in oxygen across the Ediacaran/Cambrian boundary,\u201d he says; oxygen could already have been abundant enough \u201cfor a long, long time before\u201d. \u201cThe role of oxygen in the origins of animals has been heavily debated,\u201d says Timothy Lyons, a geobiologist at the University of California, Riverside. \u201cIn fact, it's never been more debated than it is now.\u201d Lyons sees a role for oxygen in evolutionary changes, but his own work 3  with molybdenum and other trace metals suggests that the increases in oxygen just before the Cambrian were mostly temporary peaks that lasted a few million years and gradually stepped upward (see 'When life sped up'). \n               Modern mirrors \n             Sperling has looked for insights into Ediacaran oceans by studying oxygen-depleted regions in modern seas around the globe. He suggests that biologists have conventionally taken the wrong approach to thinking about how oxygen shaped animal evolution. By pooling and analysing previously published data with some of his own, he found that tiny worms survive in areas of the sea floor where oxygen levels are incredibly low \u2014 less than 0.5% of average global sea-surface concentrations. Food webs in these oxygen-poor environments are simple, and the animals feed directly on microbes. In places where sea-floor oxygen levels are a bit higher \u2014 about 0.5\u20133% of concentrations at the sea surface \u2014 animals are more abundant but their food webs remain limited: the animals still feed on microbes rather than on each other. But around somewhere between 3% and 10% oxygen levels, predators emerge and start to consume other animals 4 . The implications of this finding for evolution are profound, Sperling says.The modest oxygen rise that he thinks may have occurred just before the Cambrian would have been enough to trigger a big change. \u201cIf oxygen levels were 3% and they rose past that 10% threshold, that would have had a huge influence on early animal evolution,\u201d he says. \u201cThere's just so much in animal ecology, lifestyle and body size that seems to change so dramatically through those levels.\u201d The gradual emergence of predators, driven by a small rise in oxygen, would have meant trouble for Ediacaran animals that lacked obvious defences. \u201cYou're looking at soft-bodied, mostly immobile forms that probably lived their lives by absorbing nutrients through their skin,\u201d says Narbonne. Studies of those ancient Namibian reefs suggest that animals were indeed starting to fall prey to predators by the end of the Ediacaran. When palaeobiologist Rachel Wood from the University of Edinburgh, UK, examined the rock formations, she found spots where a primitive animal called C loudina  had taken over parts of the microbial reef. Rather than spreading out over the ocean floor, these cone-shaped creatures lived in crowded colonies, which hid their vulnerable body parts from predators \u2014 an ecological dynamic that occurs in modern reefs 5 . C loudina  were among the earliest animals known to have grown hard, mineralized exoskeletons. But they were not alone. Two other types of animal in those reefs also had mineralized parts, which suggests that multiple, unrelated groups evolved skeletal shells around the same time. \u201cSkeletons are quite costly to produce,\u201d says Wood. \u201cIt's very difficult to come up with a reason other than defence for why an animal should bother to create a skeleton for itself.\u201d Wood thinks that the skeletons provided protection against newly evolved predators. Some C loudina  fossils from that period even have holes in their sides, which scientists interpret as the marks of attackers that bore into the creatures' shells 6 . Palaeontologists have found other hints that animals had begun to eat each other by the late Ediacaran. In Namibia, Australia and Newfoundland in Canada, some sea-floor sediments have preserved an unusual type of tunnel made by an unknown, wormlike creature 7 . Called  Treptichnus  burrows, these warrens branch again and again, as if a predator just below the microbial mat had systematically probed for prey animals on top. The  Treptichnus  burrows resemble those of modern priapulid, or 'penis', worms \u2014 voracious predators that hunt in a remarkably similar way on modern sea floors 8 . The rise of predation at this time put large, sedentary Ediacaran animals at a big disadvantage. \u201cSitting around doing nothing becomes a liability,\u201d says Narbonne. \n               The world in 3D \n             The moment of transition from the Ediacaran to the Cambrian world is recorded in a series of stone outcrops rounded by ancient glaciers on the south edge of Newfoundland. Below that boundary are impressions left by quilted Ediacaran animals, the last such fossils recorded on Earth. And just 1.2 metres above them, the grey siltstone holds trails of scratch marks, thought to have been made by animals with exoskeletons, walking on jointed legs \u2014 the earliest evidence of arthropods in Earth's history. No one knows how much time passed in that intervening rock \u2014 maybe as little as a few centuries or millennia, says Narbonne. But during that short span, the soft-bodied, stationary Ediacaran fauna suddenly disappeared, driven to extinction by predators, he suggests. Narbonne has closely studied the few fauna that survived this transition, and his findings suggest that some of them had acquired new, more complex types of behaviour. The best clues come from traces left by peaceful, wormlike animals that grazed on the microbial mat. Early trails from about 555 million years ago meander and criss-cross haphazardly, indicating a poorly developed nervous system that was unable to sense or react to other grazers nearby \u2014 let alone predators. But at the end of the Ediacaran and into the early Cambrian, the trails become more sophisticated: creatures carved tighter turns and ploughed closely spaced, parallel lines through the sediments. In some cases, a curvy feeding trail abruptly transitions into a straight line, which Narbonne interprets as potential evidence of the grazer evading a predator 9 . This change in grazing style may have contributed to the fragmentation of the microbial mat, which began early in the Cambrian. And the transformation of the sea floor, says Narbonne, \u201cmay have been the most profound change in the history of life on Earth\u201d 10 , 11 . The mat had previously covered the seabed like a coating of plastic wrap, leaving the underlying sediments largely anoxic and off limits to animals. Because animals could not burrow deeply in the Ediacaran, he says, \u201cthe mat meant that life was two-dimensional\u201d. When grazing capabilities improved, animals penetrated the mat and made the sediments habitable for the first time, which opened up a 3D world. Tracks from the early Cambrian show that animals started to burrow several centimetres into the sediments beneath the mat, which provided access to previously untapped nutrients \u2014 as well as a refuge from predators. It's also possible that animals went in the opposite direction. Sperling says that the need to avoid predators (and pursue prey) may have driven animals into the water column above the seabed, where enhanced oxygen levels enabled them to expend energy through swimming. The emerging evidence about oxygen thresholds and ecology could also shed light on another major evolutionary question: when did animals originate? The first undisputed fossils of animals appear only 580 million years ago, but genetic evidence indicates that basic animal groups originated as far back as 700 million to 800 million years ago. According to Lyons, the solution may be that oxygen levels rose to perhaps 2% or 3% of modern levels around 800 million years ago. These concentrations could have sustained small, simple animals, just as they do today in the ocean's oxygen-poor zones. But animals with large bodies could not have evolved until oxygen levels climbed higher in the Ediacaran. Understanding how oxygen influenced the appearance of complex animals will require scientists to tease more-subtle clues out of the rocks. \u201cWe've been challenging people working on fossils to tie their fossils more closely to our oxygen proxies,\u201d says Lyons. It will mean deciphering what oxygen levels were in different ancient environments, and connecting those values with the kinds of traits exhibited by the animal fossils found in the same locations. This past autumn, Woods visited Siberia with that goal in mind. She collected fossils of  Cloudina  and another skeletonized animal,  Suvorovella , from the waning days of the Ediacaran. Those sites gave her the chance to gather fossils from many different depths in the ancient ocean, from the more oxygen-rich surface waters to deeper zones. Wood plans to look for patterns in where animals were growing tougher skeletons, whether they were under attack by predators and whether any of this had a clear link with oxygen levels, she says. \u201cOnly then can you pick out the story.\u201d\n \n                 Tweet \n                 Facebook \n                 LinkedIn \n                 Weibo \n               \n                     Earliest skeletal animals were reef builders 2014-Jun-26 \n                   \n                     Cambrian super-predators grew large in arms race 2011-May-25 \n                   \n                     Cambrian explosion changed ocean chemistry 2009-May-18 \n                   \n                     News and Views Forum \n                   Reprints and Permissions"},
{"file_id": "530402a", "url": "https://www.nature.com/articles/530402a", "year": 2016, "authors": [{"name": "Erika Check Hayden"}], "parsed_as_year": "2006_or_before", "body": "In the fierce debate about CRISPR gene editing, it\u2019s time to give patients a voice. Ruthie Weiss\u2019s basketball team seemed to be minutes away from its fourth straight loss. But even as she stood on the sidelines for a brief rest, the nine-year-old had not given up. She convinced the coach to put her back in the game. Then, she charged out onto the court, caught a pass from a teammate and drove straight to the basket. Swish! Ruthie scored a quick two points, putting her team in the lead. As the game clock wound down, she scored again, clinching the victory. The team had earned its first win of the season, and celebrated as if it had just taken the national championship. A couple of parents from the opposing team even stopped by to congratulate Ruthie, who had scored all of her team\u2019s 13 points: \u201cWow, she\u2019s unbelievable!\u201d they told her mum and dad. How do those with disabilities feel about the possibility of editing the genomes of future generations? What makes Ruthie\u2019s performance even more extraordinary is her DNA. Because of a misspelling in one of her genes, she has albinism: her body produces very little of the pigment melanin, which means that her skin and hair are fair, and that she is legally blind. Her visual acuity is ten times worse than average. She is still learning to read and will probably never be able to drive a car, but she can make out the basket and her teammates well enough to shoot, pass and play. In January, Ruthie\u2019s dad Ethan asked her whether she wished that her parents had corrected the gene responsible for her blindness before she was born. Ruthie didn\u2019t hesitate before answering\u00a0\u2014 no. Would she ever consider editing the genes of her own future children to help them to see? Again, Ruthie didn\u2019t blink\u00a0\u2014 no. The answer made Ethan Weiss, a physician\u2013scientist at the University of California, San Francisco, think. Weiss is well aware of the rapid developments in gene-editing technologies\u00a0\u2014 techniques that could, theoretically, prevent children from being born with deadly disorders or with disabilities such as Ruthie\u2019s. And he believes that if he had had the option to edit blindness out of Ruthie\u2019s genes before she was born, he and his wife would have jumped at the chance. But now he thinks that would have been a mistake: doing so might have erased some of the things that make Ruthie special\u00a0\u2014 her determination, for instance. Last season, when Ruthie had been the worst player on her basketball team, she had decided on her own to improve, and unbeknownst to her parents had been practising at every opportunity. Changing her disability, he suspects, \u201cwould have made us and her different in a way that we would have regretted\u201d, he says. \u201cThat\u2019s scary.\u201d Ethan and Ruthie are not the only people pondering these kinds of questions. The emergence of a powerful gene-editing technology, known as CRISPR\u2013Cas9, has elicited furious debate about whether and how it might be used to modify the genomes of human embryos. The changes to their genomes would almost certainly be passed down to subsequent generations, breaching an ethical line that has  typically been considered uncrossable . But emerging technologies are already testing the margins of what people deem acceptable. Parents today have unprecedented control over what they pass on to their children: they can use prenatal genetic screening to check for conditions such as Down\u2019s syndrome, and choose whether or not to carry a fetus to term. Preimplantation genetic diagnosis allows couples undergoing  in vitro  fertilization to select embryos that do not have certain disease-causing mutations. Even altering the heritable genome\u00a0\u2014 as might be done if CRISPR were used to edit embryos \u2014 is acceptable to some. Mitochondrial replacement therapy, which replaces a very small number of genes that a mother passes on with those from a donor, was  approved last year in the United Kingdom  for people who are at risk of certain genetic disorders. Many safety, technical and legal barriers still stand in the way of editing DNA in human embryos. But some scientists and ethicists say that it is important to think through the implications of embryo editing now\u00a0\u2014 before these practical hurdles are overcome. What sort of world would these procedures create for those currently living with disease and for future generations? So far, little has been heard from the people who could be first affected by the technology\u00a0\u2014 but speaking with these communities reveals a diverse set of views. Some are impatient, and say that there is a duty to use genome editing quickly to eliminate serious, potentially fatal conditions. Some doubt that society will embrace it to the degree that many have feared, or hoped. Above all, people such as Ethan Weiss caution that if policymakers do not consult people with disabilities and their families, the technology could be used unthinkingly, in ways that harm patients and society, today and in the future. \u201cHearing the voices of people who live with these conditions is really important,\u201d says Tom Shakespeare, a medical sociologist at the University of East Anglia in Norwich, UK. \n               The cases for \n             John Sabine, now 60, was once described as one of the brightest legal minds of his generation in England. Now, he is in the advanced stages of Huntington\u2019s disease: he cannot walk or talk, is incontinent and requires constant care. Charles Sabine, his younger brother, carries the same genetic glitch that causes Huntington\u2019s disease, and therefore knows that, like his brother and his father before him, he is destined to undergo the same deterioration of brain and body. Charles and his brother have five children between them, each of whom as a 50% chance of having inherited the mutation that causes Huntington\u2019s disease. To Charles\u00a0\u2014 and to many others who live with the mutation that causes Huntington\u2019s\u00a0\u2014\u00a0there is no legitimate ethical argument about whether gene editing should be used, either to treat people living with the condition now or to spare their children from it. \u201cAnyone who has to actually face the reality of one of these diseases is not going to have a remote compunction about thinking that there is any moral issue at all,\u201d Sabine says. \u201cIf there was a room somewhere where someone said, \u2018Look, you can go in there and have your DNA changed,\u2019 I would be there breaking the door down.\u201d Matt Wilsey, a technology entrepreneur in San Francisco, would be there too. His daughter Grace was one of the first people in the world to be diagnosed with a disease caused by a mutation in the gene  NGLY1 , which makes it difficult for her cells to get rid of misshapen proteins. Grace, now six years old, has severe movement and developmental disabilities. She can barely walk and cannot talk. Because her condition is new to medicine, doctors cannot even predict how long she might live. Wilsey is bullish on CRISPR. He says that if he had had the chance to detect and fix the mutation in Grace\u2019s genome before she was born, he would have. But he is frustrated that the debate over editing embryos seems to have monopolized discussions about the technology. He is hopeful that a gene-therapy-like approach using CRISPR, which would be free of the ethical concerns about altering the genes she passes on, could help Grace within several years. And he wonders whether a temporary moratorium on embryo editing might allow the field to focus on such approaches sooner. \u201cAs a parent with an incredibly sick child, what are we supposed to do\u00a0\u2014 sit by on the sidelines while my child dies? There\u2019s zero chance of that,\u201d Wilsey says. \u201cCRISPR is a bullet train that has left the station\u00a0\u2014\u00a0there\u2019s no stopping it, so how can we harness it for good?\u201d A meeting convened in December 2015 by the US national academies of sciences and medicine, the Chinese Academy of Sciences and the Royal Society of London  recommended such a moratorium  in light of multiple safety and ethical concerns. Still, many bioethicists and scientists have argued that if defects in single genes causing fatal and debilitating conditions could be corrected in an embryo, then they should be. Shakespeare notes that embryo editing for conditions that cause major disability and death are likely to raise less concern and criticism in the long term. But, he says: \u201cAs soon as you get away from the archetypal terrible condition, then you\u2019ve got a debate about whether a condition makes life unbearably hard.\u201d \n               Social consequences \n             Many people are concerned about where that line would be drawn. Although it may seem now that only a few, very severe conditions should be subject to gene editing, disability activists point out that the list of conditions considered as illnesses, and possibly subject to medical treatment, is expanding. \u201cMore and more, people think of obesity or predisposition to alcoholism as disease,\u201d says Carol Padden, a linguist at the University of California, San Diego. She herself is deaf, and points out that many deaf people do not consider it a disability. This stance has led to controversy when, for instance, deaf parents decline technologies such as cochlear implants for their children, or even go so far as to select, through processes such as preimplantation genetic diagnosis, children who will be deaf. Like Padden, some disability-studies researchers do not oppose the idea of gene editing, but do think that society needs to understand that it is not possible to eliminate all disability, and that humans might lose something important if they try to do so. Padden points out that accommodations originally intended for people with disabilities often end up benefiting everyone. For example, the development of closed captioning\u00a0\u2014 subtitles for the hearing-impaired on television \u2014 required major advocacy from the deaf community and legislative action to get off the ground in the United States in the 1970s. Today, people rely on it in ways that no one could have foreseen, such as in noisy airports and sports bars. Some people use it to learn to read or to learn a language. Rosemarie Garland-Thomson, a literature scholar and co-director of the Disability Studies Initiative at Emory University in Atlanta, Georgia, adds that legislative mandates, such as the 1990 Americans with Disabilities Act in the United States, have helped to integrate people with disabilities into society\u00a0\u2014 in workplaces, schools and other public spaces. As a result, the world is much more humane for everyone, says Garland-Thomson. \u201cThese kinds of interactions significantly change our attitudes about what kinds of people matter in the world.\u201d The idea that parents should edit out characteristics that are considered debilitating goes against this drive towards inclusion, Garland-Thomson warns, and could create a harsher social climate for everyone. The experience of disability, she adds, is universal; all people inevitably experience sickness, accidents and age-related decline. \u201cAt our peril, we are right now trying to decide what ways of being in the world ought to be eliminated,\u201d she says. Padden says that ethicists, patients and disabilities-studies researchers must work urgently to make a broad societal case in favour of greater acceptance of diversity. This has been a long-fought battle, and many see evidence of progress\u00a0\u2014 for instance, in the \u2018neurodiversity\u2019 movement, which champions the idea that medical conditions such as autism are part of the spectrum of human variation. \u201cWe do have to start coming up with more arguments for diversity, and quickly, because CRISPR is coming upon us faster than some of us are thinking about this issue,\u201d she says. \n               Making choices \n             The prospect of editing the genome of a human embryo is still in its early stages, but the ability to prevent the inheritance of some conditions already exists. Prenatal screening, which has advanced to the point that doctors can sample a developing fetus\u2019s DNA through its mother\u2019s blood, has given parents the option to terminate pregnancies when a disease or disability is diagnosed. This has already started to show limited effects on the population. In Europe, for example, the prevalence of a Down\u2019s syndrome diagnosis during pregnancy has risen from 20 cases per 10,000 in 1990 to 23 cases per 10,000 today, as the average age of women having babies has increased. But the number of children born with the syndrome has stayed level at about 11 per 10,000, because many women whose fetuses are diagnosed with the condition terminate their pregnancies. In the United States, pregnancies in which a Down\u2019s syndrome diagnosis is made are terminated in 67\u201385% of cases. By surveying women whose fetuses and babies are diagnosed with Down\u2019s syndrome, and by compiling similar surveys from around the world, medical geneticist Brian Skotko of the Massachusetts General Hospital in Boston has found that doctors sometimes advise women to terminate or give up for adoption babies diagnosed prenatally with Down\u2019s syndrome. They can influence the decision by using phrases such as \u201cI\u2019m sorry\u201d, or \u201cI have some bad news to share\u201d 1 . For instance, 34% of 71 Dutch women who terminated their pregnancy after a Down\u2019s syndrome diagnosis said that their doctors did not even mention the possibility of carrying the pregnancy to term when discussing their options 2 . Mark Leach, a lawyer in Louisville, Kentucky, whose 11-year-old daughter has Down\u2019s syndrome, says that he and his wife have been asked many times\u00a0\u2014 especially when his wife was pregnant with their second child\u00a0\u2014 whether they \u201cknew beforehand\u201d that Juliet would be born with Down\u2019s. (They didn\u2019t.) Some people are simply curious, Leach says, but for others, there\u2019s judgement in that question. \u201cThe ability to do something beforehand imposes a sense of, \u2018You should do not only what\u2019s right for you, but what\u2019s right for society\u2019,\u201d Leach says. It bothers him, he says, that although government and private health insurers routinely pay for prenatal diagnosis, he recently learned that his school system is discontinuing support for the learning specialist who had been helping Juliet to thrive in mathematics and reading. Dorothy Roberts, a professor of law and sociology at the University of Pennsylvania in Philadelphia, says that this kind of pressure is troubling and that it could get worse if embryo editing were to become readily available. \u201cWomen should not be given the responsibility of ensuring the genetic fitness of their children based on lack of support for children with disabilities.\u201d Leach knows that children with disabilities can live rich lives. Juliet likes ballet dancing and horse-riding, and she is especially attuned to the names of people and animals whom she knows. And Leach says that she helps to remind other people how to care for others. \u201cThe main thing that would be lost if Down\u2019s syndrome continues to diminish is a diminishment in the amount of compassion that is shown in this world,\u201d he says. Even among people who already have life-threatening conditions, many choose not to interfere with the way the genetic cards are dealt. Edward Wild, a neurologist who cares for people with Huntington\u2019s disease at University College London, estimates that fewer than 5% of patients in the United Kingdom use preimplantation genetic screening to select embryos that lack the disease-causing mutation and so avoid passing it to their children. Some people do not know that they have the mutation; some decide against screening because of the costs or risks involved; some have personal or moral objections to the technique; and some just have a sense that 50\u201350 odds of passing down the disease are not so bad. \u201cHaving kids the fun way is still much more popular than having kids the science way, even though the latter is how you guarantee the kid is free of Huntington\u2019s disease,\u201d Wild says. Even if gene editing were safe, effective and everyone opted to use it, it would not eliminate genetic diseases, because researchers still have a long way to go to understand the genes involved. Even Huntington\u2019s, which is fairly well characterized, is no easy target. The glitch that causes it is a repeat of a particular genetic sequence; the more repeats, the more severe the symptoms, and repeats are added with each successive generation. New families are diagnosed with Huntington\u2019s all the time, either because the disease is misdiagnosed in older generations or because symptoms worsen, and become recognizable, in subsequent generations. Although he is working on genetic techniques to treat Huntingon\u2019s, Wild doesn\u2019t hold out high hopes for a future free of the disease. \u201cAlthough it\u2019s nice to think about, it\u2019s little more than a dream,\u201d he says. Human biology can complicate things in other ways. Padden notes, for instance, that some mutations that predispose to genetic disease, such as the sickle-cell mutation, confer population-level benefits, such as resistance to malaria. So editing out one disease could backfire by increasing the risk of another. She argues that very little is known about the potential benefits of other mutations associated with disease, and applying genome editing too freely could have unintended consequences. And if it were adopted, the technology would almost certainly be applied unevenly around the world. Aleksa Owen, a sociologist at the University of Illinois at Chicago, predicts that genome editing  would be used first in countries  that approve of and support assisted reproductive technologies, such as the United Kingdom, some other European Union countries, China and Israel. But it would probably be too expensive for many people in developing countries. \n               Uneven access \n             Still, some scientists predict that editing human embryos could have transformative effects. During the National Academies\u2019 summit on gene editing in December, Harvard University geneticist Dan MacArthur tweeted, \u201cPrediction: my grandchildren will be embryo-screened, germline-edited. Won\u2019t \u2018change what it means to be human\u2019. It\u2019ll be like vaccination.\u201d Sandy Sufian, a historian of medicine and disability at the University of Illinois, agrees with MacArthur that CRISPR has the potential to become widely adopted, both because of the perception that it would save money that would otherwise be spent caring for disabled people and because of people\u2019s fear of disability. But she questions the idea that eliminating such conditions will necessarily improve human life. Sufian has cystic fibrosis, a disease caused by mutations that render her lung cells more vulnerable to infection and disease. She spends 40 hours a week inhaling medicine to clear her lungs of mucus, exercising and undergoing physical therapy; others have to quit their jobs to make sufficient time for treatments. Yet given the option to edit cystic fibrosis out of her bloodline, Sufian wouldn\u2019t do it. \u201cThere are some great things that come from having a genetic illness,\u201d she says. Garland-Thomson echoes that sentiment; she has one and a half arms and six fingers because of a condition called limb-reduction disorder. She says that she values traits in herself that she may have developed as adaptations to the condition: she is very sociable and wonders if that is because she\u2019s had to learn to work hard to make others feel comfortable around her. \u201cAny kinds of restrictions or limitations have created the opportunity for me to develop work-arounds,\u201d Garland-Thomson says. Shakespeare, who has achondroplasia, a genetic condition that causes shorter than average stature, says that people with disabilities are just as able to attain life satisfaction as others. \u201cI have achieved everything I hoped for in life, despite having restricted growth: career, children, friendship and love.\u201d He wouldn\u2019t want to have altered his own genes to be taller, he says. \n               Disability rights \n             People without disabilities consistently underestimate the life satisfaction of those with them. Although people with disabilities report a slightly lower overall quality of life than those without, the difference is small. One study 3  found that half of people with serious disabilities ranked their quality of life as \u2018good\u2019 or \u2018excellent\u2019. People also overestimate how severely health affects their happiness compared with other factors, such as economic or social support. One 1978 study 4 , for instance, compared people who had recently become paralysed as a result of accidents with people who had recently won between US$50,000 and $1\u00a0million in a state lottery. Although people who had had accidents ranked their happiness lower than lottery winners, both groups predicted that their future happiness would be roughly equal, and people who had accidents derived more pleasure from everyday activities, such as eating breakfast or talking to a friend. \u201cA lot of this terrific science and technology has to take into account that the assumption of what life is like for people who are different is based on prejudice against disability,\u201d says Lennard Davis, a disability-studies researcher at the University of Illinois, who was raised by two deaf parents. There is a common saying among people in the disability-rights community: \u201cNothing about us without us.\u201d People with disabilities argue that scientists, policymakers and bioethicists should take steps to ensure that the CRISPR debate reflects what is best for patients and their families, to ensure its most humane use now and for future generations. At a minimum, they say, the investment in developing CRISPR should be matched by investments in innovations to help people who are already living with conditions that cause disability. And it is essential that people with the conditions that are up for consideration as possible CRISPR targets should be included in the decision-making processes. For their part, Ruthie Weiss and her dad have already made up their minds. Ruthie must work harder than her classmates to do some routine activities. But when she is dominating the basketball court, or practising the piano, or skiing down a mountain, Ethan Weiss doesn\u2019t see a child with a disability. He sees his daughter making the most of her life, given all her strengths and challenges. And he knows that he wouldn\u2019t change a thing.\n \n                 Tweet \n                 Facebook \n                 LinkedIn \n                 Weibo \n               \n                     A world where everyone has a robot: why 2040 could blow your mind 2016-Feb-24 \n                   \n                     Sustainability: Game human nature 2016-Feb-24 \n                   \n                     Can today\u2019s decisions really be future-proofed? 2016-Feb-22 \n                   \n                     UK scientists gain licence to edit genes in human embryos 2016-Feb-01 \n                   \n                     Genome-editing revolution: My whirlwind year with CRISPR 2015-Dec-22 \n                   \n                     Where in the world could the first CRISPR baby be born? 2015-Oct-13 \n                   \n                     CRISPR: Science can't solve it 2015-Jun-23 \n                   \n                     CRISPR, the disruptor 2015-Jun-03 \n                   \n                     Nature  special: CRISPR \n                   Reprints and Permissions"},
{"file_id": "530148a", "url": "https://www.nature.com/articles/530148a", "year": 2016, "authors": [{"name": "Kendall Powell"}], "parsed_as_year": "2006_or_before", "body": "Scientists are becoming increasingly frustrated by the time it takes to publish a paper. Something has to change, they say. When Danielle Fraser first submitted her paper for publication, she had little idea of the painful saga that lay ahead. She had spent some 18 months studying thousands of fossil species spread across North America from the past 36 million years, and now she had an intriguing result: animal populations were spread widest across latitudes in warm, wet climates. Her work, crucial to earning her PhD at Carleton University in Ottawa, Canada, might be used to make predictions about the response of mammals to climate change \u2014 a key question in ecology today. So, with her PhD adviser's encouragement, she sent it to  Science  in October 2012. Ten days later, the paper was rejected with a form letter. She sent it to another prestigious journal, the  Proceedings of the National Academy of Sciences . Rejected. Next, she tried  Ecology Letters . Bounced. \u201cAt this point, I definitely was frustrated. I hadn't even been reviewed and I would've loved to know how to improve the paper,\u201d recalls Fraser. \u201cI thought, 'Let's just get it out and go to a journal that will assess the paper'.\u201d In May 2013, she submitted the paper to  Proceedings of the Royal Society B , considered a high-impact journal in her field. The journal sent it out for review \u2014 seven months after her initial submission to  Science . \u201cFinally!\u201d Fraser thought. What she didn't know was that she had taken only the first steps down the long, bumpy road to publication: it would take another three submissions, two rejections, two rounds of major revisions and numerous drafts before the paper would finally appear. By that point, she could hardly bear to look at it. Fraser's frustration is widely shared: researchers are increasingly questioning the time it takes to publish their work. Many say that they feel trapped in a cycle of submission, rejection, review, re-review and re-re-review that seems to eat up months of their lives, interfere with job, grant and tenure applications and slow down the dissemination of results. In 2012, Leslie Vosshall, a neuroscientist at the Rockefeller University in New York City, wrote a commentary that lamented the \u201cglacial pace\u201d of scientific publishing 1 . \u201cIn the past three years, if anything, it's gotten substantially worse,\u201d she says now. \u201cIt takes forever to get the work out, regardless of the journal. It just takes far too long.\u201d But is the publication process actually becoming longer \u2014 and, if so, then why? To find out,  Nature  examined some recent analyses on time to publication \u2014 many of them performed by researchers waiting for their own work to see the light of day \u2014 and spoke to scientists and editors about their experiences. The results contain some surprises. Daniel Himmelstein, a computational-biology graduate student at the University of California, San Francisco,  analysed all the papers indexed in the PubMed database that had listed submission and acceptance dates . His study, done for  Nature , found no evidence for lengthening delays 2 : the median review time \u2014 the time between submission and acceptance of a paper \u2014 has hovered at around 100 days for more than 30 years (see 'Paper wait'). But the analysis comes with major caveats. Not all journals \u2014 including some high-profile ones \u2014 deposit such time-stamp data in PubMed, and some journals show when a paper was resubmitted, rather than submitted for the first time. \u201cResetting the clock is an especially pernicious issue,\u201d Himmelstein says, and it means that the analysis might be underestimating publication delays. Some data suggest that wait times have increased within certain subsets of journals, such as popular open-access ones and some of the most sought-after titles. At  Nature , the median review time has grown from 85 days to just above 150 days over the past decade, according to Himmelstein's analysis, and at  PLoS ONE  it has risen from 37 to 125 days over roughly the same period. Many scientists find this odd, because they expect advances in digital publishing and the proliferation of journals to have sped things up. They say that journals are taking too long to review papers and that reviewers are requesting more data, revisions and new experiments than they used to. \u201cWe are demanding more and more unreasonable things from each other,\u201d says Vosshall. Journal editors counter that science itself has become more data-rich, that they work to uphold high editorial and peer-review standards and that some are dealing with increasing numbers of papers. They also say that they are taking steps to expedite the process. Publication practices and waiting times also vary widely by discipline \u2014 with social sciences being notoriously slow. In physics, the pressure to publish fast is reduced because of the common practice of publishing preprints \u2014 early versions of a paper before peer review \u2014  on the arXiv server . Some of the loudest complaints about publication delays come from those in biological fields, in which competition is fierce and publishing in prestigious journals can be required for career advancement. This month, a group of more than 70 scientists, funders, journal editors and publishers are meeting at the Howard Hughes Medical Institute campus in Chevy Chase, Maryland, to discuss whether biologists should adopt the  preprint model to accelerate publishing . \u201cWe need a fundamental rethinking of  how we do this ,\u201d Vosshall says. \n               The pitch \n             In March 2012, Stephen Royle, a cell biologist at the University of Warwick, UK, started on a publication mission of his own. His latest work answered a controversial question about how cells sense that chromosomes are lined up before dividing, so he first sent it to  Nature Cell Biology  ( NCB ), because it is a top journal in his field and an editor there had suggested he send it after hearing Royle give a talk. It was rejected without review. Next, he sent it to  Developmental Cell . Rejected. His next stop, the  Journal of Cell Biology , sent the paper out for review. It came back with a long list of necessary revisions \u2014 and a rejection. Royle and his lab spent almost six months doing the suggested experiments and revising the paper. Then he submitted the updated manuscript to  Current Biology . Rejected.  EMBO Journal . Reviewed and rejected. Finally, in December 2012, he submitted it to the  Journal of Cell Science  ( JCS ), where it was reviewed. One reviewer mentioned that they had already assessed it at another journal and thought that it should have been published then. They wrote that the work was \u201cbeautifully conducted, well controlled, and conservatively interpreted\u201d. A second reviewer said that it should not be published. The editor at  JCS  decided to accept it. The time between first submission to  Nature Cell Biology  and acceptance at  JCS  was 317 days. It appeared online another 53 days later 3 . The work went on to win the  JCS  prize as the journal's top paper for 2013. Despite the accolade, Royle says that the multiple rejections were demoralizing for his student, who had done the experiments and needed the paper to graduate. He also thinks that the paper deserved the greater exposure that comes from publication in a more prestigious journal. \u201cUnfortunately, the climate at the moment is that if papers aren't in those very top journals, they get overlooked easily,\u201d he says. And Royle, who has done several publication-time analyses and blogged about what he found, has shown that this experience is not unusual. When he looked at the 28 papers that his lab had published in the previous 12 years, the average time to gestate from first submission to publication was the same as a human baby \u2014 about 9 months (see  go.nature.com/79h2n3 ). But how much of these delays were his own doing? To publish the chromosome paper, Royle indulged in the all-too-familiar practice of journal shopping: submitting first to the  most prestigious journals  in his field (often those with the highest impact factor) and then working his way  down the hierarchy . ( Nature Cell Biology 's current impact factor is 19;  JCS 's is 5.) Journal impact factor or reputation are widely used by scientists and grant-review and hiring committees as a proxy for the quality of the paper. On the flip side, critics say that editors seek out the splashiest papers to boost their publication's impact factor, something that encourages journal shopping, increases rejection rates and adds to the wait time. Journal editors reject this; Ritu Dhand, Nature editorial director in London, says that  Nature 's policy of selecting original, important work \u201cmay lead to citation impact and media coverage, but  Nature  editors aren't driven by those considerations\u201d. How much time does journal shopping add? In the analysis of his group's research papers, Royle found that more than half were shopped around, and that this consumed anywhere from a few days to more than eight months. He went on to analyse all the papers published in 2013 that are indexed in PubMed, and examined whether higher impact factor correlated with longer median publication times. He found an inverted bell-shaped curve \u2014 the journals with the lowest and highest impact factors had longer review times than did those in the middle. For the vast majority of those in the middle, review times stood at around 100 days \u2014 matching Himmelstein's analysis. Those with the very highest impact factors (30\u201350) had a review time of 150 days, supporting the idea that pitching a paper to a series of top journals could result in significant delays in publication. Many scientists, editors and publishers have long acknowledged that journal name is a flawed measure of the quality and value of a piece of research \u2014 but the problem shows  no signs of going away . \u201cWhere your paper is published doesn't say anything about you, your paper's impact or whether it's right or wrong,\u201d says Maria Leptin, director of EMBO, an organization of Europe's leading life scientists and publisher of the  EMBO Journal . \u201cNobody has the courage to say, we, as a funding organization, or we, as a tenure committee, are not going to look at  where you publish as opposed to what you publish .\u201d And the obsession with prestigious journals is just one source of delay \u2014 as Fraser, who was battling to publish her paper on ancient animal populations \u2014 was about to found out. \n               Peer review \n             By October 2013, a full year had passed since Fraser had first submitted her paper to a journal, and she had pretty much stopped caring about impact factor. By this point, the paper had spent two months in review at  Proceedings of the Royal Society B , before coming back with mixed reports \u2014 and a rejection. So Fraser decided to try  PLoS ONE , a journal that says it will publish any rigorous science, regardless of its significance, scope or anticipated citations. It has an impact factor of 3, and a reputation for rapid publication. PLoS ONE  sent the paper out to a single reviewer. Two months later, Fraser got a decision letter that essentially stated that the paper was rejected but might be eligible for re-review if the suggested revisions were made. She made the revisions, adding citations and a small amount of reanalysis. In March 2014, she resubmitted the manuscript, which  PLoS ONE  sent out to a different reviewer. Another two months passed before she received the new review: major revisions, please. \u201cI'm just happy they didn't tell me to go away,\u201d recalls Fraser. \u201cI do have e-mails from the time that say, '1-millionth draft'!\u201d She persevered, making more revisions to meet the reviewer's demands, and in June 2014 submitted the paper to  PLoS ONE  for a third time. Success! The paper 4  was published online 23 months after she had first sent it to  Science . The long peer-review and revision process did improve the paper, Fraser says now. \u201cIt was really much better.\u201d But did the main conclusion of the paper change? \u201cNot really.\u201d Last year, Chris Hartgerink, a behavioural-sciences graduate student at Tilburg University in the Netherlands, ran an analysis of the Public Library of Science (PLOS) family of journals since the first one launched in 2003. (He chose the journals largely because they make the data easily accessible, and because he was waiting for a paper to be published in  PLoS ONE .) He found that the mean review time had roughly doubled in the past decade, from 50\u2013130 days to 150\u2013250 days, depending on the journal (see  go.nature.com/s3voeq ). And when Royle looked at eight journals that had published cell-biology papers over the past decade, he found that publication times had lengthened at seven of them, mostly because review times had stretched out. One contention is that peer reviewers now ask for more. When Ron Vale, a cell biologist at the University of California, San Francisco, analysed biology papers that had been published in  Cell ,  Nature  and the  Journal of Cell Biology , in the first six months of 1984 and compared that with the same period in 2014, he found that both the average number of authors and the number of panels in experimental figures rose by 2\u20134 fold 5 . This showed, he argued, that the amount of data required for a publication had gone up, and Vale suspects that much of the added data come from authors trying to meet reviewers' demands. Scientists grumble about overzealous critics who always seem to want more, or different, experiments to nail a point. \u201cIt's very rare for the revisions to fundamentally change a paper \u2014 the headline doesn't change,\u201d Royle says. His analysis of his group's publication times showed that almost 4 months of the average 9-month gestation was spent revising papers for resubmission. Many scientists also blame journal editors, who, they say, can be reluctant to provide clear guidance and decisions to authors when reviews are mixed \u2014 unnecessarily stringing out the review and revision process. Journal heads disagree, and say that their editors are accomplished at handling mixed reviews.  Cell  editor-in-chief Emilie Marcus in Cambridge, Massachusetts, says that editors at her journal take responsibility for publication decisions and help authors to map out a plan for revisions. Technological advances mean that research now involves handling more and more data, editors say, and there is greater emphasis on making that information available to the community. Marcus says that her journal is working to cut review times by, for example, increasing the number of papers that go through only one round of revision \u2014 14% of their papers did so in 2015. In 2009,  Cell  also restricted the amount of supplemental material that could accompany papers as a way to keep requests for \u201cadditional, unrelated experiments\u201d at bay. PLOS executive editor Veronique Kiermer, based in San Francisco, declined to discuss the specifics of Fraser's paper, but she called its total review time of nine months an \u201coutlier\u201d and said that it was \u201cnot ideal to have research being evaluated by a single person\u201d. She acknowledges that  PLoS ONE 's publication time has risen; one factor is that the volume of papers has, too \u2014 from 200 in 2006 to 30,000 per year now \u2014 and it takes time to find and assign appropriate editors and reviewers. (PLOS used 76,000 reviewers in 2015.) Another, says Kiermer, is that the number of essential checkpoints \u2014 including competing-interest disclosures, animal-welfare reports and screens for plagiarism \u2014 have increased in the past decade. \u201cWe'll do everything we can both in terms of technology and looking at workflows to bring these times to publication down,\u201d she says. Dhand says that at  Nature , too, editors find it harder to find reviewers than in the past, \u201cpresumably because there are so many more papers that need reviewing\u201d. Himmelstein found that the number of papers in PubMed more than doubled between 2000 and 2015, reaching nearly 1 million articles. \n               Technology advances \n             Digital publishing may have had benefits in shortening 'production' time \u2014 the time from acceptance to publication \u2014 rather than time in review. In Himmelstein's analysis, time spent in production has halved since the early 2000s, falling to a stable median of 25 days. Several new journals and online publishing platforms have promised to speed up the process even more. PeerJ, a family of journals that launched in 2013, is one of several that now encourage open peer review, in which reviewers' names and comments are posted alongside articles. The hope is that the transparency will prevent unnecessary delays or burdensome revision requests from reviewers. The biomedical and life-sciences journal  eLife  launched in 2012 with a pledge to make initial editorial decisions within a few days and to review papers quickly. Reviewers get strict instructions not to suggest the 'perfect experiment', and they can ask for extra analysis only if it can be completed within 2 months. Otherwise, the paper is rejected. Randy Schekman, a cell biologist at the University of California, Berkeley, and editor-in-chief of  eLife , says that these policies mean that more than two-thirds of the journal's accepted papers undergo just one round of review. In a 2015 analysis, Himmelstein created a ranking by the  median review time  for all 3,482 journals that had papers with time stamps in the PubMed database from January 2014 to June 2015 (see  go.nature.com/sscrr6 ).  PeerJ  had a relatively fast time: 74 days after submission. At  eLife , it took 108 days, and  PLoS ONE  took 117. By comparison,  Cell 's review time was 127 days;  Nature 's was 173 days;  PLoS Medicine  took 177 days; and  Developmental Cell  was among the slowest of the popular biomedical journals, at 194 days. Marcus notes that comparison between journals is difficult because the publications define received, revised and accepted days differently, and that  Developmental Cell  places a high priority on timely review. \n               Preprints reconsidered \n             One way for biologists to accelerate publication is by embracing preprints. These allow work to quickly receive credit and critique, says Bruno Eckhardt, associate editor of  Physical Review E  and a theoretical physicist at the University of Marburg in Germany. \u201cIt is almost like going on Facebook \u2014 it means you are ready to go public,\u201d he says. A preprint submitted to bioRxiv \u2014 a server run by Cold Spring Harbor Laboratory in New York \u2014 is published online within 24 hours and given a digital object identifier (DOI); subsequent revisions are time-stamped and anyone can read and comment on the paper. \u201cThe minute a research story gets into the public domain, it benefits from the collective power of different brains looking at a problem,\u201d says Vale. What's more, proponents say, preprint publishing can simply be added onto the conventional publication process.  F1000Research , which launched in 2012, does this by publishing papers first, then inviting open peer review and revision. Some scientists are going a step further, and using platforms such as GitHub, Zenodo and figshare to publish each hypothesis, data collection or figure as they go along. Each file can be given a DOI, so that it is citable and trackable. Himmelstein, who already publishes his papers as preprints, has been using the Thinklab platform to progressively write up and publish the results of a new project since January 2015. \u201cI push 'publish' and it gets a DOI with no delay,\u201d he says. \u201cAm I really gaining that much by publishing [in a conventional journal]? Or is it better to do what is fastest and most efficient to get your research out there?\u201d But preprints and real-time digital publishing platforms are no panacea. Vosshall says that many biologists are \u201cterrified\u201d of preprints because they fear getting scooped by competitors or losing credit and intellectual-property rights for their ideas. And even after preprint publishing, scientists can still find themselves slogging through peer review and chasing high-impact journals for a final publication to adorn their CV. Vosshall says that the scientific community relies on conventional journals to serve as a 'prestige filter' so that important papers are brought to the attention of the right readers. Without them, \u201cHow do we find the good stuff?\u201d she asks. For Fraser, her  PLoS ONE  publication proved a success. When the paper was finally published after its almost-two-year wait, she got positive responses, she says. It has been viewed nearly 2,000 times, had 51 shares on Facebook and Twitter and got 280 downloads. The publication also helped her to secure her current position \u2014 as a postdoctoral fellow at the Smithsonian Institution Museum of Natural History in Washington DC. \u201cI pretty much got the top postdoc that I could have gotten.\u201d Still, the whole process is not something she wants to endure again \u2014 so these days, she tends to send her papers to mid-range journals that are likely to publish her work right away. \u201cIf my ultimate goal is to get a faculty job, I can't afford to wait two years on a single paper,\u201d she says. \n                 Tweet \n                 Facebook \n                 LinkedIn \n                 weibo \n               This article appeared in print under the title  The waiting game . \n                     How to judge scientists\u2019 strengths 2015-Nov-11 \n                   \n                     'Living figures' make their debut 2015-Apr-22 \n                   \n                     Rate that journal 2015-Mar-30 \n                   \n                     The arXiv preprint server hits 1 million articles 2014-Dec-30 \n                   \n                     Publishing: The peer-review scam 2014-Nov-26 \n                   \n                     Scientific publishing: The inside track 2014-Jun-18 \n                   \n                     Preprints come to life 2013-Nov-12 \n                   \n                     Science publishing: The golden club 2013-Oct-16 \n                   \n                     Open access: The true cost of science publishing 2013-Mar-27 \n                   \n                     Nature  special: The future of publishing \n                   \n                     Chris Hartgerink\u2019s analysis of wait times at PLoS journals \n                   \n                     Daniel Himmelstein\u2019s analysis of wait times at PubMed journals for 2014\u201315 \n                   \n                     Daniel Himmelstein\u2019s analysis of wait time at PubMed journals from 1981\u201315 \n                   \n                     Stephen Royle\u2019s analysis of his lab\u2019s papers \n                   \n                     Stephen Royle\u2019s analysis of waits at cell biology journals \n                   \n                     Stephen Royles\u2019 analysis of impact factor and wait times \n                   \n                     Ron Vale\u2019s analysis of data and author increases in papers \n                   Reprints and Permissions"},
{"file_id": "530272a", "url": "https://www.nature.com/articles/530272a", "year": 2016, "authors": [{"name": "Jeff Hecht"}], "parsed_as_year": "2006_or_before", "body": "Astronomers are beginning to glimpse what exoplanets orbiting distant suns are actually like. The trickle of discoveries has become a torrent. Little more than two decades after the first planets were found orbiting other stars, improved instruments on the ground and in space have sent the count soaring: it is  now past 2,000 . The finds include 'hot Jupiters', 'super-Earths' and other bodies with no counterpart in our Solar System \u2014 and have forced astronomers to radically rethink their theories of  how planetary systems form and evolve . Yet discovery is just the beginning. Astronomers are aggressively moving into a crucial phase in exoplanet research: finding out what these worlds are like. Most exoplanet-finding techniques reveal very little apart from the planet's mass, size and orbit. But is it rocky like Earth or a gas giant like Jupiter? Is it blisteringly hot or in deep-freeze? What is its atmosphere made of? And does that atmosphere contain molecules such as water, methane and oxygen in odd, unstable proportions that might be a signature of life? The only reliable tool that astronomers can use to tackle such questions is spectroscopy: a technique that analyses the wavelengths of light coming directly from a planet's surface, or passing through its atmosphere. Each element or molecule produces a characteristic pattern of 'lines' \u2014 spikes of light emission or dips of absorption at known wavelengths \u2014 so observers can look at a distant object's spectrum to read off what substances are present. \u201cWithout spectroscopy, you are to some extent guessing what you see,\u201d says Ian Crossfield, an astronomer at the University of Arizona in Tucson. But spectroscopy has conventionally required a clear view of the object, which is generally not available for exoplanets. Most new worlds show up only as an infinitesimal dimming of a star as the otherwise invisible planet passes across its face; others are known only from the slight wobble of a star being tugged back and forth by the gravity of an unseen companion. Astronomers often say that trying to study such an object is like staring into a far-off searchlight (the star) and trying to see a firefly (the planet) hovering nearby. In recent years, however, observers have begun to make headway. Some have extracted the spectra of light passing through the atmospheres of exoplanets as they cross the face of their parent stars \u2014 the equivalent of measuring the colour of the firefly's wings as it flits through the searchlight beam. Others have blocked the light of the parent star so that they can see exoplanets in distant orbits and record their spectra directly. In the past two years, astronomers have begun to record spectra from a new generation of custom-built instruments such as the Gemini Planet Imager on the 8.1-metre Gemini South telescope at the summit of Cerro Pachon in Chile. Exoplanet spectroscopy will be a priority for several spacecraft and ground-based telescopes that are now in development. And astronomers are waiting eagerly for NASA's James Webb Space Telescope (JWST), which will bring unprecedented light-gathering power and sensitivity to the task when it launches in 2018. These are heady times for those hoping to get a deep understanding of new-found worlds, says Thayne Currie, an astronomer at Japan's Subaru Telescope on Mauna Kea, Hawaii. \u201cWe are on the cusp of a revolution.\u201d \n               Transit spectroscopy \n             The first exoplanet in orbit around a Sun-like star was discovered in 1995, when astronomers  Michel Mayor  and Didier Queloz of the Geneva Observatory in Switzerland detected a regular, back-and-forth wobble in the movement of star 51 Pegasi. They concluded 1  that it was caused by the gravity of a planet at least 150 times the mass of Earth \u2014 roughly half the mass of Jupiter \u2014 orbiting the star every 4 days or so. Other discoveries followed as exoplanet fever took hold, and led telescope managers to make more observing time available for planet-hunting. The list of finds soon sparked an idea for astronomer David Charbonneau of the Harvard-Smithsonian Center for Astrophysics in Cambridge, Massachusetts. He reasoned that when a planet 'transits', or passes in front of a star, molecules in its atmosphere will absorb some of the starlight, and leave their spectroscopic fingerprints in it. Might it be possible to detect those fingerprints? To find out, Charbonneau decided to look for sodium. \u201cIt's not particularly abundant,\u201d he says, \u201cbut sodium has very clear spectroscopic features\u201d \u2014 excited molecules of it emit two very strong lines of light, which give sodium street lights their familiar yellow-orange colour. When the sodium is backlit, the light that floods through it has dark bands at the same points of the spectrum, and Charbonneau hoped that these would be comparatively easy to spot. They were: in 2002, Charbonneau and his co-workers announced 2  that they had used the Hubble Space Telescope to detect a sodium signal from a Jupiter-sized exoplanet transiting HD 209458, a star about 47 parsecs (150 light years) from Earth. It was both the first detection and the first spectroscopic measurement of an exoplanet atmosphere. Within a few years, space-based transit observations were recording more complete spectra, and detecting gases such as carbon monoxide and water vapour. Using this technique means looking for very tiny changes in a star's spectrum, says Charbonneau \u2014 maybe 1 part in 10,000. Hubble was and is observers' first choice of instrument: it does not have to contend with absorption of light by gases in Earth's atmosphere, so its spectra are very clean and easy to interpret. But competition for observing time is intense, so astronomers also use ground-based telescopes. These do have to deal with atmospheric interference, but can overcome it by collecting more light than Hubble can. This allows them to detect fainter objects and to separate individual spectral features more clearly. That pays off because most exoplanets are in star systems that are moving relative to Earth. \u201cSo their wavelengths are Doppler-shifted,\u201d says Charbonneau, meaning that the radiation coming from them is stretched or squeezed by their movement, displacing the spectral lines slightly from the corresponding lines in Earth's atmosphere. Because the two sets of spectral lines no longer overlap, observers can know for sure how much of the signal comes from the exoplanet. Using this method, astronomers have been able to detect gases making up as little as 1 part in 100,000 of a planet's atmosphere. An extension of the transit-spectroscopy technique has allowed astronomers to measure the light reflected from a planet's face. They do this after the planet moves across the face of its star, when it will be on the far side of its orbit, with its daylight side facing Earth (see 'Star shades'). Observers will not be able to see it as a separate object \u2014 but they will know that its spectrum is combined with that of the star, says Nicolas Cowan, an astronomer at the McGill Space Institute in Montreal, Canada. Shortly afterwards, however, the planet will pass behind the star and be eclipsed \u2014 at which point, says Cowan, \u201cyou go from a planet and star to just a star. If you measure the difference in flux, you can tell how much light comes from the planet.\u201d The process is demanding, he says, but it can measure the infrared spectra of a Jupiter-sized planet in a close orbit even if it is less than 0.1% as bright as the star. An even more ambitious application of this technique is to follow an exoplanet through a complete orbit. By subtracting the star-only spectrum obtained during the planet's eclipse, observers can get spectra of the planet's atmosphere as its silhouette changes from a thin crescent just after transit to a half-moon shape as it swings to the side, then a full-face view on the far side. This allows them to produce a comparatively fine-grained map of the atmosphere and how it changes over time. Cowan and his co-workers first reported 3  using this technique in 2012, with infrared data from NASA's Spitzer Space Telescope. They showed that the exoplanet HD 189733b was hottest within about 10 degrees of its equator, as predicted. Since then, other researchers have used Hubble and Spitzer 4  to map exoplanet atmospheres in more detail. And Cowan says that with the JWST, \u201cit will be easy to make a 3D map of the atmosphere of a hot Jupiter.\u201d Transit spectroscopy does have its limitations. Some exoplanets have nearly featureless spectra characteristic of clouds, which consist of droplets or fine dust particles that do not leave their imprint on the spectrum in the same way as isolated molecules 5 . The clouds are a big headache, says Charbonneau. \u201cWe don't have any direct measurement of what the clouds are made of. We just know they block the light.\u201d They aren't necessarily made of water vapour. Charbonneau points out that the cloud-shrouded super-Earth GJ 1214b, 12 parsecs from Earth, is so hot that its clouds could be made of zinc sulfide and potassium chloride. On still hotter worlds, the clouds could contain droplets of iron or rock. Lisa Kaltenegger, director of the Carl Sagan Institute at Cornell University in Ithaca, New York, points to another limitation of the transit method. \u201cWhen light hits a transiting planet, it isn't just absorbed,\u201d she says. \u201cIt also gets bent in the atmosphere\u201d, making it impossible for an observer on Earth to see. This bending, known as refraction, increases as the atmosphere becomes thicker. If alien astronomers were trying to get a spectroscopic reading of Earth, she says, refraction would prevent them from probing any deeper than 10 kilometres from the surface 6 . But most of Earth's water is in the lowest 10 kilometres of its atmosphere, she says \u2014 so by analogy, \u201cwater is going to be one of the hardest things to find in an Earth-like exoplanet\u201d. \n               Direct imaging \n             An alternative approach to finding and studying exoplanets is trying to block out the starlight and image them directly, the equivalent of looking for the firefly by holding a hand in front of the searchlight. Early efforts to do this were futile: even the dimmest parent star is much brighter than an exoplanet. The secret of success is to seek brighter fireflies wandering well away from the searchlight \u2014 that is, young planets still glowing from the heat of formation, in orbits far from their stars. The first directly imaged exoplanets were announced by two groups simultaneously in 2008. The objects included 3 planets about 60 million years old orbiting the star HR 8799 (ref.  7 ), and a single planet more than 100 million years old orbiting Fomalhaut (ref.  8 ), a bright star some 8 parsecs from Earth. To obtain the spectra of such objects, astronomers turned to  adaptive optics , a technology that corrects for the twinkling of a star caused by turbulence in Earth's atmosphere and makes it much easier to spot any exoplanets in its vicinity. Also essential are discs inserted into the telescope's optical pathway to block light from the star, and sophisticated signal processors to digitally sharpen the images. \u201cDirect-imaging spectra are beautiful and tell you a lot about the planets and how they formed,\u201d says Bruce Macintosh, an astronomer at Stanford University in California and a co-discoverer of the HR 8799 planets. In 2011, he and his colleagues reported 9  the first detection of water vapour on one of those planets using a first-generation direct-imaging instrument that could observe only exoplanets with temperatures higher than 1,000 kelvin. Now, Macintosh is the principal investigator for the Gemini Planet Imager, which, along with the similar Spectro-Polarimetric High-contrast Exoplanet Research (SPHERE) imager at the European Southern Observatory's Very Large Telescope in Chile, is a second-generation instrument built to directly image and take spectra of exoplanets down to about 600 kelvin. The Gemini instrument launched a multiyear search for Jupiter-like planets orbiting hot, young stars in November 2014. Early observations of 51 Eridani, a 20-million-year-old star about 30 parsecs away, spotted a Jupiter-like world 2.5 times farther from the star than Jupiter is from the Sun 10 . The spectrum showed that this exoplanet, dubbed 51 Eridani b, has an atmosphere containing more methane \u2014 a known component of Jupiter's atmosphere \u2014 than any other exoplanet. \u201cThe really exciting thing with 51 Eridani b and other new exoplanets,\u201d says Currie, \u201cis that we see them when their spectra look a little more normal\u201d and Jupiter-like than those of planets that are even younger and hotter, where methane is strangely absent. That could provide crucial insight into planet formation, the current theory of which is based mostly on data from the Solar System. SPHERE has embarked on a similar survey, but started later, in February 2015, and has less to report. Thus far, says team member Anthony Boccaletti, an astronomer with the Paris Observatory, the most interesting discovery 11  is a group of five gas clumps moving at high velocity away from the young star AU Microscopii, which is known to be unusually prone to flares and other activity. \u201cWe don't really know what they are,\u201d he says. \n               Star surveys \n             Exoplanet spectroscopy has come a long way from its early days, when practitioners were struggling to extract extremely faint signals from noisy environments. The first results were often problematic. Now, Crossfield says, \u201cfor the most part what we are finding holds up and is repeatable\u201d. A coming generation of instruments promises to reveal even more. NASA's Transiting Exoplanet Survey Satellite (TESS), scheduled to launch in August next year, will spend two years searching for exoplanets transiting more than 200,000 of the brightest stars in the solar neighbourhood. Exoplanets will also be targets for the JWST. With its 6.5-metre telescope and advanced instruments, Webb should see many more than the 2.4-metre Hubble. \u201cTESS and Webb will own this space in five years,\u201d predicts Macintosh. Two other planned \u2014 but not yet approved \u2014 space missions will use exoplanet spectroscopy. NASA's 2.4-metre Wide Field Infrared Survey Telescope, expected to launch in the mid-2020s, would spend most of its time on cosmological questions, but is expected to find and study about 2,600 exoplanets. Currie says that it should be able to image Jupiter-like planets orbiting nearby stars, although smaller, colder bodies similar to Pluto or the  hypothetical 'Planet X'  speculated to exist at the edge of the Solar System \u2014 or Earth, for that matter \u2014 will remain out of reach. \u201cWe would need a 10-metre-scale telescope in space to do other Earths,\u201d says Macintosh. The second mission is ARIEL, the Atmospheric Remote-Sensing Infrared Exoplanet Large-survey, one of three candidates for a medium-class mission to be launched by the European Space Agency in 2026. The 1-metre telescope would be dedicated to transit spectroscopy and a survey of exoplanets at temperatures higher than 500 kelvin. In about a decade, astronomers hope to see the completion of three super-giant telescopes: the 24.5-metre  Giant Magellan Telescope  at the Las Campanas Observatory in Chile, the  Thirty-Meter Telescope  planned for Mauna Kea, and the  European Extremely Large Telescope  on Cerro Armazones in Chile. All three will be equipped with adaptive optics systems, and it's a safe bet that they will be doing exoplanet spectroscopy to test models based on the data gleaned up to that point. Those measurements could be astronomers' first realistic chance to find life in the wider Universe, says Charbonneau. \u201cI'm so excited.\u201d \n                 Tweet \n                 Facebook \n                 LinkedIn \n                 Weibo \n               \n                     Evidence grows for giant planet on fringes of Solar System 2016-Jan-20 \n                   \n                     The exoplanet files 2015-Nov-18 \n                   \n                     The mountain-top battle over the Thirty Meter Telescope 2015-Sep-29 \n                   \n                     Astronomy: Laser focus 2015-Jan-21 \n                   \n                     Astronomy: Planets in chaos 2014-Jul-02 \n                   \n                     Gemini Planet Imager \n                   \n                     SPHERE \n                   \n                     Transiting Exoplanet Survey Satellite \n                   \n                     Giant Magellan Telescope \n                   \n                     Thirty-Meter Telescope \n                   \n                     European Extremely Large Telescope \n                   Reprints and Permissions"},
{"file_id": "529016a", "url": "https://www.nature.com/articles/529016a", "year": 2016, "authors": [{"name": "Gabriel Popkin"}], "parsed_as_year": "2006_or_before", "body": "From flocking birds to swarming molecules, physicists are seeking to understand 'active matter' \u2014 and looking for a fundamental theory of the living world. First, Zvonimir Dogic and his students took microtubules \u2014 threadlike proteins that make up part of the cell's internal 'cytoskeleton' \u2014 and mixed them with kinesins, motor proteins that travel along these threads like trains on a track. Then the researchers suspended droplets of this cocktail in oil and supplied it with the molecular fuel known as adenosine triphosphate (ATP). To the team's surprise and delight, the molecules organized themselves into large-scale patterns that swirled on each droplet's surface. Bundles of microtubules linked by the proteins moved together \u201clike a person crowd-surfing at a concert\u201d, says Dogic, a physicist at Brandeis University in Waltham, Massachusetts. With these experiments, published 1  in 2012, Dogic's team created  a new kind of liquid crystal . Unlike the molecules in standard liquid-crystal displays, which passively form patterns in response to electric fields, Dogic's components were active. They propelled themselves, taking energy from their environment \u2014 in this case, from ATP. And they formed patterns spontaneously, thanks to the collective behaviour of thousands of units moving independently. These are the hallmarks of systems that physicists call active matter, which have become a major subject of research in the past few years. Examples abound in the natural world \u2014 among them the leaderless but coherent flocking of birds and the flowing, structure-forming cytoskeletons of cells. They are increasingly being made in the laboratory: investigators have synthesized active matter using both biological building blocks such as microtubules, and  synthetic components  including micrometre-scale, light-sensitive plastic 'swimmers' that form structures when someone turns on a lamp. Production of peer-reviewed papers with 'active matter' in the title or abstract has increased from less than 10 per year a decade ago to almost 70 last year, and several international workshops have been held on the topic in the past year. \n               The secret of life \n             Researchers hope that this work will lead them to a complete, quantitative theory of active matter. Such a theory would build on physicists' century-old theory of statistical mechanics, which explains how the motion of atoms and molecules gives rise to everyday phenomena such as heat, temperature and pressure. But it could go much further, providing a mathematical framework for still-mysterious biological processes such as how cells move things around, how they create and maintain their shapes and how they divide. \u201cWe want a theory of the mechanics and statistics of living matter with a status comparable to what's already been done for collections of dead particles,\u201d says Sriram Ramaswamy, a physicist and director of the Tata Institute of Fundamental Research's Centre for Interdisciplinary Sciences in Hyderabad, India. It could be a while before that want is satisfied, however. Experimentalists are only beginning to gain control of active materials in the lab. Even the most enthusiastic proponents of this research admit that no one has yet produced a theory of active matter that describes the behaviour of everything from cell parts to birds. And if such a theory did exist, it's far from certain that mainstream biologists would see value in it. For biologists, the idea that living matter is active \u201cwould be just so obvious as to not really contain very much information\u201d, says Jonathon Howard, a molecular biophysicist at Yale University in New Haven, Connecticut. But that has not kept proponents from imagining applications such as self-assembling artificial tissue, self-pumping microfluidic devices and new bio-inspired materials \u2014 although researchers admit that such ideas are still far from being realized. \u201cI think it's too early for the field to have an application, because we're still kind of astonished at what can happen,\u201d says Andreas Bausch, a physicist at the Technical University of Munich in Germany \u2014 \u201cbut I do think the field needs somebody doing it.\u201d \n               All together now \n             All known life forms are based on self-propelled entities uniting to create large-scale structures and movements. If this didn't happen, organisms would be limited to using much slower, passive processes such as diffusion to move DNA and proteins around inside cells or tissues, and many of life's complex structures and functions might never have evolved. Biologists and physicists have speculated for decades about the general principles of living matter, but research on cellular processes has focused on identifying the dizzying array of molecules involved, rather than on working out the principles by which they self-organize. As a result, what is now known as active-matter research did not really get under way until the mid-1990s. One of the most influential early experiments was conducted by the team of Stanislas Leibler, a biophysicist who was then at Princeton University in New Jersey and is now at the Rockefeller University in New York. The group was among the first to show that complex, life-like structures could self-assemble from microtubules and a few proteins supplied with ATP 2 . Around the same time, an influential model of active matter was being developed by Tam\u00e1s Vicsek, a theoretical biophysicist at E\u00f6tv\u00f6s Lor\u00e1nd University in Budapest. In the early 1990s, Vicsek was trying to account for the collective motions of bird flocks, bacterial colonies and cytoskeleton components when he realized that no existing theory would work. \u201cIt's not like equilibrium statistical mechanics, where you take a book and you find what to do,\u201d says physicist Jean-Fran\u00e7ois Joanny of the Curie Institute in Paris. Instead, Vicsek found a starting point in a model of magnetic materials developed in 1928 by German physicist Werner Heisenberg. Heisenberg imagined each atom as a freely rotating bar magnet, and found that large-scale magnetism emerges when interactions between these atomic magnets cause the majority of them to align. To explain active matter, Vicsek replaced the tiny magnets with moving 'arrows' symbolizing particles with velocities that aligned with the average velocity of their neighbours \u2014 albeit with a certain amount of random error. That led to what is now known as Vicsek's flocking model 3 . His simulations showed that when enough arrows were packed into a small enough space, they began to move in patterns that closely resembled the familiar movements of bird flocks and fish schools (see 'Smart swarms'). \u201cI got excited,\u201d recalls Vicsek, whose 1995 paper 3  on the model has received more than 3,500 citations. \u201cI was walking up and down the corridor and told people I had designed the moving version of the Heisenberg model.\u201d One physicist attracted to this idea was John Toner, who heard Vicsek give a talk on it in 1994. Toner, now at the University of Oregon in Eugene, saw that Vicsek's swarming arrows could be modelled as a continuous fluid. He took the standard equations for hydrodynamics, which describe fluid flow in everything from tea kettles to oceans, and modified them to account for how individual particles use energy 4 . Toner's fluid model and Vicsek's discrete-particle model gave essentially the same predictions for a wide range of phenomena, and launched a cottage industry of active-matter simulations. There was only one problem. Whereas the number of simulations was skyrocketing, says physicist Denis Bartolo of the \u00c9cole Normale Sup\u00e9rieure in Lyons, France, \u201cthe number of quantitative experiments was constant and very close to zero\u201d. Practical work was challenging: no one could hope to do controlled experiments with 10,000 real birds or fish. And at the microscopic scale, few scientists were familiar with both the necessary theoretical work \u2014 being published mainly in physics journals \u2014 and the biological lab techniques needed to purify cellular components. \n               Practical magic \n             Only in the late 2000s did the theoretical and experimental pieces begin coming together. Bausch led one of the first precise, quantitative experiments. He and his colleagues mixed actin, a filament that forms most of the cytoskeleton of complex cells, with myosin, a molecular motor that 'walks' on actin and makes muscles contract. The researchers added myosin's natural fuel, ATP, then put the mixture on a microscope slide and watched. \u201cWe didn't do anything; we just added the stuff,\u201d Bausch says. At low concentrations, the actin filaments swam around without recognizable order. But at higher densities, they formed pulsating clusters, swirls and bands. Bausch and his colleagues immediately recognized and quantified phase transitions of the kind that Vicsek and others had predicted. Their 2010 paper 5  helped to ignite the experimental active-matter field. Among the studies that followed were Dogic's 2012 microtubule experiments 1 , which used another walking protein, kinesin. The resulting patterns were much more complex and dynamic than the ones Bausch saw: the flowing microtubules looked like fingerprint whorls in motion. Dogic and his team also noticed that the orderly alignment of this flow would occasionally break down and produce 'defects': discontinuities in the pattern that resemble converging longitude lines at the North and South poles. These defects were dynamic, moving around like self-propelled particles. No theory at the time could account for this behaviour. But in 2014, Dogic teamed up with Bausch and physicist Cristina Marchetti of Syracuse University in New York to describe the behaviour of active liquid crystals swirling on spherical vesicles in terms of the movement of defects rather than of individual crystal elements 6 . Furthermore, the group found that it could tune the defects' motion by adjusting the vesicle's diameter and surface tension, suggesting a possible way to control an active crystal. Dogic and his students are now trying to do just that. By studying the spontaneous flows of microtubules and proteins confined in small, doughnut-shaped containers, they hope to lay the groundwork for a self-pumping fluid that could move molecules around in microfluidic devices similar to those that are becoming increasingly common in experimental biology, medicine and industry. Active matter \u201cchanges our ideas of what materials can do\u201d, says Dogic. But any industrial application will have to overcome at least one major roadblock. The biological materials currently used in active-matter experiments are expensive and time-consuming to purify \u2014 Dogic's microtubules come from cow brains, and Bausch uses actin from rabbit muscle \u2014 and they last only a short time in the lab. Until a cheap, robust, off-the-shelf source of active-matter materials can be found, commercial use is unlikely, says Bausch. Advances in synthetic active materials may show the way forward. In 2013, New York University physicist Paul Chaikin and his colleagues described making particles of haematite, an iron oxide mineral, inside a spherical polymer 7 . When the scientists placed these 'swimmers' in a solution of hydrogen peroxide and exposed them to blue light, a chemical reaction caused the particles to move around spontaneously, clumping and unclumping like groups of people at a cocktail party. In 2013, Bartolo and his colleagues reported large-scale flows using even simpler plastic spheres in a conducting fluid 8 . When the researchers turned on an electric field, the spheres began rotating in random directions. At high enough densities, interactions between nearby spheres caused them to spontaneously roll, flock-like, in the same direction. Such lab-made materials remain primitive, however, compared with those produced in cells by 4 billion years of evolution. Dogic notes that the kinesins he uses are much more efficient than any human-made motors at converting energy to motion. And Bartolo is also quick to discourage talk of short-term pay-offs. \u201cI'm not targeting a specific application,\u201d he says of his rotating plastic spheres. Possible applications aside, active matter excites scientists because it so closely resembles the most complex self-organizing systems known: living organisms. In 2011, Dogic and his colleagues reported 9  that microtubule bundles anchored at one end to air bubbles on a microscope slide beat in synchronized, wave-like patterns eerily reminiscent of the hair-like cilia and flagella that protrude from the surfaces of some cells. And in his 2012 paper 1 , he noted a striking similarity between his microtubule flows and cytoplasmic streaming, a process in which cytoskeletal filaments team up to whisk a cell's contents around like \u201ca giant washing machine\u201d, he says. The resemblance between lab-prepared active matter and living things can be uncanny, agrees Jennifer Ross, a physicist at the University of Massachusetts Amherst. At talks, she has shown videos of spherical microtubule\u2013kinesin systems and asked audience members whether they think they are seeing a real cell. \u201cWhenever I present these to cell biologists in particular, they are always fooled,\u201d she says. But something can look and act like a living organism without actually following the same rules, cautions Howard. He points out that Dogic's group created something that looks and acts very much like a cilium or flagellum with its multitude of proteins \u2014 but that may, in fact, work very differently. \u201cThere's something in there about the underlying mechanism, but it's extremely abstract,\u201d he says. \n               Is it enough? \n             To probe whether active-matter theory can reveal biological mechanisms, Daniel Needleman, a biophysicist at Harvard University in Cambridge, Massachusetts, studied the spindle: a microtubule-based structure that controls the separation of chromosomes during cell division. He wanted to test the idea, suggested by earlier theories and experiments, that short-range microtubule\u2013kinesin interactions by themselves were sufficient to yield spindle-like structures. He first used sophisticated microscopes to examine extracts from frog egg cells, quantifying microtubule density, orientation and stresses during spindle formation. \u201cIt really was not clear at all until Dan came along that you could measure all these things,\u201d says Howard. Needleman then merged his measurements with models of how active matter self-organizes. In 2014, he and Jan Brugu\u00e9s, a biologist at the Max Planck Institute of Molecular Cell Biology and Genetics in Dresden, Germany, reported that, consistent with theory, the interactions they observed among closely spaced microtubules are enough to produce the spindle and keep it stable 10 . \u201cPeople have argued that you need more complex processes,\u201d says Needleman. \u201cBut the fact that one can understand so much of the spindle without invoking any of that shows that it's certainly not necessary.\u201d Others are using ideas from active matter to probe how large numbers of cells organize in processes such as tissue growth, wound healing and the spread of tumours. Theorists including Marchetti, Joanny and Frank J\u00fclicher of the Max Planck Institute for the Physics of Complex Systems in Dresden have modelled tissues 11  and tumours 12  as flowing cells that self-organize through short-range cell-to-cell interactions rather than chemical signals. Experimentalists are testing such ideas, for instance, by showing that active-matter theory can help to describe cell organization in a developing fruit-fly wing 13 . Some biologists hope that such studies will reveal the fundamental principles that govern how cells divide, take shape or move. \u201cIt's like Linnaean classification before Darwin came along,\u201d says biologist Tony Hyman of the Max Planck Institute of Molecular Cell Biology and Genetics. \u201cWe've got all these molecules, just like they had all those species, and we need to put some kind of order, some kind of reason behind it all.\u201d Active matter, Hyman thinks, could provide that reason. But even enthusiasts admit that mainstream biologists may need convincing. \u201cWe used to get a lot of papers rejected at the beginning,\u201d says Hyman \u2014 in part because the manuscripts' heavy use of mathematics made it hard to find reviewers. Even the phrase 'active matter' may hinder communication, adds Howard. \u201cIt's kind of a physics-y term.\u201d Still, Howard and Hyman hope that acceptance will be aided by increasing convergence between fields. Among biologists, says Hyman, \u201cI think the new generation coming along will be trained in physics from the beginning.\u201d And that's good, adds Stephan Grill, a biophysicist at the Technical University of Dresden, because progress in active matter calls for scientists who are at the cutting edge of both physics and biology. \u201cThe pot of gold is at the interface,\u201d he says, \u201cbut you have to push both fields to their limits.\u201d \n                 Tweet \n                 Follow @NatureNews \n               \n                     The tiniest Lego: a tale of nanoscale motors, rotors, switches and pumps 2015-Sep-02 \n                   \n                     Active matter: Spontaneous flows and self-propelled drops 2012-Nov-07 \n                   \n                     Lab-made droplets roll themselves 2012-Nov-07 \n                   \n                     Zvonimir Dogic \n                   \n                     Sriram Ramaswamy \n                   \n                     Andreas Bausch Group \n                   \n                     Tam\u00e1s Vicsek \n                   \n                     John Toner \n                   \n                     Cristina Marchetti \n                   Reprints and Permissions"},
{"file_id": "529270a", "url": "https://www.nature.com/articles/529270a", "year": 2016, "authors": [{"name": "Ewen Callaway"}], "parsed_as_year": "2006_or_before", "body": "The feral chickens of Kauai provide a unique opportunity to study what happens when domesticated animals escape and evolve. \u201cDon't look at them directly,\u201d Rie Henriksen whispers, \u201cotherwise they get suspicious.\u201d The neuroscientist is referring to a dozen or so chickens loitering just a few metres away in the car park of a scenic observation point for Opaekaa Falls on the island of Kauai, Hawaii. The chickens have every reason to distrust Henriksen and her colleague, evolutionary geneticist Dominic Wright, who have travelled to the island from Link\u00f6ping University in Sweden armed with traps, drones, thermal cameras and a mobile molecular-biology lab to study the birds. Ewen Callaway investigates what happens when domestic chickens go wild As the two try to act casual by their rented car, a jet-black hen with splashes of iridescent green feathers pecks its way along a trail of bird feed up to a device called a goal trap. Wright tugs at a string looped around his big toe and a spring-loaded net snaps over the bird. After a moment of stunned silence, the hen erupts into squawking fury. Opaekaa Falls, like much of Kauai, is teeming with feral chickens \u2014 free-ranging fowl related both to the domestic breeds that lay eggs or produce meat for supermarket shelves and to a more ancestral lineage imported to Hawaii hundreds of years ago. These modern hybrids inhabit almost every corner of the island, from rugged chasms to KFC car parks. They have clucked their way into local lore and culture and are both beloved and reviled by Kauai's human occupants. Biologists, however, see in the feral animals an improbable experiment in evolution: what happens when chickens go wild? The process of domestication has  moulded animals and their genomes to thrive in human environments . Traits that ensure survival in the wild often give way to qualities that benefit humans, such as docility and fast growth. Feralization looks, on its surface, like domestication in reverse. But closer inspection suggests that the chickens of Kauai are evolving into something quite different from their wild predecessors, gaining some traits that reflect that past, but maintaining others that had been selected by humans. In this way, they are similar to other populations of animals, including dogs, pigs and sheep, that have broken free of captivity and flourished. By looking at feral animals, some evolutionary biologists hope to determine how domestic animals and their genes change in response to natural pressures. The research could also help to inform tricky conservation questions about how such animals affect native species, and ultimately whether and how to control them. The natural history of the Kauai fowl makes them an important test case. \u201cPeople have a really complicated relationship with the chickens,\u201d says Eben Gering, an evolutionary ecologist at Michigan State University's Kellogg Biological Station in Hickory Corners, who is in Kauai with Henriksen and Wright. \u201cSome people absolutely want them gone. Some consider them an integral part of the local culture.\u201d \n               Foul-weather fowl \n             The  Polynesian mariners who first settled the Hawaiian Islands  about 1,000 years ago brought what they needed to start civilization anew. Staple plants such as taro,  sweet potato  and coconut palm made the trans-Pacific voyage, as did domestic dogs, pigs and, naturally, their prized chickens. The Polynesian poultry probably bore little resemblance to the birds that provide much of the world's protein today. Archaeological and genetic evidence suggests that they were  more like red junglefowl  ( Gallus gallus ) \u2014 small, furtive birds that still roam the forests of southeast Asia and are ancestral to all domestic chickens. By the time that Captain James Cook landed in Waimea in southern Kauai in 1778, the Polynesian chickens had already, in essence, become feral. They wandered freely between Native Hawaiian villages and the neighbouring forest. Later, European and US settlers imported predators such as mongooses, which devastated birds of all kinds. Polynesian chickens were all but wiped out everywhere but Kauai and neighbouring Niihau, where the predators were never introduced. On Kauai, chickens flourished. Although the birds' numbers have not been tracked precisely, many residents contend that the population surged after hurricanes in 1982 and 1992 blew modern chickens from people's back yards into the forests, where they encountered the descendants of the Polynesian chickens. Gering says it is possible that interbreeding between the two populations allowed the birds' numbers to swell. And during his first trip to the island in 2013, he and Wright noticed 1  that many of the feral chickens they encountered had flecks of the white feathers common in modern domestic breeds, in addition to the darker plumage usually seen in wild populations. Many had yellow legs (red-junglefowl legs are grey). And some of the roosters' crows sounded conspicuously like the drawn out cock-a-doodle-doo of their barnyard brethren, rather than the truncated calls of red junglefowl. DNA from 23 chickens revealed just how far domestic-chicken genes had infiltrated 1 .  The birds' nuclear genomes  seemed to be a mixture of genes from red-junglefowl-like Polynesian chickens and domestic chickens, whereas their maternally inherited mitochondrial markers traced back to European and Pacific domestic poultry. Gering and Wright think that a single hybrid population of feral chickens now roams Kauai, bearing a mixture of traits from modern and ancient birds. In unpublished work, the two have scoured the birds' genomes for stretches of DNA with very little variation across the population. This homogeneity suggests that a gene has surged through the population in the recent past, probably because it offers some benefit. If feralization were domestication played backwards, then these 'selective sweeps' might appear around the DNA sequences that distinguish domestic chickens from red junglefowl. Instead, the researchers have found that most of the swiftest-evolving genes in the Kauai chickens are distinct from those suspected involved in modern domestication. In some cases, genes from the Polynesian chickens are helping the hybrid feral chickens to adapt to Kauai's habitats. For example, modern domestic fowl have been bred not to sit on, or brood, their eggs (making the eggs easier to collect). But in the wild, this trait puts unhatched chicks at risk. Wright and Gering found that feral chickens possess red-junglefowl gene variants that are linked to brooding. But some genes of domestication do seem to be handy outside the coop. A variant linked to increased growth rates and reproduction in domestic chickens, for example, persists in the Kauai population, even though the average adult feral chicken is half the weight of a month-old bird bred for meat. \n               Chicken and egg \n             \u201cYou won't see a bird as healthy-looking as that,\u201d Wright says of the hen that he and Henriksen had captured at Opaekaa Falls. \u201cHer plumage is perfect.\u201d In the basement of a rented house on Kauai, the researchers have set up a makeshift laboratory where they photograph the bird, draw its blood and then kill it and prepare it for dissection. Wright starts with the hen's Brazil-nut-sized brain. Their unpublished research has shown that the brains of domestic chickens are smaller than those of junglefowl, relative to their body size, and organized differently. The team hopes to identify the genes responsible for these changes and others, such as the diminished visual-processing systems of domestic birds. Life in the wild has also altered the reproductive systems of the feral chickens. Domestic breeds lay eggs almost daily, but breeding seasonally could allow feral chickens to reapportion the minerals devoted to eggs (which come from spongy tissue in the centre of their bones) to making their skeletons more robust. The researchers sample the hen's femur and also find that its ovaries are empty of egg follicles, which could be a sign of seasonal breeding. Feralization has garnered much less attention from scientists than domestication (which gets a nod in chapter one of Charles Darwin's 1859  On the Origin of Species ). But swapping of domestic and wild genes has been happening all over the world for thousands of years.  A feral-sheep population  that has lived on the island of St Kilda in the Scottish Outer Hebrides for as long as 4,000 years acquired beneficial alleles that determine coat colour from a modern domestic sheep breed some 150 years ago 2 . A 2009 study in  Science 3  found that some wolves in Yellowstone National Park, Wyoming, carry a domestic-dog version of a gene linked to dark coats that shows hallmarks of positive selection, possibly helping wolves from the Arctic to adapt to forested environments. \u201cPeople would have thought that genes to live in a farm and house aren't going to be any good in the wild, but that's not necessarily true,\u201d says Jonathan Losos, an evolutionary ecologist at Harvard University in Cambridge, Massachusetts. And like Kauai's feral chickens, other feral animals such as dingoes in Australia and urban pigeons practically everywhere have not evolved back to the state of their wild ancestors \u2014 even if certain traits may trend in that direction. Like chickens, other domesticated animals tend to have smaller brains than their wild cousins, relative to body size (see 'Free bird'). And brain regions involved in processing things such as sight, sound and smell are among the most diminished, perhaps because humans bred animals to be docile and less wary of their surroundings. Feral pigs in Sardinia seem to have regained large brains and high densities of neurons involved in olfaction, but not the abilities that come with them: their neurons do not express a protein that has been linked to the exquisite sense of smell in closely related wild boars 4 . Likewise, feral dogs, cats and pigs often lack the savvy of their wild brethren and still depend on human niches for their survival, notes Melinda Zeder, an archaeologist at the Smithsonian Institution's National Museum of Natural History in Washington DC. Packs of feral dogs, for instance, do not form the complex hierarchies that make wolves such fearsome predators. \u201cThere's no leadership the way you get in a wolf pack. It's just a bunch of shitty friends ,\u201d says Greger Larson, an evolutionary geneticist at the University of Oxford, UK, who is part of a team examining the mixed ancestry of Kauai's feral pigs. Wright and Henriksen take less than an hour to dissect the captured hen and preserve samples of its brain, bone, liver and other tissue for gene-expression studies. They will use the RNA molecules expressed in different tissues to come up with a list of genes that might influence traits that distinguish the feral chickens from domestic birds and red junglefowl. They are eager to add to their study sample, and they jump at an offer to visit a nearby farm to collect more chickens. \n               Pecking order \n             \u201cThey are a scourge. They are vermin. They cost us thousands and thousands of dollars every year,\u201d says the farm's owner (who asked not to be named). The birds scratch at tree saplings on his orchard, exposing the roots and killing fruit trees before they can mature. He patrols his property in a beaten-up luxury sedan with a high-powered air rifle and a hired hand who gets US$5 per kill. Every few months, he invites hunters with night-vision goggles to visit and pick off the birds as they roost. Few Kauaians share his malice towards feral chickens. Many locals give them a 'no-big-deal' shrug when asked. And the island's many tourists tend to view the birds with curiosity followed by mild annoyance after a couple of 3 a.m. wake-up calls. Feral-chicken merchandise \u2014 postcards, kitchen chopping boards, T-shirts \u2014 are ubiquitous. A popular children's television show hosted by a character called Russell the Rooster has been on air for nearly two decades. As descendants of the birds imported by the Polynesians, Kauai's feral chickens occupy a zoological purgatory somewhere between native plants and animals and the dreaded invasive species that plague island habitats such as Hawaii. \u201cIt's much more complicated than just a feralized chicken,\u201d says Gering. \u201cEven though junglefowl were not here before the Polynesians colonized the island, they have been a part of this ecosystem for much longer than the domestic chickens.\u201d The chickens enjoy semi-official protection as 'wild chickens' in nature preserves. But if the same birds wander into developed areas or private property, they are considered 'free-flying domestic chickens' with no sanctuary. \u201cLocals are free to take them (if they come onto your property) and put them in the pot,\u201d says a State of Hawaii website. (Gering and Wright, with a freezer full of the animals, consider making 'feral coq au vin'.) Kauai may have no shortage of feral chickens now, but if mongooses arrive on the island or political winds change, they could be at risk. Hawaii's most populous island, Oahu, has mounted a controversial culling campaign against its feral chickens (whose ancestry is uncertain). But Gering thinks that the Kauai chickens' long tenure and unique cultural position makes some form of conservation worth considering. \u201cBefore deciding how important it is conserve them, manage them or cull them, it would be good to at least know about their impact,\u201d he says. Researchers want to know about everything from the animals and plants that the birds eat to how they alter landscapes \u2014 information that Gering hopes to gather on future trips to the island. Kauai's chickens are hardly the only creatures to occupy a nebulous space between native and alien. When Przewalski's horses, which live on the Mongolian steppe, were first described in the late nineteenth century, they were believed to be the planet's last wild, undomesticated horses. But a 2015 genome study 5  found that the 2,100 or so remaining horses carry substantial amounts of domestic-horse DNA. They also show significant signs of inbreeding, owing to a captive-breeding programme begun in the 1940s. Some conservationists view domestic genes as pollutants that are tarnishing the genomes of wild animals such as wolves, coyotes and even the red junglefowl native to southeast Asia. Some even contend that there are no 'pure' red junglefowl left. \u201cWhat feral animals make us do is reconsider this all-too-obvious, all-too-easy and all-too-wrong dichotomy between wild and domestic,\u201d says Larson. And the ability of supposedly wild animals to thrive in a world increasingly altered by human activity may be due in part to the domestic genes that they now carry. What better way to adapt to human-moulded environments than to borrow traits from human-moulded creatures? \n               Home to roost \n             \u201cCock-a-doodle-doo,\u201d announces a rooster masked by dense forest in Kokee State Park, an achingly beautiful nature reserve on Kauai's western coast, on a sunny autumn morning. A faint but unmistakable \u201ccock-a-doodle-doo\u201d volleys back, from maybe a kilometre away. Although the birds are a fixture even in this rich and remote landscape, most of the chickens in the reserve stick near the car parks and picnic areas, where human hand-outs are easy to come by. The park's birds are among the most brazen and comfortable around humans in all of Kauai, and it's hard to enjoy a meal in Kokee's central meadow without attracting the attention of a flock or two. But \u201cgive them a chase and they'll disappear down a 300-metre ravine that's so thick with vegetation it's impossible to follow\u201d, says Gering. \u201cThat's not something I think about barnyard chickens as being able to do.\u201d A park website discourages visitors from feeding the chickens, in the hope of reducing their numbers and their dependence on humans. This interest in 'rewilding' the feral chickens is probably motivated by the desire to reduce their numbers through methods other than culling. It may well be a matter of time before feral animals fully shed their yokes and evolve into creatures less dependent on humans \u2014 but perhaps it will never happen. \u201cThe environmental niches that feralized animals are exploiting are very different and bear a human stamp that wasn't there when their ancestors developed,\u201d says Zeder. \u201cWhy should anyone expect a feralized animal to go out and become the noble savage animal again?\u201d Wright, however, thinks there is a possibility that if the chickens in Kokee are left alone for long enough, they may well become not a facsimile of their red-junglefowl ancestors, but some other kind of creature just as deserving of being called wild. Whatever wild means.\n \n                 Tweet \n                 Follow @NatureNews \n               \n                 weibo \n               \n                     Ancient wolf genome pushes back dawn of the dog 2015-May-21 \n                   \n                     Chicken project gets off the ground 2014-May-27 \n                   \n                     Archaeology: The milk revolution 2013-Jul-31 \n                   Reprints and Permissions"},
{"file_id": "529142a", "url": "https://www.nature.com/articles/529142a", "year": 2016, "authors": [{"name": "Jane Qiu"}], "parsed_as_year": "2006_or_before", "body": "Rapid changes in Tibetan grasslands are threatening Asia's main water supply and the livelihood of nomads. In the northern reaches of the Tibetan Plateau, dozens of yaks graze on grasslands that look like a threadbare carpet. The pasture has been munched down to bare soil in places, and deep cracks run across the snow-dusted landscape. The animals' owner, a herder named Dodra, emerges from his home wearing a black robe, a cowboy hat and a gentle smile tinged with worry. \u201cThe pastures are in a bad state and lack the kind of plants that make livestock strong and grow fat,\u201d says Dodra. \u201cThe yaks are skinny and produce little milk.\u201d His family of eight relies on the yaks for most of its livelihood \u2014 milk, butter, meat and fuel. Dodra was forced to give up half of his animals a decade ago, when the Chinese government imposed strict limits on livestock numbers. Although his family receives financial compensation, nobody knows how long it will last. \u201cWe barely survive these days,\u201d he says. \u201cIt's a hand-to-mouth existence.\u201d If the grasslands continue to deteriorate, he says, \u201cwe will lose our only lifeline\u201d. The challenges that face Dodra and other Tibetan herders are at odds with glowing reports from Chinese state media about the health of Tibetan grasslands \u2014 an area of 1.5 million square kilometres \u2014 and the experiences of the millions of nomads there. Since the 1990s, the government has carried out a series of policies that moved once-mobile herders into settlements and sharply limited livestock grazing. According to the official account, these policies have helped to restore the grasslands and to improve standards of living for the nomads. But many researchers argue that available evidence shows the opposite: that the policies are harming the environment and the herders. \u201cTibetan grasslands are far from safe,\u201d says Wang Shiping, an ecologist at the Chinese Academy of Sciences' (CAS) Institute of Tibetan Plateau Research (ITPR) in Beijing. \u201cA big part of the problem is that the policies are not guided by science, and fail to take account of climate change and regional variations.\u201d The implications of that argument stretch far beyond the Tibetan Plateau, which spans 2.5 million square kilometres \u2014 an area bigger than Greenland \u2014 and is mostly controlled by China. The grasslands, which make up nearly two-thirds of the plateau, store water that  feeds into Asia's largest rivers . Those same pastures also serve as a gigantic reservoir of carbon, some of which could escape into the atmosphere if current trends continue. Degradation of the grasslands \u201cwill exacerbate global warming, threaten water resources for over 1.4 billion people and affect Asian monsoons\u201d, says David Molden, director general of the International Centre for Integrated Mountain Development (ICIMOD) in Kathmandu, Nepal. Such concerns propelled me to make a 4,700-kilometre journey last year from Xining, on the northeastern fringe of the plateau, to Lhasa in the Tibetan heartland (see 'Trek across Tibet'). Meeting with herders and scientists along the way, I traversed diverse landscapes and traced the Yellow and Yangtze Rivers to their sources. The trip revealed that Tibetan grasslands are far less healthy than official government reports suggest, and scientists are struggling to understand how and why the pastures are changing. \n               Fenced in \n             It began to drizzle soon after we set off from the city of Xining on a stretch of newly built highway along the Yellow River. As our Land Cruiser climbed onto a 3,800-metre-high part of the plateau, the vista opened to reveal rolling hills blanketed by a thick layer of alpine meadow, resembling a gigantic golf course. We passed herds of sheep and yaks, white tents and nomads in colourful robes \u2014 along with barbed-wired fences that cut the rangeland into small blocks. This part of the Tibetan Plateau, in a region known as Henan county, is blessed with abundant monsoonal rains every summer. The herders who live here are able to maintain healthy livestock and can make a decent living. \u201cWe have plenty to go around, and the livestock are well taken care of,\u201d says herder Gongbu Dondrup. But life has been different since the government began to fence up grasslands around a decade ago, says Dondrup. Before that, he took his herd to the best pastures at high elevations in the summer, and then came back down in the winter. Now, he must keep the yaks in an 80-hectare plot that the government assigned to his family. The pasture looks worn, and he is being pressed by the government to further downsize his herd. \u201cI don't know how long it can keep us going,\u201d he says. The fencing initiative is the latest of a string of Chinese grassland policies. After annexing Tibet in 1950, the young revolutionary Chinese republic turned all livestock and land into state properties. Large state farms competed with each other to maximize production, and livestock numbers on the plateau doubled over two decades, reaching nearly 100 million by the late 1970s. But in the 1980s, as China moved towards a market-based economy, Beijing swung to the other extreme: it privatized the pastures and gave yaks back to individual households, hoping that the move would push Tibetans to better manage their land and so boost its productivity. Despite the privatization, nomads continued to use the rangeland communally \u2014 often in groups led by village elders. Then the government began to limit herds, and it built fences to separate households and villages. \u201cThis has totally changed the way livestock are traditionally raised on the plateau, turning a mobile lifestyle into a sedentary existence,\u201d says Yang Xiaosheng, director of Henan county's rangeland-management office. The fencing policy does have merits when applied in moderation, says Y\u00f6nten Nyima, a Tibetan policy researcher at Sichuan University in Chengdu. Because an increasing number of nomads now lead a settled life \u2014 at least for parts of the year \u2014 it helps to control the level of grazing in heavily populated areas, he says. \u201cFencing is an effective way to keep animals out of a patch of meadow.\u201d Many herders also say that it makes life much easier: they do not have to spend all day walking the hills to herd their yaks and sheep, and if they go away for a few days, they don't worry about the animals running off. But the convenience comes at a cost, says Cao Jianjun, an ecologist at Northwest Normal University in Lanzhou. Fenced pastures often show signs of wear after a few years. In a 2013 study, Cao and his colleagues measured growth of the sedge species preferred by livestock in two scenarios: enclosed pastures and much larger patches of land jointly managed by up to 30 households. Despite similar livestock densities in both cases, the sedge grew twice as fast in the larger pastures, where animals could roam and plants had more opportunity to recover 1 . That matches the experience of Henan county herders, who say that their land sustains fewer animals than it has in the past. \n               Water worries \n             The future of the grasslands looked even bleaker as we left relatively well-to-do Henan county and ventured into the much higher, arid territory to the west. After 700 kilometres, we reached Madoi county, also known as  qianhu xian  ('county of a thousand lakes'), where the Yellow River begins. Although this region gets only 328 millimetres of rain on average each year, about half of what Henan receives, Madoi was once one of the richest counties on the plateau \u2014 famous for its fish, high-quality livestock and gold mines. Now, the wetlands are drying up and sand dunes are replacing the prairies, which means that less water flows into the Yellow River. Such changes on the plateau have contributed to recurring water shortages downstream: the Yellow River often dries up well before it reaches the sea, an event not recorded before 1970. In 2000, China sought to protect this region, along with adjacent areas that give rise to the Yangtze and Mekong Rivers, by establishing the Sanjiangyuan (or Three-Rivers' Headwaters) National Nature Reserve, an area nearly two-thirds the size of the United Kingdom. Nearly one-tenth of the reserve area falls into core zones in which all activities, including herding, are prohibited. The government spends hundreds of millions of US dollars each year on moving nomads out of those core areas, constructing steel meshes to stabilize the slopes and planting artificially bred grass species to restore the eroded land. Outside the core regions, officials have banned grazing on 'severely degraded grasslands', where vegetation typically covers less than 25% of the ground. Land that is 'moderately degraded', where vegetation coverage measures 25\u201350%, can be grazed for half of the year. Such policies \u2014 and related initiatives to limit livestock numbers and fence off areas of pasture \u2014 have not been easy on the herders, says Guo Hongbao, director of the livestock-husbandry bureau in Nagchu county in the southern Tibetan Plateau. \u201cThe nomads have made sacrifices for protecting the grasslands,\u201d he says. But he also says that the strategies have paid off. Guo and other officials point to satellite studies showing that the plateau has grown greener in the past three decades 2 . This increase in vegetation growth, possibly the result of a combination of grazing restrictions and climate change, \u201chas had a surprisingly beneficial effect on climate by dampening surface warming\u201d, says Piao Shilong, a climate modeller at Peking University. But ecologists say that such measurements look only at surface biomass and thus are not a good indicator of grassland health. \u201cNot all vegetation species are equal,\u201d says Wang. \u201cAnd satellites can't see what's going on underground.\u201d This is particularly important in the case of the sedge species that dominate much of the Tibetan Plateau, and that are the preferred food of livestock. These species, part of the  Kobresia  genus, grow only 2 centimetres above the surface and have a dense, extensive root mat that contains 80% of the total biomass. Studies of pollen in lake sediments show that  Kobresia  and other dominant sedges emerged about 8,000 years ago, when early Tibetans began burning forests to convert them to grasslands for livestock 3 . The prehistoric grazing helped to create the thick root mat that blankets the vast plateau and that has stored 18.1 billion tonnes of organic carbon. But  Kobresia  plants are being driven out by other types of vegetation, and there is a risk that the locked-up carbon could be released and contribute to global warming. Every now and then on the trip to Lhasa, we passed fields blooming with the beautiful red and white flowers of  Stellera chamaejasme , also known as wolf poison. \u201cIt's one of a dozen poisonous species that have increasingly plagued China's grasslands,\u201d says Zhao Baoyu, an ecologist at the Northwest Agriculture and Forestry University in Yangling. Zhao and his colleagues estimated that poisonous weeds have infested more than 160,000 square kilometres of the Tibetan grasslands, killing tens of thousands of animals a year 4 . Herders also report seeing new grass species and weeds emerge in recent years. Although most are not toxic, they are much less nutritious than  Kobresia  pastures, says Karma Phuntsho, a specialist on natural-resource management at ICIMOD. \u201cSome parts of the plateau may seem lush to an untrained eye,\u201d he says. \u201cBut it's a kind of 'green desertification' that has little value.\u201d In one unpublished study of the northeastern Tibetan Plateau, researchers found that  Kobresia  pastures that had gone ungrazed for more than a decade had been taken over by toxic weeds and much taller, non-palatable grasses: the abundance of the sedge species had dropped from 40% to as low as 1%. \u201c Kobresia  simply doesn't stand a chance when ungrazed,\u201d says Elke Seeber, a PhD student at the Senckenberg Natural History Museum in G\u00f6rlitz, Germany, who conducted the field experiment for a project supported by the German Research Foundation (DFG).  The policies are not guided by science, and fail to take account of climate change and regional variations.  The changes in vegetation composition have important implications for long-term carbon storage, says project member Georg Guggenberger, a soil scientist at Leibniz University of Hanover in Germany. In moderately grazed  Kobresia  pastures, up to 60% of the carbon that is fixed by photosynthesis went into the roots and soil instead of the above-ground vegetation \u2014 three times the amount seen in ungrazed plots 5 . This underground organic carbon is much more stable than surface biomass, which normally decomposes within a couple of years and releases its stored carbon into the air. So a shift from  Kobresia  sedge to taller grasses on the plateau will ultimately release a carbon sink that has remained buried for thousands of years, says Guggenberger. Critics of the grazing restrictions in Tibet say that the government has applied them in a blanket way, without proper study and without taking on board scientific findings. In some cases, they make sense, says Tsechoe Dorji, an ecologist at the ITPR's Lhasa branch, who grew up in a herder family in western Tibet. \u201cA total grazing ban can be justified in regions that are severely degraded\u201d, he says, but he objects to the simple system used by the government to classify the health of the grasslands. It only considers the percentage of land covered by vegetation and uses the same threshold for all areas, without adjusting for elevation or natural moisture levels. \u201cPastures with 20% vegetation cover, for instance, could be severely degraded at one place but totally normal at another,\u201d says Dorji. This means that some of the grasslands that are classified as severely degraded are actually doing fine \u2014 and the grazing ban is actually hurting the ecosystem. \u201cHaving a sweeping grazing policy regardless of geographical variations is a recipe for disasters,\u201d he says. \n               Fast forward \n             China's grazing policy is only one of several factors responsible for such damaging changes, say the researchers.  Pollution, global warming and a rash of road-building  and other infrastructure-construction projects have all taken a toll on the grasslands. Ten days after leaving Xining, we caught a glimpse of Tibet's future when we arrived at Nam Tso, a massive glacial lake in the southern part of the plateau. Here Dorji and Kelly Hopping, a graduate student at Colorado State University in Fort Collins, have been turning the clock forward by surrounding small patches of grassland with open-topped plastic chambers that artificially raise the temperature. These experiments are important because Tibet is a hotspot in terms of climate change; the average temperature on the plateau has soared by 0.3\u20130.4 \u00b0C per decade since 1960 \u2014 about twice the global average. In trials over the past six years, they found that  Kobresia pygmaea , the dominant sedge species, develops fewer flowers and blooms much later under warming conditions 6 . Such changes, says Dorji, \u201cmay compromise its reproductive success and long-term competitiveness\u201d.  Having a sweeping grazing policy regardless of geographical variations is a recipe for disasters.  At the experimental site, the artificially warmed pastures have been taken over by shrubs, lichens, toxic weeds and non-palatable grass species, says Hopping. But when the researchers added snow to some heated plots,  Kobresia  did not lose out to the other plants, which suggests that the loss of soil moisture might be driving the shift in species. Higher temperatures increase evaporation, which can be especially potent at high elevations. \u201cThis is not good news for species with shallow roots\u201d, such as the  Kobresia  favoured by livestock, she says. Piao says that \u201cthis interplay between temperature and precipitation illustrates the complexity of ecosystem responses to climate change\u201d. But researchers have too little information at this point to build models that can reliably predict how global warming will affect the grasslands, he says. To fill that gap, Wang and his colleagues started a decade-long experiment in 2013 at Nagchu, where they are using heat lamps to warm patches of grassland by precise amounts, ranging from 0.5 \u00b0C to 4 \u00b0C. They are also varying the amount of rainfall on the plots, and they are measuring a host of factors, such as plant growth, vegetation composition, nutrient cycling and soil carbon content. They hope to improve projections for how the grasslands will change \u2014 and also to determine whether there is a tipping point that would lead to an irreversible collapse of the ecosystem, says Piao. \n               Plateau prognosis \n             A fortnight into the trip, we finally arrived at the outskirts of Lhasa. At the end of the day, herders were rounding up their sheep and yaks in the shadows cast by snow-capped peaks. They and the other pastoralists across the plateau will have a difficult time in coming decades, says Nyima. Climate change was not a consideration when grassland polices were conceived over a decade ago, and so \u201cmany pastoralists are ill prepared for a changing environment\u201d, he says. \u201cThere is a pressing need to take this into account and identify sound adaptation strategies.\u201d As a start, researchers would like to conduct a comprehensive survey of plant cover and vegetation composition at key locations across different climate regimes. \u201cThe information would form the baseline against which future changes can be measured,\u201d says Wang. Many scientists would also support changes to the grazing ban and fencing policies that have harmed the grasslands. Dorji says that the government should drop the simplistic practice of 'one policy fits all' across the plateau and re-evaluate whether individual regions are degraded enough to merit a ban on grazing. \u201cUnless the pastures are severely degraded, moderate grazing will help to restore the ecosystems,\u201d he says. But scientists are not banking on such reforms happening soon. Policies in Tibet are driven less by scientific evidence than by bureaucrats' quest for power and funds, says a Lhasa-based researcher who requests anonymity for fear of political repercussions. Local officials often lobby Beijing for big investments and expensive projects in the name of  weiwen  (meaning 'maintaining stability'). Because resistance to Chinese control over Tibet continues to flare up, the government is mostly concerned with maintaining political stability, and it does not require local officials to back up plans with scientific support, says the researcher. \u201cAs long as it's for  weiwen , anything goes.\u201d But officials such as Guo say that their policies are intended to help Tibet. \u201cAlthough there is certainly room for improvement in some of the policies, our primary goals are to promote economic development and protect the environment,\u201d he says. Far away from Lhasa, herders such as Dodra say that they are not seeing the benefits of government policies. After we finish our visit at his home, Dodra's entire family walks us into the courtyard \u2014 his mother in-law spinning a prayer wheel and his children trailing behind. It has stopped snowing, and the sky has turned a crystal-clear, cobalt blue. \u201cThe land has served us well for generations,\u201d says Dodra as he looks uneasily over his pasture. \u201cNow things are falling apart \u2014 but we don't get a say about how best to safeguard our land and future.\u201d \n                 Tweet \n                 Follow @NatureNews \n               \n                 weibo \n               A  related story  at SciDev.Net explores how climate change will affect Tibetan herders. \n                     Droughts threaten high-altitude Himalayan forests 2015-Jan-27 \n                   \n                     Tibetan plateau gets wired up for monsoon prediction 2014-Oct-01 \n                   \n                     Double threat for Tibet 2014-Aug-19 \n                   \n                     Floods spur mountain study 2013-Sep-04 \n                   \n                     Thawing permafrost reduces river runoff 2012-Jan-06 \n                   \n                     China: The third pole 2008-Jul-23 \n                   \n                     \n                         Nature Geoscience  \n                       \n                   \n                     \n                         Nature Climate Change  \n                       \n                   \n                     Third Pole Environment \n                   \n                     International Centre for Integrated Mountain Development \n                   \n                     Institute of Tibetan Plateau Research \n                   Reprints and Permissions"},
{"file_id": "529146a", "url": "https://www.nature.com/articles/529146a", "year": 2016, "authors": [{"name": "Maggie Koerth-Baker"}], "parsed_as_year": "2006_or_before", "body": "Implicated in everything from traumatic brain injury to learning ability, boredom has become extremely interesting to scientists. In 1990, when James Danckert was 18, his older brother Paul crashed his car into a tree. He was pulled from the wreckage with multiple injuries, including head trauma. The recovery proved difficult. Paul had been a drummer, but even after a broken wrist had healed, drumming no longer made him happy. Over and over, Danckert remembers, Paul complained bitterly that he was just \u2014 bored. \u201cThere was no hint of apathy about it at all,\u201d says Danckert. \u201cIt was deeply frustrating and unsatisfying for him to be deeply bored by things he used to love.\u201d Noah Baker investigates how to study boredom, with Maggie Koerth-Baker. A few years later, when Danckert was training to become a clinical neuropsychologist, he found himself working with about 20 young men who had also suffered  traumatic brain injury . Thinking of his brother, he asked them whether they, too, got bored more easily than they had before. \u201cAnd every single one of them,\u201d he says, \u201csaid yes.\u201d Those experiences helped to launch Danckert on his current research path. Now a cognitive neuroscientist at the University of Waterloo in Canada, he is one of a small but growing number of investigators engaged in a serious scientific study of boredom. There is no universally accepted definition of boredom. But whatever it is, researchers argue, it is not simply  another name for depression  or apathy. It seems to be a specific mental state that people find unpleasant \u2014 a lack of stimulation that leaves them craving relief, with a host of behavioural, medical and social consequences. In studies of binge-eating, for example, boredom is one of the most frequent triggers, along with feelings of depression and anxiety 1 , 2 . In a study of distractibility using a driving simulator, people prone to boredom typically drove at higher speeds than other participants, took longer to respond to unexpected hazards and drifted more frequently over the centre line 3 . And in a 2003 survey, US teenagers who said that they were often bored were 50% more likely than their less-frequently bored peers to later take up smoking, drinking and illegal drugs 4 . Boredom even accounts for about 25% of variation in student achievement, says Jennifer Vogel-Walcutt, a developmental psychologist at the Cognitive Performance Group, a consulting firm in Orlando, Florida. That's about the same percentage as is attributed to innate intelligence. Boredom is \u201csomething that requires significant consideration\u201d, she says. Researchers hope to turn such hints into a deep understanding of what boredom is, how it manifests in the brain and how it relates to factors such as self-control. But \u201cit's a ways out before we're answering those questions\u201d, says Shane Bench, a psychologist who studies boredom in the lab of Heather Lench at Texas A&M University in College Station. In particular, investigators need better ways to measure boredom and more reliable techniques for making research subjects feel bored in the lab. Still, the field is growing. In May 2015, the University of Warsaw drew almost 50 participants to its second annual conference on boredom, which attracted international speakers from social psychology and sociology. And in November, Danckert brought together about a dozen investigators from Canada and the United States for a workshop on the subject. Researchers in fields from genetics to philosophy, psychology and history are starting to work together on boredom research, says John Eastwood, a psychologist at York University in Toronto, Canada. \u201cA critical mass of people addressing similar issues creates more momentum.\u201d \n               A measure of malaise \n             The scientific study of boredom dates back to at least 1885, when the British polymath Francis Galton published 5  a short note in  Nature  on ' The Measure of Fidget ' \u2014 his account of how restless audience members behaved during a scientific meeting. But decades passed with only a few people taking a serious interest in the subject. \u201cThere are things all around us that we don't think to look at, maybe because they appear trivial,\u201d says Eastwood. That began to change in 1986, when Norman Sundberg and Richard Farmer of the University of Oregon in Eugene published their Boredom Proneness Scale (BPS) 6 , the first systematic way for researchers to measure boredom \u2014 beyond asking study participants, \u201cDo you feel bored?\u201d. Instead, they could ask how much participants agreed or disagreed with statements such as: \u201cTime always seems to be passing slowly\u201d, \u201cI feel that I am working below my abilities most of the time\u201d and \u201cI find it easy to entertain myself\u201d. (The statements came from interviews and surveys that Sundberg and Farmer had conducted on how people felt when they were bored.) A participant's aggregate score would give a measure of his or her propensity for boredom. The BPS opened up new avenues of research and made it apparent that boredom was about restlessness as much as apathy, the search for meaning as much as ennui. It has served as a launching point for other boredom scales, a catalyst for making the field more important and a tool for connecting boredom to other factors, including mental health and academic success. But it also has some widely acknowledged flaws, says Eastwood. One is that the BPS is a self-reported measure, which means that it is inherently subjective. Another is that it measures susceptibility to boredom \u2014 'trait boredom' \u2014 not the intensity of the feeling in any given situation, which is known as state boredom. Studies consistently show that these two measures are independent of each other, yet researchers are only beginning to tease them apart. This can be particularly confounding in educational settings.  Shifts in teaching style or classroom environment  are unlikely to reduce students' trait boredom, which is intrinsic and slow to change, but can be very effective at reducing state boredom, which is purely situational. The BPS has often been misused to measure both forms of boredom at the same time, yielding answers that are likely to be misleading, says Eastwood. Scientists are still hashing out how to improve on the BPS. In 2013, Eastwood helped to develop the Multidimensional State Boredom Scale (MSBS) 7 , which features 29 statements about immediate feelings, such as: \u201cI am stuck in a situation that I feel is irrelevant.\u201d Unlike the BPS, which is all about the participant's habits and personality, the MSBS attempts to measure how bored people feel in the moment. And that, Eastman hopes, will give it a better shot at revealing what boredom is for everybody. But to measure boredom, researchers must first make sure that study participants are bored. And that is a whole different challenge. \n               The most boring video ever \n             One way to create a particular mood, used for decades in psychology, is to show people a video clip. There are scientifically validated videos for inducing happiness, sadness, anger, empathy and many other emotions. So when she was working on her dissertation at Waterloo in 2014, Colleen Merrifield decided to make a video that would bore most people to tears.  When she was working on her dissertation, she decided to make a video that would bore most people to tears.  In Merrifield's video, two men stand in a white, windowless room. Silently, they take clothes from a pile between them and hang them on a white rack \u2014 a camisole, a shirt, a sweater, a sock. The seconds tick by: 15, 20, 45, 60. The men keep hanging laundry. Eighty seconds. One of the men asks the other for a clothes peg. One hundred seconds. They keep hanging laundry. Two hundred seconds. They keep hanging laundry. Three hundred seconds. They keep hanging laundry. Shown on a loop, the video can last for as long as five and a half minutes. Perhaps unsurprisingly, the people to whom Merrifield showed this found it stupefyingly dull 8 . But then she tried using the video to study how boredom affected the ability to focus and pay attention. Her protocol called for participants to carry out a classic cognitive attention task \u2014 watching for star-like light clusters to appear or disappear on a monitor \u2014 then to sit through the video to get good and bored, and finally to do the task again so that she could see how boredom affected their performance. But she found that she had to redesign the experiment: the task was boring people more than the video. This was not entirely unexpected. Previous studies of boredom had often used tasks instead of videos. But it also demonstrated the problem. There are so many ways for researchers to bore people with tasks \u2014 asking them to proofread address labels, say, or to screw nuts and bolts together \u2014 that it had always been difficult to compare individual studies. For instance, different studies have found boredom to be correlated with both rising and falling heart rate 9 . But without a standardized method for inducing boredom, it is impossible to work out who is right. In 2014, researchers at Carnegie Mellon University in Pittsburgh, Pennsylvania, published a paper 9  that aimed to begin the process of standardization. It compared six different boredom inductions, representing three broad classes \u2014 repetitive physical tasks, simple cognitive tasks, and video or audio media \u2014 as well as a control video. The researchers used the MSBS to see how intensely each task elicited boredom, and a measure called the Differential Emotion Scale to see whether each task elicited boredom alone, or a number of other emotions. All six tasks were significantly more boring than the control and all six caused boredom almost exclusively. The best of the bunch was a task that required participants to click a mouse button to rotate a computer icon of a peg a quarter of a turn clockwise, over and over. After that, says Danckert, \u201cI think I might be abandoning the video\u201d to induce boredom in the lab. Instead, he will rely on behavioural tasks. The inexactness of the tools leaves holes in what researchers can reasonably say about boredom. For instance, many real-world problems that are highly correlated with boredom are connected to the idea of self-control, including addiction, gambling and binge-eating 10 . \u201cI characterize boredom as a deficiency in self-regulation,\u201d Danckert says. \u201cIt's a difficulty of engaging with tasks in your environment. The more self-control you have, the less likely you are to be bored.\u201d But does this mean that self-control and boredom are measures of the same thing? Even Danckert is uncertain. Consider people with a history of traumatic brain injury. \u201cFailures of self-control are their problem,\u201d he says. \u201cThey might be inappropriately impulsive; there's increased risk-taking; they might also engage in drug and alcohol abuse.\u201d Danckert certainly saw his brother, Paul, experience all those things in the wake of his injury. But in Danckert's research sample of people with traumatic brain injury \u2014 who are predominantly in their 40s \u2014 ageing seems to have weakened the link between boredom and self-control. In data that are not yet published, Danckert says, his patients report levels of self-control no lower than those of the general population, but their boredom-proneness scores are much higher. By contrast, Danckert's brother seems to demonstrate the opposite effect. He struggled for years with self-control issues, but eventually became less bored and reclaimed his love of music. \u201cIt's the most important thing in his life, next to his children,\u201d Danckert says. So there is reason to suspect that boredom and self-control can exist independently \u2014 but there is not yet enough evidence to understand much beyond that. \n               Painfully dull \n             Despite all this uncertainty, researchers see themselves as laying a foundation, creating tools and standards that will allow them to tackle really important questions. \u201cWe're establishing boredom as a testable construct,\u201d says Bench.  We're establishing boredom as a testable construct.  Defining boredom is an important part of that. Different researchers have different pet definitions: a German-led team, for example, identifies five types of boredom 11 . But most workers in the field agree that, at least some of the time, people will work very hard to relieve boredom. This not only presents a more active version of boredom than most people are probably used to, but also has tangible connections to efforts to address boredom in the real world. Lench and Bench are testing whether the drive to become un-bored is so strong that people might be willing to choose unpleasant experiences as an alternative. This idea builds on research that has shown a correlation between sensation-seeking behaviour, even risky behaviour, and high boredom-proneness scores 12 . It is also similar to findings published in  Science 13  in 2014 and  Appetite 14  in 2015. In the first study, researchers asked people to sit in a room with nothing to do for as long as 15 minutes at a time. Some of the participants, particularly men, were willing to give themselves small electric shocks rather than  be left alone with their thoughts . The second paper described two experiments: one in which the participants had access to unlimited sweets, and another in which they had access to unlimited electric shocks. Participants ate more when they were bored \u2014 but they also gave themselves more shocks. Even when it is not very pleasant, apparently, novelty is better than monotony. Novelty might also have a role in overcoming boredom in the classroom. In 2014, for instance, researchers led by psychologist Reinhard Peckrun of the University of Munich in Germany reported 15  how they had followed 424 university students over the course of an academic year, measuring their boredom levels and documenting their test scores. The team found evidence of a cycle in which boredom begot lower exam results, which resulted in more disengagement from class and higher levels of boredom. Those effects were consistent throughout the school year, even after accounting for students' gender, age, interest in the subject, intrinsic motivation and previous achievement. But other studies suggest that novelty can disrupt this cycle 16 . Sae Schatz, director of the Advanced Distributed Learning Initiative, a virtual company that develops educational tools for the US Department of Defense, points to one experiment 17  with a computer system that tutored students in physics. When the system was programmed to insult those who got questions wrong and snidely praise those who got them right, says Schatz, some students, especially adult learners, saw improved outcomes and were willing to spend longer on the machines. Schatz thinks that this could be because the insults provided enough novelty to keep people engaged and less prone to boredom. Looking to the future, researchers such as Eastwood are intent on finding better ways to understand what boredom is and why it is correlated to so many other mental states. They also want to investigate boredom in people who aren't North American college students. That means testing older people, as well as individuals from diverse ethnic and national backgrounds. And, given the impact that boredom may have on education, it also means developing versions of the BPS and MSBS that can be administered to children. Many researchers likewise hope to expand on the types of study being done. To get beyond self-reported data, Danckert wants to start looking at brain structures, and seeing whether there are differences between people who score highly on the BPS and those who don't. These data could help him to understand why boredom manifests so strongly in some people with traumatic brain injury. There's also a need, Danckert says, for more scientists to realize that boredom is fascinating. \u201cWe may be on the cusp of having enough people to advance a little more quickly,\u201d he says. \n                 Tweet \n                 Follow @NatureNews \n               \n                 weibo \n               \n                     Why we are teaching science wrong, and how to make it right 2015-Jul-15 \n                   \n                     Depression: A change of mind 2014-Nov-12 \n                   \n                     We dislike being alone with our thoughts 2014-Jul-03 \n                   \n                     Advanced Distributed Learning \n                   Reprints and Permissions"},
{"file_id": "529274a", "url": "https://www.nature.com/articles/529274a", "year": 2016, "authors": [{"name": "Chris Cesare"}], "parsed_as_year": "2006_or_before", "body": "The United States has invested in a grand ecological observatory, but the project has been dogged by budget overruns and delays. Overgrown shrubs thwack the sides of a pick-up truck as it bounces along a dirt road through a forest in western Virginia. On the drive, ecologist Ty Lindberg calls out the names of the invasive species crowding either side of the path. There is mile-a-minute weed, which spreads with alarming speed. A flowering annual called Asian stiltgrass carpets the ground and stifles native plants. And a particularly prickly species of rose tears at the clothes of anybody who ventures too close. \u201cMy field techs don't enjoy that one,\u201d Lindberg says. Farther along, he stops the car at a break in the brush and picks his way through the undergrowth towards a set of plastic and aluminium stakes poking out of the ground. They mark out a 40-by-40-metre plot, one of dozens scattered throughout 1,300 hectares of forest and pasture near the town of Front Royal. From April to October, field technicians spend their days cataloguing the location, diameter and height of nearly every tree in the plot, collecting fallen leaves out of a trap and archiving pressings from invasive plants. Their main goal is to measure the ecosystem's metabolism, especially how much biomass it generates each year. At other plots, technicians trap rodents and draw blood samples to test for diseases, including those that could spread to humans. The staff collects and stores ticks and beetles, and takes soil samples to study the bacteria underfoot. Higher up in the hills, a 50-metre-tall metal tower juts above the trees, loaded with long booms holding sensors that monitor air temperature, wind speed and solar radiation at multiple altitudes. When the tower has its final instrument package installed in 2016, it will watch the forest breathe by monitoring how carbon dioxide and water vapour concentrations rise and fall. This site is one of more than 80 planned for the National Ecological Observatory Network (NEON), a US$434-million project to  build a biological observatory  that spans the United States. Its goals are grand. If all goes well, it will document the effects that climate change and land use have on ecosystems and provide scientists with a nearly real-time measure of the country's ecological vital signs. Many of the sites will operate for three decades, whereas others will be packed up and relocated periodically in response to environmental changes. And the data collected will be freely available to all through an online portal. Lindberg, who manages three NEON sites in Virginia and Maryland, says that the long-term record generated by NEON could transform ecology by helping scientists to answer questions ranging from how invasive species are altering the landscape to how quickly infectious diseases are spreading through ecosystems. The network, he says, \u201cis really an instrument\u201d. Ecologists call it their first foray into big science \u2014 a massive project that rivals the scale of big-budget physics facilities such as particle colliders or telescopes (see 'Sentry posts'). But ecologists have not had an easy journey into the world of big science. During its five-year construction phase, NEON has encountered a  series of high-profile problems  that have raised concerns about the programme, which is funded entirely by the US National Science Foundation (NSF). In June 2015, the network came under fire from the NSF and Congress after NEON, Inc. \u2014 the non-profit organization that manages the project \u2014 reported that it was running $80 million over budget. Amid revelations that the company had spent federal money on parties, Congress levied charges of mismanagement and convened hearings with officials from NEON and the NSF. Events came to a climax in December, when the NSF decided to take NEON, Inc. off the project, citing a lack of confidence in the company after years of delays and questions about accounting irregularities. The agency will now seek another operator to complete construction and take over the project's management. One of the toughest tasks will be winning the support of ecologists; some researchers felt alienated during the project's planning phase and have been critical of the way the observatory network is turning out. Still, many ecologists are eager to get their hands on NEON's data and are already thinking about how to incorporate it into their studies. Ultimately, the science that they produce will determine whether the project succeeds or fails. \u201cYou build out NEON and in 30 years you're going to have unprecedented data on how ecosystems are changing,\u201d says Peter Groffman, an ecosystem ecologist at the City University of New York. \u201cIt's very exciting and very much the next logical evolution of long-term studies.\u201d \n               Big biology \n             The debates about NEON reach back to its early evolution, when it took shape in a manner very different from a major physics project. Scott Collins, an ecologist at the University of New Mexico in Albuquerque, was the first NSF programme director for NEON back in 2000. Collins says that the idea for a large ecological observatory sprang from NSF staff who were seeking ways for biologists to get a slice of the agency's big-science money: the Major Research Equipment and Facilities Construction budget. \u201cThat put us on a very different footing from the start because this was not something that the community and vocal ecologists had wanted,\u201d Collins says. Although researchers did not dream up the project, they quickly embraced the idea and took the lead in moulding NEON's design during workshops. At these meetings, attendees were encouraged to dream big, says Katherine Gross, the director of Michigan State University's W. K. Kellogg Biological Station in Hickory Corners and the current chair of the NSF's biological-sciences advisory committee. During six workshops between 2000 and 2002, ecologists developed a plan for a flexible network of observatories and experimental centres spread across at least ten sites. But scientists disagreed over whether the NSF should specify research themes for each site or allow ecologists to choose their own focus. In its 2002 budget request to Congress, the NSF asked for $12 million to develop and build two prototype NEON sites, largely based on the reports from the workshops. But Congress denied the request, citing a lack of information about the project and an insufficient estimate of its costs. At the time, the best model that US ecologists had for NEON was the Long Term Ecological Research network, a group of investigators in the United States who study the ecology of a particular spot for five or more years with sets of measurements specialized to each site. The leaders of NEON, however, eventually settled on a one-size-fits-all approach, with standard protocols and instruments that could be deployed across the entire network to study pressing issues, including changes to biodiversity and climate change. And instead of having principal investigators propose individual observatories, an expert panel recommended that the NSF develop NEON as a nationwide network managed by one entity. Encouraged by Congress to continue refining its idea of NEON, the NSF issued a call for proposals in early 2004. A $6-million grant to design NEON went to several members of the American Institute of Biological Sciences in Reston, Virginia, a non-profit organization that had been involved in the project's earlier planning. A little more than a year later, in December 2005, the lead designers created NEON, Inc. Over the next several years, the structure of NEON took shape. The network split the country into 20 domains, each with several sites outfitted with instruments and collection protocols. \n               Feeling ignored \n             But when it came to choosing where to build sites and how best to make measurements, some ecologists objected to the choices and felt that their expertise had been ignored. Gene Kelly, a soil scientist at Colorado State University in Fort Collins \u2014 and now the interim chief executive of NEON, Inc. \u2014 says that the emphasis on measuring the same quantities everywhere meant that NEON had to sacrifice having the optimal protocol for every spot. \u201cThe only way to really handle it was to standardize it, but in doing that you lose a little,\u201d he says. Kelly says that many ecologists, including him, stopped following the progress of the project closely after these decisions were made, partly because NEON, Inc. stopped asking for input. Despite the loss of some engagement from the ecological community, the NSF approved NEON's final design in 2009, and Congress authorized the money for construction in July 2011. The network would spread 17,000 sensors measuring hundreds of variables \u2014 from soil moisture to stream pH \u2014 across nearly 100 sites. And at each site, technicians would collect a suite of biological samples, including genomic data from many organisms and whole specimens of insects and small mammals. The result would be a standard set of ecological data that would allow scientists to compare and watch for changes in ecosystems and to produce ecological forecasts. Concerns about the company's accounting and the NSF's oversight cropped up almost immediately after construction began in 2012. A review that year found that NEON's books were a mess: auditors questioned more than one-third of the total construction cost \u2014 $154 million \u2014 and determined that NEON, Inc. did not provide enough information to support its proposed budget. Later audits and investigations unearthed questionable spending by NEON management, including $25,000 for a party and $3,000 for T-shirts. Also, the company moved to a new office and paid nearly $500,000 for time left on the old lease. After the audits, the NSF's inspector-general urged the agency to keep a closer eye on the project. Despite the accounting problems, the NSF and NEON, Inc. forged ahead with construction \u2014 and ran straight into delays. Some could have been predicted, such as the difficulty of obtaining permits to build observation towers in cities. Others were simply bad luck. At the site in western Virginia, a tree fell over and destroyed a collection of atmospheric instruments. A bear damaged fibre-optic data lines running to soil-monitoring instruments near the Virginia site's tower. And concerns about the health of a pregnant cheetah at a nearby conservation facility forced NEON, Inc. to abandon plans to use a helicopter to hoist the topmost sections of the instrument tower into place. Instead, construction staff raised the final sections by hand. The delays put NEON behind schedule and over budget. In June 2015, the company told the NSF that it would take an extra $80 million on top of the $434-million budget to complete construction. With Congress already concerned about the NSF's stewardship, the agency demanded that the project be downsized to stay within its budget. It told NEON, Inc. to cull the number of sites from 95 to 81, cancel construction of its stream experiments and give up some of its embedded sensors. NEON, Inc. then fired its chief executive last September and appointed Kelly to serve as an interim. But the company sealed its fate in December when it submitted an updated budget that again had extra costs and delays. The NSF decided to look for a new company to manage the project. Whoever takes over will step into a difficult role, as many ecologists remain disconnected from the project. Yet there are still big hopes for NEON in the research community. \u201cI think it's good for scientific communities to dream big and say, 'OK this will be our unifying project',\u201d says Ash Ballantyne, a bioclimatologist at the University of Montana in Missoula. \u201cIt's analogous to our LHC.\u201d \n               Global reach \n             Interest is growing as money starts to flow towards individual researchers. In August, the NSF awarded $4.8 million in grants to investigators and workshop organizers who are interested in using NEON data. Ballantyne received $300,000 to study the effects of drought, fire and insect infestations on the carbon cycle. He plans to investigate how drought or other disturbances predispose trees to a beetle outbreak or fires. Jim Clark, an ecologist and statistician at Duke University in Durham, North Carolina, won a grant to build more-sophisticated ecological models. \u201cWe've always modelled on a species-by-species basis,\u201d Clark says. \u201cIf there's 100 species, someone has fitted 100 different models and just added them together.\u201d This ignores the interactions between species, he says, and NEON data on species abundance could help to fit and train joint models for how species respond to ecosystem changes collectively. Frank Davis, an environmental scientist at the University of California, Santa Barbara, says that he plans to use NEON's airborne observations to study tree cover at various scales, from a few centimetres to several kilometres. Many ecologists are not accustomed to thinking at the large scale that NEON covers, Davis says. \u201cUltimately, I think NEON will be ready for ecologists,\u201d he says. \u201cBut will ecologists be ready for NEON?\u201d Some are gaining valuable experience thinking at global scales by running their own distributed networks. The Global Lakes Ecological Observatory Network began in 2005 and ties together groups around the world that monitor human effects on lake ecosystems. The Nutrient Network, or NutNet, links researchers on six continents who perform a standard set of experiments looking at how plant production in grasslands is limited by phosphorus and nitrogen \u2014 two by-products of fossil-fuel combustion. Other networks are springing up to study plant populations and drought. These projects are smaller in scope than NEON, which gives researchers more control over the work. With only a handful of voices deciding how to conduct experiments or take data, projects such as NutNet can maintain a tight focus on the science. \u201cIt's very hard for NEON to do this because the entire ecological community has a say,\u201d says Elizabeth Borer, an ecologist at the University of Minnesota in St Paul and a member of NutNet. Ecologists are still struggling to learn how to manage large projects, says Nikki Thurgate, an ecologist at the University of Adelaide in Australia and leader of international engagement for the Terrestrial Ecological Research Network \u2014 a smaller, Australian cousin of NEON. But if ecology is to forecast the problems that arise from climate change and loss of biodiversity around the world, it will need the data from large-scale networks. And one of the challenges is to keep the community engaged and informed while they wait for a grand scientific instrument to be built. \u201cYou can't pop up continent-wide environmental monitoring and have data in a couple of years,\u201d Thurgate says. \u201cIt's just not that simple.\u201d NEON's early struggles may fade when data start to arrive in the next few years from sites such as the one that Lindberg manages in western Virginia. On a cold day late last year, Lindberg \u2014 who still has his job for the time being \u2014 stood below the nearly finished observation tower rising high above the surrounding forest. In a nearby shed, dozens of boxes held sensors and electronics to be installed on the tower. Despite the work that remains here and at other sites around the country, Lindberg still thinks that the project can be successful \u2014 as long as researchers embrace it. \u201cIt's a scaffolding,\u201d he says. \u201cBut this thing doesn't work unless scientists use it.\u201d \n                 Tweet \n                 Follow @NatureNews \n               \n                     Obama budget seeks big boost for science 2015-Feb-03 \n                   \n                     Critics allege misuse of funds by US ecology project 2014-Dec-03 \n                   \n                     NEON and on 2011-Aug-10 \n                   \n                     US launches eco-network 2011-Aug-09 \n                   \n                     Report raises hopes for grand network of US ecology centres 2003-Sep-25 \n                   \n                     The history of NEON \n                   Reprints and Permissions"},
{"file_id": "529452a", "url": "https://www.nature.com/articles/529452a", "year": 2016, "authors": [{"name": "Shaoni Bhattacharya"}], "parsed_as_year": "2006_or_before", "body": "Songbirds are a culinary delicacy in Cyprus \u2014 but catching and eating them is illegal. Even so, the practice is on the rise and could be threatening rare species. It wasn't until I saw the blade glinting in the sunlight that I realized how grave the situation was. Broad and belligerent in army fatigues, the man strode along the track, ranting in Greek. Behind his back, his hands flexed a knife blade in and out of its wooden handle. This man was a trapper, a poacher of birds \u2014 and he clearly didn't want company. \u201cWhat are you doing here?\u201d he demanded. My companions and I had come to this dry scrubland on the Mediterranean island of Cyprus to look for evidence of songbird trapping. The birds are caught illegally and eaten in a traditional dish called  ambelopoulia  \u2014 and I was joining a September trip to monitor the extent of trapping. With me was Roger Little, a British conservation volunteer, and Savvas, a field officer with the conservation group BirdLife Cyprus whose name has been changed to protect his identity. We didn't expect to encounter trappers at this spot in the southeastern region of Cape Pyla; they usually work at night, when the birds are active. But now it seemed that they had started patrolling the site during the day. \u201cYou are on my land,\u201d the trapper said to us in Greek. \u201cIf this is your property, then I apologize \u2014 we didn't know, we are going,\u201d Savvas said. We acted casual as the man escorted us back to the battered four-by-four in which we had come. \u201cI shouldn't really be letting you go,\u201d he muttered. Moments later, we were driving away. Bird trapping in Cyprus has grown into a controversy that encompasses crime, culture, politics and science. The practice was made illegal more than 40 years ago \u2014 but that simply forced it underground. Today, trappers routinely cut wide corridors through vegetation and string fine 'mist nets' from poles to catch the birds, which are sent to local restaurants and quietly served. A platter of a dozen birds sells for \u20ac40\u201380 (US$44\u201387), and the trade in songbirds is responsible for an estimated annual market of \u20ac15 million. The delicacy is so prized and lucrative that it is suspected to be linked to organized crime, and those trying to stop it have been subject to intimidation and violence. Conservation organizations say that the trapping is increasing and that it is threatening rare bird species that stop in Cyprus during their migration. Last March, a report by BirdLife Cyprus suggested that some 2 million birds had been killed in the previous autumn, including 78 threatened species. The group claims that trapping \u2014  on top of threats  from climate change, habitat loss and  invasive species  \u2014 could cause irreparable damage to some bird populations. \u201cIllegal bird killing just cannot be justified, it's like the last kick off the cliff for some species,\u201d says Clairie Papazoglou, executive director of BirdLife Cyprus near Nicosia. But the picture is not black and white, in part because the extent of bird killing is disputed and its effects on bird populations are unclear. Critics have questioned the methods used by BirdLife Cyprus to estimate the numbers being captured on the island. The debate led to a workshop last July to discuss the science, with representatives from all agencies involved. Attendee Alison Johnston, an ecological statistician at the British Trust for Ornithology (BTO), a charitable research institute in Thetford, says that so little is known about the population sizes and routes of migratory birds in the Mediterranean that it is difficult to assess the full impacts of trapping. \u201cIf we knew more about the numbers,\u201d she says, \u201cwe could say whether this is a critical number being killed.\u201d The debate over Cyprus's songbirds could have wider repercussions, because bird killing is rife in other parts of the world. A 2015 report from BirdLife International estimates that hunters are killing about 25 million birds a year over the whole Mediterranean region; Cyprus stands out because so many are killed in such a small country. Globally, more than half of the world's migratory bird populations are thought to be in decline. \u201cThis isn't just an issue for Cyprus, or Africa, or Europe,\u201d says Claire Runge, a conservation scientist at the University of Queensland in Brisbane, Australia, who led a study published last December showing that only 9% of migratory birds worldwide are adequately protected across their range 1 . \u201cCountries will need to work together to find a solution to what is essentially a human\u2013wildlife conflict,\u201d she says. Papazoglou worries that what is happening in Cyprus sets a dangerous precedent. \u201cThis level of rampant illegality in an EU country sends a terrible message to the rest of the world. If rich, stable and well-run countries cannot enforce wildlife law, what hope is there to get fragile countries in the Middle East and Africa to act?\u201d \n               Gateway to continents \n             Situated in the far southeastern corner of the Mediterranean, Cyprus is a gateway to three continents and has been fought over for millennia (see 'Trapped in Cyprus'). It is currently sliced up into four jurisdictions: the Republic of Cyprus and the Turkish-occupied region of Northern Cyprus, separated by a UN buffer zone, and two small pockets called Sovereign Base Areas (SBAs) that were retained by the United Kingdom after the island gained independence in 1960 because of their strategic military importance. (Britain is currently using one of these areas to deploy air strikes to Syria.) The island's location also makes it an ideal rest-stop for migratory birds. Nearly half of the bird species from Europe, North Africa and the Middle East are thought to use the island as a migratory staging post as they fly south in the autumn, and back again in spring. These include common birds such as sparrows and the European robin ( Erithacus rubecula ), as well as threatened species including the barn owl ( Tyto alba ), common kingfisher ( Alcedo atthis ) and European turtle dove ( Streptopelia turtur ). All of these creatures have been found in the trappers' nets, as have some threatened, non-migratory, endemic bird species such as the Cyprus warbler ( Sylvia melanothorax ) and the Cyprus wheatear ( Oenanthe cypriaca ). The practice of trapping dates back to a time when birds were among the few easily found sources of protein on this arid island. Originally,  ambelopoulia  would have been a plate of blackcaps ( Sylvia atricapilla ), but the dish has extended to include 22 species of songbird. The traditional trapping method is to ensnare birds in trees with strategically placed 'limesticks' \u2014 twigs coated in a goo of mud mixed with Syrian plum juice. But in 1974, laws were introduced to ban non-selective capture methods, including limesticks and mist nets. Bird trapping is also illegal under the European Union (EU) Birds Directive and the Convention on the Conservation of European Wildlife and Natural Habitats (known as the Bern Convention), both of which Cyprus has adopted. The practice never stopped. Many Cypriots argue that bird trapping for  ambelopoulia  is a tradition and a right, and it has become a highly emotive issue. In the Famagusta district, raids on restaurants and arrests related to bird trapping have sparked public protests, and some politicians either covertly or overtly support it. Last December, Evgenios Hamboullas, Famagusta member of parliament for the incumbent Democratic Rally party, posted a photo of himself on Facebook seated in front of a plate of songbirds with the caption: \u201cSoon in our restaurants! Happy holidays!\u201d The post received nearly 600 likes in 5 days, and condemnation from his party. Conservation groups believe that bird trapping is rising fast; last year's BirdLife Cyprus report said that the practice had reached \u201cindustrial scale\u201d. Trappers rip out the island's native scrub bushes, then plant and irrigate lush, bright-green acacia trees that attract birds. They cut corridors through the groves and string mist nets across them from poles. When Savvas, Roger and I stopped at a well-known trapping hotspot, the evidence was everywhere: metal poles were concreted into bases made of empty tyres; black irrigation pipes criss-crossed the dusty earth; old carpets covered the ground to stop vegetation growing where the nets hang. Earlier in the trip, we found an MP3 player high in an acacia tree broadcasting a repetitive birdsong \u2014 a 'tape lure' used to attract the birds. Nearby, a red-backed shrike ( Lanius collurio ) and a sparrow both flailed frantically, their feet and wing tips glued onto limesticks balanced high in the tree. \n               Death toll \n             Conservationists first started systematically monitoring the extent of bird trapping in 2002, using a protocol developed by BirdLife Cyprus and the United Kingdom's Royal Society for the Protection of Birds, in consultation with the Cyprus Game and Fauna Service (part of the Ministry of Interior) and the British SBA police. The figures showed an initial dip in trapping around the time that Cyprus acceded to the EU, then an upward trend from 2007. But the 2014 trapping figures, published last year, caused a particular stir. The 2 million birds that BirdLife Cyprus estimated were captured during the previous trapping season was the biggest jump since monitoring had begun. The report also broke down trapping trends by jurisdiction, and found that the SBAs accounted for much of the increase. It estimated that 900,000 birds were killed there \u2014 even though the regions take up only 3% of the land \u2014 and that there had been a 199% increase since 2002. By contrast, the Republic of Cyprus had seen a downturn in illegal trapping. (Bird killing is not thought to be a major problem in Northern Cyprus.) The record-breaking numbers prompted criticism and headlines, and led some conservationists and media to imply that the British authorities were turning a blind eye to trapping so as not to upset the local community. The SBA Administration told BirdLife Cyprus that it \u201cdoes not accept the survey findings\u201d and questioned some of the figures in the report. According to people at the July meeting, the administration was particularly concerned with how the estimates were reached. The main trapping survey is carried out over a six-week period in the autumn migratory season \u2014 also the main bird-hunting season. The surveillance team regularly visits 60 sites, each one kilometre square, that are deemed prime trapping territory and assigns them one of five categories on the basis of the scale of mist netting that it observes \u2014 from 'active set net' (where trappers have left a net unfurled on poles), to 'prepared' (where undergrowth has been freshly cut to produce a corridor, but no nets are present), to 'clear' (areas with no evidence of trapping). From these data, the team estimates how many birds are killed in the region and season overall. To do this, it must make assumptions, such as the number of birds caught in a net each day and that bird migration is relatively constant, when in reality it occurs in waves. \u201cWe always say that our estimate of numbers caught is full of assumptions,\u201d Papazoglou says. \u201cIt needs to be read with a lot of caveats.\u201d One of the major contentions of the SBAs is over the 'prepared' category, Johnston says \u2014 because deciding whether an area is about to be used for trapping is to some extent subjective. And others have expressed concerns about the accuracy of the estimates. \u201cWe have some doubts over the specifics of the monitoring and the exact numbers,\u201d says Panicos Panayides, an officer at the Cyprus Game and Fauna Service in Nicosia. The July workshop was convened to address these methodology issues. BirdLife Cyprus invited Johnston, her colleague Nick Moran, who runs a major British bird survey, other bird-monitoring experts and representatives of the SBAs. After the workshop, Johnston and Moran advised BirdLife Cyprus to do away with the 'prepared' category and to increase the number of squares sampled within the SBAs, among other recommendations. BirdLife Cyprus will adopt these in its 2016 analysis, which should be published this spring. In a statement to  Nature , the SBA Administration said that it did not wish to comment on the methodology used previously, that \u201call groups are working together to refine the recommendations produced by the BTO\u201d, and that \u201cit is imperative that we continue to work together to counter the practice [of bird trapping]\u201d. But even with some adjustments, the trapping figures still jumped between 2013 and 2014, says Johnston. \u201cThe [new] equation slightly reduces the estimated number, but not by much, by about 10%.\u201d And the year-on-year trend towards increased trapping is sound, she says, because the monitoring methods have been consistent over time. If anything, she thinks that BirdLife Cyprus's estimated numbers are \u201cconservative\u201d. On the basis of previous studies, the group estimates that about 20 birds are captured in each net per day. But this figure could be much higher if, as is common today, trappers use taped songs to attract birds. One study2 estimated that such lures can increase the number of birds flying into traps by up to 13-fold. Elsewhere in the world, hunting is thought to be playing a part in the demise of even common bird species. Last year, researchers warned that a highly abundant Eurasian bird, the yellow-breasted bunting ( Emberiza aureola ), had lost as much as 95% of its population in the past three decades or so and was  close to extinction in parts of its range . One major driver is thought to be the trapping of birds in China, where they are served as an expensive delicacy 3 . Accurately measuring the extent of bird killing is important if researchers and conservationists are to gauge the damage being done to bird populations, and to encourage efforts to clamp down. But Johnston says that getting rock-solid data is extremely difficult \u2014 particularly when visiting the monitoring sites is fraught with danger. \u201cIf the trappers had to fill in a form and say how many birds they caught on different days \u2014 we could do a great analysis,\u201d she says. Runge says that low-level hunting of common species may not have a huge impact on populations. \u201cFor other endangered species, where only a few individuals are left, it can be really critical.\u201d And whatever the precise numbers, all the agencies involved agree that bird killing in Cyprus needs to be tackled. The question is, how. \n               boxed-text \n             \n               Political sensitivities \n             Jim Guy, divisional commander of the eastern SBA police, is polite, charming and hard as nails. I met him at the police station in Dhekelia, a cluster of low-lying buildings behind a wire fence set off the road, a few kilometres from the city of Larnaka. He'd originally come from Glasgow on a 3-year posting, but has ended up staying for 17. \u201cAs far as the bases themselves are concerned, there's no denying it's one of the main trapping areas,\u201d he says. But Guy seems aggrieved about the criticism aimed at the SBAs since BirdLife Cyprus's report, and says that lax enforcement is not to blame. Rather, he says, the eastern SBA \u2014 and especially the promontory of Cape Pyla \u2014 is a target for trappers because it is a key stopping point in the flight path of migratory birds. \u201cCape Pyla in particular has no buildings or houses or anything to deter, or put off birds, so it's an ideal situation.\u201d Guy says that his team takes a three-pronged approach to tackle trappers: prevention, education and enforcement. \u201cTo some extent, enforcement is an Elastoplast,\u201d he says. It might catch some trappers, but the practice will continue as long as there is demand for high-priced  ambelopoulia  from diners and the restaurants that serve it \u2014 and these lie almost entirely in the Republic. Stopping that demand is extremely difficult, Guy adds. \u201cThe illegal practice in some cases is overtly or very often tacitly supported by people in very high political and administrative positions.\u201d What's more, officers trying to tackle trapping can find themselves threatened or worse. \u201cIn the UK, you can go home at night and you don't have to think about your home or your family being attacked,\u201d says Guy, who has had officers seriously assaulted while dealing with trappers. His sense of frustration is shared by Panayides. The walls of his office are lined with pictures of birds, and an EU Birds Directive poster perches above the table. Panayides says that there have been at least 30 cases in the past decade in which game-service officers responsible for wildlife enforcement in the republic were harassed by trappers. \u201cWe've had people put bombs in the private cars of game wardens, and cases where the houses of game wardens have been burnt down,\u201d he says. Even when trappers are caught, Panayides says, the weak punishments imposed by courts are not effective deterrents. Technically, Cypriot law allows a first-time trapper to be jailed for up to 3 years, or fined up to \u20ac17,000. In reality, most get off with a fine of a few hundred euros. Panayides tells of one poacher whom his team has caught and prosecuted eight times over the past decade. \u201cWhat else can we do as a department?\u201d he says disconsolately. The fight escalated last year. In May, a previously agreed plan to deal with bird killing was passing through Cyprus's Council of Ministers when the government added a last-minute clause that would allow selective hunting of blackcaps for  ambelopoulia . The move caused an outcry in environmental organizations, because any method used to capture blackcaps would inevitably catch other species and is in breach of the Birds Directive. In August,the altered plan was rejected by the European Commission in a letter to the Cyprus government, and observers are now waiting to see how the government will respond. Meanwhile, authorities in both the republic and the SBAs are stepping up efforts to curb bird killing. The republic authorities are looking at the use of a genetic technique known as DNA barcoding to identify the birds served up at restaurants (see \u2018DNA identifies baked birds\u2019), and the SBA Administration says that it removed 11 football-pitches' worth of planted acacia from the central poaching area of Cape Pyla last summer. The removal met with demonstrations, and people sat in the dirt tracks to stop the clearance contractors. In the area where we encountered the knife-bearing poacher, the monitoring team now enters only if it has a police escort. The conservationists and the poachers have reached \u201cthe top-end of the fight\u201d, says Savvas, who has been monitoring trapping on the island for nearly five years. Surveillance and enforcement will only go so far: most parties agree that the only real way to tackle bird killing is through education and social change. \u201cThe general public has to recognize that this is not correct,\u201d says Panayides. \u201cNot just legally, but also morally and socially.\u201d Papazoglou, too, is realistic about what needs to be done. \u201cIf we don't get the minds and hearts of people to change \u2014 we will never change it,\u201d she says. \n                 Tweet \n                 Follow @NatureNews \n               \n                     Biodiversity: Life \u00ad\u2013 a status report 2014-Dec-10 \n                   \n                     Electronics' noise disorients migratory birds 2014-May-07 \n                   \n                     Animal behaviour: Nomads of necessity 2014-Apr-16 \n                   \n                     Precision formation flight astounds scientists 2014-Jan-15 \n                   \n                     Conservation: The Endangered Species Act at 40 2013-Dec-18 \n                   \n                     The trouble with turbines: An ill wind 2012-Jun-20 \n                   \n                     Cyprus Institute loses money and support 2012-Apr-04 \n                   \n                     BirdLife Cyprus \n                   \n                     BirdLife International \n                   \n                     BirdLife international report on illegal bird killing \n                   \n                     British Trust for Ornithology \n                   \n                     Royal Society for the Protection of Birds \n                   \n                     Sovereign Base Areas, Cyprus \n                   Reprints and Permissions"},
{"file_id": "530020a", "url": "https://www.nature.com/articles/530020a", "year": 2016, "authors": [{"name": "Olive Heffernan"}], "parsed_as_year": "2006_or_before", "body": "As Earth's dry zones shift rapidly polewards, researchers are scrambling to figure out the cause \u2014 and consequences. One spring day in 2004, Qiang Fu was poring over atmospheric data collected from satellites when he noticed an unusual and seemingly inexplicable pattern. In two belts on either side of the equator, the lower atmosphere was warming more than anywhere else on Earth. Fu, an atmospheric scientist at the University of Washington in Seattle, was puzzled. It wasn't until a year later that he realized what he had discovered: evidence of a rapid expansion of the tropics, the region that encircles Earth's waist like a green belt. The heart of the tropics is lush, but the northern and southern edges are dry. And these parched borders are growing \u2014 expanding into the subtropics and pushing them towards the poles. Cities that currently sit just outside the tropics could soon be smack in the middle of the dry tropical edge. That's bad news for places like San Diego, California. \u201cA shift of just one degree of latitude in southern California \u2014 that's enough to have a huge impact on those communities in terms of how much rain they will get,\u201d explains climate modeller Thomas Reichler of the University of Utah in Salt Lake City. Since Fu and his colleagues announced their discovery 1  in 2006, many scientists have investigated the  tropical bloating  and tried to decipher its cause. Explanations range from  global warming  to ozone depletion or natural cycles that will reverse in the future. And there is little agreement on how quickly the border of the tropics is shifting: estimates run from less than half a degree of latitude per decade to several. At the more extreme end, the change in climate would be like moving London to the position of Rome over the course of a century 2 , 3 , 4 , 5 . The problem is compounded by lack of consensus on how to define the tropics, which makes it hard for scientists to agree on the extent of the changes. Nevertheless, researchers investigating this phenomenon agree that it is real. \u201cThere's a big need to be concerned about this issue,\u201d says climate scientist Chris Lucas at the Australian Bureau of Meteorology in Melbourne. That's because of the possible impacts: some of the world's most fertile fishing grounds could disappear, global grain production could shrink and biodiversity could suffer. \n               Strange skies \n             At the same time as Fu first discovered odd patterns in the satellite data, Reichler noticed something unusual in the skies. He was researching the tropopause, the boundary between the lowest level of the atmosphere (the troposphere) and the layer above it (the stratosphere). At the Equator, the tropopause is normally several kilometres higher than at the poles, because warm air rises and pushes the boundary upwards. While analysing temperature data collected from weather balloons, Reichler had found that this equatorial bulge in the tropopause was expanding towards the poles, a sign that the tropics were growing. Fu heard about Reichler's data, and they decided to publish their discoveries together 1 . Ten years after they sounded the alarm, scientists are still struggling to work out what is happening. Last July, 50 researchers gathered in Santa Fe, New Mexico, to discuss everything that is known about tropical expansion \u2014 how to measure it, what is causing it and where the future border of the tropics might be. \u201cWe're at a stage where we recognize the problem is more complex than we originally thought,\u201d explains the organizer of the conference, Dian Seidel, an atmospheric scientist with the US National Oceanic and Atmospheric Administration (NOAA) in Silver Spring, Maryland. Some of the changes in the tropics could be a result of global warming. Reichler investigated that possibility in a study 6  led by Jian Lu, an Earth systems scientist now at the Pacific Northwest National Laboratory in Richland, Washington. Working with Gabriel Vecchi, a climate scientist with NOAA in Princeton, New Jersey, the researchers looked at climate forecasts to see how warming might affect an atmospheric circulation pattern called the Hadley cell, which transports heat from the warmer parts of Earth towards the cooler regions (see 'Bulging waistline'). As part of the Hadley cell, warm, moist air soars skywards above the Equator and cool, dry air tumbles towards Earth at about 30 \u00b0 latitude in the Northern and Southern Hemispheres. That downward limb of the Hadley cell helps to create some of the driest deserts on the planet, such as the Kalahari in southern Africa and the Sahara in northern Africa, and it is one of the most common measures of the boundary between the tropics and the drier subtropics. In their study, Lu and his colleagues found that climate models generally forecast that the outer edge of the Hadley cell will shift because of global warming. But the models predict a much slower rate of tropical expansion than has been seen so far \u2014 which has led researchers to suspect that something else is going on. A common view, and one held by Lucas, is that natural climatic variability is playing some part. That variability could take the form of  large-scale climatic cycles  such as the Pacific Decadal Oscillation, in which temperatures in the Pacific Ocean swing between hot and cold across timescales of 15\u201320 years or more. \u201cOr it could be in the form of much more random, chaotic noise,\u201d says Lucas, who thinks that large cycles and noise together account for 50% or more of the expansion. Atmospheric scientist Darryn Waugh at Johns Hopkins University in Baltimore, Maryland, agrees. \u201cIt's a chaotic system, so some of the variability is just noise in the system.\u201d If that is the case, tropical expansion could slow down or even reverse in some regions when those natural variations swing back. Another answer might involve different forces in the Northern and Southern hemispheres. South of the Equator, tropical expansion has been strongest in the summer, and that leads some researchers to suspect that it is related to the pattern of ozone loss in the southern stratosphere. Pollutants chew up ozone molecules above Antarctica in the spring, which triggers circulation changes throughout other parts of the Southern Hemisphere during summer. The correlation with tropical expansion suggests that the two phenomena could be connected. What's more, climate models that factor in ozone loss are able to account for much more of the tropical expansion between 1980 and 2000, when the Antarctic ozone hole was growing bigger nearly every year, says Waugh 7 . In the Northern Hemisphere, a different explanation is called for because, in general, the Arctic does not suffer the same sort of ozone loss as the Antarctic. Research led by climate scientist Bob Allen at the University of California, Riverside, suggests that the culprits in the north might be black soot and tropospheric ozone \u2014 which are both generated by burning fossil fuels. Allen and his team ran simulations with a climate model that featured detailed atmospheric physics, and their analysis showed that black soot and tropospheric ozone have heated the atmosphere in the Northern Hemisphere and driven tropical expansion more than carbon dioxide and other greenhouse gases, particularly in summer 8 .  The change in climate would be like moving London to the position of Rome over the course of a century.  Not everyone is comfortable with the idea that entirely separate factors could drive tropical expansion to such a large extent on either side of the Equator. Fu, for one, thinks it's unlikely given the similar patterns in the north and south. \u201cIf ozone depletion was dominating the expansion in the Southern Hemisphere in the past 30 years, would you see such symmetry? I'm not convinced,\u201d says Fu. The proliferation of hypotheses shows how much researchers are struggling to explain what's happening. \u201cI think we're piecing this together slowly,\u201d says Lucas. \u201cWe don't have a full explanation yet and I don't think there's going to be one single explanation. It's going to be a little bit of this and a little bit of that.\u201d \n               Edge effect \n             Right around the time that scientists were first warning about tropical expansion, Lucas was experiencing what might have been its effects first-hand. During 2006 and 2007, Australia was deep in the middle of one of the worst droughts to have hit the continent since Europeans settled there. Lucas recalls driving from Melbourne to nearby Lake Eildon and seeing the once-brimming lake empty. Meanwhile, Melbourne's reservoirs were running low, and north of the city, forest fires raged in the mountains. The worst-affected regions of Australia \u2014 cities such as Perth, Adelaide and Melbourne \u2014 were south of 30 \u00b0 latitude, which suggests that the drying could be caused by a shift in the position of the Hadley cell and the rain-bearing jet stream. According to research published 9  in 2010, southeastern Australia has been invaded by a drier climate from the north in recent decades, which has greatly reduced rainfall. \u201cWe can't say that this is exclusively due to tropical expansion, but it's certainly consistent with tropical expansion,\u201d explains Lucas. \u201cAnd our concern is that southeastern Australia is going to keep getting drier.\u201d Elsewhere, there is evidence that tropical expansion is affecting the ocean. Where the Hadley cell descends, bringing cool air downward, it energizes the ocean and whips up currents to high speeds. This energy powers the upwelling of cold, nutrient-rich waters towards the surface, which feeds some of the world's most productive fisheries. But there are hints that some of these regions are suffering because of shifts in the Hadley cell. Edward Vizy and Kerry Cook 10 , both at the University of Texas at Austin, have found some unhealthy signs in the region of the Benguela Current, an area of coastal upwelling along the coast of west Africa and south of 30 \u00b0 latitude. According to Cook, the currents of that entire region have shifted over the past 30 years. One effect is that the upwelling has weakened, with worrying implications for the region's fisheries and biodiversity. Cook says that the same could be true of open-ocean upwelling systems, which are more susceptible to changes in the position of the Hadley cell. These upwelling zones could move south over time, or get weaker or stronger, depending on what happens to the Hadley cell, says Cook. In any case, it means that fishing communities that rely on these resources will not be able to count on traditional patterns. On land, biodiversity is also potentially at risk. This is especially true for the climate zones just below the subtropics in South Africa and Australia, on the southern rim of both continents. In southwestern Australia, renowned as one of the world's biodiversity hotspots, flowers bloom during September, when tourists come to marvel at some of the region's 4,000 endemic plant species. But since the late 1970s, rainfall there has dropped by one-quarter. The same is true at South Africa's Cape Floristic Province, another frontier known for its floral beauty. \u201cThis is the most concrete evidence we have of tropical expansion,\u201d says Steve Turton, an environmental geographer at James Cook University in Cairns, Australia. Turton worries that the rate of change will be too rapid for these ecosystems to adapt. \u201cWe're talking about rapid expansion that's within half or a third of a human lifetime,\u201d he says. In the worst-case scenario, the subtropics will overtake these ecologically rich outposts and the hotter, drier conditions will take a major toll. For the scientists working in this field, communicating the threat of tropical expansion will be tricky, given the level of uncertainty. \u201cIt's frustrating to see how much work we have left,\u201d says Thomas Birner, an atmospheric scientist at Colorado State University in Fort Collins and one of the conveners of the Santa Fe meeting. One outcome of that conference was an agreement that scientists should compare the various metrics for measuring tropical expansion in the hope of agreeing on the best way forward. More time will also help. If tropical expansion continues at a fairly constant rate, says Waugh, there will be less of a chance that natural variability is the main culprit, and the finger will point more strongly to other causes. But that long wait for an answer will be no comfort for the residents of cities such as Santiago, San Diego and Melbourne, and for the billions of others who live near the boundary between the tropics and subtropics. \u201cWe need to understand this issue,\u201d says Lucas, \u201cto have a sustainable civilization there.\u201d \n                 Tweet \n                 Facebook \n                 LinkedIn \n                 weibo \n               \n                     The fragile framework 2015-Nov-24 \n                   \n                     Accelerated dryland expansion under climate\u00a0change 2015-Oct-26 \n                   \n                     Southward shift of the northern tropical belt from 1945 to 1980 2015-Oct-19 \n                   \n                     The poleward migration of the location of tropical cyclone maximum intensity 2014-May-14 \n                   \n                     Climate science: Shifting storms 2014-May-14 \n                   \n                     Climate science: Tropical expansion by ocean swing 2014-Mar-16 \n                   \n                     Influence of anthropogenic aerosols and the Pacific Decadal Oscillation on tropical belt width 2014-Mar-16 \n                   \n                     Climate change: The case of the missing heat 2014-Jan-15 \n                   \n                     Physical processes in the tropical tropopause layer and their roles in a changing climate 2013-Feb-27 \n                   \n                     Climate adaptation: Survival of the flexible 2013-Feb-05 \n                   \n                     Recent Northern Hemisphere tropical expansion primarily driven by black carbon and tropospheric ozone 2012-May-16 \n                   \n                     Widening of the tropical belt in a changing climate 2007-Dec-02 \n                   \n                     Nature  special: 2015 Paris climate talks \n                   \n                     Nature  special: Outlook for Earth \n                   Reprints and Permissions"},
{"file_id": "529456a", "url": "https://www.nature.com/articles/529456a", "year": 2016, "authors": [{"name": "Monya Baker"}], "parsed_as_year": "2006_or_before", "body": "It may not be sexy, but quality assurance is becoming a crucial part of lab life. Rebecca Davies remembers a time when quality assurance terrified her. In 2007, she had been asked to lead accreditation efforts at the University of Minnesota's Veterinary Diagnostic Laboratory in Saint Paul. The lab needed to ensure that the tens of thousands of tests it conducts to monitor disease in pets, poultry, livestock and wildlife were watertight. \u201cIt was a huge task. I felt sick to my stomach,\u201d recalls Davies, an endocrinologist at the university's College of Veterinary Medicine. She nevertheless accepted the challenge, and soon found herself hooked on finding \u2014 and fixing \u2014 problems in the research process. She and her team tracked recurring tissue-contamination issues to how containers were being filled and stored; they traced an assay's erratic performance to whether technicians let an enzyme warm to room temperature; and they established systems to eliminate spotty data collection, malfunctioning equipment and neglected controls. Her efforts were crucial to keeping the diagnostic lab in business, but they also forced her to realize how much researchers' work could improve. \u201cThat is the beauty of quality assurance,\u201d Davies says. \u201cThat is what we were missing out on as scientists.\u201d Davies wanted to spread the word. In 2009, she got permission and financial support to launch an internal consulting group for the college, to help labs with the dry but essential work of quality assurance (QA). The group, called Quality Central, now supports more than half a dozen research labs \u2014 helping them to design systems to ensure that their equipment, materials and data are up to scratch, and helping them to improve. She is also part of a small but growing group of professionals around the world who hope to transform basic biomedical research. Many were hired by their universities to help labs to meet certain regulatory standards, but these QA consultants have a broader vision. They are not pushing for universal adoption of formal regulatory certifications. Instead, they advocate 'voluntary QA'. With the right strategies, they argue, scientists can strengthen their research and improve reproducibility. When Davies first started proselytizing to her fellow faculty members, the responses were not encouraging. \u201cNone of them found the idea compelling at all,\u201d Davies recalls. How important could QA be, they asked, if the US National Institutes of Health did not require it? How could anyone afford to spend money or time on non-essentials? Shouldn't they focus on the discoveries lurking in their data, and not the systems for collecting them? But some saw the potential, based on their own experiences. Before she had heard of Quality Central, University of Minnesota virologist Montserrat Torremorell was grateful when a colleague let her use his instruments to track transmissible disease in swine. But the results made no sense. Samples from pigs experimentally infected with influenza showed extremely low levels of the virus. It turned out that her benefactor had, like many scientists, skimped on equipment maintenance to save money. \u201cIt was a real eye-opener,\u201d Torremorell recalls. \u201cIt just made me think that I could not rely on other people's equipment.\u201d \n               Quality for all \n             Quality systems are an integral part of most commercial goods and services, used in manufacturing everything from planes to paint. Some labs that focus on clinical applications implement certified QA systems such as Good Clinical Practice, Good Manufacturing Practice and Good Laboratory Practice for data submitted to regulatory bodies. There have also been efforts to guide research practices outside these schemes. In 2001, the World Health Organization published guidelines for QA in basic research. And in 2006, the British Association of Research Quality Assurance (now simply the RQA) in Ipswich issued guidelines for basic biomedical research. But few academic researchers know that these standards exist (Davies certainly didn't back in 2007). Instead, QA tends to be ad hoc in academic settings. Many scientists are taught how to keep lab notebooks by their mentors, supplemented perhaps by a perfunctory training course. Investigators often improvise ways to safeguard data, maintain equipment or catalogue and care for experimental materials. Too often, data quality is as likely to be assumed as assured. Scientific rigour has taken a drubbing in the past few years, with reports that fewer than one-third of biomedical papers can be reproduced (see  Nature   http://doi.org/477 ; 2015 ). Scientific culture, training and incentives have all been blamed for promoting sloppy work; a common refrain is that the status quo values publication counts over careful experimentation and documentation. \u201cThere is chaos in academia,\u201d says Masha Fridkis-Hareli, head of ATR, a biotechnology consultancy in Worcester, Massachusetts, that also conducts laboratory work to help move basic research into industry. For every careful researcher she has encountered, there have been others who have thought nothing of scribbling data on paper towels, repeating experiments without running controls and guessing at details months after an experiment. Davies insists that plenty of scientists are doing robust work, but there is always room for improvement (see 'Solutions'). \u201cThere are easy fixes to situations that shouldn't be happening, but are,\u201d she says. \n               boxed-text \n             Michael Murtaugh, a swine biologist at the University of Minnesota, had tried to establish practices to beef up the reliability of his team's lab notebooks, but the attempts that he made on his own never gained traction. Then Davies got on his case. After a year or so of her \u201cplanting seeds\u201d \u2014 as she puts it \u2014 Murtaugh agreed to work with Quality Central and implement a low-tech but effective solution. On designated Mondays, each member of Murtaugh's lab draws a name from a paper bag to determine whose notebook to audit. The scientists check that their assigned books include relevant controls for experiments, and indicate where data are stored and which particular machine generated them. The group also makes sure that any problems noted in the previous check have been addressed. It takes about ten minutes per researcher every few weeks, but that's enough to change people's habits. Graduate student Michael Rahe says that the checks ensure that he keeps his notebook legible and up to date. \u201cI never used to put in raw data,\u201d he says. Albert Cirera, a technologist developing gas nanosensors at the University of Barcelona in Spain, has also embraced QA. As his lab group grew to 12 people, he found it difficult to monitor everyone's experiments, and his own efforts to implement a tracking system were inadequate. He turned to a university-based QA consulting service for help. Now, samples, equipment and their data are all linked with tracking numbers printed on stickers and recorded in individuals' notebooks, on samples and in a central tracking file. The system does not slow down experiments, and staying abreast of projects is a breeze, says Cirera. But getting to this point took about four months and frequent consultations. \u201cIt was not something that you can create from zero,\u201d he says. \n               Making a market \n             Any scientist adopting a QA system has to wager that the up-front hassle will pay off in the future. \u201cIt is very difficult to get people to check and annotate everything, because they think it is nonsense,\u201d says Carmen Navarro-Aragay, head of the University of Barcelona quality team that worked with Cirera. \u201cThey realize the value only when they get results that they do not understand and find that the answer is lurking somewhere in their notebooks.\u201d Even when experiments go as expected, quality systems can save time, says Murtaugh. Methods and data sections in papers practically write themselves, with no time wasted in frenzied hunting for missing information. There are fewer questions about how experiments were done and where data are stored, says Murtaugh. \u201cIt allows us to concentrate on biological explanations for results.\u201d The more difficult data are to collect, the more important a good QA system becomes. Catherine Bens, a QA manager at Colorado State University in Fort Collins, says that she remembers getting cold, wet and dirty when she had to monitor a study involving ultrasound scans and blood samples from a population of feral horses in North Dakota. Typical animal-identification practices such as ear tagging were not allowed. So, before the collection started, Bens supported researchers as they rehearsed procedures, pre-labelled tubes, made back-up labels and recruited animal photographers and park volunteers to ensure that samples would be linked to the correct animals. Even in a snow storm with winds so loud that everyone had to shout, the team made sure that each data point could be traced. Rare samples or not, few basic researchers are clamouring to get QA systems in place. Most are unfamiliar with the discipline, says Davies. Others are hostile. \u201cThey see it as trying to constrain them, and that you're making them do more work.\u201d Before awarding certain grants, the Found Animals Foundation in Los Angeles, California, which funds research on animal sterilization, requires proof that instruments have been calibrated and that written plans exist for tracing data and dealing with outliers. It can be a struggle, says Shirley Johnston, scientific director of the foundation. One grant recipient argued that QA systems were unnecessary because just looking over the data would reveal their quality. Part of the resistance may be down to how some QA professionals present themselves. \u201cA lot of them are there to tell you what you are doing is wrong, and a lot of them are not very nice about it,\u201d says Terry Nett, a reproductive biologist at Colorado State University who experienced this first-hand when he worked with outside consultants to incorporate Good Laboratory Practice principles in his lab. The effort was frustrating. \u201cInstead of helping us understand, they would act like a dictator,\u201d Nett recalls. \u201cI just didn't want them in my lab.\u201d A few years ago, however, the university hired its own quality managers, and things changed. The current manager, Bens, acts more like a partner, Nett says. She points out where labs are already using robust practices, and explains the reasoning behind QA practices that she introduces. To win scientists over, Bens stresses that QA systems produce data that can withstand criticism. \u201cYou build a support system around any data point you collect,\u201d she says. When there is a strange result, researchers have documentation to trace its provenance. That can show whether a data point is real, an outlier or a problem \u2014 for example if a blood sample was not kept cold or was stored in the wrong tube.  There are easy fixes to situations that shouldn't be happening, but are.  Scientists need to take the lead on which QA elements they incorporate, says Melissa Eitzen, director of regulatory operations at the University of Texas Medical Branch in Galveston. \u201cYou want to give them tips that they can take or not take,\u201d she says. \u201cIf they choose it, they'll do it. If you tell them they have to do it, that's a struggle.\u201d Rapport is paramount, says Michael Jamieson at the University of Southern California in Los Angeles, who helps other faculty members to move research towards clinical applications. Instead of talking about quality systems, he prefers to discuss concrete behaviours, such as labelling bottles with expiry dates and storage conditions. QA jargon puts scientists off, he says. \u201cUsing the term good research practice makes most researchers want to run the other way.\u201d It's a lesson that many QA specialists have taken to heart. Some say 'assessment' or 'quality improvement' instead of 'audit'. Even 'research integrity' can be an inflammatory phrase, says Davies. \u201cYou have to find a way to communicate that QA is not punitive or guilt-inspiring.\u201d \n               Not into temptation \n             Having data that are traceable \u2014 down to who did what experiment on which machine, and where the source data are stored \u2014 has knock-on benefits for research integrity, says Nett. \u201cYou can't pick out the data that you want.\u201d Researchers who must provide strong explanations about why they chose to leave any information out of their analysis will be  less tempted to cherry-pick data . QA can also weed out digital meddling: popular spreadsheet programs such as Microsoft Excel can be vulnerable to errors or manipulation if not properly locked, but QA teams can set up instruments to store read-only files and prevent researchers from tampering with data accidentally or intentionally. \u201cI can't help but think that QA is going to make fraud harder,\u201d says Davies. And good quality systems can be contagious. Melanie Graham, who studies diabetes at the University of Minnesota, often collaborates with others to test potential treatments. More than once, she says, collaborators have sent her samples in a polystyrene tube with nothing but a single letter written on it. Graham sends it back and requests a label that specifies the sample's identity and provenance, and a range of storage temperatures. 'Keep frozen' is too vague \u2014 she will not risk performing uninformative experiments because reagents stored in a standard freezer were supposed to be kept at \u221280 \u00b0C.  I can't help but think that QA is going to make fraud harder.  When she first sent documentation requirements to collaborators, she expected them to push back. Instead, reactions were overwhelmingly positive. \u201cIt's a relief for them,\u201d says Graham. \u201cThey want us to handle their test article in a trusted way.\u201d The benefits go beyond providing solid data. In 2013, Davies worked with Torremorell and other Minnesota faculty members on a proposal to monitor and calibrate equipment used by several labs. The plan that they put in place helped them to secure US$1.8 million to build shared lab space to deal with animal pathogens, says Torremorell. \u201cIf we want to be competitive to get funding, and if we want people to believe our data, we need to be serious about the data that we generate.\u201d Davies is still trying to spread the word. Her invitations to give talks and review grant applications have mushroomed. She and collaborators at other institutions have been developing online training materials and offering classes to technicians, postdocs, graduate students and principal investigators. After a presentation last year, a member of the audience told her that he had reviewed a grant from one of her clients; the QA plan had made the application stand out in a positive way. Davies was delighted. \u201cI could finally come back to my folks and say, 'It was noticed.'\u201d Davies knows it is still an uphill battle, but her ultimate goal is to make QA as much a part of research as peer review. It may not have the flash and dazzle of other efforts to ensure that research is robust and reproducible, but that is not the point. \u201cA QA programme isn't sexy,\u201d says Michael Conzemius, a veterinary researcher at the University of Minnesota and another client of Quality Central. \u201cIt's just kind of become the nuts and bolts of the scientific process for us.\u201d \n                 Tweet \n                 Facebook \n                 LinkedIn \n                 weibo \n               \n                     Poorly designed animal experiments in the spotlight 2015-Oct-13 \n                   \n                     Biomedical researchers lax in checking for imposter cell lines 2015-Oct-12 \n                   \n                     Hospital checklists are meant to save lives \u2014 so why do they often fail? 2015-Jul-28 \n                   \n                     Reproducibility crisis: Blame it on the antibodies 2015-May-19 \n                   \n                     Nature  special: Reproducibility \n                   \n                     Quality Assurance Toolkit \n                   \n                     How to Start, and Keep, a Lab Notebook \n                   \n                     Quality Systems Workbook \n                   Reprints and Permissions"},
{"file_id": "530024a", "url": "https://www.nature.com/articles/530024a", "year": 2016, "authors": [{"name": "Helen Shen"}], "parsed_as_year": "2006_or_before", "body": "Rigid robots step aside \u2014 a new generation of squishy, stretchy machines is wiggling our way. In 2007, Cecilia Laschi asked her father to catch a live octopus for her seaside lab in Livorno, Italy. He thought she was crazy: as a recreational fisherman, he considered the octopus so easy to catch that it must be a very stupid animal. And what did a robotics researcher who worked with metal and microprocessors want with a squishy cephalopod anyway? Nevertheless, the elder Laschi caught an octopus off the Tuscan coast and gave it to his daughter, who works for the Sant'Anna School of Advanced Studies in Pisa, Italy. She and her students placed the creature in a saltwater tank where they could study how it grasped titbits of anchovy and crab. The team then set about building robots that could mimic those motions. Prototype by prototype, they created an artificial tentacle with internal springs and wires that mirrored an octopus's muscles, until the device could undulate, elongate, shrink, stiffen and curl in a lifelike manner 1 . \u201cIt's a completely different way of building robots,\u201d says Laschi. This approach has become a major research front for robotics in the past ten years. Scientists and engineers in the field have long worked on hard-bodied robots, often inspired by humans and other animals with hard skeletons. These machines have the virtue of moving in mathematically predictable ways, with rigid limbs that can bend and straighten only around fixed joints. But they also require meticulous programming and extensive feedback to avoid smacking into things; even then, their motions often become erratic or even dangerous when dealing with humans, new objects, bumpy terrain or other unpredictable situations. Robots inspired by flexible creatures such as octopuses, caterpillars or fish offer a solution. Instead of requiring intensive (and often imperfect) computations, soft robots built of mostly pliable or elastic materials can just mould themselves to their surroundings. Although some of these machines use wires or springs to mimic muscles and tendons, as a group, soft robots have ditched the skeletons that defined previous robot generations. With nothing resembling bones or joints, these machines can stretch, twist, scrunch and squish in completely new ways. They can transform in shape or size, wrap around objects and even touch people more safely than ever before. Building these machines involves developing new technologies to animate floppy materials with purposeful movement, and methods for monitoring and predicting their actions. But if this succeeds, such robots might be used as rescue workers that can squeeze into tight spaces or slink across shifting debris; as home health aides that can interact closely with humans; and as industrial machines that can grasp new objects without previous programming. Researchers have already produced a wide variety of such machines, including crawling robotic caterpillars 2 , swimming fish-bots 3  and undulating artificial jellyfish 4 . On 29\u201330 April, ten teams will compete in Livorno in an international soft-robotics challenge \u2014 the first of its kind. Laschi, who serves as scientific coordinator for the European Commission-backed sponsoring research consortium, RoboSoft, hopes that the event will drive innovation in the field. \u201cIf you look in biology, and you ask what Darwinian evolution has coughed up, there are all kinds of incredible solutions to movement, sensing, gripping, feeding, hunting, swimming, walking and gliding that have not been open to hard robots,\u201d says chemist George Whitesides, a soft-robotics researcher at Harvard University in Cambridge, Massachusetts. \u201cThe idea of building fundamentally new classes of machines is just very interesting.\u201d \n               Smooth moves \n             The millions of industrial robots around the world today are all derived from the same basic blueprint. The metal-bound machines use their hefty, rigid limbs to shoulder the grunt work in car-assembly lines and industrial plants with speed, force and mindless repetition that humans simply can't match. But standard robots require specialized programming, tightly controlled conditions and continuous feedback of their own movements to know precisely when and how to move each of their many joints. They can fail spectacularly at tasks that fall outside their programming parameters, and they can malfunction entirely in unpredictable environments. Most must stay behind fences that protect their human co-workers from inadvertent harm. \u201cThink about how hard it is to tie shoelaces,\u201d says Daniela Rus, director of the Computer Science and Artificial Intelligence Laboratory at the Massachusetts Institute of Technology in Cambridge. \u201cThat's the kind of capability we'd like to have in robotics.\u201d Over the past decade, that desire has triggered an increased interest in lighter, cheaper machines that can handle fiddly or unpredictable situations and collaborate directly with humans. Some roboticists, including Laschi, think that soft materials and bioinspired designs can provide an answer. That idea was a tough sell at first, Laschi says. \u201cIn the beginning, very traditional robotics conferences didn't want to accept my papers,\u201d she says. \u201cBut now there are entire sessions devoted to this topic.\u201d Helping to fuel the surge in interest are recent advances in polymer science, especially the development of techniques for casting, moulding or 3D printing polymers into custom shapes. This has enabled roboticists to experiment more freely and quickly with making soft forms. As a result, more than 30 institutions have now joined the RoboSoft collaboration, which kicked off in 2013. The following year saw the launch of a dedicated journal,  Soft Robotics , and of an open-access resource called the Soft Robotics Toolkit: a website developed by researchers at Trinity College Dublin and at Harvard that allows researchers and amateurs to share tips and find downloadable designs and other information (see  go.nature.com/8gsq4h ). Still, says Rebecca Kramer, a mechanical engineer at Purdue University in West Lafayette, Indiana, \u201cI don't think the community has coalesced on what a soft robot should look like, and we're still picking out the core technology.\u201d Perhaps the most fundamental challenge is getting the robots' soft structures to curl, scrunch and stretch. Laschi's robotic tentacle houses a network of thin metal cables and springs made of  shape-memory alloys  \u2014 easily bendable metals that return to their original shapes when heated. Laid lengthwise along the 'arm', some of these components simulate an octopus's longitudinal muscles, which shorten or bend the tentacle when they contract. Others radiate out from the tentacle's core, simulating transverse muscles that shrink the arm's diameter. Researchers can make the tentacle wave \u2014 or even curl around a human hand \u2014 by pulling certain combinations of cables with external motors, or by heating springs with electrical currents. A similar system helps to drive the soft-robotic caterpillars that neurobiologist Barry Trimmer has modelled on his favourite experimental organism, the tobacco hornworm ( Manduca sexta ). At his lab at Tufts University in Medford, Massachusetts, 20 hornworms are born each day, and Trimmer 3D prints a handful of robotic ones as well. The mechanical creatures wriggle along the lab bench much like the real ones, and they can even copy the caterpillar's signature escape move: with a pull here and a tug there on the robot's internal 'muscles', the machine snaps into a circle that wheels away 5 . Trimmer, who is editor-in-chief of  Soft Robotics , hopes that this wide range of movements could one day turn the robot into an aide for emergency responders that can rapidly cross fields of debris and burrow through rubble to locate survivors of disasters. Whitesides, meanwhile, is pioneering robots that are powered by air \u2014 among them a family of polymer-based devices inspired by the starfish. Each limb consists of an internal network of pockets and channels, sandwiched between two materials of differing elasticity. As researchers pump air into different parts of the robot, the arms (or legs or fingers) inflate asymmetrically and curl. Whitesides' team has even built one device that can play 'Mary Had a Little Lamb' on the piano 6 . One of the team's four-legged creations has mastered a robot obstacle course: ambling towards an elevated partition with a clearance of about 2 centimetres, the machine drops down and shimmies underneath, demonstrating the potential of soft robots to tackle complex terrains 7 . \n               Grabbing market share \n             Although most soft robots remain in the lab, some of Whitesides' creations are venturing out to feed industrial demand for adept robotic hands. Conventional grippers require detailed information about factors such as an object's location, shape, weight and slipperiness to move each of its joints correctly. One system may be specialized for handling shampoo bottles, whereas another picks up only children's toys, and yet another is needed for grabbing T-shirts. But as manufacturers update their product lines, and as e-commerce warehouses handle a growing variety of objects, these companies need to swap in customized grippers and updated control algorithms for each different use \u2014 often at great cost and delay. By contrast, grippers that are made mainly of soft, stretchy materials can envelop and conform to objects of different shapes and sizes. Soft Robotics, a start-up company in Cambridge, Massachussetts, that spun out of Whitesides' research in 2013, has raised some US$4.5 million to develop a line of rubbery robotic claws. \u201cWe use no force sensors, no feedback systems and we don't do a lot of planning,\u201d says the company's chief executive, Carl Vause. \u201cWe just go and grab an object\u201d, squeezing until the grip is secure. Made entirely of elastic polymers, the claws curl when air pumps through their internal channels. Whereas stiff robotic hands must carefully compute each finger's movements, the new gripper's softness enables it to drag along or deform around an object's surface until it grabs hold, without causing damage. It can even pick up mushrooms and ripe strawberries, as well as plump tomatoes off a vine \u2014 tasks that have historically required the delicate touch of human workers. Soft Robotics released its first gripper for sale in June 2015, and it is running pilot programmes with six client companies involved in packaging and food-handling. Empire Robotics in neighbouring Boston has taken a radically different approach, by marketing a robotic 'hand' that resembles a squishy stress ball. Sandlike particles inside the ball flow freely at first, allowing it to deform as it presses firmly into an object. Then, a valve sucks air out of the ball so that the grains inside are forced tightly against each other, causing the ball to harden its grip. Based on research 8  by Heinrich Jaeger at the University of Chicago in Illinois, and Hod Lipson at Cornell University in Ithaca, New York, the 'Versaball' can pick up objects in about one-tenth of a second and lift up to about 9 kilograms. \n               Sense of place \n             As robotic octopuses, caterpillars, starfish and other malleable machines come to life, some scientists have begun to focus on better ways to control the devices' actions. \u201cWe're talking about floppy, elastic materials,\u201d says Kramer. \u201cWhen something moves on one side, you're not quite sure where the rest of the machine is going to end up.\u201d That is why many applications will probably require  extra sensors  to monitor movement. Yet conventional position and force sensors \u2014 rigid or semi-rigid electronic components \u2014 don't always work well with soft robots that undergo extreme shape changes. Engineers such as Yong-Lae Park are tackling this problem by developing stretchable electronic sensors. At Carnegie Mellon University in Pittsburgh, Pennsylvania, Park works on gummy patches that contain liquid-metal circuits sandwiched between sheets of silicone rubber. Poured in a variety of patterns, including spirals and stripes, these liquid circuits can be customized to sense when the device is squished or stretched, and in what direction 9 . \u201cStretchable sensors can be as sensitive as skin, depending on how you design them. You can tune them to respond to a slight brush of a finger or to a 30-pound weight,\u201d says mechanical engineer Robert Shepherd at Cornell, who has developed methods for 3D printing stretch-sensitive 'skins' directly onto soft robots 10 . Alternating layers of conductive and insulating material produce an electrical signal when prodded or pulled. Stretchy sensors could have an important role in the growing field of wearable robotics. Funded by the US military, Conor Walsh at Harvard University has spent years developing and honing a soft 'exosuit' for soldiers \u2014 a comfier analogue to earlier 'Iron Man'-type exoskeletons, meant to help fighters to carry heavy loads over long distances. Users can still feel the device aiding their movement, but walking in the suit feels \u201cpretty natural\u201d, says Walsh \u2014 a big improvement from conventional exoskeletons. Instead of bulky, rigid casings, Walsh's suit uses straps made from nylon, polyester and spandex placed strategically along the legs. And a smattering of position and acceleration sensors \u2014 standard rigid devices for now \u2014 helps to monitor the wearer's gait and to deliver assistance at the optimal times.The next step, says Walsh, is to incorporate stretchy sensors for a softer, more comfortable experience. Meanwhile, Kramer has created a robotic fabric that moves in response to electrical current 11 . The muslin sheet, which has shape-memory-alloy coils sewn in, can scrunch by up to 60% in length when stimulated. Smart 'threads' keep tabs on the fabric's movements; Kramer weaves in stretch-sensitive silicone filaments filled with liquid metal. The concept could be used one day for sleeves or cuffs to help injured or elderly people to move. Kramer also hopes that the material might be used to assemble robots in space. Astronauts could simply drape an active skin around a piece of foam, for example, to turn it into a working robot. But before soft robots can fly to space, much foundational work must be done on the ground. Relatively little is known about how squishy materials deform in response to external forces, and how movements propagate through soft masses. In addition, most soft robots remain attached or tethered to  hard energy sources , such as batteries or compressed-air tanks. Some researchers are already eyeing the potential of biochemical or renewable sources of energy for soft robots. The RoboSoft challenge in April could help to spur development. There, the entries will be put through their paces: challenges include racing across a sand pit, opening a door by its handle, grabbing a number of mystery objects and avoiding fragile obstacles under water. The goal, says Laschi, is to demonstrate that soft robots can accomplish some of the same tasks that stiff robots do, as well as others that they cannot. \u201cI don't think soft robotics is going to replace traditional robotics, but it will be combination of the two in the future,\u201d says Laschi. Many researchers think that rigid robots might retain their superiority in jobs requiring great strength, speed or precision. But for a growing number of applications involving close interactions with people, or other unpredictable situations, soft robots could find a niche. At Kings College London, for example, Laschi's collaborators are developing a surgical endoscope based on her tentacle technology. And her team in Italy is developing a full-bodied robot octopus that swims by fluid propulsion, and could one day be used for underwater research and exploration. The prototype already pulses silently through a tank in her lab, as the real octopuses swim in the salty waters just outside. \u201cWhen I started with the octopus, people asked me what it was for,\u201d says Laschi. \u201cI said, 'I don't know, but I'm sure if it succeeds there could be many, many applications'.\u201d \n                 Tweet \n                 Facebook \n                 LinkedIn \n                 weibo \n               \n                     'Instinctive' robot recovers from injury fast 2015-May-27 \n                   \n                     Origami robot folds itself in 4 minutes 2014-Aug-07 \n                   \n                     Explosive power makes silicone robot jump 2013-Feb-08 \n                   \n                     Four-fingered robot can replace flashlight batteries 2012-Aug-20 \n                   \n                     Artificial skins detect the gentlest touch 2010-Sep-12 \n                   \n                     Nature blog post: Rise of the (soft) robots \n                   \n                     Cecilia Laschi \n                   \n                     George Whitesides \n                   \n                     Barry Trimmer \n                   \n                     Conor Walsh \n                   \n                     Robert Shepherd \n                   \n                     Rebecca Kramer \n                   Reprints and Permissions"},
{"file_id": "the-chips-are-down-for-moore-s-law-1.19338", "url": "https://www.nature.com/news/the-chips-are-down-for-moore-s-law-1.19338", "year": 2016, "authors": [], "parsed_as_year": "2011_2015", "body": "Next month, the worldwide semiconductor industry will formally acknowledge what has become increasingly obvious to everyone involved: Moore's law, the principle that has powered the information-technology revolution since the 1960s, is nearing its end. A rule of thumb that has come to dominate computing, Moore's law states that the number of transistors on a microprocessor chip will double every two years or so \u2014 which has generally meant that the chip's performance will, too. The exponential improvement that the law describes transformed the first crude home computers of the 1970s into the sophisticated machines of the 1980s and 1990s, and from there gave rise to high-speed Internet, smartphones and the wired-up cars, refrigerators and thermostats that are becoming prevalent today. None of this was inevitable: chipmakers deliberately chose to stay on the Moore's law track. At every stage, software developers came up with applications that strained the capabilities of existing chips; consumers asked more of their devices; and manufacturers rushed to meet that demand with next-generation chips. Since the 1990s, in fact, the semiconductor industry has released a research road map every two years to coordinate what its hundreds of manufacturers and suppliers are doing to stay in step with the law \u2014 a strategy sometimes called More Moore. It has been largely thanks to this road map that computers have followed the law's exponential demands. Not for much longer. The doubling has already started to falter, thanks to the heat that is unavoidably generated when more and more silicon circuitry is jammed into the same small area. And some even more fundamental limits loom less than a decade away. Top-of-the-line microprocessors currently have circuit features that are around 14 nanometres across, smaller than most viruses. But by the early 2020s, says Paolo Gargini, chair of the road-mapping organization, \u201ceven with super-aggressive efforts, we'll get to the 2\u20133-nanometre limit, where features are just 10 atoms across. Is that a device at all?\u201d Probably not \u2014 if only because at that scale, electron behaviour will be governed by quantum uncertainties that will make transistors hopelessly unreliable. And despite vigorous research efforts, there is no obvious successor to today's silicon technology. The industry road map released next month will for the first time lay out a research and development plan that is not centred on Moore's law. Instead, it will follow what might be called the More than Moore strategy: rather than making the chips better and letting the applications follow, it will start with applications \u2014 from smartphones and supercomputers to data centres in the cloud \u2014 and work downwards to see what chips are needed to support them. Among those chips will be new generations of sensors, power-management circuits and other silicon devices required by a world in which computing is increasingly mobile. The changing landscape, in turn, could splinter the industry's long tradition of unity in pursuit of Moore's law. \u201cEverybody is struggling with what the road map actually means,\u201d says Daniel Reed, a computer scientist and vice-president for research at the University of Iowa in Iowa City. The Semiconductor Industry Association (SIA) in Washington DC, which represents all the major US firms, has already said that it will cease its participation in the road-mapping effort once the report is out, and will instead pursue its own research and development agenda. Everyone agrees that the twilight of Moore's law will not mean the end of progress. \u201cThink about what happened to airplanes,\u201d says Reed. \u201cA Boeing 787 doesn't go any faster than a 707 did in the 1950s \u2014 but they are very different airplanes\u201d, with innovations ranging from fully electronic controls to a carbon-fibre fuselage. That's what will happen with computers, he says: \u201cInnovation will absolutely continue \u2014 but it will be more nuanced and complicated.\u201d The 1965 essay 1  that would make Gordon Moore famous started with a meditation on what could be done with the still-new technology of integrated circuits. Moore, who was then research director of Fairchild Semiconductor in San Jose, California, predicted wonders such as home computers, digital wristwatches, automatic cars and \u201cpersonal portable communications equipment\u201d \u2014 mobile phones. But the heart of the essay was Moore's attempt to provide a timeline for this future. As a measure of a microprocessor's computational power, he looked at transistors, the on\u2013off switches that make computing digital. On the basis of achievements by his company and others in the previous few years, he estimated that the number of transistors and other electronic components per chip was doubling every year. Moore, who would later co-found Intel in Santa Clara, California, underestimated the doubling time; in 1975, he revised it to a more realistic two years 2 . But his vision was spot on. The future that he predicted started to arrive in the 1970s and 1980s, with the advent of microprocessor-equipped consumer products such as the Hewlett Packard hand calculators, the Apple II computer and the IBM PC. Demand for such products was soon exploding, and manufacturers were engaging in a brisk competition to offer more and more capable chips in smaller and smaller packages (see  'Moore's lore' ). This was expensive. Improving a microprocessor's performance meant scaling down the elements of its circuit so that more of them could be packed together on the chip, and electrons could move between them more quickly. Scaling, in turn, required major refinements in photolithography, the basic technology for etching those microscopic elements onto a silicon surface. But the boom times were such that this hardly mattered: a self-reinforcing cycle set in. Chips were so versatile that manufacturers could make only a few types \u2014 processors and memory, mostly \u2014 and sell them in huge quantities. That gave them enough cash to cover the cost of upgrading their fabrication facilities, or 'fabs', and still drop the prices, thereby fuelling demand even further. Soon, however, it became clear that this market-driven cycle could not sustain the relentless cadence of Moore's law by itself. The chip-making process was getting too complex, often involving hundreds of stages, which meant that taking the next step down in scale required a network of materials-suppliers and apparatus-makers to deliver the right upgrades at the right time. \u201cIf you need 40 kinds of equipment and only 39 are ready, then everything stops,\u201d says Kenneth Flamm, an economist who studies the computer industry at the University of Texas at Austin. To provide that coordination, the industry devised its first road map. The idea, says Gargini, was \u201cthat everyone would have a rough estimate of where they were going, and they could raise an alarm if they saw roadblocks ahead\u201d. The US semiconductor industry launched the mapping effort in 1991, with hundreds of engineers from various companies working on the first report and its subsequent iterations, and Gargini, then the director of technology strategy at Intel, as its chair. In 1998, the effort became the International Technology Roadmap for Semiconductors, with participation from industry associations in Europe, Japan, Taiwan and South Korea. (This year's report, in keeping with its new approach, will be called the International Roadmap for Devices and Systems.) \u201cThe road map was an incredibly interesting experiment,\u201d says Flamm. \u201cSo far as I know, there is no example of anything like this in any other industry, where every manufacturer and supplier gets together and figures out what they are going to do.\u201d In effect, it converted Moore's law from an empirical observation into a self-fulfilling prophecy: new chips followed the law because the industry made sure that they did. And it all worked beautifully, says Flamm \u2014 right up until it didn't. The first stumbling block was not unexpected. Gargini and others had warned about it as far back as 1989. But it hit hard nonetheless: things got too small. \u201cIt used to be that whenever we would scale to smaller feature size, good things happened automatically,\u201d says Bill Bottoms, president of Third Millennium Test Solutions, an equipment manufacturer in Santa Clara. \u201cThe chips would go faster and consume less power.\u201d But in the early 2000s, when the features began to shrink below about 90 nanometres, that automatic benefit began to fail. As electrons had to move faster and faster through silicon circuits that were smaller and smaller, the chips began to get too hot. That was a fundamental problem. Heat is hard to get rid of, and no one wants to buy a mobile phone that burns their hand. So manufacturers seized on the only solutions they had, says Gargini. First, they stopped trying to increase 'clock rates' \u2014 how fast microprocessors execute instructions. This effectively put a speed limit on the chip's electrons and limited their ability to generate heat. The maximum clock rate hasn't budged since 2004. Second, to keep the chips moving along the Moore's law performance curve despite the speed limit, they redesigned the internal circuitry so that each chip contained not one processor, or 'core', but two, four or more. (Four and eight are common in today's desktop computers and smartphones.) In principle, says Gargini, \u201cyou can have the same output with four cores going at 250 megahertz as one going at 1 gigahertz\u201d. In practice, exploiting eight processors means that a problem has to be broken down into eight pieces \u2014 which for many algorithms is difficult to impossible. \u201cThe piece that can't be parallelized will limit your improvement,\u201d says Gargini. Even so, when combined with creative redesigns to compensate for electron leakage and other effects, these two solutions have enabled chip manufacturers to continue shrinking their circuits and keeping their transistor counts on track with Moore's law. The question now is what will happen in the early 2020s, when continued scaling is no longer possible with silicon because quantum effects have come into play. What comes next? \u201cWe're still struggling,\u201d says An Chen, an electrical engineer who works for the international chipmaker GlobalFoundries in Santa Clara, California, and who chairs a committee of the new road map that is looking into the question. That is not for a lack of ideas. One possibility is to embrace a completely new paradigm \u2014 something like  quantum computing , which promises exponential speed-up for certain calculations, or  neuromorphic computing , which aims to model processing elements on neurons in the brain. But none of these alternative paradigms has made it very far out of the laboratory. And many researchers think that quantum computing will offer advantages only for niche applications, rather than for the everyday tasks at which digital computing excels. \u201cWhat does it mean to quantum-balance a chequebook?\u201d wonders John Shalf, head of computer-science research at the Lawrence Berkeley National Laboratory in Berkeley, California. A different approach, which does stay in the digital realm, is the quest to find a 'millivolt switch': a material that could be used for devices at least as fast as their silicon counterparts, but that would generate much less heat. There are many candidates, ranging from  2D graphene-like compounds  to  spintronic materials  that would compute by flipping electron spins rather than by moving electrons. \u201cThere is an enormous research space to be explored once you step outside the confines of the established technology,\u201d says Thomas Theis, a physicist who directs the nanoelectronics initiative at the Semiconductor Research Corporation (SRC), a research-funding consortium in Durham, North Carolina. Unfortunately, no millivolt switch has made it out of the laboratory either. That leaves the architectural approach: stick with silicon, but configure it in entirely new ways. One popular option is to go 3D. Instead of etching flat circuits onto the surface of a silicon wafer, build skyscrapers: stack many thin layers of silicon with microcircuitry etched into each. In principle, this should make it possible to pack more computational power into the same space. In practice, however, this currently works only with memory chips, which do not have a heat problem: they use circuits that consume power only when a memory cell is accessed, which is not that often. One example is the Hybrid Memory Cube design, a stack of as many as eight memory layers that is being pursued by an industry consortium originally launched by Samsung and memory-maker Micron Technology in Boise, Idaho. Microprocessors are more challenging: stacking layer after layer of hot things simply makes them hotter. But one way to get around that problem is to do away with separate memory and microprocessing chips, as well as the prodigious amount of heat \u2014 at least 50% of the total \u2014 that is now generated in shuttling data back and forth between the two. Instead, integrate them in the same nanoscale high-rise. This is tricky, not least because current-generation microprocessors and memory chips are so different that they cannot be made on the same fab line; stacking them requires a complete redesign of the chip's structure. But several research groups are hoping to pull it off. Electrical engineer Subhasish Mitra and his colleagues at Stanford University in California have developed a hybrid architecture that stacks memory units together with transistors made from carbon nanotubes, which also carry current from layer to layer 3 . The group thinks that its architecture could reduce energy use to less than one-thousandth that of standard chips. The second stumbling block for Moore's law was more of a surprise, but unfolded at roughly the same time as the first: computing went mobile. Twenty-five years ago, computing was defined by the needs of desktop and laptop machines; supercomputers and data centres used essentially the same microprocessors, just packed together in much greater numbers. Not any more. Today, computing is increasingly defined by what high-end smartphones and tablets do \u2014 not to mention by smart watches and other  wearables , as well as by the exploding number of smart devices in everything from bridges to the  human body . And these mobile devices have priorities very different from those of their more sedentary cousins. Keeping abreast of Moore's law is fairly far down on the list \u2014 if only because mobile applications and data have largely migrated to the worldwide network of server farms known as the cloud. Those server farms now dominate the market for powerful, cutting-edge microprocessors that do follow Moore's law. \u201cWhat Google and Amazon decide to buy has a huge influence on what Intel decides to do,\u201d says Reed. Much more crucial for mobiles is the ability to survive for long periods on battery power while interacting with their surroundings and users. The chips in a typical smartphone must send and receive signals for voice calls, Wi-Fi, Bluetooth and the Global Positioning System, while also sensing touch, proximity, acceleration, magnetic fields \u2014 even fingerprints. On top of that, the device must host special-purpose circuits for power management, to keep all those functions from draining the battery. The problem for chipmakers is that this specialization is undermining the self-reinforcing economic cycle that once kept Moore's law humming. \u201cThe old market was that you would make a few different things, but sell a whole lot of them,\u201d says Reed. \u201cThe new market is that you have to make a lot of things, but sell a few hundred thousand apiece \u2014 so it had better be really cheap to design and fab them.\u201d Both are ongoing challenges. Getting separately manufactured technologies to work together harmoniously in a single device is often a nightmare, says Bottoms, who heads the new road map's committee on the subject. \u201cDifferent components, different materials, electronics, photonics and so on, all in the same package \u2014 these are issues that will have to be solved by new architectures, new simulations, new switches and more.\u201d For many of the special-purpose circuits, design is still something of a cottage industry \u2014 which means slow and costly. At the University of California, Berkeley, electrical engineer Alberto Sangiovanni-Vincentelli and his colleagues are trying to change that: instead of starting from scratch each time, they think that people should create new devices by combining large chunks of existing circuitry that have known functionality 4 . \u201cIt's like using Lego blocks,\u201d says Sangiovanni-Vincentelli. It's a challenge to make sure that the blocks work together, but \u201cif you were to use older methods of design, costs would be prohibitive\u201d. Costs, not surprisingly, are very much on the chipmakers' minds these days. \u201cThe end of Moore's law is not a technical issue, it is an economic issue,\u201d says Bottoms. Some companies, notably Intel, are still trying to shrink components before they hit the wall imposed by quantum effects, he says. But \u201cthe more we shrink, the more it costs\u201d. Every time the scale is halved, manufacturers need a whole new generation of ever more precise photolithography machines. Building a new fab line today requires an investment typically measured in many billions of dollars \u2014 something only a handful of companies can afford. And the fragmentation of the market triggered by mobile devices is making it harder to recoup that money. \u201cAs soon as the cost per transistor at the next node exceeds the existing cost,\u201d says Bottoms, \u201cthe scaling stops.\u201d Many observers think that the industry is perilously close to that point already. \u201cMy bet is that we run out of money before we run out of physics,\u201d says Reed. Certainly it is true that rising costs over the past decade have forced a massive consolidation in the chip-making industry. Most of the world's production lines now belong to a comparative handful of multinationals such as Intel, Samsung and the Taiwan Semiconductor Manufacturing Company in Hsinchu. These manufacturing giants have tight relationships with the companies that supply them with materials and fabrication equipment; they are already coordinating, and no longer find the road-map process all that useful. \u201cThe chip manufacturer's buy-in is definitely less than before,\u201d says Chen. Take the SRC, which functions as the US industry's research agency: it was a long-time supporter of the road map, says SRC vice-president Steven Hillenius. \u201cBut about three years ago, the SRC contributions went away because the member companies didn't see the value in it.\u201d The SRC, along with the SIA, wants to push a more long-term, basic research agenda and secure federal funding for it \u2014 possibly through the White House's National Strategic Computing Initiative, launched in July last year. That agenda, laid out in a report 5  last September, sketches out the research challenges ahead. Energy efficiency is an urgent priority \u2014 especially for the embedded smart sensors that comprise the 'Internet of things', which will need new technology to survive without batteries, using energy scavenged from ambient heat and vibration. Connectivity is equally key: billions of free-roaming devices trying to communicate with one another and the cloud will need  huge amounts of bandwidth , which they can get if researchers can tap the once-unreachable terahertz band lying deep in the infrared spectrum. And security is crucial \u2014 the report calls for research into new ways to build in safeguards against cyberattack and data theft. These priorities and others will give researchers plenty to work on in coming years. At least some industry insiders, including Shekhar Borkar, head of Intel's advanced microprocessor research, are optimists. Yes, he says, Moore's law is coming to an end in a literal sense, because the exponential growth in transistor count cannot continue. But from the consumer perspective, \u201cMoore's law simply states that user value doubles every two years\u201d. And in that form, the law will continue as long as the industry can keep stuffing its devices with new functionality. The ideas are out there, says Borkar. \u201cOur job is to engineer them.\u201d"}
]