[
{"file_id": "d41586-017-08542-5", "url": "https://www.nature.com/articles/d41586-017-08542-5", "year": 2017, "authors": [], "parsed_as_year": "2006_or_before", "body": ""},
{"file_id": "550174a", "url": "https://www.nature.com/articles/550174a", "year": 2017, "authors": [{"name": "Erika Check Hayden"}], "parsed_as_year": "2006_or_before", "body": "How Anne Wojcicki led her company from the brink of failure to scientific pre-eminence. There's a placard in Anne Wojcicki's office enshrining the attitude that nearly ran her company, 23andme, aground. Tucked behind a toy unicorn, the small, wood-veneered nameplate reads: \u201cI'm CEO, bitch.\u201d It was with this kind of brashness that Wojcicki set out to disrupt the health-care industry in 2006. Her goal was to put sophisticated DNA analyses into the hands of consumers, giving them information about health, disease and ancestry, and allowing the company to sell access to the genetic data to fuel research. But in 2013, that vision hit a snag. Wojcicki didn't think she needed regulatory approval to provide information about her customers' health risks. The US Food and Drug Administration (FDA) disagreed, and ordered the company to stop. The FDA action prompted months of soul-searching and strategizing on how to reorient the company to work with regulators. \u201cYou just accept at some point, you're regulated, and there's no Silicon-Valley, 24-hour, easy fix,\u201d Wojcicki says. After years of effort, the pay-off came in April this year, when the FDA agreed to allow 23andme to tell consumers their risks of developing ten medical conditions, including Parkinson's disease and late-onset Alzheimer's disease. Surfing a wave of positive news, the company has since launched an advertising blitz to dramatically expand its customer base to 10 million people. 23andme has always been the most visible face of direct-to-consumer genetic testing, and it is more formidable now than ever before. In September, the company announced that it had raised US$250 million: more than the total amount of capital raised by the company since its inception. Investors estimate that it is worth more than $1 billion, making it a 'unicorn' in Silicon Valley parlance \u2014 a rare and valuable thing to behold. But for scientists, 23andme's real worth is in its data. With more than 2 million customers, the company hosts by far the largest collection of gene-linked health data anywhere. It has racked up 80 publications, signed more than 20 partnerships with pharmaceutical firms and started a therapeutics division of its own. \u201cThey have quietly become the largest genetic study the world has ever known,\u201d says cardiologist Euan Ashley at Stanford University, California. But as it matures, 23andme faces new challenges. It must sustain customers' trust, fight off competition and prove that it can use genetic data to make new medicines \u2014 a notoriously difficult goal. And 23andme still has a long way to go with the FDA, which won't allow it to tell customers many genetic results directly relevant to human health, such as those for the  BRCA  genes, which are linked to breast cancer. Still, Wojcicki is undeterred. \u201cI'm very stubborn,\u201d she says. \n               In the picture \n             23andme's headquarters in Mountain View, California, have a start-up vibe that belies the company's 11-year history. Pink and green foil balloons float over cubicles to commemorate employees' work anniversaries. The kitchenette is stocked with healthy snacks. And Polaroid photographs of all employees line the wall of the free cafeteria. Each picture is scrawled with a quirky fact about the person. (\u201cHer favorite drink is green tea,\u201d reads one. \u201cOnce won a lip-sync contest singing a New Kids on the Block song,\u201d boasts another.) Arranged by the order in which employees joined the company, the photos make clear where everyone fits in. The first photo, of course, is of Wojcicki, who grew up on the campus of Stanford University, the child of a teacher and a physics professor. She majored in biology at Yale University in New Haven, Connecticut, where she played ice hockey. (She's still an avid athlete; the bike she rides to work is often parked in 23andme's lobby.) In 1996, after graduating, Wojcicki worked for investment companies and hedge funds analysing heath-care ventures. She eventually came to dislike how the industry incentivized the development of expensive products and services that earn maximum insurance payments, rather than treatments and devices that consumers can afford to pay for on their own. Wojcicki founded 23andme in 2006 with Linda Avey and Paul Cusenza with a goal of upending conventional models of health care. The following year, it received $8.95 million from a number of high-powered investors, including the biotechnology powerhouse Genentech in South San Francisco and Google, whose co-founder Sergey Brin was married to Wojcicki from 2007 to 2015. Wojcicki aimed to attract millions of customers by selling an inexpensive test that would reveal genetic predispositions for dozens of traits. It would provide disease risks, but also genetic propensity for baldness, obesity and trivial features such as earwax consistency. Wojcicki wanted to make the genome fun and engaging, the better to attract customers. She hosted celebrity 'spit' parties to get the product in the hands of tastemakers and stir up media interest: after taking one of the company's tests, Ivanka Trump gloated that she had a very low genetic risk of becoming obese. As the tests hit the market in late 2007, Wojcicki and Avey were hailed as visionaries (Cusenza had left in 2007; Avey would depart in 2009). Scientists, meanwhile, were dubious. Family history was and is still a more powerful indicator than genes are for predicting the risk of most diseases. \u201cThe evidence is increasingly strong that the benefits of direct-to-consumer testing for these kinds of indications are somewhere between small and zero,\u201d says Stanford University lawyer and ethicist Hank Greely, a long-time critic of the company. There were also questions about 23andme's plan to sell customer data to help develop medicines. Companies have been trying to mine genetic data to design drugs for at least a decade, with little success. Take deCODE genetics, founded in Reykjavik in 1996, which recruited about half of the adult population of Iceland into a genetic study. Although the company's research has provided insights into the genetic mechanisms of disease, it hasn't yet yielded a drug. Scientists' scepticism didn't deter hundreds of thousands of customers from signing up to 23andme, nor did it stop investors from ploughing $118 million into the company in its first five years \u2014 but a problem was emerging in the background. In 2009, the FDA started asking 23andme for evidence that the company's products worked as advertised and wouldn't harm customers. The agency was worried that people might take drastic medical measures on the basis of their test results, such as deciding to change the dosage of their medications without consulting a doctor or undergoing unnecessary surgery, such as a mastectomy, or treatment based on false positives. Regulators demanded evidence that the tests were accurate, and that customers were well informed what the results meant. The next years were difficult ones for 23andme. It communicated with the agency on a few occasions and promised in January 2013 that data would be forthcoming. According to the FDA, it then ceased communicating with regulators entirely in May, even as it started a new advertising campaign. Fed up, the agency sent Wojcicki a strongly worded warning letter on 22 November 2013 ordering her company to stop marketing its product. It was a self-inflicted wound for the company. \u201cThere was a bit of arrogance,\u201d says Richard Scheller, who was an executive at Genentech at the time. As a result, 23andme was forced to drastically cut its customer offerings, threatening its viability. Wojcicki was stunned. \u201cIt became clear that we had pissed them off,\u201d she says. \u201cI really didn't know that we had done so many things that angered them.\u201d \n               Back on track \n             Soon after the letter arrived, Wojcicki called Kathy Hibbs, a lawyer then working for Genomic Health, a gene-testing company in nearby Redwood City, California. \u201cCan I get my whole company back in one year?\u201d Wojcicki asked Hibbs. \u201cYou can get it back, but it will take years,\u201d Hibbs replied. And to get there, she counselled, Wojcicki would have to cooperate with regulators. It was a tough adjustment for Wojcicki; she didn't think that the FDA should be able to stop customers from learning their own genetic information. But Hibbs and others convinced her that capitulating to the FDA's demands was the fastest way to rescue her company. \u201cIt's almost like being in a relationship,\u201d Wojcicki says. \u201cThere's things that you might disagree with, but you just have to do them.\u201d Wojcicki hired Hibbs, who began gathering evidence to respond to the FDA's concerns \u2014 a formidable task, because the FDA and the company had tussled over many issues over the years. By the end of 2014, Hibbs felt that the company was ready, so she asked the FDA to approve one test, intended to tell customers whether their children might inherit a genetic risk for a disease called Bloom syndrome. The FDA approved the test in February 2015. The news didn't make a huge public splash: Bloom syndrome is a very rare disorder, affecting about 1 in 50,000 people with Ashkenazi Jewish heritage. But 23andme was now the first company approved to market a direct-to-consumer genetic test for a disease in the United States, although it had already been offering the test overseas. But even after the FDA's decision this April, 23andme is still barred from giving customers lots of available information, such as whether they carry gene variants that raise their risk for certain cancers or that predict how well certain medications will work. Before the FDA lockdown, it had been providing information on hundreds of health conditions. Greely says that the restrictions make sense: there is very strong evidence that genetic variants cause the ten conditions listed in the FDA's approval in April. But the predictive value is much weaker for the variants linked to the vast majority of common health conditions that 23andme would like to tell its customers about. \n               Paths of discovery \n             Even as the company confronted resistance at the FDA, it was making moves into drug development. Key to this plan was bringing Scheller aboard. Wojcicki e-mailed him on the day he announced his retirement from Genentech in December 2014. Four months later, Scheller arrived in Mountain View to start 23andme's therapeutics group; by July, Wojcicki had raised $115 million more from investors. Scheller was attracted not just by the size of 23andme's database, but by its richness. Customers have each answered an average of 300 questions on a huge array of traits, including their medical histories. That enables Scheller's team to try a different approach for gene-driven drug development. The standard method has been a genome-wide association study, or GWAS, in which scientists gather people with a disease or trait, and then look for gene variants that seem to contribute to it. Scheller's team can do the reverse. They start with a particular gene that known drugs target, and then look for the diseases or health traits \u2014 the phenotypes \u2014 that are associated most strongly with different variants in the gene. \u201cWe just let the database show us what to work on,\u201d Scheller says. It's a study design called a phenome-wide association study, or PheWAS \u2014 and Erik Karrer, director of drug discovery, calls it the company's \u201csecret sauce\u201d. 23andme is banking that it will speed drug discovery by allowing scientists to select drug targets that are important in human biology, that can be targeted by drugs and that are less likely to cause side effects. To see if it works, computational biologist Fah Sathirapongsasuti studied whether 23andme's genetic and health data could predict the success of drugs developed over the past few decades. Sathirapongsasuti surveyed a database of thousands of drug compounds, some of which were approved for sale by regulators. He compiled a list of all the genes encoding proteins targeted by drugs in this database, and compared it against variations in these genes among 23andme's customers, checking to see what medical conditions they had reported to the company. The process helped to validate the genetic basis for some drugs in humans in a way that mouse studies and other preclinical research often can't. Sathirapongsasuti also found instances in which 23andme customer data correctly predicted side effects of approved drugs. And the data were able to predict which drugs approved for some conditions might work better for others. Isofagomine tartrate, for instance, was initially intended to treat Gaucher's disease, a rare genetic disorder, but it stalled after a failed clinical trial in 2009. Sathirapongsasuti's data suggest that the drug might also affect the processes underlying Parkinson's disease. The compound has been tested for this condition as well. Sathirapongsasuti's data suggested that the PheWAS approach could be useful in drug development \u2014 and helped to convince 23andme that it should invest in its own drug programme. Using the results of additional phenome-wide association studies, Scheller and his team have now decided to focus on seven drug targets in four categories of disease: cancer, cardiovascular disease, skin disease and immune disorders, such as asthma. Most scientists no longer see 23andme as a frivolous undertaking. The ability to recruit two million customers, and potentially many more, has been a huge draw, and researchers are lining up to collaborate with the company. Other major biobanks can boast no more than half a million people in their ranks. \u201cThey have the power of ' N ',\u201d says cardiologist Eric Topol, director of the Scripps Translational Science Institute in La Jolla, California. In October, the US National Institutes of Health awarded the company a $1.7-million grant to sequence the genomes of hundreds of thousands of its African American customers who had already bought the company's standard product, which provides an overview of the genome rather than an in-depth analysis. The project \u2014 one of several sequencing initiatives that the company has started \u2014 is intended to help rectify the paucity of sequencing data on racial and ethnic minorities. It's still an adjustment for scientists to work with 23andme data, because the company asks its collaborators to follow unusual rules. Its agreement with customers forbids it from sharing their actual data with collaborators, so scientists see only the results of analyses run by the company and never have access to the raw data that inform the studies. And some scientists are uneasy about the self-reported data resulting from 23andme questionnaires. Neurogeneticist Ashley Winslow, for instance, who led a high-profile collaboration with Pfizer to identify genetic markers associated with depression, says that peer reviewers of the resulting paper were concerned about the veracity of 23andme's customer data. They argued that people who said that they had been diagnosed with clinical depression might just have been feeling low on the day that they took the company's survey. Winslow's team ran internal studies on the validity of the data, such as analyses showing the percentage of people who also reported using selective serotonin re-uptake inhibitors. The analyses were sufficient to get the paper published, but such concerns will probably come up again. \u201cSome communities might still be more dubious and demand more from the data to prove its relevance,\u201d says Winslow, who is now at the University of Pennsylvania in Philadelphia. But, she adds, the results of a large study such as hers, which has since been validated by another large psychiatric genetics consortium, are encouraging more scientists to work with the company. \u201cThere is definitely an openness that didn't used to exist,\u201d Winslow says. But that doesn't mean that 23andme's model will definitely lead to new drugs. Several high-profile drugs based on human-genetics research have failed to live up to their potential, or have failed entirely. In May, for instance, pharmaceutical company Amgen, based in Thousand Oaks, California, announced that its genetically targeted osteoporosis drug romosozumab raised the risk of heart disease by as much as 30% in a clinical trial with 4,000 people. \u201cThe idea of developing drugs as a result of genetics isn't as straightforward as many of us would like,\u201d Topol says. The direct-to-consumer genetic testing market has been transformed since 23andme's early years. And although it is a small slice of the gene-testing market, it is expected to grow to $340 million in the next five years (see 'Gene drive'). And a growing crop of genetic-analysis companies are now competing for 23andme's customers. They include firms offering inexpensive, targeted medical sequencing (Color Genomics in Burlingame, California); ancestry testing (Ancestry DNA, based in Salt Lake City, Utah); whole-genome sequencing, either on its own (Veritas, based in Danvers, Massachusetts) or in combination with medical testing (Craig Venter's Human Longevity in San Diego, California) or with apps for interpreting genomic data (Helix of San Carlos, California). Wojcicki's competitors give her credit for showing that there may be a business in gathering and selling genetic data. \u201cI'm a big admirer of 23andme and what they've done for the entire industry in pioneering both consumer genetics and this difficult regulatory road,\u201d says Mirza Cifric, chief executive of Veritas. 23andme is still the only company offering FDA-approved direct-to-consumer health tests and no competitors have indicated a willingness to go down that path. Wojcicki, for her part, still wants to stay ahead. \u201cThere's all kinds of ways we want to approach genetics,\u201d she says. For instance, 23andme is watching closely as technology companies such as Apple and Google develop sensors and mobile health-data applications, and the company is looking for pilot projects in this space, which could allow it to seamlessly collect continuous data from its users. And she has no doubt that the company will achieve her goal of recruiting 10 million customers. \u201cJust based on natural growth we'll get there,\u201d she says. In the 23andme company cafeteria, the fun fact on Wojcicki's Polaroid picture seems at once trivial and telling: \u201cI once ate so many carrots that I turned orange and was told not to eat carrots for a year.\u201d Wojcicki's colour has come back. She took the advice. But whether her resolve and ability to correct course can also push 23andme from earwax and ancestry to life-saving drugs remains an open question. If she has her way, it's her doubters who will one day become the real unicorns of Silicon Valley \u2014 so rare and shy, you'd hardly believe they exist. \n                 Tweet \n                 Follow @NatureNews \n               \n                     23andMe given green light to sell DNA tests for 10 diseases 2017-Apr-06 \n                   \n                     Stop the privatization of health data 2016-Jul-20 \n                   \n                     Out of regulatory limbo, 23andMe resumes some health tests and hopes to offer more 2015-Oct-27 \n                   \n                     Scientists hope to attract millions to 'DNA.LAND' 2015-Oct-09 \n                   \n                     The FDA and me 2013-Dec-03 \n                   Reprints and Permissions"},
{"file_id": "550448a", "url": "https://www.nature.com/articles/550448a", "year": 2017, "authors": [{"name": "Megan Scudellari"}], "parsed_as_year": "2006_or_before", "body": "Killing off cells that refuse to die on their own has proved a powerful anti-ageing strategy in mice. Now it's about to be tested in humans. Jan van Deursen was baffled by the decrepit-looking transgenic mice he created in 2000. Instead of developing tumours as expected, the mice experienced a stranger malady. By the time they were three months old, their fur had grown thin and their eyes were glazed with cataracts. It took him years to work out why: the mice were ageing rapidly, their bodies clogged with a strange type of cell that did not divide, but that wouldn't die 1 . That gave van Deursen and his colleagues at Mayo Clinic in Rochester, Minnesota, an idea: could killing off these 'zombie' cells in the mice delay their premature descent into old age? The answer was yes. In a 2011 study 2 , the team found that eliminating these 'senescent' cells forestalled many of the ravages of age. The discovery set off a spate of similar findings. In the seven years since, dozens of experiments have confirmed that senescent cells accumulate in ageing organs, and that eliminating them can alleviate, or even prevent, certain illnesses (see 'Becoming undead'). This year alone, clearing the cells in mice has been shown to restore fitness, fur density and kidney function 3 . It has also improved lung disease 4  and even mended damaged cartilage 5 . And in a 2016 study, it seemed to extend the lifespan of normally ageing mice 6 . \u201cJust by removing senescent cells, you could stimulate new tissue production,\u201d says Jennifer Elisseeff, senior author of the cartilage paper and a biomedical engineer at Johns Hopkins University in Baltimore, Maryland. It jump-starts some of the tissue's natural repair mechanisms, she says. This anti-ageing phenomenon has been an unexpected twist in the study of senescent cells, a common, non-dividing cell type first described more than five decades ago. When a cell enters senescence \u2014 and almost all cells have the potential to do so \u2014 it stops producing copies of itself, begins to belch out hundreds of proteins, and cranks up anti-death pathways full blast. A senescent cell is in its twilight: not quite dead, but not dividing as it did at its peak. Now biotechnology and pharmaceutical companies are keen to test drugs \u2014 known as senolytics \u2014 that kill senescent cells in the hope of rolling back, or at least forestalling, the ravages of age. Unity Biotechnology in San Francisco, California, co-founded by van Deursen, plans to conduct multiple clinical trials over the next two-and-a-half years, treating people with osteoarthritis, eye diseases and pulmonary diseases. At Mayo, gerontologist James Kirkland, who took part in the 2011 study, is cautiously beginning a handful of small, proof-of-concept trials that pit senolytic drugs against a range of age-related ailments. \u201cI lose sleep at night because these things always look good in mice or rats, but when you get to people you hit a brick wall,\u201d says Kirkland. No other anti-ageing elixir has yet cleared that wall , and for a few good reasons. It's next to impossible to get funding for clinical trials that measure an increase in healthy lifespan. And even as a concept, ageing is slippery. The US Food and Drug Administration has not labelled it a condition in need of treatment. Still, if any of the trials offer \u201ca whiff of human efficacy\u201d, says Unity's president, Ned David, there will be a massive push to develop treatments and to  better understand the fundamental process of ageing . Other researchers who study the process are watching closely. Senolytics are \u201cabsolutely ready\u201d for clinical trials, says Nir Barzilai, director of the Institute for Aging Research at the Albert Einstein College of Medicine in New York City. \u201cI think senolytics are drugs that could come soon and be effective in the elderly now, even in the next few years.\u201d \n               The dark side \n             When microbiologists Leonard Hayflick and Paul Moorhead  coined the term senescence  in 1961, they suggested that it represented ageing on a cellular level. But very little research was done on ageing at the time, and Hayflick recalls people calling him an idiot for making the observation. The idea was ignored for decades. Although many cells do die on their own, all somatic cells (those other than reproductive ones) that divide have the ability to undergo senescence. But, for a long time, these twilight cells were simply a curiosity, says Manuel Serrano of the Institute for Research in Biomedicine in Barcelona, Spain, who has studied senescence for more than 25 years. \u201cWe were not sure if they were doing something important.\u201d Despite self-disabling the ability to replicate, senescent cells stay metabolically active, often continuing to perform basic cellular functions. By the mid-2000s, senescence was chiefly understood as a way of arresting the growth of damaged cells to suppress tumours. Today, researchers continue to study how senescence arises in development and disease. They know that when a cell becomes mutated or injured, it often stops dividing \u2014 to avoid passing that damage to daughter cells. Senescent cells have also been identified in the placenta and embryo, where they seem to guide the formation of temporary structures before being cleared out by other cells. Hear Judy Campisi and Jan van Deursen discuss why they're excited to be researching senescence. But it wasn't long before researchers discovered what molecular biologist Judith Campisi calls the \u201cdark side\u201d of senescence. In 2008, three research groups, including Campisi's at the Buck Institute for Research on Aging in Novato, California, revealed that senescent cells excrete a glut of molecules \u2014 including cytokines, growth factors and proteases \u2014 that affect the function of nearby cells and incite local inflammation 7 , 8 , 9 . Campisi's group described this activity as the cell's senescence-associated secretory phenotype, or SASP 7 . In recent unpublished work, her team identified hundreds of proteins involved in SASPs. In young, healthy tissue, says Serrano, these secretions are probably part of a restorative process, by which damaged cells stimulate repair in nearby tissues and emit a distress signal prompting the immune system to eliminate them. Yet at some point, senescent cells begin to accumulate \u2014 a process linked to problems such as osteoarthritis, a chronic inflammation of the joints, and atherosclerosis, a hardening of the arteries. No one is quite sure when or why that happens. It has been suggested that, over time, the immune system stops responding to the cells. Surprisingly, senescent cells turn out to be slightly different in each tissue. They secrete different cytokines, express different extracellular proteins and use different tactics to avoid death. That incredible variety has made it a challenge for labs to detect and visualize senescent cells. \u201cThere is nothing definitive about a senescent cell. Nothing. Period,\u201d says Campisi. In fact, even the defining feature of a senescent cell \u2014 that it does not divide \u2014 is not written in stone. After chemotherapy, for example, cells take up to two weeks to become senescent, before reverting at some later point to a proliferating, cancerous state, says Hayley McDaid, a pharmacologist at Albert Einstein College of Medicine. In support of that idea, a large collaboration of researchers found this year that removing senescent cells right after chemotherapy, in mouse models for skin and breast cancer, makes the cancer less likely to spread 10 . The lack of universal features makes it hard to take inventory of senescent cells. Researchers have to use a large panel of markers to search for them in tissue, making the work laborious and expensive, says van Deursen. A universal marker for senescence would make the job much easier \u2014 but researchers know of no specific protein to label, or process to identify. \u201cMy money would be on us never finding a senescent-specific marker,\u201d Campisi adds. \u201cI would bet a good bottle of wine on that.\u201d Earlier this year, however, one group did develop a way to count these cells in tissue. Valery Krizhanovsky and his colleagues at the Weizmann Institute of Science in Rehovot, Israel, stained tissues for molecular markers of senescence and imaged them to analyse the number of senescent cells in tumours and aged tissues from mice 11 . \u201cThere were quite a few more cells than I actually thought that we would find,\u201d says Krizhanovsky. In young mice, no more than 1% of cells in any given organ were senescent. In two-year-old mice, however, up to 20% of cells were senescent in some organs. But there's a silver lining to these elusive twilight cells: they might be hard to find, but they're easy to kill. \n               Out with the old \n             In November 2011, while on a three-hour flight, David read van Deursen and Kirkland's just-published paper about eliminating zombie cells. Then he read it again, and then a third time. The idea \u201cwas so simple and beautiful\u201d, recalls David. \u201cIt was almost poetic.\u201d When the flight landed, David, a serial biotech entrepreneur, immediately rang van Deursen, and within 72 hours had convinced him to meet to discuss forming an anti-ageing company. Kirkland, together with collaborators at the Sanford Burnham Medical Research Institute in La Jolla, California, initially attempted a high-throughput screen to quickly identify a compound that would kill senescent cells. But they found it to be \u201ca monumental task\u201d to tell whether a drug was affecting dividing or non-dividing cells, Kirkland recalls. After several failed attempts, he took another tack. Senescent cells depend on protective mechanisms to survive in their 'undead' state, so Kirkland, in collaboration with Laura Niedernhofer and others from the Scripps Research Institute in Jupiter, Florida, began seeking out those mechanisms. They identified six signalling pathways that prevent cell death, which senescent cells activate to survive 12 , 13 . Then it was just a matter of finding compounds that would disrupt those pathways. In early 2015, the team identified the first senolytics: an FDA-approved chemotherapy drug, dasatinib, which eliminates human fat-cell progenitors that have turned senescent; and a plant-derived health-food supplement, quercetin, which targets senescent human endothelial cells, among other cell types. The combination of the two \u2014 which work better together than apart \u2014 alleviates a range of age-related disorders in mice 14 . Ten months later, Daohong Zhou at the University of Arkansas for Medical Sciences in Little Rock and his colleagues identified a senolytic compound now known as navitoclax, which inhibits two proteins in the BCL-2 family that usually help the cells to survive 15 . Similar findings were reported within weeks by Kirkland's lab 16  and Krizhanovsky's lab 17 . By now, 14 senolytics have been described in the literature, including small molecules, antibodies and, in March this year, a peptide that activates a cell-death pathway and can restore lustrous hair and physical fitness to ageing mice 3 . So far, each senolytic kills a particular flavour of senescent cell. Targeting the different diseases of ageing, therefore, will require multiple types of senolytics. \u201cThat's what's going to make this difficult: each senescent cell might have a different way to protect itself, so we'll have to find combinations of drugs to wipe them all out,\u201d says Niedernhofer. Unity maintains a large atlas documenting which senescent cells are associated with which disease; any weaknesses unique to given kinds of cell, and how to exploit those flaws; and the chemistry required to build the right drug for a particular tissue. There is no doubt that for different indications, different types of drug will need to be developed, says David. \u201cIn a perfect world, you wouldn't have to. But sadly, biology did not get that memo.\u201d For all the challenges, senolytic drugs have several attractive qualities. Senescent cells will probably need to be cleared only periodically \u2014 say, once a year \u2014 to prevent or delay disease. So the drug is around for only a short time. This type of 'hit and run' delivery could reduce the chance of side effects, and people could take the drugs during periods of good health. Unity plans to inject the compounds directly into diseased tissue, such as a knee joint in the case of osteoarthritis, or the back of the eye for someone with age-related macular degeneration. And unlike cancer, in which a single remaining cell can spark a new tumour, there's no need to kill every senescent cell in a tissue: mouse studies suggest that dispatching most of them is enough to make a difference. Finally, senolytic drugs will clear only senescent cells that are already present \u2014 they won't prevent the formation of such cells in the future, which means that senescence can continue to perform its original tumour-suppressing role in the body. Those perks haven't convinced everybody of the power of senolytics. Almost 60 years after his initial discovery, Hayflick now believes that ageing is an inexorable biophysical process that cannot be altered by eliminating senescent cells. \u201cEfforts to interfere with the ageing process have been going on since recorded human history,\u201d says Hayflick. \u201cAnd we know of nothing \u2014 nothing \u2014 that has demonstrated to interfere with the ageing process.\u201d Fans of senolytics are much more optimistic, emboldened by recent results. Last year, van Deursen's lab went beyond its tests on super-aged mice and showed that killing off senescent cells in normally ageing mice  delayed the deterioration of organs  associated with ageing 6 , including the kidney and heart. And \u2014 to the joy of anti-ageing enthusiasts everywhere \u2014 it extended the animals' median lifespan by about 25%. Successful results from mouse studies have already lured seven or eight companies into the field, Kirkland estimates. At Mayo, one clinical trial has opened, pitting dasatinib and quercetin in combination against chronic kidney disease. Kirkland plans to try other senolytics against different age-related diseases. \u201cWe want to use more than one set of agents across the trials and look at more than one condition,\u201d he says. If eliminating senescent cells in humans does improve age-related illnesses, researchers will aim to create broader anti-ageing therapies, says David. In the meantime, researchers in the field insist that no one should take these drugs until proper safety tests in humans are complete. In rodents, senolytic compounds have been shown to delay wound healing, and there could be additional side effects. \u201cIt's just too dangerous,\u201d says Kirkland. Van Deursen says that continuing to answer basic biological questions is the field's  best shot at success . \u201cOnly then will we be able to understand what ageing really is, and how we can, in an intelligent way, interfere with it.\u201d \n                 Tweet \n                 Follow @NatureNews \n               \n                     Destroying worn-out cells makes mice live longer 2016-Feb-03 \n                   \n                     Ageing research: Blood to blood 2015-Jan-21 \n                   \n                     Medical research: Treat ageing 2014-Jul-23 \n                   \n                     The role of senescent cells in ageing 2014-May-21 \n                   \n                     Unity Biotechnology \n                   Reprints and Permissions"},
{"file_id": "550316a", "url": "https://www.nature.com/articles/550316a", "year": 2017, "authors": [{"name": "Emily Anthes"}], "parsed_as_year": "2006_or_before", "body": "Three ways that the digital revolution is reshaping workforces around the world. Last year, entrepreneur Sebastian Thrun set out to augment his sales force with artificial intelligence. Thrun is the founder and president of Udacity, an education company that provides online courses and employs an armada of salespeople who answer questions from potential students through online chats. Thrun, who also runs a computer-science lab at Stanford University in California, worked with one of his students to collect the transcripts of these chats, noting which resulted in students signing up for a course. The pair fed the chats into a machine-learning system, which was able to glean the most effective responses to a variety of common questions. Next, they put this digital sales assistant to work alongside human colleagues. When a query came in, the program would suggest an appropriate response, which a salesperson could tailor if necessary. It was an instantaneously reactive sales script with reams of data supporting every part of the pitch. And it worked; the team was able to handle twice as many prospects at once and convert a higher percentage of them into sales. The system, Thrun says, essentially packaged the skills of the company's best salespeople and bequeathed them to the entire team \u2014 a process that he views as potentially revolutionary. \u201cJust as much as the steam engine and the car have amplified our muscle power, this could amplify our brainpower and turn us into superhumans intellectually,\u201d he says. The past decade has seen remarkable advances in digital technologies, including artificial intelligence (AI), robotics, cloud computing, data analytics and mobile communications. Over the coming decades, these technologies will transform nearly every industry \u2014 from agriculture, medicine and manufacturing to sales, finance and transportation \u2014 and reshape the nature of work. \u201cMillions of jobs will be eliminated, millions of new jobs will be created and needed, and far more jobs will be transformed,\u201d says Erik Brynjolfsson, who directs the Initiative on the Digital Economy at the Massachusetts Institute of Technology in Cambridge. But making firm predictions is difficult. \u201cThe technology is rushing ahead, which in a way is a good thing, but we have a huge gap in understanding its implications,\u201d Brynjolfsson says. \u201cThere's a huge need, a huge opportunity, to study the changes.\u201d Researchers are beginning to do just that, and the emerging evidence resists simple storylines. Advances in digital technologies are likely to change work in complex and nuanced ways, creating both opportunities and risks for workers (see 'More research needed'). \n               boxed-text \n             Here are three pressing questions about the future of work in a digital world and how researchers are beginning to answer them. \n               Will machine learning displace skilled workers? \n             In previous waves of automation, technological advances have allowed machines to take over tasks that were simple, repetitive and routine. Machine learning opens up the possibility of automating more complex, non-routine cognitive tasks. \u201cFor most of the last 40 or 50 years, it was impossible to automate a task before we understood it extremely well,\u201d Brynjolfsson says. \u201cThat's not true anymore. Now machines can learn on their own.\u201d Machine-learning systems can translate speech, label images, pick stocks, detect fraud and diagnose disease \u2014 rivalling human performance in some new and surprising domains. \u201cA machine can actually look at many, many, many more data samples than a human can handle,\u201d says Thrun. Earlier this year, he led a team that demonstrated that some 129,000 images of skin lesions could be used to train a machine to diagnose skin cancer with a level of accuracy that matches that of qualified dermatologists 1 . Reporter Benjamin Thompson finds out how lessons from the past can help explore the future of work. These advances have raised concerns that such systems could replace human workers in fields that once seemed too complex to be automated. Early estimates seemed dire. In 2013, researchers at the Oxford Martin Programme on Technology and Employment at the University of Oxford, UK, reviewed the advances and lingering challenges in machine learning and mobile robotics to estimate how susceptible 702 different occupations were to automation 2 . Their startling conclusion was that 47% of jobs in the United States were at high risk of computerization, with jobs in transportation, logistics, production and administrative support particularly vulnerable. That spelt trouble for workers such as taxi drivers, legal secretaries and file clerks. Since then, however, other researchers have argued that the 47% figure is much too high, given the variety of tasks that workers in many occupations tend to perform. \u201cOnce you go deeper, once you look into the task structure of what people really do at work, then you find that the estimates get much lower,\u201d says Ulrich Zierahn, a senior researcher at the Centre for European Economic Research in Mannheim, Germany. For instance, the Oxford study reported that clerks in bookkeeping, accounting and auditing face an automation risk of 98%. But when Zierahn and his colleagues analysed survey data on what people in those professions actually do, the team found that 76% of them had jobs that required group work or face-to-face interaction. For now at least, such tasks are not easily automated 3 . When the authors extended their approach to other professions, they found less-alarming figures for the number of at-risk jobs in the 21 countries surveyed. In the United States, the share of workers at high risk of automation was just 9%, and the figure ranged from a low of 6% in South Korea and Estonia to a high of 12% in Germany and Austria (see 'Delaying the robot uprising'). Brynjolfsson is now working with Tom Mitchell, a computer scientist at Carnegie Mellon University in Pittsburgh, Pennsylvania, to  drill deeper into the impact of machine learning . They have developed a rubric outlining the characteristics that make certain tasks especially amenable to this approach. For instance, machine-learning systems are adept at tasks that involve translating one set of inputs \u2014 say, images of skin lesions \u2014 into another set of outputs, such as cancer diagnoses. They're also most likely to be used for tasks in which the large digital data sets required for training the system are readily available. Brynjolfsson and Mitchell are now going through several large occupational databases to determine how well a variety of workplace tasks match up with these and other criteria. Even with these kinds of analysis in hand, determining the consequences for the labour market is complex. Just because a task can be automated doesn't mean that it will be; new technologies often require costly and time-consuming organizational changes. Legal, ethical and societal barriers can also delay or derail their deployment. \u201cAI is not yet an off-the-shelf product,\u201d says Federico Cabitza, who studies health-care informatics at the University of Milano-Bicocca in Italy. Implementing medical machine-learning systems, for instance, requires both technological readiness and willingness to devote the thousands of person-hours necessary to make these systems operational, he says \u2014 not to mention buy-in from caregivers and patients. Research suggests that the workforce is flexible in adapting to new technologies. In the second half of the twentieth century, increasing automation prompted shifts within occupations as employees began performing more complex and non-routine tasks. In some future cases, these shifts could be positive; if automated systems start making routine medical diagnoses, it could free doctors to spend more time interacting with patients and working on complex cases. \u201cThe fact that computers are becoming good at medical diagnosis doesn't mean that doctors will disappear as a job category,\u201d Mitchell says. \u201cMaybe it means we'll have better doctors.\u201d Indeed, many people might find themselves working alongside AI systems, as the Udacity salespeople did, rather than being replaced by them. Self-driving cars, for instance, are not yet able to navigate all situations on their own, so car manufacturer Nissan is developing a human-powered solution. If one of its autonomous cars encounters a situation it doesn't understand, such as roadworks or a traffic accident, it will contact a remote command centre where a human 'mobility manager' can take control until the car has passed the trouble spot. \u201cMachines think in a very different way, fundamentally, than humans do, and each has its strengths,\u201d says Pietro Michelucci, executive director of the Human Computation Institute in Ithaca, New York. \u201cSo there's a real natural marriage between machines and humans.\u201d \n               Will the gig economy increase worker exploitation? \n             Flexibility, variety and autonomy: these are the promises of the burgeoning gig economy, in which workers use online platforms to find small, short-term jobs. This sort of on-demand, digitally mediated gig work can take a variety of forms, from driving for the taxi service Uber to completing microtasks \u2014 including taking surveys, translating a few sentences of text or labelling an image \u2014 on a massive crowd-working platform such as Amazon Mechanical Turk. These digital platforms allow workers to complete tasks from anywhere, meaning they could remove some geographical barriers to getting good jobs. \u201cSomeone in Nairobi is no longer constrained by the local labour market,\u201d says digital geographer Mark Graham of the University of Oxford. Graham and his colleagues have spent several years studying the digital, on-demand economy in southeast Asia and sub-Saharan Africa. They have conducted face-to-face interviews with more than 150 gig workers in these regions, surveyed more than 500 people and analysed hundreds of thousands of transactions on online labour platforms. Their preliminary results show that these jobs do pay off for some gig workers; 68% of the survey respondents said that the work makes up an important part of their household income. And digital platforms provided jobs to a variety of people \u2014 including women who were primary caregivers and migrants without work permits \u2014 who said that their employment opportunities were otherwise limited. \u201cThere are some people who really thrive in this system,\u201d Graham says. \u201cBut it's not like that for everyone.\u201d There is a pronounced oversupply of labour in the gig economy, leading some workers to drop their rates below what they consider fair. Many also work long hours at high speeds and to tight deadlines. \u201cThey tend to have a very precarious existence, so they're worried about saying no to jobs that they do get,\u201d Graham says. \u201cWe talked to quite a few people who have done things like stay up for 48 hours straight, just working solidly in order to get their contracts done on time.\u201d Considerable geographical inequities remain. In a 2014 study 4 , Graham and several colleagues analysed more than 60,000 transactions on one major platform in March 2013. Most jobs, they found, were listed by employers in high-income countries and completed by workers in low- or middle-income countries (see 'The gigs are up'). But those who live close to where the jobs are still seem to have an advantage. They win a disproportionate share of jobs and earn significantly more \u2014 US$24.13 per hour, on average \u2014 than foreign workers, who earned $11.66 per hour for comparable work. And some low- and middle-income nations attracted many more jobs than others; India and the Philippines are the top two recipients in Graham's analysis. Practical concerns could explain some of these disparities. Language and time-zone differences might make some employers reluctant to hire foreign workers, and the history of outsourcing labour to India and the Philippines may have helped make workers there more attractive to employers. But discrimination, both conscious and unconscious, could play a part, too; Graham's team found task listings explicitly stating that people from certain countries need not apply. \u201cEven though these technologies have been able to connect different parts of the world, they have not been able to bridge these kinds of differences as much as we hoped,\u201d says Mohammad Amir Anwar, a researcher who works with Graham. Another large ethnographic study of gig workers is beginning to reveal more about how this work gets done. It also provides some clues about what workers need to succeed. Between 2013 and 2015, two senior researchers at Microsoft Research \u2014 anthropologist Mary Gray in Cambridge, Massachusetts, and computational social scientist Siddharth Suri in New York City \u2014 surveyed roughly 2,000 gig workers in the United States and India and conducted longer interviews with nearly 200 of them. One of the first things they discovered was that, although gig workers are often portrayed as independent, autonomous labourers, many of them were in fact communicating and collaborating with each other 5 . Workers helped each other to set up accounts and profiles, shared information about good employers and newly posted jobs, and provided technical and social support. Workers are making a deliberate effort to add human connections back into the system, Suri says, and they're doing it on their own time. \u201cSo they clearly must value it.\u201d In a more quantitative follow-up study 6 , in which they mapped the social connections among more than 10,000 Amazon Mechanical Turk workers, Gray, Suri and their colleagues found that this kind of collaboration can have real pay-offs. Workers who had connections to at least one other person on the platform had higher approval rates, were more likely to gain elite 'master' status, and found out about a new task more quickly than unconnected workers. For people to be productive, says Gray, \u201cit turns out that they really need to collaborate. They need each other.\u201d \n               Can the digital skills gap be closed? \n             For years, experts have been sounding the alarm about a looming shortage of digital skills. They have warned that there are too few trained workers to fill high-tech jobs, and that a lack of basic digital literacy could prevent workers in certain geographical regions or demographic groups from thriving in the digital economy. In response, various innovative programmes for boosting digital literacy and skills have sprung up worldwide. Research is now starting to provide some clues about what does and doesn't work \u2014 and about where skills training might fall short. There have been some documented successes. More than a decade ago, the US Defense Advanced Research Projects Agency began developing a personalized, interactive and adaptive 'digital tutor' system to train new recruits to the US Navy for jobs as information-systems technology (IT) technicians. Students would work with the tutor one-to-one, completing lessons on different topics and solving related problems. The system prioritized conceptual learning and reflection, regularly prompting students to review what they'd learnt. When the tutoring system judged that a student had mastered the material, it would move on to the next subject. In a 2014 review 7  of the programme, researchers at the Institute for Defense Analyses in Alexandria, Virginia, found that 12 recruits who completed the 16-week course outperformed graduates of conventional, classroom-based US Navy IT training that lasted more than twice as long. The 12 even did better than a group of senior naval IT technicians \u2014 who each had an average of nearly ten years' experience \u2014 on almost every measure. \u201cIf we can do that, why not do more of it?\u201d says Dexter Fletcher, who co-authored the review. \u201cWhy not begin to apply this seriously to workforce training?\u201d In a follow-up study 8 , Fletcher found that a slightly modified version of the digital tutor yielded similar results when it was used to train 100 military veterans for civilian jobs in IT. Within six months of completing the programme, 97% of the veterans who wanted IT jobs had landed them, earning an average annual salary roughly equal to that of someone with 3\u20135 years of experience in the field. Numerous other strategies have been promoted to improve digital skills and employment, including  massive open online courses  (MOOCs) \u2014 university-level classes that are delivered over the Internet \u2014 and coding bootcamps, which are intensive, short-term training courses that teach the basics of computer programming. In a 2016 analysis 9  of 1,400 MOOC users in Colombia, the Philippines and South Africa, researchers determined that 80% of students were from low- or middle-income backgrounds and that 41% had only basic computer skills. More than half of the students (56%) were female, and computer science was the most popular MOOC topic. \u201cWomen are actually engaging in MOOCs in areas where they are underrepresented,\u201d says Maria Garrido, a co-author of the report at the University of Washington's Information School (see 'Back in the classroom'). But the quality of these programmes can vary enormously, and few have been rigorously evaluated. Coding bootcamps can be expensive, require a significant time investment and are located primarily in technology corridors and urban settings. And achievement gaps remain; in a 2015 study 10  of more than 67,000 MOOC students, two Stanford researchers found that female students and students of both genders from Africa, Asia and Latin America were less likely to reach certain course milestones \u2014 such as watching more than 50% of the lectures \u2014 and earned lower grades than male students and MOOC students from North America, Europe and Oceania. Even those who complete digital-skills courses can still face a variety of barriers to employment. When researchers interviewed students in a Kenyan IT programme at Strathmore University in Nairobi in 2004, some of the students said that they were worried about graduating into a local economy that didn't appreciate their expertise or have jobs in which they could put it to use 11 . \u201cAnd this was especially true for the women,\u201d says Lynette Yarger, an information scientist at Pennsylvania State University in University Park, who was involved in the research. As one student put it: \u201cBecause I am a woman, employers may not think that they should give me a job working in IT, so I may never fully get to use all that I have learned to do, work that I want to do.\u201d One thing the research is already making clear is that even well-designed training programmes might not be sufficient to ensure success in the world of digital work. \u201cThe fact that you have better skills and know how to use a computer doesn't necessarily mean that you automatically can get a good job,\u201d Garrido says. \u201cDigital skills are an important piece of the puzzle, but they're not enough.\u201d \n                     The future of work 2017-Oct-18 \n                   \n                     Science must examine the future of work 2017-Oct-18 \n                   \n                     The second Renaissance 2017-Oct-18 \n                   \n                     Lessons from history for the future of work 2017-Oct-18 \n                   \n                     Flexible working: Science in the gig economy 2017-Oct-18 \n                   \n                     Reboot for the AI revolution 2017-Oct-17 \n                   \n                     Track how technology is transforming work 2017-Apr-13 \n                   \n                     Hard work, little reward: Nature readers reveal working hours and research challenges 2016-Nov-04 \n                   \n                     Online learning: Campus 2.0 2013-Mar-13 \n                   Reprints and Permissions"},
{"file_id": "550026a", "url": "https://www.nature.com/articles/550026a", "year": 2017, "authors": [{"name": "Mark Peplow"}], "parsed_as_year": "2006_or_before", "body": "As shale-gas compounds flood the market, chemists are working out the best ways to convert them into the ingredients of modern life. As the  Ineos Intrepid  cruised slowly through the sapphire waters of Norway's Frierfjord, chaperone tugboats sprayed jets into the sky to herald her arrival. In giant refrigerated tanks below decks, the ship carried 27,500 cubic metres of liquid ethane \u2014 enough to fill 11 Olympic swimming pools.  Intrepid  also brought a message, painted in giant capital letters along her side: \u201cSHALE GAS FOR PROGRESS\u201d. The vessel's arrival in March 2016 brought the first ever shipment of shale gas from the United States to Europe \u2014 and marked the start of a burgeoning business. More of these 180-metre-long 'Dragon'-class vessels have followed in her wake, forming a 'virtual pipeline' for ethane across the Atlantic Ocean. This gas, which is extracted from the ground through the hydraulic fracturing of shale deposits, isn't destined to fuel power stations or domestic stoves. Instead, it will be transformed into the chemical building blocks needed to make a panoply of products, including plastics, clothes, adhesives and medicines. Intrepid 's voyage is a striking demonstration of how cheap US shale gas is reshaping the chemical industry and changing the origin of countless manufactured objects. For decades, the industry's raw ingredients have mostly come from crude oil. Chemical plants break down long hydrocarbon molecules in crude to produce a smorgasbord of smaller molecules, such as ethene, propene and benzene \u2014 all important precursors to polymers. But shale gas, which is composed mainly of methane, ethane and propane, is turning that pathway on its head. The abundance of the gas has slashed the costs of these molecules. As a result, some are now usurping large hydrocarbons as the preferred starting point for industrial synthesis. This shift from oil to gas brings enormous opportunities. According to the American Chemistry Council, a trade group based in Washington DC, the shale boom has attracted about US$160 billion in investment from the US chemical industry since 2011, and will help to create half a million jobs in plastics manufacturing over the coming decade 1 . But it also poses huge challenges. Some of the main techniques that are used to turn the components of shale gas into more valuable compounds \u2014 processes generally known as upgrading \u2014 are decades-old, dirty and energy-intensive. And they rarely produce the same mix of chemicals as conventional oil-based routes, which means that some relatively minor, yet valuable, chemicals such as butadiene, an ingredient of synthetic rubber, are becoming scarcer. These challenges are driving an intensive research effort, spanning industry and academia, to develop catalysts and reactors that can transmute small hydrocarbons in cleaner, cheaper and more efficient ways. Translating that research into commercial production will depend on the finely balanced economics of a changeable market. It will also require a reliable supply of gas. The US Energy Information Administration predicts that natural-gas extraction in the United States will continue to grow until at least 2040, but that might be too optimistic (see  Nature   516 , 28\u201330; 2014 ). Meanwhile,  concerns that fracking can contaminate groundwater  \u2014 along with the broader climate implications of extracting fossil fuels \u2014 continue to dog the technology. If the glut does persist, however, it could usher in technologies that would form the foundations of a much more sustainable chemical industry. \u201cWe could totally redesign our chemical plants,\u201d says Bert Weckhuysen, a chemist at Utrecht University in the Netherlands. \n               The ethane revolution \n             Shale gas is extracted from kilometres below ground, and typically contains about 70\u201395% methane, less than 15% ethane and less than 5% propane. After traces of oil, water and other impurities are cleaned out, the gas is chilled so that ethane and propane can be separated in liquid form, leaving methane behind. Although ethane makes up a small proportion of shale gas, it has so far had the biggest impact on the chemical industry. That's because chemists can easily use it to make ethene, also known as ethylene. Ethene is used to make various types of polyethylene and the precursors to other plastics, such as polyvinyl chloride (PVC) and polystyrene. So voracious is the world's appetite for these plastics that the chemical industry produces roughly 150 million tonnes of ethene every year, more than any other chemical building block. Most processes in the chemical industry use catalysts. But ethene can be produced simply by steam cracking ethane or larger hydrocarbons. First developed in the 1920s, steam cracking is a blunt, energy-intensive process that requires little more than water and 850 \u00b0C temperatures. \u201cYou basically just heat the snot out of it,\u201d says Jeffrey Plotkin, an industry analyst at IHS Markit in New York City. \u201cThe heart and soul of the thing is this gigantic furnace, that's where all the chemistry happens.\u201d The boom in shale-gas-derived ethane has driven the chemical industry to invest nearly $45 billion in extra steam-cracking capacity 2 . But the transition to this feedstock is also creating a headache. When steam crackers are fed with mixtures of long hydrocarbons from crude oil, they make an array of useful by-products. But when they are supplied with ethane, the output is almost entirely ethene. \u201cSo there is a shortage of other building blocks,\u201d says Weckhuysen. One of those building blocks is propene, arguably the second most important product of the chemical industry after ethene. Propene is turned into polypropylene, a plastic used in packaging and textiles, along with other polymer ingredients such as acrylic acid. But by  one estimate , propene production by US steam crackers dropped by almost half between 2005 and 2014, even as global demand rose (see 'Dwindling supply'). To combat the shortfall, the industry is rolling out alternative ways to make propene. One of the leading routes starts with the shale-gas component propane. A combination of heat and a catalyst to remove two hydrogen atoms can be used to turn it into propene. The conversion is becoming more profitable: more than 20 of these propane-dehydrogenation units are already operating worldwide, and at least 40 more have been ordered since 2011. But Weckhuysen says that there is much scope to improve the process, which tends to chew up catalysts quickly, requires a time-consuming and costly catalyst-regeneration step, and can use harsh reagents. \n               The methane question \n             Although ethane and propane are already making waves as commercial feedstocks, the big prize for chemists is to upgrade the most abundant component of shale gas: methane. Most of the world's methane is currently burnt as fuel, its lowest-value application. The gas can also be used as a chemical feedstock, but it contains strong carbon\u2013hydrogen bonds that are difficult to break in a controlled way. When methane is converted into other molecules, it is done mainly through an inefficient sledgehammer of a process called steam reforming. First commercialized in the 1930s, this involves smashing methane and water together at up to 1,100 \u00b0C, over a metal catalyst. It produces an extremely useful mixture of carbon monoxide and hydrogen called syngas \u2014 and also emits several hundred million tonnes of carbon dioxide per year, accounting for roughly 3% of all industrial emissions 3 . Syngas is the world's principal source of hydrogen, much of which goes to make the ammonia in fertilizer. Syngas can also be used to produce longer hydrocarbons, such as basic components of diesel and waxes. Such upgrading is typically done through a technique called the Fisher\u2013Tropsch (FT) process, which uses cobalt or iron catalysts and heat to create daisy-chains of carbon atoms. FT was developed in Germany in the 1920s to make petrol and a wide range of other hydrocarbons from syngas derived from coal. Producing transport fuels in this way is generally more expensive than refining oil. There are just six large-scale FT plants in the world, made economical only thanks to their proximity to huge coal or gas fields and the mind-boggling scale of the plants themselves: the world's largest, in Qatar, cost $19 billion to build and munches through 45 million cubic metres of methane every day, on a par with the natural-gas consumption of Belgium. But the shale boom has prompted chemical engineers to take a fresh look at the FT process. Shale-gas wells typically don't produce enough gas to support a conventional FT plant, so research teams and companies have been developing smaller reactors that can process modest gas flows. One of those is Velocys, based in Houston, Texas, which developed a 5-metre-long reactor that can convert syngas into substances such as naphtha, diesel and wax. Its reactor technology is being used in Oklahoma City in the first commercial mini-FT plant in the United States. The plant, which is owned by ENVIA Energy, started production earlier this year. Temperature control is a big challenge for the FT process: the reaction kicks in at about 180 \u00b0C, then generates huge amounts of heat. If not carefully controlled, it will run away with itself, turning carbon atoms into useless soot. To address this, Velocys's reactor contains corrugated layers of channels that are alternately stuffed with catalyst or filled with water. This keeps the reaction running at a steady 200 \u00b0C, so that the reactor can use an efficient catalyst without risking a runaway reaction. \u201cIt allows you to pack a lot of reaction in a very small space,\u201d says Neville Hargreaves, business-development director for Velocys in Oxford, UK. The reactor in Oklahoma City pulls methane from a landfill site, an activity that comes with renewable-energy credits. But Hargreaves thinks companies could ultimately profit by tapping remote and relatively small natural-gas reserves that are unlikely to get a pipeline. Another potential target is unwanted gas from oil wells, which is often simply burnt off. Such 'flaring' puts about 350 million tonnes of CO 2  into the atmosphere every year. According to the World Bank, it carries enough energy to meet Africa's entire current electricity requirements. \n               The direct route \n             The high temperatures involved in producing syngas will always make it a costly way to create complex chemicals \u2014 as well as a major source of CO 2  emissions. Researchers have spent decades looking for ways to convert methane directly to methanol or other products, cutting syngas out of the route altogether. The shale boom has given this effort fresh urgency, along with a burst of investment in research and development in both academia and industry. Turning methane into methanol \u2014 itself a key precursor to a wide range of other compounds \u2014 involves adding only a single oxygen atom. But first, one of methane's strong carbon\u2013hydrogen bonds must be broken, and the high temperatures or strong oxidants needed to do that can set the molecule on a one-way journey down a thermodynamic roller coaster with a messy end. Methanol sits on a brief crest about halfway down, but it is all too easy to race downhill as the reaction goes too far, producing a mixture of other molecules, including formaldehyde, formic acid or carbon monoxide. In 2005, however, a team led by Robert Schoonheydt at the University of Leuven in Belgium, found 4  that copper seeded onto a porous material called a zeolite could unite oxygen and methane to make methanol at less than 200 \u00b0C. Crucially, the methanol became trapped in the zeolite's pores, preventing further reactions. But extracting methanol from the pores and reactivating the catalyst would have proved expensive and impracticable in a commercial setting. Since then, research groups have developed a range of copper\u2013zeolyte catalysts that are more industry-friendly. Others have focused on completely redesigning chemical reactors. The European Union-funded project  Adaptable Reactors for Resource- and Energy-Efficient Methane Valorisation , for example, aims to build small reactors that use renewable electricity, rather than heat generated from fossil fuels, to turn methane into compounds such as ethene and methanol. One approach uses microwaves to generate intense hotspots in the catalyst, lowering the heating requirements for the incoming gas. Another approach to direct methane upgrading aims to couple pairs of the molecule together to make ethene. Since 2015, Siluria Technologies, a start-up in San Francisco, California, has been running a demonstration plant for this process in La Porte, Texas. It relies on a catalyst made of metal-oxide nanowires that collectively offer a surface area of about 200 square metres per gram of catalyst, hundreds of times more than a bulk catalyst could offer. The company builds its catalysts in a unique way, based on a technique 5  developed by co-founder Angela Belcher, a materials scientist at the Massachusetts Institute of Technology in Cambridge. First, viruses are genetically engineered to express proteins that bind to dissolved metal ions. The ions form orderly arrangements as they stick to the surface of the virus. When the biological template is burned away, it leaves behind a highly stable, crystalline nanowire. Rahul Iyer, Siluria's vice-president of corporate development, says that the process is cost-competitive with steam cracking ethane, and produces far fewer CO 2  emissions than steam reforming methane. Siluria has already licensed the technology to some chemical companies, and expects the first commercial facilities to be operating in 2019. Plotkin says that Siluria is currently in the lead in the race to commercialize direct methane upgrading, and is backed by multimillion-dollar investments from big players in the industry. \u201cPeople are keeping a watchful eye on it,\u201d he says. \n               Gas that's greener \n             The shale-gas boom is credited with spurring a major renaissance in the US chemical industry, which has invested heavily in chemical plants and other infrastructure, as well as research and development. Enthusiasm for shale-gas upgrading has fostered major collaborations between academia and industry. Translating laboratory results into commercial production is an ongoing challenge, although the trend towards small, modular reactors is helping to make it less daunting. The chemical industry is notoriously conservative: if a process succeeds in the lab but fails at commercial scale, tonnes of catalyst can be wasted and a plant shut down for months. \u201cIndustry will not take the risk unless they are sure it will work,\u201d says Weckhuysen. Despite these challenges, he is optimistic that gas upgrading could have a huge impact \u2014 not only on the chemical industry's processes, but also on its environmental footprint. Some of the reactor technologies being developed to feed on shale gas could be adapted to use bio-based feedstocks, such as methane from landfills, as Velocys has found. Meanwhile, shortages in some compounds caused by the shift to shale gas could improve the economic case for starting with ethanol from crops, or lignin from wood 6 . There has already been movement along these lines. In 2013, for example, French tyre-maker Michelin and partners launched a  \u20ac52-million (US$61-million) project  to make butadiene from bioethanol. But for now, US shale ethane continues its relentless march around the world. More chemical companies are commissioning ships to transport the gas to destinations in Europe, Brazil and India. By 2022, according to one estimate, about 8 million tonnes of ethane will flow through these virtual pipelines each year. They will carry this revolution in the US chemical industry to the rest of the globe \u2014 both its challenges and its opportunities. \n                     Can fracking power Europe? 2016-Mar-01 \n                   \n                     Natural gas: The fracking fallacy 2014-Dec-03 \n                   \n                     Wasted energy 2013-Mar-19 \n                   \n                     Energy: A reality check on the shale revolution 2013-Feb-20 \n                   \n                     Ineos \n                   \n                     IHS Markit \n                   \n                     Velocys \n                   \n                     Adaptable Reactors for Resource- and Energy-Efficient Methane Valorisation \n                   \n                     Siluria \n                   \n                     Dutch Chemical Building Blocks Consortium \n                   Reprints and Permissions"},
{"file_id": "550444a", "url": "https://www.nature.com/articles/550444a", "year": 2017, "authors": [{"name": "Amy Maxmen"}], "parsed_as_year": "2006_or_before", "body": "Aid organizations have been piloting a nimble approach to cut through the fog of war. Shadows shroud Issam Salim's face as he recounts the operations he's performed. Yesterday, he tended to fractures, mangled limbs and intestinal injuries caused by an explosion from an unknown source. \u201cThe situation was very tense,\u201d he says. Today, there have been no war-wounded patients, so he saw people with bladder stones and hernias instead. Salim is deputy director of a hospital in southern Syria, and he's talking to an Iraqi surgeon, Ghassan Aziz, through a flickering Skype video call. Aziz is not far away \u2014 just two hours south by car, in Jordan's capital, Amman. It is from here that the organization Aziz works for, M\u00e9decins Sans Fronti\u00e8res (MSF), has been providing medical aid to clinics in southern Syria during a conflict that has become one of the world's worst ongoing humanitarian crises. But Aziz and his colleagues dare not get much closer. After 13 MSF staff members were kidnapped in January 2014, the organization, also known as Doctors without Borders, pulled its international staff out of the country. Text messages and calls such as the one with Salim provide a glimpse of what is going on, but it is hardly enough to let MSF staff predict what Syrian doctors and nurses will need most to help their communities. An increase in severe burns might mean that C-4 plastic explosives are in heavy rotation, for example, and therefore medics will require extra antibiotics, intravenous lines and surgical equipment, because they won't have time to sterilize between operations. Or an increase in kidney failures could mean that people with diabetes have lost access to regular care. But the fog of war makes tracking such trends next to impossible. Whenever war, hurricanes or other disasters ravage part of the globe, one of the biggest problems for aid organizations is a lack of reliable data. People die because front-line responders don't have the information they need to act efficiently. Doctors and epidemiologists plod along with paper surveys and rigid databases in crisis situations, watching with envy as tech companies expertly mine big data for comparatively mundane purposes. Three years ago, one frustrated first-responder decided to do something about it. The result is an innovative piece of software called the Dharma Platform, which almost anyone can use to rapidly collect information and share, analyse and visualize it so that they can act quickly. And although public-health veterans tend to be sceptical of technological fixes, Dharma is winning fans. MSF and other organizations now use it in 22 countries. And so far, the Rise Fund, a 'global impact fund' whose board boasts U2 lead singer Bono, has invested US$14.3 million in the company behind it. \u201cI think Dharma is special because it has been developed by people who have worked in these chaotic situations,\u201d says Jeremy Farrar, director of biomedical-funding charity the Wellcome Trust in London, \u201cand it's been road-tested and improved in the midst of reality.\u201d Now, the ultimate trial is in Syria: Salim, whose name has been changed in this story to protect him, started entering patient records into the Dharma Platform in March, and he is looking at health trends even as he shares his data securely with MSF staff in Amman. It's too soon to say that Dharma has transformed his hospital. And some aid organizations and governments may be reluctant to adopt it. But Aziz, who has deployed Dharma in Iraq, Syria, Jordan and Turkey, is confident that it will usher in a wave of platforms that accelerate evidence-based responses in emergencies, or even in health care generally. \u201cThis is like the first version of the iPhone or Yahoo! Messenger,\u201d he says. \u201cMaybe something better will come along, but this is the direction we're going in.\u201d \n               Born of frustration \n             Jesse Berns dreamt up Dharma after years of first-hand experience with the injured and ill, first as a helicopter paramedic, and then as a field epidemiologist embedded in some of the world's worst disaster zones. \u201cI've worked in pretty much every conflict since 2006,\u201d she says. She became disheartened by the inability to base decisions on data. In 2013, for example, she was surveying the health condition of refugees at the Iraq\u2013Syria border with the World Health Organization. She entered her own hand-written data into an Excel spreadsheet, merged the information with other data, analysed it and generated a report. But the process took five months, and at that point, the results were too old to act on. In 2015, she worked with MSF during the Ebola crisis in West Africa as the group tried to find a way to track and transmit data on the vital signs of dying patients without a Wi-Fi connection. Berns watched as incredible sums of money were spent. But the outbreak was over before a solution materialized. She felt broken. \u201cI got burned out after seeing colossal wastes of money and time,\u201d she says. \u201cI'd come home and have Uber and Slack, but in the field I had paper and Excel and it was just the ultimate shitshow for data.\u201d Berns complained to her friend Michael Roytman, a data scientist working in Chicago, Illinois, and California's Silicon Valley. Roytman suggested that the two join forces and create software to allow an emergency responder to fill the gap in a flash, without having to ask Excel experts, information-technology departments or consultants for help. The platform also had to work offline, store data securely in the cloud and be able to pass information through Bluetooth connections in case bombs, power failures or computer viruses interrupted service. So the pair started a company based in Washington DC to build what was needed in the field. When they are asked to describe Dharma, Berns and Roytman struggle because there aren't yet many things like it. \u201cIt's not a database,\u201d says Roytman. \u201cIt's a platform or framework that lets people with no technical background create the tool they need.\u201d An early iteration of Dharma caught the attention of Pablo Marco, the head of MSF's Middle East operations, based in Amman, in 2015. His team had been struggling with the complexities of health in the region, which presented challenges MSF was unaccustomed to. For refugees in Africa, he says, the approach is generally straightforward because needs are fairly uniform: provide clean water, food, shelter, antibiotics and vaccines. \u201cWe have a checklist,\u201d Marco says, \u201cso we can act fast, fast, fast,\u201d But refugees from Iraq and Syria have a range of different requirements. They might be managing depression, hypertension or diabetes instead of malnutrition. And their needs are in flux as they move and lose assets, and as access to medicine comes and goes. Marco wanted to see whether new technology could provide faster feedback. So he asked Berns to meet Aziz, who was preparing to survey some 200,000 Iraqis who had fled south from the Islamist terrorist group ISIS in Mosul. Having completed his medical residency in Baghdad amid sectarian violence in 2007, Aziz understood the depth of the challenge before him. Acute traumas would be obvious, but not festering chronic maladies. He readied himself for the undertaking: \u201cYou need to train a large number of people to go out to households and fill out paper forms. Then it takes tonnes of time to transfer those forms into Excel, then transfer the data to an analyst and three months go by before they send back findings.\u201d Aziz, a programme manager at MSF's Center for the Advancement of Humanitarian Medicine in Amman, resembles a Silicon Valley techie with his backpack and worn T-shirt, but he has no computer-science background. Sceptical, but willing to give Dharma a try, he downloaded it onto a tablet and built a form with 145 questions. The survey was designed to move fast, asking only questions made relevant by previous responses. Each person would answer a total of about 25. Women of child-bearing age, for example, were asked whether they were pregnant, and children were asked if they had had diarrhoea or asthma attacks in the past two weeks. Iraqi medical students asking the questions sped through the surveys. By day 5, the students had collected information from 6,455 people. Then Aziz did something he never could have done before. He merged the information from their devices onto his own and he began to interrogate the data, simply by typing in questions: for example, who identifies as head of household (husband, wife, son-in-law, and so on), and what are the chronic illnesses among these household heads? The answers came back instantly, in graph form. \u201cEven though I had been up since 5 a.m. that day, I stayed awake until 4 a.m. since it was so interesting,\u201d he says. In one view, a pie chart revealed that people of various ages and backgrounds were complaining of skin irritation. Within minutes, it was obvious that the burrowing mites that cause scabies had infested mosques, motels and flats in which refugees were living. Aziz shared the data with MSF and in less than six weeks the organization was treating people with scabies and their contacts, and spraying shelters to eradicate the pests. A follow-up survey showed that the rate of scabies had dropped from 72% to 23%. Without Dharma, Aziz says, it would have taken several months to realize that something so easily fixed needed attention. He was sold, and went on to use Dharma to survey refugee health in Turkey and Syria. All the while, he kept in touch with Berns, who tweaked the product in response to feedback. The same evolution occurred as the World Health Organization applied Dharma in Iraq, and as the Paris-based aid agency M\u00e9decins du Monde piloted it in Lebanon to assess the mental health of Syrian refugees. Preliminary data from that test suggest that refugee women with children have a lower incidence of suicidal thoughts than those without. Now the group is exploring the connection in a larger survey. As Dharma's use has spread, public-health experts have taken notice. In April, Farrar told Larry Brilliant to check it out. Brilliant is an epidemiologist and former Google executive who now chairs the Skoll Global Threats Fund, a group in San Francisco, California, that identifies solutions to problems imperilling humanity. He was flabbergasted by how simple it was to use. \u201cI am pitched lots and lots of systems that mechanize emergency and public-health responses, but they take so damn long to learn,\u201d he says. \u201cThat is not true for Dharma.\u201d In July, he joined the company's board. \n               Broken records \n             In Syria, MSF has been anxious to get access to patients' medical records, which would provide a long-term view of how people are faring and what support Syrian hospitals need. But that has been next to impossible because hospitals have been targeted by the Syrian regime and terrorist groups. Since March 2011, the non-profit group Physicians for Human Rights in New York City has documented 826 deaths of health-care workers in Syria from targeted bombs, assassinations and torture \u2014 more than 90% by the government. Although MSF officially withdrew from the country in 2014, it had avoided some dangerous regions since 2011. One afternoon in 2012, Khalid Ahmad, a tropical-medicine doctor with the charity, got an idea about how the group could provide aid in areas that it was unable to reach itself. He was at an MSF office in Turkey, just across the northern Syrian border, when a young Syrian couple approached him. They showed him videos on their phones of people mangled under rubble. \u201cThey were finding the wounded and bringing them to clandestine hospitals,\u201d Ahmad says. \u201cThey weren't even doctors, but they were organized, and I was so touched by their commitment.\u201d He gave the couple first-aid kits and training on how to stop bleeding and move the wounded. Then he set out to find doctors said to be operating out of basements, in living rooms and under trees. Underground practices were \u201cmushrooming up everywhere\u201d, he recalls. In 2015, MSF forged a connection with a hospital serving a large population in southern Syria \u2014 the one where Salim now works. At first, MSF asked hospital employees to enter patient data into an electronic database that the organization has long deployed around the world. But the Syrians didn't use it. They did not work for MSF, and they had little to gain from entering data into an unfamiliar system. Trying to get meaningful analyses out of it would take training and time, which the overwhelmed hospital staff didn't have. Plus, MSF's internal system is rigid. Requests for changes have to go through technology departments in European cities, a fact that stood out as a bottleneck. Early this year, Aziz got the green light to try Dharma at the hospital. He designed questionnaires on the platform that mimicked the format of the hand-written record books that hospital staff were accustomed to keeping. Two tablets with the program arrived at the hospital on 1 March, and every day since then, hospital staff have transferred data from hard copies into the devices. Anyone with access to the system can use it to search for trends. For example, in April, Aziz noticed an unusually high number of infections among women who came for post-natal visits. Looking more closely, he saw that these women had not given birth at the hospital, so their infections probably came from stitches administered by midwives after slight rips during birth. \u201cThat means the midwife is doing this without sterile tools or in non-sterile conditions,\u201d he says. \u201cBy knowing this, we can start to think about how to fix it.\u201d As of 15 October, the hospital has shared details from 29,469 patient visits. It's an exponential boost in information. \u201cThis is the only eye we have,\u201d says Anja Braune, project coordinator for MSF's south Syria operation. \u201cThis is the only way we can try to forecast the coming period.\u201d Still, Braune says that Dharma has not suddenly solved an extraordinarily difficult situation. In 2016 alone, MSF-supported facilities in the country were bombed or shelled on 71 separate occasions. \n               Data diaspora \n             But the data gap in the Syrian crisis extends outside the country's borders. Since 2011, about 5.3 million Syrians have fled the country, 92% of them to Turkey, Lebanon and Jordan (see 'Driven to data'). Although they are no longer in imminent danger, many continue to deteriorate from chronic health conditions, despite medical care. To understand how to help them, doctors need information. One sweltering morning in July, Mohammed Manasrah carries a device loaded with Dharma to the houses of his bed-bound patients in Ar Ramtha \u2014 a northern district of Jordan where roughly 68,000 Syrian refugees have settled in concrete flats. Manasrah is a physician at an MSF hospital in Ar Ramtha specializing in non-communicable disease. Forms created on Dharma can be easily amended, and Manasrah inserts variables that might help him discover patterns. \u201cI want to see if some medication we are giving them correlates with depression or if that's tied to refugee status,\u201d he says. \u201cI want to see if we can convince women who cannot walk in the street to exercise in their homes, and to see if this leads to better medical outcomes.\u201d Some answers may lie in patient records maintained by the hospital, but analysing the information requires more expertise and time than he has. On Dharma, he could search for correlations in minutes. Doctors and other crisis responders have never had access to technology like this before: something that lets them design the tool they need for a job, and that puts analytics at their fingertips. The hope is that this will make them want to participate further and collect more information. That kind of buy-in is important, says Matthew Gee, a data scientist at the University of Chicago. \u201cWhether you are a clinician trying to treat an illness or an academic wanting to understand the propagation of an infection, you rely on the data collector,\u201d Gee says. The same data that help crisis responders react day-to-day can later be used by academics doing long-term research. Dharma makes it technically easier to share data, too. If a sudden disaster occurs, information obtained on the platform can (pending permission) be passed on to researchers more easily than before. Berns and Roytman have designed the platform to adhere to the security and formatting standards that many scientific-review boards and government agencies recommend. That's a key reason that Dharma is being piloted by scientists monitoring Middle East respiratory syndrome, or MERS, as part of the International Severe Acute Respiratory and Emerging Infection Consortium. In this way, researchers who arrive at an outbreak much later than first-responders can make use of information gathered at its unpredictable start. Still, Dharma could fail, like most start-ups. At the moment, many aid groups and governments prefer open-source tools, such as Open Data Kit, says Dykki Settle, director of digital health at PATH, a global-health organization based in Seattle, Washington. Settle explains that cost is not the reason: although open source means that the raw software is free, consultants still charge fees to maintain and modify it, or to link it with other systems for storage or analytics. Rather, open source has some of the appeal of a vintage car: tinkering is an expectation. Someone who can program computers can alter the code, and weave one component with another. But as with vintage cars, that's unlikely to be the most reliable approach in a crisis. \u201cIn an emergency, you may not have the time and money to invest in the extra labour that open source requires,\u201d Settle says. Berns argues that Dharma is just as useful for long-term health management as for emergencies. And although its code is not accessible, she says, the ease of customization has allowed humanitarian groups to assess data ranging from medical needs to housing damage in Hurricane Harvey. These attributes have caught the attention of powerful players in global health. The US Centers for Disease Control and Prevention (CDC) is planning to pilot Dharma and several other new or updated systems for data management in emergencies. Richard Garfield, an epidemiologist involved with the effort, says that the agency plans to publish a sort of \u201cconsumer report\u201d listing the pros and cons of each. New technology and analytics, he hopes, will force aid agencies to base their actions on evidence. \u201cEveryone gets by with good intentions, and that's a serious frustration for those of us who are really concerned about improving people's lives,\u201d Garfield says. With or without Dharma, technological barriers to information exchange are falling. Still, data sharing may remain an aspirational ideal. Organizations often keep information to themselves to save face when their programmes don't deliver; researchers keep it private because they want credit; and many governments like to control access. In this respect, says Farrar, \u201cthe technical side is not the challenge; it is a political one\u201d. Despite being surrounded by war, Salim pushes for data sharing as well. He would like scientists and doctors around the world to learn the details of his cases. \u201cMany websites talk about the war in Syria, but it's very general,\u201d he says. \u201cWe need more specialized people talking about our situation so that it can improve \u2014 because the situation is bad.\u201d For example, he says, what types of nerve damage are caused by chemical weapons and how do you treat those affected? Salim admits that he often considers fleeing Syria, but feels responsible because he knows too well all he leaves behind. \u201cWhen it's the worst,\u201d he says, \u201cI weigh the risks and the benefits of the services I provide.\u201d And then he decides to stay. At the very least, the world could pay attention. Travel for this story was paid for by the Pulitzer Center on Crisis Reporting in Washington DC. \n                     Chronic diseases spike in Middle East as conflicts rage 2017-Aug-04 \n                   \n                     What the numbers say about refugees 2017-Mar-01 \n                   \n                     The mental-health crisis among migrants 2016-Oct-10 \n                   \n                     Busting the billion-dollar myth: how to slash the cost of drug development 2016-Aug-24 \n                   \n                     Conflict resolution: Wars without end 2015-Mar-11 \n                   \n                     Nature special: Human migration \n                   \n                     Dharma Platform \n                   Reprints and Permissions"},
{"file_id": "550172a", "url": "https://www.nature.com/articles/550172a", "year": 2017, "authors": [{"name": "Jeff Tollefson"}], "parsed_as_year": "2006_or_before", "body": "After decades of delays, a challenging clean-up project is gaining ground. There's a building boom at the Hanford Site, a once-secret complex on the windswept plains of southeastern Washington state. Construction crews are working to finish a 27-metre-tall concrete structure there by June. If all goes well, the facility will finally enable the US Department of Energy (DOE) to begin treating the toxic, radioactive waste that accumulated at the site for more than 40 years, starting during the Second World War. Decades after the site stopped producing plutonium for nuclear weapons, the legacy of Hanford's activities is still causing trouble. Just this year, a tunnel holding railway carriages  full of radioactive material collapsed . Separately, at least a dozen employees who were tearing down a contaminated building  reportedly tested positive for plutonium inhalation . But the site's biggest challenge lies underground, in 177 carbon-steel tanks. Together, these buried containers hold more than 200 million litres of highly hazardous liquids and peanut-buttery sludge \u2014 enough to fill 80 Olympic-size swimming pools. More than one-third of the tanks have leaked, contaminating groundwater with radioactive and chemical waste. In a 1989 legal agreement with the state of Washington and the US Environmental Protection Agency, the DOE committed to immobilizing the most dangerous waste in sturdy glass logs through a process called vitrification. Several years later, the agency agreed to vitrify other tank waste as well. All told, the process is expected to generate tens of thousands of logs, each weighing multiple tonnes. Those containing high-level waste would be shipped to a permanent storage facility; the rest could be stored on site. But the effort has been plagued by cost overruns, delays and safety concerns. Although the DOE has spent roughly US$20 billion on the tank problem since 1997, no waste has been vitrified. Four years ago, the agency hit reset. Rather than making a single vitrification plant, it split the project in two. One plant \u2014 the building now under construction \u2014 would begin vitrifying the less-hazardous, 'low-activity' liquid in the tanks. A bigger, more-complex plant to process the high-level sludge would follow once researchers resolved some thorny safety questions. On both fronts, there have been signs of progress. This year, the DOE reported that it had resolved crucial questions related to treating the high-level waste. And a laboratory needed for real-time analysis of the low-level waste is nearing completion. If work continues as planned, the site could crank out its first glass logs as early as 2022. Hanford's critics, accustomed to missed deadlines and management scandals, remain sceptical. But even officials with the state of Washington, which has battled the DOE in court for nearly three decades over clean-up goals and deadlines, are hopeful that efforts are now on track. \u201cThere's reason for optimism,\u201d says Suzanne Dahl, who oversees tank activities for the Washington Department of Ecology. Scientists have been studying vitrification since the 1950s, and a number of countries have used the process to stabilize nuclear waste, including France, India, Russia and the United Kingdom. The United States vitrifies waste at the DOE's Savannah River Site in South Carolina. But the size and complexity of the problem is on a different scale at Hanford. Established as part of the Manhattan Project during the Second World War, the Hanford Site delivered the plutonium that went into the first nuclear-weapon test and the bomb that was dropped on Nagasaki, Japan, in 1945. It went on to produce the bulk of the plutonium for the US nuclear arsenal. \u201cHanford is the whole history of nuclear development,\u201d says Ian Pegg, a physicist at the Catholic University of America in Washington DC, who works with the DOE on vitrification experiments. \n               Toxic brews \n             The ever-shifting suite of technologies used at the site produced uniquely toxic brews that include radioactive caesium, strontium, americium and residual plutonium; salts; heavy metals; and myriad industrial chemicals. The containers also hold other surprises. People \u201cthrew everything imaginable into those tanks\u201d, says Albert Kruger, a glass scientist with the DOE in Richland, Washington. His list includes contaminated gloves, planks of wood, rocks and tape measures. Once such detritus is removed, vitrification calls for the waste to be combined with ingredients that include silica and boron, then heated to nearly 1,150 \u00b0C. The molten mixture is next cooled in stainless-steel canisters to create large cylinders of borosilicate glass \u2014 the same material used in oven-safe glassware. The process is complicated by that fact that each tank contains a cocktail of chemicals and radionuclides that cannot be fully characterized until the waste is extracted. Some of those substances can weaken glass. Others, such as iodine, can't be readily trapped and must be removed. Hanford scientists will have to tailor glass recipes for each batch of waste \u2014 a bit like blending different vintages to produce a fine cognac. \u201cNobody will test the nose, and nobody will take a taste test, but it's an equivalent mechanism,\u201d Kruger says. Multiple contractors have worked on the Hanford project since 1989, including British Nuclear Fuels Limited, a UK-government-owned company that exported the technology it was using at the Sellafield nuclear-decommissioning complex. After price estimates rose, in 2000 the DOE hired construction and engineering giant Bechtel of San Francisco, California, as the primary contractor. At that time, the Hanford plant was expected to cost $4.3 billion and to begin making logs in 2007. But as engineers began working through the safety and technical details, the project ballooned in price and complexity. By 2012, senior officials \u2014 including a former DOE employee and two contractors who later filed whistle-blower complaints after being fired \u2014 were raising concerns. One was that hydrogen, which is generated when heat and radiation split water molecules, would build up in tanks and pipes, creating a risk of explosion. Another was that mixing vessels meant to keep heavy particles moving would not be powerful enough. Over time, enough residual plutonium could settle out to create a dangerous chain reaction. Then-DOE secretary Steven Chu assembled an expert panel to investigate. Ultimately, Bechtel was ordered to first construct a plant that would vitrify only liquid waste. The liquid represents 90% of the waste volume but just 10% of its radioactivity, and requires less processing than the high-level waste: it can be skimmed off, stripped of highly radioactive caesium and then sent directly to vitrification. \u201cIt makes sense,\u201d says David Kosson, a chemical engineer at Vanderbilt University in Nashville, Tennessee, who was on Chu's expert panel. If you have got to pick one place to start, he says, \u201cthe low-activity waste is not a bad choice\u201d. \n               Lingering questions \n             The high-level-waste facilities remain on hold, but the DOE and its contractors have spent years investigating the technical issues using computer models and prototypes.  In February, the agency announced it had resolved issues  related to hydrogen build-up and uncontrolled reactions. Scientists familiar with the effort says tests of a newly designed mixing vessel are nearing completion, apparently without any major hitches. The vessel is equipped with six 'pulse jet mixers' that pull waste in and out like turkey basters, to keep solids from settling. Researchers are also making progress on the glass recipes. Kruger and external scientists have shown that certain compositions can accommodate more waste than previously estimated, and so potentially save on costs. The number of glass logs produced in the high-level waste facility could drop from 18,000 to as few as 7,000, Kruger says. The low-level plant may need to make just 70,000 logs or so, instead of 145,000. But questions remain. A 2015 DOE report documented more than 500 vulnerabilities that could affect low-level plant operations \u2014 including some in the electrical and mechanical systems that would be used to handle radioactive materials. Tom Carpenter, executive director of the watchdog group Hanford Challenge, hopes the plant will work as advertised. But he is concerned that the DOE, its contractors and even the state of Washington are too eager to bring the facility online. \u201cEveryone is desperate to show progress,\u201d he says. \u201cI get that, but you can't paper over the safety issues.\u201d Senior DOE officials at Hanford declined to be interviewed for this story; a Bechtel spokesperson said the company has addressed the vast majority of concerns raised in the report and has submitted its responses to the DOE for verification. Not everyone is convinced that vitrification is the way to go. The DOE is bound by legal agreements and nuclear-waste regulations to pursue the process, but from a technical standpoint there are better options, says Jim Conca, a consultant and former director of an independent research centre that supports the Waste Isolation Pilot Plant (WIPP) outside Carlsbad, New Mexico, the nation's only operating deep geological repository. Hanford's high-level wastes are currently slated for disposal at Yucca Mountain, a long-stalled geological repository in Nevada. Water infiltration is a concern there, so the waste must be encased in glass to help ensure that it remains stable over thousands of years. But Conca says that the tank sludge is safe enough to simply be dried out and sent to WIPP \u2014 if regulations could be changed to allow it. Similarly, low-activity waste could be mixed with grout to create concrete-like material, which would be cheaper and, many believe, just as safe. \u201cDoes all of that waste technically need to be vitrified for environmental safety? Probably not,\u201d says Kosson. But in the end, Kosson believes that the DOE will press forward with the plan. Chu remains confident that vitrification can work, but says the DOE should be receptive to new science and shift course as needed. More generally, he says, the country has a long way to go in resolving questions about how \u2014 and where \u2014 it will dispose of all its nuclear waste. \u201cThis is a significant problem, and there has to be a lot of good science in figuring out a better path forward,\u201d he says. \u201cAlways keep your mind open.\u201d The price tag on Hanford's vitrification facilities now stands at $16.8 billion. Assuming that the latest timetable holds, the plant for high-level waste will open for business in the early 2030s, and operations will continue for decades. In the meantime, dangerous waste will remain underground, out of sight but not out of mind. \n                 Tweet \n                 Follow @NatureNews \n                 Follow @jefftollef \n               \n                     Science and innovation policies for Donald Trump 2016-Nov-15 \n                   \n                     US government seeks sites for nuclear-waste storage 2015-Mar-24 \n                   \n                     US seeks waste-research revival 2014-Mar-04 \n                   \n                     Battle of Yucca Mountain rages on 2011-May-17 \n                   \n                     Hanford Vitrification Plant \n                   \n                     Hanford Site \n                   \n                     Consortium for risk evaluation with stakeholder participation \n                   Reprints and Permissions"},
{"file_id": "550315a", "url": "https://www.nature.com/articles/550315a", "year": 2017, "authors": [], "parsed_as_year": "2006_or_before", "body": "Digital technologies are upending the workforce. The right research can tell us how. Robots did not write this sentence, or any other part of  Nature . But that could change. Dramatic shifts in labour are reshaping society, the environment and the political landscape. Consider this disorienting estimate from the World Economic Forum: 65% of children entering primary schools now will grow up to work in jobs that do not yet exist. This week,  Nature  asks: what light is research shedding on the future of work, and how will the changes affect scientists' working world? A  News Feature  explores which jobs are most at risk of being replaced by artificial intelligence and machine learning; whether a decentralized 'gig economy' will democratize work; and what programmes will best prepare workers. \u201cThere's a huge need, a huge opportunity, to study the changes,\u201d says economist Erik Brynjolfsson. And the scientific workforce is feeling these shifts. A  Careers Feature  reports on people doing research outside the traditional career path. \u201cI love the freedom,\u201d says Cecile Menard, an independent land-surface modeller in Edinburgh, UK, \u201cbut for other people, it may be too stressful.\u201d Important lessons can be drawn from the past. Economic historian Robert Allen  synthesizes three centuries of data  to see when and where the relationship between wages and productivity was most like today's \u2014 and finds that some regions are in uncharted waters.  These changes call for new socio-economic models  and a revolution in education, concludes historian Yuval Noah Harari. And economist Ian Goldin argues  that our era has more parallels with the Renaissance  than the Industrial Revolution. This time, he urges, \u201cknowledge and enquiry must find a way to conquer prejudice and ignorance\u201d. \n                     Science must examine the future of work 2017-Oct-18 \n                   \n                     The second Renaissance 2017-Oct-18 \n                   \n                     The shape of work to come 2017-Oct-18 \n                   \n                     Lessons from history for the future of work 2017-Oct-18 \n                   \n                     Flexible working: Science in the gig economy 2017-Oct-18 \n                   \n                     Reboot for the AI revolution 2017-Oct-17 \n                   \n                     Track how technology is transforming work 2017-Apr-13 \n                   \n                     Hard work, little reward: Nature readers reveal working hours and research challenges 2016-Nov-04 \n                   \n                     Online learning: Campus 2.0 2013-Mar-13 \n                   Reprints and Permissions"},
{"file_id": "551156a", "url": "https://www.nature.com/articles/551156a", "year": 2017, "authors": [{"name": "Daniel Cressey"}], "parsed_as_year": "2006_or_before", "body": "As regulators consider a ban on neonicotinoids, debate rages over the harm they cause to bees. Maj Rundl\u00f6f remembers the moment she changed her mind about neonicotinoids. In December 2013, in her office at Lund University in Sweden, she and postdoc Georg Andersson were peering at data from their latest study. It was designed to test what would happen to bees if they fed on crops treated with neonicotinoids \u2014 the world's most widely used insecticides. \u201cI didn't expect to see any effect at all, to be honest,\u201d says Rundl\u00f6f. Hives of honeybees ( Apis mellifera ) weren't greatly affected by the chemicals in their pollen and nectar, the study suggested 1 . But the data on bumblebees ( Bombus terrestris ) told a different story. Bumblebee colonies that hadn't fed on the treated crops looked normal: they were packing on weight to survive the winter. But in the colonies exposed to neonicotinoids, the growth chart was a flat line. When the Swedish study was published in April 2015,  it made headlines around the world . It was the first to show that neonicotinoid chemicals \u2014 known as neonics \u2014 could harm bees in a real-world farming situation. Bee populations are declining in many parts of the globe, a worrying sign for the crops and wild plants that rely on these pollinators for their survival.  Parasites, disease  and  shrinking food resources  are all prime suspects. But a link to neonics has become a major flashpoint. Even before Rundl\u00f6f's results were revealed, the European Union had placed heavy restrictions on the three most widely used neonics in flowering crops \u2014 plants that might be attractive to bees \u2014 amid rising concerns that the chemicals might harm pollinators. The restricted neonics were imidacloprid and clothianidin, made by agrochemical giant Bayer, and thiamethoxam, made by Syngenta. But farmers, the agrochemical industry and some scientists pointed out that the moratorium was precautionary and based on limited evidence, gathered mostly from lab tests. Since Rundl\u00f6f's paper, studies showing real-world evidence of harm from pesticides in the field have been mounting \u2014 and environmental organizations have demanded wide-ranging bans. Regulatory agencies will soon decide what to do about neonics, which have a global market worth more than US$1.5 billion per year. This month, the EU's European Food Safety Authority is due to complete a re-evaluation of evidence for restricting neonics; the EU will then need to decide what action to take. The US Environmental Protection Agency is expected to complete its own review of the insecticides next year. France's parliament has passed a law that would ban neonics in 2018, although some exemptions will be allowed. But industry groups and some scientists say the evidence still isn't conclusive. The picture is complicated: some studies show harm to some bees in some circumstances, whereas others find no harm. The results seem to be affected by many factors, including the species of bee and the kinds of crops involved. Scientists working on the question say the subject has become toxic: any new study is instantly and furiously picked at by entrenched advocates on both sides. Even the results of the largest study on the matter, funded by the agrochemical industry, failed to produce a consensus. Published this year 2 , it launched  another round of recriminations  \u2014 including complaints from funders who criticized the paper that they had paid for. Ultimately, it's likely that political or regulatory decisions will settle the matter before opposing parties agree, says Sainath Suryanarayanan, an entomologist and sociologist at the University of Wisconsin\u2013Madison who has  studied the bee-health issue . \u201cIt is a common pattern for highly contentious and polarized debates,\u201d he says. \n               The world's favourite insecticide \n             In the early 1980s, scientists at Nihon Tokushu Noyaku Seizo in Tokyo, an arm of Bayer, started to play around with nithiazine, an insecticide created in California a decade earlier. They discovered a new compound that was more than 100 times as effective at killing crop pests, such as aphids. Named imidacloprid, the chemical was launched onto the market in the 1990s, and it quickly became one of the most widely used insecticides in the world. By the mid-2000s, imidacloprid and similar compounds made up one-quarter of all insecticides (see 'Rising tide'). The compounds damage insects' nervous systems by causing the nerves to fire continually until they fail, eventually leading to death. Many neonics are applied directly to seeds, and are taken up by growing plants. If the plant flowers, the chemicals find their way into pollen and nectar. In France, where sunflower seeds coated with imidacloprid came on the market in 1994, beekeepers raised the alarm. They said that their honeybees were failing to make it home after foraging flights, and they pinned the blame on the sunflowers. The concerns triggered a 1999 French ban on imidacloprid-coated sunflower seeds, which continues to this day \u2014 although it was based on the precautionary principle, rather than formal proofs of harm, says Axel Decourtye, a researcher at the Institute for Bees in Avignon, France. Scientists hurried to find those proofs \u2014 or evidence that the concern was overblown. Researchers quickly discovered that honeybees fed high doses of neonicotinoids died. And even sub-lethal doses triggered unusual behaviour: exposed honeybees changed their dining habits, foraging less often but for longer periods 3 . Other research showed 4  that neonics act on parts of a bee's brain associated with memory and learning. Honeybees trained to respond to particular scents by sticking out their tongues, for example, performed worse \u2014 or failed to learn the task at all \u2014 when dosed with a neonic. At every stage, critics raised new queries about how realistic the experiments were, says Decourtye. \u201cHow do we know if the neonicotinoid doses are realistic? Does the effect on the individual have any effect on the colony?\u201d \n               Out in the field \n             As work continued in the laboratory, researchers also began to turn to the fields. In 2012, Decourtye and his colleagues published a paper 5  showing that what they called \u201cthiamethoxam intoxication\u201d seemed to interfere with the ability of honeybees to return to their hives after looking for food in a realistic, outdoor setting. Yet that study still dosed bees' food with neonics, rather than allowing them to feed on treated crops. Around the same time, a UK team found 6  that it was not just honeybees that could be at risk. They reported that colonies of bumblebees exposed to \u201cfield-realistic\u201d levels of imidacloprid in the lab and then left to grow in field conditions grew slower than controls. They also produced 85% fewer new queens to carry on their line. That work was led by Dave Goulson, a bee researcher now at the University of Sussex in Brighton, UK. In 2006, Goulson had started a charity dedicated to conserving bumblebees, and people began telling him their concerns about neonics. \u201cTo start with, I was pretty dubious,\u201d he says. But by 2014, the Task Force on Systemic Pesticides (TFSP) \u2014 a group of 30 scientists, including Goulson \u2014 announced that it had analysed 800 peer-reviewed studies on neonics and bees, and found \u201cclear evidence of harm sufficient to trigger regulatory action\u201d 8 . Rundl\u00f6f's study set out to be the most realistic yet. Her team sowed eight Swedish fields with oilseed-rape seeds coated in clothianidin, and eight with untreated seeds. They found 1  not only that bumblebee colonies in treated fields grew less well than the controls, but also that the numbers of wild bees in the treated fields fell. Industry spokespeople noted that honeybee colonies weren't affected, and also quibbled with the study \u2014 arguing, for example, that the researchers had only placed a small number of wild bees into fields, so findings might not be statistically robust. Rundl\u00f6f, however, points out that the researchers also surveyed wild bees flying around, and had the bumblebee-colony data to draw on. \u201cI know we have robust evidence,\u201d she says. In mid-2017, the largest field study yet \u2014 funded with some $3 million from industry \u2014 reported its long-awaited results 2 . Scientists from the Centre for Ecology and Hydrology (CEH) near Wallingford, UK, had put honeybees, mason bees ( Osmia bicornis ) and bumblebees in 33 oilseed-rape fields in the United Kingdom, Germany and Hungary. This time, the seeds, sown in winter, had been coated with either clothianidin or thiamethoxam, or with a neonicotinoid-free pesticide treatment. The researchers, led by CEH entomologist Ben Woodcock, found that bumblebees and mason bees fared less well the more neonics they were exposed to. The honeybee picture was more complicated: in some cases, neonics seemed to affect bee health, but in others, they didn't. In the United Kingdom and Hungary, neonic compounds seemed to reduce worker-bee numbers in honeybee hives; in Hungary, researchers also saw fewer egg cells in these hives, an indication of reduced reproductive success. In Germany, however, the honeybee hives exposed to neonics had more egg cells \u2014 a puzzling result. Overall, the CEH study concluded that neonicotinoids reduced bees' ability to establish new colonies after winter. The journal editor's summary of the paper came under the headline: \u201cDamage confirmed\u201d. The agrochemical firms that funded the study don't agree. At a press conference in June, when CEH scientists presented their results \u2014 without Woodcock, who was overseas \u2014 spokespeople from Syngenta and Bayer told reporters that both the study's analysis and its conclusions were questionable. They noted that Woodcock's team had analysed more than 200 pieces of information about honeybees; 9 showed a negative effect from neonicotinoids, whereas 7 were positive. \u201cThe one-line simplistic summary conclusion published does not reflect the data presented in this paper,\u201d argued Peter Campbell, an environmental specialist at Syngenta in Reading, UK, in a separate statement released to the media. Woodcock was incensed by the criticism. In an interview with environmental group Greenpeace, he said that industry had accused him of being a liar. Now, he says, he regrets that choice of words, but he still thinks industry took a blinkered view of the results. \u201cI do feel that the sentiment of what I implied, while inappropriate, was not an unreasonable reaction,\u201d he says. The negative effects were in key areas related to bee health, he says, adding that for industrial firms to deny that neonics are having an effect on bees is \u201cprobably naive\u201d. Many of the academics  Nature  talked to agree. \u201cI think the majority of researchers highlight that the weakening of bee populations caused by neonicotinoids is proved,\u201d says Decourtye. But not everyone is so certain. \u201cThe question of whether the damage to bees is translated to an effect in fields on whole populations of bees is much harder to show,\u201d says Linda Field, head of the department of Biointeractions and Crop Protection at Rothamsted Research in Harpenden, UK. Mature colonies may survive even if individual bees are impaired, because other worker bees compensate, notes Nigel Raine, a biologist at the University of Guelph in Canada. But solitary bees, such as wild bees and queen bumblebees emerging from hibernation, might be at greater risk. Campbell thinks that many academics are \u201cneutral\u201d on the matter, but are not vocal about it. Studies showing harm to bees tend to garner media attention, and are published in widely read journals, whereas those showing no impact are relegated to less highly cited publications, he says. But Goulson and Woodcock say some of the studies that industry cites as showing no harm are statistically dubious, and more flawed than the headline-garnering trials that show harm. Christian Maus, global lead scientist for bee care at Bayer in Monheim am Rhein, Germany, picks his words carefully. \u201cI think it is clear and undebated that neonicotinoids do have some intrinsic toxicity to bees,\u201d he says. \u201cBut under realistic conditions, as prevailing in the field and agricultural practice, we have not seen any evidence that they would be harming honeybee colonies, for instance, when they are correctly applied.\u201d \n               Combinatorial effects \n             Researchers are looking beyond simple relationships between a single pesticide and bee harm. In a 2012 paper 8 , Raine and his colleagues showed that exposing bumblebees to a neonicotinoid in combination with a pesticide called a pyrethroid hampered their ability to collect pollen. Colonies exposed to both compounds experienced higher losses of worker bees than did controls, or colonies dosed with only one. The study was the first to show combinatorial effects, Raine says \u2014 which is important, because bees will be exposed to multiple compounds in the wild. And this year, in a paper 9  published alongside Woodcock's, a Canadian team studying honeybee colonies near maize (corn) plants found that the presence of the fungicide boscalid halved the dose of neonics needed to cause death. That work also suggested that neonic chemicals can migrate away from the plants that they are supposed to protect: by identifying the sources of pollen grains in the hives, the researchers showed that bees were exposed to neonics mainly through pollen from untreated plants. Neonicotinoids are water-soluble \u2014 which is how they move from seeds into growing plant tissues. \u201cBut that also means they can be washed off the seed, into the soil, and maybe into other plants,\u201d says Christian Krupke, an entomologist at Purdue University in West Lafayette, Indiana. In one study 10 , Krupke found that just 1.34% of clothianidin applied as seed treatment to maize ended up in the crop\u2019s tissues. Neonics that get into the wider environment might cause other, more indirect problems. A 2014 study 11  in the Netherlands, for instance, reported a fall in populations of insect-eating birds in areas with high concentrations of neonicotinoids in the water. It suggested that the chemicals might have depleted the birds\u2019 food resource. Some researchers are now questioning whether there is any benefit to using neonicotinoids at all. In another study 12 , Krupke\u2019s group found no benefits on maize yield from the use of neonicotinoids in Indiana. In this crop, he says, the prophylactic use of neonicotinoids \u2014 which are often part of a bundle of pesticides sold pre-applied to seeds \u2014 is foolish. \u201cThe way they\u2019re used doesn\u2019t make any sense,\u201d he says. \u201cIt only makes sense from one motive. That is the profit motive for the manufacturer.\u201d Campbell insists that neonicotinoids do provide yield increases, but much of the evidence is proprietary and unpublished. Since the EU neonicotinoid restrictions, Maus says, research suggests there has been a 4% decline in oilseed-rape yield. Whether or not the restrictions have had any effect, farmers have furiously protested against losing the ability to use neonics. Anecdotal reports suggest many are attempting to compensate by applying increasing amounts of pyrethroids, which are sprayed over crops, rather than applied to seeds; these chemicals may bring their own health risks if used in large quantities, because they are toxic to fish and aquatic insects. \n               The B word \n             Regulators in some countries will soon decide whether to take further action to restrict neonics \u2014 and here, researchers are split. Some campaign groups, such as Greenpeace and the Pesticide Action Network, have argued for a ban on the use of neonics on all outdoor crops, not just those that might be attractive to bees, such as the bright-yellow flowers of oilseed rape. \u201cA lot of farmers do fundamentally rely on neonicotinoids,\u201d says Woodcock. And clamping down severely on one chemical might mean that greater amounts of other damaging substances are used. \u201cIf people can't use neonicotinoids and they go to other insecticides, is that any better? There are lots of knock-on effects,\u201d says Field. That concern points to wider doubts about the regulatory systems that allowed agrichemicals such as neonics onto the market in the first place, says Goulson. Many researchers are hesitant to advocate outright bans. Some, such as Rundl\u00f6f, say it isn't their job to make policy recommendations. But Goulson says his view has changed as the evidence has mounted. In 2014 \u2014 at the time of the TFSP's first synthesis report \u2014 he thought that there might be certain situations in which neonics were the best option. But since then, he says, there's been even stronger evidence of collapsing insect populations \u2014 and it is hard to regulate partial bans. \u201cI think now I'd vote for a complete ban,\u201d he says. Whatever regulators do, Goulson says, he is growing increasingly downbeat about the chances of any consensus forming between industry and academia on the issue. \u201cI'm starting to come to the conclusion there will never be a game-changer,\u201d he says. \u201cThere is nothing I think any scientist could do at this point to make people all sit down and have any answer.\u201d \n                     Controversial pesticides found in honey samples from six continents 2017-Oct-05 \n                   \n                     Largest-ever study of controversial pesticides finds harm to bees 2017-Jun-29 \n                   \n                     Controversial insecticides linked to wild bee declines 2016-Aug-16 \n                   Reprints and Permissions"},
{"file_id": "d41586-017-07291-9", "url": "https://www.nature.com/articles/d41586-017-07291-9", "year": 2017, "authors": [{"name": "Elie Dolgin"}], "parsed_as_year": "2006_or_before", "body": "A tour through the most studied genes in biology reveals some surprises. Peter Kerpedjiev needed a crash course in genetics. A software engineer with some training in bioinformatics, he was pursuing a PhD and thought it would really help to know some fundamentals of biology. \u201cIf I wanted to have an intelligent conversation with someone, what genes do I need to know about?\u201d he wondered. Kerpedjiev went  straight to the data . For years, the US National Library of Medicine (NLM) has been systematically tagging almost every paper in its popular PubMed database that contains some information about what a gene does. Kerpedjiev extracted all the papers marked as describing the structure, function or location of a gene or the protein it encodes. Sorting through the records, he compiled a list of the most studied genes of all time \u2014 a sort of \u2018top hits\u2019 of the human genome, and several other genomes besides. Heading the list, he found, is a gene called  TP53 . Three years ago, when Kerpedjiev  first did his analysis , researchers had scrutinized the gene or the protein it produces, p53, in some 6,600 papers. Today, that number is at about 8,500 and counting. On average, around two papers are published each day describing new details of the basic biology of  TP53 . Its popularity shouldn\u2019t come as news to most biologists. The gene is a tumour suppressor, and widely known as the \u2018guardian of the genome\u2019. It is mutated in roughly half of all human cancers. \u201cThat explains its staying power,\u201d says Bert Vogelstein, a cancer geneticist at the Johns Hopkins University School of Medicine in Baltimore, Maryland. In cancer, he says, \u201cthere\u2019s no gene more important\u201d. But some chart-topping genes are less well known \u2014 including some that rose to prominence in bygone eras of genetic research, only to fall out of fashion as technology progressed. \u201cThe list was surprising,\u201d says Kerpedjiev, now a postdoc studying  genomic-data visualization  at Harvard Medical School in Boston, Massachusetts. \u201cSome genes were predictable; others were completely unexpected.\u201d To find out more,  Nature  worked with Kerpedjiev to analyse the most studied genes of all time (see \u2018The top 10\u2019). The exercise offers more than a conversation starter: it sheds light on important trends in biomedical research, revealing how concerns over specific diseases or public-health issues have shifted research priorities towards underlying genes. It also shows how just a few genes, many of which span disciplines and disease areas, have dominated research. Out of the 20,000 or so protein-coding genes in the human genome, just 100 account for more than one-quarter of the papers tagged by the NLM. Thousands go unstudied in any given year. \u201cIt\u2019s revealing how much we don\u2019t know about because we just don\u2019t bother to research it,\u201d says Helen Anne Curry, a science historian at the University of Cambridge, UK. \n                 In and out of fashion \n               In 2002, just after the first drafts of the human genome were published, the NLM started systematically adding \u2018gene reference into function\u2019, or  GeneRIF , tags to papers 1 . It has extended that annotation back to the 1960s, sometimes using other databases to help fill in the details. It is not a perfectly curated record. \u201cIn general, the data set is somewhat noisy,\u201d says Terence Murphy, a staff scientist at the NLM in Bethesda, Maryland. There\u2019s probably some sampling bias for papers published before 2002, he warns. That means that some genes are over-represented and a few may be erroneously missing. \u201cBut it\u2019s not awful,\u201d Murphy says. \u201cAs you aggregate over multiple genes, that potentially reduces some of these biases.\u201d With that caveat noted, the PubMed records reveal a few distinct historical periods in which gene-related papers tended to focus on particular hot topics (see \u2018Fashionable genes through the years\u2019). Before the mid-1980s, for example, much genetic research centred on haemoglobin, the oxygen-carrying molecule found in red blood cells. More than 10% of all studies on human genetics before 1985 were about haemoglobin in some way. At the time, researchers were still building on the early work of Linus Pauling and Vernon Ingram, trailblazing biochemists who pioneered the study of disease  at a molecular level  with discoveries in the 1940s and 1950s of how abnormal haemoglobin caused sickle-cell disease. Molecular biologist Max Perutz, who won a share in the 1962 Nobel Prize in Chemistry for his 3D map of haemoglobin\u2019s structure, continued to explore how the protein\u2019s shape related to its function for decades afterwards. According to Alan Schechter, a physician-scientist and senior historical consultant at the US National Institutes of Health in Bethesda, the haemoglobin genes \u2014 more than any others at the time \u2014 offered \u201can entryway to understanding and perhaps treating a molecular disease\u201d. A sickle-cell researcher himself, Schechter says that such genes were a focus of conversation both at major genetics meetings and at blood-disease meetings in the 1970s and early 1980s. But as researchers gained access to new technologies for sequencing and manipulating DNA, they started to move on to other genes and diseases, including a then-mysterious infection that was predominantly striking down gay men. Even before the 1983 discovery that HIV was the cause of AIDS, clinical immunologists such as David Klatzmann had noticed a peculiar pattern among people with the illness. \u201cI was just struck by the fact that these people had no T4 cells,\u201d recalls Klatzmann, who is now at Pierre and Marie Curie University in Paris. He showed 2  in cell-culture experiments that HIV seemed to selectively infect and destroy these cells, a subset of the immune system\u2019s T cells. The question was: how was the virus getting into the cell? Klatzmann reasoned that the surface protein (later called CD4) that immunologists used to define this set of cells might also serve as the receptor through which HIV entered the cell. He was right, as he reported 3  in a study published in December 1984, alongside a similar paper 4  from molecular virologist Robin Weiss, then at the Institute of Cancer Research in London, and his colleagues. Within three years,  CD4  was the top gene in the biomedical literature. It remained so from 1987 to 1996, a period in which it accounted for 1\u20132% of all the tags tallied by the NLM. That attention stemmed in part from efforts to tackle the emerging AIDS crisis. In the late 1980s, for example, several companies dabbled with the idea of engineering therapeutic forms of the CD4 protein that could mop up HIV particles before they infected healthy cells. But results from small human trials proved \u201cunderwhelming\u201d, says Jeffrey Lifson, director of the AIDS and Cancer Virus Program at the US National Cancer Institute in Frederick, Maryland. An even bigger part of  CD4 \u2019s popularity had to do with basic immunology. In 1986, researchers realized that CD4-expressing T cells could be subdivided into two distinct populations \u2014 one that eliminates cell-infecting bacteria and viruses, and one that guards against parasites such as worms, which cause illness without invading cells. \u201cIt was a fairly exciting time, because we really understood very little,\u201d says Dan Littman, an immunologist at the New York University School of Medicine. Just the year before, he had helped to clone the DNA that encodes CD4 and insert it into bacteria 5 , so that vast quantities of the protein could be made for research. A decade later, Littman also co-led one of three teams to show 6  that to enter cells, HIV uses another receptor alongside CD4: a protein identified as CCR5. These, and a second co-receptor called CXCR4, have remained the focus of intensive, global HIV research ever since, with the goal \u2014 as-yet unfulfilled \u2014 of blocking the virus\u2019s entry into cells. \n                 Fifteen minutes of fame \n               By the early 1990s,  TP53  was already ascendant. But before it climbed to the top of the human gene ladder, there were a few years in which a lesser-known gene called  GRB2  was in the spotlight. At the time, researchers were starting to identify the specific protein interactions involved in cell communication. Thanks to pioneering work by cell biologist Tony Pawson, scientists knew that some small intracellular proteins contained a module called SH2, which could bind to activated proteins at the surface of cells and relay a signal to the nucleus. In 1992, Joseph Schlessinger, a biochemist at the Yale University School of Medicine in New Haven, Connecticut, showed 7  that the protein encoded by  GRB2  \u2014 growth factor receptor-bound protein 2 \u2014 was that relay point. It contains an SH2 module as well as two domains that activate proteins involved in cell growth and survival. \u201cIt\u2019s a molecular matchmaker,\u201d Schlessinger says. Other researchers soon filled in the gaps, opening a field of study in signal transduction. And although many other building blocks of cell signalling were soon unearthed \u2014 ultimately leading to treatments for cancer, autoimmune disorders, diabetes and heart disease \u2014  GRB2  stayed at the forefront and was the top-referenced gene for three years in the late 1990s. In part, that was because  GRB2  \u201cwas the first physical connection between two parts of the signal-transduction cascade\u201d, says Peter van der Geer, a biochemist at San Diego State University in California. Furthermore, \u201cit\u2019s involved in so many different aspects of cellular regulation\u201d. GRB2  is something of an outlier in the most-studied list. It\u2019s not a direct cause of disease; nor is it a drug target, which perhaps explains why its moment in the sun was fleeting. \u201cYou have some rising stars that fall down very quickly because they have no clinical value,\u201d says Thierry Soussi, a long-time  TP53  researcher at the Karolinska Institute in Stockholm and Pierre and Marie Curie University. Genes with staying power usually show some sort of therapeutic potential that attracts funding agencies\u2019 support. \u201cIt\u2019s always like that,\u201d Soussi says. \u201cThe importance of a gene is linked to its clinical value.\u201d It can also be linked to certain properties of the gene, such as the levels at which it is expressed, how much it varies between populations and the characteristics of its structure. That\u2019s according to an analysis by Thomas Stoeger, a systems biologist at Northwestern University in Evanston, Illinois, who reported this month at a symposium in Heidelberg, Germany, that he could predict which genes would garner the most attention, simply by plugging such attributes into an algorithm. Stoeger thinks that the reasons for these associations largely boil down to what he calls discoverability. The popular genes happened to be in hot areas of biology and could be probed with the tools available at the time. \u201cIt\u2019s easier to study some things than others,\u201d says Stoeger \u2014 and that\u2019s a problem, because vast numbers of genes remain uncharacterized and underexplored, leaving major gaps in the understanding of human health and disease. Curry also points to \u201cintertwined technical, social and economic factors\u201d shaped by politicians, drugmakers and patient advocates. \n                 Right place, right time \n               Stoeger has also tracked how the general features of popular genes have changed over time. He found, for example, that in the 1980s, researchers focused largely on genes whose protein products were found outside cells. That\u2019s probably because these proteins were easiest to isolate and study. Only more recently did attention shift towards genes whose products are found inside the cell. That shift happened alongside the publication of the human genome, says Stoeger. The advance would have opened up a larger percentage of genes to enquiry. Many of the most explored genes, however, don\u2019t fit these larger trends. The p53 protein, for example, is active inside the nucleus. Yet  TP53  became the most studied gene around 2000. It, like many of the genes that came to dominate biological research, was not properly understood after its initial discovery \u2014 which may explain why it took several decades after the 1979 characterization of the protein for the gene to rise to the  top spot in the literature . At first, the cancer-research community mistook it for an oncogene \u2014 one that, when mutated, drives the development of cancer. It wasn\u2019t until 1989 that Suzanne Baker, a graduate student in Vogelstein\u2019s lab, showed 8  that it was actually a tumour suppressor. Only then did functional studies of the gene really begin to pick up steam. \u201cYou can see from the spike in publications that go up essentially at that point that there were a lot of people who were really very interested,\u201d says Baker, now a brain-tumour researcher at the St. Jude Children\u2019s Research Hospital in Memphis, Tennessee. Research into human cancer also brought scientists to  TNF , the runner-up to  TP53  as the most-referenced human gene of all time, with more than 5,300 citations in the NLM data (see \u2018Top genes\u2019). It encodes a protein \u2014 tumour necrosis factor \u2014 named in 1975 because of its ability to kill cancer cells. But anticancer action proved not to be  TNF \u2019s main function. Therapeutic forms of the TNF protein were highly toxic when tested in people. The gene turned out to be a mediator of inflammation; its effect on tumours was secondary. Once that became clear in the mid-1980s, attention quickly shifted to testing antibodies that block its action. Now, anti-TNF therapies are mainstays of treatment for inflammatory disorders such as rheumatoid arthritis, collectively pulling in tens of billions of dollars in annual sales worldwide. \u201cThis is an example where the knowledge of the gene and the gene product has relatively quickly changed the health of the world,\u201d says Kevin Tracey, a neurosurgeon and immunologist at the Feinstein Institute for Medical Research in Manhasset, New York. TP53 \u2019s dominance was briefly interrupted by another gene,  APOE . First described in the mid-1970s as a transporter involved in clearing cholesterol from the blood, the APOE protein was \u201cseriously considered\u201d as a lipid-lowering treatment for preventing heart disease, says Robert Mahley, a pioneer in the field at the University of California, San Francisco, who tested the approach in rabbits 9 . Ultimately, the creation of statins in the late 1980s doomed this strategy to the dustbin of pharmaceutical history. But then, neuroscientist Allen Roses and his colleagues found the APOE protein bound up in the sticky brain plaques of people with Alzheimer\u2019s disease. They showed 10  in 1993 that  one particular form of the gene,  APOE4 , was associated with a greatly increased risk of the disease. This generated much wider interest in the gene. Still, it took time to move up the most-studied chart. \u201cThe reception was very cool,\u201d recalls Ann Saunders, a neurogeneticist and chief executive of Zinfandel Pharmaceuticals in Chapel Hill, North Carolina, who collaborated with Roses, her late husband. The amyloid hypothesis, which states that build-up of a protein fragment called amyloid-\u03b2 is responsible for the disease, was all the rage in the Alzheimer\u2019s-research community at the time. And few researchers seemed interested in finding out what a cholesterol-transport protein had to do with the disease. But the genetic link between  APOE4  and Alzheimer\u2019s risk proved \u201cirrefutable\u201d, Mahley says, and in 2001,  APOE  briefly overtook  TP53 . It remains in the all-time top five, at least for humans (see \u2018Beyond human\u2019). Like other popular genes,  APOE  is well studied because it\u2019s central to one of the biggest unsolved health problems of the day. But it\u2019s also important because anti-amyloid therapies have mostly flamed out in clinical testing. \u201cI hate saying this, but what helped me were the failed trials,\u201d says Mahley, who this year raised US$63 million for his company E-Scape Bio to develop drugs that target the APOE4 protein. Those failures, he says, forced industry and funding agencies to rethink therapeutic strategies for tackling Alzheimer\u2019s. There\u2019s the rub: it takes a certain confluence of biology, societal pressure, business opportunity and medical need for any gene to become more studied than any other. But once it has made it to the upper echelons, there\u2019s a \u201clevel of conservatism\u201d, says Gregory Radick, a science historian at the University of Leeds, UK, \u201cwith certain genes emerging as safe bets and then persisting until conditions change\u201d. The question now is how conditions might change. What new discoveries might send a new gene up the chart \u2014 and knock today\u2019s top genes off their pedestal?  \n                 A radical revision of human genetics \n               \n                 Big biology: The \u2019omes puzzle \n               \n                 The top 100 papers \n               \n                 Where in the world could the first CRISPR baby be born? \n               Reprints and Permissions"},
{"file_id": "551020a", "url": "https://www.nature.com/articles/551020a", "year": 2017, "authors": [{"name": "Zeeya Merali"}], "parsed_as_year": "2006_or_before", "body": "Experiments are starting to probe the limits of the classical laws of thermodynamics. It would take a foolhardy physicist to dare attempt to break the laws of thermodynamics. But it turns out that there may be ways to bend them. At a lab at the University of Oxford, UK, quantum physicists are trying to do so with a small lump of synthetic diamond. At first, the diamond is barely visible, nestled inside a chaotic mess of optical fibres and mirrors. But when they switch on a green laser, defects in the diamond are illuminated, and the crystal begins to glow red. In that light, the team has found  preliminary evidence  of an effect that was theorized only a few years ago 1 : a quantum boost that would push the diamond's power output above the level prescribed by classical thermodynamics. If the results hold up, they will be a tangible boon for the study of quantum thermodynamics, a relatively new field that aims to uncover the rules that govern heat and energy flow at the atomic scale. There is reason to suspect that the laws of thermodynamics, which are based on how large numbers of particles behave, are different in the quantum realm. Over the past five years or so, a quantum-thermodynamics community has grown around that idea. What was once the domain of a handful of theoreticians now includes a few hundred theoretical and experimental physicists around the globe. \u201cThe field is moving so fast I can barely keep up,\u201d says Ronnie Kosloff, an early pioneer of the field at the Hebrew University of Jerusalem in Israel. A number of quantum thermodynamicists hope to find behaviour outside the remit of conventional thermodynamics that could be adapted for practical purposes, including improving lab-based refrigeration techniques, creating batteries with enhanced capabilities and refining technology for quantum computing. But the field is still in its infancy. Experiments such as the one taking place at Oxford are just starting to put theoretical predictions to the test. And physicists working at the periphery are watching such tests closely for evidence of the useful applications that theorists have predicted. \u201cQuantum thermodynamics is clearly hot \u2014 pardon the pun,\u201d says Ronald Walsworth, a physicist at Harvard University in Cambridge, Massachusetts, who specializes in developing precision atomic-scale tools. \u201cBut for those of us looking in from the outside, the question is: can it really shed new light on the development of technologies?\u201d \n               Breaking the law \n             The development of the classical laws of thermodynamics stretches back to the nineteenth century. They emerged from the effort to understand steam engines and other macroscopic systems. Thermodynamic quantities such as temperature and heat are statistical in nature and defined in reference to the average motion of large ensembles of particles. But back in the 1980s, Kosloff began pondering whether this picture would continue to make sense for much smaller systems. It wasn't a popular line of research at the time, says Kosloff, because the questions being asked were largely abstract, with little hope of connection to experiments. \u201cThe field developed very slowly,\u201d he says. \u201cI was alone for years.\u201d That changed dramatically around a decade ago, as questions about the limits of technological miniaturization became more pressing and experimental techniques advanced. A flurry of attempts were made to calculate how thermodynamics and quantum theory might combine. But the resulting proposals created more confusion than clarity, Kosloff says. Some claimed that quantum devices could violate classical thermodynamic constraints with impunity and so act as perpetual-motion machines, capable of performing work without needing any energy input. Others, suggesting that the laws of thermodynamics should hold unmodified at very small scales, were equally perplexing. \u201cIn some sense, you can use the same equations to work out the performance of a single atom engine and your car engine,\u201d says Kosloff. \u201cBut that seems shocking, too \u2014 surely as you get smaller and smaller you should hit some quantum limit.\u201d In classical thermodynamics, a single particle doesn't have a temperature. So as both the system generating work and its environment approach that limit, it becomes increasingly absurd to imagine that they would obey standard thermodynamic rules, says Tobias Schaetz, a quantum physicist at the University of Freiburg in Germany. The preponderance of conflicting theoretical claims and predictions initially undermined the burgeoning field's credibility. \u201cI have been very critical of the field because there is far too much theory and not enough experiment,\u201d says quantum physicist Peter H\u00e4nggi, at the University of Augsburg in Germany. But the community is beginning to coalesce more formally around core questions in an effort to cut through the chaos. One goal has been to use experiments to uncover the point at which the classical laws of thermodynamics no longer perfectly predict the thermal behaviour of quantum systems. Experiments are starting to pin down that quantum\u2013classical boundary. Last year, for example, Schaetz and his colleagues showed that, under certain conditions, strings of five or fewer magnesium ions in a crystal do not reach and remain in thermal equilibrium with their surroundings like larger systems do 2 . In their test, each ion started in a high-energy state and its spin oscillated between two states corresponding to the direction of its magnetism \u2014 'up' and 'down'. Standard thermodynamics predicts that such spin oscillations should die down as the ions cool by interacting with the other atoms in the crystal around them, just as hot coffee cools when its molecules collide with molecules in the colder surrounding air. Such collisions transfer energy from the coffee molecules to the air molecules. A similar cooling mechanism is at play in the crystal, where quantized vibrations in the lattice called phonons carry heat away from the oscillating spins. Schaetz and his colleagues found that their small ion systems did stop oscillating, suggesting that they had cooled. But after a few milliseconds, the ions began oscillating vigorously again. This resurgence has a quantum origin, says Schaetz. Rather than dissipating away entirely, the phonons rebounded at the edges of the crystal and returned, in phase, to their source ions, reinstating the original spin oscillations. Schaetz says that his experiment sends a warning to engineers attempting to reduce the size of existing electronics. \u201cYou may have a wire that is only 10 or 15 atoms wide, and you may think that it has successfully carried the heat away from your chip, but then boop \u2014 suddenly this quantum revival happens,\u201d Schaetz says. \u201cIt is very disturbing.\u201d Rebounding phonons could present a challenge in some applications, but other quantum phenomena could turn out to be useful. Efforts to identify such phenomena had been stalled by the difficulty in defining basic quantities, such as heat and temperature, in quantum systems. But the solution to a famous thought experiment, laid out 150 years ago by Scottish physicist James Clerk Maxwell, provided a clue about where to turn, posing an intriguing link between information and energy. Maxwell imagined an entity that could sort slow- and fast-moving molecules, creating a temperature difference between two chambers simply by opening and closing a door between them. Such a 'demon', as it was later called, thus generates a hot and a cold chamber that can be harnessed to produce useful energy. The problem is that by sorting particles in this way, the demon reduces the system's entropy \u2014 a measure of the disorder of the particles' arrangements \u2014 without having done any work on the particles themselves. This seemingly violates the second law of thermodynamics. But physicists eventually realized that the demon would  pay a thermodynamic price  to process the information about the molecules' speeds. It would need to store, erase and rewrite that information in its brain. That process consumes energy and creates an overall increase in entropy 3 . Information was once thought to be immaterial, \u201cbut Maxwell's demon shows that it can have objective physical consequences\u201d, says quantum physicist Arnau Riera, at the Institute of Photonic Sciences in Barcelona, Spain. \n               Finding the limit \n             Inspired by the idea that information is a physical quantity \u2014 and that it is intimately linked to thermodynamics \u2014 researchers have attempted to recast the laws of thermodynamics so that they work in the quantum regime. Perpetual-motion machines may be impossible. But an early hope was that limits prescribed by quantum thermodynamics might be less stringent than those that hold in the classical realm. \u201cThis was the train of thought we had learned from quantum computing \u2014 that quantum effects help you beat classical bounds,\u201d says Raam Uzdin, a quantum physicist at the Technion\u2013Israel Institute of Technology in Haifa. Disappointingly, Uzdin says, this is not the case. Recent analyses suggest that quantum versions of the second law, which governs efficiency, and the third law, which prohibits systems from reaching absolute zero, retain similar and, in some cases, more-stringent constraints than their classical incarnations. Some differences arise because the macroscopic thermodynamic quantity 'free energy'\u2014 the energy a system has available to do work \u2014 doesn't have just one counterpart at the microscale, but many, says Jonathan Oppenheim, a quantum physicist at University College London. Classically, the free energy is calculated by assuming that all states of the system, determined by the arrangement of particles at a given energy, are equally likely. But that assumption isn't true on tiny scales, says Oppenheim; certain states might be much more probable than others. To account for this,  additional free energies need to be defined  in order to accurately describe the system and how it will evolve. Oppenheim and his colleagues propose that individual second laws exist for each type of free energy, and that quantum devices must obey all of them 4 . \u201cSince the second law tells you what you aren't allowed to do, in some ways, it seems that having more laws on the microscale leaves you worse off,\u201d says Oppenheim.  The field is moving so fast I can barely keep up.  Much of the work done to calculate equivalents of the second and third laws remains, for now, theoretical. But proponents argue that it can help to illuminate how thermodynamic bounds are physically enforced at small scales. For instance, a theoretical analysis carried out by a pair of quantum physicists based in Argentina showed that as a quantum refrigerator nears absolute zero, photons will spontaneously appear in the vicinity of the device 5 . \u201cThis dumps energy into the surroundings, causing a heating effect that counters the cooling and stops you ever reaching absolute zero,\u201d explains team member Nahuel Freitas of Ciudad University in Buenos Aires. Theory has also revealed some potential wiggle room. In a theoretical analysis examining information flow between hot and cold chambers, or 'baths', of particles, a team based in Barcelona that included Riera and quantum physicist Manabendra Nath Bera discovered a strange scenario in which the hot bath seemed to spontaneously get hotter, while the cold bath became colder 6 . \u201cAt first, this looks crazy, like we can violate thermodynamics,\u201d says Bera. But the researchers soon realized that they had overlooked the quantum twist: the particles in the baths can become entangled. In theory, making and breaking these correlations provides a way to store and release energy. Once this quantum resource was budgeted for, the laws of thermodynamics fell into place. A number of independent groups have proposed using such entanglement to store energy in a 'quantum battery', and a group at the Italian Institute of Technology in Genoa is attempting to confirm the Barcelona team's predictions with batteries built from superconducting quantum bits, or 'qubits' 7 . In principle, such quantum batteries could charge considerably faster than their classical equivalents. \u201cYou won't be able to extract and store more energy than the classical bound allows \u2014 that's set by the second law,\u201d says Riera. \u201cBut you may be able to speed things up.\u201d Some researchers are looking for easier ways to manipulate qubits for quantum-computing applications. Quantum physicist Nayeli Azucena Rodr\u00edguez Briones at the University of Waterloo in Canada and her colleagues have devised 8  an operation that might enhance the cooling needed for quantum-computing operations by manipulating pairs of qubit energy levels. They are currently planning to test this idea in the lab using superconducting qubits. \n               A small spark \n             The concept that quantum effects could be exploited to improve thermodynamic performance also inspired the diamond experiment under way at Oxford, which was first proposed by Kosloff, Uzdin and Amikam Levy, also at the Hebrew University 1 . Defects created by nitrogen atoms scattered through the diamond can serve as an engine \u2014 a machine that performs an operation after being brought into contact with first a hot reservoir (in this case a laser) and then a cold one. But Kosloff and his colleagues expect that such an engine can be operated in an enhanced mode, by exploiting a quantum effect that enables some of the electrons to exist in two energy states simultaneously. Maintaining these superpositions by pulsing the laser light rather than using a continuous beam should enable the crystal to emit microwave photons more rapidly than it otherwise would (see 'Building a quantum heat engine'). Last week, the Oxford-based team posted a preliminary analysis 9  showing evidence of the predicted quantum boost. The paper has yet to be peer reviewed, but if the work holds up, then \u201cit is a groundbreaking result,\u201d says Janet Anders, a quantum physicist at Exeter University, UK. But, she adds, it's still not clear exactly what enables this feat. \u201cIt seems to be a magic fuel, not so much adding energy, but enabling the engine to extract energy faster,\u201d Anders says. \u201cTheoretical physicists will need to examine just how it does this.\u201d Focusing on experiments is a major step in the right direction for revitalizing the field, says H\u00e4nggi. But, for him, the experiments are not yet bold enough to give truly ground-breaking insights. There is also the challenge that quantum systems can be irrevocably disturbed by measurement and interaction with the environment. These effects are rarely sufficiently accounted for in theoretical proposals for new experiments, he says. \u201cThat is difficult to calculate, and much more difficult to implement in an experiment,\u201d he says. Ian Walmsley, who heads the Oxford lab where the diamond experiment was conducted, is also circumspect about the future of the field. Although he and other experimenters have been drawn to quantum thermodynamics research in recent years, he says that their interest has been largely \u201copportunistic\u201d. They have spotted the chance to carry out relatively quick and easy experiments by piggybacking on set-ups already in place for other uses; the diamond-defect set-up, for instance, is already being widely studied for quantum computing and sensor applications. Today, quantum thermodynamics is fizzing with energy, Walmsley says. \u201cBut whether it will continue to sparkle, or just explode into nothing, well, we will have to wait and see.\u201d \n                     Battle between quantum and thermodynamic laws heats up 2017-Mar-29 \n                   \n                     Quantum gas goes below absolute zero 2013-Jan-03 \n                   \n                     The unavoidable cost of computation revealed 2012-Mar-07 \n                   \n                     Demonic device converts information to energy 2010-Nov-14 \n                   Reprints and Permissions"},
{"file_id": "d41586-017-05921-w", "url": "https://www.nature.com/articles/d41586-017-05921-w", "year": 2017, "authors": [{"name": "Erik Vance"}], "parsed_as_year": "2006_or_before", "body": ""},
{"file_id": "d41586-017-07523-y", "url": "https://www.nature.com/articles/d41586-017-07523-y", "year": 2017, "authors": [{"name": "Katherine Bourzac"}], "parsed_as_year": "2006_or_before", "body": ""},
{"file_id": "d41586-017-07844-y", "url": "https://www.nature.com/articles/d41586-017-07844-y", "year": 2017, "authors": [{"name": "Shannon Hall"}], "parsed_as_year": "2006_or_before", "body": ""},
{"file_id": "d41586-017-08404-0", "url": "https://www.nature.com/articles/d41586-017-08404-0", "year": 2017, "authors": [{"name": "Richard Van Noorden"}], "parsed_as_year": "2006_or_before", "body": ""},
{"file_id": "index.html", "url": "https://www.nature.com/immersive/d41586-017-07763-y/index.html", "year": 2017, "authors": [], "parsed_as_year": "2006_or_before", "body": ""},
{"file_id": "549448a", "url": "https://www.nature.com/articles/549448a", "year": 2017, "authors": [{"name": "Elizabeth Gibney"}], "parsed_as_year": "2006_or_before", "body": "Physicist Gil Lonzarich has sparked a revolution in the study of phase transitions driven by quantum fluctuations. In 1989, surgery for detached retinas left Gilbert Lonzarich blind for a month. Rather than feel shaken or depressed, the condensed-matter physicist at the University of Cambridge, UK, seized the opportunity, inviting his graduate students to his house to share with them how exciting it was to adapt to life without sight. Lonzarich's embrace of the experience perfectly captures his approach to life, says Andrew Mackenzie, then one of those students and now a director at the Max Planck Institute for Chemical Physics of Solids in Dresden, Germany. \u201cGil is one of the most positive people I've ever met. He finds interest in everything,\u201d he says. For more than 40 years, that optimism and curiosity has led Lonzarich to probe materials in ways never thought possible. In pioneering experiments in the 1990s, his team showed that pushing magnetic compounds to extreme pressures and close to absolute zero can make some of them conduct electricity without resistance 1 . This flew in the face of convention, which declared that magnetism and superconductivity could never mix. \u201cIt was as if nowadays you were talking about finding aliens or something,\u201d says Malte Grosche, a colleague at Cambridge. That work showed physicists a new way to hunt for superconductors, which lie at the heart of technologies such as magnetic resonance imagers and particle accelerators. In recent years, it has offered a potential explanation for why some materials remain superconductors at temperatures much higher than absolute zero, which could pave the way to developing efficient, cheap devices that superconduct at room temperature. But the experiments have had an impact well beyond superconductivity. Lonzarich's method of subjecting materials to extreme conditions has become a general recipe for discovering new states of matter. Around the world, physicists now use this approach to probe a range of materials in which the collective interactions of electrons can give rise to unusual behaviour. Some of these phenomena could potentially revolutionize computing. Lonzarich's research may be legendary in his community, but the physicist's humility and generosity are what endear him to his colleagues. He is famously unconscious of time; a casual conversation with Lonzarich can easily lead to an hours-long random walk through the byways of physics, philosophy, politics and history. That might mean a missed lunch, says Michael Sutherland, a Cambridge colleague \u2014 \u201cbut it's the most productive few hours you'll have all week\u201d. Phone calls with peers frequently last into the small hours of the morning, and on the rare occasions when Lonzarich goes to a conference, he invariably attracts a mass of fellow attendees. \u201cPeople who meet him even once or twice develop a sense of attachment and awe,\u201d says Louis Taillefer, another former student, who is now a physicist at the University of Sherbrooke in Canada. At 72, Lonzarich now has a part-time role in the  Cambridge quantum-matter group , but he is still making new discoveries by pushing materials to ever-greater extremes. He sees this little-explored realm as just as fundamental to unravelling the laws of physics as the high-energy experiments at particle colliders, and expects that there is plenty more to discover. \u201cGil has never believed that we're now just filling in the details,\u201d says Piers Coleman, a theoretical physicist at Rutgers University in Piscataway, New Jersey. \u201cHe really views the exploration of quantum matter as a true frontier.\u201d \n               Collective efforts \n             Walking around a timeworn study at Trinity College, Cambridge, Lonzarich is eager to point out a portrait of the economist John Kenneth Galbraith, one of his heroes, and he talks enthusiastically about the impressive work of his colleagues. But when conversation turns to his own achievements, Lonzarich becomes reticent. It is human nature is to celebrate heroes, he says, but science is a collective activity, and singling out individuals for praise stifles a team. Although colleagues are quick to highlight Lonzarich's influence, he never would \u2014 a practice that could be traced back to his upbringing, by Italian parents, on the Istria peninsula. His father told him to \u201calways cut the larger slice of the pie for the other person\u201d, he recalls. At school, he learnt about the Roman Republic and was intrigued by the importance placed on reason, compromise and collaborative governance. His family moved to the United States when he was nine. By the 1960s, Lonzarich had grown into a studious young man. His interest in physics began at the University of California, Berkeley, where he earned a liberal arts degree. It was there that he met Gerie Simmons (now Lonzarich). The pair had admired each other from afar before she engineered their meeting by pretending to need a physics tutor; they married in 1967. \u201cOn the rare occasions I've seen pictures of him from those days, he had long hair. He was a physics hippy,\u201d says Coleman. But although Lonzarich felt strongly that people should challenge the government, including the United States' nascent war in Vietnam, he became disillusioned with the counter-culture's free use of drugs and rejection of family. \u201cI wanted to be able to do something tangible, to make good use of life. I didn't think we were doing that,\u201d he says. After a they spent a spell at the University of Minnesota in Minneapolis, the darker side of the movement eventually drove Gerie and Gil away from the United States altogether, to the University of British Columbia in Vancouver, Canada. There, Lonzarich became fascinated with magnetism, while working on his PhD in a new laboratory led by condensed-matter physicist Andrew Gold. When he left in 1976 for a stint at Cambridge, he found his new Rome. The collegiate structure had no real hierarchy and boasted two giants of condensed-matter physics \u2014 Brian Pippard and David Shoenberg. What was intended to be a one-year European adventure ended up lasting more than 40 years. Lonzarich arrived at Cambridge wanting to study magnets \u2014 materials in which the spins of electrons all spontaneously align. His approach raised a few eyebrows at first: he developed his own mathematical notation and would spend weeks preparing his experiments while seeming to do nothing. But his methods soon began to bear fruit. In magnetic materials, spins maintain their orderly arrangement only up to a point; above a certain temperature, electrons have so much energy that they can easily overcome the forces that cause their spins to align. Lonzarich reckoned that the best way to understand magnetic materials was to push them to that point, where they would be poised on the knife-edge between order and disorder. In particular, he was interested in exploring what might happen if the magnetic transition were shifted so that quantum effects could potentially alter the material's state. At higher pressures, the transition occurs at lower temperatures. And with enough pressure, a material can be 'tuned' so that its magnetic transition point occurs close to absolute zero. Here, thermal vibrations don't provide enough energy for the material to lose its magnetic order. Instead, quantum fluctuations \u2014 transient changes in electron properties, such as velocity and position, caused by the inherent uncertainty of the quantum world \u2014 dominate and can cause the material to switch states. In this regime, a region around a spot at absolute zero called the quantum critical point, magnetic materials become unstable and teeter on the brink of magnetism: they lack order but itch to align. With larger physical forces suppressed around the quantum critical point, ordinarily weak interactions between electrons could have huge effects. And they might, Lonzarich reasoned, give rise to new states of matter through their collective interaction. \u201cIt's like in a forest; the little plants won't grow until the big tree is cut down,\u201d he says. In particular, Lonzarich predicted that antiferromagnets \u2014 magnetic materials in which neighbouring spins align in opposite directions below a certain temperature \u2014 would become superconducting near the quantum critical point. On the verge of magnetism, he reasoned, the electrons would be so eager to align that they might spontaneously form pairs with opposite spins. Such antiparallel pairs would stick together, and their attraction to each other would stabilize their journey through the material's atomic lattice (see 'Hidden powers'). Since the mid-1980s, various theorists had suggested that such magnetically mediated superconductivity could arise, but Lonzarich's team was the first to provide solid experimental proof. When the group pushed a sample of the antiferromagnet cerium indium-3 close to the quantum critical point by cooling it at high pressure, the researchers saw it flip into a superconducting phase \u2014 something never before seen in a magnetic material 1 . The work, which was performed in 1994, demonstrated a new category of superconductor. It also provided a road map by which to search for other superconducting materials. Today, physicists routinely push the phase transition in magnetic materials down to absolute zero to see whether this behaviour emerges. \n               New terrain \n             The quantum critical point, and the strong quantum interactions that can take place around it, can give rise to other exotic states, not just superconductivity. \u201cIt's like a breeding ground for discovering new states of matter,\u201d says Cambridge physicist Stephen Rowley. Physicists around the world now manipulate a range of different factors \u2014 pressure, magnetic fields and chemical composition \u2014 to push phase transitions towards lower temperatures and so approach a quantum critical point. In the late 1990s, this method led Lonzarich and then-student Christian Pfleiderer to discover strange behaviour in the material manganese silicide 2 . Experiments done in the past few years have hinted that this may be connected to swirling two-dimensional magnetic vortices, known as skyrmions, that were later described by Pfleiderer and his colleagues 3  and are now being touted as a super-efficient way to store information. By probing around a quantum critical point of strontium ruthenate oxide, in 2007 Mackenzie and his team confirmed the existence of a new phase of matter, in which electrons flow but still show an orderly spatial structure 4 . Fellow physicists say Lonzarich is unique in that he is not only a good theorist but also an exceptional experimenter. \u201cYou have to look back to Enrico Fermi to someone able to think so deeply about theory and do really good experiments,\u201d says David Pines, a physicist and distinguished research professor at the University of California, Davis. Lonzarich grows his own samples to extreme levels of purity and pioneered a technique, known as quantum oscillation, that allows physicists to determine the electronic structure of complex, interacting systems 5 . Patricia Alireza, who runs the high-pressure laboratory at Cambridge's Cavendish Laboratory, says that Lonzarich will often encourage her to create devices that squeeze samples well beyond what was thought possible. \u201cGil will smile and say, 'I think we could probably do a factor of 100 better than that',\u201d she says. \u201cAnd you know what? We always do.\u201d Many of Lonzarich's students have continued in physics and flourished. Suchitra Sebastian, for example, led work with Lonzarich a few years ago on samarium hexaboride, an insulator that exhibits metal-like behaviour when exposed to strong magnetic fields 6 . She says that without his advice she would probably have left the field. \u201cHe is not just teaching you 'this is how you do physics' but 'this is how to survive in the world of physics',\u201d she says. Lonzarich is modest about how much he contributed to the success of those he has mentored, saying that they taught him at least as much as the other way around. One thing he always has for people is time, says Rowley. It helps that he is adept at escaping unnecessary bureaucracy, adds Pines. \u201cHe has many different offices so he can always hide at one.\u201d But Lonzarich's freedom to think is largely enabled by his wife, Gerie. She ensures that grant applications are handed in on time and that flights are caught. Gil says his wife is like the Sun: \u201cSo big and important that sometimes you forget it's the reason everything is there.\u201d \n               Lingering mystery \n             In the past few years, Lonzarich's ideas about the intimate link between superconductivity and magnetism have gained new relevance. Physicists explain conventional superconductivity using BCS theory 7 , named after the initials of the surnames of the three people who published it in 1957. The theory states that an electron speeding through some materials creates a positively charged distortion in the atomic lattice behind it. This pulls in a second electron, which follows the first like a cyclist riding in a competitor's slipstream. If enough of these relatively stable 'Cooper pairs' form, they create an ordered state in which the two electrons keep one another on course and flow without resistance. But this explanation cannot account for sandwiches of copper-based insulators known as cuprates and for  iron-based semiconductors . These two classes of superconductor can carry currents without resistance at temperatures up to 133 kelvin. If such transitions can be boosted to room temperature, around 300 kelvin, those superconductors could allow for cheaper energy, medical imaging and transportation. But  debate about how they work has raged for 30 years . From the start, one camp thought that magnetic interaction \u2014 which can be more resilient to temperature than are interactions caused by distortions in the lattice \u2014 might somehow bind electrons together to create superconductivity in cuprates. Lonzarich theorized that this magnetic glue might stem from the same quantum fluctuations that ramp up around antiferromagnet quantum critical points. This idea is now hotly debated, and gained some supporting evidence last year in experiments conducted by Taillefer's team, with collaborators at the  National Laboratory for Intense Magnetic Fields  in Toulouse, France. The group found that stripping a cuprate of its superconductivity with a powerful magnetic field and adding increasing levels of impurities revealed a sharp phase transition \u2014 an otherwise hidden quantum critical point 8 . Although the precise nature of that point is still not clear, it seems likely that antiferromagnetic correlations are at play, says Taillefer. \u201cWhich would mean Gil had a hell of an intuition,\u201d he says. Lonzarich is now looking beyond conventional high-temperature superconductors. With Rowley and other colleagues, he is examining the nature of ferroelectrics, a little-studied class of ionic materials that generate their own electric field. At low temperature, ferroelectrics can become superconductors in a manner that parallels how superconductivity emerges in magnetic materials. Lonzarich has a hunch that in ferroelectric materials that also exhibit magnetism, electron pairs bind so strongly that the state could survive to room temperature. The Universe is richer than most scientists give it credit for, Lonzarich says. Each newly discovered state of matter emerges only when conditions are right and a material is sufficiently pure. Lonzarich speculates that probing the boundaries around those states could reveal more phases, and studying the boundaries of those could reveal yet more, with discoveries unfolding in a fractal manner. \u201cWhat if each quantum critical point is just the beginning of another generation? There's some indication we're heading in that direction,\u201d he says. The idea is highly speculative, but Taillefer says people would be wise to listen. The notion that a now-familiar principle could hide deep, complex behaviour \u201cis typical Gil\u201d, he says. \u201cI would definitely put my money on him.\u201d \n                 Tweet \n                 Follow @NatureNews \n                 Follow @LizzieGibney \n               \n                     Superconductor breaks high-temperature record 2012-Feb-22 \n                   \n                     High-temperature superconductivity at 25: Still in suspense 2011-Jul-20 \n                   \n                     Superconductivity fights back 2006-Oct-10 \n                   \n                     Quantum criticality 2005-Jan-19 \n                   \n                     Shoenberg Laboratory for Quantum Matter \n                   Reprints and Permissions"},
{"file_id": "549445a", "url": "https://www.nature.com/articles/549445a", "year": 2017, "authors": [{"name": "Asher Mullard"}], "parsed_as_year": "2006_or_before", "body": "How machine learning and big data are helping chemists search the vast chemical universe for better medicines. In 2016, the pharmaceutical firm Sunovion gave a group of seasoned employees an unusual assignment. At the firm's headquarters in Marlborough, Massachusetts, the chemists were all asked to play a game to see who could discover the best leads for new drugs. On their workstations was a grid of hundreds of chemical structures, just ten of which were labelled with information on their biological effects. The experts had to select other molecules that could turn out to be drug candidates, using their hard-earned knowledge of chemical structure and biology. Of the 11 players, 10 struggled through the task for several hours. But one breezed through in milliseconds \u2014 because it was an algorithm. That computer program was the brainchild of Willem van Hoorn, head of chemoinformatics at Exscientia, a start-up that uses artificial intelligence (AI) to design drugs. The firm, based in Dundee, UK, wanted to extend a nascent partnership with Sunovion, so the stakes were high. \u201cMy credibility was on the line,\u201d says van Hoorn. Twenty rounds of gameplay later, he tallied up the points. Relief swept over him. His algorithm had mastered at least some of the dark arts of chemistry; only one drug-hunting expert had beaten the machine. Exscientia and Sunovion have continued to work together to discover psychiatric drugs ever since. \u201cThis competition really helped to get buy-in from the people who make the chemistry research decisions,\u201d says Scott Brown, Sunovion's director of computational chemistry. Exscientia is just one of a growing number of groups in industry and academia that are turning to computers to explore the mind-bogglingly large chemical universe. Chemists estimate that 10 60  compounds with drug-like characteristics could be made \u2014 that's more small molecules than there are atoms in the Solar System. The hope is that algorithms will catalogue, characterize and compare the properties of millions of compounds  in silico  to help researchers quickly and affordably find the best drug candidates for a target. Proponents argue that these strategies could make medicines safer, ensure that fewer drugs fail in clinical trials and enable the discovery of new classes of therapeutics. They could also help to open up areas of chemical space left unexplored or assumed to be barren. But many medicinal chemists remain sceptical of the hype, unconvinced that the ineffable complexity of chemistry can be reduced to mere lines of code. Even advocates of AI acknowledge that many attempts have fallen flat: computer-generated compounds can be riddled with components that are difficult to make, such as 3- or 4-atom rings, and infested with reactive groups that would set off safety alarms. \u201cThe execution of some computational approaches can suffer badly when researchers just don't know the field,\u201d says van Hoorn. \u201cThe compounds they come up with are just laughable.\u201d But he says that an expert human touch could yet tame these overzealous digital designers. \u201cI think some of these ideas could work if the computer scientists would just collaborate with people who actually breathe chemistry.\u201d \n               Space exploration \n             To navigate the chemical universe, it helps to have a map. In 2001, chemist Jean-Louis Reymond, at the University of Berne in Switzerland, started using computers to chart as much of the massive space as possible. Sixteen years on, he has amassed the largest database of small molecules in the world, a gigantic virtual collection of 166 billion compounds. The database, called GDB-17, includes all the chemically feasible organic molecules made of up to 17 atoms \u2014 as many as Reymond's computers could cope with. \u201cJust for a computer to compile a list of the compounds in the database would now take over 10 hours,\u201d says Reymond. To make sense of this plethora of possible drug starting points, Reymond has come up with a way to organize his chemical universe. Taking inspiration from the periodic table, he has grouped compounds in a multidimensional space in which neighbouring compounds have related properties. Positions are assigned according to 42 characteristics, such as how many carbon atoms each compound has. For each drug that has made it to market, there are millions of compounds that are chemically almost identical to it \u2014 just sporting an extra hydrogen here or double bond there. And some of these will work better than the drug that was approved. Chemists couldn't possibly conceive of all of these variations unaided. \u201cThere is no way you can get at these isomers using a pen and a piece of paper,\u201d says Reymond. But Reymond and his team can identify therapeutically promising 'near neighbours' of proven drugs by searching for similarities between compounds. By using a particular drug as a starting point, the team can comb through all 166 billion compounds in the database for compelling follow-on candidates in just 3 minutes. In a proof-of-principle experiment, Reymond started with a known molecule that binds the nicotinic acetylcholine receptor, a useful target for disorders involving the nervous system or muscle function, and compiled a shortlist of 344 related compounds. The team synthesized three, and found that two could activate the receptor potently, and could be useful for treating muscular atrophy in ageing 1 . The approach is like using a geological map to work out where to dig for gold, Reymond says. \u201cYou need some way to choose where you are going to dig,\u201d he says. An alternative approach uses computers to pan lots of locations for gold without worrying too much about the starting location. In drug-hunting terms, this means screening vast chemical libraries  in silico  to find small molecules that bind to a given protein. First, researchers have to take a snapshot of a protein using X-ray crystallography to determine the shape of its binding site. Then, using molecular-docking algorithms, computational chemists can chug through compound collections to find the best fits for any given site. As computing power has exploded, the capabilities of these algorithms have improved. Chemists at the University of California, San Francisco, led by Brian Shoichet, showcased the potential of this approach in 2016 in a search for a new class of painkiller. The team screened more than 3 million commercially available compounds to find candidates that would selectively activate \u03bc-opioid receptor signalling to relieve pain without disturbing the closely related \u03b2-arrestin signalling pathway \u2014 which is thought to be associated with opioid side effects including a lowered breathing rate and constipation. The researchers quickly whittled down a massive compound library to just 23 highly ranked compounds for follow-up 2 . In a test tube, seven of the candidates had the desired activity. Further development turned one of these into PZM21, a compound that acts on the \u03bc-opioid receptor without activating \u03b2-arrestin. The biotechnology firm Epiodyne, based in San Francisco, California, and co-founded by Shoichet, is now trying to develop a safer painkiller based on the findings. Shoichet plans to use the same approach to find compounds that modulate other G-protein-coupled receptors (GPCRs), a family of proteins that accounts for an estimated 40% of drug targets. His team is also running similar experiments with a virtual nebula of 100 million compounds that have never been made before but that should be easy to synthesize. Industry drug developers are also testing out this approach: the biotech firm Nimbus Therapeutics, based in Cambridge, Massachusetts, incorporates into its docking screens virtual compounds with characteristics of naturally occurring chemicals that usually have to be laboriously sourced from natural environments such as soil. The jury is still out on whether these will lead to drugs, but Don Nicholson, chief executive of the company, says that for at least one drug-design programme, \u201cthis is where all our hits are coming from\u201d. Preliminary results from such virtual screens are shaking one of Shoichet's core assumptions about chemical space: that it's only worth looking in established, drug-rich regions. Well-characterized galaxies of molecules are so awash with biologically active compounds that some argue it is a waste of time searching elsewhere. \u201cThroughout my career I have believed that line of reasoning. It just made sense, even if there wasn't that much evidence to support it,\u201d says Shoichet. But unpublished results from his screens of 100 million compounds are stoking his interest in the less-explored regions of chemical space. \u201cI'm starting to think that those galaxies are full of gold.\u201d \n               In silico \n                insight \n             These data-searching approaches are tried and tested, but the computers involved can follow only scripted instructions. The latest frontier in computational drug discovery is machine learning, in which algorithms use data and experience to teach themselves which compounds bind to which targets, finding patterns that are invisible to the human eye. Around a dozen firms have sprung up to create drug-hunting algorithms that they can test in partnership with large pharmaceutical companies. Andrew Hopkins, chief executive of Exscientia, makes a strong case for the power of these approaches. It takes on average 4.5 years to discover and optimize candidates for preclinical testing 3 , and chemists often synthesize thousands of compounds to get to a promising lead (which even then has only a slim chance of making it to market). Exscientia's approach \u2014 which uses various algorithms, including the one that impressed Sunovion's research and development executives \u2014 may be able to reduce this timeline to just one year, and shrink the number of compounds that a drug-discovery campaign needs to consider. In 2015, Exscientia finished a 12-month campaign for Sumitomo Dainippon Pharma, which owns Sunovion and is based in Osaka, Japan. The researchers trained their AI tools to find small molecules that modulate two GPCRs at the same time, and found they needed to synthesize fewer than 400 compounds in order to identify a good candidate. The drug that emerged is now moving towards clinical trials for psychiatric disease, says Hopkins. Since May, the company has inked deals worth hundreds of millions of dollars with Sanofi, based in Paris, and GlaxoSmithKline, based in Brentford, UK. In addition to identifying leads, machine-learning algorithms can also help drug developers to decide early on which compounds to kill, says Brandon Allgood, chief technology officer of Numerate, an AI drug-design firm based in San Bruno, California. There's no point in making and testing a compound if it's going to fail on toxicity or absorption testing a few months later, he says. With AI, \u201cit takes just a millisecond to rule it in or out\u201d, says Allgood, who trained as a cosmologist before he started using AI tools to study the chemical cosmos. Numerate has struck two deals with pharmaceutical companies this year, including one with Servier, based in Suresnes, France, to put AI-discovered drugs through clinical trials for heart failure and arrhythmias. Industry investment is blossoming, but computational approaches still have a lot to prove. Reymond's collection is gigantic compared with other libraries, but it covers the minutest fraction of the chemical universe (see 'Chemical cosmos'). Despite the 166 billion compounds in his database, he still has further to go in his quest than an astronomer who is trying to count all the stars in the night sky but has only managed to record one. Screens that rely on matching proteins with drugs need accurate crystal structures to yield the best results, and these data take time, money and expertise to generate. These methods also struggle to cope with proteins in motion and they cannot rank their suggestions very well. Machine-learning algorithms, for their part, are only as good as the training data sets that they are based on, performing particularly poorly when they encounter compounds that look unlike molecules they have seen before. What's more, the programs run as black boxes, and cannot indicate why they predict a compound will be a good fit. Many computational approaches also have an annoying habit of suggesting candidates that are nightmares to cook up in a lab. Chemists must then laboriously figure out a recipe for the suggested compound, which can take months or more. Even then, there is no guarantee that the molecule will work once it is made. Reymond's approach predicts a compound's activity profile correctly only 5\u201310% of the time, and that means chemists have to toil away on up to 20 compounds to find one that acts as expected. \u201cI would say the bottleneck in our exploration of chemical space is the ability to dare to make compounds,\u201d says Reymond. To this end, he recently shaved his chemical universe down to a shortlist of 10 million molecules that are easy to make, and yet still cover a broad range of properties. Mark Murcko, chief scientific officer at Relay Therapeutics in Cambridge, Massachusetts, thinks computational chemists should focus less on coming up with new algorithmic strategies, and more on improving the data sets they learn from. \u201cOne of the best ways that I know of to make a predictive model better is to keep feeding it more and more, and better and better, data,\u201d he says. Relay and others have bench chemists working closely with computational scientists, synthesizing compounds proposed by both humans and algorithms and using the resulting findings to inform future decisions. For Hopkins, such collaborations are key. It took decades for computer scientists to write programs that could compete with chess grandmasters. Then, in 1997, IBM's Deep Blue beat Garry Kasparov. But the loss did not mark the end of chess. Instead, Kasparov created a doubles version in which each team consists of a human player and an AI. \u201cTogether the human and AI can outperform any human, but they can also outperform any algorithm,\u201d says Hopkins. He wants the same mix of data-crunching, creativity and common sense to transform drug discovery. \u201cI believe we are at the Kasparov\u2013Deep Blue moment.\u201d \n                 Tweet \n                 Follow @NatureNews \n               \n                     AI-powered drug discovery captures pharma interest 2017-Jul-12 \n                   \n                     Can we open the black box of AI? 2016-Oct-05 \n                   \n                     Project ranks billions of drug interactions 2013-Nov-26 \n                   \n                     Virtual screening of chemical libraries 2004-Dec-15 \n                   \n                     Nature  Special: Crystallography \n                   \n                     \n                         Nature Reviews Drug Discovery  \n                       \n                   \n                     \n                         Nature Biotechnology  \n                       \n                   \n                     Jean-Louis Reymond: Chemical space browsers \n                   \n                     Brian Shoichet's lab \n                   \n                     Exscientia \n                   Reprints and Permissions"},
{"file_id": "550022a", "url": "https://www.nature.com/articles/550022a", "year": 2017, "authors": [{"name": "Emma Marris"}], "parsed_as_year": "2006_or_before", "body": "The seasteading movement is getting close to building its first prototype, an artificial archipelago where people will live, play and do research. The view is unbeatable. To the right, steep volcanic mountains, draped in green, rise up from a beachside coconut grove. To the left, the Pacific Ocean glitters turquoise under the midday sun. It is here in this Tahitian lagoon that a group of entrepreneurs plans to build an artificial island \u2014 three-quarters of a hectare of floating housing and research space, made up of linked platforms. If the team is successful, the vision could become reality by 2020. But it would be just the first step, says self-described \u201cseavangelist\u201d Joe Quirk. The ultimate goal is to build whole sovereign nations on the open seas, composed of modular floating units. \u201cFrench Polynesia has all the stepping stones: lagoons, atolls, shallow waters right next to deeper waters,\u201d Quirk says. Quirk, one of five managing directors for the company behind the project, and his colleagues propose that artificial islands could serve as laboratories for testing out new technologies and exploring different social structures, or act as life rafts for coastal peoples displaced by sea-level rise. The non-profit Seasteading Institute was founded by former Google engineer Patri Friedman in 2008, and it has garnered support from influential people in the linked worlds of Silicon Valley, libertarian politics and the anything-goes desert festival, Burning Man. Most media reports have been sceptical, however. The project has been characterized as the dream of \u201ctwo guys with a blog and a love of Ayn Rand\u201d 1  and \u201ca hacker's approach to government with a  Waterworld -esque conception of Manifest Destiny\u201d 2 . But the Seasteading Institute and the new for-profit spin-off, Blue Frontiers, have racked up some real-world achievements in the past year. They signed a memorandum of understanding with the government of French Polynesia in January that lays the groundwork for the construction of their prototype. And they gained momentum from a conference of interested parties in Tahiti in May, which hundreds of people attended. The project's focus has shifted from building a libertarian oasis to hosting experiments in governance styles and showcasing a smorgasbord of sustainable technologies for, among other things, desalination, renewable energy and floating food-production. The shift has brought some gravitas to the undertaking, and some ecologists have taken interest in the possibilities of full-time floating laboratories. Reporter Geoff Marsh investigates ambitious plans to build artificial floating cities. But the project still faces some formidable challenges. The team must convince the people of French Polynesia that the synthetic islands will benefit them; it must raise enough money to actually build the prototype, which it estimates will cost up to US$60 million; and once it is built, the group must convince the world that artificial floating islands are more than just a gimmick. Producing solid science and broadly useful technology would go a long way towards making that case. \u201cWhat we are dreaming is that this structure will be a scientific laboratory,\u201d says Winiki Sage, head of the Economic, Social, and Cultural Council of French Polynesia in Tahiti, who has been concerned about brain drain from his country. \n               Aesthetic appeal \n             Designs are surfacing for the prototype island, and its look is a key part of Blue Frontiers's public-relations strategy. The company's current plans don't entirely align with the concept art on the Seasteading Institute's website, which swings from tiki bar to Tomorrowland in various iterations. Bart Roeffen, a 'water pioneer' at the Dutch design firm Blue21 in Delft, has been drawing up new plans that fit with the landscape and culture. \u201cWe are working together with Tahitian designers to make something that is not like an alien invasion,\u201d Roeffen says. In particular, he plans to take cues from Polynesian shipbuilding. The elegant outrigger canoes, or  va'a , used by islanders are stable and light; oceangoing versions are the type of boat rowed by the Tahitian voyagers who discovered Hawaii and New Zealand around  AD  1100. Linked platforms would be arranged to ensure that no coral below is completely shaded and killed. The goal is to actually expand the habitat for reef species (see 'Seasteaders in paradise'). The team would not provide direct information about funding. Paypal founder and one-time Donald Trump enthusiast Peter Thiel provided a reported $1.7 million to the Seasteading Institute, but he last contributed to the project in 2014, and any recent investors are keeping a low profile. Quirk says that they have \u201ca nice amount\u201d of seed money and are preparing for what is called an initial coin offering \u2014 an i nvestment mechanism that uses digital cryptocurrency . Looking ahead, the company hopes to generate revenue by renting out space on the island and acting as consultants for other would-be island builders. Along with hiring Quirk and the other four managing directors, Blue Frontiers has recruited ten staff members and commissioned environmental, legal and economic studies on the impacts of the project for investors and the government. The \u201cwhy?\u201d \u2014 everyone's first question about seasteading \u2014 is answered differently by everyone involved. Some are captivated by the project because it is an excuse to push sustainable design to the next level. For people on low-lying islands, it looks like a life raft. F\u00e9lix Tokoragi, mayor of Makemo, an atoll in the Tuamotu archipelago in French Polynesia, told Blue Frontiers that he's interested. The Tuamotus have experienced widespread flooding, and Tokoragi is worried that his  people will become climate-change refugees . \u201cWe are attached to our atoll; we are attached to our culture,\u201d he says. \u201cWe are not against this idea, since the technology can respond to the problems that we face.\u201d For others, the pull of the project comes down to autonomy and self-reliance, particularly with respect to governance: anyone who decides their island's political style is not for them can detach and depart for another system that they like better. For at least one scientist advising the project, Neil Davies, executive director of a field station of the University of California, Berkeley, on the neighbouring island of Moorea, the island's appeal is as a base for research that would \u201cfill the gap between oceanographic-research vessels and coastal marine labs\u201d. Ships are on the water, but they are \u201cphenomenally expensive\u201d, he says, and they don't stay put. Coastal labs can gather long time-series of data in one place, but don't provide access to deeper water. Davies dreams about floating \u201csea stations\u201d that would allow low-cost, long-term access to the ocean for research, especially for students in tropical countries \u201cwhere natural systems are among the most sensitive to human activities\u201d, he says. Experiments could include modifying pH or temperature on small sections of a reef to simulate future environmental conditions, and 'planting' different corals to investigate  which will thrive best in the future . Data could be gathered using semi-permanent sensors and cameras, along with regular biological-sample collection. Some scientists not involved in the project see value in the concept, as well. \u201cIf you have a floating island and you want long-term study, that is a perfect way to do it,\u201d says Ross Barnes, marine-operations superintendent at the University of Hawaii Marine Center in Honolulu, who oversees two large research vessels and on-shore labs. The university has been conducting research at a spot in the ocean that it calls Station ALOHA, which scientists have visited nearly 300 times by boat since 1988. A floating platform, he says, would mean that scientists could leave behind some instruments \u2014 and that some of them could stay as well \u2014 allowing for continuous measurement. \u201cIt's a good idea,\u201d Barnes says. Currently, Davies is advising the seasteaders on site selection and environmentally positive design choices. He also plans to help them to document the installation's performance using sensors that measure things such as energy expenditure and waste generation on the platforms, as well as water temperature and quality. And he sees it as a great teaching opportunity for the many students who visit his station. \u201cSeasteading raises many social, legal, ethical, environmental issues, even if it never gets anywhere,\u201d he says. Whether the seasteaders make progress depends on whether the project is embraced by French Polynesia, a largely autonomous 'overseas collectivity' of France with a population of 287,000 on 67 islands spread out across an area nearly the size of Europe. At one level, a grand floating project could appeal to a nation of voyagers and boat builders. But French Polynesia has been burnt by big-science and technology projects before. From 1966 to 1996, France conducted 193 nuclear tests in its Polynesian possessions, many in the atmosphere. In February 2016, then-president of France Fran\u00e7ois Hollande admitted that the testing had harmed the environment and human health. And the place is littered with defunct projects and closed hotels. \u201cWe have a history of being taken for fools,\u201d says Pauline Sillinger, a sustainable-development specialist at Te Ora Naho, a federation of environmental groups in French Polynesia, who took a job with Blue Frontiers this year, and also teaches Tahitian dance. \u201cNuclear testing, big hotels, nice, smiling, white, intelligent people telling us it'll be good for us.\u201d But their wariness vies against their desperation for new revenue streams, Sage says. After winding down nuclear testing, France began paying French Polynesia more than US$100 million per year in compensation for lost income from military activity. But in 2016, that amount was reduced. Meanwhile, tourism revenues have never recovered from the 2008 recession. Thanks to increased political stability and other factors, things have improved since 2014, when the collectivity was so broke that it risked not being able to pay its civil servants, according to Sage. But it is still dangerously reliant on a small number of income sources \u2014 tourism, pearls, coconut oil. Unemployment stands at nearly 18%. \u201cWe are looking for new ideas,\u201d Sage says. \u201cWe are really open to any ideas, any investors.\u201d If Sage is sceptical but willing to give it a shot, there are others who have had enough of grandiose project ideas. Among them is a religious leader in Tahiti, Fr\u00e8re Maxime Chan, who heads Association 193, which advocates on behalf of those harmed by nuclear testing. Chan is also vice-president of Te Ora Naho. (Sage, incidentally, is the organization's president.) Chan says that his old friend Sage and the rest of the government are \u201cdazzled\u201d by the flash and money of the Seasteaders. He talks about recent projects \u2014 including a tourist resort, an aquaculture scheme and an eco-resort \u2014 that were all announced with fanfare and optimistic job projections, only to be cancelled, scaled back or put on indefinite hold. Chan wishes the government would admit that the standard of living for the average Tahitian has been artificially inflated by nuclear-test payments and must come down. This can be done without suffering, Chan contends, by gracefully returning to a version of the pre-1960s subsistence economy. \u201cSmall is beautiful,\u201d he says. Convincing French Polynesia to support the project will fall mainly to Marc Collins, another managing director of Blue Frontiers. Collins is Tahitian and lives there now, but in the early 1990s he lived in Silicon Valley, and fell in love with its fast-paced culture of big ideas and endless possibility. Ever since, he's kept his toe in those waters in part by maintaining a subscription to  Wired  magazine. In May 2015, the digital lifestyle glossy ran a story 3  about how the seasteading movement planned to scale back its grand, high-seas concept, reorienting towards safer, shallower waters and looking for \u201ccost-reducing solutions within the territorial waters of a host nation\u201d. Collins, a serial entrepreneur who has dabbled in every major French Polynesian industry, from hotels to black pearls and telecommunications, saw an opportunity to, as he puts it, \u201cbring some of the DNA of Silicon Valley to Tahiti\u201d. Tahiti joined the world of high-speed Internet in 2010, with the completion of an undersea fibre-optic cable linking it to Hawaii. It has calm lagoons aplenty and daily flights from Los Angeles, California, and, as a minor bonus, is widely regarded as paradise on Earth. Collins fired off a LinkedIn request to the Seasteading Institute's executive director, Randolph Hencken. The Seasteaders were interested in Collins's pitch, but they wanted a more official gesture of support. So Collins, who served as French Polynesia's minister of tourism in 2007 and 2008, began working his government contacts. By August, the president of French Polynesia, \u00c9douard Fritch, signed a letter formally inviting the Seasteaders to present their ideas. A delegation of nine took him up on the offer the next month, and by January, a memorandum of understanding with pledges of cooperation was signed. The next step in making the island a reality will be the passage of a law defining the 'special economic zone' that will cover the synthetic island. Blue Frontiers isn't asking French Polynesia for any subsidies to build the island, but it is asking for a 0% tax rate, among other regulatory exceptions. It has hired French firm GB2A, based in Paris, to prepare legal research and a set of requests, which Blue Frontiers presented to the government at the end of September. The team hopes to see a bill emerge before the end of the year. In the meantime, the Seasteading Institute is building excitement and courting potential investors with a series of gatherings. In May, it held talks, networking events and tours in Tahiti. Speakers included Fritch; Tony Hsieh, chief executive of online retailer Zappos in Las Vegas, Nevada; Tua Pittman, a master canoe navigator from the Cook Islands; and engineers, nanotechnologists and a 'blockchain strategist', a specialist in the distributed information systems behind cryptocurrencies. The seasteaders hope to use such systems to handle their financials, as well as any scientific data that they generate. But the event wasn't all work. An announcement for a party on outrigger canoes cheerfully suggested: \u201cDo not wear heels. Bring a swimsuit for an optional moonlight swim.\u201d On 22\u201329 October, Blue Frontiers will hold an Insiders Access Week for supporters and potential investors, a mix of tours, discussion and morning yoga with Hencken. Always ambitious, the team hopes to have draft legislation from the Polynesian government by then, and some detailed architectural plans. The goal is to break ground \u2014 or rather, sea \u2014 in 2018. While all this work goes on behind the scenes, the lagoon remains fairly quiet. On a day in July, locals compete in a stand-up paddle-board race while families play on the shore and young women drink beer with their feet in the waves. By the roadside, freshly caught tuna are for sale. On one level, it is hard to imagine this place being improved upon. Time will tell whether the Seasteaders' island becomes a refuge for Polynesians facing rising seas and an incubator for Polynesian science and business, or merely a playground for wealthy foreigners who want to dodge bothersome regulations. That is, if it materializes at all. \n                     Bacteria could be key to freeing South Pacific of mosquitoes 2017-Aug-01 \n                   \n                     Before we drown we may die of thirst 2015-Oct-28 \n                   \n                     Tropical paradise inspires virtual ecology lab 2015-Jan-14 \n                   \n                     Climate-change adaptation: Designer reefs 2014-Apr-23 \n                   \n                     Stone tool reveals lengthy Polynesian voyage 2007-Sep-27 \n                   \n                     Seasteading Institute \n                   \n                     Blue Frontiers \n                   Reprints and Permissions"},
{"file_id": "d41586-017-08548-z", "url": "https://www.nature.com/articles/d41586-017-08548-z", "year": 2017, "authors": [], "parsed_as_year": "2006_or_before", "body": ""},
{"file_id": "548384a", "url": "https://www.nature.com/articles/548384a", "year": 2017, "authors": [{"name": "Jane Palmer"}], "parsed_as_year": "2006_or_before", "body": "Scientists investigate why mountain slopes can slip slowly for years and then suddenly speed up, with potentially fatal effects. The explosive volcano Sabancaya looms large over the hamlet of Maca in southern Peru. Smoky plumes frequently spew from the mountain and sometimes rain ash down on the town. But the geological threat that keeps Maca's residents up at night is not 15 kilometres away at Sabancaya, but right underfoot. For about 30 years, the very ground around Maca has been in retreat. Some 60 million cubic metres of earth \u2014 the equivalent of more than 20,000 Olympic swimming pools \u2014 are slinking their way to the valley floor and taking part of the town with them. The impact of this slow-moving landslide is clearly visible: in recent years, it has destroyed a section of the region's main road and torn apart farmland, threatening the community's key source of income. What is not clear is the landslide's future: whether it will continue to lurch along as it always has or speed up dramatically, potentially endangering lives. \u201cIt's like a sword of Damocles hanging over the town,\u201d says Pascal Lacroix, a geoscientist at the Institute for Earth Science in Grenoble, France. In some ways, Maca is not unique. Landslides are a major threat in all the mountainous regions of the world,  killing thousands of people each year . Many, such as the devastating Regent slide in Sierra Leone last week, are fast and furious. Others, like the one threatening Maca, play out slowly, with hillsides slipping a few centimetres to a few metres each year. Those slow-moving landslides rarely claim lives, but can inflict serious damage on houses and infrastructure. And although they may amble along for months or years, they can also speed up unexpectedly, with sometimes fatal consequences. The  Vajont landslide  in the Italian Alps, for example, moved slowly for at least 3 years before accelerating and killing as many as 2,500 people in 1963. More recently, the  2014 Oso slide  in Washington state killed 43 people. In the weeks before, some residents  reportedly saw the slow creep of the land above . And after two years of slow movement, the Mud Creek landslide in Big Sur, California,  rapidly wiped out half a kilometre of the state's popular Highway 1  in May this year. Such speed-ups may be common. It's almost certain that every fast landslide started as a slow one, says William Schulz of the Landslides Hazards Program at the United States Geological Survey (USGS) in Golden, Colorado. \u201cJust no one noticed.\u201d William Schulz explains why research on slow landslides is so important. Now, geologists are noticing. They have, in recent years, begun to take advantage of improved sensor technology \u2014 as well as advances in radar imaging and laser ranging \u2014 to study these slow-motion natural disasters. And they are poised to identify more slow landslides with the help of the European Space Agency's two Sentinel-2 satellites, the second of which launched in March. These eyes in the sky can between them snap a picture of the same portion of the Earth every five days, allowing scientists to spot new areas of movement and monitor them for changes that could herald a dangerous slide. Ultimately, scientists hope to develop enough of an understanding of these lethargic landslides to be able to predict when they might pick up speed. \u201cOne of the great questions really, for us as geologists, is under what circumstances does a slow-moving landslide become a catastrophic landslide,\u201d says natural-hazards specialist Dave Petley of the University of Sheffield, UK. \u201cI think technology is giving us insights that we've struggled to gain for a long time.\u201d \n               Science in slow motion \n             The world's most well-studied slow landslide is hard to miss amid the San Juan mountains of southwestern Colorado. A congealed mass of yellow soil and boulders topped by tilted trees sticks out amid the forest-covered slopes. Called the Slumgullion slide, it has an active zone that snakes its way nearly 4 kilometres down a hillside. Scientists have spent more than a century studying this 20-million-cubic-metre mess, named by settlers to the region after an everything-but-the-kitchen-sink-style stew. In places, it is hard to walk more than a few metres without tripping over a field computer, a sensor or one of the many other instruments that dot its surface. Slumgullion is a sensitive beast. Although it has trundled along fairly steadily for the past 300 years, its pace changes dramatically on a daily basis in response to snowmelt, rainfall and even the  ebb and flow of air pressure. The main mover of earth \u2014 here and elsewhere \u2014 is water. Researchers can see that clearly in data they collected from ground overlooking Slumgullion using Interferometric synthetic aperture radar (InSAR). InSAR compares radar images of the land taken at different times to determine how its surface has deformed. When scientists used it to track the slide's movement, they found 1  that a mere quarter of a millimetre of rain can cause the ground to speed up by a factor of five within just a few hours. Minor changes in groundwater pressure are enough to trigger the change. The fact that a large, complex landslide such as Slumgullion reacts to the slightest amount of rain helps to explain why the more common, smaller slides can be even more sensitive, says Schulz. A two-metre-deep slide \u2014 less than one-fifth the depth of Slumgullion \u2014 can start swiftly because rainfall can destabilize its full volume rapidly. And because it is shallow, it is likely to have few geometric and geological irregularities that can hinder motion. Geologists are learning that many factors determine whether a landslide moves quickly when it rains. Important influences include the shape, roughness and composition of a slide's underside, or base; the proportions of the base to the sides; and the slide's depth, especially in relation to the water table. One site that has proved particularly helpful in teasing apart the impact of geometry and other factors is northern California's Eel River basin. There, researchers have mapped approximately 200 slow-moving landslides and are currently monitoring 10 or so using InSAR. These slides all reside in almost identical geological settings and climates; only their sizes and shapes differ. At Eel River, scientists have learnt that not every landslide has a hair-trigger response to rainfall: it can rain for weeks or months before there is movement. The team has found 2  that the fine-grained composition of these landslides, rather than geometry or climate, has the largest role in determining their behaviour. \u201cIn general, fine-grained soils with clay-rich material have a slower response, and course-grained soils or rocks with large cracks or fissures have a faster response,\u201d says Alexander Handwerger, a geoscientist at NASA's Jet Propulsion Laboratory in Pasadena, California. But pulling all this information together to make predictions will be a challenge. For decades, geologists have been gathering data to better quantify the correlations between landslide motion and the duration and intensity of rainfall. But geological materials and hydrologic conditions are infinitely variable, Schulz says, and the level of accuracy needed to forecast slides is still beyond scientists' reach. What's more, in many countries, landslide motion is also heavily affected by another phenomenon whose influence hasn't been so thoroughly studied: earthquakes. \n               Earthquakes in the equation \n             Seismic shaking is a fact of life in the Colca Valley, where Maca sits. Quakes come several times a year, and the hills around the town are streaked with tan-coloured scarps \u2014 the telltale sign of fallen earth \u2014 that stand out against the green backdrop of painstakingly carved farming terraces. More than a dozen major earthquakes have hit the town since 1990. One of the most devastating in recent memory occurred in 1991, when a shallow, magnitude-5.4 earthquake destroyed the western part of the village. It killed 14 people and knocked down half of the town's centrepiece, a small white church called Santa Ana. By then, the villagers had noticed that land on the outskirts of town was on the move. As concerns grew, the Geophysical Institute of Peru in Lima installed three temporary Global Positioning System (GPS) units between 2001 and 2004 to monitor the motion. In the years since, the land has continued its march, creating fissures in farmland that widen each year and draw away water meant for crops. Unable to keep the land productive, nearly one-third of the town's population has left in the past decade. In 2011, scientists from Peru's Mining and Metallurgical Geological Institute (INGEMMET) in Arequipa, along with Lacroix and colleagues based in France, began to step up the monitoring with the installation of permanent GPS instruments. The sensors were in place in time to catch the effects of a magnitude-6 earthquake that struck during the dry season of 2013, its epicentre less than 20 kilometres from Maca. The Maca landslide is usually stationary during that part of the year, but sensors showed 3  that it moved 2 centimetres during the quake, followed by a further 6 centimetres over the next 5 weeks \u2014 a small amount for the wet season but anomalous for a time with no rainfall. Researchers observed even more curious behaviour in August 2016, when a magnitude-5.4 earthquake hit the region. Inspecting the data, Lacroix found that they mirrored the findings from three years earlier: seismic shocks sped up the Maca landslide during the quake and over the following weeks. This time, however, he observed that when the rainy season came and triggered further motion, the earth and debris moved much faster than normal for that time of year. Lacroix put together a rough model of what had happened. As seismic waves hit a slope, they can trigger movement that continues well after the original shock. In the case of Maca's landslide, he says, that movement slows down eventually, owing to an increase in friction during sliding \u2014 a behaviour that has also been proposed to describe creep along geological faults. For other slides, however, t he initial motion can reduce friction , and so help the slide to speed up. The difference comes down to both internal factors, such as the material properties of the earth, and external ones, including temperature, pressure and gravity. Landslides without sufficient braking can speed up catastrophically after an earthquake or rainfall triggers their motion. With Maca, however, the friction becomes stronger once the earth starts to slide, and works to slow it down. But this braking activity can be undermined by earthquakes. The ground-shaking can loosen the material of the landslide and weaken it. The seismic shocks can also open holes and cracks in its body, providing more pathways for water to infiltrate the soil. Lacroix thinks that earthquake damage in 2016 left the Maca landslide prone to faster movement when the next rainfall came. Further damage could potentially lead to more catastrophic speed-ups. \u201cEven a small earthquake can create large effects,\u201d he says. \n               The whole picture \n             By fitting various pieces \u2014 rainfall, geometry, earthquakes and more \u2014 into the puzzle, scientists are starting to understand what causes landslides to start, speed up or stop. But how far can observations be translated to different sites? \u201cEach case is, to some degree, unique,\u201d says Richard Iverson, a hydrologist at the USGS's Cascades Volcano Observatory in Vancouver, Washington. \u201cThe materials are so variable from place to place, and some of those nuanced differences can really make a difference in the behaviour.\u201d Schulz concurs, but he stresses that even landslides separated by great distances can share properties and behaviours. By studying a wide range of them, scientists can find the underlying rules that link environmental triggers to movement. Hundreds of residents of La Conchita, California, could attest to that point. The hillside behind their community had been moving for several months before it finally accelerated in 1995, following heavy rains. County officials and consultants, however, had been able to use data from standard surveying instruments to detect the slope's imminent failure and warn residents to evacuate. Now, remote sensors, such as the Sentinel-2 satellites, could help to extend the study and monitoring of landslides to areas that are too difficult or expensive to study on the ground. \u201cInstrumenting all slow landslides is not possible, so remote-sensing data really is the future for monitoring a whole territory,\u201d says Lacroix. Already, using such data, Lacroix and his colleagues have found 4  that there is a landslide near the village of Madrigal, 6 kilometres from Maca, that behaves much like the Maca slide during the rainy season. As in Maca, the movement is affecting the locals' ability to farm the land. But should the slide speed up, it could block the river running through the valley, forming a dammed lake. \u201cIf the dam then breached, it could create floods,\u201d Lacroix says. Aware of this possibility, the international team installed the first instrument on the Madrigal landslide in May, to monitor its behaviour. It took nearly a week to build the red-brick housing \u2014 a metre or so high \u2014 used to protect the GPS instrument from theft and weather damage. The sensor sits on farmland that has supported crops for nearly 2,000 years, the very picture of permanence and stability. But beneath it, the ground slowly creeps. \n                     Killer landslides: The lasting legacy of Nepal\u2019s quake 2016-Apr-25 \n                   \n                     Landslide risks rise up agenda 2014-Jul-15 \n                   \n                     Afghan landslide was 'an accident waiting to happen' 2014-May-06 \n                   \n                     Air tides cause landslides 2009-Nov-01 \n                   Reprints and Permissions"},
{"file_id": "549322a", "url": "https://www.nature.com/articles/549322a", "year": 2017, "authors": [{"name": "Monya Baker"}], "parsed_as_year": "2006_or_before", "body": "Networks of nanotubes may allow cells to share everything from infections and cancer to dementia-linked proteins. Yukiko Yamashita thought she knew the fruit-fly testis inside out. But when she carried out a set of experiments on the organ five years ago, it ended up leaving her flummoxed. Her group had been studying how fruit flies maintain their sperm supply and had engineered certain cells involved in the process to produce specific sets of proteins. But instead of showing up in the engineered cells, some proteins seemed to have teleported to a different group of cells entirely. Yamashita, a developmental biologist at the University of Michigan in Ann Arbor, and the postdoctoral researcher with whom she was working, Mayu Inaba, called the phenomenon \u201cmysterious trafficking\u201d. They were convinced it was real \u2014 but they couldn't understand how it worked. So they shelved the project until one day, more than a year later, Inaba presented Yamashita with some images of tiny tubes reaching out from one cell to another \u2014 delicate structures that might have been responsible for the trafficking. Yamashita was sceptical, but decided to dig out images from her own postdoc project 12 years earlier. Sure enough, slender spikes jutted out towards the targeted cells. \u201cIt was really eye-opening,\u201d Yamashita says. The group published its work in 2015, arguing that the tubes help testis cells to communicate precisely, sending a message to some of their neighbours and not others 1 . \u201cWe thought the protein was trafficked,\u201d Yamashita says, \u201cbut we didn't think there was an actual track.\u201d Yamashita's tubes joined a growing catalogue of cryptic conduits between cells. Longer tubes, reported in mammalian cells, seem to transport not just molecular signals but much larger cargo, such as viral particles, prions or even mitochondria, the cell's energy-generating structures. These observations suggest an unanticipated level of connectivity between cells, says Amin Rustom, a neurobiologist at the University of Heidelberg in Germany, who first spotted such tubes as a graduate student almost 20 years ago. If correct, he says, \u201cit would change everything in medical applications and biology, because it would change how we see tissues\u201d. But Richard Cheney, a cell biologist at the University of North Carolina in Chapel Hill, is not ready to start revising the textbooks. Cheney has followed the field and at one point collaborated with Rustom's PhD adviser. There's no question that long, thin protrusions are popping up all over the place, he says. The question is, what are they doing \u2014 sending simple messages when cells reach out and touch each other, or opening a breach and facilitating wholesale transport? \u201cI'd probably bet on contact-based signalling, where you don't need very many copies of a molecule, as opposed to them acting like interstate highways,\u201d he says. The problem with betting either way is that these tiny tubes are tough to study. Arguing that they exist at all is hard enough, let alone making the case that they actually have a function. Yamashita used the tried-and-tested genetic- engineering methods and well-characterized genes available in the fruit fly to argue that her tubes were sending signals by direct contact. But researchers looking for tubes in mammalian cells don't have those resources. More than one researcher has been accused of mistaking a scratch on a cell plate for a cell-produced nanotube. Evidence derived from real mammalian tissue is even sparser. Adam Levy takes a closer look at the tiny tubes spotted between cells. Nonetheless, there has been a recent rash of interest in the tubes. One of the believers is George Okafo, a director of emerging platforms at the drug company GlaxoSmithKline (GSK) in Stevenage, UK. He thinks that cell-to-cell protrusions could explain why diseases such as Alzheimer's disease, Parkinson's disease and malaria, as well as HIV and prion infections, are so difficult to treat (see 'Live wires'). \u201cThere's a characteristic that isn't targeted by a lot of conventional therapies, and that's how a disease spreads from cell to cell.\u201d Last September, Okafo organized an invitation-only conference to bring together GSK staff and around 40 researchers in the field. (He is now collaborating with some of them.) In March this year, the US National Institutes of Health asked for grant applications from groups studying how organelles communicate in stressed or cancerous cells, a move that excites tube enthusiasts. And in December, the American Society for Cell Biology will host a session devoted to the topic at its annual meeting. \n               Long pipeline \n             Scientists know that some cells build wire-like extensions as a kind of temporary foothold to move themselves from place to place. The first important hint that they might be involved in something more complex came in 1999, from cell biologist Thomas Kornberg at the University of California, San Francisco. He was watching fly larvae develop wings, and saw a sea of filaments projecting from the wing buds towards the signalling centre that is essential for their growth 2 . He coined the term cytoneme \u2014 or cell thread \u2014 to describe these filaments. He suggested that some cellular chatter that was thought to happen by diffusion could, in fact, be orchestrated by cytonemes. The idea was surprising and was slow to catch on, but it is now making its way into textbooks. In 2004, two research groups separately published observations of something even more radical: nanotubes in mammalian cells that seemed to move cargo such as organelles and vesicles back and forth. Rustom spotted thin, straight tubes connecting cultured rat cells after he forgot a washing step in an experiment. He and his adviser at the University of Heidelberg, Hans-Hermann Gerdes, engineered cells to make fluorescent proteins and watched the molecules flow from one cell to another. Their accidental sighting grew into a  Science  paper 3  that described the structures as \u201cnanotubular highways\u201d. (Some sceptics think that Gerdes chose the term nanotube to ride on the coat-tails of carbon nanotubes, a hot topic in materials science.) In the same year, Daniel Davis and his team at Imperial College London described networks of 'membrane nanotubes', strands of cells' outer membranes that stretched for several cell lengths to connect different types of immune cell; lipids produced by one cell showed up on the surface of another 4 . Davis attributes their discovery to his team's willingness to think through the implications of their sighting. \u201cThe crucial thing is not that we saw them,\u201d he says. \u201cThe crucial thing is deciding what you're going to dig into and investigate.\u201d His team went on to describe different sorts of nanotube, some holding vesicles and mitochondria inside, and others with bacteria 'surfing' the casing 5 . Meanwhile, other labs have reported cell-connecting tubes in neurons, epithelial cells, mesenchymal stem cells, several sorts of immune cell and multiple cancers. Further types of tube have been spotted as well. In 2010, Gerdes and his team reported that some tubes end in gap junctions: gateways that bestow the neuron-like ability to send electrical signals and can also pass along peptides and RNA molecules 6 . Yamashita speculates that such connections may be more than conceptually related to neuronal synapses. \u201cMembrane protrusions might have evolved first, and higher organisms could have started upgrading them to make neurons for more complicated functions,\u201d she says. Most researchers who study these cellular pipelines care less about their evolutionary origin than about their role in human health and disease. The strongest evidence for a role in disease came in 2015, also from a team at the University of Heidelberg, led by cancer researcher Frank Winkler. Like others, his team had not set out to study cell protrusions; they wanted to test a system for watching human gliomas grow. Cells derived from the tumours were injected into the brains of mice with windows in their skulls \u2014 hardened glass kept in place with dental cement \u2014 through which the researchers could watch the cells. As the tumour cells invaded, they sent tubular protrusions ahead of them. A closer look showed many tubes connecting cells through gap junctions. Interconnected cells managed to survive doses of radiation that killed isolated cells, apparently because gap junctions helped to spread the load of toxic ions to neighbours 7 . When radiation did kill linked tumour cells, nuclei from those cells sometimes travelled down a tube, with the tube then expanding into the cleared space to form a vigorous new cancer cell. These 'tumour microtubes' were also found in biopsies from patients, and denser, longer tubes correlated with more resistant forms of cancer and a poorer prognosis. Winkler speculates that a drug that could keep these tubes from sprouting or extending might create a new class of cancer treatment; indeed, he thinks that existing cancer drugs such as paclitaxel may work by disrupting tumour microtubes. Winkler's team has filed a patent application for a compound that interferes with microtubes as a treatment for glioma. The work has captured imaginations. \u201cIt was a seminal paper,\u201d says Okafo. \u201cPrior to that there was still some scepticism about whether these phenomena existed  in vivo .\u201d But it's not clear whether Winkler's results apply to other scenarios. Various sorts of brain cell are known to send out cell protrusions as they grow and proliferate. The tubes that Winkler's team reported are much larger than the 'tunnelling nanotubes' that were originally described by Gerdes, and, unlike most tunnelling nanotubes reported so far, contain microtubules \u2014 filaments that move components around in cells. However, Winkler thinks that his work provides evidence for a broad role for tunnelling-nanotube-like structures. He thinks they may not be able to reach full size in culture, and the tubes he does see vary considerably in length and thickness. Winkler recalls discussing his work with Gerdes before Gerdes' death in 2013. \u201cHe said that this was what the field was waiting for. It was exactly the proof that he thought we could find.\u201d In other fields, too, the tubes are gaining traction. Eliseo Eugenin, who studies HIV at Rutgers New Jersey Medical School in Newark, suggests that HIV-infected cells send out multiple nanotubes filled with virus to reach uninfected cells. Circulation and one-on-one cellular contact would be too inefficient to cause the rapid amplification of the virus seen in newly infected patients. \u201cThe mathematics don't work,\u201d he says. He thinks that other researchers are sceptical of nanotubes because they are unable to reconcile themselves to the idea that cells are constantly exchanging materials, including genetic information. \u201cOur definition of a cell is falling apart,\u201d Eugenin says. \u201cThat is why people don't believe in these tubes, because we have to change the definition of a cell.\u201d \n               Battle lines \n             When the definition of the cell is at stake, it is little wonder that scepticism remains strong. Emil Lou, a cancer researcher at the University of Minnesota in Minneapolis, says his grant proposal to hunt for and characterize nanotubes in human cancers was pooh-poohed because a reviewer was not convinced that the structures existed. Others argue that they do exist \u2014 but only in the rarefied world of the Petri dish. Michael Dustin, an immunologist at the University of Oxford, UK, says that he has seen cells in dishes form structures that would never occur in the dense tissue of an organism. For example, white blood cells primed to produce antibodies produce a \u201cbeautifully symmetric\u201d bull's-eye pattern in a dish, very different from the chaos and asymmetry they show in the body. Then there are mechanistic quibbles: some researchers think that the tubes are open at both ends, with cargo flowing in and out. But that would cause cytoplasm to mix and result in the cells fusing, says Jennifer Lippincott-Schwartz, a cell biologist at the Howard Hughes Medical Institute Janelia Research Campus in Ashburn, Virginia. \u201cThe people who think there is a connection need to talk to some biophysicists,\u201d she says. Instead, she thinks that membrane tubes may jut out and make minimal contact, just enough to allow recipient cells to reach out and engulf the tube contents. These disagreements could be contributing to a lack of rigour in the field. Chiara Zurzolo, a cell biologist at the Pasteur Institute in Paris, who has spotted prions and other neurodegenerative proteins travelling through nanotubes, says that many papers do not try to assess whether a tube is closed or open-ended, for example, or even whether the tubes allow the movement of vesicles or similar material. The proliferation of tube types, and the different names for them, make coherent discussion difficult. \u201cWe have to be rigorous in what we call these structures. At the moment it is very messy,\u201d she says. But getting clear images of living cells will always trump semantics, says Ian Smith, a cell biologist at the University of California, Irvine. \u201cWhat is really needed in the field is direct visualization of this process,\u201d he says. Most microscopy techniques can't get a clear view of these structures in action, even in cultured cells. Smith is developing methods to visualize membrane nanotubes using lattice light-sheet microscopy, which monitors planes of light to build up 3D images. He hopes that the technique will be able to capture the process of material transfer from one cell to another, from start to finish 8 . Smith admits that he's taking a career risk: a colleague recently warned him this area was 'fringey'. But he takes this as a challenge. Lou is encouraged that the criticism against membrane tubes has morphed. At first people would tell him that the structures were artefacts or optical illusions, he recalls. \u201cThen it graduated to, 'well, just because they grow in a plate doesn't mean that it has anything to do with biology', and then it was, 'well you are probably misidentifying these or mischaracterizing them'.\u201d He likes that direction. \u201cI think we have to take it seriously as a therapeutic target. I couldn't have said that five years ago. \n                     Cell biology: The new cell anatomy 2011-Nov-30 \n                   \n                     Nanotubes help cells pass messages 2010-Sep-20 \n                   \n                     Nature  special: Single-cell biology \n                   Reprints and Permissions"},
{"file_id": "549018a", "url": "https://www.nature.com/articles/549018a", "year": 2017, "authors": [{"name": "Alison Abbott"}], "parsed_as_year": "2006_or_before", "body": "With a national election this month, Germany proves that foresight and stability can power research. Ask any German researcher why the country\u2019s science base is blooming, and they are bound to mention Chancellor Angela Merkel. The world\u2019s most powerful woman, they say, has not forgotten her roots as an East German physicist.\u00a0 During a decade of global financial turbulence, her government has increased annual science budgets in a stable, predictable, quintessentially German way. It has spurred competition among universities and improved collaboration with the country\u2019s unique publicly funded research institutions. Under Merkel\u2019s watch, Germany has maintained its position as a world leader in areas such as  renewable energy and climate ; and with the guarantee of strong support for basic research, its impact in other sectors has grown.\u00a0 Foreign researchers are increasingly choosing to make their careers in Germany  rather than opting for traditional brain magnets such as the United States or the United Kingdom. With its safe-but-dull reputation, Germany is starting to look like the tortoise to their hare. And as the country prepares for a national election on 24 September, most onlookers expect the trends to continue. The reasons behind Germany\u2019s success go beyond science budgets or some sort of \u2018Merkel effect\u2019, says Wolfgang Sch\u00f6n, a director of the Max Planck Institute for Tax Law and Public Finance in Munich and vice-president of the DFG, Germany\u2019s main university-research funding agency. Like Merkel, the country has deep science roots, he says.\u00a0 Germany was a world leader in science and technology before the turbulence of the twentieth century; it established traditions that many countries still follow. Although it struggles with the remnants of male-dominated hierarchies and pervasive, inflexible regulations, German research is looking as strong as ever, particularly on a global stage that seems increasingly indifferent to science. \u201cI\u2019d love it if our science-policy and budget decision-makers in the US were willing to take lessons from Germany again today,\u201d says Kenneth Prewitt, a political scientist at Columbia University in New York City. The structure of modern German science rests on concepts developed two centuries ago by Wilhelm von Humboldt, a Prussian educator who pioneered ideas that continue to hold sway around the world. It was he, for example, who suggested that university professors should do front-line research as well as teaching. His philosophy that education should be both broad and deep, and that academic life should be free from politics and religion, remains engraved in the German psyche. \u201cThe Humboldtian system is in our DNA,\u201d says Thorsten Wilhelmy, general secretary of the Berlin Institute for Advanced Study. \u201cThat\u2019s why politicians are not so tempted to cut basic research when times get tough.\u201d (See \u2018Build, link and trust\u2019.)  \n               boxed-text \n             These ideals have weathered dramatic political upheaval. Adolf Hitler\u2019s Third Reich perverted science and led to the country\u2019s devastation in the Second World War. In 1949, Germany was refounded as two countries, which rebuilt their scientific strengths under opposing political systems. West Germany\u2019s democratic constitution, which remains in force, declared: \u201cArts and sciences, research and teaching shall be free.\u201d To ensure that centralization and abuse of power could never happen again, it created a highly federalized country in which responsibility for culture, science and education lies with the  L\u00e4nder , or states \u2014 a feature that was to have negative as well as positive effects on university development.\u00a0 By contrast, the communist German Democratic Republic (DDR) centralized research and kept it under tight control. Scientists were isolated from their colleagues in the West and their system became impoverished as the DDR\u2019s economy gradually failed.\u00a0 Merkel grew up in this system, graduating from the Karl Marx University in Leipzig in 1978 with a degree in physics and then moving to the Central Institute for Physical Chemistry in Berlin, one of the most prestigious research centres in the DDR. There, she met her second husband, quantum chemist Joachim Sauer, and earned her PhD with honours. Her zeal for physics did not extend to the required political education. In the DDR, no one got their PhD without an accompanying certificate in the study of Marxism\u2013Leninism; Merkel\u2019s dissertation for that subject, \u2018What is the socialist lifestyle?\u2019, was accepted with the lowest passing grade. When the two Germanys were unified in 1990, special committees from the West evaluated the DDR scientists for competence. Many lost their jobs, but Sauer was accepted for transfer to Berlin\u2019s Humboldt University. Merkel, who had not been overtly political before, jumped into democratic politics and soon joined the centre-right Christian Democratic Union (CDU). Doggedly she climbed to the top of the party and became Germany\u2019s first female chancellor in 2005. She won federal elections in 2009 and 2013 and looks set to maintain her position. (In Germany, there is no time limit on serving as head of government.) In March, she opined: \u201cI came from basic research myself and have always said, you can\u2019t predict things there \u2014 you just have to leave space.\u201d \n               Stable support \n             German publicly funded science is organized into five pillars: the universities and its four unique research organizations, each named after a scientific giant in German history.\u00a0 The Max Planck Society, founded in 1948, now runs 81 basic-research institutes whose directors are given extraordinary budgets and free rein to tread their own paths. A director in life sciences typically gets a basic package of \u20ac2 million (US$2.4 million) a year to run their research programme, not including major equipment purchases. The Fraunhofer Society was founded a year later and is dedicated to applied research. It is named after the Bavarian physicist Joseph von Fraunhofer (1787\u20131826), a pioneer of precision optics. National research centres, which carry out large-scale strategic research according to government priorities, are now bundled within the Helmholtz Association, named after pioneering physiol\u00adogist and physicist Hermann von Helmholtz (1821\u201394). A collection of other scientific institutes and facilities has been bundled into an association named after polymath Gottfried Wilhelm Leibniz (1646\u20131716). In a deal that goes back to 1949, the federal government shares the costs of the research organizations with the  L\u00e4nder . But in general, the  L\u00e4nder  have to finance the universities on their own. There are around 110 of these, and 230  Fachhochschulen , universities of applied sciences that can\u2019t offer PhDs but train the work force for industry.\u00a0 \u201cThe clarity and transparency of this structure appeals to the German order-loving mentality,\u201d says Ferdi Sch\u00fcth, a director at the Max Planck Institute for Coal Research in M\u00fclheim. \u201cIt makes the system easier for outsiders, including politicians, to understand.\u201d\u00a0 Support for research quickly built up during the years of West Germany\u2019s  Wirtschaftswunder , or post-war economic miracle. Although the reunification of Germany exacted a heavy cost on the country, politicians have in most years maintained steady and strong support for science. Until 2015, the government increased support for all research organizations and the DFG by 5% per year; that annual increase has dropped in the current \u2018Pact for Research and Innovation\u2019 between the federal government and  L\u00e4nder , which runs until 2020, but remains enviable at 3%.\u00a0 \u201cThis security about future funding allows us to really plan our research strategies in the long term,\u201d says chemist Martin Stratmann, president of the Max Planck Society. \u201cIt\u2019s a big advantage that few other countries share.\u201d\u00a0 \n               Funding flow \n             It was confidence in long-term funding that kept immunologist Dolores Schendel from returning to her native United States after what was meant to be a two-year postdoc placement at the Ludwig Maximilian University (LMU) in Munich in the late 1970s. She had intended only to help establish a mouse lab for the LMU\u2019s bone-marrow-transplant programme. But the facilities were seductive, and as her research became increasingly translational \u2014 and no longer lent itself to a regular flow of high-profile papers \u2014 she knew she could rely on secure local funding. She later moved to the Helmholtz Centre Munich to scale up her work. Then, when a start-up she had founded was bought out, she became chief executive and chief scientific officer of Medigene, an immunotherapy company in Munich. Now she is running clinical trials of candidate cancer vaccines. \u201cI\u2019m not sure I could have achieved this in the United States, where funding tends to be more erratic,\u201d she says. But Schendel is a rare case. Although Germany is an undisputed world leader in engineering (see \u2018Get behind electric cars\u2019), it has had few success stories in the practical application of work from emerging fields, such as biotechnology. Decisions and changes happen slowly, thanks to the layers of bureaucracy between the federal and  L\u00e4nder  governments. What\u2019s more, the abuse of science under the Third Reich, including eugenics and human experimentation, left Germans suspicious of genetics in any form and prone to moral outrage. All this has led to sluggish development on some fronts.  \n               boxed-text \n             The disruption of reunification in 1990 forced the country to fix some systemic problems, such as a lack of collaboration across institutions. Politicians set about chipping away at the numerous obstacles to cooperation.\u00a0 In 1999, the federal government that preceded Merkel\u2019s \u2014 a coalition between the Social Democratic Party and the Greens \u2014 amended a law that required  L\u00e4nder  ministries to make all university decisions, from allocating budgets to making academic appointments. One by one, the  L\u00e4nder  began allowing universities to run their own affairs. The same government proposed a major shake up for universities, which had traditionally been considered all of equal status. As one of its last acts, it launched the \u2018Excellence Initiative\u2019 in 2005. Now well established, this encourages universities to compete for federal money to promote top-level research, graduate schools and, most importantly, \u2018clusters of excellence\u2019 \u2014 major collaborations with scientists in other research organizations. Universities that win in all categories also earn the title of \u2018elite\u2019, which comes with extra cash. When Merkel became chancellor later that year, she appointed as education and research minister her like-minded colleague and friend Annette Schavan, who drove the Excellence Initiative through a series of rounds that fundamentally changed German universities. So far, the federal government has poured \u20ac4.6\u00a0billion into the scheme and a total of 14\u00a0universities have won elite status in various rounds. Those that have not yet earned that title have upped their game by trying for it, and by collaborating within clusters, which have opened up other streams of funding. The once-isolated pillars of German science are now working together. Merkel and Schavan have championed laws that allow the federal government to fund university research directly and allow universities to offer high salaries to attract or keep the best scientists (as civil servants, German academics generally earn less than scientists in other countries or those in industry). As a result of all these changes, German universities have climbed up the world rankings. In 2005, only 9 German universities appeared in the Times Higher Education top 200. Now, there are 22. The LMU, which tops the German list in most years and has won in each round of the Excellence Initiative, rose from 61st place in 2011 to 30th in 2017. Physicist Axel Freimuth has been rector of the University of Cologne since 2005 and says the university has changed beyond recognition. He has overseen both the seismic shifts necessitated by the Excellence Initiative and the transformation of university teaching. Around the time he became rector, Germany began converting from its own idio\u00adsyncratic, drawn-out diploma system to the European standard of bachelor\u2019s and master\u2019s degrees, which process students more efficiently, in three to five years. With the arrival of university autonomy, Freimuth coordinated a new governance system for his institution. \u201cWe have learnt how to act strategically as a university,\u201d he says. \u201cThere is a whole new spirit here.\u201d\u00a0 \n               Cluster bugs \n             In the meantime, research-cluster fever has taken over Germany. Schavan launched several initiatives to get scientists from different pillars to work together and with industry. Most strikingly, she created a network of national institutes of health under the umbrella of the Helmholtz Association, which bundles nationwide competencies across institutions in health areas such as neurodegeneration or metabolic disease.\u00a0 Berlin is experimenting with gathering together parts of its health-related research at the Charit\u00e9 teaching hospital and the Max Delbr\u00fcck Centre for Molecular Medicine, a Helmholtz centre, into a translational-research structure called the Berlin Institute of Health. And the state of Baden-W\u00fcrttemberg has poured hundreds of millions of euros into the Cyber Valley initiative. Launched in December last year, this clusters all regional research in artificial intelligence and is heavily supported by big companies such as BMW, Daimler, Porsche, Bosch and Facebook. \u201cThis clustering really does have a lot of advantages,\u201d says neuro\u00adscientist Hannah Monyer, who has joint positions at the University of Heidelberg and the German Centre for Cancer Research, a Helmholtz centre in the same city. Although it requires researchers to spend more time talking and organizing, she says, \u201cit\u2019s the best thing we can do these days\u201d. A\u00a0cluster set up under one of the rounds of the Excellence Initiative saved her enormous work when her research led her briefly into the unfamiliar area of pain mechanisms, she says. Rather than having to learn everything from scratch, she enjoyed a seamless collaboration with a local behavioural lab, which provided advice, equipment and technical support. The mega-collaborations are still in a test phase. Vascular biologist Holger Gerhardt left a permanent post at the Crick Institute in London to join the Berlin Institute of Health initiative in 2014. \u201cI know it is all one big experiment,\u201d he says. \u201cBut I feel I really might be able to build up something new here.\u201d\u00a0 The improvements that researchers now enjoy are sometimes challenged by the German cultural desire for administrative and moral order. Gerhardt says he often finds himself reminding cluster partners not to create unnecessary organizational structures.  Primate research, although permitted, is very difficult to do.  And use of human embryonic stem cells, aside from a few older cell lines, is forbidden \u2014 Merkel remains unshakeable on this point.\u00a0 Germans\u2019 moral outrage can also be brutally swift. Merkel made a rare blunder in 2011, when she supported  defence minister Karl-Theodor zu Guttenberg after he was proved to have plagiarized his PhD thesis . Merkel immediately argued that such accusations shouldn\u2019t matter to his current job; he was not acting as a scientific assistant. But within two weeks, he was forced to resign. Many prominent politicians in Germany have PhDs, and the affair unleashed a crusade to check each of them.  Schavan herself faced accusations over her 1980 thesis . Although many scientists did not consider what she did to be plagiarism, she nevertheless had to resign in 2013. Overall, however, the numbers tell a positive story for science (see\u00a0\u2018Germany by the numbers\u2019). The proportion of foreign academics in Germany\u2019s universities has jumped from 9.3% in 2005 to 12.9% in 2015. Germany now ranks above the United States for the percentage of papers it publishes among the top 10% most highly cited.\u00a0 But German science still has some catching up to do, particularly in university infrastructure (see \u2018Adapt to stay ahead\u2019). Compared with the pristine modernity of non-university research institutes, university facilities look positively shabby. The  L\u00e4nder  have to bear the costs of increasing numbers of students \u2014 who attend for free \u2014 and cannot keep up with building repairs. The crumbling concrete of science labs and lecture theatres that shot up when the universities expanded in the 1960s and 1970s is embarrassing, says Wilhelm Krull, general secretary of the Volkswagen Foundation in Hanover, Germany\u2019s largest private research funder: \u201cThere is a contrast of  Glanz und Elend  \u2014 splendour and misery.\u201d  \n               boxed-text \n             Few scientists in Germany see the country leaping back to the very top of the scientific world. For one thing, the German language can be off-putting \u2014 even though English is generally spoken in the country\u2019s labs these days. The regulations and need for form-filling frustrate many. And, says Krull, \u201cGermany is still somewhat risk-averse. Radical, disruptive innovation is less common here.\u201d What\u2019s more, the country has much to do to improve the representation of women in research. At research institutions, the proportion of women in top scientific positions has risen from a dismal 4.8% in 2005 to a still-meagre 13.7% in 2016. At universities, the share of women holding top-level academic positions has gone from 10% in 2005 to 17.9% in 2014. That still falls well below the average for the European Union. And things hardly look better in industry; Schendel is one of only 3 female board members out of 160 at the country\u2019s top 30 technology companies. But scientists are generally confident that things will continue to improve steadily. Merkel\u2019s election platform pledges to continue supporting research and innovation, and to raise annual budget increases to 4%. Each day when not travelling, the chancellor goes home to her flat near the Humboldt University to spend what is left of the evening with her chemist husband. Sch\u00fcth says that it simply comes down to her roots. \u201cShe knows what it is to be a scientist, the value of research,\u201d he says. \u201cThat tone trickles down from the top.\u201d \n                 Tweet \n                 Follow @NatureNews \n                 Follow @alison_c_abbott \n               \n                     Academic excellence: Golden Germany 2017-Sep-06 \n                   \n                     Merkel deserves another term as German chancellor 2017-Sep-06 \n                   \n                     Germany must go back to its low-carbon future 2017-Sep-06 \n                   \n                     Germany's researchers welcome \u20ac5-billion funding boost 2015-Apr-17 \n                   \n                     Ministers promise basket of gifts for German science 2014-Oct-31 \n                   \n                     Federal boost for German science 2014-Jun-04 \n                   \n                     Germany hits science high 2013-Sep-18 \n                   \n                     Germany plans for healthy future 2010-Nov-16 \n                   \n                     Science in Germany: A beacon of reform 2007-Jun-06 \n                   Related external links Reprints and Permissions"},
{"file_id": "547150a", "url": "https://www.nature.com/articles/547150a", "year": 2017, "authors": [{"name": "Carina Storrs"}], "parsed_as_year": "2006_or_before", "body": "An unprecedented study in Bangladesh could reveal how malnutrition, poor sanitation and other challenges make their mark on child development. In the late 1960s, a team of researchers began doling out a nutritional supplement to families with young children in rural Guatemala. They were testing the assumption that providing enough protein in the first few years of life would reduce the incidence of stunted growth. It did. Children who got supplements grew 1 to 2 centimetres taller than those in a control group. But the benefits didn't stop there. The children who received added nutrition went on to score higher on reading and knowledge tests as adolescents, and when researchers returned in the early 2000s, women who had received the supplements in the first three years of life completed more years of schooling and men had higher incomes 1 . \u201cHad there not been these follow-ups, this study probably would have been largely forgotten,\u201d says Reynaldo Martorell, a specialist in maternal and child nutrition at Emory University in Atlanta, Georgia, who led the follow-up studies. Instead, he says, the findings made financial institutions such as the World Bank think of early nutritional interventions as long-term investments in human health. Since the Guatemalan research, studies around the world \u2014 in Brazil, Peru, Jamaica, the Philippines, Kenya and Zimbabwe \u2014 have all associated poor or stunted growth in young children with lower cognitive test scores and worse school achievement 2 . A picture slowly emerged that being too short early in life is a sign of adverse conditions \u2014 such as poor diet and regular bouts of diarrhoeal disease \u2014 and a predictor for intellectual deficits and mortality. But not all stunted growth, which affects an estimated 160 million children worldwide, is connected with these bad outcomes. Now, researchers are trying to untangle the links between growth and neurological development. Is bad nutrition alone the culprit? What about emotional neglect, infectious disease or other challenges? Shahria Hafiz Kakon is at the front line trying to answer these questions in the slums of Dhaka, Bangladesh, where about 40% of children have stunted growth by the age of two. As a medical officer at the International Centre for Diarrhoeal Disease Research, Bangladesh (icddr,b) in Dhaka, she is leading the first-ever brain-imaging study of children with stunted growth. \u201cIt is a very new idea in Bangladesh to do brain-imaging studies,\u201d says Kakon. The research is innovative in other respects, too. Funded by the Bill & Melinda Gates Foundation in Seattle, Washington, it is one of the first studies to look at how the brains of babies and toddlers in the developing world respond to adversity. And it promises to provide important baseline information about early childhood development and cognitive performance. Kakon and her colleagues have run magnetic resonance imaging (MRI) tests on two- and three-month-old children, and identified brain regions that are smaller in children with stunted growth than in others. They are also using other tests, such as electroencephalography (EEG). \u201cBrain imaging could potentially be really helpful\u201d as a way to see what is going on in the brains of these young children, says Benjamin Crookston, a health scientist at Brigham Young University in Provo, Utah, who led studies in Peru and other low-income countries that reported a link between poor growth and cognitive setbacks. \n               The long shadow of stunting \n             In 2006, the World Health Organization (WHO) reported an extensive study to measure the heights and weights of children between birth and the age of five in Brazil, Ghana, India, Norway, Oman and the United States 3 . The results showed that healthy, well-fed children the world over follow a very similar growth trajectory, and it established benchmarks for atypical growth. Stunted growth, the WHO decided, is defined as two standard deviations below the median height for a particular age. Such a difference can seem subtle. At 6 months old, a girl would be considered to have stunted growth if she was 61 centimetres long, even though that is less than 5 centimetres short of the median. The benchmarks helped to raise awareness about stunting. In many countries, more than 30% of children under five meet the definition; in Bangladesh, India, Guatemala and Nigeria, over 40% do. In 2012, growing consensus about the effects of stunting motivated the WHO to pledge to reduce the number of children under five with stunted growth by 40% by 2025. Even as officials started to take action, researchers realized there were serious gaps in protocols to identify the problems related to stunting. Many studies of brain development relied on tests of memory, speech and other cognitive functions that are ill-suited to very young children. \u201cBabies do not have much of a behavioural repertoire,\u201d says Michael Georgieff, a paediatrician and child psychologist at the University of Minnesota in Minneapolis. And if parents and doctors have to wait until children are in school to notice any differences, it will probably be too late to intervene. That's where Kakon's work fits in. At 1.63 metres, she is not tall by Western standards, but at the small apartment-building-turned-clinic in Dhaka where she works, she towers over most of her female colleagues. On a recent morning she was with a mother who had phoned her in the middle of the night: the woman's son had a fever. Before examining the boy, Kakon asked his mother how the family was and how he was doing at school, as she usually does. Many parents call Kakon  apa  \u2014 a Bengali word for big sister. About five years ago, the Gates Foundation became interested in tracking brain development in young children living with adversity, especially stunted growth and poor nutrition. The foundation had been studying children's responses to vaccines at Kakon's clinic. The high rate of stunting, along with the team's strong bonds with participants, clinched the deal. To get the study off the ground, the foundation connected the Dhaka team with Charles Nelson, a paediatric neuroscientist at Boston Children's Hospital and Harvard Medical School in Massachusetts. He had expertise in brain imaging \u2014 and in childhood adversity. In 2000, he began a study tracking the brain development of children who had grown up in harsh Romanian orphanages. Although fed and sheltered, the children had almost no stimulation, social contact or emotional support. Many have experienced long-term cognitive problems. Nelson's work revealed that the orphans' brains bear marks of neglect. MRIs showed that by the age of eight, they had smaller regions of grey and white matter associated with attention and language than did children raised by their biological families 4 . Some children who had moved from the orphanages into foster homes as toddlers were spared some of the deficits 5 . The children in the Dhaka study have a completely different upbringing. They are surrounded by sights, sounds and extended families who often all live together in tight quarters. It is the \u201copposite of kids lying in a crib, staring at a white ceiling all day\u201d, says Nelson. But the Bangladeshi children do deal with inadequate nutrition and sanitation. And researchers hadn't explored the impacts of such conditions on cerebral development. There are brain-imaging studies of children growing up in poverty \u2014 which, like stunting, could be a proxy for inadequate nutrition 6 . But these have mostly focused on high-income areas, such as the United States, Europe and Australia. No matter how poor the children there are, most have some nutritious foods, clean water and plumbing, says Nelson. Those in the Dhaka slums live and play around open canals of sewage. \u201cThere are many more kids like the kids in Dhaka around the world,\u201d he says. \u201cAnd we knew nothing about them from a brain level.\u201d \n               The marks of adversity \n             By early 2015, Nelson's team and the Bangladeshi researchers had transformed the humble Dhaka clinic into a state-of-the-art lab. For their EEG equipment, they had to find a room with no wires in the walls and without air-conditioning units, both of which could interfere with the device's ability to detect activity in the brain. The researchers also set up a room for functional near-infrared spectroscopy (fNIRS), in which children wear a headband of sensors that measure blood flow in the brain. The technique gives information about brain activity similar to that from functional MRI, but does not require a large machine and the children do not have to remain motionless. fNIRS has been used in infants since the late 1990s, and is now gaining traction in low-income settings. The researchers are also performing MRIs, at a hospital near the clinic. So far, they have scanned 12 babies aged 2 to 3 months with stunted growth. Similar to the Romanian orphans and the children growing up in poverty in developed countries, these children have had smaller volumes of grey matter than a group of 20 non-stunted babies. It is \u201cremarkably bad\u201d, Nelson says, to see these differences at such a young age. It's hard to tell which regions are affected in such young children, but having less grey matter was associated with worse scores on language and visual-memory tests at six months old. Some 130 children in the Dhaka study had fNIRS tests at 36 months old, and the researchers saw distinct patterns of brain activity in those with stunting and other adversity. The shorter the children were, the more brain activity they had in response to images and sounds of non-social stimuli, such as trucks. Taller children responded more to social stimuli, such as women's faces. This could suggest delays in the process by which brain regions become specialized for certain tasks, Nelson says. EEG detected stronger electrical activity among children with stunted growth, along with a range of brainwaves that reflect problem solving and communication between brain regions. That was a surprise to the researchers, because studies in orphans and poor children have generally found dampened activity 7 . The discrepancy could be related to the different types of adversity that children in Dhaka face, including food insecurity, infections and mothers with high rates of depression. Nelson's team is trying to parse out which forms of adversity seem to be most responsible for the differences in brain activity among the Dhaka children. The enhanced electrical signals in EEG tests are strongly linked to increases in inflammatory markers in the blood, which probably reflect greater exposure to gut pathogens. If this holds up as more children are tested, it could point to the importance of improving sanitation and reducing gastrointestinal infections. Or maternal depression could turn out to be strongly linked to brain development, in which case helping mothers could be just as crucial as making sure their babies have good nutrition. \u201cWe don't know the answers yet,\u201d says Nelson. The participants tested at 36 months are now around 5 years old, and the team is getting ready to take some follow-up measurements. These will give an idea of whether or not the children have continued on the same brain-development trajectory, Nelson says. The researchers will also give the five year olds IQ and school-readiness tests to gauge whether the earlier measurements were predictive of school performance. \n               A better baseline \n             One of the challenges of such studies is that researchers are still trying to work out what normal brain development looks like. A few years before the Dhaka study began, a team of British and Gambian researchers geared up to do EEG and fNIRS testing on children in rural Gambia during the first two years of life. They were also funded by the Gates Foundation. Similar to the Dhaka study, the researchers are looking at how brain development is related to a range of measures, including nutrition and parent\u2013child interaction. But along the way, they are trying to define a standard trajectory of brain function for children 8 . There is a big push at the Gates Foundation and the US National Institutes of Health to nail down that picture of normal brain development, says Daniel Marks, a paediatric neuroscientist at Oregon Health & Science University in Portland, and a consultant for the foundation. \u201cIt is just a reflection of the urgency of the problem,\u201d he says. One of the hopes for the Dhaka study, and the motivation for funding it, is that it will reveal distinct patterns in babies' brains that are predictive of poor outcomes later in life and could be used to see whether interventions are working, says Jeff Murray, a deputy director of discovery and translational sciences at the Gates Foundation. Any such intervention will probably have to include nutrition, says Martorell. He and his colleagues are doing yet another follow-up study of the Guatemalan villagers to see whether those who got protein supplements before the age of 7 have lower rates of heart disease and diabetes 40 years later. But nutrition alone is unlikely to be enough \u2014 either to prevent stunting or to promote normal cognitive development, Martorell says. So far, the most successful nutritional interventions have helped to overcome about one-third of the typical height deficit. And such programmes can be very expensive; in the Guatemalan study, for example, the researchers ran special centres to provide supplements. Nevertheless, researchers are striving to improve interventions. A group involved in the vaccine study in Bangladesh is planning to test supplements in pregnant women in the hope of boosting babies' birth weight and keeping their growth on track in the crucial first two years of life. Tahmeed Ahmed, senior director of nutrition and clinical services at the diarrhoeal-disease research centre, is planning a trial of foods such as bananas and chickpeas, to try to promote the growth of good gut bacteria in Bangladeshi children aged 12 to 18 months. A healthy bacterial community could make the gut less vulnerable to infections that interfere with nutrient absorption and that ramp up inflammation in the body. Ultimately, it's not about whether children have stunted growth or even what their brains look like. It's about what their lives are like as they grow older. Studies such as the one in Dhaka strive to help determine whether interventions are working sooner rather than later. \u201cIf you have to wait until kids are 25 years old to see whether they are employed,\u201d Murray says, \u201cit could take you 25 years to do every study.\u201d \n                     A new global research agenda for food 2016-Nov-30 \n                   \n                     Where to put the next billion people 2016-Sep-28 \n                   \n                     The developing world needs basic research too 2016-Jun-01 \n                   \n                     Poverty linked to epigenetic changes and mental illness 2016-May-24 \n                   \n                     Poverty shrinks brains from birth    2015-Mar-30 \n                   \n                     Poor trapped in poverty by disease 2009-Dec-08 \n                   \n                     Nature Outlook: Food security \n                   Reprints and Permissions"},
{"file_id": "547272a", "url": "https://www.nature.com/articles/547272a", "year": 2017, "authors": [{"name": "Davide Castelvecchi"}], "parsed_as_year": "2006_or_before", "body": "Topological effects might be hiding inside perfectly ordinary materials, waiting to reveal bizarre new particles or bolster quantum computing. Charles Kane never thought he would be cavorting with topologists. \u201cI don't think like a mathematician,\u201d admits Kane, a theoretical physicist who has tended to focus on tangible problems about solid materials. He is not alone. Physicists have typically paid little attention to topology \u2014 the mathematical study of shapes and their arrangement in space. But now Kane and other physicists are flocking to the field. In the past decade, they have found that topology provides unique insight into the physics of materials, such as how some insulators can sneakily conduct electricity along a single-atom layer on their surfaces. Some of these topological effects were uncovered in the 1980s, but only in the past few years have researchers begun to realize that they could be much more prevalent and bizarre than anyone expected. Topological materials have been \u201csitting in plain sight, and people didn't think to look for them\u201d, says Kane, who is at the University of Pennsylvania in Philadelphia. Now, topological physics is truly exploding: it seems increasingly rare to see a paper on solid-state physics that doesn\u2019t have the word topology in the title. And experimentalists are about to get even busier. A study on page 298 of this week\u2019s  Nature  unveils an atlas of materials that might host topological effects 1 , giving physicists many more places to go looking for bizarre states of matter such as Weyl fermions or quantum-spin liquids. Scientists hope that topological materials could eventually find applications in faster, more efficient computer chips, or even in fanciful quantum computers. And the materials are already being used as virtual laboratories to test predictions about exotic and undiscovered elementary particles and the laws of physics. Many researchers say that the real reward of topological physics will be a deeper understanding of the nature of matter itself. \u201cEmergent phenomena in topological physics are probably all around us \u2014 even in a piece of rock,\u201d says Zahid Hasan, a physicist at Princeton University in New Jersey. Some of the most fundamental properties of subatomic particles are, at their heart, topological. Take the spin of the electron, for example, which can point up or down. Flip an electron from up to down, and then up again, and you might think that this 360\u00b0 rotation would return the particle to its original state. But that\u2019s not the case.  Nature reporters discuss the strange world of topology, and why it\u2019s proving powerful in physics. In the strange world of quantum physics, an electron can also be represented as a wavefunction that encodes information about the particle, such as the probability of finding it in a particular spin state. Counterintuitively, a 360\u00b0 rotation actually shifts the phase of the wavefunction, so that the wave\u2019s crests become troughs and vice versa. It takes another full 360\u00b0 turn to finally bring the electron and its wavefunction back to their starting states. This is exactly what happens in one of mathematicians\u2019 favourite topological oddities: the M\u00f6bius strip, formed by giving a ribbon a single twist and then gluing its ends together. If an ant crawled one full loop of the ribbon, it would find itself on the opposite side from where it started. It must make another full circuit before it can return to its initial position. The ant\u2019s situation is not just an analogy for what happens to the electron\u2019s wavefunction \u2014 it actually occurs within an abstract geometric space made of quantum waves. It\u2019s as if each electron contains a tiny M\u00f6bius strip that carries a little bit of interesting topology. All kinds of particles that share this property, including quarks and neutrinos, are known as fermions; those that do not, such as photons, are bosons. Most physicists study quantum concepts such as spin without worrying about their topological meaning. But in the 1980s, theorists such as David Thouless of the University of Washington in Seattle began to suspect that topology might be responsible for a surprising phenomenon called the quantum Hall effect, which had just been discovered. This effect sees the electrical resistance in a single-atom-thick layer of a crystal jump in discrete steps when the material is placed in magnetic fields of different intensities. Crucially, the resistance remains unchanged by fluctuations in temperature, or by impurities in the crystal. Such robustness was unheard of, says Hasan, and it is one of the key attributes of topological states that physicists are now eager to exploit. \n               Physics with a twist \n             In 1982, Thouless and his colleagues 2  unravelled the topology behind the quantum Hall effect, which ultimately helped to win Thouless a share of last year\u2019s Nobel Prize in Physics. Like the electron\u2019s spin, this topology occurs in an abstract space. But in this case, the underlying shape is not a M\u00f6bius strip, but the surface of a doughnut. As the magnetic field ramps up and down, vortices can form and disappear on the surface, like the wind pattern around the eye of a hurricane (see \u2018All wound up\u2019). Vortices have a property known as a winding number, which describes how many times they loop around a central point. Winding numbers are topological invariants \u2014 they do not change as the shape is deformed. And the total sum of the winding numbers of vortices that wink in and out of existence as a magnetic field is applied around the doughnut always stays the same. That sum is called the Chern number, named after the Chinese-American mathematician Shiing-Shen Chern. It had been known to topologists since the 1940s. The most astounding discovery was yet to come. Until the mid-2000s, the quantum Hall effect and other topological effects had been seen only in the presence of strong magnetic fields. But Kane and his colleagues 3 , and separately another team 4 , realized that some insulators made from heavy elements could provide their own magnetic fields through internal interactions between electrons and atomic nuclei. This gave electrons on the surface of the material robust, \u2018topologically protected\u2019 states, which allowed them to flow with next to no resistance. By 2008,  Hasan\u2019s group had demonstrated the effect  in crystals of bismuth antimonide 5 , which were dubbed topological insulators. \u201cThat was the beginning of the fun,\u201d he says. The discovery shook the physics world, says Edward Witten, a theoretician at the Institute for Advanced Study in Princeton and the only physicist ever to have won a Fields Medal, the most coveted award in mathematics. Far from being exotic exceptions, topological states now seemed to offer a vast array of possibilities for discovering unknown effects in nature, he says. \u201cThe paradigm has changed.\u201d  Emergent phenomena in topological physics are probably all around us \u2014 even in a piece of rock.  One of the biggest surprises was that these states could often be explained by theories that had been invented to solve completely different problems, such as reconciling gravity with quantum physics. Concepts such as Witten\u2019s topological quantum-field theories, which had subsequently led to breakthroughs in pure mathematics, were now coming back to physics in unexpected places. \u201cIt was a marvellous circle of ideas,\u201d says mathematician Michael Atiyah, another Fields medallist, who is now at the University of Cambridge, UK, and who also worked on these theories. \n               Sheer weirdness \n             Another major source of excitement is that in a topological material, electrons and other particles can sometimes form states in which they collectively behave as if they were one elementary particle. These \u2018quasiparticle\u2019 states may have properties that are not present in any known elementary particle (see page 324) 6 . They could even mimic particles that physicists have yet to discover. Some of the most hotly anticipated quasiparticles were found two years ago. Known as Weyl fermions, or fermions without mass, they were conjectured in the 1920s by the mathematician Hermann Weyl. All of the fermions that have been discovered in the menagerie of conventional particles have some mass. But Hasan calculated that topological effects inside crystals of tantalum arsenide should create massless quasiparticles that act like Weyl fermions. For a quasiparticle, being massless means that it moves at the same speed whatever its energy. In 2015, Hasan\u2019s team confirmed 7  that experimentally, as did a group led by Hongming Weng at the Chinese Academy of Sciences in Beijing 8 . Researchers hope that these sorts of material might one day be used in applications such as superfast transistors. Electrons moving through a crystal usually scatter when they hit an impurity, which slows their progress, but the topological effects in Hasan\u2019s tantalum arsenide crystals allow electrons to travel unimpeded. Meanwhile, Marin Solja\u010di\u0107, a physicist at the Massachusetts Institute of Technology in Cambridge, and his colleagues had observed something very similar to Weyl fermions, but in electromagnetic waves rather than in a solid crystal 9 . First, they built a gyroid structure \u2014 a mesmerizing 3D pattern that looks like a system of interlocking spiral staircases \u2014 by carefully drilling holes through a stack of plastic slabs. Then they fired microwaves at the gyroid, and saw that the photons \u2014 which are massless bosons \u2014 were behaving like the Weyl fermion quasiparticles in Hasan\u2019s material. One of the most exciting prospects for this booming area of topological photonics would be to use crystals to create optical fibres that allow light to go in only one direction. This would prevent light from bouncing back off imperfections and would dramatically increase the efficiency of long-distance transmissions. On the sheer weirdness scale, perhaps the only quasiparticles that top Solja\u010di\u0107\u2019s boson\u2013fermions are curious things called anyons. Ordinarily, individual particles can be either fermions or bosons. But anyons \u2014 quasiparticles that live in 2D, atom-thin materials \u2014 break that rule. Researchers can observe this transgression when two identical particles swap places. In bosons, the swap has no effect on the collective wavefunction; for fermions, it shifts their wavefunctions\u2019 phases by 180\u00b0, similar to what happens when a single electron does a 360\u00b0 turn. But for anyons, the phase of the wavefunction changes by an angle that depends on the type of anyon. What\u2019s more, theory suggests that in some cases, swapping the anyons back again does not restore their original wavefunction. So if researchers could create several of these anyons next to each other and shuffle them around, their quantum states would \u2018remember\u2019 how they had been shuffled. Physicists can visualize this process by adding the anyon\u2019s 2D spatial motions to a third dimension, representing time. The result is a tracery of lines that tangle together into beautiful braids. In principle, such braided states could be used to encode quantum bits, or qubits, the units of information in quantum computers. Their topology would protect the qubits from external noise, something that has plagued every other technology for storing quantum information. In 2005, Microsoft made a big investment in quantum braids when it put mathematician Michael Freedman in charge of its efforts on quantum computing. Freedman had bagged a Fields Medal in 1986 for cracking the topology of 4D spheres, and went on to develop some of the key ideas about braiding qubits in the 1990s. Initially, Freedman\u2019s team focused mostly on the theory side. But late last year, Microsoft hired several star experimentalists from academia. One of them was physicist Leo Kouwenhoven of the Delft University of Technology in the Netherlands, who in 2012 was the first to confirm experimentally that particles such as anyons remember how they are swapped 10 . He is now setting up a new Microsoft lab at the Delft campus, which aims to demonstrate that anyons can encode qubits and do simple quantum computations. The approach is at least two decades behind other forms of quantum computing, but Freedman thinks that the robustness of topological qubits will ultimately win the day. \u201cIf you\u2019re going to build a new technology, you have to get the foundation right,\u201d he says. Hasan is attempting similar experiments, but thinks that topological quantum computers are at least four decades away. \u201cMy projection is that topological phases of matter will remain in university labs for many years,\u201d he says. \n               A topological atlas \n             There might be a way to speed up the work, however. Experimentalists looking for new topological insulators have conventionally relied on a laborious process that involves calculating the possible energies of electrons in each material to predict its properties. A team led by theoretical physicist Andrei Bernevig of Princeton University has now found a shortcut. The researchers created an atlas of topological matter by looking at all 230 different symmetries that can exist in a material\u2019s crystal structure. Then they systematically predicted which of these symmetries could, in principle, accommodate topological states, without having to first calculate all their energy levels. They think that between 10% and 30% of all materials could display topological effects, potentially amounting to tens of thousands of compounds 1 . Until now, only a few hundred of these topological materials had been identified. \u201cIt turns out that what we know so far is just a small part of a multitude of topological materials that can exist, and there\u2019s a lot more,\u201d Bernevig says. The team included three specialists in the mathematics of crystals at the University of the Basque Country in Bilbao, Spain, and researchers will soon be able to consult the Bilbao Crystallographic Server to find out whether a particular crystalline material is potentially topological. Wei Li, a physicist at Tsinghua University in Beijing, says that Bernevig\u2019s method is \u201cdefinitely a more efficient way\u201d to search for new topological insulators. \u201cI believe there will be a lot of new materials coming out,\u201d he says. \u201cKnowing that a material has some topological state of matter, however, does not mean immediately predicting its properties,\u201d cautions co-author Claudia Felser, a materials scientist at the Max Planck Institute for Chemical Physics of Solids in Dresden, Germany. These properties will still have to be calculated and measured for each material, she says. Most of the topological materials studied so far \u2014 including those in Bernevig\u2019s atlas \u2014 have been relatively easy to understand, because the electrons inside them feel very little of each other\u2019s electrostatic repulsion. The next big challenge for theorists is to understand \u2018strongly interacting\u2019 topological materials, in which the electrons push hard against one another. If theorists can crack that, Hasan says, \u201cyou\u2019ll find a whole zoo of new physics phenomena that we cannot even imagine\u201d. It is this interplay between maths and physics that lies at the heart of the field, says Kane: \u201cWhat drives me is the intersection of something which is both incredibly beautiful, and also comes to life in the real world.\u201d \n                     All shook up over topology 2017-Jul-19 \n                   \n                     Inside Microsoft\u2019s quest for a topological quantum computer 2016-Oct-21 \n                   \n                     Physics of 2D exotic matter wins Nobel 2016-Oct-04 \n                   \n                     New particle is both matter and antimatter 2014-Oct-03 \n                   \n                     Twisted magnetic fields tie information in a knot 2013-Aug-08 \n                   \n                     Hopes surface for exotic insulator 2012-Dec-11 \n                   \n                     A solid case for Majorana fermions 2012-Mar-06 \n                   \n                     Topological insulators: Star material 2010-Jul-14 \n                   \n                     Quantum computation: The dreamweaver's abacus 2008-Apr-16 \n                   \n                     Bilbao Crystallographic Server \n                   Reprints and Permissions"},
{"file_id": "548512a", "url": "https://www.nature.com/articles/548512a", "year": 2017, "authors": [{"name": "Alexandra Witze"}], "parsed_as_year": "2006_or_before", "body": "As the mission speeds towards its conclusion,  Nature  takes a look at what researchers have learnt about the planet\u2019s moons, rings and tempest-filled skies. Twenty years ago, in the wee hours of a muggy Florida morning, the Cassini spacecraft lit up the skies as it blasted off from Cape Canaveral. Now, after a 3.5-billion-kilometre journey and 13 years spent circling Saturn, the orbiter is running low on fuel. On 15 September, Cassini's controllers on Earth will send the craft plunging into Saturn's cloudtops to prevent it from accidentally crashing into and contaminating any moon that might be able to harbour life. Cassini will send data back to Earth right up until that incandescent coda \u2014 a fitting end for one of history's most successful interplanetary missions. A joint venture between NASA, the European Space Agency and the Italian Space Agency, Cassini was the first spacecraft to orbit Saturn. And with much more time to gather science than the earlier fly-bys of Pioneer 11 in 1979, Voyager 1 in 1980 and Voyager 2 in 1981, the mission delivered discoveries in spades, racking up an impressive list of findings as it looped around the majestic planet, danced along its glorious rings and whizzed past many of its bizarre moons. \u201cCassini was a long wait, but it was definitely worth it,\u201d says Linda Spilker, a planetary scientist at NASA's Jet Propulsion Laboratory in Pasadena, California, and the mission's project scientist. \u201cIt has so many incredible accomplishments we can be so proud of.\u201d The spacecraft revealed the chaotic dynamics that shape Saturn's rings, found geysers spraying from the moon Enceladus and watched gigantic storms roil the planet's atmosphere. It observed seasons change for nearly half of a Saturn year, as first the equinox and then the solstice passed, transforming weather patterns. Over the life of Cassini's mission, Saturn has become less of a stranger and revealed itself to be a vibrant system churning with continual change. The spacecraft's observations became a touchstone for understanding the complexity of gas-giant planets, a legacy that NASA's Juno spacecraft is currently continuing at Jupiter. Cassini also made history when it released the Huygens probe, which became the first craft to touch down in the outer Solar System. After a daring two-and-a-half hour descent to the surface of the moon Titan in 2005, Huygens sent back snapshots of a frozen floodplain littered with rocks. Cassini's mapping later revealed Titan to be a world teeming with hydrocarbon lakes and rivers, replenished by methane and ethane rain. With no official plans to return to Saturn anytime soon, 15 September will mark the end of an era. \u201cOn Cassini's final day, we will be watching the signal as we go as deeply into the atmosphere as we can,\u201d says Spilker. \u201cThat day to say goodbye will be a tough day.\u201d \n               A menagerie of moons \n             Cassini's biggest surprises came as it studied some of Saturn's 60-plus moons, raising as many questions as it answered. Researchers finally solved the mystery of Iapetus \u2014 which boasts one light-coloured side and one dark side \u2014 when they discovered an enormous ring of material streaming off another of Saturn's moons, Phoebe. Iapetus seems to get its two-faced look as its leading surface ploughs through Phoebe's debris. And in their study of how crater-pocked Mimas wobbles on its axis, Cassini scientists realized that the world may have either a buried ocean or a stretched-out core. A look at the planet's littlest moons \u2014 never before seen up close \u2014 uncovered a panoply of strange shapes. Hyperion resembles a sponge, and Pan has been compared to a piece of space ravioli. Pandora features an enormous impact crater, a scar from some long-ago collision. But the most astonishing observations were of Titan and Enceladus. On Titan, Saturn's largest moon, Cassini discovered a world with complex chemistry similar to Earth's before life arose. In the 72 minutes that Huygens survived on Titan's surface, the battery-powered lander snapped images of a landscape strewn with frozen rocks and cloaked in an orange haze. From above, Cassini mapped the moon using radar and other instruments, revealing enormous dunes of water ice coated with a hydrocarbon glaze, which wind for hundreds of kilometres in wavy bands near the equator. Liquid methane and ethane rain down, forming rivers and lakes of hydrocarbons. Cassini captured images of sunlight reflecting off these bodies of liquid \u2014 and even used radar to chart their bottoms, sketching out the depths through which a future mission's submersible might glide. Even after all that, Enceladus stole the show. Thought to be inert before Cassini arrived, the moon actually spews ice and water vapour from enormous fractures that decorate its south-pole region like tiger stripes. Powered by Saturn's gravitational pull, the geysers spurt out 200 kilograms of salty, organic-laced material every second. Cassini scientists were surprised to find that this material contains small particles of silica, which may be formed by the interaction of water and rock at hydrothermal vents deep inside Enceladus. On Earth, similar deep-ocean vents are home to microbes that thrive off chemical energy, far from sunlight \u2014 and so Enceladus has vaulted to the top of the list of places to search for extraterrestrial microbes. Planetary scientists are already plotting return missions to fly through Enceladus's plumes and sniff for hints of life. \n               The ever-changing rings \n             Saturn's rings \u2014 the planet's most iconic feature \u2014 are populated by billions of icy particles. From afar, the rings appear fixed and perfectly sculpted, but Cassini revealed some of the processes that shape them, and showed how dynamic they truly are. Ring features form, change shape and vanish \u2014 sometimes in a matter of hours. Cassini discovered how the gravitational forces of even the smallest of Saturn's moons can help to shepherd ring particles into beautifully manicured bands. For example, little Pan, just 28 kilometres across, has cleared a wide path through the rings. Dark and bright bands in the rings on either side of this gap reflect the pull of Pan's gravity. Images taken over the years revealed how some of Saturn's moons continuously shape and sculpt its rings \u2014 a phenomenon that was not fully apparent until Cassini was able to watch them over time. But the moons are not perfect shepherds. In Saturn's F ring, a narrow band along the outside edge of the main rings, Cassini found ephemeral sprays of material called mini-jets (image, below). The gravitational pull of the nearby moon Prometheus probably causes ice particles in the ring to clump together like snowballs. Those bigger objects then punch outwards, trailing particles behind them like a dusty veil that can stretch up to 180 kilometres long, marring the otherwise perfect rings. Out here on the fringes of the ring system, features such as these come and go. Dramatic changes can also play out on large scales. Around the Saturnian equinox, as sunlight fell at a steep slant across the rings, Cassini observed spoke-like features that rotate with the rings much like the pattern in a bicycle wheel. These spokes, which may be huge stripes of electrostatically charged particles drifting just above and below the rings, can form and disappear over the course of a few hours. \n               Depths of the atmosphere \n             With Saturn's gorgeous ring system distracting the eye, the planet's swirling cloudtop patterns are sometimes underappreciated. Cassini changed that by observing how storms roiled Saturn's atmosphere over the course of many Earth years, providing deep insights into the currents that shape the planet's atmosphere. In late 2010, the spacecraft had a front-row seat as a thunderstorm developed into an enormous, swirling white cloud more than 10,000 kilometres across. The storm churned from deep inside the atmosphere all the way to the its upper layers, and in the ensuing months, wrapped entirely around the northern hemisphere until the 'head' of the storm crashed into the tail. Similar storms appear every two to three decades, a rate that is probably controlled by the amount of water vapour in the atmosphere. Other planets in the Solar System, such as Jupiter, have massive storms but do not see such planet-circling giants. Cassini also probed a unique hexagon-shaped feature, some 30,000 kilometres across, at Saturn's north pole. Confined by winds flowing at more than 300 kilometres an hour, the hexagon is home to smaller hurricane-like vortices that rotate within it. Oddly, Saturn has no such feature at its south pole. Even Saturn's interior came into better focus thanks to the mission. The planet has a strong and complex magnetic field, generated by liquid churning deep within it. The bright auroras that glow around Saturn's poles served as guide posts by helping to reveal the patterns and intensity of its polar magnetic fields. Some fundamental mysteries remain. Mission scientists are still working to determine how long a Saturnian day is. Because the planet has no solid surface, researchers cannot track a fixed feature to measure its rotation rate. Instead, they have tried to measure its true spinning speed by observing the planet's powerful rotating radio emissions, which should reflect the movement of the magnetic field stemming from deep within. But Cassini found that these emissions were more intricate than expected, which complicates efforts to use them to understand the rotation rate. More-detailed information about the magnetic field may come during this final phase of the mission, as Cassini loops between the planet and its rings. Although the mission will come to a close soon, it will leave behind a wealth of information for future studies. \u201cCassini's treasure of data is 100 times as broad and deep as Voyager's, and it will take decades to get to the bottom of it,\u201d says Jeff Cuzzi, a planetary scientist at NASA's Ames Research Center in Moffett Field, California. \u201cThe end of Cassini's active operations may be only the beginning of real advances in our understanding of what it has discovered.\u201d \n                 Tweet \n                 Follow @NatureNews \n                 Follow @alexwitze \n               \n                     Saturn spacecraft begins science swan-song 2017-Apr-12 \n                   \n                     Dust reveals ancient origin for Saturn's rings 2014-Aug-19 \n                   \n                     Icy Enceladus hides a watery ocean 2014-Apr-03 \n                   \n                     First hints of waves on Titan's seas 2014-Mar-17 \n                   \n                     Saturn's rings formed by destruction of giant moon 2010-Oct-05 \n                   \n                     Enceladus shoots supersonic jets of water 2008-Nov-26 \n                   Reprints and Permissions"},
{"file_id": "549146a", "url": "https://www.nature.com/articles/549146a", "year": 2017, "authors": [{"name": "Chelsea Wald"}], "parsed_as_year": "2006_or_before", "body": "Entrepreneurs are finding profits turning human waste into fertiliser, fuel and even food. On the outskirts of Kigali, Rwanda, septic trucks full of human excrement bump and slosh their way up orange dirt roads to their final destination: the Nduba landfill. Until recently, the trucks would spill their contents into giant open pits. But since 2015, workers in green jumpsuits have greeted them outside a row of sheds and plastic-roofed greenhouses, ready to process the faecal sludge into a dry, powdery fuel. The facility is called Pivot, and its founder is Ashley Muspratt, a sanitation engineer who lived in Ghana, Kenya and Rwanda for more than seven years before moving back to the United States last year. Muspratt insists that Pivot is not a treatment plant. It's a business. Its product powers local industries such as cement and brick plants. \u201cI describe us as dual sanitation and renewable-fuel company,\u201d Muspratt says. \u201cOur model really is to build factories.\u201d Muspratt is part of a growing band of entrepreneurs trying to address one of the biggest challenges in public health \u2014  poor sanitation  \u2014 and to turn a profit doing it. According to a report published by the World Health Organization and United Nations children's charity Unicef in July, 2.8 billion people \u2014 38% of the world's population \u2014 have no access to sewers and deposit their waste in tanks and pit latrines (see 'Sanitation across nations'). These often overfill or are emptied without regard to safety. By 2030, some estimate that the number of people using tanks and pits will rise to 5 billion, while at the same time international aid for water and sanitation is predicted to shrink. High-profile initiatives such as the Millennium Development Goals have been pretty good at \u201cgetting bums on toilet seats or feet on squat pans\u201d, says Claire Furlong, an environmental engineer at the IHE Delft Institute for Water Education in the Netherlands. \u201cBut those toilets filled up. What do we do with that?\u201d Muspratt and others have a few answers. Making fertilizer or fuel is the most obvious, but researchers and entrepreneurs are exploring other uses. Some are growing plants in drying beds or breeding catfish in the artificial ponds that facilities typically use to treat sludge. Others are drying out sludge and incorporating it into building materials such as cement and bricks. Beyond that, companies are exploring whether certain fatty acids in sludge could provide important components of bioplastics and industrial chemicals. Larvae that feed on faeces are being pressed to make an oil for industrial uses, and in the future they could be used as animal food. These approaches reflect a rethinking of sludge treatment \u2014 with the end product, and not just public health, in mind from the start. The economic model of sanitation is also changing, moving from an entirely public service to one run at least partly by private enterprises that are finding value in excrement, says Doulaye Kone, deputy director of the Water, Sanitation and Hygiene Program at the Bill & Melinda Gates Foundation in Seattle, Washington. Under the old model, he says, \u201cthere's no opportunity to sell anything, and then the government has to pay for operational costs. The day the budget dries up, everyone is in trouble.\u201d As a result, many treatment plants in developing countries now lie abandoned. Financing isn't the only reason that waste-to-resource initiatives fall short, warns Furlong. Many promising projects have met with resistance because they failed to address cultural elements, which can affect buy-in, whether from toilet users or national politicians. That could be something as visceral as negative attitudes to human waste, or an unwillingness to use new toilet technologies designed to capture waste in usable form. Muspratt and others instead deal with sludge as it is found in pits and latrines that already exist, to prevent their plants falling into neglect or becoming too expensive to maintain. \u201cThe driving force for me was trying to figure a way to not have white elephants all over the continent of Africa.\u201d \n               Human resource \n             Some people need no convincing of the benefits of sludge. In Ghana, some farmers short on fertilizer ask septic-truck drivers to dump their loads onto their fields, where they compost it using traditional methods and spread it onto millet and maize (corn). But this boost of nutrients for the crops poses a risk for those who eat and tend them: the slurry is not safely treated, and increases the chance that the produce will transmit typhoid, cholera, roundworms and various other pathogens that can cause diarrhoea, and lead to anaemia and malnutrition. In young children, repeat  exposure can affect both physical and cognitive development . Even if these farmers weren't using sludge on food crops, disease would still probably be a problem. Less than 5% of people in Ghana have sewers, and there are few treatment facilities for sludge; much of it ends up dumped into ditches or the sea. Turning sludge into fertilizer is not a big ask, technologically \u2014 but it's hard to make a profit because market prices are cheap. Many wastewater treatment plants worldwide, including in the United States, where biosolids are a common by-product of treated sludge, give it away to avoid disposal costs. In Tema, a city east of Ghana's capital Accra, however, a new plant just sold its first few 50-kilogram bags. The operation should turn a profit within three years, says business economist Solomie Gebrezgabher, who works in the Accra office of the International Water Management Institute (IWMI). The Tema plant uses a process that treats the sludge and composts it simultaneously. Powered by the Ghanaian sun, it consumes much less energy than composting methods that use drying and heating machines. But it takes a lot of space and time, and can be smellier. For about the first ten days, the sludge, which comes from both private homes and public toilets, dries in sand-filled beds, which allow the water to drain out and evaporate away. Then it's mixed with sawdust or food waste and transferred to a covered shed. Workers turn it regularly, and it breaks down over two months, thanks to naturally occurring microbes. During this process, it gets hot enough to destroy pathogens. Then it's spread out to cool and mature. The inexpensive process is appropriate for the conditions in Ghana, Gebrezgabher says. \u201cIt doesn't have to be high-tech.\u201d The team approached would-be clients with this bulky soil additive, which improves poor soil's physical qualities \u2014 such as its ability to retain water \u2014 but doesn't substantially increase the supply of nutrients. When Gebrezgabher spoke to farmers, many weren't interested. So she and her colleagues mixed in ammonium sulfate or urea to add more nutrients, as well as compressing it into easier-to-manage pellets. For farmers who were squeamish about using a product made from sludge, the team got a government safety certification. This time farmers were keen. \u201cThey were really excited about it because it has everything that they are looking for,\u201d says Gebrezgabher. Another boost came when the government included the product \u2014 called Fortifer \u2014 in its fertilizer-subsidy programme. With the product and the potential buyers in hand, the IWMI team partnered with the district government and a private local waste management company called Jekora Ventures, based in Accra. At full capacity, the plant, which opened in April, will process the waste from about 65,000 to 100,000 people every year into 500 tonnes of fertilizer. The company will start splitting profits with the municipality once the plant breaks even. The idea is to use those funds to improve sanitation, Gebrezgabher says. She is working with IMWI teams in other regions to replicate this model, starting with Sri Lanka. \u201cThrough not-so-sophisticated technologies, business models can be designed in developing countries that would be commercially viable,\u201d she says. \n               Waste power \n             There is energy in sludge, too. According to a 2015 report by the United Nations University in Hamilton, Canada, if all the human faeces produced annually were converted into biogas, it would provide electricity to more than 138 million households. The leftover slurry could be dried into a charcoal-like fuel for use by a further 130,000. At Pivot's plant, workers make a solid fuel. They take most of the water out of the sludge by passing it through a microscreen. Then they spread it in greenhouses to dry. Finally, they further desiccate and sanitize it in a thermal dryer that runs on scavenged cardboard. The end result, provided as a powder or in granules, has 20% more energy than other biomass fuels such as sawdust or coffee husks, Muspratt says. Pivot sells its fuel to cement and brick-making companies, whose ever-glowing furnaces and kilns have a constant need for the kinds of fuel that Pivot makes. The major customers are usually international firms that value sludge as a renewable energy source that they can use in place of coal. Pivot is on track to break even on operating costs, but it still relies on a little outside support. Its spot on the landfill was donated by the municipality, and its infrastructure was paid for with grants. Expecting wild profits from sludge salvage is unrealistic, says Linda Strande, an environmental engineer at the Swiss Federal Institute of Aquatic Science and Technology in D\u00fcbendorf. \u201cWe would be selling shit here if that was really going to make a huge amount of money,\u201d she says. Most projects could expect to recoup 10\u201320% of annual operating costs, she says. And that's fine, because at least in making some money they reframe sludge as something of value, to be handled with more care. Ironically, the main barrier Pivot faces is getting enough sludge. In theory, a city of at least one million people such as Kigali should be able to supply it, but nobody was bringing sludge from the hard-to-reach pit latrines in the informal settlements. Here, where two-thirds of the population lives, unlicensed workers were simply shovelling out pit latrines by hand and dumping the contents into nearby ditches or waterways. So Pivot started a side venture to provide a safe pumping service to the settlements. It has proved popular but, partly because the latrines are unlined and leaky, Muspratt says, \u201cthe volumes we are getting out of pits are relatively modest so it hasn't been this, like, windfall of faecal sludge that we'd hoped for\u201d. Pivot plans to start grinding up other kinds of combustible waste to blend with its fuel. Like IWMI, Pivot also intends to expand throughout Africa and to India, where millions of people who previously defecated in the open are building latrines thanks to a government initiative. \u201cOur ultimate mission is to be the lowest cost provider of urban faecal-sludge treatment on the market,\u201d says Muspratt. \n               Manna from manure \n             Following a cholera outbreak in 2000, the eThekwini Municipality, which includes Durban, South Africa, installed more than 85,000 urine-diverting dry toilets into rural areas on its outskirts. The diverted urine seeped into the ground, and the authorities asked households to bury the solids on their properties. But burying was a burden on the growing proportion of elderly people, and the increasing population density meant there was less land in which to bury. Even when the faeces was decomposing underground, pathogens survived for much longer than expected. Teddy Gounden and his colleagues at the water and sanitation department wanted to start collecting the waste. \u201cBut what do we do to it?\u201d he wondered. More solid than sewage, it would gum up the town's wastewater treatment plants. Lacking urine, it didn't have enough nutrients to make good compost. Disposal at a hazardous-waste site would be expensive. Then Gounden and his colleagues heard that a certain fly species could make much more valuable products than compost. Flies are normally a health hazard because they feed on both human faeces and food, transmitting pathogens as they flit back and forth. But the black soldier fly ( Hermetia illucens ), which is native to tropical climates, is different: it feeds voraciously in its larval stage, when it stays more or less in one place, and not at all as an adult, making it much less of a health risk. The fly was put to work on food waste by a Cape Town-based firm, AgriProtein. It developed factories to harness the fly's special habits. The company breeds flies in cages, hatches the eggs in a nursery and then transfers the larvae to the food waste, where they eat their fill. Two weeks after hatching, the larvae naturally migrate off the waste to pupate, making both them and the remaining compost easy to harvest separately. The factories dehydrate the larvae to make an animal feed or extract a fatty oil, which has a range of uses from cosmetics to biodiesel. The leftover organic matter becomes a soil conditioner. Last year, AgriProtein opened the first industrial-scale plant of this type, with a plan for worldwide expansion close behind. With the food-waste process working well, the company turned to a trickier source material \u2014 human waste \u2014 under the business name BioCycle. The larvae treated the new food much like the old, says David Wilco Drew, the firm's co-founder and director. In partnership with the eThekwini Municipality, and supported by the Bill & Melinda Gates Foundation, it opened a pilot plant on the premises of a wastewater-treatment works in Durban at the end of 2016. The material itself has proved tricky because of all the rubbish that toilet users have thrown in, Drew says. He's surprised by the ingenuity of some people, because the toilets aren't exactly open pits. \u201cHow can you get an old telephone around a U-bend?\u201d Also conscious of the health risks associated with sludge, BioCycle has adjusted its food-waste process to account for the new input. It tests thoroughly for pathogens and heavy metals. And, instead of making products for agriculture, the plant presses the larvae into oil and the leftover organic matter into solid briquettes, both for use as fuel. Deliveries from the urine-diverting toilets started in late July this year. At full capacity, the plant will accept 40 tonnes of material from the urine-diversion toilets per day, which it then mixes with food waste. \u201cThis is the largest faecal insect site by a mile,\u201d Drew says. With further research, the black soldier flies could treat sewage sludge from the city's sewer system. \u201cThere's a lot of potential,\u201d Gounden says. Other governments \u201care all essentially waiting to see what the outcome is\u201d. To make it easier for municipalities everywhere to jump on the sludge bandwagon, Strande's team has developed a booklet and online courses to help local engineers design systems that can churn out marketable products. And to better understand the inputs to such systems, an international team led by researchers at the University of KwaZulu-Natal in Durban is developing standard methods and procedures for characterizing faecal-sludge properties such as moisture, rubbish and pathogen content, and nutrient and calorific values. Everybody poos, says Drew. One day, he dreams, \u201cevery citizen of the world can contribute to our supply chain\u201d. \n                 Tweet \n                 Follow @NatureNews \n               \n                     The secret history of ancient toilets 2016-May-24 \n                   \n                     Ecodesign: The bottom line 2012-Jun-13 \n                   \n                     Sanitation for all 2012-Jun-13 \n                   \n                     Nature  archive: Public health \n                   \n                     WHO/UNICEF sanitation reports \n                   \n                     Pivot Works, Rwanda \n                   \n                     BioCycle \n                   Reprints and Permissions"},
{"file_id": "548272a", "url": "https://www.nature.com/articles/548272a", "year": 2017, "authors": [{"name": "David Cyranoski"}], "parsed_as_year": "2006_or_before", "body": "Fertility centres are making a massive push to increase preimplantation genetic diagnosis in a bid to eradicate certain diseases.\n Getting time with Qiao Jie is not easy. At 7:30\u00a0a.m., the line coming out of the fertility centre that she runs blocks the doorway and extends some 80 metres down the street. Inside, about 50 physicians on her team are discussing recent findings, but Qiao, a fertility specialist and president of Peking University Third Hospital in Beijing, is still in an early-morning consult. When she finally emerges, she jumps to the topic at hand: spreading awareness of preimplantation genetic diagnosis (PGD), a procedure that helps couples undergoing  in vitro  fertilization (IVF) to avoid passing on genetic mutations that could cause disease or disability in their children. Qiao typically refuses interview requests, but she\u2019s concerned that people aren\u2019t getting the message about PGD fast enough. \u201cNow, more and more diseases can be stopped\u00a0\u2014\u00a0if not immediately, in the generation after next,\u201d she says.\u00a0 Early experiments are beginning to show how genome-editing technologies such as  CRISPR might one day fix disease-causing mutations  before embryos are implanted. But refining the techniques and getting regulatory approval will take years. PGD has already helped thousands of couples. And whereas the expansion of PGD around the world has generally been slow, in China, it is starting to explode. The conditions there are ripe: genetic diseases carry heavy stigma, people with disabilities get very little support and religious and ethical push-back against PGD is almost non-existent. China has also lifted some restrictions on family size and seen a subsequent rise in fertility treatments among older couples. Genetic screening during pregnancy for chromosomal abnormalities linked to maternal age has taken off throughout the country, and many see this as a precursor to wider adoption of PGD. Although Chinese fertility doctors were late to the game in adopting the procedure, they have been pursuing a more aggressive, comprehensive and systematic path towards its use there than anywhere else. The country\u2019s central government, known for its long-term thinking, has over the past decade stepped up efforts to bring high-quality health care to the people, and its current 5-year plan has made reproductive medicine, including PGD, a priority, an effort that Qiao is leading. Researchers are hunting down various mutations in the Chinese population that might be screened for in PGD. And well-equipped and powerful clinical-research groups, including Qiao\u2019s, are stepping up efforts to improve the technology, increase awareness and bring down costs.\u00a0 Why is preimplantation genetic diagnosis is taking off in China? Comprehensive figures are difficult to come by, but estimates from leading PGD providers show that China\u2019s use of the technique already outpaces that in the United States, and it is growing up to five times faster. Qiao\u2019s clinic alone now performs more procedures with PGD each year than all of the United Kingdom.\u00a0 \u201cLooking over the development in China over the past 10 years, they might start to think it\u2019s possible to get rid of these diseases,\u201d says Kangpu Xu, a Chinese-born reproductive biologist at Weill Cornell Medical College in New York City. Such systematic efforts raise thorny questions for bioethicists. Some worry that pushes to eliminate disabilities  devalue the lives of those who already have them . The cost and accessibility of the procedure raises concerns about genetic traits further widening the divide between rich and poor people. Then there are concerns about the push to select for non-disease-related traits, such as  intelligence  or athletic ability. The ever-present spectre of eugenics lurks in the shadows. But in China, although these concerns are considered, most thoughts are focused on the benefits of the procedures. \u201cThere are ethical problems, but if you bring an end to the disease, I think it\u2019s good for society,\u201d says Qiao.\u00a0 \n               Heyday for PGD \n             Physicians in the United Kingdom  pioneered PGD in humans about 30 years ago , initially to help genetic carriers of a disorder that affects mainly boys. Thanks to the procedure, the parents were able to select for girls. Generally, the process involves removing one or a few cells from an embryo created during IVF and then using various techniques to test the structure and number of chromosomes and even the sequence of individual genes. Physicians typically discard embryos that don\u2019t pass the tests. Uncertain about the procedure\u2019s safety, and wary of its potential for abuse (selecting for males in China is illegal, for example), the Chinese government restricted the practice to hospitals with a licence. By the end of 2004, only four centres in the entire country had such a licence. By 2016, the number had risen to 40.\u00a0 The clinics are huge and growing. Qiao\u2019s centre carried out 18,000 IVF procedures in 2016. The biggest clinic, the Reproductive and Genetic Hospital CITIC-Xiangya in Changsha, recorded 41,000 IVF procedures in the same year. That\u2019s roughly one-quarter of the annual number for the entire United States. One reason for the dramatic rise is China\u2019s policy change last year that now allows families to have two children. This has led to a huge number of older women seeking fertility treatment. Another factor is the changing culture in China. Ten years ago, people who couldn\u2019t conceive would take traditional Chinese medicine, or they might adopt a child. \u201cNow they know assisted reproductive technologies can help,\u201d says Qiao.\u00a0 And the centres with licences to do PGD have created a buzz in their race to claim firsts with the technology. In 2015, CITIC-Xiangya boasted China\u2019s first \u201ccancer-free baby\u201d. The boy\u2019s parents had terminated a prior pregnancy after genetic testing showed the presence of retinoblastoma, a cancer that forms in the eyes during early development and often leads to blindness. In their next try, the couple used PGD to ensure that the gene variant that causes retinoblastoma wasn\u2019t present. Other groups have helped couples to avoid passing on a slew of conditions: short-rib-polydactyly syndrome, Brittle-bone disease, Huntington\u2019s disease, polycystic kidney disease and deafness, among others. Qiao, working with biochemist Sunney Xie at Harvard University in Cambridge, Massachusetts, has also introduced a method that can do both chromosomal analyses and next-generation genetic analyses on a single cell. China might have got a slow start, but it is now overtaking Western nations in its use of PGD. Qiao\u2019s clinic screened embryos for individual disease-causing genes about 100 times last year. It screened for abnormal chromosome counts, such as that associated with Down\u2019s syndrome, in another 670 cases. For comparison, 578 such procedures were done in the entire United Kingdom in 2014, the latest year for which numbers are available. And China\u2019s uptake is growing fast. At CITIC-Xiangya, the number of preimplantation testing procedures rose by 277% over just 2 years, from 876 in 2014 to 2,429 in 2016, and 700 of these were for single-gene disorders.  What\u2019s more, many fertility centres in China have the capacity for high-quality research. Qiao is interested in safety and is studying whether extracting the cells for PGD causes subtle damage to the embryo. She is in the middle of compiling data from all IVF clinics in China for a 10-year study on such effects.\u00a0 Qiao is also working with Xie and Sijia Lu, the chief technology officer of Shanghai-based Yikon Genomics, to develop a technique to do all the necessary sequencing without removing cells, by sampling free-floating DNA in the media the embryos are cultured in. Such an advance could make PGD safer and easier to do. Joe Leigh Simpson, a medical geneticist at Florida International University in Miami, and former president of the Preimplantation Genetic Diagnosis International Society, is impressed by the quality and size of the Chinese fertility clinics. They \u201care superb and have gigantic units. They came out of nowhere in just 2 or 3 years,\u201d he says.\u00a0 Chinese researchers are also looking for more disease-associated gene variants, specifically to expand the impact of PGD. The most concentrated efforts are being orchestrated by He Lin, a geneticist at Shanghai Jiao Tong University. He has set out an ambitious project: to pin down all the mutations in all the genes that cause diseases and put them into a single database. \u201cWe just do them one by one until we get the whole set,\u201d he says, referring to the roughly 6,000 known genetic diseases. As disease\u2013gene links are verified, they could be added to the list of things that PGD can screen for.\u00a0 The first target, He says, is deafness. Wang Qiuju, a hearing-loss specialist at the Chinese PLA General Hospital in Beijing and head of the project, says that she plans to get up to 200,000\u00a0samples from 150 hospitals throughout China to identify associated mutations.\u00a0 The large numbers are needed because there are a handful of genes involved in hearing loss, and each of them have dozens, even hundreds, of mutations. \u201cWhen we have big databases, we can see the contribution of each gene more clearly. Then it\u2019s easy to do PGD,\u201d says Wang.\u00a0 \n               Culture clash \n             Such efforts, for hearing loss in particular, can seem jarring because many people in the West do not consider it a problem to be avoided. In the United States, some deaf couples have used PGD to select for congenital deafness, in an effort to preserve  Deaf culture . Such sentiments wouldn\u2019t make sense to many parents in China, says Wang, because there is little support for them: \u201cIf they have a deaf child, they feel the need to have a normal child to help them take care of the deaf child.\u201d People in China seem more likely to feel an obligation to bear the healthiest child possible than to protect an embryo. The Chinese appetite for using genetic technology to ensure healthy births can be seen in the rapid rise of pregnancy testing for Down\u2019s syndrome and other chromosomal abnormalities. Since Shenzhen-based BGI introduced a test for Down\u2019s syndrome in 2013, it has sold more than 2 million kits; half of those sales were in the past year. Although such testing has become routine in the United Kingdom and United States, many in the West won\u2019t terminate a pregnancy just because of Down\u2019s syndrome.\u00a0 Jiani Chen, a genetic counsellor at the University of Oklahoma Health Sciences Center in Oklahoma City, says that this isn\u2019t the case in China. \u201cIn China, if you want to abort a baby with Down\u2019s syndrome, no one will scold you.\u201d Since moving from her native Taiwan to Oklahoma, Chen herself says that she is no longer sure what she would do. In the West, PGD still raises fears about the creation of an elite genetic class, and critics talk of a slippery slope towards eugenics, a word that elicits thoughts of Nazi Germany and racial cleansing. In China, however, PGD lacks such baggage. The Chinese word for eugenics,  yousheng , is used explicitly as a positive in almost all conversations about PGD.  Yousheng  is about giving birth to children of better quality. Not smoking during pregnancy is also part of  yousheng .\u00a0 This is not to say that the Chinese haven\u2019t thought about abuses of the technology. The Chinese government was worried, as were many Western governments, that PGD would be used to select physical characteristics, such as height or intelligence. The clinics licensed to do PGD can use it only to avoid serious disease or assist infertility treatments. And sex selection through PGD is off the table. Yikon\u2019s Lu says that some families ask to weed out the mutation that renders many Asians unable to process alcohol, something that could affect the ability to take part in the often alcohol-fuelled Chinese business lunches. \u201cThey want their son to be able to drink,\u201d says Lu. \u201cWe say no.\u201d Shanghai Jiao Tong University\u2019s He has made training genetic counsellors\u00a0\u2014\u00a0people versed in the risks, benefits and ethical issues related to PGD\u00a0\u2014\u00a0a priority. Currently, they are  almost non-existent in China . The UK Human Fertilisation & Embryology Authority also tightly regulates PGD \u2014 limiting its use to 400 conditions. But in the United States, clinics have fairly free rein. Sex selection, for example, is acknowledged as controversial by the American Society for Reproductive Medicine, but its ethics committee largely leaves it to individual clinics to decide what is permissible.\u00a0 To many fertility specialists, what\u2019s most striking about China\u2019s adoption of PGD is the speed and organization of its uptake. China already seems to provide more procedures than the United States, and with growth estimated at 60\u201370% per year, is on target to catch up in per capita terms in the next few years. This could be a boon for the country, given the economic arguments for PGD. For instance, one study has compared the average costs of the PGD procedure needed to avoid cystic fibrosis\u00a0\u2014\u00a0US$57,500\u00a0\u2014\u00a0with the medical costs incurred in a lifetime by an average patient, which amounted to $2.3\u00a0million (I. Tur-Kaspa  et al. Reprod. Biomed. Online    21,  186\u2013195; 2010). The authors calculated net savings on health care of all patients born in a year over the average patient\u2019s lifespan of 37 years to be $33.3\u00a0billion. That is just for one of hundreds of diseases that can be avoided with PGD.\u00a0 But PGD has not been an easy sell in the West. The Catholic Church, for example, opposes embryo manipulation, including the removal of cells for testing, as well as the destruction of embryos. \u201cThe idea that scientists are playing god is always a theme,\u201d says Natasha Bonhomme, chief strategy officer at Genetic Alliance, a lobbying group in Washington DC that focuses on genetic diseases.\u00a0 There are also social and economic concerns. Some parents of affected children argue that reducing the number of children with those diseases would reduce government funding for research into treatments. Others object to the idea that they are being discouraged from conceiving children the usual way. The debate has made physicians and scientists wary. \u201cThe scientific community is not interested in getting too forward out in front of public opinion,\u201d says Simpson, even though he thinks that the evidence is on the side of employing more PGD. \u201cWith every reproductive-biology advance,\u201d he says, \u201cwe get the same questions: \u2018won\u2019t there be a slippery slope that leads to abuse?\u2019 But it never happens.\u201d\u00a0 The upshot is that there has never really been advocacy organized around PGD in the United States, says Bonhomme. And without government support, it remains for many a prohibitively expensive procedure. Insurance coverage is \u201cpitiful\u201d, says Svetlana Rechitsky, director of the genetic-testing firm Reproductive Genetic Innovations in Northbrook, Illinois. Sitting at her desk, sorting through letters from insurers\u00a0\u2014\u00a0mostly refusals to offer coverage for PGD\u00a0\u2014\u00a0she says, \u201cIt\u2019s getting worse and worse.\u201d\u00a0 Already the procedure is much cheaper in China\u00a0\u2014\u00a0about one-third of what it costs in the United States. Cheaper tests will make it more palatable for national insurance coverage, something Qiao has already started pushing for. \u201cBefore I retire, I want to get the government involved. I have 12 years,\u201d she says. \n                     CRISPR fixes disease gene in viable human embryos 2017-Aug-02 \n                   \n                     Secrets of life in a spoonful of blood 2017-Feb-07 \n                   \n                     Should you edit your children\u2019s genes? 2016-Feb-23 \n                   \n                     Reproductive medicine: The power of three 2014-May-21 \n                   \n                     Making babies: the next 30 years 2008-Jul-16 \n                   Reprints and Permissions"},
{"file_id": "547394a", "url": "https://www.nature.com/articles/547394a", "year": 2017, "authors": [{"name": "Jane Qiu"}], "parsed_as_year": "2006_or_before", "body": "With major spaceflight milestones behind it, China is working to build an international reputation for space science. Time seems to move faster at the National Space Science Center on the outskirts of Beijing. Researchers are rushing around this brand-new compound of the Chinese Academy of Sciences (CAS) in anticipation of the launch of the nation's first X-ray telescope. At mission control, a gigantic screen plays a looping video showcasing the country's major space milestones. Engineers focus intently on their computer screens while a state television crew orbits the room with cameras, collecting footage for a documentary about China's meteoric rise as a space power. The walls are festooned with motivational slogans. \u201cDiligent and meticulous,\u201d says one. \u201cNo single failure in 10,000 trials,\u201d encourages another. For director-general Wu Ji, this 19.4-hectare, 914-million-yuan (US$135-million) campus represents the coming of age of China's space-science efforts. In the past few decades, Wu says, China has built the capacity to place satellites and astronauts in orbit and send spacecraft to the Moon, but it has not done much significant research from its increasingly lofty vantage point. Now, that is changing. \u201cAs far as space science is concerned,\u201d he says, \u201cwe are the new kid on the block.\u201d China is rushing to establish itself as a leader in the field. In 2013, a 1.2-tonne spacecraft called Chang'e-3 landed on the Moon, delivering a rover that used ground-penetrating radar to measure the lunar subsurface with unprecedented resolution. China's latest space lab, which launched in September 2016, carries more than a dozen scientific payloads. And four additional missions dedicated to astrophysics and other fields have been sent into orbit in the past two years, including a spacecraft that is conducting pioneering experiments in quantum communication. These efforts, the work of the CAS and other agencies, have made an impact well beyond the country's borders. \u201cThe space-science programme in China is extremely dynamic and innovative,\u201d says Johann-Dietrich W\u00f6rner, director-general of the European Space Agency (ESA) in Paris. \u201cIt's at the forefront of scientific discovery.\u201d Eagerly anticipated missions in the coming decade include attempts to bring back lunar samples, a joint CAS\u2013ESA project to study space weather and ground-breaking missions to probe dark matter and black holes. But despite the momentum, many researchers in China worry about the nation's future in space science. On 2 July, a Long March-5 rocket failed during the launch of a communications satellite, raising concerns about an upcoming Moon mission that will use a similar vehicle. And broader issues cloud the horizon. \u201cThe international and domestic challenges are formidable,\u201d says Li Chunlai, deputy director at the CAS's National Astronomical Observatories in Beijing and a senior science adviser on the country's lunar programme. China is often sidelined in international collaboration, and in recent years it has had to compete with the United States for partners because of a US law that prohibits NASA from working with China. Within China, the government has not conducted strategic planning for space science or provided long-term financial support. \u201cThe question is not how well China has been doing,\u201d says Li. \u201cBut how long this will last.\u201d \n               Reaching for the Moon \n             China's entry into the space age started with a song. In 1970, the country's first satellite transmitted the patriotic tune 'The East is Red' from low Earth orbit. But it was only after the cultural revolution ended in 1976 that the nation made serious progress towards establishing a strong presence in space. The first major milestone came in 1999 with the launch of Shenzhou-1, an uncrewed test capsule that marked the start of the human space-flight programme. Since then, the country has notched up a series of successes, including sending Chinese astronauts into orbit and launching two space labs (see 'Earth orbit and beyond'). \n               boxed-text \n             \u201cChina's space programme has made tremendous advances in a short period of time,\u201d says Michael Moloney, who directs boards covering aerospace and space science at the US National Academies of Sciences, Engineering, and Medicine in Washington DC. And science has progressively become a bigger part of missions run by both the China National Space Administration (CNSA), which governs lunar and planetary exploration, and the China Manned Space Agency. The country's newest space lab, Tiangong-2, for example, hosts a number of scientific payloads, including an advanced atomic clock and a $3.4-million detector called POLAR for the study of \u03b3-ray bursts \u2014 blasts of high-energy radiation from collapsing stars and other sources. The country's first lunar forays \u2014 orbiters launched in 2007 and 2010 \u2014 were more engineering demonstrations than scientific missions, but that changed with the first lander, Chang'e-3. The mission made China the third nation to accomplish a soft landing on the Moon. More importantly, Chang'e-3 touched down in an area that had never been studied up close. Radar measurements and geochemical analyses unveiled a complex history of volcanic eruptions that could have happened as recently as 2 billion years ago 1 . \u201cIt has really helped to bridge the gap in our understanding of the Moon's past and deep structure,\u201d says study leader Xiao Long, a planetary geologist at the China University of Geosciences in Wuhan. The results have captured the attention of planetary scientists in other countries. \u201cThere is an urgent need to determine the precise age and composition of the Moon's youngest volcanism,\u201d says James Head, a specialist in planetary exploration at Brown University in Providence, Rhode Island. This might soon be possible. As early as December, the Chang'e-5 spacecraft will launch on a mission to return samples from near Mons R\u00fcmker, a region known to host volcanic rocks much younger than those obtained from the Apollo landing sites. \u201cIt would be a fantastic addition to lunar science,\u201d Head says. \n               Five years \n             The rising fortunes of Chinese space science have come in part from efforts by the CAS, which worked through the 2000s to convince China's government to boost the scientific impact of its missions. The academy's efforts were eventually rewarded with a pot of money: the five-year Strategic Priority Program on Space Science kicked off in 2011 and provided $510 million for the development of four science satellites. One of the missions that has yielded early results \u2014 and garnered worldwide attention \u2014 is the $100-million Quantum Experiments at Space Scale (QUESS) mission. The spacecraft launched in August 2016 and has been testing a peculiar phenomenon called entanglement, in which the quantum states of particles are linked to each other even if the particles are far apart. Last month, the QUESS team reported that it had used the satellite to beam a pair of entangled photons to two ground stations spaced 1,200 kilometres apart 2  \u2014 far exceeding an earlier record of 144 kilometres (ref.  3 ). The team is also using the satellite to test the possibility of establishing a quantum-communication channel between Graz, near Vienna, and Beijing. The aim is to transmit information securely by encrypting it with a key encoded in the states of photons. \u201cIf successful, a global quantum-communication network will no longer be a science fiction,\u201d says Pan Jian-wei, a physicist at the CAS's University of Science and Technology of China in Hefei and the mission's principal investigator. Researchers are also expecting great things from the $300-million  Dark Matter Particle Explorer  (DAMPE). The detector, which launched in 2015, is the most cutting-edge equipment for picking up high-energy cosmic rays, says Martin Pohl, an astrophysicist at University of Geneva in Switzerland and a co-principal investigator of the mission. DAMPE's data could help to determine whether a surprising pattern in the abundance of high-energy electrons and positrons \u2014 detected by the Alpha Magnetic Spectrometer (AMS) aboard the International Space Station \u2014 comes from dark matter or from astronomical sources such as pulsars, says Pohl, who also works on the AMS. Because DAMPE is more sensitive than the AMS to high-energy particles, Pohl says, it \u201cwill make a significant contribution\u201d. \n               Science for all \n             The dark-matter and quantum missions launched just before the CAS's space-science funding expired. Scientists, including Wu, had to battle for continued support. The Chinese government has lately prioritized applied research, and it took intense lobbying for the better part of 2016 before researchers convinced the government to allocate an additional $730 million to the CAS for space science over the next five years. \u201cIt was not without a fight,\u201d Wu says. \u201cBut we've managed to pull it off.\u201d The new plan, which began this year, funds a number of missions slated for launch in the 2020s, including China's first solar exploration mission and a remote-sensing spacecraft to study Earth's water cycle. The CNSA and the China Manned Space Agency have also been ramping up their space-science efforts. One source of excitement is a $440-million X-ray telescope led by the CNSA, called  Enhanced X-ray Timing and Polarimetry  (eXTP). Planned for launch by 2025, the mission is being financed in part by European partners and involves hundreds of scientists from 20 countries. It is designed to study matter under extreme conditions of density, gravity and magnetism that can be found only in space \u2014 in the interior of neutron stars or around black holes, for instance. The most innovative aspect of the satellite is its ability to simultaneously measure with high precision the timing, energy distribution and polarization of X-ray signals, which will provide insight into a range of X-ray sources, says co-principal investigator Marco Feroci, an astrophysicist at the Institute of Space Astrophysics and Planetology in Rome. eXTP will also carry a wide-field telescope to hunt for unusual, transient signals. \u201cOnce it finds a potentially interesting source, all the other instruments will be zoomed in that direction,\u201d says Zhang Shuangnan, an astrophysicist at the CAS's Institute of High Energy Physics in Beijing, who is leading the mission. \u201cIt's the total weapon for X-ray astronomy.\u201d Work is also progressing on projects led by the China Manned Space Agency. One is a dark-matter detector that has 15 times the sensitivity of DAMPE; it's set to be installed on China's permanent space station, which is slated for completion by 2022. There are also plans for a $730-million optical telescope to orbit near the space station. With a field of view 300 times that of the Hubble telescope, it will produce survey data that could be ideal for studying dark matter and dark energy as well as hunting for exoplanets, says Gu Yidong, a physicist at the CAS's Technology and Engineering Center for Space Utilization in Beijing and a senior science adviser to the China Manned Space Agency. \n               International ties \n             Such projects suggest that collaboration is strengthening between the CAS and China's other agencies involved with space. And a similar spirit is reflected abroad. China's space programme \u201chas become increasingly confident and outward looking\u201d, says W\u00f6rner. In the past, announcements were made only after a mission was successful; now, China routinely broadcasts launches as they happen. And Chinese scientists are increasingly reaching out to their international colleagues, building ties through small-scale partnerships. Most major CAS-led missions have European partners, with collaborations initiated by researchers on both sides. But ESA hopes to establish high-level cooperation with the rising space power. In early 2015, ESA and the CAS issued a call for proposals for space-science missions. They selected a project called Solar Wind Magnetosphere Ionosphere Link Explorer (SMILE), to be led jointly and funded with $53 million from each group. \u201cThe agencies work intimately together at every stage of the development,\u201d says Wu. ESA and China collaborated more than a decade ago on a project called Double Star to study magnetic storms, but it was a China-led mission. Through SMILE, the agencies are testing a new, more intimate cooperation model. \u201cIt's about building trust and bridges, so we could better understand each other,\u201d says Fabio Favata, head of strategy planning and coordination at ESA. \u201cHopefully, this will open the way for larger-scale cooperation in the future.\u201d A nation that is notably absent from China's current list of collaborators is the United States. In the past, China contributed key components to NASA missions. But NASA is now forbidden from such collaboration by a US law passed in 2011, and as a result China is excluded from participation in the International Space Station. On board is a product of earlier collaboration between the United States, China and a number of other countries \u2014 the AMS. Representatives from NASA and Chinese agencies still visit each other regularly. But with no official cooperation possible, there may be some inevitable replication of effort. In March, STROBE-X (Spectroscopic Time-Resolving Observatory for Broadband Energy X-rays) \u2014 a project similar to China's eXTP mission \u2014 was selected by NASA for further study. STROBE-X could launch by 2030, some five years after eXTP. \u201cHaving two very similar missions at the same time is not ideal,\u201d says Colleen Wilson-Hodge, an astrophysicist at NASA's Marshall Space Flight Center in Huntsville, Alabama, and a member of the STROBE-X team. \u201cI wish there were a way we could all work together rather than competing with each other.\u201d \n               Moving forwards \n             For China's space scientists, however, the main challenge is to convince their own government of the need for long-term investment. Zhang, the leader of several astrophysics missions including eXTP, refers to the situation as \u201ca constant state of  zhaobu baoxi \u201d, which translates as \u201cnot knowing where the next meal will come from\u201d. \u201cWe'll be safe for another five years,\u201d he says. \u201cBut nobody knows what will happen afterwards.\u201d Feats of engineering and exploration still get priority over science. The Chinese space station, for instance, has a budget of $14.5 billion. But even though Chinese President Xi Jinping has said that the station will be China's national laboratory in space, there is no dedicated fund for the development of its scientific payloads. The station might support science as Tiangong-2 does, providing power and communications to various experiments. But there is also the danger, Zhang says, that \u201cit will be a house without furniture\u201d. At China's sprawling National Space Science Center, the furniture is new, and the air still smells of fresh paint. Having secured the next bout of funding, Wu looks relaxed as he settles into a big leather armchair behind his desk. He acknowledges the institutional flaws but is optimistic about the future. \u201cSo far, so good,\u201d he says, glancing at the satellite models that line his shelves. \u201cWe can't expect things to change overnight.\u201d \n                 Tweet \n                 Follow @NatureNews \n               \n                     China launches second space lab 2016-Sep-15 \n                   \n                     Chinese satellite is one giant step for the quantum internet 2016-Jul-27 \n                   \n                     China\u2019s quantum space pioneer: We need to explore the unknown 2016-Jan-13 \n                   \n                     China\u2019s dark-matter satellite launches era of space science 2015-Dec-17 \n                   \n                     China and Europe pore over proposals for joint space mission 2015-Mar-19 \n                   \n                     Head of China's space science reaches out 2014-Mar-06 \n                   \n                     China aims for the Moon 2013-Nov-26 \n                   \n                     Nature  special: Science in China \n                   \n                     National Space Science Center \n                   \n                     Chinese Academy of Sciences \n                   \n                     European Space Agency \n                   Reprints and Permissions"},
{"file_id": "548020a", "url": "https://www.nature.com/articles/548020a", "year": 2017, "authors": [{"name": "Elizabeth Gibney"}], "parsed_as_year": "2006_or_before", "body": "In the shadow of the Large Hadron Collider, six teams are competing to answer one of the Universe\u2019s deepest existential questions. In a high-ceilinged hangar at CERN, six rival experiments are racing to understand the nature of one of the Universe's most elusive materials. They sit just metres apart. In places, they are literally on top of one another: the metallic beam of one criss-crosses another like a shopping-centre escalator, its multitonne concrete support hanging ominously overhead. \u201cWe're constantly reminded of each other,\u201d says physicist Michael Doser, who leads AEGIS, an experiment that is vying to be the first to discover how antimatter \u2014 matter's rare mirror image \u2014 responds to gravity. Doser and his competitors have little choice but to get cosy. CERN, Europe's particle-physics laboratory near Geneva, Switzerland, boasts the world's only source of antiprotons \u2014 particles that seem identical to protons in every way except for their opposite charge and spin. The lab's Antiproton Decelerator is a ring, 182 metres around, that feeds from the same accelerators as the lab's bigger and more famous sibling, the Large Hadron Collider (LHC). Antiprotons enter the machine travelling close to the speed of light. As the name implies, the decelerator slows the particles down, providing a stream of antiprotons from which experiments must take turns to sip. All this must be done carefully; upon meeting matter, the antiparticles vanish in a puff of energy. For decades, scientists have worked to pin down antiprotons, and the antihydrogen atoms they can be used to build, for long enough to study. The past few years have seen rapid advances: experimentalists can now control enough antiparticles to start probing antimatter in earnest and to perform increasingly precise measurements of its fundamental properties and internal structure. Jeffrey Hangst, who leads the experiment known as ALPHA, says that in principle at least, his team can now do with antihydrogen anything others do with hydrogen. \u201cFor me, this period is what I've worked towards for 25 years,\u201d he says. The experiments have a lot riding on them: even a slight difference between the properties of matter and antimatter could explain why anything exists at all. As far as physicists know, matter and antimatter should have been created in equal amounts in the early Universe and so blasted each other into oblivion. But that didn't happen, and the origin of this fundamental imbalance remains one of the biggest mysteries in physics. Physicists Michael Doser and Jeffrey Hangst explain what motivates them to keep measuring antimatter. The CERN efforts are unlikely to crack the case any time soon. Antimatter has so far proved maddeningly identical to matter, and many physicists think it will remain that way, because any difference would shake the foundations of modern physics. But the six experiments, the latest in a line of investigations that began at CERN more than 30 years ago, are attracting attention as the LHC continues to draw a blank in its hunt for particles that could explain the antimatter paradox. Moreover, the teams' rapid advances in manipulating antimatter have earned them a major upgrade to the facility's antiproton factory \u2014 a cutting-edge decelerator that will start operation by the end of this year and eventually enable experiments to work with up to 100 times more particles (see \u2018The decelerators\u2019). The dozens of physicists working on the CERN experiments know they face a tough challenge. Antimatter is exasperating to work with, the competition between teams is intense and the odds of finding anything new seem low. But CERN's antimatter wranglers are motivated by the thrill of opening a new window on the Universe. \u201cThese are such tour de force experiments that, no matter what answer you get, you can be proud that you do this,\u201d says Hangst. There's no guarantee that antimatter will yield a major discovery. But \u201cif you can get your hands on some\u201d, he says, \u201cit would be completely reprehensible not to look.\u201d \n               The fact of the matter \n             The roots of antimatter physics can be traced to 1928, when British physicist Paul Dirac wrote an equation that described an electron moving close to the speed of light 1 . Dirac realized that there had to be both a positive and a negative solution to his equation. He later interpreted this mathematical quirk as suggestive of the existence of an anti-electron, now called a positron, and theorized that antimatter equivalents should exist for every particle. Experimentalist Carl Anderson confirmed the positron's existence in 1932, when he found a particle that seemed like an electron except that when it travelled through a magnetic field, its trajectory bent in the opposite direction. Physicists soon realized that positrons were routinely produced in collisions: smash particles together with enough energy and some of that energy can turn into matter\u2013antimatter pairs. By the 1950s, researchers had begun to exploit this energy-to-particle conversion to produce antiprotons. But it took decades to find a way to make enough of them to capture and study. One motivation was the tantalizing idea that antiprotons and positrons could be paired to make antihydrogen, which could then be compared with the well-studied hydrogen atom (see \u2018Wrangling antimatter\u2019). Creating positrons is fairly straightforward. The particles are produced in certain types of radioactive decay, and can be readily caught with electric and magnetic fields. But the higher-mass antiproton is another story. Antiprotons can be made by slamming protons into a dense metal, but they emerge from such collisions moving too fast to be held by an electromagnetic trap. Antimatter hunters needed a way to massively slow down, or cool, the particles. CERN's first dedicated attempt to decelerate and store antimatter began in 1982, with the Low Energy Antiproton Ring (LEAR). In 1995, the year before LEAR was slated to be shut down, a team used antiprotons from the facility to produce the first antihydrogen atoms 2 . LEAR's replacement, the Antiproton Decelerator, came online in 2000 with three experiments. Similar to its predecessor, it tames antiparticles, first by focusing them using magnets and then by slowing them using strong electric fields. Beams of electrons also exchange heat with the antiprotons, cooling but not touching them because the particle types are both negatively charged and so repel each other. The overall process slows the antiprotons to one-tenth of the speed of light. That is still too fast to work with, so each of the six experiments uses techniques to  further slow and trap the antiprotons . There is plenty of attrition along the way. Each 'shot' of 30 million antiprotons fed to an experiment starts by smashing 12 trillion protons into a target. By the time Hangst's ALPHA experiment, for example, has slowed its antiprotons enough to pair them with positrons and create antihydrogen, just 30 of the particles remain, the rest having escaped, been annihilated or been discarded because they are too fast or in the wrong condition to study. Experimenting with such tiny numbers of antiatoms is a real pain, says Hangst: \u201cYou get a whole new attitude about all the rest of physics when you have to work with this stuff.\u201d \n               Race for the prize \n             Antimatter research at CERN will eventually have some competition from the Facility for Antiproton and Ion Research, a \u20ac1-billion (US$1.16-billion) international accelerator complex in Darmstadt, Germany, that will be completed around 2025. But for the moment, CERN has the monopoly on producing antiprotons slow enough to study. Today, there are five experiments running at the antimatter facility (one, GBAR, is still being built). Each has its own way of working with antiprotons, and although some do unique experiments, they often compete to measure the same properties and independently corroborate each other's values (see \u2018The experiments\u2019). The experiments share one beam, which means that in any two-week period, just three of the five experiments get beam time, each taking their turn in an 8-hour shift. A weekly coordination meeting ensures that each experiment knows when its neighbours' magnet will be running, so as not to ruin sensitive measurements. But despite the close proximity, teams usually find out about breakthroughs made by their neighbours by reading about them in a paper. \u201cThis is built on competition, and that's good. That motivates you,\u201d says Hangst. Today, only one of the six experiments \u2014 BASE \u2014 directly studies the antiprotons from the Antiproton Decelerator. BASE holds the particles in a Penning trap, a complex array of electric fields (which pin particles vertically) and magnetic fields (which make them orbit in a circle). The team can store antiprotons for more than a year, and has used the orbits of antiprotons in the trap to determine the particle's charge-to-mass ratio with record precision 3 . The group also uses a complex method to reveal the antiproton's magnetic moment 4  \u2014 akin to its intrinsic magnetism. The measurement involves switching individual particles rapidly between two separate traps and detecting changes caused by minuscule shifts in an oscillating microwave field. Mastering the technique has become a passion for collaboration leader Stefan Ulmer, a physicist at RIKEN in Wako, Japan. \u201cMy entire heart is in this,\u201d he says. Antihydrogen, which is studied by the other experiments at CERN, comes with its own challenges. Because it has a neutral charge, it is immune to electric fields, and so nearly impossible to control. Experiments must exploit the antiatoms' weak magnetic properties, restraining the particles with a 'magnetic bottle'. For the bottle to work, the magnetic fields inside must vary enormously over a tiny distance, changing by 1 tesla \u2014 the strength of a car-lifting scrapyard magnet \u2014 over just 1 millimetre. Even so, the antihydrogen atoms must have a temperature of less than 0.5 kelvin, or they will escape. The first antihydrogen atoms, created using antiprotons on the move,  lasted about 40 billionths of a second . In 2002, two experiments, ATRAP and ALPHA's predecessor ATHENA, became the first to slow antiprotons enough to make significant amounts of antihydrogen, amassing many thousands of the atoms each 5 . The major breakthrough came almost a decade after that, when the teams learnt to trap the antiatoms for minutes at a stretch 6 . They have since measured properties such as charge and mass and used laser light to probe energy levels 7 . On page 66, ALPHA reports its latest advance: the most precise measurement yet of antihydrogen's hyperfine structure, the tiny internal energy shifts caused by interactions between its antiproton and positron 8 . Together, the CERN experiments explore a range of antimatter properties, any of which could display a difference from matter. The goal for all of them is to keep shrinking the uncertainty, says antimatter veteran Masaki Hori. He leads the ASACUSA experiment, which uses lasers to study antiatoms in flight, free from the disruptive forces of traps. Last year, the team made a precise measurement of the ratio of antiproton mass to electron mass, using exotic helium atoms in which an antiproton takes the place of an electron 9 . Like other measurements so far, it showed no difference between matter and antimatter. But each result is a more stringent test of whether matter and antimatter really are exact mirror images. \n               What difference does it make? \n             If the experiments were to detect any difference between matter and antimatter, it would be a radical discovery. It would mean the violation of a principle called charge, parity and time reversal (CPT) symmetry. According to this principle, a mirror-image Universe that is filled with antimatter and in which time runs backwards will have the same laws of physics as our own. CPT symmetry is the backbone of theories such as relativity and quantum field theory. Breaking it would, in a way, break physics. In fact, only exotic theories predict that the antimatter experiments will find anything at all. For this reason, the physicists at the LHC tend to view the antimatter researchers next door \u201cwith bemused attention\u201d, says Doser, who has been working on antimatter for 30 years. \u201cThey think this stuff is fun and interesting, but unlikely to lead to something new,\u201d he says. CERN theorist Urs Wiedemann seems to confirm that. He says that the experiments' ability to manipulate antimatter is \u201cmind-boggling\u201d and that such tests of theory are essential, but \u201cif you ask is there a firm physics motivation that at some accuracy something new will be discovered, I think a fair statement is, 'No'\u201d. Still, the LHC has fared little better in solving the antimatter mystery. Experiments dating back to the 1960s have shown that some physical processes, such as the decay of exotic kaon particles into more familiar ones, have tiny biases in favour of producing matter. LHC experiments have been hunting more such biases, and even a raft of as-yet-undiscovered particles whose behaviour in the early Universe could have accounted for the huge matter\u2013antimatter imbalance that remains. There has been good reason to suspect such particles exist: they were predicted by supersymmetry, a theory that was proposed to tie up some troubling loose ends in particle physics. But no such particles have turned up in eight years of searching. Now, the simplest, most elegant versions of supersymmetry \u2014 the ones that made the idea appealing in the first place \u2014 have been largely ruled out. \u201cToday, the LHC is looking for hypothetical particles, which may or may not be there, with very little guidance from theory. In a way, this is the same situation we're in,\u201d says Doser. A few teams are now jumping into the next big challenge: the race to measure antimatter's acceleration under gravity. Physicists generally expect antimatter to fall just like matter. But some fringe theories predict that it has 'negative mass' \u2014 it would be repelled by, rather than attracted to, matter. Antimatter with this property might account for the effects of dark energy and dark matter, the identities of which are still unknown. But most mainstream theorists say such a Universe would be inherently unstable. \n               Up is down \n             Measuring antihydrogen in free fall will, as ever, be a question of making it cool enough. Even the tiniest thermal fluctuations will mask the signal of a falling atom. And only neutral particles such as antihydrogen can be used, because even distant sources of electromagnetic fields can expose charged particles to forces bigger than gravity. Next year, Hangst's group aims to use proven technology \u2014 a vertical version of its ALPHA experiment \u2014 to get a definitive determination of whether antimatter falls up or down. \u201cObviously I think we'll succeed first, or I wouldn't get into it,\u201d he says. But two other experiments \u2014 Doser's AEGIS and the antimatter facility's newest member, GBAR \u2014 are hot on the team's heels. Both use laser-cooling techniques to boost precision, which will enable them to pick up subtler differences between the acceleration of antimatter and matter than ALPHA currently can. AEGIS will measure the bend of a horizontal beam of antihydrogen, whereas GBAR will let its antiatoms free-fall for 20 centimetres. Both aim to bring the antiatoms' temperature down to a few thousandths of a degree above absolute zero, allowing measurements of gravitational acceleration as sensitive as 1 part in 100, and have plans to go even further. Later this year, GBAR will be the first to benefit from ELENA, a new 25-million-Swiss-franc ($26-million), 30-metre-circumference ring that sits inside the Antiproton Decelerator and is designed to further slow the antiprotons coming from the machine. Eventually, ELENA will supply particles to all of the experiments, nearly simultaneously. The antiprotons will be slower by a factor of seven and arrive in sharper beams. Because they'll be more efficiently cooled at early stages, experiments should be able to trap more of the particles. Now that the teams can manipulate and test antimatter, Hangst says, more and more physicists are becoming interested in the work. They even pitch ideas for experiments and values to check. And the groups are looking outwards, for ways in which their technologies could aid other areas of research. The GBAR team, for example, is working on a portable trap to carry antiprotons to a CERN experiment called ISOLDE, where they can be used to map the neutrons in unstable radioactive atoms. Assuming a technical impasse doesn't grind progress to a halt, Doser reckons that by the end of the 2020s, physicists will be adept enough at handling antimatter to be able to replicate a range of atomic-physics feats, including constructing antimatter atomic clocks. \u201cI see lots of ideas popping up now, and that's a sign the field is moving forward quickly,\u201d he says. \u201cI hope CERN never kicks me out, because I've got plans for the next 30 years.\u201d \n                 Tweet \n                 Follow @NatureNews \n                 Follow @LizzieGibney \n               \n                     Ephemeral antimatter atoms pinned down in milestone laser test 2016-Dec-19 \n                   \n                     Particle physics: Matter and antimatter scrutinized 2015-Aug-12 \n                   \n                     Proton's magnetism measured with greatest precision yet 2014-May-29 \n                   \n                     Antimatter trapped for more than 15 minutes 2011-Jun-06 \n                   \n                     Particle physics: Antimatter matters 2003-Aug-07 \n                   \n                     50,000 atoms of anti-hydrogen made 2002-Sep-19 \n                   \n                     CERN's Antiproton Decelerator \n                   Reprints and Permissions"},
{"file_id": "545399a", "url": "https://www.nature.com/articles/545399a", "year": 2017, "authors": [{"name": "Kendall Powell"}], "parsed_as_year": "2006_or_before", "body": "With competition for research funding approaching an all-time high, experts reveal their top tips and tricks.\n Anaesthesiologist and clinical researcher Peter Nagele started his first independent position in good shape. It was 2007 and he had already earned two early career grants for his laboratory at Washington University in St Louis, Missouri. But when he applied for his first major research grants from the US National Institutes of Health (NIH) he got two crushing rejections. Nagele had made some rookie mistakes: one proposal, for a 10,000-patient clinical trial, was too large in scope to be eligible, and the other was not a priority research area for the agency. \u201cThose projects never saw the light of day,\u201d he says, \u201cand rightfully so.\u201d By his third attempt he had learnt some invaluable tips and tricks. He got feedback from colleagues on his draft proposal, he talked to a grants programme officer at the NIH to work out the best strategy, and he added experienced co-investigators to his proposal. In 2015, his homework paid off. His application for a smaller clinical trial to look at the use of beta-blockers to prevent post-surgery heart problems was funded for roughly US$500,000 a year. The difference between failure and success, in his opinion, was \u201csignificance of the research and feasibility.\u201d He demonstrated to reviewers that he and his collaborators would be able to do the work on time and on budget. Competition for funding is ruthless, and the stakes are particularly high at the NIH \u2014 the largest single source of funding for biomedical research in the world. The agency\u2019s research-project grants \u2014 R01s and other, similar grants \u2014 are the main mechanism for funding investigator-initiated biomedical research in the United States, supporting about 27,500 investigators at any given time. The 5-year average success rate is 18% of the applicant pool \u2014 a historic low that shows little sign of moving, given the  relatively flat NIH budget since 2008 and uncertainty about its prospects . As a result, grant reviewers resort to finding any flaw they can to weed out applications. That creates a daunting challenge \u2014 particularly for young investigators, who don\u2019t yet know the ropes. \u201cThe system has many biases in it \u2014 unintended, by and large. But certainly the more experience someone has, the more these biases work in their favour,\u201d says Jon Lorsch, director of the National Institute of General Medical Sciences (NIGMS) in Bethesda, Maryland, which awards more than 11% of the research grants funded by the NIH. Experienced researchers and grant managers know that scientists can increase their chances of success, for example, by taking full advantage of the programmes designed to help new investigators, teaming up with senior colleagues when appropriate, choosing the right budget and funding mechanisms, and talking early and often with NIH staff who are there to advise.  Nature  spoke with experts in \u2018grantsmanship\u2019 and delved into the data to find out what works \u2014 and what common advice is best ignored. Much of the guidance translates to grant applications elsewhere in the world, particularly for the young scientists that many funders are looking to court and nurture. Early-career investigators  are unknown quantities to a grant-review panel, says Lorsch. \u201cBut they are no less important\u201d to the system. \n               Embrace your inexperience \n             Since 2008 the NIH has tried to reverse the tilt in the playing field that gives established investigators a funding advantage. One strategy has been to prioritize \u2018new investigators\u2019, those who have never had NIH funding for an independent project. New investigators who obtained their final research degree or completed their medical residency within the past ten years are considered early-stage investigators (ESIs). When applications come in, they are split into groups; those from new investigators are compared against one another, but not against those from more-established researchers. This allows scientists to compete against applicants who have similar experience and resources. Applications from new investigators and ESIs must win funding for new applications at roughly the same rate as do those from established investigators. And half of the successful proposals from new investigators must be from ESIs. Age is no barrier: in 2016, about 300 investigators won their first R01 awards aged 50 or older. The data show that these rules even out the success rates across age groups. But they have done less to spread out total funding dollars (see \u2018The NIH\u2019s long tail\u2019). Just 10% of NIH-funded investigators receive more than 40% of NIH funding. So, the NIH also conducts \u2018special council reviews\u2019 of any proposal from investigators who already hold $1\u2009million or more in funding. This month, it announced a points system, called the Grant Support Index, to limit the amount of funding and the number of large grants that any one scientist can hold. The proposed index assigns a value to each type of grant and limits researchers to 21 points \u2014 the equivalent of three R01 grants at a time. It\u2019s important that researchers with new-investigator or ESI status use it to their advantage, says John D. Robertson, owner of the Grant Writers\u2019 Seminars and Workshops, a company based in Buellton, California, that helps researchers with their grant applications. Young investigators should also enquire about extending their ESI status if their research has been interrupted for reasons such as parental leave, medical leave, extended medical training beyond residency, active military duty or even natural disasters. But applications from such researchers must still demonstrate competence and independence in ways that might not be required of more established researchers, who are well known among reviewers. ESIs must provide enough detail in their proposals to show that they can carry out the planned research. \n               Add someone senior \u2014 maybe \n             Scott Fears, a psychiatrist and geneticist at the VA West Los Angeles Medical Center in California, has struggled to earn his own R01 for work studying the developing brains of vervet monkeys ( Chlorocebus pygerythrus ). But in another line of research, he earned a smaller, two-year grant known as an R03 after including a more-established collaborator to make it a multi-investigator grant. The reviewers, Fears says, indicated that his collaborator\u2019s experience was a factor in their scoring. \u201cAdding her has gotten me nothing but positive comments,\u201d he says. Many young investigators wonder if teaming up with a better-known researcher in their field would boost their chances, too. Anecdotally, the approach seems to help some researchers, but experts warn that the strategy can backfire. There are two ways to include other investigators in an application. One is to add a co-investigator who brings specific expertise or equipment to a project. The other, for projects that are multidisciplinary in nature, is for two or more scientists to apply for a multi-investigator grant; in this case, each researcher is responsible for different components of the project. At the NIH, multi-investigator applications come with some caveats: ESIs who team up with non-ESIs will negate their early-career advantage for that application. And under the current vision for the Grant Support Index, a share in a multi-investigator grant scores nearly as highly (6 points) as a single-investigator R01 (7 points). Robertson advises adding a senior person as co-investigator instead, which does not jeopardize the ESI status and, at least for now, doesn\u2019t add points to the co-investigator\u2019s Grant Support Index. Nagele tried this in his third attempt at an R01, and included two co-investigators who had the experience and expertise to get his project done. It worked, he says. But this strategy could have drawbacks, too. The partnership must make sense from a scientific perspective, other\u00adwise reviewers could see it as an attempt to ride the coat-tails of a bigger name. And if the co-investigator is a past supervisor, reviewers might criticize the applicant as not being sufficiently independent \u2014 a consideration that could also affect tenure decisions even if the application is successful. \u201cIt\u2019s only an advantage if the person is really a co-investigator, doing the work as a collaborator,\u201d Robertson says. \n               Ask for more money \n             Another decision young investigators face in their first R01 application is whether to go with a research budget that fits the modular-budget mechanism \u2014 that is, one for budgets of less than $250,000 a year. Applications for more than that must include a detailed accounting of how the money will be spent on personnel, equipment, travel and research. Many young investigators report that senior departmental colleagues advise them not to ask for more than $250,000 in their first applications \u2014 the rationale being that the NIH won\u2019t want to hand big sums to an inexperienced scientist. \u201cThis is a consistent refrain,\u201d says Casey Greene, a computational biologist at the University of Pennsylvania School of Medicine in Philadelphia. \u201cAs I scientist, I did not view the evidence as compelling.\u201d Taking a modular budget, especially in light of the yearly budget-slimming cuts that the NIH applies to all awards, might hurt a young lab\u2019s research. And the data suggest that it won\u2019t improve the chances of winning a grant. Of the 22,765 R01s that were being funded in 2016, 55% had budgets of less than $250,000 and 45% had budgets of between $250,000 and $5\u2009million. About 56% of new awardees aged 45 or under held \u2018non-modular\u2019 budgets, of more than $250,000. And nearly half of those investigators were classified as new investigators (see \u2018The \u2018mod\u2019 squad\u2019). When the NIGMS analysed the new grants it had awarded in the past five years, it found that although only 14% of ESIs apply for non-modular budgets, their success rate, of 25%, was better than that for established investigators. It was even slightly better than for their ESI peers who applied for the lower, modular budgets. Experts advise researchers to apply only for as much money as they genuinely need. Even so, perceived inexperience with large budgets can be a hindrance in the review room. Fears confronted the budget issue with his monkey experiments, which are notoriously expensive. Reviewers\u2019 feedback told him that they thought his study was statistically underpowered, but they were unwilling to give him the bigger budget that he would need to increase the sample size. \u201cDamned if you do, damned if you don\u2019t sometimes seems like a theme at NIH,\u201d he says. \n               Don\u2019t bank on the R21 \n             Wendy Walwyn, an addiction researcher at the University of California, Los Angeles, thought she had stumbled on exactly the kind of translational research that the NIH was interested in when she found a connection between dietary omega-3 fatty acids and reduced anxiety during opioid withdrawal. It suggested that a simple change in diet might help drug addicts to quit. She called various programme officers at the National Institute on Drug Abuse to ask which study section, or reviewer panel, was most appropriate. \u201cThey all said the same thing: you shouldn\u2019t combine preclinical research and clinical work in the same application,\u201d she recalls. They suggested that, instead, she split the proposal into two applications for a two-year grant mechanism designed to fund exploratory studies: the R21. \u201cI had already tried that. Twice,\u201d says Walwyn. It wasn\u2019t a successful strategy. The R21 is often a go-to grant for young investigators starting up labs. Many look at it as a way to gather preliminary data to support an R01 application, or as \u2018bridge\u2019 funding to tide them over once they\u2019ve exhausted their start-up funds, until they get an R01. But Walwyn\u2019s story of R21 failure and frustration is not uncommon. Many investigators feel that the R21\u2019s two-year payout is not worth the time and effort spent writing the application: \u201cI only bother writing R01s \u2014 as opposed to R21s \u2014 for the amount of science one can do for the amount of headache,\u201d says Greene. Not only that, but fewer R21s are given out each year \u2014 just 2,219 in 2016, compared with 6,065 R01s and equivalent grants. And they are harder to get: the overall success rate for R21s in 2016 was 15%, several percentage points lower than for R01s. Stephen Piccolo, a bioinformatician at Brigham Young University in Provo, Utah, learned this lesson personally. He had been told to stay away from R21s, but he ignored that advice when he saw an announcement requesting applications from people in his field of cancer informatics. His proposal earned a competitive percentile score, but he found out in April that it is unlikely to be funded. He\u2019s still optimistic about the R21 mechanism and plans to submit a revised proposal. But he is also practical about its limitations. \u201cIf you feel like your project is the right scope for an R01, then don\u2019t go for the shorter grant \u2014 keep trying until you get the R01.\u201d \n               Talk to the programme officers \n             Programme officers, also called programme directors, are NIH employees who shepherd grant applications through the system, from submission to award. Their role includes advising investigators by e-mail and on the phone \u2014 but not every scientist takes full advantage of this opportunity. \u201cThere\u2019s not any question that is off-limits,\u201d says Alexandra Ainsztein, a programme director in the division of cell biology and biophysics at the NIGMS. New investigators should ask about the various institute missions and research priorities that can affect the decision to fund, says Stacia Friedman-Hill, a programme director at the National Institute of Mental Health. These considerations can often mean that proposals with scores outside the fundable range, especially from ESIs, might get a \u2018reach\u2019 and be funded, she says. Programme officers can also advise on each institute\u2019s favoured grant mechanisms and probable paylines \u2014 the percentile-score cut-offs for funding. The best time to start talking is before a proposal is written. From just a page or outline of specific aims, a programme officer can help to guide researchers to the right study section or a specific funding opportunity, or can suggest adjustments that align with the institute\u2019s current research priorities. They will also point out if experimental approaches or research questions are not likely to be funded. Nagele knows this. After a discovery that the anaesthetic gas nitrous oxide could act as an antidepressant, he and his collaborators wanted to submit an R01 application for a clinical trial of that idea. A discussion with a programme officer at the National Institute of Mental Health revealed that the institute does not fund clinical trials unless the intervention works on a specific biological target in the brain; the mechanism was unknown for nitrous oxide. \u201cHad we prepared that R01, after several weeks or months of work, it would have been dead on arrival,\u201d says Nagele. Instead, he applied for and won an R21 to investigate where nitrous oxide acts in the brain. Another time to seek advice is on receipt of a review-summary statement after the study-section meeting. If an application\u2019s score falls well outside the probable payline, a programme officer can tell investigators which criticisms carried the most weight during review and should therefore be given priority when the application is revised for resubmission. The programme officer can also offer guidance if an application\u2019s score is close to what might be funded; paylines can shift for various reasons. Ainsztein often advises investigators in this \u201cdoughnut hole\u201d range to resubmit an application before funding decisions are made because, as experiments continue, they usually have updates. And a resubmission does not prevent the first attempt from being successful. \u201cIt is more work,\u201d Ainsztein says, \u201cBut there are no guarantees until the award notice goes out.\u201d Programme officers ask that researchers be respectful of their time, however \u2014 waiting until three days before a deadline to seek help with a grant is ill-advised. Ainsztein also encourages researchers to get involved in the review process itself. Young scientists, she says, should apply to the Early Career Reviewer programme at the NIH, which allows them to serve as grant reviewers in a limited capacity. Learning the process from the inside can be invaluable (see \u2018Grant guidance\u2019). Piccolo did this and says that the experience bolstered his confidence. After seeing other people\u2019s proposals, he says, \u201cI thought, \u2018I can do this. If I put the effort in, I can be successful\u2019.\u201d \n               boxed-text \n             But optimism is not in vast supply these days. Dara Ghahremani, an addiction researcher at the University of California, Los Angeles, says he feels as if he is on a grant-writing treadmill and someone keeps turning up the speed. Ghahremani earned an R21 early in his associate research-faculty position, but has since failed in seven attempts to obtain R21s and other grants. He has resorted to applying as a co-investigator on other people\u2019s grant applications to cover his salary. With his time spread thinly, he has not been able to publish his own work in a timely manner, making it harder to apply for independent funding. And the treadmill doesn\u2019t stop after that first R01 is secured. Lorsch and his NIH colleagues are very much aware that renewing an R01 grant is also a daunting task. The success rate for first-time renewals has dropped dramatically over the past decade, from about 53% to 32%. \u201cWe don\u2019t want to bring a whole bunch of ESIs into the system if there\u2019s not much for them downstream,\u201d says Lorsch. He has some simple advice for those investigators who have earned their first R01: \u201cFocus on the work you are doing for that grant.\u201d That means ignoring the temptation to immediately try to win a second major grant, he says. The number one reason that investigators fail to renew a grant is that they have not shown enough productivity or progress, Lorsch says. For the many young investigators whose first R01 attempts are triaged without being scored, Friedman-Hill offers encouragement: \u201cFifty per cent of the applications didn\u2019t get discussed. They have good company.\u201d Those applications come from a mix of new, experienced and very senior investigators, she says. \u201cThe difference is that experienced investigators will just keep trying.\u201d \n                 Tweet \n                 Facebook \n                 LinkedIn \n                 Wechat \n                 Weibo \n               \n                     NIH grant limits rile biomedical research community 2017-May-05 \n                   \n                     NIH to limit the amount of grant money a scientist can receive 2017-May-03 \n                   \n                     NIH research grants yield economic windfall 2017-Mar-30 \n                   \n                     US science agencies face deep cuts in Trump budget 2017-Mar-16 \n                   \n                     Young, talented and fed-up: scientists tell their stories 2016-Oct-26 \n                   \n                     Extra scrutiny for \u2018grandee grantees\u2019 2012-Feb-20 \n                   \n                     Nature  special: Young scientists \n                   \n                     NIH Center for Scientific Review \n                   \n                     NIH policies for new and early-stage investigators \n                   \n                     NIGMS blog post on first R01 renewal rates \n                   Reprints and Permissions"},
{"file_id": "548150a", "url": "https://www.nature.com/articles/548150a", "year": 2017, "authors": [{"name": "Kerri Smith"}], "parsed_as_year": "2006_or_before", "body": "Neuroscientists want to understand how tangles of neurons produce complex behaviours, but even the simplest networks defy understanding. Marta Zlatic owns what could be the most tedious film collection ever. In her laboratory at the Janelia Research Campus in Ashburn, Virginia, the neuroscientist has stored more than 20,000 hours of black-and-white video featuring fruit-fly ( Drosophila ) larvae. The stars of these films are doing mundane maggoty things, such as wriggling and crawling about, but the footage is helping to answer one of the biggest questions in modern neuroscience: how the circuitry of the brain creates behaviour. It's a major goal across the field: to work out how neurons wire up, how signals move through the networks and how these signals work together to pilot an animal around, to make decisions or \u2014 in humans \u2014 to express emotions and create consciousness. Even under the most humdrum conditions \u2014 \u201cnormal lighting; no sensory cues; they're not hungry\u201d, says Zlatic \u2014 her fly larvae can be made to perform 30 different actions, including retracting or turning their heads, or rolling. The actions are generated by a brain comprising just 15,000 neurons. That is nothing compared with the 86 billion in a human brain, which is one of the reasons Zlatic and her teammates like the maggots so much. \u201cAt the moment, really, the  Drosophila  larva is the sweet spot,\u201d says Albert Cardona, Zlatic's collaborator and husband, who is also at Janelia. \u201cIf you can get the wiring diagram, you have an excellent starting point for seeing how the central nervous system works.\u201d Zlatic and Cardona lead two of the dozens of groups around the world that are generating detailed wiring diagrams for brains of model organisms. New tools and techniques for slicing up brains and tracing their connections have hastened progress over the past few years. And the resulting neural-network diagrams are yielding surprises \u2014 showing, for example, that a brain can use one network in multiple ways to create the same behaviours. Reporter Adam Levy takes a look at efforts to painstakingly map brains. But understanding even the simplest of circuits \u2014 orders of magnitude smaller than those in Zlatic's maggots \u2014 presents a host of challenges. Circuits vary in layout and function from animal to animal. The systems have redundancy that makes it difficult to pin one function to one circuit. Plus, wiring alone doesn't fully explain how circuits generate behaviours; other factors, such as neurochemicals, have to be considered. \u201cI try to avoid using the word 'understand',\u201d says Florian Engert, who is  putting together an atlas of the zebrafish brain  at Harvard University in Cambridge, Massachusetts. \u201cWhat do you even mean when you say you understand how something works? If you map it out, you haven't really understood anything.\u201d Still, scientists are beginning to detect patterns in simple circuits that may operate in more complex brains. \u201cThis is what we hope,\u201d says Willie Tobin, a neuroscientist at Harvard Medical School in Boston, Massachusetts: \u201cthat we can come across general principles that can help us understand larger systems.\u201d \n               Circuit training \n             The simplest brain for which scientists have the full wiring diagram is that of the nematode worm  Caenorhabditis elegans , which has just over 300 neurons. Its connectome \u2014 a map of every single neural connection \u2014 was completed in the 1980s 1 . But getting a close look at those connections in action has been difficult. And some neuroscientists are sceptical that the worm brain works in the same way as larger brains. That's why many, like Zlatic, have relied on another invertebrate bastion of the biology lab, the fruit fly.  Drosophila  larvae are complex enough to display some interesting behaviours, but have few enough neurons to make a circuit-mapping project feasible. Plus, Zlatic and her colleagues have a suite of techniques such as optogenetics, in which light-sensitive proteins are used to control or monitor neuronal activity as the flies go about their business. Zlatic and Cardona are developing methods for collecting high-resolution cross-sectional images of the larval fly brain, and for automating the laborious process of tracing all the connections from section to section. Then, by matching up behaviours and activity patterns with their maps, the teams can find out which circuits contribute to which behaviours. One puzzle, for example, is how brains choose between two competing actions. Last year, Cardona, Zlatic and their teams traced the circuitry that allows maggots, when faced with an annoying puff of air, to choose between scrunching their heads in or bending them away 2  (the same animal, puffed with air twice, might choose to bend the first time and scrunch the next). The teams identified which neurons they thought were responding to the air-puff, and used optogenetics to activate them in turn. They could watch the circuit for scrunching become inhibited while the one for bending strengthened, all in the space of a few milliseconds. Then they built a computational model that predicts the response when larvae are stimulated in a particular way. Plenty of labs are studying the adult fruit-fly connectome, too. The whole brain, at 135,000 neurons, is too big to reconstruct in its entirety, so instead scientists are looking at smaller pieces of the nervous system, where they can study the wiring and the activity together. Tobin, for example, works with a speck of the fruit-fly brain that helps to process odours \u2014 a circuit called an olfactory glomerulus. The fly brain has 50 such glomeruli, each hosting a few dozen neurons in a region measuring no more than 20 micrometres across, and each split in half to receive signals from the fly's left and right antennae. In Tobin's latest study 3 , published in May, he and his team took one glomerulus, sliced it finely and used electron microscopy to reconstruct the layout of all 50 neurons of a particular type, including which others they connect to and how strongly. Comparing the two halves revealed some noticeable differences in the number of cells and the wiring, even though the function of the circuit was unchanged. Tobin suggests that the circuit's wiring is compensating for vagaries of development that led to the two halves looking slightly different. This robustness, he says, is likely to be a general characteristic of all brains, and could be lost in some disorders. \u201cDisease is a failure of robustness that the system has been unable to compensate for,\u201d he says. Engert is focusing his efforts on the brain of the larval zebrafish ( Danio rerio ), which has about 100,000 neurons. In May this year, his team published 4  a reconstruction of a whole larval zebrafish brain, and used it to look at the paths that similar neurons take as they extend and connect during development. They expected some degree of randomness in the journey from the brain to the spinal cord, because in mammals such projections often become tangled and haphazard. But the zebrafish neurons they surveyed stayed together in bundles, and took mirror-image routes on each side of the animal. What seems to be important to guiding these cells, Engert says, is their genetic programs. These wiring cues \u201care much more dogmatic than we thought previously\u201d, he says. Some teams are building up circuit diagrams for regions of the mouse brain. In 2014, for instance, a group led by Sebastian Seung, now at Princeton University in New Jersey, published a  map of neurons and their connections in the mouse retina 5 . By looking at the shapes of neurons and the connections they made \u2014 star-shaped neurons have more synapses than have neurons with fewer branches, for example \u2014 the team could speculate about how the cells were passing signals along. Some of the newly mapped cells were known to send signals to others with a time delay, which might explain how the eye transmits information about an object in motion. \n               Traffic jam \n             If neural circuits can teach one lesson, it is that no network is too small to yield surprises \u2014 or to frustrate attempts at comprehension. For 30 years, neuroscientist  Eve Marder  of Brandeis University in Waltham, Massachusetts, has been working on a simple circuit of 30 neurons in the crab gastric system. Its role is simple and the wiring diagram has been in hand for decades. Still, the circuit has mysteries to offer. Marder has shown, for instance, that although the circuits of individual animals may look the same and produce the same output, they vary widely in the strength of their signals and the conductance at their synapses 6 . Today, she is preoccupied with how circuits maintain their identity over time, as things such as ion channels and receptors are replaced. \u201cWhat rules do you use to replace all the components while maintaining a circuit?\u201d she asks, adding that all these challenges will also apply to larger networks. \u201cWe are so far from knowing how to confront the kinds of information you get from an animal behaving and doing a complex task.\u201d Scientists are preparing for that confrontation. The effort has required several new ways of collecting and analysing data, and these have come into their own in the past five years or so. Zlatic's group worked with others at Janelia to fine-tune its optogenetic tools. And to analyse the maggot videos, Zlatic enlisted statisticians and computer scientists who specialize in machine learning to devise ways of classifying the larvae's movements. Then, in Cardona's lab, scientists worked through mapping the larval brains, compiling thousands of images of brain slices taken with electron microscopes and painstakingly tracing the connections between neurons. This map forms the starting point for the rest of their efforts \u2014 map the circuit, manipulate the circuit, watch the behaviour (see 'Connecting the dots'). On page 175, the team uses this protocol to reveal how a circuit in the  Drosophila  brain called the mushroom body controls learning and memory, by linking feelings of reward or punishment with sensory information 7 . But the mapping process is a big hold-up in the field right now, Cardona says. Reconstructing a 160-neuron portion of the fly smell-detection circuit for another paper 8  took Cardona's team more than 1,100 hours. One estimate 9 , extrapolating from previous fruit-fly work, suggests that a map of the full adult fly brain would take hundreds of person-years to complete. Automating the process would help, but algorithms can add bogus connections or miss some entirely. Those working on larger circuits often break the problem down \u2014 assembling a list of cell types first. The Mouse Brain Connectivity Atlas at the Allen Institute for Brain Science in Seattle, Washington, is taking this approach. In work published in 2014, the team identified 10  49 types of cell in the mouse visual cortex alone; the cells vary in size and shape, how fast they fire and what genes they express. The team expects orders of magnitude more cell types across the whole brain. \u201cUp to 10,000 neuronal types would be my guess,\u201d says Hongkui Zeng, who works on the atlas at the Allen Institute. When asked to estimate the amount of data required to map the whole mouse brain, Zeng first laughs. Then she says: \u201cIt's going to be astronomical numbers. I don't even know if there is a word to describe this. It's beyond petabytes. Petabytes of petabytes.\u201d That quantity of data would be generated by just one animal's connectome, but many scientists would like to get to a point where they could produce several and compare them. Tobin thinks that different animals' wiring maps are likely to show important \u2014 and perhaps functionally interesting \u2014 differences. So far, \u201cit's been a land of  n  = 1\u201d, he says. Another priority on many neuroscientists' wish lists is recording from lots of neurons simultaneously. In that way, researchers could stimulate one neuron and see which others are activated, then build up a dynamic picture of the chain of command that leads to behaviour. That will be \u201cthe next huge challenge for the more complex brains\u201d, says Zeng. Even in the 30-cell circuit favoured by Marder, this is still hypothetical. Marder can stick electrodes into a handful of cells at once. Others studying small circuits use various techniques to provide a proxy for which cell is firing and when. For example, researchers can measure calcium released from neurons after they fire, or look at fluorescence in response to a change in voltage across a cell's membrane. But this is like measuring the speed of a car by the strength of the breeze it creates: the proxies aren't as fast as the firing rate itself. \u201cRight now you can record from all neurons but a bit slowly, twice a second,\u201d says Zlatic. \u201cThings happened in between that you missed.\u201d Grasping the dynamics of circuits with more precision could help inform medical questions. Marder has spent 25 years teaching students about brain networks, including those drawn up by specialists in Parkinson's disease. \u201cThe more I stare at their circuit diagrams, the more the paths connect.\u201d She admits that the details of the circuit don't really matter if the treatment works, but they might help get to the bottom of why drugs are effective in some people and not others, or what correlates with success. Clinical evidence suggests that different people with Parkinson's disease have different underlying abnormalities in certain brain regions and circuits 11 . But some researchers find it short-sighted to insist on clinical relevance, arguing that the circuit-mapping quest is worthwhile in its own right. \u201cIt's difficult for me to develop a research plan that will end at the bedside,\u201d says Engert of his work on zebrafish. For now, at least, many researchers are content to embrace the dizzying complexity of the task at hand. Zlatic takes some comfort in the fact that she is starting to see repeating patterns in how neurons in her fly larvae arrange themselves and how they create feedback loops. This modular arrangement, she says, could make the going easier once the team has a finished map. \u201cWhen you have partial information it looks like a big mess,\u201d she says. \u201cMaybe the most surprising thing is that once you start seeing a relatively complete system, how much sense it makes.\u201d \n                     Worldwide brain-mapping project sparks excitement \u2014 and concern 2016-Sep-21 \n                   \n                     'Wiring diagrams' link lifestyle to brain function 2015-Sep-28 \n                   \n                     Neuroscience: Solving the brain 2013-Jul-17 \n                   \n                     Mapping brain networks: Fish-bowl neuroscience 2013-Jan-23 \n                   \n                     Nature Video: The Ultimate Brain Map \n                   Reprints and Permissions"},
{"file_id": "546341a", "url": "https://www.nature.com/articles/546341a", "year": 2017, "authors": [{"name": "Alison Abbott"}], "parsed_as_year": "2006_or_before", "body": "Machine-learning project will analyse 1,000 years of maps and manuscripts from the floating city's golden age. Only metres away from the tourist throngs that bustle through Venice's crowded piazzas, the silence inside Santa Maria Gloriosa dei Frari is so profound it hurts the ears. State archivists long ago took over this fourteenth-century friary, but they are just as studious as the Franciscan brothers who once lived here, as they tend the historical records that fill some 80 kilometres of shelving within. Now, a crew of scientists laden with high-tech equipment is stirring things up in these hallowed stacks. History hangs heavy at the Frari, and computer scientist Fr\u00e9d\u00e9ric Kaplan likes it that way. He has an ambition to capture well over 1,000 years of records in dynamic digital form, encompassing the glorious era of the Most Serene Republic of Venice. The project, which he calls the Venice Time Machine, will scan documents including maps, monographs, manuscripts and sheet music. It promises not only to open up reams of hidden history to scholars, but also to enable the researchers to search and cross-reference the information, thanks to advances in machine-learning technologies. If it succeeds, it will pave the way for an even more ambitious project to link similar time machines in Europe\u2019s historic centres of culture and commerce, revealing in unprecedented detail how social networks, trade and knowledge have developed over centuries across the continent. It would serve as a Google and Facebook for generations long past, says Kaplan, who directs the Digital Humanities Laboratory at the Swiss Federal Institute of Technology in Lausanne (EPFL). Although the previous decade has seen many digital-humanities projects that scan, annotate and index manuscripts, this one stands out because of its ambitious scale and the new technologies it hopes to use: from state-of-the-art scanners that could even read unopened books, to adaptable algorithms that will turn handwritten documents into digital, searchable text. The boon for scholarship should extend well beyond historians. Economists and epidemiologists, for example, are eager to access the written records left by tens of thousands of ordinary citizens, which could reveal how financial markets developed or how diseases such as the plague spread. \u201cWe are in a state of electrified excitement about the possibilities,\u201d says Lorraine Daston, a director of the Max Planck Institute for the History of Science in Berlin. \u201cI am practically salivating.\u201d \n               The Serene Republic \n             Venice is the perfect city for the experiment because of its wealth of historically important, well-ordered documentation. It was founded in the fifth century  AD  by citizens of the Roman empire escaping barbarian invaders from the north. Its inhospitable lagoons provided much-needed protection, and its location at the north end of the Adriatic Sea also had strategic advantages. It soon became the most important trading post between Western Europe and the east, bringing it riches and power. As Venice\u2019s empire grew, it developed administrative systems that recorded vast amounts of information: who lived where, the details of every boat that entered or left the harbour, every alteration made to buildings or canals. Modern banking was invented in the Rialto, one of Venice\u2019s oldest quarters, and notaries there recorded all trading exchanges and financial transactions. Crucially, those records survived through turbulent centuries. While the rest of Europe was roiled by its perpetually warring monarchs, from the eighth century onwards Venice began to develop into a stable republic that provided the peace and order required for trade to flourish. In many ways it was a model democracy. The people elected a leader \u2014 the doge \u2014 supported by various councils, whose members were also usually elected. Governance was secular, but for the most part co-existed tolerantly with religion. French emperor Napoleon Bonaparte put an end to the Serene Republic in 1797. En route to Vienna during his attempt to conquer the Austro-Hungarian Empire, he declared Venice\u2019s secular and democratic governance to be a form of autocracy, and the city to be an enemy of the revolution. He forced the republic to dissolve itself. In 1815, the old Frari was turned into the State Archives of Venice. Over the next decades, all state administrative documents, including death registers, were transferred there, along with medical records, notary records, maps and architectural plans, patent registers and a miscellany of other documentation, some from elsewhere in Italy. Particularly significant are ambassadors' reports from wider Europe and the Ottoman Empire, providing a unique source of detailed information about daily life. \u201cVenetian ambassadors were the most observant travellers, trained to find out things like what was being unloaded at the docks, or what a prince or other high-up was like as a person,\u201d says Daston. \u201cTheir reports were full of gossip and intrigue.\u201d Most of the archive, predominantly written in Latin or the Venetian dialect, has never been read by modern historians. Now it will all be systematically fed into the Venice Time Machine, along with more unconventional sources of data, such as paintings and travellers\u2019 logs. \n               Birth of a career \n             Kaplan has spent his career applying artificial intelligence (AI) in the humanities, mostly in linguistics. He has modelled the evolution of language, for example, by using AI to search centuries of newspaper reports for patterns of words and phrases. But he had always yearned to apply these techniques to building a time machine in a European city with a couple of centuries' worth of archives. His thoughts first turned to Paris, Amsterdam or Geneva, Switzerland. But when the rectors of EPFL and the Ca\u2019Foscari University of Venice decided to collaborate and called for ideas, he immediately offered to develop his time-machine idea for Venice. He vividly recalls the first time he entered the archives, in 2012. Time stands still in the warren of more than 300 rooms, which are neither air-conditioned nor heated. In winter they are bitingly cold, in summer stiflingly hot. The fragile documents are stacked floor to ceiling, and occasional flakes of yellowing paper drift down from their edges. \u201cI felt completely overwhelmed,\u201d he says. \u201cSeeing what a thousand-year archive looks like, knowing that most of it was not available \u2014 I knew we needed to do it.\u201d When the project officially launched in 2012, Kaplan knew that it demanded much more than his computational prowess. It would need historians to annotate the manuscripts, to provide the necessary context for data handling. They might note the role of each person mentioned in a contract to clarify exactly who was the recipient, for example, or assess the reliability of a particular source of information. Archivists, too, would be needed, for their deep knowledge of the immense collection of documents. So he engaged as his co-director Isabella di Lenardo, a Venice-trained historian who is now at EPFL. She had no hesitation: \u201cIt was what I had been waiting for all my life.\u201d Venice state archivists, used to the old ways of guardianship, took a little longer to buy in to the idea, but within a year they were full partners. The interdisciplinary collaboration immediately started to harvest the sort of obscure archival knowledge that tends not to penetrate to the outside world. For example, although ambassadorial reports are a particularly rich source of detail, they were often written in code to keep the messages secret, a source of frustration for historians. Yet one casual conversation among team members led to the serendipitous discovery of a small, sixteenth-century book called  Libro de le cifre , which provided the encryption code for some Venetian ambassadors\u2019 reports. Historians are now eagerly preparing to decode its secrets. \n               Scanning history \n             Even before the Venice Time Machine arrived, the State Archives had started a digitization project funded by the Italian Ministry of Cultural Heritage. In 2006, a huge, purpose-built scanner began to digitize the archive\u2019s precious store of more than 3,000 maps of Italian towns, including many commissioned by Napoleon. These \u2018cadastral\u2019 maps delineate property boundaries and record the ownership of small parcels of land; some of the documents are as large as 4 metres by 7 metres. The Venice Time Machine has shifted this process into overdrive, bringing in other state-of-the-art, high-speed scanners specially adapted for the project. They include one with a robotic arm to turn the pages of books and an imposing rotary scanner with a 2-metre-wide turntable that allows technicians standing on opposite sides to feed it multiple A3-sized documents at the same time. These scanners now form a pipeline that produces several thousand high-definition images per hour, feeding terabytes of information to servers in Venice for long-term storage, and to Lausanne, where high-performance computers transform the images into digital text ready for annotation. The automatic reading of old handwritten manuscripts is a major challenge. Standard character-recognition software allows printed books to be read letter by letter despite variations in fonts, and thus rendered searchable. But this doesn\u2019t work for handwritten manuscripts, where shapes of individual letters can vary enormously between scribes, and can evolve over time. Various approaches to solving the problem are being developed in a European Union collaboration called Recognition and Enrichment of Archival Documents (READ). Kaplan, a member of the collaboration, is currently applying his preferred approach to the Venice Time Machine, using machine learning to recognize the shapes of whole words. Machine learning relies on algorithms that modify their own rules and behaviour as they harvest examples from data sets, honing their skills with every new experience. The time machine\u2019s algorithms are designed to analyse the structure of written text and pull out graphical shapes that look similar, forming a link between them (see \u2018Hacking history\u2019). That allows a user to find a name in one document, and then ask the system to reveal where the same name appears in all the other manuscripts in the database. In the next decade, these scanners could be joined by an instrument that reads books without even opening them. Now being developed at EPFL, the concept is based on computed tomography (CT) scanning techniques used in medicine, where X-ray images taken at different angles build up a 3D picture of the inside of a body, slice by slice. EPFL scientists are researching the composition of ancient inks to identify molecules that could act as X-ray contrast agents. \u201cIt may need more than five years before the tomography scanner can be put into operation,\u201d says Kaplan. But it would offer huge advantages: it could scan books much faster, probe delicate volumes without damage, and access the hundreds of thousands of fragile sealed wills in the Venice archives that would be destroyed if opened. \n               Social networks \n             Even as these technologies are being developed and refined, the Venice Time Machine is already demonstrating how it can help to reshape scholars\u2019 understanding of the past. The narratives that fill history textbooks are usually built around famous people, because so much more is known about them. Yet the time machine will bulge with the sort of mundane records that state administrators everywhere routinely gather to keep track of their populations. This will enable historians to reconstruct the lives of hundreds of thousands of ordinary people \u2014 artisans and shopkeepers, envoys and traders \u2014 and build much more rounded historical narratives. Napoleon\u2019s efficient approach to state administration has been particularly valuable to the project. One cadastral map of Venice that he commissioned in 1808 has provided a backbone of reliable data, allowing historians to add geographical context to a 1740 census that lists citizens who owned and rented property in the city. By combining this with 3D information about buildings from paintings such as those of Canaletto, the time-machine team has produced an animated tour through Venice, showing which businesses were active in each building at the time. \u201cNapoleon may have brought the Republic of Venice to an end,\u201d says Kaplan, \u201cbut for us, he was the starting point for the recovery of its history.\u201d Kaplan and di Lenardo have also made a series of other animations of Venice over space and time, which will be updated and enriched as more data feed into the machine. One is a dynamic video of the development of the Rialto from  AD  950 onwards, using diverse sources of information at different time points. The simulation shows how the buildings \u2014 and the iconic Rialto Bridge \u2014 sprung up among the salt marshes, along with the area\u2019s periodic destruction by fires and subsequent reconstructions. Other simulations tag buildings in the Rialto with the names of family businesses, or depict the social networks that formed between Venetians and others throughout Europe. The Venice Time Machine assumes that there is a connection between people whose names appear in the same document, and this allows it to show each person as a node in a web of connections. When the same individuals crop up in other documents, the web begins to grow into a giant network \u2014 just as scientists draw up social networks from Facebook or Twitter data. This network should allow historians to discover details about the lives of large numbers of previously unknown people in Venice and beyond, and their place in society. \n               Units, banking and plague \n             Daston thinks that the time machine could help to answer an almost endless list of historical questions. For example, it could show how language developed to describe the strange animal species brought to the Venice docks from newly discovered countries, or it could track the trajectories of scholars and scientists as they wandered across Europe. Her personal passion is the epistemology of measurement. \u201cEveryone was crazy about measuring the world in the seventeenth century, yet units of measurement were barely mentioned in the fifteenth and sixteenth centuries,\u201d she says. \u201cBeing able to do keyword searches over the centuries could help us understand how the science of measuring became established.\u201d That enthusiasm spills over from history to other fields. Economic historian Joan Ros\u00e9s of the London School of Economics and Political Science says that centuries of searchable data from notaries in a city as important to economic history as Venice \u201ccould help change our understanding of how financial markets work\u201d. Much of economic theory was developed without hard data, he says, and economists seeking a more robust evidence base are stymied by a lack of suitable data sets on things such as transactions and the flow of money. Modern records, including those from banks, are of limited value: the data have already been processed according to the economic theory to which the institution subscribes. Historical data sets are cleaner because they record raw, intuitive behaviour \u2014 simply who sold what, for how much. But the large financial archives of Europe, such as the Notarial Archive of Catalonia in Barcelona, are not online. \u201cWhen I go to the Barcelona archive for research, I may read just three documents a day,\u201d says Ros\u00e9s, \u201cso the Venice Time Machine will be a game-changer.\u201d And there is much to be learnt from people who were economic failures. \u201cYou can deduce a lot of stupid things if you only study successful, famous people \u2014 the only people that we know a lot about,\u201d he says. Epidemiologist Marcel Salath\u00e9 of EPFL is already collaborating with the Venice Time Machine, peering into records that reveal the names and locations of people who died, often with details about the circumstances of their demise. \u201cIt is like a primitive electronic health record,\u201d he says. Plague wiped out one-third of the population of Venice in the mid-seventeenth century, and Salath\u00e9 hopes to discover more about how the disease spreads. Outbreaks still occur around the world, but there are big gaps in data about its transmission. Animal research alone cannot fill them, and modern human data sets are too small to help, he says. Kaplan hopes that Venice is just a starting point. The Venice Time Machine has applied, with partners around Europe, to become one of the next billion-euro flagship programmes funded by the European Union. If it wins, it will create time machines in other cities with similarly important archives, and link them together. Earlier this year a consortium of Dutch academics launched the Amsterdam Time Machine, although it has yet to secure funding. Its coordinator \u2014 Julia Noordegraaf of the University of Amsterdam, who studies the history of the creative industries \u2014 says it is \u201ca great opportunity to study the cultural traffic between Amsterdam and Venice during its golden age in the seventeenth century\u201d. A Paris Time Machine is also under discussion. The unbridled ambitions of the time-machine project are a concern for some researchers, not least because many of its core technologies are still being developed. \u201cThe vision of extending digital representation into different time slots is absolutely, self-evidently right \u2014 but it might be better to develop things more in a lot of different, small projects,\u201d says J\u00fcrgen Renn, a digital-humanities pioneer and a director at the Max Planck Institute for the History of Science. Nevertheless, Daston suspects that the time machine heralds a new era of historical study. \u201cWe historians were baptized with the dust of archives,\u201d she says. \u201cThe future may be different.\u201d \n                     3D images remodel history 2014-Jun-17 \n                   \n                     Culturomics: Word play 2011-Jun-17 \n                   \n                     Digital history 2001-Feb-01 \n                   \n                     Venice Time Machine \n                   Reprints and Permissions"},
{"file_id": "546200a", "url": "https://www.nature.com/articles/546200a", "year": 2017, "authors": [{"name": "Carrie Arnold"}], "parsed_as_year": "2006_or_before", "body": "With algorithms in hand, scientists are looking to make elections in the United States more representative. Leaning back in his chair, Jonathan Mattingly swings his legs up onto his desk, presses a key on his laptop and changes the results of the 2012 elections in North Carolina. On the screen, flickering lines and dots outline a map of the state\u2019s 13 congressional districts, each of which chooses one person to send to the US House of Representatives. By tweaking the borders of those election districts, but not changing a single vote, Mattingly\u2019s maps show candidates from the Democratic Party winning six, seven or even eight seats in the race. In reality, they won only four \u2014 despite earning a majority of votes overall. Mattingly\u2019s election simulations can\u2019t rewrite history, but he hopes they will help to support democracy in the future \u2014 in his state and the nation as a whole. The mathematician, at Duke University in Durham, North Carolina, has designed an algorithm that pumps out random alternative versions of the state\u2019s election maps \u2014 he\u2019s created more than 24,000 so far \u2014 as part of an attempt to quantify the extent and impact of gerrymandering: when voting districts are drawn to favour or disfavour certain candidates or political parties. Gerrymandering has a long and unpopular history in the United States. It is the main reason that the country ranked 55th of 158 nations \u2014 last among Western democracies \u2014 in a 2017 index of voting fairness run by the Electoral Integrity Project, an academic collaboration between the University of Sydney, Australia, and Harvard University\u2019s John F. Kennedy School of Government in Cambridge, Massachusetts. Although gerrymandering played no part in the tumultuous 2016 presidential election, it seems to have influenced who won seats in the US House of Representatives that year. \u201cEven if gerrymandering affected just 5 seats out of 435, that\u2019s often enough to sway crucial votes,\u201d Mattingly says. The courts intervene when gerrymandering is driven by race. Last month, for example, the Supreme Court upheld a verdict that two North Carolina districts were drawn with racial composition in mind (see \u2018Battleground state\u2019). But the courts have been much less keen to weigh in on partisan gerrymandering \u2014 when one political party is favoured over another. One reason is that there has never been a clear and reliable metric to determine when this type of gerrymandering crosses the line from acceptable politicking to a violation of the US Constitution. Mattingly and several other mathematicians hope to change that. Over the past five years, they have built algorithms and computer models that reveal biases in district borders. And they\u2019re starting to be heard. In December 2016, a Wisconsin court considered a statistical analysis when ruling against partisan gerrymandering. And Mattingly will serve as an expert witness in a case this summer in North Carolina. Although such fights have begun to crop up in other countries, such as the United Kingdom and Australia, the stakes are particularly high in the United States. Lawsuits fighting partisan gerrymandering are pending around the country, and a census planned for 2020 is expected to trigger nationwide redistricting. If the mathematicians succeed in laying out their case, it could influence how those maps are drawn. \u201cThis is what the courts have been waiting for,\u201d says Megan Gall, a social scientist with the Lawyers\u2019 Committee for Civil Rights Under Law in Washington DC. \u201cThis is our way to stop it,\u201d she says. \n               Draw the line \n             In 1812, Massachusetts governor Elbridge Gerry signed a bill that redrew some voting districts to benefit his party. One odd-looking district wrapped around the city of Boston in the shape of a salamander. Political satirists dubbed the new district the 'Gerry-mander'. Since then, this strategy has become a staple of US politics as state legislators redraw voting blocs with tortuous creativity. The two predominant approaches to gerrymandering are often referred to as packing and cracking. In packing, legislators from the party drawing the map try to pack likely opposition voters into as few political districts as possible. Cracking divides supporters of the rival party into several districts, reducing their ability to elect a representative, and ensuring victory for the party in power (see \u2018Packing and cracking\u2019). The Supreme Court historically has not intervened, as long as districts meet four criteria: they are continuous; they are compact; they contain roughly the same number of people; and they give minority groups a chance to elect their own representatives in accordance with the Voting Rights Act of 1965. In the 1986 case  Davis  v.  Bandemer , the court agreed that it had the power to intervene in cases of partisan gerrymandering, but it declined to do so because it lacked a clear measure to indicate when this had occurred. As a specialist in statistics and probability, Mattingly had never given much professional thought to the issue. But his general interest in the political process led him to attend a public meeting in 2013, where he heard a speaker rail against North Carolina's 2012 election outcomes. For about a decade, the state had had a relatively even split in its 13 electoral districts. Sometimes Democrats took six seats, sometimes seven. But Republican redistricting before the 2012 election packed Democrats into three districts, putting the party at a severe disadvantage. Even though its candidates won 50.3% of the votes, the party captured only four seats. Mattingly was struck both by the passion of the rant and the puzzle it posed. \u201cIf it really was unfair, there should be a way to show that mathematically,\u201d he says. \u201cI wanted to move beyond \u2018he said, she said\u2019 and create something more objective.\u201d Reading around the issue, he realized he had a chance to create the metric that judges had been looking for. Packing and cracking result in some telltale signs of interference: the opposition party tends to win by a landslide in packed districts, but lose by a narrow margin in cracked ones. And heavily gerrymandered districts are more likely to be geographically spread out and of unusual shape. With a student, Christy Graves, Mattingly got to work to combine these measures into a single, quantitative Gerrymandering Index for North Carolina. Reporter Shamini Bundell finds out how scientists are helping get to the bottom of gerrymandering. The duo began with the state\u2019s 2012 election districts and public data that broke down voting by neighbourhood. They then made thousands of tiny shifts to the boundaries of the districts, essentially testing every iteration that would meet the four Supreme Court criteria. Ensuring continuity \u2014 and that each district varied in population size by only 0.1% \u2014 was relatively straightforward. So was guaranteeing that the map included a representative number of African American and Hispanic-majority districts to comply with the Voting Rights Act. But evaluating compactness was a challenge. One problem was that it\u2019s difficult to analyse mathematically whether a district meets a rather vague written criterion of being \u2018compact\u2019. For another, mathematicians have more than 30 different ways to calculate a shape\u2019s compactness, each of which gives slightly different results. There is no consensus on which is the best for voting districts. Mathematician Moon Duchin at Tufts University in Medford, Massachusetts, has spent the past few years trying to devise a compactness metric for gerrymandering. \u201cBut the field is a giant mess,\u201d she says. Complicating the issue even further, many districts have odd shapes owing to rivers and other natural boundaries. Mattingly and Graves developed a compactness score calculated as the length of a district\u2019s perimeter squared divided by its area, a version of what's known as the Polsby\u2013Popper measure (see \u2018Compact division\u2019). A circle has the lowest ratio of perimeter to area; but as borders meander to include and exclude specific areas, the perimeter expands, giving a higher ratio. With thousands of maps and their resulting voting outcomes in hand, Mattingly and Graves could begin to analyse just how gerrymandered the North Carolina voting districts were. Three of the 13 districts for the 2012 elections were more than three-quarters Democrat, much more packed than in any of the team\u2019s randomly drawn maps, even for their bluest-of-blue Democratic districts. More telling, however, was the impact on election outcomes. Using the randomly drawn maps, 7.6 seats went to Democrats on average, compared with the 4 they actually won (J. Mattingly and C. Vaughn Preprint at  http://arxiv.org/abs/1410.8796 ; 2014 ). \u201cThe more you learn, the more infuriating it gets,\u201d Mattingly says. Their analysis of data from other states revealed a partisan gerrymander in Maryland perpetrated by the Democrat-controlled legislature to freeze out its conservative rivals. States such as Arizona and Iowa, which have independent or bipartisan commissions that oversee the creation of voting districts, fared much better. In a separate analysis, Daniel McGlone, a geographic-information-system data analyst at the technology firm Azavea in Philadelphia, Pennsylvania,  ranked each state\u2019s voting districts  for compactness as a measure of gerrymandering, and found that Maryland had the most-gerrymandered districts. North Carolina came second. Nevada, Nebraska and Indiana were the least gerrymandered. \n               Measuring up \n             In the summer of 2016, a bipartisan panel of retired judges met to see whether they could create a more representative set of voting districts for North Carolina. Their maps gave Mattingly a chance to test his index. The judges\u2019 districts, he found, were less gerrymandered than in 75% of the computer-generated models \u2014 a sign of a well-drawn, representative map. By comparison, every one of the 24,000 computer-drawn districts was less gerrymandered than either the 2012 or 2016 voting districts drawn by state legislators, which Mattingly, Graves and their colleagues reported in April 2017 (S.Bangia  et al . Preprint at http://arxiv.org/abs/1704.03360; 2017 ). \u201cThis is the result that I hope gets traction,\u201d Mattingly says. \u201cIt shows that the election results really didn\u2019t represent the will of the people.\u201d When representatives from Common Cause, a pro-democracy advocacy group based in Washington DC, saw the work, they asked Mattingly to serve as an expert witness in a North Carolina partisan-gerrymandering case coming up this summer. The question for researchers and judges, however, is whether Mattingly\u2019s approach is the best. Mathematicians in other states have also been developing methods for evaluating gerrymandering. At the University of Illinois Urbana\u2013Champaign, political statistician Wendy Tam Cho has designed algorithms to draw district maps that use the criteria mandated by state law, but do not include partisan information such as an area\u2019s voting history. By altering the importance of the compactness score, or how equal the different populations in each district need to be, she can generate a new set of districts. Cho measures how closely a state\u2019s existing legislative districts line up with billions of non-partisan maps drawn by her supercomputing cluster. If they diverge significantly, then the people who drew the districts probably had partisan motives for placing the lines where they did, Cho says. Cho\u2019s approach creates more maps than Mattingly\u2019s, which she says gives it an advantage. But Mattingly argues that his algorithms are more transparent and so can be used to calculate a score that judges might prefer. Both strategies are highly technical and require professional expertise to implement and interpret, says Sam Wang, a neuroscientist at Princeton University, New Jersey, who analyses elections and voting in his spare time at the blog Princeton Election Consortium. \u201cThe Supreme Court has said it is looking for a \u2018manageable\u2019 standard. For constitutional questions, judges might find it more manageable to avoid having to call upon outside experts,\u201d Wang says. Political scientist Nicholas Stephanopoulos at the University of Chicago, Illinois, takes a much simpler approach to measuring gerrymandering. He has developed what he calls an \u201cefficiency gap\u201d, which measures a state\u2019s wasted votes: all those cast for a losing candidate in each district, and all those for the victor in excess of the proportion needed to win. If one party has lots of landslide victories and crushing losses compared with its rivals, this can be a sign of gerrymandering. The simplicity of this metric is a strength, says Wang. But Duchin argues that methods that analyse only one aspect of gerrymandering, whether it\u2019s lopsided wins or low compactness scores, are less than ideal. She favours a metric, such as Mattingly\u2019s, that incorporates the variety of factors that contribute. Michael McDonald, a political scientist at the University of Florida in Gainesville, questions the validity of all these quantitative metrics, however, because they rely on creating a random sample of all possible voting districts. It is impossible to calculate how random a sample they are looking at, he argues. \u201cThere are more ways to draw voting districts in the US than there are quarks in the Universe.\u201d Accusations of gerrymandering have also cropped up in the United Kingdom. Until 20 years ago, the creation of voting districts by the independent Boundary Commissions was a largely apolitical process, according to geographer Ron Johnston at the University of Bristol, UK. In the 1990s, supporters of the Labour party, then in opposition, realized that they could influence the creation of parliamentary constituencies by submitting their own maps to the Boundary Commissions for consideration, which opened the door to all parties jockeying for power, Johnston says. An overhaul of UK constituencies currently under way could cut the number of Members of Parliament by 50; the final result of the Boundary Commissions' review is expected in 2018. Political parties are expected to try to shift the results in their favour, but quantitative solutions could help to depoliticize the process. \n               Solution in sight \n             US legislators have been reluctant to embrace a mathematical solution to gerrymandering. But current court cases show that pressure to do so is mounting, Gall says. In the Wisconsin case  Whitford  v.  Gill , federal judges used the efficiency gap to rule that the state\u2019s voting districts represented an unconstitutional partisan gerrymander. The case could end up before the Supreme Court later this year. If judges are to accept a mathematical test for gerrymandering, they will need testimony from expert witnesses such as Mattingly to explain how and why these tests work. But the handful of mathematicians researching the subject will not be enough for the country\u2019s pending lawsuits. Even if the courts settle on a standard metric, judges might need an expert in each case. That\u2019s why Duchin is organizing a week-long summer camp to help mathematicians learn the underlying subtleties of the various gerrymandering models and how to apply and explain them. Duchin expected 50 people to sign up; more than 1,000 have applied. \u201cThe response blew us out of the water,\u201d she says, and several camps will now be held. Mattingly and his model will have their day in court this summer. Even if his algorithms don\u2019t become the standard, Mattingly hopes that the judicial system will find a way to curb gerrymandering and restore his faith in the electoral system. \u201cI\u2019m a citizen, too,\u201d he says. \n                 Tweet \n                 Follow @NatureNews \n               \n                     How Facebook, fake news and friends are warping your memory 2017-Mar-07 \n                   \n                     Compare voting systems to improve them 2017-Jan-10 \n                   \n                     Post-truth: a guide for the perplexed 2016-Nov-28 \n                   \n                     The polling crisis: How to tell what people really think 2016-Oct-19 \n                   \n                     The elephant in the room we can\u2019t ignore 2016-Mar-16 \n                   \n                     Democracy isn't all it is cracked up to be 2012-Jan-27 \n                   \n                     Democracy isn't all it is cracked up to be 2012-Jan-27 \n                   \n                     Nature  special: Science and the US election \n                   Reprints and Permissions"},
{"file_id": "546590a", "url": "https://www.nature.com/articles/546590a", "year": 2017, "authors": [{"name": "Philip Ball"}], "parsed_as_year": "2006_or_before", "body": "Logic-defying experiments in quantum causality can twist the notion of time itself. Albert Einstein is heading out for his daily stroll and has to pass through two doorways. First he walks through the green door, and then through the red one. Or wait \u2014 did he go through the red first and then the green? It must have been one or the other. The events had have to happened in a sequence, right? Not if Einstein were riding on one of the photons ricocheting through Philip Walther's lab at the University of Vienna. Walther's group has shown that it is impossible to say in which order these photons pass through a pair of gates as they zip around the lab. It's not that this information gets lost or jumbled \u2014 it simply doesn't exist. In Walther's experiments, there is no well-defined order of events. This finding 1  in 2015 made the quantum world seem even stranger than scientists had thought. Walther's experiments mash up causality: the idea that one thing leads to another. It is as if the physicists have scrambled the concept of time itself, so that it seems to run in two directions at once. In everyday language, that sounds nonsensical. But within the mathematical formalism of quantum theory, ambiguity about causation emerges in a perfectly logical and consistent way. And by creating systems that lack a clear flow of cause and effect 2 , researchers now think they can tap into a rich realm of possibilities. Some suggest that they could boost the already phenomenal potential of quantum computing. \u201cA quantum computer free from the constraints of a predefined causal structure might solve some problems faster than conventional quantum computers,\u201d says quantum theorist Giulio Chiribella of the University of Hong Kong. What's more, thinking about the 'causal structure' of quantum mechanics \u2014 which events precede or succeed others \u2014 might prove to be more productive, and ultimately more intuitive, than couching it in the typical mind-bending language that describes photons as being both waves and particles, or events as blurred by a haze of uncertainty. And because causation is really about how objects influence one another across time and space, this new approach could provide the first steps towards uniting the two cornerstone theories of physics and resolving one of the most profound scientific challenges today. \u201cCausality lies at the interface between quantum mechanics and general relativity,\u201d says Walther's collaborator \u010caslav Brukner, a theorist at the Institute for Quantum Optics and Quantum Information in Vienna, \u201cand so it could help us to think about how one could merge the two conceptually.\u201d \n               Tangles in time \n             Causation has been a key issue in quantum mechanics since the mid-1930s, when Einstein challenged the apparent randomness that Niels Bohr and Werner Heisenberg had installed at the heart of the theory. Bohr and Heisenberg's Copenhagen interpretation insisted that the outcome of a quantum measurement \u2014 such as checking the orientation of a photon's plane of polarization \u2014 is determined at random, and only in the instant that the measurement is made. No reason can be adduced to explain that particular outcome. But in 1935, Einstein and his young colleagues Boris Podolsky and Nathan Rosen (now collectively denoted EPR) described a thought experiment that pushed Bohr's interpretation to a seemingly impossible conclusion. The EPR experiment involves two particles, A and B, that have been prepared with interdependent, or 'entangled', properties. For example, if A has an upward-pointing 'spin' (crudely, a quantum property that can be pictured a little bit like the orientation of a bar magnet), then B must be down, and vice versa. Both pairs of orientations are possible. But researchers can discover the actual orientation only when they make a measurement on one of the particles. According to the Copenhagen interpretation, that measurement doesn't just reveal the particle's state; it actually fixes it in that instant. That means it also instantly fixes the state of the particle's entangled partner \u2014 however far away that partner is. But Einstein considered this apparent instant action at a distance impossible, because it would require faster-than-light interaction across space, which is forbidden by his special theory of relativity. Einstein was convinced that this invalidated the Copenhagen interpretation, and that particles A and B must already have well-defined spins before anybody looks at them. Measurements of entangled particles show, however, that the observed correlation between the spins can't be explained on the basis of pre-existing properties. But these correlations don't actually violate relativity because they can't be used to communicate faster than light. Quite how the relationship arises is hard to explain in any intuitive cause-and-effect way. But what the Copenhagen interpretation does at least seem to retain is a time-ordering logic: a measurement can't induce an effect until after it has been made. For event A to have any effect on event B, A has to happen first. The trouble is that this logic has unravelled over the past decade, as researchers have realized that it is possible to imagine quantum scenarios in which one simply can't say which of two related events happens first. Classically, this situation sounds impossible. True, we might not actually know whether A or B happened first \u2014 but one of them surely did. Quantum indeterminacy, however, isn't a lack of knowledge; it's a fundamental prohibition on pronouncing on any 'true state of affairs' before a measurement is made. \n               Ambiguous action \n             Brukner's group in Vienna, Chiribella's team and others have been pioneering efforts to explore this ambiguous causality in quantum mechanics 3 , 4 . They have devised ways to create related events A and B such that no one can say whether A preceded and led to (in a sense 'caused') B, or vice versa. This arrangement enables information to be shared between A and B in ways that are ruled out if there is a definite causal order. In other words, an indeterminate causal order lets researchers do things with quantum systems that are otherwise impossible. The trick they use involves creating a special type of quantum 'superposition'. Superpositions of quantum states are well known: a spin, for example, can be placed in a superposition of up and down states. And the two spins in the EPR experiment are in a superposition \u2014 in that case involving two particles. It's often said that a quantum object in a superposition exists in two states at once, but more properly it simply cannot be said in advance what the outcome of a measurement would be. The two observable states can be used as the binary states (1 and 0) of quantum bits, or qubits, which are the basic elements of quantum computers. The researchers extend this concept by creating a causal superposition. In this case, the two states represent sequences of events: a particle goes first through gate A and then through gate B (so that A's output state determines B's input), or vice versa. In 2009, Chiribella and his co-workers came up with a theoretical way to do an experiment like this using a single qubit as a switch that controls the causal order of events experienced by a particle that acts as second qubit 3 . When the control-switch qubit is in state 0, the particle goes through gate A first, and then through gate B. When the control qubit is in state 1, the order of the second qubit is BA. But if that qubit is in a superposition of 0 and 1, the second qubit experiences a causal superposition of both sequences \u2014 meaning there is no defined order to the particle's traversal of the gates (see 'Trippy journeys'). Three years later, Chiribella proposed an explicit experimental procedure for enacting this idea 5 ; Walther, Brukner and their colleagues subsequently worked out how to implement it in the lab 1 . The Vienna team uses a series of 'waveplates' (crystals that change a photon's polarization) and partial mirrors that reflect light and also let some pass through. These devices act as the logic gates A and B to manipulate the polarization of a test photon. A control qubit determines whether the photon experiences AB or BA \u2014 or a causal superposition of both. But any attempt to find out whether the photon goes through gate A or gate B first will destroy the superposition of gate ordering. Having demonstrated causal indeterminacy experimentally, the Vienna team wanted to go further. It's one thing to create a quantum superposition of causal states, in which it is simply not determined what caused what (that is, whether the gate order is AB or BA). But the researchers wondered whether it is possible to preserve causal ambiguity even if they spy on the photon as it travels through various gates. At face value, this would seem to violate the idea that sustaining a superposition depends on not trying to measure it. But researchers are now realizing that in quantum mechanics, it's not exactly what you do that matters, but what you know. Last year, Walther and his colleagues devised a way to measure the photon as it passes through the two gates without immediately changing what they know about it 6 . They encode the result of the measurement in the photon itself, but do not read it out at the time. Because the photon goes through the whole circuit before it is detected and the measurement is revealed, that information can't be used to reconstruct the gate order. It's as if you asked someone to keep a record of how they feel during a trip and then relay the information to you later \u2014 so that you can't deduce exactly when and where they were when they wrote it down. As the Vienna researchers showed, this ignorance preserves the causal superposition. \u201cWe don't extract any information about the measurement result until the very end of the entire process, when the final readout takes place,\u201d says Walther. \u201cSo the outcome of the measurement process, and the time when it was made, are hidden but still affect the final result.\u201d Other teams have also been creating experimental cases of causal ambiguity by using quantum optics. For example, a group at the University of Waterloo in Canada and the nearby Perimeter Institute for Theoretical Physics has created quantum circuits that manipulate photon states to produce a different causal mash-up. In effect, a photon passes through gates A and B in that order, but its state is determined by a mixture of two causal procedures: either the effect of B is determined by the effect of A, or the effects of A and B are individually determined by some other event acting on them both, in much the same way that a hot day can increase sunburn cases and ice-cream sales without the two phenomena being directly causally related. As with the Vienna experiments, the Waterloo group found that it's not possible to assign a single causal 'story' to the state the photons acquire 7 . Some of these experiments are opening up new opportunities for transmitting information. A causal superposition in the order of signals travelling through two gates means that each can be considered to send information to the other simultaneously. \u201cCrudely speaking, you get two operations for the price of one,\u201d says Walther. This offers a potentially powerful shortcut for information processing. Although it has long been known that using quantum superposition and entanglement could exponentially increase the speed of computation, such tricks have previously been played only with classical causal structures. But the simultaneous nature of pathways in a quantum-causal superposition offers a further boost in speed. That potential was apparent when such superpositions were first proposed: quantum theorist Lucien Hardy at the Perimeter Institute 8  and Chiribella and his co-workers 3  independently suggested that quantum computers operating with an indefinite causal structure might be more powerful than ones in which causality is fixed. Last year, Brukner and his co-workers showed 9  that building such a shortcut into an information-processing protocol with many gates should give an exponential increase in the efficiency of communication between gates, which could be beneficial for computation. \u201cWe haven't reached the end yet of the possible speed-ups,\u201d says Brukner. \u201cQuantum mechanics allows way more.\u201d It's not terribly complicated to build the necessary quantum-circuit architectures, either \u2014 you just need quantum switches similar to those Walther has used. \u201cI think this could find applications soon,\u201d Brukner says. \n               Unity in the Universe \n             The bigger goal, however, is theoretical. Quantum causality might supply a point of entry to some of the hardest questions in physics \u2014 such as where quantum mechanics comes from. Quantum theory has always looked a little ad hoc. The Schr\u00f6dinger equation works marvellously to predict the outcomes of quantum experiments, but researchers are still arguing about what it means, because it's not clear what the physics behind it is. Over the past two decades, some physicists and mathematicians, including Hardy 10  and Brukner 11 , have sought to clarify things by building 'quantum reconstructions': attempts to derive at least some characteristic properties of quantum-mechanical systems \u2014 such as entanglement and superpositions \u2014 from simple axioms about, say, what can and can't be done with the information encoded in the states (see   Nature   501 , 154\u2013156; 2013). \u201cThe framework of causal models provides a new perspective on these questions,\u201d says Katja Ried, a physicist at the University of Innsbruck in Austria who previously worked with the University of Waterloo team on developing systems with causal ambiguity. \u201cIf quantum theory is a theory about how nature processes and distributes information, then asking in which ways events can influence each other may reveal the rules of this processing.\u201d And quantum causality might go even further by showing how one can start to fit quantum theory into the framework of general relativity, which accounts for gravitation. \u201cThe fact that causal structure plays such a central role in general relativity motivates us to investigate in which ways it can 'behave quantumly',\u201d says Ried. \u201cMost of the attempts to understand quantum mechanics involve trying to save some aspects of the old classical picture, such as particle trajectories,\u201d says Brukner. But history shows us that what is generally needed in such cases is something more, he says \u2014 something that goes beyond the old ideas, such as a new way of thinking about causality itself. \u201cWhen you have a radical theory, to understand it you usually need something even more radical.\u201d\n \n                 Tweet \n                 Follow @NatureNews \n               \n                     Battle between quantum and thermodynamic laws heats up 2017-Mar-29 \n                   \n                     Cosmic test backs 'quantum spookiness' 2017-Feb-02 \n                   \n                     Physicists propose football-pitch-sized quantum computer 2017-Feb-01 \n                   \n                     Quantum computers ready to leap out of the lab in 2017 2017-Jan-03 \n                   \n                     The quantum source of space-time 2015-Nov-16 \n                   \n                     Quantum physics: What is really real? 2015-May-20 \n                   \n                     Nature  special: The quantum atom \n                   \n                     Nature  special: Light fantastic \n                   Reprints and Permissions"},
{"file_id": "546466a", "url": "https://www.nature.com/articles/546466a", "year": 2017, "authors": [{"name": "Alexandra Witze"}], "parsed_as_year": "2006_or_before", "body": "Geophysicists are ramping up their efforts to monitor major undersea faults for movement, and search for signs of the next catastrophic quake. Jerry Paros is worried about the geological time bomb ticking away just off the coast near his home in Washington state. But unlike the millions of people who fear the earthquake and tsunami that will one day rock that region, Paros is doing something about it. His company made millions of dollars building exquisitely precise quartz sensors for oil, gas and other industry applications. Now he wants to use them to save the world from natural disasters. At the Redmond headquarters of his company, Paroscientific, the 79-year-old inventor picks up a volleyball-sized metallic rack from a table, lifts it to shoulder height and then lowers it. Inside the contraption, sensors pick up the tiny change in atmospheric pressure as the device travels up and down. \u201cHere, I'll give you a very expensive doorbell,\u201d he says, opening and shutting the office door to change the air pressure yet again. In the air, Paros's instrument can register such delicate shifts in pressure. But the ultimate destination of this device is offshore, a few kilometres below the waves, where it will sense the weight of the water above it to detect changes in the depth of the sea floor. Paros wants his ultra-precise gauges to be the heart of an early-warning system designed to detect when an earthquake shifts the sea floor, unleashing a tsunami. He has donated US$2 million to the University of Washington and collaborated with its researchers to test the sensors off the coastline of the Pacific Northwest. Many other coastal nations, including Japan and Chile, are working to monitor the movements of the ocean bottom, an effort known as sea-floor geodesy. They are racing to install sensors because geological faults in these regions produce the most powerful earthquakes on the planet \u2014 and some of the most devastating disasters. In 2004, a subsea quake off Indonesia triggered a tsunami that killed nearly a quarter of a million people. Geophysicists have  long struggled to get a handle on the behaviour of offshore faults , but sensors such as the one created by Paros are giving them their first opportunity to spy on geodetic movements of the 70% of Earth's crust that is covered by water, making it inaccessible to standard tools. These networks could reveal which parts of undersea faults are slipping harmlessly and which parts might be storing energy for the next big quake. \u201cIt will help us answer the big question of where are those zones,\u201d says Emily Roland, an oceanographer at the University of Washington in Seattle, who works with Paros. \u201cIt's the thing that we've been missing.\u201d \n               Sleeping giant \n             When Paros first moved to the Pacific Northwest in 1970, few recognized the region's risk for giant earthquakes. The largest shock in the region's recorded history was a magnitude-7.1 jolt that struck Olympia, Washington, in 1949. But by the late 1980s, researchers started to uncover signs that the entire coastline from northern California to southern British Columbia in Canada can experience magnitude-9 earthquakes and giant tsunamis. The source of danger lies about 50 kilometres offshore, where one patch of Earth's outer shell dives beneath another. Called the Cascadia subduction zone, this junction is 1,000 kilometres long and is part of the \u2018ring of fire\u2019, a series of similar features that encircle the Pacific Ocean. Subduction zones produce the biggest earthquakes ever measured, including the record-setting magnitude-9.5 quake in Chile in 1960. In 1700, Cascadia ruptured in an estimated magnitude-9 quake, releasing a tsunami that annihilated villages along the Cascadian coast and raced across the Pacific Ocean, drowning people in Japan as well. Seismologists aren't sure when the next big one might strike Cascadia. It could be tomorrow, or it could be centuries from now. At other subduction zones, scientists monitor geological activity and assess the risk of future big quakes by listening to the patterns of smaller ones. Cascadia, however, is \u201ceerily quiet\u201d, says Kelin Wang, a seismologist at the Geological Survey of Canada in Sidney, British Columbia. It experiences very few of the small quakes that might otherwise illuminate how the two tectonic plates are moving against one another. That makes Cascadia something of a sleeping giant \u2014 and a dangerous one, with major cities such as Portland and Seattle at risk. On land, engineers can use measurements from the Global Positioning System (GPS) to track the more subtle signs of geological unrest \u2014 including the ground uplifting around a volcano before it erupts, or rocks sliding along major geological faults, such as the San Andreas fault in California. But making these measurements on the sea floor is difficult and expensive. Only in the past few years has sea-floor geodesy started to catch up with its land-based counterpart, thanks to new tools and innovative ways to deploy them in the ocean (see \u2018Underwater threat\u2019). From New Zealand and Japan to Chile, geophysicists are working to understand the long-term geological risk and to develop ways to alert coastal communities about earthquakes and tsunamis that have already begun. Much of the work is based on government-funded networks of sea-floor sensors. Other networks have private support, from funders such as Paros. Six of his quartz pressure sensors currently rest on the sea floor off Oregon, monitoring which parts of Cascadia are creeping along slowly and which parts are locked in place. From GPS measurements on land, geophysicists have developed two competing models for Cascadia (G. M. Schmalzle  et al .  Geochem. Geophys. Geosyst.   15 , 1515\u20131532; 2014 ). In one, the descending tectonic plate is moving very slowly beneath the upper plate, releasing strain as it creeps along. In the other, the two plates are locked together, allowing the dangerous build-up of strain. \n               Take the strain \n             Using only land-based instruments, there is no way to tell which of the models is correct \u2014 if either. \u201cWe just don't know to what degree it's locked,\u201d says Wang. \u201cThat's why we need offshore measurements. We've kind of exhausted the information from land-based observations.\u201d From time to time,  oceanographers have peppered Cascadia's sea floor with monitoring instruments . A team led by the University of Washington and the Scripps Institution of Oceanography in La Jolla, California, has been working to create a system that can measure movements of the sea floor over time and identify the nature of the threat. Key to that work is Paros's quartz sensor. Fifty years ago, Paroscientific began developing quartz sensors to measure physical factors such as acceleration, pressure changes and temperature. The sensors rely on the piezoelectric qualities of quartz \u2014 it generates an electrical charge when squeezed. When sent down to the sea floor, a Paroscientific pressure sensor measures the changing pressure of the water column above it. After correcting for factors such as waves and tides, oceanographers can detect up or down movements of the sea floor to within about 1 centimetre.  We've kind of exhausted the information from land-based observations.  Paroscientific is one of many companies that manufacture oceanographic pressure sensors. But Paros himself is an unusual mix: an entrepreneur-turned-amateur-scientist, who now hobnobs with many of the region's leading geophysicists. \u201cJerry likes interacting with the engineers and technically minded scientists,\u201d says William Wilcock, a marine geophysicist at the University of Washington. \u201cHe really does push the community forward with his single-minded desire to get this done.\u201d As early as 1983, Paroscientific sensors were sent out into the Pacific as part of the US National Oceanic and Atmospheric Administration's tsunami-observing system. In 2006, shaken by the devastation of the Indian Ocean tsunami two years earlier, Paros gave $1 million to the University of Washington to stimulate research in sensor networks. That money, plus another $1 million in 2012, helped university researchers to design and test new generations of ocean-bottom pressure sensors (G. Sasagawa and M. A. Zumberge  IEEE J. Ocean. Eng.   38 , 447\u2013454; 2013 ). The latest ocean-bottom gauges developed by the Scripps\u2013Washington team are arranged in a rough line from near the Oregon coast all the way to the subduction zone. They rest there quietly, taking the pulse of the water above them. Researchers can compare those data to their models of how Cascadia is slipping. \u201cWithin a decade, we will know if the fault is locked,\u201d says Wilcock, who helps to lead the effort. But even the best pressure sensors can reveal only one aspect of sea-floor motion \u2014 up and down. They cannot detect horizontal shifts. For that, researchers must turn to a different technique, which involves two or more transponders that sit 2\u20133 kilometres apart on the sea floor. Every year or so, scientists visit the transponder locations by ship, and ping acoustic signals to the devices. By measuring the time it takes for the signals to travel through the water, the researchers can tell whether the transponders have shifted relative to one another since the last visit, and hence whether the sea floor has moved horizontally. \n               The sound of movement \n             This type of sea-floor acoustic ranging is used around the world. The GEOMAR Helmholtz Centre for Ocean Research in Kiel, Germany, installed such a network along the subduction zone off Chile in late 2015, to monitor earthquake threats there. Japan's coast guard spends months every year collecting data at dozens of sites off the country's coastline. And by using autonomous vehicles called Wave Gliders to gather data, rather than using ships, the studies can be done at a fraction of the cost, says David Chadwell, a geophysicist at Scripps. \u201cIt's been a transformation,\u201d says Chadwell, who has been testing the Wave Gliders off Oregon and hopes soon to bring them into wider use. To understand Cascadia's true danger, geophysicists need to deploy many types of tool, including seismometers as well as geodetic instruments, both offshore and on land. But geophysicists debate where to place these sensors and how many of each type would be ideal. The disagreements sometimes come down to a split between those pursing basic research and those focusing on developing early-warning systems for earthquakes and tsunamis. The University of Washington researchers hope that their network can serve both groups. \u201cWe need to, and can, make these scientific instruments serve multiple purposes, advancing scientific understanding as well as monitoring for hazards,\u201d says seismologist Heidi Houston, who is also at the University of Washington but not part of the sensor-network effort. In early April, during a damp couple of days on the University of Washington campus, leading researchers gathered to brainstorm about the best way to monitor Cascadia's danger. After two days of talks, the attendees broke into small groups to design their ideal network. Each group got a large printout of a Cascadia coastal map, a bunch of coloured pens and an exhortation to dream big. \u201cWho's ready to draw?\u201d asked Wilcock as he shepherded the groups into breakout rooms. Some groups envisioned lines of sea-floor geodetic arrays off the coast, with Wave Gliders passing to collect data. Studded among them were seismometers to measure ongoing earthquake activity, and tsunami-alert buoys to warn of any dangerous waves. Other groups sketched powered cables laid across the sea floor, festooned with scientific instruments. Instead of using gliders or buoys to transmit data, these arrays would send their information back to shore directly through the cable. Two basic observatories already exist in Cascadia. The Ocean Observatories Initiative Cabled Array operates a 900-kilometre-long cable that runs from the coast of Oregon out to an underwater volcano and back again. On the northern side of the border,  Ocean Networks Canada has a similar-length cable  looping out to the subduction zone. Both carry geodetic and seismic instruments at several nodes along their length. The cables dreamed up at the workshop would be a massive expansion of those. They would more closely resemble a $100-million Japanese sea-floor observatory called DONET-2, completed last year in the Nankai trough, part of a subduction zone near the cities of Osaka and Kobe. Its backbone cable runs for 500 kilometres and has 29 separate observatories studded along it, says Katsuyoshi Kawaguchi, the deputy director of the observatory at the Japan Agency for Marine-Earth Science and Technology in Yokosuka.  The problem with tsunamis that happen every 300 years is that you can't get much traction with officials.  A second, even more ambitious project is under way in Japan to string 150 observatories along 5,700 kilometres of powered cable. The $320-million S-NET project is being installed in stages offshore, south of Hokkaido. The first segments began operating in May 2016, and the deepest-water section is being installed in the next few months. Each of the observatories includes Paroscientific pressure sensors at a cost of roughly $50,000 per package. Data from both Japanese observatories feed into the nationwide early-warning system for earthquakes and tsunamis, which was radically beefed up after the 2011 Tohoku earthquake killed nearly 16,000 people. That event also sent a tsunami into the Fukushima nuclear power plant,  triggering a reactor accident and a nationwide energy crisis . One day, Paros would like to see his sensors peppering the sea floor off Cascadia as part of a broad monitoring network for natural hazards. \u201cThe problem with tsunamis that happen every 300 years is that you can't get much traction with local officials,\u201d he says as he drives around Seattle in a sensible car, his Ford Five Hundred, with personalized plates reading QUARTZ. And so he goes out among the scientists, working to get his pressure gauges into as many oceans as he can. Last week, University of Washington engineers deployed a new set of sensors on a small cabled sea-floor observatory off Monterey, California; it will remain there for a number of months for testing. \u201cI've been doing this Sisyphus thing, rolling the boulder up the hill, for so long,\u201d Paros says. \u201cI just want to plant the seeds to show this is feasible, with the hope the government will recognize that this is an important public-safety issue.\u201d \n                 See Editorial \n                 p.451 \n               \n                     Seabed samples cast doubt on earthquake risk for Pacific Northwest 2014-Aug-04 \n                   \n                     Canadian quake refines Pacific tsunami risk 2013-Jun-21 \n                   \n                     Tsunami forecasting: The next wave 2012-Mar-07 \n                   \n                     Earthquakes from the ocean: Danger zones 2011-Aug-24 \n                   \n                     Giant shock rattles ideas about quake behaviour 2011-Mar-15 \n                   \n                     Cascadia quake zone gets wired up 2010-Jan-28 \n                   \n                     Paroscientific \n                   \n                     Early-warning offshore Cascadia workshop \n                   Reprints and Permissions"},
{"file_id": "545280a", "url": "https://www.nature.com/articles/545280a", "year": 2017, "authors": [{"name": "Jeff Tollefson"}], "parsed_as_year": "2006_or_before", "body": "Large timber buildings are getting safer, stronger and taller. They may also offer a way to slow down global warming. One building stands out in the old logging town of Prince George, Canada. Encased in a sleek glass facade, the structure towers above most of its neighbours, beckoning from afar with the warm amber glow of Douglas fir. Constructed almost entirely from timber in 2014, the 8-storey, 30-metre building is among the tallest modern wooden structures in the world. But it is more than an architectural marvel. As the home of the Wood Innovation and Design Centre at the University of Northern British Columbia (UNBC), it is also an incubator for wooden buildings of the future \u2014 and a herald for a movement that could help to tackle global warming. The building, which is owned by the government of British Columbia, is less like a log cabin and more like a layered cake, constructed from wooden planks glued and pressed together, precision cut by factory lasers and then assembled on site. All told, the university avoided the release of more than 400 tonnes of carbon dioxide by eschewing energy-intensive concrete and steel, and the building locks up a further 1,100 tonnes of CO 2  that was harvested from the atmosphere by British Columbian trees. In total, that's enough to offset the emissions from 160 households for a year. Wooden construction has ancient roots, but only in the past two decades have scientists, engineers and architects begun to recognize its potential to stave off global warming. By substituting concrete and steel with wood from sustainably managed forests, the building industry could curb up to 31% of global carbon emissions, according to research 1  by Chad Oliver, a forest ecologist at Yale University in New Haven, Connecticut. In time, such a shift could help humanity to pull CO 2  out of the atmosphere, potentially reversing the course of climate change. \u201cIt's the plywood miracle,\u201d says Christopher Schwalm, an ecologist at Woods Hole Research Center in Falmouth, Massachusetts. \u201cThis is something that could have a significant impact on the riddle that is global environmental change.\u201d The renaissance in tall wooden buildings is already under way. Norway set a world height record in late 2015 with a 52.8-metre tower block; that was edged out in September 2016 by a 53-metre student dormitory at the University of British Columbia in Vancouver. This year, Austria will take the lead with the 84-metre HoHo building in Vienna, comprising a hotel, apartments and offices. The United States saw its first tall wooden building go up in Minneapolis, Minnesota, in 2016, and others are in the works in Portland, Oregon, and in New York City. Wooden construction has attracted political interest in part because of the economic benefits for rural communities surrounded by forests. But turning these pioneering projects into a global trend won't be easy. Building costs are often high, and the global construction industry is almost entirely focused on concrete and steel, particularly when it comes to big buildings. And the climate benefits of building with wood hinge on a questionable assumption: that the world's forests will be managed sustainably. Some researchers worry that harvesting more timber could harm forest ecosystems, particularly in developing countries that are already plagued by poor and often illegal logging practices. \u201cIf we're going to cut wood, we've got to do it in a way that not only sustains the forest but also sustains the biodiversity and everything else,\u201d says Oliver. \n               Timber technology \n             Steel and concrete weren't an option when Buddhist monks set about building a 32-metre pagoda at the Learning Temple of the Flourishing Law in Ikaruga, Japan, 14 centuries ago. They put their faith in wood, as did the monks at the Sakyamuni Pagoda in Yingxian, China. Erected in 1056, that structure rises a staggering 67 metres towards the heavens. These pagodas are still standing today, a testament to the strength and durability of wood. Kilogram for kilogram, wood is stronger than both steel and concrete, and wooden buildings are generally good at withstanding earthquakes. But wood has developed a bad reputation over the centuries, because of catastrophic blazes that levelled cities such as London, New York and Chicago before modern fire-suppression strategies emerged. In fact, in case of fire wood maintains its structurally integrity much better than the non-flammable alternatives favoured by modern building codes. It chars at a predictable rate, and doesn't melt like steel or weaken like concrete. \u201cThe fact that it actually can withstand fire better than steel took a long time for people to realize,\u201d says Guido Wimmers, who chairs a master's programme in wood engineering at UNBC. By some accounts, the modern era of tall wooden buildings began 20 years ago, with a simple experiment at the Technical University of Graz in Austria. Researchers glued layers of standard planks perpendicular to each other, and discovered that alternating the direction of the grain effectively negated the imperfections and weaknesses in any given plank. The result, known as cross-laminated timber, is a strong and lightweight wood panel that puts conventional plywood to shame. It can be made as large as desired and cut with sub-millimetre precision at the factory, which speeds up construction and reduces waste. And given the strength of these panels, there's no theoretical limit to how high wooden buildings can grow. \u201cIt transforms wood from a suburban material to a very urban material,\u201d says Michael Green, the Vancouver-based architect behind the design centre in Prince George, and a leading advocate for wooden construction. Wimmers says the initial goal of the technology was to make better use of low-grade wood products. \u201cThe wood construction industry was slowly vanishing, so they started to reinvent themselves,\u201d he says. Then the market for advanced timber technologies \u2014 including beams that are either glued or nailed together to increase strength \u2014 expanded as European countries put strict regulations on energy efficiency and greenhouse-gas emissions, forcing architects to reduce the climate footprints of their buildings. Wimmers estimates that in Europe, wood is now used in about 25% of residential construction, up from 5\u201310% in the 1990s. The science of safety and engineering has also advanced. Douglas fir \u2014 the exposed layer at the design centre \u2014 chars at 39 millimetres per hour. The provincial building code requires that the structure be able to endure at least one hour of fire on any given storey, so Green's team opted for floors made of a 5-layer panel that could afford to sacrifice a portion without losing its structural integrity. Meanwhile, Wimmers's team is collaborating on the Tall Wood Project, funded by the US National Science Foundation, to improve earthquake resistance for high wooden buildings. Work by the consortium has shown that the buildings can withstand earthquakes as well as or better than concrete and steel 2 , and the researchers will begin testing a two-storey wooden structure on a quake-simulator table at the University of California, San Diego, in June. They aim to test a ten-storey building there by 2020. Asif Iqbal, a civil engineer who is working on the project, came to UNBC from New Zealand, where he saw the damage from the 2011 earthquake in Christchurch at first hand. Most of the steel-reinforced concrete buildings in the city remained standing, but around 1,800 were irreparably damaged owing to cracked concrete and warped steel. Iqbal says that many of the replacement buildings are being constructed from wood, precisely because it is more likely to survive another major earthquake and the steel connectors can be replaced relatively easily if damaged. The long-term performance and economic viability of these buildings remains an open question. Wood is susceptible to mould and water damage, for example, and there is a higher risk of fire during construction. In September 2014, a \u00a320-million (US$26-million) wooden sustainable-chemistry building being built at the University of Nottingham, UK, was destroyed by an electrical fire \u2014 in part because fire doors and windows were not yet in place to contain the blaze. Still, advocates say the future looks bright. \u201cWe are still fine-tuning wood technologies, but so far we haven't found any major issues that we cannot solve,\u201d Iqbal says. \n               Tracking carbon \n             One of the main attractions of wooden construction is its potential to help stave off global warming. Oliver's research 1  suggests that humans currently harvest only 20% or so of the global forest growth each year, and more timber could be extracted without reducing the overall amount of carbon locked up in forests. The eventual climate impact of this harvest depends on the end use. If the wood were simply burned for energy, the CO 2  that the tree had absorbed years earlier would immediately return to the atmosphere. Regrowing forests eventually pull that CO 2  back out of the air, so the idea of carbon-neutral wood energy is a function of time. It is also controversial: some argue that current policies in Europe overstate the climate benefits of wood fuel and create perverse incentives to cut down trees. But this debate doesn't apply to wooden buildings. \u201cJust the fact that you have solid wood means that you are keeping CO 2  out of the atmosphere,\u201d says Oliver. Aside from the carbon sequestered in the wood itself, wooden construction offers further emissions savings. When researchers tallied the environmental impact of the design centre, they accounted for the manufacture and transport of every material \u2014 right down to the fossil-fuel-derived glue that binds the plywood together. Overall, the emissions related to construction were 12% of those for an equivalent concrete building 3 , largely owing to differences in fossil-fuel use. \u201cWhen you compare a wood building with a concrete building, wood wins every time,\u201d says Jim Bowyer, an emeritus engineer at the University of Minnesota in St Paul. The design centre might have a uniquely low carbon footprint at the outset, but over time its environmental impact will grow as its heating, cooling and lighting requirements generate greenhouse-gas emissions. Day-to-day energy use and maintenance account for 80\u201390% of lifetime emissions for a typical building, and unfortunately the design centre is no different. The consequence is that its long-term climate benefits are relatively modest. But the most advanced buildings today, which combine energy-efficient designs and technologies with on-site renewable energy generation, can eliminate emissions over the life of the structure. In such scenarios, construction and materials \u2014 the building's 'embodied emissions' \u2014 account for 100% of a building's climate impact, giving wood an increasingly important advantage. \u201cWe're moving towards really low-energy buildings,\u201d says Jennifer O'Connor, president of the Athena Sustainable Materials Institute, a non-profit research organization in Ottawa. \u201cQuite frankly, if we are going to make a difference, then we had better start looking at those embodied emissions.\u201d \n               The long game \n             The wooden-building movement is, for now, focused mostly on Europe and North America. In the United States, more than 80% of houses are already wood-based, says Bowyer. Yet with the nation's timber industry currently extracting roughly one-third of annual forest growth, there is capacity to expand wood construction in mid-rise commercial and industrial structures without reducing the volume of carbon that is locked up in forests. Bowyer is leading an expert assessment convened by the American Wood Council, an industry body in Leesburg, Virginia; the team has found that the United States could roughly double the amount of carbon that it sequesters in buildings each year, offsetting the emissions from nine additional coal-fired power plants. By contrast, builders in Europe still rely mostly on concrete and steel: a 2010 Finnish government report 4  estimated that a mere 4% increase in annual wood use in construction throughout Europe would avoid 150 million tonnes of carbon emissions, almost as much as the Netherlands emits each year. But to have a truly global impact, the movement must expand to developing countries, where forest management remains a challenge. Forests across the tropics are already being pillaged for timber and razed for agriculture. Indonesia, for example, has struggled to halt the palm-oil industry's destruction of rainforests. And although Brazil has made huge improvements in forest management over the past decade, demand for beef and soya beans is once again boosting land-clearing in the Amazon. Some fear that wooden construction would mean more trouble for some of the world's most precious ecosystems. \u201cI've seen enough abuses of what you would call the wood-product sector that I'm leery of sweeping solutions that make big assumptions,\u201d says William Laurance, a tropical ecologist at James Cook University in Cairns, Australia. Oliver argues that the push for wooden construction could help developing countries to establish sustainable industries that actually protect forests, if they are pursued in parallel with efforts to bolster governance. The challenge is to ensure that managed forests maintain the full suite of crucial ecosystems, including old-growth habitat and forest clearings. \u201cIt should all be preplanned and transparent,\u201d says Oliver. \u201cThat's kind of a utopia, but you've got to dream.\u201d He is working with the United Nations Development Programme (UNDP) to design a comprehensive forest-management plan that would kick-start modern wooden construction in Turkey. Government figures indicate that the country erected 956 million square metres of building space between 2004 and 2014, and just 0.13% of that total was framed in wood. Yet 27% of the country is forested, and 7 million of Turkey's poorest citizens live in these areas, says Nuri \u00d6zba\u011fdatl\u0131, a forestry expert with the UNDP in Ankara. \u201cWe want to create a new value chain for wood,\u201d he says. \u201cIt will start with the forest villages and end up with the construction sector.\u201d As wooden construction matures, it will face one final challenge: what happens when a building is decommissioned and torn down. Buddhist pagodas may last for centuries, but the general assumption for many modern buildings \u2014 including the design centre in Prince George \u2014 is that they will outlive their usefulness and be replaced in several decades. If the wood is dumped into landfill and left to rot, its carbon will slowly leak back into the atmosphere. But if the wood is recycled \u2014 reused in future construction projects, for example \u2014 then the climate benefits are locked in. Advocates of wood are pushing long-term strategies that promote recycling and other carbon-neutral options, but Green isn't too worried about the longevity of his building. Properly maintained, he says, there's no reason why it can't last as long as a Buddhist pagoda. Instead, he's focusing on getting this budding industry off the ground through a free online training course that will be translated into 30 languages, giving anybody with an interest \u2014 from architects and engineers to builders, developers and government officials \u2014 a more technical understanding of wooden construction. \u201cWe need to globalize the conversation,\u201d Green says. \u201cThis is the only hope of accelerating this to be competitive with concrete and steel, which have a 150-year head start.\u201d \n                 Tweet \n                 Follow @NatureNews \n                 Follow @jefftollef \n               \n                     Carbon is not the enemy 2016-Nov-14 \n                   \n                     Paris climate deal to take effect as EU ratifies accord 2016-Oct-04 \n                   \n                     Paris climate deal: what comes next 2016-Apr-22 \n                   \n                     The rise of the urbanite 2016-Mar-16 \n                   \n                     How cities can beat the heat 2015-Aug-26 \n                   \n                     How green is your campus? 2009-Sep-09 \n                   \n                     Architecture: Architects of a low-energy future 2008-Apr-02 \n                   \n                     UNBC wood-engineering programme \n                   \n                     Athena Sustainable Materials Institute \n                   \n                     NSF-funded Tall Wood Project \n                   Reprints and Permissions"},
{"file_id": "547024a", "url": "https://www.nature.com/articles/547024a", "year": 2017, "authors": [{"name": "Anna Nowogrodzki"}], "parsed_as_year": "2006_or_before", "body": "Aviv Regev is a maven of hard-core biological analyses. Now she is part of an effort to map every cell in the human body. Aviv Regev likes to work at the edge of what is possible. In 2011, the computational biologist was collaborating with molecular geneticist Joshua Levin to test a handful of methods for sequencing RNA. The scientists were aiming to push the technologies to the brink of failure and see which performed the best. They processed samples with degraded RNA or vanishingly small amounts of the molecule. Eventually, Levin pointed out that they were sequencing less RNA than appears in a single cell. To Regev, that sounded like an opportunity. The cell is the basic unit of life and she had long been looking for ways to explore how complex networks of genes operate in individual cells, how those networks can differ and, ultimately, how diverse cell populations work together. The answers to such questions would reveal, in essence, how complex organisms such as humans are built. \u201cSo, we're like, 'OK, time to give it a try',\u201d she says. Regev and Levin, who both work at the Broad Institute of MIT and Harvard in Cambridge, Massachusetts, sequenced the RNA of 18 seemingly identical immune cells from mouse bone marrow, and found that some produced starkly different patterns of gene expression from the rest 1 . They were acting like two different cell subtypes. That made Regev want to push even further: to use single-cell sequencing to understand how many different cell types there are in the human body, where they reside and what they do. Her lab has gone from looking at 18 cells at a time to sequencing RNA from hundreds of thousands \u2014 and combining single-cell analyses with genome editing to see what happens when key regulatory genes are shut down. The results are already widening the spectrum of known cell types \u2014 identifying, for example, two new forms of retinal neuron 2  \u2014 and Regev is eager to find more. In late 2016, she helped to launch the International Human Cell Atlas, an ambitious effort to classify and map all of the estimated 37 trillion cells in the human body (see 'To build an atlas'). It is part of a growing interest in characterizing individual cells in many different ways, says Mathias Uhl\u00e9n, a microbiologist at the Royal Institute of Technology in Stockholm: \u201cI actually think it's one of the most important life-science projects in history, probably more important than the human genome.\u201d Such broad involvement in ambitious projects is the norm for Regev, says Dana Pe'er, a computational biologist at Memorial Sloan Kettering Cancer Center in New York City, who has known Regev for 18 years. \u201cOne of the things that makes Aviv special is her enormous bandwidth. I've never met a scientist who thinks so deeply and so innovatively on so many things.\u201d \n               Undecided \n             When Regev was an undergraduate at Tel Aviv University in Israel, students had to pick a subject before beginning their studies. But she didn't want to decide. \u201cToo many things were interesting,\u201d she says. Instead, she chose an advanced interdisciplinary programme that would let her look at lots of subjects and skip a bachelor's degree, going straight to a master's. A turning point in her undergraduate years came under the tutelage of evolutionary biologist Eva Jablonka. Jablonka has pushed a controversial view of evolution that involves epigenetic inheritance, and Regev says she admired her courage and integrity in the face of criticism. \u201cThere are many easy paths that you can take, and it's always impressive to see people who choose alternative roads.\u201d Jablonka's class involved solving complicated genetics problems, which Regev loved. She was drawn to the way in which genetics relies on abstract reasoning to reach fundamental scientific conclusions. \u201cI got hooked on biology very deeply as a result,\u201d she says. \u201cGenes became fascinating, but more so how they work with each other. And the first vehicle in which they work with each other is the cell.\u201d Regev did a PhD in computational biology under Ehud Shapiro from the Weizmann Institute of Science in Rehovot, Israel. In 2003 she moved to Harvard University's Bauer Center for Genomics Research in Cambridge, through a unique programme that allows researchers to leapfrog the traditional postdoctoral fellowship and start their own lab. \u201cI had my own small group and was completely independent,\u201d she says. That allowed her to define her own research questions, and she focused on picking apart genetic networks by looking at the RNA molecules produced by genes in cells. In 2004, she applied this technique to tumours and found gene-expression patterns that were shared across wildly different types of cancer, as well as some that were more specific, such as a group of genes related to growth inhibition that is suppressed in acute lymphoblastic leukaemias 3 . By 2006, at the age of 35, she had established her lab at the Broad Institute and the Massachusetts Institute of Technology in Cambridge. \n               Shattering similarities \n             At Broad, Regev continued working on how to tease complex information out of RNA sequencing data. In 2009, she published a paper on a type of mouse immune cell called dendritic cells, revealing the gene networks that control how they respond to pathogens 4 . In 2011, she developed a method that could assemble a complete transcriptome 5  \u2014 all the RNA being transcribed from the genes in a sample \u2014 without using a reference genome, important when an organism's genome has not been sequenced in any great depth. It was around this time that Levin mentioned the prospect of sequencing the RNA inside a single cell. Up to that point,  single-cell genomics had been almost impossible , because techniques weren't sensitive enough to detect the tiny amount of RNA or DNA inside just one cell. But that began to change around 2011. The study with the 18 immune cells \u2014 also dendritic cells \u2014 was meant to test the method. \u201cI had kind of insisted that we do an experiment to prove that when we put the same cell types in, everything comes out the same,\u201d says Rahul Satija, Regev's postdoc at the time, who is now at the New York Genome Center in New York City. Instead, he found two very different groups of cell subtypes. Even within one of the groups, individual cells varied surprisingly in their expression of regulatory and immune genes. \u201cWe saw so much in this one little snapshot,\u201d Regev recalls. \u201cI think even right then, Aviv knew,\u201d says Satija. \u201cWhen we saw those results, they pointed the way forward to where all this was going to go.\u201d They could use the diversity revealed by single-cell genomics to uncover the true range of cell types in an organism, and find out how they were interacting with each other. In standard genetic sequencing, DNA or RNA is extracted from a blend of many cells to produce an average read-out for the entire population. Regev compares this approach to a fruit smoothie. The colour and taste hint at what is in it, but a single blueberry, or even a dozen, can be easily masked by a carton of strawberries. Reporter Shamini Bundell finds out what can be learned from studying cells one by one. By contrast, \u201csingle-cell-resolved data is like a fruit salad\u201d, Regev says. \u201cYou can distinguish your blueberries from your blackberries from your raspberries from your pineapples and so on.\u201d That promised to expose a range of overlooked cellular variation. Using single-cell genomics to sequence a tumour, biologists could determine which genes were being expressed by malignant cells, which by non-malignant cells and which by blood vessels or immune cells \u2014 potentially pointing to better ways to attack the cancer. The technique holds promise for drug development in many diseases. Knowing which genes a potential drug affects is more useful if there's a way to comprehensively check which cells are actively expressing the gene. Regev was not the only one becoming enamoured with single-cell analyses on a grand scale. Since at least 2012,  scientists have been toying with the idea of mapping all human cell types  using these techniques. \u201cThe idea independently arose in several areas of the world at the same time,\u201d says Stephen Quake, a bioengineer at Stanford University in California who co-leads the Chan Zuckerberg Biohub. The Biohub, which has been funding various biomedical research projects since September 2016, includes its own cell-atlas project. \n               The Human Cell Atlas \n             Around 2014, Regev started giving talks and workshops on cell mapping. Sarah Teichmann, head of cellular genetics at the Wellcome Trust Sanger Institute in Hinxton, UK, heard about Regev's interest and last year asked her whether she would like to collaborate on building an international human cell atlas project. It would include not just genomics researchers, but also experts in the physiology of various tissues and organ systems. Regev leapt at the chance, and she and Teichmann are now co-leaders of the Human Cell Atlas. The idea is to sequence the RNA of every kind of cell in the body, to use those gene-expression profiles to classify cells into types and identify new ones, and to map how all those cells and their molecules are spatially organized. The project also aims to discover and characterize all the possible cell states in the human body \u2014 mature and immature, exhausted and fully functioning \u2014 which will require much more sequencing. Scientists have assumed that there are about 300 major cell types, but Regev suspects that there are many more states and subtypes to explore. The retina alone seems to contain more than 100 subtypes of neuron, Regev says. Currently, consortium members whose labs are already working on immune cells, liver and tumours are coming together to coordinate efforts on these tissues and organs. \u201cThis is really early days,\u201d says Teichmann. In co-coordinating the Human Cell Atlas project, Regev has wrangled a committee of 28 people from 5 continents and helped to organize meetings for more than 500 scientists. \u201cI would get stressed out of this world, but she doesn't,\u201d Jablonka says. \u201cIt's fun to have a vision that's shared with others,\u201d Regev says, simply. It has been unclear how the project would find funding for all its ambitions. But in June, the  Chan Zuckerberg Initiative  \u2014 the philanthropic organization in Palo Alto, California, that funds the Biohub \u2014 contributed an undisclosed amount of money and software-engineering support to the Human Cell Atlas data platform, which will be used to store, analyse and browse project data. Teichmann sees the need for data curation as a key reason to focus on a large, centralized effort instead of many smaller ones. \u201cThe computational part is at the heart of the project,\u201d she says. \u201cUniform data processing, data browsing and so on: that's a clear benefit.\u201d In April, the Chan Zuckerberg Initiative had also accepted applications for one-year pilot projects to test and develop technologies and experimental procedures for the Human Cell Atlas; it is expected to announce which projects it has selected for funding some time soon. The applications were open to everyone, not just scientists who have participated in planning meetings. \n               Brain drain \n             Some scientists worry that the atlas will drain both funding and effort from other creative endeavours \u2014  a critique aimed at many such international big-science projects . \u201cThere's this tension,\u201d says Atray Dixit, a PhD student in Regev's lab. \u201cWe know they're going to give us something, and they're kind of low-risk in that sense. But they're really expensive. How do we balance that?\u201d Developmental biologist Azim Surani at the University of Cambridge, UK, is not sure that the project will adeptly balance quantity and depth of information. With the Human Cell Atlas, \u201cyou would have a broad picture rather than a deeper understanding of what the different cell types are\u201d and the relationships between them, he says. \u201cWhat is the pain-to-gain ratio here?\u201d Surani also wonders whether single-cell genomics is ready to converge on one big project. \u201cHas the technology reached maturity so that you're making the best use of it?\u201d he asks. For example, tissue desegregation \u2014 extracting single cells from tissue without getting a biased sample or damaging the RNA inside \u2014 is still very difficult, and it might be better for the field, some say, if many groups were to go off in their own directions to find the best solution to this and other technical challenges. And there are concerns that the project is practically limitless in scope. \u201cThe definition of a cell type is not very clear,\u201d says Uhl\u00e9n, who is director of the Human Protein Atlas \u2014 an effort to catalogue proteins in normal and cancerous human cells that has been running since 2003. There may be a nearly infinite number of cell types to characterize. Uhl\u00e9n says that the Human Cell Atlas is important and exciting, but adds: \u201cWe need to be very clear, what is the endpoint?\u201d Regev argues that completion is not the only goal. \u201cIt's modular: you can break this to pieces,\u201d she says. \u201cEven if you solve a part of a problem, it's still a meaningful solution.\u201d Even if the project just catalogues all the cells in the retina, for example, that's still useful for drug development, she argues. \u201cIt lends itself to something that can unfold over time.\u201d Regev's focus on the Human Cell Atlas has not distracted her from her more detailed studies of specific cell types. Last December, her group was one of three to publish papers 6 , 7 , 8  in which they used the precision gene-editing tool CRISPR\u2013Cas9 to turn off transcription factors and other regulatory genes in large batches of cells, and then used single-cell RNA sequencing to observe the effects. Regev's lab calls its technique Perturb-seq 6 . The aim is to unpick genetic pathways very precisely, on a much larger scale than has been possible before, by switching off one or more genes in each cell, then assaying how they influence every other gene. This was possible before, for a handful of genes at a time, but Perturb-seq can work on 1,000 or even 10,000 genes at once. The results can reveal how genes regulate each other; they can also show the combined effects of activating or deactivating multiple genes at once, which can't be predicted from each of the genes alone. Dixit, a co-first author on the paper, says Regev is indefatigable. She held daily project meetings at 6 a.m. in the weeks leading up to the submission. \u201cI put in this joke sentence at the end of the supplementary methods \u2014 a bunch of alliteration just to see if anyone would read that far. She found it,\u201d Dixit says. \u201cIt was 3 a.m. the night before we submitted.\u201d Regev's intensity and focus is accompanied by relentless positivity. \u201cI'm one of the fortunate people who love what they do,\u201d she says. And she still loves cells. \u201cNo matter how you look at them, they're just absolutely amazing things.\u201d \n                 Tweet \n                 Follow @NatureNews \n               \n                     The trickiest family tree in biology 2017-Jul-05 \n                   \n                     The race to map the human body \u2014 one cell at a time 2017-Feb-20 \n                   \n                     How iPS cells changed the world 2016-Jun-15 \n                   \n                     The boom in mini stomachs, brains, breasts, kidneys and more 2015-Jul-29 \n                   \n                     Tissue engineering: How to build a heart 2013-Jul-03 \n                   \n                     Genomics: The single life 2012-Oct-31 \n                   \n                     Fruitfly development, cell by cell 2012-Jun-03 \n                   \n                     Zebrafish development tracked cell by cell 2008-Oct-09 \n                   \n                     Nature special: Single-cell biology \n                   \n                     Human Cell Atlas \n                   Reprints and Permissions"},
{"file_id": "544020a", "url": "https://www.nature.com/articles/544020a", "year": 2017, "authors": [{"name": "Linda Nordling"}], "parsed_as_year": "2006_or_before", "body": "New investments promise to get precision medicine and precision public health off the ground. But experts debate how much work needs to be done first. It took a public-health disaster for the Zimbabwean government to recognize the power of precision medicine. In 2015, the country switched from a standard three-drug cocktail for HIV to a single-pill combination therapy that was cheaper and easier for people to take every day. The new drug followed a World Health Organization recommendation to incorporate the antiretroviral drug efavirenz as a first-line therapy for public-health programmes. But as tens of thousands of Zimbabweans were put onto the drug, reports soon followed about people quitting it in droves. Collen Masimirembwa, a geneticist and founding director of the African Institute of Biomedical Science and Technology in Harare, was not surprised. In 2007, he had shown that a gene variant carried by many Zimbabweans slows their ability to break down efavirenz 1 . For those with two copies of the variant \u2014 about 20% of the population \u2014 the drug accumulates in the bloodstream, leading to hallucinations, depression and suicidal tendencies. He had tried to communicate this to his government, but at the time efavirenz was not a staple of the country's HIV programme, and so the health ministry ignored his warnings. Masimirembwa continued to publish his research, but the authorities took no heed until there was trouble. A lot of confusion could have been avoided if the government had listened, he says, \u201cIt's not a bad drug. We just know it can be improved in Africa.\u201d Masimirembwa is a rare breed. Although scientists worldwide have been pushing for ways to improve health care by tailoring diagnostics and treatment to the environment, lifestyle and genes of individual patients, few researchers have taken this precision-medicine approach in Africa. That may be changing. In the past five years, international research-funding organizations have invested more than US$100 million in projects to boost genetic research on people in Africa. These studies could lead to improved treatments for Africans as well as for people of recent African descent in Europe and the Americas, who tend to experience more ill health than other ethnicities \u2014 a situation that is often attributed to socioeconomic challenges, but which some scientists say could also have genetic roots. Although few would question the importance of African genomics, opinions differ on whether this will translate into better care. Globally, precision medicine has failed to live up to its promise, even in countries that spend lots of money on health. And some argue that the money spent on investigating genes should instead be used to improve basic health care on the continent. Many African scientists bristle at that simple calculus. They are frustrated that they have been left out of research on everything from health to human origins \u2014 a field that has particularly benefited from African genome data \u2014 and they want Africans to gain from the work. For Masimirembwa and others, the money presents an opportunity to take control of how genetic data are collected and used. \u201cUnless capacity is built on the continent, Africans won't have a chance to participate,\u201d he says. \n               Population-scale precision \n             There's a big problem, however. Precision medicine is expensive. For a continent that, for the most part, struggles to provide even basic health care, tailor-made treatments for individual patients may seem like an unaffordable luxury. Enter 'precision public health' \u2014 a new approach to precision medicine that bases health decisions on populations and communities rather than on individuals. It would use genomic insights into a country's population to inform general treatment programmes. For instance, a country might tweak its essential medicines list that specifies the drugs it buys in bulk at reduced rates from pharmaceutical companies, to avoid medicines that are known to cause problems in its population. This is already happening in some places. Botswana \u2014 a middle-income country \u2014 stopped using the three-in-one drug containing efavirenz in 2016, opting instead for a newer and better-performing, but more expensive, drug called dolutegravir. The gene variant that causes problems with efavirenz is common in Botswana \u2014 around 13.5% of the population has two copies of it. And in 2015, Ethiopia banned the use of the painkiller codeine, because a high proportion of people in the country carry a gene variant that causes them to rapidly convert the drug into morphine, which can cause breathing problems or even death. The precision public-health approach has great appeal for technology-savvy funding organizations that are eager to make a big impact on health. For instance, last October, the Bill & Melinda Gates Foundation and the Alliance for Accelerating Excellence in Science in Africa (AESA), a funding platform based in Nairobi, Kenya, held a precision public-health summit in Ethiopia's capital Addis Ababa. The European Commission is drawing up plans for a precision public-health initiative. And AESA, which is supported by global funders and African organizations, also plans to expand into precision public health. But to fulfil this vision, a lot of research needs to be done on African genomes. Most genomic studies so far have focused on white people of European descent. A meta-analysis published in  Nature  last year 2  revealed that only 3% of global genome-wide association studies \u2014 which link genetic traits to patterns in health, disease or drug tolerance \u2014 had been performed on Africans, compared with 81% on people of European ancestry. An added challenge is that Africans are the most genetically varied people on Earth. Africa is where humanity originated and where humans have lived the longest, so populations there have diverged more than on other continents. Its people have genetic variants that are found nowhere else. These two factors mean that scientists are missing a big piece of the puzzle when it comes to human genetics, says Charles Rotimi, founding director of the National Institutes of Health's Center for Research on Genomics and Global Health in Bethesda, Maryland. Tests developed to inform treatment options for white people might be unsuitable for Africans and people of recent African descent. \u201cWe are in a position to make wrong diagnoses,\u201d he says. Rotimi is one of the founders of the Human Heredity and Health in Africa (H3Africa) Initiative, created in 2010 by the London-based biomedical charity the Wellcome Trust and the US National Institutes of Health. Aiming to build genomics research capacity in Africa, the first round of the programme distributed $70 million to African scientists who teamed up with partners from the United States and Europe (see 'An evolving consortium'). A second round, worth around $64 million, is at the application stage. The research targets conundrums that have dogged clinicians for some time \u2014 such as why Africans have a higher risk of developing chronic kidney disease, and do so at a younger age, than do white people. Nephrologist Dwomoa Adu at the University of Ghana Medical School in Accra, one of the principal investigators in the H3Africa Kidney Research Network, says there are no known environmental factors that explain this. But many Africans carry variants in the gene for apolipoprotein L1 ( APOL1 ) that seem to confer an increased risk of developing kidney disease 3 . These variants have probably flourished in Africa because they confer resistance to trypanosomiasis, or sleeping sickness, a parasitic disease transmitted by the tsetse fly. But as life expectancy has increased in African countries, the incidence of kidney disease has risen markedly. And because there is little dialysis or kidney-transplant capacity on the continent, most people who develop the condition die, says Adu. \u201cIt's a nightmare illness.\u201d Adu's study is testing the link between the  APOL1  gene and kidney disease in Africa at a greater sensitivity than previous studies. But being able to predict the disease with a gene test will be of little use in places where treatment is inaccessible. So Adu is also looking to understand the mechanism by which the gene causes disease, in the hope that this will lead to new, more-affordable, treatments. \u201cIt might be possible to block the mechanism,\u201d he says. Other H3Africa projects are looking for genetic clues to people's varying susceptibility to HIV progression, type 2 diabetes and stroke. One project is studying susceptibility to sleeping sickness. To find the genetic variations that might be causing this clinical diversity, H3Africa has created a chip for quickly assessing variation in Africans. Such chips act as a tool for genome-wide association studies by giving researchers a catalogue of variants called single nucleotide polymorphisms (SNPs) that could be linked to risk for a particular disease or drug reaction. So far, H3Africa has identified 2.7 million previously unrecorded SNPs, and many have made it onto the chip. Samples from the San \u2014 a southern African indigenous group identified as the earliest genetic pool to split off from the rest of the human family tree \u2014 have a particularly rich vein of new SNPs to study. \u201cWe can't wait to explore them,\u201d says Nicola Mulder, a bioinformatician at the University of Cape Town in South Africa, who led work on the chip. Although most of the H3Africa projects have yet to publish results, examples of the types of finding it might provide are starting to appear in the literature. For example, in March this year, Rotimi and his colleagues reported 4  that about 1% of West Africans, African Americans and others of recent African ancestry carry a gene variant that increases their risk of obesity. And a collaboration between South African and Italian scientists resulted, also last month, in the identification of a genetic variant 5  that seems to increase the carrier's risk of heart disease and cardiac arrest. The researchers identified the variant by studying a South African family that has been hit hard by the disease, whose members did not carry any gene variants previously associated with the illness. Although it is not known how common the variant is in South Africa, it could play a part in the high levels of heart disease seen in the country. These insights could lead to better treatment for Africans and people of recent African descent, and perhaps result in discoveries about human genetics. \u201cWe are all African beneath our skin, so understanding African genomes is going to be of global benefit,\u201d says Rotimi. \n               Cost and capacity \n             The attention that genomics research is getting in Africa has not been without critics. Cost is a major concern. Like most developing regions, Africa is seeing a rapid rise in non-communicable diseases such as cancer. In developed countries, cancer treatments are profoundly informed by genomics. But many African nations have only a handful of cancer specialists, and limited capacity for diagnosis and treatment. Although breast-cancer rates, for example, are lower in parts of Africa than in developed countries, more Africans die from the disease and not just because of a lack of access to care \u2014 standard treatments sometimes seem less efficient in some African women. Still, basic cancer-therapy equipment may be higher on the wish list than new genomic tests tailored to African people's tumours. In April last year, for example, Uganda's only radiotherapy machine broke down, forcing people to travel to neighbouring Kenya for treatment, at their own cost. There are those who think that projects such as H3Africa are over-stating the significance of the genetic variance between Africans and Europeans, and its effects on treatment options. Reinhard Hiller, director of the Centre for Proteomic and Genomic Research, a non-profit bioinformatics organization in Cape Town, is pleased that there is growing interest in African genomics. But he thinks that many genomic approaches, especially for treating cancer, can be applied to Africans now. A biopsy from a black woman's breast tumour can undergo the same analysis as that of a European's to look for the tell-tale genetic signs of its origin, he argues. And starting to do this, even on a small scale, might provide more informative data than focusing only on the differences, he says. \u201cWe shouldn't try and prevent governments and societies in Africa from having access to cutting-edge solutions merely because they are deemed imperfect.\u201d Insights from that could feed back to basic genomic research where outcomes warrant it. \u201cWe have to be a lot more pragmatic and do whatever we can do now. If we don't get on with it we'll be sitting here in 50\u2013100 years still without answers.\u201d His lab is one of the few in Africa that can do genomic sequencing. At the moment, most of its therapeutic work is for the private health sector in South Africa. But he's hopeful that genomic medicine can make it into the public sector. The main constraint, besides the cost, is the lack of technicians and counsellors, he says \u2014 something that is also true in many wealthy countries, he adds. But apart from the time it takes to do the research, the slow pace of government policy in Africa presents another stumbling block for the rollout of precision medicine. Masimirembwa's long-ignored advice on efavirenz in Zimbabwe is a case in point. As it turns out, the three-in-one HIV drug that the country rolled out in 2015 works well in people who tolerate it. But differentiating those individuals from the 20% or so who will probably have a bad reaction is difficult. Masimirembwa and his colleagues developed a genetic test for the gene variant that makes carriers sensitive to the drug. This, he says, could be used to identify people who need to be given a lower dose of efavirenz \u2014 something that he and his colleagues have determined decreases the risk of side effects while maintaining its efficacy. Last year he won a 500,000 rand ($39,000) commercialization grant from the South African government for his test. But he's up against the clock. Zimbabwe's government, along with those in South Africa and Uganda, are considering going the same way as Botswana did, and ditching the efavirenz-based treatments entirely. Although the replacement drugs would not necessarily be any less effective, it would mean that Masimirembwa's test would no longer have a market \u2014 a disappointing fate for his discovery. But Masimirembwa thinks that there is still time to make good on his idea. It takes governments years to make decisions on public health, he says, and the new drugs might be unaffordable. And while the Zimbabwean government mulls over options, many HIV-positive people in the country still face a difficult choice: take the drugs that are available and experience serious side effects, or stop taking them and risk developing AIDS. There are second-line alternatives, but most patients are told to 'hang in there' to see if the side effects subside, he says. Few are offered different drugs. One good thing has come out of the debacle so far: it has opened the government's eyes to the value of Masimirembwa's research. In February this year he was awarded a $15,000 national science award. \u201cWhilst there was initially poor acceptance of our findings, the current national and regional support is very encouraging for the future of genomic medicine,\u201d he says. And if his test makes it from the bench to the bedside, it will set a good precedent, he adds. \u201cWe will have demonstrated that African scientists can take an idea from the lab to the market.\u201d \n                 Tweet \n                 Follow @NatureNews \n               \n                     South Africa\u2019s San people issue ethics code to scientists 2017-Mar-20 \n                   \n                     Genomics is failing on diversity 2016-Oct-12 \n                   \n                     A radical revision of human genetics 2016-Oct-12 \n                   \n                     Geneticists attempt to heal rifts with Aboriginal communities 2016-Sep-21 \n                   \n                     Error found in study of first ancient African genome 2016-Jan-29 \n                   \n                     Diversity: A  Nature  &  Scientific American  special Issue \n                   \n                     H3Africa Initiative \n                   Reprints and Permissions"},
{"file_id": "547019a", "url": "https://www.nature.com/articles/547019a", "year": 2017, "authors": [], "parsed_as_year": "2006_or_before", "body": "Analyses of life's most basic elements promise to improve therapies and provide insights into some of the most fundamental processes in biology. Cell theory, the concept of the cell as the basic unit of life, is a cornerstone of biology. But despite nearly 180 years under biologists' microscopes, cells are still enigmatic. This special issue examines how researchers are trying to learn about the nature of cells \u2014 how many different kinds exist, what they do and how they change over time \u2014 by looking at them singly. A  News Feature  details the fruits of lineage tracing, which reveals how complex organisms are built from one, then two, then four seemingly identical cells of an embryo. The diversity of cell type and activity turns out to be much greater than conventional studies on populations of cells could reveal. That's why scientists such as  Aviv Regev  at the Broad Institute in Cambridge, Massachusetts, are taking part in a massive effort to catalogue every sort of cell in the human body. Such work could have important therapeutic implications. Keeping track of the differences between cells in a heterogeneous tumour could guide treatment. Or scientists could improve ways to scrutinize the vast array of immune cells that fight infections or cause inflammation,  say immunologists Amir Giladi and Ido Amit  at the Weizmann Institute of Science in Rehovot, Israel. And single-cell studies will continue to illuminate the inner life of the cell itself. Takashi Nagano at the Babraham Institute near Cambridge, UK, and his colleagues look at how the genome of a mouse cell is packaged and oriented in three dimensions throughout a division cycle  in a research paper . Such efforts presage a real-time 3D view of genome interaction, say Robert Beagrie at the Medical Research Council Molecular Haematology Unit in Oxford, UK, and Ana Pombo at the Max Delbr\u00fcck Center for Molecular Medicine in Berlin, in a  News and Views article . More and more scientists are jumping into single-cell analysis , which spans classical cell biology, developmental biology, genomics and computational biology. And as the t echnologies to study single cells expand , they will require sophisticated analytical tools to tame and make sense of results. Just as the proliferation of cell theory powered extraordinary advances in biology, it is clear that single-cell analysis will open new vistas for scientists to explore. \n                 Tweet \n                 Follow @NatureNews \n               \n                     Single-cell sequencing made simple 2017-Jul-03 \n                   \n                     Immunology, one cell at a time 2017-Jul-03 \n                   \n                     The race to map the human body \u2014 one cell at a time 2017-Feb-20 \n                   \n                     How iPS cells changed the world 2016-Jun-15 \n                   \n                     The boom in mini stomachs, brains, breasts, kidneys and more 2015-Jul-29 \n                   \n                     Tissue engineering: How to build a heart 2013-Jul-03 \n                   \n                     Genomics: The single life 2012-Oct-31 \n                   \n                     Fruitfly development, cell by cell 2012-Jun-03 \n                   \n                     Zebrafish development tracked cell by cell 2008-Oct-09 \n                   \n                     Nature  special: Single-cell biology \n                   Reprints and Permissions"},
{"file_id": "544284a", "url": "https://www.nature.com/articles/544284a", "year": 2017, "authors": [{"name": "Elie Dolgin"}], "parsed_as_year": "2006_or_before", "body": "A simple process seems to explain how massive genomes stay organized. But no one can agree on what powers it. Leonid Mirny swivels in his office chair and grabs the power cord for his laptop. He practically bounces in his seat as he threads the cable through his fingers, creating a doughnut-sized loop. \u201cIt's a dynamic process of motors constantly extruding loops!\u201d says Mirny, a biophysicist here at the Massachusetts Institute of Technology in Cambridge. Mirny's excitement isn't about keeping computer accessories orderly. Rather, he's talking about a central organizing principle of the genome \u2014 how roughly 2 metres of DNA can be squeezed into nearly every cell of the human body  without getting tangled up like last year's Christmas lights . He argues that DNA is constantly being slipped through ring-like motor proteins to make loops. This process, called loop extrusion, helps to keep local regions of DNA together, disentangling them from other parts of the genome and even giving shape and structure to the chromosomes. Scientists have bandied about similar hypotheses for decades, but Mirny's model, and a similar one championed by Erez Lieberman Aiden, a geneticist at Baylor College of Medicine in Houston, Texas, add a new level of molecular detail at a time of explosive growth for research into the 3D structure of the genome. The  models neatly explain the data flowing from high-profile projects  on how different parts of the genome interact physically \u2014 which is why they've garnered so much attention. But these simple explanations are not without controversy. Although it has become increasingly clear that genome looping regulates gene expression, possibly contributing to cell development and diseases such as cancer, the predictions of the models go beyond what anyone has ever seen experimentally. For one thing, the identity of the molecular machine that forms the loops remains a mystery. If the leading protein candidate acted like a motor, as Mirny proposes, it would guzzle energy faster than it has ever been seen to do. \u201cAs a physicist friend of mine tells me, 'This is kind of the Higgs boson of your field',\u201d says Mirny; it explains one of the deepest mysteries of genome biology, but could take years to prove. And although Mirny's model is extremely similar to Lieberman Aiden's \u2014 and the differences esoteric \u2014 sorting out which is right is more than a matter of tying up loose ends. If Mirny is correct, \u201cit's a complete revolution in DNA enzymology\u201d, says Kim Nasmyth, a leading chromosome researcher at the University of Oxford, UK. What's actually powering the loop formation, he adds, \u201chas got to be the biggest problem in genome biology right now\u201d. \n               Loop back \n             Geneticists have known for more than three decades that the genome forms loops, bringing regulatory elements into close proximity with genes that they control. But it was unclear how these loops formed. Several researchers have independently put forward versions of loop extrusion over the years. The first was Arthur Riggs, a geneticist at the Beckman Research Institute of City of Hope in Duarte, California, who first proposed what he called \u201cDNA reeling\u201d in an overlooked 1990 report 1 . Yet it's Nasmyth who is most commonly credited with originating the concept. As he tells it, the idea came to him in 2000, after a day spent mountain climbing in the Italian Alps. He and his colleagues had recently discovered the ring-like shape of cohesin 2 , a protein complex best known for helping to separate copies of chromosomes during cell division. As Nasmyth fiddled with his climbing gear, it dawned on him that chromosomes might be actively threaded through cohesin, or the related complex condensin, in much the same way as the ropes looped through his carabiners. \u201cIt appeared to explain everything,\u201d he says. Nasmyth described the idea in a few paragraphs in a massive, 73-page review article 3 . \u201cNobody took notice whatsoever,\u201d he says \u2014 not even John Marko, a biophysicist at Northwestern University in Evanston, Illinois, who more than a decade later developed a mathematical model that complemented Nasmyth's verbal argument 4 . Mirny joined this loop-modelling club around five years ago. He wanted to explain data sets compiled by biologist Job Dekker, a frequent collaborator at the University of Massachusetts Medical School in Worcester. Dekker had been looking at physical interactions between different spots on chromosomes using  a technique called Hi-C , in which scientists sequence bits of DNA that are close to one another and produce a map of each chromosome, usually depicted as a fractal-like chessboard. The darkest squares along the main diagonal represent spots of closest interaction. The Hi-C snapshots that Dekker and his collaborators had taken revealed distinct compartmentalized loops, with interactions happening in discrete blocks of DNA between 200,000 and 1 million letters long 5 . These 'topologically associating domains', or TADs, are a bit like the carriages on a crowded train. People can move about and bump into each other in the same carriage, but they can't interact with passengers in adjacent carriages unless they slip between the end doors. The human genome may be 3 billion nucleotides long, but most interactions happen locally, within TADs. Mirny and his team had been labouring for more than a year to explain TAD formation using computer simulations. Then, as luck would have it, Mirny happened to attend a conference at which Marko spoke about his then-unpublished model of loop extrusion. (Marko coined the term, which remains in use today.) It was the missing piece of Mirny's puzzle. The researchers gave loop extrusion a try, and it worked. The physical act of forming the loops kept the local domains well organized. The model reproduced many of the finer-scale features of the Hi-C maps. When Mirny and his colleagues posted their finished manuscript on the bioRxiv preprint server in August 2015, they were careful to describe the model in terms of a generic \u201cloop-extruding factor\u201d. But the paper didn't shy away from speculating as to its identity: cohesin was the driving force behind the looping process for cells not in the middle of dividing, when chromosomes are loosely packed 6 . Condensin, they argued in a later paper,  served this role during cell division , when the chromosomes are tightly wound 7 . A key clue was the protein CTCF, which was known to interact with cohesin at the base of each loop of uncondensed chromosomes. For a long time, researchers had assumed that loops form on DNA when these CTCF proteins bump into one another at random and lock together. But if any two CTCF proteins could pair, why did loops form only locally, and not between distant sites? Mirny's model assumes that CTCFs act as stop signs for cohesin. If cohesin stops extruding DNA only when it hits CTCFs on each side of a growing loop, it will naturally bring the proteins together. But singling out cohesin was \u201ca big leap of faith\u201d, says biophysicist Geoff Fudenberg, who did his PhD in Mirny's lab and is now at the University of California, San Francisco. \u201cNo one has seen these motors doing these things in living cells or even  in vitro ,\u201d he says. \u201cBut we see all of these different features of the data that line up and can be unified under this principle.\u201d Experiments had shown, for example, that reducing the amount of cohesin in a cell results in the formation of fewer loops 8 . Overactive cohesin creates so many loops that chromosomes smush up into structures that resemble tiny worms 9 . The authors of these studies had trouble making sense of their results. Then came Mirny's paper on bioRxiv. It was \u201cthe first time that a preprint has really changed the way people were thinking about stuff in this field\u201d, says Matthias Merkenschlager, a cell biologist at the MRC London Institute of Medical Sciences. (Mirny's team eventually published the work in May 2016, in  Cell Reports 6 .) \n               Multiple discovery? \n             Lieberman Aiden  says that the idea of loop extrusion first dawned on him during a conference call in March 2015. He and his former mentor, geneticist Eric Lander of the Broad Institute in Cambridge, Massachusetts, had published some of the most detailed, high-resolution Hi-C maps of the human genome available at the time 10 . During his conference call, Lieberman Aiden was trying to explain a curious phenomenon in his data. Almost all the CTCF landing sites that anchored loops had the same orientation. What he realized was that CTCF, as a stop sign for extrusion, had inherent directionality. And just as motorists race through intersections with stop signs facing away from them, so a loop-extruding factor goes through CTCF sites unless the stop sign is facing the right way. His lab tested the model by systematically deleting and flipping CTCF-binding sites, and remapping the chromosomes with Hi-C. Time and again, the data fitted the model. The team sent its paper for review in July 2015 and published the findings three months later 11 . Mirny's August 2015 bioRxiv paper didn't have the same level of experimental validation, but it did include computer simulations to explain the directional bias of CTCF. In fact, both models make essentially the same predictions, leading some onlookers to speculate on whether Mirny seeded the idea. Lieberman Aiden insists that he came up with his model independently. \u201cWe submitted our paper before I ever saw their manuscript,\u201d he says. There are some tiny differences. The cartoons Mirny uses to describe his model seem to suggest that one cohesin ring does the extruding, whereas Lieberman Aiden's contains two rings, connected like a pair of handcuffs (see 'The taming of the tangles'). Suzana Hadjur, a cell biologist at University College London, calls this mechanistic nuance \u201cabsolutely fundamental\u201d to determining cohesin's role in the extrusion process. Neither Lieberman Aiden nor Mirny say they have a strong opinion on whether the system uses one ring or two, but they do differ on cohesin's central contribution to loop formation. Mirny maintains that the protein is the power source for looping, whereas Lieberman Aiden summarily dismisses this idea. Cohesin \u201cis a big doughnut\u201d, he says. It doesn't do that much. \u201cIt can open and close, but we are very, very confident that cohesin itself is not a motor.\u201d Instead, he suspects that some other factor is pushing cohesin around, and many in the field agree. Claire Wyman, a molecular biophysicist at Erasmus University Medical Centre in Rotterdam, the Netherlands, points out that cohesin is only known to consume small amounts of energy for clasping and releasing DNA, so it's a stretch to think of it motoring along the chromosome at the speeds required for Mirny's model to work. \u201cI'm willing to concede that it's possible,\u201d she says. \u201cBut the Magic 8-Ball would say that, 'All signs point to no'.\u201d One group of proteins that might be doing the pushing is the RNA polymerases, the enzymes that create RNA from a DNA template. In a study online in  Nature  this week 12 , Jan-Michael Peters, a chromosome biologist at the Research Institute of Molecular Pathology in Vienna, and his colleagues show that RNA polymerases can move cohesin over long distances on the genome as they transcribe genes into RNA. \u201cRNA polymerases are one type of motor that could contribute to loop extrusion,\u201d Peters says. But, he adds, the data indicate that it cannot be the only force at play. Frank Uhlmann, a biochemist at the Francis Crick Institute in London, offers an alternative that doesn't require a motor protein at all. In his view, a cohesin complex might slide along DNA randomly until it hits a CTCF site and creates a loop. This model requires only nearby strands of DNA to interact randomly \u2014 which is much more probable, Uhlmann says. \u201cWe do not need to make any assumptions about activities that we don't have experimental evidence for.\u201d Researchers are trying to gather experimental evidence for one model or another. At the Lawrence Livermore National Laboratory in California, for example, biophysicist Aleksandr Noy is attempting to watch loop extrusion in action in a test tube. He throws in just three ingredients: DNA, some ATP to provide energy, and the bacterial equivalent of cohesin and condensin, a protein complex known as SMC. \u201cWe see evidence of DNA being compacted into these kinds of flowers with loops,\u201d says Noy, who is collaborating with Mirny on the project. That suggests that SMC \u2014 and by extension cohesin \u2014 might have a motor function. But then again, it might not. \u201cThe truth is that we just don't know at this point,\u201d Noy says. \n               Bacterial battery \n             The experiment that perhaps comes the closest to showing cohesin acting as a motor was published in February 13 . David Rudner, a bacterial cell biologist at Harvard Medical School in Boston, Massachusetts, and his colleagues made time-lapse Hi-C maps of the bacterium  Bacillus subtilis  that reveal SMC zipping along the chromosome and creating a loop at a rate of more than 50,000 DNA letters per minute. This tempo is on par with what researchers estimate would be necessary for Mirny's model to work in human cells as well. Rudner hasn't yet proved that SMC uses ATP to make that happen. But, he says, he's close \u2014 and he would be \u201cshocked\u201d if cohesin worked differently in human cells. For now, the debate rages about what cohesin is, or is not, doing inside the cell \u2014 and many researchers, including Doug Koshland, a cell biologist at the University of California, Berkeley, insist that a healthy dose of scepticism is still warranted when it comes to Mirny's idea. \u201cI am worried that the simplicity and elegance of the loop-extrusion model is already filling textbooks, coronated long before its time,\u201d he says. And although it may seem an academic dispute among specialists, Mirny notes that if it his model is correct, it will have real-world implications. In cancer, for instance, cohesin is frequently mutated and CTCF sites altered. Defective versions of cohesin have also been implicated in several rare human developmental disorders. If the loop-extruding process is to blame, says Mirny, then perhaps a better understanding of the motor could help fix the problem. But his main interest remains more fundamental. He just wants to understand why DNA is configured in the way it is. And although his model assumes a lot of things about cohesin, Mirny says, \u201cThe problem is that I don't know any other way to explain the formation of these loops.\u201d \n                 Tweet \n                 Follow @NatureNews \n               \n                     Single-nucleus Hi-C reveals unique chromatin reorganization at oocyte-to-zygote transition 2017-Mar-29 \n                   \n                     Zika mosquito genome mapped \u2013 at last 2017-Mar-23 \n                   \n                     Complex multi-enhancer contacts captured by genome architecture mapping 2017-Mar-08 \n                   \n                     A cellular puzzle: The weird and wonderful architecture of RNA 2015-Jul-22 \n                   \n                     Genomics: The single life 2012-Oct-31 \n                   \n                     Culturomics: Word play 2011-Jun-17 \n                   Reprints and Permissions"},
{"file_id": "547020a", "url": "https://www.nature.com/articles/547020a", "year": 2017, "authors": [{"name": "Ewen Callaway"}], "parsed_as_year": "2006_or_before", "body": "Scientists are striving for a deeper view of development, from embryo to adult, cell-by-cell. For 18 months in the early 1980s, John Sulston spent his days watching worms grow. Working in twin 4-hour shifts each day, Sulston would train a light microscope on a single  Caenorhabditis elegans  embryo and sketch what he saw at 5-minute intervals, as a fertilized egg morphed into two cells, then four, eight and so on. He worked alone and in silence in a tiny room at the Medical Research Council Laboratory of Molecular Biology in Cambridge, UK, solving a Rubik's cube between turns at the microscope. \u201cI did find myself little distractions,\u201d the retired Nobel prize-winning biologist once recalled. His hundreds of drawings revealed the rigid choreography of early worm development, encompassing the births of precisely 671 cells, and the deaths of 111 (or 113, depending on the worm\u2019s sex). Every cell could be traced to its immediate forebear and then to the one before that in a series of invariant steps. From these maps and others, Sulston and his collaborators were able to draw up the first, and so far the only, complete \u2018cell-lineage tree\u2019 of a multicellular organism 1 . Although the desire to record an organism\u2019s development in such exquisite detail preceded Sulston by at least a century, the ability to do so in more-complex animals has been limited. No one could ever track the fates of billions of cells in a mouse or a human with just a microscope and a Rubik\u2019s cube to pass the time. But there are other ways. Revolutions in biologists\u2019 ability to edit genomes and sequence them at the level of a single cell have sparked a renaissance in cell-lineage tracing. The effort is attracting not just developmental biologists, but also geneticists and technology developers, who are convinced that understanding a cell\u2019s history \u2014 where it came from and even what has happened to it \u2014 is one of biology\u2019s next great frontiers. The results so far serve up some tantalizing clues to how humans are put together. Individual cells from an organ such as the brain could be related more closely to cells in other organs than to their surrounding tissue, for example. And unlike the undeviating developmental dance of  C. elegans , more-complex organisms invoke quite a bit of improvisation and chance, which will undoubtedly complicate efforts to unpick the choreography. But even incomplete cellular ancestries could be informative. Sulston\u2019s maps paved the way for discoveries surrounding programmed cell death and small, regulatory RNA molecules. New maps could elucidate the role of stem cells in tissue regeneration or help combat cancer \u2014 a disease of unharnessed lineage expansion. \u201cThere\u2019s a real feeling of a new era,\u201d says Alexander Schier, a developmental biologist at Harvard University in Cambridge, Massachusetts, who is using genome editing to trace the cell-lineage history of zebrafish and other animals. \n               Reconstructing history \n             A cell\u2019s history is written in its genome: every mutation acquired that gets passed on to daughter cells serves as a record. In 2005, the computer scientist Ehud Shapiro at the Weizmann Institute of Science in Rehovot, Israel, calculated that researchers could use the natural mutations in individual human cells to piece together how they are related 2 . He conceived of a corollary (in concept at least) to the  C. elegans  cell map, which he called the Human Cell Lineage Project. But the field, he says, wasn\u2019t ready. \u201cWhen we offered this vision, neither the field nor the name of single-cell genomics existed.\u201d Fast forward a decade, and researchers have developed a suite of powerful tools to probe the biology of lone cells, from their RNA molecules and proteins to their individual and unique genomes. Now, he envisions a way of capturing the developmental course of a human, frame by frame, from fertilized egg to adult. \u201cYou want the whole movie with 3D frames from beginning to end,\u201d he says. To make such a film, it\u2019s not even necessary to look at the entire genome. Shapiro\u2019s team is focusing on repetitive stretches of DNA peppered across the genome called microsatellites. These sequences tend to mutate more frequently than other bits of the genome, and his team is working on sequencing tens of thousands of them across the genomes of hundreds of individual human cells to determine how they relate. Christopher Walsh, a neuroscientist and developmental biologist at Boston Childrens Hospital and Harvard Medical School, doubts that researchers will ever reconstruct a complete human cell-lineage map to match that of  C. elegans , but even a less than complete tree will pay dividends, he says. \u201cI\u2019ve been studying cell lineage in the cortex for 25 years, and the idea of studying it directly in the human brain was an inconceivable dream. Now it\u2019s a reality.\u201d In experiments described in 2015, Walsh\u2019s team sequenced the complete genomes of 36 cortical neurons from 3 healthy people who had died and donated their brains to research 3 . Reconstructing the relationship between the brain cells in an individual revealed that closely related cells can be spread across the cortex, whereas local areas can contain multiple distinct lineages. Successive generations of cells seem to venture far from their ancestral homes. One cortical neuron, for instance, was more closely related to a heart cell from the same person than to three-quarters of the surrounding neurons. \u201cWe were not expecting to find that,\u201d Walsh says. Walsh\u2019s team is trying to understand how mosaicism in the brain \u2014 in which some cells harbour different gene variants \u2014 affects health. They have identified, for example, forms of epilepsy that occur even when just a small percentage of cells in a tiny brain region carry a disease-causing mutation. And they have found that individual neurons from healthy individuals can bear mutations that would cause seizures and schizophrenia if present more widely. It seems from this work that it matters which cells end up with a mutation. \u201cThe lineage basically determines what diseases are possible,\u201d Walsh says. Reporter Shamini Bundell finds out what can be learned from studying cells one by one. Other scientists are uncovering records of life\u2019s earliest events in the genomes of adult cells. In experiments published this year 4 , Michael Stratton, a geneticist at the Wellcome Trust Sanger Institute in Hinxton, UK, and his team sequenced white blood cells from 241 women with breast cancer and looked for mutations found in only a subset of their blood cells. The study revealed mutations that occurred very early in development, perhaps as far back as the two-cell embryo. And they noted that the descendants of these cells do not contribute equally to the blood system of adults. This could be because one cell multiplies more efficiently than the other; or it could, as Stratton suspects, be that by chance one ends up contributing more to a developing fetus than to a placenta or other supporting tissues. Future studies, Stratton says, will look for bottlenecks in development that limit the contribution of some cell lineages. \u201cWe\u2019re beginning to see the rules of development in normal human beings,\u201d he says. \n               From blobs to barcodes \n             Jay Shendure, a geneticist at the University of Washington in Seattle, still remembers the day he became fascinated with cellular histories. As a 14-year-old with an interest in biology and computers, he wrote a program that modelled a mass of multiplying cells to impress his uncle, a reconstructive surgeon visiting from India. \u201cHe said, \u2018This is amazing. One day you\u2019ll do the same thing, and instead of a blob it will be a whole baby,\u2019\u200a\u201d Shendure recalls. Nearly a decade later, Shendure was a first-year graduate student working for the Harvard geneticist George Church. Church presented a list of ideas (\u201call of which, at the time, seemed totally absurd\u201d, Shendure says); one of them was to reconstruct the lineages of many cells at once, in a single experiment. Shendure toiled for six months trying to use DNA-flipping enzymes called recombinases to create a readable record in the genomes of bacteria as they divide. Rather than relying on naturally acquired mutations in the genome, the system would essentially create variants to keep track of. Shendure eventually switched projects, but he revived the idea a few years ago when graduate students Aaron McKenna and Greg Findlay joined his laboratory in Seattle. They realized that the popular genome-editing tool CRISPR\u2013Cas9 would be ideal for introducing traceable mutations to whatever part of the genome they wanted (see \u2018The lines of succession\u2019). Teaming up with Schier\u2019s lab, they unleashed CRISPR\u2013Cas9 in two single-cell zebrafish embryos and instructed it to edit DNA \u2018barcode\u2019 sequences that had been engineered into their genomes. They then sequenced these barcodes in cells of an adult animal and used the mutations in them to piece together their lineage 5 . The trees they produced show that a small number of early-forming embryonic lineages give rise to the majority of cells in a given organ. More than 98% of one fish\u2019s blood cells, for instance, came from just 5 of the more than 1,000 cell lineages that the team traced. And although these five contributed to other tissues, they did so in much lower proportions. They were almost entirely absent from the muscle cells in the heart, for example, which was mostly built from its own small number of precursors. \u201cIt was profoundly surprising to me,\u201d says Shendure. His colleague Schier says he is still trying to make sense of the data. Jan Philipp Junker, a quantitative developmental biologist at the Max Delbr\u00fcck Center for Molecular Medicine in Berlin, says that the cell-lineage trees of early embryos probably vary greatly between individuals, and that the dominance of particular lineages observed by Shendure and Schier\u2019s team could be the result of chance events. The cells of an early embryo move around, and only a fraction of them contribute to the final organism, for example. It would be more revealing, he adds, to track later developmental events, such as the formation of the three germ layers that give rise to different organs, because these events are less governed by luck. Junker and others have developed a bevy of other CRISPR-based techniques for piecing together developmental histories. He and Alexander van Oudenaarden, a systems biologist at Utrecht University in the Netherlands, applied such an approach to track the regeneration of a damaged fin in zebrafish. Regeneration, they discovered, occurred in the same kind of way as development: few of the cell lineages that gave rise to the original fin were lost when it was remade from stem cells. The finding confirmed previous studies, but the CRISPR-based methods allowed the team to trace lineages of thousands of cells in a single experiment 6 . Church says his team has used CRISPR to study mouse development and has managed to record the embryonic cell divisions that give rise to the three major germ layers, which form all the body\u2019s organs 7 . \u201cI don\u2019t think we\u2019re that far away from doing a complete lineage,\u201d he says. Some researchers strive to know not just how an organism\u2019s cells relate to one another, but what happened to them along the way. Michael Elowitz and Long Cai, both at the California Institute of Technology in Pasadena, have developed a lineage tracer that creates fluorescent probes to help them observe the histories of cells as they develop 8 . Their method can track whether certain developmental genes have been turned on in the past for a given lineage. On 5 July, Elowitz, along with Shendure and Schier, were awarded a 4-year, US$10 million grant from the Paul G. Allen Frontiers Group to combine their technologies. The trio plan to develop synthetic chromosomes that act as tape recorders for cell-lineage history and molecular events. Such recordings might allow scientists to tinker with a cell\u2019s development in more delicate ways than current cell-reprogramming techniques allow, says Tim Lu, a synthetic biologist at the Massachusetts Institute of Technology in Cambridge who is also working on a technology to record a cell\u2019s history 9 . \u201cYou might see some version of these recorders being inserted into the cell therapies of the future,\u201d although it won\u2019t be for a while, he cautions. \u201cI\u2019m not going to go and inject my CRISPR recorder into a patient.\u201d \n               Lineages for life \n             Cancer is where new lineage-tracing methods are likely to make waves first. \u201cCancer is a disease of lineage \u2014 it\u2019s a disease of stem cells,\u201d says Walsh. One question that researchers are starting to tackle is the origin of metastatic cells, which emerge from the primary tumour and invade sometimes distant organs. They tend to be the hardest tumour cells to vanquish and the ones most likely to kill patients. A team led by cancer geneticist Nick Navin at the University of Texas MD Anderson Cancer Center in Houston published lineage maps of two colon cancers in May 10 . The results showed that liver-invading metastatic cells shared many DNA mutations with the primary tumours they came from, suggesting that the metastasis had emerged at a late stage and hadn\u2019t needed a bunch of new mutations to spread. Lineage mapping could also show whether tumours really develop from single cells, as geneticists have argued, or whether they originate from multiple cells, as some imaging studies have suggested. Navin suspects that similar work could be used to direct treatment. His team and others are tracing cancer-cell lineages in patients as they begin taking drugs. They hope these studies can spot resistant lineages, allowing doctors to pick better treatments and switch medicines in time to make a difference. At the moment, however, promise in the field far exceeds the reality. And Sulston\u2019s lineage maps of  C. elegans  still loom large over current efforts. Stephen Quake, a bioengineer at Stanford University in California, devised his own method for tracking cellular ancestry through CRISPR and decided to test it in the worm 11 . \u201cIt\u2019s nice to have a gold standard,\u201d Quake says. He and his team sequenced the cells of a mature animal after CRISPR had mutated its genome during development. The efforts took much less time than the year and a half that Sulston spent with his microscope. But Quake says that the picture they developed was also less than complete. Yes, it captured a key transition in roundworm development \u2014 the segregation of cells bound for the intestine and those that give rise to the rest of the body \u2014 but it lacked the exquisite detail Sulston observed with his eyes. \u201cI\u2019ll be perfectly blunt. I\u2019m not very impressed with my results,\u201d says Quake, who hadn\u2019t even planned to publish the work until he saw the rush of other papers using similar techniques. \u201cNo one has really got it licked yet,\u201d he says. There is an argument to be made that Sulston set the bar too high with  C. elegans . \u201cThis whole concept of a lineage tree is very much influenced by this classic work,\u201d says Junker. And that may deserve a rethink. In fish, mice and humans, no two individuals\u2019 cell lineage trees are likely to look exactly the same, and each probably changes throughout the individual\u2019s lifetime, as tissues repair and regenerate themselves. Junker and others hope that the new techniques will allow biologists to ask questions about the variability in lineage trees \u2014 between individuals, between their organs and as they age. As Schier puts it: \u201cWe don\u2019t know how many ways there are to make a heart.\u201d It is that vast unknown that could make such work transformative, says Elowitz: \u201cIt would change the kinds of questions you could ask.\u201d Sulston\u2019s map led biologists into uncharted territory, says Schier, and this could do the same. \u201cWe can\u2019t tell you what exactly we\u2019re going to find, but there is a sense that we\u2019re going to find some new continents out there.\u201d \n                 Tweet \n                 Follow @NatureNews \n                 Follow @ewencallaway \n               \n                     The race to map the human body \u2014 one cell at a time 2017-Feb-20 \n                   \n                     How iPS cells changed the world 2016-Jun-15 \n                   \n                     The boom in mini stomachs, brains, breasts, kidneys and more 2015-Jul-29 \n                   \n                     Tissue engineering: How to build a heart 2013-Jul-03 \n                   \n                     Genomics: The single life 2012-Oct-31 \n                   \n                     Fruitfly development, cell by cell 2012-Jun-03 \n                   \n                     Zebrafish development tracked cell by cell 2008-Oct-09 \n                   \n                     Nature  Special: single-cell biology \n                   Reprints and Permissions"},
{"file_id": "544152a", "url": "https://www.nature.com/articles/544152a", "year": 2017, "authors": [{"name": "Jane Qiu"}], "parsed_as_year": "2006_or_before", "body": "The massive East Antarctic Ice Sheet looks stable from above \u2014 but it\u2019s a dangerously different story below. On a glorious January morning in 2015, the Australian icebreaker RSV  Aurora Australis  was losing a battle off the coast of East Antarctica. For days, the ship had been trying to push through heavy sea ice. It rammed into the pack, backed up and crashed forward again. But the ice, several metres thick, hardly budged. Stephen Rintoul, an oceanographer at the University of Tasmania in Hobart, Australia, nearly gave up his goal \u2014 to reach a part of the continent that had thwarted all previous expeditions. \u201cI really thought that would be it,\u201d he says. \u201cIt\u2019d be another failed attempt.\u201d Then the weather came to the rescue, with a wind change that blew the ice away from the shore, opening a path through the pack. The ship managed to break free and wove its way through 100 kilometres of ice, reaching the edge of the frozen continent shortly after midnight. Rintoul and his team were the first scientists to reach the Totten Ice Shelf \u2014 a vast floating ice ledge that fronts the largest glacier in East Antarctica. \u201cIt was a really exhilarating experience,\u201d says Rintoul, chief scientist of the expedition. The team had to work fast before the ice closed again and blocked any escape. For more than 12 hours, Rintoul and his colleagues carried on non-stop, probing the temperature and salinity of the water, the speed and direction of ocean currents as well as the shape and depth of the ocean floor. They also deployed instruments that would continue taking measurements after the ship had departed. These first direct observations confirmed a fear that researchers had long harboured: that  warm waters from the surrounding ocean  can sneak underneath the floating glacier tongue and eat the ice away from below 1 . \u201cThis could explain why Totten has been thinning in the past few decades,\u201d says Rintoul. Tas Van Ommen explains how researchers are uncovering an uncertain future for the East Antarctic. Findings such as these are revealing some scary truths about East Antarctica \u2014 the vast, remote landmass to the east of the Transantarctic Mountains (see \u2018Ice king\u2019). This region is about as big as the entire United States and the majority of it stands on a high plateau up to 4,093 metres above sea level, where temperatures can plunge to \u221295 \u00b0C. Because the East Antarctic Ice Sheet seems so cold and isolated, researchers thought that it had been stable in the past and was unlikely to change in the future \u2014 a stark contrast to the much smaller West Antarctic Ice Sheet, which has raised alarms because  many of its glaciers are rapidly retreating . In the past few years, however, \u201calmost everything we thought we knew about East Antarctica has turned out to be wrong\u201d, says Tas van Ommen, a glaciologist at the Australian Antarctic Division in Kingston, near Hobart. By flying across the continent on planes with instruments that probe beneath the ice, his team found that a large fraction of East Antarctica is well below sea level, which makes it more vulnerable to the warming ocean than previously thought. The researchers also uncovered clues that the massive Totten glacier, which holds about as much ice as West Antarctica, has repeatedly shrunk and grown in the past 2  \u2014 another sign that it could retreat in the future. Although  East Antarctica doesn\u2019t seem to be losing much ice today , there are indications that it is feeling the heat of climate change and is responding in measurable ways. This is disconcerting because its ice sheet is more than ten times bigger than the one in the west. If all the ice below sea level in East Antarctica were to disappear, the height of the world\u2019s oceans would swell by nearly 20 metres. Researchers are now trying to gather as much information as possible about East Antarctica to better predict what\u2019s to come. Their concern is that over the next few centuries, the ice sheet there might reach a tipping point. \u201cOnce glaciers retreat beyond a certain point, things may go downhill very quickly and cause rapid sea level rise,\u201d says Eric Rignot, a glaciologist at the University of California, Irvine. \u201cWe don\u2019t want to sleepwalk into a calamity like this.\u201d \n               Deep danger \n             Rignot was one of the first scientists to warn about possible trouble in East Antarctica \u2014 a region long neglected by climate researchers. In 2013, his team detailed the behaviour of ice around the margin of Antarctica by combining satellite imagery, airborne surveys and climate models. The researchers found evidence that six East Antarctic ice shelves, including Totten, were melting from below at rates much higher than expected \u2014 with some even rivalling those of fast-retreating glaciers in West Antarctica 3 . More surprises emerged when the researchers took a closer look at some of those East Antarctic glaciers. Satellite imagery and airborne surveys between 1996 and 2013 showed that the surface of the Totten glacier dropped by 12 metres and that its grounding line \u2014 the point at which the ice flowing off the continent begins to float on the ocean \u2014 retreated inland by a shocking amount of up to 3 kilometres 4 . \u201cThis is not an isolated incidence,\u201d says Chris Stokes, a glaciologist at Durham University, UK. His team analysed satellite imagery obtained between 1974 and 2012 that covers all the coastal regions in East Antarctica. Most areas had no net ice gain or loss. The only exception is the Wilkes Land region \u2014 an area larger than Greenland that includes Totten glacier 5 . Three-quarters of the glaciers there retreated between 2000 and 2012. \u201cWilkes Land may be East Antarctica\u2019s weak underbelly,\u201d says Stokes. As researchers were pondering the surprising retreat of East Antarctic glaciers, van Ommen and his colleagues were flying over Totten to probe its underside. \u201cThe landscape underneath the ice is fundamentally important for how glaciers flow and how they respond to climate change,\u201d he says. When the team launched an international initiative called ICECAP (International Collaboration for Exploration of the Cryosphere through Aerogeophysical Profiling) a decade ago to systematically survey the hidden landscape of East Antarctica, \u201cwe almost knew nothing about what\u2019s going on down there\u201d, he says. Every Antarctic summer since then, ICECAP's aircraft have been criss-crossing the vast continent to peer through the ice using radar as well as gravitational and magnetic sensors. \u201cThey are the best flights in the world,\u201d says Martin Siegert, a glaciologist at Imperial College London and a principal investigator of the project. The seemingly featureless ice sheet is ever-changing \u2014 with wind-sculpted snow dunes and ice shimmering in thousands of shades under the unearthly Antarctic light. \u201cIt\u2019s just like another planet,\u201d he says. The flights have revealed an astoundingly dramatic landscape hidden beneath the relatively flat ice sheet. Preliminary results from airborne surveys this January, led by glaciologist Sun Bo at the Polar Research Institute of China in Shanghai, confirmed the existence of a 1,100-kilometre-long canyon \u2014 the longest in the world, and almost as deep as the Grand Canyon in the United States. In previous flights over Wilkes Land, van Ommen\u2019s team discovered that 21% of the Totten glacier catchment is more than 1 kilometre below sea level \u2014 an area 100 times larger than previous estimates. \u201cWe really didn\u2019t expect it to be as extensive as it has turned out to be,\u201d says Donald Blankenship, a geophysicist at the University of Texas at Austin and another ICECAP principal investigator. The team also found underwater troughs that extend all the way from the edge of the Totten Ice Shelf to the grounding line 125 kilometres inland, and as deep as 2.7 kilometres below sea level 6 . This deeply contoured landscape could allow warming waters from offshore to quickly reach and erode the ice. The first chance to study the fate of that water came when the RSV  Aurora Australis  reached Totten in 2015. Near the glacier tongue, Rintoul and his colleagues detected waters as warm as 0.3 \u00b0C \u2014 much warmer than the \u22122 \u00b0C freezing point of sea water 1 . \u201cThey are driving rapid rates of melt,\u201d says Rintoul. The instruments he and his team left behind show that warm waters are present all year round. If these waters follow the recently discovered channel beneath Totten to the grounding line, they will be at least 3.2 \u00b0C warmer than the freezing point at that depth. \u201cThat would be really bad news,\u201d he says. Threats to ice shelves could also come from the Antarctic interior \u2014 from lakes under the ice sheet that periodically send flood waters towards the coast. A decade ago, Lake Cook beneath the ice sheet in Wilkes Land suddenly drained, gushing 5.2 billion cubic metres of flood water \u2014 the largest event of this type ever reported in Antarctica. Such floods could be another destabilizing factor, causing faster ice flow and more iceberg calving, says Leigh Stearns, a glaciologist at the University of Kansas in Lawrence. \n               Troubled past \n             These scenarios are not just hypothetical, say researchers. Studies in the past few years have revealed that East Antarctica has lost a lot of ice in the past, and could do so again in the near future. Some of the evidence for that comes from a 2010 expedition supported by the Integrated Ocean Drilling Program, which retrieved sediments from the sea floor off the coast of East Antarctica. Getting those sediments was a dangerous endeavour. The ship had to repeatedly stop drilling and dodge massive icebergs. \u201cThe waters around Antarctica present some of the most challenging environments for ocean drilling,\u201d says Tina van de Flierdt, a geochemist at Imperial College London and a principal investigator of the expedition. The efforts paid off, however, by revealing surprising changes in the ice sheet\u2019s history. \u201cWe had long thought when the East Antarctic Ice Sheet grew to the current size about 14 million years ago, it\u2019s the end of the story,\u201d says van de Flierdt. \u201cIt\u2019s this big stable block of ice that isn\u2019t really doing anything in the face of climate change.\u201d Instead, the sea-floor sediments revealed that the ice sheet waxed and waned many times between 5.3 million and 3.3 million years ago 7  \u2014 an epoch called the Pliocene, when air temperatures were up to 2 \u00b0C higher than today. \u201cWe got a clear signal every time it was warm, suggesting that the ice sheet was sensitive to climate warming,\u201d says van de Flierdt. The researchers say that they have some intriguing preliminary results from the most recent interglacial period, between 129,000 and 116,000 years ago \u2014 when the globe was as warm as it is today. The ice sheet retreated just slightly less at that time than it did during the much warmer Pliocene. \u201cThat\u2019s a big surprise,\u201d says van de Flierdt. \u201cIf the results prove to be robust, I\u2019d say it\u2019s really interesting,\u201d says Maureen Raymo, a geochemist at the Lamont-Doherty Earth Observatory in Palisades, New York. \u201cThis may mean that you can lose a certain amount of ice quite easily with a little bit of warming,\u201d she says. \n               Fast forward \n             As East Antarctica\u2019s vulnerability comes into focus, researchers are increasingly concerned about the future. The only way to forecast decades or centuries ahead is to use computer models that simulate how ice sheets respond to a changing climate. But the models are relatively simplistic, and until recently they couldn\u2019t accurately reproduce some past events, such as the significant glacial retreats that scientists have been discovering in East Antarctica\u2019s history. Climate researchers Robert DeConto of the University of Massachusetts in Amherst and David Pollard of Pennsylvania State University in University Park have tried to make the simulations more realistic by factoring in some processes that were left out of earlier studies. Their model allows meltwater on the ice surface to deepen crevasses and splinter the ice shelves, and it simulates how ice cliffs collapse once they lose the ice shelves that buttress them. When DeConto and Pollard included these processes, their model showed East Antarctica\u2019s glaciers retreating substantially during the last interglacial period and in the Pliocene 8 . \u201cIt\u2019s really the first successful attempt to roughly match ice-sheet simulations with our best understanding of past glacier retreat and sea-level rise,\u201d says van Ommen. After looking back in time, the researchers turned their model to the future. There, they saw a mix of good and bad news. In their simulations, the entire Antarctic Ice Sheet does not change much in the next 500 years if global warming is limited to less than about 1.6 \u00b0C above pre-industrial levels by the end of the century \u2014 roughly in line with what the Paris climate agreement aims to achieve. But if temperatures rise more than about 2.5\u00b0C above pre-industrial levels by 2100 and continue climbing, Antarctic ice melt will raise ocean levels by 5 metres by 2500 (ref.  8 ), with nearly half of that coming from East Antarctica. With Greenland ice also melting, the global sea level would rise by at least 7 metres \u2014 enough to inundate large parts of major coastal cities such as Mumbai, Shanghai, Vancouver and New York. \u201cThis would drastically reshape the world\u2019s coastline and affect millions,\u201d says DeConto. He cautions that the model is still rather crude \u2014 mainly because observations of East Antarctica are so limited. \u201cMost of the coastline is simply unmapped,\u201d he says. The lack of data has also resulted in extremely poor ocean models that grossly underestimate the amount of warm water reaching the ice shelves, says DeConto. \u201cThis really calls for long-term monitoring of ocean conditions.\u201d In East Antarctica now, temperatures are dropping rapidly as the austral winter sets in; researchers are cosy at home reviewing the latest haul of data from the field season. A priority for the future is to map the bedrock beneath all major ice shelves. That will help researchers to identify other glaciers that might be eaten away by warm ocean waters, and to predict how the interior might respond once the ice on the coastal margins disappears. One of the scariest finds would be large valleys in the continental interior that get deeper as they slope towards the ocean. These could destabilize large sections of East Antarctica\u2019s glacial cap when its margins start to disintegrate over the coming decades and centuries. \u201cThen the entire ice sheet could slide off easily,\u201d says Blankenship. \u201cThere would be nothing to hold it back.\u201d \n                 Tweet \n                 Follow @NatureNews \n               \n                     How much longer can Antarctica\u2019s hostile ocean delay global warming? 2016-Nov-16 \n                   \n                     Antarctic model raises prospect of unstoppable ice collapse 2016-Mar-30 \n                   \n                     Gains in Antarctic ice might offset losses 2015-Oct-02 \n                   \n                     Lakes under the ice: Antarctica\u2019s secret garden 2014-Aug-20 \n                   Reprints and Permissions"},
{"file_id": "544408a", "url": "https://www.nature.com/articles/544408a", "year": 2017, "authors": [{"name": "Mark Peplow"}], "parsed_as_year": "2006_or_before", "body": "Superfast imaging techniques are giving researchers their best views yet of what happens in the atomic world. Chemists are dreamers. Every day, they imagine molecules floating in space, with atoms moving about in a stately dance. They spin the structures mentally to examine them from all angles, perhaps twisting each molecule until a bond pops open and another snaps into place. Such movies play inside the minds of most chemists because they offer a way to visualize how reactions happen. \u201cThe unifying thought experiment across all disciplines of chemistry is to imagine atoms moving in real time,\u201d says Dwayne Miller, a physical chemist at the Max Planck Institute for the Structure and Dynamics of Matter in Hamburg, Germany, and the University of Toronto in Canada. \u201cThis is a dream the entire field has.\u201d Chemists have been dreaming like this for more than 150 years, ever since the idea of molecular structure was first conceived. But now these fantasies are becoming a reality. Researchers are directing molecular movies in the lab using a range of techniques, most of which illuminate the scene with incredibly brief pulses of light or electrons. Some rely on the atomic precision of scanning tunnelling microscopes (STMs), whereas others use intense bursts of X-rays to reveal their target's structure. Their goal is to film events that take place in picoseconds (ps, 10 \u221212  s) or femtoseconds (fs, 10 \u221215  s), with atoms moving mere picometres (a hydrogen atom is roughly 100 pm in diameter). At this resolution, researchers can for the first time directly observe a molecule writhing in slow motion, atomic bonds vibrating and breaking, or even electrons washing back and forth. As these techniques become more mainstream, the pay-offs could be huge. They could provide crucial information that leads to better catalysts, artificial forms of photosynthesis or new ways to manipulate the quantum properties of molecules for computing and communication. The stars of these movies are often composite characters, brought to life by filming ensemble casts of billions of identical molecules stacked neatly into tiny crystals. Increasingly, however, researchers are putting individual molecules in the spotlight. Single molecules are in thrall to quantum mechanics, rather than the classical, statistical laws that govern the properties of bulk materials, so imaging them in splendid isolation could give a more revealing portrait of their true nature than can a group shot. As research teams around the world develop new ways to capture individual molecules in motion, they are discovering that each technique has the potential to bring a different view of molecular behaviour into focus. Some are better at pinpointing atoms in space; others glimpse molecules in vanishingly small moments of time. \u201cThe idea of a molecular movie covers a vast landscape,\u201d says Louis DiMauro, a physicist at Ohio State University in Columbus. \u201cIt\u2019s the difference between making an action movie or a Woody Allen picture.\u201d Yet together, he says, the methods promise to show how chemistry works in unprecedented detail. \u201cA combination of these techniques \u2014 that\u2019s the way to produce a true molecular movie.\u201d \n               Lights, camera, action! \n             Molecular cinematography traces its origins back to methods that emerged in the 1980s to capture snapshots of molecules. The leading technique \u2014 called pump\u2013probe spectroscopy \u2014 uses a pulse of laser light lasting mere femtoseconds to trigger a chemical reaction (see \u2018Small world\u2019). An instant later, a second femtosecond pulse arrives and interacts with the molecules in the sample, mid-reaction. This changes the light in ways that can be measured by a detector and turned into a \u2018picture\u2019 of the molecule. And by repeating the experiment over and over again while varying the delay between the two pulses, researchers can build a flip book of pictures showing each stage of a chemical transformation. This technique, a form of femtochemistry, exposed the inner workings of chemical reactions as never before, revealing the identities of fleeting intermediates formed as one molecule transformed into another 1 . But the laser light used in femtochemistry has a wavelength much larger than the distance between individual atoms, so it cannot directly pick out the positions of atoms in molecules. To get clear pictures of individual atoms, scientists have long relied on X-ray crystallography or electron diffraction, which study how photons or electrons scatter as they pass through molecules. Meanwhile, instruments such as STMs and atomic force microscopes (AFMs) offer even more detailed images of atoms in individual molecules and the shrouds of electrons around them. But those techniques usually take milliseconds or longer to acquire an image, much too slow to see atoms moving back and forth. So in the past few years, molecular film-makers have combined various aspects of femtochemistry, diffraction and atomic imaging to create a toolbox of hybrid techniques that offer the best of different worlds, uniting temporal and spatial resolution to show atoms and molecules in their natural habitats. Last year, researchers at the University of Regensburg in Germany used laser pulses to dramatically improve the shutter speed of an STM 2 . This kind of microscope relies on a sharp tip \u2014 narrowed to a single atom at its apex \u2014 that moves over a molecule stuck to a surface. Thanks to quantum behaviour at short distances, electrons can bleed, or \u2018tunnel\u2019, between the molecule and the tip, creating an electric current. As the tip moves, changes in the size of that current reveal the topography of electrons smeared around the molecule. In the Regensburg experiments, the researchers triggered each snapshot by firing a laser pulse of terahertz (THz) radiation \u2014 between microwaves and infrared \u2014 at the STM tip. That created just enough difference in voltage between the tip and the target molecule, called pentacene, to allow an electron to tunnel out of the molecule. This passageway opened within a single cycle of the THz pulse, giving the STM a shutter speed of about 100 fs \u2014 short enough to produce a freeze-frame of pentacene\u2019s electron orbitals at that instant. After it lost the electron, the pentacene molecule was yanked towards the surface by electrical forces, causing it to wobble up and down. The researchers used further THz pulses at various intervals to witness this kind of vibration for the first time. \u201cThere is no other way to see this oscillation in a single molecule,\u201d says physicist Jascha Repp, one of the leaders of the research team. Although that experiment was essentially a proof of concept, Repp thinks his team can shrink the time resolution of THz-STM down to 10 fs, which could reveal even faster processes: electrons gliding across a molecule after it has absorbed light, or hydrogen ions hopping back and forth between different sites, a process called tautomerism that affects the reactivity of many biological molecules. \u201cIt could be transformational,\u201d says DiMauro. \u201cYou could watch reactions on a surface with atomic specificity.\u201d Repp and physicist Leo Gross at IBM Research in Zurich, Switzerland, also hope to throw an AFM into the mix. This instrument has a sharp tip that acts like a record stylus as it skims over a sample, its quivering affected by slight attractions to \u2014 and repulsions from \u2014 the atoms and bonds beneath it, which offers sharper pictures of the molecule than an STM alone. \n               Blockbuster productions \n             One of the attractions of STMs and AFMs is that the equipment \u2014 clusters of stainless-steel vacuum chambers and probes \u2014 can fit in a small laboratory. The techniques are the indie studios of molecular film-making, relatively accessible to many researchers. At the opposite end of the scale are the blockbusters produced at facilities such as the US$414-million Linac Coherent Light Source (LCLS) at the SLAC National Accelerator Laboratory in Menlo Park, California. This gigantic X-ray free-electron laser (XFEL) produces bright, coherent pulses to reveal stunning protein structures. Competition for experimental time on the machine is fierce. Last year, an international team of researchers reported using the LCLS\u2019s X-ray pulses to watch a key biological process for the first time. The team\u2019s target was photoactive yellow protein (PYP), a light sensor used by some bacteria 3 . At the heart of PYP is a light-absorbing region containing a rigid carbon\u2013carbon double bond that cannot twist freely. The bulky groups at either end of the double bond usually point in opposite directions \u2014 a configuration called  trans . But the team used a blue laser pulse to temporarily break one of the bonds, allowing the bulky groups to twist into a \u2018 cis \u2019 configuration, pointing in the same direction (see \u2018Exciting light\u2019). This kind of  trans \u2013 cis  isomerism happens often in biological systems, such as the chemical process underlying vision. The team followed the initial laser blast with a string of 40-fs-long X-ray pulses, which produced diffraction patterns that revealed the locations of the atoms. Stringing these together into a movie showed that the isomerization took place about 550 fs after light excited the PYP. \u201cThe big surprise is that it\u2019s not instant,\u201d says biochemist Petra Fromme of Arizona State University in Tempe, who was part of the team. \u201cIt completely changed our view of how this reaction happened.\u201d This experiment targeted micrometre-scale crystals floating in solution, but other researchers have managed to use the LCLS to film individual molecules in a gas. In 2015, they produced a movie of a ring-shaped molecule breaking open 4  \u2014 a classic reaction in chemistry and biochemistry (see \u2018Opening day\u2019). The wavelength of the X-rays was too long to resolve atoms directly, so the team relied on theoretical simulations to sharpen the images into a 16-frame molecular movie. But a $1-billion upgrade called LCLS-II is now under construction and should offer shorter-wavelength X-rays, in briefer, more frequent pulses, which will improve the time and spatial resolution of the movies. And Fromme hopes that a new generation of compact XFELs, potentially costing less than $15 million each, could open up the technique to many more scientists. She is currently working on two prototypes with collaborators, and says that next year could see the completion of the first one \u2014 called AXSIS and located at the German Electron Synchrotron (DESY) in Hamburg. These tabletop XFELs will produce X-ray pulses just a few hundred attoseconds (10 \u221218  s) long, so brief that they will not destroy the target molecule. Attosecond X-ray pulses from compact XFELs would not contain enough photons to produce clear pictures of single molecules; using them would be like taking a photograph in low light. But one idea under discussion is that a compact XFEL could feed its big brother at a facility such as SLAC with X-rays, electrons or both, to hone its brighter bursts. If this did enable truly single-molecule XFEL imaging, Fromme would like to train the new camera on one of the most fundamental processes of nature: the moment a photon is absorbed by a biomolecule and forms an excited state. \u201cNobody has ever been able to see how fast that process is,\u201d she says. \n               A molecular selfie \n             The most energetic X-rays at the LCLS currently have a wavelength of 150 pm, slightly too long to pick out individual carbon or hydrogen atoms. To zoom in even more, researchers can use fast-moving electrons, which have much shorter wavelengths and offer better spatial resolution as they diffract through a molecule. This is the principle behind cryo-electron microscopy, which is currently revolutionizing the world of structural biology \u2014 not least because it provides detailed structures of proteins in frozen samples without the need to coax them to form crystals. Although cryo-electron microscopy provides crowd shots of many molecules together, other techniques use electrons to image single molecules. Last year, a team led by Jens Biegert, head of research at the Institute of Photonic Sciences in Barcelona, Spain, reported 5  using laser-induced electron diffraction (LIED) to study individual molecules of acetylene (C 2 H 2 ). In this technique, an infrared pulse lines up the molecule in a defined direction, and then a second pulse knocks two electrons out of it, breaking one of acetylene\u2019s carbon\u2013hydrogen bonds. Just like any form of light, these laser pulses are made of oscillating electric and magnetic fields. The electric field of the second pulse picks up one of the liberated electrons and sends it slamming back towards the molecule. The electron arrives about 9 fs after it first escaped, travelling fast enough to pass right through the fragmenting molecule. As it does so, it diffracts like a wave breaking over a rocky seashore, forming a pattern that reveals the positions of the atoms with a shutter speed of less than 1 fs. It is perhaps the ultimate molecular selfie. Every time this happens, the electron diffracts in a slightly different direction, so Biegert\u2019s team had to run the process over and over again to gather enough data to build up clear pictures of the acetylene fragment and the hydrogen ion that splinters off it. After about one billion repetitions, each imaging a fresh molecule drawn from a gas supply, the team had made a few frames of a molecular movie that showed the bond breaking. The group hopes soon to increase the number of frames and to tackle more-complex molecules. By imaging each molecule with just one of its own electrons, Biegert says, this LIED technique avoids key problems with conventional electron diffraction, which uses an \u2018electron gun\u2019 to fire bunches of electrons at ensemble samples of molecules. Those electrons repel each other as they fly, increasing the length of the pulse and making it difficult to set the shutter speed below 10 fs, he says. In the next stage in molecular moviemaking, other researchers hope that moving from femtosecond to attosecond laser pulses will produce unprecedented slow-motion sequences. At those shutter speeds, atoms seem to move at a glacial pace, and electron movement comes into clear view. This will be a crucial step, says DiMauro, because the behaviour of electrons ultimately controls the motion of atoms in a molecule. \u201cWe\u2019ve developed good techniques to watch atomic actors,\u201d he says. \u201cBut to watch the real movie, we also need to watch the electrons.\u201d Most of the researchers involved also agree that it is high time to move on from demonstration projects, and to apply the techniques to research problems across a range of fields. \u201cIf the people who are developing these tools can convince chemists and materials scientists, it will really give it a boost,\u201d says Biegert. After all, \u201cthe first step to understanding is seeing\u201d. \n                 Tweet \n                 Follow @NatureNews \n               \n                     The revolution will not be crystallized: a new method sweeps through structural biology 2015-Sep-09 \n                   \n                     X-ray science: The big guns 2014-Jan-29 \n                   \n                     Crystallography: Atomic secrets 2014-Jan-29 \n                   \n                     Labs vie for X-ray source 2013-Jul-30 \n                   \n                     Linac Coherent Light Source \n                   Reprints and Permissions"},
{"file_id": "545020a", "url": "https://www.nature.com/articles/545020a", "year": 2017, "authors": [{"name": "Douglas Fox"}], "parsed_as_year": "2006_or_before", "body": "An experimental procedure is exposing the links between the nervous and immune systems. Could it be the start of a revolution? Six times a day, Katrin pauses whatever she's doing, removes a small magnet from her pocket and touches it to a raised patch of skin just below her collar bone. For 60 seconds, she feels a soft vibration in her throat. Her voice quavers if she talks. Then, the sensation subsides. The magnet switches on an implanted device that emits a series of electrical pulses \u2014 each about a milliamp, similar to the current drawn by a typical hearing aid. These pulses stimulate her vagus nerve, a tract of fibres that runs down the neck from the brainstem to several major organs, including the heart and gut. The technique, called vagus-nerve stimulation, has been used since the 1990s to treat epilepsy, and since the early 2000s to treat depression. But Katrin, a 70-year-old fitness instructor in Amsterdam, who asked that her name be changed for this story, uses it to control rheumatoid arthritis, an autoimmune disorder that results in the destruction of cartilage around joints and other tissues. A clinical trial in which she enrolled five years ago is the first of its kind in humans, and it represents the culmination of two decades of research looking into the connection between the nervous and immune systems. For Kevin Tracey, a neurosurgeon at the Feinstein Institute for Medical Research in Manhasset, New York, the vagus nerve is a major component of that connection, and he says that electrical stimulation could represent a better way to treat autoimmune diseases, such as lupus, Crohn's disease and more. Several pharmaceutical companies are  investing in 'electroceuticals'  \u2014 devices that can modulate nerves \u2014 to treat cardiovascular and metabolic diseases. But Tracey's goal of controlling inflammation with such a device would represent a major leap forward, if it succeeds. He is a pioneer who \u201cgot a lot of people onboard and doing research in this area\u201d, says Dianne Lorton, a neuroscientist at Kent State University in Ohio, who has spent 30 years studying nerves that infiltrate immune organs such as the lymph nodes and spleen. But she and other observers caution that the neural circuits underlying anti-inflammatory effects are not yet well understood. Tracey acknowledges this criticism, but still sees huge potential in electrical stimulation. \u201cIn our lifetime, we will see devices replacing some drugs,\u201d he says. Delivering shocks to the vagus or other peripheral nerves could provide treatment for a host of diseases, he argues, from diabetes to high blood pressure and bleeding. \u201cThis is the beginning of a field.\u201d \n               Shock value \n             It was only by accident that Tracey first wandered down the path of neuroimmunity. In 1998, he was studying an experimental drug designated CNI-1493, which curbed inflammation in animals by reducing levels of a potent immune protein called tumour-necrosis factor-\u03b1 (TNF-\u03b1). CNI-1493 was usually administered through the bloodstream, but one day, Tracey decided to inject it into a rat's brain. He wanted to see whether it would lower TNF-\u03b1 in the brain during a stroke. But what happened surprised him. CNI-1493 in the brain reduced production of TNF-\u03b1 throughout the animal's body. Other experiments showed that it did this about 100,000 times more potently than when injected straight into the bloodstream 1 . Tracey surmised that the drug was acting on neural signals. His follow-up experiments supported this idea. Minutes after he injected CNI-1493 into the brain, Tracey saw a burst of activity rippling down the rat's vagus nerve 2 . This neural highway regulates a handful of involuntary functions, including heart rate, breathing and the muscle contractions that push food through the gut. Tracey reasoned that it might also control inflammation. When he severed the nerve and the drug's potent effect disappeared, he was convinced. \u201cThat was a game-changer,\u201d says Tracey. The finding meant that if one could stimulate the vagus nerve, the drug wouldn't even be necessary. And so he tried a pivotal experiment. He injected a rat with a fatal dose of endotoxin, a component of the bacterial cell wall that sends animals into a spiral of inflammation, organ failure and death. The drug's effects roughly mirror septic shock in humans. Then, Tracey stimulated the animal's vagus nerve using an electrode. The treated rats had only one-quarter as much TNF-\u03b1 in the bloodstream as untreated animals, and they didn't go into shock 3 . Tracey instantly saw medical potential for vagus-nerve stimulation as a way to block surges in TNF-\u03b1 and other inflammatory molecules. Companies were already selling implantable stimulators to treat epilepsy. But to extend the technique to inflammatory conditions, Tracey would need to present a clearer picture of how it might work and what the side effects might be. Over the next 15 years, Tracey's team performed a series of animal experiments to identify where and how vagus-nerve stimulation acted. They tried cutting the nerve in different places 4  and using drugs that block specific neurotransmitters 5 . These experiments seemed to show that when the vagus is zapped with electricity, a signal pulses down it into the abdomen, and then through a second nerve into the spleen. The spleen serves as an immunological truck stop of sorts, where circulating immune cells periodically  park for a while before returning to the bloodstream . Tracey's team found that the nerve entering the spleen releases a neurotransmitter called noradrenaline 6 , which communicates directly with white blood cells in the spleen called T cells. The junctions between nerve and T cell actually resemble synapses between two nerve cells; the T cells are acting almost like neurons, Tracey says. When stimulated, the T cells release another neurotransmitter, called acetylcholine, which then binds to macrophages in the spleen. It is these immune cells that normally spew TNF-\u03b1 into the bloodstream when an animal receives endotoxin. Exposure to acetylcholine, however, prevents macrophages from producing the inflammatory protein (see 'A shock to the immune system'). Tracey's findings lent new significance to research that had been going on for decades. In the 1980s and 1990s, David Felten, a neuroanatomist then at the University of Rochester in New York, captured microscopic images of hybrid neuron\u2013T-cell synapses in various animals 7  \u2014 not just in the spleen, where Tracey saw them, but also in the lymph nodes, thymus and gut. These neurons belong to what is called the sympathetic nervous system, which regulates body responses to certain stressors. Just as Tracey found in the spleen, Felten observed that these sympathetic neurons stimulate their T-cell partners by secreting noradrenaline \u2014 and often, this stimulation serves to blunt inflammation. In 2014, neuroimmunologist Akiko Nakai of Osaka University in Japan reported evidence that sympathetic-nerve stimulation of T cells limits them from exiting the lymph nodes and entering the circulation, where they might stir up inflammation in other parts of the body 8 . But in many autoimmune diseases, this neural signalling is disrupted. Lorton and her twin sister, neuroscientist Denise Bellinger of Loma Linda University in California, have found sympathetic-nerve pathways to be altered in rat models of autoimmune disorders 9 . The same is seen in humans. Sympathetic nerves are damaged by over-release of noradrenaline, which causes them to withdraw from the immune cells that they should be moderating. As the disease progresses, these nerves advance back into the tissues that they abandoned \u2014 but they do so in abnormal ways, making connections with different subsets of immune cells. These rearranged neural pathways actually maintain inflammation rather than dampen it 9 . It happens in places such as the spleen, lymph nodes and joints, and is causing a lot of pathology, says Bellinger. But she, Lorton and others are sceptical of Tracey's account of the pathway by which vagus-nerve stimulation lowers inflammation. Robin McAllen, a neuroscientist at the University of Melbourne in Australia, has searched for connections between the vagus nerve and the nerve that stimulates T cells in the spleen \u2014 but so far, he has found none. Vagal stimulation \u201cis acting indirectly\u201d through other nerves, says Bellinger. It's important that these neural circuits are properly mapped before moving onto treatment in people, she says. \u201cThe anatomy makes a big difference in what kind of side effects you might see.\u201d Yet, even these sceptics see potential in Tracey's methods. Bellinger points out that in many autoimmune diseases, not only do sympathetic nerves become overactive as they rearrange themselves into proinflammatory circuits, but also the vagus nerve, which opposes them, becomes underactive. Vagal stimulation might partially restore the balance between these two neural systems. \u201cIt's a first step,\u201d she says. \u201cI believe that they will introduce it to the clinic, and they will show remarkable effects.\u201d \n               A patient approach \n             People given vagus-nerve stimulation for seizures or depression experience some side effects \u2014 pain and tightening in the larynx, or straining in their voice, for example; Katrin feels a minor version of this when she stimulates her vagus. Shocking this nerve can also lower the heart rate or increase stomach acid, among other effects. In this respect, Tracey has cause for optimism. The human vagus nerve contains around 100,000 individual nerve fibres, which branch out to reach various organs. But the amount of electricity needed to trigger neural activity can vary from fibre to fibre by as much as 50-fold. Yaakov Levine, a former graduate student of Tracey's, has worked out that the nerve fibres involved in reducing inflammation have a low activation threshold. They can be turned on with as little as 250-millionths of an amp \u2014 one-eighth the amount often used to suppress seizures. And although people treated for seizures require up to several hours of stimulation per day, animal experiments have suggested that a single, brief shock could control inflammation for a long time 10 . Macrophages hit by acetylcholine are unable to produce TNF-\u03b1 for up to 24 hours, says Levine, who now works in Manhasset at SetPoint Medical, a company established to commercialize vagus-nerve stimulation as a medical treatment. By 2011, SetPoint was ready to try the technique in humans, thanks to animal studies and Levine's optimization efforts. That first trial was overseen by Paul-Peter Tak, a rheumatologist at the University of Amsterdam and at the UK-based pharmaceutical company GlaxoSmithKline. Over the course of several years, 18 people with rheumatoid arthritis have been implanted with stimulators, including Katrin. She and 11 other participants saw their symptoms improve over a period of 6 weeks. Lab tests showed that their blood levels of inflammatory molecules, such as TNF-\u03b1 and interleukin-6, decreased. These improvements vanished when the devices were shut off for 14 days \u2014 and then returned when stimulation was resumed. Katrin, who has continued to use the stimulator ever since, still takes weekly injections of the anti-rheumatic drug methotrexate, as well as a daily dose of an anti-inflammatory pill called diclofenac \u2014 but she was able to stop taking high-dose, immune-suppressive steroids, and her joints improved enough for her to return to work. The results of this trial were published last July in  Proceedings of the National Academy of Sciences 11 . Findings from another vagal-stimulation trial were published around the same time 12 . Bruno Bonaz, a gastroenterologist at the University Hospital in Grenoble, France, implanted stimulators into seven people with Crohn's disease. Over a period of six months, five of them reported experiencing fewer symptoms, and endoscopies of their guts showed reduced tissue damage. SetPoint is also midway through a clinical trial of its own, using vagus-nerve stimulation to treat Crohn's disease. Tracey and Bonaz aren't the only people looking to harness neural circuits to treat inflammation. Raul Coimbra, a trauma surgeon at the University of California, San Diego, is studying it as a way to treat septic shock, which affects hundreds of thousands of people each year. Many people who die from the condition are pushed past the point of no return by a singular event: the rapid deterioration of the gut lining, which releases bacteria into the body \u2014 triggering inflammation that damages organs, including the lungs and kidneys. Like Tracey, Coimbra has successfully counteracted this fatal sequence in animals by stimulating the vagus nerve, either with electricity 13  or by administering an experimental drug called CPSI-121 (ref.  14 ). Coimbra hopes to carry this work into a clinical trial. But his research has also unearthed another major challenge that vagus-nerve stimulation must overcome: unlike rats, some humans are probably resistant to the technique. The human genome codes for an extra, non-functioning acetylcholine receptor protein not found in other animals. Todd Costantini, a collaborator of Coimbra's also at the University of California, San Diego, has discovered that if this abnormal receptor is produced in sufficient quantities, it can disrupt signalling and render macrophages unresponsive to acetylcholine. They may then continue releasing TNF-\u03b1 despite vagal stimulation 15 . There's a 200-fold range in the amount of this protein that people produce, says Costantini. He plans to test people to determine whether high levels really block the anti-inflammatory effects of vagal stimulation. Anecdotal evidence suggests that this might be the case. The small clinical trials run so far have revealed that some people don't respond to vagal stimulation. It may be that testing could determine who will benefit from the treatment before people receive implants. Despite the uncertainties, however, the field of electroceuticals is starting to gain momentum. Last October, the US National Institutes of Health announced a programme called Stimulating Peripheral Activity to Relieve Conditions (SPARC), which will provide US$238 million in funding until 2021 to support research updating the maps of neural circuitry in the thoracic and abdominal cavities. GlaxoSmithKline is also showing interest. It has invested in SetPoint, and it announced last year the formation of a joint venture with Google \u2014 called Galvani Bioelectronics \u2014 that will develop therapies for a range of conditions, including inflammatory diseases. Whether vagus-nerve stimulation lives up to expectations remains to be seen. The number of people who have been treated so far is minuscule \u2014 just 25 individuals in 2 completed trials. And treatments often look promising in early trials such as these, but then flop in larger ones. But people with autoimmune disorders are starting to take notice. Treatments for rheumatoid arthritis and Crohn's disease carry some risks, and they don't help everyone. Katrin was one of more than 1,000 people who inquired about the trial for vagal stimulation. \u201cI had nothing else,\u201d she says. \u201cI wanted it.\u201d \n                 Tweet \n                 Follow @NatureNews \n               \n                     The inside story on wearable electronics 2015-Dec-01 \n                   \n                     The tantalizing links between gut microbes and the brain 2015-Oct-14 \n                   \n                     Drug that boosts nerve signals offers hope for multiple sclerosis 2015-Apr-22 \n                   \n                     Neuroscience: The brain, interrupted 2015-Feb-03 \n                   \n                     Electroceuticals spark interest 2014-Jul-02 \n                   \n                     Immunology: The pursuit of happiness 2013-Nov-27 \n                   \n                     Neuroscience: Brain buzz 2011-Apr-13 \n                   Reprints and Permissions"},
{"file_id": "546022a", "url": "https://www.nature.com/articles/546022a", "year": 2017, "authors": [{"name": "Rachel Cernansky"}], "parsed_as_year": "2006_or_before", "body": "Ecologists are increasingly looking at how richness of traits \u2014 rather than number of species \u2014 helps set the health of ecosystems. Emmett Duffy was about 5 metres under water off the coast of Panama, when a giant, tan-and-white porcupinefish caught his eye. The slow-moving creature would have been a prime target for predators if not for the large, treelike branches of elkhorn coral ( Acropora palmata ) it was sheltering under. The sighting was a light-bulb moment for Duffy, a marine biologist. He'd been to places in the Caribbean where corals were more abundant and more diverse, but smaller; the fish there were always small, too. Here, in the Bocas Del Toro archipelago, he was seeing a variety of big fish among the elkhorns. \u201cThe reason these large fish were able to thrive,\u201d he says, \u201cwas that there were places for them to hide and places for them to live.\u201d For Duffy, that encounter with the porcupinefish ( Diodon hystrix ) brought to life a concept that had long been simmering in the back of his head: that the health of an ecosystem may depend not only on the number of species present, but also on the diversity of their traits. This idea, which goes by the name of functional-trait ecology, had been part of his lab work for years but had always felt academic and abstract, says Duffy, now director of the Smithsonian Institution's Tennenbaum Marine Observatories Network in Washington DC. It's an idea that's increasingly in vogue for ecologists. Biodiversity, it states, doesn't have to be just about the number of a species in an ecosystem. Equally important to keeping an ecosystem healthy and resilient are the species' different characteristics and the things they can do \u2014 measured in terms of specific traits such as body size or branch length. That shift in thinking could have big implications for ecology. It may be necessary for understanding and forecasting how plants and animals cope with a changing climate. And functional diversity has started to influence how ecologists think about conservation; some governments have even started to incorporate traits into their management policies. Belize, for example, moved several years ago to protect parrotfish species from overfishing \u2014 not necessarily because their numbers are dwindling, but because the fish clean algae from coral and are crucial to reef survival. \u201cJust going for species numbers basically doesn't allow us to harness all this incredibly rich information we have of how the real world operates,\u201d says Sandra D\u00edaz, an ecologist with Argentina's National Scientific and Technical Research Council (CONICET) and the University of C\u00f3rdoba. Still, some experts are concerned. How traits are defined remains a source of debate, and without robust data on trait and species diversity in settings around the world, any choices directed by the approach could turn out to be short-sighted. \u201cI'm really excited, but I worry,\u201d says Walter Jetz, an ecologist and evolutionary biologist at Yale University in New Haven, Connecticut. \u201cWe as a community need to be really careful in appreciating the data limitations that exist.\u201d \n               Quality versus quantity \n             For decades, the study of biodiversity was essentially a numbers game: the more species an ecosystem had, the more stable and resilient to change it was thought to be. That mindset made sense because there was so little information available about the structures of an ecosystem and the functions of species within it. The technology didn't exist to measure many traits or to process the large amount of data that would have resulted if they could have been measured. Various developments have changed that. Advances in molecular biology have enabled the study of microbes en masse. Satellites can assess traits such as tree-canopy height and marine plankton productivity. And leaps in statistical tools and computing power have helped to make use of all the data that are now being generated. Some trace the new way of thinking about ecosystems \u2014 at least in formal research \u2014 to ecologist David Tilman at the University of Minnesota in St Paul. In 1994, he published a landmark paper 1  that tracked species diversity in Minnesota grasslands through a major drought in the 1980s. Species-rich areas tended to weather the drought much better than those with few species, supporting the link between diversity and stability. But the relationship wasn't linear. Only a few drought-resistant grasses were needed to greatly enhance a plot's ability to rebound. Three years later, Tilman and his collaborators published findings 2  from 289 grassland plots they had planted with varying numbers of species and levels of functional diversity. Here, the presence of certain traits, such as the C 4  photosynthesis pathway or nitrogen fixation, made a bigger difference to the plots' overall health than the number of species. Around the same time, Shahid Naeem, director of Columbia University's Earth Institute Center for Environmental Sustainability in New York City, was also looking beyond species numbers to study ecosystem function, zeroing in on the diversity of species at different levels of the food web. Looking at species number alone, he says, is like listing the parts of a car without saying what they do. That provides no guidance for when things start to break down, he says. \u201cWe just sort of stand there scratching our heads like primitive people who've never seen a car before, saying, 'The car's not working now, I wonder what's wrong with it'.\u201d From the mid-1990s, studies of functional diversity started to take root. Work on plants and forests led the charge because it is relatively easy to manipulate such systems. But the approach gradually expanded to include birds, sea life and soils. Diana Wall, a soil ecologist at Colorado State University in Fort Collins, says that she and her colleagues have focused on functional traits and diversity for years, in part because the  activities of soil microorganisms  are often easier to identify than the species themselves. She is excited that researchers are developing a firmer grasp of traits and species above and below ground. \u201cNew knowledge on both fronts brings us understanding of the dependence on species and functions,\u201d she says. \n               Get your priorities straight \n             Conservation biologists are excited about functional traits because they could influence decisions about what to protect. Researchers and environmentalists have typically focused on regions brimming with species, such as the Amazon rainforest and Australia's Great Barrier Reef. But Rick Stuart-Smith, an ecologist at the University of Tasmania in Taroona, Australia, has suggested reframing the definition of a biodiversity hotspot. Integrating functional traits could point to the importance of previously understudied areas. For Stuart-Smith, it's too early to identify specific places that would qualify \u2014 more in-depth research is needed. But, he says, functional-trait ecology should ultimately extend to conservation strategies and how governments choose which areas to protect. And the new way of thinking about diversity could reveal vulnerabilities that weren't recognized before. Species-rich areas may seem to have a sort of insurance against loss of traits because the functions the traits provide are assumed to be found in many species, says David Mouillot, a marine ecologist at the University of Montpellier in France. But some functions are provided by only one species, or a few. He and his colleagues are racing to locate these rare functions. The lens of functional diversity helps to create a more nuanced picture of ecosystems. Greg Asner, an ecologist with the Carnegie Institution for Science's Department of Global Ecology at Stanford University in California, has used a unique spectral imager to map 15 traits for forests across Peru. Conventional studies recognized three types of forest in the country using the species-richness concept, says Asner \u2014 dryland, floodplain and swamp forest. But Asner and his team looked at which traits could help to distinguish new functional groups, and found that seven were key. They then classified the forests based on those traits, and came up with 36 classes representing different combinations of the seven traits 3 . The researchers used their findings to help Peru rebalance its conservation portfolio. Asner says he's also been asked to identify a 400,000-hectare area in northern Borneo to set aside for protection on the basis of traits. \u201cThey want to know, where is the million acres where you can get the most variation in traits?\u201d he says. \u201cWhere can you put a fence around the most functional variation?\u201d That level of interest is encouraging to him and other researchers because ecosystems are so complex that once certain species, functions or ecosystem processes are lost, there's no getting them back \u2014 at least not using current techniques or knowledge. \u201cWe don't have the science or technology on Earth to engineer a forest from scratch the way that nature and evolution have,\u201d says Asner. Some experts, however, advise against making decisions based on functional traits until more complete data are available. \u201cAs soon as you're missing a single species in your data matrix, you may be missing a key function that is only represented by that species,\u201d says Jetz, who has studied functional traits in plants and vertebrate animals, particularly birds. He warns not only about gaps in data, but also about biases \u2014 such as where researchers choose to sample, which can skew a data set towards or away from certain regions or types of environment. Naeem, too, would like to see a concerted global effort to create a more complete and comprehensive database of traits for the natural world. \u201cWhen we get really excited about a field, one of the big, major investments and efforts that everybody has to get behind is getting the data that we need,\u201d he says. Some work is afoot to build such databases for both terrestrial and aquatic environments.  TRY , hosted at the Max Planck Institute for Biogeochemistry in Jena, Germany, is an international network of plant scientists who have been building a publicly accessible database of traits and functions since 2007. It now contains records for 100,000 plant species. There's also the  ReeFish database , now led by Mouillot, which aims to provide trait and geographic information for all tropical reef fish. And the  Reef Life Survey , begun in Tasmania by Stuart-Smith and marine ecologist Graham Edgar in 2007, has trait records for more than 5,000 species from all ocean basins. Duffy, meanwhile, is spearheading the Smithsonian's Marine Global Earth Observatory programme, which he says is a \u201cmajor opportunity to map out the links between diversity and functioning of marine ecosystems on a global scale\u201d. There are currently ten sites in the network, which aims to establish a global, pole-to-pole presence. These are all works in progress, and despite wide agreement on the importance of focusing on functional traits across ecosystems, there doesn't yet seem to be a clear definition of what a trait is. Agreeing on one that spans the plant and animal kingdoms will be difficult. How detailed should one get? Is it appropriate to stop at observable traits, such as leaf size, or to dig into individual gene sequences? Diet seems to be a grey area. Some researchers include dietary patterns when they evaluate an organism's functional traits, for example, by looking at whether it can eat a variety of organisms or is specialized to feed on a single flower species. Others scoff at including diet. \u201cIf it's not on a genome, it's not a trait,\u201d says Naeem, who points out that foxes may have certain dietary preferences, but will still eat packaged dog food, given the chance. He says that traits linked to genes \u2014 tooth size in a predator, for example \u2014 will influence diet and can be used to infer feeding patterns. \n               Trait talking \n             Interactions between species open up another area of debate. Some might interpret a porcupinefish taking shelter among corals, as Duffy observed in Panama, as an interaction between species \u2014 and not count it as a trait. For Duffy, however, traits can influence, and be a reflection of, how species interact with each other. The traits of the coral \u2014 its branch structure and size \u2014 are what enabled the fish to thrive. Whether or not to rank the importance of traits to an ecosystem is another area of contention. Some researchers are working to identify the most valuable traits, whereas others, such as Mouillot, take a more agnostic approach. \u201cWe do not rank them. We do not say two or three traits are the most important and the other ones are marginal,\u201d he says. And for all the focus on functional diversity, it is probably just one step towards finding a truly comprehensive view of biodiversity \u2014 the ultimate goal for ecologists and conservationists. Simultaneous work is being done on the evolutionary histories of species in an ecosystem in an attempt to understand and mitigate the effects of biodiversity loss. Some view this 'phylogenetic diversity' as the third leg of the stool with functional and species diversity. And researchers around the world are working to fill in other gaps, too. A large German consortium has been studying how land-use intensification affects functional diversity, and more work needs to be done on the role of spatial data and interactions at the landscape level, rather than in microcosms or individual study sites. For now, however, researchers are embracing functional traits for the sophistication they have already added to understanding of ecosystems. That includes Jetz \u2014 despite his warnings against making decisions based on functional diversity too soon. The data may be incomplete, but functional traits could potentially convey the importance of ecosystems to people outside the scientific community, including policymakers and economists, in a more tangible way than species richness ever has. \u201cIf you lose a species or two, it's hard to interpret what that means,\u201d Jetz says. But being able to show explicitly how the loss of a function could decimate an ecosystem might have a bigger impact. \u201cIt's something that more people are able to relate to.\u201d \n                 Tweet \n                 Follow @NatureNews \n               \n                     It\u2019s time to get real about conservation 2016-Oct-11 \n                   \n                     Secrets of life in the soil 2016-Sep-13 \n                   \n                     Biodiversity: The ravages of guns, nets and bulldozers 2016-Aug-10 \n                   \n                     Environmental science: Agree on biodiversity metrics to track from space 2015-Jul-22 \n                   \n                     Biodiversity: Life \u00ad\u2013 a status report 2014-Dec-10 \n                   Reprints and Permissions"},
{"file_id": "545148a", "url": "https://www.nature.com/articles/545148a", "year": 2017, "authors": [{"name": "David Cyranoski"}], "parsed_as_year": "2006_or_before", "body": "China has a lucrative market for fake research reagents. Some scientists are fighting back. In 2013, Huang Song walked into a printing shop in northwestern Beijing and stumbled upon evidence of a brazen and widespread criminal enterprise. Huang was just 15\u00a0kilometres from Beijing\u2019s National Institute of Biological Sciences, where he does synthetic-biology research. Scouting out a small desktop machine to produce the hundreds of labels needed for his experiments, he asked if a certain model could print on heat-resistant paper. The shop owner proudly pulled out some samples he had made for customers using the same machine. Huang was shocked to see names such as Abcam and Cell Signaling Technology on labels that looked exactly like those on vials of expensive antibodies produced by the Western companies. Although the writing meant nothing to the friendly shop owner, for Huang it directly corroborated what he and a number of his colleagues had long suspected: many of the antibodies sold by Chinese distributors were not what they were supposed to be. Counterfeiters were getting fake and diluted research reagents on to the market, and this shop in Zhongguancun, Beijing\u2019s premier technology park, was one of the places they were buying machines to make their labels. \u201cI had a suspicion. That confirmed it,\u201d Huang says. China is famous for knock-off DVDs, Louis Vuitton bags and Rolex watches. But counterfeit reagents aren\u2019t on sale in busy public markets. They are sold through sophisticated websites, mixed in with legitimate supplies, and sourced and sold using a network of unwitting partners, such as the Zhongguancun shopkeeper. Even university cleaning staff have been implicated in the hidden process that creates counterfeit laboratory products, including basic chemistry reagents, serum for cell culture and standard laboratory test kits. Although it\u2019s difficult to quantify the effects of this illegal trade, Chinese scientists and some in Europe and North America say that fake products have led them astray, wasting time and materials. Reporter David Cyranoski explains why researchers are worrying about counterfeit reagents in China. Some in China fear that the problem could undermine the country\u2019s efforts to become a world leader in science. Options for combating the counterfeiters are limited. Reagent companies whose brands are tarnished \u2014 and the scientists taken in by fakes \u2014 shy away from legal action, partly because of embarrassment and partly because they have little faith that law-enforcement agencies can make much of a dent in the trade. \u201cYou cannot stop them from trying. The profit margin is just too high,\u201d says Huang. Scientists and suppliers are now devising strategies that could help change the equation. Major reagent manufacturers have launched educational campaigns. Scientists are sharing their tales of frustration, along with tips for avoiding fraudulent supplies. And Huang has helped to establish a partly government-owned reagent-importing enterprise that takes advantage of new customs and quarantine procedures \u2014 something that could help shrink the market for fakes. But these measures won\u2019t help everyone. Researchers at universities and institutes outside hubs such as Beijing and Shanghai are especially at risk. \u201cI know a lot of labs who still buy and use fake imported chemical reagents,\u201d says Can Xie, a biophysicist at Peking University in Beijing. \u201cI feel sorry for them.\u201d \n               Supply chain \n             China is an attractive target for this specialized form of counterfeiting. Investment in research has expanded rapidly \u2014 the biomedical-science budget for the National Natural Science Foundation of China has quadrupled over the past decade. And the sheer size of the country means that foreign companies, unable to keep up with demand and loath to navigate China\u2019s tricky distribution system, have become dependent on local distributors. \u201cThe country poses many distribution challenges and shipping is logistically difficult,\u201d says Jay Dong, global vice-president and Asia Pacific general manager for Cell Signaling Technology, an antibody manufacturer based in Danvers, Massachusetts. So local companies often carry out the much-needed role of distribution. Some are authorized by the manufacturers. Many are not, however, and it is often difficult for scientists to tell the difference, says Jack Leng, chief executive of Shanghai Universal Biotech, one of the largest distributors of antibodies in China. Disreputable merchants can take advantage of the inflated prices and long waits created by China\u2019s laborious customs and quality-control measures. They offer low prices and fast service for what appear to be the same products, sometimes claiming that the goods have been smuggled into the country. \u201cWe do notice counterfeiting in China more than other countries,\u201d Dong says. Xie, who worked in the United States as a postdoc, says that it took him a few years after his return to China in 2009 to realize that some chemical reagents he was buying were sub par. Distributors, he says, claimed to represent foreign companies with premium products, but were actually selling cheap, domestically produced versions. He cannot say conclusively that impure, low-quality reagents were to blame for failed experiments, but he adds that \u201cmysterious, insoluble stuff\u201d he found in some solutions should have been a warning sign. He now buys only from well-known companies with branch offices in China. Huang, who is deputy director of administration at his institute, witnessed a colleague facing similar frustrations in 2012, when, for six months after publishing a paper, he found that he couldn\u2019t repeat the results of some experiments. The researcher went through all the normal troubleshooting steps and asked colleagues for help. Finally, he discovered that a reagent used to introduce DNA into cells was hampering his replication efforts. Huang now attributes the problems to a counterfeit. \u201cThe last thing you think about is the reagent,\u201d he says. \u201cThis is the kind of stress you cannot put a price on.\u201d Counterfeit antibodies are a particularly widespread source of frustration. Antibodies are crucial in a variety of biological experiments, offering the ability to label and track proteins in a range of living systems. But even untainted ones present some difficulties: there can be natural variation from batch to batch, and  they may target unanticipated proteins . These layers of uncertainty make fakes hard to ferret out. \u201cWhen you look at a negative result it could be many reasons,\u201d says Zhu Weimin, senior vice-president of antibody technology for Abcam, which is headquartered in Cambridge, UK, but has a regional base in Shanghai. \u201cThe problem is serious.\u201d The effects of this confusion and uncertainty are not limited to China. In 2012, for example, researchers in London and Bia\u0142ystok, Poland, reported using an antibody-based kit, called an ELISA, to detect a certain protein in the blood of people with chronic kidney disease 1 . But when kidney-disease specialist Herbert Lin of Massachusetts General Hospital in Boston purchased the same kit \u2014 branded as a product of USCN Life Science in Wuhan, China \u2014 and subjected it to rigorous testing, he found that it targeted another protein altogether 2 . The authors of the original study agreed it was now clear that the antibody was targeting the wrong protein 2 . \u201cThe fact that we did not receive replies from the manufacturers in relation to a couple of e-mails about their assay should perhaps have alerted us that something was not quite right,\u201d they wrote. Cancer researcher Ioannis Prassas of Mount Sinai hospital in Toronto, Canada, had a similar experience with USCN-branded ELISA kits. Prassas says his team spent two years and some US$500,000 trying to identify the problem 3 . Chris Sun, who heads technology development at Cloud-Clone Corporation, the company in Wuhan that sells USCN products, says the company investigated the kit purchased by Prassas, but never identified the problem. It partially reimbursed Prassas. Sun denies that the company intentionally sold bad antibodies. \u201cWe have thousands of antibodies that we produce ourselves. We have no reason to use fake antibodies when we have the real ones,\u201d she says, adding that they have no record of a complaint about the kit Lin found problems with. Most of USCN\u2019s kits are sold through distributors, Sun adds, and the company has sometimes found counterfeits posing as USCN products. Estimating the scale of the issue is difficult, although some companies are trying. Late last year, Abcam tallied up roughly a year\u2019s worth of concerns that it had received from scientists in China about the authenticity of Abcam-labelled products. After checking barcodes, lot numbers and purchase times, the company determined that counterfeit products were to blame for 42% of the hundreds of cases raised. \n               Secret ingredients \n             What scientists are getting in the vials can vary. Sometimes, cheap, common antibodies are relabelled and sold as expensive, rare ones, says Jade Zhang, general manager of Abcam\u2019s Shanghai branch. The counterfeiters will try to find an antibody of similar molecular weight so that scientists who do a quick test to verify the reagents won\u2019t be alarmed. But in experiments, the antibodies will miss their targets. More common than antibody substitution is dilution. Counterfeiters buy authentic products from Chinese distributors or from overseas, then dilute one packet to make five, says Leng. \u201cCustomers get much weaker versions. Sometimes they can use them, sometimes not.\u201d The counterfeiters \u201cwork hard to replicate our packaging, creating tubes and labels that resemble our own so closely that it can be difficult to tell the difference\u201d, says Dong. \u201cThe counterfeiting problem seems to come from a small but active segment in the market.\u201d And many of the players don\u2019t realize they are involved. The Zhongguancun shop owner had no idea he was mixed up in illegal activity. \u201cThey are all part of a chain, but they are not evil,\u201d Huang says. In 2015, Huang noticed a cleaner in his lab plucking empty bottles out of the rubbish and setting them aside. Confused, he asked why. \u201cI warned her that she shouldn\u2019t drink from them,\u201d he says. She told Huang that someone was coming to buy them for 40\u00a0yuan (about US$5) a piece. It was another \u2018a-ha\u2019 moment. The bottles had originally contained fetal bovine serum (FBS), a ubiquitous cell-culture product derived from blood harvested at slaughterhouses. But a ban on imports of beef products from the United States, Australia and New Zealand, because of infectious diseases, had put a stranglehold on the supply of high-quality FBS. The price for reserves of serum from banned locations has doubled over the past few years, to about 10,000 yuan per bottle. Low-quality FBS from other sources costs about one-quarter as much as the banned imports, but it is a poor substitute. Thermo Fisher Scientific of Waltham, Massachusetts, which makes one of the most popular brands of serum, noticed the problem and created labels and bottles that are difficult to duplicate. That\u2019s where the cleaner\u2019s \u2018recycling\u2019 efforts came in. Counterfeiters can simply refill the bottles with low-quality FBS and charge premium prices. It\u2019s hard to know how widespread the problem is, but Huang offers a back-of-the-envelope estimate: given the number of bottles consumed and discarded by major labs, the potential market for FBS counterfeiters in Beijing alone could be tens of millions of yuan per year. Counterfeiters are slippery, moving targets. In most cases, distributors will return payment or replace goods if a customer complains. That means there is no way for researchers to make a legal claim about their lost time and resources, which are the real cost. \u201cPolice will only look at direct loss \u2014 which is nothing,\u201d says Leng. Companies lose revenue and may suffer dilution of their brand, but they also have little recourse. Abcam confronted some of the un-authorized distributors that were supplying apparent counterfeits of its products. The distributors said that they did not know where the antibodies had come from or how the problem occurred. Lawyers have advised against pursuing legal action, which would be costly and probably not get far. \u201cIf we shut one down, another would just pop up,\u201d says Zhang. Leng agrees. He says the counterfeit companies, usually one or two people, \u201cregister a new company every year, then do the same business again\u201d. And some scientists, although angry, don\u2019t want to make a fuss, which would draw attention to the fact that they had been using counterfeits, says Zhang. The admission might raise questions about their previous research results. \n               Disincentive plan \n             Huang himself doesn\u2019t want to follow up with the cleaners, printers and others who are  cogs in the counterfeiting machine , because they are just trying to earn a living. \u201cIf the printer makes 1,000 copies of a label, what\u2019s wrong with that? The people who sterilize the bottles \u2014 they are probably doing a really good job,\u201d he says. But scientists can take action in other ways. Huang centralized ordering for his institute\u2019s most common reagents, so that for the majority of purchases he can ensure scientists are not being duped. And he set up a system that requires researchers to return an old FBS bottle before they can get a new one; the used bottles are destroyed. Others told  Nature  that after having been burnt, they pay higher prices to avoid third-party distributors. Luo Wei, a chemist at the Shenzhen Academy of Metrology and Quality Inspection, a third-party testing company, says that a starch-catalysing enzyme he bought had a suspicious smell and packaging. Its label said it came from Sigma-Aldrich of St Louis, Missouri, and the batch number and related information matched details of products on the company\u2019s website. But Sigma confirmed that the white bottle it came in was not something it used for that product. It was counterfeit. Some reagent companies have also developed programmes to fight counterfeiters. Abcam, Cell Signaling Technology and Universal Bio have been teaching current and prospective customers how to spot fakes through seminars and online manuals. They\u2019ve also opened complaint lines for those who suspect forgery. \u201cThe choice was to take legal action or educate our customers. We chose the latter,\u201d says Zhang. Scientists can work together to spread awareness. Online chat rooms are full of advice, often based on experience, about how to avoid counterfeits. Some include blacklists of companies that have been found to deliver bad products. But for the many scientists in China outside major research hubs, there are fewer choices of distributor, and the word may not be reaching them, says Zhang. They may also have less funding, so price becomes a factor. They are more likely to be persuaded by claims that they are buying smuggled, high-quality goods at a low price. \u201cWe think most customers do not know they\u2019ve been given a counterfeit,\u201d says Zhang. Huang says the ultimate solution is to destroy the profitability of the enterprise. He helped to establish iBio, a 60% state-owned company that opened in December 2015 and brings customs and quarantine inspection under one roof, right on his institute\u2019s campus. Huang, who doesn\u2019t profit from the business, says most reagents are now available within ten days, compared with the month or more it might have taken before. Similar companies have been established in Shanghai and Suzhou. The speed puts Chinese scientists on an even footing internationally. \u201cFor each experiment there are one or two reagents that are a bottleneck,\u201d Huang says. If Chinese scientists need months to get something that others get in days, \u201cthere\u2019s no way Chinese science can compete with the outside world\u201d, he says. It was that logic that in 2012 helped convince government officials to amend regulations, enabling expedited imports of biological reagents. But change has taken time. Huang is grateful for these improvements because they promise to make Chinese science more competitive. An added benefit might be the direct impacts on counterfeiters. \u201cIf you get rid of the customs burden, you destroy their profit margin,\u201d Huang says. That\u2019s better than tracking down culprits, to his mind. \u201cIf you cut out the source, you don\u2019t have to go after them,\u201d he says. \n                 Tweet \n                 Follow @NatureNews \n               \n                     How to hack the hackers: The human side of cybercrime 2016-May-11 \n                   \n                     Reproducibility crisis: Blame it on the antibodies 2015-May-19 \n                   \n                     Reproducibility: Standardize antibodies used in research 2015-Feb-04 \n                   \n                     Developing world: Bring order to unregulated health markets 2012-Jul-11 \n                   \n                     China's deadly drug problem 2007-Apr-04 \n                   \n                     Nature  special: Challenges in irreproducible research \n                   \n                     iBio \n                   \n                     abcam \n                   \n                     Thermo-Fisher Scientific \n                   \n                     Cell Signaling Technology \n                   \n                     Sigma-Aldrich \n                   \n                     Universal Bio \n                   \n                     Cloud-Clone Corporation \n                   Reprints and Permissions"},
{"file_id": "542152a", "url": "https://www.nature.com/articles/542152a", "year": 2017, "authors": [{"name": "Julia Rosen"}], "parsed_as_year": "2006_or_before", "body": "Researchers look into the future of the far North for clues to save species and maybe even bring back sea ice. As the Arctic slipped into the half-darkness of autumn last year, it seemed to enter the Twilight Zone. In the span of a few months, all manner of strange things happened. The cap of sea ice covering the Arctic Ocean started to shrink when it should have been growing. Temperatures at the North Pole soared more than 20 \u00b0C above normal at times. And polar bears prowling the shorelines of Hudson Bay had a record number of run-ins with people while waiting for the water to freeze over. It was a stark illustration of just how quickly  climate change is reshaping the far north . And if last autumn was bizarre, it's the summers that have really got scientists worried. As early as 2030, researchers say, the Arctic Ocean could lose essentially all of its ice during the warmest months of the year \u2014 a radical transformation that would  upend Arctic ecosystems  and disrupt many northern communities. Change will spill beyond the region, too. An increasingly blue Arctic Ocean could amplify warming trends and even scramble weather patterns around the globe. \u201cIt\u2019s not just that we\u2019re talking about polar bears or seals,\u201d says Julienne Stroeve, a sea-ice researcher at University College London. \u201cWe all are ice-dependent species.\u201d With the prospect of  ice-free Arctic summers  on the horizon, scientists are striving to understand how residents of the north will fare, which animals face the biggest risks and whether nations could save them by protecting small icy refuges. But as some researchers look even further into the future, they see reasons to preserve hope. If society ever manages to reverse the surge in greenhouse-gas concentrations \u2014 as some suspect it ultimately will \u2014 then the same physics that makes it easy for Arctic sea ice to melt rapidly may also allow it to regrow, says Stephanie Pfirman, a sea-ice researcher at Barnard College in New York City. Reporter Adam Levy finds out why the Arctic\u2019s vanishing ice matters, and what it would take to restore it. She and other scientists say that it\u2019s time to look beyond the Arctic\u2019s decline and start thinking about what it would take to restore sea ice. That raises controversial questions about how quickly summer ice could return and whether it could regrow fast enough to spare Arctic species. Could nations even  cool the climate quickly through geoengineering , to reverse the most drastic changes up north? Pfirman and her colleagues published a paper 1  last year designed to kick-start a broader conversation about how countries might plan for the regrowth of ice, and whether they would welcome it. Only by considering all the possibilities for the far future can the world stay one step ahead of the ever-changing Arctic, say scientists. \u201cWe\u2019ve committed to the Arctic of the next generation,\u201d Pfirman says. \u201cWhat comes next?\u201d \n               Blue period \n             Pfirman remembers the first time she realized just how fast the Arctic was unravelling. It was September 2007, and she was preparing to give a talk. She went online to download the latest sea-ice maps and discovered something disturbing: the extent of  Arctic ice had shrunk past the record minimum  and was still dropping. \u201cOh, no! It\u2019s happening,\u201d she thought. Although Pfirman and others knew that Arctic sea ice was shrinking, they hadn\u2019t expected to see such extreme ice losses until the middle of the twenty-first century. \u201cIt was a wake-up call that we had basically run out of time,\u201d she says. In theory, there\u2019s still a chance that the world could prevent the total loss of summer sea ice. Global climate models suggest that about 3 million square kilometres \u2014 roughly half of the minimum summer coverage in recent decades \u2014 could survive if countries fulfil their commitments to the newly ratified  Paris climate agreement , which limits global warming to 2 \u00b0C above pre-industrial temperatures. But sea-ice researchers aren\u2019t counting on that. Models have consistently underestimated ice losses in the past, causing scientists to worry that the declines in the next few decades will outpace projections 2 . And given the limited commitments that countries have made so far to address climate change, many researchers suspect the world will overshoot the 2 \u00b0C target, all but guaranteeing essentially ice-free summers (winter ice is projected to persist for much longer). In the best-case scenario, the Arctic is in for a 4\u20135 \u00b0C temperature rise, thanks to processes that amplify warming at high latitudes, says James Overland, an oceanographer at the US National Oceanic and Atmospheric Administration in Seattle, Washington. \u201cWe really don\u2019t have any clue about how disruptive that\u2019s going to be.\u201d The Arctic\u2019s 4 million residents \u2014 including 400,000 indigenous people \u2014 will feel the most direct effects of ice loss. Entire coastal communities, such as many in Alaska, will be forced to relocate as permafrost melts and shorelines crumble without sea ice to buffer them from violent storms, according to a 2013 report 3  by the Brookings Institution in Washington DC. Residents in Greenland will find it hard to travel on sea ice, and reindeer herders in Siberia could struggle to feed their animals. At the same time, new economic opportunities will beckon as open water allows greater access to fishing grounds, oil and gas deposits, and other sources of revenue. People living at mid-latitudes may not be immune, either. Emerging research 4  suggests that open water in the Arctic might have helped to amplify weather events, such as cold snaps in the United States, Europe and Asia in recent winters. Indeed, the impacts could reach around the globe. That\u2019s because sea ice helps to cool the planet by reflecting sunlight and preventing the Arctic Ocean from absorbing heat. Keeping local air and water temperatures low, in turn, limits melting of the Greenland ice sheet and permafrost. With summer ice gone, Greenland\u2019s glaciers could contribute more to sea-level rise, and permafrost could release its stores of greenhouse gases such as methane. Such is the vast influence of Arctic ice. \u201cIt is really the tail that wags the dog of global climate,\u201d says Brenda Ekwurzel, director of climate science at the Union of Concerned Scientists in Cambridge, Massachusetts. But Arctic ecosystems will take the biggest hit. In 2007, for example, biologists in Alaska noticed something odd: vast numbers of walruses had clambered ashore on the coast of the Chukchi Sea. From above, it looked like the Woodstock music festival \u2014 with tusks \u2014 as thousands of plump pinnipeds crowded swathes of ice-free shoreline. Normally, walruses rest atop sea ice while foraging on the shallow sea floor. But that year, and almost every year since, sea-ice retreat made that impossible by late summer. Pacific walruses have adapted by hauling out on land, but scientists with the US Fish and Wildlife Service worry that their numbers will continue to decline. Here and across the region, the effects of Arctic thawing will ripple through ecosystems. In the ocean, photosynthetic plankton that thrive in open water will replace algae that grow on ice. Some models 5  suggest that biological productivity in a seasonally ice-free Arctic could increase by up to 70% by 2100, which could boost revenue from Arctic fisheries even more. (To prevent a seafood gold rush, five Arctic nations have agreed to refrain from unregulated fishing in international waters for now.) Many whales already seem to be benefiting from the bounty of food, says Sue Moore, an Arctic mammal specialist at the Pacific Marine Environmental Laboratory. But the changing Arctic will pose a challenge for species whose life cycles are intimately linked to sea ice, such as walruses and Arctic seals \u2014 as well as polar bears, which don\u2019t have much to eat on land. Research 6  suggests that  many will starve if the ice-free season gets too long  in much of the Arctic. \u201cBasically, you can write off most of the southern populations,\u201d says Andrew Derocher, a biologist at the University of Alberta in Edmonton, Canada. Such findings spurred the US Fish and Wildlife Service to list polar bears as threatened in 2008. \n               The last of the ice \n             Ice-dependent ecosystems may survive for longest along the rugged north shores of Greenland and Canada, where models suggest that about half a million square kilometres of summer sea ice will linger after the rest of the Arctic opens up (see \u2018Going, going \u2026\u2019). Wind patterns cause ice to pile up there, and the thickness of the ice \u2014 along with the high latitude \u2014 helps prevent it from melting. \u201cThe Siberian coastlines are the ice factory, and the Canadian Arctic Archipelago is the ice graveyard,\u201d says Robert Newton, an oceanographer at Columbia University\u2019s Lamont\u2013Doherty Earth Observatory in Palisades, New York. Groups such as the wildlife charity WWF have proposed protecting this \u2018last ice area\u2019 as a World Heritage Site in the hope that it will serve as a life preserver for many Arctic species. Last December, Canada announced that it would at least consider setting the area aside for conservation, and indigenous groups have expressed interest in helping to manage it. (Before he left office, then-US president Barack Obama joined Canadian Prime Minister Justin Trudeau in pledging to protect 17% of the countries\u2019 Arctic lands and 10% of marine areas by 2020.) But the last ice area has limitations as an Arctic Noah\u2019s ark. Some species don\u2019t live in the region, and those that do are there in only small numbers. Derocher estimates that there are less than 2,000 polar bears in that last ice area today \u2014 a fraction of the total Arctic population of roughly 25,000. How many bears will live there in the future depends on how the ecosystem evolves with warming. The area may also be more vulnerable than global climate models suggest. Bruno Tremblay, a sea-ice researcher at McGill University in Montreal, Canada, and David Huard, an independent climate consultant based in Quebec, Canada, studied the fate of the refuge with a high-resolution sea-ice and ocean model that better represented the narrow channels between the islands of the Canadian archipelago. In a report 7  commissioned by the WWF, they found that ice might actually be able to sneak between the islands and flow south to latitudes where it would melt. According to the model, Tremblay says, \u201ceven the last ice area gets flushed out much more efficiently\u201d. If the future of the Arctic seems dire, there is one source of optimism: summer sea ice will return whenever the planet cools down again. \u201cIt\u2019s not this irreversible process,\u201d Stroeve says. \u201cYou could bring it back even if you lose it all.\u201d Unlike land-based ice sheets, which wax and wane over millennia and lag behind climate changes by similar spans, sea ice will regrow as soon as summer temperatures get cold enough. But identifying the exact threshold at which sea ice will return is tricky, says Dirk Notz, a sea-ice researcher at the Max Planck Institute for Meteorology in Hamburg, Germany. On the basis of model projections, researchers suggest that the threshold hovers around 450 parts per million (p.p.m.) \u2014 some 50 p.p.m. higher than today. But greenhouse-gas concentrations are not the only factor that affects ice regrowth; it also depends on how long the region has been ice-free in summer, which determines how much heat can build up in the Arctic Ocean. Notz and his colleagues studied the interplay between greenhouse gases and ocean temperature with a global climate model 8 . They increased CO 2  from pre-industrial concentrations of 280 p.p.m. to 1,100 p.p.m. \u2014 a bit more than the 1,000 p.p.m. projected by 2100 if no major action is taken to curtail greenhouse-gas emissions. Then they left it at those levels for millennia. This obliterated both winter and summer sea ice, and allowed the ocean to warm up. The researchers then reduced CO 2  concentrations to levels at which summer ice should have returned, but it did not regrow until the ocean had a chance to cool off, which took centuries. By contrast, if the Arctic experiences ice-free summers for a relatively short time before greenhouse gases drop, then models suggest ice would regrow much sooner. That could theoretically start to happen by the end of the century, assuming that nations take very aggressive steps to reduce carbon dioxide levels 1 , according to Newton, Pfirman and their colleagues. So even if society cannot forestall the loss of summer sea ice in coming decades, taking action to keep CO 2  concentrations under control could still make it easier to regrow the ice cover later, Notz says. \n               Global cooling \n             Given the stakes, some researchers have proposed global-scale geoengineering to cool the planet and, by extension, preserve or restore ice. Others argue that it might be possible to chill just the north, for instance by artificially whitening the Arctic Ocean with light-coloured floating particles to reflect sunlight. A study 9  this year suggested installing wind-powered pumps to bring water to the surface in winter, where it would freeze, forming thicker ice. But many researchers hesitate to embrace geoengineering. And most agree that regional efforts would take tremendous effort and have limited benefits, given that Earth\u2019s circulation systems could just bring more heat north to compensate. \u201cIt\u2019s kind of like walking against a conveyor the wrong way,\u201d Pfirman says. She and others agree that managing greenhouse gases \u2014 and local pollutants such as black carbon from shipping \u2014 is the only long-term solution. Returning to a world with summer sea ice could have big perks, such as restoring some of the climate services that the Arctic provides to the globe and stabilizing weather patterns. And in the region itself, restoring a white Arctic could offer relief to polar bears and other ice-dependent species, says Pfirman. These creatures might be able to weather a relatively short ice-free window, hunkered down in either the last ice area or other places set aside to preserve biodiversity. When the ice returned, they could spread out again to repopulate the Arctic. That has almost certainly happened during past climate changes. For instance, researchers think the Arctic may have experienced nearly ice-free summers during the last interglacial period, 130,000 years ago 10 . But, one thing is certain: getting back to a world with Arctic summer sea ice won\u2019t be simple, politically or technically. Not everyone will embrace a return to an ice-covered Arctic, especially if it\u2019s been blue for several generations. Companies and countries are already eyeing the opportunities for oil and gas exploration, mining, shipping, tourism and fishing in a region hungry for economic development. \u201cIn many communities, people are split,\u201d Pfirman says. Some researchers also say that the idea of regrowing sea ice seems like wishful thinking, because it would require efforts well beyond what nations must do to meet the Paris agreement. Limiting warming to 2 \u00b0C will probably entail converting huge swathes of land into forest and using still-nascent technologies to suck billions of tonnes of CO 2  out of the air. Lowering greenhouse-gas concentrations enough to regrow ice would demand even more. And if summer sea ice ever does come back, it\u2019s hard to know how a remade Arctic would work, Derocher says. \u201cThere will be an ecosystem. It will function. It just may not look like the one we currently have.\u201d \n                 Tweet \n                 Follow @NatureNews \n               \n                     Incredibly thin Arctic sea ice shocks researchers 2016-Dec-14 \n                   \n                     No safe haven for polar bears in warming Arctic 2016-Sep-14 \n                   \n                     Speedier Arctic data as warm winter shrinks sea ice 2016-Mar-01 \n                   \n                     Is the 2\u2009\u00b0C world a fantasy? 2015-Nov-24 \n                   \n                     Climate science: Vast costs of Arctic change 2013-Jul-24 \n                   \n                     Ice loss shifts Arctic cycles 2012-Sep-12 \n                   \n                     Special issue on the Arctic: After the ice 2011-Oct-12 \n                   \n                     Nature  special: After the ice \n                   \n                     Nature  special: 2015 Paris climate talks \n                   \n                     US National Snow and Ice Data Center \n                   Reprints and Permissions"},
{"file_id": "542286a", "url": "https://www.nature.com/articles/542286a", "year": 2017, "authors": [{"name": "Alison Abbott"}], "parsed_as_year": "2006_or_before", "body": "Political upheaval threatens Turkey\u2019s ambitious plans for research and development. \u00d6mer Ilday was in a buoyant mood on 15 July last year. He was preparing a press release to promote his latest research, which used super-fast laser bursts to cut away at materials with as little wasted energy as possible. He was confident of media interest. His technique had exciting applications for industry and medicine \u2014 such as reducing burn damage in certain types of surgery. And the paper describing it had just been published in  Nature 1  \u2014 the first in the journal led by a group from Turkey in nearly 25 years. But that evening Ilday, a materials scientist at Bilkent University in Ankara, started to get alarming text messages from friends: a military coup was under way. In an instant, scientific discovery was swept from the news agenda. The coup was swiftly suppressed. But Turkish President Recep Tayyip Erdo\u011fan declared a state of emergency that is still in force and has thrown the country's science into turmoil.  Thousands of academics have been sacked . The national research agency became so depleted of personnel that it stopped functioning entirely for many months. And Erdo\u011fan, fearful of enemies in places of higher learning, took direct charge of appointing university presidents. The country was supposed to have been  in the middle of a scientific rebirth . Although it had struggled for decades to stem a brain drain, Turkish research funding had taken off since 2005, when the country  started negotiations to join the European Union . Research hotspots emerged. Some optimistic young scientists returned home and set up thriving labs. And in 2014, the government announced serious plans to expand research on many fronts. No one expected progress to be easy. Scientists have long been ill at ease with Turkey's complex politics and its tensions between secular, religious and military forces. But now, faced also with rising rates of terrorism, concerns about Erdo\u011fan's increasing authoritarianism and a currency in free fall, many of the best scientists are wondering whether they should leave the country. \u201cI think worse will come,\u201d says \u00d6zg\u00fcr Akan, a physicist working in neuroscience who decided a few months ago to depart Ko\u00e7 University in Istanbul for the University of Cambridge, UK. \u201cI don't think it will be possible to do high-tech, high-risk research in the next ten years.\u201d Not everyone has abandoned hope \u2014 but the mood among scientists is universally nervous. \n               Global ambition \n             Sitting in his tidy, light-filled office at the Turkish Council of Higher Education (Y\u00d6K) in Ankara, Hasan Mandal says he sees something positive in the instability. \u201cThese are challenging times, but turbulence gives us opportunities,\u201d says the engineer, who trained and did postdocs in the United Kingdom and Germany. \u201cIt is harder to make changes in normal times.\u201d As a deputy president of Y\u00d6K, he is one of the architects of the government's research-expansion plans, which include making universities more competitive and creating new research centres and positions (see 'Elevating Turkish science'). The plans emerged from the government's overarching aim for Turkey to become one of the ten largest economies in the world by 2023, the 100th anniversary of the foundation of the Turkish state. (It is now ranked 17th when accounting for purchasing-power parity.) Expanding research capacity on all fronts is integral to that plan, says Hasan. In Turkey's heavily centralized education system, Y\u00d6K controls all the significant decisions of state universities, from the distribution of academic positions between departments to salaries and student numbers. The reforms will inject a degree of flexibility for a select few. Inspired by Germany's Excellence Initiative and Russia's Project 5-100, Turkey launched competitions between its universities, with the winners getting more funding and positions. Last year, five provincial institutions were selected as 'regional universities' owing to their close connections with local industries and social programmes. And in January, Y\u00d6K opened a second competition for another five to earn the title of 'research university' on the basis of their potential to produce internationally competitive research. On top of other spoils, research universities will win the freedom to open new courses and distribute academic positions without seeking Y\u00d6K approval. \u201cThe status would make a big difference to us,\u201d says urologist Haluk \u00d6zen, president of the research-strong Hacettepe University in Ankara. Outside the university system, the government has established the Health Institutes of Turkey (T\u00dcSEB), which will be headquartered in Istanbul and broadly modelled on the US National Institutes of Health. Its six institutes will emphasize translational research and personalized medicine. T\u00dcSEB is creating 300 research positions this year alone, and will have a generous budget for extramural research. The government is also converting a handful of university institutes into  securely funded national research centres , allowing them to manage their own operations and budgets. \u201cI'm particularly happy that we will be able to pay competitive salaries that could be attractive to top scientists from abroad,\u201d says molecular biologist Mehmet \u00d6zt\u00fcrk, director of the International Biomedicine and Genome Institute in \u0130zmir (iBG-izmir), one of the four nominated for this elevated status last December. Y\u00d6K has also opened 2,000 new PhD positions to ensure a future generation of scientists. The government hopes that these expansive plans will tantalize Turkey's large scientific diaspora. In fact, many Turkish scientists living abroad had been starting to feel optimistic about their home country. Research spending rose more than tenfold between 2000 and 2011. And some private universities, such as Bilkent and Ko\u00e7, have developed into havens from the bureaucratic excesses of public universities. For scientists with large international grants to help them establish and equip their labs, the possibility of an excellent research career in their home country seemed real. \u201cWith such grants, and a supportive university like mine, you have the right conditions to achieve things,\u201d says molecular biologist Ebru Erbay, who returned from Harvard University in Cambridge, Massachusetts, to Bilkent in 2010. She has since won grants from both the European Research Council (ERC) and the European Molecular Biology Organization (EMBO), and in the past five months has published papers in both  Science Translational Medicine 2  and  Proceedings of the National Academy of Sciences 3  on new mechanisms to counter the artery-hardening condition atherosclerosis. A walk through some of Turkey's research hotspots reveals the same energy, enthusiasm and productivity that one sees in top labs anywhere in the world \u2014 and they are often as well equipped. But, everywhere, scientists say the political uncertainty is becoming unbearable. They have been shaken by both the coup attempt and the government's response to it. One immediate consequence was the collapse of the national science and technology agency T\u00dcB\u0130TAK, the country's main research funder, which also publishes educational books. Problems had been brewing for a long time: T\u00dcB\u0130TAK had been deeply infiltrated by the religious organization known as the G\u00fclen movement, which is believed to have orchestrated the coup attempt. Over the past few decades, these followers of exiled preacher Fethullah G\u00fclen had established themselves in Turkey's military, judiciary and government offices, as well as in universities. The G\u00fclenists were political allies of Erdo\u011fan's ruling Justice and Development Party until 2013, when the extent of their clandestine power base became clear. Under the state of emergency after the coup attempt, Erdo\u011fan started to purge suspected G\u00fclenists from public organizations. T\u00dcB\u0130TAK's former president, Y\u00fccel Altunba\u015fak, was jailed in October 2016 for his alleged role in supporting the coup attempt; the agency also lost large numbers of employees, and those who remained seemed afraid to make decisions, with scientists receiving no information at all about grant opportunities or meetings. \n               Systematic bias \n             Scientists generally agree that removing G\u00fclenists from the system was necessary, and not just because of the coup attempt. They have long complained that T\u00dcB\u0130TAK's distribution of money had become skewed and that processes were no longer transparent. Some social scientists say their projects seemed to be rejected for political reasons. Ali \u00c7arko\u011flu of Ko\u00e7 University studies electoral systems, and has for years contributed data about Turkey to international collaborations such as the International Social Survey Programme and the Comparative Study of Electoral Systems. When T\u00dcB\u0130TAK rejected his proposals to run surveys during the 2015 and 2016 elections in Turkey, he obtained money from external sources, including the Open Society Foundations in New York City, founded by philanthropist George Soros. \u201cBut then critics said I was accepting 'big-capital, Jewish money', insinuating I was part of an international conspiracy,\u201d he says. Chemist Engin Umut Akkaya of Bilkent University, an outspoken government critic, was banned for a year from seeking T\u00dcB\u0130TAK funding in November 2014 \u2014 ostensibly, for ethical breaches in his reports to the agency, charges that he denied. He took the case to court, which ruled in his favour \u2014 two months after the ban was finished. \u201cThe situation here makes us feel sad and unfortunate,\u201d he says. One former member of a T\u00dcB\u0130TAK scientific board, who asked not to be named for fear of retribution, told  Nature  that he resigned in 2015 because the agency had started to replace some of the scientists he had recommended for reviewing panels with people he did not know. \u201cThere was a slide over the years towards arbitrary rules and decisions behind closed doors \u2014 I felt that the administration was interfering with the choices about grants.\u201d The hobbled agency slowly started to work again last December, although some scientists report that they have heard nothing about applications that they submitted as long ago as spring last year. The long gap in funding has been disruptive, researchers say. And scientists have been unnerved by the purges at state universities, which they say have swept away innocent colleagues \u2014 mostly social scientists \u2014 along with the G\u00fclenists. In waves of presidential decrees issued under the state of emergency since last September, more than 7,300 academics from across Turkey have been dismissed from universities. Independent analysts confirm that many of them have no association with the coup \u2014 but were known critics of Erdo\u011fan and his policies. Many had  simply signed a petition  calling for peace between government forces and Kurdish separatists. No one knows when the waves of dismissals will stop. One social scientist in Istanbul, who also asked not to be named, was put under official investigation by his university in December, on suspicion of being a sympathizer of the Kurdistan Workers' Party (PKK), which Erdo\u011fan lists as a terrorist organization. His colleagues confirm that he does not support the PKK, and he did not sign the petition. But he did participate in protests against his university's administration a few years ago. He is now nervously waiting to see whether his name is on the next purge list. If convicted, he will lose his job and his pension, and will never be able to work for a public organization again. He may also lose his passport. \u201cI asked them to show me their evidence, but in the state of emergency they do not have to,\u201d he says. \u201cThings are very bleak.\u201d \n               Atmosphere of unrest \n             In the reigning confusion, the government further upset scientists by removing evolution from a draft high-school curriculum that it published last month. Evolution has been under pressure from Islamists in Turkey for years. In 2009, T\u00dcB\u0130TAK  censored a special issue of its own popular-science magazine  dedicated to Charles Darwin's centenary. Since then, it has stopped publication of any books that mention evolution, says evolutionary biologist \u0130smail Sa\u011flam of Hacettepe University and the University of California, Davis. And last year, the Natural History Museum in Ankara removed its permanent exhibition on evolution. \u201cIt adds up to a systematic trend,\u201d says Sa\u011flam. \u201cThis is not a matter of politics, but of education.\u201d The atmosphere has frightened off foreign scientists. Nearly all international meetings planned to take place in Turkey in 2017 have been cancelled, and individual researchers have dropped plans to visit. Some Turkish scientists, such as Ilday, are not completely fazed by the political atmosphere \u2014 they are watching and waiting to see what happens. \u201cI am not personally affected, and I am happy with the way the country supports science,\u201d he says in his spacious, high-tech labs at Bilkent. His ERC grant allows him to do basic research, which he says is liberating, given that large T\u00dcB\u0130TAK grants are available only for applied science. But even with excellent facilities, Ilday has found that the best students rarely want to stick around. Principal investigators throughout the country routinely report that it is getting ever-harder to recruit PhD students, and almost impossible to hire postdocs. Those who are good enough choose to leave as soon as possible. \u201cI try to concentrate on my science and my students \u2014 but we can't help but be distracted by the general news,\u201d says G\u00fcne\u015f \u00d6zhan, a developmental biologist at the iBG-izmir. \u201cEvery day you just don't know what is going to happen.\u201d Still, \u00d6zhan, who studies brain regeneration in zebrafish, doesn't regret her return home in 2014, after ten years in Germany. The historic Mediterranean city of \u0130zmir makes for pleasant living, and her EMBO Installation grant, along with funds from other sources, gives her as much money as she needs. It has allowed her to build a state-of-the-art zebrafish facility that is better than the one she enjoyed at the Max Planck Institute of Molecular Cell Biology and Genetics in Dresden. She even has a social room where students can sleep when they do overnight experiments. And the iBG-izmir's new status as a national research centre will make things more sustainable, she says. \u201cBut it's getting a bit hard to survive,\u201d she admits. Her colleague Yongsoo Park, a South Korean who researches neurotransmission, expresses the same ambivalence. \u201cThe research facilities are fine \u2014 and, wow, it's so great here that I can go with the kids to the sea every weekend.\u201d But fears linger. His family speaks very little Turkish and he constantly worries about their safety. \u201cThe state of emergency has not served science well,\u201d says Gokhan Hotamisligil, a geneticist at Harvard who closely follows developments in science in his homeland and hosts monthly scientific colloquia for the local Turkish diaspora. These meetings regularly attract more than 100 attendees. \u201cThe country has made wonderful investments in science \u2014 now the government needs to do more to signal that it has not forgotten about it.\u201d See Editorial:  Steps to help Turkey build a future on research \n                 Tweet \n                 Follow @NatureNews \n                 Follow @alison_c_abbott \n               \n                     Steps to help Turkey build a future on research 2017-Feb-15 \n                   \n                     Turkey sacks thousands of university staff 2016-Sep-06 \n                   \n                     Turkish scientists rocked by accusations of supporting terrorism 2016-Jan-18 \n                   \n                     Turkish biomed hub spurs hope amid political strife 2015-Oct-07 \n                   \n                     Turkey election results delight scientists 2015-Jun-16 \n                   \n                     Beyond headscarf symbolism 2013-Aug-14 \n                   \n                     A very Turkish coup 2011-Sep-07 \n                   Reprints and Permissions"},
{"file_id": "543168a", "url": "https://www.nature.com/articles/543168a", "year": 2017, "authors": [{"name": "Laura Spinney"}], "parsed_as_year": "2006_or_before", "body": "Research on collective recall takes on new importance in a post-fact world. Strange things have been happening in the news lately. Already this year, members of US President Donald Trump's administration have alluded to a 'Bowling Green massacre' and terror attacks in Sweden and Atlanta, Georgia, that never happened. The misinformation was swiftly corrected, but  some historical myths have proved difficult to erase . Since at least 2010, for example, an online community has shared the apparently unshakeable recollection of Nelson Mandela dying in prison in the 1980s, despite the fact that he lived until 2013, leaving prison in 1990 and going on to serve as South Africa's first black president. Memory is notoriously fallible, but some experts worry that a new phenomenon is emerging. \u201cMemories are shared among groups in novel ways through sites such as Facebook and Instagram, blurring the line between individual and collective memories,\u201d says psychologist Daniel Schacter, who studies memory at Harvard University in Cambridge, Massachusetts. \u201cThe development of Internet-based misinformation, such as recently well-publicized fake news sites, has the potential to distort individual and collective memories in disturbing ways.\u201d Collective memories form the basis of history, and people's understanding of history shapes  how they think about the future . The fictitious terrorist attacks, for example, were cited to justify a travel ban on the citizens of seven \u201ccountries of concern\u201d. Although history has frequently been interpreted for political ends, psychologists are now investigating the fundamental processes by which collective memories form, to understand what makes them vulnerable to distortion. They show that social networks powerfully shape memory, and that people need little prompting to conform to a majority recollection \u2014 even if it is wrong. Not all the findings are gloomy, however. Research is pointing to ways of dislodging false memories or preventing them from forming in the first place. To combat the influence of fake news, says Micah Edelson, a memory researcher at the University of Zurich in Switzerland, \u201cit's important to understand not only the creation of these sites, but also how people respond to them\u201d. \n               All together now \n             Communication shapes memory. Research on pairs of people conversing about the past shows that a speaker can reinforce aspects of an event by selectively repeating them 1 . That makes sense. Things that get mentioned get remembered \u2014 by both speaker and listener. There's a less obvious corollary: related information that goes unmentioned is more likely to fade than unrelated material, an effect known as retrieval-induced forgetting. These cognitive, individual-level phenomena have been proposed as a mechanism for memory convergence \u2014 the process by which two or more people come to agree on what happened. But in the past few years, clues have emerged that group-level forces influence convergence, too. In 2015, psychologists Alin Coman at Princeton University in New Jersey and William Hirst of the New School for Social Research in New York City reported that a person experiences more induced forgetting when listening to someone in their own social group \u2014 a student at the same university, for example \u2014 than if they see that person as an outsider 2 . That is, memory convergence is more likely to occur within social groups than between them \u2014 an important finding in light of survey data suggesting that 62% of US adults get their news from social media, where group membership is often obvious and reinforced 3 . Groups can also distort memories. In 2011, Edelson, then at the Weizmann Institute of Science in Rehovot, Israel, showed 30 volunteers a documentary. They watched the film in groups of five and, a few days later, answered questions about it individually. One week after the viewing session, participants answered questions again \u2014 but only after seeing answers that members of their group had supposedly given. When most of the fabricated responses were false, participants conformed to the same false answer about 70% of the time \u2014 despite having initially responded correctly. But when they learnt that the answers had been generated randomly, the participants reversed their incorrect answers only about 60% of the time 4 . \u201cWe found that processes that happen during initial exposure to erroneous information make it more difficult to correct such influences later,\u201d says Edelson. Studying those processes as they happen \u2014 as collective memories are shaped through conversation \u2014 has been difficult to do in large groups. Five years ago, monitoring communication in groups of ten or more would have required several rooms for private conversations, many research assistants and lots of time. Now, multiple participants can interact digitally in real time. Coman's group has developed a software platform that can track exchanges between volunteers in a series of timed chats. \u201cIt takes one research assistant 20 minutes and one lab room,\u201d Coman says. Last year, the group used this software to ask, for the first time, how the structure of social networks affects the formation of collective memories in large groups. The researchers fed information about 4 fictional Peace Corps volunteers to 140 participants from Princeton University, divided into groups of 10. First, the participants were asked to recall as much information as they could on their own. Then, they took part in a series of three conversations \u2014 online chat sessions lasting a few minutes each \u2014 with other members of their group, in which they recalled the information collaboratively. Finally, they tried to recall the events individually again. The researchers investigated two scenarios \u2014 one in which the group formed two sub-clusters, with almost all conversations taking place within the sub-clusters, and one in which it formed one large cluster (see 'Hello operator'). Although people in the single cluster agreed on the same set of information, says Coman, those in the two sub-clusters generally converged on different 'facts' about the fictional volunteers 5 . This effect is evident in real-world situations. Palestinians living in Israel and those in the West Bank, who were separated by force during the Arab\u2013Israeli wars of 1948 and 1967, have gravitated to different versions of their past, despite a shared Arab\u2013Palestinian identity 6 . Similarly divergent truths emerged after the erection of the Berlin Wall. In the lab, Coman can manipulate social networks and look at the memories that form. His comparison of the two scenarios revealed the importance of 'weak links' in information propagation. These are links between, rather than within, networks \u2014 acquaintances, say, rather than friends \u2014 and they help to synchronize the versions held by separate networks. \u201cThey are probably what drives the formation of community-wide collective memories,\u201d he says. One function of those weak links might be to remind people of information expunged through the processes of memory convergence. But timing is important. In unpublished work, Coman has shown that information introduced by a weak link is much more likely to shape the network's memory if it is introduced before its members talk among themselves. Once a network agrees on what happened, collective memory becomes relatively resistant to competing information. Coman thinks that memory convergence bolsters group cohesion. \u201cNow that we share a memory, we can have a stronger identity and might care more about each other,\u201d he says. Abundant research links strong group identity with higher reported individual well-being. This is shown by research on the family. At Emory University in Atlanta, Georgia, psychologist Robyn Fivush is studying the stories that families tell themselves. \u201cWhat we find is that adolescents and young adults who know more family stories show better psychological well-being,\u201d she says. Although shared memories may foster more-closeknit groups, they can also distort the role of outsiders, driving a wedge between groups. Memory shapes group identity, which in turn shapes memory, in a potentially vicious cycle. Weak links have an important corrective effect, but in their absence, two groups might converge on mutually incompatible versions of the past. These may be preserved for posterity in statues and history books. But they can evolve over time. \n               Making memories, making histories \n             In Ostend, Belgium, a public monument depicts the Belgian king Leopold II, surrounded by two groups of grateful subjects \u2014 one Belgian, the other Congolese. In 2004, protesters, who felt that the monument misrepresented history, severed the bronze hand of one of the Congolese figures. They explained anonymously to a local newspaper that the amputation more accurately reflected Leopold's role in Belgium's African colony: not genial protector, but brutal tyrant. In 2010, social psychologists Laurent Licata and Olivier Klein of the French-speaking Free University of Brussels carried out a survey to explore different generations' attitudes towards Belgium's colonial past. They found that Belgian students expressed higher levels of collective guilt and support for reparative actions towards what is now the Democratic Republic of the Congo than did their parents, who in turn expressed higher levels than their parents 7 . An important factor shaping that evolution, the researchers suggest, was Adam Hochschild's influential book  King Leopold's Ghost  (Houghton Mifflin, 1998), which painted a much darker picture of the colonial period than had been accepted previously. \u201cThose who were young when that book came out were particularly marked by it,\u201d says Licata, \u201cwhereas older Belgians had grown up with a different set of facts.\u201d Not all collective memories pass into history . Cognitive psychologists Norman Brown of the University of Alberta in Edmonton, Canada, and Connie Svob of Columbia University in New York City have proposed that something besides cognitive and social processes determines whether an event survives the transition across generations: the nature of the event itself. \u201cIt is the amount of change to a person's fabric of daily life that is most crucially at stake,\u201d says Svob. In a study published last year 8 , they reported that the children of Croatians who had lived through the Yugoslav wars of the 1990s were more likely to recall their parents' war-related experiences \u2014 getting shot, for example, or the house being bombed \u2014 than their non-war-related ones, such as marriage or birth of a first child. Wars, like immigration, bring great upheaval in their wake, and so are highly memorable, says Svob. This 'transition theory' she says, could also explain one of the biggest voids in Westerners' collective memory of the twentieth century \u2014 why they easily recall the two world wars, but not the Spanish flu pandemic of 1918\u201320 that probably killed more than either of them. \u201cThe degree of change wrought by war tends to be greater than the degree of change wrought by a pandemic,\u201d says Svob. Others find that explanation puzzling: \u201cIf you lost a loved one in the flu epidemic,\u201d says Fivush, \u201cthen it certainly disrupted your daily life.\u201d The set of collective memories that a group holds clearly evolves over time. One reason for this is that people tend to be marked most by events in their adolescence or young adulthood \u2014 a phenomenon known as the 'reminiscence bump'. As a new generation grows up, events that happen to its members during their youth override the events that previously dominated society, and thus 'update' the collective memory. A 2016 survey by the Pew Research Center in Washington DC showed that the defining historical moments for baby boomers in the United States were the assassination of John F. Kennedy and the Vietnam War, whereas for those born since 1965, they were the terror attacks on 11 September 2001 and the election of former US president Barack Obama 9 . And over time, each generation adds some events and forgets others. Psychologists Henry Roediger of Washington University in St. Louis, Missouri, and Andrew DeSoto of the Association for Psychological Science in Washington DC report, for example, that successive US generations forget their past presidents in a regular manner that can be described by a power function 10 . They predict that Harry Truman (1945\u201353) will be as forgotten by 2040 as William McKinley (1897\u20131901) is today. That evolution is reflected by evolving attitudes towards the future. Roediger and anthropologist James Wertsch, also of Washington University, have observed that US politicians debating the invasion of Iraq in the early 2000s fell into two groups: those who advocated invasion on the grounds that Saddam Hussein had to be stopped like Adolf Hitler before him, and those who opposed it because they feared another bloody, protracted Vietnam War. Although each might have chosen their historical precedent for political reasons, they in turn reinforced that precedent in the memory of anyone who heard them speak. \n               Spotting the fake \n             Research into collective memory is pointing to ways that it might be shaped for the collective good. Edelson and his team gave grounds for optimism when, in a 2014 follow-up to their earlier study, they reported that although some false memories are resistant to change, the people who hold them can nevertheless be influenced by credible information 11 . The team used functional magnetic resonance imaging to scan volunteers' brains as they recalled information about a film. The scans revealed changes in brain activation that correlated with the degree of confidence in an inaccurate memory \u2014 and, ultimately, with whether they reverted to their initial, accurate one. \u201cBy exposing them to the fact that this information is not credible, in most cases, individuals will take that into account,\u201d says Edelson. \u201cIn 60% of cases, they will flip their answer. But even if they maintain a wrong answer, they'll be less confident about it.\u201d Coman has two suggestions from his findings. The first is  directed at the justice system . In some US states, jurors are forbidden to take notes made during a trial into the deliberation room \u2014 a legacy of historically high illiteracy rates and a belief that the group remembers more reliably than the individual. In fact, says Coman, using notes could protect jurors from retrieval-induced biases and group-level social influences. His group is hoping to explore the impact of such rules in more depth. His second suggestion concerns the diffusion of crucial information to the public during emergencies such as epidemics. Having observed that retrieval-induced forgetting is enhanced in high-anxiety situations, he has come up with some advice for officials: draw up a short but comprehensive list of key points, make sure that all officials have the same list, repeat those points often and keep tabs on bad information that enters circulation. During the 2014 Ebola outbreak, for example, concerns in the United States were stoked by a misconception that being in the same room as a person with the infection was enough to catch it. The best way to kill that rumour, Coman says, would have been to explain \u2014 often \u2014 that Ebola could be transmitted only through bodily fluids. \u201cIf you understand the nature of the false information, you can target it for suppression just by mentioning information that is conceptually related but accurate,\u201d he says. Collective memory is a double-edged sword. Some will no doubt  use it to mislead . \u201cThe fact that information can freely circulate in the community has been considered one of the most important and constructive features of open and democratic societies,\u201d Coman says. \u201cBut creating such societies does not inherently guarantee positive outcomes.\u201d False collective memories might be the price of defending free speech. But understanding how they form might offer some protection the next time people are reminded about a massacre that never happened. \n                 Tweet \n                 Follow @NatureNews \n               \n                     Demand decisions based on evidence, not ideology 2017-Jan-31 \n                   \n                     Meet the scientists affected by Trump\u2019s immigration ban 2017-Jan-29 \n                   \n                     Post-truth: a guide for the perplexed 2016-Nov-28 \n                   \n                     Evidence-based justice: Corrupted memory 2013-Aug-14 \n                   \n                     Neuroscience: Idle minds 2012-Sep-19 \n                   \n                     Nature  special: Tracking the Trump White House \n                   Reprints and Permissions"},
{"file_id": "542406a", "url": "https://www.nature.com/articles/542406a", "year": 2017, "authors": [{"name": "Cassandra Willyard"}], "parsed_as_year": "2006_or_before", "body": "How rediscovered chemical tags on DNA and RNA are shaking up the field. Some big ideas seem to appear out of nowhere, but in 2008 Chuan He deliberately went looking for one. The US National Institutes of Health had just launched grants to support high-risk, high-impact projects, and He, a chemist at the University of Chicago in Illinois, wanted to apply. But he needed a good pitch. He had been studying a family of proteins that repair damaged DNA, and he began to suspect that these enzymes might also act on RNA. By a stroke of luck, he ran into molecular biologist Tao Pan, who had been investigating specific chemical marks, called methyl groups, that are present on RNAs. The pair worked in the same building at the University of Chicago, and began meeting regularly. From those conversations, their big idea took shape. At the time, biologists were getting excited about the epigenome \u2014 the broad array of chemical marks that decorate DNA and its protein scaffold. These marks act like a chemical notation, telling the cell which genes to express and which to keep silent. As such, the epigenome helps to explain how cells with identical DNA can develop into the multitude of specialized types that make up different tissues. The marks help cells in the heart, for example, maintain their identity and not turn into neurons or fat cells. Misplaced epigenetic marks are often found in cancerous cells. When He and Pan began working together, most epigenetic research focused on the tags associated with DNA and the histone proteins that it wraps around. But more than 100 different types of chemical mark had been identified on RNA, and nobody knew what they did. Some of the enzymes He was studying could strip off methyl groups, and He and Pan wondered whether one of them might work on RNA. If the marks could be reversed, they might constitute an entirely new way of controlling gene expression. In 2009, they got funding to hunt for reversible marks on RNA and the proteins that erase them. Nine years later, such research has given birth to an 'ome of its own, the epitranscriptome. He and others have shown that a methyl group attached to adenine, one of the four bases in RNA, has crucial roles in cell differentiation, and may contribute to cancer, obesity and more 1 , 2 . In 2015, He's lab and two other teams uncovered the same chemical mark on adenine bases in DNA (methyl marks had previously been found only on cytosine), suggesting that the epigenome may be even richer than previously imagined 3 . Research has taken off. \u201cI think we're approaching a golden age of epigenomics and epitranscriptomics,\u201d says Christopher Mason, a geneticist at Weill Cornell Medical College in New York City. \u201cWe can actually start to see all these modifications that we knew have been there for decades.\u201d \n               Marking the messenger \n             The governing rule of molecular biology \u2014 the central dogma \u2014 holds that information flows from DNA to messenger RNA to protein. Many scientists therefore viewed mRNA as little more than a courier, carrying the genetic information encoded in a cell's nucleus to the protein factories in the cytoplasm. That's one reason why few researchers paid much attention to the modifications made to mRNA. They weren't a secret, though. The mark that pushed He to the forefront of epitranscriptomics was first discovered on mRNA in 1974 (ref.  4 ). Fritz Rottman, an organic chemist at Michigan State University in East Lansing, was trying to understand the role of RNA in regulating gene expression when he stumbled across a methyl group on adenine. The modified base is called  N 6 -methyladenosine, a mouthful that's commonly shortened to m 6 A. Rottman and his colleagues wrote that RNA methylation could be a way to select certain transcripts for translation into protein. \u201cBut that was all speculation,\u201d says Karen Friderici, an author of the 1974 paper and a geneticist at Michigan State University. The team didn't have a good way to investigate the mark's true function. \u201cIt was the beginning of molecular biology. We didn't have many of the tools that are available now,\u201d she says. More than three decades later, He and Pan found the tools still lacking. \u201cIt's very difficult to actually study these modifications,\u201d Pan says. It requires powerful mass spectrometry and high-throughput sequencing techniques. Two members of He's lab at the time, Ye Fu and Guifang Jia, pushed forward anyway, focusing on a protein called FTO, part of the family of methyl-stripping enzymes that He's group had been studying. Fu and Jia thought that it might remove methyl groups from RNA, but they struggled to identify its target. Fu and his colleagues began to synthesize snippets of RNA that contained different modifications, to determine whether FTO could remove them. It was slow going. Over the course of three years, the team faced a string of failures, \u201cI almost thought I would never find the function,\u201d Fu says. Finally, in 2010 the team decided to test FTO's activity on m 6 A \u2014 the methylated adenine. The mark disappeared. The team had shown for the first time that RNA methylation was reversible 5 , just like the marks found on DNA and histones. To He, it seemed like proof of an RNA-based system of gene regulation. \n               Evidence mounts \n             He's group wasn't the only one thinking about m 6 A. In 2012, two teams of researchers independently published the first maps of where m 6 A appears 6 , 7 . The studies revealed more than 12,000 methylated sites on mRNAs originating from about 7,000 genes. \u201cAfter years in the dark, we were instantly facing a wide vista,\u201d wrote Dan Dominissini, an author of one of the studies, in an essay in  Science 8 . The maps showed that the distribution of m 6 A is not random. Its location suggested that the mark might have a role in alternative splicing of RNA transcripts, a mechanism that allows cells to produce multiple versions of a protein from a single gene. Over the past few years, researchers have identified some of the machinery involved in regulating these marks. Each requires a writer to place it, an eraser to remove it and a reader to interpret it (see \u2018Reading, writing and regulation'). As the identities of these proteins emerged, scientists have come to understand that m 6 A affects not only RNA splicing, but also translation and RNA stability. One m 6 A reader, for example, makes mRNA degrade faster by shuttling it to decay sites in the cell. Another m 6 A reader promotes protein production by shepherding methylated RNA to the ribosome. Whether m 6 A directs a cell to produce a protein or destroy a transcript depends on the location of the mark and on the reader that binds to it. But understanding how this selection works has been a major challenge, says Gideon Rechavi, a geneticist at Tel Aviv University in Israel who was involved in the mapping of m 6 A. What is clear is that m 6 A has fundamental roles in cell differentiation. Cells that lack the mark get stuck in a stem- or progenitor-like state. That can be lethal: when He and his colleagues disabled the m 6 A writer in mice, many embryos died  in utero . He has a possible explanation for the role of m 6 A. Each time a cell changes from one state to another \u2014 such as during differentiation \u2014 the mRNAs in it must change too. This change in mRNA content, which He calls a transcriptome switch, requires precision and careful timing. He thinks that the methyl marks might be a way for cells to synchronize the activity of thousands of transcripts. Self-described 'RNA geek' Wendy Gilbert, a biologist at the Massachusetts Institute of Technology in Cambridge, says that He's explanation is plausible. \u201cOne of the things that I really like about Chuan's presentations over the last couple of years is his effort to try to speak to what is the most important aspect of the mark,\u201d she says. But she points out that there are other ways to coordinate the expression of large groups of genes, such as microRNAs, small bits of RNA that do not code for proteins and that help to silence genes. \u201cI don't know that m 6 A is the only way that you could do that,\u201d she says. \n               The A's have it \n             Although scientists have long known that RNA carries a host of modifications that decorate all four of its bases, mammalian DNA seemed to have only a few marks, all on cytosine. The most common modification in mammals, 5-methylcytosine or  5 mC, is so important that it's often referred to as the 'fifth base', after A, C, T and G. But He wondered whether there might be other marks hiding in the genome. Bacteria carry the DNA equivalent of m 6 A \u2014 called  N 6 -methyladenine or  6 mA. \u201cThey use the methylation to distinguish between their own DNA or foreign DNA,\u201d says Eric Greer, a biochemist at the Boston Children's Hospital in Massachusetts. But researchers struggled to confirm its presence in more complex organisms. In 2013, He's postdoc Fu had found an intriguing paper from the 1970s, which showed that algal DNA contains methylated adenine 9 . \u201cNobody ever knew the function, and nobody ever followed up,\u201d Fu says. Fu and another postdoc, Guan-Zheng Luo, decided to take the investigation further and map the distribution of  6 mA in the DNA of the alga  Chlamydomonas . They found it in more than 14,000 genes. And the distribution wasn't random:  6 mA clustered around the places where transcription begins. \u201cWe saw some periodic pattern of the peaks. It's like one peak after another,\u201d Fu says. It might be promoting gene activation, they reasoned. Nearly 2,000 kilometres away in Boston, Greer and his colleagues had found  6 mA in the genome of a worm,  Caenorhabditis elegans . Greer, a postdoc at the time, had been studying epigenetic inheritance using a  C. elegans  mutant that becomes less fertile with each successive generation. He wanted to understand how this infertility is transmitted from one generation to the next.  Caenorhabditis elegans  had long been thought to lack methyl marks, but Greer decided to double-check using antibodies that can bind specific methylated bases. He and his colleagues didn't find any  5 mC, but they did detect  6 mA. What's more, the levels seemed to be higher in the less-fertile generations, \u201craising the possibility that it could indeed be a carrier of this non-genetic information\u201d, he says. The result came as a surprise. Researchers had looked for  6 mA in multicellular organisms before, but they weren't able to find it because it is present at such low levels. Greer's lab head, Yang Shi, knew that He had uncovered  6 mA in algae, and asked him for help. When He heard what Shi had found, he was excited. \u201cWe decided we're going to do this together,\u201d He says. A couple of months later, He met a researcher in China who had found  6 mA in the fruit fly  Drosophila . \u201cI almost fell to the floor,\u201d He says. In April 2015, the three papers came out simultaneously in  Cell 10 , 11 , 12 . Andrew Xiao, who studies epigenetics at Yale University in New Haven, Connecticut, read the articles with interest. Xiao and his colleagues had identified  6 mA in mammalian cells, but they hadn't published their results. \u201cLiterally we thought nobody will take interest in this field,\u201d Xiao says. The  Cell  articles proved him wrong. \u201cWe realized we should hurry up.\u201d A year later, Xiao and his colleagues showed that  6 mA can be found at exceedingly low levels in mouse embryonic stem cells. When the researchers looked at the distribution of the mark, they found the strongest peaks on the X chromosome. Here, the mark seemed to be involved in silencing gene expression. The researchers also identified an enzyme that seems to be a  6 mA eraser 13 . Xiao is still unravelling the function of  6 mA. He says that it seems to be crucial at certain developmental stages, acting like a molecular switch \u2014 barely present one moment, then there's a surge, and then it disappears. \u201cHis paper was absolutely a bombshell,\u201d says Samie Jaffrey, a researcher at Weill Cornell Medical College. \u201cIt really showed functional roles for  6 mA.\u201d Both He and Shi say they have also found  6 mA in mammalian cells, but haven't yet published their results. However, the significance of  6 mA isn't yet clear, Shi says. He points out that even with the latest technology, the modification is only borderline detectable and its precise location cannot be mapped. And the pattern of  6 mA will probably vary from tissue to tissue. There are still big questions to untangle. Mamta Tahiliani, a geneticist at New York University School of Medicine in New York City calls the  6 mA work \u201cincredibly exciting\u201d, but points out that researchers haven't yet shown that the mark passes from one generation of cells to its progeny, a hallmark of epigenetic modifications. \n               Mining for more marks \n             As some researchers dive deep to try to understand the function of m 6 A and  6 mA, others are looking for new modifications. Last year, He, Rechavi and their colleagues reported 14  the discovery of another methyl mark on adenine in RNA called  N 1 -methyladenosine (m 1 A). This mark also seems to promote translation, although the underlying mechanism is different from that of  6 mA. He says it might also have a role in synchronizing transcripts for the transcriptome switch. Then, in January, Jaffrey and his colleagues reported on yet another kind of modification that occurs near the caps of mRNAs. The researchers found that mRNAs with this mark \u2014 called m 6 Am \u2014 are more stable because their caps are harder to remove 15 . \u201cIt's exciting to people that the landscape of potentially regulated messenger RNA modifications that might influence gene expression could be an order of magnitude more complex than we thought before,\u201d Gilbert says. Along with these new discoveries also come scientific squabbles. Jaffrey's work 15  suggests that FTO, which He identified as an m 6 A eraser, actually targets m 6 Am. And in October, He's group reported 16  that the enzyme Xiao flagged as a  6 mA eraser on DNA actually does a better job of stripping m 1 A off a particular type of RNA. But such ambiguities are to be expected in a field that's experiencing a scientific gold rush. \u201cWe are only in the beginning of the story,\u201d Rechavi says. And as the techniques improve, scientists will be able to see these marks more clearly. The wealth of research possibilities makes Mason feel \u201ceuphoric\u201d, he says. \u201cIt's like the most exciting time to be working in the field.\u201d See Technology Feature:  The RNA code comes into focus \n                 Tweet \n                 Follow @NatureNews \n               Reprints and Permissions"},
{"file_id": "543021a", "url": "https://www.nature.com/articles/543021a", "year": 2017, "authors": [], "parsed_as_year": "2006_or_before", "body": "A special issue explores the intersection of science and migration. In today\u2019s hyperpolarized environment, public discussion about refugees and migrants often swerves away from fact. Numbers swell or shrink, depending on the political context, and evidence can get lost amid distortions. Scientists have an important role to play here: many are seeking to improve how nations track and deal with people forced from their homes, and they are some of the most mobile members of society themselves. This week,  Nature  takes a close look at the  intersection of science and migration . The United Nations has declared that the number of displaced people has surged to unprecedented numbers. But a close examination of data reveals that  past flows have been just as high . Yet human stories reveal what numbers cannot. Refugees often struggle to settle in foreign countries, and  three displaced scientists explain the acute challenges  of trying to restart research careers while worrying about the safety of distant families. When immigrants can successfully settle in new countries, they often bring ideas, skills and gritty determination to their new homes. The United States has  benefited greatly from foreign-born scientists and technologists  \u2014 until now \u2014 writes innovation researcher Vivek Wadhwa. He warns that the country\u2019s sluggish visa system is deterring many of the world\u2019s best technology entrepreneurs, who have settled elsewhere instead. Two other articles highlight the growing role of technology in monitoring human mobility. Policy analyst Gemma Galdon Clavell describes how the rush to amass data for security purposes, through the European Union\u2019s Smart Borders initiative, is  overstepping citizens\u2019 rights . Huub Dijstelbloem explains how officials are using data from satellites and biometric tests to  track migration and humanitarian crises , sometimes without understanding the results. With the election of US President Donald Trump and the United Kingdom\u2019s decision to leave the European Union, many foreign-born scientists working in those countries are  worried about their visa statuses, funding and job prospects . The future for them \u2014 and for many immigrants around the world \u2014 remains uncertain. But people will continue to set off for new shores in search of better lives and careers.\n \n                 Tweet \n                 Follow @NatureNews \n               \n                     Nature  special: Human migration \n                   Reprints and Permissions"},
{"file_id": "543024a", "url": "https://www.nature.com/articles/543024a", "year": 2017, "authors": [{"name": "Gunjan Sinha"}], "parsed_as_year": "2006_or_before", "body": "Displaced researchers face huge challenges making lives abroad, even if they find work. When the war in Syria reached Aleppo in 2012, geographer Mohamed Ali Mohamed fled with his family to a town about 50 kilometres to the north. For two years, he trekked back on public transport every day to teach at the University of Aleppo, despite the constant street fighting and air strikes. By 2014, it had become too dangerous to continue. Then money and food began to dwindle. Ali Mohamed had been invited to do research in Germany, but there was no easy way to get out of Syria. So he paid a smuggler to guide him into Turkey. He went in the dark of night \u2014 leaving his family behind \u2014 and travelled with three other men through the mountains on foot, \u201cwith the constant fear of being shot to death\u201d, he says. From there, he made his way to Berlin. Ali Mohamed is a refugee scientist \u2014 and one of the lucky few able to carry on his research. He is working in Germany, thanks to a new fellowship for displaced scientists offered by the Alexander von Humboldt Foundation in Berlin. The initiative is one of several that act as lifelines for scholars who are forced to flee because of their research, political views or, like Ali Mohamed, war in their home countries. For the organizations helping threatened scholars, the goal is more than saving lives. Countries in political upheaval  risk losing their intellectual capital  when researchers disappear. \u201cThey are the future of higher education in their countries. If they are killed or displaced, ruined societies can't be rebuilt,\u201d says Stephen Wordsworth, executive director of the Council for At-Risk Academics (Cara) in London. And the number of researchers in peril is skyrocketing: in the past two years, Cara has seen applications for support climb from 3\u20134 per week to 15\u201320. The organization is supporting record numbers, says Wordsworth: \u201cThe highest since our early years in the 1930s.\u201d But with the fighting in Iraq and Syria dragging on for years, a short-term job can seem like weathering a typhoon in a canoe. Refugee scientists who spoke to  Nature  expressed enormous gratitude towards the people and programmes who helped them, but they still face relentless challenges even after landing jobs. Visa troubles haunt them. Housing contracts and health insurance are tough to secure. Researchers struggle with unfamiliar tasks such as writing grant proposals and competing for support. And many find it difficult to focus on work with their families stuck in limbo elsewhere. \u201cWe do recognize that many of our scholars have an uphill battle beyond 1\u20132 years,\u201d says Sarah Willcox, director of the Institute of International Education's Scholar Rescue Fund in New York City. Her organization and others have stepped up their efforts to collaborate and provide support to researchers beyond their temporary job placements. Without this help, she says, \u201cmany of our scholars would be lost and their voices silenced forever\u201d. \n               Community ties \n             If it had not been for his mentor's help, Ali Mohamed would probably still be stuck in Syria \u2014 cold, hungry and without any hope of a career. Hilmar Schr\u00f6der, a geomorphologist at the Humboldt University of Berlin, served as Ali Mohamed's adviser when the Syrian completed his PhD in soil cartography at the university. After getting his doctorate in 2010, Ali Mohamed returned to Syria to take a position as a lecturer at the University of Aleppo. He told Schr\u00f6der that he was needed there and it was a permanent faculty position. After the war first broke out in 2011, Schr\u00f6der often sent concerned e-mails to his former student. \u201cThe reply was always 'everything is fine in Aleppo',\u201d says Schr\u00f6der. But soon it wasn't. In July 2012, the  fighting came to Aleppo  and Ali Mohamed's apartment complex was bombed, so he and his family moved in with relatives near the Turkish border. When he had to quit his job in 2014, Ali Mohamed e-mailed the distressing news to Schr\u00f6der, who leapt into action. He rallied Ali Mohamed's former colleagues in Berlin, as well as the university dean and the department director. It took a few months, but they scraped together enough money to fund a one-year position as a guest scientist and e-mailed Ali Mohamed with the offer. In November 2015, he decided to attempt the overland trip to Turkey. Once there, he applied for a German visa, and the next month Schr\u00f6der picked him up from the airport in Berlin. A recently retired colleague of Schr\u00f6der's, who knew Ali Mohamed from his student days, took him to a Christmas party. \u201cI was greeted very warmly and got presents from everyone,\u201d Ali Mohamed recalls. The colleague bought him winter clothes, invited him to stay with her and helped him navigate bureaucracy. He stayed for months. \u201cWithout her he would not have managed it,\u201d says Schr\u00f6der. About six months into Ali Mohamed's stay, he won a fellowship from the Humboldt Foundation's Philipp Schwartz Initiative, through which German academic institutions can apply for funds to host a refugee scholar. The fellowship offers up to \u20ac3,500 (US$3,700) a month for up to 24 months, plus a \u20ac12,000 lump sum to the host institution to cover additional help the scholar may need. The fellowship was \u201cmy salvation\u201d, Ali Mohamed says. Together with department support, he can now count on about three years of funding. He is working on a project to create sophisticated land-use maps of metropolitan regions for urban planning. But he cannot fully concentrate on his work. \u201cI am constantly in fear for my family,\u201d he says, with the reddened and glassy eyes of someone who doesn't sleep much. At the time Ali Mohamed left for Turkey, he felt it was too risky for his small children to travel with him, especially without a Turkish visa. His family had planned to make the trip in 2016, with assurance from the German authorities that they would be granted a German visa from the embassy in Turkey. But on the day they set out, Turkey closed its border with Syria, and they are now stuck in a refugee camp. Schr\u00f6der says that he and his colleagues have been in frequent contact with the German Federal Foreign Office to get Ali Mohamed's family out. But the political and logistical roadblocks have been insurmountable. The German embassy in Syria has been shut, and they are unable to travel anywhere else to obtain a German visa. Ali Mohamed sends money to his family for provisions, but conditions in the camp are poor at best. Winters are particularly harsh. \u201cI am especially worried about the health and safety of my children,\u201d he says. \n               On the run \n             Bioscientist Kassem Alsayed Mahmoud's torment began in 2010. He was conscripted into the Syrian army, despite the fact that he was 36 years old and held an assistant professorship at Al-Furat University in Dayr az Zawr. He was told that he was required to serve for only a year. But when war broke out, he was ordered to remain until the revolution ended. After 19 months of service, and with no end to the bloody conflict in sight, he couldn't take it any more. He deserted. \u201cIf I were caught by the regime, I would have been killed,\u201d he says. Alsayed Mahmoud hid at the homes of various friends and family members, fleeing from place to place as the fighting in Syria intensified. In September 2012, his brother, who was helping him hide, was killed during fighting in Dayr az Zawr. \u201cThe situation had become so dangerous that my family pushed me to leave immediately,\u201d says Alsayed Mahmoud. He fled by motorcycle and then car to Turkey, and sneaked across the border because he did not have a visa. A friend told him about Scholars at Risk, an organization based in New York City. He contacted them to help him find a job, but they were overwhelmed and giving priority to applicants in immediate danger. So in 2013 he flew to Qatar to live with another brother, yet he couldn't find a job there or in neighbouring countries. The following year, he decided to return to Turkey and look for jobs in Europe. He contacted Scholars at Risk again, and this time the agency found him a one-year postdoc position in the food-science department at Ghent University in Belgium. The only trouble was getting there. Like Ali Mohamed, Alsayed Mahmoud had contacts in Europe. He had lived in France for six years and become fluent in French while studying food processing and biotechnology for his doctorate at the National Polytechnic Institute of Lorraine in Nancy. With the help of a friend from those years who had pushed his plight at the French consulate, Alsayed Mahmoud managed to travel to France on a tourist visa, but he struggled to obtain a Belgian visa. He decided to apply for asylum in France and was granted refugee status for ten years. Eventually, he secured a work visa to enter Belgium. In August 2015, Alsayed Mahmoud finally arrived at Ghent University. He experimented with exotic types of chocolate, such as that made of camel's milk or flavoured with black cumin seeds. The research wasn't his area of expertise and his French-language skills weren't particularly helpful in the Flemish part of Belgium, but he was happy to be mentally engaged and to expand his lab skills. A few months into his postdoc, he heard about an opportunity in the French-speaking part of Belgium: the Free University of Brussels (ULB) announced that it would provide ten fellowships for refugee scientists. Alsayed Mahmoud applied and was accepted. Since August 2016, he has been studying environmentally efficient methods for degrading potato waste from industrial processing \u2014 an interesting new area for him, he says. \u201cI can gain experience and learn new skills.\u201d And his one-year postdoc position has been extended for another year. This has all allowed him to settle into life in Brussels, where he has become a mentor to the other refugee postdocs at the ULB, says Serge Hiligsmann, head of the university's biotechnology and bioprocess division. Last year, Alsayed Mahmoud travelled to Turkey to marry his deceased brother's widow, and become a father to her three children. The ULB is helping the family to apply for a Belgian visa, and Alsayed Mahmoud hopes they will join him in Brussels soon. \u201cPeople are so helpful and kind,\u201d says Alsayed Mahmoud. \u201cI never felt like I was not accepted.\u201d \n               Homesick \n             Zamir Al Salim, a geoscientist from Iraq, never felt that kind of welcome at the British university that offered him temporary haven. He describes his experience abroad as one of \u201cno one caring\u201d and constantly feeling \u201calone\u201d. His refugee story began in June 2014, when the Islamist terrorist group ISIS entered his home city of Mosul. A university lecturer in geoscience and an outspoken critic of ISIS, Al Salim \u2014 whose name has been changed to protect him \u2014 says he was threatened and became a target for assassination. So he fled to Turkey with only a briefcase-sized bag. At first he stayed with friends, but he would leave when he felt that he was imposing. He could not find a job and his money ran out, forcing him sometimes to sleep in parks with other refugees. \u201cIt was a nightmare for me,\u201d he says. In September, he learnt that he had been accepted into a training programme on managing artefacts in Japan. But first he needed money to reach Iraq or Oman to obtain a visa. Scholars at Risk gave him a grant to get to Oman, and then the training programme paid for his trip to Japan. Over the course of the two-month programme, he managed to save the equivalent of about $200 of his living allowance by eating only one meal per day. After he returned to Turkey, Cara found him a one-year postdoc position at a university in the United Kingdom. Following another round of visa troubles, he finally arrived at his host institution in January 2015. But Al Salim's excitement soon faded to disillusionment. The university placed him in a department only peripherally related to his expertise, and gave him little guidance. \u201cHe was paradoxically treated both very well and extremely badly,\u201d says Jack Westerly (not his real name). Westerly, who worked in a neighbouring department, had spent years in Mosul conducting fieldwork. He befriended Al Salim, and had him transferred to his own department, where Al Salim could expand on his training in preserving cultural heritage. But Westerly became increasingly alarmed at the lack of a plan for his friend. On the one hand, it was noble of the university to host him, says Westerly. But \u201cit was as if they merely checked off their 'good deed' box and then forgot about him\u201d, he says. Westerly was also very concerned about Al Salim's  mental health . He often expressed guilt over living in comfort and safety while his family and other refugees were suffering, says Westerly. Al Salim is not married, but has parents and siblings in Iraq. Meanwhile, time was running out. Westerly pleaded with the university to extend Al Salim's postdoc, but was told that he wasn't performing. \u201cAdministration were comparing him to any other postdoc. The expectations of him were ludicrous,\u201d says Westerly. By the end of 2015, Cara had found another institution to host him. Al Salim started his job at the new university in September 2016 and even began teaching, but he struggled. \u201cI felt even more down and lonely. I didn't want to leave the house,\u201d he says. \u201cI was seeing [reports of] refugees that were suffering. I started to feel like I don't know who I am. This is not me.\u201d In November 2016, he resigned. \u201cSome people pleaded with me to change my decision. I told them I just don't feel well.\u201d Back in the Kurdish-controlled region of Iraq, he now feels better, he says, even though reliving his memories by telling his story to  Nature  over Skype was \u201cpainful\u201d. He lives on his savings and volunteers where he can. \u201cHere I see students or children, and I help them and that makes me happy.\u201d But there are times when he regrets his decision. He sleeps on the floor, has no access to warm water and is often cold. But he still feels more at home where he is than he did in Britain. He longs to return to Mosul, but with the city still in the throes of battle, his friends tell him it's still too dangerous. \u201cThey say, 'Maybe you will end your life there'. \n               The future \n             In risking his life to go home, Al Salim isn't alone. More than 90% of Iraqi scholars helped by Cara have returned. That is not an option for researchers such as Ali Mohamed. He would like to stay in Germany, where he has \u201cfound community and friends\u201d. But he recognizes that finding a permanent job in science there will be difficult \u2014 such positions are rare, and he faces stiff competition from peers more accustomed to the ways of Western science. \u201cSome scholars completely leave the research field,\u201d says Enno Aufderheide, secretary-general of the Humboldt Foundation. \u201cSome become teachers or sell scientific equipment in countries where they have special cultural knowledge or language skills.\u201d Alsayed Mahmoud also knows he will struggle to succeed in science in Europe, despite his European PhD. His refugee status limits his movement in the European Union, which puts him at a disadvantage compared with other applicants. Also, he has a three-year gap in his CV and, at 43, he is older than most postdocs who might apply for similar positions. \u201cYou have to start your life from zero,\u201d he says. If Syria were safe and there were job prospects there, he would return home. But he holds little hope. \u201cI can't see that there will be a peaceful solution even within five years,\u201d he says. And although his job is secure for now, that doesn't ease his anguish over all that has vanished. \u201cImagine you have your home, your life, your friends, your history \u2014 and in one moment, you lose all of it.\u201d \n                 Tweet \n                 Follow @NatureNews \n               Reprints and Permissions"},
{"file_id": "543164a", "url": "https://www.nature.com/articles/543164a", "year": 2017, "authors": [{"name": "Elizabeth Gibney"}], "parsed_as_year": "2006_or_before", "body": "Bizarre forms of matter called time crystals were supposed to be physically impossible. Now they\u2019re not. Christopher Monroe spends his life poking at atoms with light. He arranges them into rings and chains and then massages them with lasers to explore their properties and make basic quantum computers. Last year, he decided to try something seemingly impossible: to create a time crystal. The name sounds like a prop from  Doctor Who , but it has roots in actual physics. Time crystals are hypothetical structures that  pulse without requiring any energy  \u2014 like a ticking clock that never needs winding. The pattern repeats in time in much the same way that the atoms of a crystal repeat in space. The idea was so challenging that when Nobel prizewinning physicist Frank Wilczek  proposed the provocative concept 1  in 2012, other researchers quickly proved there was no way to create time crystals. But there was a loophole \u2014 and researchers in a separate branch of physics found a way to exploit the gap. Monroe, a physicist at the University of Maryland in College Park, and his team used chains of atoms they had constructed for other purposes to make a version of a time crystal 2  (see 'How to create a time crystal'). \u201cI would say it sort of fell in our laps,\u201d says Monroe. And a group led by researchers at Harvard University in Cambridge, Massachusetts, independently fashioned time crystals out of 'dirty' diamonds 3 . Both versions, which are published this week in  Nature , are  considered time crystals , but not how Wilczek originally imagined. \u201cIt's less weird than the first idea, but it's still fricking weird,\u201d says Norman Yao, a physicist at the University of California, Berkeley, and an author on both papers. They are also the first examples of a remarkable type of matter \u2014 a collection of quantum particles that constantly changes, and never reaches a steady state. These systems draw stability from random interactions that would normally disrupt other kinds of matter. \u201cThis is a new kind of order, one that was previously thought impossible. That's extremely exciting,\u201d says Vedika Khemani, part of the Harvard team and previously part of the group that originally theorized the existence of the new kind of state. Experimental physicists are already plotting how to exploit the traits of these strange systems in quantum computers and super-sensitive magnetic sensors. \n               Break time \n             Wilczek dreamt up time crystals as a way to break the rules. The laws of physics are symmetrical in that they apply equally to all points in space and time. Yet many systems violate that symmetry. In a magnet, atomic spins line up rather than pointing in all directions. In a mineral crystal, atoms occupy set positions in space, and the crystal does not look the same if it is shifted slightly. When a transformation causes properties to change, physicists call that symmetry-breaking, and it is everywhere in nature \u2014 at the root of magnetism, superconductivity and even the Higgs mechanism that gives all particles mass. In 2012, Wilczek, now at Stockholm University, wondered why symmetry never broke spontaneously in time and whether it would be possible to create something in which it did. He called it a time crystal. Experimentalists imagined a quantum version of this entity as perhaps a ring of atoms that would rotate endlessly, cycling and returning to its initial configuration. Its properties would be endlessly synchronized in time, just as  atom positions are correlated in a crystal . The system would be in its lowest energy state, but its movement would require no external force. It would, in essence, be a perpetual-motion machine, although not one that produces usable energy. \u201cFrom a first glance at the idea, one would say this has to be wrong,\u201d says Yao. Almost by definition, a system in its lowest energy state does not vary in time. If it did, that would mean it had excess energy to lose, says Yao, and the rotation would soon halt. \u201cBut Frank convinced the community that the problem was more subtle than maybe it seemed to be,\u201d he says. Perpetual motion was not without precedent in the quantum world: in theory, superconductors conduct electricity forever (although the flow is uniform, so they show no variation in time). These conflicting issues swam around the head of Haruki Watanabe as he stepped out of the first oral exam for his PhD at Berkeley. He had been presenting work on symmetry breaking in space, and his supervisor asked him about the wider implications of Wilczek's time crystal. \u201cI couldn't answer the question in that exam, but it interested me,\u201d says Watanabe, who doubted such an entity was even feasible. \u201cI wondered, 'how can I convince people that it's not possible?'\u201d Together with physicist Masaki Oshikawa at the University of Tokyo, Watanabe began trying to prove his intuitive answer in a mathematically rigorous way. By phrasing the problem in terms of correlations in space and time between distant parts of the system, the pair derived a theorem in 2015 showing that time crystals were impossible to create for any system in its lowest-energy state 4 . The researchers also verified that time crystals were impossible for any system in equilibrium \u2014 one that has reached a steady state of any energy. To the physics community, the case was clear cut. \u201cThat seemed to be a no-go,\u201d says Monroe. But the proof left a loophole. It did not rule out time crystals in systems that have not yet settled into a steady state and are out of equilibrium. Around the world, theorists began thinking about ways to create alternative versions of time crystals. \n               Particle soup \n             When the breakthrough came, it arrived from an unlikely corner of physics, where researchers weren't thinking about time crystals at all. Shivaji Sondhi, a theoretical physicist at Princeton University, New Jersey, and his colleagues were looking at what happened when certain isolated quantum systems, made of soups of interacting particles, are repeatedly given a kick. Textbook physics says that the systems should heat up and descend into chaos. But in 2015, Sondhi's team predicted that under certain conditions, they would instead club together to form a phase of matter that doesn't exist in equilibrium \u2014 a system of particles that would show subtle correlations never seen before \u2014 and that would repeat a pattern in time 5 . That proposal caught the attention of Chetan Nayak, one of Wilczek's former students, now at the University of California, Santa Barbara, and at Microsoft's nearby Station Q. Nayak and his colleagues soon realized that this strange form of out-of-equilibrium matter would also be a type of time crystal 6 . But not Wilczek's kind: it would not be in its lowest energy state, and it would require a regular kick to pulse. But it would gain a steady rhythm that doesn't match that of the instigating kick, and that means it would break time symmetry. \u201cIt's like playing with a jump rope, and somehow our arm goes around twice but the rope only goes around once,\u201d says Yao. This is a weaker kind of symmetry breaking than Wilczek imagined: in his, the rope would oscillate all by itself. When Monroe heard about this proposed system, he initially didn't understand it. \u201cThe more I read about it, the more intrigued I became,\u201d he says. Last year, he set about trying to form his atoms into a time crystal. The recipe was incredibly complex, but just three ingredients were essential: a force repeatedly disturbing the particles, a way to make the atoms interact with each other and an element of random disorder. The combination of these, Monroe says, ensures that particles are limited in how much energy they can absorb, allowing them to maintain a steady, ordered state. In his experiment, this meant repeatedly firing alternating lasers at a chain of ten ytterbium ions: the first laser flips their spins and the second makes the spins interact with each other in random ways. That combination caused the atomic spins to oscillate, but at twice the period they were being flipped. More than that, the researchers found that even if they started to flip the system in an imperfect way, such as by slightly changing the frequency of the kicks, the oscillation remained the same. \u201cThe system still locked at a very stable frequency,\u201d says Monroe. Spatial crystals are similarly resistant to any attempt to nudge their atoms from their set spacing, he says. \u201cThis time crystal has the same thing.\u201d At Harvard, physicist Mikhail Lukin tried to do something similar, but in a very different system \u2014 a 3D chunk of diamond. The mineral was riddled with around 1 million defects, each harbouring a spin. And the diamond's impurities provided a natural disorder. When Lukin and his team used microwave pulses to flip the spins, they saw the system respond at a fraction of the frequency with which it was being disturbed. Physicists agree that the two systems spontaneously break a kind of time symmetry and therefore mathematically fulfil the time-crystal criteria. But there is some debate about whether to call them time crystals. \u201cThis is an intriguing development, but to some extent it's an abuse of the term,\u201d says Oshikawa. Yao says that the new systems are time crystals, but that the definition needs to be narrowed to avoid including phenomena that are already well understood and not nearly so interesting for quantum physicists. But Monroe and Lukin's creations are exciting for different reasons, too, says Yao. They seem to be the first, and perhaps simplest, examples of a host of new phases that exist in relatively unexplored out-of-equilibrium states, he says. They could also have several practical applications. One could be quantum simulation systems that work at high temperatures. Physicists often use entangled quantum particles at nanokelvin temperatures, close to absolute zero, to simulate complex behaviours of materials that cannot be modelled on a classical computer. Time crystals represent a stable quantum system that exists way above these temperatures \u2014 in the case of Lukin's diamond, at room temperature \u2014 potentially opening the door to quantum simulations without cryogenics. Time crystals could also find use in super-precise sensors, says Lukin. His lab already  uses diamond defects  to detect tiny changes in temperature and magnetic fields. But the approach has limits,because if too many defects are packed in a small space, their interactions destroy their fragile quantum states. In a time crystal, however, the interactions serve to stabilize, rather than disrupt, so Lukin could harness millions of defects together to produce a strong signal \u2014 one that is able to efficiently probe living cells and atom-thick materials. The same principle of stability from interactions could apply more widely in quantum computing, says Yao. Quantum computers show huge promise, but have long struggled with the opposing challenges of protecting the fragile quantum bits that perform calculations, yet keeping them accessible for encoding and reading out information. \u201cYou can ask yourself in the future whether one could find phases where interactions stabilize these quantum bits,\u201d says Yao. The story of time crystals is a beautiful example of how progress often happens when different strands of thought come together, says Roderich Moessner, director of the Max Planck Institute for the Physics of Complex Systems in Dresden, Germany. And it may be, he says, that this particular recipe proves to be just one of many ways to cook up a time crystal. See News & Views:  Marching to a different quantum beat \n                 Tweet \n                 Follow @NatureNews \n                 Follow @LizzieGibney \n               \n                     Condensed-matter physics: Marching to a different quantum beat 2017-Mar-08 \n                   \n                     Quantum computers ready to leap out of the lab in 2017 2017-Jan-03 \n                   \n                     Quantum physics: Flawed to perfection 2014-Jan-22 \n                   \n                     Can matter cycle through shapes eternally? 2013-Sep-04 \n                   \n                     Quantum physics: Time crystals 2013-Jan-09 \n                   Reprints and Permissions"},
{"file_id": "543306a", "url": "https://www.nature.com/articles/543306a", "year": 2017, "authors": [{"name": "Wudan Yan"}], "parsed_as_year": "2006_or_before", "body": "Oil palm has a reputation as an environmental menace. Can the latest genetic research change that? Nathan Lakey practically has to shout to be heard over the whirring machinery in the laboratory at Orion Biosains near Kuala Lumpur. One source of the din, and a major point of pride for Lakey, is a microwave-oven-sized robotic assembler that is snapping plastic widgets together and laser-etching them with serial numbers. The devices are leaf punches, destined for Indonesia, Malaysia and Myanmar. And Lakey \u2014 an American biochemist and chief executive at Orion \u2014 hopes that they will help revolutionize a much-maligned industry. Palm oil is a commodity that generally evokes  images of mass deforestation , human-rights violations and dying orangutans. In Indonesia and Malaysia, where some 85% of the world's palm oil is produced, more than 16 million hectares of land \u2014 rainforest, peat bogs and old rubber plantations \u2014 have been taken over by oil palm, and there is no sign of the industry slowing down. Despite its bad reputation , oil palm is the most productive oil crop in the world. Oilseed rape (canola) currently produces only about one-sixth of the oil per hectare \u2014 soya bean only one-tenth. But oil-palm plantations still aren't getting as much as they could out of their plants. The main problem is that genetic and epigenetic variables can cause some palms to underproduce. And because oil palms mature slowly, growers typically don't know for three to four years whether the trees they plant will turn out to be star performers or worthless wood. That's where Orion comes in. When the leaf punches sent out around southeast Asia return, Orion technicians process the disc of greenery within and can send growers a report on the quality of their young plants. Lakey predicts that, if adopted on a large scale, the test could raise industry revenue by about US$4 billion per year. And, importantly, it could do so without expanding plantations. \u201cWe can get more oil for an equivalent area of land \u2014 this could help take the pressure off deforestation,\u201d Lakey says. Scientists applaud Orion for trying to apply decades of research on oil-palm genetics across the industry. But the company's grand plans aren't guaranteed to bear fruit. Its efforts are being threatened by poor regulation, political corruption and significant inertia, particularly among the smallholding farmers, who produce about 40% of the palm oil consumed worldwide. \u201cUnless you also have a better national system to provide the finances, training and technical assistance, the science won't have the impact that it should,\u201d says Andrew Bovarnick, global head of the Green Commodities Programme at the United Nations Development Programme in Panama City. Some experts even worry that boosting the productivity of the trees could do more harm than good. \n               Good shells, bad karma \n             Oil-palm plantations carpet the landscape north, east and south of Kuala Lumpur. From above, they look tidy and lush, but at ground level they are muddy, hilly and a world away from the high-tech sterility of Orion's lab. Workers trudge among the rows, hoisting long poles with scythes at the end into the canopy to bring down spiky bunches of fruit. The bunches are then brought to the main roads so that trucks can load and transport them to the mills. Slicing open one of the golf-ball-sized fruits reveals an orange outer mesocarp, which generates the oil used for cooking and processed foods, then a brown shell that separates the mesocarp from the white palm kernel. Kernel oil is typically used in cosmetics, soaps and detergents. Although the plants largely look the same, growers know that there are three types of seed and that they yield dramatically different amounts of oil: dura, pisifera and tenera. Dura seeds produce a thick-shelled fruit with little oil. Pisifera seeds tend to abort during development, but when they do produce fruits, they are often shell-less. Tenera seeds, a hybrid of pisifera and dura, produce a nice thin shell and copious oil. The gene that controls shell thickness, creatively named  SHELL , was identified in 2013 by researchers at Orion, Cold Spring Harbor Laboratory in New York and the Malaysian Palm Oil Board (MPOB), a government research institute based near Kuala Lumpur 1 . Conventionally, plant breeders generated tenera seeds by taking pollen from male pisifera trees and crossing it to flowers belonging to the female dura (see ' Palm-reading '). The seeds mature and can germinate nine months later. Saplings need to spend a year in a nursery before they can be planted in the field, and trees don't start bearing fruit for another 3\u20134 years; they can then be productive for the next 20\u201330. Breeding is time-consuming and doesn't always produce the best seeds. So, in the 1970s, scientists turned to cloning. Researchers realized that they could cut open the top of the trunks of their highest-yielding trees, extract stem cells and grow up clones by the thousands in lab dishes. At first, clones helped to increase oil production on plantations, but in 1977, something strange started to happen. At the time, Tan Yap Pau was a plant breeder and researcher at United Plantations, a Danish palm-oil company based in Malaysia. Field assistants started noticing atypical fruits forming on a plot of land that was populated by clones, and brought them to Tan. The fruits were jagged, disfigured and tulip shaped. Tan knew from his research that young plants could develop abnormally in culture, but he took these misshapen fruits as a bad omen. Many of the 'mantled' fruits produced no oil at all, despite being genetically identical to high-yielding plants. It took nearly 40 years to unpick the mystery. In 2015, Meilina Ong-Abdullah, a plant biotechnologist at the MPOB \u2014 and, coincidentally, Tan's niece \u2014 identified the culprit 2 . She and her colleagues found  Karma , a mobile piece of DNA known as a transposon, inserted in the middle of an important gene for normal oil-palm fruit development called  DEFICIENS . Cells can silence transposons by attaching methyl groups to them. The fruit develop normally when  Karma  is highly methylated (scientists call this  Good Karma ). But low methylation \u2014  Bad Karma  \u2014 results in mantled fruit. In addition to selecting for tenera seeds with  Good Karma , there are other traits in the plant that could be mined for optimization. Rajanaidu Nookiah, a plant breeder at the MPOB, spent more than 30 years travelling the world to understand natural variation in oil palm. He has amassed more than 110,000 oil-palm seedlings \u2014 the largest collection in the world. His specimens host a number of traits that plant breeders would want to see in oil-palm trees. According to Sime Darby, one of the largest palm-oil corporations in Malaysia, the ideal tree would not only yield a lot of oil, it would also be on the shorter side, making it easier to reach fruit stalks. The fruit stalks themselves would be longer and therefore easier to cut. The trees would be more resistant to disease. And the fruit would contain more carotene and iodine, so the oil would potentially be healthier for consumers. \n               Punching up \n             Being able to look at  SHELL  and  Karma  alone could have a huge impact on the industry, according to Lakey. In theory, growers can squeeze as much as 18 tonnes of oil in a year from one hectare of oil palm, but currently they are attaining only about 4 tonnes per year on average (see 'Mass production'). Smallholders typically perform the worst. Without in-house research facilities to help breed and identify the best plants, they generate about half the amount of oil per hectare that large plantations can. Lakey is hoping that the leaf-punch tool that Orion sells can benefit smallholders. For about $4 per tree, Orion can screen for hybrid tenera palms with  Good Karma  and help farms and nurseries cull unproductive plants before investing too much time and resources in them. The company is looking to incorporate other genes, too, such as a gene variant that makes fruits change colour more dramatically as they ripen. This could reduce the likelihood of harvesting immature fruit. But reaching smallholders could be difficult. They are generally either independent or part of cooperatives, in which case they get planting materials and assistance from a nearby corporation. In Malaysia, independent smallholders get planting material that has been validated by the MPOB from trusted oil-palm breeders, but that layer of quality control doesn't exist in Indonesia. According to Tri Widjayanti \u2014 national project manager for the United Nations Development Programme's Sustainable Palm Oil Initiative Project in Jakarta \u2014 many of Indonesia's independent smallholders lack the resources to obtain the best plants. If farmers can't access a nursery or don't have the finances to buy high-quality seeds, they may just grow whatever falls from their trees. The offspring of high-oil-producing tenera hybrids will generate high-yielding progeny only about half the time. Researchers have found that non-tenera palms constitute about 11% of the planting material on smallholders in Malaysia 3 . But it's not just about seeds. Many of those farmers can't afford good fertilizer let alone keep abreast of the best farming practices. \u201cThese smallholders don't necessarily need seeds with a better genome,\u201d says Bovarnick, who has worked in Indonesia since 2010. Moreover, smallholders have little incentive to plant the most productive trees because they are often paid for their fruit by the kilogram. Some farmers have learnt that mantled fruits soak up moisture and gain weight in the night time. They have used this fact to their financial advantage. \u201cSmallholders know about contamination, but they don't care,\u201d Lakey says. Some mills, he adds, are starting to assess fruit quality over weight. That could shift incentives. But others are sceptical that any technology can actually amend palm oil's destructive reputation. \u201cThe current system on the ground cannot deliver deforestation-free palm oil,\u201d says Cynthia Ong, executive director of Forever Sabah, a non-governmental organization focused on environmental issues in Malaysian Borneo. \u201cWhen we go into the field or hold workshops, villagers say, 'I want to take all my remaining land that's rainforest and just grow oil palm'.\u201d For these villagers, palm oil seems to be the easiest way to make money from the land. In areas of Indonesia and Malaysia, the economic and political agendas of international investors, corporations and local officials might also thwart Orion's efforts. Agus Sari \u2014 chief executive of the Belantara Foundation, a non-profit organization focused on environmental conservation and rehabilitation in Jakarta \u2014 says that weak law enforcement is a major challenge. Large plantations bribe local officials to give them additional land titles, making it hard for others to obtain them legally. The same is true in Malaysia, according to Ong. And although scientists might think that yield-improving technology will decrease the pressure to clear more land, others predict the opposite. Technological boosts to efficiency could increase the profitability of palm oil, making it  even more attractive to developers . \u201cAdvances to increase yield must be accompanied by stricter regulation and legal limits to the amount of land that can or should be devoted to palm oil,\u201d says Jeff Conant, who directs the international forests programme for Friends of the Earth, an environmental-justice organization in San Francisco, California. Reforming the industry will require broad buy-in, and there is some promise that this is starting. In 2004, industry members formed the Roundtable for Sustainable Palm Oil (RSPO), which established standards of practice and a label for producers that comply. But the group has a backlog of complaints about non-compliance of its members. One Malaysian company, IOI Group, had its status suspended last April and then reinstated in August, leading some to question the credibility of the RSPO label. Darrel Webber, chief executive of the RSPO, acknowledges that there is room for improvement. \u201cThe only way for us to provide solutions is if others provide feedback to us.\u201d Despite the challenges, Lakey is undeterred. Back in the noisy lab space, technicians are analysing samples \u2014 more than 1,000 per day. And Lakey wants to start sending punches to Africa, South America and Thailand soon, and to analyse up to 10 million punches per year. Part of what drives him and the other scientists he works with is the knowledge that demand for palm oil is rising, even as the land to grow it on dwindles. Compared with global demand for palm oil in 2000, demand is expected to double by 2030 and triple by 2050. \u201cI don't see any other crop that can satisfy the world's needs,\u201d says Raviga Sambanthamurthi, a biochemist and former director of the MPOB's Advanced Biotechnology and Breeding Centre. \u201cWe don't have much more land to open up, so we have no choice but to make oil palm more productive.\u201d \n                 Tweet \n                 Follow @NatureNews \n               \n                     Indigenous peoples must benefit from science 2015-Oct-20 \n                   \n                     Policy: Define biomass sustainability 2015-Jul-29 \n                   \n                     Tropical protection 2015-Jul-29 \n                   \n                     Stopping deforestation: Battle for the Amazon 2015-Apr-01 \n                   \n                     Environment: Curb clearance for oil-palm plantations 2013-Aug-14 \n                   \n                     Palm-oil boom raises conservation concerns 2012-Jul-04 \n                   Reprints and Permissions"},
{"file_id": "543302a", "url": "https://www.nature.com/articles/543302a", "year": 2017, "authors": [{"name": "Brooke Borel"}], "parsed_as_year": "2006_or_before", "body": "Agricultural scientists look beyond synthetic chemistry to battle pesticide resistance. The first thing Broc Zoller does every morning is check the weather forecast. For the past five years, California farmers like him have struggled through historic drought. Now they face the opposite problem. In the first months of 2017, it has already rained more than it did all of last year in Kelseyville, where Zoller grows wine grapes and walnuts, and leases out land to pear growers. The muddy conditions have slowed pruning efforts and delayed the application of sprays used to control key insect species over the winter. If the rains continue as spring arrives, the combination of warmth and wetness could spark fungal and bacterial infections. To protect his crops, Zoller suspects he will have to use several conventional pesticides. But the selection is getting slimmer, thanks to resistance. Fire blight, a bacterial disease that can cause weeping cankers on pear-tree trunks, generally responds to antibiotics, but the drugs can stop working if over-used. And pear scab \u2014 a fungus that leaves unsightly brown lesions on the fruit \u2014 calls for multiple fungicides throughout the growing season. Zoller, who also works as an agricultural pest-control adviser, uses some of these chemicals just once before they start to lose effectiveness. \u201cThe resistance comes so quickly,\u201d he says. \u201cYou hope there aren't too many rains so that what you have in your arsenal can get you through.\u201d Resistance to conventional pesticides \u2014 among insects, weeds or microbial pathogens \u2014 is common on farms worldwide. CropLife International, an industry association based in Brussels, supports efforts that have counted 586 arthropod species, 235 fungi and 252 weeds with resistance to at least one synthetic pesticide (see 'The rise of resistance'). And that's just the cases that scientists have formally identified and recorded. For several decades, the agrochemical industry has simply rolled out new chemicals to replace the old ones. But for many crops, the pipeline is drying up. The rate of discovery of pesticides has \u201cgone almost to zero in the last ten years or so\u201d, says Sara Olson, a senior research analyst at Lux Research in Boston, Massachusetts, which specializes in emerging technologies. New chemicals are difficult and expensive to find and develop. And once one is in use, pests will soon develop resistance to it, unless its application is carefully managed. So scientists are pursuing alternatives that may reduce or replace synthetic pesticides. They are particularly interested in biological solutions, including microbes, genetic engineering and biomolecules. Even major chemical companies see enough promise to invest in the work. That doesn't spell the end of synthetic pesticides, but it could help to slow the spread of resistance. Some approaches might also help farmers to reduce costs, protect workers and please a public that is  increasingly wary of synthetic chemicals . \u201cEmerging pest resistance is a big driver for finding alternatives,\u201d says Olson. \u201cBut for the most part, it's not a choice between chemicals and biological or other options \u2014 it's the recognition that you can do more in a more nuanced way with some of these tools.\u201d \n               Microbial helpers \n             At the start of the twentieth century, a mysterious epidemic was wiping out prized silkworms across Japan. In 1901, the bacteriologist Ishiwata Shigetane uncovered the cause \u2014 an unknown soil bacterium that he found inside a dead silkworm. A decade later, in the German province of Thuringia, biologist Ernst Berliner found the bacterium in flour-moth caterpillars \u2014 a common pest \u2014 and formally described the insect killer, which he named  Bacillus thuringiensis  ( Bt ). Proteins produced by  Bt  perforate the intestines of several insect species, and have been used as a natural pesticide for decades. Scientists have long been looking for more pest-killing microbes. \u201cIt was not a young field when I was a graduate student, nearly 45 years ago,\u201d says Roger Beachy, a plant biologist and pathologist at Washington University in St. Louis, Missouri. But microbes are now making their way  to the agrochemical mainstream . In 2012, Bayer CropScience paid a reported US$425 million for AgraQuest, a biopesticide company based in Davis, California. Over the past few years, other multinational companies, including DuPont, Monsanto and Syngenta, have also invested. Beachy, who was a pioneer in developing genetically modified (GM) food crops, has plunged into microbes along with Indigo Agriculture, a start-up based near Boston. Indigo scientists select microbes to enhance crops' endobiomes \u2014 the microorganisms that live within the tissues of the plants \u2014 and incorporate them into a coating that can be applied to seeds. When the seedling sprouts, its stem gets small scrapes as it pushes through the tough seed. These scrapes should allow the microbes to colonize the plant and help protect it against environmental stress, such as drought. Last year, the company raised a reported $100 million in funding. Indigo is tight-lipped about the specific strains it uses. But already, farmers have planted the company's coated seeds on 20,000 hectares of cotton and 8,000 hectares of wheat in the United States. That's not a lot compared with the 4 million hectares of cotton and 21 million hectares of wheat that US farmers planted in 2016, but it suggests that people are willing to try it out. Beachy, who initially served as the company's chief scientific officer and still chairs its science-advisory board, says that Indigo is actively looking to add pest resistance to the advantages its coating confers. \u201cI would hope that within five years there will be a handful of products available,\u201d he says. Other companies are already using microbes as pesticides. Marrone Bio Innovations in Davis grows microbes and uses them and the chemicals they produce to kill pests. The company has screened 18,000 microbe genomes and has, so far, brought 5 products to market. One of its microbes \u2014 a strain of the bacterium  Burkholderia  \u2014 produces multiple types of chemical, depending on how it's grown. It is used to produce an insecticide and a nematicide (used to control certain worms), and it may also be able to produce a herbicide. Burkholderia  \u201chas the genetic machinery to make multiple classes of compounds\u201d, says Pamela Marrone, the company's chief executive and founder, possibly because of how it evolved to defend itself. Historically, farmers have been wary of biopesticides, in part because the materials are trickier to use than synthetics. Some can degrade quickly in sunlight or heat, for example. And they aren't generally as effective as synthetics \u2014 they simply don't pack the same lethal punch. But a direct replacement isn't necessarily the point. Instead, biopesticides can reduce synthetic-chemical use, says Marrone. The microbes \u201cdon't have to work perfectly as well as the chemicals, although some of ours do stack up\u201d, she adds. \u201cBut when they're integrated in, they get better yield and quality than chemicals alone.\u201d \n               CRISPR'd resistance \n             The powerful gene-editing tool CRISPR\u2013Cas9 has granted scientists new abilities. Although previous technologies \u2014 such those that create GM organisms by adding new genes \u2014 can directly kill insect pests or make crops impervious to powerful herbicides, engineering crops to resist disease has been trickier. One reason is how disease-resistance genes are regulated in plant cells. \u201cIn nature, resistance genes are typically held on a pretty tight leash,\u201d says Adam Bogdanove, a plant pathologist at Cornell University in Ithaca, New York. If they become too active, they can damage the plant. But in conventional GM organisms, scientists can't control where an added gene ends up in the target genome \u2014 and disease-resistance genes may not express correctly if they land in the wrong spot.  CRISPR is particularly useful , says Bogdanove, because it \u201clets you control the position of insertion, and therefore control the expression\u201d. Bogdanove is using the technique to make rice that is inherently resistant to bacterial leaf streak and blight, two of its most devastating diseases. His collaborator Jan Leach, a plant pathologist at Colorado State University in Fort Collins, is also experimenting with CRISPR and older gene-editing tools to target plant immune systems, breeding rice that is resistant to a wide range of diseases, rather than just one. Scientists are CRISPRing other crops \u2014 particularly plants that  weren't widely targeted in the earlier GM revolution , because they were too difficult to engineer. Researchers at Rutgers University in New Brunswick, New Jersey, are using the technique to make wine grapes that thwart downy mildew. A team in the United States has made tomatoes that are resistant to several  Pseudomonas  and  Xanthomonas  bacteria 1 . And scientists in Beijing have made powdery-mildew-resistant wheat 2 . Altering wheat is challenging because the plant contains three nearly identical genomes. The Beijing team essentially had to target three versions of a resistance gene. With CRISPR, \u201cyou are able to knock out several genes simultaneously\u201d, says team member Caixia Gao, a plant biologist at the Chinese Academy of Sciences'  Institute of Genetics and Developmental Biology . Industry scientists are smitten, too. Last September, for example, Monsanto signed a non-exclusive licence with the Broad Institute in Cambridge, Massachusetts, which in February  won a patent dispute over CRISPR technology . Tom Adams, Monsanto's vice-president of biotechnology, says the company is exploring how CRISPR might be used to enhance disease resistance, drought tolerance and yield in some crops. But the approach could also be used in ways that increase pesticide use. Adams says that gene editing could create crops that can withstand herbicides, much like Monsanto's existing GM crops that tolerate the chemical glyphosate. But these products are controversial; allowing farmers to  use glyphosate liberally has led to over-reliance on it . \n               Running interference \n             Long before CRISPR promised to change the world, bioscientists were excited about another genetic way to control pests: RNA interference (RNAi), a mechanism in which double-stranded RNA molecules are taken up by an organism and effectively shut down a particular gene. In some ways, the technology could make it easy to target specific pests. It is possible to start with a precise genetic sequence and then build small molecules to interfere with gene activity, says Sonny Ramaswamy, director of the National Institute of Food and Agriculture, the research-funding arm of the US Department of Agriculture, which is supporting several RNAi studies. The trick is slipping the molecule inside its target at the right place and time. For example, the RNA needs to be present on or throughout an entire plant to defend against sucking insects. That can be ensured through genetic engineering, but the process is expensive and faces the same regulatory and public hurdles as creating any GM organism. And if the pests were to become resistant to the RNAi, researchers would have to engineer an entirely new plant to replace it. Both academic and industry scientists think a better option may be to apply the RNAs directly to crop leaves or roots. That is more \u201cconvenient and flexible than transgenic crops\u201d, says Xuexia Miao, a plant\u2013insect interaction researcher at the Institute of Plant Physiology and Ecology at the Shanghai Institutes for Biological Sciences in China. In 2015, she and her team showed 3  that RNAi delivered through the roots of rice and maize helps to protect against insects. But an irrigation system may be difficult in practice: soil teems with microbes and enzymes that can chop up the RNA before it reaches the plant. Miao is also working on sprays that could deliver RNAi directly to plants and insects. Companies including Monsanto and Syngenta are interested in RNAi, too. Monsanto says its first products \u2014 one to protect against the mite  Varroa destructor , a pest of honey bees, and another against flea beetles that attack oilseed rape (canola) \u2014 will be on the market by the mid-2020s. Syngenta will have its first product, for the Colorado potato beetle ( Leptinotarsa decemlineata ), \u201cby the early 2020s\u201d, says Steven Wall, who oversees regulatory and product-safety strategy for RNAi products at Syngenta in Research Triangle Park, North Carolina. RNAi technology  faces other challenges, too . It seems to be effective against some types of insect, such as beetles, but it is harder to use against moths and their larvae, although the reasons for that aren't clear. The pests that do respond to RNAi could also evolve resistance. \u201cNature always seems to find a way,\u201d says Wall. An RNAi spray \u201chas to be managed like any other product \u2014 it shouldn't be used exclusively\u201d. And some scientists argue that, although RNAi may target pests more directly than a broad-spectrum pesticide, there's still a chance of collateral damage. The RNAi could kill beneficial insects that share genes with a pest. In a 2013 review 4  of the technology's risks, US Department of Agriculture scientists wrote that although RNAi is promising for crop protection, its benefits should be weighed against \u201cthe relative environmental risks that the technology poses\u201d. \n               Back to the land \n             In the meantime, as the pesticide pipeline continues to shrink and resistance grows, farmers need new options. The situation varies from crop to crop and farm to farm, but some fields are down to just one working pesticide. \u201cYou are literally developing resistance to the one you have left,\u201d says Zoller. \u201cAnd so you're putting it on a couple times more than you used to, just because it doesn't last as long. And you have nothing else to use.\u201d Zoller is already testing biopesticides, although the products can be inconsistent. \u201cWe have them in our research tests every year, because we are hopeful,\u201d he says. \u201cSome look pretty good one year, but not the next year, although they're good to integrate in\u201d with conventional pesticides. As for genetic approaches, Zoller thinks that buyers will be wary, thanks to a pervasive fear of GM foods. Other growers are more optimistic. \u201cCRISPR: I think this is the wave of the future if we're going to survive as an industry,\u201d says Tony DiMare, vice-president of the DiMare Company, a US tomato grower. The tech will be particularly important, he says, for environmental stress, pests and disease. Technology alone won't save the farm. Growers will still rely on old-fashioned practices and land management. Crop rotation, for example, helps to break up the life cycles of pests and pathogens \u2014 if farmers don't rotate, planting the same crop year round, they provide ample food for certain pests to thrive. Spacing crops close together can help protect weeds from sunlight. And for other crops, pruning lets in air and light, which can help dry up the dampness that lets moulds thrive. On California's pear orchards such as the ones Zoller leases, farmers let native plants, including wild oats, rye grass and morning glory, grow between the tree rows. This provides habitat for natural predators that curb insect pests. Zoller says it's necessary to use all approaches \u2014 new technology and older methods \u2014 to protect food and profits. For any crop, pest management is an ongoing thing, he says. New technologies will also be something farmers look at. \u201cIt helps to have a lot of tools.\u201d \n                 Tweet \n                 Follow @NatureNews \n               \n                     Policy: Reboot the debate on genetic engineering 2016-Mar-09 \n                   \n                     Transgenics: A new breed 2013-May-01 \n                   \n                     Case studies: A hard look at GM crops 2013-May-01 \n                   \n                     GM crops: Battlefield 2009-Sep-02 \n                   \n                     Nature  special: GM crops \u2014 promise and reality \n                   \n                     Nature Insight : Plants \n                   Reprints and Permissions"},
{"file_id": "what-the-numbers-say-about-refugees-1.21548", "url": "https://www.nature.com/news/what-the-numbers-say-about-refugees-1.21548", "year": 2017, "authors": [], "parsed_as_year": "2011_2015", "body": "Growing concerns over an \u2018invasion\u2019 of refugees and migrants helped to elect Donald Trump and sway Brexit voters. Yet the data suggest that the situation is very different from how it is often portrayed. Researchers warn that misleading reports about the magnitude of flows into Europe and the United States are creating unjustified fears about refugees. That is undermining efforts to manage the massive humanitarian problems faced by those fleeing Syria and other hotspots. \u201cThe alleged increase in migration and forced displacement tells us more about the moral panic on migration than the reality,\u201d says Nando Sigona, a social scientist at the University of Birmingham, UK. The number of refugees and migrants entering the European Union is low compared with the bloc\u2019s population. Nations in Africa and Asia are absorbing many more. \u201cThe number of refugees in Europe is a classic example of perception versus reality,\u201d says geographer Nikola Sander at the University of Groningen in the Netherlands. Experts also question assessments of the global situation. The UN refugee agency (UNHCR), declared in 2015 that the world was \u201cwitnessing the highest levels of displacement on record\u201d. Around 40 million people were \u2018internally displaced\u2019 within their home countries. But researchers say that such estimates are often unreliable. Refugee numbers are easier to track. The UNHCR estimates that there were 21.3 million refugees in 2015; that is only slightly higher than the 1992 figure of 20.6 million, when the global population was just two-thirds of today\u2019s. Researchers also warn about misinterpreting estimates of international migrants \u2014 those who move for economic or other reasons. These numbers can be problematic because the most widely cited UN figures are cumulative. Guy Abel, a statistician at the Vienna Institute of Demography, has studied the dynamic flow and found that the number of people migrating has remained stable over the past 50 years. His latest estimates indicate that migration rate, as a share of global population, has dropped to its lowest point in 50 years."},
{"file_id": "543478a", "url": "https://www.nature.com/articles/543478a", "year": 2017, "authors": [{"name": "Davide Castelvecchi"}], "parsed_as_year": "2006_or_before", "body": "Astronomers hope to grab the first images of an event horizon \u2014 the point of no return. Here's how to catch a black hole. First, spend many years enlisting eight of the top radio observatories across four continents to join forces for an unprecedented hunt. Next, coordinate plans so that those observatories will simultaneously turn their attention to the same patches of sky for several days. Then, collect observations at a scale never before attempted in science \u2014 generating 2 petabytes of data each night. This is the audacious plan for next month\u2019s trial of the Event Horizon Telescope (EHT), a team-up of radio telescopes stationed across the globe to create a virtual observatory nearly as big as Earth. And researchers hope that when they sift through the mountain of data, they will capture the first details ever recorded of the black hole at the centre of the Milky Way, as well as pictures of a much larger one in the more distant galaxy M87. The reason this effort takes so much astronomical firepower is that these black holes are so far from Earth that they should appear about as big as a bagel on the surface of the Moon, requiring a resolution more than 1,000 times better than that of the Hubble Space Telescope. But even if researchers can nab just a few, blurry pixels, that could have a big impact on fundamental physics, astrophysics and cosmology. The EHT aims to close in on each black hole\u2019s event horizon, the surface beyond which gravity is so strong that nothing that crosses it can ever climb back out. By capturing images of what happens outside this zone, scientists will be able to put Einstein\u2019s general theory of relativity to one of its most stringent tests so far. The images could also help to explain how some supermassive black holes produce spectacularly energetic jets and rule over their respective galaxies and beyond. Researchers\u2019 bold effort to capture a black hole. But first, the weather will have to cooperate. The EHT will need crystal-clear skies at all eight locations simultaneously, from Hawaii to the Andes, and from the Pyrenees to the South Pole. These and other constraints mean that the team gets only one two-week window every year to make an attempt. \u201cEverything has to be just right,\u201d says EHT director Sheperd Doeleman, an astrophysicist at Harvard University in Cambridge, Massachusetts. \u201cRadio astronomers relish the challenge of doing the almost impossible,\u201d says Roger Blandford, an astrophysicist at Stanford University in California who is not part of the collaboration. And the EHT could present them with their toughest challenge yet. \n               Monsters of the Universe \n             Astronomers have known since the 1970s that an odd source of radiation lurks in the heart of the Milky Way. Radio telescopes had picked up an unusually compact object in the dusty central region of the Galaxy, within the constellation Sagittarius. They named the object Sagittarius A \u2217  \u2014 Sgr A \u2217  for short \u2014 and eventually gathered compelling evidence that it was a supermassive black hole, with a mass equal to that of about 4 million Suns. The black hole M87 \u2217  in the centre of the galaxy M87 is even larger, at some 6 billion solar masses. In terms of angular size in the sky, these two have the largest known event horizons of any black holes. Although scientists have a pretty good idea of how smaller black holes can form, no one knows for sure how these supermassive monsters develop. And for a long time, astronomers doubted that they could achieve the resolution required to image them in any detail. The challenge comes down to basic optics. The resolution of a telescope depends mostly on its width, or aperture, and on the wavelength of the light at which it is observing. Doubling the width of the telescope allow scientists to resolve details half as wide, and so does halving the wavelength. At wavelengths of 1.3 or 0.87 millimetres \u2014 the only radiation bands that do not get absorbed by the atmosphere or scattered by interstellar dust and hot gas \u2014 calculations suggested that it would take a radio dish much larger than Earth to image Sgr A \u2217  or M87 \u2217 . But in the late 1990s, astrophysicist Heino Falcke, then at the Max Planck Institute for Radio Astronomy in Bonn, Germany, and his collaborators pointed out that the optical distortion caused by a black hole\u2019s gravity would act like a lens, magnifying Sgr A \u2217  by a factor of five or so 1 . That was good news, because it meant that Sgr A \u2217  might be within the reach of very-long-baseline interferometry (VLBI) on Earth. This is a technique that integrates multiple observatories into one virtual telescope \u2014 with an effective aperture as big as the distance between them. The reason that there is any hope of imaging Sgr A \u2217 , and the larger M87 \u2217 , is that they are surrounded by superheated plasma, possibly the residue of stars that did not get swallowed up outright but got torn apart under the intense gravitational stress. The gas forms a rapidly rotating \u2018accretion disk\u2019, with its inner parts slowly spiralling in. Falcke and his colleagues reckoned that a VLBI network spread along the entire globe, and working at around 1 mm wavelength, should be just about sensitive enough to resolve the shadow cast by Sgr A \u2217  against the halo of gas of the accretion disk. The team also made the first simulations of what such a network might see. Contrary to most artistic depictions of black holes, the accretion disk does not disappear behind the object the way Saturn\u2019s rings can partly hide behind the planet. Around a black hole, there\u2019s no hiding: gravity warps space-time, and here the effect is so extreme that  light rays go around the black hole , showing multiple distorted images of what lies behind it. This should make the accretion disk appear to wrap around the black hole\u2019s shadow like a halo. (The 2014 hit  Interstellar  was the first movie to accurately depict this kind of warping of light around a black hole.) But it won\u2019t be a standard halo, of the type seen in many Renaissance paintings. The inner regions of the accretion disk orbit at nearly the speed of light, so one side of the disk \u2014 the side rotating towards the observer \u2014 should look much brighter than the other. The result should be something similar to a crescent Moon (see \u2018Power of the dark\u2019). In 2004, Falcke, who is now at Radboud University in Nijmegen, the Netherlands, was part of a team that made one of the first VLBI observations of Sgr A \u2217 . The US-based network they used, set up by the National Radio Astronomy Observatory, spanned 2,000 kilometres, and took data at 7-mm wavelength 2 . This allowed them to get no more than a blob of light: it was like seeing the black hole through frosted glass. Meanwhile, starting in 2007, a team led by Doeleman made its own VLBI observations of Sgr A \u2217  (ref.  3 ) and M87 \u2217  (ref.  4 ). Using VLBI networks of three observatories, the team made measurements at 1.3 mm, enabling them to close in towards the event horizon. Although the researchers didn\u2019t capture an image of the event horizon, they were able to put upper bounds on its size. Eventually, the two groups joined forces and merged with others to form the current EHT collaboration. And as the team grew, so did the number of telescopes enlisted for the imaging effort. In April, the EHT will have a total of four, or possibly five, nights\u2019 observing time \u2014 a limit set mostly by their use of the state-of-the-art, US$1.4-billion Atacama Large Millimeter Array (ALMA) in Chile, one of the world\u2019s most oversubscribed observatories. They plan to spend two nights on Sgr A \u2217  and two on M87 \u2217 . At each observing station, atomic clocks will tag the arrival time of every crest and trough of every electromagnetic wave to the nearest one-tenth of a nanosecond, explains Feryal \u00d6zel, a theoretical astrophysicist at the University of Arizona in Tucson. In typical interferometry, the arrival times at different locations are compared in real time, and triangulated to their point of origin to reconstruct an image. But with so many observatories scattered around the globe (see \u2018Global effort\u2019), including in places with slow Internet links, the researchers will have to record the streams separately and compare them later. \u201cWe\u2019re not going to have a picture appear before us on the screen,\u201d Daniel Marrone, an astrophysicist at the University of Arizona, says. This means that the EHT will need to record data at a faster rate than any previous experiment of any kind, says Avery Broderick, an astrophysicist at the University of Waterloo in Canada. A typical night will yield about as much data as a year\u2019s worth of experiments at the Large Hadron Collider outside Geneva, Switzerland. The racks of hard disks containing the data will be flown to two central locations, where computer clusters will combine them into one picture, a task that could take up to six months. Only once that phase is completed will the data analysis \u2014 the actual scientific study \u2014 begin. The team will probably not have results to publish until well into 2018. \n               Jet seekers \n             Astrophysicists have high hopes for the results from the EHT. They are particularly interested in data that could help to explain one of the most spectacular phenomena in the cosmos: the giant jets of particles that certain supermassive black holes spew out into intergalactic space at close to the speed of light. Some such black holes, including M87 \u2217 , sport jets even longer than their host galaxies. But not all do: if Sgr A \u2217  has any, they are too small or too feeble to have been spotted yet. Scientists are not even sure what these jets are made of, but they seem to play an outsized role in cosmic evolution. In particular, by heating interstellar matter, jets can prevent that material from cooling down to form stars, thus shutting down galaxy growth, Broderick says. \u201cJets rule the fate of galaxies.\u201d The most likely explanation for the jets, astrophysicists say, is that they are produced by rapidly twisting magnetic fields just outside the black hole, but it is unclear where their energy comes from. In the 1970s, Blandford and his colleagues proposed two alternative models: in one, the energy comes from the accretion disk; in the other it is drawn from the spin of the black hole itself (which is not necessarily aligned with the rotation of the accretion disk). In 2015, Doeleman\u2019s group reported 5  the first hints of structure in the magnetic field around Sgr A \u2217 , using VLBI at 1.3 mm. Their results suggest that black-hole spins are a more likely candidate than accretion disks for fuelling the jets, says Blandford, but the full power of the coming experiments could make that conclusion much more solid, as well as revealing whether Sgr A \u2217  has any jets at all. At a more fundamental level, looking at the size and shape of the event horizon will test Einstein\u2019s theory of gravity for the first time in the extreme regime around a supermassive black hole. This will follow on from the historic discoveries announced last year by LIGO, the Laser Interferometer Gravitational-wave Observatory, which captured the signal of gravitational waves produced by the merger of black holes about as massive as large stars. Its findings were hailed as the most dramatic evidence yet for the existence of black holes, but they have not yet provided incontrovertible evidence. Moreover, supermassive black holes are millions or billions of times larger, Broderick points out. \u201cWhat we\u2019re looking at is a place where we don\u2019t necessarily know how the physics works.\u201d There is even the chance that the EHT will find something different from a black hole in the target areas. Theorists have produced a number of alternative ideas to explain what happens when matter collapses under its own weight. In some of these theories, black holes never form, because gravitational collapse stops before the stellar remnants cross the point of no return. That could result in a super-compact star with a hard surface that might emit radiation detectable by the EHT. But astrophysicist Carlos Barcel\u00f3, of the Institute of Astrophysics of Andalusia in Granada, Spain, says that finding anything like that is a long shot. \u201cI am a bit sceptical that this observation is going be able to distinguish between classical black holes and more exotic kinds of objects.\u201d He and others say that LIGO might have a better chance of testing those models, for example by detecting echoes in the merger of two black holes. As VLBI observations keep improving, however, they might reach the point at which scientists will be able to tell if the event horizon is as symmetrical as general relativity implies, says Alexander Wittig, a mission analyst at the European Space Research and Technology Centre in Noordwijk, the Netherlands. \u201cA future version of the Event Horizon Telescope could reach resolutions that allow us to distinguish more intricate features in the shape of the shadow,\u201d Wittig says. For that goal, Falcke is already dreaming up arrays of space telescopes that could make the EHT even bigger than Earth itself. For now, though, astronomers will gladly settle for a few pixels that will give them their first peek at these elusive behemoths. They have had so many imaginary pictures swirling in their heads, often inspired by science-fiction books and movies like  Interstellar . \u201cTo demonstrate that radio astronomers can catch up with Hollywood and show us pictures of black holes that actually exist,\u201d says Blandford, \u201cthat is a magical idea.\u201d \n                 Tweet \n                 Follow @NatureNews \n                 Follow @dcastelvecchi \n               \n                     LIGO black hole echoes hint at general-relativity breakdown 2016-Dec-09 \n                   \n                     Why galactic black hole fireworks were a flop 2014-Jul-21 \n                   \n                     Quantum bounce could make black holes explode 2014-Jul-17 \n                   \n                     Stephen Hawking: 'There are no black holes' 2014-Jan-24 \n                   \n                     Astrophysics: The heart of darkness 2014-Jan-15 \n                   \n                     Rare star probes supermassive black hole 2013-Aug-14 \n                   \n                     Astrophysics: Fire in the hole! 2013-Apr-03 \n                   \n                     Nature's Books & Arts blog: Imaging and imagining black holes \n                   \n                     ESA Black Hole Visualization \n                   \n                     Imaging black holes \n                   \n                     Black holes and  Interstellar \n                   Reprints and Permissions"},
{"file_id": "543608a", "url": "https://www.nature.com/articles/543608a", "year": 2017, "authors": [{"name": "Heidi Ledford"}], "parsed_as_year": "2006_or_before", "body": "Epigenetic discoveries are fuelling renewed interest in the fusion proteins that have bedevilled cancer biologists. On the day that her son Max was diagnosed with cancer, Ariella Ritvo stormed into the hospital pathology lab and demanded to check the results herself. \u201cI\u2019m not going to leave,\u201d she told a surprised pathologist. \u201cI have a 16-year-old lying there. I want to see it confirmed.\u201d Confronted with a sea of cells dyed blue on the microscope slide, Ariella sized up the enemy she and Max would fight together for the next nine years: a rare childhood cancer called Ewing\u2019s sarcoma. There would be countless rounds of chemotherapy, multiple operations, mice that carried tumours grown from Max\u2019s cancer cells and several experimental drugs, including two cancer vaccines and one compound that had never before been given to a human. In the end, the cancer would still take his life. All because of a scrambled protein. As in most cases of Ewing\u2019s sarcoma, Max\u2019s tumour cells contained two genes that had accidentally joined together into one. The fusion protein it produces, called EWS\u2013FLI1, is a chimaera run amok, altering the expression of thousands of genes. Fusion proteins are a common theme in childhood cancers, from brain tumours to leukaemias. And Max\u2019s struggle with this one highlights the difficulty of tackling them in young patients. The diseases they cause tend to be aggressive, and the intensive chemotherapy treatments used to fight them can be brutal. It is hard to study paediatric cancers in general, both because they are uncommon and because of the ethical concerns involved in experimenting on children. But perhaps most maddeningly, the fusion proteins themselves, the most obvious point of attack for new therapies, have proved to be slippery targets. \u201cThere aren\u2019t too many cancers that just say, \u2018Here is my Achilles heel,\u2019\u201d says Damon Reed, a paediatric oncologist at the Moffitt Cancer Center in Tampa, Florida. \u201cThese are doing that.\u201d Scientists are hopeful, however. In recent years, they have revealed that many fusion proteins, such as EWS\u2013FLI1, interact with some of the cellular machinery that controls gene expression. These epigenetic controls have become a bustling area of research in adult cancers. Therapies that target them are already in clinical trials in adults. And bolstered by advances, a new initiative aims to fund a systematic study of fusion proteins in paediatric cancers. The outlook for the field is brighter than it\u2019s been in a long time, says Stephen Lessnick, a paediatric oncologist at Nationwide Children\u2019s Hospital in Columbus, Ohio, who has studied Ewing\u2019s sarcoma for about a quarter of a century. \u201cThis has become an urgent opportunity,\u201d he says. \n               Under-studied killers \n             The first symptoms of Ewing\u2019s sarcoma are typically unremarkable. For Max, in 2007, it was a recurring backache that was impossible to differentiate from the normal aches and pains experienced by a 16-year-old wrestler with a black belt in karate. When he developed a fever, the family assumed he had the flu. But when his breathing became laboured, Ariella decided it was time to seek help. At the hospital, clinicians drew two litres of fluid from Max\u2019s lungs. A subsequent surgical biopsy led to his devastating diagnosis. Fourteen million people are diagnosed with cancer worldwide each year. Only about 300,000 of them are children or adolescents under the age of 19. The rarity of childhood cancers has made them a relatively low priority for government and industry funders. As a result, the development of therapies for them has tended to lag behind that for adult cancers, says Matthew Meyerson, a cancer geneticist at the Dana-Farber Cancer Institute in Boston, Massachusetts. \u201cAnd that shouldn\u2019t be true,\u201d he says. Fortunately, cure rates are remarkably high for some paediatric cancers: more than eight out of ten children treated for cancer will live for at least five years after their diagnosis. Most are cured. Advances in the treatment of childhood acute leukaemias, for example, are heralded as among the greatest achievements of cancer research. But paediatric cancer treatments can be aggressive. Oncologists give younger patients high doses of toxic drugs that might kill an adult, because young bodies are better at bouncing back from withering treatments. Couple that with the particular desperation that comes when a child is dying, and it\u2019s a recipe for some punishing therapeutic regimens. About 3% of children with cancer die from the treatment itself. \u201cYou go in guns blazing to try to treat these patients,\u201d says Lessnick, who also treated Max. \u201cIt\u2019s a very, very tough disease.\u201d Max\u2019s age put him at greater risk. Adolescents are caught in cancer limbo: their youthful resilience has begun to fade and, possibly as a result, their cure rates tend to be lower. But they are still too young to qualify for adult clinical trials, which are much more abundant than paediatric trials. \n               High hopes \n             Max was still a toddler when, in 1992, a lab in Paris sequenced the protein product of the  EWS\u2013FLI1 gene for the first time 1 . A year later, a team at the University of California, Los Angeles, discovered that it could be a potent trigger for cancer 2 . The work showed how FLI1, a protein that controls gene expression, takes on new properties and becomes a more potent activator of gene expression when a fragment of the protein EWS is added to it. In 1993, when Lessnick entered the field as a PhD student, the mood in his lab was ebullient. A therapy, it seemed, was just around the corner. And because EWS\u2013FLI1 is a molecular oddity found only in cancer cells, a drug that targets it might not be as toxic as conventional chemotherapy. \u201cThere was a clear idea that now we had the driver oncogene, and it shouldn\u2019t be that difficult to figure out how to turn it off,\u201d says Lessnick. \u201cTwenty-five years later, we\u2019re still working on the same issue.\u201d In that span, a small band of dedicated researchers threw everything they could at EWS\u2013FLI1. They sought compounds to interfere with its activity, but the protein was too floppy and unstable to bind a small-molecule drug readily. They tried to shut down EWS\u2013FLI1 expression using techniques such as RNA interference, but could not ensure that the RNA needed to silence the fusion gene would reach all the cancer cells. One cell left active might be all it takes to reseed the tumour, says Lessnick. As attempts to directly target EWS\u2013FLI1 failed, researchers began to sift through the hundreds of genes it regulates, in search of one that was both key to its influence on cancer and vulnerable to attack with a drug. There was a brief flutter of hope for one such target, a protein called IGF-1R. Antibodies against it were tested in clinical trials. But they ultimately disappointed, driving back tumours in only about 10% of patients 3 . Drugs that help only a small proportion of people with an already rare disease hold little commercial interest; the companies involved ended the programme. The field hit wall after wall. \u201cIf someone tells me something works downstream of a pathway affected by EWS\u2013FLI1, I shut down immediately,\u201d says Reed. \u201cWe\u2019ve tried that a million times.\u201d Around 2010, as scientists began trying to improve characterization of tumours by looking at the sequences of their entire genomes, some researchers were hoping that trawling through the genomes of Ewing\u2019s sarcomas would yield other mutations that could be worthy drug targets. Instead, three different teams found the same result: Ewing\u2019s sarcomas always contained a fusion protein, most often but not always EWS\u2013FLI1, and little else 4 , 5 , 6 . There were no other potential drug targets that could reach the majority of patients. But there was a bright side to the negative results. Many adult cancers are riddled with mutations, making it difficult to tell those that drive cancer from those that are merely background changes, with no impact on tumour growth. Tumours can also have multiple triggers, complicating attempts at therapy. Simple cancer genomes, such as the ones that turned up in Ewing\u2019s, may not offer a lot of targets, but with few drivers, they should be less likely to develop resistance to an effective therapy, says Kimberly Stegmaier, a paediatric oncologist at Dana-Farber. \u201cThat\u2019s the hope,\u201d she adds. \n               Blunt instruments \n             But hope does not equal progress. Treatment for Ewing\u2019s sarcoma has changed little since 1993. It generally comprises several rounds of harsh chemotherapy, surgery and radiation. In 2007, Max endured his chemotherapy in three-week cycles. The first week brought vomiting and diarrhoea, and sometimes an intestinal infection. During the second, a different set of drugs often landed him in hospital with severe anaemia. Week three was for recovery, and then the cycle started again. After four rounds of this came surgery, radiation and more drugs. Max braved it all with characteristic good humour, save for one drug: ifosfamide. It caused him to hallucinate and he was unable to put his thoughts into words. He could only repeat two sentences: \u201cMy brain is breaking,\u201d and, \u201cGive me the blue!\u201d The blue was the chemical methylene blue, which counteracts ifosfamide. When it was over, Max swore to his mother that he would never take the drug again. He nevertheless went into remission. Free to resume life as a teenager, he spent the next few years falling in and out of love, nurturing and then overcoming a computer-game addiction and searching for truth in poetry and philosophy. But his time as a cancer patient had left its mark. In his first year at Yale University in New Haven, Connecticut, he became obsessed with the idea that the ifosfamide had affected his memory. This fear pushed him to become a poet, he later said: he used  his writing  as a way to preserve the memories that he feared were breaking down. By his final year of university, his mood had stabilized, he had co\u2011founded a comedy troupe and his poetry had become more than just a sanctuary for his memories. But the one thing he had wanted to forget \u2014 his cancer \u2014 wouldn\u2019t let him. In 2012, his tumours came back. His mother stayed at a hotel near the Yale campus and shuttled Max from treatments to classes so that he could graduate on schedule. Around that time, research into cancer-causing fusion proteins was entering a renaissance. While Max was at Yale, biochemist Cigall Kadoch, then at Stanford University in California, and her colleagues were studying a group of proteins that work together to modify chromatin \u2014 the assemblage of DNA and numerous proteins that help to pack and organize the genetic material in the cell. Chromatin can \u2018open up\u2019, allowing the genes within to be expressed, or it can be tightly wound, preventing gene expression. In 2013, Kadoch\u2019s team had reported 7  that a protein called SS18 is part of a complex involved in the packing and unpacking of chromatin. When SS18 becomes fused to one of several SSX proteins, the resulting chimaera displaces normal SS18 from the complex, disrupting chromatin management. This, Kadoch\u2019s group found, ramps up expression of a cancer-causing gene. The work demonstrated how the fusion protein causes a childhood cancer called synovial sarcoma. The work added to a growing body of evidence showing a link between fusion proteins and epigenetics. Researchers studying leukaemia and Ewing\u2019s sarcoma found similar links (see \u2018A deadly bond\u2019). Kadoch now runs her own lab at Dana-Farber, and routinely gets requests for advice from researchers who study other fusion proteins, seeking help with the biochemical methods she used. The link she found was among the first of many between cancer-causing fusion proteins and chromatin. \u201cMany of the fusion proteins interact with chromatin-modifying complexes to change chromatin in a way that allows gene expression to happen that shouldn\u2019t,\u201d says Scott Armstrong, a paediatric oncologist also at Dana-Farber, who has been exploring epigenetic links in childhood leukaemia. This link, a hot area in epigenetics research, was reviving hopes for targeting recalcitrant fusion proteins. Sequencing the genomes of adult cancers had highlighted the importance of epigenetic processes in driving cancer. Work was already under way in academic and industry labs to target epigenetic proteins in adult cancers. Paediatric oncologists now hope they can co-opt those drugs for children and adolescents. Drugs already in development  that inhibit an epigenetic protein called BRD4, for example, could find uses in treating a range of fusion-protein-driven cancers, including the muscle cancer rhabdomyo\u00adsarcoma and some forms of leukaemia. The approach might also work for EWS\u2013FLI1, which interacts with a protein involved in epigenetic regulation, called LSD1. Lessnick is acting chief medical officer at Salarius Pharmaceuticals, a company in Houston, Texas, that is developing an LSD1 inhibitor and plans to test it in Ewing\u2019s sarcoma. \n               Desperate times \n             Max and Ariella became well versed in all of these projects and more. She runs the Alan B. Slifka Foundation in New York City, a philanthropic organization founded by her late husband that focuses on supporting the Jewish community. When Max got sick, Ariella expanded the foundation\u2019s remit to include sarcoma research. Mother and son had also plugged into a network of families of children with Ewing\u2019s sarcoma. Ariella traded tips, rumours and science with parents as desperate for therapies as she was; some of Max\u2019s closest friends shared his disease. In one way, the recurrence of Max\u2019s cancer was fortunately timed: at 22\u00a0years old, he now had access to clinical trials that were closed to children. Ariella was grateful for this, but the unfairness of it haunted her. \u201cThese kids don\u2019t have time to wait until they turn 18,\u201d she says. \u201cThey\u2019ll die.\u201d In 2012, Max began a fresh chemotherapy cycle: 12 rounds in all. His doctors recommended ifosfamide again, but Max refused. Over the next four years, he tried one experimental treatment after another. His mother asked the US Food and Drug Administration (FDA) for special permission to try an immunotherapy drug that had not yet been tested in children. There was concern that the drug might not work in cancers with few mutations, such as Ewing\u2019s sarcoma, because mutated proteins are thought to stimulate immune responses. In Max, the treatment seemed only to speed the spread of his cancer. In 2015, Ariella and Max sent his cancer cells to a company that used them to seed tumours in mice. The company then tested a battery of drugs on the tumours. The hope was that the mice could serve as stand-ins for Max \u2014 avatars. A drug that successfully battled a tumour growing in one of his avatars might send Max\u2019s tumours into retreat as well. Max, a vegetarian, found it all  fascinating and a bit disturbing : mice bearing a part of him were dying so that he might live. When an experimental drug showed promise in one of these avatars, Ariella rushed to get the FDA to grant permission for Max to take it. But the drug did nothing. Max earned a master\u2019s degree, got married, and began work on his first book of poems \u2014 all while bouncing from one experimental treatment to the next, losing weight as his health deteriorated. By July 2016, the 1.8-metre-tall poet was down to just under 51 kilograms. \n               Hope too late \n             As a child, Max used to climb into bed and watch cartoons with his mother every Monday. It was a ritual so sacred to him that he named the family dog Monday. By mid-August 2016, it was Ariella\u2019s turn to  come to Max\u2019s bed . She and Max\u2019s wife, Victoria, kept close watch, periodically lifting his prone torso to thump on his back, hoping to clear his chest. His rattling breaths gave Ariella nightmares, and she was terrified that Max, although unconscious, might feel as if he were drowning as his lungs filled with fluid. On 23 August, she was holding Max\u2019s hand when his ragged breaths finally stopped. Ariella bathed her son\u2019s body, and then sat down to wait for the mortuary workers to take him away. It was just two weeks later that advisers to the  US Cancer Moonshot \u00a0\u2014 an ambitious plan to accelerate the pace of cancer research\u00a0\u2014 recommended an effort to tackle fusion proteins. More than just an opportunity for this particular avenue of research, paediatric oncologists hope that it is a sign that childhood cancers as a whole may be getting more attention. \u201cWe wanted to focus on something that was a crying need,\u201d says James Downing, president of St. Jude Children\u2019s Research Hospital in Memphis, Tennessee, and one of the researchers who put together the Moonshot recommendations. The proposal, sketched out in brief last September, recommends a pipeline for systematically studying cancer-associated chimaeric proteins. It draws on the biochemical approach that Kadoch and her colleagues used to uncover the link between epigenetics and synovial sarcoma. And it calls for better cell culture and animal models, which have been a particular stumbling block for the field. Lessnick holds monthly conference calls with other researchers who study Ewing\u2019s sarcoma. They share data freely, without fear of competition, he says. \u201cThere\u2019s so few of us and so much work to do.\u201d Even after 25 years of frustration, he remains optimistic \u2014 hopeful that the attention and funding associated with the Moonshot programme could draw in researchers from other fields, such as epigenetics, with a fresh perspective. \u201cBefore, there wasn\u2019t a great way to build those bridges,\u201d he says. During the frantic last years of Max\u2019s life, Ariella had directed the family foundation to focus on late-stage research: therapies close to the clinic, anything that might make it in time to save her son. Now, she intends to refocus its efforts to include more basic research. It\u2019s what the field needs most, she says: \u201cAnd I now have the horrible luxury of time.\u201d \n                     Cancer experts unveil wishlist for US government \u2018moonshot\u2019 2016-Sep-07 \n                   \n                     More support for clinical trials in children 2016-Jul-27 \n                   \n                     Cancer therapy: an evolved approach 2016-Apr-13 \n                   \n                     Cocktails for cancer with a measure of immunotherapy 2016-Apr-13 \n                   \n                     End of cancer-genome project prompts rethink 2015-Jan-05 \n                   \n                     US National Institutes of Health: Ewing\u2019s sarcoma \n                   \n                     US Cancer Moonshot: Fusion Oncoproteins in Childhood Cancers \n                   Reprints and Permissions"},
{"file_id": "543603a", "url": "https://www.nature.com/articles/543603a", "year": 2017, "authors": [{"name": "Traci Watson"}], "parsed_as_year": "2006_or_before", "body": "Ichthyosaurs were some of the largest and most mysterious predators to ever prowl the oceans. Now they are giving up their secrets. Valentin Fischer had always wanted to study fossils, perhaps dinosaurs or extinct mammals. Instead, when he was in graduate school, Fischer ended up sorting through a pile of bones belonging to ancient marine reptiles known as ichthyosaurs \u2014 a group that had been mostly ignored by modern palaeontologists. It was not exactly his dream job. \u201cI said, \u2018Ohhh, ichthyosaurs, so boring\u2019,\u201d recalls Fischer. \u201cThey all look the same. It\u2019s always a pointy snout and big eyes.\u201d Fischer put his feelings aside and dutifully began combing through the fossils stored in a research centre in provincial France. Among the specimens stashed in plastic boxes was an ichthyosaur skull that had been partially destroyed by ants and tree roots while buried underground. When Fischer cleaned up the skull, he realized that it was probably a species new to science. As discoveries started to pile up, he got hooked. Fischer, now at the University of Li\u00e8ge, Belgium, and his colleagues have since described seven surprising new ichthyosaurs, ranging from a tuna-sized reptile with thin, sharp teeth 1  to an animal as big as a killer whale, with a beak like that of a swordfish 2 . Fischer is part of an ichthyosaur renaissance that is sweeping palaeontology. After ignoring them for decades, more and more researchers have started to focus on the reptiles, which were among the top predators in the seas for some 150 million years during the days of the dinosaurs. And that renewed interest has brought a slew of discoveries. In the two centuries leading up to 2000, researchers identified roughly 80 species of ichthyosaur and close relatives. In the past 17 years, they\u2019ve added another 20\u201330, says vertebrate palaeontologist Ryosuke Motani of the University of California, Davis, and the number of papers on the animals has soared. \u201cThere are more people working on ichthyosaurs right now than I have seen for my entire career,\u201d says Judy Massare of the State University of New York College at Brockport, who began studying the animals in the 1980s. The swell of research is starting to answer key questions about ichthyosaurs, such as how and where they originated and how quickly they came to rule the oceans. The group was even more diverse than once thought, ranging from early near-shore creatures that undulated like eels to giants that cruised the open ocean by swishing their powerful tails. \u201cThey could go anywhere, just like whales,\u201d Motani says. The biggest ones rivalled blue whales ( Balaenoptera musculus ) in length and were the largest predators in the Triassic seas. The work is also revealing the last chapters of the ichthyosaur story, which culminated with the animals\u2019 extinction during the Upper Cretaceous, some 30 million years before dinosaurs disappeared. Some scholars now argue that the \u2018fish-lizards\u2019 were vanquished in part by the era\u2019s drastic environmental shifts. That\u2019s a form of redemption for ichthyosaurs; an old theory had suggested that they vanished when more capable predators, such as swift sharks, emerged and eclipsed them. And palaeontologists point to one more reason to focus on ichthyosaurs: because their distant ancestors were land reptiles, the creatures offer a particularly dramatic example of one of the biggest evolutionary makeovers evident in the fossil record, says vertebrate palaeontologist Stephen Brusatte of the University of Edinburgh, UK. \u201cThey totally changed their bodies, biologies and behaviours in order to live in the water.\u201d \n               Fishy start \n             The word \u2018dinosaur\u2019 hadn\u2019t even been coined when a weird skeleton appeared along the southwestern English coast in the early nineteenth century. The bones caught the eyes of celebrated fossil hunter Mary Anning, then no older than 13, and her brother. Their fossil fetched them \u00a323, a substantial sum at the time, and inspired the first scientific paper 3  devoted to ichthyosaurs. The paper, published by British surgeon Everard Home in 1814, erroneously called the animal a \u201cfish \u2026 not of the family of sharks or rays\u201d. Other naturalists soon recognized the fossils as reptiles. The leading lights of natural history marvelled at these creatures. Frenchman Georges Cuvier, considered the father of vertebrate palaeontology, called them \u201cincredible\u201d, and held up ichthyosaurs as support for his theory that catastrophic mass extinctions have plagued Earth. British geologist Charles Lyell, meanwhile, suggested that ichthyosaurs could reappear when Earth\u2019s climate cycled through a favourable phase. Then came the discovery of monstrous land animals, many of which were armed with rows of fierce teeth. This group, named dinosaurs, captivated the public and scientists alike. According to Fischer, they \u201ckicked ichthyosaurs off the pedestal of glory\u201d. The fossils of the marine reptiles piled up unstudied in museums, and their life story was left incomplete. Today\u2019s renaissance in ichthyosaur research is filling in the gaps, especially the fish-lizards\u2019 origins. It took massive anatomical change to mould fully aquatic animals out of land reptiles. Their arms shrank and their hands enlarged, forming seaworthy flippers. They developed the ability to hold their breath for long stretches, even up to 20 minutes. Many evolved huge eyes \u2014 larger than footballs, in one species \u2014 for peering through the dark depths. Researchers suspect that those changes took place not long before or after an apocalyptic mass extinction that wiped out 80% of Earth\u2019s marine species at the end of the Permian period 4 . But until the past few years, they have lacked fossils to illustrate much of that transition. One of the early forms helping to fill in that gap is an animal that Motani calls \u201cthe most bizarre\u201d early ichthyosaur ever seen, which he and his colleagues discovered in a Chinese limestone quarry 5 . It had a head the size of an orange and a torso encased by wide slabs of bone, earning it the scientific name  Sclerocormus parviceps , or \u2018stiff-trunk small-skull\u2019. It dates to 248 million years ago, only 4 million years after the end-Permian extinction. A nearby quarry yielded a snub-nosed fish-lizard of roughly the same age,  Cartorhynchus lenticarpus 6 . About as long as a rainbow trout ( Oncorhynchus mykiss ), this primitive ichthyosaur may have heaved itself around on land atop its big flippers in much the same way that sea turtles do. These early animals weren\u2019t direct ancestors of the fish-shaped ichthyosaurs. But they are still \u201ca big step forward in understanding where ichthyosaurs came from\u201d, says Erin Maxwell, a palaeontologist at the Stuttgart State Museum of Natural History in Germany. The fossils show, for example, that ichthyosaurs originated in what is now the eastern part of south China. At the time, it was one of the few places in the world where land plants flourished. Decaying vegetation would have enriched the nearby seas, Motani says, and eventually, \u201cseafood looked attractive to animals that happened to live near the coastline\u201d. The land-worthy flippers of  Cartorhynchus  led Motani to argue that it had recent ancestors that were terrestrial, or at least amphibious. That would make it a close relative of the land-based or amphibious ancestor of all ichthyosaurs. He interprets the heavy bones of both  Sclerocormus  and  Cartorhynchus  as evidence of a bottom-dwelling lifestyle. Other animals that moved from land to sea also went through a bottom-dwelling phase, Motani says, and with his new finds, \u201cwe have proof that ichthyosaurs went through that heavy stage, and were most likely bottom feeders\u201d \u2014 unlike the later ichthyosaurs, which were creatures of the open ocean. Other researchers agree that the Chinese fossils provide a valuable window into the transformation from landlubber to sea creature. The discoveries are \u201csome of the most interesting reptile fossils that have been found recently\u201d, Brusatte says. \u201cThey are giving us a glimpse of what it actually took to turn a land-living reptile into something semi-aquatic and then something that looked like a fish.\u201d Proper ichthyosaurs, which boast long snouts and kinked tails to distinguish them from their primitive forebears, appeared during the early Triassic and quickly took over their new environment. Discoveries over the past few years have revealed the wide variety of fish-lizards that emerged at the same time as  Cartorhynchus  or soon after. Take the killer-whale-sized  Thalattoarchon saurophagis , or \u2018lizard-eating ruler of the seas\u2019, which was spotted in Nevada 20 years ago but not fully excavated 7  until 2008. Its sharp teeth reveal it as a \u201clarge meat-eater or flesh-tearer\u201d that preyed on fish and other ichthyosaurs, says vertebrate palaeontologist Martin Sander of the University of Bonn in Germany and the Natural History Museum of Los Angeles County, who helped to describe the animal. During its reign in the early Middle Triassic, only 8 million years after the end-Permian extinction, it \u201cbasically ate anything it wanted to\u201d, Sander says. On Norway\u2019s Spitsbergen Island north of the Arctic Circle, researchers have hacked away the permafrost to reveal large, primitive ichthyosaurs that seem to date to the very early Triassic. The finds have yet to be identified, but they help to show that \u201conce these things hit the water, they just went nuts\u201d, says Patrick Druckenmiller, a palaeontologist at the University of Alaska Fairbanks and part of the Spitsbergen team. The presence of large predators at this time suggests that creatures of all sizes and lifestyles restocked the oceans after the devastation of the end-Permian extinction. After their initial burst of evolution, ichthyosaurs went through some tough times. In the later Triassic, many species died out during one or more mass extinctions that also claimed large fractions of species on land and in the oceans. After this, the ichthyosaur\u2019s story gets complicated. Palaeontologists had long thought that the group was hit by a major loss of biodiversity in the Jurassic and never really recovered. The fossil record suggested that only a handful of species, all similar in appearance and lifestyle, limped across the Jurassic\u2013Cretaceous boundary 145 million years ago. Then the whole group went extinct midway through the Cretaceous, while dinosaurs thrived for another 30 million years or so, until an asteroid strike wiped them out. The lack of diversity among ichthyosaur species could have hampered their ability to compete against sharks and other emerging predators in the seas. But finds in the past few years have called that whole story into question. New fossil discoveries show that many more species thrived during the Cretaceous than previously recognized. These species also had more diverse body types and food sources than researchers thought. Ichthyosaurs\u2019 reputation has risen in good part because of Fischer\u2019s work \u2014 not at sweaty dig sites but in hushed museums. \u201cI am very bad at finding fossils in the field,\u201d Fischer confesses. \u201cThe only ichthyosaur I\u2019ve found, ever, is a single vertebra.\u201d But the hundreds of museum specimens that he has scrutinized \u2014 some of them left unexamined for a century \u2014 have yielded a bounty of Cretaceous novelties. In just over a decade, Fischer\u2019s team and other groups have reported at least nine new species from this period. Fischer and his colleagues estimate that the number of known ichthyosaur species was just as high during parts of the early Cretaceous as during spans of the Jurassic. It turns out that the fish-shaped reptiles called parvipelvians, the only group of ichthyosaurs to endure from the Triassic into the Cretaceous, had a greater range of shapes during the middle of the early Cretaceous than at any other time in their history 8 . \u201cThe diversity of ichthyosaurs in the Cretaceous, in particular, is even higher than we ever thought it was\u201d, says Fischer\u2019s co-author Darren Naish, a vertebrate palaeontologist at the University of Southampton, UK. He calls it a \u201cCretaceous ichthyosaur revolution\u201d. Then, according to Fischer\u2019s analysis, a one-two punch hit the ichthyosaurs. A good number of species went extinct roughly 100\u00a0million years ago, and the few survivors followed their relatives some 5 million to 6 million years later. To understand why, Fischer looked to environmental factors. He noticed a correlation between climate and extinctions: the greater the temperature fluctuation in a given period, the more species of ichthyosaurs winked out. Other scientists agree that climate disruptions could have played a big part in the animals\u2019 disappearance. Climate volatility \u201cis a much better hypothesis than any proposed so far. It matches what we know about extinction risk in large predators today,\u201d Maxwell says. The mid-Cretaceous was a difficult time in the oceans. Ichthyosaurs died out when sea levels were high and marine oxygen levels were low. Many other ocean groups, such as ammonites, were going downhill fast during the same period. Ichthyosaurs, then, might be \u201cjust a small facet of something more important\u201d, Fischer says. He\u2019s now looking at whether other marine predators during the Cretaceous followed the ichthyosaur pattern. Fischer\u2019s results are not universally accepted. Motani, for example, thinks that Fischer\u2019s scenario is plausible, but he takes issue with the statistical methods that Fischer uses to date ichthyosaurs\u2019 extinction. Fischer places the reptiles\u2019 disappearance close to 94 million years ago, but if the creatures went extinct at a different time, the correlation between their fate and climate volatility would not be as strong. Fischer stands by his conclusions, however, and says that the fossil record for marine reptiles actually improves during the Cretaceous, which adds confidence to conclusions about ichthyosaurs during that period. The debate over ichthyosaurs\u2019 final days will continue to rage, as scientists strive to understand the forces behind the mysterious extinction of a successful group. Researchers also hope to understand what befell the ichthyosaurs that died out at the end of the Triassic. More fossils would help. So would techniques that have already been deployed to better date the sediments containing ichthyosaurs, enabling researchers to pinpoint species\u2019 histories with higher precision. The new debates and competition characterizing the field do not perturb Motani. He has no yearning for the lonely days when he was one of so few ichthyosaur specialists that journals struggled to peer-review his manuscripts. On the contrary, he is glad that more scientists are pursuing the animals he regards as \u201cbeautiful, and beautifully adapted\u201d. To Motani and other partisans, ichthyosaurs are finally getting the attention they\u2019ve long deserved. \n                     Swimming dinosaur found in Morocco 2014-Sep-11 \n                   \n                     Kraken versus ichthyosaur: let battle commence 2011-Oct-11 \n                   \n                     Did reptile swimmer show mother love? 2011-Aug-11 \n                   \n                     Ichthyosaurs ate turtle soup 2003-Jul-23 \n                   \n                     Ryosuke Motani\u2019s ichthyosaur page \n                   Reprints and Permissions"},
{"file_id": "542020a", "url": "https://www.nature.com/articles/542020a", "year": 2017, "authors": [{"name": "Gabriel Popkin"}], "parsed_as_year": "2006_or_before", "body": "A wild plan is taking shape to visit the nearest planet outside our Solar System. Here\u2019s how we could get to Proxima b. Anybody who longs to see an alien world up close got an exciting gift last year. In August, researchers reported the discovery of a potentially habitable, Earth-sized planet orbiting the Sun\u2019s closest stellar neighbour \u2014  Proxima Centauri , a mere 1.3 parsecs, or 4.22 light years, away. It\u2019s a tempting \u2014 some might say irresistible \u2014 destination. Sending a spacecraft to the planet, dubbed Proxima b, would give humans their first view of a world outside the Solar System. \u201cClearly it would be a huge step forward for humanity if we could reach out to the nearest star system,\u201d says Bruce Betts, director of science and technology for the Planetary Society in Pasadena, California. The data beamed back could reveal whether the alien world offers the right conditions for life \u2014 and maybe even  whether anything inhabits it . The idea of reaching Proxima b is not just science fiction. In fact, a few months before the discovery of the exoplanet, a group of business leaders and scientists took the first steps towards visiting the Alpha Centauri star system, thought to be home to Proxima. They announced  Breakthrough Starshot , an  effort backed by US$100 million from Russian investor Yuri Milner  to vastly accelerate research and development of a space probe that could make the trip. When Proxima b was found (G. Anglada-Escud\u00e9  et al .  Nature   536 , 437\u2013440; 2016 ), the project gained an even more tantalizing target. Getting there won\u2019t be easy. Despite Proxima b\u2019s name, it is still nearly 2,000 times farther from Earth than any human-made object has ever travelled. To reach it within a scientist\u2019s working lifetime, a probe would have to reach around one-fifth the speed of light and navigate a treacherous path through unseen debris in our own Solar System and interstellar space. Then it would need to collect useful data during a 60,000-kilometre-per-second fly-by of the Proxima system, and beam the information back across the 4 light years to Earth. It all amounts to a monstrous engineering challenge, but project researchers say it is possible and are now moving towards that goal. Other groups are also aiming for nearby stars, but none has the momentum \u2014 or money \u2014 of Breakthrough Starshot. And even astrophysicists who are not involved with Starshot agree that it has the most realistic chance at an interstellar mission in the next few decades, thanks in part to scientists who have published many concept papers on interstellar travel. \u201cStarshot takes the best bits out of all of that, and puts them together into something new,\u201d says Caleb Scharf, an astrophysicist at Columbia University in New York City who is not on the Starshot team. Leaders of the mission plan to start funding technology-development projects within months, with the aim of launching a fleet of tiny, laser-propelled probes in the next 20 years. The effort would ultimately cost about $10 billion, leaders hope, and take another 20 years to reach Alpha Centauri. \n               The launch \n             The first truly challenging step in any mission such as Breakthrough Starshot is to accelerate the spacecraft to interstellar velocities. Conventional rockets are out of the question because they can\u2019t store enough chemical energy in the form of fuel, says Philip Lubin, an astrophysicist at the University of California, Santa Barbara, who is on the project\u2019s advisory and management committee. \u201cChemistry will get you to Mars,\u201d he says, \u201cbut it won\u2019t get you to the stars.\u201d So Starshot is focusing on harnessing light. Since the turn of the twentieth century, scientists have known that light carries momentum and can give objects a push. Researchers at the Japan Aerospace Exploration Agency (JAXA) and the Planetary Society have demonstrated this in space by launching large sails propelled by sunlight. But the Sun\u2019s light isn\u2019t powerful enough to accelerate a ship to Alpha Centauri; that would require an enormous, unwieldy sail, says Betts, who led a team that in 2015  deployed a 32-square-metre solar sail . Starshot evaluated more than 20 ideas for propulsion beyond the Solar System, but \u201cvirtually all\u201d seemed out of reach, says Pete Worden, the project\u2019s executive director. They settled on one that Lubin had proposed, involving lasers. In 2015, Lubin produced a conceptual road map for getting a spacecraft to Alpha Centauri in 20 years ( P.\u00a0Lubin  J. Br. Interplanet. Soc.   69,  40\u201372; 2016 ). He suggested using an array of lasers on Earth to generate a beam powerful enough to propel a small light sail. The Starshot team plans to use conventional rockets to send its probes into orbit. Then a 100-gigawatt laser array on Earth would fire continuously at the sail for several minutes, long enough to accelerate it to 60,000 kilometres per second (see \u2018Are we there yet?\u2019). Starshot leaders acknowledge that they are counting on breakthroughs from the laser industry. One hundred gigawatts will be a million times more powerful than today\u2019s biggest continuous lasers, which put out hundreds of kilowatts. One way around that gap would be to combine light from hundreds of millions of less powerful laser beams across an array that is at least a kilometre wide. But the beams would all need to be brought into phase with each other so that their light waves add rather than cancel each other out \u2014 making the lasers one of the mission technologies that requires the most development work. \n               The craft \n             The Starshot craft will look like nothing ever launched into space. Imagine a small collection of electronics, sensors, thrusters, cameras and a battery on a roughly one-centimetre-wide chip in the centre of a circular or square sail, roughly 4 metres wide \u2014 all weighing just a gram. The lighter the craft, the faster a given force can accelerate it. To maximize speed and minimize damage from the lasers, the sail needs to reflect almost all the incoming light, although it could let some pass through. Suitable materials already exist in the form of thin layers of electrical insulators that can reflect up to 99.999% of incoming light, close to the threshold needed. But researchers will need to increase production of the exotic materials, and lower their cost. And they need to study how the materials will respond to the intense light levels required, which could produce unpredicted optical effects. We imagine the timeline of an interstellar mission like Starshot. In the acceleration phase, the sail will need to stay extremely flat and actively sense and compensate for imperfections in the laser beam, so that the craft will stay on course; even a slight deviation early on could send it on a wildly different trajectory. One option for doing this is to set the sail spinning, creating a centrifugal force that would pull it taut and allowing beam irregularities to average out over the sail area. JAXA has already demonstrated a  spinning solar sail , and the concept \u201clooks extremely promising\u201d for Starshot, says Worden. Whatever the design, the sail must be strong. A 100-gigawatt laser beam will hit it hard, generating tens of thousands of times the acceleration that an object feels on Earth owing to gravity. Artillery shells have survived such forces in military tests, Worden notes, but for less than a second \u2014 not the several minutes for which the laser will pound the device. Starshot\u2019s plan would build strength in numbers. The spacecraft would be small and relatively low-cost, so the project could launch one or more every day, and even afford to lose some of them. Development of the probes will proceed in stages, says Worden. The first step is to build a prototype system that would accelerate to perhaps 1,000 kilometres per second \u2014 less than 2% of the speed planned for Starshot \u2014 for a total cost between $500 million and $1 billion. \n               The journey \n             The lasers will shut down after several minutes, once the probe has reached one-fifth of the speed of light and travelled a couple of million kilometres \u2014 about five times the distance from Earth to the Moon. The next 20 years will, ideally, be boring. The biggest risk at this stage is serious damage from collisions with dust specks, hydrogen atoms and other particles in the interstellar medium. Added danger comes in the form of cosmic rays \u2014 atomic nuclei that zip through space at close to the speed of light and could degrade crucial electronics. No one knows exactly how many particles fill interstellar space, or how big they are, but Starshot plans to protect its craft from potential collisions by covering the leading edge with at least a millimetre-thick coating of a material such as beryllium copper. Even if it doesn\u2019t destroy the craft, a strike could send the probe hurtling off-course. The probe will therefore need its own navigation and steering systems, powered by a lightweight generator that uses a radioactive isotope such as plutonium-238 \u2014 essentially a nuclear battery. These systems will need to include rudimentary artificial intelligence that monitors the position of stars and adjusts course by firing photon thrusters. \u201cThe way I put it to people is, you really want Neil Armstrong or Chuck Yeager on a chip, to make all of those critical decisions in real time,\u201d says Scharf. Mission designers will not be able to eliminate all risks, especially from as-yet unknown objects in the interstellar medium. That\u2019s why they are considering launching exploratory probes as soon as a prototype propulsion system is built. Those early craft could sample the interstellar medium and report back, to fill in gaps in astronomers\u2019 understanding of this environment. \n               The fly-by \n             Some time around 2060, if all goes as planned, the Starshot craft\u2019s onboard computer will wake up, ping Earth for a periodic status check, detect that it is approaching Proxima Centauri and get ready for its fly-by. The highest priority, experts agree, will be to take a photo. Lubin estimates that the craft should be able to get within one astronomical unit \u2014 the distance from Earth to the Sun \u2014 of Proxima b. Even from that distance, a photo could reveal whether the planet is watery and green like ours, or barren like Mars. It could also capture large-scale features such as mountains and craters. An on-board spectrometer could probe the make-up of the planet\u2019s atmosphere, if it has one. Researchers will be on the lookout for molecules such as oxygen, methane and more complex hydrocarbons, which are  possible signatures of life . Instruments might also attempt to measure the planet\u2019s magnetic field or other variables that could reveal whether Proxima b has a life-nurturing environment or a much harsher one. When the craft gets to Proxima Centauri, there will be no way to slow it down, so it will whip through the star system in about two hours. This will create challenges for the design of its measuring instruments. No photo has ever been taken by a camera moving at one-fifth the speed of light. The craft\u2019s cameras will have to swivel to keep the planet in view, and Earth-based computers will have to correct images for distortions caused by the effects of relativity and the camera\u2019s changing angle and distance from the planet. Then will come one of Starshot\u2019s toughest challenges, one that leaders admit they have not yet solved: how to transmit data from Proxima back home to eager astronomers using a roughly 1-watt laser beam while still making the signal strong enough to be detectable on Earth after the 4.22-year journey. Lubin envisions building a kilometre-wide array of detectors on Earth, possibly occupying the same area as the acceleration lasers, to capture the craft\u2019s faint transmission. The onboard nuclear battery will power capacitors that will make the beam as bright as possible, similar to a camera flash. And it may be possible to use the sail as an antenna to boost the signal. But the light beam will still be a tenuous wisp amid the vast darkness of space. An alternative approach could be to launch a succession of craft to serve as relays, so that each chip\u2019s signal would only have to travel perhaps one-tenth of a parsec (0.2 light years), instead of the full distance. But such a scheme would create further complications, Lubin and others note. \n               A new capability \n             Experts not involved with the project express a mix of tempered optimism and scepticism. \u201cI think there are tremendous challenges\u201d in scaling up laser power and other needed technologies, says Gregory Quarles, chief scientist at the Optical Society in Washington DC. But he adds that with the right level of private and public funding for research into optics and materials, \u201cthere will be returns on that investment\u201d. Some say that Starshot\u2019s minimalist approach sets the mission apart from previous, less plausible proposals. \u201cThere\u2019s nothing I can see that\u2019s obviously totally impossible,\u201d says Scharf. \u201cThey\u2019re not talking about a large ship to another star.\u201d Others, however, worry that the multiple technological hurdles may prove overwhelming. \u201cI\u2019m cautious about the near-term future of this,\u201d says Betts. \u201cAny one piece seems surmountable, until you realize you have to cram it into a little, tiny, low-mass object.\u201d Even if Starshot gets to Proxima b, Andreas Tziolas, president of the space-exploration organization Icarus Interstellar, thinks it is unlikely to provide useful data. \u201cIt has an extremely slim to non-existent chance of succeeding in sending an image back from Alpha Centauri,\u201d he says. \u201cYou can\u2019t have that small a spacecraft carry enough power to transmit back a signal.\u201d Although his organization also studies laser propulsion, he is focusing on a nuclear-fusion-powered mission that could send a much larger spacecraft to Alpha Centauri in a century \u2014 something he says would be powerful enough to beam back useful data and maybe even transport robotic rovers. Before any craft gets off the ground, astronomers could learn a lot about Proxima b without sending anything beyond our immediate neighbourhood. The James Webb Space Telescope is scheduled to launch in late 2018, and  several giant Earth-based telescopes are likely to come online in the next decade . Using these, astronomers might be able to determine whether the exoplanet\u2019s atmosphere contains signatures of life. But as any explorer would say, there is no substitute for going to a new place. The 2015 fly-by of Pluto, for instance,  revealed ice mountains and nitrogen glaciers  that Earth\u2019s most powerful telescopes were blind to. Similarly, Proxima b \u2014 and every other nearby exoplanet \u2014 may hold surprises that will be visible only in a close encounter. There will also be broader pay-offs, say mission proponents. \u201cI see Starshot as about the development of capability,\u201d says Kelvin Long, director of the London-based Initiative for Interstellar Studies and a member of the project\u2019s advisory committee. \u201cIt\u2019s like going to the Moon.\u201d A laser array with the power to push spacecraft to Proxima Centauri could send probes anywhere in the Solar System in a few days, or to the interstellar medium within a week or two, he says. That kind of capability could make Solar System exploration routine. \u201cHow would you like to deliver next-day Amazon to Mars?\u201d says Lubin. \u201cThis is a radical transformation of how we might be able to explore.\u201d \n                 Tweet \n                 Follow @NatureNews \n               \n                     Nature\u2019s 10 2016-Dec-19 \n                   \n                     Earth-sized planet around nearby star is astronomy dream come true 2016-Aug-24 \n                   \n                     Billionaire backs plan to send pint-sized starships beyond the Solar System 2016-Apr-13 \n                   \n                     On the hunt for a mystery planet 2016-Mar-15 \n                   \n                     The truth about exoplanets 2016-Feb-17 \n                   \n                     The exoplanet files 2015-Nov-18 \n                   \n                     NASA spies Earth-sized exoplanet orbiting Sun-like star 2015-Jul-23 \n                   \n                     Exoplanet bounty includes most Earth-like worlds yet 2015-Jan-06 \n                   \n                     Solar sail to hitch free ride on light breeze in 2016 2014-Jul-17 \n                   \n                     Nature 's astronomical highlights: Exoplanets \n                   \n                     Breakthrough Starshot \n                   \n                     Planetary Society \n                   \n                     Icarus Interstellar \n                   \n                     Initiative for Interstellar Studies \n                   Reprints and Permissions"},
{"file_id": "541450a", "url": "https://www.nature.com/articles/541450a", "year": 2017, "authors": [{"name": "Megan Scudellari"}], "parsed_as_year": "2006_or_before", "body": "Moonshots, road maps, frameworks and more are proliferating, but few can agree on what these names even mean. On 12 January 2016, nearly half a century after humans first set foot on the lunar surface, a president and a billionaire each announced a new moonshot. In Washington DC, US President Barack Obama described the establishment of a government-led Cancer Moonshot to accelerate oncology research. Across the country at a press conference in California, billionaire entrepreneur Patrick Soon-Shiong announced Cancer MoonShot 2020, an industry\u2013academia collaboration. If the name sounded familiar at the time, that might be because the University of Texas\u2019s MD Anderson Cancer Center had established its own Moon Shots Program back in September 2012. (It has since filed a lawsuit alleging trademark infringement by Soon-Shiong and his businesses.) Shooting for the metaphorical moon, it turns out, is a popular pastime in science-planning circles. \u201cI\u2019ve been in Washington for a very long time, and I feel like every time I turn around, somebody says we need a new moonshot,\u201d says Kathy Hudson, who has served as deputy director for science, outreach and policy at the US National Institutes of Health (NIH) in Bethesda, Maryland. \u2018Moonshot\u2019, \u2018road map\u2019, \u2018initiative\u2019 and other science-planning buzzwords have meaning, yet even some of the people who choose these terms have trouble defining them precisely. The terms might seem interchangeable, but close examination reveals a subtle hierarchy in their intentions and goals. Moonshots, for example, focus on achievable, but lofty, engineering problems. Road maps and decadal surveys (see \u2018Alternate aliases\u2019) lay out milestones and timelines or set priorities for a field. That said, many planning projects masquerade as one title while acting as another. Strategic plans that bear these lofty names often tout big price tags and encourage collaborative undertakings. In the United States, for instance, science-funding agencies are hoping to start or continue many\u2018big science\u2019 efforts in 2017. The National Science Foundation has requested US$74\u00a0million for the Obama administration\u2019s BRAIN (Brain Research through Advancing Innovation and Neurotechnologies) Initiative and $33\u00a0million to the National Strategic Computing Initiative. The NIH hopes to continue funding the BRAIN Initiative, as well as the Precision Medicine Initiative and possibly the Cancer Moonshot. And the Department of Energy\u2019s Office of Science requested $818\u00a0million for high-energy-physics projects, including the Large Hadron Collider, and $398\u00a0million for continued contribution to the large, international ITER fusion project. NASA, of course, takes the cake with literal moonshots, including a $1.5-billion planetary exploration proposal involving a mission to Jupiter\u2019s moon Europa and continued missions to Mars. The value of such projects is continually debated. On one hand, many argue that the coalescence of resources, organization and long-term goals that comes with large programmes is crucial to science advancement in an era of increasing data and complexity. \u201cThere are some problems that demand marshalling resources,\u201d says Thom Mason, director of the Oak Ridge National Laboratory in Tennessee, which hosts numerous shared research facilities and participates in ITER. \u201cOftentimes, those big initiatives are really defining for the whole field.\u201d\u00a0 Big thinking and big actions have often led to success. But critics argue that buzzword projects add unnecessary layers of bureaucracy and overhead costs to doing science, reduce creativity and funding stability and often lack the basic science necessary to succeed. Science, by its nature, needs stability, says Arturo Casadevall, a microbiologist at the Johns Hopkins Bloomberg School of Public Health in Baltimore, Maryland, and editor-in-chief of  mBio . And big projects, especially if they are politically driven, can threaten that. \u201cWe all worry that because of need or political expediency, sometimes a lot of money is spent on a specific area, and a field expands disproportionately to what it can handle, and then the money is pulled out.\u201d To help understand the ecology of science-planning efforts, here is a field guide to the various terms that thrive there (see \u2018What to call your science mega-project\u2019). \n               Moonshot \n             n.  /\u02c8mu\u02d0n\u0283\u0252t/ The launching of a spacecraft to the moon That\u2019s the most literal definition according to the  Oxford English Dictionary , and \u2018moonshot\u2019 is often attributed to NASA\u2019s Apollo programme, which first landed people on the Moon in 1969. There is evidence, however, that it was used figuratively to refer to a lofty, but not impossible, goal way back in 1891, when a Minnesota newspaper column referred to keeping within \u201cmoon shot\u201d of demand for housing. Others trace the concept of shooting for the Moon to Jules Verne\u2019s 1865 science-fiction novel,  From the Earth to the Moon . What is clear, however, is that the moonshot has become one of the most inspiring things in US science. Today, scientists use the term to describe ambitious engineering projects. \u201cThe Human Genome Project was like the NASA moonshot, because there was a very specific goal \u2014\u00a0to sequence the genome\u00a0\u2014\u00a0and because the technology to do it did not exist when the project began,\u201d says biologist Barbara Wold at the California Institute of Technology in Pasadena, who served as an adviser for the project. The Manhattan Project to develop the first nuclear weapons has also been compared to the NASA moonshot. But, says Wold, the description isn\u2019t appropriate \u201cfor large-scale efforts that simply apply existing technologies to programmes with diffuse or unbounded goals\u201d. Apollo 11, the Manhattan Project and the Human Genome Project were successful because they had national interest and politicians supporting them, plus a solid grasp on the basic science needed to achieve those goals, be it Newtonian physics or nuclear fission. But some worry that contemporary cancer moonshot projects tend to lack the basic-science component. \u201cLarge research projects succeed once you have done the basic science,\u201d says Casadevall. \u201cThe question with the cancer moonshot is whether the basic science is there.\u201d Elizabeth Jaffee, director of the Sidney Kimmel Comprehensive Cancer Center at Johns Hopkins University School of Medicine says that the science is \u201cripe\u201d for the $1-billion Cancer Moonshot announced by Obama in 2016. \u201cWe\u2019ve made a lot of progress in the basic sciences and cancer biology and technologies, so now we need to transform these discoveries into rational therapies to not only treat, but intercept, cancer.\u201d \n               boxed-text \n             Last September, the Cancer Moonshot\u2019s 28-member Blue Ribbon Panel, which Jaffe co-chaired, identified ten research opportunities that are ready to move forward in the next five years \u2014 areas in which a concentrated investment could make a difference. Those opportunities include projects that promote early screening for families at genetic risk of colon cancer, and creating an online network of databases of patient tumour profiles\u00a0\u2014\u00a0goals that are well within reach. \u201cIt\u2019s not rocket science,\u201d she says. Nevertheless, the terminology used to describe the project could be misconstrued, says Jarle Breivik, who studies cancer and science communication at the University of Oslo. And that might mislead the public. \u201cI\u2019m concerned about this belief that if we just put enough money into cancer research and chemotherapy, then we will get rid of the problem,\u201d he says. The public can be forgiven for thinking that the \u2018moon\u2019 in the Cancer Moonshot is a cure. In his address last January, Obama said: \u201cLet\u2019s make America the country that cures cancer once and for all.\u201d In reality, the government\u2019s goal is to complete a decade\u2019s worth of progress in five years \u2014 not to cure the disease. \u201cIn retrospect, \u2018moonshot\u2019 was probably not the best name for it,\u201d admits Jaffe. A better, though less sexy, name might have been the Cancer Road Map. \n               Road map \n             n.  /\u02c8r\u0259\u028ad \u02ccmap/ A map that shows the roads of a country or area Europeans love \u2018road maps\u2019. There\u2019s the European Roadmap for Astroparticle Physics \u2014 published in 2008 and updated three times since. Then there\u2019s the European Astrobiology Roadmap and the European Hematology Association\u2019s Roadmap for European Hematology Research \u2014 a beast of a document that was compiled over 2 years by 300 contributors, and is broken into 9 sections spanning 60\u00a0disease groups. The European Commission, the executive arm of the European Union, has published 384 road maps for various government programmes since 2015 alone. Road maps are strategic plans or schedules that propose short- and long-term milestones. The 1998 International Technology Roadmap for Semiconductors, for example, transformed Moore\u2019s law from an observation about the speed of development of integrated circuits into an i ndustry-wide research and development plan for how to improve microprocessors . Road maps came into fashion in Europe in the early 2000s, after they were used widely by the US Department of Energy and by NASA, says Stefano Fontana, who works at the European Institute of Innovation and Technology in Budapest. One of the US efforts that caught attention in Europe was NASA\u2019s 2004 Vision for Space Exploration, which laid out an agenda encompassing returning to the Moon, completing the International Space Station and continuing robotic exploration of Mars. Fontana included NASA\u2019s road map in his 2007 round-up of 16 such efforts \u2014 a road map of road maps, if you will, for large-scale research infrastructures 1 . The analysis led to a large European report on road maps 2 , which encouraged strategic planning so much that more than 20\u00a0countries have started their own road mapping, says Fontana. Road maps are particularly suited to Europe because they can organize collaborations between the European Union\u2019s 28 member countries and help to avoid overlap. They are \u201chow we deal with complexity\u201d, says Fontana. In his analysis, he uncovered four aspects that are common to successful strategic plans: they are long-term, include funding, have built-in mechanisms for quality assessment and include competition, such as a marketplace where anyone can participate. The smallest scale at which a road map can be meaningfully implemented is that of a national funding agency, Fontana concluded, such as NASA or the Department of Energy. More commonly, however, road maps are created at the national level. The Cancer Moonshot Blue Ribbon Panel\u2019s plan, for example, is really a road map for the field compiled by representatives from academia, the private sector and government, and funded by four federal agencies. At the larger end of the scale, road maps can span several nations, as is often the case in Europe. Many of them are written as a document to provide pragmatic guidance for \u2014 drum roll, please \u2014 an initiative. \n               Initiative \n             n.  /\u026a\u02c8n\u026a\u0283\u026a\u0259t\u026av/ That which initiates, begins, or originates; the first step in some process or enterprise Initiatives are in vogue in the United States right now, with two mega-projects bearing that designation: Obama\u2019s 2013 BRAIN Initiative, and his 2015 Precision Medicine Initiative (PMI). The BRAIN Initiative \u2014 a proposed $4.5-billion, 12-year effort involving almost every research arm in the government \u2014 has already awarded $150 million in neuroscience research grants. The PMI launched with a $215-million investment, including $130\u00a0million for a centrepiece programme that aims to enrol and collect health data from 1\u00a0million Americans over a long period of time. The PMI is like a large project dressed up as an initiative, but it has also been called a moonshot. Hudson, who co-chairs the PMI\u2019s Working Group, says that the two initiatives were named in the same office, but they have more in common than that. Both coordinate large entities \u2014 from the National Cancer Institute to the Food and Drug Administration and the Department of Defense \u2014 to progress towards a common goal. Plus, they are creating large, technology-based research platforms to be applied across many types of disease \u2014 something that would not otherwise be done, says Hudson. Rather than disease-specific planning towards a single goal, as we\u2019ve seen in moonshots and road maps, she says, the two initiatives focus on \u201cresources that are going to have a broad, disease-agnostic impact\u201d \u2014 resources that require coordination, planning and governance. But when these kinds of large efforts come to an end, the technology hubs created for them face the challenge of continuing to be useful and can be perceived as a drain on resources. At the conclusion of the Human Genome Project, for example, Francis Collins, then at the National Human Genome Research Institute, proposed a series of grand challenges for genomics researchers, including the creation of the HapMap study to identify the most common genetic variations between people and the ENCODE project to catalogue all of the genome\u2019s functional DNA sequences, in an attempt to put the technology resources developed during the project back to work 3 . But gathering data and knowledge is not enough to make a scientific initiative successful, argues policy analyst Daniel Sarewitz of Arizona State University in Tempe (a regular columnist for  Nature ). Last year in an essay for  The New Atlantis , Sarewitz criticized the BRAIN Initiative and the PMI as being part of a \u201cdatageddon\u201d, in which big-data projects try to tackle complex problems with massive data sets, creating an almost infinite number of possible hypotheses to test within that system 4 . Those data sets therefore generate results that look meaningful but have no real application in medicine, Sarewitz says. In a complex system such as the brain, \u201cthe science can look like it\u2019s making great progress when in fact it\u2019s just adding to noise\u201d. Those concerns remain when people consider big projects today. \u201cThere is a real responsibility that comes with resources on this scale,\u201d says Mason. \u201cYou\u2019ve got to deliver a benefit to society that\u2019s commensurate with that investment.\u201d \n               Framework \n             n.  /\u02c8fre\u026amw\u0259\u02d0k/ an essential or underlying structure; a provisional design, an outline; a conceptual scheme or system Frameworks provide a plan for a field or area. As well as helping an organization or nation to plan for the future, they serve as a way to coordinate and communicate ideas about policy and science to the public, says Tracy Merlin, managing director of the Adelaide Health Technology Assessment, a research unit that evaluates health interventions for the Australian Department of Health. Australia is big on frameworks. In 2009, the government announced a Climate Change Science National Framework to identify \u201cnational climate-change science priorities for the coming decade\u201d. It backed up that proposal with a Aus$31.2-million (US$23.5-million) funding boost over four years. The country also has government frameworks for safety and quality of health care, chronic diseases, mental-health services, cosmetic medical procedures, postnatal depression and more. \u201cIt\u2019s easy if you\u2019ve got a name for a collection of activities targeting the same thing, so you call it a \u2018framework\u2019 and it gains prominence,\u201d says Merlin, who in 2013 co-authored a national framework for reviewing personalized medicines 5 . \u201cIt\u2019s their way of describing what they\u2019re trying to do\u201d plus \u201ca bit of marketing\u201d, she adds. Used in the right way, a framework or outline not only assists policymakers and the public \u2014 it can also help individual scientists to understand and explain how their work fits into a bigger picture, says Mason. \u201cWe can think, \u2018If I can solve this problem that\u2019s keeping me up late tonight, it\u2019s another contribution in a larger enterprise that is going to lead to cures for disease or elimination of poverty or the creation of energy for those who need it\u2019.\u201d This kind of big-picture thinking \u2014 whether through frameworks, moonshots or other grand designs \u2014 helps to focus attention. There will always be more good ideas than there is enough money, says Mason, so working together as a community to decide on priorities can be crucial to a field\u2019s success. \u201cScience partly progresses by serendipity, but it also progresses by the technical capabilities that are developed through going to try to do big, difficult things,\u201d he says. \u201cIf you don\u2019t have those big problems that you\u2019re trying to solve, pushing forward the frontier of what\u2019s possible, you\u2019re going to miss a lot of scientific opportunity.\u201d \n                 Tweet \n                 Follow @NatureNews \n               \n                     Obama\u2019s science legacy: betting big on biomedical science 2016-Aug-22 \n                   \n                     China, Japan, CERN: Who will host the next LHC? 2016-Aug-19 \n                   \n                     Physicists need to make the case for high-energy experiments 2016-Aug-10 \n                   \n                     Scientists worry as cancer moonshots multiply 2016-Apr-27 \n                   \n                     The chips are down for Moore\u2019s law 2016-Feb-09 \n                   \n                     The fragile framework 2015-Nov-24 \n                   \n                     Human Genome Project: Twenty-five years of big biology 2015-Sep-30 \n                   Reprints and Permissions"},
{"file_id": "542156a", "url": "https://www.nature.com/articles/542156a", "year": 2017, "authors": [{"name": "Claire Ainsworth"}], "parsed_as_year": "2006_or_before", "body": "The intricate development of the fetus is yielding its long-held secrets to state-of-the-art molecular technologies that can make use of the mother's blood. Life starts with a puzzle. Out of sight in a mother's womb, 3 billion letters of DNA code somehow turn into 3D bodies, all in the space of a mere 40 weeks. Fetuses form eyes, brains, hearts, fingers and toes \u2014 in processes that are meticulously coordinated in both time and space. Biologists have pieced together parts of this puzzle, but many gaps remain. Now, a crop of molecular technologies is giving scientists tantalizing hints about how to fill in those gaps. Improved ways of reading and interpreting the information in fetal genetic material are uncovering a raft of genes involved in human development, and letting researchers eavesdrop on the hum of gene activity before birth. They can see which genes turn on or off at pivotal moments, and sense how the environment nurtures or intrudes on this. Even the vital life-support system that we jettison at birth \u2014 the placenta \u2014 is laying bare its secrets. \u201cIt really is this great mystery in reproduction,\u201d says Zev Williams, a reproductive endocrinologist and infertility specialist at the Albert Einstein College of Medicine in New York City. \u201cIt's obviously such a critical part of human development, but it's been so understudied.\u201d Until now, much of the work has relied on amniotic or placental samples obtained during routine invasive tests such as amniocentesis. But scientists are eyeing the next step: studies that are non-invasive for the fetus and are done on a teaspoonful of blood drawn from a pregnant woman's arm. In this way, researchers could monitor fetuses as they develop and, down the line, develop non-invasive tests for a broad range of conditions, in both fetus and mother. Physicians are already moving towards treating fetuses in the womb on the basis of such diagnoses. \u201cIt's an exciting time,\u201d says Mark Kilby, a fetal-medicine specialist at the University of Birmingham, UK. But it won't be plain sailing. The technologies are developing so quickly that scientists are struggling to interpret the information they yield and  are facing knotty ethical quandaries . What, for example, should doctors do if non-invasive prenatal testing (NIPT) reveals a DNA sequence that sometimes causes disease \u2014 but not always? \u201cThat is what we have to discuss as a whole community,\u201d says clinician and geneticist Dennis Lo of the Chinese University of Hong Kong, who was the first to find fetal DNA in a mother's blood 1 . \n               Development work \n             Probing fetal development starts, naturally enough, with DNA, the recipe for life. Developmental biologists have already gathered a trove of information here, through studies of laboratory animals from worms to mice, identifying many genes and processes that have human equivalents. Painstaking detective work on families with inherited genetic diseases has yielded even more insight. But the advent of next-generation DNA sequencing is transforming the field. It is now relatively easy to sequence genomes, in whole or in part, to look for the causes of rare genetic disorders. And discoveries are piling up: how key signalling proteins help cells to adopt their myriad identities, and how the packaging of DNA influences brain development, to name just two. \u201cWe are currently in a very rich vein of accelerated understanding,\u201d says Matthew Hurles, a geneticist at the Wellcome Trust Sanger Institute near Cambridge, UK. Most studies so far have parsed genomes after birth. But researchers are pushing to use the same methods on fetuses in the womb, in the hope of improving the diagnoses and prognoses they can offer to expectant parents. Hurles and his colleagues, for example, are studying 1,000 fetuses with structural abnormalities spotted through ultrasounds. Using cells from fetus, mother and father, the team is sequencing the 1\u20132% of the genome that carries instructions for making proteins (the exome), as well as the entire genomes of a smaller subset, to try to identify the genetics behind the disorders. Researchers want to go still further, and sequence entire fetal genomes using blood from the mother. This would give them ready access to DNA at nearly all stages of fetal development, in healthy fetuses as well as ones that may have problems. The approach is realistic, they say. The field is racing ahead: a flurry of papers, from Lo 2 , 3 , Stephen Quake at Stanford University in California 4 , 5  and genome scientist Jay Shendure at the University of Washington in Seattle 6  have honed the resolution with which scientists can  analyse a fetal genome from tiny bits of DNA  floating in the mother's blood. They can now count the number of chromosomes in a fetus 2 , 4 , and are developing ever-more-accurate ways to sequence genomes. In principle, they can now detect single-letter variants in the DNA sequence that might cause inherited diseases, and are building up their ability to find mutations that underlie some developmental disorders but are not present in either parent. Several companies have been formed to develop the technologies. There are barriers to overcome before the newest technologies will see widespread use in lab or clinic. One is the cost. Whole-genome sequencing is getting cheaper, but researchers often need to repeat it many times to boost the resolution of their results 7 . But researchers are confident that these roadblocks won't remain. \u201cThere's work to be done here, but it's not an unsolvable problem from a technical perspective,\u201d says Shendure. Interpreting the results will be another sticking point. Not all DNA changes cause disorders. And even if an individual carries a specific mutation, scientists cannot yet be sure that it will always result in disease. But as costs drop, scientists say, they will be able to sequence enough genomes to learn which mutations predict disorders with high probability. They then hope to see non-invasive whole-genome sequencing applied as a screening tool during pregnancy. \u201cThis is the kind of thing you could imagine would be incredibly useful in diagnosing metabolic and immune disorders where you want to treat the baby right when they're born,\u201d says Quake. And even before birth might be an option. A team of scientists is already using next-generation sequencing of specific genes to diagnose brittle-bone disease in fetuses as part of a clinical trial that uses stem cells to treat the condition in the womb. The researchers are currently obtaining the fetal cells through invasive sampling techniques, but aim to switch to non-invasive testing. \n               Full transcript \n             DNA is but the start of the story of human development. Researchers are keen to understand how instructions in the genome are deployed in time and space as a fetus grows, and how this goes wrong during disease. Many are therefore focusing on the molecule RNA, which the cell uses to copy \u2014 and then act on \u2014 a given set of DNA instructions. And that presents fresh challenges. RNA breaks down very quickly, so it is harder to work with than DNA, especially when trying to untangle a fetus's output of RNA \u2014 its transcriptome \u2014 from the mother's. To simplify things, clinician and geneticist Diana Bianchi, now director of the National Institute of Child Health and Human Development in Bethesda, Maryland, began by studying the transcriptome of amniotic fluid, which contains freely floating RNA from fetus and placenta. Over the past decade, her team has built up intriguing snapshots of gene activity through the second and third trimesters (from discarded samples taken during amniocentesis tests), and at term (from samples gleaned during Caesarean sections), as well as some work with maternal blood, which bears free-floating RNA fragments from fetus, mother and placenta. She has shown how a full-term fetus switches on just the sorts of genes that might be expected for a baby gearing up to be born \u2014 including ones involved in lung and gut physiology, energy metabolism, the immune system and the eye 8 . Genes involved in smell ramp up, too, \u201cwhich we think has some evolutionary advantage\u201d, says Bianchi, \u201cbecause the baby needs to know the smell of its own mother, for survival reasons\u201d. Much of Bianchi's work has focused on amniotic-fluid samples from fetuses affected by chromosomal abnormalities, such as Down's syndrome (an extra chromosome 21) and Edward's syndrome (an extra chromosome 18). She finds that gene activity is abnormal across the whole genome, not just on the extra chromosome, and even in genes needed for brain development 9 . She's also found that cells of fetuses with Down's incur damage from the by-products of metabolism, a condition known as oxidative stress 10 . This raises the provocative possibility of treating fetuses in the womb to ameliorate the cognitive impairment associated with Down's. To explore this, Bianchi's team compared transcriptome data from fetuses with and without Down's, and mouse models of the syndrome, to pinpoint patterns associated with the condition 11 . Then they scoured a database for molecules that might reverse some of the abnormal patterns, including some drugs that are already approved for human use. They fed one of these molecules, called apigenin, to pregnant 'Down's syndrome' mice and in unpublished data found that the offspring had improved memory and met developmental milestones sooner than those whose mothers did not get the compound. \u201cIt's not that everything gets better, but certain areas do improve,\u201d says Bianchi. \u201cWe are very encouraged.\u201d Bianchi and others in the field are now seeking ways to get more detailed information, non-invasively, about fetal RNA. Until recently, the work has been done using devices called microarrays, which allow scientists to detect known RNA sequences. Although valuable, they offer limited insight because much about the transcriptome remains mysterious. A version of next-generation DNA sequencing called RNA-seq reveals the transcriptome in all its complex glory, and quantifies each RNA type much more accurately. Researchers have shown that such an approach is possible 12 , 13 . In 2014, for example, Quake's team examined blood samples from pregnant women using RNA-seq, in combination with other methods, to detect RNAs that probably originated in the fetus and placenta 12 . They could track the ebbs and flows of transcripts through all three trimesters, including the activity of genes that are crucial for normal brain development. Now they are hunting for transcripts that could yield insight into conditions associated with pregnancy such as pre-eclampsia, in which problems with the placenta cause dangerously high blood pressure in the mother. The placenta is also the focus of an RNA-seq project led by Williams and RNA biologist Thomas Tuschl at the Rockefeller University in New York City. They are focusing on microRNA (miRNA), a kind of RNA that's known to control the activity of genes, in the hope of uncovering insight into placental biology and devising early-warning tests for pre-eclampsia and other pregnancy conditions. Existing tests, such as looking for protein in the mother's urine, don't reveal the disease until the mother has already started to develop organ damage, says Williams. His team hopes to use miRNA to monitor the placenta non-invasively, and detect pre-eclampsia before damage takes hold. But such methods still need more work to ensure accuracy and reproducibility before their full potential can be realized, he says. \n               Environmental impact \n             The third piece of the puzzle is how conditions in the womb affect fetal development. Researchers have long known that environmental exposures during this delicate time can influence an individual's lifelong health. Babies whose mothers smoke during pregnancy, for example, grow more slowly in the womb and have an increased risk of developing respiratory diseases and obesity, and studies 14  suggest that smoking alters the transcriptome of the placenta. One way in which the environment exerts such effects is by altering chemical marks on DNA and the proteins that package up the genome \u2014 thereby altering the activities of genes without changing the DNA sequence. The best-studied of these 'epigenetic' marks are methyl groups that, when added to or removed from DNA, boost or silence gene activity. Researchers are using microarrays as well as a form of DNA sequencing known as bisulfite sequencing to lay bare these methylation patterns across the whole genome in samples from maternal blood and fetuses. That includes the all-important placenta. One surprise from studies of placental tissue is just how dynamic placental methylation is, says Wendy Robinson, a developmental biologist at the University of British Columbia in Vancouver, Canada. The most striking trimester-to-trimester changes are in genes related to immune-system functions, possibly reflecting the placenta's role as a peacemaker between the mother's immune cells and the fetus. Researchers are itching to understand changes in DNA methylation in pregnancy conditions and after environmental assaults such as smoking. Indeed, studies already suggest that smoking during pregnancy may lead to altered methylation patterns in placental DNA 15 . Lo's group has shown that it can do bisulfite sequencing on fetal DNA in blood samples. But the complexity of unravelling links between environment and epigenetics makes it hard to draw definitive conclusions for these samples yet, says Robinson. Researchers are therefore focusing on studies of placental tissue for now. \n               Thorny ethics \n             The promises of all these technologies raise issues that should be debated sooner rather than later, say scientists and bioethicists \u2014 not least because there are concerns about the NIPT tests already on the market. These tests have spread fast: since  becoming commercially available in 2011 , NIPT for missing or extra chromosomes (aneuploidies) is now being used in at least 90 countries. And millions of women have had the tests. NIPT for aneuploidy is a dramatic improvement, says Bianchi. Globally, it has led to a 70% reduction in invasive procedures such as amniocentesis, which carry a small risk of triggering miscarriage. But NIPT can't diagnose aneuploidies reliably, she says: it is a screen, and other, more-invasive diagnostic tests must be used to follow up on the findings. Nonetheless, some women have opted to terminate their pregnancies on the basis of NIPT results alone. Concerns such as these have led several societies to publish position statements that give recommendations for how to counsel patients. The situation stands to get even murkier. Careless talk about the epigenetics of pregnancy  risks scapegoating women  for their babies' ill health, when problems such as obesity and gestational diabetes can stem from many factors, including poverty and poor access to health care, say social scientists. Women must also prepare for unexpected findings, researchers say \u2014 and not just about their fetuses. In several cases, non-invasive fetal screening has picked up undiagnosed cancers and diseases such as lupus in pregnant women. And sequencing will sometimes reveal fetal DNA variants that increase the risk of conditions later in life, such as breast cancer or neurodegenerative diseases. Medical researchers say that clinicians must prepare what to share with patients, even as the light-speed pace of invention and discovery outstrips their ability to interpret the findings. It is always hard to balance the right to know against the potential harm of revealing the presence of a DNA variant \u2014 especially if scientists can't be sure what the effect of that variant will be, says Shendure. \u201cIt's just going to get really tricky.\u201d\n \n                 Tweet \n                 Follow @NatureNews \n               \n                     The flip side of personal genomics: When a mutation doesn't spell disease 2016-Nov-15 \n                   \n                     NIH invests US$41.5 million in placenta research 2015-Feb-27 \n                   \n                     Blood test tracks cancer 2013-Mar-13 \n                   \n                     Fetal genome deduced from parental DNA 2012-Jun-06 \n                   \n                     Fetal gene screening comes to market 2011-Oct-25 \n                   \n                     Epidemiology: Study of a lifetime 2011-Mar-01 \n                   Reprints and Permissions"},
{"file_id": "541148a", "url": "https://www.nature.com/articles/541148a", "year": 2017, "authors": [{"name": "Brian Owens"}], "parsed_as_year": "2006_or_before", "body": "The country is gearing up to get rid of rats, possums, stoats and other invasive predators by 2050. Is it a pipe dream? Razza the rat nearly ended James Russell\u2019s scientific career. Twelve years ago, as an ecology graduate student, Russell was releasing radio-collared rats on to small islands off the coast of New Zealand to study how the creatures take hold and become invasive. Despite his sworn assurances that released animals would be well monitored and quickly removed, one rat, Razza, evaded capture and swam to a nearby island. For 18 weeks, Russell hunted the animal. Frustrated and embarrassed, he fretted about how the disaster would affect his PhD. \u201cI felt rather morose about the prospects for my dissertation,\u201d he says. Although there was a lot of literature on controlling large rat populations, little had been written about tracking and killing a single rodent, which turns out to be rather important in efforts to completely\u00a0eradicate a species. \u201cIt demonstrated how hard it is to catch that very first rat as it arrives on an island \u2014 or, conversely, the very last rat that you\u2019re trying to get off,\u201d says Russell, now at the University of Auckland. Razza\u2019s escape became the subject of a paper in  Nature 1 as well as a popular children\u2019s book. And now, with more than a decade of successful pest-eradication projects behind him, Russell is taking on a much bigger challenge. He is coordinating research and development for a programme that the government announced last July to eliminate all invasive vertebrate predators \u2014 rats, brushtail possums, stoats and more \u2014 from New Zealand by 2050 to protect the country\u2019s rare endemic species. The audacious plan is not as far-fetched as it sounds, says Josh Donlan, director of Advanced Conservation Strategies, a consultancy that has designed invasive-species eradication projects in\u00a0Europe, South America and the United States. Around the world, more\u00a0than 1,000 islands have been cleared of invasive species through \u2018 mega eradications \u2019. And New Zealand, home to some of the leading experts in the field, carried out more than 200 of them. With enough money, time and political will, Donlan says, it should be possible to clear the entire country. But the size of this latest target represents a tremendous leap. The largest\u00a0island ever cleared is Australia\u2019s Macquarie Island, which covers about 128 square kilometres. New Zealand\u2019s total area is about 268,000 square kilometres, and the country\u2019s cities and towns complicate eradication efforts and provide countless places in which animals can hide. \u201cWith current techniques, it\u2019s not feasible,\u201d says Richard Griffiths, an\u00a0ecologist based in Auckland with the environmental group Island Conservation. To scale up, new approaches will be required. New Zealand wants to rid itself of all its invasive pests on a tight deadline. Hear how it plans to do it. That\u2019s where Russell and his colleagues come in. They are about to start a major research project to develop some of the necessary technologies, such as new baits, species-specific poisons and genetic tweaks that interfere with animal fertility. To succeed, the project will require public and political support \u2014 and money. In a 2015 paper 2 , the team estimated the entire cost at around NZ$9 billion (US$6 billion), arguing that the savings to pest-control programmes, and the reduction in environmental damage and crop loss, would more than cover the outlay. Their argument has been convincing. \u201cOur government just grabbed that paper, and the surrounding evidence and public goodwill, and announced this policy,\u201d Russell says. \u201cIt\u2019s been pretty hectic here ever since.\u201d \n               A dying wish \n             New Zealand is a poster child for the havoc wrought by invasive species. For millennia it was an island of small lizards and flightless birds, such as the iconic kiwi. Since land mammals, including humans, first arrived some 750 years ago, the number of species of native vertebrate fauna have nearly halved \u2014 at least 51 species of bird have disappeared in that time. Losses sped up dramatically after Europeans arrived in the late eighteenth century. The mammalian pests are a drain on New Zealand\u2019s economy. The government spends around NZ$70\u00a0million each year on pest-control programmes for animals, and invasive predators cost the country an estimated NZ$3.3\u00a0billion a year in lost productivity 3 . Most of the losses come from agriculture, but government officials also worry about the hit to the country\u2019s reputation as a destination for unspoilt natural beauty. \u201cLast year, tourism overtook agriculture as our biggest revenue earner,\u201d says Maggie Barry, the minister of conservation. \u201cOur environment is what attracts people here.\u201d Although the environmental and economic arguments had been around for some time, many people credit physicist Paul Callaghan with getting the public to back eradication plans. Callaghan was an eminent scientist and a household name in New Zealand \u2014 a sort of Kiwi David Attenborough \u2014 writing popular books and presenting television shows about science and innovation. In a public address in 2012, he encouraged New Zealanders to save the nation\u2019s native fauna by eradicating its invasive pests. \u201cIt\u2019s crazy and ambitious but I think it might be worth a shot,\u201d he said. The address would be his last; he died of cancer a few months later. Callaghan\u2019s plea caught the public\u2019s imagination, tapping into a groundswell of support for local conservation programmes designed to protect native birds and other animals. A lot of the techniques for clearing an island are well established. The standard practice for killing rats and other invaders is to lace bait stations with a poison \u2014 usually sodium fluoroacetate, known as 1080, or the anticoagulant brodifacoum \u2014 and to spread the poison across the landscape by helicopter. The few animals that survive the chemical onslaught are caught in traps or shot. The active phase of eradication is very quick. It takes just a few days to spread the bait, and within a few weeks all the invaders are gone, says Griffiths. Most of the time is spent on preparation. \u201cYou generally have just one chance to get it right,\u201d he says, mainly because of the high cost. \u201cSo 90% of the work is planning and logistics.\u201d In 2011, Griffiths reached the end of a four-year project costing NZ$3.5 million 4  to eradicate all invasive mammals from Rangitoto and Motutapu, two inhabited islands with a combined size of 38\u2009square kilometres. After two years of planning and consultations with local people, rats were wiped out in 3\u20134 weeks; conservationists then moved on in stages to deal with rabbits, stoats, hedgehogs and feral cats. The effort was complicated by the presence of human inhabitants, and by the islands\u2019 proximity to Auckland, New Zealand\u2019s largest city, which provides a deep pool of potential reinvaders. \u201cThe ferry goes there six times a day, with hundreds of people, and\u00a0boats\u00a0pull up every weekend,\u201d says Russell. Hitchhiking rats and mice are intercepted about once a year, but the island has remained pest-free for the past five. \n               New technologies \n             Tackling all of New Zealand isn\u2019t just about scaling up efforts. \u201cWe\u2019re good at killing things,\u201d says Barry, \u201cbut we\u2019ll rely on scientific breakthroughs to get us over the line.\u201d Some of the first innovations that Griffiths would like to see are new baits, poisons and traps, as well as tools for detecting invaders. The poison 1080 has been in widespread use since the 1950s and is an effective pesticide, but it can kill game animals such as deer and pigs (which are also introduced species, but not the target of eradication efforts); it also threatens the kea ( Nestor notabilis ), a native alpine parrot. Many hunters and animal-rights groups oppose use of the chemical, especially when it is sprayed from helicopters. \u201cSomething that targets only rats or mice would be wonderful,\u201d Griffiths says. For possums, Russell and his colleagues plan to sequence the creature\u2019s genome in the hope of identifying targets unique to its marsupial biology. Traps could be improved by developing devices that need minimal human intervention. A New Zealand company called Goodnature already makes rat and possum traps with a skull-crushing piston that is powered by compressed gas. It can reset itself 24 times (clean-up is provided by\u00a0scavenging birds and cats). Russell\u2019s colleague Andrew Kralicek is working on wireless electronic biosensors that can detect species-specific molecules given off by a pest. Such devices could be used to monitor traps or send warnings about new invaders. And drones, which have already been used to monitor sheep herds in the country, could be fitted with those biosensors to sniff out targets and quickly drop a precise dose of poison. This could be useful in areas where releasing tonnes of laced bait by helicopter is not feasible. \u201cThat\u2019s kind of a Skynet future,\u201d says Russell, \u201cbut it could work in pest control.\u201d The ideas that are generating the most excitement in conservation circles are genetic biocontrols that might be able to suppress invaders by introducing harmful traits. The powerful gene-editing tool CRISPR\u2013Cas9 could be used to disrupt a gene that is vital for survival or reproduction or that makes an animal more susceptible to a certain poison. Then, using what is known as a gene drive, scientists could engineer that gene to spread through the population. \u201cIt can go from 1% to 100% of the population in around 10 generations,\u201d says Ethan Bier, a geneticist at the University of California, San Diego, who is using  gene drives to engineer mosquitoes  that are resistant to the malaria parasite 5 . So far, gene drives have been used only in the lab and mostly with insects, but there is nothing to suggest that they wouldn\u2019t work in the wild on possums or rats. The problem, says Bier, is that once you start introducing harmful traits, you\u2019re fighting against evolution, which tends to eliminate problematic mutations. There is also the danger of reverse invasions. The possums that have become invasive in New Zealand originated in Australia. If some sort of gene-driven \u2018suicide possum\u2019 made its way back there, it could wreak havoc on the native populations. Another genetic technique, being developed in New Zealand as part of Russell\u2019s project, could avoid some of these difficulties. The Trojan Female Technique targets mitochondria, the tiny power plants inside cells. Mutations in mitochondrial DNA can seriously impair the ability of sperm to swim. Because these mutations affect the fitness only of males, and because mitochondria are passed down only through the female line, these traits can survive natural selection. Females carrying the mutations would have sterile male offspring, but their daughters would be able to breed, producing yet more sterile males. Daniel Tompkins, an ecologist in Dunedin, New Zealand, and his colleagues have already shown in computer models and lab experiments that this technique can work in fruit flies 6 : a single release of Trojan females kept population numbers low over ten generations, with no sign of natural selection fighting back. Of course, when it comes to mammals, releasing thousands of Trojan female rats would be counterproductive: those rats would be just as much of a threat to the ecosystem as the ones you\u2019re trying to get rid of. So Tompkins, who works for Landcare Research, a government research institute, sees it more as a  coup de gr\u00e2ce  \u2014 a way to prevent pest populations from recovering after they have been cut back by conventional techniques. Once the numbers are small, releasing a few Trojan females would cap population regrowth, he says. Those small populations might then simply die out naturally, or survive at such low levels that they would no longer pose a threat to native species. \n               Back-yard battles \n             All these techniques are several years away from large-scale deployment, and none is a silver bullet, cautions Russell, who says that the answer will be to use a mixture of methods, staggered over a long period. \u201cWhat might be the cheapest or most appropriate in the forest won\u2019t be the most appropriate in someone\u2019s back yard,\u201d he says. And getting access to those back yards will make or break the project, says Donlan. An eradication has to be close to 100% successful for it to work, and that means getting buy-in from almost everyone concerned. If any large groups of people refuse to cooperate with the plan, areas could be left uncleared, providing havens for invaders. \u201cThe all-or-nothing nature of eradication makes social issues more important and challenging,\u201d says Donlan. \u201cSupport has to be greater than just a simple majority.\u201d That\u2019s an area where New Zealand is relatively lucky, says Russell. The country is already home to thousands of volunteer community groups that spend their free time setting and checking traps. People in the Wellington suburb of Crofton Downs, for example, think that the region is already free of predators after they managed to get a trap placed in every fifth back yard. \u201cWe\u2019re in a relatively unique position in New Zealand, where people are really, really willing to kill for conservation,\u201d Russell says. \u201cIt\u2019s kind of a national pastime.\u201d Nevertheless, some aspects of the project could test the limits of public support. Biocontrol techniques for mosquitoes, for example, have faced stiff opposition from residents in Florida and Brazil. New Zealanders may be heavily in favour of conservation, but they are generally suspicious of genetic engineering 7 . And gene-drive technologies are  controversial throughout the world . Then there\u2019s the money. The government and philanthropic groups have committed to donate about NZ$3\u00a0billion by the 2050 deadline \u2014 well short of the NZ$9\u00a0billion that Russell estimates would be needed. But the\u00a0government hopes that further scientific breakthroughs will bring the cost down. Russell is sure those breakthroughs will come. He points out that the first rat eradication was achieved on a 1-hectare island off New Zealand in 1963, at a time when no one thought it would be possible. \u201cWe don\u2019t know how we\u2019ll do it in 2050, but back in 1960 we didn\u2019t know we\u2019d be doing what we were doing in 1980 or 2010,\u201d he says. In some ways, it is his experience with Razza that gives him hope. Although the rat was eventually caught in a decidedly low-tech way \u2014 a convenient penguin carcass proved to be irresistible bait \u2014 the hunt forced Russell\u2019s team to refine cutting-edge techniques that are still in use. For example, biosecurity dogs can hunt down individual hold-outs, and genetic sequencing of faeces can identify remaining populations. \u201cI am proud to look back and see how far we\u2019ve come in just ten years,\u201d he says. \n                 Tweet \n                 Follow @NatureNews \n               \n                     Invasive-species control: Bounty hunters 2014-Sep-12 \n                   \n                     China battles army of invaders 2013-Nov-27 \n                   \n                     Invasive species: The 18-km2 rat trap 2013-May-15 \n                   \n                     Where eagles die 2011-Jan-18 \n                   \n                     Ecology: A world without mosquitoes 2010-Jul-21 \n                   \n                     Nature  Special: CRISPR \n                   \n                     Predator Free New Zealand \n                   Reprints and Permissions"},
{"file_id": "542023a", "url": "https://www.nature.com/articles/542023a", "year": 2017, "authors": [{"name": "XiaoZhi Lim"}], "parsed_as_year": "2006_or_before", "body": "As demand for air conditioning climbs, some see a solution in the very thing that makes us sweat: the Sun. At Hotel Star Sapphire in Dawei, Myanmar, guests sip from coconuts in cool, air-conditioned comfort as the steamy tropical night rolls on. Seven thousand kilometres to the west, in dry Khartoum, Sudan, patients rest in a United Nations Hospital, cocooned from the baking desert heat. In both buildings, the pleasant conditions come courtesy of air-conditioning units that rely in part on dark glass tubes that turn sunlight into cooling power. These aren\u2019t the familiar solar panels that harvest light to make electricity. Instead, they harness heat from the Sun to chill buildings through a neat bit of thermodynamic sleight of hand. Researchers and some energy experts say that this form of cooling \u2014 known as solar thermal \u2014 could help to slake the growing global demand for fuel to run energy-hungry air conditioning. The Intergovernmental Panel on Climate Change predicts that by 2100, the  need for electricity to power cooling will have surged  to more than 30 times what it was in 2000. Hopeful that solar-thermal technology is nearing a crucial turning point, research groups are showing off their systems at a growing number of hotels, shopping centres and other buildings across the world. Today, there are some 1,200 installations \u2014 more than 10 times the total from a decade ago. Companies that produce solar-thermal chillers say that they use 30\u201390% less electricity than the conventional air conditioners that operate in most buildings, depending on the type and size of the installation. And researchers are working to make the systems more efficient and cheaper to build. But the technology faces daunting hurdles, and some experts doubt that it will ever be more than niche in a world that each year adds 100 million conventional air conditioners, which rely on compressors powered by electricity. Solar-thermal chillers are just too expensive, typically costing about five times more than conventional ones, says Daniel Mugnier, an engineer with the solar-technologies company Tecsol in Perpignan, France. Although the price is dropping, the technology lacks the subsidies and investment it needs to make it more competitive, he says. That is a pity, he adds, because thermal systems have several advantages. They could lower peak demand on the electrical grid, reducing blackouts and the need to tap dirtier energy sources. They are also silent, and typically use environmentally friendly refrigerants \u2014 a point that took on new importance in October, when  more than 170 countries agreed to phase out the hydrofluorocarbon chemicals  used in most air conditioners and refrigerators. And solar heat is available in large quantities just where demand for cooling is highest. \u201cIt\u2019s almost like a marriage made in heaven,\u201d says Christos Markides, a solar researcher at Imperial College London. \n               All in the phase \n             The key to air conditioning is evaporation: the cooling occurs when a liquid absorbs energy from its surroundings and changes phase to evaporate as a gas. That\u2019s how perspiration cools our bodies and it also happens in nearly every air conditioner, from small window units to the 8-metre-long giants used to chill large buildings in Qatar. In modern electrical air conditioners, a liquid refrigerant is forced through a small nozzle into a large chamber. That causes its pressure to plummet, so it evaporates rapidly and removes heat from the indoor air. The gaseous refrigerant then travels to another chamber, where a mechanical compressor powered by electricity squeezes the gas to drive up its temperature further. That hot, gaseous refrigerant then passes through a condenser \u2014 often a coil of thin tubing \u2014 where it changes back into a liquid and expels heat outdoors. The liquid refrigerant is then squirted back into the evaporation chamber and the cycle repeats. The gas-squeezing step is needed because to shed heat outdoors efficiently, the refrigerant must be very hot before it goes through the condenser, explains Colin Chia, co-founder of the Singaporean company Ecoline, which developed Hotel Star Sapphire\u2019s air-conditioning system. In electrical units, this is done mechanically. But there is another way \u2014 simply using heat. One of the oldest air conditioners to be built on this principle burned wood to supply the heat and was introduced at the World Exhibition in Paris in 1878. It was \u201ca marvellous old machine\u201d, says Christian Holter, chief executive of SOLID, a company in Graz, Austria, that specializes in large-scale solar-thermal cooling and heating systems. Called absorption chillers, the devices use heat from the Sun to boil the refrigerant out of a solution \u2014 typically water from a salt solution, or ammonia gas from water. Then the gaseous refrigerant goes through condensation and evaporation stages similar to those in compression systems (see \u2018Two ways to chill\u2019). Compression dominates the market because \u201cit is easy to buy, plug and start\u201d, says Holter. But as far back as the 1980s, growing concern over the ozone-depleting refrigerants used in compression air conditioners revived interest in thermal systems. They never caught on, however, because they could not compete with those powered by cheap electricity and because their heat source \u2014 burning biomass or natural gas \u2014 is difficult to manage. Heat from the Sun doesn\u2019t have those problems. In modern solar-thermal systems, special collecting tubes or plates absorb energy from the Sun\u2019s rays and then transfer that heat to an absorption chiller. So far, SOLID has installed large-scale systems in 18 schools, offices and warehouses in 10 countries. One of these, the world\u2019s largest solar-thermal cooling system so far, has since 2014 been chilling a high school in Arizona, where air conditioning typically makes up a significant fraction of an annual electricity bill. Academic researchers and companies are also trying to improve performance in other ways. Most absorption chillers, including SOLID\u2019s, heat the refrigerants to around 80 \u00b0C. If the temperature could be raised to 120\u2013170 \u00b0C, then more refrigerant would evaporate and circulate as gas in the system at the same time, making the unit more efficient. That means the solar collector must concentrate the Sun\u2019s heat more effectively. Some specialized collectors can follow the Sun and achieve temperatures of up to 400 \u00b0C, but they are expensive. To develop a cheaper alternative, a team led by engineer Roland Winston at the University of California, Merced, is improving the design of the collecting tubes. The team\u2019s tubes contain a special metallic piece that transfers heat rapidly to a glycol fluid in an inner copper pipe. Winston\u2019s team also puts curved sheets of reflective material under the outer tubes, which helps them gather solar energy as the Sun moves through the sky. The system can heat the glycol to 200 \u00b0C and is now being tested with different chillers. Other teams are leaving absorption chillers behind and building entirely new systems. A group led by Stephen White at the Commonwealth Scientific and Industrial Research Organisation (CSIRO) in Newcastle, Australia, has developed a desiccant-wheel system that since June 2016 has been cooling a shopping centre in Ballarat, Victoria. First, ambient air passes through a slowly rotating wheel containing a material that adsorbs moisture, leaving the air hot and dry. This dry air moves into a chamber where it causes water to evaporate, thereby lowering the temperature. The chilled, moist air is used to cool air from the building that runs through a separate conduit. That moist air is then expelled outside, and solar heat is used to dry the moisture-adsorbing material in the wheel. Fresh approaches are in order because absorption chillers are expensive and complicated to build, says Mike Dennis, a private solar consultant in Canberra. \u201cThey don\u2019t make any sense,\u201d he says. It is easier  to use photovoltaic panels  to turn sunlight into electricity, which can then run standard compression air conditioners. Falling prices for photovoltaics are making that kind of system increasingly attractive. Photovoltaics now benefit from economies of scale, as well as from massive government subsidies and investments that solar-thermal technologies do not have, says Mugnier. \u201cMy fear is that the competition is unfair.\u201d Another approach is to create a hybrid: a conventional electrical compression machine that uses heat from the Sun to help the energy-guzzling compressor. Ecoline\u2019s air-conditioning system at Hotel Star Sapphire is an example. To create the system, Chia inserted a U-shaped loop of copper into each solar collector tube and then linked up the copper pipes into a long ribbon. Glycol inside the pipes quickly transfers heat from the tubes to a glycol tank. Another set of copper pipes containing refrigerant snakes through the tank, heating up the refrigerant. The refrigerant then passes through a compressor. It turns into a gas much more easily than in a standard system because it\u2019s already piping hot. The company has installed more than 1,000 air-conditioning units in 6 countries and, in mid-2018, will be air conditioning a dormitory at Singapore\u2019s Nanyang Technological University. In side-by-side tests, Ecoline says, its air conditioner delivered 35% energy savings compared with a standard high-efficiency air conditioner. The hybrid systems cost 15% more to install but are cheaper to run and recoup the extra expense in 2 years, based on electricity prices in Singapore, says Chia. Proponents are confident that costs would drop significantly if the market for solar thermal expanded. Winston\u2019s postdoc Lun Jiang notes that in the 1990s, evacuated tubes used for solar water heating cost more than US$100 per metre, but they now cost just $2\u20133 because of mass production driven by widespread use of the systems in China. Others say that thermal technologies can access waste heat that photovoltaics, which collect only light, cannot. They could mop up energy that concentrates in hot cities, industrial plants and data centres. In fact, Ecoline is now working with a data-centre management company in Indonesia to cool facilities using its own waste heat. That kind of approach makes good thermal sense, says Chia. \u201cThe hotter the better.\u201d \n                 Tweet \n                 Follow @NatureNews \n               \n                     Print flexible solar cells 2016-Nov-23 \n                   \n                     Can wind and solar fuel Africa's future? 2016-Nov-02 \n                   \n                     Energy policy: Push renewables to spur carbon pricing 2015-Sep-02 \n                   \n                     Solar energy: Springtime for the artificial leaf 2014-Jun-04 \n                   \n                     Nature  special: The circular economy \n                   \n                     Nature  special: Sustainability \n                   \n                     Nature  special: 2015 Paris climate talks \n                   Reprints and Permissions"},
{"file_id": "541016a", "url": "https://www.nature.com/articles/541016a", "year": 2017, "authors": [{"name": "Margaret Munro"}], "parsed_as_year": "2006_or_before", "body": "Researchers brave polar bears, mosquitoes and gull attacks in the Canadian Arctic to investigate an alarming die off Four gun-toting biologists scramble out of a helicopter on Southampton Island in northern Canada. Warily scanning the horizon for polar bears, they set off in hip waders across the tundra that stretches to the ice-choked coast of Hudson Bay. Helicopter time runs at almost US$2,000 per hour, and the researchers have just 90 minutes on the ground to count shorebirds that have come to breed on the windswept barrens near the Arctic Circle. Travel is costly for the birds, too. Sandpipers, plovers and red knots have flown here from the tropics and far reaches of the Southern Hemisphere. They make these epic round-trip journeys each year, some flying farther than the distance to the Moon over the course of their lifetimes. The birds cannot, however, outfly the threats along their path.  Shorebird populations have shrunk , on average, by an estimated 70% across North America since 1973, and the species that breed in the Arctic are among the hardest hit 1 . The crashing numbers, seen in many shorebird populations around the world, have prompted wildlife agencies and scientists to warn that, without action, some species might go extinct. Although the trend is clear, the underlying causes are not. That\u2019s because shorebirds travel thousands of kilometres a year, and encounter so many threats along the way that it is hard to decipher which are the most damaging. Evidence suggests that  rapidly changing climate conditions in the Arctic  are taking a toll, but that is just one of many offenders. Other culprits include  coastal development , hunting in the Caribbean and agricultural shifts in North America. The challenge is to identify the most serious problems and then develop plans to help shorebirds to bounce back.\u00a0 \u201cIt\u2019s inherently complicated \u2014 these birds travel the globe, so it could be anything, anywhere, along the way,\u201d says ecologist Paul Smith, a research scientist at Canada\u2019s National Wildlife Research Centre in Ottawa who has come to Southampton Island to gather clues about the ominous declines. He heads a leading group assessing how shorebirds are coping with the powerful forces altering northern ecosystems. Researchers say it is now more crucial than ever to understand how conditions in the Arctic are altering breeding and survival. \u201cThis is high urgency,\u201d says ecologist Jan van Gils of the Royal Netherlands Institute for Sea Research in Texel, who is studying the decline of wading birds called red knots ( Calidris canutus ) that breed in the Russian Arctic. \u201cIt\u2019s indeed now or never.\u201d \n               Gunshots and pancakes \n             At his base camp on the island, Smith looks as if he could have walked out of a tech lab in Silicon Valley \u2014 except for the shotgun slung over his shoulder. He is constantly in motion, up before 7 a.m. and in the helicopter by 9, ready to head north to survey shorebirds. At 6 p.m., he\u2019s in the kitchen cabin, humming away as he cooks dinner for the hungry crew returning from their nest hunts. He deals with a computer glitch before bed, and then scrambles out of his sleeping bag at 2 a.m. when a curious polar bear wanders into camp. He deters it with several warning shots, which makes for animated conversation over the pancakes that he whips up later that morning. Then, he hikes off to set contaminant collectors in thigh-deep melt ponds to check for pollution that might be affecting the birds. Smith has been coming to this site, known as East Bay, every year since 2000, when he was sent here as a biology student to help build a camp to study poorly understood shorebirds. He was soon intrigued by the birds \u2014 some no bigger than a sparrow \u2014 that fly across continents to lay eggs on the wide-open landscape. Sleet is not uncommon in June, cold winds blow in off Hudson Bay in July and snowdrifts can persist well into August. Smith now heads studies at the 12-square-kilometre research plot at East Bay, one of the longest-running shorebird-research camps in the Arctic. He is also co-leader of a joint effort between Canada and the United States called the\u00a0Arctic Program for Regional and International Shorebird Monitoring (Arctic PRISM). The project, which began in 2002, has dispatched crews to more than 2,000 sites, stretching from Alaska to Baffin Island in eastern Canada, to survey the 26 shorebird species that breed in the North American Arctic. Smith and his Canadian co-leader, Jennie Rausch, cover the central and eastern Arctic, and the first round of the massive PRISM survey is nearing completion. It is a short, intense season for both the birds and the biologists. The East Bay site, several hundred kilometres north of the tree line, comes alive in June with the mating and territorial calls of a dozen shorebird species. Among them are the robin-sized red knot, which flies up from the tip of South America; several plovers and sandpipers; and the ruddy turnstone ( Arenaria interpres ) that winters in Latin and South America. \u00a0Shorebirds stream north on four main flyways in North America and Eurasia, and many species are in trouble. The  \n                   State of North America\u2019s Birds 2016 \n                  report 1 , released jointly by wildlife agencies in the United States, Canada and Mexico, charts the massive drop in shorebird populations over the past 40 years. The East Asian\u2013Australasian Flyway, where shorelines and wetlands have been hit hard by development, has even more threatened species. The spoon-billed sandpiper ( Calidris pygmaea ) is so \u201ccritically endangered\u201d that there may be just a few hundred left, according to the International Union for Conservation of Nature. Red knots are of major concern on several continents. The subspecies that breeds in the Canadian Arctic, the rufa red knot, has experienced a 75% decline in numbers since the 1980s, and is now listed as endangered in Canada. \u201cThe red knot gives me that uncomfortable feeling,\u201d says Rausch, a shorebird biologist with the Canadian Wildlife Service in Yellowknife. She has yet to find a single rufa-red-knot nest, despite spending four summers surveying what has long been considered the bird\u2019s prime breeding habitat. The main problem for the rufa red knots is thought to lie more than 3,000 kilometres to the south. During their migration from South America, the birds stop to feed on energy-rich eggs laid by horseshoe crabs ( Limulus polyphemus ) in Delaware Bay (see  \u2018Tracking trouble in the Arctic\u2019 ). Research suggests that the crabs have been so overharvested that the red knots have become deprived of much-needed fuel. \n               boxed-text \n             In other cases, climate change might be the prime problem. Van Gils\u2019 team in the Netherlands\u00a0has found that red knots that breed in the Russian Arctic produce smaller offspring during summers when the snow melts early 2 . He suspects the reason to be malnutrition. During warm years, the red-knot chicks may miss the peak abundance of insects, so the birds do not grow as big. When those undersized knots migrate to their West African wintering grounds, they run into further problems because their short bills cannot reach deeply buried clams, their preferred food. \u201cWe show the smaller individuals live shorter lives and have lower survival than larger individuals,\u201d van Gils says. He says that he had his first close-up look at the knots\u2019 Arctic breeding grounds last summer, when he joined a US team to study a subspecies that migrates to Alaska, where chicks\u2019 growth rates also seem to be slowing as temperatures climb. The data collected last year are still being analysed, but van Gils suspects another timing mismatch there \u2014 chicks miss the peak insect emergence. Conditions are also changing rapidly at Smith\u2019s research site in Canada, where sea ice last year broke up more than a month earlier 3  than it did three decades ago. But when it comes to the shorebird population declines at East Bay, says Smith, \u201cthere may be more immediate threats than climate change\u201d. Snow geese ( Chen caerulescens ) are high on his list of suspects. The goose population has exploded in North America, and they have severely degraded wetlands along the coast of Hudson Bay that serve as key refuelling stops for millions of migrating shorebirds 4 . Geese are also showing up in shorebird breeding territory, where they mow down the grasses that the shorebirds use to protect their nests on the wide-open landscape. Perhaps even more threatening, says Smith, is that the geese attract foxes and other predators that eat shorebird eggs and chicks. Southampton Island is an ideal spot for gauging the impact because there are now about 1 million nesting snow geese on the island. A sister research site that Smith runs on nearby Coats Island \u2014 less than an hour away by Twin Otter aircraft \u2014 provides a control because the island does not have a snow-goose colony. \n               Hidden nests \n             One day in late June, biologist Lisa Kennedy calls out a warning as she surveys the East Bay research site. \u201cCareful where you step,\u201d says Kennedy, sticking to the larger rocks for fear of crushing the speckled eggs of a plover. A doctoral student at Trent University in Peterborough, Canada, Kennedy led the six-person crew that combed for nests last summer. The young biologists instantly identify birds by their silhouettes, calls and behaviour. They are also quick to spot polar bears that can appear seemingly from nowhere, which is why shotguns are taken everywhere \u2014 even to the outhouse. The biologists hike 10\u201315 kilometres a day across the tundra and melt ponds to find and monitor nests. They also spend a lot of time lying motionless on the soggy ground waiting for shorebirds that they spooked to return to their eggs. It can sometimes take days to locate the well-hidden nests \u2014 some are circular cups the birds dig in the ground; others just grass, moss and the odd feather pulled together. The researchers weigh and measure\u00a0the birds and their eggs and put the nests under surveillance. They also fit many birds with bling \u2014 bands around the legs and pearl-sized nanotags that are glued to their feathery backsides. \u201cYou have to be careful not to stick yourself to the bird,\u201d says Kennedy, holding a nanotag in place on a semipalmated plover until the Krazy Glue sets. Then the bird is off, the hair-like antennae on the tag emitting an electronic pulse that can be picked up by the receiving station on top of a cabin back at camp. This is part of the Motus tracking system, a network of some 300 receiving towers that is expanding across the Americas. The Motus nanotags weigh less than 0.3 grams \u2014 so light that they can be carried by the smallest shorebirds and their chicks. Their signals are picked up when the birds are within 15 kilometres of a receiving station. On his computer, Smith has watched red knots make the 3,000-odd-kilometre flight to the Arctic from Delaware Bay on the US coast in three days. \u201cThey go \u2018ding, ding, ding\u2019 as they hit the towers,\u201d he says. Shorebirds make some of the longest migrations in the animal kingdom. One red knot, sporting leg band B95, journeyed to and from the southern tip of South America and the Arctic for more than 20 years. The nanotags that Smith\u2019s group uses in the Arctic are helping to fill in details about the ultramarathon migrations. In 2014, Smith and his colleagues discovered that red knots were stopping to refuel at a previously unknown spot along the coast of Hudson Bay. Nanotags are also valuable on the breeding grounds because they enable Smith\u2019s team to monitor how much time adults spend on their nests, and how far hatchlings wander in search of insects \u2014 two of many variables affected by geese. Nesting shorebirds will take flight to defend their nests from grazing geese, leaving their eggs and chicks vulnerable to the foxes and predatory birds. This year, predators took most of the\u00a0shorebird eggs laid on the East Bay research plot. Just 20 eggs out of 296 survived long enough to hatch. At the snow-goose-free site on Coats Island, more than half the eggs hatched. Smith says that the reproductive success of shorebirds on Southampton Island has dropped so low that the population can no longer sustain itself. The\u00a0research adds to long-standing concerns about North America\u2019s goose explosion. The geese used to winter in the coastal marshes of\u00a0Louisiana\u00a0and Texas, but now\u00a0spend the season feasting on leftover crops on farm fields in the southern and Midwest United States. In the spring, the geese fly to the Arctic to breed. Robert Rockwell, a population biologist and ecologist at the American Museum of Natural History in New York City, says that it has been \u201cpretty staggering\u201d to watch the goose population explode from 1.5 million in the 1960s to what he estimates could be as many as 20 million today. He has run a decades-long monitoring project on La P\u00e9rouse Bay on the coast of Hudson Bay, where he and his colleagues first showed 5  how geese can rip up lush green grass and marshland and render it inhospitable to both plants and animal species such as shorebirds. Subsequent work 6  has shown that geese have triggered long-term damage that has reduced the biodiversity of plants, insects and birds at several other sites. Whether geese have enough of an effect to be a major factor in the shorebird declines is still unclear. Rockwell says that the question of goose impacts is a crucial issue, and applauds Smith\u2019s team for trying to answer it. \n               Fight for flight \n             Brad Andres, national shorebird coordinator of the shorebird conservation plan for the US Fish and Wildlife Service in Falls Church, Virginia,\u00a0says that there is a huge need to understand how different threats and disturbances impact shorebird survival \u2014 be it snow geese in Arctic Canada, insect abundance in Alaska and Russia or the destruction of feeding grounds and refuelling stops caused by coastal development in the tropics and midlatitudes. Researchers are building models\u00a0to pinpoint the biggest dangers and help managers to develop the most efficient conservation actions. \u201cBut it\u2019s a data-hungry system,\u201d says Andres. \u201cSo until we have the sources of information, it is hard to do.\u201d Van Gils also stresses the need to find the mechanisms driving the declines, which he expects to worsen, given the unrivalled warming rates seen in the Arctic. \u201cFor knots, there is no way out \u2014 they are already at the northern edge of the world,\u201d says van Gils, who predicts that many knot subspecies will collapse over the next 50 years because of warming and trophic mismatches. For now, the shorebirds are back in the sunny southern climes, settled onto beaches and wetlands often shared with tourists, shrimp farmers and hunters. Rausch and Smith have hung up their waders and are back at their desks, drawing up plans for the camps and aircraft that they\u2019ll use to catch up with the shorebirds in the Arctic again next June. One priority is to send a nest-hunting crew to Prince Charles Island in northwestern Hudson Bay, which was alive with shorebirds when it was last surveyed in the 1990s. Rausch and Smith flew there to scope out locations for a research camp in late July, and as their plane came in to land, they spotted one species they had hoped not to see, says Smith: \u201cThe island was covered with breeding geese.\u201d \n                 Tweet \n                 Follow @NatureNews \n               \n                     The sparrow with four sexes 2016-Nov-23 \n                   \n                     Record-breaking common swifts fly for 10 months without landing 2016-Oct-27 \n                   \n                     Slaughter of the song birds 2016-Jan-26 \n                   \n                     Electronics' noise disorients migratory birds 2014-May-07 \n                   \n                     China\u2019s cordgrass plan is \u2018overkill\u2019 2013-Jul-24 \n                   \n                     Nature  special: After the ice \n                   \n                     State of North America\u2019s Birds 2016: \n                   \n                     Arctic Program for Regional and International Shorebird Monitoring \n                   Reprints and Permissions"},
{"file_id": "541274a", "url": "https://www.nature.com/articles/541274a", "year": 2017, "authors": [{"name": "Alexandra Witze"}], "parsed_as_year": "2006_or_before", "body": "NASA is now building the rover that it hopes will bring back signs of life on the red planet. Adam Steltzner rose to engineering stardom in 2012, when NASA\u2019s Curiosity rover plummeted to a  perfect landing on Mars , thanks to a daring, fiery manoeuvre designed by his team. Now, all Steltzner wants to talk about is how to clean. The object of his sanitary obsession is a dark-grey metallic tube about the size of his hand. It sits on a workbench inside a warehouse-like building at the Jet Propulsion Laboratory (JPL) in Pasadena, California, where Steltzner works as chief engineer for NASA\u2019s next Mars rover. He needs the tube to be one of the cleanest objects ever created so that the rover can complete its mission. As early as July 2020, the 1-tonne, 6-wheeled vehicle will  blast off from Florida , carrying 43 such tubes on a 7-month trip to the red planet. Once it arrives, the rover will drive across the Martian surface and fill each tube with dirt, rock or air. Then it will seal the tubes, place them on the ground, and wait \u2014 for years, or possibly decades \u2014 for another spacecraft to retrieve them and fly them back to Earth. It will be humanity\u2019s first attempt to bring back part of the red planet. If all goes to plan, these will become the most precious extraterrestrial samples ever recovered. Tucked inside one of those metallic tubes could be evidence of life beyond Earth in the form of a microorganism, biominerals or organic molecules. Which is why Steltzner and his team have to be very, very clean. Just one Earth cell or specks of other contaminants would ruin any chance of unambiguously detecting a Martian microbe. So the project team is trying to design a robotic sampling system that will keep things spotless. \u201cWe are going to be more serious about cleanliness than anyone\u2019s been before,\u201d Steltzner says, shaking the tube as if to knock errant microbes off it. \u201cWe\u2019re just going to engineer this shit.\u201d The stakes could not be higher. NASA is gambling $2.4 billion and the future of its Mars exploration programme on the 2020 rover. If it gathers a pristine set of rock samples that eventually return to Earth, they will shape the course of Solar System science. If it fails \u2014 and Mars is notorious as a graveyard for space missions \u2014 the agency will have to relinquish a dream it has had for decades. In conference rooms, laboratories and clean rooms at the JPL, scientists and engineers are now finalizing crucial decisions for the mission. They are exploring and questioning every detail, from how to keep the tubes cool on the Martian surface to what the rover will do every minute on the planet in order to accomplish all of the planned work. Next month is a key period, because NASA will both narrow down the shortlist for possible landing sites and perform a crucial design review that the project must pass to keep moving ahead. By the time the mission blasts off in 2020, its success or failure will have been dictated, in part, by choices being made now. \n               A rover is born \n             On the JPL\u2019s sunny campus, nestled against the mountains north and east of Los Angeles, engineers in shirtsleeves stroll along a eucalyptus-shaded path. Some turn and enter the building from which mission controllers drive the two working rovers on Mars. Others continue on to Building 179, the historic spacecraft-assembly facility where numerous missions to the Moon, Mars and interplanetary space were born. Today, this is also where the Mars 2020 rover is taking shape. So far, the building\u2019s enormous clean room contains only one major item for the mission \u2014 a disc-shaped heat shield, wrapped in a crinkled silvery sheet. It was left over from the Curiosity mission, and will be reused for the new spacecraft. NASA touted this reusability when it  announced the 2020 mission  four years ago. The agency had already sent a string of successful rovers to Mars, beginning with the 11-kilogram  Pathfinder  in 1996, through the 180-kilogram twins known as  Spirit  and  Opportunity  in 2003, to the behemoth 900-kilogram  Curiosity  in 2012. The JPL engineered all of these machines, each a step up in complexity and scientific ambition. But now, from an engineering standpoint, the 2020 rover can partly coast on work done for Curiosity. Roughly 85% of the new rover will be heritage designs \u2014 the chassis, the power and communications systems and other elements can be copied from the previous rover. \u201cWe\u2019re getting a lot of bang for our buck,\u201d says Matt Wallace, a deputy project manager who worked on several rover missions. What\u2019s new are the parts that will do science: the tools that will make measurements on Mars and those that will gather and store the rock samples. The rover\u2019s scientific payload will consist of seven instruments, all either brand new or improved designs. The panoramic camera atop the rover\u2019s mast, for instance, will have a zoom function to zero in on areas of interest. The vehicle\u2019s laser instrument will add extra wavelengths to augment investigations of rock chemistry and mineralogy. And the rover\u2019s robotic arm will sport ultraviolet and X-ray spectrometers that will map rocks in more detail than the instruments on Curiosity can. These tools represent the mission\u2019s only chance to collect geological context for the precious rock samples. That information is key to understanding the Martian material \u2014 and the planet, say scientists. After all, they already have hundreds of rocks from Mars \u2014 but those samples are context-free. They arrived on Earth as  meteorites that had been blasted off the red planet  during impacts millions and billions of years ago. The point in flying there, picking up rocks, and bringing them back is to help researchers to decipher the history of the Martian landscape where the samples were collected and to piece together the planet\u2019s evolution. \u201cWe want a really good set of field notes that people can refer to for centuries,\u201d says Abigail Allwood, principal investigator of the X-ray spectrometer and an astrobiologist at the JPL. \u201cIf we\u2019re going to prove this is life, we\u2019re going to have to scrutinize it at the highest level.\u201d \n               Treasure hunt \n             That\u2019s where Steltzner and his team come in. They started from scratch to try to dream up the best sample-collection system possible. Early ideas included wild configurations, such as a rover with multiple arms to deploy different science instruments. In the end they settled on a system in which the rover will reach out its arm to a rock, then drill and extract a 15-gram sample. It will seal the tube hermetically and stash it back inside the rover\u2019s body \u2014 all in the course of an hour, to reduce the time the samples are exposed to the Martian air and to possible contamination. The rover will carry enough supplies to fill and seal at least 31 tubes, each roughly 14 centimetres long and 2 centimetres across. (It carries several spares in case of problems.) Not all of the tubes are destined to hold Martian samples. Some will serve as \u2018witness\u2019 tubes, filled with material such as aluminium mesh or ceramics to trap environmental contaminants. On the way to Mars, one of the tubes will be left uncovered to capture whatever might vaporize off the spacecraft during flight. That tube will be sealed on arrival. Other tubes will be exposed sequentially on the Martian surface to gather samples of anything that happens to be blowing in the air at each location. Later, scientists will be able to use the witness tubes to work out whether the drilled samples were contaminated and when. Project scientist Ken Farley and the rest of the Mars 2020 team decided to carry the witness tubes only recently, on the advice of a scientific panel that represents the researchers of the future. \u201cWe need to read their minds about what kinds of investigations they will want to do on returned samples,\u201d says Hap McSween, a planetary geologist at the University of Tennessee in Knoxville who co-chairs the panel. Above all, that means ultrapure samples. By the time the tubes are built, cleaned, baked and tucked away inside the spacecraft, they might just be the most pristine environments on this planet. \u201cIt\u2019s the combination of inorganic, organic and biological requirements that makes this so challenging and makes it unique among missions flown by NASA so far,\u201d says Ken Williford, the deputy project scientist. Other spacecraft have accomplished impressive levels of cleanliness, driven by concerns about  not contaminating planets  with Earth microbes. As early as the 1970s, the Viking Mars landers had their key instruments cleaned with solvents and then baked in helium gas for four days. Similar protective cleaning is planned for the  European Space Agency\u2019s ExoMars rover,  which is also slated to launch in 2020 and search for signs of past life. China plans to send its own Mars rover in 2020, but not with life-detection capabilities. NASA\u2019s 2020 mission has to go beyond the usual planetary-protection requirements to ensure the scientific integrity of the samples slated for the return trip to Earth. They will be handled as carefully as \u2014 and perhaps even more so \u2014 the Moon rocks brought back by the Apollo astronauts, says Cassie Conley, NASA\u2019s planetary-protection officer, who works at the agency\u2019s headquarters in Washington DC. Practically speaking, there is no way to get the spacecraft entirely clean. Instead, mission scientists are deciding what levels of contamination they can live with. Both organic and inorganic materials must be kept beneath certain limits \u2014 an advisory panel has recommended no more than 40 parts per billion of total organic carbon in any sample, for instance. But the samples will be unavoidably contaminated with tungsten because the drill teeth are made with tungsten nitride. That means that future scientists won\u2019t be able to date the Martian rocks using a radioactive decay system that relies on tungsten and hafnium; they will have to choose from several other alternatives. \u201cWe\u2019re just going to have to live with that,\u201d says McSween. Another consideration is how hot the tubes might get while sitting on the Martian surface waiting for a flight home. At Farley\u2019s request, McSween and his panel analysed what scientific information would be lost at different temperatures. They concluded that 60 \u00b0C was the acceptable upper limit; above that, some organic compounds begin to degrade, some minerals begin to break down and other changes happen that could compromise the research. So engineers decided to coat the tubes with aluminium oxide to reflect sunlight and keep them beneath that 60\u2009\u00b0C threshold. NASA has not yet planned a mission to bring the samples back, but when they do reach Earth, researchers will have an arsenal of techniques to test for possible Martian life. They will hunt for amino acids, the precursors of proteins, and for other complex organic compounds. Other evidence could come from the ratios of isotopes in key molecules, which on Earth can provide clear signals of biological processes. There is no combination of measurements that researchers agree will prove the existence of Martian life \u2014 but by building up a suite of observations about the rock and what it contains, scientists might be able to make a convincing case. That doesn\u2019t mean it will be easy. Farley is a geochemist who studies how cosmic rays alter the chemistry of rocks. As such, he worries that any organic compounds from ancient Mars would have degraded over millions of years of lying around on the surface. A strategy to get the best samples might be to target areas such as the base of Martian cliffs, where fresh material is exposed when rock from high up breaks off. NASA\u2019s plans call for gathering 20 carefully chosen and documented samples in 1\u20131.5 Martian years. That means driving between many possible sampling locations and assessing which is likely to yield the most intriguing information from a diversity of geological environments. As it collects, the rover will probably stash the tubes on the ground in one or more locations. In its 4.5 years on Mars, Curiosity has drilled just 15 holes and driven more than 16 kilometres. The 2020 team will have to work much, much faster. \u201cIt\u2019s very clear that we have to maintain a very high pace,\u201d says Farley. \u201cThe science team can\u2019t just sit around and discuss, do we want to drill here?\u201d \n               Settling down \n             In the end, the scientific success of the 2020 rover depends heavily on where it lands. NASA is currently considering eight sites. Half of those are in environments featuring  former lakes,  deltas or other evidence of long-standing water and sediments that can preserve evidence of ancient life. The other possible landing sites are among older rocks, where water once percolated through the Martian crust in warm hydrothermal springs that might have allowed life to thrive long ago. Which of them gets selected will shape the direction of Mars science, says Bethany Ehlmann, a planetary geologist at the California Institute of Technology in Pasadena. Scientists will shorten the list by half at a workshop next month in Monrovia, California, and recommend a final site for NASA\u2019s consideration a year or two before launch. Also in February, the mission will go through a final review of all aspects of its design. If it passes, the JPL will continue building the scientific instruments, the sampling system and other hardware in earnest. Once the machine is completed, it will go through testing before being shipped off for a launch in July or August 2020. NASA has yet to decide on when the samples might come home. It has no Mars missions budgeted or approved after the 2020 rover. Managers at the agency\u2019s headquarters have begun to hint that they would like to see an orbiter launch in 2022, to serve as a communications relay for future missions and to replace the ageing orbiters already there. After that, the priority will be to get the Martian samples back to Earth, while also supporting possible plans for human exploration of Mars. NASA is funding early studies into the idea of a Mars ascent vehicle \u2014 which would carry a small package of samples, perhaps the size of a bowling ball \u2014 into Mars orbit. Then an as-yet-unplanned spacecraft would collect the orbiting package and return it to a strict quarantine on Earth. Farley recalls that he first started talking seriously about Mars sample return in the late 1980s. At the time, NASA estimated that it might take a decade to accomplish. It will still be at least a decade before Martian samples will be carried back to Earth, says Farley. \u201cBut at least we\u2019re starting now.\u201d \n                 Tweet \n                 Follow @NatureNews \n                 Follow @alexwitze \n               \n                     NASA rethinks approach to Mars exploration 2016-Oct-06 \n                   \n                     Mars contamination fear could divert Curiosity rover 2016-Sep-07 \n                   \n                     NASA plans Mars sample-return rover 2014-May-13 \n                   \n                     The planetary police 2009-May-20 \n                   \n                     Mars 2020 mission \n                   \n                     Landing-site discussions \n                   Reprints and Permissions"},
{"file_id": "541280a", "url": "https://www.nature.com/articles/541280a", "year": 2017, "authors": [{"name": "Heidi Ledford"}], "parsed_as_year": "2006_or_before", "body": "Where did it come from? How do organisms use it without self-destructing? And what else can it do? Francisco Mojica was not the first to see CRISPR, but he was probably the first to be smitten by it. He remembers the day in 1992 when he got his first glimpse of the microbial immune system that would launch a  biotechnology revolution . He was reviewing genome-sequence data from the salt-loving microbe  Haloferax mediterranei  and noticed 14 unusual DNA sequences, each 30 bases long. They read roughly the same backwards and forwards, and they repeated every 35 bases or so. Soon, he saw more of them. Mojica was entranced, and made the repeats a focus of his research at the University of Alicante in Spain. It wasn't a popular decision. His lab went years without funding. At meetings, Mojica would grab the biggest bigwigs he could find and ask what they thought of the strange little repeats. \u201cDon't care about repeats so much,\u201d he says that they would warn him. \u201cThere are many repeats in many organisms \u2014 we've known about them for years and still don't know how many of them work.\u201d Today, much more is known about the clustered, regularly interspaced short palindromic repeats that give CRISPR its name and help the CRISPR\u2013Cas microbial immune system to destroy invading viruses. But although most in biomedicine have come to revere the mechanics of the system \u2014 particularly of a version called CRISPR\u2013Cas9 \u2014 for the ways in which it can be harnessed to edit genes, Mojica and other microbiologists are still puzzling over some basic questions about the system and how it works. How did it evolve, and how did it shape microbial evolution? Why do some microbes use it, whereas others don't? And might it have other, yet-to-be-appreciated roles in their basic biology? \u201cA lot of the attention paid to CRISPR systems in the media has really been around its use as a technology \u2014 and with good reason. That's where we're seeing incredible impact and opportunities,\u201d says Jennifer Doudna, a molecular biologist at the University of California, Berkeley, and one of the first scientists to  reveal CRISPR\u2013Cas's agility as a gene-editing tool . \u201cAt the same time, there's a lot of interesting fundamental biology research to be done.\u201d \n               Where did it come from? \n             The biological advantages of something like CRISPR\u2013Cas are clear. Prokaryotes \u2014 bacteria and less-well-known single-celled organisms called archaea, many of which live in extreme environments \u2014 face a constant onslaught of genetic invaders. Viruses outnumber prokaryotes by ten to one and are said to kill half of the world's bacteria every two days. Prokaryotes also swap scraps of DNA called plasmids, which can be parasitic \u2014 draining resources from their host and forcing it to self-destruct if it tries to expel its molecular hitch-hiker. It seems as if nowhere is safe: from soil to sea to the most inhospitable places on the planet, genetic invaders are present. Prokaryotes have evolved a slew of weapons to cope with these threats. Restriction enzymes, for example, are proteins that cut DNA at or near a specific sequence. But these defences are blunt. Each enzyme is programmed to recognize certain sequences, and a microbe is protected only if it has a copy of the right gene. CRISPR\u2013Cas is more dynamic. It adapts to and remembers specific genetic invaders in a similar way to how human antibodies provide long-term immunity after an infection. \u201cWhen we first heard about this hypothesis, we thought that would be way too sophisticated for simple prokaryotes,\u201d says microbiologist John van der Oost of Wageningen University in the Netherlands. Mojica and others deduced the function of CRISPR\u2013Cas when they saw that DNA in the spaces between CRISPR's palindromic repeats sometimes matches sequences in viral genomes. Since then, researchers have worked out that certain CRISPR-associated (Cas) proteins add these spacer sequences to the genome after bacteria and archaea are exposed to specific viruses or plasmids. RNA made from those spacers directs other Cas proteins to chew up any invading DNA or RNA that matches the sequence (see 'Lasting protection'). How did bacteria and archaea come to possess such sophisticated immune systems? That question has yet to be answered, but the leading theory is that the systems are derived from transposons \u2014 'jumping genes' that can hop from one position to another in the genome. Evolutionary biologist Eugene Koonin of the US National Institutes of Health in Bethesda, Maryland, and his colleagues have found 1  a class of these mobile genetic elements that encodes the protein Cas1, which is involved in inserting spacers into the genome. These 'casposons', he reasons, could have been the origin of CRISPR\u2013Cas immunity. Researchers are now working to understand how these bits of DNA hop from one place to another \u2014 and then to track how that mechanism may have led to the sophistication of CRISPR\u2013Cas. \n               How does it work? \n             Many of the molecular details of how Cas proteins add spacers have been worked out in fine detail 2  in recent years. But viral DNA is chemically nearly identical to host DNA. How, in a cell packed with DNA, do the proteins know which DNA to add to the CRISPR\u2013Cas memory? The stakes are high: if a bacterium adds a piece of its own DNA, it risks suicide by autoimmune attack, says Virginijus Siksnys, a biochemist at Vilnius University in Lithuania. \u201cThese enzymes are a double-edged sword.\u201d It may be that populations of bacteria and archaea can absorb some error, says Rodolphe Barrangou, a microbiologist at North Carolina State University in Raleigh. A few cellular suicides may not matter if other cells can thrive after a viral attack. In fact, when viruses infiltrate a bacterial ecosystem, often only about one bacterium in 10 million will gain a spacer that lets it defend itself. Those odds make it hard to study what drives spacer acquisition, and to learn why a cell succeeded where others failed. \u201cIt's difficult to catch that bacterium when it actually is happening,\u201d says Luciano Marraffini, a microbiologist at the Rockefeller University in New York City. Sorting out how suitable spacers are recognized \u2014 and boosting the rate at which they are incorporated \u2014 could be useful. Some work has shown that cells containing CRISPR\u2013Cas machinery could serve as a recording device of sorts, cataloguing DNA and RNA sequences that they have encountered 3 . This might allow researchers to track a cell's gene expression or exposure to environmental chemicals over time. Researchers would also like to learn how old memories are pruned from the collection. Most microbes with CRISPR\u2013Cas systems contain a few dozen spacers; some have only one. The archaeon  Sulfolobus tokodaii , by contrast, dedicates 1% of its genome to its 5 CRISPR\u2013Cas systems, including 458 spacers. There may be little incentive to hang on to old spacers: if a virus mutates to avoid CRISPR\u2013Cas, a spacer becomes obsolete. And it can be a burden for microbes to retain extra DNA. \u201cA bacterium cannot inflate its genome forever,\u201d says Rotem Sorek, a geneticist at the Weizmann Institute of Science in Rehovot, Israel. \n               What else might it be doing? \n             The origin of some spacers presents another mystery. Less than 3% of spacers observed so far match any known sequences in DNA databases. It could be a reflection of how little is known about viruses. Most sequencing efforts have concentrated on those that infect people, livestock or crops. \u201cWe know very little about the enemies of bacteria, and especially the enemies of crazy archaea,\u201d says Michael Terns, an RNA biologist at the University of Georgia in Athens. It is also possible that some spacers are the ghosts of viruses no longer around or mutated beyond recognition. But a third possibility has the field buzzing. Researchers have found examples of CRISPR\u2013Cas systems doing more than warding off genetic intruders. In some bacteria, CRISPR\u2013Cas components control DNA repair, gene expression and the formation of biofilms. They can also determine a bacterium's ability to infect others:  Legionella pneumophila , which causes Legionnaires' disease, must have the Cas protein Cas2 in order to infect the amoeba that is its natural host. \u201cA major question is how much biology is there that goes beyond defence,\u201d says Erik Sontheimer, a molecular biologist at the University of Massachusetts Medical School in Worcester. \u201cThat is something where there's still quite a few shoes to drop in the coming years.\u201d Sontheimer adds that it creates an enticing parallel with the discovery of RNA interference, a system that silences gene expression in plants, animals and other non-prokaryotic organisms. RNA interference was also primarily thought of as a defence mechanism early on, and it was only later that researchers noticed its role in regulating host gene expression. This could also explain why some spacers do not match known viruses or plasmids, says Stan Brouns, a microbiologist at Delft University of Technology in the Netherlands. \u201cThe systems are not tuned to be perfect: they grab the viral DNA as well as their own,\u201d he says. \u201cAs soon as they start pulling in new pieces of DNA, they can gain new functions \u2014 if they don't die.\u201d \n               Why do only some microbes use it? \n             Whatever other functions CRISPR\u2013Cas has, it is clear that some microbes use it more than others. More than 90% of archaea have CRISPR-based immunity, whereas only about one-third of sequenced bacteria bother with it, says Koonin. And no non-prokaryotic organisms, even single-celled ones, have been caught troubling with CRISPR\u2013Cas at all. One archaeon, called  Nanoarchaeum equitans , lives as a parasite on another archaeon in near-boiling waters and has dispensed with many of its genes related to energy production and general cellular housekeeping. Yet in its minuscule, 490,000-letter DNA instruction manual,  N. equitans  has held on to a CRISPR\u2013Cas system with about 30 spacers. \u201cA big chunk of its genome is still dedicated to CRISPR,\u201d says Malcolm White, a molecular biologist at the University of St Andrews, UK. \u201cCRISPR must be so important, yet we don't really know why.\u201d Such differences suggest that there are key ecological factors that favour CRISPR\u2013Cas systems, prizing viral defence \u2014 or other benefits \u2014 over the risks of cellular suicide, says Edze Westra, a microbiologist at the Penryn campus of the University of Exeter, UK. Extreme environments seem to favour CRISPR\u2013Cas systems, but Westra notes that the frequency of such systems also varies among bacteria in more-hospitable habitats. The bird pathogen  Mycoplasma gallisepticum , for example, tossed out its CRISPR\u2013Cas equipment when it switched hosts from chickens to wild finches. Why the system was useful in a chicken but not a finch is anyone's guess, says Westra. Mathematical models and some early laboratory experiments suggest that CRISPR\u2013Cas may be more of an advantage when there are only a few types of virus to contend with 4 , 5 . CRISPR\u2013Cas spacers can record a limited number of viral sequences before the added DNA becomes a genomic burden. If the diversity of viruses in the environment greatly outweighs the number of possible spacers, CRISPR\u2013Cas systems may be of little use, says Koonin. Another possibility is that archaea in extreme environments cannot rely as heavily on other means of defence. One common way for bacteria to thwart invaders is to mutate the proteins found in their own outer casing, called an envelope. Some archaea, however, may have less freedom to tinker with these envelopes because the envelopes' structure is so crucial to the organism's survival in harsh conditions. \u201cThis makes alternative systems such as CRISPR more relevant,\u201d says Mojica. \n               How many flavours of CRISPR\u2013Cas exist? \n             Humans tend to focus on the CRISPR\u2013Cas9 system, which is prized for its simplicity and versatility in genome editing, but microbes don't play favourites. Instead, they tend to mix and match different systems, quickly picking up new ones from other bacteria and discarding the old. Researchers have officially recognized 6 different types of CRISPR system, with 19 subtypes. \u201cAnd we really only know how a fraction of them actually work,\u201d says Marraffini. Unravelling those mechanisms could hold the key to finding  new biotechnological applications for CRISPR\u2013Cas systems . The beloved CRISPR\u2013Cas9, for example, is a type II system, which uses RNA molecules transcribed from spacer sequences to direct an enzyme to cut invading viral or plasmid DNA. But enzymes in type VI systems \u2014 discovered last year 6  \u2014 cut up RNA rather than DNA. And type IV systems contain some genes associated with CRISPR\u2013Cas, but lack the repeats and the machinery to insert spacers. Type III systems are among the most commonly found CRISPR\u2013Cas systems in nature \u2014 and among the least understood. Evidence so far suggests that they respond not to the invading DNA or RNA itself, but to the process of transcribing DNA into RNA. If that proves to be the case, it would be a new form of regulation that could expand the CRISPR\u2013Cas toolbox for genome editing, says Doudna. Other systems may yet crop up, particularly as researchers extend their search beyond microbes that have been grown in culture, to include  genetic sequences from environmental DNA samples . \u201cWe have already said a couple of times that we reached the end,\u201d says van der Oost \u2014 only to be surprised when a new CRISPR\u2013Cas system surfaced. For Mojica, exploring that diversity and answering basic questions about CRISPR systems hold more allure than the revolution they sparked. This puzzles many of his colleagues, he says. He has immersed himself in CRISPR\u2013Cas biology for a quarter of a century, and although there's a lot of funding available for those who wish to edit genomes, there is considerably less for the kind of work he does. \u201cI know that it's a great tool. It's fantastic. It could be used to cure diseases,\u201d says Mojica. \u201cBut it's not my business. I want to know how the system works from the very beginning to the end.\u201d \n                 Tweet \n                 Follow @NatureNews \n                 Follow @heidiledford \n               \n                     First CRISPR clinical trial gets green light from US panel 2016-Jun-22 \n                   \n                     CRISPR: gene editing is just the beginning 2016-Mar-07 \n                   \n                     Where in the world could the first CRISPR baby be born? 2015-Oct-13 \n                   \n                     CRISPR, the disruptor 2015-Jun-03 \n                   \n                     Nature  special: CRISPR \n                   Reprints and Permissions"}
]