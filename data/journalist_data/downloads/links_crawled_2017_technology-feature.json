[
{"file_id": "d41586-017-07528-7", "url": "https://www.nature.com/articles/d41586-017-07528-7", "year": 2017, "authors": [{"name": "Brian Owens"}], "parsed_as_year": "2006_or_before", "body": ""},
{"file_id": "545511a", "url": "https://www.nature.com/articles/545511a", "year": 2017, "authors": [{"name": "Amber Dance"}], "parsed_as_year": "2006_or_before", "body": "Microfluidic devices filled with intricate channels that exploit fluid behaviour promise to make it easier to diagnose genetic disease. The bioengineers in Dino Di Carlo's lab at the University of California, Los Angeles, spend a lot of time wrapped in head-to-toe suits and looking a bit jaundiced. The engineers work in a clean room, where a steady flow of filtered air removes particulates. Blue or purple light would harden the photosensitive material with which they work, so they limit lighting in the room to butter-yellow. They and others in the field are building tools for preparing and analysing blood and other fluid samples to diagnose genetic anomalies, such as the mutations carried by cancer cells. Few such tools require a clean room, but these ones depend on the ability of fluids to travel through channels so small that even one speck of dust could block them \u2014 a field of technology development called microfluidics. In theory, these assays, encapsulated in chips the size of a microscope slide, could allow for rapid and automatic diagnosis: sample in, answer out; so easy that a novice could use it. In practice, the devices rarely work this way, and usually, some pre-processing of the sample is required. Researchers such as Di Carlo are working to address those shortcomings, making the chips easier to manufacture and experimenting with materials and designs. They are tackling challenges such as predicting the behaviour of fluids in small places, and determining how to make the chips both effective and affordable. Solving these problems requires an interdisciplinary approach, notes Amy Shen, a chemical engineer at the Okinawa Institute of Science and Technology Graduate University in Japan. The payoff could run from cost and time savings in the lab to medical devices that speed diagnosis of genetic and infectious diseases. Microfluidic circuits enable scientists to work with samples that are precious or in limited supply, and to squeeze more results out of expensive reagents. Working with minute volumes makes it possible to conduct many analyses in parallel \u2014 and often rapidly. Because only machines can manipulate such tiny volumes, microfluidics is conducive to automation, which reduces human error. Ideally, even minimally trained technicians would be able to perform testing. That goal remains elusive. Developers so far have focused on miniaturizing processes used to analyse DNA or RNA in blood and other bodily fluids, such as by creating miniaturized versions of polymerase chain reaction (PCR) machines to copy and quantify rare gene sequences, or hybridizing to link nucleic acids with fluorescent probes. As a result, the microchip method often requires biological materials to have already undergone some processing, for example, to remove components that would interfere with the reactions. The major bottleneck, says Jean-Louis Viovy, research director at France's basic-research agency, the CNRS, in Paris and scientific founder of the nearby microfluidics company Fluigent, is \u201ctrying to expand the toolbox of microfluidics to be able to go from the real sample to the results, all in microfluidics\u201d. \n               Chipping away at disease \n             Di Carlo's lab developed a method 1  for a specific kind of sample preparation: isolating circulating tumour cells (CTCs) \u2014 bloodborne hallmarks of cancer that can reveal a tumour's origin and the mutations that make it tick. To produce the chips, the lab uses a common technique called photolithography to make microchips out of PDMS, a transparent rubber. In the clean room, engineers spread a liquid mixture onto a circular plate of silicon \u2014 the material used for computer microchips. Then, mimicking the semiconductor industry, they cover the polymer with a printed black 'photomask' that contains clear portions in the shape of their desired channels. They then expose it to ultraviolet light to harden the liquid in only the exposed sections, creating an inverse cast of the chip. Di Carlo's engineers then move to their normal lab. To create a chip, they pour liquid PDMS over the cast and bake it at 65 \u00b0C to harden it. Finally, they fuse a glass slide to the bottom of the PDMS, creating a chip prototype that has the look and feel of clear, extra-firm jelly. The whole procedure takes about a day. Once they settle on a design that works for their purposes, they order plastic versions of the chips, made using the same process used to manufacture plastic toys, says Di Carlo. Most techniques for fabricating microchips produce 2D designs. But sometimes, a 3D structure is valuable. In one chip design he's working on, Di Carlo uses a magnetic field to pull liquid from a narrow channel into a higher, wider one. As the fluid begins to expand in the larger chamber, surface tension causes it to form a sphere, which buds off as a droplet. \u201cThis now is a nanolitre pipette, basically, and that's impossible to do by hand,\u201d says Di Carlo. Such partitioning enables the chip to separate fluids such as blood into multiple, discrete reaction chambers so that many tests can be performed simultaneously 2 . boxed-text To make a 3D chip, scientists have generally had to stack successive layers of a polymer into the photolithography moulds. But 3D printing is changing that, because it requires neither much expertise nor much equipment, say the designers of one entry-level method. Vittorio Saggiomo, a chemist at Wageningen University in the Netherlands, happened upon the idea at home. Saggiomo 3D-prints plastic tools, such as small lamps or pipette holders, as well as fun stuff, such as bird houses. One day, he submerged a 3D-printed  Star Wars  helmet in acetone to smooth out the surface, but left it in too long \u2014 and the whole piece dissolved. He realized that he could fashion microchannels in the same way. Saggiomo and his colleague Aldrik Velders, also a chemist, adapted the process for the lab. They use a 3D printer to create the shape of their desired channel, and suspend that piece of plastic in PDMS. They then soak it in acetone overnight to dissolve the plastic, which leaves behind a ready-to-use microchip 3 . Saggiomo and Velders are playing with this strategy, producing coils or interwoven channels that would otherwise be difficult to make. For example, they designed a chip with a straight channel surrounded by a coiled one. Users could run hot or cold liquid through the coil, says Saggiomo, and thereby change the temperature of a sample in techniques such as PCR. Even with standard manufacturing procedures, chip designers are getting creative, using channel layouts such as chevrons, angles and squiggles. And although the field is starting to develop standardized designs, Di Carlo says, there's a lot of room for variety in planning the fluid's narrow path. Chip designers also struggle to predict fluid dynamics at this level. \u201cThe underlying physics at this scale, it's completely different from the water in your bathtub,\u201d says Walter Minnella, an engineer at the Elvesys Innovation Center, a microfluidics company in Paris. Some forces, such as gravity, become negligible, whereas the high surface-area-to-volume ratio gives surface tension and the interaction between the fluid and channel walls an outsized influence. Aqueous solutions turn viscous, similar to honey, but there's no turbulence, says Di Carlo. As a result, fluid motion becomes predictable \u2014 but it still might take a supercomputer a day or two to solve, Di Carlo estimates, which makes repeated simulations impractical. Most scientists opt instead for an empirical approach: build, test, repeat (see 'Tips for chips'). \n               Blood breakdown \n             Mechanical engineer Shannon Stott at Massachusetts General Hospital in Charlestown and her team built multiple iterations of one chip before settling on its current form. They are pursuing liquid biopsies, a method for detecting and diagnosing disease from genetic clues in the blood. Their goal was to create a system that can purify and analyse CTCs from a minimally invasive blood sample 4 . They call their design the CTC-iChip \u2014 'i' for 'inertial focusing', the technique used to line cells up in single file so that the chip can separate out the CTCs from other blood cells (see 'Anatomy of a chip'). Among other things, the chip enables the team to count CTCs in patient blood samples and study their genetic composition. Built out of plastic, the CTC-iChip consolidates three steps into one device. In the first stage, the chip eliminates unwanted blood components. Scientists label white blood cells with magnetic beads and then send the fluid through a chamber containing a series of plastic posts. The smaller bits, such as red blood cells and proteins, whizz through like a moth flying through a dense forest. The larger cells \u2014 the white blood cells and the rarer CTCs \u2014 are more like lumbering bears. As they bounce off the posts, the large cells are funnelled into stage two \u2014 the S-curves, or 'wigglers', as Stott calls them, which line the cells up in single file. In stage three, the device uses a magnet to yank the white blood cells out of line, leaving behind the CTCs. Di Carlo's lab has developed its own microfluidic methods to sort blood samples 1 , using channels dotted with a series of side chambers, like transepts in a church. His former student SJ Claire Hur, a mechanical engineer now at Johns Hopkins University in Baltimore, Maryland, noticed that bigger cells become trapped in vortices created by the widening of a microfluidic channel, much as leaves and rubbish accumulate around bends or rocks in a river 5 . The group designed a system, now manufactured by Vortex Biosciences in Menlo Park, California, that exploits this property to isolate CTCs for further analysis. The researchers are running clinical studies using the Vortex machine to identify markers on CTCs that might indicate how well a tumour will respond to specific immunotherapies. The Vortex microchip itself fits in the palm of Hur's hand, but the system also includes external tubing and pumps to feed the sample through the system, plus fraction collectors to recover the purified CTCs. The whole apparatus is a bit bigger than a microwave, making it less of an all-in-one 'lab-on-a-chip' \u2014 as many scientists want \u2014and more of a 'chip-in-a-lab'. Often, a chip-in-a-lab design is just fine, says Di Carlo. It still saves money over conventional methods and improves results by minimizing experimenter variation. But a true lab-on-a-chip device would make rapid genetic testing possible for clinics or field stations in developing countries, where purchasing and running PCR machines or centrifuges to separate blood samples might be impractical. \n               Labs on the go \n             Engineers have come up with a variety of possible solutions. Some, for example, are developing inexpensive devices made out of paper, which can amplify and detect the genes of infectious microbes in blood samples 6 . Hain Lifescience of Nehren, Germany, has designed strip-based tests that can detect specific DNA sequences. Some can identify a person's risk for Alzheimer's disease by searching for variants of a gene called  APOE . Another can report on genes related to ankylosing spondylitis, a form of arthritis that affects the spine. Syed Hashsham, an environmental engineer at Michigan State University in East Lansing, is developing a chip-based device for genetic diagnosis in the fields of cancer and infectious disease. \u201cWe have to simplify everything,\u201d he says. To make production cheaper, and to enable field scientists to make the chips airtight, he switched from silicon-based chips, which were difficult to seal under field conditions, to plastic ones that are cut using lasers and sealable with film. Another challenge was how to amplify rare genetic material enough for it to be detected in the field. The standard method, PCR, requires repeatedly heating and cooling a sample to precise temperatures. But it's difficult to design a small, portable, inexpensive machine that can switch between those temperatures, says Hashsham. \u201cIn the field, thermocycling never works,\u201d he says. He adopted an alternative method of sequence amplification to use in his handheld microfluidic 'Gene-Z' device, which identifies and quantifies known sequences such as the microRNAs that indicate cancer, or the genes of infectious organisms. Called loop-mediated isothermal amplification, the reaction uses a different enzyme from the one in PCR, and requires no temperature cycling. Researchers mix a body-fluid sample, such as spit, with a fluorescent dye that will be incorporated into any DNAs made in the reaction, and then use a syringe to push it into a channel leading to 16 individual chambers. There, the DNA-amplification reagents are preloaded, dried and ready. After the reactions are complete, the device uses light-emitting diodes and sensors to detect the dyes, which indicate a positive reaction 7 . The whole device runs off an iPod Touch and costs no more than US$200 to make, says Hashsham. Each disposable chip, comprising 64 chambers, for a total of four samples, costs less than a dollar. He has validated Gene-Z's results for more than 100 diseases. The challenge now, he says, is persuading funders to manufacture a device that won't be immediately profitable, because he wants to deploy it in regions such as Africa, where quick diagnoses could change the practice of medicine and save lives. It can be difficult to translate ideas into the commercial space, agrees Shen, who points out that companies might not go for a design if it's too expensive or doesn't work with their existing manufacturing process. That leaves microfluidics a long way from the lab-on-a-chip promise. \u201cThere is still a gap, but I think we're slowly bridging that gap,\u201d she says. \u201cEventually, we will get there.\u201d \n                     Single-cell barcoding and sequencing using droplet microfluidics \n                   \n                     Microfluidic platform combining droplets and magnetic tweezers: application to HER2 expression in cancer diagnosis \n                   \n                     Microfluidics-assisted fluorescence in situ hybridization for advantageous human epidermal growth factor receptor 2 assessment in breast cancer \n                   \n                     Microfluidic diagnostic technologies for global public health \n                   \n                     Microfluidics: reframing biological enquiry \n                   \n                     Microfluidic cell isolation technology for drug testing of single tumor cells and their clusters \n                   \n                     Latest research & reviews on microfluidics \n                   \n                     Nature  Technology Features hub \n                   \n                     Hashsham lab \n                   \n                     Fluigent \n                   \n                     Dolomite \n                   \n                     Fluidigm \n                   \n                     Agilent Technologies \n                   \n                     Veldink/Saggiomo protocol on on YouTube \n                   \n                     uFlow \n                   \n                     Vortex Biosciences \n                   \n                     Shen lab \n                   \n                     OIST workshop on microfluidics \n                   Reprints and Permissions"},
{"file_id": "546687a", "url": "https://www.nature.com/articles/546687a", "year": 2017, "authors": [{"name": "XiaoZhi Lim"}], "parsed_as_year": "2006_or_before", "body": "Researchers are exploiting the structural properties of DNA to build nanoscale models for use in medicine and materials science. Vincent van Gogh's  The Starry Night  is a classic of post-Impressionist art. Its whimsical whorls have entranced art lovers since the Dutch artist painted it in 1889. In 2016, Ashwin Gopinath, a bioengineer at the California Institute of Technology in Pasadena, recreated the work. But instead of oils, he drew his copy in DNA. Drawn on a silicon chip, Gopinath's creation demonstrates the growing power of a once-obscure branch of materials science: DNA nanotechnology. The field emerged in the 1990s when scientists began to dream up nanoscale machines. Today, more than 300 research groups are trying to harness the base-pairing properties of DNA, with the goal of manipulating the molecule as if it were a building material, rather than a carrier of genetic information. \u201cOnce we started to realize that you can use the information in DNA to organize stuff, it started a cascade of activity,\u201d says Ned Seeman, a synthetic chemist at New York University who is widely acknowledged to be the founder of DNA nanotechnology. DNA's dimensions make it ideal for building nanostructures: the double helix is a flexible, configurable rod, 2 nanometres wide, with a twist that repeats every 3.4\u20133.6 nm. Researchers have exploited the well-characterized structure, and the ease of synthesizing custom DNA, to build ever-more-elaborate designs for applications from drug delivery and diagnostics to nanofabrication. But challenges remain, and nanotechnologists are rethinking the fundamentals of building with DNA. \n               Construction strategies \n             The collection of shapes assembled from DNA ranges from 2D smiley-faced emojis to 3D geometrical objects and blocks of alphabetic characters. But the underlying technology is based on one simple rule: base-pair complementarity. Driven by hydrogen bonds that pair the bases adenine and thymine, and cytosine and guanine, complementary DNA strands will spontaneously form a double helix. In nature, the two strands are usually fully complementary. If strands are only partially complementary, however, both can accept multiple DNA partners. This concept, is the foundation of DNA nanotechnology, says Paul Rothemund, Gopinath\u2019s supervisor. During cell division, DNA forms a four-armed intermediate structure known as a Holliday junction. The structure is unstable and disintegrates quickly into two double helices. In the early 1980s, Seeman managed to stabilize it 1  by pairing each strand's sequence with another at the junction. He went on to produce a junction with six strands, forming the first branched DNA structure in 3D. A series of increasingly complex designs followed: a stick cube in 1991, branched DNA crystals in 1998 and DNA tubes in 2005. In 2004, William Shih, a biochemist now at the Wyss Institute for Biologically Inspired Engineering at Harvard University in Boston, Massachusetts, took a different approach. He formed a 22-nm-wide octahedron from just a single strand of DNA 2 . The 1,669-base DNA strand was held in shape using five 40-base strands. Building on this idea, two years later, Rothemund used hundreds of 26- to 32-base segments of DNA that he called staples to guide the folding of a 7-kilobase 'scaffold' strand into a variety of 2D shapes roughly 100 nm in diameter 3 . This was \u201ca landmark achievement\u201d, says DNA scientist Peng Yin, also at the Wyss Institute, because it greatly increased the complexity and size of DNA nanostructures. Rothemund built his structures using the single-stranded DNA of a virus as a scaffold \u2014 the DNA required was too long for conventional oligonucleotide synthesis. He worked out how the DNA could be folded and where the 200 or so staples would need to attach to form shapes such as squares, triangles, stars and smiley faces. By mixing the DNA with a 100 times more staples than were needed, heating to 95 \u00b0C and cooling to room temperature over 2 hours, the shapes formed spontaneously on the basis of the instructions programmed into their sequences. DNA 'origami' has come a long way since then. Initially, says Shawn Douglas, a biophysicist at the University of California, San Francisco, it could take an entire month to work out where the folds and staples go for just one design. \u201cIt was easy to make mistakes,\u201d he says, \u201cand also hard to make modifications.\u201d This challenge inspired Douglas to develop software to accelerate origami design (see  'DNA origami' ). The first working version of caDNAno was built in 2009, while Douglas was completing his PhD in Shih's group at Harvard. The software cut origami design to one day 4 . \u201cIn the next 3 months, we made 30 shapes,\u201d says Douglas, including protractors, twisted ribbons and an octahedral globe. A couple of years later, another team, led by biophysicist Mark Bathe at the Massachusetts Institute of Technology in Cambridge, developed an ancillary tool called CanDo 5  to check the DNA origami blueprint from caDNAno. \u201cIt will tell you what it thinks the structure looks like in 3D,\u201d says Bathe. Bathe's group has since developed a tool called DAEDALUS that tells users all the sequences, including the scaffold, they need just by entering a desired geometry 6 . Another way to build with DNA is using DNA bricks. In 2012, while a postdoctoral fellow in Shih's lab, Yonggang Ke, a biochemist now at Georgia Institute of Technology and Emory University in Atlanta, developed a technique in which every brick in a DNA nanostructure has a unique sequence of 32 or 42 bases. A quarter of each sequence is complementary to another quarter on a different brick. By connecting and extending the bricks, researchers can assemble a canvas like building a brick wall. \u201cEach brick can bind to two at the top and two at the bottom,\u201d Yin explains. For a flat, 2D canvas, the bricks contain 10.5 bases per quarter, which allows them to connect to each other in a single plane; any 2D pattern can be prepared by simply picking the correct bricks. To add a third dimension, Ke shortened the bricks to eight bases per quarter, which forced them to connect perpendicularly. The researchers produced 102 distinct structures, including hearts, spheres and the Roman alphabet 7 . \u201cIn that first paper, we produced more 3D structures than the whole field combined,\u201d says Yin. \n               Nanofabrication applications \n             One use for these novel DNA shapes is to carry materials such as drug molecules, metal nanoparticles and proteins. Positioning these useful materials on the DNA is generally easiest before it is coaxed into a structure. The cargo is typically carried on the staple strands, and because each structure can include some 200 staples, says Rothemund, they offer plenty of opportunities to precisely place the molecular cargo. DNA molecules are charged, which means that nanostructures can be arranged electrostatically by etching a pattern of negatively charged binding sites on a flat surface using an electron beam. \u201cYou can get them exactly where you want, oriented how you want,\u201d says Rothemund. This is just what his team demonstrated when it recreated  The Starry Night  from a dense array of photonic crystal cavities \u2014 micrometre-sized devices in which light can resonate \u2014 that contained meticulously placed DNA nanostructures carrying dyes 8 . Another idea is to cast nanoparticles using DNA nanostructures as the mould. This requires fairly large and stiff DNA nanostructures with internal cavities. In collaboration with Bathe's team, Yin's group built such structures using DNA bricks. The teams then introduced silver nanoparticle seeds into the cavities, and allowed them to develop in the presence of soluble silver, like rock sugar growing in supersaturated solution. The seeds developed to fill the cavities, producing cubic, spherical, triangular and Y-shaped nanoparticles 9 . Chad Mirkin, a chemist at Northwestern University in Evanston, Illinois, is pursuing yet another nano-strategy, which he calls programmable atom equivalents. These nanoparticle cores can range from metals and polymers to proteins. Hundreds of partially double-stranded DNA molecules are attached to the core's surface to form a dense DNA shell. The single-stranded free ends are complementary to the free ends of other 'atom equivalents'. When those structures are mixed together, they link up and extend into a crystal lattice that positions the desired atoms precisely in space. \u201cThis is an incredibly reliable method,\u201d says Mirkin. Remarkably, the crystal's structure and properties can be controlled by varying the sizes and shapes of the nanoparticle cores, as well as the length of the DNA strands \u2014 no small achievement, given that crystallization processes are notoriously tricky. \u201cWe are trading ill-defined materials chemistry for well-defined and programmable DNA interactions to form high-quality crystals, and we can guide it down a path,\u201d says Mirkin, whose research group has churned out more than 40 crystal symmetries, 6 of which have never been observed in nature. \n               Nano cargo \n             One popular adornment to nanostructured DNA is light-emitting materials called fluorophores. GATTAquant DNA Nanotechnologies in Braunschweig, Germany, for instance, makes nanorulers from DNA origami structures and fluorescent molecules to validate super-resolution microscopes. Super-resolution microscopy allows researchers to take images beyond the resolution limit set by the diffraction of light, but \u201cthere is no standard to measure the resolution of the system,\u201d says Max Scheible, head of research and development at GATTAquant. \u201cDNA nanotechnology really enabled this.\u201d GATTAquant attaches fluorescent molecules at precise distances on an origami structure and mounts them on glass slides. These nanoscale rulers allow researchers to verify the resolution of sub-diffraction-limit microscopes. The co-founders of Ultivue, a start-up company in Cambridge, Massachusetts, are hoping to use nanostructures to make an impact in cancer research. In cancer tissues, biomarkers such as the proteins BRCA1 and HER2 can herald the onset or progression of disease, and can potentially aid diagnosis, prognosis and treatment. Until now, most biomarkers have been studied in isolation. \u201cWhat's missing is a fingerprint\u201d of biomarkers as they are seen in cancerous tissue, says Mael Manesse, lead researcher at Ultivue. At Ultivue's headquarters, Manesse demonstrates the company's technology. Lit on the computer monitor are cells from a thin slice of lung tissue that Manesse has positioned under a microscope. When he switches the microscope's light to red, the cells disappear. In their place is a smattering of bright spots, indicating CD3 \u2014 a biomarker for immune cells called T cells. These proteins are marked with Ultivue's DNA-based imaging probe: a short 'docking' strand attached through an antibody, and its complementary 'imaging' strand carrying a fluorescent dye. Each biomarker of interest has its own docking strand; the complementary imaging strands can be added, imaged and removed one at a time. The images are then superimposed to obtain a composite picture of the tissue. This allows almost unlimited numbers of biomarkers to be studied, but the tissue sample remains preserved, says Manesse. DNA nanostructures can also be used to build sensors, drugs and vaccines for therapeutic or diagnostic applications. For example, researchers have made a synthetic vaccine by anchoring the antigen streptavidin and oligonucleotides with an immune-response-boosting, repeating cytosine\u2013guanine motif on tetrahedral DNA nanostructures 10 . In mouse studies, the vaccine produced higher levels of antibodies against streptavidin than a mixture of just streptavidin and oligonucleotides. Eventually, Shih hopes to make drug nanofactories: DNA origami nanocapsules that can produce drugs on demand inside the body using building blocks from the cell. \u201cIt is very exploratory at this point,\u201d he says. In theory, the nanocapsules would hold RNA polymerase \u2014 an enzyme that makes RNA \u2014 and DNA templates. Once triggered, it would begin manufacturing and releasing its payload, like a virus using cellular materials to replicate itself. \n               DNA alternatives \n             Although well into its third decade, DNA nanotechnology still faces a number of challenges. One key obstacle, says biophysicist Hendrik Dietz at the Technical University of Munich, Germany, is production yield: researchers have yet to break into gram-scale synthesis. \u201cDNA origami can make a very big difference in health,\u201d Dietz says. \u201cBut the problem is we can't even make quantities that are big enough to use.\u201d Another hurdle is the limited variety of materials that can be attached to DNA. Researchers are working to expand origami designs to use materials other than DNA. Earlier this year, for example, Dietz reported the preparation of DNA structures that fold using protein staples 11 , and Douglas is updating caDNAno to include RNA and protein building blocks. Perhaps the biggest limitation is the lack of control over the self-assembly process. As structures get larger, the chances of misfolding increase. \u201cWe need new strategies to suppress self-assembly errors,\u201d says Shih. One possibility, Rothemund suggests, would be to move away from the standard  in vitro  method of mixing, heating and cooling, and allow cells to build the structures instead. Last year, bioengineer Christopher Voigt at the Massachusetts Institute of Technology engineered the bacterium  Escherichia coli  to produce a simple, branched, four-part junction from single-stranded DNA 12 . But for more-complex origami nanostructures, Rothemund says, a shift to RNA may be necessary. Unlike DNA, single-stranded RNA can hold its shape without staples. Building with RNA is largely uncharted territory, but Rothemund is excited to explore it. \u201cIt is like building with wood, but now you can't use nails or notches or glue,\u201d he says. \u201cWe still need to learn a lot of things.\u201d \n                     DNA nanotechnology 2.5 \n                   \n                     Hybrid nanostructures: DNA stitched with proteins \n                   \n                     DNA Nanotechnology: A nucleosome clamp \n                   \n                     DNA origami tiles: Nanoscale mazes \n                   \n                     Nature Nanotechnology focus on DNA nanotechnology \n                   \n                     Nature Tech Collection \n                   \n                     CaDNAno \n                   \n                     DAEDALUS \n                   \n                     CanDo \n                   \n                     Tilibit Nanosystems \n                   \n                     Gattaquant DNA Nanotechnologies \n                   \n                     Ultivue \n                   Reprints and Permissions"},
{"file_id": "541559a", "url": "https://www.nature.com/articles/541559a", "year": 2017, "authors": [{"name": "Esther Landhuis"}], "parsed_as_year": "2006_or_before", "body": "Neuroscientists are starting to share and integrate data \u2014 but shifting to a team approach isn't easy. As big brain-mapping initiatives go, Taiwan's might seem small. Scientists there are studying the humble fruit fly, reverse-engineering its brain from images of single neurons. Their efforts have produced 3D maps of brain circuitry in stunning detail. Researchers need only a computer mouse and web browser to home in on individual cells and zoom back out to intertwined networks of nerve bundles. The wiring diagrams look like colourful threads on a tapestry, and they're clear enough to show which cell clusters control specific behaviours. By stimulating a specific neural circuit, researchers can cue a fly to flap its left wing or swing its head from side to side \u2014 feats that roused a late-afternoon crowd in November at the annual meeting of the Society for Neuroscience in San Diego, California. But even for such a small creature, it has taken the team a full decade to image 60,000 neurons, at a rate of 1 gigabyte per cell, says project leader Ann-Shyn Chiang, a neuroscientist at the National Tsing Hua University in Hsinchu City, Taiwan \u2014 and that's not even half of the nerve cells in the  Drosophila  brain. Using the same protocol to image the 86 billion neurons in the human brain would take an estimated 17 million years, Chiang reported at the meeting. Other technologies are more tractable. In July 2016, an international team published a map of the human brain's wrinkled outer layer, the cerebral cortex 1 . Many scientists consider the result to be the most detailed human brain-connectivity map so far. Yet, even at its highest spatial resolution (1 cubic millimetre), each voxel \u2014 the smallest distinguishable element of a 3D object \u2014 contains tens of thousands of neurons. That's a far cry from the neural connections that have been mapped at single-cell resolution in the fruit fly. \u201cIn case you thought brain anatomy is a solved problem, take it from us \u2014 it isn't,\u201d says Van Wedeen, a neuroscientist at Massachusetts General Hospital in Charlestown and a principal investigator for the Human Connectome Project (HCP), a US-government-funded global consortium that published the brain map. So it goes in the world of neurobiology, where big data is truly, epically big. Despite advances in computing infrastructure and data transmission, neuroscientists continue to grapple with their version of the 'big data' revolution that swept the genomics field decades ago. But brain mapping and DNA sequencing are different beasts. A single neuroimaging data set can measure in the terabytes \u2014 two to three orders of magnitude larger than a complete mammalian genome. Whereas geneticists know when they've finished decoding a stretch of DNA, brain mappers lack clear stopping points and wrestle with much richer sets of imaging and electrophysiological data \u2014 all the while wrangling over the best ways to collect, share and interpret them. As scientists develop tools to share and analyse ever-expanding neuroscience data sets, however, they are coming to a shared realization: cracking the brain requires a concerted effort. Scientists can chart the brain at multiple levels. The HCP seeks to map brain connectivity at a macroscopic scale, using magnetic resonance imaging (MRI). Some labs are mapping neural tracks at a microscopic level, whereas others, such as Chiang's, trace every synapse and neural branch with nanoscale precision. Still others are working to overlay gene-expression patterns, electrophysiological measurements or other functional data on those maps. The approaches use different methods \u2014 but all create big data (see 'Big data by the numbers'). \n               How big? \n             In part, this is because the brain, no matter the species, is so large and interconnected. But it also stems from the cells' unwieldy dimensions. A mammalian neuron's main extension \u2014 its axon \u2014 can be 200,000 times as long as its smallest branches, called dendrites, are wide. If a scale model were built such that spaghetti strands represented the dendrites, the neuron itself would be more than one-third of a kilometre long, or four American-football fields. In the lab, researchers chart each neuron by tracing its thousands of projections through stacks of hundreds of overlapping brain-slice images. Light-based microscopy affords 0.25\u20130.5-micrometre resolution, which is sufficient to trace the main body of an individual neuron. But to reveal synapses \u2014 the minute signalling junctions through which electrical or chemical signals flow \u2014 nanometre-resolution electron microscopy is required. Higher resolution means smaller fields of view and so more pictures. And more pictures mean more data. \u201cWe're not dealing with megabytes anymore, or even gigabytes,\u201d says Arthur Toga, who leads the Laboratory of Neuro Imaging at the University of Southern California in Los Angeles. \u201cWe're dealing with terabytes. Just getting it from one place to another is an issue\u201d \u2014 2 terabytes of data would fill the hard drive of many desktop computers. Chiang's fruit-fly team combed through a terabyte of images to reconstruct 1,000 nerve cells \u2014 less than 1% of the  Drosophila  brain. And to map the human cerebral cortex, HCP researchers analysed 6 terabytes of MRI data from 210 healthy young adults, says Kamil Ugurbil, the HCP's co-principal investigator at the University of Minnesota in Minneapolis. Labs can download those data from the project's website or, for larger data sets, order 8-terabyte hard drives for US$200 apiece. Electrophysiology studies have also become computationally demanding. Today, researchers routinely record hundreds of neurons at a time. Soon, it will be thousands; in five years, hundreds of thousands, says Alexandre Pouget, a neuroscientist at the University of Geneva in Switzerland. \u201cThat's the kind of leap we'll go through.\u201d And those data come in multiple formats. Brain activity can appear as peaks amid squiggles on electrophysiological charts, or as green flashes of calcium ions moving in and out of neurons. On those green images, other fluorescent hues can indicate which neurons are sending and receiving signals. And researchers can collect these data as subjects navigate mazes, find food or watch flashing dots on a screen. If you record 20 minutes of neural activity in a mouse brain, you produce about 500 petabytes of 'flickering', in which nerve-cell firing is represented as changes in pixel values, says neuroscientist Florian Engert of Harvard University in Cambridge. But \u201cnobody cares about pixels. People are interested in which neurons connect to which others, and when they fire.\u201d By isolating each neuron and assigning time-stamps as they fire, he says, you can shrink the data set to a more manageable 500 gigabytes. \u201cThe information content in raw data is mostly irrelevant,\u201d says Engert. He draws an analogy with genome sequencing: before they had automated sequencers, researchers read DNA as ordered patterns of bands on polyacrylamide gels exposed to X-ray film. Now, computer algorithms convert those bands to a sequence of Gs, As, Ts and Cs \u2014 the bases that make up the DNA strands \u2014 and no one saves the original images. Similarly, Engert says, brain scientists should \u201cfocus not on curating and distributing raw data, but rather on developing algorithms\u201d to encode the information using fewer bits. Ideally, he says, such algorithms would enable the microscopes that collect the data to compress them as well. The idea is sensible, but could prove challenging for the brain, in part because of mathematics. To determine protein structure using X-ray crystallography, for example, there's a \u201creally clean theoretical model\u201d \u2014 a series of equations that relates specific characteristics of a protein to quantifiable features in its diffraction pattern, says Greg Farber, who manages the US National Institute of Mental Health (NIMH) data archive in Rockville, Maryland. To work out the 3D structure, \u201cyou'd just measure the intensities of the spots. You don't need to keep the many, many other pixels of data on that film,\u201d he says. Neuroscientists have no comparable model \u2014 no map that associates neural connectivity and activity with behaviour, memory or cognition. Given the brain's immense intricacy, Farber says, the problem \u201cis not that we have too much data, but that we don't have nearly enough for the complexity we're trying to address\u201d. The issue of \u201cnot enough\u201d data resonates with Julie Korenberg, a systems neuroscientist who studies neurodevelopmental disorders at the University of Utah in Salt Lake City. A common assumption about such diseases is that changes in genes skew protein expression in certain neurons, which in turn alters the brain's wiring to cause characteristic behavioural deficits. MRI can detect gross neuroanatomic changes, such as enlarged brain areas. But subtler changes require higher-resolution approaches, such as confocal or electron microscopy. But these imaging data are represented in completely different formats, and there's no way to switch between the two: once scientists zoom in to the level of single cells, they cannot pan out again to see those cells in the context of the whole brain. \n               Building a bridge \n             For the past 17 years, Korenberg and her colleagues have been working to bridge that gap by mapping the limbic system in macaques. These primates have 6 billion neurons in their brains, as compared to the human brain's 86 billion. But among research models, macaques are our nearest relative \u2014 much closer than a mouse or fruit fly. Korenberg's team is developing a 3D coordinate system to align various types of neuroimaging data in the macaque brain, from whole-brain MRI connectivity to single-cell confocal data and, for some areas, subcellular resolution with electron microscopy. They're creating \u201ca system that allows you to pick a point on one image and look at the same point at another resolution\u201d, says Janine Simmons, who heads the NIMH's Affect, Social Behavior and Social Cognition Program, which partially funds Korenberg's project. It's similar to Google Earth, Simmons says \u2014 for example, you can zoom from 40 \u00d7 directly to 1 \u00d7, but cannot necessarily access in-between magnification scales. Mapping the entire macaque limbic system using a 20 \u00d7 confocal lens will require massive data sets \u2014 well over 600 terabytes per animal. So far, the team has collected about 100 terabytes of data, accessible from a network-attached storage device that combines local 30-terabyte servers with cloud storage. The researchers can address some questions using downsized data sets and a good laptop, Korenberg says. But manipulating large 3D confocal data sets requires special workstations, and even so, the rendering of a single tiled image is slow. Nevertheless, the work, yet to be published, \u201chas the potential to be a major advance in the field of connectomics\u201d, says Patrick Hof, a neuroanatomist at the Mount Sinai School of Medicine in New York City who has previously collaborated with Korenberg. For instance, says Korenberg, the data could help scientists to link genes that seem important in certain mental disorders, such as schizophrenia or autism, to specific brain-wiring abnormalities. As scientists push the limits of what is possible, they are creating computational pipelines to handle the expanding workflow, and new tools \u2014 such as Thunder and BigDataViewer \u2014 to share and visualize the resulting data. But it will take more than tool development to ease neuroscientists' data woes. A culture shift is also required. It's hard \u201cgetting people to let go of their data\u201d, says Russell Poldrack, a psychologist at Stanford University in California who uses neuroimaging to study learning and memory. It could be \u201ca generational thing\u201d, he says: millennials are \u201cmuch more into sharing code and data than my generation\u201d. Poldrack worries that top minds might leave the field out of frustration with science \u201cnot aligning with the values they think it should have\u201d. But slowly, attitudes are shifting \u2014 first those towards software, then data. Conventionally, neuroimaging labs spend a lot of time downloading and installing the same beta software, \u201chacking through various software malfunctions and computing bottlenecks, writing redundant chunks of code and implementing their own data-management solutions to tackle the same problems\u201d, says David Grayson, a neuroscience PhD student at the University of California, Davis. Worse, many non-research tasks are relegated to students, postdocs and young investigators, who tend to be tech-savvy, but \u201cdid not sign up to be sys-admins\u201d, Grayson says. The International Neuroinformatics Coordinating Facility (INCF), a non-profit organization based in Stockholm, was created in 2005 to develop and promote standards, tools and infrastructure for brain researchers around the globe. A few years later, the United States launched the Neuroimaging Informatics Tools and Resources Clearinghouse (NITRC) as a platform for sharing neuroimaging computational tools. Back then \u201cno one was even thinking about sharing data, only software\u201d, says Nina Preuss, a programme manager for the NITRC, headquartered in Washington DC. That changed in late 2009, when researchers at the Nathan S. Kline Institute for Psychiatric Research in Orangeburg, New York, released resting-state functional MRI (fMRI) data into the NITRC from more than 1,200 volunteers, collected for the 1000 Functional Connectomes Project (FCP). These were just pooled raw data \u2014 yet within a few weeks, NITRC users had downloaded the data set 700 times. \u201cThere was such a pent-up demand for data people could freely download and play with,\u201d says Preuss. Download numbers soared to the thousands once the authors had cleaned up the fMRI data and made them searchable. After the data were published 2 , the paper logged more than 1,000 downloads in the first 2 weeks. In the same year, the first paper by independent authors \u2014 who had downloaded the consortium's fMRI data for their own analyses, but weren't involved in collecting it \u2014 was also published 3 . Since the HCP made its first data set available in March 2013, dozens of outside labs have published papers analysing the project's data. In total, the HCP has released some 50 terabytes of brain-imaging data on more than 1,000 people, says Jennifer Elam, an outreach coordinator for the project at the Washington University School of Medicine in St. Louis, Missouri. Few smaller-scale projects release their data, however \u2014 possibly because they don't have to. A few journals require all data supporting published findings to be made available to the community, but by and large, data sharing is not incentivized. There is \u201cno strong impetus\u201d to do that bit of extra work, says Grayson. The conventional academic model doesn't help. Researchers typically develop hypotheses and work on their own ideas independently of peers in their group. In such an environment, research does not drive people together \u2014 it pulls them apart, says Hongkui Zeng of the Allen Institute for Brain Science in Seattle, Washington. \u201cYou need to distinguish yourself. To establish your identity in the field, you have to do something different from others.\u201d Zeng joined the Allen Institute in 2006 in search of a culture change: the institute sets out ambitious five-year goals that require teams to work collaboratively and systematically, driving a project to completion rather than piecemeal, as can happen in individual labs. When it comes to the brain, 'complete' can be a moving target. But so, too, is the neuroscience toolset. During his Society for Neuroscience talk, Chiang lamented that it's taken ten years to map half the fly brain. Working with physicists at Taiwan's Academia Sinica, Chiang's team has started to use a technique called synchrotron X-ray tomography to boost data-acquisition speed dramatically. \u201cIt took less than 10 minutes to image a fly brain containing thousands of Golgi-stained single neurons,\u201d says Chiang, whose crew is now trying the method in mice and pigs. They plan to integrate confocal and X-ray images on a single platform from which scientists can download data. \u201cWith synchrotron X-ray imaging, mapping the human connectome at single-neuron resolution is now more realistic,\u201d Chiang says. How easy it will be to meld the maps with other data remains to be seen. \n                     Neuroscience: Connectomes make the map \n                   \n                     Neurobiology: Brain mapping in high resolution \n                   \n                     Neuroscience: Solving the brain \n                   \n                     Worldwide brain-mapping project sparks excitement \u00e2\u201d and concern \n                   \n                     Brain-data gold mine could reveal how neurons compute \n                   \n                     Nature Neuroscience : Focus on big data \n                   \n                     Neuroimaging Informatics Tools and Resources Clearinghouse (NITRC) \n                   \n                     OpenfMRI \n                   \n                     Allen Brain Atlas \n                   \n                     Human Connectome Project \n                   \n                     BigDataViewer \n                   \n                     Thunder \n                   Reprints and Permissions"},
{"file_id": "542503a", "url": "https://www.nature.com/articles/542503a", "year": 2017, "authors": [{"name": "Kelly Rae Chi"}], "parsed_as_year": "2006_or_before", "body": "As researchers open up to the reality of RNA modification, an expanded epitranscriptomics toolbox takes shape. In 2004, oncologist Gideon Rechavi at Tel Aviv University in Israel and his colleagues compared all the human genomic DNA sequences then available with their corresponding messenger RNAs \u2014 the molecules that carry the information needed to make a protein from a gene. They were looking for signs that one of the nucleotide building blocks in the RNA sequence, called adenosine (A), had changed to another building block called inosine (I). This 'A-to-I editing' can alter a protein's coding sequence, and, in humans, is crucial for keeping the innate immune response in check. \u201cIt sounds simple, but in real life it was really complicated,\u201d Rechavi recalls. \u201cSeveral groups had tried it before and failed\u201d because sequencing mistakes and single-nucleotide mutations had made the data noisy. But using a new bioinformatics approach, his team uncovered thousands of sites in the transcriptome \u2014 the complete set of mRNAs found in an organism or cell population \u2014 and later studies upped the number into the millions 1 . Inosine is something of a special case: researchers can readily detect this chink in the armour by comparing DNA and RNA sequences. But at least one-quarter of our mRNAs harbour chemical tags \u2014 decorations to the A, C, G and U nucleotides \u2014 that are invisible to today's sequencing technologies. (Similar chemical tags, called epigenetic markers, are also found on DNA.) Researchers aren't sure what these chemical changes in RNA do, but they're trying to find out. A wave of studies over the past five years \u2014 many of which focus on a specific RNA mark called  N 6 -methyladenosine (m 6 A) \u2014 have mapped these alterations across transcriptomes and demonstrated their importance to health and disease. But the problem is vast: these marks coat not only mRNA but other RNA transcripts as well, and they cut across all the domains of life and beyond, marking even viruses with their presence. The modifications themselves are not new. What has given them meaning and driven epitranscriptomics into the spotlight is the discovery of enzymes that can add, remove and interpret them. In 2010, chemical biologist Chuan He at the University of Chicago, Illinois, proposed that these chemical tags could be reversible and important regulators of gene expression. Not long afterwards, his group demonstrated 2  the first eraser of these marks on mRNA, an enzyme called FTO. That discovery meant that m 6 A wasn't just a passive mark \u2014 cells actively controlled it. And this realization came at about the same time that global approaches, harnessing the power of next-generation sequencing, made it possible to map m 6 A and other modifications across the transcriptome. Today, epitranscriptomics is blossoming. Yet its toolbox remains a work in progress. Current methods lack the sensitivity required for use with rare and precious samples. It's also not possible to quantify the amount of a given modification in the transcriptome, nor to map more than one modification in a single experiment. \u201cThere's an urgent and high demand for additional technology developments for all kinds of RNA modifications,\u201d says molecular biologist Tao Pan at the University of Chicago, who collaborated on He's FTO studies. That said, epitranscriptomics researchers are excited about the direction their field is taking. \u201cJust as you wouldn't think of DNA without thinking about how DNA is packaged, or epigenetically modified,\u201d says geneticist Chris Mason at Weill Cornell Medical College in New York City, who has led m 6 A-mapping efforts, \u201cI think now and in the future, no one will think of RNA without thinking 'How is it modified?'\u201d \n               Mapping with antibodies \n             In the early 1970s, scientists first showed that mRNA was chemically modified by using radioisotope labelling of m 6 A. But because those studies enriched the mRNA transcripts by selecting their 3\u2032 ends, which contain strings of adenosines, researchers worried that those preparations might contain trace amounts of other classes of RNA molecules, as well. \u201cPeople stopped working on this because it was so difficult to get clear insights into whether the m 6 A in mRNA was a contaminant,\u201d says Samie Jaffrey, a chemical biologist at Weill Cornell Medical College. Also difficult was working out where in the transcriptome m 6 A was located, which could provide clues to its function. Conventional sequencing approaches involve reverse transcription \u2014 converting RNA into complementary DNA (cDNA), which is then amplified and sequenced. The problem is that the reverse transcriptase enzyme used to make cDNA erases the modifications. \u201cThere was no way to see m 6 A,\u201d Jaffrey says. \u201cWhen you reverse-transcribe it, it behaves exactly like an A.\u201d Despite the technical challenges, the discovery of unexpected bacterial RNA modifications 3  piqued Jaffrey's interest, and he decided to look for them in mammalian RNA. Working with Mason, his team sheared RNA into tiny pieces, pulled out those that contained m 6 A using antibodies, and sequenced the RNAs 4 . \u201cWe were clearly seeing labelling of mRNAs and that was remarkable. It was not a contaminant,\u201d Jaffrey says. A similar study 5  by Rechavi's group unearthed a hilly landscape of m 6 A peaks, roughly 12,000 sites in 7,000 human genes. The modifications, Rechavi's team discovered, tended to be concentrated on the protein-coding sequences called exons and on stop codons, the three-letter codes in mRNA that signify the end of the protein-coding sequence. The methods these researchers used, m 6 A-seq and MeRIP-seq, have since been broadly used to map m 6 A in different disease contexts and organisms. Antibodies and reagents targeting m 6 A are available from Active Motif ( go.nature.com/2kqgzu8 ), MilliporeSigma ( go.nature.com/2kw39m3 ) and New England BioLabs ( go.nature.com/2kqjjaz ), among others. Researchers think that the modification could control the way cells develop into different types, a process that goes awry in cancer. Indeed, the first links between the epitranscriptome and cancer have already emerged. He's group, for example, showed that in some forms of acute myeloid leukaemia, FTO is present in higher-than-normal levels and seems to remove m 6 A from certain transcripts 6 , which could spur cells to differentiate. A parallel line of research has turned that finding on its head. Using an antibody-mapping method called miCLIP, which is higher in resolution than its predecessors, Jaffrey's team showed that its m 6 A antibodies also bind to  N 6 , 2\u2032-O-dimethyladenosine (m 6 Am), a modification of the chemical structures that cap the 5\u2032 end of mRNAs. At the time, Jaffrey didn't know if m 6 Am carried any biological meaning. But his team has since shown that m 6 Am (and not m 6 A) is in fact the major target of the FTO eraser, and that it affects the stability and subcellular location of mRNAs 7 . To Jaffrey, that suggests that He's findings linking FTO to acute myeloid leukaemia mean that m 6 Am, not m 6 A, is now implicated in the origin and development of cancer. Such discrepancies are par for the course in an emerging field, says cancer biologist Howard Chang at Stanford University, California. \u201cThis particular issue is not that different from the early days of the histone-modification field,\u201d he says, referring to the study of chemical alterations to the histone 'spools' around which DNA is tightly packaged in cells. \n               Many modifications \n             Other RNA modifications have also attracted researchers' attention. In 2016, teams led by chemist Chengqi Yi at Peking University in Beijing and by Rechavi and He used antibody-based methods to map  N 1 -methyladenosine (m 1 A) in mouse and human cell lines and tissues 8 , 9 . Using different approaches to prevent m 1 A from interfering with reverse transcription, the two teams showed that m 1 A, which was discovered in total RNA in the early 1960s, is present on mRNA at the position at which the translation machinery initiates protein production. Stress conditions alter the maps, suggesting that they are dynamic. The researchers don't yet know what m 1 A does, but they have a tantalizing clue: most transcripts have only one m 1 A site, and these seem to be translated more often than those that lack the modification. \u201cThis is very exciting \u2014 and of course challenging \u2014 because we are dealing with a new regulatory mechanism for translation of messenger RNA,\u201d Rechavi says. An antibody that targets m 1 A is available from MBL International ( go.nature.com/2kvqpfs ). Other global mapping strategies rely on the fact that some RNA modifications can serve as handles for attaching chemical tags. When Yi started his lab in late 2011, modified RNA building blocks called pseudouridines or 'pseudoUs' were well known in other classes of RNA but had not been seen on mRNA. In 2015, his group described a chemical labelling and pulldown method for enriching the modification on transcripts 10 . To Yi's surprise, pseudoUs are much more abundant in human and mouse mRNA than was previously thought. Now his focus is on finding the mark's function. \u201cI think pseudoU can have multiple functions in mRNA, depending on when, where and how it is installed and regulated on RNA transcripts,\u201d Yi says. Some pseudoU 'writers' are already known, he adds, but whether there also are readers or erasers is an open question. \n               Moving beyond mapping \n             Whether using antibodies or chemical approaches, mapping RNA modifications is a tricky business. Antibodies can cross-react with other modifications, so researchers should use at least two different antibodies and cross-correlate the hits, Mason says. Chemical methods can cut, bind or enrich some areas more frequently than others, producing biased fragmentation patterns. Sequencing depth and choice of bioinformatics algorithms can affect detection of modification sites, Yi says. Even the length of time for which cells are kept alive in culture could influence modification levels, so it is crucial (and not necessarily trivial) to capture baseline maps for comparison, Mason says. But in any event, it's not enough to show that a sample contains a particular modification. Instead, it will be crucial to quantify all RNA modifications, because cells probably rely on just the right amount of a given modification for their proper function, Pan says. Quantification is particularly important for those researchers who want to tune levels of modifications by boosting the enzymes that write and erase them. The mere presence of such enzymes suggests that precisely tuned levels of modification are important, Pan says. In 2015, Pan's team described a potential way to quantify modifications, at least in another class of RNA called transfer RNAs 11 . The approach uses a reverse transcriptase that can read through the modifications efficiently, thereby capturing sequences downstream of the first modification it encounters. Now Pan says his group is working to apply the same strategy to mRNA modifications such as m 1 A. But perhaps the fastest way to define function is to identify the readers, writers and erasers of these tags. In their 2012 m 6 A-mapping study 4 , Rechavi's group created short stretches of RNA with and without m 6 A modifications, using these fragments as bait to pull out any proteins that might be bound to the RNA. In 2014, He's group discovered several m 6 A readers using a comparable strategy 12 . Other studies have implicated other cellular functions as well. Now Rechavi plans to try a similar baiting approach to pull out readers of m 1 A, although that may prove more difficult because the site at which m 1 A concentrates (where protein translation begins) is more highly structured than are the sequences that contain m 6 A. Once enzyme readers are identified for a particular chemical tag, gene-editing technologies should make it easy to tune their expression, allowing researchers to glean some insight from global changes to a modification. For instance, Chang's work deleting a writer of m 6 A showed the importance of this modification in determining cell fate 13 . But as with all things epigenetic, interpreting such findings may also prove complicated, as the same enzyme may well work across broad swathes of RNA species, and a given modification can have different functions in each type of RNA, says Chang. \u201cIt wouldn't surprise me if some of the effects happen through other species of RNAs that aren't on people's radars but they're still prevalent in the cell and very important,\u201d he says. \n               Enhancing the toolbox \n             In the past few years, He's group has discovered evidence 14  suggesting that RNA modifications provide a way to regulate transcripts involved in broad cellular roles, such as switching on cell-differentiation programs. Researchers need better technologies to explore these links; and, in October 2016, the US National Institutes of Health awarded He and Pan a 5-year, US$10.6-million grant to establish a centre to develop methods for identifying and mapping RNA modifications. One big focus is to come up with a way to generate mutations at the sites of a modification, and to amplify those, He says. With new imaging techniques, it might eventually be possible to resolve single marks on a given transcript for visual inspection. \u201cI'm dying to say that someone developed a technology to image the m 6 A modification in mRNA,\u201d He says, but at the moment, that is not the case. Ye Fu, a former graduate student in his lab, is exploring this approach in biophysicist Xiaowei Zhuang's lab at Harvard University in Cambridge, Massachusetts. Fu is combining super-resolution microscopy with another method Zhuang has pioneered for visualizing RNA in single cells, called MERFISH (multiplexed error-robust fluorescence  in situ  hybridization). Fu says that he has made progress in the past two years, but the data are noisy and need to be optimized for these modifications to be detected efficiently. Others are working to circumvent the problems associated with conventional sequencing by sequencing RNA directly. Scientists at Oxford Nanopore Technologies in the United Kingdom reported one such method 15 , which extends to RNA the company's capability to thread DNA and other polymers through a nanopore embedded in a membrane. Researchers at Pacific Biosciences in Menlo Park, California, have also demonstrated direct RNA sequencing using the company's single-molecule real-time (SMRT) sequencing chemistry 16 . \u201cThe idea is certainly as old as SMRT sequencing itself,\u201d says Jonas Korlach, chief scientific officer. SMRT sequencing uses an enzyme called DNA polymerase to replicate strands of DNA, capturing the addition of fluorescent nucleotides in real time. To adapt the technology to RNA, Korlach, Mason and their collaborators substituted the polymerase with the reverse transcriptase from HIV. As with the DNA platform, this enzyme incorporates fluorescent molecules across modified bases more slowly than it does across unmodified ones, giving each modification its own 'kinetic signature'. Technical hurdles remain, however, as RNA poses challenges not seen in DNA. One is that RNA readily folds in on itself to form loops and knots, so it is highly structured. In its study 15 , Oxford Nanopore attached RNA to cDNA, which helps 'iron out' the secondary structures and move the RNA through the pore. A second challenge is that RNA degrades more easily than DNA, a problem that may stymie long-read sequencing approaches. Yet another challenge, on the data side, is the sheer number of RNA modifications. Recognition of multiple different modifications on the same RNA would require massive training sets to teach the detection software to distinguish one modification from another. Winston Timp, a biomedical engineer at Johns Hopkins University in Baltimore, Maryland, has been using Oxford Nanopore technology to develop new methods for detecting specific DNA modifications 17 . He now plans to move into RNA modifications, developing a training set that will help recognize m 6 A modifications. \u201cThe problem is, we don't know how diverse on a given molecule the modifications are,\u201d he says. \u201cBut this is something we can probe. It's an exciting area of research.\u201d Footnote  1 \n                     Method of the Year 2016: Epitranscriptome analysis \n                   \n                     Epitranscriptomics: mixed messages \n                   \n                     RNA: Expanding the mRNA epitranscriptome \n                   \n                     The dynamic epitranscriptome: N6-methyladenosine and gene expression control \n                   \n                     Molecular biology: Messenger RNAs marked for longer life \n                   \n                     Epigenetics: A new methyl mark on messengers \n                   \n                     Nature  Web Collection: \u201cThe Epitranscriptome\u201d \n                   \n                     The RNA Modification Database \n                   Reprints and Permissions"},
{"file_id": "544255a", "url": "https://www.nature.com/articles/544255a", "year": 2017, "authors": [{"name": "Michael Eisenstein"}], "parsed_as_year": "2006_or_before", "body": "Innovative tools are revealing the forces that guide cellular processes such as embryonic development and tumour growth. Under a microscope, cells often appear static \u2014 solid elements of biological architecture. Yet in reality, they are dynamic structures, jostling for space in a packed environment. Cells squeeze, stretch, flex and pull on their surroundings and each other, exerting force as they do so. These forces are incredibly small \u2014 on the scale of piconewtons, or roughly one-billionth of the weight of a paperclip. But they can have profound biological impact. The shifting forces in a rapidly growing embryo can alter cells' developmental programs, telling them when to stop dividing and begin transforming into brain or bone. The idea that physical forces affect cellular function was put forth a century ago, by Scottish scientist D'Arcy Thompson in his seminal work  On Growth and Form 1 . \u201cCell and tissue, shell and bone, leaf and flower, are so many portions of matter,\u201d wrote Thompson, \u201cand it is in obedience to the laws of physics that their principles have been moved, moulded and conformed.\u201d Thompson's largely theoretical work paved the way for numerous experimental studies of biomechanical principles. \u201cBiomechanics is a very old field \u2014 people just ignored it for a long time,\u201d says Carsten Grashoff, who studies cellular forces at the Max Planck Institute of Biochemistry in Munich, Germany. In part, that's because researchers lacked the technology to measure molecular-scale forces with precision. Now, scientists have tools that make it possible to microscopically map the forces that skin cells exert as they crawl forward during wound healing, or to program cells to blink on and off as proteins stretch and relax. Hurdles remain \u2014 researchers still struggle to tell apart the true effects of cellular forces from random biological noise, and they have yet to work out how these processes play out in the complex environment of a living organism. But by combining 'mechanobiology' tools with other measurements of genetic and biochemical activity, researchers can begin to understand how force is translated into function. \u201cLife processes aren't just a biochemical signalling pathway,\u201d says Beth Pruitt, a mechanical engineer at Stanford University in California. \u201cWhen you pull on a protein, you might open or close a binding site that flips a switch to choose which program is going to be run by a cell.\u201d \n               Gaining traction \n             Cells interact with their environment largely through proteins embedded in their membranes, which represent crucial foci for generating and interpreting cellular forces. Some proteins respond to being 'pushed' by the flow of liquid, as seen in blood vessels, whereas others generate tension-related signals when a cell gets tugged by its neighbour or latches onto other proteins nearby. A method called traction force microscopy (TFM) took off in the 1990s, and gave the field its first real tool for quantitatively measuring such forces. In 1999, for example, Yu-Li Wang, then at the University of Massachusetts Medical School in Worcester, and Micah Dembo at nearby Boston University, plated connective-tissue cells called fibroblasts on a gelatinous material embedded with fluorescent beads. They then showed that they could use TFM to deduce the forces that those cells generated by measuring the displacement of the beads 2 . \u201cIt's like a spring-scale,\u201d says Ben Fabry, a biophysicist at Friedrich Alexander University of Erlangen-Nuremberg in Germany, \u201cwhere you put a weight onto a spring and measure the deformation, and you can determine the force if you know the stiffness of the spring.\u201d TFM has become a standard method for studying both individual cells and tissue-like sheets of interconnected cells. Clare Waterman at the National Heart, Lung and Blood Institute in Bethesda, Maryland, has used TFM to study cell migration, a process mediated in part by the forces that cellular structures known as focal adhesions exert as they anchor themselves onto the surrounding extracellular matrix (ECM). Waterman's group has developed strategies for increasing the number of beads that can be imaged in a TFM experiment, producing ultra-high-resolution force maps. \u201cWe can get 50 markers under each focal adhesion, so we're down to submicrometre resolution,\u201d she says. This has enabled her group to reveal how forces generated at focal adhesions trigger the molecular events that coordinate directed cellular movement during processes such as embryonic development 3 . Of course, the multidimensional shifting of beads in response to cellular motion is much more complicated than a one-dimensional spring scale. TFM initially required powerful supercomputers to interpret the data, although modern computational methods have made the technique more accessible. Even so, converting bead-displacement data into force measurements remains challenging, and there are many sources of potential error. \u201cYou can have a single cell pulling in opposite directions that makes it look like there's basically no deformation,\u201d says Waterman. \u201cAnd when there's movement of beads well beyond the boundaries of the cell, that can be hard to deal with.\u201d Other groups are extending TFM into three dimensions, in an effort to better mirror biological reality. Fabry and his colleagues, for example, developed a TFM method to track cellular forces in 3D using gels built of collagen, a key protein component of the ECM. His team was able to probe the relationship between the shape of breast cancer cells, the forces that they generate and their speed and direction as they propel themselves through a synthetic 3D 'tissue'\u2014 potentially modelling metastatic growth 4 . But his method also ups the analytical and computational challenges. \u201cCollagen is a very awkward material \u2014 its behaviour is highly nonlinear, meaning that if you stretch it a little bit it's soft, but if you stretch it a bit more it's suddenly very stiff,\u201d he says. To work around this computational burden, Celeste Nelson, a biomedical engineer at Princeton University in New Jersey, settles for lower-resolution data in her studies of organ development. \u201cWe care more about finding the relative differences in the magnitude of force across an entire population of hundreds or thousands of cells,\u201d she says. However, force generation is inherently dynamic; by collecting these 3D data at multiple time-points, she says, \u201cthe computational demand just explodes\u201d. \n               On pins and needles \n             As a simpler option, some researchers use small, precisely designed chips moulded from various polymers, which provide a direct readout of cellular forces. One widely used design, developed by bioengineer Christopher Chen of Boston University, Massachusetts, and his colleagues, consists of an elastic material called PDMS that displays an array of flexible pillars, like the bristles on a toothbrush. These 'micropillars' are topped with ECM proteins that allow cells to form attachments 5 . \u201cThey basically act like mini springs,\u201d says Jianping Fu, a former postdoc of Chen's who now uses similar devices to study human stem cells at the University of Michigan in Ann Arbor. \u201cBy measuring the deflection, people can identify and measure the forces that cells exert at individual pillars.\u201d Micropillar-array data are easier to interpret than the results of TFM experiments, and they require less computational analysis. And the devices themselves are reasonably straightforward to manufacture. They are also compatible with fluorescence microscopy, which facilitates molecular-scale investigation of the events that produce cellular forces. However, these arrays also impose a specific \u2014 and unnatural \u2014 pattern of interaction between cells and their substrates, governed by the distribution and size of the pillars, which could deviate from how cells behave in living organisms. Researchers can also customize the culture surface by manipulating the design of the micropillar array. Shorter, thicker pillars are more rigid and unyielding, whereas taller, slender pillars are flexible and more responsive to force. Such changes in the rigidity of the pillar surface can trigger considerable reorganization of a cell's cytoskeleton \u2014 the network of proteins that form the cell's physical infrastructure and help it to transmit and respond to force. This, in turn, can influence cellular proliferation, movement and maturation. Fu, for instance, has found a relationship between surface rigidity and adult stem-cell differentiation. \u201cWith stump-like pillars that are hard to bend, they become bone cells, but when you seed them on taller pillars, they have a greater tendency to become fat cells.\u201d By precisely tuning the design, Fu's team was even able to develop a culture system that strongly favours the development of human embryonic stem cells into functional spinal motor neurons 6 . \n               Tiny tugs \n             Other researchers are measuring the forces applied by proteins or protein complexes using molecular sensors that generate a fluorescent signal in response to small-scale changes in tension. Such sensors generally are based on F\u00f6rster resonance energy transfer (FRET), a phenomenon in which one fluorescent molecule, or fluorophore, excites another \u2014 but only when they are in close physical proximity. As a postdoc in the lab of biomedical engineer Martin Schwartz at the University of Virginia in Charlottesville, Grashoff worked with colleagues to develop one of the first FRET-based tension sensors, which they described in a 2010 paper 7 . \u201cWe had this idea that we would use this very elastic protein that you find in spider silk to link the two fluorophores to each other,\u201d Grashoff says. \u201cWhen there is mechanical tension, it would elongate, and you could measure the decreased FRET signal.\u201d At rest, the silk protein forms a compact coil, but a gentle tug can stretch out the spring and uncouple two flurophores. As proof of concept, Grashoff and Schwartz incorporated their sensor into a protein called vinculin, a component of focal-adhesion complexes that form a bridge between the ECM and a cell's cytoskeleton. When they expressed the sensor protein in cells, they observed that vinculin experiences piconewton-scale forces that change as focal adhesions assemble and disassemble during cell migration 7 . In principle, FRET sensors can be inserted into a diverse range of proteins to obtain measurements otherwise difficult to collect. For example, there are few tools for quantifying how cells push and pull on each other in a tissue. Stanford University chemical engineer Alexander Dunn and his colleagues were able to measure these forces by integrating Grashoff and Schwartz's FRET sensor into E-cadherin, a protein that couples cells together 8 . And sensors can be built around other linker proteins, as well, enabling researchers to fine-tune a sensor's sensitivity. Grashoff notes that his FRET-sensor collection can selectively respond to forces of 1\u201312 piconewtons. But there's still room for improvement, he adds, because intracellular proteins can experience forces as high as 20\u201330 piconewtons. But the design and validation of such sensors is labour-intensive, and sensors can 'go dark' for reasons other than force detection \u2014 such as degradation. FRET signals can also be challenging to interpret, requiring careful measurement to eliminate false positives. And there are consequences from inserting a bulky sensor into proteins whose function is strongly structure-dependent. \u201cYou cannot fully predict how well it will work,\u201d says Grashoff. \u201cIt may not be a problem that you've disturbed the protein, but you have to know how great that disturbance is.\u201d Other groups are measuring extracellular forces using sensors that need not be shoehorned into a protein. Biophysicist Khalid Salaita's group at Emory University in Atlanta, Georgia, has developed several such probes, in which one end is anchored to a solid surface, such as a glass slide, and the other displays a biomolecule that binds to a cell-surface protein of interest 9 . In between, Salaita places various linkers that are responsive to forces of different magnitudes. \u201cPolyethylene glycol polymers are like wet spaghetti, where you have this random coil that gets stretched, whereas DNA has this fixed secondary structure,\u201d says Salaita. His group also uses linkers derived from a protein called titin, which produces elastic recoil in muscle. \u201cIt's a naturally evolved spring that can withstand greater forces,\u201d says Salaita. These titin-based sensors were strong enough to measure the powerful pull that cells exert on their environment through integrin proteins 10 , a component of focal adhesions that can generate tens of piconewtons of force \u2014 enough to tear apart weaker DNA duplexes and polymer-based sensors. \u201cThe integrin is basically a brute-force grappling hook that anchors cells and applies extensive forces,\u201d says Salaita. \n               Forces for good \n             That scientists can measure intracellular forces is itself remarkable. The resulting insights might yield valuable clinical dividends. Salaita thinks that assays for measuring forces at the single-cell level could help scientists to identify safer drugs that interfere directly with physical mechanisms of tumour progression. \u201cThe migration and the invasion of the cancer cell is what is most deadly, and if you can shut down that mechanical process but make the drug non-cytotoxic, that's a much more precise tool,\u201d he says. However, many biological questions need to be explored at the tissue or organ scale. \u201cYou can't predict much about a tissue from isolated cells,\u201d says Nelson. \u201cThe connections of individual cells seem to be necessary for the generation and transmission of force in a tissue.\u201d In many of her experiments, Nelson uses engineered epithelial tissues that provide a controlled, reproducible model for studying the forces involved in organ formation. Other groups use stem cells to generate specialized tissues; for example, Pruitt uses stem-cell-derived cardiomyocytes to study the mechanobiological effects of heart disease. \u201cWe have this confluence of tools that makes it possible right now to create cardiomyocytes from human cells and apply and measure force, displacement and stretch in single cells and microtissues,\u201d she says. Nelson is excited that scientists are finally able to explore the implications of Thompson's century-old hypotheses. \u201cI think the field as a whole is revealing that mechanical forces can play as big a role in the eventual shape of a tissue that develops as the genes that are activated during the process \u2014 if not bigger,\u201d she says. Still, more tools are needed. Most force-measurement experiments remain time-consuming, which limits their usefulness for applications, such as drug screening, that require parallel analysis of large numbers of cells. Fabry's group is developing ways to automate and accelerate TFM experiments. \u201cWe want to measure the response of hundreds or thousands of cells in a 3D tissue at the same time,\u201d he says. Measuring cellular forces in living organisms is also a significant challenge. FRET sensors offer one solution, and mechanical engineer Otger Camp\u00e0s at the University of California, Santa Barbara, and his colleagues recently devised another. His group injects living organisms with fluorescently labelled oil droplets, which are decorated with proteins that can bind to cell surfaces 11 . By documenting how these droplets deform in the spaces between cells, they can computationally derive the 3D forces those cells are exerting on the droplets. Perhaps most fundamentally, there is a need for experimental techniques that allow scientists to manipulate force-responsive molecules more precisely. The ability to inactivate individual focal adhesions, for instance, similarly to how geneticists knock out individual genes, could reveal how the timing and spatial distribution of molecular forces alters their effect on cells, Nelson says. \u201cThat would allow us to directly answer many questions that we're kind of dancing around right now.\u201d \n                     Appreciating force and shape \u2014 the rise of mechanotransduction in cell biology \n                   \n                     Mechanobiology in harness \n                   \n                     Mechanotransduction: Golden nanoprobes \n                   \n                     DNA-based digital tension probes reveal integrin forces during early cell adhesion \n                   \n                     A DNA-based molecular probe for optically reporting cellular traction forces \n                   \n                     The molecular clutch model for mechanotransduction evolves \n                   \n                     NatureTech: Nature's Technology Feature hub \n                   \n                     Naturejobs  blog: US research centres create opportunities \n                   \n                     ' On Growth and Form  100' \n                   \n                     How to measure molecular forces in cells \n                   Reprints and Permissions"},
{"file_id": "545119a", "url": "https://www.nature.com/articles/545119a", "year": 2017, "authors": [{"name": "Jeffrey M. Perkel"}], "parsed_as_year": "2006_or_before", "body": "Mobile phones are helping to take conventional laboratory-based science into the field, the classroom and the clinic. As spring turns to summer along the east coast of the United States, thoughts turn to holidays, beaches, picnics \u2014 and mosquitoes. Prince William County, Virginia, southwest of Washington DC, is no exception. In 2016, county officials set traps to collect and test mosquitoes for the presence of disease-causing viruses. Usually, the testing involves taking the insects back to the lab and analysing them for signs of the pathogens' nucleic acids \u2014 a process that can take days. But last September, Joseph Russell was able to get those same data from the air-conditioned comfort of his car \u2014 all thanks to his smartphone and a handheld instrument known as the two3, made by Biomeme of Philadelphia, Pennsylvania. About the size of a small laptop speaker and controlled by a plugged-in iPhone, the two3 can test each of three nucleic-acid extracts for two sequence targets at a time. Simply pop open the top, add sample tubes and press start. \u201cIt was phenomenal,\u201d enthuses Russell, a postdoc at MRIGlobal in Gaithersburg, Maryland, who ran the device from his car's cup-holder as he drove from site to site. \u201cBy the time I had collected the next round of mosquitoes, I knew the results from the previous spot.\u201d Russell's experience illustrates the ease with which researchers are migrating their science from the lab into the field, thanks to an increasingly powerful and enabling tool that many people already carry in their pockets \u2014 the smartphone. Combining a computer, camera, Global Positioning System, networking, sensors and batteries in one compact package, smartphones are like \u201ca Swiss army knife\u201d that can be used almost anywhere, says Aydogan Ozcan, an electrical and biological engineer at the University of California, Los Angeles. Ozcan has spent the past decade fashioning apps and hardware that turn the phones into ever-more-powerful microscopes and biosensors. And with billions of devices in circulation and cellular networks that are constantly improving in terms of coverage and data-transmission speed, researchers are using the phones to take their science ever-farther afield. But powerful as they are, smartphones were initially made with the consumer market in mind, not science. In their quest to gain customers, manufacturers continually push the envelope of what their phones can do, especially in terms of camera quality. \u201cHow many consumers 'need' that 40-megapixel camera? Maybe a fraction,\u201d Ozcan says. But, he adds, scientists can capitalize on the improving image sensors and the advantages that those bring. Often, researchers can gain those benefits right out of the box, no custom apps required. Matthew Dietz, an orthopaedic surgeon at the West Virginia University School of Medicine in Morgantown, devised a method to use the iPhone's accelerometer \u2014 the built-in sensor that allows users to control video games by tilting their screens \u2014 to measure the range of motion of a limb joint. His colleagues' reaction was mostly one of surprise, Dietz says: \u201cI didn't know my phone could do that!\u201d Today, smartphones are used for a wide range of scientific and medical purposes. Ozcan's group has exploited the technology to design successively more sophisticated and sensitive imagers, including ones that can visualize individual viruses and DNA molecules. Mechanical engineer David Erickson and nutrition scientist Saurabh Mehta, both at Cornell University in Ithaca, New York, have developed an iPhone-based system called the NutriPhone that can test for 'micronutrients' such as vitamin B 12  and iron in patients' blood. Originally, users placed a test strip into an accessory placed in front of the phone camera. But Erickson has now developed a wireless version that minimizes contact between bodily samples and the phone. \u201cIf you're performing a diagnostic, particularly for a number of people, on your own mobile phone, there's a possibility of contamination from the sample to the phone, and then you're putting it up to your ear and then, who knows what [can happen]?\u201d he says. Erickson founded a company, VitaScan in Ithaca, to commercialize the technology. There are even smartphone-based DNA sequencers. In the United Kingdom, Oxford Nanopore Technologies has announced a commercial device called SmidgION, which the company anticipates will be ready by the end of 2017. And in January, Mats Nilsson of Stockholm University, working with Ozcan, demonstrated a 3D-printed smartphone attachment to detect DNA mutations in sections of tissue. The team used the device (with a Nokia Lumia 1020 Windows phone) to count cancerous cells using fluorescence techniques, and Nilsson hopes it will ultimately enable rapid diagnosis of antibiotic-resistant tuberculosis in India \u2014 a condition that all too frequently takes months to detect in that part of the world. \n               Democratizing diagnostics \n             Perhaps nowhere is the game-changing potential of smartphones more obvious than in developing countries. In 'resource-poor' environments, trained personnel tend to be scarce and laboratory equipment even more so. Key infrastructure, such as electricity and clean running water, are often unreliable. Cellular networks, however, offer some resilience. In 2014, Isaac Bogoch, a tropical-medicine specialist at the Toronto General Hospital Research Institute in Canada, spent time in the rural town of Grand Moutcho in the south of C\u00f4te d'Ivoire, looking for evidence of the parasitic infection called schistosomiasis, which can cause liver, gastrointestinal and urogenital complications. Schistosomiasis is endemic in C\u00f4te d'Ivoire, Bogoch says. Spread by contact with contaminated water, the disease is easy to diagnose and easy to treat \u2014 assuming health-care workers have access to a microscope and are trained in how to use it. All too often, they don't. To close that gap, Bogoch and his colleagues turned smartphones into portable microscopes and taught local technicians how to use them to test urine and faeces from potential patients. \u201cRather than transferring people or specimens to laboratories that are far away, we can bring the lab to the people,\u201d he says. Such strategies are democratizing health care, Bogoch notes. But they also facilitate epidemiological surveillance, and open the door to remote or even automated image analysis. For instance, Johan Lundin, research director at the Institute for Molecular Medicine Finland at the University of Helsinki, has developed an automated fluorescent slide scanner using mobile-phone components, which he recently tested in Tanzania, also looking for schistosome infection. Although the school at which the trial was conducted had no electricity, Lundin says, slide images collected in the field could be uploaded to servers in Helsinki at the rate of 20 fields of view per second through the cellular network. They could then be downloaded back in Tanzania for immediate assessment. Lundin has also formed a company, Fimmic, to commercialize automated pathology slide analysis in the cloud. Bogoch's team trained local microscopists to identify schistosome eggs in human urine and faeces while located in more-rustic environs. Rather than working in the usual laboratory setting, they would do their analysis from \u201ca picnic table outside of a clinic in a field\u201d. The test used a simple 3D-printed mobile-phone attachment called CellScope Schisto, developed in the lab of Daniel Fletcher, a bioengineer at the University of California, Berkeley. The CellScope is basically a snap-on case that positions an inverted mobile-phone lens over a smartphone's existing camera to magnify the image. The team also tested a commercial handheld microscope called the Newton Nm1, and compared the findings to those taken using a conventional clinical microscope. Both handheld devices were sensitive to low levels of infection, but the Nm1 performed better, probably because the CellScope has no built-in slide-scanning functions, Bogoch says; the team is now addressing that limitation ( J. T. Coulibaly  et al .  PLoS Negl. Trop. Dis.   10 , e0004768; 2016 ). Meanwhile, Fletcher's team has used the CellScope to look for evidence of another parasite, the filarial nematode  Loa loa , in Cameroon. To do so, Fletcher's team supplemented the CellScope's inverted lens with an array of light-emitting diodes (LEDs), an Arduino (devices that make use of the low-cost minicomputers increasingly being used by researchers to collect and analyse data), Bluetooth and an automated sample translation stage, all of which were controlled by an attached phone. To run the test, the researchers, working with partners in France, Cameroon and the US National Institutes of Health, put a droplet of unprocessed patient blood into a capillary, load that capillary into the iPhone attachment, and then capture short 5-second movies of the capillary one field of view at a time. In this way, they can look for disturbances in the distribution of red blood cells that would indicate the presence of a wiggling  L. loa  worm. Images are analysed on the phone itself, with total time from finger prick to diagnosis of about 3 minutes. The device gives comparable results to conventional blood-smear analysis, with no false negatives and only two false positives in the 33 samples (M. V. D'Ambrosio  et al .  Sci. Transl. Med.   7 , 286re4; 2015); Fletcher says that it has since been validated in hundreds of people and used to test thousands of people with river blindness, which is caused by the  Onchocerca volvulus  worm. \u201cMaking a device in the lab, as academics like to do, and showing that it can work, is one thing,\u201d he says. \u201cBut actually making devices that work reliably in the field is a very stressful transition \u2014 but one that's incredibly satisfying when it does indeed work.\u201d \n               Wider applications \n             Others use their smartphones for pedagogical purposes. University of Pennsylvania bioengineering graduate students Megan Sperry and Heidi Norton worked with Biomeme and the educational group TechGirlz to introduce 18 schoolgirls to modern molecular biology using the iPhone. The team ordered fresh sashimi from three Philadelphia restaurants and tested the fish using the two3 to see whether the menu accurately described what species was served. In about half the cases, it didn't. For students, Sperry says, being able to \u201cconnect the dots\u201d between the classroom and real life made the exercise particularly interesting. \u201cIt was the perfect experiment as a first exposure to lab experience,\u201d she says. \u201cThere's a real-world example: there's fish, we're genotyping it, we're going to see if it's the correct fish or not.\u201d Others have used their phones to build instructional microscopes. Bioengineer Ingmar Riedel-Kruse at Stanford University in California, for instance, developed a 3D-printed LudusScope. The device includes a joystick-controlled LED array that students can use to drive light-responsive single-celled protozoa around the field of view. And Julien Colombelli, an engineer and manager of the Advanced Digital Microscopy Core Facility at the Institute for Research in Biomedicine in Barcelona, Spain, has combined the power of smartphones and LEGO to illustrate the principles behind light-sheet microscopy. The 'LEGOLish' system is not a true microscope, Colombelli says \u2014 it contains no magnification lenses. But it can image objects measuring 1\u20132 centimetres, about the size of a mouse embryo. The system passes light from a cheap laser diode through a water-filled tube, which acts as a cylindrical lens to create a thin sheet of light. A series of LEGO gears translates and rotates the sample through that light sheet to produce optical sections, which are then captured on the phone. The set-up costs around US$200, not including the phone. Colombelli and his colleague Jordi Andilla at the Institute of Photonics Sciences, also in Barcelona, first designed the LEGOLish to be used as prizes for the best posters at a light-sheet microscopy conference they organized in 2014. But they have since upgraded the design to make it suitable for scientific applications, albeit at ten times the cost. Researchers could use that modified design, which is built on top of a stereomicroscope, to perfect their sample preparation procedures before reserving time on a core facility's instrument, he says. \u201cWe believe this would help a lot of labs, because they would have easy access, for less than $2,000, to a system that they can use and build in a week's time.\u201d \n               From lab to field \n             Their portability makes smartphones particularly useful in remote locales. Late last year, for example, Peter Countway, a marine microbiologist at the Bigelow Laboratory for Ocean Sciences in East Boothbay, Maine, took the Biomeme two3 to Palmer Station in northern Antarctica. He and his team used the device to study how ocean bacteria metabolize dimethylsulfoniopropionate, an organic sulfur compound produced by phytoplankton that has been implicated in global weather patterns. And Emmanuel Reynaud at University College Dublin has taken his smartphone to the tiny coral atoll of Fakarava in French Polynesia to study the health and structure of coral reefs across a series of length scales. To get the widest-angle view, his team blends twenty-first-century technology with a Cody kite, an ultra-stable design developed by plane pioneer Samuel Franklin Cody in 1901. The team used the kite to loft a cheap Android phone into the air, then dragged the kite behind a kayak for about six hours. The phone takes a picture every 20 minutes, then compresses the image and beams the data to a computer down below. The images are later processed to map the reef in 3D. The total cost for the hardware is about $400 \u2014 cheap enough that they can leave it behind for local researchers to continue the surveillance once the team has returned to Dublin. After all, says Reynaud, even in Fakarava, which has a population of just 400, phones are everywhere. \u201cYou're just showing them that, instead of texting all day, you can also do useful things.\u201d \n               And to the clinic \n             Increasingly, smartphones (and related, wearable devices such as the Apple Watch, Fitbit, and Alphabet's newly announced Study Watch) can collect medically relevant data, such as step-counts and heart rate. In April, the US business news outlet CNBC reported that Apple was developing sensors to measure blood sugar through the skin. Researchers are finding new ways to use such data to answer scientific questions. Apple's ResearchKit, for instance, allows scientists to use iPhones to recruit people into and conduct clinical studies. \u201cI thought it was a pretty brilliant idea,\u201d says Yvonne Chan, director of Personalized Medicine and Digital Health at the Institute of Genomics and Multiscale Biology at the Icahn School of Medicine at Mount Sinai in New York City. Many users take their smartphones everywhere they go, she says; she confesses to being \u201ca smartphone addict \u2014 it literally is on me 24/7\u201d. And ResearchKit provides a way to marry that universal appeal with people's innate scientific curiosity, to do something \u201creally awesome\u201d, she notes. When Apple launched ResearchKit in 2015, it announced five preliminary studies that make use of the software. Some studies leverage smartphone sensors to document patient symptoms; others used them to survey patients or to collect data that are input by the phone users. Chan was principal investigator on one such study, the Asthma Health study, which asked participants to answer questions about their health each day, then correlated that information with where the people had been. Thanks to Apple's marketing savvy, she says, uptake of the app wildly exceeded her expectations, with some 35,000 downloads and 3,000 participants fully enrolled and consented in just three days. Ultimately, however, the ResearchKit model may allow smartphone-based studies to move beyond observation to provide truly personalized health care. Jennifer Radin, an epidemiologist at the Scripps Translational Science Institute in La Jolla, California, is a member of a team that just released a ResearchKit-enabled app that will survey pregnant women about their symptoms. By tapping into a large and diverse subject pool, she says, the team hopes to use the data to offer personalized recommendations that are tailored to a person's body type or ethnicity, to identify complications earlier and even reduce to the number of visits to the doctor. Whether such benefits ever come to pass, one thing is certain: the global game of technical one-upsmanship between smartphone developers shows little sign of slowing. That's good news for consumers. And it's great news for science. \n                     The scientist and the smartphone \n                   \n                     Technology: Smartphone science \n                   \n                     Smartphones set to boost large-scale health studies \n                   \n                     Mobile-phone health apps deliver data bounty \n                   \n                     The DIY electronics transforming research \n                   \n                     Mobile science \n                   \n                     Nature  Technology Features hub \n                   \n                     Nature  blogpost on smartphone science \n                   \n                     MIT App Inventor \n                   \n                     Apple ResearchKit & CareKit \n                   \n                     Biomeme \n                   \n                     Cellscope \n                   \n                     LEGOLish \n                   \n                     Oxford Nanopore SmidgION \n                   Reprints and Permissions"},
{"file_id": "543743a", "url": "https://www.nature.com/articles/543743a", "year": 2017, "authors": [{"name": "Rosie Mestel"}], "parsed_as_year": "2006_or_before", "body": "Innovative techniques are giving researchers unprecedented access to the inner workings of the immune system.  The experimental cancer drug looked promising at first. But then the monkey results started coming in. The creatures were dying, poisoned by a treatment that was intended to target and kill pancreatic cancer cells. Nothing in the tissue samples collected by the scientists at Genentech in South San Francisco, California, had suggested this would happen, says team member Simon Williams. But when the researchers imaged live creatures, and tracked how the drug moved through the animals' bodies, they finally found the problem: the animals' bone marrow was greedily sucking up the antibody-based drug, killing the developing white blood cells inside the bones. The drug was abandoned. When biological drugs go into live bodies, researchers often have little idea of what is going to happen. Surprises greet them at a drug's earliest stages, and all the way through to the clinic. Sometimes patients will respond; sometimes they won't. Either way, researchers want to know why. But often, they lack the tools to find out. Imaging scientists and cancer researchers are now trying to change that, using antibodies and similar molecules in a technology known as immunoPET. As cancer therapies grow more precise and sophisticated, the researchers say, so too should the tools used to assess the treatments. Modern biological therapies work only for certain patients, and physicians can't yet reliably predict who those people are. And whereas biopsies can tell you what's happening in one part of one tumour, immunoPET can take snapshots of the entire body and every tumour in it. ImmunoPET marries positron emission tomography (PET), a technique that uses radioactive tracers to visualize the functions of human tissues, to an antibody's propensity to home in on the cells it's made to recognize. Interest in such imaging has heightened with the mushrooming of cancer immunotherapies, strategies aimed at revving up the body's immune system to fight tumours. But designing an immunoPET imaging probe isn't easy. The choice of radioactive tracer, antibody design and imaging kinetics all require thought. Still, scientists are making progress. They can now identify an increasing variety of immune-cell and cancer tissues  in vivo , and are tweaking antibody structures to improve their properties. New therapeutic and imaging strategies could be on the horizon. This 'immuno-toolbox' is sorely needed, says Sam Gambhir, chair of radiology at Stanford University in California, who works on molecular imaging for the early detection and management of cancer. \u201cMost therapeutic interventions we do are pretty much shooting blindly. We have no idea whether a therapy is working or not, especially in the early phases,\u201d he says. \u201cAll you can see is, does the tumour actually shrink? But if it doesn't shrink, you have no idea what went wrong.\u201d And that means you may not know what to do next. \n               Good PET \n             When defined narrowly, immunoPET is a tool that uses antibodies or related molecules as imaging agents. Researchers select one such molecule to recognize a specific protein on the cell of interest \u2014 PD-L1, for instance, which helps cancers to protect themselves from the patient's immune system, or CD8, which marks killer T cells. When injected into an animal, the antibody will travel through the body until it reaches its target cells and binds to them. So that they can see those cells, researchers label the antibodies with short-lived radioactive isotopes, generally zirconium-89 or iodine-124, which have half-lives of 3.27 and 4.18 days, respectively. PET labels emit positrons, which are the antimatter version of electrons. When positrons collide with electrons in the body, they produce a pair of \u03b3-ray particle, which rocket away from each other at 180\u00b0 angles. Simultaneous detection of the paired particles reveals the location and abundance of the target in the body. Researchers can then overlay those data onto computed-tomography or magnetic resonance imaging scans, to determine the label's position relative to anatomical landmarks. Biologists and clinicians are using immuno-PET to unravel why some patients \u2014 and not others \u2014 respond to cancer therapies. A few years ago, for instance, Elisabeth de Vries, a medical oncologist at the University Medical Center Groningen in the Netherlands, and her colleagues conducted immunoPET imaging of 56 people with advanced breast cancer who received trastuzumab emtansine (Kadcyla) 1 , a conjugate in which the anti-cancer antibody trastuzumab, which binds to the tumour protein HER2, is attached to a chemotherapy drug that poisons the target cell. Using radioactive labelling, the team found that in 29% of cases, patients' tumours didn't vigorously take up the antibody. That implies that the patients would be less likely to benefit from the therapy; indeed, the treatment in this group failed after a median of 2.8 months, compared with 15 months for those who did show antibody uptake. De Vries and her colleagues at Groningen and three other centres in the Netherlands are now conducting a trial to test whether up-front imaging such as this can improve treatment decisions for 200 women with newly diagnosed metastatic breast cancer; enrolment is slated to be completed later this year. In a separate study 2 , de Vries and others demonstrated that antibodies can reach a type of brain tumour known as a glioma when cancer has damaged the blood\u2013brain barrier, suggesting that, contrary to conventional wisdom, antibody-based treatments may be effective against this disease. \u201cThe idea was that antibodies were too big and could not get into the brain, and that's not the case,\u201d de Vries says. ImmunoPET's ability to scan the whole body can also help to address the fact that many cancers evolve quickly, and when they spread to other parts of the body, or metastasize, they may differ from the original tumour, and from each other. \u201cRarely does a patient come in with one metastasis. They can have multiple. And you can't biopsy them all,\u201d says Jason Lewis, director of the Center for Molecular Imaging and Nanotechnology at Memorial Sloan Kettering Cancer Center in New York City. But you can image them. In a 2016 immunoPET study 3  of nine patients, Lewis and his colleagues identified two women who had HER2-positive metastases whose primary tumour was HER2-negative; the women went on to benefit from trastuzumab (Herceptin) treatment. \n               Aiding immunotherapy \n             The many immunotherapies under development might also benefit from accompanying immunoPET analyses, says Antoni Ribas, a medical oncologist at the University of California, Los Angeles (UCLA), who is researching treatments for malignant skin cancers known as melanomas. Take, for instance, an approach that tackles cancer cells' defence against T-cell attack. This brake, called an immune checkpoint, occurs when a receptor on a tumour cell interacts with a receptor on the surface of a T cell. By administering antibodies that bind to, and block, either of those receptors, researchers should be able to free up the T cells clustered around the tumour to move in and kill.  But that, says Ribas, assumes that those killer T cells, which are identifiable through the cell-surface protein CD8, are positioned near the tumour. This isn't always the case. With immunoPET, \u201cyou could see if the immune system is ready to be turned on or not\u201d, he says. \u201cIf there's no CD8 cells pre-existing where the tumours are, there may be nothing to be activated.\u201d Such patients would warrant a different approach. Treatments that release immune checkpoints can also induce nasty side effects when immune cells go into overdrive and wreak havoc in other tissues, Ribas notes. Gut inflammation, for example, is a common side effect of the melanoma checkpoint-inhibitor drug ipilimumab (Yervoy). Perhaps, he speculates, immunoPET could allow clinicians to detect this early. Clinical questions such as these led Ribas to collaborate with UCLA imaging scientist Anna Wu, co-founder of the imaging company ImaginAb in Inglewood, California, who is tinkering with antibodies to optimize them for immunoPET. In 2015, Wu's team, with Ribas and others, showed that they could use immunoPET to track killer T cells in mouse studies of three types of immunotherapy 4 . The radiolabelled probe, a shortened fragment of antibody targeting the CD8 receptor, revealed T cells amassing in tumours and altering their distribution in other parts of the body. ImmunoPET can even reveal whether a treatment is starting to work at an early stage, before the tumours start to shrink, says Wu. Indeed, researchers and clinicians have sometimes been led astray when tumours seemed to enlarge, rather than shrink, with initial treatment, when immune cells flooded the site and swelled the tissue. ImmunoPET can expose the difference between the cell types, and show that tumour cells are starting to respond to treatment before they visibly begin to die, says Wu. In drug-development labs, immunoPET can help to guide decisions on whether to keep pursuing a therapeutic antibody, says Williams. In preclinical work, immunoPET confirmed the likely promise of an antibody called STEAP1, which targets metastatic prostate cancer cells and poisons them with an attached drug. In early (unpublished) clinical trials, imaging showed the antibody component could even home in on metastases in tissues once thought to be inaccessible, such as bone. \u201cThat used to be a discussion point in the field: can you deliver an antibody into bony metastases?\u201d Williams says. \u201cThe imaging immediately showed us, yes, you can.\u201d And then there was the case of the antibody\u2013drug conjugate that unexpectedly harmed non-human primates. Williams's immunoPET study was instrumental in working out why that drug was so lethal. \n               Express delivery \n             Monoclonal antibodies produce gorgeous immunoPET images, researchers say. But they can take up to a week to make their way to the tissue of interest and clear out of general circulation in the body, to produce images with good contrast and specificity.  Sometimes, that time scale is acceptable. In drug development, for example, researchers need to make decisions about whether to pursue drug candidates, and generally don't have time to wait for the creation of faster imaging agents of similar quality. \u201cManagement has to decide: are we going to pay for a phase III trial or not? If the imaging isn't done in time for that decision, it's pointless,\u201d Williams says. But in the clinic, time is an overarching concern for imaging tests. \u201cThey are used in advanced-cancer patients, often with metastasis,\u201d says Sridhar Nimmagadda, a molecular-imaging scientist at Johns Hopkins University in Baltimore, Maryland. \u201cFor a patient in such a dire state to be able to come back for one more scan would be very difficult.\u201d So researchers are pursuing molecules that retain the precision of antibodies but are smaller, and have more desirable pharmacokinetics. Wu's lab, for example, has engineered slimline antibody variants called 'minibodies' and 'diabodies'. These retain the parts of the antibody that interact with antigens (known as variable domains), but lose those that engage other parts of the immune system, such as the cells that clear away bacteria and debris. So other than the ability to bind to their targets, they are inert. And with them, patients could go from injection to imaging in under a day, Wu says. These protein variants have other useful properties, too, Wu says. For instance, whether they exit the body through the liver or the kidney depends on their size. To improve the contrast and get a clear image of a pelvic tumour, clinicians would select a design that clears through the liver; for pancreatic cancer, a renal route would be desirable. Wu can also alter these engineered antibody fragments to render them as 'human' in sequence as possible to minimize immune reactions against the imaging agents (because they are not native human proteins), or so that the radioisotope label attaches to the protein in a uniform way, which can help to ensure consistent and reliable activity. \u201cThese are the things we can do as protein engineers,\u201d Wu says. \u201cAs long as you're going to produce a recombinant protein, you might as well optimize everything you can.\u201d Other labs, such of that of molecular biologist Hidde Ploegh at Boston Children's Hospital in Massachusetts, exploit the antibodies of llamas, alpacas and camels to obtain faster-acting immunoPET agents. These camelid species produce antibodies that consist of just one type of chain instead of the usual two, and weigh just one-tenth as much as conventional antibodies. But they also clear faster than conventional antibodies and penetrate more deeply into tissues. Anything that allows a patient to get scanned and out of the office in a few hours would work, says radiologist and molecular-imaging scientist Martin Pomper, who directs Johns Hopkins' division of nuclear medicine and molecular imaging, where Nimmagadda works. But Pomper and Nimmagadda have come to believe that the best approach for the clinic lies not with antibodies or stripped-down versions thereof, but with even smaller peptides and other low-molecular-weight molecules. The team has developed a radiolabelled peptide that latches onto PD-L1 and is ready for imaging in two hours, he says. The group is seeking even smaller molecules. \n               Labelling two-step \n             Another trick to retain the precision of antibodies involves taking a two-step approach. Researchers infuse an antibody, then wait the needed week for any antibody that doesn't bind to a target to clear the body. Then they inject a second, smaller labelled probe, which rapidly binds to the previously administered antibody. That would allow them to use isotopes with even faster radioactive decay, such as fluorine-18 or copper-64, which have half lives of less than 2 hours and 12.7 hours, respectively. The method depends on the creation of 'bispecific' antibodies, which have two binding sites \u2014 one for the protein target (such as PD-L1) and one for the radiolabelled probe, called a hapten. Like immunoPET itself, the two-step strategy is not a new idea, says Steven Larson, head of the molecular pharmacology programme at Memorial Sloan Kettering. But today's advances in imaging and antibody engineering make it an exciting one, he says. Larson is confident that the approach can breathe new life into an old therapeutic strategy: radioimmunotherapy, which uses antibody specificity to deliver radioactive poison to tumours. Modern imaging enables physicians to calibrate doses exquisitely to spare healthy tissues, Larson says. Very few people are cured of solid tumours if they're not caught early, he adds. \u201cWe want to bring targeted radiotherapy into this in a big way, and we feel it's very practical to do that.\u201d Researchers who develop and test cancer therapies often don't realize how imaging can help them, Gambhir says. But they're learning. Gambhir says that the January publication of a paper 5  showing that engineered T cells could be tracked by PET as they homed in to gliomas in an immunotherapy trial generated a flurry of inquiries from industry. \u201cI've been inundated by drug companies,\u201d he says. \u201cAs soon as they see the human results, they get very excited.\u201d \n                     Cancer: Novel PDL1 inhibitor identified \n                   \n                     Non-invasive nuclear imaging for localization of viral reservoirs \n                   \n                     The cancer vaccine resurgence \n                   \n                     Cocktails for cancer with a measure of immunotherapy \n                   \n                     Natural killer cells blaze into immuno-oncology \n                   \n                     Immuno-oncology drugs jostle for first-line setting \n                   \n                     Web focus: Tumor immunology & immunotherapy \n                   \n                     Nature  articles on cancer immunotherapy \n                   \n                     Nature  Reviews Drug Discovery focus on Cancer Immunotherapy \n                   \n                     ImaginAb \n                   \n                     PNAS article 'Noninvasive imaging of immune responses' \n                   Reprints and Permissions"},
{"file_id": "547477a", "url": "https://www.nature.com/articles/547477a", "year": 2017, "authors": [{"name": "Jeffrey M. Perkel"}], "parsed_as_year": "2006_or_before", "body": "After tackling the genomes of bacteria and yeast, synthetic biologists are setting their sights on rewriting those of more complex organisms, including humans. Under typical laboratory conditions, strain JF1 of the bacterium  Escherichia coli  looks like any other \u2014 a spatter of yellow-tinged colonies on an amber agar plate. But bathe the colonies in wavelengths of red, green or blue light and their cells convert chemicals in the growth medium into pigments in a pattern that matches that of the coloured light to which they were exposed, yielding a muted and blurred image that is reminiscent of a 1970s Polaroid. Christopher Voigt, whose lab at the Massachusetts Institute of Technology in Cambridge created the intricate genetic circuit that drives this transformation, reported in May 2017 that his team had used the system to recreate a multicoloured geometric illustration of lizards by Dutch artist M. C. Escher 1 . That exercise was just for fun, he says, and a way to demonstrate the state-of-the-art in synthetic biology. But it was not easy: the circuit contained 18 genes and 32 regulatory elements, spread over 4 small circular molecules of DNA known as plasmids, and 46,198 base pairs of DNA. It responds separately to red, green and blue light. \u201cWhen you add it all up, it's quite a sophisticated project,\u201d Voigt says. And it's not the only one. Synthetic biology is awash with projects of similar or even greater complexity. Improvements in techniques for synthesizing and editing DNA have brought reduced costs and enormous precision, helping biologists to build from scratch or re-engineer the genomes of microorganisms such as  E. coli  and brewer's yeast ( Saccharomyces cerevisiae ). Synthetic-biology researchers are now having serious discussions about re-engineering the genomes of more-complex organisms, including humans, although substantial hurdles stand in the way. For instance, the manipulation of large pieces of DNA presents technical challenges, and despite the falling cost of DNA synthesis, the cost is still prohibitive when billions of bases must be rewritten. \u201cResults over the past two years have certainly increased my optimism that we may be able to do some really profound engineering in animals,\u201d says Peter Carr, a synthetic biologist at the MIT Lincoln Laboratory in Lexington, Massachusetts. In a 2015 letter 2  to the journal  Trends in Biotechnology , Carr asked: \u201cIs there a synthetic biology equivalent of the sound barrier, or of the speed of light?\u201d The question was rhetorical because immutable limits clearly exist \u2014 the growth rate, for example, cannot be infinitely fast. But what constitutes a sound barrier in synthetic biology is evolving, he says. Designs that were on the cusp of feasibility a few years ago are now practical. Researchers who once struggled to produce a few kilobases of synthetic DNA are now building whole genomes on the scale of megabases. In March 2016, sequencing and synthetic biology pioneer Craig Venter and his colleagues announced that they had pruned and rewritten the genome of the bacterium  Mycoplasma mycoides  from about 1 megabase to 531 kilobases to create a 'minimal' genome 3  \u2014 the smallest set of genes that is required for life. In August 2016, researchers led by George Church, a geneticist at Harvard Medical School in Boston, Massachusetts, and Nili Ostrov, a postdoctoral researcher in his lab, reported that they had produced a bacterium, dubbed 'rE.coli-57', in which seven codons \u2014 the triplets of nucleotides that encode particular amino acids \u2014 had been stripped out and replaced with synonymous alternatives 4  in a process known as genetic recoding. And in March 2017, a team led by Pamela Silver, a biochemist at the Wyss Institute for Biologically Inspired Engineering at Harvard University in Boston, Massachusetts, described its initial attempts to recode the genome of strain LT2 of the bacterium  Salmonella typhimurium 5 , replacing around 200 kilobases of genomic DNA and eliminating a specific leucine codon in the hope of preventing the transfer of genes between pathogenic microbes. Most dramatically, in March 2017, an international consortium led by Jef Boeke at the New York University Langone Medical Center and Joel Bader of the Johns Hopkins University in Baltimore, Maryland, reported the end-to-end rewrite of 5 of the 16 chromosomes of  S. cerevisiae 6  \u2014 a milestone in an international project called the Synthetic Yeast Genome Project (Sc2.0). Sc2.0 aims to optimize and synthesize the complete genome of  S. cerevisiae  for both industrial and pure research applications. For instance, says Boeke, by removing all DNA sequences that do not encode proteins (introns), the team can assess the biological roles of the cellular machinery required to handle those genetic elements. The artificial yeast chromosomes designed for Sc2.0 have been streamlined and stabilized by deleting repeated sequences and introns, by moving sequences that encode crucial pieces of the protein translational machinery to a dedicated chromosome, and by eliminating the codon TAG, which signals a stop in translation, and replacing it with an alternative stop codon, TAA, to facilitate protein engineering. A customized software package called BioStudio enabled the team to manage the genetic bookkeeping required to complete such a massive task. The logistics of Sc2.0 were substantial, says Patrick Yizhi Cai at the University of Edinburgh, UK, who is the project's international coordinator. Yet the actual process of editing the yeast chromosomes was fairly routine, requiring just a few plates of yeast in the incubator, says Leslie Mitchell, a postdoc in Boeke's lab who led the subgroup that synthesized chromosome VI. The Sc2.0 team uses a strategy called SwAP-In to rewrite chromosomes piece by piece (see 'Rebuilding a chromosome'). Researchers first assemble short single-stranded molecules of DNA known as oligonucleotides into building blocks of about 750 bases, and then into 'chunks' of 10 kilobases or fewer that, in turn, are combined into 'megachunks' of 30\u201360 kilobases. Each megachunk contains one of two marker genes that enable the selection of yeast carrying these genes \u2014 in this case,  URA3 , which enables yeast to grow in the absence of uracil, and  LEU2 , which enables growth when leucine is missing. The megachunks are then slotted into an existing chromosome through homologous recombination, a natural process in which one stretch of DNA is replaced with another, rewriting the DNA from one end to the other. As each subsequent segment is integrated, it replaces the marker gene of the previous segment, swapping the yeast cell's nutritional requirements between uracil and leucine. Following quantitative PCR analysis to ensure that each megachunk has been fully incorporated, the resulting yeast strain is tested for its ability to form colonies under relatively stringent conditions; its slowed growth or death in comparison to the wild type indicates a problem in need of repair. \u201cThe idea is: integrate the megachunk, test the fitness\u201d, and repeat, Mitchell explains. \n               Writer's block \n             A single megachunk can be integrated and tested in about two weeks, Mitchell says, assuming that there are no problems. Fitness testing and 'debugging' (error correction) \u201ctake longer than the actual build at this point\u201d, says Boeke. Each chromosome completed so far presented only a handful of notable 'bugs', he says. Some stemmed from errors in genome annotation, whereas others were caused by codon replacements that, for instance, alter the secondary structure of RNA. For the most part, the yeast rolled with the punches. Yet the glitches that did crop up hint at the challenges faced by a larger-scale engineering project called Genome Project-write (GP-write), which aims to rewrite the genomes of more-complex eukaryotes. Besides the obvious problem of scale \u2014 at 3 billion base pairs, the human genome is two orders of magnitude larger than that of  S. cerevisiae  \u2014 the genomes of more-complex organisms tend to be less well annotated. When Venter's team first tried to build a minimal genome in  M. mycoides , it applied a rational design, using published genetic data to compile a list of essential genes \u2014 an approach that didn't work. \u201cOur lack of basic biological knowledge, even with the simplest bacterial genomes, is huge,\u201d Venter says. Success came instead from a top-down approach that whittled away the genome to arrive at a core set of 473 genes. But about one-third of those have no known function. \u201cI found that to be just kind of a mind-blowing result,\u201d Silver says. And there are further challenges. Sc2.0 and other genome-rewriting projects have tended to steer clear of the regulatory regions of genes, but in more-complex organisms such as eukaryotes, these are often located far from the genes that they influence, and might not yet be fully mapped. Researchers may therefore not know which segments to rewrite, and which to leave alone. It is also unclear how such large-scale genomic changes might affect chromatin architecture and therefore gene expression. On a practical level, chromosome-sized molecules of DNA cannot be easily manipulated without being broken, and there is no efficient way to deliver them into most eukaryotic cells. Even if scientists can deliver the DNA, they might not be able to integrate it into the genome because most such cells are unable to perform homologous recombination as readily as yeast, and their slower growth drags out each experimental step. There also is the cost of synthetic DNA to consider. Silver's team received funding from the US Defense Advanced Research Projects Agency for her work in  S. typhimurium , which allowed it to negotiate a favourable price for DNA synthesis. But at a per-base price of US$0.10, she says, it will cost more than $1 million to complete her project; the human genome, by comparison, would cost hundreds of times more. Yet Church says it is just a matter of time before technology catches up with ambition. \u201cMy guess is, it's going to get easier and easier with time to build large genomes.\u201d \n               Precision rewrite \n             Genome rewrites so far have largely stuck to nature's recipe. But ultimately, biologists hope to impart new functions. Several projects, including the rE.coli and  S. typhimurium  studies, are focusing on genetic recoding, in which codons removed from the genome are freed for other uses. Jason Chin, a synthetic biologist at the MRC Laboratory of Molecular Biology in Cambridge, UK, has done extensive work to manipulate the genetic code. He says that such recoding can advance protein engineering, not to mention the design, testing and synthesis of new chemical polymers built from monomers other than standard amino acids. Other possible applications include biocontainment (preventing release of an organism outside the lab) and genetic isolation (protecting organisms from viral infection). In 2005, while working as a postdoc with Church, Farren Isaacs, now a bioengineer at Yale University in New Haven, Connecticut, began to pursue the idea of recoding the  E. coli  genome, focusing his energy on replacing the stop codon TAG throughout. Because  E. coli  contains just 321 TAG codons, Isaacs was able to accomplish this task by modifying an existing genome rather than synthesizing one from scratch 7 . Using a strategy called MAGE (multiplexed automated genome engineering) that enables multiple DNA sequences to be edited at once, Isaacs and his colleagues first divided the  E. coli  genome into 32 segments and altered the TAG codons in each to the synonymous codon TAA. Next, they joined the 32 modified segments into a single molecule by exploiting a natural process of genetic exchange between bacteria. To complete the recoding process, the team deleted a gene that encodes a protein known as RF1, which recognizes the codon TAG. (A related protein, RF2, recognizes the codon TAA.) Survival of the modified  E. coli  following removal of this otherwise essential gene showed that their recoding process had worked. For many researchers, existing technologies provide all the power they need to hack the genome. eGenesis, located in Cambridge, Massachusetts, is using the gene-editing tool CRISPR to turn pigs into sources of transplantable organs. Luhan Yang, cofounder of the company, explains that the idea is to pare back the pig genome using CRISPR to remove sequences encoding proteins that might evoke an immune response in people. New genes encoding proteins that help to make the pig tissues compatible with humans can also be introduced. \u201cWe think dozens of modifications probably would suffice,\u201d she says. Yet the approach is different for more-expansive projects. Ostrov and Church's rE.coli project, for instance, removed the TAG stop codon and two codons each for serine, arginine and leucine from  E. coli  to create a 57-codon strain 4 . That work required 62,214 changes, which the team made using bottom-up DNA synthesis rather than top-down editing. With so many necessary genetic modifications, Ostrov explains, \u201cwe might as well make the genome from scratch.\u201d None of these genome-hacking studies has actually built a chromosome-sized molecule of DNA in one continuous stretch. Most commercial suppliers of synthetic DNA rely on a decades-old method of synthesis that is unsuitable for producing molecules longer than about 200 nucleotides. Church's team, as do most groups pursuing genome synthesis, assembled the DNA it required hierarchically. It purchased pre-made segments of genes that were 2\u20134 kilobases long, assembled them into 50-kilobase-long blocks in yeast using homologous recombination, and transferred these completed segments into  E. coli . The team then deleted the corresponding region of the  E. coli  genome, and tested the resulting strain of bacteria for fitness. According to Ostrov, the recoding process went smoothly, despite a few bugs. For example, altering the coding sequence of a particular gene inadvertently weakened the promoter of an overlapping gene, reducing the fitness of the strain. \u201cThere's idiosyncratic information in the genome,\u201d explains Chin, and it can only be deciphered experimentally. \n               Circuit city \n             Other researchers are developing genetic circuitry to imbue genomes with new functionality. Generally, these circuits \u2014 such as Voigt's image-capturing strain of bacteria \u2014 are built up from simpler designs that use proteins called transcription factors as positive or negative input and output signals. Wilson Wong, a biomedical engineer at Boston University in Massachusetts, builds his designs instead using enzymes known as recombinases that invert or delete segments of DNA \u2014 a design strategy called BLADE (Boolean logic and arithmetic through DNA excision). Wong says that BLADE frees researchers from the difficulty of linking circuits to one another, which requires the output strength of one circuit to be matched with the anticipated input of the next. In one demonstration, Wong and his team created a Boolean logic look-up table 8  \u2014 a genetic circuit, about 10 kilobases long, that is capable of turning into any of 16 possible logic gates depending on whether 6 recombinases are present. Wong's team essentially designed that circuit using a pencil and paper. But ultimately, synthetic biologists hope to build their designs  in silico . Voigt, working with Douglas Densmore, an electrical engineer at Boston University has developed a tool called Cello (cellocad.org) to make that possible. Researchers specify genetic-circuit designs in a programming language called Verilog, and Cello produces the DNA sequences that are required to make them work 9 . Despite their seeming simplicity, the genomes of microorganisms demonstrate an incredible capacity for subtle genetic control, Voigt says. \u201cWe're almost taunted by what exists in nature.\u201d But through a combination of genome editing, genome synthesis and cleverly designed tools, researchers are slowly rebalancing the scales. \n                     Engineering RGB color vision into Escherichia coli \n                   \n                     'Minimal' cell raises stakes in race to harness synthetic life \n                   \n                     Programming cells by multiplex genome engineering and accelerated evolution \n                   \n                     Digital-to-biological converter for on-demand production of biologics \n                   \n                     Collection: Nature Research Tech \n                   \n                     Nature special: Synthetic biology \n                   \n                     Nature Methods focus: Synthetic Biology \n                   \n                     Blog post: Genome editing meets version control \n                   \n                     Blog post: How to build long DNA \n                   \n                     Genome Project-write \n                   \n                     The Synthetic Yeast Genome Project (Sc2.0) \n                   \n                     Integrated DNA Technologies \n                   \n                     Twist Bioscience \n                   \n                     SGI-DNA \n                   \n                     Molecular Assemblies \n                   Reprints and Permissions"},
{"file_id": "548485a", "url": "https://www.nature.com/articles/548485a", "year": 2017, "authors": [{"name": "Monya Baker"}], "parsed_as_year": "2006_or_before", "body": "Chemical probes and screening libraries can easily get mixed up or messed up, causing misleading results for unwary biologists. Stefan Knapp's results made him feel like he was riding a roller coaster. Knapp, a pharmaceutical scientist at Goethe University Frankfurt in Germany, and his colleagues had identified a protein important for cancer growth. They had also found a drug-like molecule that inhibited the protein. They bought more of that compound to run further experiments \u2014 but this time it was inactive. When they then bought the same compound from another vendor, it was even more active than in the first set of experiments. It turned out that the molecule in question exists in the form of enantiomers \u2014 chemical structures that are mirror images of each other but still distinct, like right-handed and left-handed gloves. Knapp worked out that vendors were selling mixtures with varying proportions of the two enantiomers, and only the left-handed form was active in the team's assays 1 . (Ironically, the right-handed version is the active form of the lung-cancer drug crizotinib, which acts through a different mechanism.) The team's detective work speaks to just one of the many ways in which chemical reagents can thwart biological experiments, says Guilio Superti-Furga, a systems biologist at the Research Center for Molecular Medicine in Vienna, and a leader of the work. In some cases, scientists don't know what chemicals they have in their hands. In others, molecules' effects are less specific than experimentalists imagine. \u201cThese two problems, together, reduce the wonderful potential impact of using chemistry to interrogate biology,\u201d says Superti-Furga. Kim Janda, an immunologist and chemist at the Scripps Research Institute in La Jolla, California, puts it more bluntly: \u201cIf you don't pay attention to the chemistry, the chemistry will bite you in the ass.\u201d \n               Bite marks \n             Researchers rely on chemical reagents across all areas of cellular biology. One application is as tool compounds or chemical probes that dissect a protein's function. Using a small molecule to inhibit a specific enzyme, for instance, can offer subtler clues to the protein's biology than using genetic techniques to keep it from being made altogether. Compounds are also sometimes collected in chemical libraries, where they are screened en masse in the hope of finding useful reagents and pharmaceuticals. In both situations, mix-ups, impurities and unanticipated activity can send unsuspecting scientists on wild goose chases. Scientists have sounded alarms about chemical-based artefacts in assays over the years 2 , 3 , 4 , but recognition of these problems is still not widespread. Some online resources can help. The expert-curated Chemical Probes Portal ( www.chemicalprobes.org ) assesses more than 100 individual tool compounds. Probe Miner ( http://probeminer.icr.ac.uk/ ) and the Probes and Drugs Portal ( www.probes-drugs.org ) aggregate publicly available information to help researchers select which chemicals to use. Actually assessing the quality of the chemicals that researchers buy isn't easy. Reagents often contain by-products of synthesis, or impurities formed when the reagent degrades. Chemist Josh Bittker, who heads a high-throughput screening group at the Broad Institute in Cambridge, Massachusetts, had a chance to find out just how common those contaminants can be when he and his team assembled a library of compounds that had already been tested in clinical trials. To his surprise, nearly 29% of the 8,584 molecules the team tested failed quality control, with impurities making up 15% or more of some batches of reagents 5 . Often, material from another manufacturing lot from the same vendor passed the check, especially if supplied as a dry powder rather than a solution, a form that is convenient for experiments but prone to degradation. The problem could be even worse. Bittker's screens assessed compounds by molecular weight and so would not have detected whether samples contained multiple isomers, molecules with the same chemical formula but a different arrangement of atoms. \n               Chemicals galore \n             Medicinal chemists like to say that there are more possible structures for drug-sized molecules than there are stars in the Universe. Between them, commercial vendors probably sell more than 10 million different compounds. Researchers looking for a particular molecule might have to track down specialized vendors, get their supplies from other interested researchers or have compounds synthesized to order. However, if a molecule's biological activity has been reported in a high-profile paper, and especially if it has been tested in clinical trials, it might be sold by a dozen vendors or more. Pharmaceutical companies sometimes license vendors to produce or distribute reagent-grade versions of their drugs; this results in more reliable reagents, but also higher prices. In one infamous case, nearly 20 vendors offering an approved cancer drug called bosutinib were found to actually be selling a related structure in which chemical groups were misarranged (see 'Spot the difference' and  go.nature.com/2w3dz0a ). Both bosutinib and the second structure bind a suite of cell-signalling proteins, but with different potencies, so the mix-up potentially calls into question dozens of papers. Dimitrios Tzalis, chief executive of the contract-research organization Taros in Dortmund, Germany, recommends buying from reputable vendors rather than trying to save money with untested sources. \u201cCheap can be very expensive,\u201d he says. When it comes to buying reagents, \u201cyou have to know what you're doing and what to look for\u201d, Bittker agrees. \u201cIf a deal is too good to be true, it probably is.\u201d One thing to watch out for, he says, is a vendor that is unwilling to discuss or supply quality-control data. Many buy compounds from third parties to resell to scientists, which can make for variable quality. \u201cIf there is not a chain of information back to someone who experimentally confirmed the sample, it is not to be trusted,\u201d Bittker says. Sometimes confusion stems from the literature. When chemical biologist Kilian Huber at the University of Oxford, UK, read reports that an enzyme inhibitor called SCR7 could boost the efficiency of the CRISPR\u2013Cas9 gene-editing technique, he decided to try it in his lab. He ended up abandoning the project when his graduate student could not synthesize a compound matching the reported structure. The situation exemplifies the main uncertainties of using chemical tools: the chemical-reagent company Tocris in Bristol, UK, later reported that multiple SCR7 vendors were in fact selling a structure related to, but not the same as, the molecule (see  go.nature.com/2vapstf ), and proposed that the originally reported form was inactive. Separately, other researchers questioned whether any of these molecules acted by inhibiting the specific enzyme described 6 . The original discoverer of the inhibitor tells  Nature  that his 2012 paper 7  reports inhibition of the correct enzyme, but that the structure of SCR7 that it describes is unstable. Instead, he says, SCR7 converts to a cyclic form not described by that paper or by Tocris. Janda also encountered a mix-up when he decided to do some follow-up studies on a molecule described 8  as boosting expression of a tumour-suppressor protein. The molecule turned out to be inactive, and at first, Janda doubted his postdoc's organic-synthesis skills. Then he realized that other studies had not synthesized the compound themselves, but had used reagents supplied by distributors, including the US National Cancer Institute. Those distributors had perpetuated a mistake in the original publication and supplied the wrong molecule. In fact, a company pursuing clinical trials with the compound had originally licensed intellectual property documenting the incorrect structure. That patent was eventually reissued, but not before Janda caused an uproar by filing a patent application for the correct version. It is impossible to know how common these kinds of mix-up are, Janda says, but certainly many go undetected and unreported. Scepticism is key, says Nick Levinson, who spent months as a postdoc trying to work out how bosutinib bound its protein target before showing conclusively that he was not working with bosutinib at all \u2014 and neither were many other researchers studying the compound 9 . \u201cThe main thing I do differently in my lab is, I am more suspicious with results. If we get any result that seems wrong or is surprising, the first thing that pops into our head is that maybe the compound isn't right,\u201d says Levinson, now a medicinal chemist at the University of Minnesota in Minneapolis. Detecting the problem isn't always easy. For bosutinib, the process required a relatively specialized form of nuclear magnetic resonance (NMR) spectroscopy and was aided immensely by a published crystal structure of a protein binding the isomer, he says. But researchers frequently use reagents without doing any authentication. \n               Impure products \n             Whenever using a new chemical probe, researchers should use at least two kinds of assay to verify its identity, or find colleagues who can, advises Paul Workman, head of the Institute of Cancer Research in London. \u201cWe are trying to educate the biological community to be more respectful of the need for chemical knowledge.\u201d A good biologist would never work with an antibody before doing some control experiments, or use a cloned gene from a colleague without sequencing it first, he says. They should be just as careful about using a chemical probe without testing it first by mass spectrometry, NMR or other methods (see  'Six tips for better chemistry' ). Even if the chemical supplied contains the correct compound, the results obtained with it might still be wrong. As a research scientist at Roche in Nutley, New Jersey, Johannes Hermann led a drug-discovery effort that identified a series of molecules that seemed to inhibit an exciting enzyme 10 . As expected for such molecules, the higher their concentration, the greater the inhibition, a signal that these results were not mere artefacts. Then, the medicinal-chemistry efforts stopped making sense: a tiny tweak to a molecule's structure might make a big difference to its activity, whereas a larger one had no effect. Finally, Hermann and his colleagues realized that the active compounds had something in common: they had been synthesized in the presence of zinc. When the researchers added zinc alone to their assays, they got the same results as they had seen with the supposedly active compounds. Just to be sure, they added metal-binding molecules known as chelators along with the compounds, and found no activity. It was a \u201cpainful experience\u201d, says Hermann, who is now a data scientist at Johnson & Johnson in Raritan, New Jersey. He recommends that no one invest time or resources following up on a hit before they have explored whether a chelator changes the results. Hermann's experience is unusual for industry only in that he could take the time to write it up. Other pharmaceutical chemists describe similar frustrations, but with copper and palladium as the culprits. Small, inorganic compounds used in synthesis, such as hydrazine, can also foil experiments \u2014 by inhibiting the target enzyme or by altering how its activity is assessed. Often, problems with a reagent start after the vial is opened. Repeated freezing and thawing can degrade a compound, and some chemicals aren't stable to freezing at all, says Heather Holement, head of life-science reagents at Merck KgaA in Darmstadt, Germany. Chemicals dissolved in organic solvents can also sometimes crash out of solution when added to water-based environments, such as those in cell and protein assays. Researchers should consider the quality of stocks, and always run control experiments with solvent alone, she advises. They can also rule out some false signals by running assays without cells or proteins. Sometimes a compound's apparent activity is actually due to the 'vehicle' that carries it. Jake Shortt, a haematologist at Monash University in Melbourne, Australia, followed a common protocol of dissolving anticancer compounds in a solvent known as NMP ( N -methyl-2-pyrrolidone) before injecting them into mice, and was surprised to see the tumours respond even in control groups that received the solvent but none of the test compounds 11 . Those data suggest that NMP itself has anticancer activity, and Shortt is now starting to test it in the clinic. But others still use NMP as a supposedly inert liquid to dissolve drug compounds, he says. For the types of screening experiment that Bittker runs, the time for the most intensive diligence comes when hits are first identified. It is not uncommon for stocks in chemical libraries to sit around for months or even years, which allows plenty of time for degradation and simple mix-ups. The first step in validating results, he says, is to run the screen again, using the same material; screening assays can be noisy. Next, researchers should try the same compound from a different source, ideally synthesized in-house or by collaborators. If it turns out that a hit is not the expected molecule, Bittker suggests not trying to pin down the active component. \u201cIt's a rabbit hole,\u201d he says. \u201cIf you have a contaminant, just let it go.\u201d Such advice may need to be turned on its head for researchers who are screening natural products. Extracts of plants and similar sources are almost always mixtures rather than pure reagents, says Guido Pauli, a natural-products specialist at the University of Illinois at Chicago. In many cases, he says, the purer the extract, the lower its activity, which means that some effects attributed to the most common ingredient are in fact caused by other components 12 . Even when natural products are relatively well characterized, mixtures can be a problem. For example, gentamicin is an antibiotic produced industrially by isolating it from bacteria grown in fermentation tanks. The bacteria produce several closely related molecules that have different activities and toxicities, but commercial preparations contain varying proportions of each form and different water content, says Robert Greenhouse, a medicinal-chemistry consultant for Stanford University. As a result, researchers who are not careful will not know exactly how much or what forms of antibiotic they are using in their experiments. Even if a molecule is correctly identified, it may exert effects without actually binding to a specific protein in a specific way. For example, 'aggregators' cover other molecules in a soap-like coating. These artefacts can often be revealed by adding detergent to assays. \u201cSometimes you have the right compound, but it's just a lousy compound,\u201d says Workman. Resources and education are crucial, he says, but the onus is on individual scientists. \u201cIt really is the responsibility of every researcher using any small-molecule compound that the material they are using is correct and fit for purpose.\u201d When it comes to using chemical reagents, a simple maxim is more important than any hard-and-fast rules, says Workman. \u201c Caveat emptor  \u2014 let the buyer beware.\u201d \n                     1,500 scientists lift the lid on reproducibility \n                   \n                     Reproducibility: Seek out stronger science \n                   \n                     Reproducibility: Respect your cells! \n                   \n                     Cancer reproducibility project releases first results \n                   \n                     Announcement: Towards greater reproducibility for life-sciences research in Nature \n                   \n                     Nature  Technology Hub \n                   \n                     Nature  special: Challenges in irreproducible research \n                   \n                     Chemical Probes Portal \n                   \n                     Probes and Drugs Portal \n                   \n                     Probe Miner \n                   Reprints and Permissions"},
{"file_id": "550285a", "url": "https://www.nature.com/articles/550285a", "year": 2017, "authors": [{"name": "Michael Eisenstein"}], "parsed_as_year": "2006_or_before", "body": "Offering long reads and rapidly improving accuracy, nanopore sequencing has the potential to upend the DNA sequencing market. Christopher Mason has a trick that he likes to break out at conferences. By harvesting DNA from swabs collected from a volunteer's phone, he and his colleagues can perform on-site ancestry analyses within an hour, and even recount details of a donor's day. \u201cWe were able to predict who had just eaten an orange, and who had eaten pork, from what was left on their phones,\u201d says Mason, a computational biologist at Weill Cornell Medicine in New York City. Mason achieves this speedy analysis using a handheld sequencing device called MinION, developed by the UK firm Oxford Nanopore Technologies (ONT). MinION reads sequence information by threading long DNA strands through a tiny aperture known as a nanopore and detecting minute changes in electrical current caused by DNA's four component nucleotides. Mason's demonstrations provide a light-hearted illustration of the device's capabilities, but early users have also racked up some high-profile scientific achievements. MinION played a prominent part in monitoring the 2015 Ebola virus outbreak, has voyaged to Antarctica and even gone into orbit. But MinION \u2014 which is roughly the size of a deck of cards \u2014 accounts for a relatively small fraction of the world's sequencing output, which is still dominated by Illumina of San Diego, California. Illumina has a nearly 10-year head start, but ONT and its users are also grappling with technical challenges, most notably higher error rates. Meanwhile, competing firms are hoping to surpass ONT with innovative spins on this conceptually simple but technically complex sequencing strategy. \n               A rocky start \n             Illumina's ubiquitous DNA sequencing technology generates vast numbers of exceedingly accurate 'short reads' by sequentially reading the bases that are incorporated into a sample during a DNA replication reaction. These strings of sequence data, which span a few hundred nucleotides, can then be computationally assembled into overlapping 'contigs' containing millions of nucleotides. For nanopore sequencing, entire DNA fragments are analysed directly by physically threading them through a nanoscale pore. Costing US$500\u2013900 each, MinION's flow cells contain hundreds of nanopores, so they can analyse many molecules in parallel. The system applies voltage across each pore, and as an enzyme steadily draws the DNA through, the nucleotides block the flow of ions and produce tiny changes in electrical current, which are interpreted by specialized software (see 'Nanopore sequencing'). The resulting 'long reads' can span thousands of nucleotides, simplifying assembly. On its release in 2014, the genomics community was tantalized by MinION's potential, but early users encountered numerous challenges. \u201cIt was a lot of effort to sequence just a single bacterial genome because the output was low, and the single-read accuracy was also pretty low,\u201d says Nicholas Loman, a microbial genomicist at the University of Birmingham, UK, and one of MinION's earliest adopters. Whereas Illumina typically achieves an average accuracy of 99.9% for individual reads, the first-generation MinION incorrectly identified roughly three of every ten bases. There were other problems, too. \u201cYou'd get one flow cell that was amazing, but then you'd get one that only gave you three active pores for no clear reason, even though they were from the same batch,\u201d says Mason. ONT did not wish to publicly comment on this article. These limitations pigeonholed MinION mostly to applications where speed and simplicity were paramount, such as pathogen detection. \u201cIf you were asking whether there's anthrax in a letter, you could do that on a MinION very rapidly, even without perfect read accuracy,\u201d says Adam Phillippy, head of genome informatics at the US National Human Genome Research Institute in Bethesda, Maryland. \n               Leaps and bounds \n             Today, MinION has matured, says Keith Robison, who is principal scientist at the drug-discovery company Warp Drive Bio in Cambridge, Massachusetts. More importantly, ONT has overcome the scepticism and dismissive critiques that followed the company's prelaunch announcements. \u201cThey've proven all of those people wrong, and have repeatedly delivered,\u201d says Robison. A pore protein derived from the bacterium  Escherichia coli , together with improvements in the flow-cell chemistry, have slashed the error rate for individual reads to 2\u20135% for many experiments. A big boost in data output allows researchers to better identify errors by sequencing many more molecules in parallel, and read length has jumped from about 7,000 nucleotides initially to upwards of 100,000 nucleotides today. Loman's team has pushed single-read lengths to nearly one million bases. \u201cIn our early runs, we were getting a few hundred megabases of sequence per run,\u201d says Jared Simpson, a bioinformatician at the Ontario Institute for Cancer Research in Toronto, Canada. \u201cWith the new pore, we quite quickly saw that go up to gigabases of sequence, and now up to maybe 5 or 10 gigabases per run.\u201d The platform has also benefited from aggressive software development. Contig assembly is easier with long reads than short reads because there is more overlap, but nanopore reads are also more error-laden, and manipulating longer sequences can be computationally intensive. To address this, Phillippy's group devised an algorithm called Canu, in which conventional short-read assembly processes are tailored to compensate for the quirks of long-read data. Another software tool, ONT's Scrappie, addresses sequence errors that can arise when homopolymers \u2014 sequences containing multiple adjacent instances of a nucleotide, such as AAAAA \u2014 cause the system to hiccup. \n               A match for microbes \n             MinION has proved particularly popular among infectious-disease researchers. Loman, for instance, has teamed up with colleagues in the world's virological 'hot zones' to monitor the spread of Ebola in West Africa and Zika in Brazil 1 , 2 . \u201cThey were basically able to get a sequencing laboratory up and running in 48 hours, packed in luggage you could take below-decks on an airplane,\u201d says Mark Akeson, a biophysicist at the University of California, Santa Cruz, who conducted some of the foundational research in nanopore sequencing and is a member of ONT's advisory board. Loman says that this portability was a massive boon, but he notes that the massive data output could be overwhelming. \u201cWe just about managed in Brazil, but killed my Mac by overheating!\u201d Some groups are exploring clinical microbiology applications. Lachlan Coin, a bioinformatician at the University of Queensland in St Lucia, Australia, has developed real-time data-analysis algorithms to detect drug-resistant bacteria in blood samples. In early pilot tests using cultured bacteria and older flow cells, Coin's team could identify all of the antibiotic-resistance genes in a sample within 10 hours 3 . Current technology could halve that time, he says, but working with real-world samples in which human DNA overwhelms bacterial DNA is complicating the process. \u201cI think that in a year or so, we'll be able to identify the antibiotic-resistance genes in patient samples within six hours,\u201d Coin says. Other researchers are pursuing metagenomics, in which the goal is to comprehensively profile all of the organisms in a sample. In principle, every nanopore in the flow cell could be put to work detecting a different genome at the same time. \u201cYou can get a complete genetic portrait of the species that are there \u2014 bacteria, viruses, human DNA,\u201d says Mason. He has used nanopore sequencing to conduct a metagenomic census of the famously filthy New York City subway system, and has ambitious plans for even more inhospitable environments \u2014 including Mars. Working with scientists at NASA, Mason has shown that MinION can perform robustly in zero gravity aboard the International Space Station. He and his colleagues hope one day to ship this technology to the red planet, where the technology could aid the ongoing search for extraterrestrial life. Back on Earth, geneticist Scott Tighe at the University of Vermont in Burlington ran the MinION in Antarctica's McMurdo Dry Valleys, where his team spent more than two hours sequencing microbial samples. \u201cThe reason the run stopped was that it was too cold outside: the battery ended up dying,\u201d explains Mason, who has collaborated with Tighe on several projects. \n               Go big or go home \n             Nanopore veterans such as Phillippy consider microbial genome assembly to be \u201ca solved problem\u201d. Now they are taking aim at bigger game: mammalian genomes encompassing billions, rather than millions, of nucleotides. This year, a multi-institutional team including Phillippy, Loman and Simpson reported the assembly of an entire human genome using only MinION data, achieving high contiguity and accuracy 4 . The average contig sizes ran into the megabases, Simpson says, with accuracy values as high as 99.44%. Complementary use of Illumina's short-read technology refined the team's accuracy to 99.96%, although this still trails the 99.99% 'gold standard' accuracy that assembly projects typically strive for. In other aspects of human genome analysis, however, nanopores really could excel. Human-genome assemblies are still incomplete, for instance, because highly repetitive regions are resistant to short-read analysis. A team led by genomics researcher Karen Miga at the University of California, Santa Cruz, showed that nanopores could help researchers to fill in those blanks 5 . Miga's team used 150-kilobase-pair reads to reconstruct a human centromere \u2014 the ultra-repetitive genomic stretches at the pinched waists of eukaryotic chromosomes, which previously represented a daunting void. A truly complete genome sequence could be just a few years away, predicts Akeson, who collaborated with Miga. Nanopore analysis is also well suited for mapping epigenetic marks \u2014 minor chemical modifications to individual nucleotides that can influence gene expression. Most sequencing platforms use sample-preparation methods that erase those marks, but nanopore platforms can directly analyse modified DNA. Simpson and Winston Timp at Johns Hopkins University in Baltimore, Maryland, showed that they could train software to distinguish the electrical signatures of methylated cytosine nucleotides from normal cytosines with roughly 90% accuracy 6 . Akeson has achieved similar success 7 . \u201cWe've been able to detect any modification we've attempted to see,\u201d says Akeson. \u201cIt can distinguish differences as little as two hydrogen atoms.\u201d \n               More to come \n             Although still alone in the nanopore market, ONT faces an established long-read competitor. Pacific Biosciences (PacBio), based in Menlo Park, California, has built a reputation on generating extremely accurate data from DNA fragments measuring tens of thousands of bases. PacBio's platform is bulkier and more expensive than MinION \u2014 its smallest system, the Sequel, is roughly the size of a refrigerator, and costs $350,000. It also can't quite reach ONT's read-length extremes. But PacBio chief scientific officer Jonas Korlach notes that the system reliably delivers average reads of 10\u201318 kilobases, topping out at about 100 kilobases, and PacBio is trusted in the genomics world for its high-quality data. \u201cYou can get up to 99.99% accuracy pretty easily,\u201d says Phillippy, who adds that this technology is still his go-to when building a large genome assembly for the first time. It is also faster for large-scale projects: the recently reported nanopore-only human genome assembly 4  took ten times longer than it would have on a PacBio machine, says Phillippy. Some users have found that nanopore sample preparation kits can be unsettlingly unpredictable, with some DNA samples requiring extensive optimization. \u201cSome people are doing spectacularly well and getting amazing results, while others just struggle,\u201d says Robison. At a presentation in December 2016, ONT chief technology officer Clive Brown announced that: \u201cA lot of effort is being put in ... to giving people debugged protocols for specific sample types that will help them optimize the yield they get.\u201d Similarly, one of the company's biggest assets \u2014 routine reinvention and refinement to improve performance \u2014 can leave loyal users scrambling. For example, ONT's newest pore created a technical headache for customers who were familiar with the older technology. \u201cThat sort of thing has happened to us several times now,\u201d says Loman. \u201cThat's the flip side of being on the cutting edge \u2014 things don't last quite as long as you'd hope.\u201d These issues represent opportunities for competitors. Farthest along is Roche, headquartered in Basel, Switzerland, which acquired the California-based nanopore start-up Genia Technologies in 2014. Roche remains secretive about its system, but a 2016 publication from Genia describes a \u201cnanopore sequencing by synthesis\u201d strategy in which a DNA-synthesizing enzyme is coupled to a protein nanopore 8 . The enzyme reads the target DNA, building a complementary sequence from chemically tagged nucleotides. As each base is incorporated into the growing strand, its tag is released, passing through the nanopore to produce a distinctive electrical signal. Neil Gunn, head of sequencing solutions at Roche, says that although that research predates Roche's acquisition, the core principles of the technology are largely unchanged. \u201cThat's very much in line with the design of the product,\u201d he says. \u201cSince then, we've proceeded to focus on improving accuracy, reading rate and the sequencing length that we can acquire.\u201d Gunn notes that Roche's platform will be squarely targeted at the  in vitro  diagnostics space, with the goal of surpassing competitors' accuracy and reproducibility. Robison sees Roche's platform as a potentially strong contender, with early publications indicating accuracy ranging from 78% to 99% for any given nucleotide. \u201cTheir device could be very interesting, but the devil is always in the details,\u201d he says. \u201cWe'd need to see real data at a large scale.\u201d But ONT is not resting on its laurels: its two latest bench-top systems can deliver much greater data volumes than previous models. GridION, released in March, essentially runs multiple MinION devices in parallel. By contrast, PromethION uses an entirely different type of flow cell, and is designed for human-genome-scale projects. \u201cThey've obviously targeted it at as something that could be competitive with the Illumina platform in terms of output,\u201d says Loman. Instruments have already shipped to 'early access' users, but no data have yet been publicly released. However these developments pan out, nanopore sequencing is undeniably ascendant. And its promise of low-cost, reliable sequencing for the masses has researchers excited. \u201cAs a computer scientist, I'm always hungry for data,\u201d says Phillippy. \u201cThe thought of all of these microbiology labs and all of these college classrooms being able to generate sequence data is tantalizing.\u201d Reprints and Permissions"},
{"file_id": "549293a", "url": "https://www.nature.com/articles/549293a", "year": 2017, "authors": [{"name": "Marissa Fessenden"}], "parsed_as_year": "2006_or_before", "body": "Improvements in mapping protein\u2013protein interactions are allowing researchers to deconstruct the delicate mechanics of cells. In 1987, researchers in Switzerland described two sisters who were born separately but had similar abnormalities. A curl of tissue in their cerebellums was missing. Their hearts contained holes and clefts. One died aged three following cardiac surgery; her sister had a similar operation at age four, but survived. Because neither of the girls' parents had these abnormalities, the researchers concluded that their daughters had inherited two copies of an atypical gene, leading to a previously unknown syndrome 1 . The scrambled nucleotides responsible for the girls' condition may reside in a single gene. Yet several other genes have subsequently also been associated with what has been dubbed Ritscher\u2013Schinzel syndrome. The functions of those genes, and how they related to the syndrome, remained a mystery for years. Today, those molecular underpinnings are coming into focus thanks to the systematic study of protein\u2013protein interactions, a discipline called interactomics. By mapping the network of connections between proteins, three research teams independently discovered a complex called Commander that's made up of proteins produced by the mutated genes 2 . Commander is an essential cell component that sorts and delivers proteins, and its malfunction causes the devastating defects of Ritscher\u2013Schinzel syndrome. Proteins and other biological molecules rarely work alone; they brush up against one another in fleeting interactions or band together to form complex cellular machines. Only through such partnerships can proteins perform their many functions. Breakdowns in those interactions can affect human health. \u201cIf you break a gene coding a protein that goes into a complex, then that complex is dysfunctional in some way and that gives rise to a condition or disease,\u201d says Edward Marcotte, a systems biologist at the University of Texas at Austin. Biochemists have long studied the ways in which one or a few proteins interact with others. But now they are developing tools to chart more comprehensive sets of protein\u2013protein interactions at levels from organellar to organismal. These interactomes typically look like dense starbursts, with protein dots or nodes joined by the interactions between them. Self-contained clusters of interconnected proteins that emerge from these webs may represent key complexes and communal functions or, as in the case of Ritscher\u2013Schinzel syndrome, provide clues to the roots of disease. In the past three years, research groups have published the first high-quality maps of the human interactome 3 , 4 , 5 , 6 . Together, the most recent iterations of those maps have identified around 93,000 unique protein\u2013protein interactions. The technologies underlying these maps are not new; protein-interaction mapping dates back to the 1990s. And researchers have been producing interactome maps since at least the early 2000s. But methodological refinements as well as advances in protein purification, mass spectrometry and gene-editing techniques have empowered researchers to explore the interactome \u2014 and the insights it promises into development and disease \u2014 with ever-finer precision. It isn't easy: capturing all interactions is a challenge, as the set of protein partners varies across different tissues, cells and even time. The interactome is dynamic, breaking and forming connections as the cell responds to its environment. Mapping it to completion may require fresh methods and ways of thinking about systems biology. Still, the field is yielding results. \u201cNew machines that are ubiquitous but deeply understudied \u2014 that's fundamental biology coming out of the maps.\u201d Marcotte says. \u201cWe've clearly passed a critical threshold.\u201d \n               Numbers game \n             There essentially are two approaches to building interactome maps. The yeast two-hybrid assay tests for direct interactions between protein pairs by coupling gene expression to protein interactions in the cell. The second approach maps both direct and indirect protein contacts by isolating complexes with antibodies and identifying their component parts with mass spectrometry (see  'Pick A or B'  and 'Mapping tools'). Marcotte's laboratory uses a variation on the second approach that involves biochemically separating proteins \u2014 for instance, using sucrose density gradients \u2014 to see which molecules tend to stay together. The resulting maps allowed Marcotte and Anna Mallam, a postdoctoral researcher in his lab, to draw inferences about the Commander complex's cellular role 2 . Previous studies revealed that two components were structurally similar to proteins that build and maintain eukaryotic hair-like structures called cilia and flagella; other components seem to move proteins across membranes. Those data and other findings suggest that Commander moves specific proteins from the cell membrane to a compartment called the Golgi apparatus, where they are recycled. The largest maps encompass thousands of proteins, resembling tangled hairballs more than starbursts. But by unravelling them, researchers have identified signatures that distinguish cancer-causing genes from 'normal' ones, and that define key biological processes, such as chromosome segregation during cell division. Even with multiple approaches, interactome maps are \u201cstill largely incomplete\u201d, says computational biologist Katja Luck at the Dana-Farber Cancer Institute in Boston, Massachusetts. It's a question of numbers. The human genome contains roughly 20,000 protein-coding genes. If one assumes that each protein has only one form \u2014 a massive oversimplification \u2014 there are approximately 200 million possible interactions. The real number is likely to be much smaller because many interactions are indirect; estimates for one-to-one interactions range from 120,000 to 1 million. Proteins are incredibly diverse, biochemically speaking, and thus their interactions cannot be captured by every assay equally. Membrane- protein interactions, for instance, are difficult to study because when the membrane is stripped away, their shape and behaviour changes; they may not link with their typical partners. But, the extent to which this incompleteness alters the current maps isn't yet clear. \u201cWe are just at the beginning of understanding the biases of different methods,\u201d Luck says. As a postdoctoral researcher in the lab of geneticist Marc Vidal, Luck has helped to implement protocols to eliminate errors in their two-hybrid approach. The core method dates back to the 1989. \u201cWe are just doing some tweaks to make it better,\u201d she says. By tagging the protein genes with barcodes, the team can test more than one interaction at a time in a large range of growing yeast. Rigorous attention to detail, automation of key steps and sequencing in quadruplicate has allowed them to identify more than 60,000 interactions, the majority of which were previously unknown. That data set forms the bulk of the interactions reported in the collaborative Human Reference Protein Interactome Mapping Project, and it is still growing. \u201cBy 2020 we want something that people will be able to refer to as a reference map for the human interactome,\u201d Vidal says. The work hasn't always gone smoothly. The early days of interactomics generated error-prone networks. Only about 3% of identified interactions had support from more than one method, according to one 2006 review 7 . \u201cPeople were extremely cautious about using those data sets,\u201d Vidal says. \u201cBut in ten years we have made really incredible progress.\u201d \n               Better mapping with CRISPR \n             The eventual reference map Vidal envisions is likely to contain only a subset of all possible interactions. Cell and tissue variation as well as shifting cellular responses add up to many possible versions of the full interactome. For Matthias Mann, a biochemist at the Max Planck Institute of Biochemistry in Martinsried, Germany, those variations are daunting. But he is optimistic about the power of gene-editing technology, such as CRISPR\u2013Cas9, to address them. Mann's mapping method involves libraries of cell lines expressing hundreds of proteins, which are tested for interactions using an ultra-high-resolution mass spectrometer called Orbitrap. The bait proteins are fused to a green fluorescent protein, producing a luminosity profile that allows the researchers to quantify interactions through live-cell imaging. In the late 2000s, creating the cell-line library was \u201cquite laborious\u201d, he says. \u201cNow our method gets wings due to the CRISPR engineering that can be brought to bear.\u201d Since introducing the quantitative approach in 2010, Mann's team has mapped and quantified the strength of more than 28,000 interactions. Interactions in which the partners exist in one-to-one ratios are considered 'strong' and are likely to exist in stable and abundant complexes. Without such information, \u201cit is very hard to say something about the structure of the network\u201d, Mann explains. Analysis of his team's map showed that the human interactome is dominated by weak associations, which may reflect low-abundance regulatory proteins acting on more stable protein machines. \n               Subtle tweaks \n             A common trend across the field is the adoption of relatively gentle protocols for sample preparation that aim to faithfully capture all protein\u2013protein interactions in the cell. \u201cWe are trying to find less disruptive methods,\u201d says Rosa Viner, a biochemist at Thermo Fisher Scientific, a life sciences company in San Jose, California. The firm's focus on improving sample preparation, workflow and mass-spectrometry technology aims to help researchers identify interactions as they exist in cells. \u201cThis is the hardest challenge: finding the methods that will give us the best picture without any artefacts,\u201d she adds. Artefacts can include protein complexes that fall apart before their interactions are detected. To hold complexes together, Viner has worked with researchers at the University of California, Irvine, to chemically fuse complexes, an approach called crosslinking, before mass-spectrometric analysis. A strategy called QMIX (quantitation of multiplexed, isobaric-labelled crosslinked peptides) has been developed that integrates crosslinking compounds with chemical labels to allow researchers to stabilize as well as track protein complexes 8 . Good analysis also takes into account the blind spots of any given method. \u201cThere are still classes of proteins that are very challenging,\u201d says Wade Harper, a cell biologist at Harvard Medical School in Boston. \u201cWhen you do high-throughput analysis, you are limited in how much care you can take with individual protein.\u201d That's because such analyses tend treat every reaction the same, leaving little room for customization. Harper and his colleague Steven Gygi, also at Harvard, created a lab group to fine-tune their approach. \u201cWith a relatively small team of four to six people we can create four or five hundred cell lines a month,\u201d he says. That dedication has yielded the largest collection of human-protein-complex data yet achieved from a single pipeline. Their map, called BioPlex, includes around 120,000 interactions. \n               Big picture \n             But to get a closer look at interactions, researchers must dive into the crowded landscape of the cell itself. Anne-Claude Gingras, a biochemist at the University of Toronto in Canada, uses a technique called BioID, which tags proteins on the basis of their proximity to one another. The tagged protein of interest adds a chemical tag to nearby proteins, leaving evidence of its interactions like a crayon-wielding toddler's trail through the house. The result is a map of the physical neighbourhood surrounding the initial protein. Identifying a protein's larger community is likely to reveal details about its cellular function, Gingras explains. Proximity mapping also allows researchers to track proteins that cannot be picked up by other assays, such as difficult-to-isolate membrane-embedded proteins. \u201cWe and others have looked at proteins on chromatin, mapped the organization of the centrosome and detected interactions that span all kinds of membranes,\u201d Gingras says. Using BioID, the group found new components in a signalling pathway that regulates organ size during development 9 . Harper's lab uses a similar method called APEX. In it, an engineered plant enzyme called ascorbate peroxidase chemically restricts the time window during which the protein of interest can tag others, resulting in a fainter but more spatially precise signal. Having multiple approaches in interactomics means that when interactions appear on more than one map, they carry more weight. That is where the insights will come, says Jennifer Lippincott-Schwartz, a cell biologist at the Howard Hughes Medical Institute Janelia Research Campus in Ashburn, Virginia. \u201cIf we are going to understand how cells are working, it is critical to connect all the protein\u2013protein interaction maps with spatial maps within the cell,\u201d she says. Cells are packed with large structures or organelles, all floating in the protein-rich soup of the cytoplasm. Understanding which proteins are interacting and why will require researchers to actually see what this world looks like. Lippincott-Schwartz's lab has developed an arsenal of tools for visualizing proteins inside living cells using fluorescent labels. These tools have revealed six organelles \u2014 the endoplasmic reticulum, the Golgi apparatus, lysosomes, peroxisomes and lipid droplets \u2014 moving and interacting in 3D. The team calls it the organelle interactome 10 . The interactome, Lippincott-Schwartz says, is \u201ca hypothesis generator\u201d for cell biologists. \u201cYou go in and start testing once you see a protein you know interacting with a whole bunch of other proteins that have functions you didn't know.\u201d With interactome maps finally becoming fleshed out with high-quality, abundant interactions, researchers can start putting those hypotheses to the test. \n                     Protein-protein interactions: Interactome under construction \n                   \n                     Architecture of the human interactome defines protein communities and disease networks \n                   \n                     Deep interactome profiling of membrane proteins by co-interacting protein identification technology \n                   \n                     Systems biology: An expanded human interactome \n                   \n                     Nature  Technology Hub \n                   \n                     HuRI: The Human Reference Protein Interactome Mapping Project \n                   \n                     BioPlex database \n                   \n                     BioGrid \n                   \n                     Cytoscape \n                   Reprints and Permissions"},
{"file_id": "d41586-017-05479-7", "url": "https://www.nature.com/articles/d41586-017-05479-7", "year": 2017, "authors": [{"name": "Esther Landhuis"}], "parsed_as_year": "2006_or_before", "body": ""}
]