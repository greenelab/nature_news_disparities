[
{"file_id": "4681024a", "url": "https://www.nature.com/articles/4681024a", "year": 2010, "authors": [{"name": "Richard Monastersky"}], "parsed_as_year": "2006_or_before", "body": "She set out to revolutionize US ocean management \u2014 but first she faced the oil spill. Jane Lubchenco is  Nature  's Newsmaker of the Year. Jane Lubchenco smiles as a dolphin leaps out of the water, arcs in the air and splashes back down just a few metres away. The 63-year-old marine ecologist is out on a boat near Pascagoula, Mississippi, with a team of researchers studying how the recent oil spill in the Gulf of Mexico has affected dolphin communities there.  On this October day, Lubchenco wears starfish-shaped earrings and a cap emblazoned with the letters 'NOAA', for National Oceanic and Atmospheric Administration. Her shirt sports a NOAA logo, as does her life vest. Rarely does she venture out in public without some symbol of the US government agency she has proudly run since March 2009. A sprawling department of 12,800 people with a budget of US$4.7 billion, NOAA has responsibilities stretching from the bottom of the sea to the top of the atmosphere and even to the Sun, which it monitors for signs of solar storms (see  'A global reach' ). That mandate put Lubchenco at the centre of the government's response to the BP Deepwater Horizon oil-spill disaster \u2014 a brutal test for a scientist with little previous management experience. On board the boat, she relishes the chance to talk about dolphin behaviour with the NOAA researchers, but seems to get the biggest kick when the pilot gives her a turn at the wheel. Gripping the throttle, Lubchenco has to be reminded to stay below the speed limit as she motors through the narrow waterway.  Going slow does not come easily to the NOAA leader. As a celebrated scientist and vocal conservationist, she made her name urging other researchers to speak out on issues of public importance, a stance that not all of her academic colleagues were comfortable with. Now, at an age when many of her cohort are easing back, she is taking on the most ambitious challenge of her career: reorienting how the nation responds to pressing environmental problems such as dwindling fish stocks, rising seas and a changing climate. She has bold plans to strengthen scientific research at NOAA, make it more relevant to society and improve the health of ecosystems and coastal communities.  But the path has not been smooth for Lubchenco, who took over the agency in troubled times. With the economy in a nose dive and many coastal communities struggling, NOAA's policies to limit fishing have proved so contentious that members of US President Barack Obama's own party called for Lubchenco to resign. And the oil-spill disaster has severely tested her political skills. Some of her natural constituency \u2014 scientists and environmentalists \u2014 have accused her of quashing independent researchers, suppressing information and misleading the public.  Although she admits to some communications problems during the crisis, Lubchenco shakes off the broader criticisms. \"I'm very proud of what we did during the heat of the moment,\" she says. NOAA closed down fisheries, forecast where currents would sweep the oil, monitored storms during one of the most active hurricane seasons on record, protected endangered marine species and is leading the effort to assess damage done by the oil. \"I give her very high marks as a leader in what has been a difficult time for NOAA,\" says Michael Jackson, who was deputy director of the US Department of Homeland Security in 2005, during Hurricane Katrina. Throughout this day on the Gulf of Mexico, Lubchenco keeps up a hectic pace, visiting multiple sites in the Alabama and Mississippi area. This is her eleventh trip to the Gulf of Mexico region since the Deepwater Horizon oil rig exploded on 20\u00a0April, unleashing the largest single marine spill in US history.  In person, Lubchenco makes an easy connection with strangers. She looks them in the eye and asks about their jobs and how the spill affected them. Before lunch, she meets more than two dozen teachers from across the Gulf and starts by telling them how much she appreciates their work. \"My sister is a middle-school science teacher. My daughter-in-law is a high-school science teacher, and I was strongly affected by teachers,\" she says. The teachers introduce themselves and talk about how the spill touched their students, many of whose parents were put out of work when the spreading oil closed fishing grounds and drove away tourists. The teachers thank Lubchenco for all the information that NOAA posted on its website, which their classes used to find out which fishing areas were closed, where the winds were going and whether currents would carry the oil out of the Gulf. \"We would check your site every day,\" said one teacher. \"We used so much of that data.\"  \n                Crisis management \n              With the well capped and the oil dispersing, Lubchenco has entered calmer waters after the tumultuous spring and summer of the crisis. She was one of the 'principals' \u2014 the top administration officials working on the spill, who regularly briefed President Obama and rarely rested. Two weeks after the rig exploded, she ran into an old friend at a party in Washington.  \"Jane, you look really tired,\" he told her. \"Yeah, I'm sleeping three or four hours a night,\" she confided to him. Such was the toll of running the lead ocean agency during one of the biggest environmental disasters in US history. The task was complicated by a series of communications missteps, her own and those of other officials, which drew accusations that she had withheld information about the environmental toll of the spill. The first flashpoint was the question of how much oil was leaking from the wellhead and where it was going. Days after the spill, when BP was estimating that 1,000 barrels of oil were pouring out each day, a NOAA researcher arrived at a far higher figure of 5,000\u201310,000 barrels \u2014 a \"very rough estimate\", his e-mail warned. But that was not released to the public. Instead, a Coast Guard admiral in charge of responding to the spill said in a press conference on 28 April that \"NOAA experts believe the output could be as much as 5,000 barrels\".  That figure stood as the sole government estimate for a month. At the same time, independent researchers came up with estimates in the range of 25,000\u2013100,000 barrels a day. Months later, the government concluded that the well had gushed 62,000 barrels a day initially and then declined to 53,000 (a figure that BP contends is too high).  Other issues also suggested to some that NOAA and the rest of the government were downplaying the magnitude of the problem. In mid-May, academic scientists working in the Gulf started finding evidence that untold amounts of oil were spreading away from the wellhead and forming vast plumes some 1,200\u00a0metres below the surface 1 . NOAA initially questioned the evidence and dismissed media reports as \"misleading\", even as more evidence emerged. Donald Boesch, president of the University of Maryland Center for Environmental Science in Cambridge and a member of a commission that subsequently reviewed the government's response, says that was a mistake. \"Jane was too dismissive about the fact that there could be a significant deep-water plume there,\" he says. On 8 June, after analysis of more data collected by academic scientists, NOAA acknowledged the presence of diffuse plumes of oil beneath the surface.  \n                The fate of the oil \n              On 15 July, BP finally succeeded in capping the well, but there were still major questions about what had happened to all the oil that had escaped over the past three months. In early August, NOAA and other agencies released an 'oil budget', which tallied the fate of all the released oil. Carol Browner, director of the White House Office of Energy and Climate Change Policy, announced on television that three-quarters of the oil was \"gone\". But that did not match the government's own numbers. Later that day, Lubchenco appeared with Browner at a White House press conference and corrected the record. \"It's important to point out that at least 50% of the oil that was released is now completely gone from the system,\" said Lubchenco. Illustrating her statistics with a pie chart produced by NOAA and other agencies, Lubchenco said that containment efforts had removed roughly a quarter of the oil and another quarter had either evaporated or dissolved. The rest had dispersed as tiny subsurface droplets or as visible oil, and some of that had been collected from beaches or naturally degraded.  But in making that correction, Lubchenco made a different mistake by saying that the oil budget had been \"peer reviewed\", a statement at odds with the reports of scientists who supposedly reviewed it. Academics and members of Congress also criticized NOAA's decision to release the four-page oil budget without uncertainty ranges or the background data that justified the conclusions. Reacting to the series of gaffes, the national commission investigating the oil spill declared in October that \"the federal government created the impression that it was either not fully competent to handle the spill or not fully candid with the American people about the scope of the problem\". At the very least, those issues undermined the public's trust in the government, said the commission. For Lubchenco, the judgement was both troubling and ironic. Given her record of urging scientists to speak out, she says, \"I would be the last person in the world to be not valuing or promoting communication\". She says that she initially baulked at the 5,000-barrel-a-day flow-rate statement. \"My inclination was to correct the record, but in the grand scheme of things, since we didn't have the accurate numbers and we were working on getting them, it didn't seem to be that important relative to all the other stuff that was going on.\" Knowing how much oil was flowing would not have helped the effort to contain it, she argues \u2014 an assertion challenged by the oil-spill commission, which says that knowledge of the true flow rate might have helped BP to avoid some problems in its attempts to cap the well. \"In hindsight,\" says Lubchenco, \"it took far too long to come up with the eventual answer.\"  During a press conference in November, she also acknowledged that she had erred in declaring that the oil budget had been peer reviewed. In a subsequent interview, she took personal responsibility for the miscommunication. \"I misunderstood what kind of review it had had, so that was my mistake,\" she said. But Lubchenco defends her agency's statements about the subsurface plumes, saying that NOAA was just insisting on careful science. \"It's frustrating to get crosswise with my academic colleagues when we thought all we were asking them to do was to be good scientists and to double check and make sure that what they were finding was in fact what they thought it was.\"  Some scientists are still bothered by NOAA's slow acknowledgement of the deep oil, but others agree with her approach. \"There was a lot of speculation early on,\" says Richard Camilli of the Woods Hole Oceanographic Institution in Massachusetts, who led a cruise that uncovered signs of a deep plume of oil in June. \"Good science requires peer review. If you're going to say something public it should go through peer review first,\" says Camilli, who published his findings in  Science   in August 2 . Many scientists laud NOAA's overall performance during the spill. Boesch, although critical of Lubchenco's initial response to reports of deep plumes, says that she and NOAA provided \"very critical science support to help direct the spill response where it was needed\". And he praises the agency for doing something that gets little mention \u2014 successfully keeping the nation's seafood safe by closing fishing areas and reopening them only after rigorous testing. \"That protected the public,\" he says, \"and in the long run protected the industry.\"  \n                Defying expectations \n              By late October, the sheen of oil had disappeared from the surface of the Gulf and NOAA had shifted towards assessing the damage. \"It's far from over,\" says Lubchenco. \"It's going to be years, if not decades, before we really understand the impact this massive infusion of hydrocarbons has had on this system.\" In Mississippi Sound earlier that day, Lubchenco relished the chance to spend part of her weekend on the water. As a scientist, she has studied ocean ecosystems for 40 years \u2014 an unlikely focus for a girl growing up in the 1950s in Denver, Colorado, in the middle of the continent. But the women in the Lubchenco family have long challenged expectations. In the early 1900s, her paternal grandmother left her parents' cotton farm in South Carolina to train in medicine, only to find that the dean of one of the nearest medical schools, in North Carolina, would not accept a woman. She finally wore him down, became the first female graduate in 1912 and then married a Ukrainian agricultural researcher who had visited her family's farm years earlier. (He narrowly made it to her graduation ceremony, after having missed the steamer he had originally booked to America \u2014 the  Titanic .) Lubchenco's parents were also doctors, and her mother worked part-time so that she could have a career and raise her six girls. In that household, everybody was expected to follow their interests. \"Mom and Dad were always great about encouraging us to explore. Of the six of us, we all do completely different things,\" says Lubchenco. In secondary school, young Jane was a classic overachiever: an athlete, scholar and leader, she won the school's highest award. But rather than go to a powerhouse university, she chose tiny Colorado College in Colorado Springs and enrolled in an unusual programme with no classes, no grades and no tests. She discovered that she liked biology and took a summer class at the Marine Biological Laboratory in Woods Hole, Massachusetts, where she fell in love \u2014 with invertebrates and research. \"That whole summer was magical for me,\" she recalls. \"It made me decide I was going to go to grad school and it was going to be marine science.\"  After getting her PhD at Harvard University in Cambridge, Massachusetts, and teaching there for two years, Lubchenco took what some considered a step down by moving to Oregon State University in Corvallis, where she and her husband, ecologist Bruce Menge, bargained to split an academic position. It was perhaps a first in the United States, and it gave them both a chance to teach, conduct research and raise their children. The two also split their research on tidal communities, with Lubchenco studying the herbivores and seaweeds and Menge the predators and prey. At the time, ecology was largely a descriptive science, but Lubchenco was part of a group pushing to introduce experimental approaches. In graduate school, she started moving herbivorous snails around tide pools to tease apart the factors controlling the distribution of seaweeds.  Most researchers had assumed the answer had to do with physical limitations, such as how much a tide pool dries out. But Lubchenco demonstrated that the herbivores had an important role in controlling the plant populations 3  \u2014 a finding that also turned out to be true in some terrestrial ecosystems. Her simple, elegant experiments became a staple in ecology courses, and her papers garnered hundreds of citations.  Lubchenco also made a name for herself by urging fellow ecologists to speak out on environmental issues. As vice-president of the Ecological Society of America in 1988\u201389, she chaired a panel that called for ecologists to communicate to the public and policy-makers. \"It was a coming of age for our society, to admit that relevance was not a four-letter word,\" recalls Lubchenco (see  page 1032 ). Later, while serving as president of the American Association for the Advancement of Science \u2014 the premier scientific organization in the United States \u2014 in 1996\u201397, she continued to push scientists to become more socially relevant. Now she has a chance to bolster science and its connection to policy-making at the highest level. NOAA has a long history of conducting some top-notch science and has nurtured pioneering researchers such as ozone specialist Susan Solomon and climate modeller Syukuro Manabe. But it has been perpetually strapped for cash, and previous administrations have at times focused less on the science than on the divisions that provide services, such as forecasting weather and managing fisheries.  When Lubchenco discussed the NOAA post with Obama soon after he was elected in 2008, she told him that one of her goals would be to renew that commitment to science. Obama's response to this proposal and others that she made, she says, was \"let's do it\".  Once she took office, Lubchenco set out to resurrect the chief-scientist position at NOAA, which has been vacant for 14 years. But she got a lesson in the slow ways of Washington. Much to her frustration, it took months for the Obama administration to approve her choice, Scott Doney of the Woods Hole Oceanographic Institution, and a senator this month put a block on Doney's nomination to protest against the administration's moratorium on offshore drilling. In the meantime, Lubchenco has increased the number of senior scientific positions at NOAA from 10 to 25, and altered the career structure within the agency so that scientists can advance in seniority and salary without having to leave research for a purely management position.  Lubchenco has made significant progress on her other priorities, say many who have watched NOAA under her leadership. \"She's done the job certainly as well \u2014 and I would argue better \u2014 than anyone else,\" says Andrew Rosenberg, a senior vice-president at Conservation International and deputy director of NOAA's fisheries service from 1998 to 2000.  When Lubchenco arrived in Washington, one of the first problems she had to tackle was the National Polar-orbiting Operational Environmental Satellite System (NPOESS). Designed to collect weather and climate data, it was running years late and more than $5 billion over budget. Lubchenco and her colleagues in the administration developed a plan to split the unwieldy system into a military part and a civilian part to be jointly managed by NOAA and NASA \u2014 a step that could finally get the NPOESS back on track.  Lubchenco has also pushed forward an initiative to create a NOAA division called the Climate Service, which the agency had been discussing since just after it was founded in 1970. The goal is to gather NOAA's decentralized climate expertise into a single office to enhance the science and provide an authoritative voice on climate information. The biggest reorganization in NOAA's history, this office \u2014 which awaits congressional approval \u2014 will give the public and businesses forecasts such as long-term temperature projections and flooding maps that take into account sea-level rise.  \n                Fishing woes \n              For environmentalists, one of the biggest successes of Lubchenco's tenure so far has been the administration's new ocean policy, which Obama signed on 19 July. A centrepiece of the policy is a strategy \u2014 long championed by Lubchenco \u2014 called coastal and marine spatial planning, which seeks to assess and balance human activities in particular ocean regions so that they do not conflict with each other or harm ecosystems. In the past, the government has tended to manage activities such as fishing individually, without considering how other factors, such as oil drilling and coastal development, might interact with them. \"What Jane has done is catalysed the most important transformation in ocean management in our history,\" says Elliot Norse, president of the Marine Biology Conservation Institute in Bellevue, Washington.  All that change has brought some strong criticism, especially from the fishing industry. Under her leadership, NOAA has moved to implement the 2007 Magnuson\u2013Stevens Reauthorization Act, which requires the agency to end overfishing. NOAA's actions so upset some fishermen in Gloucester, Massachusetts, that they built a life-sized model of Lubchenco hanging fishermen. The rhetoric in Congress, with the calls for her resignation, was only slightly less inflamed.  The source of the strife in New England goes back long before Lubchenco took office. Oversight of fishing in US federal waters is complicated; NOAA shares management duties with eight regional councils made up of federal and state government officials and members of the public, including the fishing industry. The councils choose how they want to control fishing and propose annual limits on each type of seafood. NOAA assesses the plans and then approves or rejects them.  In the past, NOAA had given management councils more latitude, but when Lubchenco took office, she made it clear that she expected them to meet the congressional deadline to end overfishing by this year. As part of that, NOAA last year encouraged the councils to consider a strategy called catch shares. In this scheme, councils allocate fishing 'shares' to individuals or groups, usually on the basis of how much they have previously caught. The recipients of shares can use or sell them. Proponents say that catch shares give fishing communities a long-term economic incentive to rebuild stocks.  Although the strategy has been used around the world and in parts of the United States for decades, the transition to a catch-shares system can be difficult. \"It has to be done very carefully. It has to involve the community, from the bottom up,\" says Brian Rothschild, a professor of marine science at the University of Massachusetts at Dartmouth who has close ties to the New England fishing community. He contends that NOAA and the New England Fishery Management Council moved too quickly in May to implement a programme based on catch shares, without properly involving the local fishing community or explaining the system. Some fishing communities say that the policy has caused major job losses. Lubchenco and others argue that New England's policy was five years in the making and the community had ample time to get involved. They also contend that fishermen in the area have been struggling economically for years \u2014 long before the management council adopted the new programme. \"The reality is that this isn't about catch shares,\" says Lubchenco. \"It really is about the economy.\" Peter Baker, manager of the Pew Environment Group's New England overfishing campaign, agrees. He says that Lubchenco \"has taken a stand to fix things for the future\". Those who have criticized her policy have not offered a viable alternative, he says. \"I'm not sure that anything would be enough to appease her detractors.\" As difficult as this year has been for Lubchenco, the next few will offer further challenges. NOAA's budget increased by 21% during the past two years, but Obama and Congress are now committed to cutting spending and the outlook for NOAA is bleak. The agency has never enjoyed the same support in Congress as some other science agencies, such as the National Institutes of Health. But Lubchenco thinks that the recent crises deliver a message on the value of NOAA's research and science-based management. \"It seems NOAA's relevancy has been more obvious in the last couple of years,\" she says.  Nowhere is that clearer than out on the Gulf of Mexico, where signs of dead coral and other long-lasting effects of the oil spill are starting to appear. While travelling through the region, Lubchenco recalls that she turned down Obama's transition team several times when she was first offered the job. Leaving her husband and research behind in Oregon seemed too big a sacrifice. But in the end, she says, she believed in the new president and in the opportunity to achieve her lifelong goals. \"I came to NOAA to lead and enable change where it would make a difference,\" she later explained. The rough days so far have not discouraged her. \"Meaningful change is not for the timid.\"  See Editorial  p.1002 \n                     Deepwater Horizon Disaster \n                   \n                     Nature Geoscience \n                   \n                     National Oceanic and Atmospheric Administration \n                   \n                     Lubchenco/Menge Lab \n                   \n                     National Commission on the Oil Spill \n                   Reprints and Permissions"},
{"file_id": "467899a", "url": "https://www.nature.com/articles/467899a", "year": 2010, "authors": [], "parsed_as_year": "2006_or_before", "body": "With the majority of the human population now living in cities,  Nature   takes a look at the implications for scientists. After spending tens of thousands of years living mostly in small settlements, humans have entered an urban stage of evolution. As of 2008, more than half the world's people live in cities, and the urban population is swelling by 1 million every week. By 2030, almost 6 in 10 people will live in metropolitan areas, which exert a powerful pull as economic and social magnets. That concentration of people gives rise to some of the world's greatest problems, such as air and water pollution, poverty-stricken slums and epidemics of violence and illness. Yet throughout history, urbanites have produced soaring achievements, ranging from Notre Dame Cathedral to the mobile-phone networks that have revolutionized communication. Cities are also home to considerable scientific capital; they hold most of the world's top universities and the vast majority of its researchers (see  page 900 ). This week,  Nature   examines that special relationship between scientists and cities and how each can bring out the best in the other. The resources that cities offer can stimulate outstanding science for reasons that researchers are just starting to explore (see  page 906 ). On the other side of the equation, scientists can assist cities in tackling their biggest problems. The Nobel laureate Mario Molina sets a good example, having redirected his research to improving the environment in Mexico City, one of the world's biggest megacities (see  page 902 ). Scientists are also helping cities to assume a lead position in combating global warming. With nations largely paralysed on this front, cities have emerged as a testing ground for cutting greenhouse-gas emissions and for adapting to the changes that warming will bring (see  page 909 ). But these efforts are hampered by a disproportionate lack of data at the city level (see  page 883 ). Cities must find a way to grow sustainably, which will require scientists across many disciplines to collaborate with leaders in other sectors of society to develop general rules for urban expansion (see  page 912 ). The threats to cities and the opportunities they present are attracting increasing attention from researchers in many areas. Synthetic biologists, for example, are exploring molecules that could clad skyscrapers and trap carbon dioxide (see  page 916 ). Scientists have a responsibility to supply many more advances of that nature to ensure the viability of humans as an urban species. Reprints and Permissions"},
{"file_id": "4671028a", "url": "https://www.nature.com/articles/4671028a", "year": 2010, "authors": [{"name": "Lee Billings"}], "parsed_as_year": "2006_or_before", "body": "NASA's next-generation space observatory promises to open new windows on the Universe \u2014 but its cost could close many more. It has to work \u2014 for astronomers, there is no plan B. NASA's  James Webb Space Telescope  (JWST), scheduled to launch in 2014, is the successor to the Hubble Space Telescope and the key to almost every big question that astronomers hope to answer in the coming decades. Its promised ability to peer back through space and time to the formation of the first galaxies made it the top priority in the 2001 astronomy and astrophysics decadal survey, one of a series of authoritative, ten-year plans drafted by the US astronomy community. And now, the stakes are even higher. Without the JWST, the bulk of the science goals listed in the 2010 decadal survey, released this August, will be unattainable. \"We took it as a given that the JWST would be launched and would be a big success,\" says Michael Turner, a cosmologist at the University of Chicago, Illinois, and a member of the committee for the past two decadal surveys. \"Things are built around it.\" Hence the astronomers' anxiety: the risks are also astronomical. The JWST's 6.5-metre primary mirror, nearly three times the diameter of Hubble's, will be the largest ever launched into space. The telescope will rely on a host of untried technologies, ranging from its sensitive light-detecting instrumentation to the cooling system that will keep the huge spacecraft below 50 kelvin. And it will have to operate perfectly on the first try, some 1.5 million kilometres from Earth \u2014 four times farther than the Moon and beyond the reach of any repair mission. If the JWST \u2014 named after the administrator who guided NASA through the development of the Apollo missions \u2014 fails, the progress of astronomy could be set back by a generation. And yet, as critical as it is for them, astronomers' feelings about the JWST are mixed. To support a price tag that now stands at roughly US$5 billion, the JWST has devoured resources meant for other major projects, none of which can begin serious development until the binge is over. Missions such as the Wide-Field Infrared Survey Telescope, designed to study the Universe's dark energy and designated the top-priority space-astronomy project in the most recent decadal survey, will have to wait until after the JWST has launched. \"Until then, we're not projecting being able to afford large investments\" in new missions, says Jon Morse, director of NASA's astrophysics division. And all the space telescopes currently operated by NASA and the European Space Agency will reach the end of their planned lifetimes in the next few years. Worse, the JWST's costs keep growing. In 2009, NASA required an extra $95 million to cover cost overruns on the telescope. In 2010 it needed a further $20 million. And for 2011 it has requested another $60 million \u2014 even as rumours are swirling that still more cash infusions will be required (see  'Cost curve' ). boxed-text Senator Barbara Mikulski (Democrat, Maryland), chairwoman of the government subcommittee that oversees NASA's budget, responded to these requests in June by calling for an independent panel to investigate the causes of the JWST's spiralling cost and delays, and to find a way to bring them to resolution. \"Building the JWST is an awesome technical challenge,\" Mikulski says. \"But we're not in the business of cost overruns.\" John Casani, chairman of Mikulski's investigative panel and a former project manager for NASA's Voyager, Galileo and Cassini missions, emphasizes that the panel is making suggestions, not decisions. Those will be up to NASA, which is expected to announce a budgetary plan incorporating the panel's suggestions on 2 November. But in considering potential solutions for the JWST's woes, Casani says that \"everything will be on the table\" \u2014 including, conceivably, scrapping instruments or otherwise downgrading the programme.  \n                The Goldin Opportunity \n              The first concept for a Hubble replacement emerged in 1989, when Hubble was still a year away from launch. Astronomers already knew that its vision would not quite reach back to the 'cosmic dawn', 500 million years after the Big Bang, when the first stars and galaxies formed. So a next-generation space telescope that could fill the gap seemed like the logical next step. In 1993, NASA asked a committee of astronomers, chaired by Alan Dressler of the Carnegie Observatories in Pasadena, California, to define what such a telescope would need. The new telescope's mirror would have to be big to gather the dim light of those first galaxies. So the committee recommended that the primary mirror be at least 4 metres across. The telescope would also have to be cryogenically cold, because at any temperature higher than 50 kelvin, infrared heat radiation from the telescope itself would wash out the faint photons that the astronomers were looking for. \"That was the science that propelled the whole thing,\" says Dressler. Finally, it would have to operate far from Earth. At infrared wavelengths, this planet glows like a light bulb. So the committee recommended that the telescope be placed 1.5 million kilometres outside Earth's orbit, at the second Lagrangian point (L 2 ), where the combined gravitational pull of the Sun and Earth creates a region of stability. Any spacecraft at L 2  will also lie in the shadow cast by Earth, making it easier to keep cool (see  'The James Webb Space Telescope' ). In December 1995, Dressler briefed NASA's then administrator, Daniel Goldin, on the recommendations. Goldin was intrigued. He was shaking up NASA's science programmes, pushing a 'faster, better, cheaper' strategy to deliver more capable and inspiring missions at lower costs. Taking his cues from Silicon Valley and aerospace 'skunkworks' projects \u2014 small, highly autonomous ventures pursuing innovation within larger organizations \u2014 Goldin was pushing for miniaturization of bulky electronics, more off-the-shelf components, lower organizational overheads, and a continuous expansion of the technological boundaries with each mission. Dressler's proposal seemed like a perfect opportunity to test that approach. Instead of a 4-metre telescope, Goldin asked, why not try one with a primary mirror 6\u20138 metres in diameter? Some of the technology was in hand: NASA was developing the cryogenic infrared Spitzer Space Telescope with a 0.85-metre mirror made of beryllium, a metal that needs special handling \u2014 it corrodes skin at a touch \u2014 but is lightweight and keeps its shape through extreme temperature changes. That and other innovations could give the JWST a mega-mirror while reducing costs. As Goldin put it in a speech: \"Let's throw away glass. Glass is for the ground.\" \n               boxed-text \n             Some astronomers were dubious about initial cost estimates for the ambitious mission, which ranged from $500 million to $1 billion. But in the beginning, Goldin's methods seemed to deliver: the first missions using the approach were wildly successful. Among them were 1997's landmark Mars Pathfinder mission and its accompanying rover, Sojourner, and the 1998 Lunar Prospector mission that found evidence of water ice on the Moon. But they were followed in 1999 by the disastrous losses of the Wide-Field Infrared Explorer telescope and two planetary missions, the Mars Climate Orbiter and the Mars Polar Lander. This string of failures tarnished the agency's reputation, and reminded everyone that 'faster, better, cheaper' was also riskier. By the end of Goldin's tenure in 2001, NASA had already begun shifting back to its traditional, risk-averse and far more expensive strategy of exhaustive testing and extensive oversight. That shift would send the cost of the JWST soaring past the billion-dollar mark. The mirror diameter would be cut from 8 metres to 6.5 metres to help reduce costs. But in the meantime, as NASA carried out the many engineering trade-off studies and scientific working groups required to solidify the telescope's design, a more insidious factor came into play: scientists started to pile on complexity. It happens with almost every major mission, says Peter Stockman, former head of the JWST mission office at the Space Telescope Science Institute in Baltimore, Maryland. \"Everyone fears it will be the last opportunity in their scientific lifetime.\" And there seemed little reason for restraint: in the 1990s, when the bulk of the design work was done, NASA's astrophysics budget was projected to keep growing by a few per cent a year.  \n                Stretched capabilities \n              With each iteration, the JWST's science objectives swelled. The core instrument package came to include a large-field-of-view near-infrared camera (NIRCam) and a multi-object near-infrared spectrograph (NIRSpec), primarily for investigating the earliest stars and galaxies; a general-purpose mid-infrared camera and spectrograph for observing dust-shrouded objects in the Milky Way; and a fine guidance sensor and tunable-filter imager to support the other three. These expanded capabilities would have to be supported by expensive and largely unproven technologies. The instruments needed extra-large, ultra-stable infrared detectors. A five-layered membranous sunshield would have to be folded around the spacecraft before launch, then deployed in space to allow the telescope to cool to cryogenic temperatures. Unfurled, each layer would be about the same area as a tennis court. The primary mirror, too large to fit into any existing rocket fairing, would have to be assembled in 18 hexagonal, adjustable segments that would also unfold in orbit. Each segment would be painstakingly chiselled from beryllium, then coated with gold and polished. Arrays of electromechanical devices called microshutters would allow NIRSpec to take spectra from up to 100 objects simultaneously, even if some of those objects were faint and lay next to brighter stars. Each individually controllable microshutter would be the width of a few human hairs, and NIRSpec would require more than 62,000 of them. In addition, every piece of technology in the spacecraft would have to be engineered to endure the violent vibrations of launch, the hard vacuum of outer space and the slow cool-down to cryogenic temperatures. The telescope's optical surfaces, in particular, would have to survive all this while staying aligned to a precision of nanometres. And everything would have to perform nearly flawlessly for a minimum of five years, the baseline mission length. Small wonder, then, that NASA ended up spending almost $2 billion just on the JWST's initial technology development. Nonetheless, the agency did not substantially cut any of the telescope's capabilities to bring the costs back under control. Instead, it looked for partnerships, securing major contributions from the European and Canadian space agencies. NASA also maximized support for the project on Capitol Hill by awarding contracts for spacecraft components to a small army of companies and universities scattered through many congressional districts. Aerospace giant Northrop Grumman of Los Angeles, California, became the JWST's prime contractor, under NASA's Goddard Space Flight Center in Greenbelt, Maryland, which would manage the overall project. By the time the JWST passed its preliminary design reviews in spring 2008 and NASA had officially committed to building it, the project had been transformed from its comparatively modest 'faster, better, cheaper' origins into an audacious multibillion-dollar, multi-instrument mission spanning institutions, countries and continents.  \n                Passing the Test \n              For nearly a year now, engineering models of the JWST's various components have been trickling into the clean room in Goddard's Building 29 for testing. (The centre's white-suited technicians can be seen at work on Internet  'Webb-cams'  .) Pieces of actual flight hardware are supposed to start arriving in the same room in spring and summer 2011. All of the JWST's riskiest technologies have met their critical milestones and are on schedule for the 2014 launch. The most substantial challenge remaining before launch is to integrate and test the flight components to ensure that they function as a whole \u2014 and, of course, to do all that without exceeding the remaining budget. NASA's traditional method is to 'test as you fly' \u2014 to operate the integrated flight hardware in conditions as close as possible to those it will experience in space. The problem is that the fully assembled telescope will be far too large to fit into any available thermal vacuum chamber. Just as the JWST's scientific objectives required new technology, mission planners have had to devise entirely new protocols to test it. \"With the JWST we have to do incremental modelling, building and testing, validating our model at each stage and then moving up to the next level of assembly,\" says Phil Sabelhaus, the JWST project manager at Goddard. \"We aren't only testing \u2014 we're also proving our ability to model correctly, which is how we will evaluate the JWST's absolute performance on-orbit.\" This hierarchical assembly, testing and modelling is laborious and time-consuming, more like building several telescopes than one, and is a major contributor to the JWST's remaining costs. So, unsurprisingly, it is one of the most probable targets for cost-cutting. \"There are tests that are really essential to do, and tests that would be nice to do,\" says Dressler. \"With something of this magnitude, there is a natural tendency to double-check and triple-check, and maybe we can't afford that.\" On the other hand, he says, maybe they can't afford not to: it was a decision to save money on testing that allowed a defect in Hubble's primary mirror to go undetected until it was in orbit, nearly dooming the entire mission. The JWST's supporters contend that, even with further budget overruns, the telescope will still break the historical cost pattern for large space telescopes. \"Not even including its four space-shuttle servicing missions, Hubble cost $4 billion or $5 billion in today's dollars just to build and launch,\" Dressler notes. \"Here we are, building a telescope that is almost seven times bigger, it is cryogenic, it is operating 1.5 million kilometres away, and it is costing the same amount as Hubble did, if not less. That is remarkable, and this is probably the biggest scale on which we will consider building such things in this country.\" Even so, ambivalence still surrounds the JWST. Failure is not an option, either for NASA or for the astronomers it supports. Yet, in the face of flat or declining budgets, a dwindling docket of near-term astrophysics missions and rising public outrage over perceptions of runaway government spending, tough questions are inevitable. At a mid-September meeting of the agency's astrophysics subcommittee, efforts to nail down just how many extra dollars lie between the JWST and its eventual arrival at L 2  were met with silence. Until the announcement of a new budget and schedule, informed by recent panel reviews, that is the best answer anyone is likely to get. Lee Billings is a freelance writer based in New York. \n                     James Webb Space Telescope \n                   \n                     The JWST at the Space Telescope Science Institute \n                   \n                     Space Telescope Science Institute \n                   \n                     Astro2010 Decadal Survey report \n                   \n                     JWST Webb cams \n                   Reprints and Permissions"},
{"file_id": "468026a", "url": "https://www.nature.com/articles/468026a", "year": 2010, "authors": [{"name": "Wendee Holtcamp"}], "parsed_as_year": "2006_or_before", "body": "Marine scientists are prowling the Bering Sea to learn how climate affects minute sea creatures and the lucrative fishery that depends on them. At 2 a.m. in the far north, marine scientist Alexei Pinchuk started his work as the summer sun dipped below the horizon. Using a winch, he lowered a 10-metre-long net \u2014 split like a giant pair of trousers \u2014 over the starboard side of the RV  Thomas G. Thompson , which was cruising in the Bering Sea, west of Alaska. He dropped the net to the sea floor and then hauled it back up to examine his catch. Pinchuk, a Russian working at the University of Alaska's Seward Marine Center, had harvested a whole zoo of minute animals, including larval fish, squid, crabs and octopuses, plus amphipods, copepods and krill. \"They're much more interesting than mammals,\" he says, wryly. These miniature zooplankton form critically important middle links in the food web, helping to sustain the rich life of the Bering Sea. The region provides half of the commercial seafood caught in the United States annually, including cod, sole, flounder, salmon and crabs. The tiny animals have also given Pinchuk and his colleagues a breakthrough in understanding how climate fluctuations affect Alaska walleye pollock ( Theragra chalcogramma ) in the largest single-species fishery in North America. The Bering Sea yields one million tonnes of pollock each year, amounting to one billion dollars. But the pollock population has begun to fall and scientists are concerned that climate change could put further stress on the fishery. For the past three years, Pinchuk has been studying this region as part of the six-year, US$52-million Bering Sea Project, a collaborative effort between the US National Science Foundation (NSF) and the North Pacific Research Board. The project has funded more than 100 principal investigators, including oceanographers, biologists, geochemists, social scientists, economists and ecosystem modellers, as well as their technicians and students. At its heart, the Bering Sea Project is an attempt to understand how climate change may affect the region's fisheries, by studying the ecosystem from top to bottom. Researchers hope this strategy will help fishery managers to protect the pollock and other species as the Bering Sea warms. \"The Bering effort is one of the few times a truly multidisciplinary effort has come together to create an integrated model of such depth, largely from scratch,\" says Beth Fulton, an ecosystem modeller with the Australian Commonwealth Scientific and Industrial Research Organisation's Marine and Atmospheric Research division in Hobart, which helps to manage Australia's fisheries.  \n                Home away from home \n              As he surveyed his haul of plankton, Pinchuk swirled the container, causing tiny crustaceans called copepods to emit a fluorescent blue glow. Although many of the researchers on board thrilled at the occasional sightings of whales and short-tailed albatrosses, Pinchuk remained single-minded in his plankton obsession. \"I hate birds. Walruses are ugly,\" he jokes. This summer Pinchuk spent several weeks on board the  Thompson , which left port just after the last of the winter sea ice melted in mid-June. Carrying 30 scientists, the ship cruised a zigzag trajectory across the eastern Bering Sea shelf, stopping every few hours to collect water, mud and samples of marine organisms (see  'Fishing for answers' ). \n               Click here for larger image \n               It will take some time for Pinchuk to sort his specimens. But he has already observed a distinct difference between the dominant zooplankton species during the past three years \u2014 the coldest on record \u2014 and the three previous decades, which culminated in five of the warmest years on record, from 2001 to 2005. An Arctic amphipod,  Themisto libellula , that hasn't been seen here since the 1970s, is invading the Bering Sea now that temperatures have dropped. Pollock diets have preferentially shifted to these large, fatty invertebrates and to the big copepods and krill that thrive in cold years. For researchers, one of the big questions about the Bering Sea food web is how year-to-year changes in ocean temperature influence plankton abundance, in turn affecting pollock and other commercial fish. A decade ago, oceanographer George Hunt of the University of Washington in Seattle developed an overarching model called the oscillating-control hypothesis. In simple terms, this suggests that the survival and growth of the youngest pollock fluctuate with shifts in the timing of the sea-ice melt in spring. Warm years help the cold-blooded zooplankton grow faster, and they provide ample food for the pollock born that year. In contrast, the hypothesis predicts that zooplankton and young pollock would not do so well in cold years. \"That was the first attempt to develop a broad conceptual framework for how climate might be impacting the Bering Sea,\" says Hunt. He approached the NSF for funds to test his hypothesis, but the agency instead asked Hunt to develop a broad programme for the Bering Sea, allowing others to examine his hypothesis in a much more comprehensive way. Hunt became chairman of a steering committee that planned the Bering Sea Project.  \n                Hot and cold \n              As it turns out, zooplankton collected by Pinchuk and his colleagues during the project and in the warm years before it have both supported the oscillating-control hypothesis and turned it on its ear. \"The original hypothesis considered zooplankton as one group, and assumed they would all do better in warm years,\" says Pinchuk. New data, however, paint a more nuanced picture. Small zooplankton species thrived in warmer conditions, as predicted, but larger, fattier species did not. The youngest pollock eat small copepods, but as the fish get bigger, they prefer energy-dense large copepods, amphipods and krill, says Pinchuk. As a result, the pollock initially fare well in warm years, but then start to falter for lack of nutritious food, and are unable to lay down enough fat to survive winter. Warmer conditions can also hurt by revving up the metabolism of the pollock, increasing the amount of food they need, says Franz Mueter, a fisheries biologist from the University of Alaska in Juneau who is collaborating with Pinchuk and others to revise the hypothesis. As part of the Bering Sea Project, Mueter looked at pollock data going back to 1964 and found that survival of the younger fish does not improve in a linear way with temperature, as the oscillating-control hypothesis predicts. Instead, the relationship is a bell curve: they don't survive winter well in extreme warmth or in extreme cold. (Unusually chilly water during the past few cold years has forced the young pollock into relatively warmer regions, where they can more easily fall prey to older pollock.) The results gleaned from work in the Bering Sea do not bode well for pollock, says Hunt. Although the sea has been colder than normal recently, temperatures have risen significantly since the 1980s and forecasts suggest that trend will continue. Juvenile pollock suffered in the hot years of 2001\u201305 and if they do not fare any better as the water warms in the future, \"prospects for the pollock population are grim\", says Hunt. Researchers are trying to give the fish a fighting chance by providing the foundation for a new way to manage the Bering Sea fishery. Like others in the United States, it is managed by a council made up of state fisheries agencies, the general public and federal agencies. Each year, the North Pacific Fishery Management Council reviews data on the pollock population in the Bering Sea and recommends limits on how much the industry can catch. The Bering Sea Project is providing information that could help to establish a much more sophisticated strategy \u2014 one that takes into account not just the size of the pollock population but also zooplankton numbers, predator data and many other factors to decide how much pollock should be caught each year. The big question is whether this comprehensive strategy will be ready fast enough. The Bering Sea pollock fishery is regarded as one of the best-managed fisheries in the world, and the Marine Stewardship Council certified it as sustainable in 2005. But shortly after that, pollock stocks dropped dramatically, perhaps because of the cold snap. Last year, researchers who conducted surveys in the area were shocked to find relatively few three-year-old fish. That caused Greenpeace to move pollock to its 'red list' of seafood to avoid. Some researchers worried that the population was heading for a crash. \"It's quite possible to take too many fish for the ecosystem to function as it once did,\" says Jon Warrenchuk, ocean scientist with the conservation organization Oceana. He thinks that the Bering Sea Project could help in creating a better management system. Not everyone agrees that the system is broken. Jim Ianelli is a scientist with the National Oceanic and Atmospheric Administration who runs the pollock-stock assessment models used to set current catch limits. \"I think we're harvesting sustainably and responding appropriately by scaling catches according to trends in abundance,\" says Ianelli, who is also a principal investigator on the Bering Sea Project. He says that information gleaned from the project and other studies has already helped managers make more informed decisions. On board the  Thompson   last summer, researchers hurried to collect as much data as possible in the short time available. After Pinchuk finished gathering his harvest, the ship headed off through steely grey waves towards a horizon shrouded in fog. Pinchuk has been studying the northern seas for more than two decades, long enough to witness the changing climate firsthand. He has watched Alaska's glaciers melt and zooplankton populations shift in response to rising temperatures. Arctic amphipods have shown up in his hauls, lured south by the cold snap of the past three years. But if temperatures resume their climb, as expected, Pinchuk suspects the amphipods will disappear once more. And with them, some of the pollock may also head north into Russian territory, in search of cooler waters.  \n                     Wendee Holtcamp's entries on Nature's Great Beyond Blog \n                   \n                     Wendee Holtcamp's blog \n                   \n                     Bering Sea Project \n                   Reprints and Permissions"},
{"file_id": "468022a", "url": "https://www.nature.com/articles/468022a", "year": 2010, "authors": [{"name": "Roberta Kwok"}], "parsed_as_year": "2006_or_before", "body": "Behind the walls of the J. Craig Venter Institute, Ham Smith and Clyde Hutchison quietly worked to bring a synthetic cell to life. Sitting in adjoining offices on the second floor of the J. Craig Venter Institute (JCVI) in San Diego, California, Ham Smith and Clyde Hutchison carry on a fragmented conversation through their open doors. \"Clyde, did you do your timesheet?\" says Smith. \"It's due in 12 minutes.\" Hutchison pauses: \"No, it was due three hours ago.\" They work at their computers for a minute in silence. Both are dressed in green short-sleeved shirts, tan trousers and black shoes, although they swear the wardrobe coordination is an accident. \"The market finished flat,\" says Smith. The dialogue is almost constant between the two men \u2014 friends and collaborators who are rarely seen apart. They are the veteran DNA craftsmen at the JCVI, a non-profit research organization founded by the provocative genome scientist who gave it his name. Originally from academia, the pair has taken advantage of Venter's resources and unparalleled salesmanship to pursue ambitious projects in synthetic biology, research that Venter says could enable faster vaccine production and the development of organisms that churn out precursors to fuel. In May, the JCVI team announced that it had built a 1-million-base-pair genome \u2014 the longest working piece of chemically synthesized DNA yet assembled \u2014 and used it to restart a bacterial cell 1 . Although some scientists disagree on whether the resulting microorganism, called 'Synthia' in the popular press, is indeed 'synthetic' \u2014 the synthesized genome sequence was cribbed from a related bacterial species rather than being built to a novel design \u2014 few deny the technical skill demonstrated by such work. \"The ability to synthesize and put together so many nucleotides without a mistake really requires guys on the level of Smith and Hutchison,\" says David Botstein, a geneticist at Princeton University in New Jersey who has worked with Smith. \"I don't think many other people could have done it.\" The two are acclaimed for their pioneering work. Smith shared a Nobel Prize in Physiology or Medicine in 1978 for his discovery of a restriction enzyme 2 \u2014 a protein that cuts DNA at specific sites. In the 1970s, Hutchison helped to determine the first full sequence of a DNA molecule and co-developed site-directed mutagenesis 3 , a technique that enables researchers to make targeted changes to DNA sequences. The methods Smith and Hutchison helped to develop underpin much of the work done today in molecular genetics. \"They're both regarded as scientist's scientists,\" says David Baltimore, a molecular biologist at the California Institute of Technology in Pasadena. \"They've both done enormously important work in basic science.\" Now 79 and 71, respectively, Smith and Hutchison have become intellectual partners and close friends. Smith, who looms over Hutchison, is slouchy, often sporting a half smile somewhere between amusement and embarrassment; Hutchison is more deliberate, precise in movement and dry in delivery. They are still pushing the possibilities of their field and, stints at the desk aside, they try to spend about half their working hours at the bench \u2014 something that helps them maintain \"a real perspective on what is possible\", says Hutchison. This comes in handy when tasked with meeting the sometimes audacious goals set by their boss. \"At one level, each of them operates like a septuagenarian postdoc,\" says John Glass, a synthetic biologist at the JCVI's other campus in Rockville, Maryland.  \n                Three's company \n              When Smith and Venter first met in 1993, Venter already had a reputation. He was still five years away from challenging the public effort to sequence the human genome, but the US National Institutes of Health, where Venter worked until 1992, had filed patent applications on DNA fragments sequenced by Venter's team. The move didn't go down well with other scientists, nor did his tendency to antagonize scientific rivals. \"He was commonly called an asshole,\" says Smith. But over drinks in a bar in Spain, the two discovered common ground: both started in medicine and had served in the navy. Venter invited Smith, then a professor at Johns Hopkins University School of Medicine in Baltimore, Maryland, out for dinner with friends, and the group got drunk. \"Almost the first instant that I actually met him, I liked him,\" says Smith. Smith joined the scientific advisory council of the non-profit organization The Institute for Genomic Research (TIGR) in Gaithersburg, Maryland, which Venter had founded, and began collaborating scientifically with Venter, despite concerns of academic colleagues that the association might taint Smith's career. Their teams worked on determining the first genome sequence of an independently living organism, the bacterium  Haemophilus influenzae 4  \u2014 in which Smith and co-workers had originally discovered the HindII restriction enzyme. They chopped the 1.8-million-base-pair genome into pieces, sequenced the fragments and then assembled them computationally into a genome sequence. It was the first time the technique, called shotgun sequencing, had been used on such a large DNA molecule. Smith's excitement, colleagues say, outweighed practical considerations. Jean-Fran\u00e7ois Tomb, a former research associate at Johns Hopkins, recalls members of Smith's lab worrying about a lack of funding for the project, but Smith was only interested in the science. \"He said, 'Look, you sequence the genome once and it's forever,'\" says Tomb. When the  H. influenzae   genome was nearly done, Venter wanted to sequence another right away. Smith said that they should try something small, and thought of Hutchison, then at the University of North Carolina at Chapel Hill. Hutchison was studying  Mycoplasma genitalium   \u2014 a bacterium thought to have the smallest genome, at half a million base pairs, of any free-living organism. Venter liked the idea. \"He said, well, as soon as we leave lunch, why don't you call him up?\" says Smith. Hutchison agreed to help and sent Smith 10 micrograms of  M. genitalium   DNA. They finished the sequence in about two months 5 . \"I was very pleased,\" says Hutchison. New techniques appealed to him, and he often pushed his team to develop better methods. \"From the beginning, Clyde was into high-throughput,\" says Mike Conrad, a former postdoc in Hutchison's University of North Carolina lab. \"He liked stuff that was fast.\" When Hutchison took a sabbatical at TIGR in 1996, he, Smith and Venter began to discuss the idea of developing a cell with the minimum genome needed to survive. Hutchison was already investigating which genes  M. genitalium   could live without 6 , but he knew that deleting multiple genes simultaneously from this bacterium was technically difficult. The threesome speculated that they might need to synthesize whole candidate genomes and test them in recipient cells. Hutchison again collaborated with Smith in 2003 at Venter's latest non-profit institute, the Institute for Biological Energy Alternatives (IBEA) in Rockville. Their team synthesized the 5,000-base-pair genome of the bacteriophage \u03a6X174 (ref.  7 ). Hutchison had helped determine its sequence in the 1970s, and the small size made it convenient for trying out synthetic techniques. Smith and Hutchison had very different experimental styles, recalls team member Cindi Pfannkoch. \"Clyde likes to plan everything,\" she says, whereas Smith practices more casual 'bathtub biochemistry'. In spite of this, the two got along. \"They speak the same language,\" she says.  \n                Taming the cell \n              In 2005, Hutchison started full time at the JCVI, which was formed by a merger of the IBEA and other Venter organizations. Leaving university life has disadvantages: \"I can't do whatever I want,\" says Hutchison. \"We're not totally independent agents.\" But the trio's interest in big scientific challenges has kept them together. \"I think all three of them are more about just doing the home-run experiment,\" says Dan Gibson, a synthetic biologist at the JCVI in Rockville. The synthetic-cell project picked up steam in 2005 as more researchers joined Venter's synthetic-biology team, which eventually comprised some two dozen scientists. The project evolved into a two-pronged effort: one group focused on constructing a synthetic  M. genitalium   genome, while the other tried to transplant natural genomes into cells of different  Mycoplasma   strains and species (see 'The path to a synthetic cell'). boxed-text Smith and Hutchison, who worked on genome construction, were circumspect about the chance of success. \"We didn't know it would be possible till it was done,\" says Hutchison. Technical challenges loomed. The large DNA segments might break; the slow growth rate of  M. genitalium   limited the pace of progress; and \u2014 most crucially \u2014 rebooting a cell with a new genome had never been done before. Although often seen just as the public face of the JCVI, Venter contributed to the science. While Smith and Hutchison worked out the details, Venter made key strategic decisions. At first, the team tried assembling pieces of the  M. genitalium   genome from short DNA fragments rather than ordering longer, prefabricated, but more expensive segments from a DNA synthesis company. But progress was slow. \"After we'd been working on this a couple of months, Craig comes into the lab and says, 'Ham, how many pieces do you have put together so far?'\" says Smith. \"And I said, 'Well, we haven't got any of them yet.' And he says, 'All right, we're going to order them'.\" The genome-transplantation group was having no luck either. Carole Lartigue, a postdoc on the team, worked on the problem for two years \"with nothing but failure\", says Glass, who oversaw the research. Lartigue finally got the first evidence of successful transplantation in late 2006, and the following year the team announced that it had managed to transplant a natural \u2014 not synthetic \u2014 genome from  Mycoplasma mycoides   into cells of the related species  Mycoplasma capricolum , changing their identity 8 . Although the paper was announced to great fanfare, the mood at the JCVI wasn't always so jubilant. A few months after publication, Smith came into the lab distraught. He feared that some  M. mycoides   cells, from which the donor genome was originally isolated, might have become patched up and revived when mixed with the recipient cells \u2014 an idea he called the 'punctured tyre' hypothesis. Resurrected cells could have been mistaken for transplants. Hutchison, who is known for being meticulous about experimental controls, says he \"thought the evidence was pretty good that wasn't the case\". But Smith's doubts were not laid to rest until late in 2008, when the team started up  M. capricolum   cells with a natural  M. mycoides   genome that had first been inserted into and modified in yeast \u2014 a process that 'purified' the genome of any cellular remnants of its original host 9 . \"That was absolute proof,\" says Smith. By now, Smith and Hutchison were working from the JCVI's new lab in San Diego \u2014 a move prompted, according to Smith, by Mary-land's freezing winters. In 2003, Smith had slipped on the ice and broken his leg. A couple of years later, while he and Venter were walking through the sleet in Rockville, \"I turned to Craig and said, 'Why do we live here?'\" says Smith. \"He said, 'That's a good point.'\" Venter set up a lab near the University of California, San Diego, his alma mater, and Smith and Hutchison moved west. \"I came out because Ham was coming out,\" says Hutchison. By this time, JCVI researchers had also chemically synthesized the  M. genitalium   genome. Starting with segments of 5,000 to 7,000 base pairs made by DNA synthesis companies, the researchers connected them into progressively larger pieces of DNA, first  in vitro , and then in yeast 10 . But they still couldn't successfully transplant the  M. genitalium   genome, and the organism's slow growth rate meant that they had to wait at least a month to see the results of an experiment. \"We just didn't know if it would ever work,\" says molecular biologist Gwynedd Benders, a former team member. \"It's like, are we just going to keep hammering at this?\" As early as 2007, Venter had suggested switching the donor species: synthesizing the  M. mycoides   genome \u2014 which was roughly twice as large as  M. genitalium  's \u2014 and transplanting it into  M. capricolum .  Mycoplasma mycoides   grew faster, and the team had already managed to transplant the bacterium's natural genome. The researchers had been reluctant to switch because they didn't yet know whether the  M. mycoides   genome could be transplanted from yeast, where the synthesized genome would need to be assembled, into bacteria. But once they had successfully done this with the natural genome, Venter's idea seemed more attractive. In late 2008, Venter discussed a strategy for the  M. mycoides   genome synthesis with Smith, and Smith e-mailed Gibson, \"Craig wants this done PDQ\" \u2014 pretty damn quick. \"It's that kind of thing where Craig is absolutely essential,\" says Smith. \"He was pushing it. I would have dragged for months.\"  \n                Waiting game \n              But synthesizing the  M. mycoides   genome presented new problems. Pieces of this genome above a certain size didn't replicate well in  Escherichia coli , the bacterium used to amplify the DNA. Even determining the  M. mycoides   genome sequence proved difficult because of repetitive DNA sequences. \"It just seemed to go on for months and months,\" says Benders. After the  M. mycoides   genome was finally assembled, the team endured some suspense-filled weekends. Transplantation experiments were performed on Fridays, and Gibson checked for the blue bacterial colonies that would indicate success on Monday mornings. Week after week, no blue colonies appeared, and finally a single mutation was discovered in one of the synthetic DNA segments. This was corrected and transplantation tried again on Friday 26 March 2010. \"That was a really, really long weekend,\" says Gibson. On the Monday morning around 6 a.m. Gibson found a single blue colony and e-mailed Venter, Smith, Hutchison and Glass. Knowing tests were still needed to confirm the result, Gibson says he was \"sweating\" all day about the possibility of contamination, even as they celebrated with champagne. But Smith, who had told him to wake Venter that morning with the news, was more sanguine. Gibson says, \"He just knew it was real\". Since the announcement, the team has fielded criticism for calling the resulting cell 'synthetic' when the genome was essentially a replica of a natural genome and required an existing recipient cell. Hutchison argues that 'synthetic' simply means 'chemically synthesized', not newly designed, and recipient cell contents are eventually replaced. \"You'd like to design a genome from scratch,\" he says. \"You'd like to put it into a cytoplasm that you built up from scratch. But we're trying to do something we can do.\" Although many synthetic-biology researchers are tweaking existing genetic elements, assembling them into new combinations, and inserting the 'circuits' into different organisms, few aspire to design entirely new genes. On a July morning, Smith and Hutchison sit together in an auditorium at the JCVI's San Diego building listening to a presentation by summer intern Nico Enriquez. Members of the synthetic-biology team, whose lab is down the corridor from Smith and Hutchison's offices, are scattered in the audience; researchers in Rockville watch the talk by videoconference. The synthetic-biology team is now attempting to develop the 'minimal cell': Gibson and his colleagues are building new versions of the synthetic  M. mycoides   genome with genes removed, then transplanting the edited genomes into recipient cells and monitoring colony size and growth. Enriquez presents methods proposed by Smith to assess cells' growth rates by measuring DNA and protein levels, and initial results look promising. \"Seems like Ham's got something right again,\" says Enriquez. At the end of his talk, Enriquez shows a picture of a pair of frolicking otters. \"They kind of remind me of Ham and Clyde,\" he says to laughter in the audience. \"The tall one is Ham, and the shorter one is Clyde \u2014 always together.\" Once the researchers have a minimal genome, they hope to determine the function of every uncharacterized gene and build a computer model that predicts the cell's responses to genetic changes. But such a system isn't necessarily more informative than an organism with a larger genome, argues geneticist George Church at Harvard Medical School in Boston, Massachusetts. Although the cell will yield some scientific insights, he says, they will probably be specific to  Mycoplasma . \"As you delete these things, you'll end up with a cell that is weaker and weaker, less and less industrially useful, and less and less relevant to sophisticated organisms,\" he says. Smith and Hutchison agree that  Mycoplasma   is unlikely to be used industrially as it is expensive to grow, but say that lessons learned from developing and studying this system might apply to organisms better suited for commercial purposes. Hutchison says a smaller system will be easier to understand, and he predicts that some gene deletions might actually make the cell grow faster. Smith notes that scientists initially couldn't see all the uses of restriction enzymes, which have proved essential for manipulating DNA. And in any case, he is largely driven by curiosity. \"I just want to understand, thoroughly, one cell,\" he says. Venter, too, emphasizes that the minimal cell is meant to be a proof of concept. \"It's truly basic science,\" he says. JCVI researchers are investigating whether their genome-transplantation techniques can be extended to other, more complex, bacteria, such as cyanobacteria. The team is also trying to make large-scale changes to cyanobacterial genomes, with the eventual goal of enabling production of valuable chemicals, says Glass. On the more commercial side, Venter and Smith's company, Synthetic Genomics, based in La Jolla, California, has filed patent applications on the JCVI's methods, and aims to engineer algae that produce hydrocarbons that can be converted into fuel. A separate company, Synthetic Genomics Vaccines, set up by the JCVI and Synthetic Genomics, is working with pharmaceutical company Novartis, based in Basel, Switzerland, on ways of making flu vaccine production more efficient. Although other synthetic biologists are already using the JCVI's techniques to assemble pieces of DNA, many agree that it will be a while before anyone designs a whole genome from scratch. Right now, the quickest route to industrial application is unquestionably the modification of existing organisms, says Venter. But, he says, \"the future will be designing and making whole new species\". It is now down to Smith and Hutchison to make Venter's ideas a reality \u2014 their working styles distinct, but complementary. At a lab meeting, Smith sketches a plan on a whiteboard for an experiment to detect very small changes in bacterial growth rate, which will be necessary to compare  M. mycoides   strains with different genome versions. \"You should do multiple dilutions each time,\" says Hutchison. Smith hesitates. \"I want it to be very simple, so \u2026\" \"Yeah, but maybe in working out how to do it, you need to do something that's not quite as simple,\" says Hutchison. The two have no plans to retire yet. Most weekdays they take a walk together in the hills around their office, which overlook Interstate Highway 5, ruminating on ideas and keeping an eye out for rattlesnakes. \"I think this is the pinnacle of my career,\" says Smith, who wants to keep working for at least another four or five years. Hutchison adds, \"But maybe we'll do something else next.\"  Roberta Kwok is a freelance writer in the San Francisco Bay Area. \n                     Biotechnology@nature.com \n                   \n                     J. Craig Venter Institute\u2019s website \n                   \n                     Ham Smith autobiography \n                   \n                     Clyde Hutchison\u2019s website \n                   Reprints and Permissions"},
{"file_id": "4671026a", "url": "https://www.nature.com/articles/4671026a", "year": 2010, "authors": [], "parsed_as_year": "2006_or_before", "body": "Nature   surveys the sequencing landscape. Ten years ago, two fingers were enough to count the number of sequenced human genomes. Until last year, the fingers on two hands were enough. Today, the rate of such sequencing is escalating so fast it is hard to keep track.  Nature   attempted nevertheless: we asked more than 90 genomics centres and labs to estimate the number of human genome sequences they have in the works. Although far from comprehensive, the tally indicates that at least 2,700 human genomes will have been completed by the end of this month, and that the total will rise to more than 30,000 by the end of 2011. \n               THE AMERICAS  \n             \n               Click here for larger image \n               \n               EUROPE  \n             \n               Power to the people  \n             The latest sequencing technology is no longer concentrated at a few major centres. In Britain, the Wellcome Trust Sanger Institute in Hinxton houses 38 of the country's high-throughput sequencers, and the rest are scattered over an additional 32 sites. Falling costs mean that a human genome is within the reach of individual labs. \n               ASIA  \n             \n               Click here for larger image \n               \n               The rise of genome factories  \n             Individual labs may still find it cheaper and easier to outsource a human genome to a power-house 'sequencing service provider'. The BGI in Shenzen, which has global expansion plans, predicts that its machines will have completed some 10,000 to 20,000 human genomes by the end of 2011.   Methods:   Our survey focused on large, academic projects rather than individual labs; we included complete genome sequences, both high- and low-coverage, and excluded partial (exome) sequences. The list excludes all biotechnology and pharmaceutical companies and most sequencing service providers, which do not disclose their work. The sequencer locations, based on a user-generated map at  go.nature.com/b74acy , includes some 60\u201370% of all machines. \n               WHY SCIENTISTS WANT TENS OF THOUSANDS OF GENOMES \u2014 AND MORE  \n             \n               To understand populations  \n             Comparing lots of genomes lets researchers identify points at which one genome differs from the next. Costs may be falling, but sequencing and data analysis are still pricey. So most researchers face a trade-off between the number of subjects and the accuracy in the sequences they can afford. For projects examining how populations commonly differ, sequencing a large number of individuals at relatively low accuracy or 'depth of coverage' is enough. About 900 genomes sequenced so far by the 1000 Genomes Project have been read three times on average. \n               To understand disease  \n             Researchers trying to uncover rare disease-linked mutations \u2014 perhaps limited to just one family or an individual \u2014 need precision, typically sequencing each genome 30 times on average. Cancer genomes, many sequenced under the auspices of large collaborations, account for a sizeable chunk of high-coverage genome sequences completed to date. Projects scrutinizing people with diabetes, Crohn's disease and other disorders are starting to emerge. Analysing all the genome data is a huge challenge, as is turning genetic discoveries into clinical benefits. \n               To understand individuals  \n             The rate at which human genomes are being sequenced \u2014 at least in mega-projects \u2014 will probably slow once researchers have extracted most of the common variation shared by populations and diseases. But individuals are genetically unique. If the cost of a genome sequence becomes trivial and the benefits of knowing one increase (through gene-tailored medicine), then personal genome sequencing will continue to push the genome count up and up. Graphics by Nik Spencer; data complied by Alla Katsnelson. Map of next-generation sequencers compiled by Nick Loman and James Hadfield and adapated from  http://pathogenomics.bham.ac.uk/hts \n                     The Human Genome at Ten \n                   \n                     Personal Genomes web focus \n                   \n                     Big Data \n                   \n                     National Center for Biotechnology Information \n                   \n                     EMBL Nucleotide Sequence Database \n                   \n                     1000 Genomes Project \n                   \n                     Personal Genome Project \n                   \n                     International Cancer Genome Consortium \n                   \n                     The Cancer Genome Atlas \n                   Reprints and Permissions"},
{"file_id": "467906a", "url": "https://www.nature.com/articles/467906a", "year": 2010, "authors": [{"name": "Richard Van Noorden"}], "parsed_as_year": "2006_or_before", "body": "Which urban regions produce the best research \u2014 and can their success be replicated? When the \u00d8resund bridge connecting Copenhagen, Denmark, with Malm\u00f6, Sweden, opened in 2000, both sides had much to gain. Sweden would get a physical connection to the rest of mainland Europe; residents of Copenhagen would have access to cheaper homes close to the city; and economic cooperation would increase. But Christian Matthiessen, a geographer at the University of Copenhagen, saw another benefit \u2014 the joining of two burgeoning research areas. \"Everyone was talking about the transport of goods and business connections,\" he says, \"and we argued that another benefit would be to establish links between researchers.\" Ten years later, those links seem to be strong. The bridge encouraged the establishment of the '\u00d8resund region', a loose confederation of nine universities, 165,000 students and 12,000 researchers. Co-authorship between Copenhagen and the southernmost province of Sweden has doubled, says Matthiessen. The collaborations have attracted multinational funds from the European Union. And the European Spallation Source, a \u20ac1.4-billion (US$2-billion) neutron facility, is on track to begin construction in Lund, Sweden, in 2013. The region's promoters claim that it is emerging as a research hub of northern Europe, aided in part by construction of the bridge. For Matthiessen, the bridge also inspired the start of a unique research project \u2014 to catalogue the growth and connections of geographical clusters of scientific productivity all over the world. Most research activities are concentrated around major metropolitan areas. By Matthiessen's count, the top 75 science-producing clusters in the world from 2006 to 2008 generated some 57% of the research \u2014 3.9 million papers. Many argue that a fine-grained analysis might help to identify the factors that drive successful research clusters \u2014 indicating rising stars and aiding city planners and policy-makers in building profitable centres elsewhere. In a 2009 paper, Koen Frenken at the University of Utrecht in the Netherlands and his colleagues proposed that studies such as these, which quantitatively map science clusters in physical space, be collected under the general field of 'spatial scientometrics' ( K. Frenken, S. Hardeman and J. Hoekman  J. Informetrics    3,   222\u2013232; 2009 ). Most analyses of success and failure to date have relied on individual case studies. Matthiessen, by contrast, wanted to use data to make global comparative estimates of all cities, in an effort to pick winners and losers. It is not an easy task. Although individual nations and international organizations such as the Organisation for Economic Co-operation and Development do analyse countries for research spending, research quality and numbers of scientists, there are no such data organized at the city level. The fact that geographers don't even agree on how best to define the boundaries of a metropolitan area makes that task doubly difficult. Conflicting definitions of concepts such as 'science cities' and 'innovation clusters' often leave analysts talking past one another. Where Matthiessen wants to look at strength in basic research, most economists and regional planners are concerned with technological innovation: patents and related wealth. As the global economy continues to flag, bringing such data together is important. \"Anthropologists, archaeologists, historians and geographers have argued for many years that cities are the engines of innovation,\" says Jos\u00e9 Lobo, a statistician and economist who works on regions and innovation at Arizona State University in Tempe. \"But what's difficult is to connect these historical case studies of success with data, so as to create instruments that policy-makers can use.\"  \n                Quantity and quality \n              Matthiessen and his team divide the globe into urban conglomerations on the basis of distances reachable by a 40-minute commute from a city centre. In this rubric, Oxford and Reading, UK, are one urban region, as are Amsterdam, the Hague, Rotterdam and Utrecht in the Netherlands. By assigning the addresses of research papers' authors to each conurbation, they produced tables of where cities rank in terms of output ( C. W. Matthiessen, A. W. Schwarz and S. Find  Urban Stud.    47,   1879\u20131897; 2010 ). \n               Click here for larger image \n               Topping Matthiessen's list are Tokyo, London, Beijing, the San Francisco Bay Area, Paris and New York (see  'Top cities by publication' ). A similar ranking emerges from an analysis provided for  Nature   by Elsevier, headquartered in Amsterdam, which maintains the Scopus database of journals using the simple method of assigning cities from the address an author provides. Both analyses highlight cities in which scientific output is growing. In particular, Beijing, which churned out 0.76% of the global output in 1996, produced 2.74% in 2008 (319,000 research papers). Other fast-growing areas are Tehran, Istanbul, Seoul, Singapore City and S\u00e3o Paulo. \n               Click here for larger image \n               The results track the economic expansion of Asia and the Middle East and an expanding list of foreign-language journal titles. But they don't necessarily capture the quality of research being published. Elsevier makes that judgement by looking at the average number of citations that a research paper from a city attracts (see  'Top cities by citation' ). This paints quite a different picture. Boston and Cambridge, Massachusetts, come out on top \u2014 attracting more than twice as many citations per paper as the global average. US cities dominate the quality table, with only Cambridge, UK, breaking into the top 10. Cities with the most improved relative quality in the past decade include Austin, Texas, and Singapore City \u2014 which has moved from 15% below average to 22% above it. Beijing, however, is below par in the quality stakes: its papers in the five-year period ending 2008 attracted 63% of the global average-citation rate.  \n                Lessons from Boston \n             \n               Click here for larger image \n               Boston ranks top in several analyses of scientific quality (see  'Top cities by journal' ), and in one sense it is easy to explain why. \"Take three or four of the best universities in the world, put them in a city with a seaport, and  voil\u00e0 !\" says Lobo. But copying the region's formula is quite another matter. How can one city start to emulate another that attracts the most research funding in the United States and has been built up over centuries? Having top research universities with mammoth budgets is likely to create a vibrant scientific community \u2014 but what can be harder is keeping top scientists in an area long term. From case studies, Mary Walshok, a sociologist at the University of California, San Diego, picks out three important factors that make cities sticky for scientists. Promise them the freedom to work on their own ideas. Then give them the tools and infrastructure to do so. Public funding is key to achieving these first two aims, but local private corporations and philanthropists who endow new buildings or research chairs also help. \"You can see this happening in Austin, and in Seattle,\" says Walshok. Walshok's third factor is an attractive lifestyle. Richard Florida, a sociologist and economist at the University of Toronto's Martin Prosperity Institute in Canada, lists scientists among the 'creative class': mobile, talented, creative thinkers that a city must lure in with amenities and smart urban planning. What counts as attractive for this set isn't always obvious. Kevin Stolarick, a statistician also at the Martin Prosperity Institute, suggests that biotech incubators and universities or hospitals should be close enough for a cup of coffee to stay hot when travelling between them. But high culture and hot coffee are not enough to make cities successful in terms of science. A flagging job market will not draw in creative thinkers. Moreover, cities generally held to be the most 'liveable' in surveys \u2014 Vancouver and various urban centres in Canada and Australia \u2014 are often not associated with outstanding creativity, says Peter Hall, a geographer at University College London. Even with the right ingredients \u2014 freedom, funding and lifestyle \u2014 to attract and keep scientists, there is no guarantee that their work will generate economic wealth. Lobo points out that New Mexico probably has the highest number of physicists per capita in the United States, thanks to the Los Alamos and Sandia national laboratories, but it is hardly an economic powerhouse, as the research doesn't lend itself to commercialization. Boston, on the other hand, has a strong foundation of basic science that has attracted companies and institutes, which in turn has created wealth and attracted more top scientists. Boston's economic resilience, born of a diverse labour force, is key to this virtuous cycle. Science is merely the latest in a series of economic rebirths \u2014 from being the largest city in early colonial America, to a centre for global shipping and sailing in the nineteenth century, to its current position as a biotech hub. Similar tales of success are told for the San Francisco area, with its attractive climate, culture of adventurous investment and laws that favour creative workers. It is illegal in California, for example, to enforce a waiting period before employees move to a competitor's firm, allowing people and ideas to move about freely.  \n                Wealth sized up \n              When it comes to generating wealth from science and technology, a few features seem necessary \u2014 but not sufficient. In general, bigger is better. For example, Luis Bettencourt at Los Alamos National Laboratory and Deborah Strumsky at the University of North Carolina, Charlotte, have found that new patents are granted disproportionately to larger urban centres. In this sense, great economic centres such as London, Tokyo and New York are bound to be science strongholds of a sort \u2014 even though their economic strength comes from other areas, such as financial markets. Many hold that, for applied research at least, being part of the urban tapestry really works. Cities are natural places for practical research, Bettencourt argues. Still, the question of whether large cities make for better scientists \u2014 rather than simply attracting more of them \u2014 is fiercely debated. Smaller cities are not out of the running. Generally, says Lobo, new industries emerge in large cities, but once standardized, they can move to a location with cheaper rents and labour costs. For towns seeking wealth through science, the anchoring presence of a large private research and development laboratory can inject huge benefits. Matthiessen says that the electronics corporation Philips, in Eindhoven, the Netherlands (population 200,000), is an example of a private company working to a local university's advantage by seeking collaborations with research scientists. But over-reliance on one company or industry can be precarious. Hewlett-Packard's research laboratories outside Corvallis, Oregon, have made the town (population 80,000) one of the United States' most innovative per capita, churning out eight patents per 1,000 population. But if Hewlett-Packard were to fall into decline or the patents run out, Corvallis would probably fall as well, says Lobo. Given the uncertainties, cities looking for a way to fire up economic engines should not necessarily bet on scientific innovation and technology as a short cut, says Strumsky. \"A lot of cities are desperately searching for a way to create jobs, and say: 'Let's invest in biotech research.' You might as well take your money and burn it in a really big pit,\" she says. If a city has no history or expertise in biotechnology, it must hire its creative inventors from elsewhere, pay them double to move to a biotech backwater, and link their inventions to a local production industry that itself must be made from scratch. Buffalo, New York, is the archetype of a city that made a mistake by investing in biotech research and development labs (see  page 912 ), Strumsky and others have argued. Although unable to retain a skilled labour force, the city is still investing in research.  \n                National trends \n              Many factors are out of the hands of urban planners and local policy- makers, however, and more sophisticated spatial scientometrics studies into why and where scientists cluster geographically could help to explain the influence of these factors. The evolution of a metropolitan region such as \u00d8resund was shaped by national and international policies and economics. National policies, for example, have largely determined the evolution of science cities in France, Spain, Portugal, South Africa and Russia in the past few decades by pushing money, and by extension scientists, into smaller cities in need of a boost. Michel Grossetti, a sociologist at the University of Toulouse in France, has found that capital cities in these countries lost ground in scientific publications relative to other cities. Henk Moed, a researcher at Elsevier, has in an unpublished analysis shown how from 1996 to 2010, the publication outputs for five major Spanish cities \u2014 Valencia, Barcelona, Bilbao, Seville and Zaragoza \u2014 have all grown faster than that for the capital Madrid, in parallel with a political trend towards more autonomy for Spanish regions. Grossetti and his team are starting to test how science resources have been concentrated or diffused in cities in every country across the world, from 1978 to 2008. Rather than simply crunching numbers of papers, he wants to take into account population, gross domestic product and institutional shifts. Grossetti hopes that this exercise will start to get to the heart of why science cities rise and fall, and how their evolution is shaped by economic and political factors. Researchers in the field know they are only starting to illuminate trends. They cannot even agree on whether research is concentrating in particular cities. Matthiessen says it is, but Grossetti says large, rising cities such as Beijing may have skewed the pattern. How the field may ultimately benefit policy-makers and planners is far from certain. Much development happens by chance. In the \u00d8resund cluster, the universities were swept up in general discussions about transnational integration, and were only later perceived as major catalysts for regional growth, says Bengt Streijffert, retired director of \u00d8resund University. Economic, social and political factors will continue to confound the best-laid plans to optimize a city for science and innovation. And Hall suggests that global data will never displace sound case studies. \"Data may be able to show you some interesting outliers \u2014 which are not the universities that develop in old cities and flourish there.\" But, he says, \"the number-crunching won't tell you much about the why and the wherefore\". \n                 Click here for enhanced graphics \n               \n                     Science Metrics Special \n                   \n                     NatureJobs Archive by Region \n                   \n                     Enhanced graphics for citation analysis \n                   \n                     Living Science \n                   \n                     AuthorMapper \n                   \n                     GoPubMed \n                   \n                     Scopus \n                   Reprints and Permissions"},
{"file_id": "468158a", "url": "https://www.nature.com/articles/468158a", "year": 2010, "authors": [{"name": "Alison Abbott"}], "parsed_as_year": "2006_or_before", "body": "The biology is too complicated. Pharma companies are quitting. Where are schizophrenia drugs going to come from? The shock wave hit when they broke the code. It was January 2005, nearly four years since the start of a clinical trial to definitively compare schizophrenia therapies. The US$43-million trial, involving nearly 1,500 patients at 57 clinical sites in the United States, was testing whether a raft of anti-psychotic drugs introduced in the 1990s \u2014 and hailed as transformational \u2014 was any better than a 50-year-old pill called perphenazine, one of a generation of drugs that left patients with horrible side effects. Until investigators unblinded the trial, codes concealed who was receiving which drug. As it turned out, it didn't matter. The Clinical Anti-psychotic Trials of Intervention Effectiveness (CATIE) made it clear that the new therapies were barely different from the old 1 . They were just as good as perphenazine at controlling psychosis \u2014 hallucinations and delusions. But patients taking the new drugs remained confused, socially withdrawn and bereft of drive, just as they did on perphenazine. And the new antipsychotics were not even convincingly freer of side effects \u2014 overall, three-quarters of patients abandoned their drug during the 18-month treatment regime, regardless of which drug they took. \"That was frustrating and humbling for the research community,\" says Jeffrey Lieberman, a psychiatrist at Columbia University, New York, and the trial's principal investigator. \"And it had a chilling effect on the pharmaceutical industry.\" Within a few years, under intensifying pressure to rein in costs, several large companies, including London-headquartered AstraZeneca and GlaxoSmithKline, chose to pull out of psychiatric pharmacology altogether. Chastened researchers also had to regroup. \"It became a case of back to the drawing board,\" says Shitij Kapur, head of King's College London's section on schizophrenia, imaging and therapeutics. Scientists needed to learn much more about the disease's biology. They had to ensure that whatever they learned would be 'translated' more smoothly to the clinic, by way of better animal models, biomarkers and clinical trials. And they wanted to develop drugs to target not just psychosis, but also the 'negative symptoms' such as impaired cognition, blunted emotions and lack of initiative \u2014 the types of trait that render most people with schizophrenia incapable of holding down a job. The scale of the work to be done was too daunting for individual labs. So in recent years, researchers working in the public and private sectors have decided to share more ideas and resources. Few expect a single molecule to do the entire job. \"Fifteen years ago, we were naively optimistic,\" says Kapur. Now, there is still optimism \u2014 but with a hefty dose of pragmatism thrown in. No one questions the transformational impact of the first antipsychotic drugs when they were introduced in the 1950s. Psychiatric hospitals could, for the first time, release large numbers of patients with schizophrenia who would otherwise have spent their lives incarcerated. The prototype, chlorpromazine, spawned a whole class of drug known as 'typical' anti-psychotics, including perphenazine.  \n                Moving target \n              But the price of that freedom was high. Typical antipsychotics exert their effect by blocking the dopamine type 2 (D2) receptor, modifying dopamine neurotransmission. But these silenced receptors also provoked distressing side effects such as twitching and jerking, leading to the misconception that movement disorders and antipsychotic efficacy were inextricably linked. So entrenched was this idea that when, in the 1960s, industrial pharmacologists discovered a promising antipsychotic drug candidate that did not disrupt movement in animal tests, they had difficulty persuading their sceptical managers to develop it. Sandoz, a company based in Basel, Switzerland, that is now part of Novartis, eventually introduced clozapine to the market in 1971. As well as acting on D2 receptors, it blocked the 5-HT 2A  receptor of the mood-modulating neurotransmitter serotonin, which seems to temper the movement side effects. The drug proved so much better that desperate psychiatrists lobbied for its reinstatement after Sandoz withdrew it from the market in 1975 when its own rare side effect became apparent: a susceptibility to life-threatening infections. \n               Click here for larger image \n               The US Food and Drug Administration (FDA) relicensed clozapine in 1989 for treatment-resistant cases, in conjunction with regular blood tests. Within years, other drug companies had launched their own 'atypicals' (see  'Schizophrenia drug sales' ), all with a clozapine-like pharmacology but intended to be safer. Yet some serious side effects, such as metabolic problems, emerged. By this point, clinicians were starting to question whether these new, and more expensive, drugs were any improvement on their predecessors. CATIE confirmed their worst fears.  \n                A self-fulfilling prophecy \n              A re-examination of the pharmacological profiles after the trial showed that although the atypical antipsychotics hit both D2 and 5-HT 2A  receptors, the D2 blockade seemed to be responsible for their clinical effects. That is not surprising in retrospect, given that the animal models used to test the drugs were all designed to pick up D2-receptor blockade, says Mark Tricklebank, a behavioural pharmacologist and director of the Lilly Centre for Cognitive Neuroscience in Windlesham, UK. \"It was all very circular, a self-fulfilling prophecy,\" he says. \"We'd been tuning the engine, when what we really needed was a new engine.\" To break out of this vicious circle, scientists realized that they needed some fresh thinking in basic and translational science. Clinical trials had been mostly focused on treating psychosis, but there was increasing recognition that cognitive deficits \u2014 poor memory, inability to maintain attention and poor problem solving \u2014 were a fundamental aspect of the disease. In 2005, Steve Hyman, then director of the US National Institute of Mental Health in Bethesda, Maryland, launched Measurement and Treatment Research to Improve Cognition in Schizophrenia (MATRICS). The forum aimed to bring together academics, industry and the FDA to generate consensus about how best to design clinical trials to test drugs targeted at these cognitive deficits \u2014 and later extended to negative symptoms \u2014 through a battery of specially designed tests 2 , 3 . Guided by the MATRICS recommendations, several cognitive enhancers are already in early-phase clinical trial as potential add-ons to standard antipsychotic therapies. Also in clinical trial are several candidate drugs acting on receptors of the neurotransmitter glutamate \u2014 the only approach to have shifted focus away from dopamine. But the glutamate circuitry in the brain may prove hard to manipulate safely. And for the scientific community, the real challenge lies in understanding the system they are tinkering with. \"We don't even understand schizophrenia at the biological level,\" says Thomas Laughren, the FDA's director of psychiatric drugs, voicing a frustration felt by many. A European collaboration of researchers known as Novel Methods Leading to New Medications in Depression and Schizophrenia (NEWMEDS) is throwing every cutting-edge technology available at the problem in a very unusual public\u2013private collaboration. Launched last year, the five-year, \u20ac20-million (US$28-million) effort funded by the European Commission includes seven academic partners, nine pharmaceutical companies (including AstraZeneca) and a few biotech companies. One of these is Icelandic genomics company deCODE genetics, which in 2008 identified in a large population study three variable genetic regions called copy number variations (CNVs), which, although very rare, confer a high risk of schizophrenia 4 . Scientists from industry say that it was initially hard to convince their companies of the value of sharing information in NEWMEDS. But they did \u2014 and to their evident glee, industrial pharmacologists can for the first time discuss openly, at least within the consortium, their individual approaches in psychiatric disease. Academic members, in turn, are gleeful about access to some of the industrial resources now on the table. They have pooled extensive information and material including data from many clinical trials in schizophrenia, sometimes with associated blood samples. \"We now have the biggest database ever on this disease \u2014 more than 10,000 patients,\" says Tine Bryan Stensb\u00f8l, a pharmacologist at the Danish pharmaceutical company H. Lundbeck and the coordinator of NEWMEDS. In a joint project led by geneticist Hreinn Stefansson of deCODE genetics, psychiatrist Andreas Meyer-Lindenberg from the Central Institute of Mental Health in Mannheim, Germany, and neuroscientist Michael Brammer from London's Institute of Psychiatry analysed magnetic resonance imaging data from 500 people identified by deCODE genetics as having the high-risk CNVs, as well as 500 control subjects. They hope that the study will identify brain structures that are disrupted by abnormal genetic signatures and might eventually point to new therapeutic targets. \"We wouldn't be doing this without the NEWMEDS initiative,\" says Meyer-Lindenberg. An essential but admittedly less glamorous task for NEWMEDS is to determine the robustness of methods used to test drug candidates, particularly the animal and human tests of memory, attention and other aspects of cognition that are notorious for their sensitivity to tiny differences in environment. Unreliable tests may explain why drug candidates that look hopeful in animals fail in the clinic. Industrial and academic scientists are now using standardized protocols in their own labs, then comparing results and trying to understand why some may vary. The consortium is also adopting rodent touch-screen technology, in which animals in behavioural tests tap a screen with their nose to get a reward for performing the experimental tasks, rather than press a lever or poke their noses into a hole. Being automated, it does not need constant observation. And crucially, results from such tests are potentially easier to translate into human psychology testing, which is increasingly based on touching screens. The new concerted strategies could renew industry's optimism, even if there are few concrete signs of it just yet. With up to 1% of the world's population estimated to be affected by the disease, schizophrenia represents a huge potential market for any company that can find a new drug that genuinely improves any symptoms \u2014 particularly given that most patients develop the disease in their early twenties, and could be on daily therapy for the rest of their lives. Maybe, after all, a shock wave was just what the field needed. \n                     Schizophrenia special \n                   \n                     Neuropsychiatric disease Insight \n                   \n                     NewMeds initiative \n                   \n                     MATRICS initiative \n                   \n                     CATIE initiative \n                   Reprints and Permissions"},
{"file_id": "468160a", "url": "https://www.nature.com/articles/468160a", "year": 2010, "authors": [{"name": "Kerri Smith"}], "parsed_as_year": "2006_or_before", "body": "Do the billions of non-neuronal cells in the brain send messages of their own? Halfway through a satellite meeting at the Federation of European Neurosciences conference in Amsterdam in July, researcher Ken McCarthy takes the stage to give his presentation. He sports a black shirt and jeans, and his strong cheekbones, shock of white hair and tanned skin give him the look of a film star. But he doesn't have the confidence to match. \"I find this a little bit daunting,\" he says, as he organizes his slides. McCarthy, a geneticist at the University of North Carolina School of Medicine in Chapel Hill, is about to fan the flames of a debate about whether glia, the largest contingent of non-neuronal cells in the brain, are important in transmitting electrical messages. For many years, neurons were thought to be alone in executing this task, and glia were consigned to a supporting role regulating a neuron's environment, helping it to grow, and even providing physical scaffolding (glia is Greek for 'glue'). \n               boxed-text \n             In the past couple of decades, however, this picture has been changing. Some glia, known as astrocytes, have thousands of bushy tendrils that nestle close to the active junctions between neurons the synapses (see  'Neural threesome' ). Here they seem to listen in on neuronal activity and, in turn, to influence it. Studies show that chemical transmitters released by neurons cause an increase in the levels of calcium inside astrocytes, spurring them to release transmitters of their own. These can enhance or mute the signalling between neurons, or influence the strength of their connections over time. Moreover, astrocytes activated at one synapse might communicate with other synapses and astrocytes with which they make contact. The consequences of this 'gliotransmission' could be profound. The human brain contains roughly equal numbers of glia and neurons (about 85 billion of each), and any given astrocyte can make as many as 30,000 connections with cells around it. If glia are involved in signalling, processing in the brain turns out to be an order of magnitude more complex than previously expected, says Andrea Volterra, who studies astrocytes at the University of Lausanne in Switzerland. Neuroscientists, who have long focused on the neuron, he says, would \"have to revise everything\". In the past year or so, several papers have highlighted the urgency of this revision. But the research that McCarthy is about to discuss could put a stop to the enthusiasm. \"I'm presenting work from genetic studies that fly in the face of gliotransmission,\" he begins. Most studies so far have investigated astrocytes cultured in dishes, and bombarded them with calcium to elicit an effect. It has long been suggested, however, that these methods aren't specific enough to astrocytes, and might be affecting neurons as well. What is needed is a way to target astrocytes alone. So McCarthy has developed genetically engineered mice in which astrocytes can't signal normally. The mutations seem to have no effect on neuronal transmission in the brain. His group's finding could come as a relief to some. Given the enormous neural complexity that gliotransmission would imply, \"people don't want astrocytes to be involved\", says Phil Haydon, a neuroscientist who studies glia at Tufts University in Boston, Massachusetts. But many, including most at the Amsterdam meeting, have built their careers on gliotransmission. In addition to their effects on the day-to-day functioning of the central nervous system, glia have opened new avenues of research into sleep, as well as psychiatric and neurological disease. Now, researchers are being forced to prove McCarthy wrong, or re-evaluate the fundamental precepts upon which the field was built. David Attwell, a neuroscientist at University College London, says that emotions in the community are running high. \"If someone comes along and says that everything you've done is wrong, it's like you've wasted your life,\" he says. \"It's become quite a polarized field, just in the last year or two.\"  \n                The bomb \n              Gliotransmission has had plenty of support recently. In January this year, a group led by Dmitri Rusakov at University College London and Stphane Oliet at the University of Bordeaux in France published results suggesting that the chemical D-serine, released from astrocytes, activates a particular receptor the NMDAR, or  N -methyl-D-aspartate receptor on the surface of neurons, influencing their behaviour. Communication through NMDARs is thought to be important in learning and memory, because it can help to enhance chatter between synapses and help memories to form. And in September, a team led by Justin Lee at the Korea Institute of Science and Technology in Seoul found evidence that astrocytes in the cerebellum release the neurotransmitter GABA (-aminobutyric acid). However, unlike the work of most groups, which suggests that astrocytes release chemicals packaged within tiny bubbles called vesicles, Lee's experiments imply that the cells are transmitting the chemicals directly through an ion channel in their membranes. When the researchers blocked the channel, called Bestrophin-1, GABA levels went down, suggesting that glia release GABA through this outlet. But there have been concerns about experimental techniques. In these experiments, and many before them, investigators have tried to induce astrocyte signalling in petri dishes by pipetting calcium into individual cells and watching what happens. This approach is \"like an atom bomb\" going off in the cell, says McCarthy. Under natural conditions, the level of calcium would increase much more slowly, and McCarthy is concerned that pumping a cell full of calcium could make it behave strangely perhaps forcing it to produce transmitters, or even preventing it from releasing anything. McCarthy hoped to unpick the role of glia in the brain using his genetically engineered mice. \"We thought this would definitively show gliotransmission,\" he says. Instead, it totally crushed expectations. The team, led by McCarthy's postdoc, Cendra Agulhon, studied two mouse lines: one in which calcium signalling in astrocytes had been given a boost, and another in which it had been completely obliterated. But neither change made any difference to how nearby neurons were going about their business. McCarthy and his group were forced to conclude that astrocytes couldn't possibly be releasing chemicals to signal to neurons. They published their results in  Science   in March this year. At the meeting in Amsterdam, McCarthy discusses his group's ongoing search for a behavioural effect that they could attribute to their wonky astrocytes. Again, no dice. \"I would love to be able to show you that, but I can't,\" McCarthy says, amid murmurs of disbelief from the audience. But some researchers have problems with his model. \"The way he does it bothers me,\" says Richard Robitaille, a glia biologist at the University of Montreal in Canada. Measuring the effect of astrocytes will require more subtle experimental approaches, he says. Haydon agrees that it would be better to use more sensitive methods. He pictures astrocyte responses to stimuli as a narrow bell curve: if you blast them with something, they do nothing; if you don't stimulate them at all, they do nothing. But 'talk' to them using a low level of physiologically relevant stimulation and they should talk back. In McCarthy's mice, says Haydon, taking out the activity of all astrocytes probably changes so much during brain development, and throughout life that it is impossible to tell what the normal job of glia might be and how the brain compensates for their absence. The mice are thus subject to the same 'atom bomb' criticism levelled at cell-culture studies. Part of the problem could be cultural. \"The entire field has been trained in neurocentric labs, and everybody has so far believed that astrocytes work like neurons,\" says Maiken Nedergaard, a glial biologist at the University of Rochester in New York. \"But astrocytes function totally differently. They use a different language. They use a different way of getting input and output.\" They may also work on a totally different timescale from neurons, says Rusakov. Their responses, he says, can be three orders of magnitude slower. As a consequence, common techniques for measuring neuronal responses won't work on astrocytes. Methods for imaging calcium in cells aren't good at measuring slow fluctuations or increases in the outer reaches of astrocytes, partly because calcium dyes simply don't penetrate there. At his lab at the University of California, Los Angeles, Baljit Khakh has been developing a technique that can detect calcium in the previously inaccessible branches of astrocytes. His team took an existing protein that is known to sense calcium, and modified it so that it could be targeted to cell membranes, where researchers suspect much of the calcium signalling might be going on. His work shows that a calcium increase in the main body of the astrocyte didn't necessarily cause a corresponding rise in calcium at the borders. If glia are indeed releasing transmitters, methods such as this should allow researchers to examine exactly where they are releasing them from. Robitaille aims to find out whether astrocytes can detect very low levels of synaptic transmission a neuron passing just one electrical signal, or action potential, to the next. He and a colleague, Aude Panatier, used a high-speed imaging technique called line-scan imaging to measure calcium in astrocyte branches, and activated a small number of neurons using a very weak electrical pulse. Their results, not yet published, suggest that astrocytes can indeed pick up these low levels of activity and, what's more, they can regulate transmission themselves by releasing chemicals such as the energy-transfer molecules adenosine and ATP. Robitaille and his team have come down in favour of gliotransmission. \"Based on our data, we have to sit on one side of the fence,\" he says. Studies in different areas of the brain could also be contributing to the dramatically varying results. \"Our knowledge about astrocytes is so poor that people tend to generalize findings in different circuits, different brain areas,\" says Volterra. \"That also should be looked at.\" Even the time of day at which brains are prepared for experiments could be affecting measurements, says Haydon, because certain chemicals wax and wane over time. \"I have a student go in and cut slices every four hours,\" he says at the conference, to a ripple of sympathetic chuckling from the audience.  \n                The killer experiment \n              Methods aside, there doesn't seem to be a consensus as to what experiment would resolve this dilemma once and for all. \"There's no clear simple experiment, otherwise I'd do it,\" says Attwell. \"So would most of the others.\" Most researchers in the field say that standardizing their methods and the ways in which they interpret their results could help. But the bigger personalities don't shy away from making ambitious statements. \"Lots of neuron-centric people would look at our  Science   paper and say, 'That's the killer experiment,'\" says McCarthy after his talk. Later the same day, Haydon is gearing up to present some new results from his experiments on transgenic mice. When does he think the argument will be resolved? \"Today,\" he says, \"at 4.15.\" Haydon's team works with mice with a different deficit to those that McCarthy is studying, although the two men despite obvious differences of opinion do share mouse resources and expertise. Haydon's mice have higher than normal levels of an enzyme called IP 3  phosphatase, which acts to prevent astrocytes releasing calcium. But the impairment in the mice that Haydon uses is more precise than that in McCarthy's animals, in terms of both timing and location. The calcium-blocking enzyme is expressed only after the mice are weaned, and only in a brain region called the hippocampus. Haydon shows the assembled audience evidence that calcium signalling decreases in mice that have had their astrocytes disabled in this way, and that synaptic transmission in the hippocampus is also affected implying that astrocytes help synaptic transmission to take place, under the right conditions. The work suggests that with heavy stimulation, synaptic transmission isn't changed; likewise, with a very weak stimulus, the astrocytes make no contribution. But there is a 'window of opportunity', at a medium level of stimulation, in which the effects are visible. This is music to the ears of many glia enthusiasts. Even McCarthy is pleased to hear about positive results. \"I would love to see gliotransmission,\" he says. But the evidence both for and against it still needs to be verified. With different groups approaching the question in different ways, and with the best-practice methods and techniques yet to be standardized and agreed on, it will take time before researchers can replicate each other's results and bring some clarity to the field. McCarthy for one will keep searching for a behavioural effect in his mice with altered astrocytes, but is wary of letting what he wants to see taint his results. \"If you get a vested interest in the answer, then you're in trouble,\" he says. Nonetheless, \"If we do see it, we're going to be shouting it from the rooftops.\" \n                     Insight on glia \n                   \n                     Ken McCarthy \n                   \n                     David Attwell \n                   \n                     Phil Haydon \n                   Reprints and Permissions"},
{"file_id": "468365a", "url": "https://www.nature.com/articles/468365a", "year": 2010, "authors": [{"name": "Erik Vance"}], "parsed_as_year": "2006_or_before", "body": "Sleep researcher Sara Mednick has straddled the line between media darling and respected scientist. But why is there still a line at all? Sara Mednick was flying high in January 2007. She was doing a television appearance a day, every day, for a month. And she was being featured on radio shows around the United States, repeating talking points from her just-released book,  Take a Nap! Change Your Life . Dozens of businesses were calling for her expertise and endorsements, including the Silicon Valley juggernaut, Google, which requested a 'napping strategy' for its employees. By all appearances, Mednick had joined a class of scientists that spans academia and popular culture with aplomb. But it wasn't easy. \"It's such a crazy experience where you are in a different city every day, and you're working these ridiculous hours to do these daybreak TV shows,\" she says. She was baffled by the experience, and a little flattered. \"There was a part of me that was wondering, could I still do my work and try to also be this next big thing?\" It is a question being asked by a rising number of scientists, as the 24-hour news cycle and proliferation of media outlets and blogs have made achieving 15 minutes, or more, of fame easier than ever. Polls suggest that the scientific community want a better portrayal of science in the media, but are unsure whether they should be the ones to provide it. A 2009 study by the Pew Research Center in Washington DC found that 85% of scientists see the public's lack of scientific understanding as a major problem, and most were unimpressed with the traditional media coverage of the subject. Still, a poll by  Nature   earlier this year suggests that many researchers think that their institutions put little emphasis on press exposure and that it shouldn't be a major factor when determining career advancement (see  go.nature.com/em7auj ). That is a tide that is changing, says Stephen Hinshaw, a psychology department chair at the University of California, Berkeley. What might have been seen by previous generations as garish or vain is quickly becoming another part of a scientist's workday. \"Years ago, somebody who was media savvy would have been viewed pejoratively as too slick. Today, it could well be an advantage, given fundraising, appealing to donors and appealing to a wide audience to make psychological science relevant. All of those are good things.\" But as Mednick's story shows, celebrity science is not all good. She has had an impact on people outside the tight-knit circle of her scientific peers and enjoyed the celebrity status. But she still wants to be seen as a serious scientist in traditional academia. She has found that scientific celebrity needs to be maintained, rarely pays and can have unintended consequences on one's professional reputation. Mednick conducts her research at a sleep laboratory at the University of California, San Diego. \"We need to be really quiet,\" she says, gently closing the door to her office. \"Someone is napping in the next room.\" The lab consists of hotel-like rooms for napping, plus rooms for researchers to monitor sleeping subjects \u2014 quietly. Despite being there for five years, keeping quiet still seems to be a struggle for Mednick, who has piercing blue eyes and an eruptive laugh. Within a few minutes, she seems to have forgotten the person sleeping in the next room and is animatedly describing her work. Colleagues refer to Mednick as one of the world's leading experts on naps. Her work looks at various types of sleep and its effect on human cognitive and motor skills. She and her colleagues have shown, for example, that 60- and 90-minute naps can improve performance as much as a full night's sleep on several visual-perception tasks ( S. Mednick  et al. Nature Neurosci.    6,   697\u2013698; 2003 ). Mednick's fascination with naps started in the late 1990s when she was a psychology PhD student at Harvard University in Cambridge, Massachusetts, studying visual memory in patients with schizophrenia. But after hearing lectures by sleep expert Robert Stickgold, she decided she wanted a new direction. She started working with Stickgold at Harvard, and later landed a postdoc position at the Salk Institute in San Diego, California, in 2003. In the competitive academic atmosphere at Salk, colleagues expected her to write as many papers as possible and then go on to a tenure-track position. Instead, Mednick spent her final postdoc year writing a book on napping for the public. Her publisher, Workman Publishing, is a New York company that prints titles such as  The Cake Mix Doctor ,  How to Satisfy a Woman Every Time   and  The Betty White Wall Calendar . \"'What the hell are you doing?' That's what all my scientific friends were saying,\" she says. \"'This is not helping you get tenure.'\" Mednick says that she wrote the book, together with co-author Mark Ehrman, because she wanted her research to reach people. \"It was such an obvious book to write,\" she says. \"I just like the idea of having my research being real world.\" She concedes that vanity and the hope for a pay cheque were a small part of the motivation. Ultimately, however, Mednick seems driven by a desire to overturn conventions. A former actress, Mednick marches to her own drumbeat, say friends and colleagues. The book definitely got her noticed \u2014 leading to the whirlwind of media attention in 2007 (see  'Spreading the science of napping to the masses' ). Widespread preoccupation with sleep science has fostered a bustling book market. Amazon.com carries more than 750 titles under the headings 'sleep' and 'medicine'. Only a minority of these have been written by scientists with experience in sleep research (about one-third of the 30 top-selling authors have advanced degrees). Many of the rest are written by self-help gurus, yoga teachers and even pastors. So the media jumped at the chance to talk to Mednick: a bona fide scientist with evidence that midday naps were beneficial. Despite some 150 media appearances and countless interviews, however, Mednick's book only netted her about US$30,000, which barely covered her advance. She says that Google did not pay her for the consulting work she did. A Google representative said the company could not provide details of the arrangement. The only corporate money she received was from the Dutch company MetroNaps, which markets a futuristic napping 'pod' for snoozing at work. Mednick says she made $10,000\u2013$15,000 designing sleep survey questions for the company's website, and to this day has been unable to convince them to remove her picture. \"It was before I really knew what I was doing,\" she says. \"I allowed them to use my picture and my name. I suddenly realized that that wasn't at all what I wanted to be affiliated with.\" Back in her lab, Mednick goes into the monitoring room, fretting for a moment that the noise in her office has disturbed the subject. Her current study is examining the benefits of short bursts of rapid-eye-movement (REM) sleep, so she needs the nappers to sleep well. According to an electroencephalography readout \u2014 which records the electrical activity of the brain \u2014 this individual has had a fitful nap. Much of Mednick's research, as well as her book, looks at the best nap length and the best time of day to take one. To illustrate this, she and Ehrman have designed a 'nap wheel' to help people to visualize their sleep schedule. But nap wheels don't exactly further one's career. Mednick has won grant money for her research but is still looking for a tenure-track position. \"She is taking a risk,\" says James Maas, creator of several educational documentaries on sleep and author of the  New York Times   bestselling book  Powersleep   (Harper, 1998). \"I would have advised her to wait until she had tenure,\" says Maas. He says that few academics would openly criticize such behaviour but that it can affect scientists more subtly, tarnishing them in the eyes of funders, for example, who question the dedication to daytime TV shows rather than the lab (for more on the rewards and potential pitfalls of media engagement, see  page 465 ). Stickgold says that Mednick's public persona has undoubtedly affected her career, but in ways that are hard to spot \u2014 a missed grant opportunity or a keynote address being offered to someone else, for example. Mednick can't point to specific instances in which this has happened. She does lament the fact that she has not managed to publish in either of the field's primary journals,  Sleep   and the  Journal of Sleep Research , even though she has published in higher-impact mainstream journals. David Dinges, editor-in-chief of  Sleep , says that Mednick is \"a respected scientist who has done interesting work\", but that 75% of all submitted manuscripts are rejected. Mednick doesn't blame the journal, but is concerned that her outside activities could hinder her progress. Even so, she claims to have no regrets about her book or media presence. She continues to make television appearances and write for the popular press. And she advises younger colleagues to do the same. Mednick is still deciding where she belongs. But every step in the direction of celebrity has to be negotiated carefully. In late August, Mednick got a call from the popular talk show,  Dr. Phil , known for high-drama confrontations. The talk-show producers said they loved her book and were interested in making a show about sleep. In the end, however, they decided to avoid what they called 'the scientific route', instead opting for someone to interpret the dreams of women who think their partners might be unfaithful. \"Probably for the best,\" says Mednick. \n                 See Careers  \n                 p.465 \n               \n                     Videos featuring Sara Mednick on different US news programmes \n                   \n                     Sara Mednick \n                   Reprints and Permissions"},
{"file_id": "468492a", "url": "https://www.nature.com/articles/468492a", "year": 2010, "authors": [{"name": "Lizzie Buchen"}], "parsed_as_year": "2006_or_before", "body": "What can microbiologists who study human bowels learn from those who study the bowels of Earth? Jillian Banfield trades in hell holes. In September, she could be found wading through the dark, hot, sulphurous innards of Richmond Mine at Iron Mountain, California, where blue stalactites ooze the most acidic water ever discovered, with a pH of \u22123.6. A year before that, she was pumping up a toxic soup of uranium, arsenic, molybdenum and other metals from underneath a decommissioned nuclear-processing site in Rifle, Colorado. From both sites she took samples back to her lab at the University of California, Berkeley, where she sequenced and analysed the DNA they contain in an attempt to work out which bacteria, archaea, viruses and fungi have decided to make that particular hell their home \u2014 and what it takes to survive there. About a year ago, Banfield added a new location to her repertoire of foul study sites: the pencil-thin intestines coiled inside premature infants weighing less than 1.5 kilograms, in the neonatal intensive care unit of the University of Chicago, Illinois. Banfield had never dealt with microbes that live in humans. But her well-regarded work on the microbial communities of Richmond Mine had attracted the attention of two medical researchers. One was Michael Morowitz, a neonatal surgeon then at the University of Chicago, and now at the University of Pittsburgh, Pennsylvania. Morowitz was studying necrotizing enterocolitis (NEC), a potentially fatal disease that destroys the bowels of premature babies. The other was David Relman of Stanford University in California, a leader in the burgeoning field exploring the human microbiota \u2014 the vast populations of microorganisms that live in and on the human body. Morowitz and Relman asked Banfield if she could help them understand the microbial mass in this unexplored landscape. Banfield said yes \u2014 and the three struck up a collaboration. They are now bringing Banfield's techniques to bear on humans, and are sequencing and analysing microbial genes in fine detail to resolve whether hard-to-distinguish species or strains correlate with NEC and might promote it. Elsewhere, similar collaborations are linking those exploring the human microbiota in the intestine, skin, mouth and other surfaces with microbial ecologists, such as Banfield, who have already made a career out of studying microbial universes in environments such as soil, ocean water and toxic waste sites. The human microbiologists need the help. Although work by Relman and many others over the past five years has gone a long way to building up a genetic catalogue of human microbiota \u2014 what types of microbes live where \u2014 it has also revealed its staggering and previously unappreciated complexity. With hundreds of interacting, coevolving species living in and on every individual, and frustratingly little species overlap between each person's microbial population, understanding the connection between microbes and health seems more daunting than ever. Researchers want to know what role the body's microbial inhabitants have in immune function, nutrition, drug metabolism and conditions as diverse as obesity, cancer, autism and multiple sclerosis. But to do so, they have to sort through an avalanche of genetic sequence to find out what microbes are in the community, how they change over the course of a day, a lifetime or after a change in diet, and which functions are served by particular microbes, combinations of microbes or microbial metabolites (see  'Exploring the superorganism' ). Microbial ecologists are supplying some of the expertise and bio-informatic tools to help make sense of the data mountain. They are also bringing to the human microbial field ecological principles such as colonization, succession, resilience to change, and competition and cooperation between community members. \"It's hard not to think about ecology when you enter the field,\" says Jeff Gordon, a leader in gut microbiology at Washington University in St Louis, Missouri. In return, specialists in human microbiology are attracting funding and attention that ecologists have sometimes struggled to find. \"The arbitrary and false barriers between environmental and medical microbiology are breaking down,\" Gordon says.  \n                Microbial deluge \n              An infant's first exposure to microbes is at birth, as it slides out of the sterile womb, slurping up and smearing itself with its mother's vaginal fluids and faeces. From then on, life is one long microbial onslaught. New bacteria, viruses and fungi colonize every exposed organ \u2014 skin, eyes, lungs, gastrointestinal (GI) tract. But until the past decade, scientists' ability to explore these microbes en masse was hindered by historical, cultural and technological obstacles. Ever since Robert Koch advanced the germ theory of disease in the late nineteenth century, clinical microbiologists have been fixated on foreign pathogens such as  Salmonella , Ebolavirus and  Yersinia pestis   \u2014 identifying them, growing them in isolation and determining their causal relationship with disease. Everything else living on or in the body was often dismissed as pretty inconsequential when it came to human health. Microbial ecologists have had a different perspective. Around the time Koch was growing his first pure cultures of  Bacillus anthracis   and  Mycobacterium tuberculosis , the environmental field was beginning to recognize what became known as \"the great plate count anomaly\". When analysing a sample such as a drop of pond water, they saw a dramatic discrepancy between the vast number of microbial cells they could count under a microscope and the tiny number that would grow after plating the same sample. The research community, intent on studying natural ecosystems in their entirety, made efforts to work out what all the unculturables are. In 1985, a team of microbiologists published a technique that could census bacteria and archaea in a sample by sequencing the 16S ribosomal RNA gene, which is different for every species 1 . These types of genetic survey quickly became routine in the environmental microbiology field. Banfield began using them in 1995, soon after beginning her studies on the acid mine drainage at Iron Mountain. It took longer for the clinical microbiology community to take notice. In 1999, Relman published one of the first such surveys of microbes on and in the human body, comparing a 16S survey of the plaque scraped off a healthy man's teeth with those he could grow on a culture plate 2 . The culture-based methods were missing a large proportion of species, he found, and many of those species had never been characterized. Skip forward a few years, and techniques such as these have painted a far more sophisticated picture of human microbes as ubiquitous, abundant and indispensable, harvesting energy and nutrients from food, synthesizing essential amino acids, moisturizing the skin and playing an essential part in immune-system development. \"The microbiota,\" as Gordon puts it, \"are bringing a series of utensils to the dining-room table that the human host doesn't have.\" The picture bandied around nowadays is of humans and their microbial partners as a coevolved 'superorganism' in which each provides services for the other. (Gordon even employs a cultural anthropologist, who is exploring how a person's view of the self changes when they find out that, in sheer numbers of cells, they are 99% microbial.) \"There's been a real recognition that we need to include the whole human microbiota to really understand how we function as an organism,\" says Lita Proctor, a project director at the US National Institutes of Health in Bethesda, Maryland, which launched the 5-year, US$115-million Human Microbiome Project (HMP) in late 2007. That project, dedicated to sequencing a colossal portion of the human microbiome, is one of several with similar goals formed around the world and joined under the umbrella of the International Human Microbiome Consortium. In March, one major initiative called MetaHIT, involving the European Union and China, published a catalogue of the microbial genomes strained from the faeces of 124 people, finding more than 1,000 prevalent bacterial species across the group \u2014 and 3.3 million different microbial genes 3  (that's 150 times the number of human genes). A few months later, HMP scientists completed the sequence and analysis of 178 microbial genomes from different regions of the body and discovered genes coding for more than 30,000 different proteins 4 . \"We're talking about something that's a hundred times bigger [than the human genome] that we don't have a handle on, that's intimately tied to our own health and vitality,\" says Proctor.  \n                Unusual suspects \n              Relman became familiar with Banfield and her work in the early 2000s, when she had already gained recognition for describing the dynamic structure of the acid mine drainage community and its metabolic processes. Banfield and her team developed techniques to work out, from a mass of fragmented microbial DNA sequences, which species and strains they come from. By contrast, 16S techniques and most automated bioinformatic analyses struggle to reveal differences between closely related strains, or to expose subtle genetic rearrangements. Banfield's team first assembled fragments from the dozen or so dominant species into full genomes. Then they undertook fine-grained analyses: manually inspecting points where the DNA fragments didn't quite line up to identify closely related strains of the dominant species, and developing software to tease out the identity, metabolism and function of these and much rarer species. This helped the group work out in great detail how members of the community function and change over time 5 . And the distribution of genes also revealed the biogeochemical processes that the community was performing. Relman wanted to analyse human microbiomes with the same level of detail. \"It was reasonably straightforward to see that this kind of approach would really be important,\" he says. But Banfield's approaches had been used to dissect only simple communities, such as the handful of species that can thrive in the hellish Richmond Mine, where the most abundant species make up some 40% of the cells in the community. The adult gut typically contains hundreds of microbial strains or species, and the most abundant species represent only about 4%. \"It just always seemed like her system was simple enough to make it [this type of analysis] possible, whereas mine was just hopelessly not. It was worse by at least an order of magnitude,\" says Relman. Then, in January 2009, Morowitz stepped in. \"He just called me out of the blue one day,\" Banfield recalls, and spoke to her about NEC. \"He was really concerned about this terrible disease. I was sort of taken by how passionate he was, and hoped I'd be able to do something that might be useful.\" NEC strikes about 7% of severely premature infants, but often clinicians cannot diagnose the disease to begin treatment until the symptoms, such as a ballooning belly and blood-soaked stools, are in full swing. In severe cases, surgeons must remove part or all of the intestine, and many babies die. The medical community had been searching for a causative pathogen for decades, without success. \"I had a suspicion that we weren't going to find a [pathogenic] smoking gun, so I was looking around for different ways to study the problem,\" Morowitz says. \"I became interested in people who were looking at entire communities of organisms. That's what got me reading Jill's papers.\" Banfield, Morowitz and Relman realized that the preterm infant could be the perfect human testing ground for Banfield's techniques. Each tiny neonate harbours only a dozen or so species \u2014 comparable to the acid mine drainage \u2014 and the intensive care unit creates a sterile, tightly controlled environment in which to study them. \n               Click here for larger image \n               The trio's work has focused on healthy babies so far. Morowitz takes faeces samples almost daily from birth, isolates DNA, sends it to a high-throughput sequencing centre, then passes the data and clinical information to Banfield and Relman. Relman analyses the 16S sequences at every time point to get a census of the species and their abundance; Banfield then selects a few time points for more extensive sequencing and genome analysis to identify the species, strains and genes present. The team's first results, which will be published later this year, show that the techniques work: studying the first three weeks of life in a premature infant born at 28 weeks of gestation, they were able to track the rapidly changing microbial community with strain-level detail. They distinguished, for example, the changing abundance of two strains of  Citrobacter \u2014 a species commonly found in the gut that has also been implicated in neonatal meningitis \u2014 the sequences of which are more than 99% identical. The eventual aim is to find out whether a particular strain or community structure correlates with NEC, which could help predict which babies might develop it and even give a clue to preventing it. More broadly, the research team hopes that the simple model community provided by the neonatal gut will help them to explore the extent to which ecological principles apply to the human microbial system. \"The scientific questions are really cross-cutting,\" says Banfield. One example, she says, is colonization \u2014 which organisms arrive first and how the community evolves (see  'Baby's first bacteria' ). \"It's ecological succession,\" Banfield says. \"If you look at the surface of a pool of acid mine drainage and imagine the first organisms to arrive, it's the same as imagining a newborn baby with a sterile GI tract, and the first organisms there.\"  \n                Antibiotic aftermath \n              Other collaborations are also exploring how human microbial ecosystems adjust during illness, shifts in diet or after antibiotics. \"They're probably changing all the time in response to all sorts of perturbations,\" says Claire Fraser-Liggett, a microbiologist at the University of Maryland School of Medicine in Baltimore, who, in collaboration with Janet Jansson, a soil microbiologist at the University of California, Berkeley, is studying microbiomes associated with the intestinal disorder Crohn's disease in identical Swedish twins. \"Are these communities resilient enough to rebound to where they were before a perturbation like antibiotics? What should we be measuring in order to answer that question? What's going on in the recovery period? It leads to all these questions that ecologists have been dealing with for decades.\" Ecological concepts are also helping to account for the substantial differences that most studies have found between the microbiota of individuals \u2014 even, to a lesser extent, between identical twins. Ecology offered a likely explanation in the form of redundancy. The idea now is that every person's microbes provide a core set of genes or biological functions, regardless of the specific species encoding them 6 . \"If you look at grasslands in different parts of the planet, there's a common morphology and function,\" says Gordon, drawing parallels. \"But in different locales, the component species are quite distinct.\" Gordon and other researchers hope that more extensive sequencing and analysis of many individuals' microbiomes will reveal what those core functions are. Relman, meanwhile, has become interested in finding 'keystone species', rare species that nevertheless have a vital role in a community, and he is working with a colleague at Stanford, bioengineer Stephen Quake, to sequence the genomes of single microbial cells from the gut. Yet the sheer density, diversity and complexity of the human microbiota places it in a different league from other microbial communities. \"We're getting so much data,\" sighs Jansson. \"Billions of sequences, tens of thousands of proteins and metabolites. We have data overload.\" The constant communication that goes on between human cells and their microbial inhabitants adds a whole extra layer of complexity. Elsewhere, the environment is more or less inert. \"Complexity's a big challenge,\" says Relman. \"We're not, by any means, there yet.\" \"It's a lot less clear with the Human Microbiome Project what the finish line will be,\" says Fraser-Liggett. \"For the Human Genome Project, it was to create high-quality draft sequence for one human genome. Here, given the complexity of these communities, we're not so sanguine that we think we can yet define an endpoint.\" With so much diversity, \"it seems like a black hole that may go on forever, which makes some of the funding agencies cringe\", she says. All this means that clinical application remains a distant prospect. Even if researchers find a convincing association between, say, a particular microbial species and a disease, they lack the tools to manipulate the microbial communities with any specificity to eliminate one species or insert another. Antibiotics tend to kill swathes of microbes, not individual species, and the inability to culture most species means that it is impossible to grow and transplant them. (There has been limited success transplanting entire globs of faeces.) Even if species-specific transplants were possible, there is no guarantee that the newcomer would survive. So although ecology is providing a framework with which to understand the human microbiota, when it comes to applying these ideas to the clinic, old-fashioned pathogen-centric microbiology is leagues ahead. And that, says Morowitz, doesn't look like it's changing any time soon. \"If you walk into any hospital and pull aside someone in a white coat and ask them about microbiology, most of what they know has to do with organisms that can be isolated when they send a blood sample down to the culture lab.\" Relman, a clinician himself by training, says this focus is understandable. \"They're sucked towards pathogens, and they have practical questions to deal with in the clinical workplace. They're not in need of more diversity.\" But that has to change, he says. \"We have to get away from this monolithic, one-dimensional perspective of a one-bug-one-disease picture of health,\" Relman says. \"The community is the unit of study.\" \n                     Jill Banfield \n                   \n                     David Relman \n                   \n                     Jeff Gordon \n                   \n                     Human Microbiome Project \n                   Reprints and Permissions"},
{"file_id": "468496a", "url": "https://www.nature.com/articles/468496a", "year": 2010, "authors": [{"name": "Jo Marchant"}], "parsed_as_year": "2006_or_before", "body": "The ancient Greeks' vision of a geometrical Universe seemed to come out of nowhere. Could their ideas have come from the internal gearing of an ancient mechanism? Two thousand years ago, a Greek mechanic set out to build a machine that would model the workings of the known Universe. The result was a complex clockwork mechanism that displayed the motions of the Sun, Moon and planets on precisely marked dials. By turning a handle, the creator could watch his tiny celestial bodies trace their undulating paths through the sky. The mechanic's name is now lost. But his machine, dubbed the Antikythera mechanism, is by far the most technologically sophisticated artefact that survives from antiquity. Since a reconstruction of the device hit the headlines in 2006, it has revolutionized ideas about the technology of the ancient world, and has captured the public imagination as the apparent pinnacle of Greek scientific achievement. Now, however, scientists delving into the astronomical theories encoded in this quintessentially Greek device have concluded that they are not Greek at all, but Babylonian \u2014 an empire predating this era by centuries. This finding is forcing historians to rethink a crucial period in the development of astronomy. It may well be that geared devices such as the Antikythera mechanism did not model the Greeks' geometric view of the cosmos after all. They inspired it. The remains of the Antikythera mechanism were salvaged from a shipwreck in 1901 (see  'Celestial mirror from the deep' ) and are now held in the National Archaeological Museum in Athens. A series of ever more sophisticated radiographic studies of the gearwheels hidden inside the corroded mass culminated in proposed reconstructions of the device from a team led by astronomer Mike Edmunds of the University of Cardiff, UK, in 2006 (ref.  1 ), and from London-based mechanic and curator Michael Wright in 2007 (ref.  2 ). The device, which dates from the second or early first century BC, was enclosed in a wooden box roughly 30 centimetres high by 20 centimetres wide, contained more than 30 bronze gearwheels and was covered with Greek inscriptions. On the front was a large circular dial with two concentric scales. One, inscribed with names of the months, was divided into the 365 days of the year; the other, divided into 360 degrees, was marked with the 12 signs of the zodiac. Pointers moving around this dial were thought to show the date as well as the corresponding position of the Sun, Moon and probably the five planets known at the time. A revolving ball, painted half black and half silver, displayed the phase of the Moon, and letters marked on the zodiac scale acted as a kind of index, linking to inscriptions that described the appearances and disappearances of major stars at different times of the year. On the back of the device were two spiral dials, one above the other. The top one showed a repeating 235-month calendar, popular because after 235 months or 19 years, the distribution of new Moons in the solar year is the same. The bottom spiral represented a 223-month repeating eclipse cycle. Symbols inscribed on its month divisions told the user when to expect eclipses, and gave information about the type and timing of each event. The researchers who did the 2006 reconstruction noted that the 235- and 223-month cycles were originally derived by the Babylonians. That was to be expected: the empire's priest-astronomers, who saw astronomical events as powerful omens, had identified many such cycles over the centuries, and Greek astronomers of this period often made use of their results. But this fact did not shake the researchers' central conclusion that the device embodied the Greeks' own geometrical models of the cosmos. These models, based on spheres or circles that described the motion of the planets in three-dimensional space, had originally been qualitative and philosophically pleasing rather than accurate. But by the time of the Antikythera mechanism's construction, scholars such as Hipparchus, who worked in Rhodes in the second century BC, had been inspired by the Babylonians' precision to put numbers into the Greek models, and to insist that they fit with actual observations. Modern experts were confident that the device's zodiac display \u2014 its centrepiece \u2014 reflected such state-of-the-art geometrical theories. Supporting this idea were X-ray scans revealing that a mechanism hidden within the device's clockwork directly modelled the varying motion of the Moon. Because the Moon's orbit around Earth is elliptical rather than circular, it seems to travel faster at some points in its orbit than others. Greek philosophers believed that all heavenly orbits were perfect circles, so Hipparchus explained this variation in the Moon's motion by superimposing one circular orbit onto another that had a different centre \u2014 the 'eccentric' theory.  \n                Missing gears \n              The gearwork in the Antikythera mechanism seems to put this into practice perfectly, using a pin-and-slot mechanism that enabled one gearwheel to drive another around a slightly displaced axis. In their 2006  Nature   paper 1 , Edmunds and his colleagues described it as \"a mechanical realization\" of Hipparchus' lunar theory. The team and other scholars assumed that the maker of the Antikythera mechanism must have used similar techniques to model the path of the Sun and probably the planets, too. The relevant gearing is missing, but the assumption is plausible: Greek astronomers accounted for the movement of planets \u2014 which not only seem to speed up and slow down in the sky, but sometimes change direction \u2014 using a theory that was mathematically equivalent to the eccentric model. The basic idea, which would be refined and made famous by the Greek astronomer Ptolemy in the second century AD, was that each planet travelled in a small circle called an epicycle, whose centre was simultaneously moving in a larger loop around Earth. To demonstrate how the Antikythera mechanism could have operated, Wright built a working model of it. His device incorporates small gear wheels riding on larger ones to model the epicycles of Mercury, Venus, Mars, Jupiter and Saturn, as well as the varying speed of the Sun. The Antikythera mechanism thus seemed to be a stunning demonstration of how the ancient Greeks had translated their most famous astronomical theory into physical wheels of bronze. But now a new team has noticed a detail that could turn this view of the mechanism on its head. Historian of astronomy James Evans at the University of Puget Sound in Tacoma, Washington, and his colleagues knew that the 360 divisions on the zodiac scale should be spaced slightly farther apart than the 365 divisions on the calendar scale that encircled it. But when they used X-ray scans provided by Edmunds' team to precisely measure the division widths on the surviving part of the dial, which encompasses 88 degrees, they found that the zodiac marks are actually closer together 3 . The marks on the vanished parts of the scale must have compensated somehow with a wider spacing. The researchers believe that this was done on purpose to represent the Sun's uneven progress through the sky. Instead of the device using epicyclic gearing to drive a pointer with varying speed as previously thought, Evans believes it is \"extremely likely\" that its maker used a pointer moving with constant speed around a circle split into two sections of equal overall size that were divided differently: a 'fast zone' in which the degree markings were closer together than normal, and a 'slow zone' in which they were farther apart. This scheme is identical to a theory of the Sun's movement used by the Babylonians, known as System A. If correct, this interpretation suggests that the astronomy encoded in the mechanism's gearwork does not represent state-of-the-art Greek theories after all. It is Babylonian through and through. \n               Click here for larger image \n               This is a tough assertion to prove. The uneven division of the zodiac scale could have been just the result of sloppy work by the machine's creator, and its similarity to the Babylonian scheme just a coincidence. Wright, who was the first to suggest that epicyclic gearing modelled the motions of the Sun and planets, says he is \"very uncomfortable\" with the idea that the device modelled the Moon's motion mechanically, yet used an abstract numerical scheme to do the same for the Sun. But astronomy historian Alexander Jones of the Institute for the Study of the Ancient World in New York is taking the hypothesis seriously. He argues that Greek astronomers were more interested in convenience than consistency. Such an intimate mix of geometric and arithmetic approaches fits the spirit of the period, he says. \"They were playing with different toolboxes at the same time.\"  \n                Key events \n              Evans's hypothesis forces a rethink of other parts of the mechanism, too. Previously, scholars assumed that the positions of the Sun, Moon and planets were all displayed around the same zodiac scale. But if the zodiac scale had been tweaked to accommodate the varying speed of the Sun, it would no longer be accurate for showing the positions of the other bodies. Evans thinks that the five planets were instead displayed on individual smaller dials (see  'Cross-cultural computer' ). He adds that these dials didn't necessarily have to show the planets' positions in the sky. He thinks the machine's maker would have been more interested in showing the timing of key events in each planet's cycle, such as changes of direction, or first and last appearance in the night sky. If so, the pointers on these dials could have been driven at constant speed by simple gear trains representing period relations derived by the Babylonians \u2014 no epicycles required. Jones is more cautious about this suggestion, although he says it \"makes sense in terms of how planetary motion was talked about at the time\". Both he and Evans are hoping that more clues will come from inscriptions on the front cover of the mechanism. The surviving lettering is hidden inside the mechanism's battered and corroded remains, but it is being painstakingly reconstructed and translated from X-ray scans by Agamemnon Tselikas, director of the Centre for History and Palaeography in Athens, and Yanis Bitsakis, a physicist at the University of Athens. So far the two researchers have deciphered mentions of Mars, Mercury and Venus, along with several references to the 'stationary points' at which planets seem to change direction. Evans argues that even the clearly epicyclic gearing of the Moon display may model Babylonian arithmetic, not Greek geometry. The amplitude of the variation encoded by the pin-and-slot mechanism is larger than that used by Hipparchus in his eccentric model, he points out, and is closer to the amplitude used in the lunar algorithms of the Babylonians. \"Perhaps a mechanic tried to represent the variations in the Moon's speed according to the Babylonian theory using gears,\" he says \u2014 and hit upon an epicyclic arrangement. In other words, epicycles were not a philosophical innovation but a mechanical one. Once Greek astronomers realized how well epicyclic gearing in devices such as the Antikythera mechanism replicated the cyclic variations of celestial bodies, they could have incorporated the concept into their own geometrical models of the cosmos. \"It is a new possibility,\" says Jones. \"I am quite attracted to it.\" There is little evidence for who came up with the idea of epicycles, although it is often ascribed to third-century-BC Greek geometer Apollonius of Perga. Intriguingly, gears and epicycles seem to have arisen at about the same time, with gears perhaps a little earlier. Also in the third century BC, Archimedes used simple gears to change the size of an applied force. Some two centuries later, the Roman politician and author Cicero wrote that Archimedes built a bronze astronomical device that might have been similar to the Antikythera mechanism. \"Maybe we need to rethink the connection between mechanics and astronomy,\" says Evans. \"People think of it as purely one way, but maybe there was more of an interplay.\" In other words, when that Greek mechanic shaped the Antikythera mechanism's complex gear trains, he created more than a model made of bronze. He helped to forge a view of the Universe that would hold sway for nearly 2,000 years. Jo Marchant  is a London-based writer and author of   Decoding the Heavens,  a book about the Antikythera mechanism . \n                     The Antikythera mechanism \n                   \n                     The Antikythera device \n                   \n                     The Antikythera Mechanism Research Project \n                   \n                     Michael Wright's work on the Antikythera mechanism \n                   \n                     Image library of Antikythera radiographs \n                   Reprints and Permissions"},
{"file_id": "468362a", "url": "https://www.nature.com/articles/468362a", "year": 2010, "authors": [{"name": "David Adam"}], "parsed_as_year": "2006_or_before", "body": "The release of climate-science e-mails last November ripped apart Phil Jones's life. He's now trying to patch it back together. \"I like to think the worst is over, but it's coming up to the first anniversary and it's something I'll always remember at this time of year, when the nights close in. This is the time it happened.\" Twelve months ago, Phil Jones was a productive, if not particularly outspoken, climate scientist. That was the way he liked it. Head of the Climatic Research Unit (CRU) at the University of East Anglia (UEA), UK, Jones worked with the Met Office to compile data from weather stations around the world into a monthly series showing global average temperature. He had much on his mind \u2014 not least a puzzling drop in North Atlantic sea surface temperatures during the mid-twentieth century that he had recently helped to discover. It was a curious finding, but Jones would soon have bigger things to ponder. On 19 November 2009, someone released roughly 1,000 e-mail messages and documents stolen from a server at the CRU. Many of them contained Jones's private correspondence, which sometimes showed him in an unflattering light.  He gloated about the death of a prominent climate sceptic, and suggested to colleagues they should delete e-mails to keep sceptics from gaining access to information. Most famously, he boasted that he had used a \"trick\" to \"hide the decline\" in a temperature chart. Very soon, members of the sceptic community had pounced on these messages as evidence that Jones and others had concealed flaws in their temperature data and abused the peer-review system to gag critics of climate researchers. Jones faced a storm of accusations that ranged from scientific misconduct to plans to install an autocratic world government through the spread of false hysteria about global warming. He received some 200 abusive or threatening e-mails, the most troubling of which targeted him and his family. \"Someone, somewhere, will hunt you down,\" read one. \"You are now blacklisted,\" read another. \"Expect us at your door to say hello.\"  The e-mails also triggered several official investigations, including one by the UK Parliament, which ultimately determined that Jones had not committed any serious offences. Case closed.  Not for Jones, who still faces attacks from critics and is trying to cope with unwanted memories as the anniversary approaches (see  'A career by degrees' ). Never comfortable with the media, Jones has given few interviews since the controversy began. But as part of an attempt to put the past year behind him, he agreed to show  Nature   around the CRU earlier this month and to talk at length about his experience. He proved largely unrepentant.  Aged 58, Jones looks far better than during the darkest days of last winter, when he was spiralling downhill and even contemplated suicide. Colleagues were stunned by his decline. Jones was never an extrovert, but he withdrew further and his mental collapse was mirrored by a rapid loss of weight. In March, when a frail and hesitant Jones answered questions before an investigating parliamentary committee, his appearance reminded many of the distressing 2003 case of David Kelly. Kelly was the UK weapons inspector outed as the source of a media story about government exaggeration of Iraqi weapons of mass destruction. He was also questioned by a parliamentary committee \u2014 and subsequently killed himself. \"I made the connection,\" Jones says about the Kelly case. \"But I didn't talk about it.\" Jones has regained much of the lost weight, and he no longer takes the medications that kept him calm during the day and asleep at night. He is back in charge of the CRU (he stood aside for some eight months while enquiries were pending). So, how have events of the past 12 months changed him?  \"I'm a little more guarded about what I say in e-mails now,\" he says. \"One thing in particular I'm doing is not responding so quickly. I might have got an e-mail in the past and responded with an instant thought in the next 10 to 15 minutes, whereas now I might leave it a day.\" Jones admitted in the parliamentary inquiry to sending some \"awful e-mails\", but defends the right of scientists to express themselves in what they consider personal communications. \"People would be saying much the same things at scientific meetings and discussed [them] over dinner. But in an e-mail, it is recorded. People have probably forgotten what you said after a night out.\" Although other scientists were quick to defend the reality of man-made global warming, public support for Jones was harder to find. Officially, senior figures in the UK science establishment say this was because they did not want to prejudice ongoing enquiries. Privately, they say that the e-mails looked bad, and should the CRU scientists have been found guilty of misconduct, they did not want to get dragged down with them.  \"I was getting lots of messages of support from my fellow scientists,\" Jones says. \"And I did wonder why they didn't go to the media and say the same things they were saying to me.\"  The CRU server that held the stolen information was seized long ago as evidence from the cluttered desk where it sat in one of the unit's cramped offices. The unit itself is housed in a curious four-storey cylindrical tower at the heart of the busy UEA campus, and it brings to mind a Norman keep within a medieval castle. An appropriate analogy, considering that its occupants have weathered an extended siege that left visible scars on the tower's exterior. Its doorbell was removed to shield the scientists inside from the incessant ringing of journalists and film crews.  Outsiders are often surprised at how small the unit is, with just three full-time staff scientists. Jones's office is on the top floor, where the computer on which he typed many of the e-mails sits amid a carpet of scientific reports and papers. Keith Briffa, a tree-ring specialist, has an office across the landing. Climate researcher Tim Osborn is next door, struggling with a familiar problem. \"My inbox is full and I need to delete some e-mails.\" Then, with a thin smile: \"But I'm not allowed to now, am I?\" Temperature data analysed by these researchers serve as the foundation for countless studies, which have steadily identified and analysed the signal of global warming caused by human activities. The growing importance of this work made Jones and other CRU scientists a target for Internet bloggers sceptical of their methods and the conclusions drawn from them. Long before the e-mail scandal, Jones and his team found themselves fielding enquiries about their research from outside the conventional scientific community. An independent inquiry headed by former senior civil servant Alastair Muir Russell examined many aspects of the work done at the CRU, looking specifically to see if the centre had committed fraud or some other type of scientific misbehaviour. The investigation found no reason to doubt the honesty and integrity of the CRU scientists, but it did criticize the way those scientists responded to information requests, or in some cases, failed to respond. The report said there had been a \"consistent pattern of failing to display the proper degree of openness\". Some scientists echo these conclusions. Mike Hulme, a climate researcher at the UEA who worked at the CRU from 1988 to 2000, said that certain aspects of the culture in the research unit were \"unwise and unhealthy\". He notes in particular that the CRU was slow and inconsistent in responding to data requests, and says it suffered from \"intense tribalism\". But Hulme says the work at the CRU \"was not fraudulent, and certainly did not justify the personalization of the attacks subsequently made on them\". In his defence, Jones says he wrestled with how open scientists should be to requests for information. \"I started responding to those back in 2003 and 2004, but they just asked more and more questions and it was just a drain on resources. That's when things probably went awry.\" He claims he changed tack when he saw that the information he supplied was not used by those who demanded it. Rather, each response simply triggered more questions. \"I just realized it was taking up too much time,\" he says.  By failing to answer all requests properly, Jones says he wasn't acting any differently from other researchers. \"There are some people I have sent requests to, other scientists, who have never replied. I've asked people for data and reprints of papers and I've never got a response. So I think I responded quite well and the CRU responded quite well.\" Jones complains frequently about distractions from his research. \"The amount of time we get to do research just seems to be less and less, and you see things that take away that research time, or you find yourself working at weekends or in the evenings to the annoyance of your family.\" Autumn is a \"bad time\" because his teaching load increases. He got frustrated with meetings with university officials to discuss freedom of information requests because \"it takes away your research time\". And he rarely agrees to peer review scientific papers. \"If you start doing lots of reviews, you find that your quality research time also goes.\" When he did review papers, the stolen e-mails revealed, he told colleagues he \"went to town\" to make sure that those manuscripts he did not like were not published. The Muir Russell report found there was no abuse of peer review and said such robust exchanges were typical in science. Jones says he learned long ago that he needed to be absolutely clear with editors, because in the past he had written what he thought were critical reviews only to see the papers in question get published. \"I realized that to make sure an editor rejects a paper you have to go a bit stronger in the review.\" He adds: \"The whole point about trying to pervert the peer-review process is that it is impossible to do it. There are so many journals and if people are persistent enough, they can get their papers published.\"  Another allegation was over his use of data from weather stations in China for a 1990 paper on the impact of urbanization on temperature. The paper 1 , published in  Nature , stated that data were used from stations where there had been few, if any, changes in instrumentation, location or observation times. When critics later uncovered the fact that many of the stations had moved, they cried fraud; earlier this year, Jones said in a separate interview with  Nature 2  that he was considering a correction. He now says such a step is unnecessary and that he stands by the claims in the paper. He was on medication during the previous interview, he says, and felt under pressure then to publicly concede that he had made mistakes. He says the description of weather-station movement \"has been completely misinterpreted\". The set of 84 Chinese stations referred to in the paper were drawn from a larger group of 265, for which the Chinese had location histories. Jones and his colleagues did not claim that none of the selected stations had moved, only that they picked out ones that had moved the least, he says. Such shifts do not significantly affect results, Jones says, because there was no general pattern to the station relocation: on average, ones moving to colder places were balanced by ones moving to warmer spots. But the Chinese scientist who supplied the station information has now retired and the authorities there have not released the full station-history data \u2014 making it impossible for Jones, he says, to provide the evidence to support the statement.  One issue critics continue to badger Jones about is whether he deleted e-mails that had been requested through the freedom of information process. Jones insists he never did, as that would have qualified as an offence. What about deleting e-mails that could be requested by future freedom of information requests? Britain's Information Commissioner's Office, which adjudicates such cases, says it is allowed. However, the Muir Russell report said that this kind of pre-emptive deletion is not consistent with the \"spirit and intent\" of the law, and there is evidence that CRU scientists took that questionable approach. When Jones is now asked if he deleted such messages, he says: \"No, I deleted e-mails as a matter of course just to keep them under control.\" So why did he urge colleagues to delete messages in which they discussed, among other things, the preparation of a report for the Intergovernmental Panel on Climate Change? An attempt to thwart critics, perhaps? \"That was probably just bravado at the time,\" he says. \"We just thought if they're going to ask for more, we might as well not have them.\" Then Muir Russell was correct? Had Jones broken the spirit of the law? \"Not necessarily, if you've deleted them ahead of time,\" he says. \"You can't second guess what's going to be requested.\" Jones goes back and forth on his motivations. Deleting e-mails would simplify his life if people requested them in the future, but that was not why he got rid of them, he says. \"I deleted them based on their dates. It was to keep the e-mails under control,\" he repeats. A source close to the CRU says it is almost impossible to determine who deleted what and when \u2014 much less why. More certain is the conclusion that the hack of the server was a sophisticated attack. Although the police and the university say only that the investigation is continuing,  Nature   understands that evidence has emerged effectively ruling out a leak from inside the CRU, as some have claimed. And other climate-research organizations are believed to have told police that their systems survived hack attempts at the same time. Jones and others connected to the CRU fear the hackers may be sitting on more stolen e-mails, but Jones feels confident the worst is behind him. \"It really is not somewhere I would like to go through again. But having been through it once, I think I am a bit hardened to it.\" Can Jones offer any advice to research scientists who wake up one morning to find themselves the centre of a worldwide scientific scandal? \"I don't know that I can. The thing to point out is that whatever you try to do, the goalposts keep moving.\" As soon as he responded to one criticism, another popped up. Jones has steadily begun to piece together his professional as well as his personal life. The discovery of the sudden Atlantic cooling was recently published in  Nature 3  and he has started to attend conferences again. He agrees to pose for photographs outside the CRU building, gazing at the blue sky. Then he shuffles back into the relative calm of his unit: one scientist who now realizes his castle walls cannot completely shield him from the outside world. \n                     Nature Reports Climate Change \n                   \n                     Nature Geoscience \n                   \n                     Climatic Research Unit \n                   \n                     The Muir Russell report \n                   Reprints and Permissions"},
{"file_id": "468624a", "url": "https://www.nature.com/articles/468624a", "year": 2010, "authors": [{"name": "Colin Macilwain"}], "parsed_as_year": "2006_or_before", "body": "Is a vast undersea grid bringing wind-generated electricity from the North Sea to Europe a feasible proposition or an overpriced fantasy? North Sea energy used to mean oil and gas. Today, production of both is waning, and the rough weather that challenged the drillers has itself become a resource. In a speech last September, Alex Salmond, Scotland's first minister, estimated that the winds and waves lashing the Scottish coast could generate seven times more energy than Scotland consumes. Other countries around the North Sea hold similar potential. The problem is getting all that power from the windy edge of Europe to its populous, energy-hungry heart \u2014 the region roughly bounded by London, Berlin and Milan. \"What we need above all is an efficient transmission system,\" Salmond says. \"And the most efficient one would be a grid built across the North Sea.\" On 3 December, ten northern European nations are expected to sign a memorandum of understanding spelling out how they'll build an undersea electricity 'supergrid'. The project is a major engineering and political challenge, comparable in scope, scale and ambition to the rush for oil and gas in the same waters 40 years ago. Thousands of kilometres of undersea cable would be laid, at a cost of at least \u20ac1 million (US$1.4 million) per kilometre. Unlike onshore grids, which operate on alternating current (a.c.), the subsea grid would use direct current (d.c.) and would therefore require new types of offshore and onshore substations, control systems, converters and circuit breakers in a set of projects costing billions of dollars (see  'Wiring up Europe' ). The whole project has an estimated \u20ac20-billion price tag. An even more ambitious project, called Desertec, is planned to bestride the Mediterranean Sea and North Africa, pumping electricity generated by wind and solar power from the Sahara to Europe's cities. And a group of US investors led by Google released plans in October for an undersea grid in the North Atlantic that would ship power from offshore wind farms to the eastern seaboard of the United States. But the North Sea supergrid is closest of the three to becoming reality. \n               Click here for larger image \n               Momentum for the project comes from two main sources. A 2003 European directive, updated last year, demands that European Union (EU) states open up their electricity markets to competition with each other, which will require stronger connections between their national grids. And the EU has pledged to cut carbon emissions by 20% from 1990 levels by 2020, which will require a 35% cut in emissions from electricity generation and a vast expansion of renewables. \"Without these grids, there will be no meeting of emissions targets in Europe,\" says Georg Adamowitsch, the EU coordinator for offshore grids in northern Europe. Wind energy is already a mainstay of clean power generation in Europe, with 74 gigawatts of capacity installed so far, and another 136GW anticipated by 2020, according to projections released by the European Commission (EC) in August. (By comparison, just 14GW of new nuclear generating capacity is likely to be added by then.) Analysts expect much of this capacity to be installed offshore, because it is windier and easier to get planning permission. The need to connect up those offshore farms \u2014 and future wave- and tidal-power farms \u2014 to the mainland is the first reason that a North Sea grid is inevitable, analysts say. The second is that it would permit the large-scale storage of electricity in the only type of 'battery' so far developed for that purpose: pumped-storage hydroelectric dams, mostly located in Norway. Wind and other renewable energy sources are intermittent, but by using the energy to pump water uphill and recapturing power as the water flows down again, these dams can store electricity at more than 85% efficiency, evening out fluctuations in supply. The attractiveness of such storage helped to spur the completion in 2009 of a 'point-to-point' high-voltage direct-current (HVDC) link between Norway and the Netherlands, which allows surplus power from the low-lying Netherlands to be stored in the Norwegian fjords, and brought back when needed. But on their own, such links cannot tap into offshore power sources, and cannot integrate the multiple electricity markets bordering the North Sea: only an undersea grid would do that. Last December, nine EU nations (the United Kingdom, Ireland, Sweden, Denmark, France, Germany, the Netherlands, Belgium and Luxembourg), joined later by Norway, agreed to start an initiative to get such a grid built, resulting in this week's memorandum. At the same time, the EC is supporting researchers who are looking in detail at the costs and benefits of different grid configurations \u2014 and at the technical challenges of taking a power grid offshore.  \n                Edison rules \n              A crank called Thomas Edison once expected that most electricity would move around as d.c. But almost all transmission has turned out to use a.c. instead, chiefly because it can easily be transformed from high-voltage transmission lines down to the safe 120 volts or the somewhat less safe 240 volts in the home. It is also easy to isolate parts of an a.c. grid, to deal with faults and do routine maintenance, using massive mechanical circuit breakers that slam open just as the sine wave of the alternating current hits zero. Alternating current is no good for underground or subsea transmission over more than about 80 kilometres, however, because of heavy reactive losses which arise when the aluminium or copper conductor is buried. In effect, the cable and the surrounding earth form a capacitor, draining power from the a.c. lines, and rendering them useless over long distances. So a subsea grid has to be d.c. \u2014 posing a challenge for electrical engineers who lack the technological tools they have developed for a.c. power. \"There's no such thing currently as circuit breakers for high-voltage d.c.,\" says Paul Neilson, transmission development manager at Scottish and Southern Energy in Perth, UK. \"If there was a fault in the grid, all the energy would pour straight to it, a bit like decompression in an aeroplane. You need to be able to isolate it, automatically, in milliseconds.\"  \n                Breaking the circuit \n              Electrical engineers in industry and academia are addressing this and other challenges through a three-year \u20ac60-million programme called TWENTIES, a consortium of 26 academic and industrial partners supported by the EC. One TWENTIES project, led by Energinet, an agency of the Danish Climate and Energy Ministry, is seeking to design a control system that would react when storms approach. Electrical grids are designed to cope with some degree of perturbation \u2014 but a storm could make it necessary to rapidly shut down a whole cluster of wind farms. \"This may develop into a system security problem, if we don't improve the present storm control algorithms,\" says Poul S\u00f8rensen, an electrical engineer and project partner at the Ris\u00f8 National Laboratory in Roskilde. \"One of the solutions we're looking at is to control the turbines more, and ramp them down slowly.\" Another TWENTIES project, led by transmission company RTE in France, will study the optimal configuration for a d.c. grid and test a prototype d.c. circuit breaker. Major electrical-engineering suppliers, including ABB, based in Zurich, Switzerland, and Siemens, based in Erlangen, Germany, are developing such circuit breakers, although they are not revealing details of their designs. Dragan Jovcic, an electrical engineer at the University of Aberdeen, says that existing approaches are unlikely to yield appropriate d.c. circuit breakers, being either too slow in responding to faults, or \"very high cost\". Jovcic has developed and patented a new type of d.c.\u2013d.c. converter, which involves a set of inductors and capacitors linked in a resonant circuit to step up d.c. voltage. This type of converter also doubles as a d.c. circuit breaker and, says Jovcic, could weigh five times less than some other designs that rely on conversion to a.c. and back again, because it lacks the heavy iron core transformers. Extra weight is expensive because the connection points will be mounted on platforms offshore, for maintenance access (see  D. Jovcic and B. T. Ooi  IEEE Trans. Power Deliv.    25,   2535\u20132543; 2010 ). In October, Jovcic won an award from the European Research Council to design new models for high-voltage d.c. converters. These have to work on microsecond timescales, rather than the milli-second timescales at which a.c. oscillates. The new model will also be able to deal with the complicated configurations in a substation that connects four or five high-voltage d.c. lines together. But solving the technical problems will only go part-way to getting a North Sea supergrid built. The capital costs of laying grids offshore are immense. A report published in July by the EU-funded research project OffshoreGrid, based in Brussels (see  http://go.nature.com/cssy3s ), envisages, for example, that \u20ac32 billion will be invested in offshore interconnectors in northern Europe by 2020 and a further \u20ac58 billion by 2030, if wind farms are connected up individually. It suggests that \u20ac15 billion could be shaved from this if wind farms were clustered. On top of this, the opening up of electricity markets will require wholesale legal and regulatory change: at present, for example, generating companies that receive subsidies for feeding renewable energy into a German grid receive nothing if they supply power elsewhere. Not all European countries are equally enthusiastic about the North Sea supergrid. The United Kingdom has embraced the project because it needs offshore wind capacity to meet its carbon-emissions targets. Ireland, Norway and Scotland are especially keen, because they want to build new industries that manufacture and service offshore wind and wave farms. But despite their stated intention to sign the memorandum of understanding, the French and German governments have been lukewarm, admit grid advocates, with Germany pushing instead for Desertec, which is led by German companies. The North Sea supergrid is technically more radical than this and other proposals, and could prove almost as politically taxing \u2014 despite the theoretical commitment of EU states to get it built. And however much high-level planning goes on, the supergrid's evolution is likely to be messy, much like that of a national highway system. \"Things will happen incrementally,\" says Neilson. \"It's not practical to roll out a pre-designed grid like a roll of linoleum.\" \n                 See Editorial  \n                 p.599 \n               \n                     Energy collection \n                   \n                     Freinds of Supergrid \n                   \n                     OffshoreGrid consortium \n                   \n                     Desertec Foundation \n                   Reprints and Permissions"},
{"file_id": "468154a", "url": "https://www.nature.com/articles/468154a", "year": 2010, "authors": [{"name": "David Dobbs"}], "parsed_as_year": "2006_or_before", "body": "Schizophrenia appears during adolescence. But where does one begin and the other end? Rachel had just given birth to her third child when she became overwhelmed by the noise on the obstetrics ward, grew sharply paranoid about her sister, and in short order descended into her first schizophrenic episode. She was 28. Although it was only then that she started hearing voices \u2014 those of her family, distant screams, messages from spaceships \u2014 she and her psychiatrist came to see that there had been whisperings of this long before. As a child \u2014 bright, but awkward both socially and physically \u2014 Rachel tended to keep to herself. She crammed her drawings full of the sort of elaborate fractal detail often seen in the work of psychotic artists. In her teenage years, some of her difficulties worsened. Acutely sensitive to noise, she was aware of the refrigerator cycling off and on, footfalls from the apartment next door, the traffic outside. Only in retrospect did any of these peculiarities seem ominous. As Rachel's psychiatrist Robert Freedman explains in his book  The Madness Within Us , where he writes about Rachel, that's how it is with the early flickers of paranoia, confusion, hypersensitivity and hallucination in people who develop schizophrenia. They often emerge exactly when adolescence is throwing the body and brain for a loop, and years before the disease manifests itself fully. \"The problem with early symptoms,\" says Freedman, who is chair of psychiatry at the University of Colorado, Denver, and editor-in-chief of the  American Journal of Psychiatry , \"is that they're not very specific. At a time when thinking, emotion and behaviour change a lot anyway, these early indicators are very hard to separate from normality.\" Even so, this overlap of schizophrenia's early signs with the hallmarks of adolescence has made this period a beacon to researchers. Over the past 20 years, studies have shown that the adolescent brain undergoes major developmental changes. Autopsy and imaging studies, for instance, have revealed that during childhood and adolescence the brain routinely prunes away vast numbers of synapses \u2014 the junctions between neurons across which electrical signals flow \u2014 and that this pruning seems to go on longer and farther in people with schizophrenia. Other work has shown that adolescence brings major upgrades to the neural networks that generate powers of judgement, cognition and behavioural control \u2014 building new circuits, remodelling old ones and discarding some altogether. The idea that schizophrenia arises from miscues or shoddy work in this complicated and delicate project has sparked a huge variety of research. Many basic neuroscientists are trying to work out what goes wrong on genetic, cellular, circuit and systems levels. Meanwhile, at the level of diagnostic practice, some researchers argue that subtle symptoms can not only be distinguished from normal adolescence, but can provide a reliable indicator of future disease. In a pattern all too familiar to students of schizophrenia, none of these efforts has revealed the secret of this fiendishly complex disorder. One leading researcher, David Lewis, at the University of Pittsburgh's Western Psychiatric Institute and Clinic in Pennsylvania, has spent the past two decades exploring schizophrenia's developmental roots. Yet even Lewis says it's still too soon to know whether any given line of study, no matter how promising, is homing in on the schizophrenia puzzle's most essential component, if such a thing exists. \"It's more like getting a much better picture of one part of the elephant,\" he says, referring to the old parable of blind men collectively describing an elephant's nature by individually feeling its different parts. \"I think it's working. When I talk with other researchers working other ideas, I'm encouraged that I'm onto something important, and even more encouraged that we all seem to be feeling our way around the same animal.\"  \n                Teen brain \n              By many accounts, Lewis is running one of the more comprehensive and sustained attempts to explore normal and pre-schizophrenic adolescent brains. He is taking what neurologist and depression expert Helen Mayberg at Emory University School of Medicine in Atlanta, Georgia, describes as \"one of the smartest, most creative and most promising angles I know of on schizophrenia\". \n               Click here for larger image \n               Lewis has focused on a particular circuit in the dorsolateral prefrontal cortex (DLPFC), a multilayered region that is crucial to tying the threads of experience, memory, thought and emotion into a coherent, consistent view of the world (see  'Cellular culprits in schizophrenia' ). The DLPFC builds and refines much of its complicated circuitry during childhood and adolescence, responding to both genes and experience. Much of Lewis's work examines the relationship, during this developmental period, between two types of cell: pyramidal neurons, which span several layers of the cortex; and chandelier cells, which sit near the base of the pyramidal cells. His concentration on them is one of those stubborn scientific projects that seems to produce little until, suddenly, it produces a lot. Pyramidal cells, so called because their central bodies are triangular, generate much of the complex electrical signalling that takes place in the DLPFC and in the prefrontal cortex as a whole. Their effectiveness in this task depends heavily on the richness of branching in their long, tree-like forms. Multiple postmortem studies, including some by Lewis, show that in adults with schizophrenia, these pyramidal cells have smaller cell bodies and fewer of the protrusions called dendritic spines that receive input from synapses 1 . In the pyramidal cells in layer 3 of the DLPFC, which communicates extensively with other cortical regions and is key to powers of working memory that often falter in schizophrenia, Lewis found the dendritic spines reduced in number by about a quarter. Most developmental neuroscientists suspected that this sparse branching resulted from aberrant synaptic pruning during adolescence. Pruning is a sort of clean-up job conventionally thought to eliminate weak synapses and leave strong ones. In schizophrenia, it was suspected, the pruning process hacked away indiscriminately, knocking out strong synapses along with weak ones. In 2008, Lewis's team found evidence that argued against this idea in the brains of normal monkeys, whose PFCs develop along timelines comparable to those of humans 2 . The group found that the great majority of synapses in the layer 3 pyramidal neurons were functionally mature before pruning started. This left few immature or weak synapses to trim away. If this holds with healthy humans as well, Lewis argued, then schizophrenia cannot arise from a failure to select and prune away weak synapses, for there is not much to select. This and other discoveries led Lewis to offer an alternative hypothesis: that pyramidal cells in individuals with schizophrenia had weaker synapses before pruning ever began 1 , 3 . \"We have to draw in dotted lines here,\" says Lewis. \"But our suspicion is that early on, when someone destined for schizophrenia has an excess of synapses, the quality of the synapses doesn't matter so much, and the person does okay. Then later, when the pruning starts, the problems slowly become apparent, because they've lost their reserves.\" Lewis wanted to trace the story further back than that. What might be stunting the development of pyramidal cells in the first place? He suspected chandelier cells. Over the years, Lewis's group and others have built an increasingly strong case against the ornately branched chandelier cells, which seemed to communicate only with pyramidal cells. Lewis's autopsy work, for instance, had found that chandelier cells took huge hits in schizophrenia 4 , with some proteins in their synapses reduced by about 40% \u2014 a sign that these junctions were not working normally. For many years, everyone assumed that chandelier cells only had an inhibitory role: when 'talking' to pyramidal neurons, they said only 'calm down'. Then, about five years ago, a team led by G\u00e1bor Tam\u00e1s at the University of Szeged in Hungary found that chandelier cells also have an excitatory role, sometimes shouting 'fire up' 5 . Groups led by Tam\u00e1s, Lewis and others have all done work showing that the chandelier cells seem to be key contributors to the complex activity of their pyramidal neighbours. Last year, for instance, Karl Deisseroth's lab at Stanford University in California used sophisticated genetic techniques on mice to turn on and off a class of neurons that includes chandelier cells \u2014 and found that this started and stopped much of the animals' organized PFC activity 6 . Lewis thinks that in people who later develop schizophrenia, chandelier cells fail at some crucial task of cultivating pyramidal cells during childhood or early adolescence. Together they do not generate the organized neural traffic required for building robust connections; and later, the weakness of both cell types leaves the PFC incapable of creating the vigorous, coordinated firing \u2014 including a type of electrical activity called gamma synchrony \u2014 that generates working memory. The result, subtle but increasingly apparent as synapses are pruned during adolescence, is a brain that can't consistently organize either its electrical activity or its thoughts: the shattered mind of schizophrenia. Tom Insel, the director of the National Institute for Mental Health in Bethesda, Maryland, says that Lewis's model of this chain of dysfunction \"provides something this field really needed: a framework for linking observations at the molecular, cellular and systems levels. We haven't had a story that crossed those levels of explanation before. And his story, whether it pans out in all its details or not, is invaluable for doing that.\" While still thinking big, Lewis is filling in some of those details. Chandelier cells, for instance, signal using GABA, an inhibitory neurotransmitter. Lewis is trying to figure out whether errant GABA dynamics in schizophrenia are a cause of chandelier cell dysfunction or a compensation for it. He recently found that aiming an experimental drug at certain GABA receptors in chandelier and 'basket' cells (another kind of interneuron) boosted schizophrenia patients' gamma synchrony \u2014 and showed signs of improving their working memory. He's now trying to decide whether to aim another drug at those receptors or search for a different lever to pull \u2014 something that would reveal more cleanly the links among GABA, chandelier cells and the dysfunctions of schizophrenia. \"You constantly have to balance this kind of close work with the big picture,\" he says. For Lewis, the balancing act means tracking and responding to other lines of research, such as the work of clinical psychiatrist Anissa Abi-Dargham at Columbia University, New York. Abi-Dargham is using brain-imaging tools to explore whether flaws in evolutionarily older, subcortical areas that use the neurotransmitter dopamine are driving developmental problems in PFC circuits, or whether the problems in the PFC alter dopamine function. Lewis considers these connections between research programmes another sign that schizophrenia study has advanced in the last decade or so. \"Used to be,\" he says, \"we just got tired of a hypothesis or hit a dead end and went onto something else. Now we're actually integrating hypotheses or testing one against another. \"Or if you want to put it another way,\" he says, laughing, \"we're getting a bit more synchrony in these findings. The communication between the blind men is improving. The elephant is starting to come together.\"  \n                Running ahead \n             \n               Click here for larger image \n               The focus on adolescent brain development that has been so valuable in research generates controversy among clinicians, however. Particularly contentious is the idea of clustering schizophrenia's early whisperings into a diagnosable 'prodrome' period during adolescence. (The term comes from a Latin word meaning 'running ahead'.) The disease is typically diagnosed in young adulthood (see  'Accent on youth' ). The North American Prodrome Longitudinal Study, or NAPLS, is an eight-centre project formed in 2003 that has been testing ways to reliably diagnose people in such a prodrome stage and treat them with psychotherapy, cognitive training, family therapy or drugs in the hope of forestalling worsening problems. NAPLS's main diagnostic tool is a questionnaire it calls the Structured Interview for Prodromal Syndrome. It scores symptoms such as fragmented or unusual thoughts; family histories of psychosis; social or school troubles; and paranoia or other peculiarities of emotion, behaviour or thinking. In 2008, the group reported on 291 adolescents and young adults that the questionnaire identified as being at 'very high risk' of developing schizophrenia or other psychotic disorders 7 . Within 2.5 years of screening, 35% suffered psychotic episodes. The NAPLS researchers say that a set of prediction algorithms derived from those results afterwards, and then run again on the same data, raises the screen's predictive accuracy to almost 80% \u2014 comparable, they say, to risk predictions for medical problems such as heart disease. The team says that this and other \"strong evidence for the prodromal risk syndrome \u2026 raises the question of its evaluation for inclusion in  Diagnostic and Statistical Manual of Mental Disorders   (Fifth Edition)\" or  DSM   \u2014 a move that would lead to wide use of such screens and interventions. This gravely concerns some clinicians, who point to the unavoidable false positives. \"You will inevitably tell people who are not really at risk of schizophrenia that they are 'at very high risk',\" says Til Wykes, a King's College London clinical psychologist who specializes in mental health (see also  page 165 ). The impact could go far beyond inappropriate use of antipsychotic drugs, she says. It could negatively affect how families, friends and the broader community treat that person, as well as their self-conception. \"The anxiety this produces may even generate just the thing you're trying to protect against. And you're doing this to people who are what \u2014 15, 16? This is a huge intervention to take with someone who may not be destined for schizophrenia.\" Others feel that the benefits of improved treatment outweigh the risks of overdiagnosis. \"I frankly don't understand this concern about diagnosing a prodromal period, and I find this concern about overtreatment misplaced,\" says William McFarlane, a psychiatrist who runs a prodromal diagnosis programme at the Maine Medical Center in Portland. McFarlane argues that inclusion in the  DSM   would bring consistency of diagnosis and treatment to more people. Many of the scientists untangling schizophrenia's complex developmental threads don't believe they yet have the tools to reliably discern a prodromal period \u2014 let alone treat it. \"We have better ideas about what's going on than we did in the past,\" says Lewis, \"but we do not yet have a target, or an intervention for that matter, that has a high enough likelihood of success.\" Freedman thinks the state of knowledge requires caution and humility. \"Schizophrenia research is full of people who are sure they know what they're doing, and only later do we understand that the whole paradigm was off. Then we look back in amazement at how wrong they had it. I like to think everyone in my generation would be well aware of this history, and be reluctant to say we're there.\" \n                     Schizophrenia special \n                   \n                     Neuropsychiatric disease Insight \n                   \n                     National Institute for Mental Health \n                   \n                     University of Pittsburgh Translational Neuroscience Program \n                   \n                     David Lewis lab \n                   Reprints and Permissions"},
{"file_id": "468752a", "url": "https://www.nature.com/articles/468752a", "year": 2010, "authors": [{"name": "Nicola Jones"}], "parsed_as_year": "2006_or_before", "body": "Researchers are sure that they can put lab-grown meat on the menu \u2014 if they can just get cultured muscle cells to bulk up. Mark Post has never been tempted to taste the 'fake' pork that he grows in his lab. As far as he knows, the only person who has swallowed a strip of the pale, limp muscle tissue is a Russian TV journalist who visited the lab this year to film its work. \"He just took it with tweezers out of the culture dish and stuffed it in his mouth before I could say anything,\" says Post. \"He said it was chewy and tasteless.\" Post, who works at the Eindhoven University of Technology in the Netherlands, is at the leading edge of efforts to make  in vitro   meat by growing animal muscle cells in a dish. His ultimate goal is to help rid the world of the wasteful production of farm animals for food by helping to develop life-like steaks. In the near term, he hopes to make a single palatable sausage of ground pork, showcased next to the living pig that donated its starter cells \u2014 if he can secure funds for his research. Post started out as a tissue engineer interested in turning stem cells into human muscle for use in reconstructive surgery, but switched to meat a few years ago. \"I realized this could have much greater impact than any of the medical work I'd been doing over 20 years \u2014 in terms of environmental benefits, health benefits, benefits against world starvation,\" he says. Largely because of the inefficiency of growing crops to feed livestock, a vegetarian diet requires only 35% as much water and 40% as much energy as that of a meat-eater 1 . Future 'in-vitrotarians' should be able to claim similar savings. The prospect of an alternative to slaughtering animals led People for the Ethical Treatment of Animals based in Norfolk, Virginia, to announce two years ago a US$1-million prize for the first company to bring synthetic chicken meat to stores in at least six US states by 2016. In the Netherlands, where the vast majority of work has been done so far, a consortium of researchers convinced the government to grant them \u00a32 million (US$2.6 million) between 2005 and 2009 for developing  in vitro   meat. Such incentives have helped to solve some of the basic challenges, applying human tissue-engineering techniques to isolate adult stem cells from muscle, amplify them in culture and fuse them into centimetre-long strips. But far more money and momentum will be needed to make  in vitro   meat efficient to produce, cheap and supermarket-friendly (see  graphic ). Post estimates that creating his single sausage will require another year of research and at least $250,000. So what still needs to be done?  \n                Choose the right stock \n              The first question for researchers is which cells to start with. Embryonic stem cells would provide an immortal (and therefore cheap) stock from which to grow endless supplies of meat. But attempts to produce embryonic stem cells from farm animals have not been successful. Most work so far has been on myosatellite cells, the adult stem cells that are responsible for muscle growth and repair. These can be obtained by a relatively harmless muscle biopsy from a pig, cow, sheep, chicken or turkey; the desired cells are then extracted using enzymes or pipetting, and multiplied in culture. \n               boxed-text \n             Morris Benjaminson, professor emeritus at Touro College in New York, prefers a different approach \u2014 planting the whole biopsy in a dish. \"We use the whole business without the brain,\" he says. \"We don't break it down to cells and put them back together again.\" He used this method to grow goldfish fillets in his lab in 2002, boosting the surface area by up to 79% over a week by adding a shot of extra cells from ground-up muscle 2 . It's not clear, however, whether this procedure could produce enough muscle for a commercial enterprise. The fundamental problem is that myosatellite cells will only divide dozens of times, probably because their telomeres \u2014 the protective ends of the chromosomes \u2014 wear down with age. There are ways of boosting their proliferation. One is to add a gene for the repair enzyme telomerase. Another, being investigated by the start-up company Mokshagundam Biotechnologies in Palo Alto, California, involves inserting a tumour-growth-promoting gene. But genetically modified lab-grown meat might be too much for consumers to swallow. \"Try selling that,\" laughs Post. An alternative is to get cells from a young animal and perfect the rest of the system, such as the culture medium, to maximize growth. For now, Post uses regular cell-culture medium to grow his pork myosatellite cells. This contains fetal calf serum which, as it currently comes from dead cows, largely defeats the point of synthetic meat. It also contains antibiotics and anti-fungal agents that might not be good for human consumption. \"Supposedly you could be allergic to these; you never know,\" Post says. To get the cells to differentiate into muscle, he shifts to horse serum, which has the same list of problems. Animal-free media made from a slurry of plants or microbes are commercially available for biomedical work such as  in vitro   fertilization. But like animal-based media, they're expensive \u2014 right now, growth media account for about 90% of the material costs of lab-grown meat. And their composition is proprietary, making them difficult to customize. One alternative might be to use ground-up maitake mushrooms, which Benjaminson found works just as well as calf serum for his fish fillets. At the University of Amsterdam, researchers working on  in vitro   meat have been developing a cheap medium made from blue-green algae, with added growth factors made in genetically modified  Escherichia coli . But no one has yet developed a way of making a cheap, animal-free growth serum in large quantities.  \n                Beef it up \n              Myosatellite cells grown on a scaffold will fuse into myofibres, which then bundle together to make up muscle. But lab-assembled muscles are weak and textureless. \"It's like when you take off a cast after six weeks,\" says Post. To get the muscle to bulk up with protein requires exercise. Assembling the myofibres between anchor points helps, as this creates a natural tension for the muscle to flex against. Post uses this type of arrangement to boost the protein content of a muscle strip from 100 milligrams to about 800 milligrams over a few weeks. He also administers 10-volt shocks every second, which can bump protein content up to about a gram. This much electricity would be expensive in a scaled-up industrial process, so his group is hoping to learn how to mimic chemical signals that tell muscles to contract. Vladimir Mironov of the Medical University of South Carolina in Charleston is instead using a scaffold made of chitosan microbeads \u2014 chitosan can be sourced from crabs or fungi \u2014 that expand and contract with temperature swings, thus making a natural fitness centre for his muscle strips. If lab-grown muscle gets more than about 200 micrometres thick, cells in the interior start to die as they become starved of nutrients and oxygen. Post simply grows many small strips that could be ground up into a sausage. Others, including Mironov, are using blender-sized bioreactors of the type developed by NASA to study muscle growth in low gravity. These conditions help prevent cell clumping and improve transport of oxygen and nutrients. Growing meat on an industrial scale would require large, customized bioreactors like those used by biopharmaceutical companies. Mironov estimates that a commercial  in vitro   meat facility would need a five-storey building of bioreactors; with a similarly huge investment. And all that is just for manufacturing ground meat. The prospect of growing steaks is a much bigger challenge, requiring a system of fake 'blood vessels' built into the meat. That is decades away.  \n                Market it \n              The thing that enthusiasts for fake meat talk least about is its taste, perhaps because they haven't tried it. In the United States, researchers have largely avoided eating anything grown in the lab for fear of violating a Food and Drug Administration regulation (it's unclear whether it is actually forbidden) or being seen as publicity hounds. When Benjaminson grew his goldfish fillets, his team dipped them in olive oil, fried them in breadcrumbs and gave them to an 'odour and sight' panel who said they seemed edible, but who weren't allowed to try them. Researchers generally believe that if they can get the texture right, taste will follow \u2014 particularly once flavouring is added. Fortunately, myosatellite cells can also turn into fat, which would add to the taste. At Mokshagundam Biotechnologies, the goal is to make a spam-like mix of different muscle and other cell types that provide the 'umami' taste that characterizes meat. Scientists will also have to find a way of adding nutrients such as iron (which comes from blood) and vitamin B12 (which comes from gut bacteria). The process won't be cheap. By one rough estimate, first-generation lab meat could cost \u00a33,500 per tonne (compared with \u00a31,800 per tonne for unsubsidized farmed chicken meat) 3 . Mironov thinks the best way to secure an early market is by turning  in vitro   meat into a 'functional' food attractive to the rich and famous, perhaps by filling it with compounds that promote health or suppress appetite. \"Only Hollywood celebrities like Paris will be eating this,\" he says. Alternatively, one could get an edge on the market by making meat products from exotic or even extinct animals, assuming a few of their cells could be saved. In the long run, advocates see a market in vegetarians and others who want guilt-free and environmentally friendly meat. Researchers such as Post believe that the scientific and technical advances needed to make and sell  in vitro   meat are worth the fight \u2014 but convincing funders remains the biggest obstacle. Post's funding from the Dutch government ran out in 2009; he came out of that with hundreds of pork strips, not the thousands he needs for a sausage. Today, a couple of umbrella organizations promote the cause, including the non-profit New Harvest, which provides small funds for US- and Europe-based work, and a new commercial company, California-based Pure Bioengineering, which aims to raise venture capital. But no windfalls have arrived as yet. Post will keep seeking funding for his demonstration sausage \u2014 but he knows that raising enough to commercialize the entire process will be a huge ask. \"I usually say \u00a3100 million,\" says Post. \"That's the number I forward to the government, and then they faint.\" Nicola Jones is a freelance journalist based in Vancouver, Canada. UPDATE 20 February 2012: Mark Post, speaking at the American Association for the Advancement of Science meeting in Vancouver, said that he has secured 250,000 Euros from an anonymous donor to continue his work. He expects his first 'sausage' to be made before the end of 2012. \n                     Special: Genetically Modified Food \n                   \n                     New Harvest \n                   \n                     Meat 2.0: in vitro meat bulletin board \n                   \n                     What is meat? \n                   Reprints and Permissions"},
{"file_id": "467902a", "url": "https://www.nature.com/articles/467902a", "year": 2010, "authors": [{"name": " Jeff Tollefson"}], "parsed_as_year": "2006_or_before", "body": "After winning a Nobel prize for helping to protect the planet, Mario Molina is tackling a much more difficult problem \u2014 trying to clean up Mexico City. Cab drivers have heard of him. Political leaders seek his advice. Strangers often shake his hand in a mixture of congratulations and thanks. Such is the fame of Mario Molina, the 67-year-old chemist who has become something of a national icon in his hometown of Mexico City. More than four decades ago, Molina left this city to pursue his doctorate in the United States. His first paper as a postdoc in 1974 alerted the world to the atmospheric dangers of chlorofluorocarbons (CFCs), and helped to save the ozone layer \u2014 the planet's shield against ultraviolet radiation. Molina went on to win the Nobel Prize in Chemistry and rise to the top ranks at the Massachusetts Institute of Technology (MIT) in Cambridge. But close personal and cultural ties pulled him back to Mexico City some five years ago, when he shifted away from the elegance of stratospheric chemistry to tackle the messy world of public policy, urban planning and climate change. Molina's great challenge is to help Mexico City to reach its goal of becoming the greenest megacity in Latin America. It is a tall order. This metropolis of more than 20 million people was once considered the most polluted urban area in the world. Mexico City made great strides during the years that Molina was in the United States, and he has pushed for further environmental gains since his return. But the city still suffers from problems such as persistent air pollution, rampant development and poor sanitation. Through his eponymous think tank, the Mario Molina Center for Strategic Studies in Energy and the Environment in Mexico City, Molina has assembled a team to tackle those seemingly intractable problems. As the only Mexican to garner a science Nobel, Molina uses his stature to provide behind-the-scenes advice to government and industry leaders. Some think he should take a more forceful stand on issues, but Molina's quiet style has already earned him the trust of Mexico City's mayor, Marcelo Ebrard, as well as Mexican President Felipe Calder\u00f3n. This is Molina's way of giving something back to the country that provided him with opportunities as he grew up. \"The Nobel prize is of course a big honour and so on, but it's also a responsibility,\" he says. \"If I use it wisely, then I can impact government decisions.\"  \n                It's like gospel \n              Molina grew up in an atmosphere of culture and privilege, with a father who tripled as a successful lawyer, academic and diplomat. Mario travelled the world for his education, attending boarding school in Switzerland, university in Mexico and Germany, and graduate school at the University of California, Berkeley. All this helped him to develop an international view on the world and a diplomatic personality. Although accustomed to advising lawmakers, governments and heads of state on difficult issues, he does not flaunt his status. \"Mario conveys overwhelming modesty and humbleness,\" says Adri\u00e1n Fern\u00e1ndez Bremauntz, a long-time colleague and friend who heads the National Institute of Ecology, a research arm of the federal environment ministry, based in Mexico City. Some people invoke the term \"Saint Mario\", and Fern\u00e1ndez jokes that he can almost make out a halo above Molina's head when he speaks. \"It's like gospel.\" After winning his Nobel prize in 1995, Molina took advantage of his academic freedom as an institute professor at MIT and began rethinking his scientific agenda. In 1999, he and his first wife, and long-time research partner, Luisa Molina established the Integrated Program on Urban, Regional and Global Air Pollution at MIT. Their goal was to study megacities, and the first case was Mexico City. By that time, the city had already cleaned up its act considerably. From the 1980s to the late 1990s, the city had phased out the use of leaded petrol and had reduced sulphur levels in diesel fuel, while shifting power plants and industry away from diesel and fuel oil towards cleaner-burning natural gas. It had also phased in the use of catalytic converters and other pollution-reduction technologies for new vehicles. But pollution levels above the city were still formidable. In 2003 and 2006, the Molinas organized a pair of intensive air-sampling campaigns involving hundreds of scientists from several US, Mexican and European institutions. The campaigns have generated more than 170 publications so far, and today scientists have more data about Mexico City's air pollution than about any other city in a developing country \u2014 and perhaps the world. The picture that emerges could be likened to a bubbling cauldron. Mexico City is built on a plateau 2,240 metres above sea level, where abundant ultraviolet radiation cooks up a stew of pollution. It includes black carbon, sulphur and nitrogen oxides from vehicles, and volatile organic compounds (VOCs), which come from sources such as vehicles, leaking gas canisters, solvents and paints (see 'Sources of air pollution of Mexico City'). The reactions between nitrogen oxides and VOCs form ozone, a potent constituent of smog that often gets trapped above the city by the surrounding mountains and temperature inversions. \n               Click here for larger image \n               The air-sampling campaigns showed that, contrary to expectations, VOCs rather than nitrogen oxides control how much ozone forms above the city. With that in mind, the authorities are expanding the metropolitan area's pollution regulations to also target VOC emissions from specific sources. \"In the past, we didn't pay very much attention to controlling solvents,\" says Victor Hugo, who heads the city's air-quality programme. \"Now we consider them as part of the principal measures in the new air-quality priorities that are being designed.\" In 2004, the Molinas moved to the University of California, San Diego, but the couple eventually went separate ways, both personally and scientifically. Luisa continues their megacities work in San Diego at her own institute named the Molina Center for Energy and the Environment. Mario moved back to Mexico City, where he took up the fight to combat smog by reducing the sulphur concentrations in fuels. Fern\u00e1ndez says that Molina provided a crucial voice by keeping air quality on the agenda at a time when political momentum was flagging \u2014 precisely because city and federal authorities had already made so much progress. The continued pressure from Molina and others helped to convince authorities to expand the use of low-sulphur petrol and diesel \u2014 much of which has to be imported from other countries. Now that the quality of fuel is improving, says Hugo, Mexico is contemplating new air-pollution and fuel-efficiency standards for vehicles. Molina takes heart from these accomplishments, but plays down his own role. \"It took a huge effort on the part of many people,\" he says, \"and those partnerships continue today.\" The centre in Mexico that bears Molina's name is housed in a modern office tower, perched on a hill on the west side of town. Guillermo Velasco, project coordinator at the centre, gestures out of the window towards sets of buildings that have sprouted in the past decade \"The city is growing horizontally,\" says Velasco, who met Molina in Cambridge, while studying public policy at Harvard University. \"It's not sustainable.\" Many cities around the world have encountered similar problems, Molina says, but Mexico City faces the added burden of widespread corruption, pockets of stifling poverty and limited government resources to turn things around. Once established, illegal shantytowns are hard to control. And even in wealthier areas, construction companies clear cheap land and build at will. \"If you had a good city plan and you designed the legal measures so that you can enforce them,\" says Molina, \"that wouldn't happen.\" He established his think tank to help craft the policies that could make Mexico City more sustainable. The centre started out small, made up of Molina and several colleagues from his university days in Mexico and Berkeley. Some of its initial funding came from the William and Flora Hewlett Foundation based in Menlo Park, California, and from Carlos Slim, a Mexican telecommunications magnate. Over the past five years, the centre has expanded to roughly 45 people, including environmental engineers, architects, urban planners and biologists. The team is pushing for policy changes on several fronts. It supports expansions in public transport, more mixed-use neighbourhoods that allow people to live close to their work, changes to the tax code to discourage sprawl and ways to integrate planning with the various levels of government. Molina and his team are producing a series of regional sustainability studies, the first of which is expected early next year. These and other efforts are intended to help authorities to conduct a broader environmental agenda, including meeting the city's goal of reducing greenhouse-gas emissions by 12% between 2008 and 2012. Climate change has recently emerged as a priority in Mexico. Ebrard chairs the World Mayors' Council on Climate Change and will host a climate meeting in Mexico City on 21 November, a week before the international climate summit in Canc\u00fan, Mexico. But the Molina centre hasn't been free of criticism or controversy. There have been concerns about the centre doing consulting work for industrial interests such as PEMEX, Mexico's state-owned oil company, rather than focusing on its own independent agenda. Molina is also pulled all over the world by his many international commitments, including serving on US President Barack Obama's Council of Advisors on Science and Technology as well as a part-time academic post at the University of California, San Diego. Funders and members of the advisory board, including Fern\u00e1ndez, have urged Molina to take a more active role in guiding the centre. Molina acknowledges these criticisms and is working on a strategic review of the centre. He says that the centre has never compromised its findings when working with industry \u2014 which he views as necessary and important \u2014 but is nonetheless moving away from paid consulting. The Mexican Congress made that move easier by allocating 50 million pesos (US$4 million) to the centre in 2010, roughly doubling the centre's annual budget.  \n                The Mario Molina brand \n              \"The Molina centre has the potential to be one of the two or three most credible institutions in developing countries on energy and environmental policy,\" says Joseph Ryan, a former programme officer at the Hewlett Foundation, which provided about $4 million in funding to the centre from 2004 to 2009. But Ryan says that the centre is too closely linked with its founder and must take care to develop its own credibility \"independent of the Mario Molina brand\". Molina started building that brand when he arrived at the University of California, Irvine, for his postdoctoral fellowship with Nobel-prizewinning atmospheric chemist Sherwood Rowland in 1973. Rowland offered him several projects, most of which centred on his own analyses of radioactive molecules, but the list contained one outlier: an investigation into the fate of an increasingly important class of commercial chemicals \u2014 CFCs \u2014 used as refrigerants and propellants in aerosol cans. Rowland thought that the compounds could be useful as tracer chemicals for atmospheric research. Molina chose the CFC project, attracted by the possibility that his work could have real-world implications. \"He certainly picked the right project, and was of course instrumental in working it through,\" says Rowland. \"At the end of three months, we realized that we had the bear by the tail.\" CFCs were popular in commercial products because they did not seem to react with other gases. Recognizing that the chemicals would gradually collect in the stratosphere, the two calculated that CFCs would in fact break down under a combination of cool air and the harsh ultraviolet radiation of the upper atmosphere. Those reactions would liberate chlorine atoms that could then crack ozone molecules. Molina and Rowland published their findings in  Nature 2  in June 1974. That work helped to spur the United States to ban the use of CFCs in aerosol cans in 1978, and eventually led to a global phase out through the 1987 Montreal Protocol on Substances that Deplete the Ozone Layer. The 1974 paper with Rowland also served to focus Molina's research. Studying the atmosphere would allow him to pursue his interests in fundamental chemistry, \"but it was also very close to an applied field\", he says. In 1985, British researchers showed that ozone over Antarctica was disappearing every spring 3  far faster than could be explained by the reactions proposed by Molina and Rowland. Mario and Luisa Molina, who had since moved to NASA's Jet Propulsion Laboratory in Pasadena, California, joined the race to explain the ozone hole. \"He was one of the more gentle creatures in that fight,\" says Michael Prather, an atmospheric chemist at the University of California, Irvine. Two years later, the Molinas and their colleagues cracked the problem, proposing a pathway that recycles chlorine molecules so that they can repeatedly destroy ozone. The process starts with a chlorine atom stealing an oxygen atom from an ozone molecule (O 3 ) to form chlorine monoxide. That highly reactive compound reacts with itself to form a dimer, Cl 2 O 2 . When spring arrives in Antarctica, sunlight shears off the oxygen, freeing up two chlorine atoms to repeat the reaction in a runaway cycle 4 . Although Molina won the Nobel prize for his 1974 work, some scientists regard this later discovery \u2014 made when the research community was racing for a solution \u2014 as the one that really lit up Molina's scientific credentials. Molina chalks his success up to a little luck, intuition and a lot of hard work. \"Part of the luck is choosing the right problem,\" he says, \"but then you need to take advantage of it.\" Prather says that Molina has a keen ability to step back from the detailed chemistry and think about the larger problems that affect society. He wasn't surprised when Molina turned his attention to the skies above Mexico City. \"We all knew that deep in his heart his ties were to Mexico.\" In early September, Molina delivered a 45-minute keynote lecture about climate change to financiers at the Mexican Banking Association in Mexico City. This event was co-organized by the United Nations Environment Programme Finance Initiative in advance of the November climate summit. Molina covered the science of climate change without stepping over the line into advocating specific policies, as some researchers have started to do. After the talk, Burghard Petersen, a former banker and climate consultant from Germany who has lived in Mexico for 19 years, said that Molina \"is offering credibility. He's not a sales person.\" But some think he should be selling his ideas harder. \"Mario has chosen a very friendly, moderate, constructive approach,\" says Fern\u00e1ndez. \"If he wants to accomplish his goals, I would certainly advise him to grab the government's neck every once in a while and be more pushy.\"  \n                Cool advice \n              Back in his office, Molina says that he prefers to build coalitions from the inside rather than using the press and his position of authority to pressure decision-makers into action. He is careful not to reveal his own politics and has built up relations with parties on both sides of the Mexican political spectrum. Questions about the environment are far less partisan in Mexico and many other developing countries than they are in the United States, and Molina wants to keep it that way. On the wall behind Molina's orderly desk hangs a watercolour painting of an open umbrella in muted tones, a gift from the Nobel committee symbolizing the protective ozone shield that Molina helped to save. He acknowledges that the social and political problems he is working on now are not likely to yield as easily to a solution as the stratosphere did. Some days he can see out of his window all the way to the centre of Mexico City, but today smog fills the vista, a dramatic indication of how much work remains to be done. An optimist at heart, Molina takes the long view and says civilization is improving on the whole. Only 500 years ago during Aztec times, he notes, people were sacrificed at the top of the temple downtown. \"That would be inconceivable today,\" he says with a chuckle. \"I'm not naive. I know it's going to take time, but we are taking on part of the challenge and doing the best we can.\" \n                 See Editorial  \n                 \n                     p.883 \n                   \n               Jeff Tollefson  is a reporter for   Nature  based in   Washington DC. \n                     Mario Molina Center for Strategic Studies in Energy and the Environment \n                   \n                     Mario Molina Center for Energy and the Environment \n                   Reprints and Permissions"},
{"file_id": "467266a", "url": "https://www.nature.com/articles/467266a", "year": 2010, "authors": [{"name": "Nicola Jones"}], "parsed_as_year": "2006_or_before", "body": "Singularity University tries to breed world leaders by immersing students in futuristic concepts. Nicola Jones finds it a heady mix of grand claims, brilliant minds and cool gadgets. A laser runs across Alison Lewis's face, scanning her features into a laptop in a room filled with Lego robots and other high-tech toys. Her image appears on the screen, and after a few tweaks it is ready to be printed, in three dimensions, by the minibar-sized machine in the corner. This blows the minds of many of the students in the room. More importantly, it gets them thinking about the possibilities of tomorrow's technologies, and how they can be used to improve the future. Last year, students inspired by such demonstrations came up with the idea of printing low-cost housing in the developing world. One of them left his PhD and moved to the Bay Area of California to move that project forwards. Singularity University  in California, known as SU by its alumni, isn't really a university \u2014 it doesn't award degrees or grant course credits. Instead, it is an intense, ten-week, exclusive summer school for overachieving graduate-level students from the worlds of academia and business \u2014 including, this year, the Massachusetts Institute of Technology's youngest-ever graduate student and several multimillionaires. Its mission is to educate and inspire future leaders to use emerging technologies to solve the globe's biggest problems, from poverty to ill health to resource depletion. When I drop by for a few days in August, the place feels like a think tank mashed with a geek adventure camp and a business-networking cocktail party. The experience doesn't come cheap. SU students pay US$25,000 for ten weeks' tuition and board, which would buy a full year, or even two, at most US universities. But the students are sparking ideas, forging relationships and brokering deals that will last a lifetime and might just save the world \u2014 or make the students some money while trying. The project was co-founded by Peter Diamandis, whose more famous brainchild, the  X PRIZE Foundation , hands out multimillion-dollar prizes for things such as launching the first manned private spaceflight. Diamandis was inspired by futurist Raymond Kurzweil's 2005 book  The Singularity is Near , which describes how 'accelerating technologies' are driving exponential improvements in humanity's ability to live ever longer and better. Kurzweil sees this leading to 'the singularity': a moment when computers will be one billion times more powerful than all human intelligence today, triggering a profound transformation in human capability and causing the Universe to 'wake up' \u2014 sometime around 2045. Predictions like that make most scientists squirm (and most SU students, too, who are quick to tell me they aren't 'singulatarians' like Kurzweil). But the notion of technological acceleration struck Diamandis as the right way to look at the world \u2014 and to frame a school. Diamandis had previously founded another uber-summer-camp, the International Space University in Strasbourg, France, which seeks to train the future leaders of space agencies around the world. SU was to be similarly high-powered, but broader in scope. In 2008, a group including Diamandis and Kurzweil had a meeting to explore the idea, and promptly hired Salim Ismail, the former head of Yahoo's 'ideas factory', as executive director. By summer 2009, they had their first class of 40 students. What they formed is a programme that effectively turns normal academia on its head. \"The education system for the past 200 years has been about really drilling down into a tiny subject,\" says Diamandis. \"Here we try and flip it.\" Instead of 'deep diving' into a single subject, students broaden their horizons. Instead of learning the history of their speciality, they imagine the future of the entire world. The result has made some waves, in part because of the school's unique mandate, provocative name and influential connections \u2014 Google, for one, donated $1 million to help launch SU. At SU's opening last year, Larry Page, who co-designed the software behind Google's search engine, said: \"If I was a student, this is where I would want to be.\" And the programme has caught the attention of Washington. Last month, Ismail spoke on behalf of SU at a US Agency for International Development state dinner hosted by US secretary of state Hilary Clinton. Among academics, impressions are more mixed. Andrew Maynard, director of the University of Michigan Risk Science Center in Ann Arbor, who isn't involved with the school, says he has \"reservations about a programme that runs the risk of running close to pseudoscience at times\", given some of the people involved with the project. But at the same time, he says, people probably need to break out of the usual conservatism of academia to solve the world's most pressing problems. \"Maybe there is a need for opportunities that allow scientists and engineers to let their imaginations run a little wild.\"  \n                Post-it visions \n              SU's format is simple. It starts with five weeks of lectures and field trips that introduce students to the cutting edge in ten areas, from nano-technology to biotechnology (see 'Curriculum of the future?'). This year's trips included a jaunt to the robotics research lab Willow Garage in Menlo Park, and the Tesla electric car company in Palo Alto, both in California. Students got a chance to test the da Vinci robotic surgical system \u2014 a device that lets doctors use mechanical arms for precision surgeries \u2014 and ride one of the world's top flight simulators at the  NASA Ames Research Center  in Mountain View, California \u2014 the campus on which SU is housed. In the final five weeks of the programme, the students organize themselves into groups to tackle one of five areas: water, food, energy, space and 'upcycle' (the idea of turning rubbish into something useful). Their mission is to 'harness the power of exponential technologies' and come up with a plan \u2014 commercial or non-profit \u2014 that will improve the lives of 1 billion people within a decade. By week seven, when I visit, the students are burning the midnight oil, digesting what they have learned and developing their projects. A few dozen congregate in the university's main gathering place, which is plastered with post-it notes mapping their predicted vision of the future (three-dimensional printing will be available at Kinkos photocopy centres, one note says, in 3\u20135 years). Diamandis rubs his hands together and orders a coffee to keep him going, not that it looks as if he needs it. In one room, students from Ethiopia to Romania grapple with the democratization of energy; in another, a Chilean struggles to get drinking water to the slums. When discussion turns to better ways of distributing information, Diamandis says: \"If you want to talk to the people who created Google Earth \u2014 a platform like that \u2014 I can hook you up.\" The students simply nod, now used to such extraordinary connections. The spread of nationalities among the students is impressive. Some 1,600 people applied for this year's summer camp, from 85 countries. Applicants were judged on their academic strength, their entrepreneurial or leadership expertise, and a dedication to solving humanity's grand challenges. Diamandis and Ismail made the final selections, aiming for 25\u201330% women, 20\u201325% from the developing world, and a good spread of expertise and geography. The result is 78 students from 35 countries. One of them is  David Dalrymple . Now 19, Dalrymple was the youngest person to start a graduate programme at the Massachusetts Institute of Technology in Cambridge when he began there five years ago. For his PhD, he is inventing a programming language better suited to artificial intelligence. Dalrymple says that SU \"follows the first law of education, which is that the people are more interesting than the programme\". Lewis, who had her face scanned, is a designer whose 2008 book  Switch Craft   combines electronics and sewing to make technology more accessible. At 22, Londoner Zain Jaffer has already founded a long list of start-up companies. And Santiago Bilinkis had made millions by the age of 35 by founding Officenet \u2014 Argentina's version of the US office-supply chain Staples \u2014 and then selling it to Staples. \"When I was young, I wanted to get rich and be a mad scientist. I'm here to fulfil my plan,\" he says. The result, if nothing else, has the makings of a good dinner party. But SU's organizers hope to produce more than that: tomorrow's presidents, Nobel prizewinners and top executives. \"We're essentially creating a primordial ooze of future leadership,\" says Ismail. Many of these students can't afford the $25,000 price tag; more than half of them got a full or partial scholarship from SU or other donors. The millionaires, naturally, paid for themselves. In total, the non-profit University pulls in about $1.1 million a year from students and some donations, says Ismail. But the rent in the NASA facility, along with nine full-time staff salaries and the burden of reinventing the curriculum afresh every year, soaks up about $2.5 million. The organization covers much of the shortfall through the proceeds of its executive programmes \u2014 in which 45 participants pay $15,000 for a nine-day session, or $6,500 for four days, in the autumn.  \n                Technotopia \n              Over the summer, this year's students have heard from 160 speakers, including Linda Avey, co-founder of the personal genetics company  23andMe;  Dean Kamen, inventor of the  Segway  personal transporter and brain-controlled prosthetic limbs; and Ralph Merkle, a nanotechnology and quantum-computing guru. The star power of the advisers seems brightest in the areas of information technologies: they include Vint Cerf, one of the fathers of the Internet, and John Gage, former chief researcher at Sun Microsystems. In ten weeks, students can get only a broad overview of topics through the barrage of lectures. But they also get on-demand face-time with people who know what's what in the hottest technologies. \"A lot of people come here with a mission, and meet the one person in the world who could answer their question,\" says Tony Lyu, a student from South Korea. Lyu, who studied electrical engineering and has worked with the United Nations Office for Outer Space Affairs, isn't exactly without connections. But, he says, \"I wasn't part of the inner circle before. I am now.\" In one evening, I twice hear Diamandis tell students: \"We can assume that energy will eventually be ubiquitous and essentially free.\" Almost all of the staff and students share this infectious techno-optimism and enthusiasm. It is an attitude that can make the impossible seem possible and drive innovative thought. But it can also gloss over deeper complexities and challenges. \"There are people here who have never failed at anything in their lives, now bumping up against major human problems,\" says Sam Thorp, a biotech student and entrepreneur wannabe from Sydney, Australia \u2014 and organizer of a record-breaking custard pie fight in 2009. \"There's an undercurrent of realizing that not everything has a technological or simple solution.\" The techno-utopian flavour of SU is disturbing to some. Jamais Cascio, a senior fellow at the US-based non-profit organization Institute for Ethics and Emerging Technologies, who lives near San Francisco but isn't involved with SU, criticizes the programme for having \"an abundance of 'look at this cool stuff we'll be able to do real soon now!' with little countervailing scepticism or caution\". Worse, he adds, the non-technological discussions don't seem to get to the heart of pressing societal issues: the economics sessions seem to be about finance, he says, and the policy sessions about how to avoid barriers to technological development.  \n                Ideas into action \n              A way of evaluating SU's balance is to look at the class projects. One of last year's products is  GetAround  \u2014 a company, started with seed money gained through SU, that lets car owners in Mountain View (and, soon, a local college campus) lease their vehicles out during idle hours to others. It is a good idea, but hardly seems like the paradigm-shifting type of concept promised by the shiny school propaganda. Ismail tells me I'm thinking about it all wrong. The real project pitched by these students was a vision of a future with autonomous fleets of zero-emissions vehicles, without private owners. To get there, the hardest part will be shifting the cultural model from owning cars to accessing them. GetAround is the means to start that transformation. As Diamandis repeatedly tells the students, \"The vision inspires, but you get believability from the first step.\" Similarly, three-dimensional printing can seem like little more than a cool toy \u2014 student Derek Jacoby brought his own homemade 'MakerBot' printer with him to SU, which can spit out little objects such as whistles or replacement parts for itself. But the students here have been trained to wrap their brains around exponential development. As Kurzweil puts it, the twenty-first century won't see 100 years of advancement at year-2000 rates, but a mind boggling 20,000 years of progress. The students see three-dimensional printing fundamentally changing global economics, eliminating shipping from China and putting money and power into the hands of designers rather than manufacturers. Last year's SU graduate Devin Fidler left his PhD in Budapest to work on the project he and his team called 'Acasa', which seeks to use concrete-extrusion technology to print houses in the developing world. They foresee a future in which a simple home can be made in a day and a half for $4,000, using the equivalent of 30 light-bulbs worth of power, Ismail said in his speech in Washington last month. For now, Fidler is trying to get more money to his scientific partner for the necessary research and development. This year's crop of a dozen projects includes a business plan for a shop and restaurant selling only food grown on the premises, to encourage and develop urban farming using all the latest tricks of hydroponics \u2014 growing plants without soil \u2014 and genetically modified foods ( agropolisfarm.com ). One team proposes installing a cloud of cheap nanosatellites in low-Earth orbit that can be patched together to perform nearly any space-based service. Another, initiated by SU student Dmitriy Tseliakhovich, a theoretical physicist from Belarus, aims to develop cheaper and more environmentally friendly rocket launchers driven by land-based microwave lasers. Not all of the project ideas will turn out to be novel or feasible, but some are already garnering attention. Many students had meetings with 'angel investors' or venture capitalists who were invited by SU to the final presentations. Whether or not they gain funding for their ideas, the students I spoke to said they were newly invigorated to \"save the world\". According to Ismail, a poll of the students found that 60% want to pursue their projects immediately, and about half want to stay in the Bay Area, enticed by the connections they have found there. Nearly half of last year's graduates never left California. The outcome of the school won't be known for years \u2014 when the students go on to bigger and better things. The expectation that they will do so may turn into a self-fulfilling prophecy. \"We're being told it's our job to change the world. It's our responsibility,\" says Jaffer. \"I walk out of here and I'm like: holy shit, now I really have to do something.\" Nicola Jones is a commissioning editor for the Opinion section of  Nature . \n                     X Prize special \n                   \n                     Singularity University \n                   \n                     Singularity YouTube Channel \n                   \n                     2009 Project videos \n                   \n                     Acasa \n                   \n                     Getaround \n                   Reprints and Permissions"},
{"file_id": "466916a", "url": "https://www.nature.com/articles/466916a", "year": 2010, "authors": [{"name": "Brian Vastag"}], "parsed_as_year": "2006_or_before", "body": "Researchers have rallied round a promising molecule for rescuing dying nerves. But getting it into the brain remains a daunting challenge, finds Brian Vastag. Nine blocks from the beach in Santa Monica, California, a small biotechnology company occupies one bay of a single-storey commercial block, about the width of a three-car garage. Founded by William Pardridge, an endocrinologist at the University of California, Los Angeles (UCLA),  ArmaGen Technologies  employs just five people: call it garage-band biotech. From a small refrigerator in the cramped space, Pardridge pulls out a black plastic tray. Within are four dozen small bottles labelled 'AGT-190', a drug that Pardridge hopes will revolutionize treatment for several debilitating brain diseases. Starting with a radical idea for sneaking therapeutic proteins into the brain, Pardridge launched ArmaGen in 2004. This year, the company is close to achieving a milestone: a green light from the  US Food and Drug Administration  (FDA) to carry out human safety tests of its first product. AGT-190, says Pardridge, acts as a Trojan Horse. It sneaks across the barrier that separates blood and brain tissue and delivers its contents \u2014 a growth factor that can protect and repair neurons. A long line of researchers has heard the siren song of this naturally occurring brain protein, called glial-cell-derived neurotrophic factor (GDNF) \u2014 and for good reason. A thick stack of reports on animal studies and a smattering of evidence from initial trials in humans show that GDNF can halt the damage that follows stroke, interrupt drug addiction, and slow or even reverse the neuronal death march that incapacitates patients with Parkinson's or Huntington's disease 1 . \"The excitement behind growth factors such as GDNF is that not only could they be protective, but there's the possibility for regenerating or rejuvenating some of the sick cells in the brain,\" says Todd Sherer, vice-president of research programmes at the  Michael J. Fox Foundation for Parkinson's Research , based in New York, which has awarded about US$20 million in grants for growth-factor research. But a maddening hurdle remains: delivery. Early surgical trials in which GDNF was delivered through catheters to the brains of patients with Parkinson's disease failed to spread the protein to a sufficient proportion of damaged regions to do much good. Advancing on these techniques, companies are moving forward with more trials looking to circumvent the blood\u2013brain barrier, but Pardridge is convinced that his technology can reliably deliver GDNF to the entire brain without the need for surgery. Still, his company is running out of funds, and there is no hint of investment forthcoming. \"Nobody has succeeded at crossing the blood\u2013brain barrier before, so why should we?\" he asks, echoing the feedback he has heard from prospective investors. Pardridge \u2014 like other researchers \u2014 soldiers on, nevertheless, knowing that the pay-off could be huge. \"The therapeutic potential of GDNF is just enormous,\" he says.  \n                A hit-or-miss history \n              GDNF was a star from the start. Discovered in 1991 by researchers at Synergen, a biotechnology company based in Boulder, Colorado, GDNF dramatically revived dishes of dying neurons. When Synergen tested GDNF in monkeys with an induced form of Parkinson's disease, the treated monkeys trembled and spasmed much less than their untreated cage mates. A year after announcing their results, in 1994, Synergen was snapped up by a larger biotechnology firm,  Amgen , for about $240 million. Kevin Sharer, Amgen's chief operating officer at the time, said the company made the decision to buy Synergen after seeing the before-and-after footage of treated monkeys. As Sharer told  The New York Times : \"We looked at that movie and said, 'Buy this company'. Literally.\" Amgen quickly moved GDNF into human trials. Between 1996 and 1999, 38 patients with advanced Parkinson's disease underwent surgery to place catheters in their brains. Hooked to a pump implanted in the abdomen, the catheter delivered GDNF into the fluid-filled spaces between the main lobes of the brain in the hope that GDNF would migrate deeper into the brain, to the structures most affected by Parkinson's disease. This wishful strategy failed. Instead of improvement, patients experienced nausea, delusions and chest pains, the exact cause of which has still not been uncovered. Amgen halted the trial early. After shelving further trials, the company made the protein available to interested researchers. Steven Gill, a neurosurgeon at Frenchay Hospital in Bristol, UK, partnered with researchers at the University of Kentucky in Lexington and designed a new catheter to push the drug directly into brain regions affected by the disease. Delivered under pressure, the GDNF solution continuously diffused from the tip of the catheter into the putamen, a thumb-sized structure at the base of the forebrain that degenerates in individuals with Parkinson's disease. In 2001, Gill inserted his catheters into the brains of five patients. Although designed as a safety trial, the changes were swift and positive 2 . Over 12 months, all five patients showed improvements in standard scores of movement and motor skills. It was a \"very successful trial\", says Erich Mohr, chief executive of MedGenesis Therapeutix, based in Victoria, Canada, which earlier this year acquired the rights to GDNF from Amgen. After that trial, Amgen quickly launched a larger trial to test GDNF against a placebo. But in this case, patients given GDNF fared no better than patients who received the surgery without GDNF. \"That trial was basically designed to fail,\" says Clive Svendsen, a neuroscientist who was a consultant to Amgen on the study. Amgen chose a catheter that was thicker than Gill's, dripping GDNF into the brain rather than delivering it under pressure. Consequently, the GDNF solution simply refluxed up the outside of the catheter, says Svendsen, now director of the Regenerative Medicine Institute at Cedars-Sinai Medical Center in Los Angeles. Amgen declined to comment about the trial for this story. In 2004, amid a huge spate of negative publicity, it mothballed its GDNF project. But shortly after, Gill and his colleagues made a tantalizing discovery. One of the patients in Gill's trial had died of a heart attack, and when the team carefully sliced the man's brain the researchers saw something amazing. Neuronal fibres had sprouted in the patient's putamen on the right side, where the catheter had been placed 3 . The man had described a huge improvement in his quality of life over the 43 months for which he received the drug. To those in the field, the message was clear: his brain had been healing. \n                New surgical trials \n              GDNF isn't the only drug that has run up against delivery troubles. The blood\u2013brain barrier has been an enormous \u2014 often unacknowledged \u2014 show-stopper in drug development. Almost none of the hundreds of potential drugs for treating brain disorders can penetrate the tight mesh of endothelial cells lining the blood vessels in the brain. This barrier protects the brain and keeps most large molecules out of the cerebrospinal fluid that bathes neurons. Still, the tantalizing hints of effectiveness seen in some trials convinced researchers that GDNF was worth pursuing. Although Amgen had little interest in GDNF between 2004 and 2010, researchers continued to seek other strategies for getting the protein into the brain. Now, several biotechnology companies are launching a new round of surgical trials. One project will test next-generation catheters \u2014 similar to Gill's design \u2014 for delivering GDNF protein. A second will, as early as next year, implant neural stem cells that have been programmed to produce GDNF into the spinal cords of patients with the degenerative disorder amyotrophic lateral sclerosis. And one study, already under way, is delivering viruses that carry the gene encoding a growth factor that is closely related to GDNF through a brain-implanted catheter. Ceregene  in San Diego, California, is the company taking this gene-therapy approach. Jeffrey Ostrove co-founded the biotechnology company in 2001 and chose a growth factor called neurturin, which acts much like GDNF, as a potential treatment for Parkinson's disease. The team at Ceregene packages the gene encoding neurturin into a gutted virus, and then infuses the virus into the patient's putamen under pressure. There, the virus delivers the gene to brain cells, which in turn should pump out the growth factor, perhaps indefinitely, says Ostrove. But results released in 2008, from the company's trial in 58 patients with advanced Parkinson's disease, were disappointing. A year after surgery, patients who had received the altered virus fared no better than patients who received sham surgery and no virus. Both groups improved about equally on standardized symptom scores. (For unknown reasons, sham surgery for Parkinson's disease produces a strong placebo effect.) Despite raising some $70 million, Ceregene laid off 30 of its 50 staff, and Ostrove pondered shuttering the programme. Then, donated brains from two trial patients who had died suggested a path forwards. In the trial, surgeons had infused the virus in eight locations across each patient's putamen. Yet Ostrove says that just 15% of the putamen expressed the neurturin gene \u2014 the delivery problem again. Persuaded that neurturin might still work if its delivery could be improved, this June the Michael J. Fox Foundation gave Ceregene $2.5 million to support another trial, in which surgeons will infuse virus into both the putamen and the substantia nigra, a structure in the midbrain. The company is also increasing the viral dose fourfold. Twenty-six patients will undergo the revised procedure over the next few months, and the same number will receive sham surgery.  \n                Back-door protein \n              ArmaGen is taking a different approach to breaching the blood\u2013brain barrier. Rather than going around it, with all the risks that brain surgery entails, Pardridge wants to enter through a biological back door. Pardridge has been studying the barrier since 1970. After about a decade, he discovered a potential way in, an insulin receptor. Pardridge showed that insulin receptors in the capillaries that feed the brain are transporters \u2014 grabbing molecules of insulin and pulling them into the brain tissue. Over the next 15 years, while at UCLA, Pardridge developed a monoclonal antibody that latches onto part of the brain's insulin receptor without interfering with insulin binding. The receptor pulls both the insulin and the antibody through the blood\u2013brain barrier, says Pardridge. He published this 'Trojan Horse' antibody design 4  in 1995, and then set to work engineering it as a vehicle for therapeutic proteins. Pardridge and Ruben Boado, a molecular biologist also at UCLA and ArmaGen, stitched the gene encoding the antibody together with the gene encoding GDNF and, after several painstaking years, worked out how to scale up production of the hybrid protein, AGT-190. Animal studies funded by ArmaGen show that the Trojan Horse approach works: it gets GDNF into the brain 5 . A large meeting poster hanging in Pardridge's office displays the results: sections of a rhesus monkey brain stained a lurid blue. (Rhesus monkeys, unlike other monkeys or mice, have a blood\u2013brain barrier very similar to that of humans.) When the researchers injected the antibody into the animals' veins and then looked for it in the brain, they found it everywhere. About 2% of the AGT-190 injected into veins arrives in the brain. That's about the same as for antidepressants and other traditional, small-molecule, brain drugs that can cross the blood\u2013brain barrier unassisted. Pardridge says there is a fatal flaw in surgical, catheter-based delivery. The zone immediately surrounding the catheter is blasted with the drug, whereas tissue more than seven or eight millimetres away receives almost none. \"Unless the region of the brain you're trying to reach is the size of a pinhead, a transcranial delivery system isn't going to work,\" he says. The scientists planning the catheter-based GDNF trials disagree. Ostrove thinks that Ceregene's new surgical protocol will deliver GDNF to 25\u201330% of the putamen \u2014 and he predicts that this will be sufficient coverage to reverse the progression of Parkinson's disease. Mohr is confident that MedGenesis Therapeutix's next-generation catheter will push GDNF into a larger proportion of the structure than earlier catheters did. When asked about ArmaGen's strategy, Ostrove, Mohr and other scientists involved in the surgical trials argue that the Trojan Horse will be felled ultimately by the very trait that Pardridge touts most: it hits the entire brain. \"You don't want GDNF to go all over,\" says Svendsen, pointing out that high doses of GDNF can cause neurons to make connections that they shouldn't. It could result in the same side effects as seen in Amgen's first human trial, which essentially bathed the brain in GDNF. Pardridge argues that the doses of GDNF delivered by AGT-190 will be much lower than those in the Amgen trials. But whether the Trojan Horse approach is safe remains an open question, because no one has received the drug. That could change as early as this October, when 12 healthy volunteers in Kansas are slated to receive three doses of AGT-190. Pardridge doesn't know what will happen after that. Even if all goes well, his company is almost out of money. ArmaGen is $1 million in the red on AGT-190. A $3-million National Institutes of Health (NIH) grant funded the drug's development, and the company has spent a total of $4 million on the project, including $1.5 million on the monkey study. Large biotechnology companies, big drug makers and venture capitalists alike have all rebuffed Pardridge's entreaties for investment. He has a stack of grant applications that he has submitted to the NIH; it is nearly half a metre high. Most have been rejected. Venture capitalists, too, \"are very risk averse, right now\", says Casey Lynch, a biotechnology analyst and president of the  Neurotechnology Development Foundation , based in San Francisco, California. It is just one of several impediments for Pardridge, including the expense of producing biological drugs, as opposed to small molecule drugs, which can be synthesized chemically. So Pardridge is stuck. He has developed a new approach to delivering a promising brain drug. He has compiled thousands of pages of data required by the FDA. And by the end of this year, he may know whether AGT-190 is safe to use in humans. But that might be the end of the line. A larger study of the drug's efficacy would cost at least $15 million, and Pardridge has no idea where to get this money. In many ways, GDNF is stuck, too. It is unclear whether surgical procedures, even if effective, will be practical for a condition such as Parkinson's disease, which affects millions of people worldwide, not to mention the many other indications for which people have been eyeing it. The costs and dangers associated with putting a hole in someone's head or spine make it prohibitive for all but the most severe cases, those unresponsive to other treatments. And yet, the development of GDNF as a treatment will probably continue, because its brain-healing properties are too tantalizing to pass up. \"It has this lovely regenerative capacity and rejuvenating ability,\" says Svendsen. \"But how it's going to work across these different diseases, which all have different mechanisms, I don't think we'll find out until after clinical trials.\" Brian Vastag is a freelance reporter in Washington DC. \n                     Biotechnology@nature.com \n                   \n                     ArmaGen Technologies \n                   \n                     Ceregene \n                   \n                     MedGenesis Therapeutix \n                   \n                     Amgen announces end of GDNF testing \n                   Reprints and Permissions"},
{"file_id": "467383a", "url": "https://www.nature.com/articles/467383a", "year": 2010, "authors": [{"name": "Kendall Powell"}], "parsed_as_year": "2006_or_before", "body": "Careers are made and broken by grant-funding committees. So how are the key decisions really made? There are six outstanding grant applications listed on the flip chart. There is money for two, or maybe three. And the decision as to which will be funded rests in the hands of the 15 members of the peer-review panel who are meeting inside the glassy rectangle of the  American Cancer Society (ACS)  building in central Atlanta, Georgia. It is only 45 minutes into the committee's two-day meeting in June, and the conversation is already tense. \"It seems pretty pedestrian,\" says the committee chairman, referring to the first application on the list. The applicant wants to investigate the molecular signals that could shut down runaway cell division in a particularly deadly cancer \u2014 but much of this pathway has already been worked out in other cell types. \"This is good solid work,\" argues another reviewer, slightly exasperated. \"Not everything has to be a bright, shiny idea. Valuable information will come out of it. The innovation is less than in other grants, but I think the other aspects make up for that.\" The real question is whether good, solid work is enough when as much as US$800,000 is at stake \u2014 the cost of supporting a cancer investigator and his or her lab over four years. The competition is extreme. At the ACS, the largest private non-profit funder of cancer research in the United States, the average success rate for grant applications has slipped by a few percentage points in the past two years to roughly 15%, owing largely to fewer donations \u2014 the organization's sole source of income \u2014 in the economic downturn. At the  National Institutes of Health (NIH)  in Bethesda, Maryland, which funds the majority of biomedical research in the United States, several years of flat federal funding combined with a rise in the number of applications means that 21% of research-project grant applications were funded in 2009, down from 32% ten years earlier (see graph). The situation in many other countries is just as tough. \n               Click here for larger image \n               All of this puts immense pressure on the grant-review panels. Senior reviewers say that when the top one-third of proposals can be funded, the review process works well at identifying the best science. But when the success rate drops, they see the process start to fall apart. Conversations turn nit-picky and negative, with reviewers looking for any excuse not to fund a project, rather than focusing on its merits. Reviewers say that they feel forced into making impossible choices between equally worthy proposals, especially when success rates are less than 20%. \"That's in a range where you have lost discrimination,\" says Dick McIntosh, professor emeritus of cell biology at the University of Colorado in Boulder. \"That's a situation where you are grading exam papers by throwing them down the stairs.\" The chairman of the ACS panel agrees. \"Deciding between the top grants, I don't want to say it's arbitrary, but it's not really based on strong criteria,\" he says. \"It's subtle things.\" To find out how subtle,  Nature   secured access to an ACS review-panel meeting. The organization spends a total of $120 million a year on research grants. That is a drop in the funding bucket compared with a behemoth such as the NIH, which awarded $16 billion in research- project grants alone last year \u2014 but federal law prohibits members of the public from attending meetings of NIH 'study sections'. The ACS allowed a reporter to sit in with the stipulation that the identities of the reviewers and the grant applicants were to be protected. And when it comes to deciding who should get a share of the pot, the tensions, agonies and battles are the same everywhere.  \n                Top marks \n              At the start of the day, as the reviewers \u2014 13 biomedical scientists, a cancer survivor and an oncology nurse \u2014 are taking their seats, the ACS programme officer, who shepherds the proposals through review, announces the results of the round six months earlier: \"Two out of 23 Research Scholar Grants were funded.\" Eyebrows furrow as everyone quickly does the mental maths \u2014 that is a success rate of just below 9%. The rate for this session will be decided by the ACS in September, on the basis of the available budget and the total pool of proposals recommended for funding from all of its 20 peer-review panels. But it is likely to be just as low as, if not lower than, the previous one. This means that only the exceptional applications will even have a chance at securing one of the grants, which are similar in scope to the NIH's  R01 Research Project Grants  \u2014 a mainstay of funding for many US labs \u2014 but are awarded only to investigators who are in the first six years of their independent careers. The panel members have already done their homework. Each of them has been assigned half-a-dozen 25-page grant applications to review in detail, producing a written critique of each one and a preliminary score of outstanding (1.0\u20131.5), excellent (1.5\u20132.0), good (2.0\u20132.5), fair (2.5\u20133.0) or poor (3.0\u20135.0). Panel members serve as 'primary reviewer' for half of their stack, and secondary reviewer for the rest. The vice-chairman lists the six proposals with 'outstanding' scores on the flip chart at the front of the room, and the panel discusses these first. Two of the applications stand head-and-shoulders above the rest. One, with a preliminary score of 1.1, uses an unusual animal model to investigate the human genes that drive an aggressive blood cell cancer. \"Of the cancer approaches [in this animal], this is the most innovative system I've seen,\" says one reviewer. \"If this works with this tumour, he can apply it to any cancer.\" The second high-scoring proposal \u2014 also 1.1 \u2014 is aimed at answering fundamental questions about a rare but highly malignant childhood tumour. It, too, wins praise for its ambition \u2014 impressive but not over-reaching. \"This would be huge, it's incredibly innovative and it addresses key questions,\" says the primary reviewer. \"Apparently, he was even more ambitious the first time around [on his first submission]. He tried his best to curb his enthusiasm, but he couldn't do it.\" The next proposal up for discussion also gets high marks \u2014 1.4 \u2014 but one reviewer notices something that doesn't sit well. The applicant has focused the proposal on a handful of genes from a longer list that are important to metastasis, the spreading of the cancer. But from the application, the reviewer can see that the investigator has submitted a nearly identical proposal to the NIH. \"Splitting the two to get two grants doesn't seem right,\" the reviewer says. \"I like it, but I don't want to support it if she's not going to put all of her best things in there.\" After some discussion, the same reviewer lowers his score to 1.9, which pushes the proposal out of the top 'competitive' range of projects that stand a chance of getting funding. Another outstanding application \u2014 this one on stem cells \u2014 runs into trouble because of a lack of scientific details. It is already borderline, with the primary reviewer giving it a score of 1.8 and the secondary reviewer giving it 1.5. The primary reviewer praises the applicant's productivity and thinking. \"So, why didn't I give it a 1.1?\" she says. \"I think a lot of this grant is open-ended.\" She can't see how the applicant will filter the genes that are pulled from the proposed screen. The problem with this particular fishing expedition, says the second reviewer, is that \"he didn't explain how he would sort through all the fish\". This proposal, too, is knocked out of the competitive range. By 11 a.m., every outstanding proposal on the flip-pad has been discussed and its adjusted scores have been marked beside it. Four remain in the outstanding range. Now the room turns to the remaining proposals, which will be scored and critiqued to help the investigators revise their applications and try again. The reviewers get irritated by applicants who don't follow the rules, or who leave out essential data. \"What annoyed me to no end,\" a panel member says of one applicant, \"is that he put the most important figures in the appendix, where there is not supposed to be any data.\" \"The entire proposal is based on the success of aim one,\" says another reviewer, referring to a proposal to isolate a cancer-cell population through cycles of specialized cell culture. \"If she doesn't achieve that, she doesn't say what she will do next.\" Another application is criticized because the investigator does not seem truly 'independent' \u2014 he seems to be continuing his postdoctoral work at the same institution. And testiness sets in when the reviewers think that the wrong model system has been chosen, or budgets seem extravagant. \"My main problem is, why isn't she just doing this in a mouse?\" says a panel member of an experiment with an unusually expensive approach. \"She could [do the experiment] much easier and cheaper.\" By the end of the morning, the panel has distilled the list of 17 applications into the four outstanding proposals and six 'non-competitive' ones which can be resubmitted in the next funding cycle. They have rejected seven proposals with scores higher than 2.0 \u2014 a range unlikely to win funding even on resubmission. \"I reviewed this last time and I don't want to review it again,\" says one panel member shortly, as lunchtime nears. \"I spent three hours reading the grant and it is just hard to follow. I read sentences out loud and had a hard time deciding what's an adjective and what's a verb.\" At lunch, the committee members sit together at two tables, wary of a reporter listening in. But one member, a physician scientist, discusses what he has learned about negotiating the review process for himself. A senior investigator at his own institution had explained to him that a well written proposal can transform the two or three main reviewers, who will read your proposal in depth, into your cheerleaders. \"If you wow those two people in the room of 20, the other 18 will vote similarly,\" he says. A look around the room suggests that only three of the scientist reviewers are older than 50 \u2014 reflecting a wider concern about participation in peer review. \"Sometimes, very good scientists are not willing to serve on study sections\" because of the time commitment, says Pietro De Camilli, a neuroscientist at Yale School of Medicine in New Haven, Connecticut, who has taken part in NIH study sections. \"You might be respected for serving, but there are no tangible rewards for it.\" Each ACS reviewer is paid a small honorarium of $250 for their in-depth critiques and attendance at the meeting, but it is a task that can easily take up to two weeks twice a year, for four years \u2014 a typical term as a reviewer for the ACS. Gregory Petsko, a structural biologist at Brandeis University in Waltham, Massachusetts, who has also participated in numerous review panels, says that having senior, well established investigators on the panel is important because they can push back against the play-it-safe arguments that creep into the discussion these days. \"Isn't ambition the reason why we are here?\" he says.  \n                Tough choices \n              With lunch over, the reviewers have a difficult conversation ahead: they have to rank the four most competitive proposals. The unique animal-model and childhood-tumour proposals will win the top two slots and almost certainly get funding: these two \"are clearly above the rest\", says the chairman. It is the two proposals jockeying for the third and fourth spots \u2014 the 'deadly' cancer application with a score of 1.3 and another scoring 1.4 \u2014 that are sticky. Slot three has a reasonable chance of getting funded. But slot four will almost certainly land in what is called the 'pay-if' category, meaning that it will be funded only if there is an unexpected budget surplus, a top-slot recipient turns down their grant or a big donor asks to fund that proposal's specific field. In 2009, 151 proposals for ACS Research Project Grants and postdoctoral fellowships landed in this range \u2014 twice as many as usual \u2014 and 45 were eventually funded. The deadly cancer application, which has been submitted twice before, is on its last chance according to ACS rules. \"Every time, [this proposal] has been ranked as 'outstanding'. It's now or never for this one,\" says one panel member. \"The science is strong, but there is this issue of novelty that seems to be dogging this grant,\" says another. Its rival is a first-time submission, with the aim of studying the downstream events in a signalling network important in numerous cancers. It is technically superb, the reviewers agree, but the committee chairman, who reviewed it, has concerns about the applicant's productivity. \"This is well conceived, nicely written and, by the end of it, it's really great science,\" he says. \"But this investigator had an extended postdoc and she had very few first-author publications.\" \"The publication rate out of her postdoctoral laboratory is slower than most other labs,\" points out another panel member. \"The stories that come out [of that lab] are very big. That rate of publication is not unusual.\" The debate reaches a deadlock. Both sets of reviewers feel strongly that their grant deserves a shot.  \n                Final orders \n              The chairman breaks the impasse, arguing that the field will learn more from the signalling-network grant. Heads nod around the room, and the chairman adjusts the scores on the flip-pad to move the signalling proposal ahead of its rival. Everyone seems satisfied \u2014 yet a silent pulse of regret can be felt for the losing application. Finally, the full panel is instructed to 'vote their conscience' and, by secret ballot, rank each of the top four proposals on the basis of the recommended scores. The ultimate order will be decided by a tally of the votes, which only the programme officer will see, but it is very likely to be the same as the order on which the panel has already agreed. Later, back at his office, a panel member who had reviewed the deadly cancer proposal says he was frustrated at not being able to back strong science. \"If I hadn't got my own ACS grant when I did, I would not be here,\" he says. \"This could be a bad omen for this person. Stuff does fall through the cracks. I just hope other funding agencies will pick up this grant.\" He says the current strategy for most researchers is to apply to as many funding agencies, with as many proposals as possible. \"It's brutal, the funding situation has ended lots of careers,\" he says. The vice-chairwoman, who had also promoted the deadly cancer proposal, expresses similar regret. The grant might not have gained \"amazing new information about a particular [molecular] pathway, but it might have been really important for this cancer system, which doesn't have much else out there\". She worries that, in the current economy, this type of science loses out. \"There is routine stuff that has to be done in the [research] system, so how does that get funded?\" she asks. \"That's the thing all of the funding agencies have to think about.\" And that is what the ACS applicants have to think hard about, too. In the last week of July, they were all furiously scanning e-mails notifying them of the panel's conclusions. Just two of them saw, \"I am pleased to inform you that the committee recommended your application be considered for funding\". The other two 'outstanding' applicants had disappointing news. \"Unfortunately, due to budgetary constraints\", the message began, before going on to tell them that they were being considered for the pay-if programme. So how can applicants gain an edge in this environment? The vice-chairman advises investigators to test-run their applications through a peer-review process of their own making, by showing their proposals to colleagues with varied perspectives. She also says that applicants should use their contacts to sniff out the personality of the panel and the nature of the competition. \"Will 10 senior people in the field also be applying? Does the panel like  X   versus  Y   types of approaches? The bottom line is to ask for help, don't try to do it on your own,\" she says. \"It makes me sad that people who are really strong are struggling to get funded,\" says the committee chairman. \"Ultimately, peer review is going to be an imperfect process. But we're not doing a bad job.\" \n                     American Cancer Society \n                   \n                     National Institutes of Health Center for Scientific Review \n                   \n                     National Institutes of Health Peer Review Revealed \n                   Reprints and Permissions"},
{"file_id": "468620a", "url": "https://www.nature.com/articles/468620a", "year": 2010, "authors": [{"name": "Elie Dolgin"}], "parsed_as_year": "2006_or_before", "body": "Bob Klein founded the California Institute for Regenerative Medicine, the biggest state-run research project in US history. What legacy will he leave behind? James Harrison had just stepped out to grab a sandwich when his mobile phone rang. Bob Klein, chairman of the California Institute for Regenerative Medicine (CIRM), was on the line telling Harrison, the agency's legal counsel, to skip lunch and come back to the office right away. It was 23 August, and a district court judge in Washington DC had just issued an injunction barring the use of federal grant money for human embryonic stem-cell research. At that instant, CIRM became the world's largest funder of such research, and needed to issue a public statement. At CIRM headquarters in San Francisco's Mission Bay neighbourhood, executives from legal and communications branches soon gathered around the long, white board table in Klein's corner office. Klein sat at the head. The mood in the room was bittersweet, he says \u2014 a mixture of concern about the setbacks to stem-cell science and to CIRM-funded researchers who also received federal money, and vindication that at least Californian research dollars would continue to flow. The injunction \u2014 coming six years after Klein first convinced voters in California to fund embryonic stem-cell research despite major political and religious opposition \u2014 \"became a huge reinforcer of the conclusion that [CIRM] is a critical safeguard for science\", Klein says. As always, Klein took charge. He listened attentively to the advice of his colleagues and then delegated tasks. He asked the legal team to draw up an analysis of the decision's impact on CIRM grant recipients while he worked with the agency's press officer to issue a public statement that captured the nuanced emotion in the room. The agency \"deplores the decision\", the statement read, although the injunction \"points to the importance of CIRM's California model of sustained funding\". \"It illustrated to me what Bob does best,\" recalls Harrison, \"which is to bring people together and respond to crisis in a very thoughtful and intelligent way.\" On 17 December, however, Klein is stepping down as chairman of the board \u2014 a position he has held since CIRM's inception. He leaves behind an agency with a long list of accomplishments, including more than US$1.15 billion in grants, six new facilities dotted across the state and close to 700 scientific papers (see  'Top earners' ). \n               Click here for larger image \n               Yet many critics say that Klein and CIRM have failed to fully deliver. Despite promises that money borrowed from the state \u2014 at least $6 billion over ten years, when interest is factored in \u2014 would be returned through commercial spin-offs and savings to health care, the first marketable therapies have yet to materialize. Only two CIRM-funded projects have made it to early-stage clinical trials, and neither of these involves embryonic stem cells \u2014 the main impetus for launching the agency in the first place. The embryonic stem-cell clinical trials that have recently been approved in the United States are the product of privately funded research. Klein's critics say his promotion of stem cells' therapeutic promise was zealous and oversimplified. He \"left voters with the impression that people will be jumping out of their wheelchairs and not being diabetic within a year\", says John Simpson, a long-time observer and critic of the agency's governance, who is at the consumer-advocacy group Consumer Watchdog based in Santa Monica, California. \"There's been this constant compulsion for [Klein] to say, 'See, we're delivering, we're delivering', and that's something that's haunted him throughout the whole thing.\" Throughout CIRM's existence, Klein has pulled the strings, maintaining control over nearly every aspect of its structure and science, often to the chagrin of its other leaders. Still, many observers say that no one else could have weathered CIRM's early storms. \"With Bob, there's always this indefatigableness,\" says Douglas Wick, a movie producer and diabetes advocate who worked with Klein to get CIRM funded. \"His personal energy and charisma are so strong, and he has this ability to get punched, stand up and go at it again.\" Klein was a Stanford-educated lawyer who had made millions in real-estate development when, in September 2001, a week after the terrorist attacks on the Twin Towers and the Pentagon, his youngest son Jordan was diagnosed with type 1, or juvenile-onset, diabetes. Klein was devastated. \"It's a life-changing shock when you know your child's life is in danger,\" he says.  \n                The three families \n              Klein wanted to speed the search for cures. \"I thought, 'we've got to get some broader-based research funding'.\" He soon approached the Juvenile Diabetes Research Foundation International (JDRF) in New York to ask how he could help. Klein had some political experience from working with the state and with the national Democratic Party on housing issues. And in 2002, he put it to use, leading the JDRF's efforts in lobbying Congress to pass a $1.5-billion federal funding measure to support diabetes research. The experience of getting that bill approved, Klein says, \"demonstrated to me that dedicated, well-informed, focused patient advocacy could be very effective\". By that time, US President George W. Bush had imposed tough restrictions on federal funding for human embryonic stem-cell research. Convinced that such research offered the best hope for reversing his son's disease, Klein turned his attention to an idea then percolating in California: that the state directly fund biomedical research that federal money couldn't support. \"Getting a Bush override was not feasible,\" Klein recalls. \"So the question then was: what can I do back home?\" Klein teamed up with several other affluent and politically savvy parents of diabetic children \u2014 including movie director Jerry Zucker and his wife Janet, and home developer Tom Coleman and his wife Polly \u2014 and the 'three families', as they called themselves, together with political consultants and lawyers, devised a ballot initiative that would ask California taxpayers to support stem-cell science to the tune of around $300 million per year for ten years. The measure \u2014 which became known as the California Stem Cell Research and Cures Bond Act of 2004, or simply Proposition 71 \u2014 did not require approval or regular appropriations from the legislature. Instead, the proposed initiative relied on long-term state-issued bonds, effectively shielding the endowment from the whims of lawmakers. Klein had experience in bond financing for housing development and quickly took control of the campaign. He personally donated around $1.2 million to get the initiative off the ground, later adding another $3.1 million out of his pocket and raising $30 million more in non-tax-deductable campaign contributions from others. After a star-studded campaign endorsed by the likes of Brad Pitt, Christopher Reeve, Michael J. Fox and state governor Arnold Schwarzenegger, the campaigners gathered at the Millennium Biltmore hotel in Los Angeles on election night, in November 2004, to watch as the votes came in. The proposition passed with 59% approval. That night, says Wick, \"I remember saying to the celebratory gathering that if our daughter is cured of her diabetes, the person who will be more responsible than any living human will be Bob Klein\". But not all the early organizers of Proposition 71 remain enthusiastic about the way Klein led the charge. \"It became Bob's show almost entirely, and there was some friction about that,\" recalls Peter Van Etten, former JDRF president and chief executive. Coleman has not spoken to Klein since the initiative passed, following disagreements over what Coleman viewed as Klein's self-promotional approach. Zucker remains on better terms with Klein, but still feels some lingering resentment. \"If I had to do it over again I'd make the same call to Bob Klein because I don't think the rest of us would have got it done without him,\" Zucker says. But, he adds, \"what I was most unhappy about was the realization after a while that [Klein] wrote the initiative for him to be the chairman. That was something I was too naive to realize. It's shameless almost.\" Under the terms of Proposition 71, the 29-member governing board must include appointees with experience in academia, research, disease advocacy and biotechnology. The chair of the board, meanwhile, must meet a laundry list of mandatory criteria. These include a history of patient advocacy, leadership experience with a government agency, legal experience passing medical legislation and a direct knowledge of bond financing. Scientific expertise is not a requirement. Sound familiar? \"Look at the qualifications. They don't fit a lot of people,\" notes board member David Serrano Sewell, a lawyer with the San Francisco city attorney's office. Klein defends the job qualifications that he wrote into the statute. \"I wrote the job description based on what I thought would be the challenges. I'm trained as a lawyer, so I'm going to think that legal is an important criterion. I'm trained in finance, and I'm going to think that finance during the projected period of economic distress for the state is going to be very important. So I wrote those requirements in knowing that if no one else could qualify, I could meet those. But someone had to meet those criteria.\" Many people maintain that Klein was simply the best person for the job. \"He lived and breathed the mission,\" says Jeannie Fontana, executive director of patient advocacy at the Sanford-Burnham Medical Research Institute in La Jolla, California, who has often acted as a stand-in on the CIRM board. Bernard Siegel, director of the non-profit Genetics Policy Institute in Palm Beach Gardens, Florida, adds: \"He was able to blend in his passion with his networking skills, which are formidable, with this knowledge of bonds. When you put all this together he was able to create a state agency with unprecedented resilience that has been extraordinary successful.\" That resilience would be tested almost immediately after Proposition 71 passed. Critics of embryonic stem-cell science mounted legal challenges against the agency; as a result, bond sales were frozen until the court cases were settled. Klein, thinking ahead, had written a workaround into the bylaws. He was able to take out loans from elsewhere on the basis that the bonds would eventually be paid \u2014 a little-known instrument called a 'bond anticipation note'. Buoyed by these and other loans from the state's general fund, Klein managed to keep administrative operations going and fund the agency's first training and research grants even before the lawsuits were eventually thrown out, in May 2007. In the first two years of legal and financial setbacks, the board was struggling to find a president to lead the day-to-day operations of the agency. Zach Hall, then an associate dean at the University of Southern California School of Medicine in Los Angeles, was brought in as an interim president. He had the administrative chops, having previously directed the US National Institute of Neurological Disorders and Stroke. And as the lawsuits dragged on, Klein asked Hall to stay on full time. Hall agreed. But it wasn't long before he and Klein butted heads. One of the main points of contention revolved around the agency's scientific strategic plan \u2014 a policy measure adopted in December 2006. Some maintained that the president's office alone should set the agency's scientific agenda, yet Klein made sure that he and several board members had a seat on the subcommittee that crafted the plan. As a result, many people felt that the original strategic plan, as well as last year's update approved by Hall's successor, the Australian assisted-reproduction pioneer Alan Trounson, focuses too heavily on clinical applications at the expense of more fundamental basic science. For example, the strategic plan allocates 16% of CIRM's $2.4 billion projected research budget to what it calls \"innovation science\", exploratory open-ended research, and more than half is allocated to \"mission-directed science\", which is focused on developing therapies. Joel Adelson, a health-policy researcher at the University of California, San Francisco, who interviewed 17 of CIRM's key stakeholders and co-wrote an independent review of the agency earlier this year 1 , says that Klein's disagreements with Hall, and to a lesser extent with Trounson, stemmed from Klein's insistence on being involved in every aspect of CIRM's operations, including the scientific decisions. \"Klein has in effect acted like the chief operating officer beside Trounson and beside Hall, and I can only say that this looks like it must have been very uncomfortable for these guys,\" Adelson says. \"It's an unusual situation,\" says Trounson. \"And if you ask me what I prefer, I prefer the simple situation where the president is in charge of all management and reporting to a board on policies. But it's bifurcated, and it was set up that way, so you don't have a choice.\" (Hall declined to comment for this story.)  \n                Stick to the vision \n              Klein defends his march to the clinic as adhering to the vision he presented to voters on the campaign trail. And although some basic scientists take issue with CIRM's funding allocations, most have come to embrace the translational emphasis. For example, Jeanne Loring, a CIRM-funded stem-cell researcher at the Scripps Research Institute in La Jolla, says that Klein \"has taken purely academic scientists who didn't give a damn about the clinical applications of their work, and turned them into scientists who will now talk, without a trace of embarrassment, about the benefit of their research to patients\". Patient advocates praise Klein as well. \"He's an historic figure with real genius in terms of moving biomedicine forward,\" says Jeff Sheehy, a CIRM board member and director for communications at the University of California, San Francisco's AIDS Research Institute. \"He's as good as they get if not better.\" Developments in both science and politics have challenged CIRM's original rationale. In November 2007, researchers in Japan and Wisconsin reported that human skin cells could be coaxed in the laboratory to form embryonic-like pluripotent stem cells 2 , 3 . This discovery provided a new path to patient-specific stem cells without the need for embryos. Then, a year and a half later, US President Barack Obama issued an executive order widening the scope of federal funding for embryonic stem-cell research, easing the need for state and private initiatives. But Klein says CIRM's mission goes beyond simply serving as a stopgap for embryonic stem-cell research during Bush-era restrictions, stressing that its focus on translational medicine distinguishes the California agency from the National Institutes of Health (NIH). For instance, he points to the disease team grants, launched last year, that require recipients to have a strategy for landing an investigational new drug application within four years. \"The purpose of CIRM is not science for science's sake,\" Klein says. \"The purpose of CIRM is medical science with a plan to drive that science all the way through to therapies.\" Marie Csete, a former chief scientific officer at CIRM, says that Klein embraced the new 'induced pluripotent' stem cells. \"There was a transient moment where hanging on to embryonic stem cells was important, but he very quickly grasped that they were only one tool in the toolbox of regenerative medicine,\" she says. After dedicating nine years and millions of dollars to the agency, Klein says it's time to step aside and focus on family issues \u2014 his son is still battling diabetes, he lost his mother to Alzheimer's disease last year and his wife is currently undergoing chemotherapy for breast cancer. Agency insiders are sad to see him go. \"The joke is to clone Bob Klein,\" says Lynn Harwell, CIRM's deputy to the chair for finance, policy and outreach. She pauses before quickly adding: \"Although of course we don't condone cloning.\" Geoffrey Lomax, CIRM's senior officer for medical and ethical standards, commends Klein's many accomplishments, but thinks that fresh leadership might help to clarify boundaries between the board and the staff. \"As Mr Prop 71, Bob's relationship to the organization is unique,\" Lomax says. \"I would suspect that there might be cleaner lines with someone coming in who doesn't bring that intimacy with the proposition.\" Depending on who replaces him \u2014 nominations were made earlier this week by state officials including Schwarzenegger, and the new chair will be elected by the board on 15 December \u2014 Klein's departure might also trigger the president to leave, thereby causing a complete overhaul of CIRM's leadership. Trounson says he told Schwarzenegger that he would like that next chairperson to be \"somebody who's in the delivery end of the spectrum \u2014 that is, somebody who has worked with the biotech or pharmaceutical industry\". But as this issue was going to press, the leading internal candidate to replace Klein, many say, is vice-chair Art Torres, a former state senator and chairman of the California Democratic Party. Torres and Trounson reportedly cannot stand each other. Trounson notes that Torres is \"a politician, so he's in that end of the spectrum\". Torres, for his part, declined to comment on his relationship with the president. Whoever takes the reins will continue to deal with the fallout from the federal injunction. But Klein leaves the agency in strong legal and scientific positions, with several projects \u2014 including a few that rely on embryonic stem cells \u2014 likely to enter early clinical development in the next few years. Gerald Levey, an ex-board member and former dean of the David Geffen School of Medicine at the University of California, Los Angeles, says that Klein's record at CIRM stands for itself. \"If he did nothing else with his life, he did a wonderful thing.\" But Klein vows to return to the agency's service in 2014 to help CIRM secure another $3 billion commitment from California's taxpayers. \"I have four years to put my life back into a position where I can commit myself to another campaign,\" says Klein. He has no plans to retire or stop the search for a cure for his son's diabetes: \"You're either learning and growing or you're dying, and I want to continue to learn and grow.\" \n                     The California Institute for Regenerative Medicine \n                   \n                     The Genetics Policy Institute \n                   \n                     California Stem Cell Report \n                   Reprints and Permissions"},
{"file_id": "466914a", "url": "https://www.nature.com/articles/466914a", "year": 2010, "authors": [{"name": "Alla Katsnelson"}], "parsed_as_year": "2006_or_before", "body": "The skeleton may provide more than just structural support. Alla Katsnelson investigates the rise of bone as a metabolic regulator. At first, Patricia Ducy was not particularly fond of the mice she found herself working with in 1994. One of the first strains genetically engineered to lack a bone-related protein \u2014 osteocalcin \u2014 their cages reeked of urine. And when Ducy dissected the mice, their bellies overflowed with fat. \"They are disgusting,\" Ducy remembers telling her supervisor. \"They pee so much; I hate them!\" Ducy was a postdoc at the University of Texas  MD Anderson Cancer Center  in Houston. Working with junior professor Gerard Karsenty, she was using the mice to test the idea that osteocalcin helped regulate the way calcium phosphate is deposited onto bone cells. Although the fat, smelly mice grew slightly stronger and thicker bones than ordinary mice, osteocalcin didn't seem to have any effect on mineralization. Its function remained a mystery. However, for Karsenty, who had originally trained in Paris as an endocrinologist, there was something familiar about the unfortunate rodents' symptoms: they seemed to be diabetic. Maybe, he thought, the osteocalcin-deficient mice indicated a direct link between the skeleton and energy metabolism, which goes askew in diabetes. The connection made sense to Karsenty. The body's maintenance of bone is a highly dynamic process orchestrated by two types of cell. Osteoblasts form and shape bone tissue, and osteoclasts break it down. Karsenty reasoned that this energy-intensive back-and-forth process of remodelling might be coupled by necessity to metabolism. Adult humans who don't eat \u2014 such as those with anorexia \u2014 lose bone mass, and obese people have beefy bones. Obesity even seems to protect some women from osteoporosis. The idea was radical. Most scientists who study bone believed the role of the skeleton was limited to support and mineral exchange, and the possibility that it could interact with other organ systems was unheard of. But over the past 15 years, Karsenty and Ducy, who are now married and run collaborating labs at  Columbia University  in New York, have gradually shifted the bulk of their work towards testing the hypothesis, which could have clinical implications. \"Gerard has really been one of the most creative thinkers in the bone field for many years,\" says Sundeep Khosla, an endocrinologist at the Mayo Clinic in Rochester, Minnesota. Still, Karsenty is not without detractors, who say he has overstated the case for a bone\u2013metabolism connection. And because almost no one in the field has worked directly on the idea, \"the question people have is validation in another lab\", says Khosla. That is beginning to change. In a study published last month 1 , bone researchers at  Johns Hopkins University  in Baltimore, Maryland, largely stumbled on findings that affirm bone's role in metabolism. It has taken a long time, however, to put the pieces together.  \n                The missing link \n              Karsenty put his osteocalcin-deficient mice aside for a time in the late 1990s and plunged into his new hypothesis using a protein with a well-established metabolic role: leptin. The gene for leptin \u2014 which is secreted by fat cells and suppresses appetite \u2014 had been cloned, and by then, a mouse in which the leptin gene was deleted (a knockout) was commercially available. More intriguing for Karsenty, leptin had been found only in animals with bony skeletons, suggesting that it evolved with bone. Perhaps, he thought, the hormone also serves as a 'molecular link' connecting energy metabolism to bone's growth and decay. When the leptin-knockout mice arrived in late 1998, it was clear by a quick glance at an X-ray that their bones were denser than those of wild-type mice. By then, Karsenty and his group had moved to Baylor College of Medicine, also in Houston, and in 1999 Karsenty brought in another postdoc, endocrinologist Shu Takeda. Initially, Takeda says, he didn't believe the idea would pan out, but the findings supported Karsenty's vision. The knockout mice had a 40\u201350% increase in bone mass compared with wild-type mice, suggesting that leptin blocks bone growth. But leptin didn't seem to be signalling directly to bone cells. The researchers had to infuse it into the brain to return the knockouts' bones to normal 2 . Karsenty's lab spent the next decade filling in details of the signalling pathway that connected leptin, osteocalcin and bone (see graphic), piecing the puzzle together largely by knocking out different genes. Most homeostatic processes are regulated by the brain, and this one was no exception. Leptin exerts its brake-pedal effect on bone growth by inhibiting synthesis of the neurotransmitter serotonin in the brainstem. This in turn dials down bone building \u2014 which is regulated by the sympathetic nervous system, a neuronal pathway that controls many unconscious bodily functions 3 . But the endocrine system works by feedback loops. If a hormone released from fat cells can affect bone metabolism, the scientists reasoned, bone must in turn regulate energy metabolism. And when they went back to their osteocalcin knockouts they confirmed that the mice were glucose intolerant, insulin resistant, and had fewer insulin-producing \u03b2-cells than normal mice \u2014 they were diabetic. Karsenty concluded that an activated form of osteocalcin acts as an energy-regulating hormone 4 . Click here for larger image The idea still hadn't gained much traction when Thomas Clemens, a bone biologist at Johns Hopkins University, came to study it. In 2006, one of his graduate students, Keertik Fulzele, became interested in the conflicting medical literature on whether or not diabetic patients, who have poor insulin-mediated control of glucose, are prone to osteoporosis. Fulzele suggested knocking out the insulin receptor specifically in osteoblasts. \"I did everything I possibly could to discourage him,\" says Clemens, who was convinced that they'd see an uninformative effect, but Fulzele insisted. Like the osteocalcin and leptin knockouts, the insulin-receptor-knockout mice were fat and insulin-resistant, but unlike the other two mutants, they had low bone mass. It was puzzling, Fulzele recalls. \"We spent about six or eight months arguing about it and doing all sorts of controls.\" Just then, the Karsenty lab's 2007 paper 4  was published, and for both Clemens and Fulzele, the light dawned. Bone and metabolism were connected. Coincidentally, Karsenty and his team had created precisely the same knockout, and emerged with a similar story. Eating promotes insulin release, which activates bone remodelling. Bone formation by osteoblasts produces more osteocalcin, and bone resorption (or break down) by osteoclasts activates the hormone, releasing it from bone into the bloodstream. Osteocalcin in turn acts on pancreatic \u03b2-cells to boost insulin production and hence glucose uptake in the body. The two studies 1 , 5  were published together in  Cell   this July. Karsenty and Clemens suggest that this bone pathway may facilitate glucose absorption on a different time frame from the well-established insulin pathway, by which a jump in glucose levels spurs the quick release of insulin. That direct pathway would provide a rapid-response system for glucose uptake, whereas the bone pathway might oversee the longer-term equilibrium. Findings from the osteoblast insulin-receptor knockouts 1 , 5 , \"sort of complete the loop that insulin is really one of the feedbacks on the bone-remodelling system\", says Clifford Rosen, an osteoporosis researcher at the Maine Medical Center Research Institute in Scarborough. \"I think it was very important that Tom came along,\" says Karsenty. \"Novelty comes with the extra price of scrutiny, and the best way to address scrutiny is if your neighbour gets the same results as you.\" The link between the skeleton and glucose metabolism has more than just theoretical implications. The model has \"great clinical relevance\", says Rosen. The most tantalizing possibility in the short term, he says, is that osteocalcin could be used as a way of promoting insulin secretion to treat diabetes. With that in mind, Karsenty spun out a company called Escoublac in late 2007. Head-quartered in Cambridge, Massachusetts, it aims to capitalize on the connections between osteocalcin and bone growth, insulin sensitivity and weight gain.  \n                Thrown a bone \n              Despite the therapeutic potential, several pieces of the puzzle are still missing. \"I guess if I wasn't in it, I'd be sceptical too,\" says Clemens. The Karsenty lab has yet to close the leptin loop and identify exactly how bone regulates leptin levels. And so far, neither group has identified an osteocalcin receptor. Both are now examining how osteocalcin might prompt \u03b2-cells to stimulate insulin production. The main outstanding issue, however, is how closely these studies in mice reflect the physiology of humans. \"My personal conviction is that he is right overall, but that it may not be a key component of the human regulatory pathway,\" says Roland Baron, a bone biologist at Harvard School of Dental Medicine in Boston, Massachusetts. \"Part of the problem is that [Karsenty] has a tremendous tendency to overstate his findings. And that makes a lot of people uncomfortable.\" Karsenty says he isn't surprised that his ideas garner criticism. \"I will say nothing to them \u2014 I will go back to the lab and do more experiments to weaken the resistance.\" Rosen says that the work is pushing at the edges of knowledge in valuable ways. \"Even if he is 50% right, he has done a tremendous service to the field.\" Yet the other thorn in Karsenty's theory, he says, is that women who take medication for osteoporosis to prevent bone resorption \u2014 bisphosphonates, for example, or the newer antibody-based drug Prolia (denosumab) that was approved in Europe and the United States in June \u2014 don't seem to have significantly altered glucose levels. This could mean that any control bone has on insulin and glucose is minor in humans, says Rosen. Karsenty argues that such drugs decrease osteocalcin levels and insulin secretion resulting in higher glucose levels, but that the change is subtle because they are not potent enough to whack the metabolism far out of balance. Also, he notes, studies in the past few years have reported a link between blood osteocalcin levels and insulin sensitivity in humans 6 . Although supportive of a link, says Khosla, those observations have largely been made in overweight or obese individuals, who tend to be insulin resistant and have low bone turnover to begin with. \"So is it cause and effect, or just two things that happen together but are not causally related?\" Clinical data exist from studies on the osteoporosis drugs, but without a prospective study examining the relationship between osteocalcin and obesity or diabetes in humans, it will be impossible to know for sure. Karsenty is starting to collaborate with clinical researchers to gather such data. \"This has to be proved right or wrong quickly,\" says Clemens. \"There are lots of observations in the basic literature that are not repeated because they are not going to be changing clinical practice.\" But the implications for current patients make this important, he says. \"This won't be left behind.\" \n                     Gerard Karsenty's website \n                   \n                     Thomas Clemens's website \n                   Reprints and Permissions"},
{"file_id": "4661036a", "url": "https://www.nature.com/articles/4661036a", "year": 2010, "authors": [{"name": "David Cyranoski"}], "parsed_as_year": "2006_or_before", "body": "Stymied in the search for genes underlying human neuropsychiatric diseases, some researchers are looking to dogs instead. David Cyranoski meets the geneticist's new best friend. Solo takes a double dose of Xanax (alprazolam) for his nerves during the 4 July festivities in the United States. That is in addition to the antidepressant, fluoxetine or amitriptyline, that the 11-year-old border collie takes year-round. Fireworks just set him off, as do thunderclaps, gunshots \u2014 practically any explosive sounds \u2014 sending him into nervous fits. Panting and drooling with eyes dilated, he desperately searches for a place to hide. If another dog is nearby, he might attack. \"It's called anxiety redirection,\" says Melanie Chang, Solo's owner and an evolutionary biologist at the  University of Oregon  in Eugene. As a postdoctoral researcher at the University of California, San Francisco, Chang helped to collect hundreds of border-collie DNA samples, including Solo's, as part of a project studying the genes for noise phobia. She estimates that at least 50% of collies suffer from it, with 10% severely affected, sometimes injuring themselves or others in response to loud noises.  Steven Hamilton , a psychiatrist at the University of California, San Francisco, who runs the project, says that he sees parallels between the dogs' panic and human anxiety. And the same drugs work in about the same proportion of cases for man and beast. \"It is easy to see similarities,\" he says. A growing number of projects like Hamilton's are underway to both help suffering dogs and untangle the roots of human neuropsychiatric disease. The hunt for genes causing psychiatric problems in humans has been \"hard work with slim pickings\", says Jonathan Flint, a geneticist at the  Wellcome Trust Centre for Human Genetics  in Oxford, UK. This is partly because human genomes are complex and these disorders are hard to diagnose consistently. But owing to 200 years of selective inbreeding, dogs have a bevy of breed-specific behaviours, and their genomes make it relatively easy to track down the genes responsible. \"They are the only naturally occurring models of psychiatric disorders, and perfect for genetic mapping and cloning. It's just beautiful,\" says Guoping Feng, a mouse geneticist at the Massachusetts Institute of Technology in Cambridge, who is setting up collaborations with dog researchers. Border collies were bred to herd grazing animals and to hear the calls of their masters from great distances. This, some have reasoned, might have produced hearing so sensitive that loud noises overwhelm some of the animals \u2014 inducing something akin to an anxiety disorder in humans. \"In general, a lot of anxiety probably resulted from a long period of selection for dogs that can respond to human social cues,\" says Chang. The provenance of other traits is less clear. Dobermann pinschers, for example, were bred to be faithful watchdogs but often have fixations and quirks akin to obsessive\u2013compulsive behaviour. And Dalmatians, bred for speed and endurance \u2014 probably so they could run with horses \u2014 tend to be aggressive. Click here for larger image Whether certain canine conditions arose by chance or are an unintentional outcome of selection for a specific quality is a matter of speculation. But behaviour problems are definitely frequent. Nicholas Dodman, an animal-behaviour specialist at Tufts University in North Grafton, Massachusetts, estimates that, at minimum, 40% of the 77.5 million dogs owned in the United States have some kind of behavioural disorder. Pet pharmaceuticals, including psychotropic drugs, are a thriving market. And sadly, many dogs with such problems are euthanized as a result of their temperament. Researchers have good reason to believe that dogs will give up their genetic secrets more easily than humans. A study this year, for example, showed that variants at six locations in the  dog genome  could explain 80% of the variation in dog body size 1 . In contrast, 294,831 common human variants, considered simultaneously, explained only 45% of height differences between humans 2 . But if the genetics of height is so different in dogs and humans, one might wonder why the genetics of anxiety, compulsion or aggression would be similar. Patrick Sullivan, a geneticist at the University of North Carolina in Chapel Hill, says that \"behaviour that appears intriguingly similar in human and another species could have a completely different genetic architecture\", meaning that the same trait could map to different genes or to different parts of the brain. Proponents of canine studies suggest, however, that dog genes might hint at the pathways involved in human disease, and that might be enough.  \n                Sleeping dogs don't lie \n              At least one success story shows that studies in dogs can lead to answers in humans. For decades, researchers vainly sifted through the DNA of human narcoleptics to find the genes behind the sleep disorder. But many genes were involved, environmental factors were inconsistent and no clear mechanism emerged. \"People were arguing whether it was an autoimmune disease, but no one knew what to do next. It was too difficult,\" says Emmanuel Mignot, a sleep researcher at the Stanford University School of Medicine in Redwood City, California, with a background in molecular pharmacology. But Dobermann pinschers are often susceptible to narcolepsy, and they held the key. In 1989, Mignot started to use classical genetic techniques to breed narcoleptic Dobermanns and trace the inheritance pattern of the disorder. Without the benefit of modern genetic and genomic tools it took him ten years to zero in on the mutation that caused the disease, in a gene called hypocretin receptor 2 (ref.  3 ), which regulates the brain's uptake of the neurotransmitter hypocretin, also known as orexin. Click here for larger image Mignot did not find the same mutation in the corresponding human gene, but he did find changes in the hypocretin pathway 4 . \"We started to measure hypocretin in cerebrospinal fluid. In narcoleptics, it was gone. It was striking,\" says Mignot. Researchers are homing in on human gene mutations that lead to hypocretin depletion and to narcolepsy 5 , and drug companies are targeting hypocretin as a possible lead in the search for insomnia treatments.  \n                Same dogs, new tricks \n              Since Mignot published his studies, the canine genome has been sequenced 6 . That has ultimately allowed researchers to quickly and easily compare the genomes of hundreds of dogs by looking at single nucleotide polymorphisms (SNPs) \u2014 single-letter changes in the genome that act as markers for inherited blocks of DNA. The genome-wide association studies (GWAS) that researchers can carry out using these markers are much simpler in dogs than in humans. Most dog breeds are extremely homogeneous; individual animals in the same breed share significantly larger DNA blocks than are shared by any two humans. That means that researchers can look at fewer SNPs and fewer individuals to find a block of DNA that associates reliably with a disease 7 . According to Kerstin Lindblad-Toh of the  Broad Institute  in Cambridge, Massachusetts, human GWAS might require 5,000 individuals with a trait of interest and 5,000 controls without it to show that the trait is convincingly associated with a particular genome region. Dog studies can sneak by on as few as a hundred cases and a hundred controls. And a study requiring hundreds of thousands of SNPs in humans might need only 15,000 in canines. GWAS have proved successful in finding the genes for several dog traits that are relevant to human diseases, including the bone disorder osteogenesis imperfecta \u2014 pinned to the gene causing stubby legs in dachshunds 8  \u2014 and the autoimmune disease systemic lupus erythematosus, which was shown in a study published this year to be controlled by five separate genes in Nova Scotia duck-tolling retrievers 9 . And more are coming. Anne-Sophie Lequarr\u00e9, a veterinarian at the University of Li\u00e8ge in Belgium, coordinates the European dog-genetics initiative  LUPA . The project, started in 2008 with a \u20ac12-million (US$15.4-million) budget, brings together some 100 researchers to study single-gene and complex disorders \u2014 including cancer, cardiovascular disease and neurological disorders \u2014 by genotyping 10,000 dogs. Researchers involved will soon publish findings on two mutations in dog genes that cause disorders corresponding with human disease, says Lequarr\u00e9: \"The first results really show that once you find a mutation [related to a disease] in dogs, 90% of the cases involve the same gene in humans.\" Compulsive disorders may be among the first successes in unravelling human behavioural conditions through dogs. More than 60 studies on genes in mice thought to have a role in human obsessive\u2013compulsive disorder (OCD) have so far failed to find significant, reproducible associations 10 . But there are lots of dogs with obsessive behaviour. A high proportion of bull terriers, for example, chase their tails relentlessly. Many large-breed dogs, such as Dobermanns, German shepherds, Great Danes and golden retrievers, chew their flanks or lick their legs until they lose hair, develop lesions and in some cases cripple themselves \u2014 a habit some compare with obsessive hand washing and other rituals of people with OCD. Click here for larger image In January, Lindblad-Toh and Dodman reported a link between canine compulsive disorder and a region on the dog's chromosome 7 (ref.  11 ). Their study was based on an analysis of 14,700 SNPs in the genomes of more than 90 compulsively chewing Dobermanns and about 70 controls. It linked the behaviour to variations in a 400-kilobase-long stretch of DNA. The connection between the variant that confers risk and the compulsive behaviour is not airtight, but it is good: 60% of the dogs that chewed their flanks, blankets and anything else they could get their teeth on had the variant, compared with 43% of those with milder chewing compulsion and just 22% of those with no signs of compulsive behaviour. One gene in the targeted region has already captured the imaginations of other researchers.  CDH2   encodes the protein cadherin 2, which is involved in forming connections between neural cells. Deanna Benson, a neuroscientist at  Mount Sinai School of Medicine  in New York, says the possibility that cadherins are involved in OCD has inspired others. Feng, who makes mouse models for OCD, is exploring the link. Last autumn, he and Lindblad-Toh struck up a collaboration to find brain circuitry related to compulsion that is shared by mice, dogs and humans. Feng is now knocking out  Cdh2   function in specific brain regions of mice to test whether that produces OCD-like behaviours.  \n                Dogged progress \n              Lindblad-Toh is now seeking a tighter genetic fit to human OCD. Researchers approach dog genetic studies in two steps: first narrowing in on a large DNA chunk within one breed and then looking for overlap with that region in the DNA of dogs of other breeds with the same disease. Mignot used narcoleptic dachshunds to home in on the mutation expressed by his sleepy Dobermanns. And by comparing DNA loci in flank-sucking German shepherds and tail-chasing bull terriers, Lindblad-Toh hopes to narrow the implicated region on chromosome 7 to a more manageable 10 kilobases. Similarly, Hamilton intends to broaden his noise-phobia studies from border collies to bearded collies and Australian shepherds that show similar anxieties. But canine genetics is challenged by some of the same issues that have foiled researchers studying human illnesses. Diagnoses for neuropsychiatric disease are slippery. Schizophrenia, for example, could represent a collection of many different disorders, each with separate genetic and environmental triggers. And if the subjects grouped by symptoms have different underlying diseases, GWAS can become confused. \"A few dogs can spoil a cohort,\" says Lequarr\u00e9. She cites an epilepsy study that was not delivering any significant correlations. The researchers later found that some of the dogs in the disease group actually had a form of late-onset epilepsy that was different from that being studied. \"Phenotyping is crucial. You need to have dogs that have exactly the same disease,\" she says. LUPA is making an effort to clarify diagnosis. To identify neurological disorders consistently, the group selected veterinarians who follow standard procedures in parsing dog temperament. Standardization is the right approach, says Hamilton. For his work on collies, he leads owners through a 24-page questionnaire that elicits objective observations. \"We don't ask, 'Is your dog aggressive?' We ask, 'When there is a thunderstorm, what does your dog do?'\" LUPA's neurological-disorder division is focusing on aggression in the English cocker spaniel and English springer spaniel, both given to sudden fits of rage. The researchers hope that the studies will identify mutations in genes related to human bipolar disorder, schizophrenia and other mental disorders involving aggression. Click here for larger image Excitement over dog models has been spreading. At the University of Tokyo's Laboratory of Veterinary Ethology, Yukari Takeuchi has collected DNA samples from 200 Japanese shiba inu and 200 labrador retrievers to look for the genes underlying the former's aggression and latter's lapses in concentration. It could help solve a practical problem, she says. Distracted retrievers do not make good guide dogs, and knowing the gene variant responsible could help breeders to limit the trait in their stocks 12 . Whether or not the dog studies live up to their promise for understanding and relieving human suffering, they are sure to benefit pets. Breeders are already taking notice of some of the gene variants that ravage certain breeds. For better and, in terms of scientific research, for worse, through screening and more selective breeding, the next generations of border collies will probably have fewer anxiety-ridden dogs such as Solo who can be studied. But Elaine Ostrander, dog geneticist at the National Human Genome Research Institute in Bethesda, Maryland, is confident that dogs have much to offer human health beyond the pleasure of warm fur and a cold, wet nose. \"For 10,000 years, dog has been man's best friend. When we transitioned to hunter-gatherer, when we switched to agrarian, they were there. Now, in the genomic era, dog is serving man again by helping us identify genes,\" she says. David Cyranoski is  Nature 's Asia\u2013Pacific correspondent. \n                     Canine Behavioral Genetics Project, UCSF \n                   \n                     LUPA \n                   \n                     The NHGRI Dog Genome Project \n                   Reprints and Permissions"},
{"file_id": "467022a", "url": "https://www.nature.com/articles/467022a", "year": 2010, "authors": [{"name": "Amanda Mascarelli"}], "parsed_as_year": "2006_or_before", "body": "When oil stopped gushing into the Gulf of Mexico, the ecosystems under assault started on a long road to recovery. Amanda Mascarelli meets the researchers assessing their chances. Oil has been here. It has blasted this tiny barrier island on the southeastern edge of Louisiana, turning the entire rim of wetland vegetation yellow and the surrounding soil black. The flagging marsh grass stems are tinged dull brown, as if they've been dipped in turpentine. As for the animals living in the water below \u2014 well, it is hard to know their story. Kim de Mutsert, a postdoctoral coastal ecologist from  Louisiana State University  (LSU) in Baton Rouge, is here on a blistering July day to find out. The juvenile crabs, shrimp and fish she is collecting spawned tens to hundreds of kilometres away on the continental shelf in April and May \u2014 just when the  Deepwater Horizon  well was spilling some 10 million litres of oil a day into the Gulf of Mexico. Their eggs and larvae drifted for weeks offshore, bathed in oily water, before the juveniles at last took refuge in the shallow coastal estuaries, where they will mature. De Mutsert is here to discover what harm they have sustained, and what scars will be left on their offspring and on the generations to come. By the time the Deepwater Horizon well was finally plugged on 15 July, it had spewed some 750 million litres of crude oil into the Gulf and earned the title of the biggest accidental marine oil spill ever. Much of the oil has already vanished from surface waters, and so far the most visible effects have been oiled seabirds, turtles and salt-marsh fringes. Drawing on lessons from past oil spills, many scientists agree that ecosystems have a remarkable capacity to heal. \"This is not the end of the Gulf of Mexico,\" says George Crozier, a marine biologist and head of the  Dauphin Island Sea Lab  in Alabama. But history provides an incomplete reference. Aside from its unprecedented size, the spill was the first to release a massive amount of oil 1.5 kilometres down on the sea floor and the first involving widespread use of oil dispersants below water. On top of that, the coastal areas hit hardest \u2014 the Louisiana wetlands \u2014 are already under acute stress from subsidence, erosion and the damage caused by Hurricane Katrina in 2005. All this means that the long-term consequences for life in the deep water and coastal ecosystems in the Gulf remain unknown. Researchers worry that toxic components in the oil could wipe out generations of some species, but there is no way to predict the effects. And the oil could linger both in the deep ocean and in sediments for months or years, slowly bleeding more pollutants (see  'The unknown fate of oil' ). \"It's a huge lab experiment, but there are no controls,\" says Harriet Perry, a fisheries biologist at the University of Southern Mississippi's  Gulf Coast Research Laboratory  marine-science centre in Ocean Springs, Mississippi. \"That's frightening to scientists who always have a control to measure against.\" All this means that when the oil stopped, the work for researchers began. \"If you look at this as the basis of an experiment, that's  t -zero,\" says William Hawkins, director of the Gulf Coast Research Laboratory. \"Now the assessment of damage can start in earnest. We can start to determine the process of recovery.\"  \n                Probing the wetlands \n              At first, scientists worried that the spill would devastate Louisiana's vast wetlands, which cover some 12,000 square kilometres. Around 75\u201390% of creatures in the northern Gulf of Mexico spend part or all of their lives in the estuarine waters surrounding these wetlands, which are enriched with nutrients from the Mississippi River. Animals such as oysters and bay anchovies are permanent residents, whereas others such as shrimp move out to sea to spawn and complete their life cycles. Researchers were concerned that important habitat for these creatures could be lost if the oil poisoned wetland root systems \u2014 essential for holding sediments in place \u2014 and accelerated erosion and subsidence. \"Initially, my worst fear was that we would lose the offshore habitat, which is critical, but that we would also lose the nursery grounds, the marshes,\" says Perry. But those fears may not be realized. Thanks to favourable winds and currents, aggressive oil skimming and trapping at the surface, and the likelihood that a substantial portion of oil is still drifting in underwater plumes, it seems that the damage so far may be limited. In late August, more than 212 kilometres of Gulf coastline were moderately to heavily oiled, according to an official government estimate. By contrast, a substantial area of some 75 square kilometres of wetlands are lost to erosion and drainage every year. \"Back of the envelope [calculations] suggest that what we're going to lose from this spill is nowhere close to the background rates of wetland loss,\" says Alex Kolker, a coastal geologist with the Louisiana Universities Marine Consortium in Chauvin. The most serious damage may have been done out in the ocean \u2014 to organisms that were spawned there and exposed to submerged and dispersed oil as embryos and larvae. To measure such effects, de Mutsert and research technician Elise Roch\u00e9 repeatedly cast a giant trawl net into the water at oiled sites, pulling in juvenile crustaceans and fish such as anchovies, catfish and menhaden. De Mutsert plans to study their size, growth rates and populations and compare them with baseline data that date back to the 1960s, collected by the Louisiana Department of Wildlife and Fisheries. At the Gulf Coast Research Laboratory, Perry is also assessing the damage by collecting, identifying and measuring larvae and juveniles of various species and recording their numbers. She peers into the microscope and reveals a blue crab larva ( Callinectes sapidus ), about the size of a pinhead, with droplets of dispersed oil wedged between its skin and outer shell. Perry suspects that the numbers of blue crab larvae, which spawn in open waters 15\u2013150 kilometres offshore, will drop because of their exposure. She estimates that some 40% of offshore larval grounds across the north-central Gulf were exposed to oil. Scientists are keeping a close watch on variables that might affect life in the open ocean, including depleted oxygen levels caused by a feeding frenzy from oil- and gas-eating microbes, and the unknown effects of dispersants, which break the oil into droplets but may keep it suspended in the water 1 , 2 . Among the greatest concerns is that compounds in oil called polycyclic aromatic hydrocarbons (PAHs) could have long-term sublethal effects on marine organisms that were at the peak of their spawning season when the spill began. PAHs could stunt growth, an effect that could ripple through the ecosystem: smaller organisms are picked off by predators at a young age, leaving less food for larger fish such as red snapper and bluefin tuna. This could have a direct economic impact on Louisiana's fisheries \u2014 which account for 75% of US fisheries productivity in the Gulf of Mexico. Exposure to PAHs early in an organism's life cycle can also lead to infertility and a host of developmental problems, says Jeffrey Short, an environmental chemist based in Juneau, Alaska, who works with the marine conservation organization Oceana. Short was working for the National Oceanic and Atmospheric Administration in 1989 when the  Exxon Valdez   oil tanker dumped 42 million litres of oil into Prince William Sound in Alaska, and he led much of the research into its effects. Months after the spill, pink salmon eggs that were spawned in intertidal streams where oil was present showed depressed survival compared with those in streams with no oil 3 . And rare mutations that caused salmon to grow an extra fin or an enlarged heart sac, for example, began to turn up in a small fraction of the population, says Short. \"It was really difficult to detect because the nature of the damage was that they didn't die at the same time of the same thing,\" he says. \"But it turned out to be an extraordinary impact.\" Some of these effects subsided after about five years, Short says. Results from laboratory studies showed that as little as one part per billion of PAHs can damage pink salmon eggs and that similar concentrations affect herring eggs 4 . A study performed under the US government's Natural Resource Damage Assessment programme calculated that oil exposure led to a 52% loss in larval herring following the spill 5 , although an Exxon-funded study did not support that link 6 . Short says that it is still too early to tell how PAHs might affect Gulf organisms in either shallow or deep water. The effects will depend on the concentration of oil, how far it has travelled and the exposure time \u2014 all unknowns. He remains very concerned about the exposure of animals that inhabit the depths and migrate upwards at night to feed, as well as animals at the sea floor. \"It's going to be tough to get those data because it's just so hard to work there.\" A throng of reef-dwelling organisms live on the edge of the Gulf of Mexico's continental shelf some 200 kilometres offshore, from corals in the shallower regions to sponges, sea fans and other soft corals, and numerous fish species in the deep. Where the shelf ends and the continental slope drops steeply downwards, waters at 1,500\u20132,000 metres teem with long-lived predatory species such as bluefin tuna, swordfish, marlin and wahoos, which probably all swam through the plume. And on the sea floor, animals that strain food from the water, including tube worms, clams, mussels and barnacles, could also be exposed to oil microdroplets in the water or to residues buried in the sediment. It could take several years to understand how exposure to the oil in early life will affect fish populations that might not mature and be fished for some time. \"I'll be looking for year class failures \u2014 having a hole punched in that year's spawning reproductive success,\" says Richard Shaw, a fisheries oceanographer at LSU. Fish that live for several decades, such as red snapper and groupers, can recover from such a punch. But animals that live for just one to three years, including shrimp and menhaden, could be knocked flying. \"A failed year class for a couple of years in a row could dramatically reduce their populations for a while,\" says James Cowan, an oceanographer at LSU, in whose lab de Mutsert works. \"It could be catastrophic for both the populations and the community of people that relies on them.\"  \n                A shift in focus \n              In late August, Short and his colleagues took a cruise to make some of the first highly sensitive measurements of PAHs at depth. Until now, the instruments used to monitor the water around the spill have not been able to adequately detect PAHs and related long-lasting toxic molecules, he says. Kolker and his colleagues, meanwhile, are using genomic analyses to look at changes in bacterial communities in soils as proxies to understand how the wetlands may be affected by oil. \"Bacterial communities respond very quickly to environmental changes,\" he says. And starting in August and continuing into September, Samantha Joye, a biogeochemist at the  University of Georgia  in Athens, and a group of scientists are tracking underwater plumes, retracing the steps of earlier studies to determine how the oil has been travelling in the water column. Her team is also following the path of gas and oil through the food web by tracking how signature carbon isotopes are incorporated into the tissues of organisms. Joye says the spill is \"going to change the way that I do research and what I focus on for the rest of my career\". De Mutsert, who is busy writing further grant proposals for oil-related studies, agrees. \"We are going to have a lot of oil research scientists,\" she predicts. \"I think this will shape a lot of people's careers.\" But that is only if funding keeps on coming. BP has pledged US$500 million for research in the Gulf, although only a small fraction of that has yet been handed out, and many researchers are working under rapid-response grants from the National Science Foundation. But scientists fear that the commitment to spill research and to the recovery may wane. \"I'm worried that everybody is going to say, 'OK, we don't see oil any more so we're just going to forget about this',\" says Nancy Rabalais, executive director of the Louisiana Universities Marine Consortium. \"What I don't want to happen is for everybody to think we dodged a bullet and everything is going to be OK,\" says Rabalais. \"There will be ecological consequences, we just don't really know what they all are at this point.\"\n \n                     Deepwater Horizon special \n                   \n                     NOAA \n                   Reprints and Permissions"},
{"file_id": "467018a", "url": "https://www.nature.com/articles/467018a", "year": 2010, "authors": [{"name": "Corie Lok"}], "parsed_as_year": "2006_or_before", "body": "The US National Nanotechnology Initiative has spent billions of dollars on submicroscopic science in its first 10 years. Corie Lok finds out where the money went and what the initiative plans to do next. Richard Smalley's cheeks were gaunt and his hair was nearly gone when he testified before the US House of Representatives in June 1999. The Nobel laureate chemist had been diagnosed with non-Hodgkin's lymphoma a few months earlier, chemotherapy was taking its toll, and the journey from  Rice University  in Houston, Texas, had been exhausting. But none of that dimmed his obvious passion for a subject that his listeners found both mystifying and enthralling: nanotechnology. \"We are about to be able to build things that work on the smallest possible length scales, atom by atom, with the ultimate level of finesse,\" said Smalley, whose  prizewinning co-discovery  of spherical carbon buckminsterfullerene molecules, or 'buckyballs', in 1985 had helped to trigger a frenzy of research into such possibilities. As an example, Smalley told the legislators about his own laboratory's work with carbon nanotubes, which had been discovered in 1991. These hollow cylinders of carbon, only a few nanometres across, not only promised to conduct electricity better than copper, but also had the potential to produce fibres 100 times stronger than steel at one-sixth of the weight. Smalley also predicted that the \"very blunt tool\" of chemotherapy that had ravaged his own body would be obsolete within 20 years, because scientists would engineer nanoscale drugs that were \"essentially cancer-seeking missiles\" able to target mutant cells with minimal side effects. \"I may not live to see it,\" he said, \"but, with your help, I am confident it will happen. Cancer, at least the type that I have, will be a thing of the past.\" For all these reasons, Smalley concluded, the US government should back a recently proposed  National Nanotechnology Initiative  (NNI): a multi-agency funding effort that would catalyse these breakthroughs and more by realizing the systematic control of matter down to the scale of atoms. It was a message that Washington was ready to hear. US President Bill Clinton formally announced the initiative in 2000, with bipartisan support from Congress. The initiative has faced some criticism in the decade since \u2014 most notably for its slowness to address environmental, health and safety concerns about nanomaterials. But it has also created more than 70 nano-related academic or government centres across the United States; catalysed new interdisciplinary collaborations between physical, biomedical and social scientists; and fostered a whole system of investors, analysts and start-up companies devoted to commercializing laboratory discoveries. Along the way, the NNI has seen its budget increase steadily (see 'The NNI funding surge'), to the point at which its cumulative funding of more than US$12 billion places it among the largest US civilian technology investments since the Apollo Moon-landing programme. As such, the NNI story could provide a useful case study for newer research efforts into fields such as synthetic biology, renewable energy or adaptation to climate change. These are the kinds of areas in which the science, applications, governance and public perception will have to be coordinated across several agencies, points out David Rejeski, director of the  Science and Technology Innovation Program  at the Woodrow Wilson International Center for Scholars in Washington DC. That is precisely what the NNI was designed to do, he says. \"So I would argue that, for emerging areas like this, the concept of the NNI is a good one.\"  \n                A knack for persuasion \n              The most obvious lesson of the NNI is that success depends crucially on timing. The initiative happened when it did in part because the science was already moving fast in the late 1990s, thanks to discoveries during the previous decade such as buckyballs, nanotubes and the development of the atomic force microscope, which can image any surface with nanometre-level resolution (see 'The road to the NNI'). A uniquely favourable political climate also helped. The US economy was booming, particularly in the high-tech sector. The government was enjoying a budget surplus. And the Clinton administration, nearing the end of its term in office, was eager to end on a positive note. But timing alone isn't always enough. Any major initiative also needs its champions: well-placed visionaries with a knack for communication and persuasion. Smalley was one. Sadly, his June 1999 testimony was all too prescient: he did not live to see the targeted nanoparticle-based delivery of cancer drugs (although several are now in development). Given only a limited reprieve by chemotherapy, he died on 28 October 2005. But until then, Smalley was a tireless advocate for nanotechnology in general and the NNI in particular. Another champion is Mihail Roco, an engineer who had studied nanoscale particle interactions at the University of Kentucky in Lexington for 10 years before becoming a programme manager at the US National Science Foundation (NSF) in 1990. By 1996, he had come to believe that nanotechnology was not just a collection of individual research projects. He saw it as a new, unified discipline with the potential to revolutionize wide areas of science and industry, from health and agriculture to space, information technology, manufacturing and energy. He was also convinced that a major research investment was needed to give the nascent field momentum. Roco, an affable man with thick red hair, an even thicker Romanian accent, and an infectious enthusiasm for what he calls 'nano', says people regularly warned him against hyperbole as he tried to get the initiative off the ground. But you have to have the courage to articulate your vision, he says. \"You have to promise, then you have to fight to realize it.\" He found plenty of others thinking along the same lines: by the end of the decade, Roco and like-minded officials at seven other agencies were hammering out a proposal for the NNI, and bringing in leading scientists to help. It was Roco who recommended Smalley as a panellist for the June 1999 congressional hearing. Political support was also beginning to build from within the White House. Thomas Kalil, a lead adviser on technology issues for Clinton's  National Economic Council , saw the potential of nanotechnology to yield major economic pay-offs in many industries, including electronics. In March 1999, he helped to get Roco a 10-minute slot to pitch the NNI idea to key White House officials who were considering what to include in the president's 2001 budget proposal. Neal Lane, a physicist at Rice who became Clinton's chief science adviser in 1998 after a stint as NSF director, was familiar with Smalley's work and had already given his own testimony to Congress about nanotechnology's potential. In December 1999, Lane encouraged the President's Council of Advisors on Science and Technology, of which he was co-chair, to formally recommend that Clinton include the NNI in his budget. \"Nano was a good story,\" recalls Lane. \"It was real and exciting science, and you had a story that you could sell to a congressman or congresswoman that they could then take to their constituents.\" They bought it \u2014 and so did Clinton. On 21 January 2000, in a speech at the California Institute of Technology (Caltech) in Pasadena, the president announced that his 2001 budget request would include $500 million for the NNI. \"Just imagine,\" he said, \"materials with ten times the strength of steel and only a fraction of the weight; shrinking all the information at the Library of Congress into a device the size of a sugar cube.\"  \n                Small is effective \n              James Heath, a Caltech chemist, still remembers his excitement when he first found out about the NNI's creation. \"A couple of years earlier, I couldn't even convince people that nano was a real field,\" says Heath, who had been one of Smalley's students at Rice during the buckyball discovery. \"Now it is a big national initiative. Boy, we had better deliver something,\" Heath recalls thinking. And they did. Roco, who chaired the NNI's interagency coordinating committee until 2006 and is now the NSF's senior adviser for nanotechnology, notes that the number of US nano-related publications and patent applications increased by an average of 17% and 30%, respectively, every year from 2000 onwards. He can rattle off any number of favourites. In 2006, for example, researchers at Rice tested specially tailored iron nanoparticles for the removal of arsenic from drinking water 1 . In 2008, researchers at the University of California, Berkeley, reported a three-dimensional 'metamaterial' that could bend light in the opposite direction to other natural materials 2 , 3  \u2014 a process known as negative refraction, which could have uses in optical imaging and computing. And last month, a group at Harvard University in Cambridge, Massachusetts, showed that a nanoscale transistor inserted into a living heart cell could measure its electrical activity 4 . The NNI website ( http://www.nano.gov ) lists hundreds of other examples, from the creation of nanostructured battery materials for ultra-fast charging and discharging, to the development of nanostructures that aid the regeneration of nerves after spinal-cord injuries. But many participants argue that counting papers and patents is not the best way to measure the initiative's real impact. By 1999, after all, several science and technology fields were already moving towards the nanoscale, whether in materials research, semiconductor fabrication or the study of molecular machinery inside the cell \u2014 much of the ensuing research may have been funded anyway. \"What is due to the NNI and what is due to simply maturing of the field? It is very hard to tell,\" says Phaedon Avouris, manager of the nanoscale science and engineering group at IBM's T. J. Watson Research Center in Yorktown Heights, New York. Many observers say that the initiative's most important pay-off has been psychological. Simply by having a name and being recognized as an 'initiative', nanotechnology became a priority programme that has been easier to promote and protect at budget time, says Altaf Carim, a programme manager with the US Department of Energy and a current member of the NNI coordinating committee. Similarly, the NNI's government stamp of approval legitimized the nanotechnology field and made it look like a less costly and risky investment for venture capitalists. \"The NNI was the spark,\" says Josh Wolfe, managing partner of the venture-capital firm Lux Capital Management in New York City. Industry acceptance of nanotechnology \"was faster than we predicted\", agrees Roco \u2014 to the point at which an industry association, the Nano-Business Alliance, based in Skokie, Illinois, had formed by late 2001. That industry interest, in turn, helped the NNI to survive and flourish through the transition from the Democratic Clinton administration to the Republican administration of President George W. Bush. The initiative got $464 million its first year, and its annual budget has steadily expanded to some $1.7 billion today (plus a one-off addition of $500 million in 2009 from the US stimulus bill; see 'The NNI funding surge'). That money is now spread across 25 federal agencies \u2014 albeit with the vast majority of it going to just five: the NSF, the National Institutes of Health, the Department of Energy, the Department of Defense and the National Institute of Standards and Technology \u2014 and supports the 70-odd nanotechnology research centres that perform much of the NNI's work. Getting all of these agencies to coordinate their nanotech research activities has been one of the NNI's key successes, says Clayton Teague, director of the NNI's coordination office in Arlington, Virginia. \"Bringing this huge breadth of expertise from all the different departments together to see how they can move the field of nanotechnology forward is quite powerful.\" This involvement from so many different agencies has also helped to boost the awareness of nanotechnology outside the physical-sciences community. The US National Cancer Institute, for example, has funded eight Centres of Cancer Nanotechnology Excellence, which have brought together chemists, materials scientists and biologists to apply nanotechnology to cancer therapeutics and diagnostics. \"What the NNI has done really well is expand the view within nano of what it means to be interdisciplinary. It is not just between scientists and engineers, but also social scientists, philosophers and economists,\" says Kevin Ausman, a chemist at Oklahoma State University in Stillwater. As another example, the NSF is funding two 'nanotechnology in society' centres devoted to issues such as public risk perception and the media's coverage of nanotechnology.  \n                Bottom-up science \n              Still, the same decentralization that has enabled the NNI to foster interdisciplinary research has also created a management challenge. There are various coordination mechanisms, including Teague's office and the interagency council once chaired by Roco. But there is no central body that controls the NNI budget, which is a compilation of the individual budgetary decisions made by the 25 member agencies. So major decisions for the NNI require the agreement of all 25. Such 'bottom-up' science initiatives tend to be more successful in fostering collaboration and generating knowledge, says Craig Boardman, a science-policy expert at Ohio State University in Columbus, who has studied the NNI and other initiatives. But Andrew Maynard, director of the Risk Science Center at the University of Michigan in Ann Arbor, points to the obvious drawback: \"It is hard to measure the success of the initiative and actually hold someone accountable for what it has or hasn't done.\" Nowhere has this been felt more strongly than in what many regard as the NNI's biggest setback: its slow response to considering nanotechnology's environmental, health and safety (EHS) risks. Unfortunately, those risks are potentially serious: not only are many nanoparticles small enough to pass through or puncture cell membranes, but they tend to be far more chemically reactive than the equivalent bulk material \u2014 in ways that are not always well understood. From the beginning, says Maynard, \"I had a sense that the people driving the process really didn't fully understand how you begin to approach risk and uncertainty with new products. So there was a certain degree of naivety there.\" The NNI didn't begin to fund EHS research in a concerted way until 2005. And even then, many of its efforts continued to be poorly coordinated \u2014 much to the frustration of EHS researchers such as Ausman. Because nanomaterials span the periodic table and have such a wide range of properties, he says, it is difficult for researchers to prioritize which ones to study. \"There needs to be a list of recommended nanomaterials for basic science research on EHS issues,\" says Ausman, rather than having what he calls the \"scattershot\" approach to selecting materials. A watershed came in December 2008, when a National Research Council review committee blasted an EHS research strategy that the initiative had released earlier that year: \"The document \u2026 lacks input from a diverse stakeholder group, and it lacks essential elements, such as a vision and a clear set of objectives, a comprehensive assessment of the state of the science.\" In response, the NNI has held a series of four workshops to gather outsider input, with the aim of releasing a revamped EHS research strategy by the end of 2010, along with its new overall strategic plan. And the NNI's funding for EHS research has grown to around $92 million this year, roughly 5% of the total. Overall, says G\u00fcnter Oberd\u00f6rster, a toxicologist at the University of Rochester in New York and a member of the 2008 review committee, the NNI now seems to be on the right track with EHS issues. \"It is laudable that the NNI has taken them seriously,\" he says.  \n                Hottest prefix in science \n              Given all the attention being paid to nano-technology, a certain amount of hype was inevitable. To the extent that these things can be measured, it began at the birth of the NNI and peaked in the middle of the decade. Researchers who perhaps hadn't previously called their work nanotechnology looked for ways to relabel their research to take advantage of the new funding. The media published optimistic stories. Research-intensive technology companies started up nanotech research and development teams. Students enrolled in specialized university courses and degree programmes. Venture capitalists called up nanotechnology companies begging to invest in them. Nano-technology journals, websites and conferences proliferated. 'Nano' soon became the hottest prefix in science. Many scientists found the craze a cause for concern. If promises are made that don't deliver, says Mildred Dresselhaus, a materials scientist who studies carbon nanotubes and bismuth nanowires at the Massachusetts Institute of Technology in Cambridge, \"we lose our credibility\". Others saw the hype as a fair price to pay for the much-needed attention the physical sciences were finally receiving because of the NNI. \"As chemists, we were dying to have the community take notice of chemistry and the importance of it,\" says James Tour, a synthetic organic chemist at Rice. After the creation of the NNI, he says, he began receiving e-mails from high-school students wanting to become nanotechnologists. \"I would rather have overpromising than underpromising,\" he says, \"because then you get young people excited.\" As chemists, we were dying to have the community take notice of chemistry and the importance of it.  \n                The applications decade \n              Just as inevitably, the hype and excitement surrounding nanotechnology have waned as the newness has worn off \u2014 which illustrates a final lesson from the NNI: these things take time. If Smalley was right about a 20-year timescale for pay-offs, then the NNI is only halfway there. That is Roco's view. The initiative's first decade was mostly about basic science and laying the foundations, he says. But he has also seen a definite maturing of the field, as researchers have gone from developing simple nanostructures using trial-and-error methods to the deliberate design of nanosystems that can have more 'active' roles, such as delivering drugs to specific cells in the body. As passionate about 'nano' as ever, Roco expects the next ten years to be the decade of applications. To flesh out what that could mean, Roco is at it again, tirelessly brainstorming with scientists from around the world to formulate a nanotechnology vision for 2020. He is preparing to present that vision at the NSF later this month. The NNI's latest annual report also stresses applications. It lays out three 'signature initiatives' for 2011: applications for solar energy, nanoelectronics for 2020 and beyond, and sustainable nanomanufacturing. The need to turn scientific findings into commercialized products is also a key theme in the latest assessment from the President's Council of Advisors on Science and Technology, as well as in legislation pending in the US Senate to continue the NNI's funding. That need is considerably more urgent than it was in 2000. Although support for nanotechnology is still strong in Washington, the shift in emphasis towards practical applications reflects the changing mood of the country. The optimism of the late 1990s has now been largely replaced by a sense of national self-doubt, fed by challenges in the economy, jobs, energy, climate change, health care and national security. Nanotechnology promises to help in every case, but so far these are still just promises. \"Success will depend on the commercialization of nanotechnology,\" says Avouris.\n \n                     Nature Nanotechnology \n                   \n                     Nature Materials \n                   \n                     National Nanotechnology Initiative \n                   \n                     Project on Emerging Nanotechnologies \n                   \n                     NanoBusiness Alliance \n                   Reprints and Permissions"},
{"file_id": "467386a", "url": "https://www.nature.com/articles/467386a", "year": 2010, "authors": [{"name": "Jeff Tollefson"}], "parsed_as_year": "2006_or_before", "body": "An ambitious project to track greenhouse gases from a perch high above the Amazon forest will provide crucial data \u2014 but only if scientists can get it built. Meinrat Andreae walked into a clearing where a fallen paric\u00e1 tree had torn a hole in the canopy of the rainforest. He recorded the location on his Global Positioning System device and then paused to marvel at a blue Morpho butterfly that fluttered past. In place of the fallen giant, Andreae and a team of scientists from Germany and Brazil will soon plant more than 100 tonnes of steel that will dwarf even the largest trees, offering scientists an unprecedented perch above the rolling hills of the central Brazilian Amazon. Once complete, the 320-metre-high structure \u2014 roughly as tall as the Eiffel Tower \u2014 will help to plug a vast gap in terrestrial greenhouse-gas monitoring. Europe and the United States have dozens of tall towers, and Germany's Max Planck Society erected a 300-metre facility in Siberia several years ago. But the Amazonian Tall Tower Observatory (ATTO) project, consisting of one giant and several smaller towers, would be unique in the tropics and Southern Hemisphere. More importantly, says Andreae, an atmospheric chemist at the Max Planck Institute for Chemistry in Mainz, Germany, it would provide the first long-term monitoring capacity in an ecosystem that processes more carbon than anywhere else on Earth. \"It's a beautiful project,\" comments Carlos Nobre, a climate modeller at the National Institute for Space Research (INPE) in S\u00e3o Jos\u00e9 dos Campos, Brazil. Scientists already use smaller flux towers to measure how much carbon dioxide gets sucked up and released every day by small patches of forest. Among the trees, CO 2  levels can drop to 360 parts per million (p.p.m.) during the day owing to photosynthesis, and rise to 500\u2013600 p.p.m. at night because of respiration. But those swings weaken higher up, where air from the forest mixes with trade winds from the Atlantic Ocean, and CO 2  levels remain much closer to the global average of about 387 p.p.m. With ATTO sticking far above the 35-metre-high forest canopy, the tower will provide large-scale measurements that reflect conditions across much of the eastern Amazon, says Nobre. \"We are going to be able to say significant things about 50% of the Amazon Basin.\" Jointly funded by Brazil and Germany, the tower will monitor air blowing from an elongated area that extends 850 kilometres towards the coast ( see map ). The data will help scientists to quantify how much CO 2  flows through the forest and how that flux changes year by year, which is crucial to understanding and projecting the effects of global warming. Without that kind of long view of the forest, researchers can't tell for certain whether the Amazon is sponging up CO 2  overall, or releasing it into the atmosphere. The budget for the study, including initial operations, is \u20ac8.4 million (US$10.9 million). The project includes several towers that will monitor air chemistry and interactions between soil, forest and sky, making the facility one of the most sophisticated atmospheric-monitoring stations in the world. But first the team needs to start building the towers. \n               Click here for larger image \n               Andreae, one of the world's leading experts in atmospheric aerosols, developed a hankering for the tropics early in his career. Over the years, he has witnessed a shoot out in the Amazon and got lost in a plane above the Congo Basin. But ATTO represents one of his most ambitious efforts yet. The project grew out of governmental meetings in 2007, and has encountered considerable red tape and logistical difficulties. Andreae says he often feels more like a real-estate developer than a scientist. \"The logistics, the infrastructure and the politics are more foreign and difficult terrain for us,\" he says. Pieter Tans at the National Oceanic and Atmospheric Administration in Boulder, Colorado, who runs the US carbon-monitoring system, says that building a giant monitoring tower in the middle of the jungle \"is an impressive undertaking\". Because data from this region are so scarce, he says, the tower project \"could be very significant\". The first challenge for the German and Brazilian researchers was to choose a site, and they settled on a plateau 155 kilometres upwind of Manaus, a sprawling metropolis of almost 2 million people in the centre of the Amazon. At that distance, the tower would be far enough from the city's pollution to avoid contamination but close enough to allow relatively easy access. Brazilian researchers hope that the facility will help to improve local science by serving as a base camp for many types of study. \"We need to have the capacity to do this science here in the Amazon,\" says Ant\u00f4nio Manzi, a researcher at the National Institute of Amazonian Research in Manaus. On a visit in August, Andreae and his team drove north and then east from Manaus, then took boats down the Uatum\u00e3 River for two hours before making a final trek nine kilometres inland to a base camp. From there, they hiked another five kilometres up a steep dirt road to the plateau, where the towers would be located. Several problems have slowed down construction. The scientists have been waiting for months for the state of Amazonas to improve the roads for heavy equipment. Because the plan to run a power line to the site proved too expensive, they are now planning to build a diesel-fired generator near the river and run a power line up to the towers, also a costly scheme. Similarly, high cost estimates from local companies for the towers themselves have Andreae and his team thinking about buying the entire set-up in Europe and then shipping it to Brazil. As he toured the study site on the plateau, Andreae encountered another problem. The team had already installed an 80-metre-tall tower to monitor local meteorology (see  'The height of research' ) but the crew cut down one tree too many, knocking down a second and opening up a huge hole in the canopy. The opening would heat the soil and promote updrafts, rendering the tower's readings meaningless, explains Andreae. The tower would have to be moved. \"It's a sort of fog-of-war situation,\" Andreae says. \"Shit happens.\" But the August trip also brought some positive news. \"It looks as if there will be a road constructed during this dry season,\" says J\u00fcrgen Kesselmeier, a plant physiologist at the Max Planck Institute for Chemistry who heads the ATTO project as part of Andreae's team. The main ATTO tower will measure CO 2 , methane and other atmospheric constituents; a series of four smaller satellite towers will be used to track local air currents. All the data will be plugged into a computer model that calculates atmospheric trends in an effort to determine where the gases come from and how the forest influences them. The team hopes to complete construction of the towers by next year.  \n                Too much diversity \n              Studies in small plots 1 , 2  show that tropical forests are accumulating CO 2 , and these data have been used to suggest that the entire Amazon is acting as a sink for the greenhouse gas. But such local studies may not tell the full story because the Amazon is a patchwork of myriad ecosystems. The path from the river to the site crossed clay bogs and a desert-like area dominated by shrubs, lichens and white sands before reaching the terra-firma forest of the plateau. Such diversity makes it hard to extrapolate local measurements to the whole forest. Local measurements also miss some crucial processes. Recent work by Peter Raymond and David Butman, biogeochemists at Yale University in New Haven, Connecticut, presented at an American Geophysical Union (AGU) meeting last month in Foz do Igua\u00e7u, Brazil, suggests that some carbon escapes from the forest through rivers and streams. And extreme events \u2014 such as the 2005 drought or the violent storm that knocked down more than half a billion trees the same year \u2014 could wipe out decades' worth of research. Initial results from a 2008\u201309 aircraft campaign conducted by Andreae in conjunction with a team of scientists in the United States and Brazil suggest that the Amazon Basin is more or less carbon neutral. But those data, presented at the AGU meeting, are just a snapshot. Only a tall tower can provide detailed measurements of a large area of forest over time. Nobre is looking forward to analysing the CO 2  concentrations over the course of a year. If CO 2  levels at the tower are consistently lower than known concentrations in ocean air that gets blown inland, then the forest is taking up CO 2  as the air passes through. Further down the line, Andreae wants to know how the situation will change. Could extra CO 2  stimulate forest growth? Will the forest recede in a warmer climate, pumping more CO 2  into the atmosphere? What about the effects of deforestation and development? \"What we want to see over this 10- to 20- to 30-year timescale is if there is a change in the carbon greenhouse-gas budget,\" says Andreae, who is 61 and recognizes that he will hand over the reins long before answering some of these questions. \"This is a long-term effort.\" \n                     Meinrat Andreae \n                   \n                     ZOTTO \n                   \n                     INPA \n                   Reprints and Permissions"},
{"file_id": "467150a", "url": "https://www.nature.com/articles/467150a", "year": 2010, "authors": [{"name": "Jane Qiu"}], "parsed_as_year": "2006_or_before", "body": "A project to drill a 10-kilometre-deep hole in China will provide the best view yet of the turbulent Cretaceous period. Jane Qiu reports. The rock columns on the table are not much to look at. More than a metre long, 10 centimetres in diameter and mostly made up of oil shale and sandstone, they are a dull greyish green. But these, says Wang Chengshan, a geologist at the  China University of Geosciences  in Beijing, \"are not ordinary rocks\". Taken from depths of more than 2 kilometres into the Songliao Basin in northeastern China (see map), the rocks may hold clues to one of the strangest and most dynamic ages of Earth's history: the Cretaceous period. Beginning about 145 million years ago, the Cretaceous was the heyday of the dinosaurs. It was a time of climatic extremes, when global temperatures exceeded even the most alarming forecasts for the greenhouse world of 2100, and sea levels were up to 250 metres higher than today, covering about one-third of the current landmass. It was also a period of great geological and biological unrest, associated with frequent volcanic eruptions, the formation of major mountain ranges and ocean oxygen depletion. And it ended in spectacular style, with the global catastrophe that saw off dinosaurs some 65 million years ago, an event known as the Cretaceous/Palaeogene (K/Pg) extinction. Earth scientists have pieced together their understanding of conditions in the Cretaceous mainly from sediment cores drilled from the bottom of the ocean. But the cores being drilled from an oilfield in the Songliao Basin \u2014 which could eventually extend 10 kilometres deep \u2014 promise the deepest and best record yet of what was happening on land, and a chance to understand what drove the extremes of the time. \"They are the key to unlocking the secrets of that fascinating period of Earth's history,\" says Wang, who, as the lead principal investigator, chaired a workshop in Beijing in early July on the Songliao Scientific Drilling Project. The cores that the researchers have seen so far, from depths of up to 2.5 kilometres, have offered insight into the Cretaceous climate and its massive fluctuations in temperature, atmospheric carbon dioxide and lake levels. The team is now hoping to muster support for a push to the very bottom of the basin, a further 7.5 kilometres down, where the rocks should date from before the start of the Cretaceous.  \n                Deep details \n              The peculiar geology of the basin allows researchers to look at the record in extraordinary detail. Ten kilometres deep and covering an area of 260,000 square kilometres, Songliao is a rift basin that was formed as Earth's crust was pulled apart by the same tectonic forces that transport continents over geological time. For nearly 100 million years, mostly during the Cretaceous, it was home to a series of gigantic lakes fed by vast rivers. The lakes seem to have captured in their sediments an uninterrupted record of climate and environmental indicators. \"Most lakes are rather ephemeral,\" says Judith Parrish, a palaeoclimatologist at the University of Idaho in Moscow. \"It is extremely rare to find a palaeo-lake as large and long-lived as Songliao.\" This geological record is, in effect, \"an encyclopaedia of the Cretaceous\", says Stephan Graham, a sedimentary geologist at  Stanford University  in California, and one of the five principal investigators now involved in the project. \"You just don't have something like this anywhere else on the planet.\" The Songliao Basin is home to the Daqing oilfield, the largest oilfield in China. Chinese geologists had already drilled more than 50,000 wells across the basin and generated a comprehensive picture of the region. This type of drilling does not generally take cores, which is a much more costly enterprise \u2014 but it has helped researchers to select the best spot for the core drilling, where rock layers have not slid apart or folded together to complicate the geological record. The Daqing Oilfield Company in Heilongjiang province contributed half of the 10-million-yuan (US$1.3-million) cost of drilling out the 2,500-metre core, which started in 2006, and China's science ministry and various research institutions have also contributed to the drilling and analyses. On the basis of the early results, the project won endorsement in 2009 from the  International Continental Scientific Drilling Programme  (ICDP), which is run by experts at research institutions and funding bodies worldwide. A one-third segment of the entire length of the core is now sealed in transparent bags to protect its original shape and structure from drying and disintegrating.  \n                Time of great unrest \n              Preliminary analyses have offered a glimpse of terrestrial conditions in the Middle and Late Cretaceous, adding to what was known from marine cores. \"It seems to be a time of great unrest with a lot of ups and downs,\" says Wang. The team analysed oxygen and carbon isotopes in fossil crustacean shells as a proxy for ancient temperature and carbon dioxide levels, finding that atmospheric CO 2  levels doubled and then halved over 3 million years in the Late Cretaceous. (The team is putting together its results for publication.) The same techniques showed that the temperature plunged by more than 7 \u00b0C during a 10-million-year period \u2014 possibly around the Cretaceous 'super greenhouse', when global temperatures were substantially higher than today. The lake levels also seemed to have fluctuated greatly: the researchers discovered signs of surface soil in a few core segments, suggesting that the basin might have completely dried out a few times. If proved correct, this could help to build up a picture of the climate dynamics behind these dry periods, says Parrish. The researchers hope to flesh out this picture, and pinpoint the causes of the climatic fluctuations with further analyses of the sediment compositions. They also want to examine the sources of the sediments and water that entered the lake by using isotopes of elements such as strontium. Deltas contained in the Songliao Basin are likely to be as large as the modern Nile Delta, so rivers that fed the lake may have flowed for hundreds or thousands of kilometres, leaving deposits that could provide insight into geological and ecological events in distant areas. The Songliao core may also shed fresh light on a contentious scientific debate: whether a large ice cap, half the size of the modern Antarctic ice cap, existed during a period as hot as the Cretaceous super greenhouse 1 . A more detailed temperature record built from the core might, for example, show whether there were more short cooling periods, such as the 7 \u00b0C drop the team has already observed. And the core could answer pressing questions about the K/Pg extinction, which many researchers believe was caused by an asteroid or comet strike at Chicxulub on Mexico's Yucat\u00e1n Peninsula, and the climatic aftershock 2 . Most of the samples corroborating the theory have come from marine sediments. A terrestrial record at Songliao could reveal how the asteroid strike affected life on land, at a huge distance from the impact. \"Sediment cores from Songliao will help to build a more complete picture of those extraordinary events,\" says Christian Koeberl, director of the  Natural History Museum in Vienna  and another principal investigator on the Songliao project. The key to answering all of these questions will be accurate dating of the core. This will help to correlate the Songliao records with their marine counterparts. \"Without a precise timescale, the values of any other pieces of information that can be recovered from the core would be diminished tremendously,\" says Bradley Singer, a geochronologist at the University of Wisconsin\u2013Madison. To date the core, Deng Chenglong, a geophysicist at the Chinese Academy of Sciences'  Institute of Geology and Geophysics  in Beijing, and his colleagues took samples every half-metre along it and measured the orientation of magnetic mineral grains in nearly 4,400 samples. The iron-rich grains in every rock layer point in the direction of Earth's magnetic field at the time the rock was forming. That field flips its polarity every few hundred thousand years, and those reversals get imprinted in the rocks.  \n                Magnetic calendar \n              Deng and his team established a rough magnetic calendar for the Songliao core, which they could then compare with the global geomagnetic record. But to make an accurate match, the researchers needed to scour the core for volcanic layers \u2014 enriched in isotopes of uranium and other elements \u2014 which can be dated using the known rates of radioactive decay. They identified a handful of centimetre-thick ash beds with enough material for such dating, and they hope to find more. Other as-yet-unpublished results also point to a possible position for the K/Pg boundary. But it is about 100 metres below the depth determined by Wan Xiaoqiao, a palaeontologist at the Beijing-based China University of Geosciences who used fossils of spores, pollen, phytoplankton and ostracod to locate the boundary. The researchers are trying to determine why the estimates differ, and to nail the boundary down to 2\u20133 metres, so that detailed geochemical analyses can be performed to look for rare elements, such as iridium, that are common in meteorites and were spread around the globe by the cosmic impact. The second phase of the drilling, an extra 7.5 kilometres, is contingent on further funding. The ICDP will provide $1.3 million, and Wang hopes to get another 200 million yuan for the drilling operation from the Chinese government. It is not yet clear whether the Daqing Oilfield Company will continue to offer substantial financial support. Feng Zhiqiang, a geologist and vice-president of the company, hopes that the drilling project will result in a better understanding of the geological composition and sedimentation processes of the basin. \"The knowledge will ultimately help us to locate new resources more efficiently,\" he says. Wang is more excited about the science. He picks up a rock segment, his eyes instantly lighting up, and weighs it in his hand. \"This is not the end,\" he says. \"It is just the beginning of an exciting scientific adventure.\" Jane Qiu writes for  Nature   from Beijing. \n                     Nature Geoscience \n                   \n                     Nature Climate Change \n                   \n                     Nature Reports Climate Change \n                   \n                     Terrestrial scientific drilling project of the Cretaceous Songliao Basin \n                   \n                     International Continental Scientific Drilling Project \n                   \n                     China University of Geosciences (Beijing) \n                   \n                     Stephan Graham \n                   \n                     Christian Koeberl \n                   \n                     Brad Singer \n                   \n                     Judith Parrish \n                   Reprints and Permissions"},
{"file_id": "467388a", "url": "https://www.nature.com/articles/467388a", "year": 2010, "authors": [{"name": "David Cyranoski"}], "parsed_as_year": "2006_or_before", "body": "A global survey of the scientifically literate public reveals significant differences on key issues in science. Science, it is often said, is an international language. But how international are attitudes towards science and scientists?  Nature   and our affiliated publication  Scientific American   set out to learn how the views of the scientifically literate public vary around the world. Our web-based survey of more than 21,000 readers of  Scientific American   and its translated editions in 18 countries revealed that although these science enthusiasts read the same publication and share many attitudes in their perception of science, they seem to diverge on some of the hottest-button issues. The differences are most striking between east Asia and the rest of the world. For example, a startling 35% of the Japanese and 49% of the Chinese respondents agreed that there is \"reason for doubt\" about evolutionary theory's ability to explain the variety of species on Earth. In contrast, the numbers for the rest of the world fluctuated around 10%. Japanese and Chinese respondents were also less likely than others to say that they trust scientific explanations of the origins of the Universe. And almost one-third of scientifically literate Chinese people say that scientists should not get involved in politics, compared with around 10% of respondents in most of the rest of the world. \n               Click here for larger image \n               The results are not conclusive. Far from being rigorous, the survey sampled countries unevenly, with thousands of respondents in the United States and several European countries, 1,195 in Japan and just 269 in China. The respondents were self-selected, so some subsets of readers may simply have bypassed the questionnaire. And cultural differences may have influenced how people from different countries responded to identical questions. Still, the results are sobering for anyone who believes that a public informed about science will necessarily share scientists' views. Many of the results match expectations for a scientifically literate group. Worldwide, poll respondents overwhelmingly agreed that scientists are more trustworthy than other public figures, that investment in science is key to future economic growth and that animal research should be permitted. Strikingly, a large fraction of respondents in every country said that in the past year they have grown more confident that human activity is changing the global climate, despite noisy controversies over the United Nations' climate panel and leaked e-mails by climate researchers. And some of the regional differences were also familiar from earlier reports. Europeans tend to be wary of genetically modified organisms (GMOs), for example, and Germans want to phase out nuclear energy. But how to explain the scepticism about evolution and the Big Bang in Japan and China? Christian creationism has little traction in these countries, but some scientists, in response to follow-up questions from  Nature , pointed to Asian philosophical systems such as Shinto or Buddhism, which have their own explanations for the origin of life. Naruya Saitou, a population geneticist at the National Institute of Genetics in Mishima, Japan, suggests that the apparent doubts stem from a recognition of complexity in nature. This also explains other trends in Japan, he says: systems biology is a relatively unpopular career path for scientists because \"life is too complex to be reduced to a formula\", engineering and robotics are popular fields because \"you can control almost everything\", and GMOs are shunned because \"people are afraid of manipulating the huge complexities in nature\". The ambivalence could also reflect a greater appreciation among east Asians for the limitations of knowledge, says Kazuto Kato, a specialist in the public perception of science at Kyoto University in Japan. The phrasing of the evolution question, which asked whether there is \"any doubt\" about the theory's explanatory power or whether \"all the evidence\" supports evolution, probably provoked a guarded response, he says. \"Many scientifically literate people would say there is room for doubt,\" adds Kato. \"It doesn't mean they believe in creationism.\" Wu Yishan, a chief engineer at the Institute of Scientific and Technical Information of China in Beijing, says that the evolution question probably also triggered a natural, and healthy, scepticism in Chinese respondents: \"As a scientist you should be able to doubt anything,\" he says, noting that even among physicists there is debate over the origins of the Universe. \"People are often guessing what kind of answers pollsters hope to hear. In this case, if I say I have no doubts, it seems I am not open-minded.\" Western respondents may have been less equivocal in their support for science and scientists because of political debate in their countries, says Dietram Scheufele, a science-communications and public-policy expert at the University of Wisconsin\u2013Madison. \"Particularly in the United States, dichotomies dominate political issues, like a sporting event with two sides. Dichotomies are prominent in discussions about climate change, stem cells and so on,\" he says. \"And they can damage the debate.\" A comparison with previous surveys on attitudes towards evolution highlights one dichotomy in the United States. A typical survey of the public at large shows that a mere 26% of Americans believe that evolution can explain the variety of animal life on Earth, whereas 87% of  Scientific American   readers said in this poll that \"all the evidence supports evolution\". \"These are not the people you would tap in a general population survey,\" says Scheufele. Subtleties in language probably also affected the results, says Wu. Chinese respondents said that scientists \"should speak freely about science-related policy questions\" and \"should take part in public debate\" in similar ratios to the Western respondents, and they were even more likely to think that \"scientists know what is good for the public\". Yet a very high proportion, 29%, felt strongly that \"scientists should stay out of politics\". Wu suggests that the negative connotation of 'politics' in China, deriving from a 'politics first' movement during the communist Cultural Revolution that was used as an excuse for various abuses, might be to blame: it is not that scientists have nothing to contribute, but that politics itself is tainted. Furthermore, the 269 respondents from China can't represent the entire country, says Wu. \"In a country of 1.3 billion, that is nothing. A change in attitudes of a few people can really shift a result.\" The US cohort \u2014 4,779, from a country of 300 million \u2014 also had a broad range of ages and specialities, including many social scientists, whereas the median age of the Chinese respondents was a young 35, and the sample was heavily skewed towards natural scientists. Still, Scheufele says that although the poll is far from scientific, it holds interesting clues \u2014 even about the sparsely represented people of China. \"We can say there is a group of young and educated people who feel strongly that science needs to have an impact on policy and in daily life. It's a young group that thinks that science will drive politics instead of what you would expect in China \u2014 the other way around.\" Survey conducted by Sara Grimme and Fiona Watt.  Click here  to download the full survey results. \n                     Full survey results \n                   \n                     More survey graphics at Scientific American \n                   \n                     Institute of Scientific and Technical Information of China \n                   \n                     China Research Institute for Science Popularization \n                   Reprints and Permissions"},
{"file_id": "467516a", "url": "https://www.nature.com/articles/467516a", "year": 2010, "authors": [{"name": "Brendan Maher"}], "parsed_as_year": "2006_or_before", "body": "Postdoc Vipul Bhrigu destroyed the experiments of a colleague in order to get ahead. It took a hidden camera to expose a surreptitious and malicious side of science. It is sentencing day at Washtenaw County Courthouse, a drab structure of stained grey stone and tinted glass a few blocks from the main campus of the University of Michigan in Ann Arbor. Judge Elizabeth Pollard Hines has doled out probation and fines for drunk and disorderly conduct, shoplifting and other mundane crimes on this warm July morning. But one case, number 10-0596, is still waiting. Vipul Bhrigu, a former postdoc at the university's Comprehensive Cancer Center, wears a dark-blue three-buttoned suit and a pinched expression as he cups his pregnant wife's hand in both of his. When Pollard Hines calls Bhrigu's case to order, she has stern words for him: \"I was inclined to send you to jail when I came out here this morning.\" Bhrigu, over the course of several months at Michigan, had meticulously and systematically sabotaged the work of Heather Ames, a graduate student in his lab, by tampering with her experiments and poisoning her cell-culture media. Captured on hidden camera, Bhrigu confessed to university police in April and pleaded guilty to malicious destruction of personal property, a misdemeanour that apparently usually involves cars: in the spaces for make and model on the police report, the arresting officer wrote \"lab research\" and \"cells\". Bhrigu has said on multiple occasions that he was compelled by \"internal pressure\" and had hoped to slow down Ames's work. Speaking earlier this month, he was contrite. \"It was a complete lack of moral judgement on my part,\" he said. Bhrigu's actions are surprising, but probably not unique. There are few firm numbers showing the prevalence of research sabotage, but conversations with graduate students, postdocs and research-misconduct experts suggest that such misdeeds occur elsewhere, and that most go unreported or unpoliced. In this case, the episode set back research, wasted potentially tens of thousands of dollars and terrorized a young student. More broadly, acts such as Bhrigu's \u2014 along with more subtle actions to hold back or derail colleagues' work \u2014 have a toxic effect on science and scientists. They are an affront to the implicit trust between scientists that is necessary for research endeavours to exist and thrive. Despite all this, there is little to prevent perpetrators re-entering science. In the United States, federal bodies that provide research funding have limited ability and inclination to take action in sabotage cases because they aren't interpreted as fitting the federal definition of research misconduct, which is limited to plagiarism, fabrication and falsification of research data. In Bhrigu's case, administrators at the University of Michigan worked with police to investigate, thanks in part to the persistence of Ames and her supervisor, Theo Ross. \"The question is, how many universities have such procedures in place that scientists can go and get that kind of support?\" says Christine Boesz, former inspector-general for the US National Science Foundation in Arlington, Virginia, and now a consultant on scientific accountability. \"Most universities I was familiar with would not necessarily be so responsive.\"  \n                First suspicions \n              Ames, an MD PhD student, first noticed a problem with her research on 12 December 2009. As part of a study on the epidermal growth factor receptor, a protein involved in some cancers, she was running a western blot assay to confirm the presence of proteins in a sample. It was a routine protocol. But when she looked at the blot, four of her six samples seemed to be out of order \u2014 the pattern of bands that she expected to see in one lane appeared in another. Five days later, it happened again. \"I thought, technically it could have been my mistake, but it was weird that they had gone wrong in exactly the same way,\" says Ames. The only explanation, she reasoned, was that the labelled lids for her cell cultures had been swapped, and she immediately wondered whether someone was sabotaging her work. To be safe, she devised a workaround: writing directly on the bottoms of the culture dishes so that the lids could not be switched. Next, Ames started having an issue with the western blots themselves. She saw an additional protein in the sample lanes, showing that an extra antibody was staining the blot. Once again, it could have been a mistake, but it happened twice. \"I started going over to my fianc\u00e9's lab and running blots overnight there,\" she says. As the problems mounted, Ames was getting agitated. She was certain that someone was monkeying with her experiments, but she had no proof and no suspect. Her close friends suggested that she was being paranoid. Some labs are known to be hyper-competitive, with principal investigators pitting postdocs against each other. But Ross's lab is a small, collegial place. At the time that Ames was noticing problems, it housed just one other graduate student, a few undergraduates doing projects, and the lab manager, Katherine Oravecz-Wilson, a nine-year veteran of the lab whom Ross calls her \"eyes and ears\". And then there was Bhrigu, an amiable postdoc who had joined the lab in April 2009. Bhrigu had come to the United States from India in 2003, and completed his PhD at the University of Toledo, Ohio, under cancer biologist James Trempe. \"He was an average student,\" says Trempe. \"I wouldn't say that he was a star in the lab, but there was nothing that would make me question the work that he did.\" Ross thought Bhrigu would be a good fit with her lab \u2014 friendly, talkative, up on current trends in the field. Ames says that she liked Bhrigu and at the time had little reason to suspect him. \"He was one of the last people I would have suspected didn't like me,\" she says. On Sunday 28 February 2010, Ames encountered what she thought was another attempt to sabotage her work. She was replacing the media on her cells and immediately noticed that something wasn't right. The cells were \"just dripping off the plate\", as if they'd been hit with something caustic. She pulled the bottle of medium out from the fume hood and looked at it. Translucent ripples, like those that appear when adding water to whisky, were visible in the dark red medium. When she sniffed it, the smell of alcohol was overpowering. This, she thought, was the proof she needed. \"It was clearly not my mistake,\" says Ames. She fired off an e-mail to Ross. \"I just found pretty convincing evidence that somebody is trying to sabotage my experiments,\" she wrote. Ross came and sniffed the medium too. She agreed that it didn't smell right, but she didn't know what to think.  \n                Lab investigation \n              Some people whom Ross consulted with tried to convince her that Ames was hitting a rough patch in her work and looking for someone else to blame. But Ames was persistent, so Ross took the matter to the university's office of regulatory affairs, which advises on a wide variety of rules and regulations pertaining to research and clinical care. Ray Hutchinson, associate dean of the office, and Patricia Ward, its director, had never dealt with anything like it before. After several meetings and two more instances of alcohol in the media, Ward contacted the department of public safety \u2014 the university's police force \u2014 on 9 March. They immediately launched an investigation \u2014 into Ames herself. She endured two interrogations and a lie-detector test before investigators decided to look elsewhere. At 4:00 a.m. on Sunday 18 April, officers installed two cameras in the lab: one in the cold room where Ames's blots had been contaminated, and one above the refrigerator where she stored her media. Ames came in that day and worked until 5:00 p.m. On Monday morning at around 10:15, she found that her medium had been spiked again. When Ross reviewed the tapes of the intervening hours with Richard Zavala, the officer assigned to the case, she says that her heart sank. Bhrigu entered the lab at 9:00 a.m. on Monday and pulled out the culture media that he would use for the day. He then returned to the fridge with a spray bottle of ethanol, usually used to sterilize lab benches. With his back to the camera, he rummaged through the fridge for 46 seconds. Ross couldn't be sure what he was doing, but it didn't look good. Zavala escorted Bhrigu to the campus police department for questioning. When he told Bhrigu about the cameras in the lab, the postdoc asked for a drink of water and then confessed. He said that he had been sabotaging Ames's work since February. (He denies involvement in the December and January incidents.)  \n                Motives for misconduct \n              Misbehaviour in science is nothing new \u2014 but its frequency is difficult to measure. Daniele Fanelli at the University of Edinburgh, UK, who studies research misconduct, says that overtly malicious offences such as Bhrigu's are probably infrequent, but other forms of indecency and sabotage are likely to be more common. \"A lot more would be the kind of thing you couldn't capture on camera,\" he says. Vindictive peer review, dishonest reference letters and withholding key aspects of protocols from colleagues or competitors can do just as much to derail a career or a research project as vandalizing experiments. These are just a few of the questionable practices that seem quite widespread in science, but are not technically considered misconduct. In a meta-analysis of misconduct surveys, published last year ( D. Fanelli  PLoS ONE    4,   e5738; 2009 ), Fanelli found that up to one-third of scientists admit to offences that fall into this grey area, and up to 70% say that they have observed them. Some say that the structure of the scientific enterprise is to blame. The big rewards \u2014 tenured positions, grants, papers in stellar journals \u2014 are won through competition. To get ahead, researchers need only be better than those they are competing with. That ethos, says Brian Martinson, a sociologist at HealthPartners Research Foundation in Minneapolis, Minnesota, can lead to sabotage. He and others have suggested that universities and funders need to acknowledge the pressures in the research system and try to ease them by means of education and rehabilitation, rather than simply punishing perpetrators after the fact. But did rivalry drive Bhrigu? He and Ames were collaborating on one of their projects, but they were not in direct competition. Chiron Graves, a former graduate student in Ross's lab who helped Bhrigu learn techniques, says that Ross is passionate but didn't put undue stress on her personnel. \"The pressures that exist in the system as a whole are somewhat relieved in Theo's lab,\" says Graves, now an assistant professor running a teacher-education programme at Eastern Michigan University in Ypsilanti. \"Her take was to do good science.\" Bhrigu says that he felt pressure in moving from the small college at Toledo to the much bigger one in Michigan. He says that some criticisms he received from Ross about his incomplete training and his work habits frustrated him, but he doesn't blame his actions on that. \"In any kind of workplace there is bound to be some pressure,\" he says. \"I just got jealous of others moving ahead and I wanted to slow them down.\"  \n                Crime and punishment \n              At Washtenaw County Courthouse in July, having reviewed the case files, Pollard Hines delivered Bhrigu's sentence. She ordered him to pay around US$8,800 for reagents and experimental materials, plus $600 in court fees and fines \u2014 and to serve six months' probation, perform 40 hours of community service and undergo a psychiatric evaluation. But the threat of a worse sentence hung over Bhrigu's head. At the request of the prosecutor, Ross had prepared a more detailed list of damages, including Bhrigu's entire salary, half of Ames's, six months' salary for a technician to help Ames get back up to speed, and a quarter of the lab's reagents. The court arrived at a possible figure of $72,000, with the final amount to be decided upon at a restitution hearing in September. Before that hearing could take place, however, Bhrigu and his wife left the country for India. Bhrigu says his visa was contingent upon having a job. A new hearing has been scheduled for October in which the case for restitution will be heard alongside arguments that Bhrigu has violated his probation. Ross, though, is happy that the ordeal is largely over. For the month-and-a-half of the investigation, she became reluctant to take on new students or to hire personnel. She says she considered packing up her research programme. She even questioned her own sanity, worrying that she was the one sabotaging Ames's work via \"an alternate personality\". Ross now wonders if she was too trusting, and urges other lab heads to \"realize that the whole spectrum of humanity is in your lab. So, when someone complains to you, take it seriously.\" She also urges others to speak up when wrongdoing is discovered. After Bhrigu pleaded guilty in June, Ross called Trempe at the University of Toledo. He was shocked, of course, and for more than one reason. His department at Toledo had actually re-hired Bhrigu. Bhrigu says that he lied about the reason he left Michigan, blaming it on disagreements with Ross. Toledo let Bhrigu go in July, not long after Ross's call. Now that Bhrigu is in India, there is little to prevent him from getting back into science. And even if he were in the United States, there wouldn't be much to stop him. The National Institutes of Health in Bethesda, Maryland, through its Office of Research Integrity, will sometimes bar an individual from receiving federal research funds for a time if they are found guilty of misconduct. But Bhigru probably won't face that prospect because his actions don't fit the federal definition of misconduct, a situation Ross finds strange. \"All scientists will tell you that it's scientific misconduct because it's tampering with data,\" she says. Still, more immediate concerns are keeping Ross busy. Bhrigu was in her lab for about a year, and everything he did will have to be repeated. Reagents that he used have been double-checked or thrown away. Ames says her work was set back five or six months, but she expects to finish her PhD in the spring. For her part, Ames says that the experience shook her trust in her chosen profession. \"I did have doubts about continuing with science. It hurt my idea of science as a community that works together, builds upon each other's work and collaborates.\" Nevertheless, she has begun to use her experience to help teach others, and has given a seminar about the experience, with Ross, to new graduate students. She says that the assistance she got from Ross and others helped her cope with the ordeal. \"It did help restore the trust,\" she says. \"In a sense I was lucky that we could catch it.\" \n                 Brendan Maher is Nature's biology features editor.  \n                 Listen to Brendan Maher talk about this News Feature on the  \n                     Podcast \n                   . \n                     Misconduct special \n                   \n                     Theo Ross\u2019s Laboratory \n                   \n                     Office of Research Integrity\u2019s definition of research misconduct \n                   Reprints and Permissions"},
{"file_id": "467146a", "url": "https://www.nature.com/articles/467146a", "year": 2010, "authors": [{"name": "Lizzie Buchen"}], "parsed_as_year": "2006_or_before", "body": "Can epigenetics underlie the enduring effects of a mother's love? Lizzie Buchen investigates the criticisms of a landmark study and the controversial field to which it gave birth. After dropping a pair of male and female adult rats into a rectangular Plexiglas container, Frances Champagne can expect one of a few scenarios to ensue. The male will definitely try to mate with the female \u2014 but the female is less predictable. She might approach him, appraise his scents and arch her back to allow him to mount her. Should a second male enter the cage after she's mated with the first, she may be similarly hospitable. Some females play it coy, however, evading the male, requiring more courtship and, if mating does occur, avoiding another go. A number of factors can influence what the female does, but to Champagne, a behavioural scientist at  Columbia University  in New York, one is particularly beguiling: how often the female rat's mother licked and groomed her during her first week of life 1 . Doting mothers have prudish daughters, whereas the daughters of inattentive rats cavort around like mini Mae Wests. At the heart of these differences lies the sex hormone oestrogen, which drives female sexual behaviour. Champagne says that neglected rats might respond to it more strongly than those raised by attentive mums. The phenomenon is just one example of how experiences early in life can shape behaviour, and it may apply to humans. It is known, for example, that children who grow up in poverty are at greater risk as adults for problems such as drug addiction and depression than those with more comfortable upbringings, regardless of their socioeconomic situation later in life. But what is it about early experiences that has such a lasting effect? For Champagne and many of her colleagues, the answer has been apparent for nearly a decade. Life experiences alter DNA; not necessarily its sequence but rather its form and structure, including the chemicals that decorate it and how tightly it winds and packs around proteins inside the cell. These changes, often referred to as epigenetic modifications, make genes easier or more difficult for the cell's protein-making machinery to read (see 'The marking of a genome').  \n               Click here for larger image \n               The most enduring epigenetic change is thought to be the attachment of methyl groups to specific nucleotides in DNA, which can completely silence the expression of nearby genes. Champagne says that her neglected rats might have less methylation near the oestrogen receptor gene. And such differences occur specifically in regions of the hypothalamus known to be involved in sexual behaviour. Less methylation leads to increased expression of the oestrogen receptor throughout life, she reasons, making the adult daughters more responsive to the hormone's influence when sizing up suitors. The idea that epigenetics could explain the lasting effects of something as short-lived but profound as a mother's affection has breathed life into the behavioural sciences, providing a molecular middle ground in the centuries-old debate over nature versus nurture. Epigenetic changes could be the conduit through which environment elicits life-long biological change. Many behavioural scientists have latched onto the idea, searching for epigenetic explanations for a number of differences in behaviour, including homosexuality, intelligence and conditions such as autism and schizophrenia. Although experience has been connected to altered methylation for only a handful of genes, epigenetics has become one of the hottest areas of behavioural science. But it is also one of the most hotly contested.  \n                A struggle for acceptance \n              Behavioural epigeneticists have run up against deep resistance to their ideas \u2014 generally from molecular biologists and biochemists, who have been studying DNA methylation since the 1960s. When methyl groups coat DNA early in embryonic development, the affected genes are turned off for the life of the animal; for example, this mechanism permanently shuts down one of the two X chromosomes in female mammalian cells. Many find the idea that DNA methylation could be influenced by maternal care hard to swallow. Behavioural epigenetics \"is a field that has a lot of deep problems\", says Timothy Bestor, a geneticist also at Columbia University who studies DNA methylation in sex cells. The evidence supporting it is weak and grossly over-interpreted, according to Bestor and other critics, and the mechanism by which it works remains unclear. To prove the field's worth to the hard-core molecular biologists, the behaviourists will have their work cut out for them. The debate has been raging since the early 2000s, when Champagne was in graduate school at  McGill University  in Montreal, Canada. Her adviser, Michael Meaney, a behavioural scientist, was trying to explain why rats raised by attentive mothers were, as adults, able to deal with stress better than rats raised by more negligent mothers. Meaney's group found that levels of the glucocorticoid receptor, a protein that regulates the reaction to stress hormones, were different in these two groups, but the group puzzled over how the difference came about. Serendipitously, Meaney met Moshe Szyf, a molecular biologist at McGill who was studying DNA methylation in cancer. His research had shown that DNA methylation might act as an on\u2013off switch for cancer-causing genes 2 . As the two discussed Meaney's stressed-out rats, they wondered whether the same mechanism might be at play. They collaborated, and found different methylation patterns between groomed and ungroomed pups. Their work suggested that a mother rat's preening could remove methyl groups from her pups' DNA. This alteration, they argued, made the gene for the glucocorticoid receptor more accessible to protein-making machinery. Szyf was excited, but surprised. DNA methylation was thought to be stable. For this reason, says Szyf, the paper struggled through the review process, and was rejected by both  Nature   and  Science . \"The main review was, 'We never heard that DNA methylation changes after birth',\" he says. \"Something that doesn't fit with their dogma has to be wrong.\" After two-and-a-half years, the paper found an outlet in  Nature Neuroscience 3  in 2004. And among behavioural neuroscience researchers, Meaney says, it caused a stir. \"They understood immediately that epigenetic mechanisms were a great candidate that could explain the enduring effects of the early environment,\" he says. A deluge of research projects ensued, and are beginning to bear fruit. In December, Dietmar Spengler at the  Max Planck Institute of Psychiatry  in Munich, Germany, and his colleagues showed that separating mouse pups from their mothers for short periods of time reduced the methylation near the arginine vasopressin gene, possibly leading to a depression-like condition 4 . In May, David Sweatt at the  University of Alabama at Birmingham  showed that stress in early life changed the methylation status of the rat  Bdnf   genes, which encodes a growth factor involved in brain development and plasticity 5 . The work is starting to extend to humans as well. In 2009, Meaney and his collaborators compared the brains of people who had committed suicide and who were severely abused as children with those who were not. His data suggested that those who had been subject to abuse showed methylation changes in stress-related genes similar to those found in rats raised by inattentive mothers 6 . That same year, epigeneticist Jessica Connelly at  Duke University  in Durham, North Carolina, and her colleagues found methylation differences in the gene encoding the receptor for oxytocin \u2014 a hormone believed to influence social behaviour \u2014 in people with autism 7 . Now at the University of Virginia in Charlottesville, Connelly is pursuing this link further in humans and in prairie voles, which form close social bonds, so can potentially be used to help study human social behaviour. And Champagne, who started her own lab in 2006, is teaming up with researchers at the Columbia Center for Children's Environmental Health to see if pollution in northern Harlem and the South Bronx in New York is leading to epigenetic changes in children, making them more prone to conditions such as asthma and obesity. Meany's 2004 study 3  eventually became one of  Nature Neuroscience 's most cited papers. But the criticisms have not stopped. Molecular biologists' main problem with the behavioural neuroscientists' data is that they are highly correlative, and the underlying mechanisms are still largely unknown. Scientists who cut their teeth on  in vitro   systems \u2014 with their exacting control of variables and unambiguous data \u2014 cannot wrap their heads around messy data and tenuous links. \"What's really important to understand is the enormous gap in mechanistic knowledge between people who work in simple systems, such as epigenetic inheritance in yeast, where people have spent years to go over mechanistic details and really understand how it works, to someone who looks at the enormous complexity of the brain,\" says Catherine Dulac, a molecular biologist at  Harvard University  in Cambridge, Massachusetts.  \n                The mechanics of upbringing \n              One of the biggest bones of contention with the work of Meaney and Champagne is that there is no proven mechanism for actively removing methyl groups from DNA. A number of groups have proposed and published evidence of a 'demethylase' that would do this. Szyf's group, for example, published results in 1999 showing that a protein called MBD2 could rapidly remove methyl groups from DNA 8 . But critics argue that these have not stood the test of reproducibility. Adrian Bird, who studies DNA methylation at the  University of Edinburgh , UK, calls it one of many \"false alarms\". He showed in 2001 that mice lacking the  Mbd2   gene have normal patterns of DNA methylation, suggesting that it does not have a demethylase role 9 . The crucial missing piece, he says, is a pure biochemical demonstration of enzymatic activity. \"No one can take a piece of methylated DNA, mix it with some enzyme  in vitro   and demethylate it. It doesn't mean it can't happen, but show me the incontrovertible evidence.\" Another issue is that the methylation changes documented by that Meaney, Szyf, Champagne and others seem trivially tiny, appearing on only a handful of nucleotides in a small region of a gene, for example. And those changes only occur in a fraction of the total cells. To epigeneticists who study robust changes in methylation such as those that occur in embryos and zygotes, this looks like noise. It is also unclear whether these changes are actually happening in neurons. They could, for example, be happening in glia, cells that mainly provide support and protection for neurons. \"Quite often one sees statistically significant differences in DNA methylation but they're very small,\" says Bird. \"The question is, are they biologically significant in addition to statistically significant?\" Proponents are unfazed by the wall of scepticism. Szyf insists that his group has shown that small changes make a difference. In their human study, he says, \"we only got a few cytosines methylated, and that made us worried\". So they engineered a version of the DNA region they were studying \u2014 near a glucocorticoid receptor gene \u2014 that had only those specific sites methylated. It shut down gene expression for cells in a dish 6 . Bestor, still unconvinced, responds that because the researchers only provide percentages of methylation in the brain, it is hard to tell whether any given cell would actually have all the sites methylated. As for the demethylase, Champagne says one has to exist. \"There's a chemical reaction. DNA methyl groups are coming off. So there has to be an enzyme to do it.\" Plant cells have a well characterized demethylase, and many suspect that one will turn up in animal cells. Szyf, moreover, stands by the  Mbd2   findings that Bird could not corroborate. As for the size of the changes, Champagne understands why many molecular biologists are wary. \"If you're used to seeing effect sizes in a cell-culture dish, the kinds of effect sizes reported in behavioural models might mean nothing to you,\" she says. \"If you saw that effect in a dish, it'd be error.\" But she points out that small changes can have big effects in the nervous system. \"With behaviour, it's so dependent on where the changes are happening in the brain, what part of the circuits are affected,\" says Champagne. She acknowledges, however, that there are still many unknowns. She also appreciates that scepticism is healthy for the field, which even she thinks may be getting too hot, too fast. Critics, she says, \"keep everyone honest. The enthusiasm in the field is obviously great, but I think people's expectations of what this means need to chill out a little bit. There are a few things we need to work out\". Szyf says, however, that the responses to behavioural epigenetics reflect a difference in ethos in fields that need to come together. \"The psychiatry field is glad to have this mechanism they were missing,\" he says. \"It was the thing that bothered them and now it's like, 'Oh wow, this makes so much sense.' But then the epigeneticists say, 'Oh come on, that's just magic.'\" Szyf says that almost all of the papers that cite his and Meaney's 2004 study are from the behavioural sciences \u2014 not genetics or molecular biology. Some molecular biologists are warming to the idea, however. \"We're starting to see this in more than one gene, more than one neuronal region,\" says Huda Zoghbi, a geneticist at  Baylor College of Medicine  in Houston, Texas, who studies the role of DNA methylation in a form of mental retardation. \"There are a few issues, but I think it's intriguing and we really have to take stock in it, and start thinking about how this is happening.\" Determining exactly how it is happening remains the challenge. Even if researchers can work out how methyl marks are removed from a gene, they have to show the mechanism by which a life experience such as maternal care would cause that change. Meaney has proposed that mother rats' licks increase levels of the neurotransmitter serotonin, and that this increase could result in methylation changes, but no experiments have provided evidence that would explain how this link would work. The behavioural epigeneticists know they have work to do to answer the criticisms and they are setting about doing it. Szyf's lab is using a technique that can separate neurons from glia and other cell types so he can work out where the methylation is occurring, and Meaney says experiments are in progress to understand the series of events by which the environment might be altering methylation. Champagne is working with mice in hopes of doing more genetic experiments, and says that behavioural scientists are increasingly collaborating with classically trained molecular biologists. She has recently hired a postdoc trained in epigenetics. \"The question always becomes, how do you transduce a social experience to the level of DNA methylation? Right now it's very speculative. We don't know. To really study that, you have to go back to a dish, ultimately.\" Until these sorts of hard data arrive, many molecular biologists say that they will stay in the sceptics' corner. \"It's an interesting possibility, but I do think people have jumped the gun and seen more positive results than are really out there,\" says Bird. \"I'm perfectly happy to believe that it's very important, but I'm also happy to believe that it's irrelevant.\" Lizzie Buchen is a freelance writer based in San Francisco, California. \n                     Frances Champagne \n                   \n                     Michael Meaney \n                   \n                     Moshe Szyf \n                   \n                     Adrian Bird \n                   \n                     Tim Bestor \n                   Reprints and Permissions"},
{"file_id": "467514a", "url": "https://www.nature.com/articles/467514a", "year": 2010, "authors": [{"name": "Daniel Cressey"}], "parsed_as_year": "2006_or_before", "body": "The ten-year Census for Marine Life is about to unveil its final results. But how deep did the $650-million project go? It took just an hour and a half to get the ball rolling, says Jesse Ausubel, thinking back to the day in July 1996 when Frederick Grassle came to his office at the Marine Policy Center of the Woods Hole Oceanographic Institution in Massachusetts. Grassle, a marine scientist at Rutgers, the State University of New Jersey in New Brunswick, had come armed with a year-old report from the US National Research Council highlighting just how little scientists understood about marine biodiversity. Even well-explored ecosystems such as coral reefs, temperate bays and estuaries contained vast numbers of undiscovered species, to say nothing of the unknown organisms lurking in remote, under-sampled areas such as the polar seas and hydrothermal vents. The report, which Grassle had helped to write, argued that there was an \"urgent need\" to expand such research, not least because it is so important for fish management and marine conservation. Ausubel, who is vice-president of programmes for the Alfred P. Sloan Foundation in New York and an adjunct scientist at Woods Hole, was astounded. \"I knew that the measurements of life, especially at the species level, were not very good or plentiful,\" he recalls. \"But I learned from him that just the most basic things hadn't been done.\" None of the usual government agencies seemed willing or able to tackle the problem, said Grassle, who had been doing his best to talk them into it. But the Sloan Foundation had a mandate to back ambitious projects that had trouble securing funding from traditional sources \u2014 which was why Grassle had come to see Ausubel. \"At the end of the conversation, we agreed that we should try to do something big,\" says Ausubel. That 'something big' \u2014 originally a fairly straightforward survey of marine fish \u2014 evolved into perhaps the largest and most expensive programme of marine-biology research ever (see  'An oceanic inventory' ). The decade-long  Census of Marine Life , which will officially conclude with the announcement of the full census on 4 October, ended up involving scientists from more than 80 countries, in studies not only of fish, but also of organisms such as sea birds, marine mammals, invertebrates and plankton. The scientific goals of the census are as simple as they are ambitious: diversity, distribution and abundance. What lives in the sea? Where does it live? And how much of it is there? Granted, the project is still a long way from fully answering those questions; a multitude of gaps remains to be filled by future research. And there are doubts about how much of a future there will be: in many countries, marine census projects are still seeking continuing funding. Nonetheless, the idea that Grassle and Ausubel concocted on that July day in 1996 \"has exceeded our wildest dreams\", says Ronald O'Dor, a biologist at Dalhousie University in Halifax, Nova Scotia, Canada, echoing a sentiment widely expressed by census participants. Discoveries include a tubeworm that drills for oil in seeps at the bottom of the Gulf of Mexico, and then eats it; the finding that despite the 11,000 kilometres between the polar seas, at least 235 species are found in both; and the existence of a 'brittlestar city', in which tens of millions of starfish-like creatures live arm-tip to arm-tip atop a seamount south of New Zealand. \"The programme has produced, to date, more than 2,500 publications and has made accessible more than 30 million distributional records that are available to everyone,\" says Ian Poiner, chief executive of the Australian Institute of Marine Science in Townsville, Queensland, and chair of the census's scientific steering committee. \"I would doubt we could be criticized for our contribution to science.\" With Ausubel's support, the Sloan Foundation eventually put some US$75 million into the census, which formally began in 2000. But that was only a down payment to cover the project's organizational infrastructure \u2014 the committees, meetings and interactions between the thousands of scientists worldwide. To fund the research itself, these scientists had to seek out further funding from their respective governments and other sources. The global, ten-year total comes to roughly $650 million.  \n                All the fish in the sea \n              The various national efforts were coordinated under 14 census field projects. One example was the Mid-Atlantic Ridge Ecosystem Project, which mapped the organisms living over and around the ridge using everything from manned submersibles and robotic gliders to more traditional fishing equipment such as trawl nets. Another was the Census of Marine Zooplankton, which used techniques ranging from DNA bar-coding to specially developed upwards-scanning sonar to monitor the roughly 6,800 species of plankton. The census also included projects to understand the history of marine animals, and to model how they would be affected in the future by ecological forces such as fishing and climate change. Most importantly, according to many participants, the census created an Ocean Biogeographic Information System database to hold the millions of records generated by the surveys. Broadly speaking, says Ausubel, \"the greatest advances of the census are in diversity, somewhat less in distribution\". When the full roster of results is unveiled next month, those advances will include at least 5,000 new species \u2014 many of them strikingly photogenic (see  'Highlights from the deep' ) \u2014 and the publication of many new range maps. But the results on abundance have been patchy. \"Abundance is the hardest,\" says Ausubel. First the species have to be discovered, then enough observational data have to be collected to create a range map, and then more data are needed on the numbers. Only then can an estimate of biomass be extrapolated. This incompleteness has fuelled critics of the census, who fault its decentralized organization and the huge number of broad projects that resulted. \"Unfocused\", is the sceptical summary of Alan Longhurst, a retired marine biologist and author of  Ecological Geography of the Sea . Perhaps so, says Paul Snelgrove, an oceanographer at the Memorial University of Newfoundland in St John's, Canada, and chair of the census synthesis group. But without the census, Snelgrove argues, the various national survey projects might have been performed \"on a smaller scale and also more in a haphazard fashion\" \u2014 if at all. The census was \"a bit of a roulette\", says Carlo Heip, general director of the Royal Netherlands Institute for Sea Research in Texel and a member of the census's scientific steering committee. \"It was not precise planning of what was going to be funded or not.\" But Heip maintains that there were no major gaps in the census, as the committee made a point of identifying key individuals in the various countries with the power to get proposals financed in priority areas. Some census participants even hold up its decentralized structure as a model for future big science projects. It does offer practical advantages, says Niki Vermeulen, who researches scientific collaboration in biology at the University of Vienna in Austria, and who studied the census for her book,  Supersizing Science . She says that large international research projects often falter because of the desire of member countries to fund only their own researchers. \"The census structure at least provides a way of solving that issue,\" she says. \"To say, 'Okay, we do the global coordination from separate money, and for the research projects we can still go to the national funding.'\" Looking back on it, says Ausubel, \"have we done everything that the public expects a census to do? Probably not.\" But the creation of the framework is \"historic\", he says. \"That in itself is huge.\" \"I don't think ten years is the time we should be assessing it,\" agrees James Sanchirico, who studies marine management at the University of California, Davis. \"Maybe it's at 20 years you can look back and say, what has been the impact?\" he says, once it has become clear how the data have been used by scientists and decision-makers alike. Meanwhile, most of the scientists involved in the first census would like to see a second. Without it, the collaborative framework they built in the first decade \u2014 which many cite as the census's most valuable achievement \u2014 could begin to dissipate. \"Unless we find a sugar daddy who is committed to holding these projects together,\" says O'Dor, \"they're going to drift farther and farther apart.\" But the Sloan Foundation has always been clear that its funding would not continue beyond ten years. And no other organization has, as yet, agreed to take its place. Complicating the situation is the fact that there are two very different possibilities for future work in this area, says O'Dor. One is to repeat the census over another ten-year period to monitor how the known populations change. The other is to continue looking for more species. Although O'Dor says he can put a back-of-the-envelope figure of \"a few hundred million dollars\" on the first option, there is no real limit on the money scientists could spend on the second. \"These two jobs are competing with each other,\" says O'Dor \u2014 and the community has yet to agree how to divide the available funding between them. Grassle, who is on a quest to find support for a repeat census, is undaunted. \"Somehow it will happen,\" he insists. \"The rewards are too great to ignore it.\" Daniel Cressey is a reporter for Nature. \n                     Global patterns and predictors of marine biodiversity across taxa \n                   \n                     Census of Marine Life \n                   \n                     Ocean Biogeographic Information System \n                   \n                     Alfred P Sloan Foundation \n                   Reprints and Permissions"},
{"file_id": "467648a", "url": "https://www.nature.com/articles/467648a", "year": 2010, "authors": [{"name": "Quirin Schiermeier"}], "parsed_as_year": "2006_or_before", "body": "After a near-death crisis, the best gravity sensor in space is back to full strength, providing data that will keep scientists on the level. On 18 July, geophysicist Reiner Rummel received a phone call that made his heart sink. The European Space Agency (ESA) had stopped receiving data from a \u20ac350-million (US$471-million) satellite that Rummel had spent nearly 20 years designing, building, testing and shepherding into orbit. The craft, burdened with the unwieldy name of the Gravity Field and Steady-State Ocean Circulation Explorer (GOCE), had been in space for little more than a year. Now it looked as if it might be lost for good, undone by a glitch in the communications system that sends data back and forth from the satellite to the ground. Rummel, who works at the Technical University of Munich in Germany and is joint principal investigator of the GOCE project, got little sleep throughout the summer as he and his colleagues tried various strategies to restore the on-board computer system, which had failed once before. Mission scientists developed software to combine the system with a back-up computer so that they could fix the apparent problem, but still the craft failed to respond. Then, in early September, ground controllers tried something new. They sent a signal to raise the temperature of GOCE's computer compartment. At stake was the most finely tuned gravity sensor ever to fly in space.  GOCE  was designed to map the subtle gravitational differences that arise across the globe because Earth's mass of roughly six sextillion (10 21 ) tonnes is not distributed evenly. Using data collected by the small satellite, researchers planned to construct a sophisticated gravitational map called the geoid, accurate to the nearest centimetre: a fivefold improvement over previous efforts to map gravity from space. Such data would provide geoscientists with a global reference for precisely measuring the heights of continents, mountain peaks and the ocean surface, which is rising because of global warming. GOCE data could also reveal the scars from a giant extraterrestrial impact 250 million years ago, keep tabs on the shifting tectonic plates that cause huge earthquakes, and help to measure the strength of ocean currents such as the Gulf Stream \u2014 critical information that will improve climate forecasts. All this explains why researchers were mightily relieved when, as the GOCE computer warmed up, it sputtered back to life.  \n                A massive mission \n              GOCE races across the skies at 30,000 kilometres per hour, skimming the edge of Earth's atmosphere at an altitude of about 250 kilometres. The craft has to maintain such a low orbit to make its measurements because gravity falls off rapidly with distance. But flying through the atmosphere creates drag on the satellite, so project engineers have given it an unusually aerodynamic design and an ion engine to counteract the air friction. From its low orbit, GOCE's main on-board instrument, a gravity gradiometer, is sensitive enough to measure the gravitational tug of giant reservoirs on Earth's surface. \n               Click here for larger image \n               The heart of the sensor consists of three pairs of cubes, arranged perpendicular to each other to form a three-dimensional cross. As the satellite passes overhead, the cubes each feel a different pull from the mountains and other masses on and below the planet's surface. An electrostatic control system measures those differences with such precision that GOCE can detect changes on the order of one-millionth of the average strength of Earth's gravity at the surface (see  'The pull of the planet' ). Because the gradiometer is so sensitive, engineers could not truly test it before launch. They simulated zero gravity by dropping the sensor within a tall tower, but this was not an ideal test. \"We're basically flying a prototype in orbit,\" says Rummel. After GOCE was launched in March 2009, it took several months to fix problems such as difficulties in orienting the satellite with the required accuracy. But the team eventually achieved the planned sensitivity for the gravity measurements, and released its first big batch of data at an ESA symposium in Norway in June. If no further glitches occur, GOCE will complete its planned mission in April 2011. \"It's a tremendous achievement,\" says Philip Woodworth, a sea-level expert at the National Oceanography Centre in Liverpool, UK. \"GOCE-derived gravity data will be usable for decades.\" Researchers are already turning the initial harvest of data into a highly accurate geoid, a mathematically derived surface resembling a hypothetical mean global sea level. Real sea levels respond to winds, currents and other dynamic features of the planet. The geoid, by contrast, reflects what the ocean surface would look like if the world were covered by a static skin of water whose height is influenced only by gravity. It is a bulbous affair, with bumps and dips matching mountains, ocean trenches and density variations deep within the planet. At every point, the force of gravity operates perpendicular to the surface of the geoid. The geoid was first conceived of in 1828 by German mathematician Carl Friedrich Gauss, who recognized that surveyors would need such a reference surface to determine the precise elevation above sea level for any point on Earth. To this day, uncertainties in the height of the geoid make it difficult to compare altitude measurements for different parts of Earth's surface. GOCE will finally remedy this. The global geoid derived from the gravity measurements taken since April shows, among other things, a pronounced 'depression' in the Indian Ocean and 'plateaus' in the North Atlantic and western Pacific \u2014 mirroring convective activity and density anomalies in Earth's mantle. More of these delicate features will be added to the map in the coming months.  \n                Current events \n              Geodesists \u2014 scientists who study Earth's shape \u2014 have for the past few years been using a different space mission to map the planet's gravity field. The Gravity Recovery and Climate Experiment ( GRACE ), a joint mission of NASA and the German space agency, was launched in 2002 and uses a pair of satellites to measure relatively large-scale gravity variations, such as the loss of mass from melting sections of the Greenland ice cap. GOCE adds detail to the picture, with gravity measurements five times more precise than those of GRACE and with almost three times higher spatial resolution. Armed with its data, researchers will be able to distinguish between the ocean topography shaped by gravity and that of the 'hills' and 'valleys' of water created by wind, pressure gradients and Earth's rotation. That will help scientists to measure ocean circulation, because large currents are slightly higher than the surrounding ocean, with the topographic height of the current directly related to its strength. \"You can think of the Gulf Stream as a gentle hill that you'd need to climb if you could walk from Boston to the Bermudas,\" explains Woodworth. \"If you're able to accurately measure the shape of the hill you can calculate the mean strength of the flow.\" GOCE-derived data should therefore allow researchers for the first time to calculate the mean strength of the Gulf Stream, which carries warm water north from the Gulf of Mexico towards the Arctic. Such data on this and other currents are essential for improving computer models of the ocean and the atmosphere. Geoid information should also help to determine the mean sea level to within a few millimetres, something not possible before. Local sea-level changes, such as those from one year to the next, can be measured by radar altimetry and tide gauges. But geodesists need an accurate global reference to compare measurements on different continents. Even regional comparisons are difficult without an accurate geoid. For example, the UK National Tide Gauge Network suggests that the sea level off the north coast of Scotland is 30\u201340 centimetres lower than that off the coast of Cornwall, a difference that is almost certainly an artefact caused by levelling errors, says Woodworth. The high-resolution gravity data from GOCE will also help to measure ice loss from Antarctica and Greenland, and can aid geologists in assessing the potential for large earthquakes. The biggest shocks, such as the magnitude-8.8 quake that struck Chile in February, rearrange Earth's tectonic plates, causing a change in the gravity field that can be sensed from space. GOCE data will give researchers new clues as to how masses of different densities readjust before and after large seismic events \u2014 and whether it is possible to measure gravity effects before the ground ruptures, says Roberto Sabadini, a geophysicist at the University of Milan in Italy, and a participant in GOCE. The gravity data may even help to shed light on a planetary disaster: the 'Great Dying' that wiped out a large fraction of existing species 250 million years ago, at the end of the Permian period. Ralph von Frese, a geophysicist at Ohio State University in Columbus, belongs to a school of scientists who suspect that this mass extinction was triggered by the impact of a giant meteorite, more than twice the size of the one thought to have killed the dinosaurs 65 million years ago. Their search for the crater left by this enormous impact focuses on eastern Antarctica, where GRACE data and airborne-radar imagery indicate there might be a depression, 500 kilometres in diameter, under the 2\u20133-kilometre-thick ice in Wilkes Land ( R. R. B. von Frese  et al .  Geochem. Geophys. Geosyst.    10,   Q02014; 2009 ). The impact would probably have lifted high-density mantle material into Earth's crust, causing a lasting gravity anomaly. But because the GRACE signal is blurry, von Frese is eagerly awaiting GOCE data that could back his theory. He and others are crossing their fingers that the fragile satellite will stay healthy for the next six months and complete its mission without any more drama. \n                     GOCE \n                   \n                     GRACE \n                   Reprints and Permissions"},
{"file_id": "467650a", "url": "https://www.nature.com/articles/467650a", "year": 2010, "authors": [{"name": "Heidi Ledford"}], "parsed_as_year": "2006_or_before", "body": "Amateur hobbyists are creating home-brew molecular-biology labs, but can they ferment a revolution? Rob Carlson's path to becoming a biohacker began with a chance encounter on the train in 1996. Carlson, a physics PhD student at the time, was travelling to New York to find a journal article that wasn't available at his home institution, Princeton University in New Jersey. He found himself sitting next to an inquisitive elderly gentlemen. Carlson told him about his thesis research on the effects of physical forces on blood cells, and at the end of the journey, the stranger made him an offer. \"You should come work for me,\" said the man, \"I'm Dr Sydney Brenner.\" The name meant little to Carlson, who says he thought: \"Yeah, OK. Whatever, 'Dr Sydney Brenner.'\" It wasn't until Carlson got back to Princeton and asked a friend that he realized that \"Dr Sydney Brenner\" was a famed biologist with a knack for transforming the field. He took the job. Within a year, Carlson was working with a motley crew of biologists, physicists and engineers at Brenner's Molecular Sciences Institute (MSI) in Berkeley, California, learning molecular biology techniques as he went along. The institute was a hotbed of creativity, and reminded Carlson of the scruffy hacker ethos that had spurred the personal-computing revolution just 25 years earlier. He began to wonder if the same thing could happen for biotechnology. What if a new industry, even a new culture, could be created by giving everyone access to the high-tech tools that he had at his fingertips? Most equipment was already for sale on websites such as eBay. Carlson penned essays and articles that fanned the embers of the idea. \"The era of garage biology is upon us,\" he wrote in a 2005 article in the technology magazine  Wired . \"Want to participate?\" The democratization of science, he reasoned, would bring in new talent to build and improve scientific instrumentation, and maybe help to uncover new industrial applications for biotechnology. Eventually, he decided to follow his own advice, setting up a garage lab in 2005. \"I made the prediction,\" he says, \"so I figured maybe I should do the experiment.\" Carlson is not alone. Would-be 'biohackers' around the world are setting up labs in their garages, closets and kitchens \u2014 from professional scientists keeping a side project at home to individuals who have never used a pipette before. They buy used lab equipment online, convert webcams into US$10 microscopes and incubate tubes of genetically engineered  Escherichia coli   in their armpits. (It's cheaper than shelling out $100 or more on a 37 \u00b0C incubator.) Some share protocols and ideas in open forums. Others prefer to keep their labs under wraps, concerned that authorities will take one look at the gear in their garages and label them as bioterrorists. For now, most members of the do-it-yourself, or DIY, biology community are hobbyists, rigging up cheap equipment and tackling projects that \u2014 although not exactly pushing the boundaries of molecular biology \u2014 are creative proof of the hacker principle. Meredith Patterson, a computer programmer based in San Francisco, California, whom some call the 'doyenne of DIYbio', made glow-in-the-dark yogurt by engineering the bacteria within to produce a fluorescent protein. Others hope to learn more about themselves: a group called DIYgenomics has banded together to analyse their genomes, and even conduct and participate in small clinical trials. For those who aspire to change the world, improving biofuel development is a popular draw. And several groups are focused on making standard instruments \u2014 such as PCR machines, which amplify segments of DNA \u2014 cheaper and easier to use outside the confines of a laboratory, ultimately promising to make DIYbio more accessible. \n               Click here for larger image \n               Many traditional scientists are circumspect. \"I think there's been a lot of overhyped and enthusiastic writing about this,\" says Christopher Kelty, an anthropologist at the University of California, Los Angeles, who has followed the field. \"Things are very much at the beginning stages.\" Critics of DIY biology are also dubious about whether there is an extensive market for garage molecular biology. No one needs a PCR machine at home, and the accoutrements to biological research are expensive, even if their prices fall daily (see  graphic ). Then again, the same was said about personal computers, says George Church, a geneticist at Harvard Medical School in Boston, Massachusetts. As a schoolboy, he says, he saw his first computer and fell in love. \"Everybody looked at me like, 'Why on earth would you even want to have one of those?'\" Carlson started his garage lab as something of a hobby, but he needed to do it without sapping resources from his lab at the University of Washington in Seattle. He bought equipment such as refurbished micropipettes \u2014 a staple in any molecular biology lab \u2014 and a used centrifuge on eBay. In 2007, fed up with grant applications and eager to spend more time working in his garage lab, he gave up his position at the university altogether. Carlson decided to follow up on work at the MSI. There, he had been part of a team developing a way to quantify small amounts of proteins in single cells using 'tadpoles', in which a protein 'head' is attached to a DNA 'tail'. The head was designed to bind to a protein of interest, and the DNA tail could be amplified and quantified by PCR, allowing researchers to calculate the number of proteins present ( I. Burbulis  et al. Nature Meth.    2,   31\u201337; 2005 ). The tadpoles have economic potential, providing an alternative to the standard approach of using fluorescently tagged antibodies, which provide at best only rough estimates of protein levels. But the original formulation was too expensive to commercialize, says Carlson. \"If I could use this protein in the garage in a simple way to show that it would work, then hopefully it would be a product that would be useful in a low-tech setting, out in the field or in a doctor's office,\" he says. As Carlson worked, the idea of garage biohacking was taking off. In May 2008, Jason Bobe, director of community outreach for the Personal Genome Project at Harvard Medical School, and Mackenzie Cowell, a web developer in Cambridge, Massachusetts, organized the first meeting of DIYbio at the Asgard Irish pub, up the road from the Massachusetts Institute of Technology. About 25 people turned up. Two years later, there are more than 2,000 subscribers on the DIYbio e-mail list. No one knows how many of those 2,000 are serious practitioners \u2014 Bobe jokes that 30% are spammers and the other 70% are law-enforcement officials keeping tabs on the community. But many DIY communities are coalescing: not only in Cambridge, but also in New York, San Francisco, London, Paris and the Netherlands. Some of these aim to develop community lab spaces with equipment that users could share for a monthly fee. And several are already affiliated with local 'hacker spaces', which provide such services to electronics enthusiasts. For example, the New York DIYbio group meets every week at the work-space of an electronics-hacker collective called NYC Resistor, which now has a few pieces of basic molecular biology equipment, including a PCR machine. DIYbio is an offshoot of the open-science movement, which encourages an open exchange of materials, data and publications and has its origins in the push for open-source software in the 1990s, says Kelty. Many biohackers are also keen to tackle projects that involve engineering cells by piecing together new genetic circuits, an approach often called 'synthetic biology'. DIYbio has picked up both momentum and stigma from this field, which has been alternately hyped and decried as the solution to society's ills or the nursery for a bioterrorist scourge. The thought of hundreds of biohackers creating pathogens in unmonitored garage biology labs set off alarm bells, and in 2009, the Federal Bureau of Investigation (FBI) began sending representatives from its directorate for weapons of mass destruction to DIYbio conferences. Biohackers are wary. They recall what happened to Steve Kurtz, an artist who was using bacteria shipped to him by a Pittsburgh geneticist. In 2004, federal agents stormed his house in hazmat suits with guns drawn. Kurtz was arrested and saddled with mail-fraud charges that took him four years to clear. Bobe has interacted with and advised the FBI, but says he finds many of the biosecurity fears of the FBI and the public to be unfounded. \"The amateur activity right now is at the seventh- or eighth-grade level,\" he says. \"We're making $10 microscopes and all of the discussion around us is about weaponized anthrax. Sure we're concerned about that just like everybody else, but I don't know what to say except 'Yeah, that sounds scary as hell. Let's be sure nobody does that.'\" The FBI seems to have taken that message on board, and has adopted what some call a 'neighbourhood watch' stance. The approach relies on biohackers monitoring their own community and reporting behaviour they find threatening, says Edward You, a special agent in the FBI's bioterrorism unit. Carlson's projects are more advanced than those of the average DIYbio hobbyist, and he has found that the garage-hacker ethos eventually suffered. He says he sometimes found it hard to persuade companies to deliver lab supplies to a residential address. Carlson also wanted his garage back to restore a boat. So, Carlson and his business partner, engineer Rik Wehbring, moved their lab out of the garage and into a small commercial space in 2009. The two fund the space and their experiments through a small consulting firm called Biodesic. Through the firm, they have advised companies on a range of technology issues from biosecurity to designing brainwave-based game controllers. Other biohackers have also come up with creative ways to fund their projects. Several have used websites such as Kickstarter, which allows inventors to post their projects and funding targets online. Visitors to the site make donations, usually small ones, but the hope is that enough visitors making tiny contributions will add up. Two California garage biohackers, Tito Jankowski and Josh Perfetto, used Kickstarter to fund the development of a small, low-cost PCR machine known as OpenPCR. They reached their fundraising goal of $6,000 in ten days. By the time their Kickstarter listing closed 20 days later, they had doubled that figure. Another group of biohackers used Kickstarter to raise funds for a hackerspace called BioCurious, based in Silicon Valley, California. They raised more than $35,000. But all of this is tiny compared to the cost of launching an actual business. Joseph Jackson, a self-proclaimed \"professional entrepreneur-slash-activist\" from Mountain View, California, and Guido Nunez-Mujica, a computational biologist from Venezuela, have teamed up with other hackers to build a portable PCR machine known as LavaAmp, which can be run from a computer's USB port. The team has poured tens of thousands of dollars into the project, says Jackson, but will need closer to $100,000 to achieve its goal of producing PCR machines that could be used by hobbyists, teachers and by researchers in developing countries. Jim Collins, a synthetic biologist at Boston University, says that the costs of doing molecular-biology research make the comparison between amateur biologists and the hackers who drove the personal-computer revolution inappropriate. There's a vast chasm between these tinkerers and those with access to a traditional lab. Faculty members, Collins says, typically ask for hundreds of thousands of dollars from a university to start a molecular-biology lab. Smart amateurs might be able to bring fresh perspective, he says, but they face an uphill battle. \"I'm not saying you need to be appropriately pedigreed. I'm saying you need to be appropriately resourced.\" Carlson says that the cost of biological research is decreasing. \"The predominant thought about biology used to be that it was expensive and hard,\" he says. \"And it's still hard. It's just not so expensive.\" In 2003, he projected the falling costs of sequencing and synthesis of DNA and proteins, and the accelerating pace of research into areas such as protein structure determination ( R. Carlson  Biosecur. Bioterror.    1,   203\u2013214; 2003 ). His predictions echo Moore's law of computing, and some have dubbed them the 'Carlson curves'. But the curve trajectory isn't as steep as Carlson might like. He has redesigned the protein heads of his tadpoles, and decided early on that instead of producing the protein himself \u2014 an expensive and arduous process \u2014 he would pay a company to make it for him. He could either buy cheap protein that was contaminated with other proteins, for about $3,000, or buy clean protein for about $50,000. \"There was nothing in between,\" he said. He took the cheap route, but found that the batches he received weren't clean enough to publish his results or start selling the finished tadpoles. The project stalled. A few months ago, Carlson realized that more protein-synthesis companies had entered the scene, including several that filled the middle range pricing gap. He ordered a fresh batch of protein that was supposed to arrive more than a month ago, but still hasn't been delivered. \"If we had a million dollars in the bank, this problem would have been solved a long time ago,\" he says. \"And if I had an experienced biochemist or molecular biologist at the bench for a year or two it probably would have cost the same and would have been done faster.\" Still, five years after taking science into his garage, Carlson says he's convinced that biohacking has the potential to trigger a technological revolution. \"We're going to see a lot more at the garage level that will produce a variety of products in the marketplace, one way or another,\" he says. Once his tadpoles have been optimized, Carlson hopes that publishing his work will attract further investors. Meanwhile, he feels his experiment in garage-based innovation has so far been a success, despite the delays and personal sacrifices. \"Part of the exercise was to determine whether or not we could bootstrap this thing,\" he says. \"The answer appears to be 'yes'. As long as you are willing to be patient and to eat nothing but rice for dinner occasionally.\" \n                 See Editorial  \n                 \n                     p.634 \n                   \n               \n                     Nature Biotechnology: Basement biotech \n                   \n                     Nature Medicine: Personalized investigation \n                   \n                     Nature Network DIYbio Group \n                   \n                     DIYbio \n                   \n                     DIYbio Google group \n                   \n                     DIYbio NYC \n                   Reprints and Permissions"},
{"file_id": "467772a", "url": "https://www.nature.com/articles/467772a", "year": 2010, "authors": [{"name": "Geoff Brumfiel"}], "parsed_as_year": "2006_or_before", "body": "More than 100 cold-war era research reactors run on uranium pure enough to be used in a nuclear weapon. But switching to safer fuel isn't easy. It is a warm day in early autumn, and speckled sunlight shines through spruce and linden trees onto a crumbling road. Ahead stands a dilapidated brick building encircled by a chain-link fence. It seems peaceful enough. But attached to the fence is a sign that says  Teren Nadzorowany   \u2014 'supervised area' \u2014 topped by the faded remains of a red trefoil: the symbol used world-over to denote a radiation hazard. The sign is the only visible hint of what once lay inside the building: enough highly enriched uranium reactor fuel to make more than 18 nuclear bombs. A group of visiting journalists and US officials files into the building and looks down into pools of ultrapure water in which the fuel once lay. The water is still and clear, the fuel basins empty. Here at the Polish Institute of Atomic Energy in Otwock-\u015awierk, 30 kilometres southeast of Warsaw, fuel-removal operations have been under way for a year. They end today: the last shipment has been pulled out of the pools and packed away in seven bright-blue shipping containers, which are now sitting on trucks back at the gate. Soon, the containers will begin a high-security journey to a reprocessing plant in Russia, east of the Ural Mountains, where the fuel's uranium will be converted into a safer form for use in power reactors.  The trip will be a homecoming of sorts: this fuel originally came to Poland from Russia more than 30 years ago. But officials of the US National Nuclear Security Administration (NNSA), sensitized by the attacks of 11 September 2001, feared that this kind of highly enriched fuel posed too tempting a target for terrorists seeking to build a nuclear weapon. So a year ago, after much negotiation with the Polish government, they began clearing out the dangerous uranium and converting the reactor to run on safer fuel. For this last day of removal,  Nature   was granted a rare chance to watch the American programme in action. The reactor at the Polish institute is hardly unique: globally, roughly 130 research reactors in state laboratories and university campuses continue to run on bomb-grade fuel (see  map ). US President Barack Obama did not explicitly mention these reactors in April 2009, when he announced a substantial acceleration of established non-proliferation efforts and made a commitment to ensure the security of all vulnerable nuclear material within just four years. But many of the facilities clearly do fall into that 'vulnerable' category. \n               boxed-text \n             Research-reactor operators are not always enthusiastic about co\u00adoperating with the American initiative. Safer alternative fuels are available, but converting the reactors to use them is difficult, time-consuming and costly. \"We are not so happy to convert our reactor,\" says Grzegorz Krzysztoszek, the director of the Polish facility. But the NNSA is winning the operators over by offering to remove old uranium, help with the conversion process and pay for new fuel \u2014 a US$70-million proposition in this case. The US agency's Global Threat Reduction Initiative has already helped to convert or shut down 72 vulnerable research reactors worldwide. Working on Obama's accelerated timetable, the NNSA is now hoping to spend nearly US$3 billion over the next four years to secure or convert as many reactors as possible and clear out uranium from ten countries. \"It's an aggressive schedule,\" says Andrew Bieniawski, the head of the operation. \"But I do think we can make it.\"  \n                Left behind \n              Poland's research reactor was built during the cold war, and is one of many that the United States and the former Soviet Union helped to construct during that period. Named Maria after the Polish-born scientist Marie Curie, the 30-megawatt behemoth was actually one of two large research reactors that the Soviets helped to build at the institute starting in the 1950s. An important part of the deal was that the Soviets would also keep the reactors supplied with highly enriched uranium fuel. But the fall of the Soviet Union in 1991 led to hard times. Plans for a Polish commercial power reactor in the north of the country fell through, and the older of the institute's reactors was shut down in 1995. That left only Maria \u2014 which has become something of a relic. Trailblazing nuclear physics is now done at particle accelerators, not research reactors. Yet Maria hums on, Poland's last remaining tie to the nuclear age. Even if Maria is no longer at the cutting edge, Krzysztoszek says that the reactor has found its niche in more utilitarian research \u2014 primarily the use of neutrons from the core for imaging, and the production of medical isotopes. About 60% of Maria's US$5\u2011million annual budget comes from industrial clients, including a US company that hopes to use it to produce molybdenum-99, a short-lived isotope used in medical imaging. Outages in reactors in Canada and the Netherlands have created a dire worldwide shortage of this isotope (see   Nature  460, 312\u2013313; 2009 ), and the Polish institute hopes to profit from it. Domestically, the reactor also features in plans for a new nuclear age of energy independence. The Polish government wants to move away from using dirty coal or Russian-supplied gas, says Krzysztoszek. By 2021\u201322, it hopes to have two nuclear power plants in operation. But that will require training Polish scientists and engineers in the nuclear arts, and this reactor is the only domestic facility available. Similar stories can be heard at other research facilities around the world: they still do worthwhile work. \"We're not running a reactor just to run a reactor. We do things with it,\" says David Moncton, director of the research reactor at the Massachusetts Institute of Technology (MIT) in Cambridge, one of a handful on US soil that continues to operate with bomb-grade fuel. The MIT reactor is used to test new nuclear fuels and components, but it also serves less-obvious functions: for example, doping silicon semiconductors by transmuting some of the silicon into phosphorus. The semiconductor wafers are used as high-performance electronic components in hybrid vehicles. Whether a reactor is useful or not, the fuel is still a security nightmare. The concern is that a terrorist group might somehow get hold of it and extract enough of the fissile isotope uranium-235 to construct a crude bomb. When atoms of uranium-235 split, they produce energy and neutrons. The neutrons strike nearby uranium-235 nuclei, causing them to split and creating a self-perpetuating chain reaction, which, if uncontrolled, leads to an explosion. To prevent just anyone from gaining a nuclear weapon, fuel for commercial power reactors is diluted with copious amounts of uranium-238 \u2014 a more common isotope that does not split. Because the two isotopes are chemically identical, they are impossible to separate without highly specialized equipment. Most research reactors, however, run on up to 93% pure uranium-235, mixed with elements such as aluminium. This lets the reactors operate for longer and produce more neutrons with less fuel. But the uranium can easily be separated from other elements in the fuel, so extracting enough for a nuclear bomb is comparatively easy \u2014 and within the means of a sophisticated terrorist group.  \n                Vulnerable to attack \n              Compounding the problem of fuel security, research reactors are often housed in dilapidated state laboratories or the basements of university research buildings. \"Many of these places have very minimal security,\" says Matthew Bunn, an expert in nuclear terrorism at Harvard University in Cambridge, Massachusetts. The threat is more than academic: in 2007, two teams of armed men assaulted the Pelindaba reactor near Pretoria, South Africa. While one team engaged the site's security forces, who fled, the other men penetrated an electrified fence and made their way to an emergency control centre inside the facility. There, they shot a worker in the chest before fleeing. They were never apprehended. No uranium-235 fuel was reported missing \u2014 indeed, no one knows what the assailants were after \u2014 but the incident underscores the vulnerability of civilian nuclear facilities. Security was much on the mind of the NNSA negotiators as they worked out a deal with the Polish institute, where 454.9 kilograms of fresh and used highly enriched uranium fuel were left stranded by the collapse of the Soviet Union. And security remains very much on the minds of the US and Polish team members today. By mid-afternoon, police cars have gathered at a gate near the reactor building. Officers in black T-shirts cluster in small groups near an adjacent yard where seven trucks are parked, each carrying one of the blue containers of the last remaining spent fuel. A military helicopter suddenly roars over the treetops and begins to circle overhead; the airborne escort has arrived, and the convoy quickly lurches into action. One by one, the trucks file out of the gate towards the railway yard \u2014 the first stop on the long journey to Russia. With their departure, the only highly enriched uranium remaining on site lies inside the reactor itself. As in other research reactors covered by the NNSA programme, that fuel will systematically be replaced with a safer one. This is not as easy as it sounds. Researchers in the United States and elsewhere had to go to great lengths to develop safe blends of around 20% uranium-235 that will work in the cores of research reactors but can't be fashioned into a bomb. Because the reactors are designed for highly enriched uranium, they can run only on a very high density of uranium-235 atoms, which can make the fuel less stable. So to keep it from breaking down in the intense environment at the reactor's core, researchers have had to come up with exotic mixtures of uranium and elements such as silicon and molybdenum. That development effort was a success, and the fuel is now available for many research reactors, including Maria. But most institutions with research reactors don't have the millions of dollars needed to buy the new fuel \u2014 or to convert the core to use it. \"It's a hell of a lot of work,\" says Michael Corradini, a mechanical and nuclear engineer at the University of Wisconsin\u2013Madison, which last year made the switch to the safer fuel.  At Maria, even the most mundane-seeming issues can be a challenge. For example, the cooling water that helps to regulate the fission reaction doesn't circulate as well through the new blended fuel, Krzysztoszek says, so the pumps must be upgraded. Moreover, the core will have to be converted little by little in a laborious operation that will take nearly two years.  \n                Final destination \n              By dusk, the convoy has reached a railway yard on the outskirts of Warsaw. Shipping containers are stacked around the train, part of an effort to shield the shipment from view during loading. The watchful helicopter peels off as a heavy loader lifts the containers onto flatbed cars. As darkness falls, the train begins to roll towards the Baltic port of Gdynia, some 400 kilometres to the north. In the end, the promise of ridding the facility of the old fuel helped to convince the Poles to convert Maria to the blended fuel. Like most countries, Poland has no long-term facility for storing spent nuclear material, and the prospect of US aid to take it back to Russia was simply too tempting to pass up. It is hoped that the expansion of the US repatriation programme, and the development of new fuels for research reactors, will convince many countries to surrender their uranium. Over the next four years, the NNSA hopes to make a similar switch over in about ten countries around the world, and to complete most of its work worldwide by 2020. Right now, the US teams are stretched thin, frantically trying to accelerate the repatriation of fuel. American-supplied fuel will go to the United States, and Soviet material will go to Russia. Meanwhile, scientists at the US Department of Energy's laboratories are close to certifying another kind of even-higher density fuel that will work in reactors that cannot take the current mixtures available. The biggest problem, says Bunn, is that Russia itself continues to operate 60-odd reactors on bomb-grade material. \"They've been very resistant\" to changing, he says. The NNSA still hopes to win the Russians over, and says that it already has some six reactors in the country considering conversion. The next morning finds Bieniawski and the rest of the American team standing on the deck of a Russian cargo ship that has been specially outfitted to carry the fuel. Bieniawski watches the containers being lowered slowly into the hold. \"This is the last we'll physically see of it,\" he says. He and his team are already mustering for a trip to another reactor site, which holds more fuel that could be fashioned into a weapon. When asked where it is, he demurs: \"Let's talk about that later.\" \n                     National Nuclear Safety Administration \n                   \n                     Global Threat Reduction Initiative \n                   \n                     Polish Institute of Atomic Energy \n                   Reprints and Permissions"},
{"file_id": "467900a", "url": "https://www.nature.com/articles/467900a", "year": 2010, "authors": [], "parsed_as_year": "2006_or_before", "body": "The explosion in urban population looks set to continue through the twenty-first century, presenting challenges and opportunities for scientists. \n               WHERE THE PEOPLE ARE  \n             In less than a human lifespan, the face of Earth has been transformed. In 1950, only 29% of people lived in cities. Today that figure is 50.5% and is expected to reach 70% by 2050.   1 The United States   is one of the few developed countries where cities have continued to grow. Its urban population of 261 million is set to hit 308 million in 2025. \n               Click here for larger image \n                 2 Europe  's urban population is not expected to change much in the coming decades, rising from 920 million in 2010 to 1.1\u00a0billion in 2030. \n               Click here for larger image \n                 3 Latin America   is highly urbanized \u2014 80% of its population lives in cities \u2014 up from 41% in 1950. Its urban population of 500 million in 2010 is expected to grow to 650 million by 2025. \n               Click here for larger image \n                 4   Only 40% of  Africa  's population currently lives in cities, but its urban population is growing quickly. Many of the world's fastest growing big cities are in Africa. \n               Click here for larger image \n                 5 Asia  's urban population is growing faster than in any other region, continuing the trend from the twentieth century. The urban population, 234 million in 1950, reached 1 billion in 1990, and is expected to reach 3.4 billion by 2025.  \n               Click here for larger image \n                 6 Megacities   \u2014 which have populations of more than 10 million \u2014 are home to 1 in 10 people. \n               Click here for larger image \n               \n               WHERE THE PROBLEMS ARE  \n             Cities are gluttons when it comes to resources. In 2006, about 50% of the world's population was urban, but they consumed two-thirds of the total energy used and emitted more than 70% of the energy-related carbon dioxide emissions. \n               Click here for larger image \n               \n               Click here for larger image \n               \n               WHERE THE SCIENCE IS  \n             The solutions to many global problems are in cities. According to one geographic analysis in 2004, the greatest concentrations of scientific publications are in the major cities that hold most of the scientific resources. \n               Click here for larger image \n               \n                 Graphics by Nik Spencer; data compiled by Declan Butler. For the full cities special see  \n                 nature.com/cities \n               \n                     Science netrics \n                   \n                     World Urbanization Prospects, The 2009 Revision \u2014 UN Population Division \n                   \n                     State of the World's Cities 2008/2009: Harmonious Cities \u2014 UN-Habitat \n                   \n                     Cities and Climate Change \u2014 OECD report \n                   \n                     H. Kroll, \u201cIndicator-Based Reporting on the Chinese Innovation System 2010 \u2013 The Regional Dimension of Science and Innovation in China.\u201d \n                   \n                     The United Nations Habitat program, Cities in climate Change Initiative \n                   \n                     United Nations Environment Program, Urban Environment Unit \n                   Reprints and Permissions"},
{"file_id": "467775a", "url": "https://www.nature.com/articles/467775a", "year": 2010, "authors": [{"name": "Zeeya Merali"}], "parsed_as_year": "2006_or_before", "body": "\u2026why scientific programming does not compute. When hackers leaked thousands of e-mails from the Climatic Research Unit (CRU) at the University of East Anglia in Norwich, UK, last year, global-warming sceptics pored over the documents for signs that researchers had manipulated data. No such evidence emerged, but the e-mails did reveal another problem \u2014 one described by a CRU employee named \"Harry\", who often wrote of his wrestling matches with wonky computer software. \"Yup, my awful programming strikes again,\" Harry lamented in one of his notes, as he attempted to correct a code analysing weather-station data from Mexico. Although Harry's frustrations did not ultimately compromise CRU's work, his difficulties will strike a chord with scientists in a wide range of disciplines who do a large amount of coding. Researchers are spending more and more time writing computer software to model biological structures, simulate the early evolution of the Universe and analyse past climate data, among other topics. But programming experts have little faith that most scientists are up to the task. A quarter of a century ago, most of the computing work done by scientists was relatively straightforward. But as computers and programming tools have grown more complex, scientists have hit a \"steep learning curve\", says James Hack, director of the US National Center for Computational Sciences at Oak Ridge National Laboratory in Tennessee. \"The level of effort and skills needed to keep up aren't in the wheelhouse of the average scientist.\" As a general rule, researchers do not test or document their programs rigorously, and they rarely release their codes, making it almost impossible to reproduce and verify published results generated by scientific software, say computer scientists. At best, poorly written programs cause researchers such as Harry to waste valuable time and energy. But the coding problems can sometimes cause substantial harm, and have forced some scientists to retract papers. As recognition of these issues has grown, software experts and scientists have started exploring ways to improve the codes used in science. Some efforts teach researchers important programming skills, whereas others encourage collaboration between scientists and software engineers, and teach researchers to be more open about their code.  \n                A proper education \n              Greg Wilson, a computer scientist in Toronto, Canada, who heads Software Carpentry \u2014 an online course aimed at improving the computing skills of scientists \u2014 says that he woke up to the problem in the 1980s, when he was working at a physics supercomputing facility at the University of Edinburgh, UK. After a series of small mishaps, he realized that, without formal training in programming, it was easy for scientists trying to address some of the Universe's biggest questions to inadvertently introduce errors into their codes, potentially \"doing more harm than good\". After decades griping about the poor coding skills of scientists he knew, Wilson decided to see how widespread the problem was. In 2008, he and his colleagues conducted an online survey of almost 2,000 researchers, from students to senior academics, who were working with computers in a range of sciences. What he found was worse than he had anticipated 1  (see 'Scientists and their software'). \"There are terrifying statistics showing that almost all of what scientists know about coding is self-taught,\" says Wilson. \"They just don't know how bad they are.\" \n               Click here for larger image \n               As a result, codes may be riddled with tiny errors that do not cause the program to break down, but may drastically change the scientific results that it spits out. One such error tripped up a structural-biology group led by Geoffrey Chang of the Scripps Research Institute in La Jolla, California. In 2006, the team realized that a computer program supplied by another lab had flipped a minus sign, which in turn reversed two columns of input data, causing protein crystal structures that the group had derived to be inverted. Chang says that the other lab provided the code with the best intentions, and \"you just trust the code to do the right job\". His group was forced to retract five papers published in  Science , the  Journal of Molecular Biology   and  Proceedings of the National Academy of Sciences , and now triple checks everything, he says. \"How many fields have been held back, and how many people have had their careers disrupted, because of a buggy program?\" asks Wilson. More-rigorous testing could help. Diane Kelly, a computer scientist at the Royal Military College of Canada in Kingston, Ontario, says the problem is that scientists rely on \"validation testing\" \u2014 looking to see whether the answer that the code produces roughly matches what the scientists expect \u2014 and this can miss important errors 2 . The software industry relies on a different approach: breaking codes into manageable chunks and testing each piece individually, then visually inspecting the lines of code that stitch these chunks together (see 'Practicing safe software'). \n               Click here for larger image \n               Many programmers in industry are also trained to annotate their code clearly, so that others can understand its function and easily build on it. But scientists often lack these communication and documentation skills. Even if researchers lift a whole working code and reuse it, rather than writing their own, they can apply the program incorrectly if it lacks clear documentation. Aaron Darling, a computational biologist at the University of California, Davis, unwittingly caused such a mistake with his own computer code for comparing genomes to reconstruct evolutionary relationships. He had designed the program to work only with closely related organisms, but discovered that an independent group had used it to look at sequences far outside the code's working range. \"It was lucky that I came across it, because their published results were totally wrong, but they couldn't know that because I hadn't clearly documented how my code worked,\" says Darling. \"It's not something that I am proud of, but I am careful to be more clear now.\"  \n                Slaying the monster \n              Problems created by bad documentation are further amplified when successful codes are modified by others to fit new purposes. The result is the bane of many a graduate student or postdoc's life: the 'monster code'. Sometimes decades old, these codes are notoriously messy and become progressively more nightmarish to handle, say computer scientists. \"You do have some successes, but you also end up with a huge stinking heap of software that doesn't work very well,\" says Darling. The mangled coding of these monsters can sometimes make it difficult to check for errors. One example is a piece of code written to analyse the products of high-energy collisions at the Large Hadron Collider particle accelerator at CERN, Europe's particle-physics laboratory near Geneva, Switzerland. The code had been developed over more than a decade by 600 people, \"some of whom are excellent programmers and others who do not really know how to code very well\", says David Rousseau, software coordinator for the ATLAS experiment at CERN. Wilson and his students tried to test the program, but they could not get very far: the code would not even run on their machines. Rousseau says that the ATLAS group can test the software only on the Linux operating system at the moment, but is striving to make the code compatible with Mac computers. This is important, he says, \"because different platforms expose different types of errors that may otherwise be overlooked\". Some software developers have found ways to combat the growth of monster code. One example is the Visualization Toolkit, an open-source, freely available software system for three-dimensional computer graphics. People can modify the software as they wish, and it is rerun each night on every computing platform that supports it, with the results published on the web. The process ensures that the software will work the same way on different systems. That kind of openness has yet to infiltrate the scientific research world, where many leading science journals, including  Nature ,  Science   and  Proceedings of the National Academy of Sciences , do not insist that authors make their code available. Rather, they require that authors provide enough information for results to be reproduced.  \n                The search for Solutions \n              In November 2009, a group of scientists, lawyers, journal editors, and funding representatives gathered for the Yale Law School Data and Code Sharing Roundtable in New Haven, Connecticut, where they recommended that scientists go further by providing links to the source-code and the data used to generate results when publishing. Although a step in the right direction, such requirements don't always solve the problem. Since 1996, The  Journal of Money, Credit and Banking   has required researchers to upload their codes and data to an archive. But a 2006 study revealed that of 150 papers submitted to the journal over the preceding decade that fell under this requirement, results could be independently replicated with the materials provided for fewer than 15 (ref.  3 ). Proponents of openness argue that researchers seeking to replicate published results need access to the original software, but others say that more transparency may not help much. Martin Rees, president of the Royal Society in London, says it would be too much to ask reviewers to check code line by line. And in his own field of astrophysics, results can really be trusted only in cases in which a number of different groups have written independent codes to perform the same task and found similar results. Still, he acknowledges that \"how to trust unique codes remains an issue\". There are signs that scientific leaders are now taking notice of these concerns. In 2009, the UK Engineering and Physical Sciences Research Council put out a call for help for scientists trying to create usable software, which led to the formation of the Software Sustainability Institute (SSI) at the University of Edinburgh. The SSI unites trained software developers with scientists to help them add new lines to existing codes, allowing them to tackle extra tasks without the programs turning into monsters. They also try to share their products across disciplines, says Neil Chue Hong, the SSI's director. For instance, they recently helped build a code to query clinical records and help monitor the spread of disease. They are now sharing the structure of that code with researchers who are trying to use police records to identify crime hot spots. \"It stops researchers wasting time reinventing the wheel for each new application,\" says Chue Hong. Another solution is to bring trained computer scientists into research groups, either permanently or as part of temporary alliances. Software developer Nick Barnes has set up the Climate Code Foundation, based in Sheffield, UK, to help climate researchers. He was motivated by problems with NASA's Surface Temperature Analysis software, which was released to the public in 2007. Critics complained that the program, written in the scientific programming language Fortran, would not work on their machines and they could therefore not trust what it said about global warming. In consultation with NASA researchers, Barnes rewrote the code in a newer, more transparent programming language \u2014 Python \u2014 reducing its length and making it easier for people who aren't software experts to understand how it functions. \"Because of the immense public interest and the important policy issues at stake, it was worth taking the time to do that,\" says Barnes. His new code shows the same general warming trend as the original program. In the long term, though, Barnes says that there needs to be a change in the way that science students are trained. He cites Wilson's online Software Carpentry course as a good model for how this can be done, to equip students with coding skills. Wilson developed the week-long course to introduce science graduate students to tools that have been software-industry standards for 30 years \u2014 such as 'version control', which allows multiple programmers to make changes to the same code, while keeping track of all changes. Science administrators also need to value programming skills more highly, says David Gavaghan, a computational biologist at the University of Oxford, UK. \"There needs to be a real shift in mindset away from worrying about how to get published in  Nature   and towards thinking about how to reward work that will be useful to the wider community.\" Gavaghan now uses the software industry's 'master\u2013apprentice' approach to train graduate students in his lab. New software projects are split up into bite-sized chunks, with each segment assigned to a pair of programmers \u2014 one experienced and one novice \u2014 who work together on it. \"It forces students to become consistent code-builders,\" says Gavaghan. Bringing industrial software-development practices into the lab cannot come too soon, says Wilson. The CRU e-mail affair was a warning to scientists to get their houses in order, he says. \"To all scientists out there, ask yourselves what you would do if, tomorrow, some Republican senator trains the spotlight on you and decides to turn you into a political football. Could your code stand up to attack?\" \n                 See World View,  \n                 \n                     p.753 \n                   \n               \n                     Yale Law School Data and Code Sharing Roundtable \n                   \n                     The Software Sustainability Institute \n                   \n                     Greg Wilson's Software Carpentry website \n                   Reprints and Permissions"},
{"file_id": "467768a", "url": "https://www.nature.com/articles/467768a", "year": 2010, "authors": [{"name": "Emily Waltz"}], "parsed_as_year": "2006_or_before", "body": "Barack Obama promised a new era of integrity and openness for American science. Government scientists are now asking what has changed. Slide 11 in Robert Wall's presentation showed just three sentences. \"Sheep and cattle have been cloned since 1980. Clones produced in the 80s were likely eaten. So why is there such a fuss about clones now?\" But the words were enough to cause a fuss of their own. It was September 2008, and Wall, a federal scientist at the US Agricultural Research Service (ARS), was preparing to give a talk at a meeting held at the Economic Research Service, a sister agency within the US Department of Agriculture (USDA) in Washington DC. The audience would consist of researchers from the agency and representatives from international embassies. Wall planned to explain that although Dolly the sheep, born in 1996, was the first mammal to be cloned from an adult cell by nuclear transfer, scientists had been cloning animals by splitting embryonic cells for some 16 years before that. But on the morning of the presentation, he received a call from Steven Kappes, a deputy administrator at the ARS staff headquarters in Beltsville, Maryland. The message was clear: slide 11 had to go, as did a second slide about nuclear transfer. \"They didn't offer me a rationale,\" Wall says. Wall was stunned. His talk was a history and overview of animal cloning \u2014 nothing, he felt, that was new, and all well within his expertise. Still, Wall did what he was told. \"My boss's boss's boss is on the phone saying 'don't talk [about that] to those people'. There's no expectation I'm going to argue with him,\" he says. Neither Kappes nor Sandy Miller Hays \u2014 the director of information staff at the ARS \u2014 responded to  Nature  's repeated enquiries about the incident or Wall's experiences. The whole episode was the \"most disappointing\" instance of censorship in Wall's career, he says, but not the first \u2014 or the last. Wall retired from the ARS in January 2010 at the age of 65, after studying transgenic animals for 30 years and publishing nearly 80 papers on the topic. \"He is one of the main leaders in this field,\" says Elizabeth Maga, who researches transgenic animals at the University of California, Davis. But in the final six years of Wall's tenure \u2014 five under George W. Bush's administration and one under President Barack Obama's \u2014 he noticed increasing sensitivity and caution at his agency towards cloning and animal transgenics. The ARS frequently turned down requests from journalists to interview Wall about \"purely scientific things\", he says. \"Over the last two or three years, nearly every press request to speak to me was denied.\" Stories such as Wall's surfaced frequently during the Bush era, when US researchers complained bitterly that politics trumped science across federal agencies, leading to censorship and suppression of data. But Obama promised change. He headlined it in his inaugural address on 20 January 2009, saying: \"We will restore science to its rightful place.\" Seven weeks later, Obama wrote a 'scientific integrity' memorandum to the heads of executive departments and agencies stating: \"Political officials should not suppress or alter scientific or technological findings and conclusions. If scientific and technological information is developed and used by the Federal Government, it should ordinarily be made available to the public.\" The memo ordered John Holdren, the director of the White House Office of Science and Technology Policy, to write a set of recommendations within 120 days that would ensure scientific integrity at federal agencies.  \n                Science for science's sake \n              Almost halfway through Obama's tenure, federal scientists have yet to see those recommendations \u2014 and some feel that the administration has not delivered on its promises. But many do describe a renewed sense of freedom and encouragement in the Obama era. \"The atmosphere has been a lot more conducive to science being accepted for science's sake,\" says Kimberly Trust, an environmental toxicologist at the US Fish and Wildlife Service in Anchorage, Alaska. However, some researchers find themselves facing the same frustrating requirements to get approval before they publish or talk publicly about their work \u2014 and gag orders may still linger in the system. Francesca Grifo, director of scientific integrity at the Union of Concerned Scientists, a non-profit group in Washington DC that tracks political interference in science, says that since Obama took office she has received fewer calls from federal scientists complaining about censorship. But the calls still come in, she says \u2014 about ten so far. It is not clear whether such incidents reflect high-level political interference or the hand of middle managers or press officers, who alter or block communication because they are wary of stirring up controversy. \"They just do it to protect their backsides,\" says Neal Lane, a senior fellow in science and technology policy at Rice University in Houston, Texas, and a former science adviser to US president Bill Clinton. Interviews with nearly 30 federal scientists reveal that policies governing researchers' communication with the public vary from agency to agency, and even from office to office (see  'Permission to speak' ). Even when formal policies exist, they are often communicated poorly to employees and enforced unevenly. The Department of the Interior is the only federal department to have issued a scientific-integrity policy since Obama took office \u2014 that was on 29 September this year. All this might go some way to explaining why scientists find themselves running up against barriers, despite the directives from the top. \"The leadership is there from the president,\" says Grifo. \"It's just that we have this giant disconnect between that and implementation of it at the agencies.\" Whatever the cause, there is an effect. Scientists say that navigating approval processes for communicating their work can be frustrating and slows down research. Just the perception that science is being muzzled dampens morale and erodes the public's trust in their agencies, they say. \"Public interaction is part of our jobs as government scientists,\" says Jonathan Lundgren, an entomologist at the ARS office in Brookings, South Dakota. \"If the public doesn't know what we're doing, how do they know whether it's important to fund our work?\" The ARS serves as a good case study. Formal written policies there state that scientists must submit any manuscript intended for publication to their research leaders for review, but if the topic of the manuscript is deemed sensitive it must be passed up the chain of command for approval by national-programme staff at the ARS headquarters. The sensitive topics aren't always spelled out, however \u2014 for this, some researchers consult the agency's 'high-profile topics' list, which includes cloning and the creation of transgenic organisms, in a separate handbook. Miller Hays says that the ARS formal publication policy is outdated and that the list has not been enforced for years. The scattered instructions could explain scientists' differing experiences in getting approval to publish. Many say they have never hit roadblocks. Some find the process cumbersome. The approval process \"is excessive, and typically delays publication by about two months\", says Lundgren. Thomas Sappington, a research entomologist at the ARS office in Ames, Iowa, says that his only hold-up was when he attempted to publish a recent commentary on biotech crops. \"It was very difficult for me to get permission to be the lead author on the paper,\" says Sappington. \"The ARS gets very nervous when its scientists write non-research pieces.\" The commentary expressed concern about the biotech seed industry's control over the information reaching the US Environmental Protection Agency (EPA), which regulates biotech crops. \"The main hang up was whether it would cause problems with the EPA or embarrass the EPA,\" he says. \"In the end, I made a couple of minor wording changes that satisfied everyone and I was allowed to submit.\" The process took a couple of months and the commentary was published this year ( T. W. Sappington  et al. GM Crops    1,   55\u201358; 2010 ). \"As a scientist it's frustrating, but if you can make your case they'll let it go through,\" he says. When it comes to interviews with journalists, the official rule and the one heard by ARS scientists seem to differ. According to the formal policies, employees are \"encouraged to respond willingly to requests from the media\", but alert information staff to all national media inquiries, \"particularly when the topic is related to sensitive issues or the policies of other government agencies\". Miller Hays says that scientists are encouraged to call her office for advice beforehand, but that \"it's not at all uncommon for an ARS scientist to speak to the media and simply let me and others know about it afterwards\". ARS scientists tell a different story. \"Over the years we've been left with the firm conviction that heads can roll if we talk to reporters without permission,\" says Sappington. Press requests involving high-profile topics can cause considerable anxiety at headquarters, ARS researchers say. \"They'll have a panic attack over anything on that list,\" says an ARS scientist who asked to remain anonymous.  \n                One more hoop \n              In May this year, Steven Naranjo, a research leader at the ARS office in Maricopa, Arizona, ran up against a wall when he received an interview request from National Public Radio to comment on a paper published in  Science . The paper discussed the emergence of 'non-target' pests in areas where genetically engineered insect-resistant crops are grown. Naranjo had reviewed the paper for  Science   and wanted to do the interview. But when he asked for permission from the ARS information staff, the request was passed up to the communications office of the USDA, and ultimately denied, he says. \"They decided it was too controversial,\" says Naranjo. \"It was a little frustrating, but not a big deal.\" The process is more than a little frustrating for one ARS researcher, who says he is so fed up with the system that he has simply stopped asking for permission. For years, this scientist has been writing columns for regional newspapers and speaking to journalists without reporting it to the information staff. \"I don't want them changing my words,\" this scientist said, under the condition of anonymity. \"Getting permission is one more hoop and it's a pain in the neck.\" Scientists at the National Oceanic and Atmospheric Administration (NOAA) encountered problems when they started talking about the Deepwater Horizon blowout in the Gulf of Mexico. NOAA employees are allowed to speak to the press without getting permission from the press office as long as they are talking about science, according to policies set in 2007. But Mark Powell, a hurricane expert at NOAA's Atlantic Oceanographic and Meteorological Laboratory in Tallahassee, Florida, says that after the oil spill, a team of NOAA experts was assembled and 'cleared' to talk to the media. As Powell understood it, no one else was allowed to speak publicly. \"I decided to turn down a local TV interview because I had not yet been cleared,\" he says. Communication staff at NOAA say that they used the term cleared to identify subject experts who had been put through some quick media training after the explosion. \"This effort did not preclude anyone from speaking to the media or public, as per the NOAA policy,\" says Justin Kenney, director of communications for NOAA. But Powell wasn't the only one with the impression that communications rules had been tightened. \"I did feel early on that I would make people a lot happier if I worked with media relations,\" says Michelle Wood, director of the ocean chemistry division in the same lab as Powell, a process that she says was \"maddeningly slow\" and often sent journalists to experts outside of NOAA. Wood says it was a great relief to later learn that it was coordination with external groups working on the spill that was delaying NOAA's responses to the media, rather than NOAA itself, and that communication lines eventually opened up. It can be hard to trace restrictions back to their origin or determine when they are politically driven, says Lane. \"The tone gets set by the administration or political appointees or the secretary, and people react \u2014 or sometimes overreact \u2014 to what they think is the right thing to do.\" Grifo adds: \"It's pretty typical that a supervisor at a certain level takes it upon themselves to be the great defender of secrecy at their agency.\" Lane is concerned about the effect of these restrictions on scientists and their work. \"It kills morale,\" he says. \"It makes scientists feel like their work is not valued, and it makes it harder for agencies to recruit and retain the best scientists.\" Keeping information from the public could put the credibility of the agency at risk, and some scientists say it affects their careers. \"The restrictions limit my overall stature in the research community,\" says an ARS scientist who asked to remain anonymous. Not everyone feels this strongly. Some ARS scientists say that the agency's internal review process for their research papers is appropriate, and is just part of working for the government. Still, outside experts say that policies on communication should be spelled out to government researchers before they take the job. \"There's not a clear expectation of what it is to be a government scientist,\" says Roger Pielke Jr, who has testified to Congress on scientific integrity and is a science-policy analyst at the University of Colorado at Boulder. Pielke says the White House's scientific integrity guidelines could help to clarify those expectations \u2014 when they finally arrive. At a meeting of the President's Council of Advisors on Science and Technology on 2 September, Holdren said that the delay is partly a result of \"a long process\" of public input and consultation with the agencies that would be affected. He said that the recommendations will be issued by the end of the year. It is not yet clear how those recommendations will be implemented. \"If this is a priority of our president's, the [cabinet members] are going to go back to their agencies and ask that the message be sent all the way down through the organization,\" says Lane. \"They don't want to read in  Nature   that one of their scientists was prevented from talking after the president has just told them that he's serious about this.\" Some change is already afoot at the ARS. Miller Hays says that the agency will post a manuscript-publishing policy this month that does not require scientists to get approval from national programme staff. Wall is not so sure that things are going to change. \"Avoiding controversy,\" he says, \"may always be a part of what bureaucrats see as their mission.\" \n                 See Editorial  \n                 \n                     p.751 \n                   \n               Emily Waltz is a freelance writer based in Nashville, Tennessee. \n                     Mid-term election 2010 \n                   \n                     US election 2008 \n                   \n                     Agricultural Research Service \n                   \n                     NOAA \n                   \n                     OSTP \n                   \n                     Obama's memo on scientific integrity \n                   \n                     Union of Concerned Scientists \n                   Reprints and Permissions"},
{"file_id": "4651000a", "url": "https://www.nature.com/articles/4651000a", "year": 2010, "authors": [{"name": "Declan Butler"}], "parsed_as_year": "2006_or_before", "body": "The completion of the draft human genome sequence was announced ten years ago.  Nature 's survey of life scientists reveals that biology will never be the same again. Declan Butler reports. \"With this profound new knowledge, humankind is on the verge of gaining immense, new power to heal. It will revolutionize the diagnosis, prevention and treatment of most, if not all, human diseases.\" So declared then US President Bill Clinton in the East Room of the White House on 26 June 2000, at an event held to hail the completion of the first draft assemblies of the human genome sequence by two fierce rivals, the publicly funded international Human Genome Project and its private-sector competitor Celera Genomics of Rockville, Maryland (see  Nature    405,   983\u2013984; 2000 ). Ten years on, the hoped-for revolution against human disease has not arrived \u2014 and  Nature 's poll of more than 1,000 life scientists shows that most don't anticipate that it will for decades to come ( go.nature.com/3Ayuwn ). What the sequence has brought about, however, is a revolution in biology. It has transformed the professional lives of scientists, inspiring them to tackle new biological problems and throwing up some acute new challenges along the way. Almost all biologists surveyed have been influenced in some way by the availability of the human genome sequence. A whopping 69% of those who responded to  Nature 's poll say that the human genome projects inspired them either to become a scientist or to change the direction of their research. Some 90% say that their own research has benefited from the sequencing of human genomes \u2014 with 46% saying that it has done so \"significantly\". And almost one-third use the sequence \"almost daily\" in their research. \"For young researchers like me it's hard to imagine how biologists managed without it,\" wrote one scientist. The survey, which drew most participants through  Nature 's print edition and website and was intended as a rough measure of opinion, also revealed how researchers are confronting the increasing availability of information about their own genomes. Some 15% of respondents say that they have taken a genetic test in a medical setting, and almost one in ten has used a direct-to-consumer genetic testing service. When asked what they would sequence if they could sequence anything, many respondents listed their own genomes, their children's or those of other members of their family (the list also included a few pet dogs and cats). Some are clearly impatient for this opportunity: about 13% say that they have already sequenced and analysed part of their own DNA. One in five said they would have their entire genome sequenced if it cost US$1,000, and about 60% would do it for $100 or if the service were offered free. Others are far more circumspect about sequencing their genome \u2014 about 17% ticked the box saying \"I wouldn't do it even if someone paid me\". Click here for larger image   Nature 's poll also gauged where the sequence has had the greatest effect on the science itself. Although nearly 60% of those polled said they thought that basic biological science had benefited significantly from human genome sequences, only about 20% felt the same was true for clinical medicine. And our respondents acknowledged that interpreting the sequence is proving to be a far greater challenge than deciphering it. About one-third of respondents listed the field's lack of basic understanding of genome biology as one of the main obstacles to making use of sequence data today.  \n                Sequence is just the start \n              Studies over the past decade have revealed that the complexity of the genome, and indeed almost every aspect of human biology, is far greater than was previously thought (see  Nature    464,   664\u2013667; 2010 ). It has been relatively straightforward, for example, to identify the 20,000 or so protein-coding genes, which make up around 1.5% of the genome. But knowing this, researchers note, does not necessarily explain what those genes do, given that many genes code for multiple forms of a protein, each of which could have a different role in a variety of biological processes. \"The total sequence was needed, I think, to allow us to see that our one gene\u2013one protein model of genetics was much too simplistic,\" wrote one respondent. A decade of post-genomic biology has also focused new attention on the regions outside protein-coding genes, many of which are likely to have key functions, through regulating the expression of protein-coding genes and by making a slew of non-coding RNA molecules. \"Now we understand,\" wrote another survey respondent, \"that, without looking at the dynamics of a genome, determining its sequence is of limited use.\" Some big projects are under way to fill in the gaps, including the Encyclopedia of DNA Elements (ENCODE) and the Human Epigenome Project, an effort to understand the chemical modifications of the genome that are now thought to be a major means of controlling gene expression. The biggest effects of the genome sequence, according to the poll, have been advances in the tools of the trade: sequencing technologies and computational biology. Technological innovation has sent the cost of sequencing tumbling, and the daily output of sequence has soared (see  Nature    464,   670\u2013671; 2010 ). \"Deep sequencing technology is now becoming a staple of scientific research. Would this have occurred if it wasn't for the technological push required to finish the human genome?\" read one response.  \n                Data dreams, analysis nightmares \n              Cheaper and faster sequencing has brought its own problems, however, and our survey revealed how ill-equipped many researchers feel to handle the exponentially increasing amounts of sequence data. The top concern \u2014 named by almost half of respondents \u2014 was the lack of adequate software or algorithms to analyse genomic data, followed closely by a shortage of qualified bioinformaticians and to a lesser extent raw computing power. Other concerns include data storage, the quality of sequencing data and the accuracy of genome assembly. Commenting on the survey results, David Lipman, director of the US National Center for Biotechnology Information in Bethesda, Maryland, says that the worries about data handling and analysis were an issue even in the earliest discussions of the genome project. Perhaps, he suggests, \"there's a sort of disappointment that despite having so much data, there is still so much we don't understand\". Click here for larger image Eric Green, director of the National Human Genome Research Institute (NHGRI) in Bethesda, says that the institute is well aware of the need for more bioinformatics experts, better software and a clearer understanding of how the differences between genomes influence human health. He says the institute is planning to publish in late 2010 its next strategic five-year plan for the genomics field. One possible solution to the computing challenge, which was discussed at an NHGRI workshop in late March, is cloud computing, in which laboratories buy computing power and storage in remote computing farms from companies such as Google, Amazon and Microsoft. The European Nucleotide Archive, launched on 10 May at the European Molecular Biology Laboratory's European Bioinformatics Institute in Cambridge, UK, will also offer labs free remote storage of their genome data and use of bioinformatics tools. Given ten years' of hindsight and the current set of obstacles, it's no surprise that researchers now state somewhat modest expectations for what human genomics can deliver and by when. The rationale for sequencing and exploring the human genome \u2014 to revolutionize the finding of new drugs, diagnostics and vaccines, and to tailor treatments to the genetic make-up of individuals \u2014 is the same today. But almost half of respondents now say that the benefits of the human genome were oversold in the lead up to 2000. \"While I do feel that the gains made by the human genome project are extraordinary and affect my research significantly, I still feel that it was overhyped to the general population,\" read one typical response. More than one-third of respondents now predict that it will take 10\u201320 years for personalized medicine, based on genetic information, to become commonplace, and more than 25% even longer than that. Some 5% don't expect it will happen in their lifetime. \"Our understanding of the genome will not come in a single flash of insight. It will be an organized hierarchy of billions of smaller insights,\" says David Haussler, head of the Genome Bioinformatics Group at the University of California, Santa Cruz. Green says that when the Human Genome Project was envisioned, scientific leaders of the day predicted that it would take 15 years to generate the first sequence, and a century for biologists to understand it. \"I think they got that about right,\" he says. \"While we still don't have all the answers \u2014 being a mere 10% of the way into the century with a human genome sequence in hand \u2014 we have learned extraordinary things about how the human genome works and how alterations in it confer risk for disease.\" Haussler agrees. \"All that happened in the first ten years is still just early rumblings of much more dramatic changes to come when we begin to truly understand the genome,\" he says. \n                 Survey work was aided by Sara Grimme.  \n               \n                     The Human Genome at Ten \n                   \n                     Unlocking the secrets of the genome \n                   \n                     Meetings that changed the world: Santa Fe 1986: Human genome baby-steps \n                   \n                     Moving AHEAD with an international human epigenome project \n                   \n                     Completing the map of human genetic variation \n                   \n                     Human Genome Collection \n                   \n                     Historic Nature Human Genome issue \n                   \n                     NHGRI \n                   \n                     NCBI \n                   \n                     EMBL-EBI \n                   \n                     UCSC Genome Browser \n                   \n                     ENCODE \n                   \n                     Human Epigenome Consortium \n                   \n                     HAPMAP \n                   \n                     1000 Genomes project \n                   \n                     Genome 10K Project \n                   \n                     Genome-Wide Association Studies (GWAS) \n                   \n                     Transcript of 26 June 2000 Whitehouse briefing \n                   Reprints and Permissions"},
{"file_id": "466808a", "url": "https://www.nature.com/articles/466808a", "year": 2010, "authors": [{"name": "Meredith Wadman"}], "parsed_as_year": "2006_or_before", "body": "Having taken on the biggest job in biomedicine \u2014 leading the US National Institutes of Health \u2014 Francis Collins must now help his agency over a funding cliff. Meredith Wadman looks at his record so far, and his plans to cushion the fall. There were three scans of Francis Collins's genome, and all showed the same thing: the geneticist and physician has an increased risk of developing type 2 diabetes. After Collins received the results from the genetic-testing companies in the spring of 2009, shortly before he became  director of the US National Institutes of Health  (NIH), he hired a personal trainer and began working out three times a week. He jettisoned his favourite junk food \u2014 honey buns and oversized muffins \u2014 in exchange for yoghurt, granola bars and broccoli. The 60-year-old now dead lifts 48 kilograms, chest presses 43, and has lost more than 11 kilograms himself. \"It has helped me a lot in terms of being able to take on the intensity of the job,\" he says. That salubrious slimming is nothing compared with the crash diet that Collins's US$31-billion-a-year agency is about to go on. Collins took control of the NIH \u2014 the world's largest biomedical-research funder \u2014 in the middle of a feast: a $10.4-billion, two-year boon delivered in 2009 by the American Recovery and Reinvestment Act, as part of the US government's effort to revive a moribund economy. Next month, the last of that money will go out of the door, and its recipients will have spent the bulk of it by September 2011. \"The Recovery Act provided an enormously timely and appropriate stimulus for the community after five years of flat funding,\" Collins said in an interview with  Nature   at the NIH's Bethesda, Maryland, campus last month. \"But now we face this potential of falling off a cliff. That's the biggest challenge\" of his job, he says. Collins comes equipped for challenges, intellectually and temperamentally. From his co-discovery of the gene for cystic fibrosis 21 years ago, to his 15 years of leadership of the NIH's  National Human Genome Research Institute  \u2014 and, with it, the Human Genome Project \u2014 he has proved that he combines serious scientific know-how with a leader's vision (see  'Francis Collins: in sequence' ). With his boy-scout manners and folk-guitar habit, he is also a decided contrast to his immediate predecessor, the sharp-suited Elias Zerhouni, a radiologist whom many bench investigators viewed warily for not being a scientist's scientist. Collins's exceptional self-discipline extends well beyond dieting. By the time he started the job, he had already formulated a  'pocket list' of 22 goals  for his first year in office, from hosting a visit to the NIH by President Barack Obama to hiring a new cancer-institute director. Now, he proudly hands over the list of mostly ticked-off accomplishments: Obama visited the NIH last September, and Harold Varmus, a former NIH director, took the reins of the  National Cancer Institute  in July. \"He's in a hurry,\" says Susan Shurin, the acting director of the NIH's  National Heart, Lung and Blood Institute  (NHLBI). \"He moves fast and he likes to be surrounded by people who are going to make things happen.\" Collins has detractors as well as fans. When he was appointed, some scientists voiced loud scepticism that he could separate his very public Christian faith from his policy decisions. There were also fears that his roots heading the Human Genome Project would lead him to favour NIH-initiated mega-projects over proposals by individual scientists. Others scolded him \u2014 and still do \u2014 for what they call his perennial overpromising on the fruits of the genomic revolution. \"He is still leading people to believe that genetics is the key to everything,\" says Neil Greenspan, an immunologist at the Case Western Reserve University School of Medicine in Cleveland, Ohio. If, five or ten years from now, only a handful of therapies emerge as a direct result of the genome project, \"you could end up with a lot of people [in Congress] getting upset and cutting the NIH because they are not producing what they claimed\". Such concerns do not worry a lean, list-checking Collins. \"My job it seems to me is not to spend my time apologizing for being optimistic. But rather to try to take that optimism and turn it into reality,\" he says.  \n                Morning to night \n             On a sultry morning in mid-July, Collins straps on his black motorcycle helmet and rides his Harley-Davidson the 15 minutes from his suburban Maryland home to the NIH campus. Collins had grabbed his usual, abbreviated night of sleep, after recording an interview for the  Charlie Rose Show , marking the tenth anniversary of the draft sequencing of the human genome, and then staying up until nearly midnight to watch the popular talk-show air. In between, he had participated in a conference call with senior government officials, discussing how to enrol 20,000 subjects in a long-term study of the health effects on workers cleaning up the Gulf of Mexico oil spill. Having risen at his usual time of 5:00 a.m. \u2014 \"that is a protected time, before all hell breaks loose, when I can actually try to think and plan,\" he says \u2014 he is now on his way to a 7:45 a.m. interview with a candidate to head the NHLBI. Collins wasted no time on his first day as NIH director either, when he announced five 'themes' \u2014 areas of what he calls \"exceptional opportunity\" \u2014 that would receive special priority during his tenure (see  Nature  460, 939; 2009 ). Collins targeted translational medicine, health-care reform, global health and \"empowering and energizing the research community\". And he said he wanted to apply high-throughput technologies including genomics and proteomics to answer, as he puts it, questions with 'all' in them, such as \"what are all of the major pathways for signal transduction in the cell?\" He also had to deal with some of the issues left over from Zerhouni's watch. He was faced with the delicate job of making new human embryonic stem-cell lines available for federal funding fast enough to suit a community that was hankering for them after eight years of drought \u2014 without any missteps that would provide ammunition to opponents of the research. Between December and June, the agency approved 75 new  stem-cell lines . (Collins points to the approvals as evidence that he \"will not allow my own personal spiritual beliefs to interfere with decision-making or priority setting\".) But the agency has also drawn criticism for rejecting scores of disease-specific cell lines because of the broad legal language used in patient-consent forms (see  Nature  465, 852; 2010 ). Collins also faced the aftermath of several scandals in which NIH-supported academics had flouted reporting rules by failing to disclose five- and six-figure sums that they had collected from drug companies. In May, the NIH published proposed changes that would tighten the rules governing financial-interest reporting by its grantees. Still, nothing Collins has faced so far comes close to the budget straits that the agency now confronts as the government struggles to control ballooning deficits, fight two wars and deal with the detritus of a major economic crisis. As NIH director, \"what happens to you is going to depend on things beyond your control\", says Anthony Fauci, director of the  National Institute of Allergy and Infectious Disease  since 1984. \"I hope that circumstances beyond his control start leaning towards helping him rather than hindering him.\"  \n                Slim chances \n              Already, this year, success rates for scientists applying for the agency's research-project grants have dipped to an estimated 19%, down from 21% in 2009 and far lower than the comfortable 32% of a decade earlier (see  'Grant applications to the NIH' ). The worsened odds partly reflect an increase of about 10% in the number of applications, many of which are recycled from failed stimulus grant proposals. In 2011 and 2012, the grant success rates are expected to fall further as stimulus funding runs out and its recipients attempt to extend support for their projects. \n               Click here for larger image \n               The NIH's baseline budget is also approaching dangerous waters. Although agency supporters were heartened last month when key subcommittees of the Senate and House of Representatives approved Obama's request for a 3.2%, $1-billion boost that would bring the budget to $32 billion in 2011, the increase is not guaranteed to survive final congressional wrangling this autumn or winter. And it does no more than match the government's predicted biomedical inflation rate. Things could be even bleaker in 2012: this June, Collins, like every other federal agency director, was asked by the White House's Office of Management and Budget, as part of its planning process for the 2012 US budget, to identify cuttable programmes amounting to 5% of the agency's budget. This is hardly a calamity compared with the deep research cuts occurring in some European countries, but still a shock to the NIH, which has faced only one absolute funding cut since 1970, and that only a 0.1% shave (see  'NIH budget' ). Late last month, Collins collected from the directors of the NIH's 27 institutes and centres a list of targeted programmes, constituting 7% of their budgets \u2014 the 7% giving him some flexibility to cut less here and more there. The final list is due to the White House in mid-September. The initial response of the institute directors to his request was \"full of angst\", says Collins. \"But there has also been a sense of 'We need to look hard at everything we are doing at a time like this'.\" He remains hopeful that given Obama's emphasis on science, \"when the dust all settles and they [the White House] decide exactly what to do, we will be at some level a bit protected, but we don't know that\".  \n                All or none \n              All this has been a growing cloud on the horizon even as Collins has been fleshing out his five themes. He has emphasized translational research, throwing his weight behind a programme aimed at speeding treatments for rare and neglected diseases towards human trials. He has embraced health reforms by overseeing the spending of $400 million in Recovery Act money earmarked for research into the 'comparative effectiveness' of medical treatments. And he has promoted his global health priority with initiatives such as a collaboration involving Britain's Wellcome Trust medical charity, in which the NIH will contribute $25 million over five years to study the genetic and environmental underpinnings of chronic diseases in sub-Saharan Africa. Collins has also been launching high-tech assaults on the 'all' questions, committing $175 million in Recovery Act money to accelerate  The Cancer Genome Atlas  \u2014 a five-year-old effort to develop a detailed catalogue of all of the mutations associated with 20 common cancers. Collins's emphasis on these types of ambitious projects has led some to question his commitment to the individual investigator and the mainstay, multi-year 'R01' grants that fund many such scientists. But his defenders say there is no evidence that Collins is advancing the first at the expense of the second. \"Francis fully gets the importance of funding some of the larger efforts that can be so transforming. But I think he's also paying very close attention to maintaining a vigorous pipeline of R01-funded research,\" says Levi Garraway, a cancer biologist at Harvard Medical School and Dana-Farber Cancer Institute in Boston, Massachusetts, who holds investigator-initiated NIH grants and also participates in The Cancer Genome Atlas project. Collins says that big-team science is the only way to produce some tools that greatly benefit individual investigators. But he says that the individual lab \"is where almost all of the discoveries of the present and the future are going to come from\". And these labs are at the centre of his push to \"energize and empower\" the research community by addressing peer review, training and other workforce issues. Anaemic success rates for research-project grant applicants have created \"a terribly stressful circumstance, particularly for early-stage investigators\", says Collins, noting that the average age for winning a first R01 award has now crept above 42 years old. As a partial response to this, he has been planning the launch in 2011 of an award that will allow promising young investigators to skip postdoc positions entirely, giving them five-year funding to launch independent labs. As for the immediate concerns of thousands of NIH grantees edging towards the funding cliff, Collins says that the agency will be \"sympathetic\" in allowing Recovery Act-funded grantees to spend their money over more than two years, \"making it more of a ramp instead of a cliff\". \"We will be doing other things which may assist the ability to give new grants, but hurt the people who already have them,\" he adds. Those will include cutting individual grant budgets \"as we have to, in order to keep as many researchers going as possible\". These measures bring cold comfort for many in postdoc purgatory with little prospect of securing independent funding. \"I didn't think it would be some Glory Hallelujah moment when Collins was appointed,\" says one 35-year-old scientist in his second postdoc, who asked to remain anonymous. He would like Collins to make it possible for those more than five years beyond their PhDs to secure transition funding such as a coveted 'K99' award, which supports postdocs in the shift to independent positions. \"To be brutally honest, I haven't noticed any difference in his tenure after the first year compared to Zerhouni,\" he says. But if Collins hasn't impressed some struggling bench scientists, his skill as a public communicator may nonetheless help to improve the NIH's prospects \u2014 or at least lessen its immediate peril. William Talman, president of the Federation of American Societies for Experimental Biology, attributes the White House's request for a $1-billion boost for the NIH \u2014 even in a stark funding climate \u2014 to Collins's persuasive powers. \"He has been a superb advocate for the NIH with the administration and with Congress.\" Collins has the rare gift of being able to translate complex concepts into simple language, leaving his audiences \u2014 including all-important congressional audiences \u2014 feeling brilliant about their grasp of his material. (In one typical analogy he describes a haplotype, a group of genetic markers that are inherited together, as being like a neighbourhood of houses that moves together \u2014 with a causative mutation residing at one street address.) \"The most important thing he has done really is his public outreach,\" says Shurin, who recalls as typical Collins's May guitar performance for patient advocacy groups affiliated with her institute. Set to the tune of Del Shannon's hit  Runaway , his lyrics described the anxieties raised by confronting a readout of one's own genome \u2014 \"I'm a walking through the genes/Don't know what all this means/Oh what can the meaning be?/Behind that G and T?/And I wonder \u2026\" He received a standing ovation. Collins is going to need all of that support and more to help those funded by the agency over the cliff \u2014 or down the ramp \u2014 ahead. \"I don't have any magic here,\" says Collins. \"I wish I did.\" \n                 Online extra:  \n                 \n                     Click here \n                   \n                  for Collins's biographical timeline  \n               \n                     NIH director's home page \n                   \n                     Reports, data and analysis of NIH research \n                   \n                     NIH databook \n                   \n                     The Cancer Genome Atlas \n                   Reprints and Permissions"},
{"file_id": "466024a", "url": "https://www.nature.com/articles/466024a", "year": 2010, "authors": [{"name": "Jeff Tollefson"}], "parsed_as_year": "2006_or_before", "body": "Many climate researchers worry that scepticism about global warming is on the rise. Jeff Tollefson investigates the basis for that concern and what scientists are doing about it. Last November, a catchy music video popped up on YouTube and attracted thousands of fans. Called 'Hide the Decline', the video featured a caricature of climate researcher Michael Mann admitting that he had committed fraud while creating his famous 'hockey-stick' graph of temperatures over the past millennium. Accompanied by a kitten playing the guitar, the cartoon image of Mann joyfully sings, \"Making up data the old hard way, fudging the numbers day by day.\" The video wasn't funny to the real Mann, director of the Earth System Science Center at Pennsylvania State University in University Park. A lawyer wrote to the group responsible for it, threatening to sue them for defamation and for using a copyrighted image. The video was promptly taken down and a new version \u2014 without the copyrighted photo \u2014 appeared on YouTube. Mann has grown weary of dealing with the various groups that are criticizing him. \"In reality, these groups are guilty over and over again of defamation, slander and libel, but that is far more difficult to fight legally,\" Mann says. \"Even if you were to prevail, you would have invested potentially several years of your career, and frankly those of us who love doing science are not willing to do that.\" Mann isn't alone in wondering how to respond to the wave of attacks that followed November's leak of e-mails from climate researchers at the University of East Anglia in Norwich, UK. Beyond the satire and vitriol appearing on blogs, researchers have endured threatening phone messages and other forms of harassment. And they're frustrated that governments have yet to mobilize in the face of solid evidence for global warming. All of this has spread fear among climate scientists that they are losing the war over public opinion, just a few years after a swell of support followed the most recent report from the Intergovernmental Panel on Climate Change (IPCC) in Geneva, Switzerland, which garnered a Nobel Peace Prize in 2007. However, polling data suggest that the situation is not as dire as many researchers suspect. Studies in the United States and the United Kingdom show that belief in global warming has dropped in recent years, but a majority of people still trust climate scientists. There are also signs that public support for actions on global warming have grown in recent months. Still, scientists and scientific societies have decided that they need to fight back against the proliferating misinformation. They are using novel approaches to get their message across, such as trying to humanize climate scientists. \"We're trying to see if we can inoculate against some of the distrust in climate scientists,\" says Brenda Ekwurzel, head of the climate-science education group at the Union of Concerned Scientists in Cambridge, Massachusetts, which is seeking to make individual scientists more accessible by introducing them to the media and the public. But will these efforts work? And are they even necessary? Better communication never hurts, but some social scientists say that it won't be nearly enough to resolve the problems facing climate experts.  \n                Critical climate \n              The e-mail scandal in November started a string of revelations that have kept climate researchers on the defensive. Just a few weeks later, the IPCC came under fire for a flaw in its forecasts about the future of Himalayan glaciers, and another error was discovered in its statements about how much of the Netherlands is below sea level. The problems have prompted several reviews of the IPCC, including one commissioned by the Dutch parliament, which is due imminently. Meanwhile, panels have investigated the Climate Research Unit at East Anglia and individual researchers to see whether they have improperly withheld data from the public. The recent bad publicity has exacerbated a several-year decline in public confidence about climate science. A poll in January by Yale University in New Haven, Connecticut, and George Mason University in Fairfax, Virginia, found that the number of Americans who believe in global warming dropped from 71% to 57% between 2008 and 2010. The proportion who trust scientists for information about global warming dipped from 83% to 74%. In both cases, the decline is concentrated more among Republican supporters than among liberals and independents, says Anthony Leiserowitz, who headed the poll as director of the Yale Project on Climate Change. He suggests that the trend reflects a change in US politics: when Congress began talking seriously about steps to reduce greenhouse-gas emissions, conservatives didn't like it. Debates on those issues often focused on how pollution controls would harm or benefit different sectors of the economy. \"As soon as the discussion hits the level of people drafting legislation and making political choices about who is going to win and who is going to lose, the discussion is inevitably politicized,\" says Leiserowitz. The same trends contributed to the way people responded to the recent scandals, he adds. \"Certain groups in society were much more predisposed to think the worst of climate science than others.\" Jon Krosnick, a social psychologist who studies public perceptions of climate change at Stanford University in California, has also seen a slight erosion in public belief in global warming over recent years, although he stresses that overall support remains high. He thinks that the cool weather of 2008 helps to explain why the population changed its opinion. \"The way they decide whether climate change is happening is by sticking their finger out the window,\" he says. \"If we get another hot year, those numbers will go up again.\" In the United Kingdom, too, polls indicate that public confidence in climate research has declined over the long term. A survey this year by researchers at Cardiff University found that 78% of UK residents believed that the climate was changing, compared with 91% in 2005. And the trend continued early this year. A BBC study found that the proportion of people who believe that global warming is largely caused by human activity dropped from 41% in November to 26% in February. But researchers warn against ascribing the decline to the recent events. In the BBC poll, nearly three-quarters of people who had heard about the controversies said that their views on climate change had not altered as a result. Moreover, those who reported shifting their positions were likely to be more convinced of global warming, not less. Some scientists suggest that the recent results reflect the abnormally cold winter more than anything else (see 'UK responses to climate change'). Even with the recent erosion of belief in global warming, researchers point out that confidence in climate science and in the scientists remains strong. In the Cardiff poll, more than three-quarters of respondents attributed global warming at least in part to human activity. Just 18% said that it is mostly due to natural causes. Barring further scandals, many researchers expect the current controversies to register as just a short-term blip in public opinion. There is evidence to back up that idea. In a second poll, conducted in May, the Yale\u2013George Mason team found that support for climate legislation had grown across the board. Approval of carbon dioxide regulation increased from 71% to 77%, and support for an international treaty committing the United States to reducing emissions by 90% by the year 2050 increased from 61% to 65% (see 'US support for carbon dioxide regulation'). For the most prominent scientists, widespread support for climate research is easy to overlook in the face of mounting attacks. After the release of the East Anglia e-mails, researchers say that they saw an increase in threatening e-mails and phone calls. In February, US Senator James Inhofe (Republican, Oklahoma) released a report accusing at least 17 climate scientists of potentially criminal behaviour. Two weeks later, when an internal e-mail conversation at the US National Academy of Sciences in Washington DC was leaked to a conservative media outlet, it sparked another round of harassing messages against those involved. Paul Ehrlich, a biologist at Stanford University, received a voicemail labelling him a communist bent on \"destroying America\". In April, Kenneth Cuccinelli, attorney-general of Virginia, launched an investigation into correspondence from Mann's time at the University of Virginia in Charlottesville, seeking to determine whether he had violated any laws or conditions of his grants. These events have prompted scientists to rethink the way in which they address critics and engage the public. That has sometimes translated into an aggressive personal response. In April, for example, Andrew Weaver, a climate modeller at the University of Victoria in British Columbia, Canada, filed a libel lawsuit against the  National Post   newspaper, alleging that it had misrepresented his work and the facts about global warming. Some scientists have begun to push for more coordinated action. Scientific societies elected not to comment on Inhofe's report, but on 18 May the American Association for the Advancement of Science's board of directors, based in Washington DC, called on Cuccinelli to withdraw his subpoena, saying that such investigations \"could have a long-lasting and chilling effect on a broad spectrum of research fields\". The University of Virginia has elected to challenge Cucinelli in court.  \n                Out in the open \n              Scientists are also trying to take a proactive approach, designed to counter charges that they are a cabal that won't share data and that blocks dissenting views. Ralph Cicerone, president of the US National Academy of Sciences, says that the climate-science community must find ways to open up. He plans to talk to editors of science journals about setting standards for how much and what kind of raw data should be made available when climate studies are published. In addition to making the field more transparent, he says, standards will help scientists to separate legitimate requests for data from harassment. \"In fields where we don't have standards on how much data is enough, we have to create them, and climate science is such a field,\" says Cicerone. Other groups are experimenting with new methods for introducing climate science to the public. The Union of Concerned Scientists has begun profiling individual researchers on its website in an effort to put a face on the IPCC, which has been an easy target for critics in part because of its status as an impersonal international entity. One of the profiles focuses on Julia Cole, a geologist at the University of Arizona, Tucson, who contributed to the last IPCC assessment and studies palaeoclimate data extracted from stalagmites in an Arizonan cave. At Climate Central, a non-profit organization based in Princeton, New Jersey, scientists work with journalists and writers to develop climate stories in partnership with media outlets. The idea came together in 2008, backed by high-profile scientists such as Jane Lubchenco, who oversees much of the nation's climate science as head the US National Oceanic and Atmospheric Administration. Climate Central has published work in major magazines and newspapers as well as on broadcast television; one story in  Time   magazine (see  http://go.nature.com/BgyVSP ) covered a  Nature   paper documenting increasing ocean temperatures (J. M. Lyman  et al .  Nature   465 , 334-337; 2009). Researchers at George Mason University have teamed up with Climate Central on a project to see whether meteorologists on television can change the way people think about climate issues by making global warming into a local phenomenon. Beginning this summer on the television network WLTX in Columbia, South Carolina, weather forecaster Jim Gandy will integrate global warming into his coverage. Topics might include projections for increasing weather extremes over the next century, and how local gardeners are adapting to climate change. The George Mason team will use surveys at the start and end of the project to see whether it has any effect on public opinion. It is no coincidence that the team is starting with weather forecasters: a recent poll found that, after scientists, they are the most trusted source of information on global warming, despite their lack of formal training in climate science. \"The nation's weather forecasters are basically standing by, ready to teach their local populations,\" says Edward Maibach, director of George Mason University's Center for Climate Change Communication. \"We feel that we know them and trust them, and that means that they actually have greater potential to engage the public and teach them about climate change than do climate scientists, as a profession.\" Similar discussions have unfolded in the United Kingdom. In March, the Science Media Centre in London brought together a number of climate researchers in an effort to expand the roster of scientists talking to the media, which has tended to consult only a few high-profile researchers. Sheila Jasanoff, a science-policy expert at Harvard University in Cambridge, Massachusetts, says that more communication is good, particularly if scientists can help people to understand the local effects of a global phenomenon. But she warns against the assumption that public doubts and the lack of political action on climate change reflect a problem that can be solved simply by transferring knowledge. Even though a large fraction of the US public has believed for several years that humans are causing the globe to warm up, \"it was never clear that people were lined up to take painful action on the basis of what was said by the IPCC\", says Jasanoff. As a model for how to move forward, Jasanoff points to the US government's health and environmental regulatory process, which seeks public input through comments on proposed actions and includes non-scientists on advisory boards. She says that researchers should look for ways to build trust by taking on board the concerns of the public. Leiserowitz agrees that scientists should engage with the public, but he also urges researchers to be realistic about their influence. \"Even if climate-change scientists suddenly had the abilities of Carl Sagan to bring complex ideas to the public, there's only so much they can do,\" says Leiserowitz. \"It's hubristic to think that if we could just communicate better, suddenly we would change the world.\"    See Editorial,  \n                     page 7 \n                   , and Opinion,  \n                     page 30 \n                   . \n                     Yale Project on Climate Change Communication \n                   \n                     Woods Institute for the Environment surveys \n                   Reprints and Permissions"},
{"file_id": "465864a", "url": "https://www.nature.com/articles/465864a", "year": 2010, "authors": [{"name": "Richard Van Noorden"}], "parsed_as_year": "2006_or_before", "body": "Scientific performance indicators are proliferating \u2014 leading researchers to ask afresh what they are measuring and why. Richard Van Noorden surveys the rapidly evolving ecosystem. Scientists have been sizing up their colleagues since science began. But it was American psychologist James McKeen Cattell who first popularized the idea that systematically ranking scientists by 'performance' could provide benefits beyond scratching the itch of professional envy. In the 1910 second edition to his 1906 work,  American Men of Science: A Biographical Directory , he argued that tracking performance over time could assist the progress of research. \"It is surely time for scientific men to apply scientific method to determine the circumstances that promote or hinder the advancement of science,\" he wrote. That rationale for systematic evaluation hasn't changed much in 100 years, but the evaluation techniques have evolved dramatically. Where Cattell simply asked experts to rank the star performers in a field by merit \u2014 \"Expert judgment is the best, and in the last resort the only, criterion of performance,\" he wrote \u2014 a host of objective indicators, or metrics, are now used to quantify nebulous notions of scientific quality, impact or prestige. Within the past decade, the development of ever more sophisticated measures has accelerated rapidly, fuelled by the ready availability of online databases such as the Web of Science from Thomson Reuters, Scopus from Elsevier and Google Scholar. \"Right now we're going through a Cambrian explosion of metrics,\" says Johan Bollen, an informatics scientist at Indiana University in Bloomington. It has become all but impossible even to count today's metrics. Bibliometricians have invented a wide variety of algorithms, many of them unknown to the everyday scientist, some mistakenly applied to evaluate individuals, and each surrounded by a cloud of variants designed to help them apply across different scientific fields or different career stages (see 'Metrics explosion'). Here,  Nature   categorizes metrics old and new, lays out their strengths and weaknesses \u2014 and examines a growing feeling among researchers that it is time to slow down and discuss what these measures are actually for. The era of quantitative, computer-tabulated science metrics dates back to the 1950s, when linguist Eugene Garfield began indexing the scientific literature using punch cards. A company in Philadelphia, Pennsylvania, that Garfield founded in 1955 was renamed the Institute for Scientific Information (ISI) in 1960, the same year it began to publish the Science Citation Index. This was a systematic effort to track citations \u2014 the footnotes by which journal authors acknowledge their intellectual debts. (ISI is now owned by the publishing firm Thomson Reuters.) In 1965, Garfield and his colleagues used ISI's databases to show that Nobel laureates published five times the average number of papers, and that their work was cited 30 to 50 times the average \u2014 a finding that for decades established citations as the pre-eminent quantitative measure of a scientist's influence 1 . \n               Click here for larger image \n               For all that, Garfield's best-known and most widely used citation-based metric, the 'impact factor' (see  'Field guide to metrics' ), which he developed in 1963, is of little use in measuring an individual's performance; it applies only to the popularity of the journal. \"If there is one thing every bibliometrician agrees, it is that you should never use the journal impact factor to evaluate research performance for an article or for an individual \u2014 that is a mortal sin,\" says Anthony van Raan, director of the Centre for Science and Technology Studies at Leiden University in the Netherlands.  \n                Big impact \n              A better metric for assessing an individual by their citations is the  h -index, which has been swiftly adopted by major online databases since being introduced in 2005 by physicist Jorge Hirsch of the University of California in San Diego. According to Hirsch's definition, someone who had written, say, 50 papers that had each been cited at least 50 times would have an  h -index of 50. An author's  h -index has the virtue of measuring his or her article productivity and citation-based impact simultaneously. But it does have flaws, including the fact that an author's  h -index can reflect longevity as much as quality \u2014 and can never go down with age, even if a researcher drops out of science altogether. To combat this, \"there have probably been more than a dozen variants of the  h -index suggested since 2005, and even scholars in the field of bibliometrics have still not established which are the best ones to use,\" says Anne-Wil Harzing, a professor of international management at the University of Melbourne in Australia. For that reason, she adds, most scientists stick to the original  h -index, whatever its limitations. A third, increasingly popular, class of measure is the 'evaluative informetric', which gives heavier weight to citations from papers that are themselves highly cited. The principle is much the same as the PageRank algorithm that Google uses to order its search results: a link from a popular page is more highly weighted than one from a not-so-popular page. Both Thomson Reuters and Elsevier now offer to compute this kind of metric for journals \u2014 the companies refer to the result as the Eigenfactor and the SCImago Journal Rank (SJR), respectively. Unlike the resolutely journal-oriented impact factor, the page-rank concept has been usefully applied to individuals by some researchers. Filippo Radicchi, a researcher in complex networks at the Institute for Scientific Interchange in Turin, Italy, and his colleagues have used weighted citations to derive a network of links between more than 400,000 papers published between 1893 and 2006 in the Physical Review journals. By slicing through the network year by year, the researchers then showed how the influence of each scientist's articles spread through a community over time \u2014 which they in turn used to produce a quantitative ranking of physics authors 2 . For all their popularity, however, citation-based metrics share some fundamental weaknesses when it comes to evaluating individual researchers. One is that research papers commonly have multiple authors \u2014 \"possibly hundreds of them\", says Henk Moed, a senior science adviser at Elsevier in Amsterdam. A number of corrections can be applied to give the various authors fractional credit. But in some research fields, such as high-energy physics, there can be so many co-authors that assigning credit to individuals makes little sense, Moed says: \"Here one seems to reach the limits of the bibliometric system.\" Another weakness is that the scores depend on the database being used. Thomson Reuters's science, social science and arts and humanities databases \u2014 accessible through its Web of Knowledge interface \u2014 include data from about 11,500 journals. Elsevier's Scopus, introduced in 2004, includes abstracts and references from 16,500 peer-reviewed journals. And the free automatically indexed database Google Scholar, also introduced in 2004, includes details of patents as well as scientific papers, and covers many more journals in engineering, social sciences and the humanities than either of the others. A search in May showed that papers in international management by Harzing had been cited 815 times according to Thomson Reuters, 952 times according to Scopus and 2,226 times according to Google Scholar.  \n                Push for normality \n              For bibliometricians, the most daunting problem with citation-based metrics is getting the 'normalization' right: if molecular biologists tend to cite more often than physicists, then molecular biologists will have higher  h -indices or citation counts, making it difficult to compare individuals from the two fields. In principle, such variations can be evened out by dividing a researcher's citation rate by the average citation count for his or her field. But in practice, any attempt to do so swiftly gets bogged down in categorization: what constitutes a 'field'? A stem-cell researcher, for example, may bridle at being normalized by the average citation rate of cell biologists in general. \"Everyone has made a contribution to their particular granular subject area. If you define the area too broadly, you miss subtleties; too narrowly and you get nothing useful out of it,\" says Charles Oppenheim, emeritus professor of information science at Loughborough University, UK. One way to get around that problem is to let the citations define the categorization. This is the idea behind various attempts to construct 'maps of science', using networks of interconnecting citations to spot discrete research fields or intellectual environments. The process is hard to standardize, says van Raan. Nonetheless, he says, \"for individual scientists, mapping is the most interesting development in bibliometrics today\". Bollen agrees: such maps often show how research papers or novel disciplines lie at the centre of particular fields of activity, he says \u2014 which could allow a scientist to assert, \"my work connected nanotechnology to archaeology\", or \"if I hadn't published this paper, these domains would never have been connected\". Bibliometricians have suggested a host of measures to quantify such statements. These include 'betweenness centrality' \u2014 how often a paper in the network lies on the shortest path between any other two papers \u2014 and 'closeness centrality': the average number of connections required to get from a paper to any of the other papers. What aspects of scientific impact these measure is not entirely clear, but they probably give an indication of interconnectedness and interdisciplinarity.  \n                Cyberstalking \n              Meanwhile, some metrics researchers are looking to make a break from citations. As most scientific articles are now accessed and read online, why not just track the readers' actions in cyberspace through article or journal page views or downloads? Publishers such as the Public Library of Science already offer download statistics for their articles, together with social-bookmarking tools that allow scientists to flag papers that they find particularly useful. (Similar tools are offered by the online services Mendeley and Faculty of 1000.) The disadvantage of this approach is that it apportions the impact of a research paper according to all public views, not just those by scientists. But that can also be seen as an advantage, in that it expands the idea of scientific impact. For example, medical researchers might find that doctors, nurses and public-health policy-makers frequently view articles online, although the researchers never receive a traditional citation from these end-users. One early hurdle for this nascent field is that there are not yet global standards for journals to report data files of user activity. But COUNTER (Counting Online Usage of Networked Electronic Resources), a consortium of librarians and publishers based in Oxford, UK, is working to reach agreement on such a standard by 2012. Bollen's team is exploring whether online-usage data might help funding agencies to pick out fast-moving areas of innovation before citation-based statistics have a chance to catch up. The researchers have obtained a database of 1 billion usage events \u2014 records of users accessing scientific articles, newspapers and magazines in the years 2002 to 2007. They can also see in what order a user in any one session clicks through resources, allowing them to track the general flow of activity and produce maps showing which articles are central to which networks of activity. There are now maps that show how work in the social sciences and humanities formed bridges between scientific disciplines 3 . \"In principle you could use these records to track scientific activity in real time, and to follow science taking place on Twitter, blogs, or through online software, none of which can be recorded by citation data,\" says Bollen. Before this vision of instant influence-tracking can become solid, however, data on the Internet need to be organized and referenced in more consistent ways, and publishers need to agree to release information on usage statistics. Even as they push forward innovative ideas, many researchers in the metrics field say that it is high time for some reflection and consolidation. Little, if any, of the recent buzz has made it past the pages of scholarly journals into regular use on scientists' CVs, and, says Peter Binfield, publisher of  PLoS ONE , \"it feels like the field is going off in multiple directions\". More widely, says Bollen, although bibliometricians know that the idea of measuring scientific performance shares a fuzziness with the idea of measuring intelligence, many are too keen to promote their own innovations rather than focus on what they actually measure. \"The point should not be to come up with a new metric. It should be to explain what metrics represent, and why we want them,\" says Bollen. \"Can we come back to the scientific community and say 'if this is what you want to measure, then this is a good way to do that'?\" Much of the next few years of clearing through the rubble of metrics will involve this kind of process, he says. Similarly, although using a variety of metrics gives the clearest picture of scientific impact, some published research demonstrates that many people still desire a single index. \"There is some mind-numbing detail on how 'my version is better than yours'; all these people should know better than to think that there is a single measure you can use,\" says David Pendlebury, a consultant for Thomson Reuters based in Bend, Oregon. Many metrics correlate strongly with one another, suggesting that they are capturing much of the same information about the data they describe. Bollen's team last year published a study 4  comparing correlations between 39 measures of scientific impact for journals, attempting to tease out what different aspects of scholarly impact they captured. For example, the most important factor seems to be whether a metric measures 'rapid' or 'delayed' impact. Meanwhile, modern metrics are slowly finding users outside the traditional groups: journals hoping to promote their products or research-performance managers who, like Cattell, hope to boost research. Individual researchers are beginning to explore how new tools such as network mapping and online usage data could help them to identify other scientists who are close to their special interests, deliver relevant papers to literature searches more speedily or to pinpoint emerging innovative fields. Soon they could start to claim bibliometrics for themselves \u2014 assisting research in ways that Cattell never envisioned.   See Editorial,  \n                     page 845 \n                   , and metrics special at  \n                     www.nature.com/metrics \n                   . \n                     Metrics special \n                   \n                     Metrics Survey data \n                   \n                     The MESUR project, tracking online usage data \n                   \n                     PLoS online metrics \n                   \n                     Thomson Reuters metrics essays \n                   \n                     Elsevier's Scopus metrics \n                   \n                     'Publish or Perish': a software tool for calculating metrics from Google Scholar \n                   Reprints and Permissions"},
{"file_id": "465860a", "url": "https://www.nature.com/articles/465860a", "year": 2010, "authors": [{"name": "Alison Abbott"}, {"name": "David Cyranoski"}, {"name": "Nicola Jones"}, {"name": "Brendan Maher"}, {"name": "Quirin Schiermeier"}, {"name": "Richard Van Noorden"}], "parsed_as_year": "2006_or_before", "body": "Many researchers believe that quantitative metrics determine who gets hired and who gets promoted at their institutions. With an exclusive poll and interviews,  Nature   probes to what extent metrics are really used that way. No scientist's career can be summarized by a number. He or she spends countless hours troubleshooting experiments, guiding students and postdocs, writing or reviewing grants and papers, teaching, preparing for and organizing meetings, participating in collaborations, advising colleagues, serving on editorial boards and more \u2014 none of which is easily quantified. But when that scientist is seeking a job, promotion or even tenure, which of those duties will be rewarded? Many scientists are concerned that decision-makers put too much weight on the handful of things that can be measured easily \u2014 the number of papers they have published, for example, the impact factor of the journals they have published in, how often their papers have been cited, the amount of grant money they have earned, or measures of published output such as the  h -index. Last month, 150 readers responded to a  Nature   poll designed to gauge how researchers believe such metrics are being used at their institutions, and whether they approve of the practice.  Nature   also contacted provosts, department heads and other administrators at nearly 30 research institutions around the world to see what metrics are being used, and how heavily they are relied on. The results suggest that there may be a disconnect between the way researchers and administrators see the value of metrics. Three-quarters of those polled believe that metrics are being used in hiring decisions and promotion, and almost 70% believe that they are being used in tenure decisions and performance review (see 'Metrics perceptions'). When asked to rate how much they thought administrators were relying on specific criteria for evaluation, poll respondents indicated that the most important measures were grants and income, number of publications, publication in high impact journals and citations of published research. And a majority (63%) are unhappy about the way in which some of these measures are used (see 'No satisfaction'). \"Too much emphasis is paid to these flawed, seemingly objective measures to assess productivity,\" wrote a biologist from the United States. Respondents doubted that traditional, qualitative review counts for much. From a field of 34 criteria, \"Review of your work by peers outside your department or institution\" and \"Letters of recommendation from people in your field\" were tenth and twelfth, respectively \u2014 with 20\u201330% of the respondents stating that their institutions placed no emphasis on these factors at all. Yet in  Nature 's interviews, most administrators insisted that metrics don't matter nearly as much for hiring, promotion and tenure as the poll respondents seem to think. Some administrators said that they ignore citation-based metrics altogether when making such decisions, and instead rely largely on letters of recommendation solicited from outside experts in a candidate's field. \"Outside letters basically trump everything,\" says Robert Simoni, chairman of the biology department at Stanford University in California. That sentiment was echoed by academic administrators worldwide. \"Metrics are not used a great deal,\" says Alex Halliday, head of the Mathematical, Physical and Life Sciences Division at the University of Oxford, UK. \"The most important things are the letters, the interview and the CV, and our opinions of the papers published,\" he says. \"I don't look at impact factors\" of the journals a candidate publishes in, says Kenichi Yoshikawa, dean of the Graduate School of Science at Japan's Kyoto University. \"These usually highlight trendy papers, boom fields and recently highlighted topics. We at Kyoto don't want to follow boom.\" Metrics are not wholly excluded, of course. Those 'qualitative' letters of recommendation sometimes bring in quantitative metrics by the back door. \"We do not look at publication records or tell the reviewers to,\" says Yigong Shi, dean of the School of Life Sciences at Tsinghua University in Beijing. \"But in reality, they do have an impact, because the reviewers will look at them.\"  \n                Mixed messages \n              Administrators may also send mixed signals: metrics don't matter, except that they do. \"Each year we collect the average performances of people across various different things: student evaluations of lectures, teaching loads, research income, paper output,  h -indices,\" says Tom Welton, head of the chemistry department at Imperial College London. Welton insists that this information is reported back to researchers as a guideline, \"not a hurdle that has to be leapt over to get a promotion\". Nevertheless, the fact that such measures are being made could give the impression that they are being relied on heavily. At the Massachusetts Institute of Technology in Cambridge, Claude Canizares, vice-president for research and associate provost, says that \"we pay very little attention, almost zero, to citation indices and counting numbers of publications\". But, he says, \"if someone has multiple publications in a higher-impact journal, it's like getting another set of letters \u2014 the peers that reviewed that paper gave it high marks\". A separate reason for the disparity is that the use of metrics can vary markedly between countries (see  'Around the world with metrics' ) \u2014 or even between disciplines. Poll respondents and administrators agree that metrics have potential pitfalls. For example, 71% of respondents said that they were concerned that individuals at their institutions could manipulate the metrics, for example by publishing several papers on the same basic work. Most deans and provosts seemed less concerned about that possibility, arguing that such practices were unlikely to slip past reviewers. But they were wary of the more insidious effects of using metrics. \"If you decide that publishing a large number of papers is important, then you've decided that's what quality is,\" says Gregory Taylor, dean of the Science Faculty at the University of Alberta in Edmonton, Canada. \"That's always a very dangerous route to go down, because then you get people working to achieve by the formulae, which isn't a very good way to encourage people to use their imagination.\" Indeed, half the poll respondents said that they shaped their research behaviours on the basis of the metrics being used at their university. Although many of the altered behaviours given were fairly innocuous \u2014 for example, \"work harder\" \u2014 some had the potential to compromise scientific ideals. \"It discourages me from doing important research work that may be of null association,\" said one respondent, a US postdoctoral fellow.  \n                Breaking the old-boys' networks \n              Despite general dissatisfaction with the way in which metrics are being used, some poll respondents welcome them. Many said that they appreciated the transparency and objectivity that quantitative metrics could provide. \"I prefer this to qualitative metrics,\" wrote one, a department head in chemistry and engineering from Europe. Others who were dissatisfied with the use of metrics at their institution said they felt that the metrics weren't being used enough or weren't being used consistently. \"The metrics can be nullified at the college or provost level,\" complained a US professor of neuroscience. If nothing else, says Welton, the use of quantitative measures can reassure young researchers that the institution is not perpetuating an old-boys' network, in which personal connections are valued over actual achievement. Administrators who say that they do consider metrics in the decision-making process stress that they recognize the limitations of such measures in defining the career of an individual. Researchers in different fields and different specialities publish and cite at different rates. An intimate understanding of the fields \u2014 and more importantly the individuals being assessed \u2014 is crucial, they say. This ultimately makes the use of metrics more subjective by necessity. Surprisingly, if poll respondents desire change, it's not necessarily away from quantitative metrics. When  Nature   gave respondents a list and asked them to choose the five criteria that they thought should be used to evaluate researchers, the most frequently chosen was \"Publication in high-impact journals\", followed by \"Grants earned\", \"Training and mentoring students\" and \"Number of citations on published research\". In other words, what respondents think they are being measured on roughly matches what they want to be measured on. The challenge for administrators, it seems, is not to reduce their reliance on metrics, but to apply them with more clarity, consistency and transparency. \"The citation index is one of those things that is interesting to look at, but if you use it to make hiring decisions or use it as a sole or main criterion, you're simply abrogating a responsibility to some arbitrary assessment,\" says Jack Dixon, vice-president and chief scientific officer of the Howard Hughes Medical Institute in Chevy Chase, Maryland. While he says that the institute eschews such metrics, he recognizes that they will continue to be used. \"All decisions are based on various criteria. The thing you hope for is that the decisions are fair, and are based upon criteria that the reviewers know and understand.\"   See Editorial,  \n                     page 845 \n                   , and metrics special at  \n                     http://www.nature.com/metrics \n                   . Full results of the survey are available at  \n                     http://go.nature.com/em7auj \n                   . \n                     Metrics Special \n                   \n                     Metrics Survey data \n                   Reprints and Permissions"},
{"file_id": "4651002a", "url": "https://www.nature.com/articles/4651002a", "year": 2010, "authors": [{"name": "Colin Macilwain"}], "parsed_as_year": "2006_or_before", "body": "The grandfather of scientific national academies is staging major celebrations this week for its 350th birthday. But, like similar elite groups around the world, Britain's Royal Society has had to work hard to stay relevant and influential, reports Colin Macilwain. One thing that scientists have learned since the seventeenth century is how to throw a party. This week, the Queen is set to celebrate with hundreds of Britain's most brilliant minds, kicking off a summer of festivities to mark the 350th anniversary of the Royal Society of London for the Improvement of Natural Knowledge. The public will be invited to partake in a carnival of celebrity lectures, debates, live TV shows and exhibits to showcase science and the Royal Society's role in it. The choice of the South Bank \u2014 London's main arts centre and a major tourist bazaar \u2014 for the ten-day extravaganza signals the society's hunger to be seen as up to date, inclusive and important, not exclusive and aloof. National academies of science in more than 100 nations are aiming for the same goal, with varying success. Many were born in an era when a few select individuals practiced science, and those groups evolved to offer behind-the-scenes advice to governments. Now, the academies represent much more diverse communities, and they must take their messages not only to governments but also directly to the public. The Royal Society and its kindred academies have had to evolve in their own unique ways to meet the challenges of the twenty-first century. They try to offer sober advice on some of the most divisive issues \u2014 such as climate change, reproductive biology and genetically modified food \u2014 without offending their patrons or members. They must be seen to be independent of government, despite considerable reliance on public funding. And they need to reflect the growing ethnic and gender diversity of the scientific community, while still selecting members on the basis of their scientific reputations. Ever more nations are establishing academies of their own. They range from the Ethiopian Academy of Sciences in Addis Ababa, which opened for business two months ago, to the US National Academy of Sciences (NAS) in Washington DC, which employs 1,100 full-time staff members to turn out 200 reports each year for the government. \"The academy's function is to provide the consensus view of the scientific community,\" says Bruce Alberts, former president of the NAS. Given the range of topics that it handles and the diversity of views within that community, he says, \"it is very difficult to do\". The Royal Society and the NAS are two of the largest independent scientific academies in the world (see 'Two elites'), and illustrate two principal models of operation. The Royal Society is a self-constituted club with no formal, official role in government; the NAS is chartered to provide advice at the behest of the US Congress. (A different type of academy, of which the Chinese Academy of Sciences is an example, is effectively part of the state and runs many of the government science programmes in several communist and formerly communist countries.) \n               Click here for larger image \n               Some of the differences between the Royal Society and NAS models can be traced back to their respective histories. The NAS, like many other national academies, was set up by a patron \u2014 President Abraham Lincoln, at the height of the American Civil War, in 1863. The Royal Society, in contrast, was started by scientists themselves, expressly to promote science (see  'The Royal Society through the ages' ). But these founders were strong supporters of a monarchy recently restored after the English revolution \u2014 and their society soon sought, and got, the patronage of King Charles II. Early on, the Royal Society made clear that it owed allegiance not to king and country but to scientific truth. The society maddened King George III, for example, by siding with its fellow Benjamin Franklin in a debate about the shape of lightning conductors, even as Franklin fomented rebellion in the colonies. The society has continued to chart its own course. Like other national academies, it establishes its rules and elects its own members \u2014 an arrangement that draws charges of elitism. \"There's a sense of pride here in being elitist: the proportion of scientists who are fellows is very small,\" says Martin Rees, who became the society's president in 2005. \"But we're elite only in the sense that we ought to be elite.\"  \n                High standards \n              The society maintains its standards through a complex annual election process, which has evolved since it first started elections to select members in 1847. Recommendations for new fellows go to a network of sectional committees, which examine the nominations. They pick the top candidates and put them before the full society to endorse, rather than having a general election of all the nominations. The committees have tried to pull down the average age at which fellows are elected. \"There's more likelihood now of electing fellows in mid-career, in their mid-40s rather than their late 50s,\" says John Krebs, a zoologist at the University of Oxford, UK, and chairman of the Royal Society's science policy advisory group, who was only 39 when elected in 1984. Members are also asked to submit letters nominating individuals for positions in office. Some 200 of them did so before April's announcement that Paul Nurse, a cell biologist and president of Rockefeller University in New York, would succeed Rees at the end of this year. The empire that Nurse will inherit has two principal arms. Most of the Royal Society's work, paid for by a \u00a352-million (US$77-million) block grant from the government, supports early-career researchers through various types of awards, as well as 305 highly prestigious university research fellowships. The society's endowment financed \u00a313 million of policy work and other activities this year, generating statements and reports that provide the Royal Society's public face. Peter Hennessy, a historian at Queen Mary, University of London and a leading authority on British governance, says that the Royal Society's stock in the corridors of power is high. \"It has always had an influence in Whitehall, where it is seen as a gold-standard institution,\" he says. \"When the Royal Society has an input, people listen to it.\" William Waldegrave, science minister in the Conservative UK government from 1992 to 1994, agrees. He relied on it, in particular, to help him pick scientific advisers. \"I think the Royal Society does have very high prestige; it is one of these brands that always carries weight,\" he says. He is dismissive of the idea that the block grant could enable the government to influence the society. \"It would be a very rash minister who would try to pull that lever,\" he says. Since Waldegrave's time in office, the Royal Society has assumed a more aggressive and professional approach to public affairs. The change was driven partly by high-profile scientific crises a decade ago, concerning genetically modified food and bovine spongiform encephalopathy (BSE). These shook public confidence in science, and, to an extent, science's confidence in itself. The Royal Society responded in 2000 by electing Robert May, an Australian physicist-turned-ecologist, as its 58th president. May had previously served as the UK government's chief scientific adviser, and he helped transform the Royal Society from an inward-looking body into a public force, says Krebs. With his government experience, May \"could see how an independent voice like the Royal Society's could get real traction\", Krebs says. Although less voluble than May, Rees is a media-savvy scientist who has helped to keep the society in the public eye. Under his presidency, the society set up a Science Policy Centre, advised by Krebs' group, to consider policy issues and decide which the Royal Society should report on. One of the centre's first publications was a report on geoengineering, released in September 2009. James Wilsdon, the centre's director, says that the report has \"helped to change the terms of the debate\", prompting British research councils to officially consider some geoengineering approaches, such as cloud seeding, for the first time, as well as triggering their serious discussion in the media. Rees has used the society's anniversary celebrations to encourage a large number of TV documentaries on the BBC and other channels, most of them celebrating the achievements of Royal Society fellows past and present. (As part of the festivities next week, the Royal Society and  Nature   will co-host a conference that looks ahead 50 years on topics such as data storage and scientific careers.) Rees has also raised about \u00a3100 million from individuals and businesses for the society's own endowment, almost doubling its size. Rees's reign is not without its detractors. Some say that the Royal Society has retreated from politically contentious issues, to place greater emphasis on advocating science funding. In April, Richard Pike, chief executive of the Royal Society of Chemistry in London, Britain's largest learned society, criticized a government plan to confer with the Royal Society on budget matters, questioning its independence from government. \"They seem not to be challenging government policy as forthrightly as they could be,\" says Pike. Rees says that Pike is the only individual to make this criticism, and that others accept that the Royal Society's policy arm is independent of government.  \n                Speaking out on policy \n              Across the Atlantic, the NAS has even closer ties with its government, and so has evolved in different ways from the Royal Society. Most of the academy's studies are requested by Congress and are paid for through contracts with federal agencies. These contracts enable the academy and its sister organizations in Washington DC, the National Academy of Engineering and the Institute of Medicine, to support a large staff of specialists. The quasi-governmental status of the academy places its operations under more scrutiny than those of the Royal Society. A decade ago, for example, environmental groups went to court to argue that the academy should not be allowed to bar the public from panel discussions. \"At the time it was a very serious threat,\" says William Colglazier, the academy's executive officer. The NAS adopted reforms, such as public consultation on panel membership and some open meetings, that have strengthened the academy's processes, he says. The academy has also come under fire for the lack of diversity in its membership. Its president, atmospheric scientist Ralph Cicerone, concedes that the number of female members \u2014 about 12% \u2014 is \"not good enough\", and is also concerned about its under-representation of Asian-Americans, the causes of which, he says, the academy is now looking into. The academy is cautious about taking a stand on policy issues, although it has, for example, issued statements on the teaching of evolution and creationism, most recently in 2008. The NAS has also joined with other academies around the world in several consensus statements on what is known about global warming. \"We make policy statements of our own pretty rarely,\" says Cicerone. \"We have to choose topics where what we say is going to matter.\" May, who is also a member of the NAS, observes that the US academy is \"more constrained\" than the Royal Society \"by the fact that it has 1,100 staff producing reports, and depending on government money\", so that it \"has to think twice\" about issuing critical reports that might make enemies in Congress. Colglazier says that the NAS has honed its procedures over the past century to ensure the independence of its reports. \"The academy is perceived as a place to go for impartial study,\" he says, \"and that reputation could be lost very easily.\" Some would like the academy to speak out more. Robert Park, a physicist at the University of Maryland in College Park who comments on science policy issues in his online newsletter  What's New , says that he'd like to see the academy take a more strident line on hot issues such as creationism. \"They would be better off in the long run if they ignored Congress and said the things that need to be said, without any hesitation,\" he says. Others contend that the US academy is wiser to hold its fire and maintain its influence. That remains strong, particularly among the 'barons' who run congressional committees, says David Goldston, former chief of staff on the House science committee. \"They are among the only honest brokers in an increasingly polarized political environment,\" he says. In 2001, for example, the academy issued a report on climate change that helped to constrain the Bush administration, and its allies in Congress, from openly questioning the evidence linking human activity to global warming. And last month, the academy published a trio of massive reports called 'America's Climate Choices', which explains the science of climate change and makes the case for reducing emissions and for adapting to anticipated conditions. Although many of its conclusions could be found in the 2007 assessments issued by the Intergovernmental Panel on Climate Change (IPCC) in Geneva, Switzerland, the academy reports could influence US lawmakers as they debate climate legislation this summer.  \n                Global movement \n              Around the globe, national academies are seeking to expand their public role to achieve the kind of influence enjoyed by the Royal Society and the NAS. \"There's a whole move now to make academies a voice for science in every nation of the world,\" says Bruce Alberts. In Europe, only a few countries have established national academies as influential in their own capitals as the Royal Society. Two of the strongest are the Royal Swedish Academy of Sciences in Stockholm, founded by Carl Linnaeus and other scientists in 1739, and the Royal Netherlands Academy of Arts and Sciences in Amsterdam, founded in 1808. Germany took a major step in 2007, when the federal government officially recognized the Leopoldina, in Halle, as the country's scientific academy for the first time. J\u00f6rg Hacker, the biologist who assumed its presidency in March, says that it is increasing its staff to about 70, and that it will be fully independent, despite its reliance on federal and state government funds. In Africa, numerous academies have been established or reconstituted, including the Ethiopian Academy of Sciences. The track record of African academies has been mixed so far, with several struggling to attract young scientists and effective leaders. But Alberts credits the Academy of Science of South Africa with helping to finally reverse the government's denial that HIV causes AIDS. Increasingly, national academies have sought to unite on global issues such as climate change. The InterAcademy Panel based in Trieste, Italy, was created in 1993 and has issued numerous joint policy statements on behalf of its 104 member academies. An InterAcademy Council, meanwhile, was set up in 2000 with a secretariat in Amsterdam, to undertake major studies. Its current review of the IPCC for the United Nations is due in late August. Critics may contend that the public is indifferent to the academies' grand pronouncements, and that their reports are valued by politicians more for the cover they provide than for the carefully nuanced information they contain. But supporters remain assured that a wider purpose is being served. \"We only address scientific issues,\" says Stephen Cox, executive secretary of the Royal Society, \"and we have become more important because there are so many more issues today that have a scientific component.\"   See Editorial,  \n                     page 986 \n                   , and Opinion,  \n                     page 1009 \n                   . Colin Macilwain is a freelance writer based in Edinburgh, UK. \n                     Marc van Roosmalen's web site \n                   \n                     Help Marc van Roosmalen Web site \n                   \n                     Convention on Biological Diversity \n                   \n                     The Royal Society web site \n                   \n                     The US National Academies web site \n                   Reprints and Permissions"},
{"file_id": "466022a", "url": "https://www.nature.com/articles/466022a", "year": 2010, "authors": [{"name": "Erik Vance"}], "parsed_as_year": "2006_or_before", "body": "When emergencies happen in remote settings, field researchers can be left with little recourse. Erik Vance meets a man trying to change that. It started with a minor cut. A rock had come loose during a palaeontological dig and grazed a worker's leg. But when Matthew Lewin checked on him several days later, an infection had spread to his blood. The man had a dangerously high fever and was close to kidney failure. The most serious complication, however, was his location: a remote sun-blasted corner of the Gobi Desert in Mongolia, where he was working as part of a 2005 expedition searching for fossils from the Cretaceous period. Most patients in his condition would be admitted to a hospital's intensive-care unit. \"With good roads it would have been a two-day drive to the nearest hospital,\" says Lewin. Where they were, the roads were terrible. Lewin, an emergency-care doctor with a speciality in wilderness medicine, gave the man a steady drip of antibiotics. Over the next two days he recovered. To save intravenous drugs, though, Lewin switched him to pills. Within a few hours, the man's condition worsened: ulcers sprouted around his mouth and his skin began to peel off. Lewin recognized and treated the rare and often fatal drug response known as Stevens\u2013Johnson syndrome. \"That night, I never left the guy's bedside. I was just praying he wouldn't lose his lungs,\" says Lewin. With a combination of geeky awkwardness and outdoorsy bravado, Lewin shifts easily from talking about life-and-death emergency care to discussing obscure parasites, fossils and the best ways to trim 100 grams from his medical kit. Wilderness medicine \u2014 care given in remote settings with limited resources \u2014 is a loosely knit field that has grown in prominence over the past two decades. The Utah-based Wilderness Medical Society, the field's only membership organization, has 2,800 licensed members involved in the military, search and rescue, and disaster relief throughout the world.  \n                Medicine in no-man's land \n              For far-flung research projects, having a doctor there is often seen as a luxury rather than a necessity. For example, there are only eight spots for people on the Gobi expedition, which is run by the American Museum of Natural History in New York, says Lewin. \"Any time you are taking a physician, you are not taking a scientist.\" Except when you're taking both: with an MD and a PhD, Lewin attends to both the scientists and the science. In addition to working as a medic in places such as Peru, Mongolia and Chile, he has published more than 40 papers on work ranging from exotic parasites and the defensive behaviour of moth larvae to how best to open a person's throat during improvised surgery. \"I have a portable skill, which is medicine,\" says Lewin. \"And then I have a scientific background to go with it, which allows me to do productive research.\" Lewin is looking to spread the word. He has trained national-park responders, US special-forces medics, and medical students at the University of California, San Francisco, in methods for treating patients in the field. But he is also looking to expand the ranks of physician\u2013scientists like himself. For those doing field research, someone with medical expertise can be a blessing. \"I appreciate somebody like that who's thinking about [potential emergencies] so that I don't have to,\" says Nicholas Pyenson, a palaeontologist at the University of British Columbia in Vancouver, who led an expedition to Peru's Pisco Basin in 2008 and brought Lewin along. The California Academy of Sciences hired Lewin in the wake of a tragedy. In 2001, while leading a team of researchers into the jungles of Myanmar, snake expert Joseph Slowinski was bitten by a many-banded krait ( Bungarus multicinctus ), one of the world's most venomous snakes. The group had no anti-venom and minimal first-aid supplies. Slowinski died 28 hours later. From harsh lessons like these, Lewin preaches a message of preparation. Before even stepping off the plane in Mongolia, he says he had located the country's 14 computed tomography (CT) and magnetic resonance imaging scanners. He knew that there were just two dialysis machines in the entire country, and that one of them was broken. And perhaps most importantly, he contacted heads of hospitals and medical schools around the region offering lectures and much needed antibiotics. The goodwill could help ensure hospitality, should he bring in an injured scientist. Then there are his medical kits. Lewin co-authored the 'Wilderness Preparation, Equipment, and Medical Supplies' chapter in  Wilderness Medicine , considered to be the most comprehensive text on the topic, and he takes his kits seriously. Sizes range from a beverage cooler to a sandwich bag, depending on the demands of the trip, but he calls his medium lunchbox-sized container his \"single proudest professional accomplishment\". After more than a decade of adventures, Lewin has winnowed its contents down to the perfect balance of essentials and efficiency (see  'The perfect kit' ). \"The two big things people never bring are pregnancy tests and thermometers,\" he says, and includes both. There is also superglue for small cuts and a special stapler for larger ones. Snake bites, sepsis and rare drug reactions aside, most medical care he provides is fairly mundane: watching for dehydration, or urging a rest day for a group that is dangerously tired. Something as simple as an abscessed tooth can ruin an expedition. Field doctors can also find themselves making psychological interventions. Some researchers break down during remote trips. Fights happen, especially when alcohol is allowed. And once, Lewin had to surreptitiously sedate an agitated team member who was endangering the lives of his colleagues by provoking a band of local gunmen.  \n                A curious mind \n              Lewin says a doctor should be able to carry his weight in the infirmary and in the field. He has published work on a previously unknown species of ostrich-like dinosaur in northern China 1 , infectious diseases of Mongolia 2  and even the defensive behaviour of hornworms in Texas 3 . During a 2007 trip to the Gobi Desert, he found what looked like a dinosaur egg with the embryo still inside. Others on the trip doubted its contents, but rather than letting it go into storage, Lewin brought it home and snuck it into a CT scanner at his hospital. Inside was the fossil of an intact  Yamaceratops   embryo. It was the first embryonic example of its genus, and more detailed scans led to a paper that shed light on how the animals might have developed 4 . \"He has a curious mind,\" says Paul Auerbach, a surgeon at Stanford University in California who edited  Wilderness Medicine . \"He takes an approach that is a combination of enthusiasm, curiosity and wonderment to these settings. He isn't there just to be a doctor.\" In the wake of Slowinski's death, Lewin has been assembling funds to establish an \"exploration and travel health\" institute with the California Academy of Sciences. He hopes to launch the centre within a few years. The idea is to partner knowledgeable medical doctors with scientists travelling the globe. A doctor with diving experience and a classics degree could pair well with a trip to uncover sunken Mediterranean antiquities, for example. The group will also conduct research on how to improve wilderness medicine, evaluating resource-limited and ad-hoc remedies to determine best practices. They will teach classes, help scientists apply for expedition grants and prepare medical kits. David Mindell, who oversees research at the academy and nominated Lewin for membership, says that just having Lewin around has been helpful.  \n                Different risk thresholds \n              A big part of Lewin's work is advising scientists going to the field. He is careful not to push, though. Some researchers, such as Brian Fisher, an entomologist at the academy, prefer to go it alone. On his path to discovering more than 1,000 new species of ant, Fisher has visited the most inhospitable and dangerous places on Earth. He has been airlifted from war zones, has sewn up his own arm with fishing line and has been infected by just about every pathogen imaginable. \"I don't need a freaking thermometer. I am not taking this,\" says Fisher as he goes through a modest packet that Lewin prepared for him. For Fisher, speed is the key. He spends just a few days at one deep-jungle site before moving on to the next, and draws a hard line between \"making-you-feel-good medicine\" and \"making-sure-you-come-out-alive medicine\". Anything beyond pain medication, bandages and antibiotics is not necessary, he maintains. Fisher was initially wary of Lewin, assuming that Lewin would think his work was too dangerous and try to load him with things he didn't need. But Lewin is tactful. Field experience has taught him that different people accept different levels of risk. He honed his advice to a narrow set of recommendations. Fisher quickly warmed to Lewin and now refuses to deal with other doctors. He has also added an extra emergency provision. \"I do have a satellite phone, and Lewin's is the first number I have on there\", says Fisher, adding that he recently dropped his previous emergency service \u2014 a commercial company that provides medical advice and evacuation. \"Now I just rely on Matt.\"  Erik Vance is a freelance writer in Berkeley, California. \n                     Matthew Lewin's website \n                   Reprints and Permissions"},
{"file_id": "466176a", "url": "https://www.nature.com/articles/466176a", "year": 2010, "authors": [{"name": "Rex Dalton"}], "parsed_as_year": "2006_or_before", "body": "Finds in Turkey could answer key questions about ancient human origins, but palaeoanthropologists there must first bury their disputes. Rex Dalton reports from the field. Sparkling like black diamonds in the sun, countless obsidian pieces carpet a hillside in central Turkey. The slope is an ancient rubbish heap, littered with castoffs from toolmakers who chipped away at the prized rock to make knives, blades and other implements. For more than 500,000 years, obsidian veins formed by a nearby volcano were a stopping place for human ancestors trekking out of Africa to colonize Europe or Asia. Now the same volcanic glass is drawing people from even farther afield. Steven Kuhn, an archaeologist from the University of Arizona in Tucson, visited the site last summer, along with Turkish colleagues, to seek the remnants of archaic humans or their ancestors. Hiking up a sheep trail, the research team found one cave with a promising base of sediments \u2014 deep enough, and therefore old enough, to preserve fossils of the ancient migrants. In a brief survey, the scientists were encouraged when they found stone tools of the type used by Neanderthals between 250,000 and 100,000 years ago. This August, the research team, led by archaeologist Nur Balkan-Atl\u0131 of Istanbul University, will return to this cave and other sites around the volcano, known as G\u00f6ll\u00fc Da\u011f. The researchers have high hopes, as do others excavating in Turkey, which has the potential to become the next hotspot in the world of palaeoanthropology. Because Turkey lies between Africa and Europe, the country offers an unrivalled opportunity to find fossils that reflect the initial evolution of the first humans who left Africa. And caves on the Anatolian plateau \u2014 with their relatively cool conditions \u2014 have the potential to yield bones with intact DNA. So far, however, Turkey has failed to live up to its promise in palaeoanthropology. For nearly 20 years, several leaders in the field have been locked in bitter personal conflicts that have stymied progress and have had a detrimental effect on some young scientists. When work does proceed, researchers rarely publish in a timely manner, and they keep their specimens stored for years. To date, there has been only one full publication on an archaic human discovered in Turkey 1 . On top of those issues, the rise of Islamic fundamentalism in Turkey has made the working environment uncomfortable for palaeoanthropologists in much of the country. \"Turkey has a great potential for better understanding Neanderthal and modern human evolution and biogeography, but it is as yet unrealized,\" says Tim White, a palaeoanthropologist at the University of California, Berkeley, who has worked in Turkey. The lure of finding archaic human tools and fossils keeps researchers coming back to places such as G\u00f6ll\u00fc Da\u011f (see 'Ancient crossroads'). On their hike along the obsidian-strewn hillside last year, Kuhn and his colleagues could hardly take a step without treading on implements such as blades, points and scrapers, as well as the cores from which the tools were knapped. The researchers found entire hillsides of artefacts with similar characteristics, suggesting that these were made by the same ancient toolmakers. boxed-text One collection of sharp blades \u2014 as wide as a finger and the length of a hand \u2014 seems to be the work of a particular craftsman. The toolmaker's style has perplexed Kuhn and his colleagues, particularly those who have practised knapping to understand how early humans worked. \"No one can replicate this blade,\" says Balkan-Atl\u0131, who has spent more than a decade examining this wealth of stone tools. The team has taken a multipronged approach to studying the obsidian products. They use the shape of a tool to identify its overall style, which provides general clues about its origin. Then, geochemical analyses of its constituent elements can help to pinpoint the source quarry and possibly the workshop where the tool was made. The researchers can compare these details with those of distant specimens to track where the tools were carried and traded over the broader region. Relatively young items from G\u00f6ll\u00fc Da\u011f \u2014 those less than 7,000 years old \u2014 have been traced to Crete, Cyprus, the Dead Sea, Iran and Iraq. But researchers are still trying to determine which ancient humans made the older tools at this site \u2014 and how far the tools spread. Although stone tools appear everywhere at G\u00f6ll\u00fc Da\u011f, fossils have been hard to find. The highly acidic soil in this region of the Anatolian plateau is part of the problem, because it does not favour the preservation of bones over hundreds of thousands of years. Balkan-Atl\u0131's team therefore hopes to find fossils in the sedimentary deposits of caves, where rainwater cannot reach the soil and form acidic fluids. Finding fossils, however, does not guarantee academic success. The research atmosphere in Turkey can be every bit as corrosive as the soil on the plateau. Several leading palaeoanthropologists in Turkey have been involved in arguments for years, some of which have ended in court. One of the researchers involved in the disputes is Erksin G\u00fcle\u00e7, who previously chaired the anthropology department at Ankara University and is now vice-chancellor for research and education at Ahi Evran University in K\u0131r\u015fehir, one of eastern Turkey's newer institutions. She has led several successful research expeditions and has trained a generation of researchers. But colleagues say that she has been bruised by the infighting that has severely disrupted progress in the field.  \n                Legal battles \n              According to several Turkish scientists, the conflicts started nearly two decades ago as an intense rivalry between G\u00fcle\u00e7 and two other Ankara University researchers who rose to prominence at the same time: palaeoanthropologist Berna Alpagut and prehistorian I\u015fin Yal\u00e7inkaya. In the mid-1990s, a complaint was filed with Turkish government officials alleging that G\u00fcle\u00e7 had carried out illegal excavations and had smuggled fossils out of the country. The complaint was filed under a false name, and the source was never identified. The government investigated the allegations and eventually dropped the case. G\u00fcle\u00e7 says that she was cleared. But she says that she became progressively angry as Yal\u00e7inkaya and Alpagut continued to repeat the allegations. In 2000, G\u00fcle\u00e7 sued Yal\u00e7inkaya for making false statements and won 30,000 Turkish lira (US$48,000 then) in a judgment that Yal\u00e7inkaya has appealed. A final decision has been delayed for years. A spokesperson for Yal\u00e7inkaya says that she cannot comment because the case is pending. In 2008, G\u00fcle\u00e7 also sued Alpagut, claiming that she too had made false statements. Alpagut settled by paying 4,000 Turkish lira early last year to resolve the case. In an interview, Alpagut acknowledged the payment but added that she didn't want to answer questions about \"the very, very personal thing\" with G\u00fcle\u00e7. G\u00fcle\u00e7 says that her adversaries have backed off since her litigation. \"I wish I'd gone to court earlier. My life would have been more wonderful,\" she says. But the conflicts have continued, spreading to involve students as well. Last year, G\u00fcle\u00e7 sued one of her former PhD students, Ayla Sevim, who now works at Ankara University, in a dispute over credit for a 2007 research article 2  on the discovery of a rare ape from the Miocene epoch, which ran from 23 million to 5 million years ago. Sevim is listed as a co-author, but she says that the article was published without her permission. G\u00fcle\u00e7 denies that charge and sued Sevim for making false accusations. The case is still pending. Once the litigation is over, Sevim says that she hopes to publish her own article about the ape. She declined to discuss the issue further. Battles aside, the discovery of this Miocene ape,  Ouranopithecus turkae , helped to solidify Turkey's position as the richest location outside Africa for finding fossil apes.  Ouranopithecus turkae   is the largest known Eurasian hominid and became extinct between 8.7 million and 7.4 million years ago. Three other apes are known from Miocene formations in Turkey. Alpagut and colleagues discovered one 3 , and Alpagut was the lead author on a report on the rare skull of another 4 . To David Begun, a Miocene ape specialist from the University of Toronto, Canada, Turkey's resources are \"a palaeontologist's dream\". But Begun, who did fieldwork in Turkey for several years, believes that the research atmosphere there suffers because of infighting and a resistance by Turkish experts to work with foreign collaborators. \"It is a shame,\" says Begun, who is also one of the editors of the  Journal of Human Evolution . \"They need to cooperate more. But I wouldn't hold my breath on a quick solution. They may have to wait for the next generation of researchers to address the issues.\" Some researchers are seeking opportunities abroad to avoid such conflicts. One scientist, \u00d6mer G\u00f6k\u00e7\u00fcmen, received a doctorate in anthropology from the University of Pennsylvania in Philadelphia in 2008. He is now a postdoc at Harvard University in Cambridge, Massachusetts, where he studies the genomics of human biology and cultural diversity. G\u00f6k\u00e7\u00fcmen would consider returning home to work, but he worries that academic discord in Turkey would slow his research. \"I would have to commit half my time to internal politics and petty bureaucracy,\" he says. Archaeologists who study recent civilizations in Turkey have had more success working together and have made seminal discoveries, particularly in understanding Hittite culture 5 . They typically publish their results quickly, but those who study the Palaeolithic are generally much slower about releasing information. To researchers such as Alpagut, the pace of publication is appropriate. \"I do not agree with some scientists, who publish immediately,\" she says. She has continued her excavations at a cave inland from Antalya on the Mediterranean coast, where 14 years ago she discovered bones belonging to a Neanderthal or another type of archaic human. Dated to between 160,000 and 60,000 years old, the bones are some of the oldest human remains in the region. Researchers outside Turkey regard the specimens as important and eagerly await a description, but Alpagut has yet to publish a report on them. \"In my opinion, you need to wait and consider what to publish,\" she says. \"When the time is correct, the results will come out.\" G\u00fcven Arseb\u00fck, an emeritus professor at Istanbul University who pioneered Turkish anthropology instruction 40 years ago, says that his country needs a more competitive publishing climate. \"Unfortunately, we don't have a publish-or-perish principle in Turkey,\" says Arseb\u00fck. \"Publishing is a real problem for us.\" One instance of delayed publication relates to the Yar\u0131mburgaz Cave, west of the centre of Istanbul \u2014 one of Turkey's oldest habitation sites, possibly dating to 500,000 years ago. Arseb\u00fck and the late Clark Howell of the University of California, Berkeley, carried out a series of digs for three years ending in 1991. Tools and animal fossils \u2014 particularly from bears \u2014 were abundant. Yet, more than 20 years after it was excavated, the site has not been fully described in the literature. And, today, the cave that once was adorned with Greek and Roman altars is now littered with rubbish from drug addicts and prostitutes, who frequent the site. \"We hope to complete the monograph this year,\" says Arseb\u00fck, who acknowledges his own slow pace and is now working with Kuhn to wrap up the draft. Recently, a new problem for palaeoanthropologists has emerged. As the country has grown more religious, particularly in its eastern and central regions, Islamic creationists are rejecting the work of anthropologists, particularly that relating to evolution. This is creating an uncomfortable atmosphere for researchers and may be retarding Turkey's capacity to build up an anthropology infrastructure at central and eastern universities. Young graduates, who have studied and lived in cosmopolitan cities, are reluctant to move their families to these regions. Seventeen years ago, several dozen people died in Sivas, when Islamic extremists set fire to a hotel where intellectuals were staying. More recently, researchers have worried that local religious groups are watching dinner parties to determine whether scientists are violating Muslim tenets, for example, by drinking alcohol. For international researchers, the difficulties in Turkey cause considerable frustration, which at times drives them away. But some have persevered because of the tremendous potential. In addition to collaborating with Balkan-Atl\u0131 at G\u00f6ll\u00fc Da\u011f, Kuhn and his archaeologist wife, Mary Stiner, also based at the University of Arizona, have been working with G\u00fcle\u00e7 for more than a decade at a picturesque cliff-side cave overlooking the Mediterranean Sea near Turkey's southeast border with Syria. The cave, called \u00dc\u00e7a\u011fizli, turned out to be a rich site. By 2005, the team had found a wealth of tools, ornaments, burial artefacts and animal fossils, representing human movements through the region from 41,000 to 29,000 years ago, during the last ice age 6 . Then, two years ago, G\u00fcle\u00e7 discovered the palate of a human skull, a few teeth and some associated bones, a potentially important find that has not yet been published. She says that she hopes to complete a manuscript this year. Recent genetic studies 7  have made the fossils from this site even more intriguing because modern humans and Neanderthals are thought to have interbred in this region, creating lineages that migrated around the world.  \n                Land of the apes \n              Even as G\u00fcle\u00e7 works on the ice-age material, she is seeking answers from much older formations. On a warm, early morning during last year's field season, G\u00fcle\u00e7's team readied their backpacks for a hike up a ridge to a site about 20 kilometres north of Sivas. Their destination was a ravine packed with fossils from about 9 million years ago. The geological formations in this part of the country offer enormous potential for discovering early apes, ranging in age from 5 million to 18 million years old. Long before humans and their ancestors migrated from Africa, Turkey was the route for apes spreading from Africa to colonize Europe or Asia. The ravine near Sivas has produced a wealth of fossils since digs began in 2006. Elephant, rhinoceros, horse and hyena-like carnivore bones are jumbled together in bands running across the hillside. Fossils are so plentiful that they spill out of the earth as erosion uncovers them. The researchers say that it is an ideal environment for finding a hominid from the time of the last common ancestor of humans and apes. On that warm day, along the ravine, Cesur Pehlevan, a palaeoanthropologist from Ankara University and Y\u00fcz\u00fcnc\u00fc Y\u0131l University in Van, chipped away sediment, unearthing the attached leg bones of a three-toed horse. \"Oh, beautiful, this is rare,\" he said. From dawn to dusk, G\u00fcle\u00e7's team continues searching. They make casts to protect the bones and pack out the bounty to study it back at their home institutions. Last year, the team's schedule allowed only ten days for excavations, and they found no hominid fossils. This year, they hope for a longer field season \u2014 one that not only produces an ape fossil but also a prompt publication. \n                     Early apes and humans \n                   \n                     \u00dc\u00e7a\u011fizli Cave \n                   \n                     Sivas Museum \n                   Reprints and Permissions"},
{"file_id": "466174a", "url": "https://www.nature.com/articles/466174a", "year": 2010, "authors": [{"name": "Laura Spinney"}], "parsed_as_year": "2006_or_before", "body": "A once-threatened population of African fish is now providing a view of evolution in action. Laura Spinney asks what Lake Victoria cichlids have revealed about speciation. Ole Seehausen didn't expect to find much when he dropped his trawling net into Lake Victoria in 1991. The fish he was studying, called cichlids, had been disappearing from the East African lake for years. So he was astounded when he hauled in dozens of them. Close inspection of their coloration and shapes revealed five distinct species. The graduate student couldn't wait to deliver the news to his supervisor, Frans Witte, at Leiden University in the Netherlands. \"The quality of the phone line was so horrible that I wasn't sure he had understood that we had caught cichlids offshore again,\" he recalls. Seehausen's catch suggested a reversal of more than two decades of declining population and collapsing biodiversity. Lake Victoria, the world's second-largest freshwater lake, had been home to no fewer than 500 species of cichlid. The fish displayed such a variety of colours, shapes and adaptations that evolutionary biologist Tijs Goldschmidt, formerly at Leiden University, dubbed the lake 'Darwin's dreampond'. In one of the most spectacular demonstrations of what is known as adaptive radiation, cichlids are thought to have diversified from a handful of species in as few as 15,000 years \u2014 essentially a new species every 30 years. But that menagerie was all but obliterated when humans introduced a much larger predator, Nile perch ( Lates niloticus ), into the lake in the 1960s and then flocked to its shores to harvest them. Now, scientists such as Seehausen, at present a professor of ecology at the University of Bern and the Swiss Federal Institute of Aquatic Science and Technology in Kastanienbaum, have documented the cichlids' return and are capitalizing on the opportunity to watch a new radiation unfold. The researchers are starting to see how different environments shape the dynamics of speciation. And they are finding that hybridization between species may be a crucial step in generating the remarkable diversity that comes with adaptive radiation. This may help scientists to understand the timescales at which speciation occurs.  \n                Collapse and regrowth \n             Lake Victoria has dried up and been refilled several times in its 400,000-year history. Ancestors of today's cichlids arrived there after the last such cycle, roughly 15,000 years ago, possibly from neighbouring Lake Edward or Lake Kivu. Those founding cichlids exploited a wide range of niches, each of which encouraged specialized adaptation. Species numbers exploded. Around 15 major cichlid 'guilds' \u2014 each with many sub-specializations \u2014 have been identified in Lake Victoria, from streamlined open-water hunters to slope-headed algae scrapers. \"It is, by far, the fastest large-scale adaptive radiation known,\" says Seehausen. In 1979, when Witte and his group began studying the cichlids in the Mwanza Gulf of Lake Victoria (see 'Great lakes of East Africa'), they were catching 1,000 or more in 10 minutes of experimental trawling, representing, on average, 100 species. But by this time their numbers were already falling. The lake was changing. Whether for sport or to increase the amount of protein in the local diet, Nile perch were added to the lake in the early 1960s. A fecund and voracious predator, the fish grows to almost two metres in length and is a more attractive food source for humans than small and bony cichlids. Within a few decades, Nile-perch numbers expanded rapidly, along with a fishing industry. In a cautionary parable on globalization, film-maker Hubert Sauper chronicled the growth of slum-like 'fish cities' around filleting factories at Lake Victoria that were sending much of their produce to the developed world. In homage to Goldschmidt, he called his film, released in 2004,  Darwin's Nightmare . Although his conclusions have been controversial, the effects of development on Lake Victoria are undeniable. An influx of nutrients, mainly due to deforestation, industrialization and human pollution, fed large algal blooms that in turn fed oxygen-devouring bacteria. Over time, the water grew murkier and less able to sustain life \u2014 a process known as eutrophication. Owing to predation, eutrophication or both, cichlids had all but vanished from Witte's nets by the late 1980s. Researchers trawling in open waters were lucky to find any. Hence Seehausen's excitement in 1991. Today, Witte's data indicate that the biomass of cichlids in the lake has returned to 1979 levels, and that biodiversity is increasing. Genetic analyses and simple observation show that the cichlid population structure has changed markedly. Of the 20 species in the average catch, says Seehausen, two or three are species that seem to have survived the population crash unchanged. Another two or three look like new species, and are probably hybrids of old ones. But most fall in-between: they may have different colours or shapes, occupy different habitats or eat different foods, but genetically they aren't different enough from their predecessors to be termed new species. Crucially, old, new and in-between remain close enough to interbreed.  \n                Blind dates \n              Hybridization tends to reduce genetic diversity. Breeding between species is, indeed, antithetical to the reproductive isolation often deemed necessary for speciation. But this homogenizing step could be a precursor to adaptive radiation. In 2002, by comparing nuclear and mitochondrial DNA sequences, Walter Salzburger, an evolutionary biologist at the University of Basel in Switzerland, showed that two ancient cichlid species from the oldest of Africa's Great Lakes, Lake Tanganyika, may have hybridized roughly 100,000 years ago, producing a new species 1 . Salzburger speculates that some radical change in the lake environment brought the two parental species together. Although that ancient event can only be guessed at, Seehausen could be observing a contemporary parallel. Eutrophication has reduced visibility in the waters of Lake Victoria, especially near the shore, and female cichlids are less able to distinguish the different hues of breeding males, which is how they choose their mates 2 . Seehausen has found that less-discriminating encounters between species are producing cichlids that are not as genetically diverse, particularly in the more turbid, inshore waters. Where there were once hundreds of different specialists, there are now what he calls \"hybrid swarms\". But many of the gene variants that previously promoted the cichlids' prolific adaptation still persist in these swarms. Hybridization has essentially thrown much of the genetic record of the last adaptive radiation into an evolutionary blender, allowing those variants to come together in different combinations. Seehausen speculates that the lake's coastal bays and gulfs, where the water is murkiest, act as a 'hybridization belt'. Individuals of new genetic make-up are born here and move out into the altered lake. As they do so, they are confronted by the choice of new or vacated niches to exploit, and those specimens best adapted take up residence. Farther from the shore, visibility is better. Seehausen's group has shown that male breeding colours and female mate preferences co-evolve to suit the light conditions 3 . The changes that take place ultimately drive reproductive isolation. Thus, sexual selection reinforces ecological selection, resulting in new species. This seems to happen without the aid of geographical or physical barriers, and might therefore make the cichlids one of very few examples of side-by-side, or sympatric, speciation, says Seehausen.  \n                Learning to get along \n              That the returning cichlids are better suited to life in the altered lake is supported by evidence that they are now resisting the predator. What is puzzling, says Seehausen, is that many returning species seem to have increased rather than decreased habitat overlap with Nile perch. Take  Yssichromis pyrrhocephalus , one species that has returned with modifications. Before the population crashed, Witte found that this fish fed on zooplankton floating in the water. In 2008, his analyses of its stomach contents suggested that it was rooting up organisms from the lake bed 4 . Perhaps to cope with the lower oxygen levels at that depth, the surface area of its gills has increased. But Nile perch are abundant at the same depth. Seehausen can't explain why the two are now cohabitating successfully, when they couldn't 20 years ago. Better genetic tools might help researchers track the genes that are driving morphological and behavioural adaptation. For now, they are hampered by the limited genetic maps at their disposal, which have marked out only a fraction of the 30,000 or more genes in the cichlid genome. The genes that can be tracked using well-established markers may not be the ones that are driving adaptation. Moreover, because the different species have diverged from each other so recently, in evolutionary terms their DNA is similar. The sequences that differentiate them may not be those that can be tracked by existing markers, making it hard to identify distinct species. That could change once the cichlid genome has been fully sequenced \u2014 a project being undertaken by an international research group led by the US National Institutes of Health based in Bethesda, Maryland, and the Broad Institute in Cambridge, Massachusetts. Armed with a complete sequence, researchers will be able to select more sensitive markers of speciation, and feed the resulting data about the relationships between species into mathematical models to calculate when they diverged from each other. Such work may help to predict how long it will take for the current radiation to return cichlids to their former diversity in Lake Victoria. \"Even though we have good reasons to believe that evolution can be fast, this is likely going to take centuries, if not millennia,\" says Seehausen. What caused the cichlids' return is uncertain, but it is probably a combination of fishing pressures on the Nile perch and some measures taken to reduce pollution in the lake, coupled with the cichlids' own capacity for adaptation. Nobody is complacent about the recovery, however. Oliva Mkumbo, a senior scientist at the Lake Victoria Fisheries Organization in Jinja, Uganda, says that the water quality may have stopped deteriorating, thanks in part to the construction of new sewage works, but deforestation and erosion are still major problems. As Seehausen puts it, eutrophication could still \"close the show\", resulting in an even more catastrophic collapse of cichlid biomass and diversity. \n                     Darwin 200 Special \n                   \n                     Aquatic Ecology at the University of Bern \n                   \n                     Walter Salzburger's website \n                   \n                     The Lake Victoria Fisheries Organization \n                   Reprints and Permissions"},
{"file_id": "466312a", "url": "https://www.nature.com/articles/466312a", "year": 2010, "authors": [{"name": "Naomi Lubick"}], "parsed_as_year": "2006_or_before", "body": "An imminent swarm of tiny quakes beneath western North America could help seismologists prepare for a big one \u2014 but only if they can learn to interpret the tremors, finds Naomi Lubick. For the past few weeks, seismologists at the University of Washington in Seattle have been on high alert. Any day now, they expect a flurry of microtremors deep under the nearby Olympic Peninsula, just as occurs roughly every 12\u201314 months. And when that wave of vibrations comes along, the researchers will be ready to catch it. Over the past year, the seismology team has set up an elaborate net \u2014 an array of more than 100 seismic sensors planted in the ground throughout the peninsula's mountains. When those instruments start picking up signs of the tremors, the researchers will rush back out to install extra seismometers above the spots where the earth is shuddering. Every few days the scientists will tend to the sensors, replacing batteries and downloading data, with the hope of capturing as much information as possible about the seismic events underfoot. The quakes will pass within a few weeks without anybody feeling them. But for seismologists, these swarms of tremors, sometimes called non-volcanic tremors, are among the most significant discoveries of the past decade because they could yield insight into the behaviour of destructive faults. Researchers first noticed them in 2002 in Japan, and a year later teams in the Pacific Northwest detected them beneath the Cascadia region near Seattle. The Cascadian tremors are attracting interest because they hail from deep beneath the coast, along a massive fault that is thought to have unleashed earthquakes approaching magnitude 9. Researchers hope that the detailed measurements they are about to make will help them monitor activity along the fault. It may be possible to use the recurring swarms as a probe to track changes in stress underground; the tremors may even yield clues about the timing and location of future large quakes. \"It's pretty exciting,\" says Greg Beroza, a geophysicist at Stanford University in California, who is watching for the results from the University of Washington team. The regularity of the quakes lets researchers prepare for them, providing an exceedingly rare chance to study seismic events in great detail. \"If you had told me 10 years ago that there was going to be some kind of activity, happening on deep faults, that we could predict was going to happen again soon with some regularity, I would have told you that you were crazy,\" says Beroza.  \n                Elusive signals \n              The seismic trace of a tremor looks different from that of a classic earthquake. The latter starts with a bang \u2014 a burst of high-frequency energy that rapidly tails off. Tremors are tamer; their vibrations are rich in low frequencies, and they come and go with less fanfare. These characteristics, and the weakness of the tremors, make them hard to spot. Preliminary observations of tremors have come from Alaska and Mexico, among other places, but information remains sketchy because only a few places have networks of seismometers sensitive enough to catch them. Seismologist Kazushige Obara of the University of Tokyo Earthquake Research Institute was the first person to spot tremors, using signals recorded by a permanent monitoring array buried below the Earth's surface 1 . The Japanese government established the country-wide network of 600 monitors after the devastating 1995 Kobe earthquake; the instruments are sunk more than 100 metres below ground, to shield them from human-made noises \u2014 such as the rumble from trucks \u2014 that can drown out the tremors' vibrations. When Obara detected extremely deep microearthquakes lasting for several weeks, 30 kilometres beneath Japan, researchers around the world pored over their own data to see whether they could spot similar traces. Tremors in Japan occur mostly along junctions between tectonic plates where one dives or 'subducts' beneath another. The same structure underlies the Pacific Northwest, along what is known as the Cascadian subduction zone, which stretches from Vancouver Island in British Columbia, Canada, to northern California (see 'The origin of tremor swarms'). Here, the Juan de Fuca plate is slipping beneath the North American continental plate. Since the last huge quake, which is thought to have occurred in 1700, the dangerous part of the Cascadian subduction zone has remained locked, but researchers expect it to give way cataclysmically within the next century. \n               Click here for larger image \n               With that risk in the background, researchers are especially interested in capturing tremor swarms as a way to spy on what is happening deep down in the subduction zone. Last year, they missed their chance. The University of Washington team expected the swarm to occur in August, but it arrived in early May, before many of the instruments were in the field. The researchers recorded bits of the event using a permanent array, but in much less detail than they had hoped. This year, they are better prepared. Once their permanent array picks up the first signs of the tremors, they will head out to augment the network above the swarm. Each completed array will consist of about two-dozen seismometers stationed 200\u2013300 metres apart in remote areas, away from human noise. The team's work is supported by a US$500,000 grant from the US National Science Foundation to Earthscope, a non-profit research consortium. \"The experiment we have out now will be unprecedented with regard to what we can see,\" says John Vidale, a seismologist at the University of Washington and one of the lead scientists on the project. The team's network, called an 'array of arrays', can pinpoint the source of the tremors by comparing when the seismic waves arrive at each seismometer, says Vidale. \"As with your ears, you can tell which direction a sound is coming from,\" he explains. The origin of tremor swarms is generating intense debate. In Japan, seismologists have observed that tremors come from the plane where the plates meet, the subduction-fault boundary, which also gives birth to mammoth quakes. But in Cascadia, seismic signals indicate that some tremor may also occur above the fault plane, according to Honn Kao and his colleagues, geophysicists at the Pacific Geosciences Centre in Sidney, British Columbia 2 .  \n                Deep origins \n              Locating the source of the vibrations could help researchers to understand the physical processes that create them. The tremors originate tens of kilometres underground, where Earth's cold crust is pulled into the warmer mantle. Rocks in the upper crust are brittle enough to break under stress, which leads to earthquakes. But those in the mantle are so hot that they warp under pressure instead of cracking. Tremors seem to happen in the complex zone where the subducting plate has warmed enough for its rocks to become malleable. A study published this week 3  could help to explain the process that produces tremors under parts of Japan. Satoshi Ide, a seismologist at the University of Tokyo, suggests that tremor behaviour beneath Shikoku island could be controlled by the types of rock present in the subduction zone there. Ide found that the locations of some tremors line up in stripes matching the direction in which the subducting plate has travelled during the past 10 million years. He hypothesizes that these bands could trace the path left by seamounts, the remnants of former volcanoes embedded in the oceanic plate that is diving under Japan. If he is right, the seamounts could influence the behaviour of tremors in two possible ways, he says. They might snag on the overlying plate as the crust slides into the mantle, creating occasional bursts of tremors. Alternatively, their chemical composition might cause them to act as lubricants, like lumps of grease, smoothing out the movement of the plates and turning what could have been catastrophic ruptures into bumpy jitters. Beroza calls Ide's findings preliminary, but \"a really important result\" that indicates how the properties of the rocks along faults might be preserved for millions of years and affect the occurrence of tremors today. As in Japan, tremors in Cascadia sometimes occur in bands that might reflect some characteristic of the subducting rocks. Researchers are trying to determine what conditions create tremors instead of full-blown earthquakes, using data from old and new laboratory experiments that test how friction, porosity and other attributes of rock change under pressure. These studies could help reveal what controls the movement of faults at subduction-zone depths, says geophysicist Paul Segall of Stanford University.  \n                Quake clues \n              Scientists also wonder how tremors relate to larger earthquakes on the same faults. The better-observed tremor events in Japan and Cascadia tend to be close to the parts of subducting plates that produce great earthquakes; the tremors originate below the 'locked' areas that store up energy for future large quakes. Joan Gomberg, a geophysicist at the University of Washington, says researchers are looking for changes in tremor patterns that may hint at stress shifts in a fault. If such connections emerge, seismologists could start to monitor tremors for signs that precede a big earthquake. They have found some intriguing clues but nothing solid so far. The San Andreas Fault in California, for example, has experienced tremor events alongside earthquakes of magnitude 1 or 2, but direct associations between the two have yet to be confirmed 4 . Meanwhile, Obara and his colleagues plan to put out an extra monitoring system, to gather more details about tremor events beneath Japan. And researchers in Cascadia are ready to pounce when the swarm hits. \"These things happen almost like clockwork, but not exactly,\" says seismologist Ken Creager of the University of Washington, recalling last year's disappointment when the team missed the tremors. This year, he is more confident. \"We'll catch it one way or another.\" \n                     Nature Earth Sciences \n                   \n                     Nature Geoscience \n                   \n                     Pacific Northwest Seismic Network (PNSN) \n                   \n                     PNSN maps \n                   \n                     PNSN blog for 2010 \n                   \n                     Earth & Space Sciences at the University of Washington, Seattle \n                   \n                     USGS seismology page for the 1700 Cascadia earthquake \n                   \n                     Earthscope \n                   Reprints and Permissions"},
{"file_id": "466432a", "url": "https://www.nature.com/articles/466432a", "year": 2010, "authors": [{"name": "Janet Fang"}], "parsed_as_year": "2006_or_before", "body": "Eradicating any organism would have serious consequences for ecosystems \u2014 wouldn't it? Not when it comes to mosquitoes, finds Janet Fang. Every day, Jittawadee Murphy unlocks a hot, padlocked room at the Walter Reed Army Institute of Research in Silver Spring, Maryland, to a swarm of malaria-carrying mosquitoes ( Anopheles stephensi ). She gives millions of larvae a diet of ground-up fish food, and offers the gravid females blood to suck from the bellies of unconscious mice \u2014 they drain 24 of the rodents a month. Murphy has been studying mosquitoes for 20 years, working on ways to limit the spread of the parasites they carry. Still, she says, she would rather they were wiped off the Earth. That sentiment is widely shared. Malaria infects some 247 million people worldwide each year, and kills nearly one million. Mosquitoes cause a huge further medical and financial burden by spreading yellow fever, dengue fever, Japanese encephalitis, Rift Valley fever, Chikungunya virus and West Nile virus. Then there's the pest factor: they form swarms thick enough to asphyxiate caribou in Alaska and now, as their numbers reach a seasonal peak, their proboscises are plunged into human flesh across the Northern Hemisphere. So what would happen if there were none? Would anyone or anything miss them?  Nature   put this question to scientists who explore aspects of mosquito biology and ecology, and unearthed some surprising answers. There are 3,500 named species of mosquito, of which only a couple of hundred bite or bother humans. They live on almost every continent and habitat, and serve important functions in numerous ecosystems. \"Mosquitoes have been on Earth for more than 100 million years,\" says Murphy, \"and they have co-evolved with so many species along the way.\" Wiping out a species of mosquito could leave a predator without prey, or a plant without a pollinator. And exploring a world without mosquitoes is more than an exercise in imagination: intense efforts are under way to develop methods that might rid the world of the most pernicious, disease-carrying species (see  'War against the winged' ). Yet in many cases, scientists acknowledge that the ecological scar left by a missing mosquito would heal quickly as the niche was filled by other organisms. Life would continue as before \u2014 or even better. When it comes to the major disease vectors, \"it's difficult to see what the downside would be to removal, except for collateral damage\", says insect ecologist Steven Juliano, of Illinois State University in Normal. A world without mosquitoes would be \"more secure for us\", says medical entomologist Carlos Brisola Marcondes from the Federal University of Santa Catarina in Brazil. \"The elimination of  Anopheles   would be very significant for mankind.\"  \n                Arctic pests \n              Elimination of mosquitoes might make the biggest ecological difference in the Arctic tundra, home to mosquito species including  Aedes impiger   and  Aedes nigripes . Eggs laid by the insects hatch the next year after the snow melts, and development to adults takes only 3\u20134 weeks. From northern Canada to Russia, there is a brief period in which they are extraordinarily abundant, in some areas forming thick clouds. \"That's an exceptionally rare situation worldwide,\" says entomologist Daniel Strickman, programme leader for medical and urban entomology at the US Department of Agriculture in Beltsville, Maryland. \"There is no other place in the world where they are that much biomass.\" Views differ on what would happen if that biomass vanished. Bruce Harrison, an entomologist at the North Carolina Department of Environment and Natural Resources in Winston-Salem estimates that the number of migratory birds that nest in the tundra could drop by more than 50% without mosquitoes to eat. Other researchers disagree. Cathy Curby, a wildlife biologist at the US Fish and Wildlife Service in Fairbanks, Alaska, says that Arctic mosquitoes don't show up in bird stomach samples in high numbers, and that midges are a more important source of food. \"We (as humans) may overestimate the number of mosquitoes in the Arctic because they are selectively attracted to us,\" she says. Mosquitoes consume up to 300 millilitres of blood a day from each animal in a caribou herd, which are thought to select paths facing into the wind to escape the swarm. A small change in path can have major consequences in an Arctic valley through which thousands of caribou migrate, trampling the ground, eating lichens, transporting nutrients, feeding wolves, and generally altering the ecology. Taken all together, then, mosquitoes would be missed in the Arctic \u2014 but is the same true elsewhere?  \n                Food on the wing \n              \"Mosquitoes are delectable things to eat and they're easy to catch,\" says aquatic entomologist Richard Merritt, at Michigan State University in East Lansing. In the absence of their larvae, hundreds of species of fish would have to change their diet to survive. \"This may sound simple, but traits such as feeding behaviour are deeply imprinted, genetically, in those fish,\" says Harrison. The mosquitofish ( Gambusia affinis ), for example, is a specialized predator \u2014 so effective at killing mosquitoes that it is stocked in rice fields and swimming pools as pest control \u2014 that could go extinct. And the loss of these or other fish could have major effects up and down the food chain. Many species of insect, spider, salamander, lizard and frog would also lose a primary food source. In one study published last month, researchers tracked insect-eating house martins at a park in Camargue, France, after the area was sprayed with a microbial mosquito-control agent 1 . They found that the birds produced on average two chicks per nest after spraying, compared with three for birds at control sites. Most mosquito-eating birds would probably switch to other insects that, post-mosquitoes, might emerge in large numbers to take their place. Other insectivores might not miss them at all: bats feed mostly on moths, and less than 2% of their gut content is mosquitoes. \"If you're expending energy,\" says medical entomologist Janet McAllister of the Centers for Disease Control and Prevention in Fort Collins, Colorado, \"are you going to eat the 22-ounce filet-mignon moth or the 6-ounce hamburger mosquito?\" With many options on the menu, it seems that most insect-eaters would not go hungry in a mosquito-free world. There is not enough evidence of ecosystem disruption here to give the eradicators pause for thought.  \n                At your service \n              As larvae, mosquitoes make up substantial biomass in aquatic ecosystems globally. They abound in bodies of water ranging from ephemeral ponds to tree holes 2  to old tyres, and the density of larvae on flooded plains can be so high that their writhing sends out ripples across the surface. They feed on decaying leaves, organic detritus and microorganisms. The question is whether, without mosquitoes, other filter feeders would step in. \"Lots of organisms process detritus. Mosquitoes aren't the only ones involved or the most important,\" says Juliano. \"If you pop one rivet out of an airplane's wing, it's unlikely that the plane will cease to fly.\" The effects might depend on the body of water in question. Mosquito larvae are important members of the tight-knit communities in the 25\u2013100-millilitre pools inside pitcher plants 3 , 4  ( Sarracenia purpurea ) on the east coast of North America. Species of mosquito ( Wyeomyia smithii ) and midge ( Metriocnemus knabi ) are the only insects that live there, along with microorganisms such as rotifers, bacteria and protozoa. When other insects drown in the water, the midges chew up their carcasses and the mosquito larvae feed on the waste products, making nutrients such as nitrogen available for the plant. In this case, eliminating mosquitoes might affect plant growth. In 1974, ecologist John Addicott, now at the University of Calgary in Alberta, Canada, published findings on the predator and prey structure within pitcher plants, noting more protozoan diversity in the presence of mosquito larvae 5 . He proposed that as the larvae feed, they keep down the numbers of the dominant species of protozoa, letting others persist. The broader consequences for the plant are not known. A stronger argument for keeping mosquitoes might be found if they provide 'ecosystem services' \u2014 the benefits that humans derive from nature. Evolutionary ecologist Dina Fonseca at Rutgers University in New Brunswick, New Jersey, points as a comparison to the biting midges of the family Ceratopogonidae, sometimes known as no-see-ums. \"People being bitten by no-see-ums or being infected through them with viruses, protozoa and filarial worms would love to eradicate them,\" she says. But because some ceratopogonids are pollinators of tropical crops such as cacao, \"that would result in a world without chocolate\". Without mosquitoes, thousands of plant species would lose a group of pollinators. Adults depend on nectar for energy (only females of some species need a meal of blood to get the proteins necessary to lay eggs). Yet McAllister says that their pollination isn't crucial for crops on which humans depend. \"If there was a benefit to having them around, we would have found a way to exploit them,\" she says. \"We haven't wanted anything from mosquitoes except for them to go away.\" Ultimately, there seem to be few things that mosquitoes do that other organisms can't do just as well \u2014 except perhaps for one. They are lethally efficient at sucking blood from one individual and mainlining it into another, providing an ideal route for the spread of pathogenic microbes. \"The ecological effect of eliminating harmful mosquitoes is that you have more people. That's the consequence,\" says Strickman. Many lives would be saved; many more would no longer be sapped by disease. Countries freed of their high malaria burden, for example in sub-Saharan Africa, might recover the 1.3% of growth in gross domestic product that the World Health Organization estimates they are cost by the disease each year, potentially accelerating their development. There would be \"less burden on the health system and hospitals, redirection of public-health expenditure for vector-borne diseases control to other priority health issues, less absenteeism from schools\", says Jeffrey Hii, malaria scientist for the World Health Organization in Manila. Phil Lounibos, an ecologist at the Florida Medical Entomology Laboratory in Vero Beach says that \"eliminating mosquitoes would temporarily relieve human suffering\". His work suggests that efforts to eradicate one vector species would be futile, as its niche would quickly be filled by another. His team collected female yellow-fever mosquitoes ( Aedes aegypti ) from scrap yards in Florida, and found that some had been inseminated by Asian tiger mosquitoes ( Aedes albopictus ), which carry multiple human diseases. The insemination sterilizes the female yellow-fever mosquitoes \u2014 showing how one insect can overtake another. Given the huge humanitarian and economic consequences of mosquito-spread disease, few scientists would suggest that the costs of an increased human population would outweigh the benefits of a healthier one. And the 'collateral damage' felt elsewhere in ecosystems doesn't buy much sympathy either. The romantic notion of every creature having a vital place in nature may not be enough to plead the mosquito's case. It is the limitations of mosquito-killing methods, not the limitations of intent, that make a world without mosquitoes unlikely. And so, while humans inadvertently drive beneficial species, from tuna to corals, to the edge of extinction, their best efforts can't seriously threaten an insect with few redeeming features. \"They don't occupy an unassailable niche in the environment,\" says entomologist Joe Conlon, of the American Mosquito Control Association in Jacksonville, Florida. \"If we eradicated them tomorrow, the ecosystems where they are active will hiccup and then get on with life. Something better or worse would take over.\" Janet Fang is an intern in Nature's Washington DC office. \n                     Malaria Special \n                   \n                     CDC mosquito-borne diseases \n                   \n                     WHO vector-borne diseases \n                   \n                     Mosquito catalogue \n                   \n                     Armed Forces Pest Management Board \n                   \n                     USDA Agricultural Research Service \n                   Reprints and Permissions"},
{"file_id": "466310a", "url": "https://www.nature.com/articles/466310a", "year": 2010, "authors": [{"name": "Geoff Brumfiel"}], "parsed_as_year": "2006_or_before", "body": "A new class of materials is poised to take condensed-matter physics by storm. Geoff Brumfiel looks at what is making topological insulators all the rage. For a brief time in Portland, Oregon, this past spring, thousands of physicists moved from session to session at the annual March meeting of the American Physical Society (APS) on the lookout for the next big thing. It was a talent search not unlike the one that unfolds every night in the bars and converted dance halls of Portland's famous music scene, where locals listen for the next big sound. The physicists' quest is a lot harder, though. Trends in music come and go, but the disciplines that dominate the APS March meeting \u2014 such as optics, electronics and condensed-matter physics \u2014 are rooted in the original theories of quantum mechanics, which were more-or-less completed in the 1930s. When it comes to describing how light and matter behave, only a few phenomena have emerged since then to become the physics equivalent of superstars. At this year's APS meeting, however, the hallways were filled with talk of a promising newcomer \u2014 an eccentric class of materials known as topological insulators. The most striking characteristic of these insulators is that they conduct electricity only on their surfaces. The reasons are mathematically subtle \u2014 so much so that one physicist, Zahid Hasan of Princeton University in New Jersey, tried to explain the behaviour using 'simpler' concepts such as superstring theory. (\"It's awfully beautiful stuff,\" he said reassuringly.) Yet the implications are rich, ranging from practical technology for quantum computing to laboratory tests of advanced particle physics. Hence the excitement. It is still too early to say whether topological insulators are the next big thing. But physicists are auditioning various formulations of the insulators in their labs, eager to determine whether the material can live up to its many promises. A topological insulator sounds simple enough \u2014 a block of material that lets electrons move along its surface, but not through its inside. In fact, it is far from straightforward. Ordinary metals conduct electrons all the way through, whereas ordinary insulators don't conduct electrons at all. A copper-plated block of wood conducts only on the surface, but that is two materials, not one. The idea of a topological insulator is so strange that for a long time, physicists had no reason to believe that such a material would exist. \n                Quantum dance \n              Things changed in 2004, when Charles Kane, a theoretical physicist at the University of Pennsylvania in Philadelphia, was studying sheets of carbon called graphene. Kane's calculations suggested that electrons would move through this one-atom-thick material in a way that reminded him of the quantum Hall effect: a phenomenon first observed in 1980. This effect occurs when electrons are confined to thin films of certain materials, subjected to large electric and magnetic fields, and cooled to within just a fraction of a degree above absolute zero. At that point, the ordinarily chaotic motion of the electrons gives way to a more orderly collective behaviour governed by quantum mechanics. The transition shows up in the laboratory as a series of discrete quantum steps in the capacity of the material to conduct electricity. What Kane and his group saw in their graphene calculations wasn't exactly the same as the quantum Hall effect. Even so, further analyses showed that there might be other thin-film materials with similar behaviour. This time there would be no need for huge external magnetic fields or ultra-low temperatures to get the electrons moving in unison. Such a material would produce the magnetic field from the nuclei of its own atoms \u2014 possibly even at room temperature. These coordinated electrons would mostly end up just spinning in place. Intriguingly, however, those at the edge of the thin film would be forced to skip along the boundary. The net result would be that thin samples would conduct electricity along the edge \u2014 and only along the edge \u2014 but in separate quantum steps, similar to those seen in the quantum Hall effect 1 . The work of Kane and his colleagues got noticed almost immediately. Joel Moore, a theorist at the University of California, Berkeley, and his co-workers built on Kane's calculations to show that three-dimensional blocks of material would also display quantum effects 2 , although the way electrons moved along the surface would be more complex than in the flat sheet used by Kane. Moore also gave the materials a new name. They were originally termed \"novel  Z 2  topological invariants\" by Kane, in reference to the quantum-mechanical properties that cause the electrons to skip along the edge. \"We got tired of typing that out, so we called it a 'topological insulator',\" says Moore. \"I don't know that that term is particularly explanatory, but at least it's short.\" Meanwhile, Shoucheng Zhang at Stanford University in Palo Alto, California, and his team were researching what types of real material could be topological insulators. In most materials, Zhang realized, the link between electrons and nuclei is too weak to create a topological-insulating behaviour. But the link gets stronger as the nuclei get heavier. In 2006, Zhang predicted that one material in particular, a crystal made of the heavy elements mercury and tellurium, would be able to do the trick 3 . And within a year, Laurens Molenkamp, a physicist at the University of W\u00fcrzburg in Germany, and his group had grown a thin layer of mercury telluride crystal and showed that its conductance hopped from one quantum value to the next along the edge of the sample 4 . The experiment by Molenkamp proved that the theorists were onto something, but by itself it didn't cause much excitement. Mercury telluride crystals are difficult to obtain \u2014 they have to be grown one layer at a time using a laborious process known as molecular beam epitaxy \u2014 and they are not pure topological insulators because they conduct some electricity on their inside. New compounds based around bismuth, which are simple to make and cheap to work with, have caused the field to take off. \"What got so many talks at the March meeting was the bismuth-based compounds,\" says Hasan. \"Anybody can grow them, you can buy them off the shelf, and you don't need a high-purity crystal to see the topological effects.\" Those effects go beyond the way electrons move on the surface. For example, all electrons are spinning in a quantum mechanical way. Usually, the spins are constantly knocked about by random collisions and stray magnetic fields. But spinning electrons on the surface of a topological insulator are protected from disruption by quantum effects (for more, see  page 323 ). This could make the materials beneficial for spin-related electronics, which would use the orientation of the electron spin to encode information, thereby opening up a whole new realm of computer technology.  \n                Mathematical mimicry \n             Researchers also believe that the collective motions of electrons inside topological insulators will mimic several of the never-before-seen particles predicted by high-energy physicists. Among them are axions, hypothetical particles predicted in the 1970s; magnetic monopoles, single points of north and south magnetism; and Majorana particles \u2014 massless, chargeless entities that can serve as their own antiparticles. This mimicry is not entirely surprising. Almost by definition, collective electron motions can be described by just a handful of variables obeying simple equations, says Frank Wilczek, a Nobel-prizewinning particle physicist at the Massachusetts Institute of Technology in Cambridge. \"There are only a few kinds of equations that you can write down that are really simple,\" he says. So topological-insulator theorists and particle physicists have almost inevitably ended up in the same place. Majorana particles could prove particularly useful for practical quantum computing. The idea is to perform calculations using the laws of quantum mechanics, which could make computers much faster than the normal variety at certain tasks such as code-breaking. But the fragile quantum states essential to their operation are easily destroyed by jolts from the outside environment. Majorana particles would spread quantum information across particles, making them far more resistant to interference, says Kane. If Majorana particles could be harnessed on the surface of a topological insulator, \"this would be big\", he says. The wealth of calculations, experiments and applications offered by topological insulators, together with their availability, have given the field a white-hot status at the moment \u2014 as has a certain thirst for glory. Two variations of the quantum Hall effect have won their discoverers Nobel prizes, and some researchers think that a Nobel awaits whoever can contribute the most to the growing field. \"I'm not thinking about that at this point,\" says Kane, but the competitiveness has forced him to ensure that others are aware of his work. \"I sort of feel like if I don't assert myself, then I'm going to get buried,\" he says. Yet trips to Stockholm are some way off. Although samples of topological insulators are now easy to get hold of, most still contain impurities that cause them to conduct electricity on the inside, disrupting the states on the surface. Getting things perfect remains more of an art than a science. Furthermore, some of the sought-after effects will require topological insulators to be combined with more common materials. To create Majorana particles, for example, topological insulators will have to merge with superconductors. Many experiments on how best to do that are under way. The results of these studies will determine whether topological insulators are more than a one-hit wonder. Regardless, says Moore, there is an undeniable appeal in how the collective behaviour of electrons can lead to so many wonderful things. \"There's something about many-particle quantum mechanics that causes perfection to emerge out of imperfection,\" he says. \"That's somewhat cheering as far as our everyday lives are concerned.\"   See News & Views,  \n                     page 323 \n                   , and Letters,  \n                     page 343 \n                   . \n                     Exotic Matter Insight \n                   \n                     Charles Kane's homepage \n                   \n                     Joel Moore's group \n                   \n                     Shoucheng Zhang's group \n                   Reprints and Permissions"},
{"file_id": "466428a", "url": "https://www.nature.com/articles/466428a", "year": 2010, "authors": [{"name": "Eric Hand"}], "parsed_as_year": "2006_or_before", "body": "NASA and Germany have spent 15 years and billions of dollars on SOFIA, an airborne telescope that is about to produce its first results. Eric Hand asks whether the science will justify the cost. The hangar is so big that it once held a fleet of pirate ships. That was back when this 20,000 square-metre NASA facility in the desert town of Palmdale, California, was being used as a soundstage for the 2007 film  Pirates of the Caribbean: At World's End . Today, the film crews have long since packed up and gone. But the NASA mechanics pedalling around the squeaky-smooth concrete floor on bicycles are still tending to a diva as high-maintenance as any in Hollywood \u2014 a Boeing 747 that fits into one corner with room to spare. Like many a Tinseltown star, the jumbo jet is ageing: its fresh coat of paint covers an airframe that first carried passengers in 1977. And it has clearly undergone plastic surgery. There is the telltale swelling of the fuselage just behind the wings, for example, where incisions outline a retractable door. There are also a multitude of less visible fixes on the inside, all focused on what lies behind the door: a 2.5-metre telescope that has turned this formerly plain jet into the much heralded Stratospheric Observatory for Infrared Astronomy (SOFIA). SOFIA's interior is a beehive of activity on this evening of 17 May. Some scientists are tinkering in the combined cabin and control deck. Others are pouring liquid nitrogen into one of the telescope's instruments. Still others are frantically trying to fix a broken cooling system. Their sense of urgency is palpable. Only when everything is working can SOFIA roll out of the hangar onto the tarmac, where it will open its telescope door, point its mirror at Polaris, and begin taking 'first-light' data from the ground. And only when this final 'ground ops' test is completed can SOFIA embark on its first science flight, scheduled for a week afterwards. SOFIA's long-suffering science team has a lot to prove. Inaugurated in 1996 as a joint project between NASA, which modified the 747, and the German Space Agency (DLR), which built the telescope and pays 20% of the costs, SOFIA is designed to give astronomers a clear view of the Universe at infrared wavelengths \u2014 a part of the spectrum rich with information about galaxies, planets and newborn stars. But to accomplish that mission, the plane will have to lift the 20-tonne telescope at least 12 kilometres into the air, and then fly through the stratosphere at some 1,000 kilometres an hour with the open door forming a 3-metre-wide hole in its fuselage. The technical challenges of engineering such a radically modified plane, combined with management failures, have already put SOFIA almost a decade behind schedule \u2014 the original completion date was supposed to be 2001 \u2014 and roughly tripled its development costs. Its estimated total cost, including 20 years of operation, now comes to about US$3.75 billion \u2014 a price tag that by one measure, dollars per hour of observation, would make SOFIA as costly as the Hubble Space Telescope, NASA's most expensive astronomy mission ever (see ). The delays have also meant that competing infrared astronomy missions such as the European Space Agency's Herschel Space Observatory\u2014 which was supposed to launch well after SOFIA \u2014 have instead gone up first and scooped some of the creamiest science. The result is that any mention of SOFIA now leads many astronomers to respond with eye-rolling and shoulder shrugs. \"I'm worried about this,\" says Garth Illingworth, an astronomer at the University of California, Santa Cruz, and the former chair of an astronomy advisory committee to the National Science Foundation. \"The science missions we do should have very high science return and be cost effective. In my view, SOFIA meets neither of those criteria.\" Hence the urgency in Palmdale, where the first science flight is seen as a chance to rebut SOFIA's many critics. \"It's a lot of pressure for us,\" says Pasquale Temi, a SOFIA facility scientist. \"We would like to have some proof \u2014 proof that we have something flying and taking data.\" The plan is to follow the initial science flight with limited science operations starting in November. From there, the project will gear up step by step to full operations, reaching 800 hours of night-time observations a year with all eight of the telescope's instruments in 2014. But tonight, no one dares summon the diva from its dressing room. The cooling system is finally fixed, but towering clouds are tumbling over the mountains that stand between Palmdale and Los Angeles. Opening the telescope door on the tarmac is out of the question: raindrops could fall on the mirror. \"There's nothing we can do,\" says Temi, shaking his head mournfully.  \n                Chasing eclipses \n              For all the contention surrounding this latest (and probably last) of NASA's flying observatories, airborne astronomy has a rich history. Astronomers used biplanes to chase solar eclipses as early as the 1920s. And in 1968, when NASA started flying a Learjet equipped with a 30-centimetre telescope stuck through a passenger window, they began to stare beyond the Sun at stars, galaxies and planets. The next year, the agency approved work on what would become the Kuiper Airborne Observatory (KAO), which featured a 0.9-metre telescope staring through a hole in the roof of a converted Lockheed C-141 Starlifter, once a military transport aircraft. By 1974, the KAO was taking data at altitudes of 12 kilometres and above \u2014 and in the process, demonstrating the advantages of astronomy from the stratosphere. Because atmospheric water vapour at lower altitudes absorbs most of the infrared light at wavelengths longer than a micrometre or so, an airborne telescope can gather infrared photons far faster than any ground-based counterpart, and thus increase its sensitivity to faint objects. The KAO's mobility proved to be helpful, too. Just as the early airborne astronomers chased solar eclipses, the KAO could seek the best paths across the globe for observing occultations: eclipses in which comets, asteroids or planets pass in front of distant stars that backlight the object and create a faint shadow across Earth. Occultations are a chance to measure the size, shape and even the atmospheric chemistry of the passing object. It was in this way that the KAO made some of its most famous observations: the rings around Uranus 1  and an atmosphere on Pluto 2 . SOFIA scientists are hoping that the new mobile observatory will extend such studies to the thousands of poorly understood icy bodies in the Kuiper belt beyond Neptune's orbit. Even more helpful was that astronomers were able to refurbish and upgrade the KAO's instruments 50 times over the years, says Allan Meyer, a SOFIA scientist who flew on hundreds of KAO flights. This allowed the KAO to stay  au courant   as the efficiency of infrared detectors improved. And that, in turn, gave it a distinct advantage over space telescopes, despite the orbital instruments' much clearer vision at every wavelength: their instruments become outdated very quickly. (The one exception is Hubble, the only space telescope designed to be upgraded.) By the early 1990s, however, airborne astronomers were looking past the ageing KAO towards SOFIA. The bigger plane wouldn't fly higher than the C-141. But with a telescope almost three times the diameter of the KAO's, it would provide astronomers with about eight times the light-gathering power, thus allowing them to study much fainter objects. Dan Lester, an astronomer at the University of Texas at Austin who helped to develop conceptual designs for SOFIA, recalls laying out this rationale when he met with members of the US Congress in the early 1990s. They loved it, he says. They wouldn't have to go to some frigid, forbidding mountain top to see astronomy in action. SOFIA could land in any congressional district with a big enough airport, and carry aloft politicians \u2014 along with teachers, journalists and other interested parties \u2014 in the first-class seats. \"The taxpayers could see their investment in the flesh,\" says Lester. \"People on the Hill liked this.\" NASA accordingly got the go-ahead for SOFIA, but at a price: the KAO had to be retired in 1995 so that its operations money could be shifted over to SOFIA construction. In 1996, NASA awarded a management contract to the Universities Space Research Association (USRA) of Columbia, Maryland. It was a grand experiment in privatization, and supposedly more efficient than having NASA manage the project directly. Lester recalls how disappointed the University of Texas was to lose its bid for the contract. But in retrospect, he says, \"we took a deep breath and said, 'We were sure lucky we didn't get that'\".  \n                Dynamic problems \n              It turned out that cutting a 3-metre square out of the side of a 747 and making it fly smoothly with a 20-tonne telescope in its rear was a far tougher problem than modifying the KAO. Engineers spent years finding ways to stiffen the fuselage and direct airflow around the hole, lest the slipstream create the 1,000-kilometres-per-hour equivalent of blowing across the lip of a bottle; the resulting resonance in the cavity would have shaken the telescope uncontrollably. Meanwhile, two subcontractors, responsible for building the retractable door, went bankrupt. By 2003, development costs had risen to $373 million, and the projected completion date was receding farther and farther into the future; the USRA knew astronomy, but not aeronautics. By 2006, fed-up agency officials were plotting to kill SOFIA. \"It was me,\" says Mary Cleave, tapping her nose. \"I didn't think it was ever going to fly.\" In 2005, Cleave, now retired, found herself in the unenviable position of being NASA's science chief at a time when the science budget was being cut. Her boss, former administrator Michael Griffin, was championing the new NASA policy of returning astronauts to the Moon, but Congress wasn't giving him any extra money to get them there. Cleave was told to siphon off a few billion dollars from her budget to the human spaceflight programme. And an obvious source was the chronically over-budget SOFIA. \"USRA had no background in managing a project like that,\" says Cleave. \"None. That's why it fundamentally got into trouble.\" But when SOFIA was cancelled in the presidential budget request of February 2006, the uproar came quickly \u2014 much of it from Germany. \"It would have been a disaster,\" says Sigmar Wittig, who was DLR director at the time. Wittig immediately went to see Griffin, and explained the political embarrassment that a cancellation would cause for the DLR, which had already sunk millions into building the telescope. Congress, too, protested the loss of contracts in so many members' districts, and hauled Griffin and Cleave into hearings where they were grilled about SOFIA's future.  \n                Under review \n              Cleave was beginning to realize that it was nearly impossible to cancel an international partnership. But she could still reshape it. She set up a review team, which later in 2006 recommended transferring management of SOFIA aircraft operations from the KAO's former base, the Ames Research Center in Mountain View, California, to the Dryden Flight Research Center near Palmdale, long the home of NASA's aeronautics experts. The only thing left to be managed by the USRA and Ames was SOFIA's science programme. Dryden, with its extensive flight-test experience, did succeed in getting SOFIA up in the air \u2014 albeit with extreme caution, opening the retractable door bit by bit and incrementally flying at higher and higher altitudes. And that step-by-step approach paid off: today, test pilot Timothy Williams says he doesn't even notice when SOFIA's door opens. \"It's amazingly benign,\" he says. \"It's almost imperceptible.\" But adding Dryden to the mix increased development costs, says Xander Tielens of Leiden University in the Netherlands, a former SOFIA project scientist who is also principal investigator for an instrument on Herschel. \"There's enough guilt to go around for everybody,\" he says. Dryden's incremental approach also meant that other infrared science experiments have beaten SOFIA out of the gate. SOFIA does cover a huge swathe of infrared wavelengths, from less than a micrometre up to a millimetre (see  'Common ground' ). Yet the Herschel satellite, launched in 2009, uses its 3.5-metre-wide telescope and deep space location to cover a large part of SOFIA's spectrum with far greater sensitivity. \"Herschel has indeed scooped the area they were optimized for,\" says John Mather, a Nobel laureate in physics at Goddard Space Flight Center in Greenbelt, Maryland. That includes studying the Universe's earliest galaxies \u2014 shrouded in glowing dust that is dark to optical telescopes \u2014 in an attempt to understand the timing and mechanics of the starbursts that contributed to their formation. Mather is also the project scientist for the James Webb Space Telescope (JWST), which, after its launch in 2014 will use its finely honed, 6.5-metre mirror to encroach on SOFIA's turf from the shorter wavelength side of the infrared spectrum. Given that competition, and its comparatively limited sensitivity to targets beyond the Milky Way, SOFIA will probably stick to studying objects slightly closer to home, such as the disks of dust and gas around young stars that eventually coalesce into planets. But within that domain, says Mather, it will have some undeniable advantages. SOFIA has a superior ability to perform spectroscopy, which doesn't require objects to be as bright. So a natural role for SOFIA would be measuring the chemistry and velocity of these gases and dusts, which would help astronomers understand how planetary building blocks take shape. Tielens adds that there is a vast need for spectroscopic follow-up to the many objects that Herschel won't have time to completely understand before its 3-year supply of liquid helium is exhausted.  \n                A niche of one's own \n              But SOFIA's uses go beyond follow-up, emphasizes Eric 'the Infrared' Becklin, an astronomer who served as a principal investigator on the KAO, and then as 'director designate' of SOFIA from 1996 until he stepped down last year to make way for someone younger. Now an emeritus professor at the University of California, Los Angeles, Becklin says that SOFIA will have the 30\u201360-micrometres wavelength region all to itself \u2014 at least until the 2018 launch of a proposed Japanese mission called SPICA (Space Infrared Telescope for Cosmology and Astrophysics). This wavelength band is where Solar System planets emit some of their brightest infrared light, which in turn should carry crucial information about their still unresolved atmospheric chemistries. Even as SOFIA astronomers look for their niche, others wonder if it will ever be worth the cost. \"It's shockingly expensive,\" says Barth Netterfield, an astrophysicist at the University of Toronto in Canada. As a scientist on the Balloon-borne Large-Aperture Sub-millimeter Telescope (BLAST), Netterfield in 2006 made pioneering observations of cosmic infrared background radiation from a 2-metre telescope that was carried to the edge of space by a balloon, and then circled the South Pole for 11 days 3 . Total project cost, including another planned flight this winter: $7 million. SOFIA's far heftier price tag has consequences for the rest of the astrophysics programme at NASA. Cleave says that by cancelling SOFIA, she was trying to save competitive mission lines such as the 'explorer' programme, which awards small satellites to principal investigator-led teams, often from universities. For many years, the explorer programme funded the launch of one relatively inexpensive mission nearly every year. Some of these missions, such as Swift, a \u03b3-ray telescope, and the Wilkinson Microwave Anisotropy Probe, which mapped the cosmic microwave background, have had a major scientific impact 4 . But now, 57% of the NASA astrophysics budget goes to just three missions: Hubble, the JWST and SOFIA. The launch rate of small explorers has dropped considerably. SOFIA, says Cleave, will be a weight on the future astrophysics programme for a long time. Becklin acknowledges SOFIA's high costs. But he says it shouldn't be judged in terms of dollars per hour of observation, but by the science per dollar. \"It's the unique science that we produce,\" he says. Unfortunately, it is hard to say what the 'science per dollar' is for SOFIA until it actually does some science. Without that, NASA and the astronomical community have both had to proceed by guesswork. In 1990, in the regular prioritization exercise dubbed the 'decadal survey', American astronomers ranked SOFIA as their third most important medium-sized project for the coming ten years. But that was when SOFIA was supposed to be flying by 1998 for $230 million. The airborne observatory wasn't ranked at all in the 2000 decadal survey, because it was already under development, nor will it be ranked in the 2010 survey, due out later this year. Meanwhile, because SOFIA won't be fully operational until 2014, it has also been exempt from the 'senior reviews' that NASA carries out on it operational missions every two years \u2014 reviews that evaluate and rank the missions in the science-per-dollar terms that Becklin wants. So in effect, the broader research community hasn't had a chance to critically reassess the scientific case for SOFIA for 20 years \u2014 a situation that Netterfield, like many other critics, finds inexcusable. \"Put me on a review panel,\" he says.  \n                First light \n              On 25 May, a week after the failed dress rehearsal, a noisy tug pushes SOFIA out of its hangar in Palmdale as the Sun goes down. Little birds in the roof scatter as the big bird backs out, its tail slipping through the door with just a metre of clearance. Crewmen, airhorns in hand, watch the wing tips as the jumbo jet rolls out, pivots and heads to the runway. Williams, the test pilot, has been given control of the throttle for this first-light flight. He sends the plane hurtling down the runway, then eases it up into the moonlit night above a bank of clouds that have piled up sluggishly against the California coast. He points SOFIA southwest, guiding it past the Los Angeles airport, past Santa Catalina Island off the coast, and into Whisky two-niner-one: Warning Area 291, a huge swathe of the Pacific Ocean sometimes used for military training exercises. Williams and the other pilots then take turns guiding SOFIA along straight bearings while the scientists in the cabin gather their first science images from a few bright test targets: Jupiter and the galaxy Messier 82. Back at the hangar in Palmdale, Eric Becklin follows SOFIA for a little while on a public tracking website before heading to bed at midnight. Still involved in the project as an adviser, Becklin no longer has a special line to communicate with the plane. That's for the new Erick the Infrared: Erick Young, who took over as director designate last year. Yet both scientists are back by the runway before dawn to watch as Williams gently guides SOFIA back to a 5:35 a.m. touchdown. Soon enough, the aircraft is parked again inside its hangar. \"Spectacular!\" enthuses Becklin as he recalls the landing. \"Then the most important thing: we got those beautiful images that were way beyond our expectations.\" Young is equally ecstatic. \"It's the first time that we can really say we have an observatory,\" he says. Even Cleave says she's happy to hear that SOFIA is flying (although she is a bit incredulous). And that's not an uncommon feeling among critics, says Lester, whose involvement in the SOFIA project has waxed and waned over the years. He says he has had to detach himself from the mission, in the same way that parents have to avoid projecting too many of their dreams and desires onto their children. \"They grow up \u2014 and they don't end up achieving your dreams. But you love them anyway. That's the way I feel about SOFIA.\" Lester pauses. \"And that's the way a lot of the community feels about SOFIA.\" Eric Hand is a reporter for  Nature   in Washington DC. \n                     SOFIA \n                   Reprints and Permissions"},
{"file_id": "466548a", "url": "https://www.nature.com/articles/466548a", "year": 2010, "authors": [{"name": "Natasha Gilbert"}], "parsed_as_year": "2006_or_before", "body": "Feeding the world is going to require the scientific and financial muscle of agricultural biotechnology companies. Natasha Gilbert asks whether they're up to the task. Do Not Water, says the small notice by the pots of withered, brown maize seedlings, the genetically unlucky ones in an experiment testing maize's tolerance to drought. Five minutes after stepping into the huge greenhouse in which these plants are attempting to grow at the research headquarters of  Monsanto  in St Louis, Missouri, I am beginning to feel genetically disadvantaged too. Sweat is beading on my skin. Like the desiccated plants, I am clearly not cut out for the fierce summer temperatures that the greenhouse's climate is set to imitate. Just next to them though, a row of green, sprightly seedlings is faring better thanks to a gene that researchers inserted from the bacterium  Bacillus subtilis . Just as lively is Dianah Majee, the plant biologist showing me around. Her face hasn't even worked up a shine. These green plants and the scientists that produced them are unusual in ways not visible to the eye. They are Monsanto's entry in a race to make the first transgenic, drought-tolerant maize (corn) that is commercially available to farmers. The race is tight. But after more than 20 years of research and development (R&D), Monsanto says it is now two years away from launching the seeds onto the market. And within the next few years, the company and its major competitors hope to bring to market other transgenic crops, resistant to stresses such as soils starved of nitrogen, phosphorus and other essential nutrients. In pursuing these crops, Monsanto and the other giants of agricultural biotechnology are making a significant departure from what until now has been a mainstay of their business: developing and selling pesticide- or herbicide-resistance crops, such as Monsanto's  Bt   maize. When these plants were first introduced in the 1990s they produced dramatic increases in yield for farmers \u2014 and a windfall in profits for the companies supplying the seed. But the yields have peaked, and so have the profits. Now the next big commercial gains lie in crops that can withstand water- and nutrient-deficient soils. US farmers lose on average 10\u201315% of their annual yield because of drought and water stress. Crops that can beat these stresses are also a vital part of the solution to the global food crisis. If the 9 billion people expected to inhabit the world by 2050 are to be fed, then farms in low-income countries must grow more food, sustainably, on water- and nutrient-poor soils (see  page 546 ). Researchers and policy-makers realize that they can't meet the food-security challenge without the private sector, which makes up a significant share of the global agricultural research effort (see  'Public vs private' ). Monsanto's annual research budget alone is US$1.2 billion, just topping the US federal government's total spend on agricultural science of $1.1 billion in 2007 (the most recent figures available). In contrast, the  Consultative Group on International Agricultural Research  (CGIAR), the world-leading group of centres carrying out agricultural R&D for developing countries, has an annual budget of $500 million.  \n                Getting together \n              So in their demand for hardier crops, the commercial aims of the biotechnology companies and the requirements of the developing world have aligned \u2014 and companies such as Monsanto hope to fulfil them. In June 2008, Monsanto pledged to double yields in its core crops of maize, soya bean and cotton by 2030 over 2000 levels. In September of the same year, Monsanto's chairman promised to \"improve the lives of an additional 5 million resource-poor farmers\", in large part by making some of its seed technology available to increase their productivity. Other companies have made similar pledges. All this leads to another reason why the green, transgenic seedlings in the stifling Missouri greenhouse stand out. In 2008, Monsanto partnered with the  African Agricultural Technology Foundation , a non-profit research organization in Nairobi, Kenya, to apply the techniques and discoveries it has made with its commercial drought-tolerant maize to developing drought-tolerant varieties for subsistence farmers in sub-Saharan Africa, to be available as quickly as possible after commercialization in the United States. The partnership, which is also funded with $47 million in grants from the  Bill & Melinda Gates Foundation  in Seattle, Washington, and the Howard G. Buffett Foundation in Decatur, Illinois, is one of a handful of exceptionally large projects established in recent years in which public and private sectors have joined forces to tackle food scarcity in developing countries. The companies say that these investments are just good business sense because they will create future customers as developing-world farmers gradually move from subsistence to profits, making money to spend on seed. The companies also see an opportunity to buff their corporate images with a humanitarian cloth.  \n                Slow progress \n              It will take more than buffing to overcome critics' deep scepticism about commercial biotechnology. Genetically modified (GM) crops, they say, have so far done little for the developing world. Earlier humanitarian initiatives have yet to reach fruition.  Golden rice , for example \u2014 transgenic rice designed to combat vitamin A malnutrition \u2014 has been in development since 1990 (see  page 561 ). Critics ask what has taken so long; they worry that industry's grasp on intellectual property is holding up research progress; they question why these supposedly transformative transgenic technologies have yet to put food in the hungriest bellies. \"I don't think the private sector is doing enough,\" says Achim Dobermann, deputy director general for research at CGIAR's  International Rice Research Institute  (IRRI) in Manila, the Philippines. Roger Beachy, director of the US Department of Agriculture's  National Institute of Food and Agriculture  in Washington DC, wonders how far the agricultural biotech companies are willing to go. \"Have they made as much progress in developing countries as they should have?\" he asks. \"What do they see as their responsibility in the developing world?\" To many scientists, the answers to these questions are hidden behind a corporate facade. Which is why I'm here, slowly wilting in Monsanto's greenhouse, and why I travelled to two other giants in the sector \u2014  Pioneer Hi-Bred  in neighbouring Iowa, and the UK research headquarters of Swiss company  Syngenta  \u2014 to tour their labs, greenhouses and test fields, where the next generation of crops are sprouting. I wanted to see them and talk to senior researchers and executives about the future of their science, their business \u2014 and, inextricably, the future of the planet's food. I sit in the small waiting room of Monsanto's main building, A, with its single bench and friendly security guard. Buildings B through to Z are scattered around the manicured gardens and endless car parks that make up the rest of its headquarters. Monsanto employs around 5,000 scientists and technical assistants worldwide and splits its R&D budget equally between biotechnology and traditional plant breeding. (Monsanto, like the other companies I visited, does not break down how much of the budget is spent on its humanitarian projects.) For its GM crop work, Monsanto's scientists screen hundreds or even thousands of genes from plants, bacteria and other organisms for ones that might endow plants with a desired trait. The drought-tolerant  B. subtilis   gene,  cspB , that they found helps bacteria deal with environmental stress such as cold temperature. When inserted into maize plants it helps them cope with drought by disentangling RNA, which folds up abnormally when the plant is water-starved. The theory is that the energy the plant would have spent fixing drought-entangled RNA can now be sunk into grain. Away from the sweltering greenhouses, posters provide a regular reminder of Monsanto's 'pledge' to the world in six different languages. The company promises dialogue, transparency, respect, sharing and benefits. And Bob Reiter, vice-president for breeding technologies at Monsanto, is up front about the company's business-minded approach to its humanitarian work. Crops that will make the company money in the short term, in richer countries, could also eventually make money in lower income ones. \"The initial approach is to help the subsistence farmer get on his feet,\" he says. \"There has to be a humanitarian element to it. But you have to think about what a viable agricultural industry in Africa looks like, and the idea that these farmers get free handouts forever is not sustainable.\"  \n                Long-term plan \n              It is with these sentiments that Monsanto entered into its public\u2013private partnership with the African Agricultural Technology Foundation. It is not giving away the green strain that I saw thriving in the greenhouses. It is giving away the resources it used to make it \u2014 such as the sequence of the  cspB   gene, plus information about other drought-tolerant genes and traits that the researchers are introducing into maize through traditional breeding. Crops developed through the partnership will be made available royalty free to subsistence farmers. If a country moves from subsistence farming to commercial farming then, in theory, the company could start charging for the seed. But first Monsanto has to get its 'first generation' drought-tolerant maize into fields in the developed world. The company has finished testing the seed; now it has to secure regulatory approval from US federal agencies and scale up seed manufacture. Researchers at Monsanto are already working on 'second generation' crops \u2014 the details of which the company is keeping close to its chest \u2014 that can grow in a wider range of environments across the United States. Behind the rows of silver doors to the company's 108 growth chambers, an even hardier strain of maize is surely growing.  \n                Mechanized engineering \n              One state north of Missouri, on the outskirts of the small midwestern town of Johnston, Iowa, the last few rows of houses suddenly drop away and a sea of young green maize rolls up to the horizon. In patches the maize has turned yellow and its growth is stunted. Recent intense rainstorms have flooded parts of the fields, washing nutrients from the soil that are vital to the crop's healthy growth, including nitrogen fertilizers. Pioneer Hi-Bred, part of the chemical giant DuPont, saw an opportunity here to increase its customers' yields. When global nitrogen fertilizer prices peaked in 2008 at more than $450 a tonne, nearly double the previous year's cost, the company ramped up a research project that it had begun in 2005 to develop maize hybrids that produce the same yield on less fertilizer. Pioneer isn't quite the biotech behemoth that Monsanto is: in 2009 DuPont spent $734 million on its agriculture and nutrition R&D, which includes Pioneer Hi-Bred's work on seeds and crop protection. The company has now mechanized much of the process of linking the genes inserted into plants to desired traits. A robot hauls maize plants off conveyor belts; another takes digital images to rapidly assess how novel genes have changed the plants' growth. In Pioneer's case, researchers hit on one possible gene in the red alga  Porphyra perforata , which can grow in environments with nitrogen levels 100 times lower than maize. The gene codes for the enzyme nitrate reductase, which converts nitrate into nitrite. \"We don't really know how it works,\" says Dale Loussaert, a senior scientist working on the project, of the algal gene. Even so, he says, \"the plant models in the lab look promising. The yields look good.\" The company does not expect to have a product on the market for another 10\u201312 years though. Pioneer has agreed to donate the transgenic technologies, molecular markers and other resources associated with its nitrogen-use project to a public\u2013private partnership. The  Improved Maize for African Soils  (IMAS) project was launched in February 2010. It is led by the  International Maize and Wheat Improvement Centre  (CIMMYT) in Mexico, part of the CGIAR, and it received $19.5 million from the Bill & Melinda Gates Foundation and the United States Agency for International Development. The maize varieties that will be developed through IMAS will be made available royalty free to seed companies that sell to small-scale farmers in sub-Saharan Africa. Pioneer is also involved in a project to increase the nutritional content of sorghum, a crop that is a staple food for hundreds of millions of people throughout Africa and Asia. Sorghum has high levels of phytate \u2014 the form in which phosphorous is stored in plants \u2014 which binds strongly to essential amino acids, vitamin A, iron and zinc, so these nutrients are not available in a digestible form. Consequently, people who depend on sorghum as their main food source are often malnourished. Since it joined in 2005, the company has donated technologies worth $4.8 million to the scheme, led by  African Harvest , a non-profit foundation based in Nairobi, Kenya. Florence Wambugu, founder and chief executive of African Harvest, used to sit on a science advisory panel for DuPont, and so knew that the company was developing technologies that would be useful to African Harvest's sorghum project. She approached the company for help. \"It is not just the technology donation; this won't amount to a product. We had to get outside expertise to help manage the money and people, and ensure we are meeting milestones,\" she says. Marc Albertsen, a senior research fellow at Pioneer Hi-Bred and co-principal investigator on the sorghum project, says that tests in June showed that transgenic sorghum varieties developed by Pioneer produced 80% less phytate but 20% more iron and 30% more zinc than conventional varieties. Such results are not going to assuage the critics. Gregory Graff, an agricultural economist at Colorado State University in Fort Collins, says that the majority of companies' R&D spending and effort still goes towards blockbuster crops with traits, such as pest control, that benefit agribusiness, leaving neglected many crops that are important in the developing world. \"They bring out one or two examples of public good research, such as drought-resistant varieties and golden rice, but research on these has been going on for a very long time and none are actually ready yet,\" he says. Graff says that the lack of progress is in large part a consequence of the hold that the private sector has on intellectual-property rights to crucial technology, such as genetic markers, and the sequences of key genes and 'promoters' that drive gene expression. Dobermann, of the IRRI, agrees that access to intellectual property is a problem. His institution would like to experiment with traits to improve the drought tolerance of plants and their efficiency in using nitrogen, but there are \"so many restrictions\" on the use of patented technology that researchers at his institute concluded \"it was not worth getting into,\" he says. \"We either have to reinvent the technology ourselves or use a second-class solution,\" he says. John Bedbrook, vice-president of agricultural biotechnology at DuPont, agrees that \"tensions\" over access to intellectual property exist, but says the company has to remain \"dispassionate\". Without intellectual property, he says, companies would have little incentive to invest in the research to begin with. But, he adds, companies could be \"more open source with enabling technologies\" such as promoters. Reiter says that restrictions on access to intellectual property are often misconceived. When public researchers ask the company for access to patented technology, he says, it often turns out that the subject of their research was not actually covered by a patent. All this leaves a question: what has really been holding up these projects?  \n                The real delays \n             This was the issue that I discussed at Syngenta, whose modern UK research headquarters sit in 260 hectares of verdant English farmland near Bracknell. Syngenta has a history in public\u2013private partnerships through the golden rice project, which AstraZeneca (the agribusiness part of which became Syngenta) joined in 2001. Syngenta worked to increase the amount of a precursor of vitamin A in the rice and make seeds available royalty free to subsistence farmers in sub-Saharan Africa, but the company retains commercial rights elsewhere. (The IRRI, part of the Golden Rice Humanitarian Board, which now directs the project, expects to introduce seeds to farmers by 2012.) But some critics view golden rice as an agonizing failure because it has taken so long, and have been highly distrustful of the company's involvement, assuming that the project was mired because of the numerous patents involved. Not true, says Ingo Potrykus, chairman of the Golden Rice Humanitarian Board and, as an academic researcher, one of the inventors of golden rice. He says that the team initially thought that they had to obtain free licences for 70 patents protecting technologies used in the rice development. But when Syngenta joined the project, its lawyers found that only a handful of these patents applied to the countries where golden rice was targeted. So in fact, he says, intellectual property has not been a major problem. \"Without the cooperation of the private sector we would probably never have been able to solve the intellectual-property mess and the project would have ended at this stage,\" says Potrykus. Mike Bushell, Syngenta's chief scientist, says complex technology and regulations are the real hold-ups for transgenic crops. \"R&D takes around 10 years and then you have to go through the regulatory stage,\" he says. Bushell says critics overlook how long it takes to develop crop varieties with complex traits such as drought tolerance, which involve many genes and are greatly influenced by environmental conditions. And passing regulatory hurdles involves reams of tests showing, for example, that a gene is stably and safely expressed. As we stroll past Syngenta's 'monsoon machine', which recreates harsh weather conditions, the discussion turns to the volatile topic of GM crops and their regulation. In 2004\u201305, the company moved the bulk of its GM research out of Europe and to the United States, in part because of Europe's difficult climate for GM research and the nonexistent market. But this year has seen some signs that the continent's strict stance on GM crops is softening (see  D. Butler  Nature   doi:10.1038/news.2010.112; 2010 ). That could be good news for the developing world, Bushell says. Although he acknowledges that transgenic crops are not the only solution to increased food production, particularly in the developing world, he argues that they are an important component in a tool box that also includes improved agronomic practices and traditional breeding methods. Nicholas Kalaitzandonakes, an agricultural economist at the University of Missouri-Columbia who tracks the agricultural biotech industry, says that the industry is making a substantial investment in these public\u2013private partnerships. \"I have the impression that people in the industry know they can't make money on [these products] in developing countries but they honestly want to make it available. But they also want to watch their backs.\" If something goes wrong \u2014 for example the research fails, the partnership breaks down, or a transgene contaminates local commercial supplies \u2014 a company could face heavy financial liability and public relations fall-out, Kalaitzandonakes says. \"It's not a simple thing to manage risk and potential risk.\" This cautiousness is partly why only a handful of these partnerships exist. Yet Kalaitzandonakes is optimistic that once one product comes on the market \u2014 be it golden rice, a drought-tolerant maize or a biofortified sorghum \u2014 then businesses, governments and the public will become more confident in backing the next. The optimism is tangible at Syngenta too. Earlier this year the company started a project with the CIMMYT to research and develop more productive wheat varieties for farmers in the developing world. Bushell says that the company has learnt a lot from its involvement in helping to develop golden rice. Outside, fields of winter wheat are bordered by an unruly metre-wide strip of wild grasses and flowers designed to attract bees and other pollinating insects. This farming practice, which Syngenta is hoping to encourage across Europe, is also part of the company's efforts to make agriculture sustainable. The world's future food depends not just on crops, however cleverly they are engineered \u2014 the ecosystems to support them must have a future too. See Editorial,  \n                     page 531 \n                   , and food special at  \n                     http://www.nature.com/food \n                   . \n                     Nature Biotechnology \n                   \n                     Food special \n                   \n                     Monsanto \n                   \n                     Syngenta \n                   \n                     Pioneer Hi-Bred \n                   \n                     Golden Rice \n                   \n                     Consultative Group on International Agricultural Research \n                   \n                     Water Efficient Maize for Africa \n                   \n                     International Rice Research Institute \n                   \n                     The Bill & Melinda Gates Foundation \n                   \n                     African Agricultural Technology Foundation \n                   \n                     Improved Maize For African Soils project \n                   \n                     African Harvest \n                   Reprints and Permissions"},
{"file_id": "466554a", "url": "https://www.nature.com/articles/466554a", "year": 2010, "authors": [{"name": "Jeff Tollefson"}], "parsed_as_year": "2006_or_before", "body": "With its plentiful sun, water and land, Brazil is quickly surpassing other countries in food production and exports. But can it continue to make agricultural gains without destroying the Amazon? Jeff Tollefson reports from Brazil. Mateus Batistella used to be a vegetarian, but Brazilian cuisine has worn him down. At lunchtime, virtually all the restaurants offer a classic dish of thin-cut beef with salad, rice and beans, served with a cooked-flour dish called farofa. In cities and towns, traditional butchers and supermarkets alike sell every cut of beef imaginable. \"It's everywhere, and it's cheap,\" says Batistella, who heads a satellite-monitoring research centre in the southern city of Campinas for  Embrapa , the research arm of Brazil's  agriculture ministry . \"Today I eat beef all the time.\" That isn't the most politically correct course of action in a country in which cattle ranching is often linked with destruction of the Amazon rainforest. Batistella even has a satellite image on his office wall, showing the world's largest tropical forest under siege from the south by agriculture. Nonetheless, the world, like Batistella, is consuming more and more beef each year. All that meat has to come from somewhere, and increasingly it is coming from Brazil. This rising agricultural powerhouse has quadrupled beef exports over the past decade, and in 2003 it vaulted past Australia as the world's largest exporter. Capitalizing on its vast natural resources and a booming economy, Brazil is competing with the United States for the title of world's largest soya exporter. The United Nations  Food and Agriculture Organization  forecasts that Brazil's agricultural output will grow faster than that of any other country in the world in the coming decade, increasing by 40% by 2019. There was a time when such figures would have spelt doom for the Amazon. In the past, when demand for commodities such as beef, maize (corn) and soya went up, trees came down. But the opposite has happened in recent years. Despite rising production and persistently high commodity prices since the height of the global food crisis in 2007\u201308, Amazon deforestation plunged to a historic low last year, nearly 75% below its 2004 peak, and some expect more good news this year. This trend fuels hopes that Brazil is establishing a sustainable agricultural system that will help to feed a growing world in the decades to come \u2014 and lower the environmental cost of beef habits like that of Batistella. \"We broke the paradigm in the past five years,\" he says. \"There is no longer a direct correlation between food and deforestation.\" Brazil has managed that feat through policy, improvements in agricultural science, better enforcement of environmental laws and pressure from consumers. But the country still faces numerous challenges as it seeks to boost food production. Conflicts over land-use policies are common, and climate change will take a bite out of many important crops unless plant breeders can keep up.  \n                Fields of soya \n              Brazil's rise as an agricultural giant began with soya beans, the country's largest food crop, which had a value of nearly US$17 billion in 2008. In the 1960s, soya's range was largely limited to the south of Brazil, but since then breeders have developed varieties that can grow across most of the country. Agricultural scientists tamed the highly acidic soils of the Brazilian savannahs with applications of lime and other nutrients, and reduced fertilizer costs by developing methods to inoculate seeds with rhizobia, bacteria that colonize the roots of plants such as soya and fix nitrogen. Brazilian farmers are now competing with the United States to set the record for soya-bean yields (see  graphic ). \n               Click here for larger image \n               And after a long delay, Brazil is also making up ground on transgenic crops. A decade ago, the fate of genetically modified (GM) crops in the country was uncertain. A federal commission had approved the first GM soya plant for cultivation in 1998, but a judge later issued a moratorium on planting the herbicide-resistant beans, developed and sold by the US-based company  Monsanto , calling the seeds a \"foreign monster\". Rather than abide by the legislation, however, Brazilian farmers turned to Argentina for illegal imports of the Monsanto seed, which earned a nickname in honour of Argentina's most famous football player, Diego Maradona. The illicit 'Maradona' soya bean became so widespread that Brazilian president Luiz In\u00e1cio Lula da Silva signed a law in 2003 legalizing it in an effort to bring order to imports, institute basic quality controls and protect Brazilian seed companies that were unable to compete with illegal vendors. Two years later the Brazilian Congress enacted a biosafety law overhauling the process for approving transgenic crops, and by 2006 the National Technical Commission on Biosecurity was busy approving transgenic plants, beginning with soya beans, cotton and maize. Brazil now has more than 21 types of GM plant approved for use in the field and is second only to the United States in the number of hectares planted with transgenic crops. GM soya will make up 70% of the Brazilian soya market this year and could hit 75% in 2011, according to Alda Lerayer, executive director of the  Council of Information on Biotechnology , a non-profit organization based in S\u00e3o Paulo. \"I lived four years of hell there, but I believe we did things that will be recognized as very important for Brazilian agriculture in the years to come,\" says Walter Colli, a biochemist who stepped down in February as president of the commission. He pushed through the approval of GM crops by ignoring ideological debates during commission meetings and focusing on technical questions about public and environmental safety, a strategy quietly endorsed by da Silva's government. Legally, food containing transgenic plants must be labelled with a T, but Lerayer says that although environmental groups have raised concerns, public opposition to the spread of GM crops has so far been muted. Brazil currently relies on GM products developed abroad, but earlier this year the biosafety commission approved the first transgenic seed to be developed by Brazilian scientists. Researchers at Embrapa had enhanced soya with a gene supplied by the German chemical giant BASF that provides resistance to a new class of herbicides. For El\u00edbio Rech, who headed the project at Embrapa's centre on genetic resources and biotechnology, the work showcases Brazil's budding capacities in biotechnology while serving as a model for how governmental researchers from Embrapa can partner with the private sector. \"The planet will have to work together in order to assure that we will be able to double the food production by 2050, and Brazil will play an important role,\" he says. For now, transgenic crops in Brazil and elsewhere help farmers battle against weeds and insects, but they do not directly increase the amount of food produced by individual plants. However, Embrapa is working on new techniques that may one day open the door to plant varieties that are more nutritional and more productive. Some Brazilian crops have a long way to go; maize varieties there produce less than half the yield of those in the United States.  \n                The land of plenty \n              More productive varieties may eventually take pressure off the rainforest, which has been extensively cleared to make way for agriculture. But Brazil has already slowed deforestation by trying to make better use of land that has already been cleared. Spurred by pressure from consumers and environmental groups such as Greenpeace, soya-bean producers were the first to commit to protecting the Amazon. Four years ago, the major exporters agreed to a moratorium on trade in soya beans grown on land deforested after July 2006. Monitoring is done by satellite, and Greenpeace says that the pact has helped to reduce the most egregious violations. Environmentalists secured a similar promise last year from the major slaughterhouses, which have committed to mapping out their direct suppliers by November 2010 to ensure that beef does not come from newly deforested land. To increase production without sacrificing forests, Brazilian researchers have to monitor how land is actually being used. \"Everything starts with the maps,\" says Paulo Adario, who manages the Amazon campaign for Greenpeace, which is working with industry to analyse satellite images. The organization also conducts monitoring flights over suspect terrain \u2014 something that government agencies often don't have the resources to do. \"There is no environmental policy that can run without having land use figured out,\" says Adario. Batistella's team at Embrapa is running multiple studies analysing satellite data in an effort to tease out information about land use. In one, researchers are designing ways to assess photosynthetic activity and determine the amount of crops planted and cut down each year. The goal is to more easily identify existing agricultural lands that can be targeted by policies to increase agricultural production. \n               Click here for larger image \n               By far the largest potential for increasing production is in pastures, which in Brazil cover more than 200 million hectares, according to some estimates \u2014 nearly a quarter of the country, or an area three times the size of France. Brazilian ranchers on average raise just over one cow per hectare of land, but many well-managed pastures, with better grass production, carry three, four or even five cows per hectare (see  map ). The situation is slowly getting better; over the past decade, pasture in the Amazon region has increased by 30% and the number of cattle has increased by 80%. Lu\u00eds Barioni, an agricultural modeller at Embrapa, has conducted as-yet unpublished research suggesting that Brazil would need to nearly double productivity on cattle pastures between 2010 and 2030 to accommodate future demand without clearing further forest. The numbers suggest that it is more than doable, says Sergio Salles, an agricultural economist with the  State University of Campinas  (UNICAMP). Squeezing the current cattle population onto half as much pasture \u2014 which is possible from a technical stand point \u2014 would free up enough land to more than double grain production, he notes, \"without cutting down a single tree\". As part of a broader effort to reduce greenhouse-gas emissions and increase agricultural intensity, the government has instituted a US$2-billion programme, which will among other things improve 15 million hectares of degraded pasture over the next decade. A second component aims to expand systems that rotate crops and livestock by 4 million hectares over the same period; research suggests that such systems can improve soils, increasing production of crops and grasses for livestock. New incentives will be needed to get farmers to adopt such systems. \"The banks have always been behind deforestation in Brazil, and the idea is to change that logic,\" says Arnaldo Carneiro, a landscape ecologist and science adviser to the Strategic Affairs Secretariat, a cabinet-level body in charge of long-term planning. Rather than funding farmers to clear land, he says, the banks could provide discount rates to pay for land improvements, such as fertilizing soils, planting new grasses or rotating crops through the pastures. The secretariat is currently exploring zero-deforestation policies and their implications for agriculture.  \n                A risky future \n              The government is also hoping to boost farm production by helping farmers pick the best seeds to plant. In 1996, Embrapa began to produce climate-zoning maps for several key crops to ensure that government loans weren't being spent on plants that were likely to fail. The maps are published state by state for each crop and take into account factors including topography, soils, past weather and seasonal patterns. When farmers go to apply for a loan, the banks look up their location and can determine exactly what kind of crop is allowed on any given day of the year. The system now covers most crops, says Eduardo Assad, a researcher with Embrapa's agriculture information centre. \"We think we can increase productivity by 20% using climate zoning,\" he says. The zones will be a moving target because of climate change. Assad and a colleague, Hilton Pinto at UNICAMP, are now trying to assess how global warming might affect crop zones in the coming decades. Their projections suggest that annual agricultural losses could surpass US$4 billion annually by 2020 because of increasing temperatures. More than half of the losses are in soya; the lone winner is sugarcane, the optimal territory of which more than doubles in the forecasts. These projections are based on temperature alone, because global climate models differ markedly in their predictions for precipitation and broader effects on the Amazon. Nonetheless, the researchers have enough confidence in the results to urge plant breeders to take note and begin preparing for a warmer future. They should start now, says Pinto, because it takes a decade to bring new varieties to market. Climate is just one of many challenges that Brazil faces as it attempts to expand and modernize its agricultural system. The biggest corporations already run world-class operations, but many of the country's farmers in remote rural areas are desperately poor and are using equipment that seems to date from the nineteenth century. Improving rural agriculture thus involves expanding access to information and reducing social inequities. It will require a change in attitudes as well. Although researchers have signed up to sustainable growth policies, many ranchers and farmers are not yet on board. Agricultural interests prevailed over environmental concerns this month when a special congressional commission approved a proposal to scale back Brazil's landmark forest-protection code, which lays out minimum standards for protecting native habitats. Scientists and environmentalists are gearing up for a prolonged battle against the legislation, and it is not at all clear that any radical changes will survive the broader congressional debate. But the very tone of the discussion strikes many as a setback. The various challenges have so far prevented Brazil from producing a coherent plan to advance agricultural intensification, says Salles. \"The potential is big, really big, but we are still not intensifying production on millions and millions of hectares of land,\" he says. \"If you ask me why, I can't tell you.\" Yet the agricultural research community has demonstrated that Brazil can advance quickly. \"Twenty years ago, we were thinking only about frontier expansion and monocrops,\" says Batistella. \"Now all agricultural researchers are talking about is intensification, no-tillage agriculture, about crop rotation and agroforestry.\" Ways, in other words, to feed the world without levelling the forest. See Editorial,  \n                     page 531 \n                   , and Food special at  \n                     http://www.nature.com/food \n                   . \n                     Embrapa \n                   \n                     FAO/OECD forecast \n                   \n                     Ministry of Agriculture \n                   \n                     Council of Information on Biotechnology \n                   Reprints and Permissions"},
{"file_id": "466552a", "url": "https://www.nature.com/articles/466552a", "year": 2010, "authors": [{"name": "Virginia Gewin"}], "parsed_as_year": "2006_or_before", "body": "Plant breeders are turning their attention to roots to increase yields without causing environmental damage. Virginia Gewin unearths some promising subterranean strategies. Tangled, dirty and buried underfoot, roots are a mess to study. Digging them up is a time-consuming and sometimes back-breaking process. The shovel must be wielded with care to preserve the roots' delicate branching patterns, the root hairs and the microbes that cling to them. All of this explains why roots have been largely out of mind, as well as out of sight, for agricultural researchers \u2014 until now. Many scientists are starting to see roots as central to their efforts to produce crops with a better yield \u2014 efforts that go beyond the Green Revolution. That intensive period of research and development, starting in the 1940s, dramatically boosted food production through the breeding of high-yield crop varieties and the use of pesticides, fertilizers and more water. But the increases were accompanied by a depletion of groundwater and, by 1998, an eightfold increase in nitrogen-based fertilizer usage 1 , bringing environmental problems such as polluted waterways. The leaps in yield have still left many hungry. And the revolution missed many developing nations, some of which have poor soils and limited access to irrigation and expensive fertilizers. \"Those strategies of the past aren't working now to meet growing food needs,\" says Jonathan Lynch, a plant nutritionist at  Pennsylvania State University  in University Park. \"Roots are the key to a second green revolution \u2014 one that doesn't rely on expensive inputs,\" says Lynch. Roots deliver water and nutrients, two of the most essential, often-limiting, factors that a plant needs. Why keep putting in more water and fertilizers, he and others reason, when they might instead improve roots' ability to use what's already there and, in the process, help to convert 'marginal' lands into productive ones. There is room for improvement. Although plant breeders have already made huge gains by manipulating 'above ground' traits \u2014 for example, by breeding dwarf plant varieties, which put more energy into producing grain rather than the stalk \u2014 the same is not true for root traits. \"One reason we now have any potential to increase yields is because the tremendous genetic variation trapped in roots has been neglected,\" says Lynch. Here,  Nature   reports on four of the most promising leads for boosting food production through roots.  \n                Designer roots \n              Roots are most efficient when their architecture is tailored to their environment. Deep roots can tap water beneath parched soils, whereas fine, shallow roots can exploit soils in which limiting nutrients are trapped at the surface. Michelle Watt, a plant biologist at the  Commonwealth Scientific and Industrial Research Organisation  (CSIRO) in Canberra, is working to produce varieties of wheat that are better suited to drought-prone areas. In a recent study of wheat lines, Watt's team found that the roots of some lines penetrate 25% deeper than others 2 . The team crossed lines that had deeper, faster-growing roots with widely used cultivars to develop 400 new wheat lines, which are now being field-tested in India and Australia. Watt is also taking advantage of new genetic tools. Rather than wade through the 17 billion base pairs of the bread wheat genome, though, her group is searching for genetic markers that are associated with deep roots in the much smaller (271 million base pair) genome of  Brachypodium distachyon , a temperate grass in the same subfamily as wheat whose genome was sequenced earlier this year. The team hopes that the markers will make it possible to identify, from seeds, which wheat varieties are likely to have deep roots, without going through the laborious process of growing the seedlings, digging them up and measuring their roots. At Penn State, Lynch has found that, when water is limited, maize lines that incorporate a large amount of intercellular air space in root tissue have an eightfold higher yield than plants without this ability 3 . When stressed, it may be that plants reduce the metabolic costs of growing new root tissue by incorporating more air into them, leaving extra energy to invest in grain, says Lynch. He is not yet sure to what extent this trait could be beneficial in future breeding efforts. Root architecture research is in its infancy, he says. \"Right now, it is like going from analysing the shape of a font to predicting the content of a Shakespeare play \u2014 there are emergent properties of root architecture that cannot yet be predicted.\"  \n                Stealth scavengers \n              Roots seek nutrients. And some researchers are finding ways to help them, often by enhancing the ability to liberate nutrients from the soil or to neutralize toxins. In the savannah of central Brazil, known as the cerrado, the high acidity of most soils solubilizes the aluminium present, making it toxic to plant roots. Some crop varieties can protect themselves: their root tips release organic acids that render the aluminium ions chemically inert. In 2007, Leon Kochian, a plant geneticist at  Cornell University  in Ithaca, New York, and his team reported that they had identified a gene responsible for aluminium tolerance by comparing aluminium-tolerant and aluminium-sensitive sorghum varieties from the cerrado 4 . They are now working to find genetic markers that will allow breeders to screen regional varieties of sorghum and other crops for superior variants of these and other aluminium-tolerance genes. In initial fieldwork in Brazil, lines identified to have genetic variants that provide aluminium tolerance had about one-third higher yield than other varieties when grown on acidic soils, says Kochian. Trevor Garnett, a plant biologist at the  University of Adelaide  in Australia, is working with  Arcadia Biosciences , an agricultural biotechnology company headquartered in Davis, California, to commercialize a method that tricks roots into taking up nitrogen from the soil more efficiently. It does this through the overexpression of genes involved in synthesizing the amino acid alanine, which contains nitrogen. \"Currently only 40\u201350% of the nitrogen applied as fertilizer gets into the plant \u2014 which is dreadful,\" says Garnett. The unused nitrogen not only goes to waste but also pollutes lakes and streams. \"We want a greedier plant that takes up nitrogen early in the season before it is lost to the environment, so that it will be stored and remobilized as needed later in the season,\" says Garnett.  \n                Microbial manipulations \n              Another group of root aficionados wants to improve crop yields by harnessing microbes that grow on and around the rhizosphere \u2014 the narrow band of soil that surrounds the roots. The concept is in its early stages. It's not clear whether, for example, introducing a new fungus-fighting gene into a microbe or a new microbe into a poorly understood microbial community will be feasible approaches. Ian Sanders, an ecologist at the  University of Lausanne  in Switzerland, recently stumbled on one potential technique. He studies  Glomus intraradices , a mutualistic fungus that typically benefits plants by supplying inorganic nutrients, such as phosphate, in exchange for carbon. Last month, he showed how crossing  G. intraradices   individuals results in progeny with novel combinations of nuclei, and when he applied some of these to greenhouse-grown rice, they boosted plant growth fivefold 5 . He's now working to find out why. Sanders thinks that this technology might help to maintain yields in soils that are low in phosphorus. Some microbes need to be discouraged. Seeking plants that are resistant to root rot caused by the fungus  Rhizoctonia , plant breeder Kim Kidwell, at  Washington State University  in Pullman, used the chemical ethylmethane sulphonate to create wheat mutants. After screening 500,000 of them, her group found one with a higher level of tolerance than they'd ever seen. \"We were sky-high optimistic,\" she says. But the team has struggled to identify the gene responsible \u2014 and without genetic markers with which to follow its inheritance, it is difficult to select plants with the trait on a large scale. In addition, Kidwell is not sure whether the root-rot resistance gene, if found, will produce similar results in field conditions. \"For one thing, the environment plays such a big role; the trait doesn't always manifest in the field,\" she says.  \n                A healthy fixation \n              The notion of engineering cereal plants such as wheat, maize and rice to supply their own nitrogen will not go away, despite decades of failed attempts. If the crops could 'fix' nitrogen from the atmosphere instead of absorbing it from the soil, this would reduce, or eliminate, the need for costly and environmentally damaging fertilizers. But to mimic legumes, such as lentils and soya beans, which can already do this, plants need to forge a complex symbiotic interaction with a nitrogen-fixing microbe such as  Rhizobium . Most efforts have focused on getting plants to form nodules \u2014 the oxygen-free bumps on the roots where  Rhizobium   resides. In the early 1990s, researchers hailed the identification of nodulation, or Nod, factors \u2014 the signalling molecules that the microbes use to initiate nodule formation on legume roots. But efforts to introduce receptors for these Nod factors into other crops have failed so far. More recent findings \u2014 for example, that certain species of the symbiotic bacterium  Bradyrhizobium   can fix nitrogen but lack Nod-factor genes \u2014 indicate that other nitrogen-fixation genes exist. \"It's not so much about nodulation any more, but about simply establishing nitrogen-fixing bacteria intracellularly,\" says Edward Cocking, a plant physiologist and director of the Centre for Crop Nitrogen Fixation at the  University of Nottingham , UK. Many researchers believe that putting nitrogen-fixation genes of some kind into non-leguminous plants is a vital goal for agricultural science. Eric Triplett, chair of the  Department of Microbiology and Cell Science  at the University of Florida in Gainesville, says that it will require a team \"with the courage and resources to take on what will be at least a ten-year effort\". \"We've gone quite a long way over the past 40 years without worrying about roots at all, but the economic and environmental consequences of inefficient nutrient applications are now apparent,\" says Peter Gregory, chief executive of the  Scottish Crop Research Institute  in Dundee, UK. \"The only way we can avoid these costs is to get smarter about roots.\" \n                     Food Special \n                   \n                     Michelle Watt \n                   Reprints and Permissions"},
{"file_id": "466546a", "url": "https://www.nature.com/articles/466546a", "year": 2010, "authors": [], "parsed_as_year": "2006_or_before", "body": "World hunger remains a major problem, but not for the reasons many suspect.  Nature   analyses the trends and the challenges of feeding 9 billion by 2050. \n               1. Where the hungry people are  \n             In 2009, more than 1 billion people went undernourished \u2014 their food intake regularly providing less than minimum energy requirements \u2014 not because there isn't enough food, but because people are too poor to buy it. At least 30% of food goes to waste. Although the highest rates of hunger are in sub-Saharan Africa \u2014 tracking closely with poverty \u2014 most of the world's undernourished people are in Asia. \n               boxed-text \n             \n               2. Hunger isn't going away  \n             The percentage of hungry people in the developing world had been dropping for decades (bottom) even though the number of hungry worldwide barely dipped (top). But the food price crisis in 2008 reversed these decades of gains. \n               boxed-text \n             \n               3. It's not about the bomb  \n             Scientists long feared a great population boom that would stress food production, but population growth is slowing and should plateau by 2050 as family size in almost all poorer countries falls to roughly 2.2 children per family. Even as population has risen, the overall availability of calories per person has increased, not decreased. Producing enough food in the future is possible, but doing so without drastically sapping other resources, particularly water, will be difficult. \n               boxed-text \n             \n               4. And it's not about land  \n             An outlook published in 2009 by the Food and Agriculture Organization of the United Nations and the Organisation for Economic Co-operation and Development ( http://go.nature.com/DdNYvk ) says that current cropland could be more than doubled by adding 1.6 billion hectares \u2014 mostly from Latin America and Africa \u2014 without impinging on land needed for forests, protected areas or urbanization. But Britain's Royal Society has advised against substantially increasing cultivated land, arguing that this would damage ecosystems and biodiversity ( http://go.nature.com/YJ2jsB ). Instead, it backs 'sustainable intensification', which has become the priority of many agricultural research agencies. \n               boxed-text \n             \n               5. It's about doing more with less  \n             Many countries can make gains in productivity just by improving the use of existing technologies and practices. But sustainable intensification also means generating greater yields using less water, fertilizer and pesticides. Increased public investment in agricultural research will be crucial to doing this, say experts. Yet this investment makes up only 5% of total research and development spending on science. Worldwide public investment in agricultural research is increasing but at a much slower rate than in the 1970s during the green revolution. One exception is China, where funding has more than doubled over the past decade. \n               boxed-text \n             \n               Graphics by Nik Spencer, data compiled by Declan Butler.  \n             \n                     Food special \n                   \n                     OECD-FAO Agricultural Outlooks \n                   \n                     World Summit on Food Security 2009 \n                   \n                     FAO hunger site \n                   \n                     Global Conference for Agricultural Research for Development \n                   \n                     Agricultural science & technology indicators \n                   \n                     Center for International Earth Science Information Network (CIESIN) hunger and poverty site \n                   Reprints and Permissions"},
{"file_id": "466680a", "url": "https://www.nature.com/articles/466680a", "year": 2010, "authors": [{"name": "Mark Schrope"}], "parsed_as_year": "2006_or_before", "body": "Vernon Asper was one of the first researchers in the Gulf of Mexico to study the oil gushing out from the BP well. But it has not all been smooth sailing, reports Mark Schrope. On 12 May, Vernon Asper was cruising through the Gulf of Mexico, just a few kilometres south of where the Macondo well was gushing tens of thousands of barrels of oil a day into the ocean. Asper, an oceanographer at the University of Southern Mississippi near Diamondhead, wasn't there to see the carnival of response ships and drilling rigs at the site, or to look for oil slicks on the surface. He and his colleagues were hunting for something more elusive \u2014 an answer to what might be happening to the unseen oil and natural gas billowing into the bottom of the Gulf. As the first group of academic scientists on the scene, having arrived less than two weeks after the well blowout, Asper (pictured) and his team knew that valuable information about the spill was being lost and that they were the only ones in a position to capture the disappearing data. The researchers, funded by the US National Oceanic and Atmospheric Administration (NOAA), lowered a constellation of instruments into the Gulf that would beam up data in real time to their ship, the RV  Pelican . A fluorometer scanned the water with a narrow beam of light that would cause any dissolved oil to fluoresce at a telltale wavelength. A transmissometer measured how particles or cloudiness in the water blocked the transmission of light. And another sensor gauged levels of dissolved oxygen. For most of the day, the monitors showed little of interest. But towards the afternoon, when the instruments were passing through water about 1,000 metres deep, the fluorometer and transmissometer readings spiked. The team went on to track remnants of that signal for some 45 kilometres southwest of the wellhead, in a layer between about 1,000 metres and 1,400 metres deep. It took some time for researchers to make sense of the data, but all the signs suggested that a deep, hidden plume of oily water was spreading away from the gusher. The news came as a shock, because oil is supposed to float on water. \"That was a very, very disturbing and fascinating development,\" says Jeffrey Short, an environmental chemist based in Juneau, Alaska, who works with the conservation-advocacy group Oceana, and was a leader in the damage assessment of the 1989  Exxon Valdez   oil spill before he retired from NOAA. If oil was spreading in the deeper parts of the Gulf, there could be major consequences. That oil might harm a host of organisms, ranging from delicate deepwater corals to migrating plankton, that help support the Gulf's food web. It might expose BP, the company responsible for the leaking oil well, to a new area of liability for environmental damages. And it would raise a question about whether the use of oil dispersants at the wellhead had contributed to the deep plume \u2014 something that scientists would need to answer quickly. Oil at the surface, said Asper, \"could be contained or monitored or defended against\". The deep plume was something entirely different. \"It's far more complicated than I expected,\" he said on the boat. Although he was talking about the oil, Asper might as well have been forecasting his life. Following the  Pelican   cruise, he would find himself in the middle of a political and scientific maelstrom that he could never have anticipated. Several teams of researchers would later confirm the  Pelican 's discovery. But throughout the agonizing three months that the well spouted, Asper was surprised to find his team's work alternately ignored and challenged by NOAA. The agency temporarily requested that Asper and his colleagues stop talking to the media. And BP is now trying to hire Asper and other scientists, in what some view as an attempt to silence them 1 . It has all been more than enough to probe the limits of his otherwise composed and good-natured disposition. \"The whole experience has been both exciting \u2014 to be involved in cutting-edge research into an incredibly important event \u2014 and frustrating,\" says Asper. It was chance that brought Asper and his colleagues to the centre of the oil spill just days after it started. When the Deepwater Horizon rig, operated by a contractor for BP, suffered a catastrophic blowout on 20 April, Asper's team was making final plans for a research cruise to study natural methane seeps and shipwrecks at the bottom of the Gulf of Mexico. His group, including the cruise's chief scientist, Arne Diercks from the University of Mississippi in Oxford, was part of the National Institute for Undersea Science and Technology (NIUST), a NOAA-funded multi-university cooperative effort to apply new technology to undersea research. After the blowout, the researchers requested approval from NOAA to switch plans and investigate the spill instead. The NIUST team was still working out its research strategy when it departed from shore on 2 May, with a general mission to track where the oil was going and to collect samples of sediments in areas not yet affected by the spreading oil. \n               Click here for larger image \n               Two weeks into the voyage, the instruments picked up signs that were consistent with the presence of oily water at depth. The researchers started to wonder whether the oil was getting trapped in a relatively stable layer, rather than rising to the surface as expected. As they cruised southwest of the well site, they kept encountering hints of oil at depths below 1,000 metres (see  map ). None of their data was conclusive. The researchers knew, for example, that natural seeps emit more than 400,000 litres of oil and gas into the Gulf each day 2 . And when the  Pelican   team pulled up water from the region where the instruments pointed to oil, the initial samples looked clear and had no oily scent. \"I don't know what to think,\" Asper said at the time, \"but that's why we're here.\" Over the course of the cruise, the researchers collected enough evidence to build what they considered a strong circumstantial case for the existence of a deep plume of some form of oil. They also found unusually low concentrations of oxygen in the water, which they suspected could be caused by bacteria metabolizing oil and methane at depth. Many oil wells produce substantial amounts of methane and later measurements of samples collected by the  Pelican   team would find methane levels 100\u2013100,000 times higher than normal in the plume. Towards the end of the trip the group was asked by NOAA to take a detailed inventory of all the water samples it had collected, in case they ended up as evidence in legal proceedings. The agency needed the data to fulfil its challenging roles in responding to oil spills. NOAA must work closely with BP to guide response efforts, but it must also lead the environmental assessment that will ultimately determine BP's liability. Assembling the inventory was a time-consuming task at a point when the group was frantically trying to finish its work. At first the scientists feared that they might have to halt their research early to get it done. \"I'm a scientist,\" said Asper soon after the directive came through. \"This legal crap is not what I got into the business for.\"  \n                A media storm \n              But Asper and his colleagues could not avoid the issue of liability. Shortly before the  Pelican   crew found the plume, BP had begun to apply dispersants at the wellhead \u2014 the first time these chemical brews designed to break up oil had ever been used underwater. Asper and his colleagues discussed from the outset the possibility that the dispersants might explain why they were seeing oil forming a plume 1,000 metres down, rather than rising to the surface. \"I really wonder if the plumes are a result of that or if they would have been there without it,\" Asper said at the time. The question had some urgency because the US Environmental Protection Agency would require BP to stop using the dispersants if they were shown to be creating a hazardous situation \u2014 for example by depressing oxygen concentrations enough to harm life. The team felt that it was important to get its findings out quickly so that scientists could mobilize to collect more information as fast as possible. But once he was on shore, Asper found himself at the centre of a mess that would in some ways prove even more challenging than understanding the deep oil. He had agreed to be the  Pelican   team's media face, and he did interviews from just after dawn until the evening on the day they returned. \"It was just a crazy, crazy, crazy day,\" says Asper. \"It was a twilight zone.\" During the interviews, he described the evidence for a hidden plume of deep oil that was spreading an untold amount of hydrocarbons into the Gulf. Asper believes he was careful to note that more analyses were needed before anything could be said for sure. Still, some media reports gave the impression that huge lakes of crude oil were hiding in the deep \u2014 a view not supported by the data. \"It was a surprise to us that we had been misinterpreted,\" says Asper, who admits that he entered the fray with little media experience. But he says that he did what he could to keep the record straight, and doesn't know how he could have better controlled the picture that the media painted. Other researchers were also unprepared for the crush of attention, which might have caught even the most media-savvy scientists off guard. Samantha Joye, a biogeochemist at the University of Georgia in Athens who collaborated with the  Pelican   crew and has grant funding through NIUST, was quoted by  The New York Times   as saying \"There's a shocking amount of oil in the deep water, relative to what you see in the surface water.\" Joye says that she now chooses her words more carefully and makes a concerted effort to be \"less excited\" when giving interviews, but that she does not regret spreading the news about the plumes because the opportunity to study them might have been missed if the press had not learned about the  Pelican   data. The deep-oil discovery was not good news for BP. At the time, efforts to contain the oil and study its effects were focused on the surface, where the battle against all previous oil spills had been fought. Executives and spokespersons at the oil company questioned the existence of any deep plume of oil or gas, arguing simply that oil floats. (When  Nature   contacted BP, the company provided information already publicly available but did not give specific responses to several questions.) What baffled Asper and his colleagues, however, was NOAA's cool response to the  Pelican   data. The day after the ship returned to shore, NOAA asked the researchers to postpone talking to the press to allow time for regrouping. On the same day, the agency issued a statement about the plumes calling media reports on the team's work \"misleading, premature and, in some cases, inaccurate\". The researchers were taken aback. \"We took it personally,\" says Asper. \"We thought it was talking about us.\" His team was proud of the work it had accomplished under difficult circumstances. \"We expected NOAA to be as proud of it as we were,\" he says. \"To instead have NOAA basically say that our results were invalid was quite a surprise.\" The scientists were further surprised by the rest of NOAA's statement, which said that the scientists wished to clarify that they had not yet reached definitive conclusions. It also said that the team's findings showed that oxygen levels were not low enough to be of concern, and that any connection to subsea dispersant use was only speculative. Asper says that they fully agreed with the statements attributed to them, but that the  Pelican   researchers had not seen the text before NOAA released it. \"They were doing damage control and trying to make sure people didn't panic,\" says Asper. \"I kind of understand that, but I do wish they would have communicated with us a little bit better.\" Short saw the statement as a way to divert attention. \"I think the agency probably felt like they should have been the ones to catch this and they weren't,\" he says. Although NOAA never completely denied the possibility that the oil might spread far below the surface, it consistently backed away from confirming the  Pelican   findings, and pointed to a need for definitive confirmation. Weeks after the cruise, Jane Lubchenco, head of NOAA, still seemed uncertain about the evidence for a significant plume of oil at depth. \"Obviously it would be highly unusual if we didn't find oil right close to the well; the question is what's happening farther afield,\" she said. \"I think the bottom line is that there is a lot of potential out there for jumping to conclusions that may not be warranted and that we are all served best by proceeding in a careful, thoughtful and quantifiable manner.\" Justin Kenney, NOAA's communications chief, told  Nature   last week: \"Throughout this event, all researchers have been committed to providing scientifically accurate information as soon as possible. Specifically in the case of the  Pelican , all of us agreed that laboratory analyses of water samples collected on site had to be completed before definitive statements could be made about the presence of oil.\" In hindsight, the  Pelican   discovery should not have been much of a surprise. Ten years earlier, the US Minerals Management Service in collaboration with 23 oil companies, including BP, released some 120,000 litres of oil at a depth of 844 metres off the coast of Norway, as part of an experiment aimed at simulating a deepwater blowout. They found that a small but significant amount of the oil was confined to lower levels and did not rise quickly to the surface 3 . But few people seemed to recall the Norwegian experiment as NOAA set about coordinating the response to the blowout in the Gulf. A few days after asking the  Pelican   scientists to stop speaking to the press, NOAA rescinded its request. Since then, Asper has been interviewed regularly. \"It's extremely time consuming,\" he says. \"There are so many phone calls and inquiries, but it's hard to say no. We're paid to collect data and obtain information, so you don't want to withhold anything when someone asks about your findings.\"  \n                Corroborating evidence \n              Within weeks of the  Pelican 's return, other researchers were finding corroborating evidence for the deep oil plume. Researchers at the University of South Florida in Saint Petersburg went out to study the spill area on the  Weatherbird II   twice in May, and NOAA presented data collected by the Florida researchers on 8 June. Lubchenco announced that NOAA had confirmed the presence of low concentrations of oil from the Deepwater Horizon well in deep plumes: specifically hydrocarbons in the parts per million range, and polycyclic aromatic hydrocarbons \u2014 carcinogenic oil-breakdown products \u2014 in the parts per trillion range. \"It was gratifying,\" says Asper, \"I thought, 'Great, at last now we're vindicated'.\" But the  Weatherbird II   team had its own challenges with NOAA. Representatives from the agency and from BP travelled with the scientists on their first boat trip, and much of the work was carried out as part of the government's Natural Resource Damage Assessment (NRDA) process for gathering evidence that might be used in future spill liability cases. The NRDA process is a foreign one to many scientists because there are restrictions on how samples and data are handled. \"Everything was kept under a very, very strict chain of custody,\" says Ernst Peebles, a biological oceanographer from the University of South Florida and one of the lead researchers on the vessel. His group relinquished its samples to NOAA and has not been given the opportunity to analyse them or most of those collected during the second cruise. The Florida team is scheduled to head out on another research cruise this week, but university administrators arranged funding for the trip independent of NOAA and BP. Despite the corroborating data collected by the  Weatherbird II   and other cruises that found multiple shifting plumes, NOAA continued to publicly criticize parts of the  Pelican   team's work. Speaking at a conference in Baton Rouge in early June, Lubchenco said: \"Unfortunately, some data collected have not been usable because the protocols that have been well identified have not always been followed.\" Lubchenco was referring to samples taken by the  Pelican   crew during its cruise. At the time, the scientists had followed established protocols by collecting water in glass containers for oil analysis and in plastic bottles for methane measurements. By a prior arrangement, the  Pelican   crew sent the glass containers off to researchers in Texas who were scheduled to do the oil tests. After the ship returned, NOAA requested water samples so that the agency could conduct its own oil analyses. The  Pelican   researchers provided some of the remaining samples, which had been collected in plastic and were less ideal for oil analysis because of potential interactions with the plastic. But Lubchenco blamed the  Pelican   crew for failing to follow protocol. \"Did you hear what she was saying up there?\" asked an incensed Asper, after hearing Lubchenco's accusations in Baton Rouge. He felt better when he spoke directly to Lubchenco afterwards. \"I got the impression that she really did not understand what our situation had been and that the information she had been provided was very incomplete,\" says Asper. \"I don't blame her personally.\" But Asper and his colleagues were again dismayed when NOAA issued a research update on 13 June that criticized the sample collection on the  Pelican . \"It's like they were saying, 'Shame on you for not being psychic and reading our minds and knowing what we wanted',\" says Joye, who maintains that the samples were collected properly for their intended use.  \n                Offending words \n              Steve Murawski, director of scientific programmes and chief science adviser for NOAA Fisheries, says that no criticism was intended and he considers the sample issue a mix-up. \"Those guys jumped into the breach,\" he says. \"I think they did the nation an incredible service and they should be congratulated.\" However, NOAA has not yet acted on the team's request to take down the press statement. \"That whole issue to my nose had a bad odour,\" says Short, who feels that the protocol issue was part of an overall tendency by NOAA to be overcautious in its response to the spill. \"I don't feel it's responsible to hide behind excuses like, 'Oh, you used the wrong sample bottle.' That's the kind of behaviour you expect out of an oil company trying to minimize liability. It's not what you expect out of a government that is supposed to be telling us what is happening.\" It is not yet certain whether oil and gas in the plumes are harming life in midwaters, or the delicate deepwater corals and other bottom dwellers found in fertile areas on the ocean floor throughout the Gulf. Scientifically, it is all new territory. \"I suspect that the concentrations found may pose threats to plankton and larvae of many species,\" says Tom Shirley, a marine biologist at Texas A&M University in Corpus Christi. Murawski has similar fears. \"Personally, I'm concerned about that much oil in a community that is long lived and slow growing, although we don't have any indications of mortalities.\" In addition to toxicity, a key concern is the oxygen depletion caused by microbes consuming oil and methane in deep plumes. On their cruises, Asper and his colleagues found that oxygen concentrations had dropped by 30\u201355% within the plumes. In June, a team led by John Kessler, an oceanographer at Texas A&M University in College Station, found reductions of up to 30%, in patterns similar to those seen by the  Pelican   team. Neither group detected oxygen levels that would be considered hypoxic \u2014 too low to support aerobic organisms \u2014 which is the level set by the Environmental Protection Agency to cease use of dispersants. NOAA has been slow to compile and assess the oxygen data. Although several scientific groups found signs of oxygen depletion, an initial report in late June by NOAA and various federal agencies and BP did not describe any concerns 4 . \"We don't see significant oxygen depletion,\" said Murawski soon after the report's release. A report on 23 July by the same group acknowledged that oxygen depletion has been observed but suggested that available data are inconclusive 5 . The report did not include data from Kessler's group or from the  Pelican , even though later work could have been compared with the first glimpse of the plumes to help assess their evolution. \"It's still a mystery to us why NOAA is not recognizing our data set,\" says Asper. The recent report is hesitant about the oxygen-depletion data, suggesting that backup measurements are needed because oil could potentially cause problems with the standard oxygen sensor used in most of the studies. But Kessler's team and others performed further analyses and found no such problems 6 . Kessler has tried several times to make his data available to NOAA and is working with the agency so it can incorporate his results into the ongoing analysis of the plumes. \"They seem as eager to understand this as we are,\" he says, although he is not sure why it is taking NOAA so long to receive the team's data. Short says that NOAA may be reluctant to acknowledge the oxygen impacts because it will make it harder for the agency to carry out its damage assessment. The effects of a diffuse oil plume are not clear, but there are more studies of how lowered oxygen concentrations can harm marine ecosystems. The news of oxygen depletion, he says, \"complicates NOAA's relationship with BP because these assessment studies would also create the potential for liability on BP's part\". Kessler thinks the answer may be simpler. Although he is not privy to the inner workings of NOAA, he surmises that the agency is simply overwhelmed. \"I know these guys are unbelievably busy. They've got to be running 12 different directions at once.\" Some scientists suggest there are broad problems in the way the US government has handled research into the oil spill. \"One critical shortfall is that there is no overall coordination of the many types of research that are being undertaken, so that important issues are not missed,\" says Nancy Rabalais, executive director of the Louisiana Universities Marine Consortium in Cododrie.  \n                Scientists for hire \n              Given the scale of the environmental problem, many researchers worry that the government and BP are not doing enough to understand the gusher and its consequences. Ira Leifer, an oil-spill specialist at the University of California, Santa Barbara, and about a dozen collaborators including Asper, have been pushing BP to fund a comprehensive series of studies examining how the gas, oil and dispersants behave as they enter the water at the wellhead and spread up the water column. \"What no one really knows is why the oil is going where people are finding it,\" says Leifer. \"And because the underlying science is unknown we have no predictive capability.\" Researchers can't answer basic questions such as whether deep oil and gas will be exported from the Gulf into the Atlantic, and what is the most effective ratio of dispersant to oil. Leifer's funding requests have been rebuffed for several weeks, despite being championed by Congressman Ed Markey (Democrat, Massachusetts). Early on, BP said that it was very close to reducing the flow, so the study would be moot. Although the oil flowed for several more weeks, the capping of the well in July prevented the studies that Leifer has in mind. For now, Asper is planning his group's next cruise to the spill zone and trying to keep up with interview requests. Previously, he was pondering whether to accept an offer to work as a consultant for BP to help guide its response to the spill. When first contacted by a BP lawyer about the possibility of working on retainer, Asper was sceptical. \"I think he wants to make sure I don't testify against BP,\" he said. More than a dozen scientists have already signed contracts with BP, according to the company 1 . Some academics found the offers by BP appealing because they said it would provide a way to bring strong science into efforts to respond to the spill. After he learned more about what would be involved, Asper found the offer more tempting, both scientifically and financially. But he recognized that there was a potential conflict of interest and eventually decided not to accept the offer. Despite his frustrations with certain aspects of the oil-spill experience, Asper says that it is by far the most interesting scientific problem of his career. He recognizes the benefits of scientific stardom, which have helped to focus international attention on the needs of the Gulf Coast. \"I'm also pleased that people are interested in the things we've been interested in for so long,\" he says. \"It is flattering to have people call up and ask what you do. I say, 'Well, I've been doing this for 25 years and nobody ever asked before but I'm happy to tell you about it.'\" On a recent weekday afternoon the wind was blowing the oil stench from the Gulf to Asper's office building at the university. He had spent the morning with the journalist Dan Rather, doing an interview for a cable show. After work, Asper decided to take some time off to fly an experimental plane that he had finished building after 24 years of effort. He flew up through the clouds and above trucks loaded with dispersants under military guard. \"It is gorgeous up there,\" says Asper. \"It's nice to get above it all.\" \n                     Deepwater Horizon disaster \n                   \n                     23 June report from the Joint Analysis Group \n                   \n                     23 July report from the Joint Analysis Group \n                   \n                     NOAA Office of Response and Restoration \n                   \n                     Vernon Asper \n                   Reprints and Permissions"},
{"file_id": "464670a", "url": "https://www.nature.com/articles/464670a", "year": 2010, "authors": [], "parsed_as_year": "2006_or_before", "body": "At the time of the announcement of the first drafts of the human genome in 2000, there were 8 billion base pairs of sequence in the three main databases for 'finished' sequence: GenBank, run by the US National Center for Biotechnology Information; the DNA Databank of Japan; and the European Molecular Biology Laboratory (EMBL) Nucleotide Sequence Database. The databases share their data regularly as part of the International Nucleotide Sequence Database Collaboration (INSDC). In the subsequent first post-genome decade, they have added another 270 billion bases to the collection of finished sequence, doubling the size of the database roughly every 18 months. But this number is dwarfed by the amount of raw sequence that has been created and stored by researchers around the world in the Trace archive and Sequence Read Archive (SRA). \n                 See Editorial,  \n                 \n                     page 649 \n                   \n                 , and human genome special at  \n                 \n                     http://www.nature.com/humangenome \n                   \n               \n                     Human Genome at Ten \n                   \n                     Personal Genomes Web Focus \n                   \n                     Web Special: Big Data \n                   \n                     National Center for Biotechnology Information \n                   \n                     EMBL Nucleotide Sequence Database, EBI \n                   \n                     DNA Databank of Japan \n                   Reprints and Permissions"},
{"file_id": "464668a", "url": "https://www.nature.com/articles/464668a", "year": 2010, "authors": [{"name": "Alison Abbott"}], "parsed_as_year": "2006_or_before", "body": "What was it like to participate in the fastest, fiercest research race in biology? Alison Abbott talks to some of the genome competitors about the rivalries and obstacles they faced then \u2014 and now. In many people's minds, May 1998 marked the real start of the race to sequence the human genome. In that month, Craig Venter announced that his upstart company, Celera Genomics in Rockville, Maryland, would sequence the genome within two years. The publicly funded Human Genome Project, which had been plodding along until that point, had a competitor \u2014 and each side assembled and prepped its team.  \n                The shotgunner \n              Venter was willing to flout convention, and he recruited  Gene Myers   to help him. As a mathematician at the University of Arizona in Tucson, Myers had developed a technique for blasting a genome to pieces and reassembling the sequenced debris. But he despaired of ever using this 'whole-genome shotgun sequencing' method on the human genome. The field was signed up en bloc to sequencing the genome piece by consecutive piece to avoid gaps, and Myers's algorithms had been scorned for being error-prone and unworkable. At Celera, Myers never felt he was on the 'wrong side'. He arrived before the computers and furniture did, yet little more than a year later the group had lined up most of the 120-million-base-pair genome of the fruitfly  Drosophila melanogaster   ( E. W. Myers  et al .  Science    287,   2196\u20132204; 2000 ), proving that the shotgun technique could work. The human genome came next. Myers still feels sore about his early rejection \u2014 \"it hurt deeply\" \u2014 and expresses a gleeful triumph that the technique is now standard in genomics. The academic world was hypocritical, he says. It castigated him for pushing the technique and joining industry, then sneaked him job offers at the first inkling that he might have been right. When Myers left Celera in 2002, he was looking for a new direction. He eventually found it in neuroinformatics, a field that provides its own computational challenges. Advances in microscopy combined with sophisticated genetic techniques now make it possible to observe how individual neurons behave when genes are turned on and off. Doing this across an entire mouse brain allows biologists to observe development in unprecedented molecular detail \u2014 if, that is, they can make sense of the vast numbers of high-resolution images. Myers is tackling this data challenge at the Janelia Farm Research Campus in Ashburn, Virginia. \"Sequences: been there, done that,\" Myers says. \"Cell-resolution models of nervous systems or developing organisms: daunting but looking more and more doable.\"  \n                The mega-manager \n              The huge sequencing effort of the Human Genome Project was biology's first foray into the world of 'big science'. It required big money, and a level of teamwork that came as a major sociological shock to participating scientists. These were the problems with which  Jane Rogers   had to contend as manager of the Human Genome Project for the Wellcome Trust Sanger Institute near Cambridge, UK. In 1998, Rogers was part of a small posse of senior scientists from Sanger who persuaded governors of the Wellcome Trust to inject more momentum into the project by doubling the Sanger centre's budget so that it could sequence a full one-third of the genome. The trust's senior administrator, Michael Morgan, revealed the decision to scientists at that year's genome meeting at Cold Spring Harbor Laboratory in New York. The scientists were demoralized by Venter's recent announcement that he was entering the race, and Morgan's news brought the crowd to its feet. \"It was an incredible moment, seeing everyone stand up,\" Rogers says. \"We felt we had saved the day.\" Back home, Rogers had to cajole and coerce scientists who were used to working in their own small groups into working together on a central project, using standardized methods and procedures. There were emotional moments, she concedes with some diplomacy. Rogers, one of very few women involved at a high level in the Human Genome Project, developed a taste for big science. After finishing the major sequencing, the Sanger Institute reverted to principal-investigator-led research groups focused on the genomics of human health. But Rogers set about lobbying the UK Biotechnology and Biological Sciences Research Council for funds to establish a centre for sequencing plant, animal and microbial genomes. She now heads the council's Genome Analysis Centre in Norwich, UK, which opened last year\u2014 a management challenge that, for her, matches the buzz of the Human Genome Project.  \n                The patent pioneer \n               Robert Millman   believed he'd landed in patent-attorney heaven when he joined Celera as head of intellectual property in 1999. It was Millman's task to work out which of the company's intended products \u2014 the human genome sequence, its constituent genes, and the software and algorithms to analyse it \u2014 could be patented. In earlier days, Millman had been a street artist, performing outrageous feats of escapology in his free time. Life at Celera turned out to be similarly challenging. He enjoyed the buzz of testifying in front of Congress with Venter, helping to shape the US patent office's policies in gene patenting. Academics scorned Venter for making a business out of the human genome, but Millman remembers that although Venter \"revelled in his bad-boy image, he didn't always act like he really believed in patents and he didn't make my life easy\". Millman found himself caught between Venter's academic principles and his business drive, and thought that the company could have pursued patents more aggressively. In the end, Millman patented 150 genes and proteins that were considered likely drug targets, a handful of 'SNP' patterns linked to disease, and technologies linked to shotgun gene sequencing, none of which Celera fully exploited. Frustrated, Millman says that when he left the company in 2002, he didn't want to hear the suffix '-omics' ever again. He obviously changed his mind. Millman has since been involved in start-up companies that are pursuing other hot new biotechnologies, including, in 2004, Alnylam Pharmaceuticals in Cambridge, Massachusetts, which has led the way in RNA-interference technologies for regulating genes. In his current position at the venture-capital company MPM Capital in Boston, Massachusetts, he has invested in firms exploring epigenetics and stem cells. Gene patenting, however, remains controversial, even though patents are no longer granted for sequences alone and now require information about a gene's function and utility. Millman still sports his colourful clothes and his red ponytail. Occasionally he yearns to don his straightjacket and ride his unicycle across a tightrope, but, these days, he resists.  \n                The freedom fighter \n              Whenever Celera put out a bullish press release to reassure shareholders that it was winning the race,  John Sulston   went on television to explain that, actually, it wasn't. \"I was a reluctant media star,\" he recalls. Sulston never worked directly on the human genome, but his work sequencing that of the nematode worm at the Sanger Institute paved the way for the Human Genome Project \u2014 and he became one of its most righteous political and scientific champions. Sulston fought to ensure that sequence data were released daily into the public domain, helping to establish principles at a 1996 strategy meeting on human-genome sequencing in Bermuda that are still largely followed by the genomics community. And he put the kibosh on a compromise with Celera, proposed in 1999, because the company was not prepared to release data early enough to satisfy the public effort's principles. In retrospect, Sulston still thinks it was right to fight. \"Otherwise the biological databases that we have today would have collapsed \u2014 everything could have ended up in the hands of an American corporation. The race made for a crazy and irrational time.\" Yet his battles over the ownership of biology haven't stopped. Now emeritus at the Sanger Institute, he is a part-time faculty member at the University of Manchester's Institute for Science, Ethics and Innovation, which is engaging patent attorneys in heated debate about ownership issues in biology, such as the extent to which donors of biological material deserve compensation. Sulston thinks that the biology should be able to be exploited by businesses but that better checks are needed to stop basic researchers from becoming secretive.  \n                The diplomatic coder \n              When  Todd Taylor   moved to Japan from the United States in 1998, he was a molecular geneticist in need of employment. Taking a chance, he presented himself as a bioinformatics expert to the RIKEN Genomic Sciences Research Complex in Yokohama, newly created to allow Japan to contribute to the Human Genome Project. Then he started reading up like crazy. The centre was collaborating on chromosome 21 with another Japanese group and two German teams. He soon found himself as the centre's English-speaking representative at its meetings, and experienced the occasionally sharp edge of international tensions. The Japanese side was not well organized at first, he says, and sequenced some parts of the genome assigned to its partners. He recalls a meeting at the Sanger Institute when one of the Germans, beside himself with anger, shouted that by doing so the Japanese had wasted German taxpayers' money. Once the Japanese groups hit their stride, they bid for the unassigned chromosomes 11 and 18. The researchers flew over to Washington University in St Louis to negotiate with the rival US contingent. \"We stepped off the plane and went straight into a three-hour meeting where no one even offered us a glass of water,\" Taylor remembers. After some fairly hostile bargaining, they came away with a compromise \u2014 the long arm of chromosome 11, the short arm of 18 and no dinner invitation. \"It was crazy to split the chromosomes that way, but at least I got two  Nature   papers,\" he jokes. Taylor, now a recognized bioinformatician, works at the RIKEN Advanced Science Institute that replaced the former genome centre. His group has shrunk from 70 to 20 people. One of his main projects is with the International Human Microbiome Consortium, developing software for analysing the hundreds of microbial species in the intestines of healthy Japanese people. But these and other international efforts cannot rival the Human Genome Project, says Taylor, who calls it \"a once-in-a-lifetime project, something the likes of which we probably won't see again. Not that we all wouldn't mind working like that together again. I'd jump at the opportunity.\" See Editorial,  \n                     page 649 \n                   , and human genome special at  \n                     http://www.nature.com/humangenome \n                   . \n                     Human Genome at Ten \n                   \n                     Human Genome collection \n                   \n                     Nature paper: Initial sequencing and analysis of the human genome \n                   \n                     Science paper: The sequence of the human genome \n                   \n                     Gene Myers \n                   \n                     Robert Millman \n                   \n                     BBRSC Genome Analysis Centre \n                   \n                     John Sulston \n                   \n                     Todd Taylor \n                   Reprints and Permissions"},
{"file_id": "465680a", "url": "https://www.nature.com/articles/465680a", "year": 2010, "authors": [{"name": "Naomi Lubick"}], "parsed_as_year": "2006_or_before", "body": "With chytrid fungus rapidly spreading around the world, researchers are testing an extreme approach to saving endangered amphibian populations. Naomi Lubick reports from a rescue site. On a mid-April afternoon with rain threatening, Jaime Bosch clambered down the carbonate cliffs of northern Mallorca to a pond overlooking the Mediterranean Sea. After removing his shoes and wading into the shallow pond, Bosch quickly netted 30 wriggling tadpoles and dropped them into a bag filled with water. He gently grabbed one in plastic-gloved hands, swabbed its mouth with a cotton-tipped stick, and returned the tadpole to the pond. The important information was in the DNA captured on the end of the stick. Bosch, an evolutionary biologist at Spain's National Museum of Natural History in Madrid, and his colleagues had conducted a back-breaking experiment in 2009 to try to rid this particular pond of the chytrid fungus  Batrachochytrium dendrobatidis . The fungus causes a disease called chytridiomycosis that has wiped out amphibian populations around the world. Given that history, Bosch and his team resorted to extreme measures to save the frogs on Mallorca. At this small pond, the researchers removed all the resident tadpoles during the spring and summer, treated them with an antifungal medication and returned them to the water after cleaning the pond. When Bosch returned to the site this spring, he brought his swabbing kit to see whether the frogs were free of the fungus. He hopes that results from the experiment in Mallorca can help other scientists, who are testing similar approaches in Switzerland and California. These and other frog researchers are closely watching the work in Mallorca because the species in the pond, the midwife toad, makes this an important test case. \"Midwife toads are a sentinel species because they are so susceptible to chytrid,\" says Matthew Fisher, an epidemiologist at Imperial College London. Fisher heads a consortium called RACE (Risk Assessment of Chytridiomycosis to European amphibian biodiversity), which is working with Bosch. Funded by the European Union's Biodiversa research project, RACE brings together specialists such as mycologists and herpetologists to find ways to control the spread of chytrid fungus across Europe. When Bosch's team evacuated the Mallorcan tadpoles last summer, its goal was to completely eradicate the fungus, which gained a foothold on the island about 20 years ago when frogs were imported to help boost a native species. \"Only four populations are infected, which is why we need to take action very quickly on the island,\" says Bosch. The local government agreed and pitched in, donating employees' time and funding. Some of that money covers the tests to detect fungal DNA on the tadpoles. More funding comes from the Spanish National Research Council (CSIS) and from the regional government of Madrid \u2014 which has also spent about \u20ac75,000 (US$90,000) on a small breeding centre for two endangered frog populations in Spain's Pe\u00f1alara Nature Reserve, the site of the first European outbreak of chytridiomycosis in 1997. Last year, several scientists \u2014 including researchers from the Zoological Society of London (ZSL) and others affiliated with RACE \u2014 evacuated more than 2,000 tadpoles from the Mallorcan pond, which is near the coast, west of Pollen\u00e7a. In four trips between late March and early August, the team carried hundreds of tadpoles at a time in well-cleaned, 2-litre bottles, filled with pond water and rigged with aquaria air pumps to get oxygen to the tadpoles during the three-hour hike out to the nearest road. The researchers then drove for several hours to a lab facility at Marineland, a dolphin tourist attraction across the island. There, the tadpoles completed a week-long regimen of daily 5-minute baths in itraconazole, an antifungal medicine, and were placed in glass aquaria for up to seven months. Meanwhile, Bosch returned to the pond several times, to attempt to dry it out, because it is suspected that chytrid fungus needs moisture to survive. Eventually, he emptied the pond as much as he could with a bucket, and left it to dry in the hot Mallorcan summer. When the pond refilled with rain in the autumn, Bosch's team airlifted the tadpoles across the island to their home, with the hope that they would survive in the now-clean pond.  \n                Sweet sounds \n              Signs were good when Bosch arrived back at the pond in April and heard faint, bell-like pings. Tracking the sound, he raced up a jagged cliff and found two healthy-looking adults, one of which had several eggs attached to its posterior. Male midwife toads carry the fertilized eggs and normally spend their days away from the pond, which makes them difficult to find, says Bosch. This male frog would soon be returning to the water to release the eggs. Adult midwife toads are particularly vulnerable to the chytrid fungus because it attaches to keratin, which covers the adults but only the mouths of tadpoles. So finding the adults near the pond lifted Bosch's hopes, as did the hundreds of tadpoles swimming in the water. Bosch chose Mallorca as a test site partly because the dry environment and the widely separated amphibian populations slow the spread of the fungus. That provides researchers with a chance to wipe out the pathogen, says Bosch. By contrast, it would be out of the question to eliminate the fungus from rainforest communities, such as in Central America, where water is plentiful and the fungus is widespread 1 . In places where complete eradication is impractical, researchers are hoping to take a modified approach, which relies on the fact that some populations of frogs survive chytrid attack. Vance Vredenburg, a biologist at San Francisco State University in California, is searching for such populations in the Sierra Nevada. The mountain yellow-legged frog in this range is one of the most endangered amphibians in the United States and has been hit by many chytridiomycosis outbreaks over the past decade. By analysing how the frogs responded to those infections, Vredenburg and his colleagues have developed a model that can help to identify the populations that are most at risk and which might benefit from limited intervention 2 ,   3 . Vredenburg is now adapting Bosch's technique to try to protect communities by treating some individuals. This spring, his group started capturing frogs in cages in the field, washing them in 5-minute antifungal bath each day for a week and then releasing them. In late April, Bosch received the results from colleagues at the ZSL who had completed the analyses on the tadpole swabs taken in Mallorca. Every sample came back positive for  B. dendrobatidis , which means that all the tadpoles in the pond probably carry the fungus, says Bosch. But the number of spores detected on each swab was far smaller than the number seen in tests the previous year, suggesting a lower level of infection. Even so, the news stunned Bosch and his colleagues, who are struggling to understand how the pathogen survived in the pond at all. Jon Bielby, an ecologist at the ZSL's Institute of Zoology, who assisted last year in carrying tadpoles out, wonders whether the froglets \u2014 a stage between tadpole and adult that temporarily leaves the pond \u2014 picked up the fungus elsewhere and brought it back. Other RACE researchers have floated the idea that the fungus may have a life stage outside water. \"But the honest answer is we don't know,\" says Bielby.  \n                Reduction not eradication \n              The results on Mallorca, although disappointing, are valuable for other researchers trying to combat outbreaks of  B. dendrobatidis   ( Bd ), says Benedikt Schmidt, coordinator for amphibian and reptile conservation with the Swiss frog-protection group KARCH, based in Neuch\u00e2tel. \"It means that we should focus on reducing prevalence rather than trying to eradicate  Bd   from our ponds,\" he says 4 . Schmidt and his colleagues hope to use a variation of Bosch's technique to bolster local populations infected by the fungus in Switzerland. Working with graduate student Corina Geiger at the University of Zurich, Schmidt will pull some frogs from three ponds, treat them, and reintroduce them. They will compare those ponds with two untreated control populations. This treatment approach may become a tool to help frogs survive if the chytrid situation in Switzerland and other sites across Europe worsens, he says. Schmidt is also focusing on other threats to frog populations, the biggest of which may be the loss or degradation of habitat. Schmidt worries that human-initiated changes to the landscape, from farming practices to creeping urbanization, are stressing amphibian populations and may be making them more vulnerable to chytrid fungus. Schmidt and others say there is still much unknown about how chytrid affects frogs. Researchers with the RACE project and on Vredenburg's team are looking into the genetics of the fungus, trying to determine whether the infectious strains vary every year, perhaps like the flu strains that infect humans. Biologists are also trying to pinpoint frogs' molecular responses to the fungus \u2014 which genes get turned on, and which proteins are produced in response to the infection. Although disappointed by his first test in Mallorca, Bosch remains undeterred. The reduced severity of infection may help the frogs survive in the pond, he says. And his team may try another round of antifungal baths this summer, perhaps treating the difficult-to-locate adult frogs as well as the tadpoles. Bosch also started up fieldwork in Portugal and Hungary earlier this spring, searching for more amphibian populations infected with chytrid fungus. He is hoping to modify the Mallorcan method to treat midwife toad populations first in Pe\u00f1alara, and then elsewhere in Spain as well as in Eastern Europe. Bosch's first work with frogs was studying their communication and mating habits. His iPhone rings with the calls of six different frog species that he recorded, and he wishes he could resume his research on sexual behaviour. But for the past ten years, Bosch has spent his days working to preserve frog populations before they disappear. \"It's nicer,\" he says, \"to work with live animals.\" Naomi Lubick is a freelance writer based in Zurich, Switzerland. \n                     RACE: Risk Assessment of Chytridiomycosis to European amphibians \n                   \n                     Surveillance map of chytrid fungus \n                   Reprints and Permissions"},
{"file_id": "466685a", "url": "https://www.nature.com/articles/466685a", "year": 2010, "authors": [{"name": "Eric Hand"}], "parsed_as_year": "2006_or_before", "body": "Networks of human minds are taking citizen science to a new level, reports Eric Hand. The whole thing began by accident, says David Baker, a biochemist at the University of Washington in Seattle. It was 2005, and he and his colleagues had just unveiled  Rosetta@home  \u2014 one of those distributed-computing projects in which volunteers download a small piece of software and let their home computers do some extracurricular work when the machines would otherwise be idle. The downloaded program was devoted to the notoriously difficult problem of protein folding: determining how a linear chain of amino acids curls up into a three-dimensional shape that minimizes the internal stresses and strains \u2014 presumably the protein's natural shape. If the users wanted, they could watch on a screen saver as their computer methodically tugged and twisted the protein in search of a more favourable configuration. Thousands of people were signing up for Rosetta@home, says Baker, which was gratifying, but not entirely surprising; this kind of digital citizen science had become almost routine by then. It was first popularized in 1999 by the  SETI@home  project at the University of California, Berkeley (UCB), which harnessed volunteers' computers to sift through radio telescope data in search of alien signals. And in 2002, UCB engineers had released a generalized version of the software known as the  Berkeley Open Infrastructure for Network Computing  (BOINC). By 2005, there were dozens of active BOINC projects \u2014 Rosetta@home among them \u2014 and hundreds of thousands of users worldwide. But what was surprising, says Baker, was that the Rosetta@home volunteers quickly began to chafe at the painfully slow progress of their screen saver. \"People started writing in saying, 'I can see where it would fit better this way',\" he says. In retrospect, this should have been obvious: even a small protein can have several hundred amino acids, so computers have to plod through thousands of degrees of freedom to arrive at an optimum energy state. But humans, blessed with a highly evolved talent for spatial manipulation, can often see the solution intuitively. Recognizing an unexpected opportunity, Baker enlisted the help of computer-scientist colleagues. By mid-2008, they had created an interface for Rosetta@home that not only allows users to assist in the computation, but gives them an incentive to do so by turning it into an online game. In the game  Foldit , players compete, collaborate, develop strategies, accumulate game points and move to different playing levels \u2014 all while folding proteins. And it works. This week, Baker and his colleagues publish evidence that top-ranked Foldit players can fold proteins better than a computer (see  page 756 ). By collaborating, these top players often come up with entirely new folding strategies. \"There's this incredible amount of human computing power out there that we're starting to capitalize on,\" says Baker, who is feeding some of the best human tactics back into his Rosetta algorithms. By harnessing human brains for problem solving, Foldit takes BOINC's distributed-computing concept to a whole new level. And it is not alone: several projects are emerging in this field, sometimes called distributed thinking, and the number of publications based on the approach is increasing. \"We're at the dawn of a new era, in which computation between humans and machines is being mixed,\" says Michael Kearns, a computer scientist at the University of Pennsylvania in Philadelphia, who evaluated the concept of distributed thinking as part of an unpublished 2008 study funded by the US Defense Advanced Research Projects Agency. Kearns says that the approach has the most promise in areas such as vision, language and complex logic puzzles \u2014 territories in which humans are expected to retain an edge on computers for some time to come. David Anderson, a UCB computer scientist and the founder of BOINC, admits that the approach is still a long way from becoming mainstream. For many sceptical scientists, he says, \"there's this idea that they're giving up control somehow, and that their importance would be diminished\". But advocates of distributed thinking, such as Fran\u00e7ois Grey, a physicist at CERN, Europe's particle-physics centre near Geneva, have few doubts. Last July, Grey helped to establish the Citizen Cyberscience Centre in Geneva, which aims to promote distributed-thinking projects, especially in the developing world. Grey is currently setting up distributed-computing projects in China. And he has helped to organize a workshop to be held in London this September to encourage scientists to adopt the new approaches. \"The whole field has a funny image, that it is just for fun or for PR,\" says Grey. \"That's what we have to break through.\"  \n                The eye of the beholder \n              Andrew Westphal, a UCB physicist, started on the road to distributed thinking almost two decades ago, when he was a lead investigator on a cosmic-ray experiment called TREK. TREK consisted of specially designed glass plates mounted on the outside of the Russian space station Mir in 1991. Cosmic-ray particles pelting the glass left microscopic traces that were revealed by chemical etching after the TREK detector had returned to Earth in 1995. To find those traces, Westphal automatically scanned and recorded images of the plates using a microscope. But image-recognition software wasn't good enough to identify the tracks, so Westphal found himself staring at image after image, counting the tracks by eye. It was excruciating, he recalls. \"Despite your best efforts, your mind wanders. You start to think about lunch or whatever.\" That reality was on Westphal's mind when he joined NASA's Stardust mission, which was launched in 1999 to collect samples of a comet and return them to Earth. Westphal's focus was not on the comet itself, but on a collecting tray that was exposed to space during the years of cruising required to get there. He and his team were confident that 100 or so microscopic pieces of interstellar dust would burrow into the tray's aerogel, a wispy material designed to decelerate and capture the dust without damaging it. But again, the challenge was to find those particles. Unfortunately, that task made TREK look easy. After the spacecraft's sample-return capsule fell to Earth in January 2006, Westphal reused the automatic imaging microscope from TREK to create 1.6 million images of the aerogel. He estimated that it would take a century for one person to peruse them all. So the following August, Westphal and his team launched  Stardust@home , a continuing project that enlists the pattern-recognition abilities of thousands of volunteer 'dusters'. Although the '@home' name pays homage to BOINC volunteer computing programs, Stardust@home is one of the pioneering distributed-thinking projects. As such, it faced plenty of early hurdles. For example, the dusters had to be given lessons on how to avoid being fooled by cracks in the brittle aerogel or by particles of Earth dust that had embedded in the aerogel from the start. Only some of the volunteers worked diligently. Others quickly slacked off. And still others tried to cheat, just flipping through as many images as possible to rise to the top of a scorecard put in place as an incentive. Westphal, working together with Anderson, realized that they would have to calibrate their volunteers just as they would any instrument. They had to find ways to assign a skill level to each volunteer; to assess how that skill level changes with time; and to determine how many volunteers had to reach the same conclusions about an image before a result could be believed.  \n                Cosmic stardust agents \n              For Bruce Hudson, a resident of Midland, Ontario, Stardust@home was a perfect way to fill the long days. In 2003, he had a stroke that rendered the right side of his body mostly useless. Even computer games weren't much fun. But somehow, the endless microscope photos of aerogel were enthralling. \"I've always liked the stars and the planets and all that kind of stuff,\" says Hudson, who previously worked as a groundskeeper for a Catholic shrine. He estimates that he spent as much as 15 hours a day on the project. His hard work paid off. In March 2010, at the Lunar and Planetary Science Conference in Houston, Texas, Westphal announced that Hudson had found the first probable piece of stardust \u2014 actually a pair of particles in the same track (see  Nature   doi:10.1038/news.2010.106; 2010 ). \"I still can't believe it,\" says Hudson, who named the particles Orion and Sirius. Westphal is already using the unique characteristics of Orion and Sirius to calibrate the expectations of a new generation of Stardust volunteers. Meanwhile, Anderson is reprising what he did with BOINC by generalizing the Stardust@home software, so that it can be used by scientists for other distributed-thinking projects. He calls the result  Bossa , which doesn't stand for anything. \"At some point, I got tired of the idea that everything has to have an acronym,\" he says laconically. Anderson's intention is that Bossa will always be open source and free, so that any scientist can use the software and adapt it to the task at hand. He foresees applications as diverse as in the BOINC ecosystem, where 68 active projects are engaging nearly two million users worldwide. An early recruit is Tim White, a UCB palaeontologist, whose research involves searching for early hominid fossils in the Great Rift Valley of east Africa. For decades, his teams have searched in the same way: slowly. Bent over. Crawling across the dry desert soil in temperatures of up to 50 \u00b0C. But with their planned Bossa-based project, which they intend to call Hominids@home, much of that work could be taken over by volunteers who would look for the white gleam of bone in pictures. \"A kid in front of a monitor isn't going to know the difference between the tooth of a colobus monkey and a baboon,\" says White, \"but they're going to know it's a tooth.\"  \n                Galaxy Zoo \n              The online astronomy project  Galaxy Zoo , which launched in 2007 at the University of Oxford, UK, is taking a rather different tack. Co-founder Chris Lintott says that the project was directly inspired by Stardust@home. \"If people would look at dust grains,\" he says, \"then surely they'd look at our beautiful images of galaxies\" \u2014 images that have been collected in the millions by the international Sloan Digital Sky Survey consortium. The idea is for volunteers to determine by eye whether the galaxies are spiral or elliptical \u2014 a task for which computers are almost worthless. Galaxy Zoo has already published 17 papers after classifying 1.25 million different galaxies, and has just begun another stage of galaxy classification with data from the Hubble Space Telescope. But as Lintott expands his domain to a 'Zooniverse' of projects \u2014 not just for galaxy classification, but for galactic mergers, supernovae, solar storms and lunar craters \u2014 he has been much pickier than Anderson is being with Bossa, where anyone can try anything. Lintott worries that Bossa projects might be hasty affairs that end up wasting the goodwill of citizen scientists. \"Rather than letting anyone pitch for volunteers, we'd like to be a place where people can come and expect a certain level of commitment,\" he says. Anderson, not surprisingly, disagrees. He says he likes the commitment of Galaxy Zoo to distributed thinking, but not its 'walled garden' approach. Galaxy Zoo \"doesn't provide flexibility to the individual scientist\", he says. Baker says that he also drew his inspiration for Foldit from Stardust@home. But any similarities to that program or to Galaxy Zoo end there. For one thing, Foldit players aren't just engaged in basic image recognition and classification tasks \u2014 they are intuitively solving much harder optimization problems. Baker argues that the program is exploiting three uniquely human talents: a superior spatial awareness; an ability to take short-term risks for long-term gain; and the converse, recognizing a dead-end early and knowing when to quit. The other important difference is that the Foldit designers take the gaming element more seriously. Neither Galaxy Zoo nor Stardust has the immersive qualities of Foldit, with its chat rooms, wikis and increasingly difficult levels of play. Zoran Popovi\u0107, Baker's computer-science collaborator at the University of Washington, points out that holding the volunteers' interest is necessary if they are to learn quickly the skills required to make a real contribution. \"It needs to be an exciting, compelling experience that's not always the same,\" says Popovi\u0107. There are also limits to games. If nothing else, says Kearns, as human computing becomes ubiquitous, \"people will no longer marvel at being a part of these networks and may start to feel exploited by them\". The day may come when scientists have to seduce volunteers by doing what many consider anathema at present: paying them. \"There will be a whole economics of this field,' says Kearns. For now, there are still plenty of volunteers who are not jaded. Scott 'Boots' Zaccanelli is one of them. A resident of McKinney, Texas, he splits his time between a day job as a buyer for a valve factory and a personal business \u2014 Good For You Massage Therapy \u2014 that takes him and his massage chair to rodeos, county fairs and flea markets. But he has also been hooked on Foldit since 2008. \"I'm pretty much there every night,\" says Zaccanelli, who has used his undergraduate biology degree to help him rise to a number-6 global Foldit ranking. \"I can look at something and see that it's not right.\" The skills of players such as Zaccanelli are so impressive that Baker has moved past protein folding and is now offering them chances to design completely new proteins. Tasks include searches for new catalysts for photosynthesis, and for proteins that can bind to pathogens such as HIV or the H1N1 influenza virus. One puzzle asked players to create a more stable variant of fibronectin, a protein scaffold that is useful for creating antibody-like compounds. Last October, Baker thought Zaccanelli's design was promising enough to be synthesized in the lab \u2014 the first time a player's recipe had been tested. It turned out that Zaccanelli's fibronectin wasn't any more stable, but Baker says it is just a matter of time before a player designs something that is. And that is a good enough motivation for Zaccanelli. \"Maybe something I do will help contribute an answer to curing cancer or AIDS or the common cold,\" he says. \n                     FoldIt \n                   \n                     Galaxy Zoo \n                   \n                     Stardust@home \n                   \n                     A Nature video on Foldit \n                   Reprints and Permissions"},
{"file_id": "464828a", "url": "https://www.nature.com/articles/464828a", "year": 2010, "authors": [{"name": "Jim Schnabel"}], "parsed_as_year": "2006_or_before", "body": "Almost every human protein has segments that can form amyloids, the sticky aggregates known for their role in disease. Yet cells have evolved some elaborate defences, finds Jim Schnabel. Of all the ways that proteins can go bad, becoming an amyloid is surely one of the worst. In this state, sticky elements within proteins emerge and seed the growth of sometimes deadly fibrils. Amyloids riddle the brain in Alzheimer's disease and Creutzfeldt\u2013Jakob disease. But until recently it has seemed that this corrupt state could threaten only a tiny fraction of proteins. Research is now hinting at a more unsettling picture. In work reported in February, a team led by David Eisenberg at the University of California, Los Angeles, sifted through tens of thousands of proteins looking for segments with the peculiar stickiness needed to form amyloid 1 . They found, says Eisenberg, that \"effectively all complex proteins have these short segments that, if exposed and flexible enough, are capable of triggering amyloid formation\". Not all proteins form amyloids, however. The 'amylome', as Eisenberg calls it, is restricted because most proteins hide these sticky segments out of harm's way or otherwise keep their stickiness under control. His results and other work suggest that evolution treats amyloids as a fundamental threat. Amyloids have been found in some of the most common age-related diseases, and there is evidence that ageing itself makes some amyloid accumulation inevitable. It now seems as though the human body is perched precariously above an amyloidal abyss. \"The amyloid state is more like the default state of a protein, and in the absence of specific protective mechanisms, many of our proteins could fall into it,\" says Chris Dobson, a structural biologist at the University of Cambridge, UK. Several laboratories are now trying to find ways to supplement or boost these protective mechanisms, in the hope of treating or preventing a host of amyloid-linked diseases. \"Advances in understanding amyloids could lead to a powerful new class of medicines for many age-related conditions,\" says Sam Gandy, a neurobiologist and clinician at Mount Sinai School of Medicine in New York.  \n                Fibrils abound \n              The recent work on amyloids has partially confirmed a prediction made 75 years ago by the British biophysicist William Astbury. Proteins start as linear chains of amino acids, but most then fold into complex, three-dimensional, 'globular' shapes. Astbury proposed that almost any globular protein could be made to form dysfunctional fibrils by damaging \u2014 or 'denaturing' \u2014 it with heat or chemicals. By the 1980s, researchers had come to understand that these artificially induced fibrils had the same peculiar structure seen in disease-linked amyloids, such as the amyloid-\u03b2 deposits in the brains of people with Alzheimer's disease. But the wider potential of proteins to naturally form this basic structure was not seen right away. \"The previous paradigm was that the whole protein unfolded and then refolded into a fibrous structure,\" says Eisenberg. By 1999, it was clear that numerous proteins could be made to form amyloids. Dobson proposed that unfolding exposes an essential stickiness in a protein's backbone of amino-acid chains 2 . Researchers were also linking more and more amyloid-forming proteins to disease, including tau proteins in Alzheimer's disease, \u03b1-synuclein in Parkinson's disease, polyglutamine in Huntington's disease, prion protein in Creutzfeldt-Jakob disease and amylin in type 2 diabetes 3 . Eisenberg and his colleagues studied such proteins using fibril-forming assays and X-ray diffraction techniques and found that their tendency to form amyloids came from specific segments within them 4 . These segments are typically about six amino acids long, and can be exposed when a protein partly unfolds. These 'amyloidogenic' segments, Eisenberg's team found, have a self-complementary 'steric zipper' structure that lets them mesh very tightly with an identical segment exposed on another protein 5 . Several of these segments are needed to seed, or nucleate, an amyloid. Segments stack atop one another to form sheets, two of which zip together to form the spine of the fibril. As it grows, the fibril is fringed by the remnants of the segments' host proteins. Eventually, this sprouting fibril breaks to form two smaller fibrils, each of which will grow from both ends again \u2014 and so on. \"The nucleation event may be rare,\" Eisenberg says, \"but once it starts, you can see how it would spread.\" In their study 1 , Eisenberg's team used a computer algorithm to determine when any short protein segment has sufficient steric-zipper-forming potential, based on its predicted three-dimensional structure. After calibrating against known amyloid segments, the team applied the algorithm to the genomes of human, budding yeast and the bacterium  Escherichia coli   and found that about 15% of the short segments coded by genes in these organisms had this property. \"At that rate most proteins contain at least several of these amyloid-prone segments,\" says Eisenberg. The work helps to clarify in a rigorous way why denaturing a protein often pushes it into the amyloid state, says Jeffery Kelly, a structural biologist and amyloid expert at the Scripps Research Institute in La Jolla, California. \"It gives us a better idea of why some proteins have to partially unfold before they can start forming amyloids.\" Eisenberg, Dobson and others have speculated that the self-complementary stickiness of these short segments might have made them useful building blocks in the earliest stages of life on Earth. Moreover, reports have started to emerge of proteins that function normally in the amyloid state, for example some pituitary hormones 6 . \"We know by now of over two dozen native amyloids, so this state is clearly used by biology in a functional way as well as a dysfunctional way,\" says Eisenberg. Even so, says Kelly, these native amyloids \"are all highly regulated\" by, for example, being tucked away inside membrane-bound compartments called vesicles. \"That's why biology can use them and not suffer the consequences.\" Most modern proteins fold into globular structures. But their folding patterns are so complex that they couldn't have evolved by accident. \"If you had a machine that could generate protein sequences randomly, you would only rarely get one that can remain stable in the globular, soluble state,\" Dobson says. Underlying that stability are a variety of evolved mechanisms. When proteins are first synthesized and start to fold, 'chaperone' proteins and related molecules are there to guard against amyloid formation. Other systems are in place to recognize, sequester and destroy amyloids when they do form. The native folded state offers its own strong protection. Eisenberg's group examined more than 12,000 proteins whose folded, three-dimensional structures are already known. They found that 95% of the predicted amyloid-prone segments within them are buried within the structures of their host proteins, and that those that are exposed are too twisted and inflexible to zip up with partner segments 1 . \"It seems that most proteins have evolved to fold in a way that effectively conceals their amyloid-prone segments,\" says Eisenberg. So it may have been unnecessary for evolution to get rid of the segments outright.  \n                Wear and tear \n              Yet all these safeguards amount to a defence line that will inevitably be breached. Some mutations and toxins, and the cellular wear and tear associated with ageing, can result in proteins that are less well folded and less protected by chaperoning and disposal mechanisms \u2014 and thus more liable to become amyloids. \"The 40 or 50 amyloid-associated diseases we've found so far are probably only the ones in which our proteins are the most vulnerable,\" says Dobson. \"If we were to live longer, we might have to contend with more of these conditions.\" By the same token, even a subtle hindrance of amyloidogenesis with drugs might have a major effect on disease and even on ageing in general. \"If we could just enhance the natural protective mechanisms that stabilize a protein,\" says Dobson, \"we could take it back over to the side of the line where it's soluble and stable.\" Amyloids may not be the prime causes of all the diseases in which they have been found, but, typically, some by-product of the amyloid process is suspected. In Alzheimer's disease, many scientists now believe that small and still-soluble forms of amyloid are the most toxic to brain cells. By contrast, the larger, insoluble fibrils \"might even be protective to the extent that they sequester more toxic forms\", says Dobson. The general hope is that by preventing or slowing the initial cascade of amyloid formation, the true 'toxic species' of amyloid will be stopped at its source. One anti-amyloid strategy is to use small molecules as extra chaperones to lower the probability that a protein will expose its amyloidogenic segments. FoldRx, a biotech company based in Cambridge, Massachusetts and founded by Kelly and Susan Lindquist of the Massachusetts Institute of Technology in Cambridge, recently demonstrated this principle in a clinical trial against familial amyloid polyneuropathy, a fatal neurodegenerative disease. Eisenberg says that this strategy is unlikely to work well against most amyloid-prone disease proteins, such as amyloid-\u03b2, because they are typically too small to stay tightly folded. \"For those I think there would be no hope of stabilizing the native structure, because they don't have one,\" he says. Instead, his group is trying to develop compounds to 'cap' the steric zippers of amyloid fibrils, slowing down their formation in the hope that innate clearance mechanisms can then keep up. A third strategy is to boost the activity of these clearance mechanisms \u2014 which, according to work by Kelly's lab, includes enzymes that specifically disaggregate amyloids 7 . \"There's a group of 500\u2013600 genes that protect us when we're young, even if we've been so unlucky as to inherit, for example, a predisposing Parkinson's or Alzheimer's mutation,\" he says. Finding ways to rejuvenate that system \"is what almost our whole lab is working on these days,\" says Kelly. \n                     Alzheimer's disease \n                   \n                     David Eisenberg \n                   \n                     Chris Dobson \n                   Reprints and Permissions"},
{"file_id": "464826a", "url": "https://www.nature.com/articles/464826a", "year": 2010, "authors": [{"name": "Matt Ford"}], "parsed_as_year": "2006_or_before", "body": "The explosion in commercial archaeology has brought a flood of information. The problem now is figuring out how to find and use this unpublished literature, reports Matt Ford. Archaeologists are used to gathering data by scratching in the dirt. But when Richard Bradley set out to write a new prehistory of Britain in 2004, he unearthed his most important finds while wearing sandals and a sweater rather than work boots and a hard hat. Bradley is one of a growing number of academics in the United Kingdom who are doing their digging in the masses of unpublished 'grey literature' generated when commercial archaeologists are brought in to excavate before any sort of construction. Bradley, a professor at the University of Reading, travelled around the country, visiting the offices of contract archaeological teams and local planning officials. There, he unearthed dozens of reports showing that settlements in England had remained strong during the Bronze Age and had not suffered a population crash, as academics had long thought. \"I became aware that what I was teaching would be out of date without looking at the grey literature,\" says Bradley. For the past 20 years, Britain has been at the centre of a revolution in the funding and practice of archaeology. The shift was spurred by a 1990 change in policy that requires local governments to consider how construction projects will affect archaeological remains. That policy has essentially forced public and private entities to pay for archaeological assessments before they start laying a road, constructing an office building or engaging in other projects that disturb the ground. In many ways the law has achieved its aim, helping to preserve relics that otherwise would have been destroyed. But at the same time, it has created problems for academics, who have struggled to keep up with the avalanche of new data, which some argue are hard to access. Similar concerns have emerged in other countries that have enacted equivalent laws. But it's in the crowded British Isles \u2014 with its densely packed archaeological record and rapid pace of development \u2014 where the effect has been particularly profound. \"There is such a vast body of untapped stuff out there,\" says Barry Cunliffe, an emeritus professor of European archaeology at the University of Oxford. \"This means there is a hold-up in academic development and the way in which the public are able to understand and appreciate archaeology.\" The contractors disagree. Commercial work now accounts for 93% of the archaeological research done in the United Kingdom, and academics must take note of the data generated by contract units, says Kenneth Aitchison, head of projects and professional development at the Institute for Archaeologists, the body representing commercial archaeologists in Britain. \"This is the mainstream,\" he says. \"The ones who complain are missing out.\" Academic archaeologists are used to a system in which researchers conduct excavations and then publish their observations in monographs and journal articles, which are then available in libraries. But now the results of most excavations get written up for clients and local government planners, and are then held in private offices or local government buildings.  \n                Skeleton in the closet? \n              Some academics, such as Bradley, are thankful for the new source of data and have responded by working more closely with commercial units. But others argue that fundamental changes are necessary to prevent market forces from letting down the archaeology. \"Where there is a real limitation is that the reports aren't necessarily publicly available,\" says Cunliffe. \"It ought to be made mandatory that all these reports should be made available to the public. Sometimes a unit may say 'I'm sorry, my client is not prepared to make such and such a report public'.\" Cunliffe says that his research, such as that on Iron Age Britain, has been affected by difficulties obtaining grey literature. Because these reports are not held in libraries, they are unavailable as inter-library loans, making it necessary to travel to read them. \"To go through all the records in all the units across the country would have taken years to do and just wasn't feasible in the context of writing a book,\" he says. \n               boxed-text \n             But that is where an increasing amount of archaeological information is stored (see 'Careers in ruins'). Statistics are limited, but it is estimated that in 2003\u201304, private developers sponsored the vast majority of UK archaeology, spending \u00a3144 million (US$220 million), compared to around \u00a319 million spent by the central government and the European Union, and around \u00a325 million by local governments 1 .  \n                Knowledge is power \n              Cunliffe's views are not unique. Another vocal critic has been Gary Lock, also at the University of Oxford. He wrote in the magazine  British Archaeology   in 2008 that while he was studying part of Oxfordshire, several developer-funded projects had been carried out nearby and he had been unable to access the reports. \"Archaeological information is being treated as a commodity to which developers control access,\" he wrote 2 . Even if academics can locate reports, that doesn't necessarily resolve all their concerns. \"I know that some bits of grey literature I have seen are barely worth the paper they are printed on,\" says Cunliffe. Commercial reports contain broadly the same sort of data as academic reports \u2014 interpretations of features, finds and chronology. But the reports are composed in response to planning, as opposed to research questions. There have also been complaints about the quality of some commercial archaeological work. Representatives of that industry, however, argue that commercial units are advancing archaeology and that academics must keep pace. The grey literature, \"does what it is supposed to do, and it is essentially accessible\", says Aitchison. Aitchison argues that the issue is not one of access, but rather of awareness, attitude and understanding. The major archaeological contractors \u2014 including Oxford Archaeology and Wessex Archaeology in Salisbury \u2014 are run as charitable trusts with educational aims. They forge links with universities and are working to get their grey literature online, says Aitchison. More material is published on the Internet than ever before, and the situation has improved massively in the past few years, he adds. Laws requiring developers to fund archaeology operate to varying extents across Europe and North America, with similar debates taking place wherever they are applied. \"I reject the concept that grey literature is unpublished,\" says Deni Seymour, who worked in US contract archaeology for more than 25 years and for a decade co-ran the Lone Mountain Archaeological Services in Albuquerque, New Mexico. Grey literature, she says, \"is no less available than many obscure journals and master's theses\". According to Seymour, \"some academics and mainstream researchers think they are above looking at contract work and they don't value it. In reality, I think many of them are five or more years behind with respect to new discoveries, concepts, method and theory.\" Although opinions are strong on both sides about working practices and management issues, a consensus is emerging that grey literature contains important information and must be used. \"It is time for all scholars to engage their colleagues more widely and meaningfully, and to bridge the divide between the academic and the professional sectors,\" says Seymour, echoing many of her colleagues in and outside universities. Ireland is showcasing a potential way to make the grey literature more easily available. A government-sponsored programme called Irish National Strategic Archaeological Research funds projects to synthesize grey data. In England, the Archaeological Data Service in York and Bournemouth University's Archaeological Investigations Project are working to put grey literature online. But finding money in the teeth of a recession, with UK universities already facing massive cuts, seems unlikely. Academics who have dug into the grey literature say it can transform ideas about the past. Bradley's work, for example, turned the standard view of late Bronze Age Britain and Ireland on its head. In the late 1980s, academics had concluded that the population in the British Isles had dropped markedly during the late Bronze Age. But since then, professional archaeologists have unearthed so many settlements from that period that \"no one mentions a population decline any more\", says Bradley.  \n                The other Roman Britain \n              Michael Fulford, one of Bradley's colleagues at the University of Reading, has been piloting a study of the grey literature about Roman Britain, with similarly exciting results. \"We've almost found 'another Roman Britain',\" he says, \"one that we would have never seen without developer-funded archaeology.\" Previously British Roman archaeology had tended to be biased towards excavating high-status sites such as villas, as these were what researchers had chosen to investigate. But commercial excavations happen wherever developers are planning to break ground, and so provide a wider sampling of the past. By embarking on a \"massive photocopying campaign\", Fulford assimilated huge amounts of data, representing a massive increase in both the number and type of sites now known. His study revealed the other side of Roman society. The low-status rural settlements showed how indigenous communities coexisted with Roman invaders, by keeping much of their vernacular architecture, but furnishing their homes with Roman manufactured goods. \"A lot of the best work is coming out of commercial units now \u2014 a lot of the worst is as well, but you can say that about universities, quite frankly,\" says Fulford. He advises PhD students who want to keep their hand in fieldwork that they might be better off working in commercial archaeology because it often involves large projects that are properly funded. \"A lot of my contemporaries feel disenfranchised, but then that's too bad,\" says Fulford. \"Despite the difficulties, we have to adapt to an archaeological record that is massively expanded and, at its best, of far better quality than has been achieved by academics, who are often very part-time fieldworkers.\" \n                 Matt Ford is a freelance writer in Bath, UK.  \n               \n                     Institute for Archaeologists \n                   \n                     The Council for British Archaeology \n                   \n                     INSTAR \n                   \n                     The Archaeological Data Service \n                   \n                     Archaeological Investigations project \n                   Reprints and Permissions"},
{"file_id": "4641122a", "url": "https://www.nature.com/articles/4641122a", "year": 2010, "authors": [{"name": "Brendan Borrell"}], "parsed_as_year": "2006_or_before", "body": "After years of wrangling over the chemical's toxicity, researchers are charting a new way forwards. Brendan Borrell investigates how the debate has reshaped environmental-health studies. In her 25 years of research, Gail Prins, a reproductive physiologist at the University of Illinois in Chicago, had got used to doing science her way. But when her experiments started to question the safety of bisphenol A (BPA), a chemical found in thousands of consumer products from food-can linings to baby bottles, she found her work under a new level of scrutiny. The experience was unnerving, she says. \"I feel I do solid science.\" Even federal evaluators in the United States agreed that her work was suitable for informing decisions about BPA's safety \u2014 at least at first. A study published in early 2006, for example, helped explain how early exposure to BPA could increase rats' susceptibility to prostate cancer 1 . The work complemented a growing body of research suggesting that the chemical posed several developmental and cancer risks (see 'Hazard warning'). That December, when a panel on reproductive health drafted a report on BPA for the US National Toxicology Program (NTP) it determined that Prins's study \"makes important contributions and is suitable for the evaluation process\". But the following year, the NTP's final report discounted Prins's study. The chemical industry had stepped in to make its views heard. BPA grosses some US$6 billion a year for the five companies that produce it in the United States. Steven Hentges, who works on BPA for the American Chemistry Council, the industry trade group, wrote a 93-page letter to the NTP panel on 2 February 2007, detailing what he perceived as flaws in a slew of studies coming out of academic laboratories. Prins's study came under attack for injecting the chemical under the rats' skin, rather than giving it orally, as humans would generally be exposed. Hentges found flaws in 60 of the roughly 80 studies that the panel found to be \"adequate\", \"useful\" or \"suitable\" for evaluating BPA's reproductive and developmental effects. Following Hentges's critique, the percentage of non-industry-funded studies deemed adequate for informing policy dropped from 70% to 30%. Most of those that remained found the chemical to be safe. Three years on, the debate over BPA's potential for harm is unresolved. Canada and Denmark have banned the chemical's use in baby bottles, toys and other products for infants. And manufacturers and retailers worldwide have begun to limit its use in response to mounting consumer concerns. For researchers, though, the issue exposes a growing gulf between basic research and the regimented world of toxicity testing. \"They are very different systems that serve different purposes,\" says Richard Denison, a chemical-risk analyst in the Washington DC office of the Environmental Defense Fund, a non-governmental organization. Scientists such as Prins were among the first to highlight concerns about ubiquitous environmental toxins, but because they were not specifically aiming to test toxicity, they were easy targets for industry. Consequently, the dispute over BPA has had an unexpected outcome: it is shaping the way studies will be funded and conducted in academic labs. To bridge the divide, the US National Institute of Environmental Health Sciences (NIEHS) in Research Triangle Park, North Carolina, is bringing researchers together to create greater collaboration and a more rigorous, integrated body of research on BPA that can compete on an equal footing with the industry-sponsored studies. Endocrinologist Frederick vom Saal of the University of Missouri-Columbia, a pioneer in the study of BPA and one of those funded by the new NIEHS programme, calls it the \"BPA master experiment\". The new approach is already being adopted for other types of toxicity investigation. \"If we didn't start doing business a little differently,\" says Jerry Heindel, the BPA programme manager at the NIEHS, \"we might not have the answers we need.\"  \n                Toxic science \n              The gap between the methods used in industry and those of basic researchers can be traced back to 1993, when vom Saal and his colleagues showed how organisms can be exquisitely sensitive to tiny amounts of hormone-like chemicals during development. These chemicals bind to the same receptors as hormones such as oestrogen and thus mimic their effect, potentially disrupting development. Vom Saal and his colleagues christened them 'endocrine disruptors' 2 . It was no secret that BPA was a potential endocrine disruptor. Scientists explored its effects on fertility in the 1930s, because of its similarities to oestrogen. It was abandoned for more powerful chemicals and ultimately found a use in making shatterproof polycarbonate plastic and epoxy resins used to coat metals. The FDA approved its use under food-additive regulations in the early 1960s. In the late 1990s and early 2000s, researchers began to amass evidence that BPA leaching from such products was acting as an endocrine disruptor, even if the mechanism remained unclear. Ana Soto, a cell biologist at Tufts University School of Medicine in Boston, found that low doses of BPA alter the development of mammary glands in mice and could lead to cancer 3 . Other researchers found a link in mice between BPA and hyperactivity and a heightened sensitivity to illegal drugs. In Prins's work, BPA had an effect on rat prostates similar to that of an injection of oestradiol, the body's main oestrogen. A mechanism for BPA's action was taking shape. For Prins's control animals, injected with corn oil, the ageing process naturally silenced the expression of a gene linked to development of cancers. Animals injected with corn oil and BPA however, had the gene locked in the 'on' position and were more than twice as a likely to develop pre-cancerous lesions. Researchers have learned that the chemical is working on an epigenetic level \u2014 modifying gene expression, but not sequence, over a long period. Yet the academic goal of such work \u2014 to uncover and explore biological mechanisms \u2014 was quite different from those of guideline studies designed to evaluate chemical safety. Therein lies the room for contention. Hentges's letter found bones to pick with the dozens of studies it attacked. Some used sample sizes that were too small; some had only looked at a single dose level; some had failed to carry studies through to the measurement of disease or dysfunction, stopping at surrogate endpoints. Some criticisms were method-specific. For example, Hentges said that an enzyme-based assay to detect concentrations of BPA in blood was not specific to BPA and can overestimate its levels. Academic researchers had also identified similar methodological problems in the published BPA literature, but they generally did not regard them as fatal flaws. A common general criticism from Hentges, however, was that none of these studies were conducted according to Good Laboratory Practice (GLP), part of the testing guidelines developed by regulators around the world, outlining basic standards for equipment calibration and the storage of raw data. In general, when called on by federal regulators to test the safety of a substance, the chemical industry has relied on private labs such as RTI International in Research Triangle Park to carry out guideline studies using GLP. Academic researchers rarely conduct such studies, but in their deliberations about chemical safety, federal agencies are expected to examine all the evidence, GLP or not. Hentges says non-GLP studies should be given lesser weight. \"There's certainly some role for academic studies in generating hypotheses, but they don't provide what regulators need to draw conclusions.\" But compared to scientific protocols, which evolve continuously, guideline standards advance in fits and starts, because adding new procedures requires a lengthy period of comment, revision and validation. The US Environmental Protection Agency (EPA) set limits for acceptable human exposure to BPA in the late 1980s. It set up a programme on endocrine disruption in 1998, but it took until October 2009 for methods to be sufficiently agreed on to request a first round of tests, and some say tests are still inadequate. This panel was the most dysfunctional thing I've ever sat on. ,  \"This was the most dysfunctional thing I've ever sat on,\" says biologist Theo Colborn, who has been on the programme committee for its duration and runs the Endocrine Disruption Exchange in Paonia, Colorado. In the early days, she says, the science of endocrine disruption wasn't ready for standardized testing, but today, although the science is stronger, the tests EPA is requesting are inadequate. \"A chemical like BPA could easily be missed in the assays they have selected,\" she says. Once the stakeholders agree on a procedure, it must be validated, which is, in a sense, a substitute for replication. Validation involves multiple contract laboratories performing the same procedure and coming back with a consistent result. But according to Thomas Zoeller, an endocrinologist at the University of Massachusetts, Amherst, well-established and reproducible scientific techniques may have trouble getting validated when contract laboratories can't perform the procedures.  \n                Inappropriate tools \n              At the EPA's request, Zoeller reviewed the raw data from three contract labs asked to measure thyroid hormone levels, and found that they could not conduct radioimmune assays that have been available for more than 40 years. \"The problem is the assay is difficult,\" says Rochelle Tyl, a toxicologist at RTI International, which was part of the validation process. \"If experienced labs can't run the assay how can we put it as a guideline?\" If experienced labs can't run the assay how can we put it as a guideline? ,  That's exactly the point, Zoeller and other critics argue: contract labs may not be able to apply the appropriate tools. Academic scientists studying BPA are convinced that their techniques are better for understanding the dangers of oestrogen mimics. Prins replied to Hentges's charges in her own comment to the NTP panel, one of a handful of researchers to do so. In her letter, she noted that \"by selectively eliminating data collected from non-oral routes of administration, the committee has introduced a significant bias in the process\". She says that the key factor was the level of BPA circulating in the bloodstream of her rats, estimated to be about 10 parts per billion and consistent with amounts generally found in humans. Such calculations aren't even required in guideline studies. \"Most academic scientists have quality-control standards that are above GLP standards,\" says Prins, who does run a GLP-compliant clinical lab. But getting certified is expensive, time-consuming and generally unnecessary for research goals. Zoeller, vom Saal, and 34 other researchers published a commentary in  Environmental Health Perspectives   last year, arguing that regulatory agencies should give no greater weight to GLP-guideline studies than rigorous, replicated peer-reviewed research 5 . Zoeller admits it is a tough balance. \"Regulatory agencies shouldn't have to run around every time some academic lab finds something, somewhere with some esoteric technology,\" he says. \"What we have to figure out is how to invest modern science in regulatory toxicology and make it work.\"  \n                A new direction \n              In 2008, in an attempt to close the holes poked in academic research on BPA, the NIEHS made plans to direct US$30 million to BPA research in 2010 and 2011, including $14 million in stimulus funds. In the past, the agency would typically put out a request for grant applications in a general area, fund researchers in different areas, and let them go off and conduct their research. This time, the NIEHS was going to do things differently. \"We may fund the best research by the best investigators, but doing that doesn't guarantee we'll fill all the data gaps that need to be filled,\" says Heindel. So last October, more than 40 scientists working on BPA, including Prins, arrived at the agency's campus in Research Triangle Park. The meeting alerted researchers to technical issues in working with BPA, and also allowed them to reach a consensus on certain study protocols. To counter concerns about the assays used to detect BPA levels in blood, Antonia Calafat, an analytical chemist at the Centers for Disease Control and Prevention in Atlanta, will do the lion's share of chemical analysis using mass spectrometry, calibrated with a BPA isotope not found in the environment. At the meeting, Calafat also advised researchers to include 'blanks' of distilled BPA-free water to prove that experiments were free from contamination, and to test food to ensure it contains minimal levels of plant-based oestrogens. Researchers also discussed the use of positive controls, such as the hormone oestradiol, to prove that experiments that fail to find a response to BPA actually work. Let's learn from the lessons of BPA and start developing collaborations and interactions to move the field right from the start. ,  The NIEHS is also encouraging researchers to use consistent BPA doses across labs, to measure additional variables such as markers related to diabetes, and to exchange tissues. Soto, for example, studies female development, but she will raise male and female mice during her experiments. She will send the males' prostates to Prins in Chicago. In addition, the Food and Drug Administration's National Center for Toxicological Research in Jefferson, Arkansas, will run animal studies as a GLP backbone for the entire project, making tissues and animals available to external grantees. To help grantees take advantage of these collaborations, the NIEHS offered them supplements of up to $50,000 per year. Heindel says that the agency has already adopted the same approach for a cohort of grantees beginning studies of nanomaterial safety this spring. \"Let's learn from the lessons of BPA and start developing collaborations and interactions to move the field right from the start,\" he says. Hentges says he is \"encouraged that the research will be moving towards studies that will really be helpful to assess the safety of BPA\". Although many of the funded researchers were quick to praise the programme, it also sometimes means that they will be doing things they don't necessarily agree with. For example, vom Saal takes issue with stopping the enzyme-based assay for BPA concentrations. The test costs several dollars, compared with the $150 for the more sophisticated methods the NIEHS now requires. He says that enzyme-based assays are only problematic for researchers not familiar with their nuances. However, he is willing to adopt the more expensive technique if it means producing unassailable results. \"If you are going to spend millions of dollars on new research, then it's best not to open it up to criticism,\" he says. Prins, like vom Saal, disputes the relevance of administering the chemical orally as opposed to under the skin, but they all agreed in October to conduct some of their experiments by feeding BPA to animals, as the chemical industry has insisted. \"I can't particularly say I like people telling me how to conduct my research,\" Prins says. \"But it's very important that things are done consistently between investigators if we are going to move the field forward.\"   See Editorial,  page 1103 \n . Brendan Borrell is a freelance writer in New York City. \n                     National Toxicology Program Center for the Evaluation of Risks to Human Reproduction \n                   \n                     NTP CERHR Final report on BPA \n                   \n                     US Food and Drug Administation update on BPA \n                   Reprints and Permissions"},
{"file_id": "4641118a", "url": "https://www.nature.com/articles/4641118a", "year": 2010, "authors": [{"name": "Josie Glausiusz"}], "parsed_as_year": "2006_or_before", "body": "A conduit from the Red Sea could restore the disappearing Dead Sea and slake the region's thirst. But such a massive engineering project could have untold effects, reports Josie Glausiusz. Standing at the rocky shore of the Dead Sea, Itay Reznik raises his arms as high as they will go. Suspended in the air about a metre-and-a-half above his fingertips is a dock to nowhere. \"When I started my PhD in 2007, we could sail from this dock,\" says Reznik, a graduate student in geology at Israel's Ben-Gurion University of the Negev in Beer-Sheva. Now the dock dangles more than three metres above the water. This super-salty lake on the border between Israel and Jordan is the lowest spot on Earth's surface, and it is getting lower each year. Over the past 50 years, the water level has dropped by almost 30 metres; recently the loss has accelerated to an average of 1.2 metres per year. The Dead Sea's surface area has shrunk by almost one-third over the past 100 years. In this desert region, more water evaporates from the sea than enters it. The Jordan river once fed the sea with 1.3 billion cubic metres of fresh water per year, but that has shrivelled to less than 100 million cubic metres, most of which consists of agricultural run-off and sewage. Israel, Syria and the Kingdom of Jordan take the river's water for drinking and agriculture. At the southern end of the sea, Israel's Dead Sea Works and Jordan's Arab Potash Company exacerbate the problem by evaporating the mineral-rich water to extract potash and magnesium.  Without action, the Dead Sea will continue to shrink. But a proposal being evaluated by the World Bank could revive the lake with a 180-kilometre-long conduit carrying water from the Red Sea 400 metres downhill to the Dead Sea through a canal, pipeline or some combination of the two. The water's flow would generate electricity to run a desalination plant, providing drinking water for local people \u2014 as much as 850 million cubic metres of water annually, the equivalent of just under half of Israel's current consumption (see 'Saving the Dead Sea'). \n               boxed-text \n             The concept of a Red\u2013Dead canal goes back to 1664, when Athanasius Kircher, a German Jesuit scholar, envisioned it as part of a regional network of transportation canals. Similar schemes have been revived and abandoned over the years, most notably following the 1973 energy crisis, when Israel considered building a hydropower plant on a canal linking the Mediterranean and the Dead Sea. Lately, however, the Red\u2013Dead plan has gained momentum, mainly because of Jordan's desperate need for drinking water, and because of a desire by Israel, Jordan and the Palestinian Authority to collaborate on a 'peace conduit'. The three governments have developed shared goals for the project and in 2005 they jointly asked the World Bank to investigate its feasibility and environmental impacts. \"It's the only place where Israel, Jordan and the Palestinian Authority are publicly working on a project together,\" says Alex McPhail, lead water and sanitation specialist at the World Bank, who is overseeing the bank's study programme of the plan. The Dead Sea is the only place where Israel, Jordan and the Palestinian Authority are publicly working on a project together. ,  Environmentalists aren't so keen. Friends of the Earth Middle East (FoEME) an Israeli\u2013Jordanian\u2013Palestinian advocacy group with branches in Tel Aviv, Amman and Bethlehem, has questioned the environmental effect of the conduit, which would cost billions of dollars to build. The intake pipe would draw up to 2 billion cubic metres of water each year from the Red Sea's Gulf of Aqaba, which would have an unknown effect on the sea's 1,000 or so species of fish and 110 species of reef-building coral. And the conduit would run through the Arava Valley, a haven for rare gazelles, hyrax and hares. The valley is also lined with a seismically active fault that could damage the water system. Most significantly, environmentalists argue that this expensive and potentially harmful project is unnecessary, and that Israel and Jordan could at least partially restore the River Jordan by conserving more of their water resources. Reznik and other researchers in the region are busy resolving these and other issues. Their results will feed into the World Bank's final report in 2011, which will help to determine whether Israel, Jordan and the Palestinian Authority pursue the project, and whether it will attract funding. The decline of the Dead Sea is obvious on a drive through the region with Reznik and his PhD supervisor, Jiwchar Ganor, a geologist at Ben Gurion. At Ein Fashkha, a nature reserve and freshwater spring on the northwestern shore, small placards mark the shore line in 1968 and 1984. The first is now about two kilometres from the lake; the second sits forlornly beside a set of crumbling stone 'stairs to the sea', which is now nowhere to be seen.  \n                A muddy mess \n              The lake's retreat has left a wasteland of exposed sediment, which is so salty that few plants can grow. Freshwater springs once fed oases of palm trees and other plants along the former lake shore but the springs' outlets have migrated downhill into the muddy zone along the current shore. Environmentalists worry that the decline of the oases will harm migrating birds that stop in the region to fatten up before crossing the Sahara Desert. Infiltration of fresh water has also dissolved salt layers in the sediments, leading to the formation of some 3,000 sinkholes. The retreat has also taken a toll on people because the Dead Sea is a tourist destination and home to several farming communities. Its retreat has halted efforts to build hotels and other amenities, and the sinkholes have undermined roads and bridges, and harmed agriculture. The Dead Sea will probably never vanish completely \u2014 as its surface shrinks, its salinity increases and evaporation slows. \"The Dead Sea will not die,\" says Ittai Gavrieli, acting director of the Geological Survey of Israel in Jerusalem, who is leading a series of modelling studies on the impact of the proposed conduit. But if nothing changes, the lake is likely to drop a further 100\u2013150 metres from its current level of 423 metres below sea level, Gavrieli says. Aside from arresting further ecological damage, the conduit could provide crucial help for the Kingdom of Jordan, says Mousa Jama'ani, secretary-general of the Jordan Valley Authority in Amman. Jordan is one of the world's poorest countries in terms of freshwater resources. The Gulf States are even more parched, but they use their oil for electricity production to power the desalination of seawater. \"Here in Jordan, there is no oil, also no water,\" Jama'ani says. A desalination plant on the Red\u2013Dead conduit would supply some of that badly needed drinking water to Jordan. \"The population here increases and increases, water resources are limited, and demands increase,\" says Jama'ani. \"What we can do? The government has a responsibility to the people.\" The desalination plant would also supply clean water to the Palestinians, says Shaddad Attili, head of the Palestinian Water Authority in Ramallah. Palestinian water sources are limited to a mountain aquifer beneath the West Bank (part of which borders the Dead Sea) and a coastal aquifer \u2014 heavily contaminated with seawater and sewage \u2014 that supplies Gaza. \"Palestinians haven't had access to the Jordan river basin since 1967,\" Attili says. Nor are they permitted to develop the northwestern shore of the Dead Sea. Attili believes that the agreement to cooperate on the Red\u2013Dead project with Israel is a big achievement \u2014 a view reiterated by Uri Shor, spokesman for the Israeli Water Authority. Israel obtains about one-third of its water from the Sea of Galilee and most of the rest from underground aquifers; it also desalinates some 165 million cubic metres of sea and brackish water per year, about 9% of the country's annual consumption of 1.8 billion cubic metres. \"Such a project is a platform for international co-operation, and therefore it's in our interest as well,\" Shor says. In 2008, with US$16.7 million in aid donated by eight countries, including the United States, France and Sweden, the World Bank launched a study programme to examine the feasibility of constructing the Red Sea\u2013Dead Sea Water Conveyance and its social and environmental impact. The programme has released a series of interim reports 1 ,   2  over the past 18 months, examining such factors as the best route for the conduit (most likely through Jordanian territory) the form it will take (canal, tunnel, pipeline or some combination thereof) the type of intake on the Red Sea, where to site the pumping stations, desalination plant and hydropower facility, and the allocation of desalinated water. The World Bank also recently started two studies looking at the impact of the conduit on both the Red Sea and the Dead Sea. Last October, under pressure from FoEME, the World Bank initiated a 'study of alternatives', conducted by a trio of British, Jordanian and Israeli experts to examine other options, such as building a water pipeline from Turkey or restoring the flow of the River Jordan. One important factor is the risk of earthquakes. The Dead Sea Fault, an active seismic zone, runs the length of the Arava Valley and forms the border of two tectonic plates. According to the interim feasibility study 1  commissioned by the World Bank, there is high risk of a major earthquake occurring within the operating life of the project, endangering pumping stations and the desalination plant. Gavrieli says that such damage could be forestalled if geologists can identify the fault location and engineers design the conduit and related facilities well enough. \"If you know where the fault line is, you prepare for it; you reinforce it, you allow for some flexibility,\" he says. The World Bank reports also examine other potential hazards of the project. For example, pumping water from the Gulf of Aqaba could change currents, damaging corals or sea grasses. Sea water could leak from the conduit and contaminate groundwater. It could harm wildlife and archaeological sites in the Arava Valley, including ancient settlements, aqueducts and reservoirs, copper smelting sites and cemeteries. The Arava ecosystem potentially faces further threat from a plan by the Israeli real estate billionaire Yitzhak Tshuva to build a 'valley of peace' along the conduit, a Las Vegas-style city filled with parks, lakes, waterfalls, hotels and a botanical garden.  \n                Extreme inhabitants \n              The Dead Sea itself could be transformed, with unknown long-term consequences. Despite its name, the sea is home to a variety of microorganisms, including a salt-tolerant unicellular green alga called  Dunaliella   and red Archaea from the family Halobacteriaceae.  Dunaliella   thrives when the sea is slightly diluted, as in rainy years such as 1992, when the lake level rose by two metres. A new conduit could also stimulate growth of the alga because the Dead Sea is much saltier than either the Red Sea or brine from a desalination plant. Experiments conducted at the Dead Sea Works by Gavrieli and Aharon Oren, a microbiologist at the Hebrew University of Jerusalem show that such an influx of less-salty water would trigger algal blooms. The effect would be enhanced by the addition of phosphate-based fertilizer, which enters the Dead Sea from the River Jordan 3 . A bloom of  Dunaliella   would in turn feed Archaea that could turn the sea red, says Gavrieli. Whether that is a problem is a matter of opinion. \"What's the big deal if the Dead Sea is red?\" Gavrieli asks. \"What's worse, having it drop, or having it red?\" What's the big deal if the Dead Sea is red? What's worse, having it drop, or having it red? ,  On the other hand, a change in the Dead Sea's chemistry might turn the surface water white. Currently, the sea is supersaturated with gypsum, a form of calcium sulphate, which barely precipitates because the kinetics of the reaction are too slow 4 . But gypsum will precipitate into white crystals if Red Sea water, which contains ten times more sulphate, is pumped into the Dead Sea as reject brine from a desalination plant. In a ten-cubic metre tank at the Dead Sea Works, Reznik, Ganor and Gavrieli have mixed equal parts of Dead Sea brine and desalination reject brine, and seen little white gypsum crystals bob on the surface. The large-scale, long-term consequences of such a change are unknown, says Gavrieli. Gypsum might sink to the lake bed, or it might form crystals that remain suspended in the upper water layer, turning it milky. A film of crystals on the surface could make the Dead Sea more reflective and slow evaporation; but if they remain in the top of the water column, the gypsum clumps could scatter light within the Dead Sea, raising its temperature and increasing evaporation. The Dead Sea Works and the Arab Potash Company are important players in the fate of the Sea, as their evaporation ponds account for some 30\u201340% of the water-level decline. According to the Dead Sea Works, their evaporation ponds help the region by preserving the southern end of the Dead Sea, which dried up in 1977. They also provide employment for some 1,000 workers and support a large complex of hotels at Ein Bokek. But Gidon Bromberg, Israeli director of FoEME, is not impressed. The potash companies, he says, could switch to extracting minerals by forcing Dead Sea water through membranes under high pressure. That would take more money and energy, but it would cause significantly less water to evaporate. A set of studies recently completed by FoEME suggests that countries in the region could restore 400 million to 600 million cubic metres of water per year to the River Jordan, at a smaller cost than desalination, by managing demand through measures such as switching to compost or vacuum toilets, or flushing toilets with 'grey' water recycled from the shower. FoEME claims that more sustainable farming practices could also conserve water.  \n                Working together \n              In the end, it will be the need for desalinated water that is most likely to drive the conduit's construction. Despite the considerable political tensions, all three governments need to cooperate, says Attili. \"We don't have another choice,\" he says, because future generations will depend on new sources of water. At the moment, the future looks uncertain in the desert beside the Dead Sea. At the end of a long, dusty day, Ganor and Reznik drive high above the lake to a rock bearing a faint red line, which marks the water level measured by British surveyors between 1900 and 1913. The Dead Sea now lies 35 metres below. If nothing is done, the situation will only get worse, but a Red\u2013Dead conduit would carry with it some real risks. The decision to stop the sea's decline, says Gavrieli, \"is a matter of choosing between bad and worse. But the question is, what is bad and what is worse?\" Josie Glausiusz is a freelance journalist in New York City. \n                     Nature Geosciences \n                   \n                     The World Bank Overview of the Red Sea-Dead Sea Water Conveyance Study Program \n                   \n                     Friends of the Earth Middle East web site \n                   \n                     Geological Survey of Israel \n                   \n                     Ben Gurion University of the Negev \n                   \n                     Wadi Arabah Project \n                   Reprints and Permissions"},
{"file_id": "4641262a", "url": "https://www.nature.com/articles/4641262a", "year": 2010, "authors": [{"name": "Jeff Tollefson"}], "parsed_as_year": "2006_or_before", "body": "Hydrogen fuel-cell vehicles, largely forgotten as attention turned to biofuels and batteries, are staging a comeback. Jeff Tollefson investigates. \"The first car driven by a child born today could be powered by hydrogen and pollution-free,\" declared former US president George W. Bush in 2003, as he announced a US$1.2-billion hydrogen-fuel initiative to develop commercial fuel-cell vehicles by 2020. The idea was appealing. Ties to foreign oil fields would be severed, and nothing but water vapour would emerge from such a vehicle's exhaust pipe. Congress duly approved the money, and the Department of Energy and other research agencies got to work. But then the whole effort faded into obscurity, as attention shifted first to biofuels and then to battery-powered electric vehicles. Both seemed to offer much quicker and cheaper routes to low-carbon transportation. The shift seemed complete when the US Secretary of Energy Steven Chu entered office last year. Chu outlined four primary pitfalls with the hydrogen initiative. Car manufacturers still needed a fuel cell that was sturdy, durable and cheap, as well as a way to store enough hydrogen on board to allow for long-distance travel. Hydrogen also required a new distribution infrastructure, and even then the greenhouse-gas benefits would be marginal until someone worked out a cost-effective way to make hydrogen from low-carbon energy sources rather than natural gas. Last May, four months after being sworn in, Chu announced that the government would cut research into fuel-cell vehicles in his first Department of Energy budget. Biofuels and batteries, he said, are \"a much better place to put our money\". The move came as a relief to the many critics of hydrogen vehicles, including some environmentalists who had come to see Bush's hydrogen initiative as a cynical ploy to maintain the petrol-based status quo by focusing on an unattainable technology. \n               boxed-text \n             But the budget proposal served only to energize the supporters of hydrogen vehicles, and it became clear during subsequent months that the debate was far from over. The same car manufacturers who were investing so heavily in biofuels and batteries felt that hydrogen fuel cells had a long-term potential that they could not afford to ignore. The hydrogen lobby was so effective that Congress eventually voted to override Chu and restore the money. Then on 9 September in Stuttgart, Germany, nine major car manufacturers \u2014 Daimler, Ford, General Motors, Honda, Hyundai, Kia, Renault, Nissan and Toyota \u2014 signed a joint statement suggesting that fuel-cell vehicles could hit dealerships by 2015. In a coordinated announcement the next day in Berlin, a group of energy companies including Shell and the Swedish firm Vattenfall joined Daimler in an agreement to begin setting up the necessary hydrogen infrastructure in Germany. This push for rapid deployment has left many people shaking their heads. \"I just don't see it,\" says Don Hillebrand, director of the Center for Transportation Research at the Argonne National Laboratory in Illinois. \"It doesn't make sense.\" Yet the proponents of hydrogen vehicles are brimming with confidence. \"This memorandum of understanding marks the will of the industry to move forward,\" says Klaus Bonhoff, who heads the National Organisation for Hydrogen and Fuel Cell Technology (NOW), a Berlin-based organization created by the German government in 2008 to spearhead that country's hydrogen programme. Here  Nature   assesses the four major challenges facing hydrogen fuel-cell vehicles, and finds that both sides have a point: some of the challenges are close to being met \u2014 but others have a long way to go.  \n                Fuel cell \n              Conceptually, at least, a fuel cell is simply a device that takes in oxygen from the air and hydrogen from a tank, and reacts them in a controlled way to produce water vapour and electric power. In a vehicle, that power can then be directed through an ordinary electric motor to turn the wheels. In practice, fuel cells are anything but simple: controlling the reaction and extracting the electric current requires a sophisticated assembly including nozzles, membranes and catalysts. And therein lies the challenge: how to pack all that complexity into a device that is light, cheap, robust and durable \u2014 as well as being powerful enough to provide rapid acceleration, plus drive all the lights, air conditioning, radio and other amenities that consumers have come to expect in a modern vehicle. Ten years ago this goal seemed far off. Car manufacturers didn't even dare to expose their experimental fuel-cell vehicles to cold weather: they worried that when the cells shut down, residual water vapour could freeze and wreak havoc on the delicate insides. Instead, the companies would shuttle the vehicles around in heated trailers. But a decade has brought fuel-cell technology a remarkably long way. \"Nobody woke up one morning and said, 'Ah-ha! Here's the salient breakthrough!'\" says Byron McCormick, who headed the fuel-cell programme of General Motors until January 2009. \"It has really been a whole lot of small steps.\" For example, General Motors' fuel-cell vehicles eliminate the cold-weather problem in part by continuing to run the cell's exhaust system for a minute or two after the car is shut down, using the cell's residual heat to drive the water out of the system. Toyota says that its experimental, fuel-cell-equipped Highlander sports-utility vehicle will start up at \u221237 \u00b0C. Engineers are also cutting back on the use of expensive catalysts. General Motors' fuel-cell assembly uses roughly 80 grams of platinum to split electrons and protons from hydrogen atoms. At the current platinum price of about US$60 per gram, this totals some $4,800. But General Motors officials say that their next fuel cell will use less than 30 grams of platinum, thanks to using ever thinner coats of the metal. And the company's scientists are continuing to experiment with measures such as increasing the surface area of the catalyst by introducing more texture at the nanoscale. Within a decade, they expect to get platinum use to below 10 grams, which would make the fuel cells competitive with today's catalytic converters in terms of precious-metal use. These and other advances translate into price reductions. The Department of Energy estimates that fuel-cell costs per kilowatt of power dropped by nearly 75% between 2002 and 2008, based on cost projections for high-volume manufacturing. Companies won't discuss retail prices except to say that the vehicles slated to appear by the middle of the decade will be priced competitively. \"I've been doing this for 10 years, and the numbers even surprise and shock me,\" says Craig Scott, manager of Toyota's advanced technologies group in Torrance, California. \"It is definitely going to be a car that is in reach of a lot of people.\" \n                On-board storage \n              In June 2009, Toyota engineers and US government monitors hopped into a pair of fuel-cell Highlanders at the company's US headquarters in Torrance and took a 533-kilometre round trip through real-world traffic \u2014 without refuelling. Calculations suggest that the vehicles' performances corresponded to a range of 693 kilometres on a single tank of hydrogen, which is on a par with the range of current petrol vehicles. Ten years ago, this feat also would have seemed daunting. Gaseous hydrogen is easy enough to store in a tank. But getting enough of it on board would require either a ridiculously large tank that would eliminate space for people, groceries and camping gear, or an exceptionally strong tank that could safely store compressed hydrogen gas at hundreds of times atmospheric pressure. Liquid hydrogen is much denser, but it would have to be maintained in an insulated tank at \u2212253 \u00b0C, which would add to a vehicle's weight, complexity and expense. In the end, the comparative simplicity of compressed hydrogen won out. Most companies have chosen to use modern carbon-fibre tanks, which can store hydrogen at up to 680 atmospheres, while still being relatively lightweight. To improve range further, many companies are also equipping their vehicles with the same 'regenerative braking' technology that allows hybrid petrol and electric cars and all-electric cars to capture energy during braking, store it in auxiliary batteries, and reuse it for later acceleration. Indeed, because hydrogen and battery-powered vehicles both use electric motors, they share many technologies. The only real difference is the power source: fuel cells versus batteries. Scott says that electric vehicles based on the lithium-ion battery chemistry are unlikely to get beyond a range of 150\u2013250 kilometres on a single charge. And although that may be enough to cover urban driving, consumers like having the option to drive cross-country. So in the shift away from petrol, the hydrogen vehicle's greater range could give it an edge in the long term. Scott says that hydrogen and electric vehicles have a space to occupy. \"I just think that fuel cells will occupy a bigger space,\" he says.  \n                Distribution infrastructure \n              Regardless of range, every vehicle needs fuel at some point. And here lies hydrogen's chicken-and-egg problem: fuel-cell vehicles will never sell in a big way until there is a viable network of service stations to fuel them. But no one is going to invest the capital required to create such a network until there is a fleet of thirsty hydrogen vehicles to provide a market. Hydrogen pumps can and have been added to existing petrol stations, where at first glance they look much the same as conventional pumps. Because the hydrogen used is a compressed gas, filling the tank is not just a matter of placing a nozzle in the petrol-tank opening and letting gravity take care of the rest. Instead, a tight seal has to be established between the nozzle and car, and high-powered pumps have to force hydrogen through the nozzle until the desired pressure is reached. In practice, the current-generation hydrogen pumps are already easy and safe enough for an average consumer to use. But they do have to work perfectly if tanks are to be filled to full pressure; at present their performance is solid but variable. A larger question facing car manufacturers is how rapidly the network of hydrogen-filling stations will spread. In the United States, for example, the number of hydrogen pumps is at present measured in dozens, and there seems to be little coordinated effort to change the situation. And until recently, things seemed much the same elsewhere. That's why hydrogen proponents see so much significance in last year's agreements in Germany, which promise to break the chicken-and-egg deadlock. The car manufacturers have promised the cars, and NOW is pushing for a network of several hundred pumps throughout Germany within a few years, and as many as 1,000 by the end of the decade. That should be enough to provide broad coverage within the metropolitan areas and regular access along the highways. Bonhoff says that the consortium expects the price to be within the range of what energy companies would normally spend to maintain, upgrade and expand their petrol infrastructure over the same interval. Charlie Freese, who heads the fuel-cell programme at General Motors, says that the hydrogen-infrastructure costs could be similarly manageable even in much larger countries such as the United States. In the early stages of a hydrogen-vehicle rollout, the Los Angeles basin could be well served with 50 hydrogen stations at a cost of roughly $200 million. Further down the line, some 11,000 stations might be needed to provide blanket coverage across the United States. \"That's something you could do for roughly the cost of the Alaska pipeline,\" he says, referring to a proposed $35-billion project intended to carry natural gas from Alaska's North Slope to the North American market.  \n                Hydrogen production \n              From a climate perspective, the main question facing hydrogen is where to get the gas in the first place. At present, the cheapest source is via a chemical reaction between steam and natural gas. But this process produces carbon dioxide, which means that the total greenhouse-gas production of a fuel-cell vehicle is not dramatically less than that of a conventional petrol vehicle. So the challenge is to derive hydrogen from carbon-free renewable sources. Vattenfall, sees this as an opportunity and is building a facility in Hamburg that will use excess wind power to split water molecules and produce hydrogen for a fleet of 20 fuel-cell buses. Power companies tend to disperse extra wind turbines in various locations to compensate for the fact that wind is inherently unreliable. But those excess turbines will produce more electricity than the grid can handle if the wind blows in too many places at once. When that happens, turbines are shut down. Once the Hamburg facility comes on line, Vattenfall will instead fire up the electrolysis unit, tapping the excess power to make hydrogen and keeping the grid stable. Cost is still an issue, says Oliver Weinmann, head of innovation management for Vattenfall in Germany. He says that the company will be able to produce hydrogen at \u20ac3\u20134 ($4\u20135.3) per kilogram, compared with \u20ac2 per kilogram for hydrogen produced from natural gas. But with Europe looking to expand its use of renewable energy over the coming decade, the growth potential is enormous, says Weinmann. \"It is not really a question of whether we can afford the hydrogen infrastructure,\" says Freese. \"The question is whether we can afford not to have hydrogen infrastructure if we want to use renewables.\"  \n                Adoption \n              Not everyone is persuaded by such arguments. Even if car manufacturers do get their fuel-cell vehicles to market by 2015, it will take years to establish a customer base, increase production and bring down costs. Few firms anticipate profitability on these vehicles until 2020 or even 2025. Meanwhile, they and the energy companies are also pushing biofuels and battery-powered electric cars, each of which would require its own distribution system. Building these transportation infrastructures simultaneously might not be possible. These concerns are felt even within the car industry. Ford, for example, is confining its fuel-cell activities to long-term research, and has no current plans to market a commercial hydrogen vehicle. And BMW is hedging its bets with research into an otherwise conventional car whose internal combustion engine can burn petrol or hydrogen. Some hydrogen advocates predict a multiple-niche scenario, in which battery vehicles are used in urban areas, whereas hydrogen pumps proliferate along the highways for long-distance travel. But perhaps the biggest mistake would be to assume that anybody in this game really knows what they are doing, says John Heywood, director of the Sloan Automotive Lab at the Massachusetts Institute of Technology in Cambridge. Heywood says that the first round of vehicles will not be finished products so much as 'production prototypes' that allow companies to assess their performance \u2014 and the consumer response. Toyota followed this approach with its Prius hybrid car in 1997, and there's no reason to think that the process will be any faster for hydrogen or battery-powered vehicles. In either case, it could take three or more decades to revolutionize the global automobile fleet, says Heywood, and that's the kind of time frame that is guiding the car makers today. \"There are two paths, and they are going to invest in the electricity and the hydrogen pathway until it becomes clearer that one is significantly better than the other,\" he says. \"Right now, we don't know the answer.\"  \n                     German consortium NOW \n                   \n                     Department of Energy fuel-cell programme \n                   Reprints and Permissions"},
{"file_id": "464972a", "url": "https://www.nature.com/articles/464972a", "year": 2010, "authors": [{"name": "Heidi Ledford"}], "parsed_as_year": "2006_or_before", "body": "Databases could soon be flooded with genome sequences from 25,000 tumours. Heidi Ledford looks at the obstacles researchers face as they search for meaning in the data. When it was first discovered, in 2006, in a study of 35 colorectal cancers 1 , the mutation in the gene  IDH1   seemed to have little consequence. It appeared in only one of the tumours sampled, and later analyses of some 300 more have revealed no additional mutations in the gene. The mutation changed only one letter of  IDH1 , which encodes isocitrate dehydrogenase, a lowly housekeeping enzyme involved in metabolism. And there were plenty of other mutations to study in the 13,000 genes sequenced from each sample. \"Nobody would have expected  IDH1   to be important in cancer,\" says Victor Velculescu, a researcher at the Sidney Kimmel Comprehensive Cancer Center at Johns Hopkins University in Baltimore, Maryland, who had contributed to the study. But as efforts to sequence tumour DNA expanded, the  IDH1   mutation surfaced again: in 12% of samples of a type of brain cancer called glioblastoma multiforme 2 , then in 8% of acute myeloid leukaemia samples 3 . Structural studies showed that the mutation changed the activity of isocitrate dehydrogenase, causing a cancer-promoting metabolite to accumulate in cells 4 . And at least one pharmaceutical company \u2014 Agios Pharmaceuticals in Cambridge, Massachusetts \u2014 is already hunting for a drug to stop the process. Four years after the initial discovery, ask a researcher in the field why cancer genome projects are worthwhile, and many will probably bring up the  IDH1   mutation, the inconspicuous needle pulled from a veritable haystack of cancer-associated mutations thanks to high-powered genome sequencing. In the past two years, labs around the world have teamed up to sequence the DNA from thousands of tumours along with healthy cells from the same individuals. Roughly 75 cancer genomes have been sequenced to some extent and published; researchers expect to have several hundred completed sequences by the end of the year. \n               Click here for larger image \n               The efforts are certainly creating bigger haystacks. Comparing the gene sequence of any tumour to that of a normal cell reveals dozens of single-letter changes, or point mutations, along with repeated, deleted, swapped or inverted sequences (see 'Genomes at a glance'). \"The difficulty,\" says Bert Vogelstein, a cancer researcher at the Ludwig Center for Cancer Genetics and Therapeutics at Johns Hopkins, \"is going to be figuring out how to use the information to help people rather than to just catalogue lots and lots of mutations\". No matter how similar they might look clinically, most tumours seem to differ genetically. This stymies efforts to distinguish the mutations that cause and accelerate cancers \u2014 the drivers \u2014 from the accidental by-products of a cancer's growth and thwarted DNA-repair mechanisms \u2014 the passengers. Researchers can look for mutations that pop up again and again, or they can identify key pathways that are mutated at different points. But the projects are providing more questions than answers. \"Once you take the few obvious mutations at the top of the list, how do you make sense of the rest of them?\" asks Will Parsons, a paediatric oncologist at Baylor College of Medicine in Houston, Texas. \"How do you decide which are worthy of follow up and functional analysis? That's going to be the hard part.\"  \n                Drivers wanted \n             \n               Click here for larger image \n               Because cancer is a disease so intimately associated with genetic mutation, many thought it would be amenable to genomic exploration through initiatives based on the collaborative model of the Human Genome Project. The International Cancer Genome Consortium (ICGC), formed in 2008, is coordinating efforts to sequence 500 tumours from each of 50 cancers. Together, these projects will cost in the order of US$1 billion. Eleven countries have already signed on to cover more than 20 cancers (see map). The ICGC includes two older, large-scale projects: the Cancer Genome Project, at the Wellcome Trust Sanger Institute near Cambridge, UK, and the US National Institutes of Health's Cancer Genome Atlas (TCGA). The Cancer Genome Project has churned out more than 100 partial genomes and roughly 15 whole genomes in various stages of completion, and intends to tackle 2,000\u20133,000 more over the next 5\u20137 years. TCGA, meanwhile, wrapped up a three-year, three-cancer pilot project last year, then launched a full-scale endeavour to sequence up to 500 tumours from each of more than 20 cancers over the next five years. Although the groups collaborate, TCGA has not yet been able to fully join the ICGC owing to differences in privacy regulations governing access to genome data. For now, members of both consortia are sequencing a subset of tumour samples from each cancer type \u2014 around 100 \u2014 and will follow this by sequencing promising areas in the remaining 400. That's useful, says Joe Gray, a cancer researcher at Lawrence Berkeley National Laboratory in California, but it's just a start. \"In the early days, I thought that doing a few hundred tumours would probably be sufficient,\" he says. \"Even at the level of 1,000 samples, I think we're probably not going to have the statistics we want.\" What bigger numbers could provide is more driver mutations like the one in  IDH1 . These could, researchers argue, provide the clearest route to developing new cancer therapies. Many scientists have looked for mutations that occur repeatedly in a given type of tumour. \"If there are lots and lots of abnormalities of a particular gene, the most likely explanation is often that those mutations have been selected for by the cancers and therefore they are cancer-causing,\" says Michael Stratton, who co-directs the Cancer Genome Project. This approach has worked well in some cancers. For example, with a frequency of 12%, it is clear that the  IDH1   mutation is a driver in glioblastoma. Such searches should be fruitful for cancers that have fewer mutations overall. The full genome sequence of acute myeloid leukaemia cells yielded just ten mutations in protein-coding genes, eight of which had not previously been linked with cancer 5 . Other cancers have proved more challenging.  IDH1   was overlooked at first, on the basis of the colorectal cancer data alone. It was not until the search was expanded to other cancers that its importance was revealed. Moreover, some mutations shown to be drivers haven't turned up as often as expected. \"It's very clear, now that all the genes have been sequenced in this many tumours, you have drivers that are mutated at very low frequency, in less than 1% of the cancers,\" says Vogelstein. To find these low-frequency drivers, researchers are sampling heavily \u2014 sequencing 500 samples per cancer should reveal mutations that are present in as few as 3% of the tumours. Although they may not contribute to the majority of tumours, they may still have important biological lessons, says Stratton. \"We need to know about these to understand the overall genomic landscape of cancer.\" Another popular approach has been to look for mutations that cluster in a pathway, a group of genes that work together to carry out a specific process, even if the mutations strike it at different points. In an analysis of 24 pancreatic cancers 6 , for instance, Vogelstein and his colleagues identified 12 signalling pathways that had been altered. Nevertheless, Vogelstein cautions that this approach is not easy to pursue. Many pathways overlap, and their boundaries are unclear. And because many have been defined using data from different animals or cell types, they do not always match what's found in a specific human tissue. \"When you layer on top of that the fact that the cancer cell is not wired the same as a normal cell, that raises even further difficulties,\" says Vogelstein.  \n                How much is enough? \n             \n               Click here for larger image \n               Separating drivers from passengers will become even more difficult as researchers move towards sequencing entire tumour genomes. To date, only a fraction of the existing cancer genomes are complete sequences. To keep costs low, most have covered only the exome, the 1.5% of the genome that directly codes for protein and is therefore the easiest to interpret. Assigning importance to a mutation found in the murky non-protein-coding depths of the genome will be more challenging, especially given that scientists don't yet know what function \u2014 if any \u2014 most of these regions usually serve. The vast majority of mutations fall here. The full genome sequence of a lung cancer cell line, for example, yielded 22,910 point mutations, only 134 of which were in protein-coding regions (see graphic, right) 7 . Nevertheless, finding them is worth the cost and effort, argues Stratton. \"It could be that none of those mutations pertain to the causation of cancer,\" he says. \"But it equally could be that some do. We'll never find out unless we systematically investigate.\" Not everyone agrees. Some researchers argue that the costs of cancer-genome projects currently outweigh the benefits. Prices are poised to drop dramatically in the next few years as a new generation of sequencing machines comes online, says Ari Melnick, a cancer researcher at Weill Cornell Medical College in New York. \"Why not wait for that?\" he asks. In the meantime there are lower-hanging fruit to pick, says Stephen Elledge, a geneticist at Harvard Medical School in Boston, Massachusetts. Mutations that affect how many copies of a gene are found in a genome, he argues, are cheaper to assess and provide a more intuitive insight into biological processes. \"If you delete something, you can turn a pathway off very efficiently,\" he says. \"And if you amplify something, you can increase flow through the pathway. Making point mutations in genes to activate them is a little dicier.\" Changes in gene copy number can be detected using fast, relatively inexpensive array-based technologies, but sequencing can provide a higher-resolution snapshot of these regions, says Elaine Mardis, a sequencing specialist at Washington University in St Louis, Missouri. Sequencing can enable researchers to map the boundaries of insertions and duplications with more precision and to catch tiny duplications or deletions that might have gone undetected by an array. Mardis, along with her colleague Richard Wilson and others, used sequencing to detect overlapping deletions in a breast cancer that had spread to other parts of the body (see  page 999 ) 8 . The deletions spanned the region containing  CTNNA1 , a gene thought to suppress the spread, or metastasis, of cancer. Meanwhile, cancer genomics is spreading out from under the large, centralized projects. For example, a $65-million, three-year paediatric-cancer genome project headed by researchers at St Jude Children's Research Hospital in Memphis, Tennessee, and Washington University aims to sequence 600 tumours. And more small projects seem poised to pop up. \"Pretty much any cancer centre with any interest in the genomics of cancer is now buying these sequencers and using them,\" says Sam Aparicio, a cancer researcher at the University of British Columbia in Vancouver, Canada. Part of the reason that cancer-genome proponents don't want to wait for sequencing costs to drop is that the real work starts after the sequencing is over. As Velculescu puts it, \"Ultimately it's going to take good old-fashioned biology and experimental analyses to really determine what these mutations are doing.\" With this in mind, the US National Cancer Institute established two 2-year projects in September last year to develop high-throughput methods to test how the mutations identified by the TCGA pilot project affect cell function. The two centres \u2014 one at the Dana-Farber Cancer Center in Boston, and another at Cold Spring Harbor Laboratory in New York \u2014 aim to systematize the way that researchers pull other needles like the  IDH1   mutation from the cancer-genomes haystack and make sense of them. The Boston team will systematically amplify and reduce the expression of genes of interest in cell cultures, and the Cold Spring Harbor centre will study cancer-associated mutations using tumours transplanted into mice. In addition, large-scale projects are being run in parallel with the cancer-sequencing consortia to assess the effects of deleting each gene in the mouse genome, enabling researchers to learn more about the normal function of genes that are mutated in cancer. Sequencing is all very well, researchers have realized, but it won't be enough. \"Some people say statistics should get us all the drivers that are worthwhile,\" says Lynda Chin, an investigator with TCGA at Harvard Medical School. \"I don't agree with that. At the end of the day, we need these functional studies to prioritize the list of potential cancer-relevant candidates.\"   See also News and Views,  \n                     page 989 \n                   . \n                     Nature Insight: Molecular cancer diagnostics \n                   \n                     Nature Collection: Cancer \n                   \n                     Nature Collection: Cancer genomics \n                   \n                     Inernational Cancer Genome Consortium \n                   \n                     The Cancer Genome Atlas \n                   \n                     Cancer Genome Project \n                   Reprints and Permissions"},
{"file_id": "4641260a", "url": "https://www.nature.com/articles/4641260a", "year": 2010, "authors": [{"name": "Jeffrey Perkel"}], "parsed_as_year": "2006_or_before", "body": "Many scientists want to keep their data and resources free; cybersecurity specialists want them under lock and key. Jeffrey Perkel reports. In 2002, bioinformatician Mark Gerstein and his colleagues set up a server to host some commonly used genomics databases. Operating within this was a free software application called Snort, which actively monitors anomalies in web traffic, surreptitious scans and server nudges that might be attempts to compromise a system's integrity. Then the researchers watched and waited. Seven months later, the picture that emerged was one of a network under siege. \"If you put up a server, it just continuously gets barraged,\" says Gerstein of Yale University in New Haven, Connecticut. Most days, the server was hit ten times or fewer, but on occasion the hit count spiked into the thousands 1 . Not all of those hits were attacks, Gerstein notes, but many were. On two high-hit days, for example, more than 90% of events attempted to induce a 'buffer overflow', whereby superfluous amounts of data are written into a system's memory in an attempt to make it fail, opening it up to exploitation. In the face of such a relentless onslaught, most ventures onto the World Wide Web should be taken with caution. Having a graduate student with little experience set up a website could be disastrous, Gerstein says. Entering either through poorly written code or open ports \u2014 digital entry points into computer hardware and operating systems \u2014 hackers could deface the content on the site or, worse, install malicious software on the machine running it. Attacks can run the gamut from the installation of programs intended to co-opt system resources to keystroke loggers and scanning software designed to purloin user information and passwords. Some hackers target university systems to steal computing resources or even intellectual property \u2014 proprietary compounds, instrument designs, patient data and personal communications. Hacking is suspected in the November 2009 release of e-mails from the Climatic Research Unit at the University of East Anglia, UK, which resulted in a global crisis of confidence in climate science. \"There is no sector that has been able to withstand this onslaught of intrusions,\" warns Steven Chabinsky, deputy assistant director in the cyber division of the Federal Bureau of Investigation (FBI) in Washington DC. Protecting research data presents particular challenges. Most information-technology (IT) professionals suggest ensuring that large or sensitive data stores are managed by a centralized IT team that can monitor and administer systems, keeping a close watch over traffic and limiting access. But this can conflict with the ethos of researchers who need such systems to be accessed by a wide variety of students, postdocs and collaborators. \"Universities tend to be fairly open kinds of environments,\" says John McCanny, lead scientist at the Centre for Secure Information Technologies at Queen's University Belfast, UK. And some researchers bristle at the idea of losing control. \"I'd rather be insecure and free,\" says Phillip Zamore, a biologist at the University of Massachusetts Medical School in Worcester.  \n                Drive-by hacking \n              It is a rough world in which to take such a stance. Michael Corn, chief privacy and security officer at the University of Illinois at Urbana-Champaign, estimates that each day his university's firewalls block some two million to three million scans \u2014 essentially Internet drive-bys looking for open communications ports. A \"significant percentage\" of these, he says, are likely to stem from \"professionals in the employ of organized crime or possibly state actors\". The network at the San Diego Supercomputer Center of the University of California, San Diego, logged an average of 27,000 intrusion attempts per day during the first three months of 2010, according to Dallas Thornton, division director of the university's cyberinfrastructure services. Thornton says that this number actually underestimates the problem, because many common attacks are simply ignored. Successful intrusions are rarer. Until recently, the Educational Security Incidents website ( http://www.adamdodge.com/esi ) reported breaches in cybersecurity for higher education, recording nearly 500 incidents of data theft, loss or exposure between July 2005 and December 2009. Most of these occurred in the United States; just 10 occurred in the United Kingdom, 11 in Canada and 8 in Japan. But that may be just the tip of the iceberg; most incidents go unreported, says Chabinsky. McCanny reports that cybercrime's growth in the United Kingdom has greatly outpaced expectation. \"The train is coming down the track much faster than most people would have anticipated even 15 months ago,\" he says. EDUCAUSE, a non-profit organization of higher-education IT professionals, undertakes annual surveys of academic chief information officers. The 2009 survey ranked security third overall among IT problems facing higher-education institutions today 2 . Security officers in such institutions face two crucial problems. One is funding, which ranked first in the survey. The chemistry department at the University of Wisconsin-Madison, for instance, has one IT person overseeing 1,300 computers used by 500 staff and several thousand students. In such a climate, says departmental chairman Robert Hamers, \"it's just very limiting in terms of what you can actually expect one person to do\". Last year, Hamers' department suffered a breach. In the absence of a firewall (now installed), foreign hackers infiltrated 40 computers, installing software that turned those systems into file-sharing servers for copyrighted music, movies and television programmes. The other problem, says Alan Paller, director of research at the cybersecurity-focused SANS Institute in Bethesda, Maryland, is researcher independence. Academic researchers need to be able to install software on demand, to collaborate with colleagues, to develop new tools for public or private consumption, and to cater for an ever-changing and heterogeneous user base. Besides that, \"researchers are the best in the world at basically ignoring the administration\", says Paller. In 2006, a researcher at Georgetown University in Washington DC independently decided to migrate a server handling confidential patient data from an IT-monitored UNIX system to an unmonitored Windows one. The server was hacked, and the names, birth dates and social-security numbers of some 41,000 individuals may have been accessed. The university was legally obliged to notify each of them. Such incidents, says Brian Voss, chief information officer at Louisiana State University in Baton Rouge, mostly occur when researchers strike out on their own. \"It's like herding cats,\" he says. \"The challenge is, how do you get them all under the umbrella?\" Generally, universities do that by promoting the benefits of their centralized services. Many academic institutions offer a battery of common but effective defences \u2014 everything from pushing operating system and antivirus patches out to users, to remotely monitoring network traffic, to the establishment of secure virtual private networks for encrypted communication and virtual machines. Virtual machines are hardware surrogates: although users can interface as if they are working on physical computers or web servers, they're actually just working through windows running on a centralized host computer. Gerstein says that most of his lab is set up on a framework of virtual machines. This allows him and his colleagues to maintain back-ups for all the resources they put up on the web. \"If something gets hacked into, we are not as worried any more,\" he says, \"because we can roll back really easily and then put the thing back up.\" The American University in Washington DC also offers virtual-machine services to its faculty members, says chief information officer David Swartz, who (with Voss) co-chairs the EDUCAUSE Higher Education Information Security Council. \"If they need a server I say, 'Come to me, I'll set you up',\" Swartz says. The resulting sites are professionally secured, continuously monitored, and easily backed up and restored. And in the event of a breach, that intrusion is often effectively contained by virtue of the virtual machines' isolated architecture. Cybercrime is evolving rapidly. At the University of Nevada in Las Vegas, Lori Temple, vice-provost for information technology, says she is seeing increased intrusion through social-networking sites. Facebook games and images, for example, are particularly problematic, she says. \"As much as you try to be ahead, you are almost always one step behind.\" How much individual researchers need to worry depends on their own cost\u2013benefit analysis, says Chabinsky \u2014 a reflection of the value of the data's confidentiality and integrity both to the researcher and his or her competitors. Many will calculate that they have little cause for concern, save the loss or corruption of their own data, which can be mitigated by routine back-ups. But for those who must store personal, confidential or economically valuable information, he says, \"they should ask some strong questions of their system security providers\". They should enquire, for example, about the confidence they have in the systems in place and what additional measures they can take to mitigate risks. This is especially important as US federal granting agencies are beginning to require certification that sensitive information such as health records will be secured in compliance with the Federal Information Security Management Act of 2002. \"An incident at the university could cause more than public embarrassment or a lawsuit,\" says Swartz, \"it might actually cause the loss of federal grants.\" Ultimately, preemptive training might be the most cost-effective weapon IT departments have. Knowledge, after all, is power (see  'Ten tips for cybersecure science' ). Especially given that no amount of infrastructure can overcome user carelessness. In 2007, the University of Illinois at Urbana-Champaign suffered a breach in which a spreadsheet containing the personal information of 5,247 students was accidentally e-mailed to 700 unauthorized individuals. As Marty Lindner, a principal engineer in the computer emergency response team (CERT) at the Carnegie Mellon Software Engineering Institute programme in Pittsburgh, Pennsylvania, says: \"The best way to secure a computer is to remove the keyboard, because then the human cannot make a mistake.\"  \n                     Data sharing special \n                   Reprints and Permissions"},
{"file_id": "465026a", "url": "https://www.nature.com/articles/465026a", "year": 2010, "authors": [{"name": "Lizzie Buchen"}], "parsed_as_year": "2006_or_before", "body": "Systems neuroscientists are pushing aside their electrophysiology rigs to make room for the tools of 'optogenetics'. Lizzie Buchen reports from a field in the process of reinvention. The centrifuge tube was the first that neuroscientist Philip Sabes had held in his hand for 15 years. The small, polypropylene container, no larger than a AAA battery, held a few drops of liquid at its base. It looked like water but, Sabes had been told by his collaborators, it contained a high concentration of viruses \u2014 and he had to get them into the brain of a monkey. \"Honestly, I really felt like I didn't know what I was doing,\" says Sabes, of his work last November at the Keck Center for Integrative Neuroscience at the University of California, San Francisco (UCSF). \"I basically knew nothing about molecular biology. This was way outside my area of expertise.\" Sabes's training was in physics, machine learning and human perception, and his lab has been working with humans and non-human primates to develop models of how the brain turns perceptions into actions; for example, seeing a fly and swatting it away. He's not alone in his molecular-biology naivety at the Keck Center \u2014 there is no cell-culture facility, no PCR machine and no bench-top centrifuge. The centre's one ice machine spits out large cubes instead of the crushed ice routinely used for chilling reagents \u2014 it was ordered by mistake, and no one has cared enough to fix the situation. Sabes and his colleagues have had no need for such apparatus. Researchers in their field of 'systems neuroscience' try to understand how networks of neurons process sensations and control behaviours such as learning and decision-making. And up to this point, much of their progress has been made using electrophysiology, stimulating and recording from the brains of animals as they perform a task or develop a new skill. Now though, advances in a five-year-old field called optogenetics are convincing these scientists to crack open molecular-biology textbooks. Using a hybrid of genetics, virology and optics, the techniques involved enable researchers to instantaneously activate or silence specific groups of neurons within circuits with a precision that electrophysiology and other standard methods do not allow. Systems neuroscientists have longed for such an advance, which allows them their first real opportunity to pick apart the labyrinthine jumble of cell types in a circuit and test what each one does. \"It has revolutionized my approach to science,\" says Antonello Bonci, a neurophysiologist at the UCSF Ernest Gallo Clinic and Research Center in Emeryville who began using the technique in 2007. \"It can clarify unequivocally the role of specific classes of cells, and solve controversies that have been going on for many, many years.\" Among the clarifications sought is the precise function of 'place' cells, hippocampal neurons that fire only when an animal finds itself in a specific location; another is the function of complex activity patterns observed when an animal is paying attention or executing a movement. \n                A field's evolution \n              The transition phase isn't easy. Optogenetic tools were first used in cell cultures and mice, which are amenable to genetic manipulation. Now systems neuroscientists must adapt them to function in organisms they traditionally study such as rats, birds and primates. With the technical challenge comes a personal one, as researchers leave their experimental comfort zone for a new field. Most, however, anticipate that any discomfort will be worthwhile. \"This is God's gift to neurophysiologists,\" says Robert Desimone, director of the McGovern Institute for Brain Research at the Massachusetts Institute of Technology (MIT) in Cambridge, who has been using electrophysiology for more than 30 years. \"Molecular techniques were always beyond us, so this is our first opportunity. It's a revolution. But we're catching up to the revolution that had been going on for the rest of the world.\" Optogenetics started out in 2005, when a team at Stanford University in California led by Karl Deisseroth and his then-postdoc Ed Boyden inserted a light-sensitive channel from green algae, called channelrhodopsin-2 (ChR2), into neurons growing in a dish. Exposed to a pulse of blue light, the channels opened and a flood of positive ions poured into the neurons, making them fire 1 . Within a year, 30 labs had contacted Deisseroth to ask for the technology. By March 2010, Deisseroth had sent protocols or genetic constructs to more than 500 labs around the world, and Boyden, now at MIT, had sent them to in excess of 250. \n               boxed-text \n             The technique has been refined greatly in the past five years, but the basic steps are the same (see 'Six steps to optogenetics'). First, researchers create a genetic construct containing the ChR2 gene or another 'opsin' gene, along with genetic elements that control its expression \u2014 for example, a specific 'promoter' sequence. Then, they package up the construct in a virus. When the virus is injected into an animal's brain, it widely infects neurons and delivers the construct, but the opsin is expressed in only a subgroup of cells with the necessary machinery to activate its promoter. The opsin proteins sit in the membrane of those neurons, and researchers trigger them with light of a specific wavelength, typically delivered through an optic fibre threaded through an animal's skull. Deisseroth and Boyden have discovered or engineered many other opsins that allow neurons to be manipulated in different ways, including neural silencers. The technique is popular, but complex. The fastest adopters were those who work with cells grown  in vitro   and animals such as flies, worms and mice, for which they could take advantage of established genetic tools and well-characterized animal lines. \"The number of tangible results are still of the order of half a dozen or so. But we're on the rising phase of the exponential,\" says Karel Svoboda, a neuroscientist at the Howard Hughes Medical Institute's Janelia Farm research campus in Ashburn, Virginia, who last year started mapping mouse cortical circuits using optogenetics 2 . From the start, these developments sent quivers through the systems-neuroscience community. \"When I first saw this stuff, at a meeting where Karl was presenting some of the earliest results, I thought, 'Oh God, finally',\" says Loren Frank, another neuroscientist at the Keck Center. \"And then I thought, oh great. Here I was, I'd gotten pretty good at the stuff I thought I needed to do, and now I have to learn an entirely new field.\" Frank \u2014 who says he has \"always had a bit of molecular-biology envy\" \u2014 began collaborating with Deisseroth to use optogenetics on rats just over three years ago, for his studies of place cells and hippocampal circuits. Silencing or activating specific cells could help show whether or how they help an animal explore a new location or recognize a familiar one. \"If you want to actually understand the system, you have to start trying to get control of the system and actually dissect the circuit,\" says Frank. Standard techniques have not allowed that type of dissection. Electrodes inserted into an animal's brain typically stimulate hundreds of thousands of cells; using lesions or drugs also hits circuits like a hammer. Optogenetics could be a scalpel, turning particular neurons in a circuit on or off within milliseconds.  \n                From brain to behaviour \n              By April 2009, Frank was making progress in rats, achieving ChR2 expression in a discrete set of hippocampal neurons and getting them to fire. And down the hall, Sabes was ready to try the technique in primates as part of his efforts to understand circuits involved in hand-eye coordination. \"I knew optogenetics was on the horizon, and it was potentially exciting, but realistically, I just didn't know anything about this stuff,\" he says. Sabes and two songbird researchers at the Keck Center, Michael Brainard and Allison Doupe, teamed up with Frank, Deisseroth and another colleague, Linda Wilbrecht, a neuroscientist at the Gallo Center. The group, one of the first big collaborations aiming to apply optogenetics to rats, birds and primates, received a US$1.6-million National Institutes of Health grant in September 2009 through the financial stimulus. The neuroscientists describe what they'd like to do \u2014 in Sabes's case, for example, alter patterns of activity that occur in the parietal lobe when an animal reaches for something, and work out which patterns are important for planning, initiating or adjusting the movement. Wilbrecht's lab leads efforts to generate the appropriate constructs and select the best viruses, and Deisseroth tries to build new viral and optogenetic tools that they need. One huge advantage for mouse researchers has been the ready-made bank of animal lines engineered to express an enzyme called Cre recombinase in subsets of neurons, such as all dopaminergic ones, which they can use in combination with specially designed genetic constructs to achieve the cellular specificity they want. In other animals, a promoter must be found that will only be turned on in dopaminergic neurons. Some promoters and viruses that work in mice do not work in rats or primates, meaning that researchers have to start from scratch. Progress has been faster in rats because the animals are relatively easy to breed and are similar enough to mice for methods to be transferable. In March 2009, Deisseroth was the first to publish a rat optogenetic study, in which he manipulated circuit components in a rat model of Parkinson's disease to work out which parts might account for the relief afforded by deep-brain stimulation 3 . An additional complication for primate researchers is that primate brains are larger than those of rodents, making it difficult to ensure the injected virus and the activating light penetrate deep enough. And troubleshooting in monkeys will take much longer than in rats, given the long lifetime and high value \u2014 experimental, financial and ethical \u2014 of the animals. But in April 2009, Desimone and his colleagues worked with Boyden to publish the first experiment showing that viruses can be used to insert opsin channels and control neural activity in a macaque 4 . By November 2009, Sabes could be found holding his tube of virus, trying to get a similar primate system going in his lab. This was a pilot experiment: he was simply trying to see if he could get some of the virus into neurons at all. After bending and breaking a few needles, he successfully injected the tube's contents into a monkey's brain. He and two graduate students then hacked together an 'optrode' by gluing together a fibre-optic cable and an electrode. They fed the wires through a head fixture normally used for electrophysiology, and rigged half of a syringe as a support. \"It felt very much like garage-development days, cutting stuff up to see what works,\" says Sabes. In January, with the optrode inserted into the region where the virus had been injected two months earlier, Sabes flipped on a blue laser and watched a screen showing electrical readout from the optrode. Yellow waveforms flashed across the screen, paired with a flurry of clicking noises: spikes of neural activity.  \n                Experts on hand \n              Like Sabes, the few other electrophysiologists starting to tackle primate optogenetics are keeping experts in the technique close by to avoid making a novice's mistakes (see  'Opto school' ). \"It's critical that at least the initial phase is collaborative,\" says Krishna Shenoy, a primate electrophysiologist at Stanford University who has been working closely with Deisseroth for two years to explore how neurons in the brain's motor cortex control arm movements. Boyden's paper is still the only optogenetics publication on primates, and the technique is some way from generating new discoveries. It's still not known whether the opsins will be expressed consistently for the year or two required to train monkeys in sophisticated behaviours and then record from neurons repeatedly. \"If it only expresses at the right level for a few months and then dies off or expresses too much, it's just untenable,\" says Shenoy. All these questions take time and money to answer, and not every lab has an appetite for the work. \"It's probably going to have a big impact, and I'm definitely interested,\" says Tirin Moore, a neurobiologist at Stanford. \"But I'm less interested in pioneering the approach in monkeys and more interested in using the tools once it's clear that they work.\" Meanwhile, Boyden and Deisseroth are hammering out some of the problems. Last month, Deisseroth reported a system in mice that could make detailed knowledge of promoters unnecessary: two viruses containing separate genetic constructs are injected into two connected brain regions. Only neurons that run between the two regions will receive doses of both constructs, which then interact to express the opsin 5 . Boyden is designing a light source that would weigh a fraction of a gram so that animals can walk around freely rather than being tethered to a fibre-optic cable. \"Soon enough, this is going to be standard technology,\" says Sabes. \"The hardware will be there, the viruses will be there, there will be a handful of constructs that everyone agrees works reasonably well.\" That aim is likely to be furthered by a two-year, $14.9-million grant from the US Defense Advanced Research Projects Agency in Arlington, Virginia, that Sabes, Deisseroth and six other labs, led by Shenoy, won in April. The team will attempt to use primate optogenetics to explore brain repair after injury, including possible light-based neural prosthetics, devices that might stimulate appropriate patterns of activity in the surviving neurons. For now, Sabes is still trying to analyse the results of his first monkey experiment. He has sent the animal's brain to a friend competent in histology, who will slice it, paper thin, onto slides. Then Sabes will have to grapple with a fluorescent microscope for the first time, trying to work out where and how well the opsin was actually expressed. \"I'm sure it's not that hard,\" says Sabes, \"but I've never done it\".  Lizzie Buchen is a freelance writer based in San Francisco. \n                     Philip Sabes' website \n                   \n                     Loren Frank's website \n                   \n                     Karl Deisseroth's website \n                   \n                     Ed Boyden's website \n                   \n                     Robert Desimone's website \n                   \n                     Krishna Shenoy's website \n                   Reprints and Permissions"},
{"file_id": "465024a", "url": "https://www.nature.com/articles/465024a", "year": 2010, "authors": [{"name": "Roff Smith"}], "parsed_as_year": "2006_or_before", "body": "Fifty years ago this month, a massive earthquake in Chile broke new ground in seismic science. Roff Smith looks back at the largest quake ever recorded. When a massive earthquake rocked southern Chile early one Saturday morning in May 1960, those residents who were lucky enough to rise from the rubble could have been forgiven for thinking that they'd just come through the worst that nature could deliver. The force of the earthquake that had just levelled their villages would later be estimated at magnitude 8.1 \u2014 the largest shock Earth had produced in more than a year. As authorities in Santiago scrambled to send aid to the stricken region, they could not have foreseen that such a monumental earthquake was merely a drum roll for the main event. The following afternoon, on 22 May, the earth convulsed so violently that it wobbled the planet and sent it ringing for days on end. In the decades afterwards, seismologists would pore over their printouts trying to understand just how incredibly strong that earthquake was. They would go on to devise a whole new way to measure seismic tremors and assign the 1960 event a value of 9.5 on the logarithmic magnitude scale \u2014 to this day the most powerful earthquake on record. With an energy more than 20,000 times greater than the bomb that destroyed Hiroshima, the earthquake killed at least 1,500 people in Chile and spawned a tsunami 25 metres high that obliterated coastal villages and threw ships at anchor more than a kilometre inland. The tsunami raced across the Pacific Ocean, taking 61 lives in Hilo, Hawaii, before hammering an unsuspecting Japan and killing about 140 people there, 17,000 kilometres away from the quake's epicentre. \"This was a planetary monster,\" says Tom Jordan, director of the Southern California Earthquake Center in Los Angeles. \"An earthquake in South America that killed people in Japan.\" Half a century after the event, the monster continues to fascinate and intrigue, even in the face of recent, deadlier quakes, such as January's disastrous shock in Haiti. This month, the American Geophysical Union will hold a conference on giant earthquakes and tsunamis in Valpara\u00edso, Chile, with many delegates making a 50th anniversary pilgrimage to the site of the 1960 quake to marvel at the seismic scars in the landscape and the sediments deposited by the tsunami that followed. \"Here is the benchmark, the superlative event in recent seismic history,\" says Brian Atwater, of the US Geological Survey's Earthquake Hazards Team in Seattle, Washington, who is a convenor of the conference. It was a benchmark in the history of seismology as well. The Great Chilean Earthquake of 1960 provided seismologists with their first unambiguous evidence of the planet's free oscillations \u2014 the harmonic vibrations that ring the planet like a bell after it has been hit by a major jolt. Over the intervening years, researchers have learned how to use those free oscillations like a planetary CAT scan to discern the inner structure of Earth.  \n                Lifesaving legacy \n              In the decade after 1960, the Chilean quake and the magnitude-9.2 Alaska earthquake of 1964 would both become persuasive arguments in support of the radical theory of plate tectonics \u2014 providing textbook examples of subduction zone earthquakes, in which one tectonic plate is forced under another. Beyond teaching basic lessons about the planet, the quake also left a legacy that has saved lives. It spurred nations around the Pacific to set up an international tsunami warning system in the 1960s. The tsunami deposits left by this event give geologists a model for identifying other spots that might be prone to giant earthquakes, such as the Cascadia subduction zone off the western coast of North America. In many ways, however, the Chilean monster was a quake before its time, coming as it did just on the cusp of a technological and theoretical revolution in Earth sciences. \"Had this earthquake happened only ten years later, we could have learned so much more from it,\" says Seth Stein, a seismologist at Northwestern University in Evanston, Illinois. In 1960, with the concept of plate tectonics still in the future, researchers did not understand how to place the quake in a geophysical context. And the global network of seismometers that could have provided so much information would not be in place for another three years. \n               boxed-text \n             The 1960 event happened when a fault zone running down Chile's coastline ruptured along about 1,000 kilometres of its length (see map). The tectonic plates on either side of the fault slipped 20\u201330 metres past each other \u2014 releasing centuries of accumulated energy in several terrifying minutes. The earthquake was not only the biggest ever recorded, at 9.5 it approaches the upper limit of what the planet is likely to ever produce in a single event. \"This is not to say a bigger quake couldn't eventually happen,\" says Richard Aster, a geophysicist at the New Mexico Institute of Mining and Technology in Socorro and president of the Seismological Society of America. \"It could. But you wouldn't get one much bigger. Faults are only so big and so strong.\" To illustrate the size of this quake, Aster has added up the seismic energy of all the world's earthquakes throughout the twentieth century, including the monster in Chile, and the 9-plus quakes in Kamchatka in 1952 and Alaska in 1964. He threw in the magnitude-9.2 Sumatra quake of 2004 for good measure and imagined all this energy unleashed in a single cataclysmic event. That would equal a magnitude-9.95 earthquake. \"You're just not going to get a 10,\" says Aster. And yet of that mountain of energy \u2014 the entire planet's seismic release for more than 100 years \u2014 one-quarter of it can be attributed to the single catastrophic event centred near Ca\u00f1ete, on the southern Chilean coast. It was twice as powerful as its nearest rival, the 1964 Alaska quake.  \n                Groundbreaking behaviour \n              Among those who will be attending the anniversary commemoration at Valdivia is Hiroo Kanamori of the California Institute of Technology in Pasadena, one of the doyens of seismology who helped create the moment magnitude scale to accurately measure the size of great earthquakes. He has spent the past year re-examining the 50-year-old data from the 1960 earthquake, intrigued not only by the quake's sheer size and scale, but also by its unique behaviour. Kanamori, who recalculated the size of the earliest earthquake in the sequence says \"who would have imagined that you could have an 8.1 as a foreshock? And yet this is just the beginning of a spectacular foreshock sequence.\" In the 33 hours before the main event, there were about 6 quakes larger than magnitude 6, and one with an estimated magnitude 7.8 coming just 15 minutes before the big one itself, he says. \"As far as I am aware there has never been a foreshock sequence like this.\" Perhaps even more curious are some strange, long-period vibrations recorded by a seismogram at Pasadena less than 15 minutes before the main earthquake, says Kanamori. Those readings suggest the big one might have been preceded by a powerful, slow earthquake deep underground, which helped catapult what might otherwise have been 'merely' a giant earthquake into a category all its own. There are some hints that other great earthquakes, such as the 1944 Tonankai shock in Japan, may also have had these slow precursors, which on their own do not produce damaging vibrations. But as with the Great Chilean Earthquake, the fragmentary evidence tantalizes more than reveals. Unfortunately, the violence of the foreshocks in Chile obscured the readings on the handful of other high-quality seismometers that were working at the time, so there is no independent confirmation for the curious readings at Pasadena. But local eyewitnesses also hint that something unusual happened in the moments preceding the magnitude-9.5 main shock. Two geophysicists in Concepci\u00f3n, about 200 kilometres from the quake's epicentre, recalled that the quake began with a gentle rocking sensation rather than an abrupt jolt that typically heralds the start of a giant shock. \"One of them noticed parked cars rolling back and forth on the street,\" Kanamori says. \"This is a very unusual description of the beginnings of a great earthquake, but these were trained seismologists so you have to take them seriously.\" If there was indeed such a deep, slow precursor event, it could make the Great Chilean Earthquake of 1960 of wider relevance. \"This is very similar to how many scientists believe a great subduction-zone earthquake would start along the Cascadia plate boundary,\" says Kanamori. That region has bouts of slow earthquakes about every 14 months or so, which prompts some researchers to suspect that the subduction zone there might share similarities with the one off Chile. There are no written records of great earthquakes happening in the Pacific Northwest, but Atwater and other geoscientists have become convinced that quakes in the magnitude-9 range have pummelled that region in the past. In fact, sand deposits left by the 1960 Chilean tsunami helped Atwater and others discover that giant waves once pounded the coasts of western North America and Japan from a Cascadian earthquake centuries ago. Giant jolts such as these make difficult subjects for scientists to study because they strike only once every 300 years \u2014 or even less frequently. But the globe is now wired up with hundreds of sensitive and readily accessible seismometers. So when the next monster strikes, whenever and wherever it may be, says Kanamori, \"we will be able to understand it far better than we did for the 1960 Chile earthquake\".  Roff Smith is a freelance writer based in Hastings, UK. \n                     Nature Geoscience \n                   \n                     AGU Chapman Conference on Giant Earthquakes and Their Tsunamis \n                   \n                     Seismological Society of America \n                   \n                     USGS webpage on 22 May 1960 Chilean Earthquake \n                   Reprints and Permissions"},
{"file_id": "465150a", "url": "https://www.nature.com/articles/465150a", "year": 2010, "authors": [{"name": "Meredith Wadman"}], "parsed_as_year": "2006_or_before", "body": "In February, a biologist gunned down three colleagues at the University of Alabama, Huntsville. Meredith Wadman reports how their department is trying to move past the tragedy. Last month, Joseph Ng, a biologist at the University of Alabama, Huntsville (UAH), sat down with very mixed feelings to write a job advertisement for a new chair of the biology department. The provisional draft said that the department was seeking \"an energetic and visionary leader\" who could preside over the hiring of several junior faculty members. What the ad didn't talk about, and couldn't possibly describe, were the events that left so many holes to fill. On a Friday afternoon in early February, Amy Bishop, an assistant professor in the department, pulled out a black 9-millimetre pistol during a biology faculty meeting. \"She just went down the line\", wearing a look that was \"cold, very cold\", says Ng. At point-blank range, Bishop shot five of her colleagues in the head, killing three of them and critically wounding two others. Ng, seated at the opposite end of the table, thought she would murder them all. In the space of seconds, Bishop cut the 14-strong faculty by more than a third. Ever since, the survivors have been struggling with the enormous task of repairing the shattered department even as they try to heal their own emotional wounds. With 473 undergraduate majors, the biology department is the second largest on the Huntsville campus, and the shooting left nine courses without teachers. Twenty-one master's and doctoral students suddenly had no mentors. And seven research grants lost their principal investigators (see  'Keeping research grants alive' ). Administrators and faculty members have rallied to keep the department running: they found substitute teachers, temporarily recalled the department's previous chairman and his assistant from retirement, and sought new mentors for graduate students. They took over the orphaned grants (except for Bishop's own project) and added dozens of student advisees to their lists of responsibilities \u2014 all on top of carrying their own teaching and research loads. \"Right now, it's a sort of managed chaos. As each thing comes up, we deal with it,\" says Debra Moriarity, a biologist who is also dean of graduate studies and was in the conference room that day in February. The surviving faculty members know that their success or failure will have effects well beyond the campus. The city of Huntsville has been building itself into a regional biotechnology research hub and has relied on the biology department to supply many of the ideas and scientists that feed that enterprise. Such ambitions were dealt a blow by Bishop's attack \u2014 the latest of the mass shootings that have become all too common on US campuses. This weekend, the remaining faculty members in the Huntsville biology department will take a moment to celebrate the graduation of 40 biology undergraduates, four master's students and a PhD candidate. But it will be only a temporary respite. All but two of the surviving faculty members witnessed the shooting and must cope with having seen friends gunned down before their eyes and thinking that their own turn was seconds away. In the aftermath of the shooting, the biologists attended counselling sessions, but the true scale of the loss is still sinking in (see  'The department that was' ). \"There are times when you feel very, very empty,\" says Ng, who has carried out research in structural biology in the department for 12 years. On 12 February at 3 p.m., members of the biology faculty gathered for a meeting in Room 369R \u2014 a small, windowless conference room tucked away in a corner of the Shelby Center for Science and Technology, where the biology department occupies the whole of the third floor. Budgeting issues were on the agenda and attendance was good: 12 of the 14 faculty members squeezed around the table. Stephanie Monticciolo, the department's staff assistant, joined them to take notes. Bishop arrived shortly before the meeting started and positioned herself at the corner of the table nearest the door. Roughly 50 minutes into the meeting, she produced what Ng remembers as \"a big, black pistol\". Aiming for the head in each case, Bishop fired systematically, getting off at least five rounds in what survivors say was probably 20 to 30 seconds. Starting on her immediate right, Bishop aimed at the face of Gopi Podila, killing the department's affable chairman, who was a nationally recognized plant biochemist. She then shot Monticciolo in the left temple, gravely injuring her. Bishop turned to her left and killed Adriel Johnson, a gastrointestinal physiology expert, who had made a career of encouraging minority students into science. The next lethal shot hit plant scientist Maria Ragland Davis, who had come to the university from Research Genetics, a local biotechnology firm. A ricocheting bullet or bone fragment hit Luis Cruz Vera, a molecular biologist and the department's newest faculty member, who sustained a minor chest wound. Bishop then fired at Joseph Leahy, sending a bullet slicing through his skull and critically wounding him. She pointed the gun next at Moriarity and pulled the trigger, but the gun jammed and wouldn't fire. After Bishop stepped into the hall, the survivors locked and barricaded the door with a coffee table and called the police. Bishop was arrested a few minutes later outside the building and has been charged with capital murder and attempted murder. She is being held without bail while prosecutors prepare to bring her case before a grand jury. Bishop's husband, James Anderson, whom she called to pick her up after the shooting, was briefly detained by police and released. The couple has four children, aged 8 to 18 at the time of the shootings. Huntsville, nicknamed 'Rocket City', has been a space- and engineering-science Mecca ever since 1950, when German rocket scientist Wernher von Braun was brought to the tiny cotton town to develop ballistic missiles for the US Army. Today, Huntsville and its suburbs boast a population of 387,000, with a higher percentage of engineers than any other US city. A decade ago, as human-genome sequencers raced to the finish line, biologists in Huntsville decided it was time to put their city on the map for a different reason. They set themselves the ambitious goal of building Huntsville into a biotechnology hub; the region's answer, in spirit if not in size, to the San Francisco Bay Area (see  Nature    453,   818\u2013820; 2008 ). They were led by Jim Hudson, a local entrepreneur whose biotech reagents company, Research Genetics, had been bought in 2000 by Invitrogen of Carlsbad, California, for more than US$200 million. Hudson, working with an anonymous donor, eventually launched the non-profit HudsonAlpha Institute for Biotechnology to conduct genomics research, educate the public and provide a home to fledgling biotech firms. The institute opened its doors in 2008, in a huge research park three miles from UAH. Today, it houses 13 of the city's 20 biotechnology companies. Ties between HudsonAlpha and the UAH biology department are tight. Many graduates from the department have gone on to work at the biotech start-ups housed at HudsonAlpha, and half of the institute's academic faculty members are adjunct professors at UAH. One of the many tenants at the institute with links to the biology department is iXpressGenes, which Ng launched to synthesize genes and enzymes for researchers. Within days of the attack, HudsonAlpha's scientists were stepping in to help. They teamed up to take over Bishop's neuroscience class. And the institute has taken in half of the now-mentorless doctoral students, who have been doing rotations in HudsonAlpha labs to find a new PhD adviser. Rick Myers, who directs the institute, is also helping in the search for a new UAH biology chairman. That person will be expected to work closely and collegially, as the late Podila did, with HudsonAlpha, says Ng. There is particular concern about supporting the university's interdisciplinary doctoral programme in biotechnology, which started in 2001 and is headed by Ng.\"For biotech to succeed here and for HudsonAlpha to succeed, we need people coming through that [PhD] programme,\" says Troy Moore, an entrepreneur at HudsonAlpha who studied biology at UAH. He went on to co-found Huntsville-based Open Biosystems, which was acquired in 2008 by Thermo Fisher Scientific of Waltham, Massachusetts. One of his first thoughts on learning of the shootings, Moore says, was: \"Oh no, who's going to want to come into that programme?\" But UAH administrators are confident it will survive. \"The faculty members who put the PhD programme together were actually very astute in making sure that it wasn't built on just one or two faculty members,\" says Vistasp Karbhari, the UAH provost. \"Normally I would worry about a programme [in this situation]. In this case, I am not worried.\" One of the longest-serving professors in the biology department is Moriarity, an outgoing scientist who studies growth-factor signalling. A department stalwart since 1982, Moriarity had become in recent years one of Bishop's closest colleagues. The two were even talking about submitting a grant proposal together to the National Institutes of Health to study an enzyme that is thought to inhibit breast cancer. Moriarity was glad to make the faculty meeting that Friday in February because her commitments as a dean often prevented her attending. As the meeting neared its end, Moriarity was jotting something on her notepad when the first shots rang out. By the time Moriarity looked up, Bishop had already hit two people and was rapidly selecting new targets. \"There was only a second or two seconds between\" each shot, Moriarity says. \"At that point I realized what was happening. And all I thought was: 'This has to stop'.\" Moriarity crawled under the table and grabbed one of Bishop's legs, pleading with her to end the attack. Bishop freed herself, chased Moriarity to the room's doorway and pulled the gun's trigger. It just clicked. Moriarity rolled into the corridor outside and Bishop followed her. Moriarity saw \"a very, very evil-looking stare\" in the face of her colleague. \"She pointed the gun at me again and it clicked,\" Moriarity recalls. \"At that point, I kind of threw myself back into the room and shut the door. As I was doing that I heard 'click, click, click'.\" Bishop had been quirky and outspoken ever since she arrived at the university in 2003. She came to Huntsville from Harvard University in Cambridge, Massachusetts, where she had earned her PhD a decade earlier. During a postdoc position in Bruce Demple's lab at the Harvard School of Public Health in Boston, she had studied the role of nitric oxide in central nervous system injury and diseases. At UAH, she continued that research, along with teaching introductory neuroscience, anatomy and physiology. She also spent what some colleagues considered an inordinate amount of time developing a portable cell incubator with her husband, a biologist and computer engineer. They helped found a company, Prodigy Biosystems, to license the incubator from the university and commercialize it. The device has drawn $2.25 million in funding from local investors \u2014 $1 million of that since early March \u2014 making Bishop and Anderson a local biotech success story. Dick Reeves, the chairman of Prodigy Biosystems, says that the company formally parted ways with Bishop's husband in April, although Anderson says he has not yet accepted the severance agreement offered by Prodigy. All that outside activity did not help Bishop \u2014 who turned 45 just before the shooting \u2014 when it came to her tenure decision. She was denied tenure in March 2009 and the university rejected her appeal in November. After the tenure denial, \"she started to get a lot more agitated about things\", says Moriarity. Bishop found someone who told her how each of her colleagues had voted on her tenure application. According to Moriarity, the decision \"was not super close\". Moriarity has no doubt that the right call was made on Bishop, who had a history of causing graduate students to leave her lab in search of more congenial work environments. At the time of the shootings, Bishop was not supervising any PhD students. \"Her teaching was weak,\" says Moriarity. \"And her research output was very weak.\" She had received repeated warnings in her annual reviews to publish more papers, says Moriarity. In the past three years, Bishop published two papers in the  Journal of Neurochemistry,   and one in the  Inter-national Journal of General Medicine , which includes three of her children as authors. Yet Moriarity is hard pressed, in hindsight, to find red flags. Bishop was vocal, and highly strung, but so are lots of academics. She had a big ego, but so do plenty of other scientists. \"As long as she worked here, she was just sort of odd in her reaction to things. But not violent,\" Moriarity says. Within days of the murders, however, Bishop's history of violence emerged. In 1986, when she was 21, Bishop shot and killed her 18-year-old brother with her father's shotgun, which was ruled to be an accident. Last month, an inquest into that shooting was held, and the District Attorney in Norfolk County, Massachusetts, may decide to seek an indictment against Bishop in her brother's death. Bishop was also charged with assault in 2002 after she punched a woman in the head in a Massachusetts restaurant; the woman had taken the last child's booster seat there. Bishop was sentenced to probation in that case. Nearly a decade earlier, Bishop and her husband were both questioned by law-enforcement authorities in connection with a mail bomb sent to Paul Rosenberg, a neurologist and neuroscientist at Children's Hospital Boston and Harvard Medical School. Bishop had worked for Rosenberg as a postdoc for eight months, until the end of November 1993. He received the bomb in December of that year after a discussion with her about whether he would be able to write her a strong letter of recommendation. That case has not been solved, but the US Attorney's office in Boston issued a statement after the February shooting, saying, \"We have commenced a thorough review of the information related to this incident to confirm that all appropriate steps were taken in that matter.\" Anderson says the case was closed in 2001 and he told  The New York Times   in February that he and his wife had nothing to do with the bomb. On 11 February, the day before the Huntsville shootings, Kimberly Hobbs, a third-year PhD student in biology, was wrestling with a problem: she had to take blood from rats for her studies of pancreatic function in diabetes, but she always seemed to miss the tiny veins in their tails. She sought out her mentor and thesis adviser, Adriel Johnson, for help. \"Watch what I'm doing and you'll be able to do it too,\" said Johnson, showing Hobbs how to infuse the tail vein with blood by stroking it from the base to the tip, and how to insert the needle directly on top of the vein. Johnson had been the first faculty member Hobbs had met when, as a high-school senior from Chicago, she toured the Huntsville campus in 1998 with her mother, trying to decide between UAH and closer-to-home competitors such as the University of Minnesota. Meeting Johnson clinched her decision. It wasn't only that he was African-American, like herself. The man's demeanour reassured Hobbs and her mother. \"We felt that he genuinely cared about students and wanted you to do well.\" But Johnson hardly pampered his recruits. On the contrary, he was known for his tough love, which included requiring them to work extra hours designing lab experiments. Without him, Hobbs is preparing for biology and biochemistry cumulative exams this summer as well as searching for a new thesis adviser. No remaining UAH biologist works in her area, so Hobbs has been observing in several labs at the HudsonAlpha Institute, and is likely to move her doctoral work there this summer. Transferring to HudsonAlpha, with its supercharged intellectual atmosphere, could be a great career move, but she has mixed feelings about taking advantage of an opportunity created by a tragedy. With Johnson gone, the department has a large hole to fill in terms of recruiting and mentoring minority students. Davis also supported disadvantaged minority students. These efforts were particularly important in Alabama, where African Americans make up more than a quarter of the population but only 13% of science graduate students. The president of UAH, David Williams, calls this dimension of the shootings \"the saddest thing in so many respects\". The biology faculty \"was in so many ways a poster child for the diverse scientific department,\" he says. \"That's something that we need to work to preserve and build on.\" The last time Hobbs saw Johnson was when he showed her how to draw blood. The next afternoon, just after 3 p.m. on Friday 12 February, Hobbs confronted the rats again, by herself. She quickly succeeded, with all the animals. \"I was so happy,\" she laughs, recalling how she resolved to find Johnson and tell him immediately. But she eventually decided to wait until Monday. A while later, Hobbs walked out of Johnson's third-floor lab, heading for her office down the corridor, past Conference Room 369R. She ran straight into two black-clad policemen bearing rifles. \"I just put my hands up and said 'I work here.' I didn't know what was going on.\" Hobbs was sent outside, where she waited in the chill air as paramedics emerged with stretchers carrying the wounded. She didn't recognize Monticciolo or Leahy as they were borne past her, beyond the fact that they were people, and that they were grievously wounded. \"That's when I knew it was real,\" Hobbs says. At the time of the shootings, Leland Cseke was not in the conference room with the other faculty members. As a research assistant professor who is not tenure track and does not normally teach classes, Cseke was not required to attend and Podila had given him the option of bowing out. Cseke had spent much of his adult life working with Podila in one way or another. They met in 1990, when Cseke was an undergraduate at Michigan Technological University in Houghton, and Podila was his adviser. Cseke was drawn by Podila's commitment to using plant molecular biology to tackle environmental issues. In 2002, when Podila was recruited to chair the biology department at UAH, he brought along Cseke, who by then had earned a doctorate and had become Podila's chief lab lieutenant. Now he is on his own. \"I lost my closest colleague,\" says Cseke, who since the shootings has been trying to do the work of two men. He routinely puts in 13-hour days running the Podila lab, aided by a cadre of students without whom he would be \"screwed\", he says. He has become faculty adviser for four doctoral students and also oversees two master's students and five undergraduates. In Podila's stead, he is teaching a course in advanced molecular techniques. In addition to his own research, he is working to keep up with the demands of Podila's grants from the Argonne National Laboratory in Illinois and the Energy and Resources Institute in New Delhi, India. When Cseke gets overwhelmed, he looks at a smiling picture of Podila he has propped up on top of a filing cabinet in his office. He is not sure what will happen in the long run \u2014 he knows he can't keep up his current pace indefinitely. Cseke may be considered for one of the tenure-track jobs that has opened up, but that is not a certainty. Shortly before 4 p.m. on the day of the faculty meeting, Cseke remembers pulling his office door closed and heading towards the lifts to the fourth-floor greenhouse. As he walked down the hall, he saw Amy Bishop coming towards him from the direction of the conference room, \"running fast \u2014 faster than I have ever seen her\". \"She said: 'Hey, Leland,' like nothing had ever happened. She clearly was panicked and I was tempted to ask her if everything was okay. I figured she must be really late to something. So I just let her go.\" Cseke went up to the greenhouse and was pondering how to tackle a white-fly infestation when a SWAT team burst through the greenhouse door. They threw him on the floor and frisked him before sending him down the stairs and out of the building. Over the past few months Cseke has disciplined himself to stop replaying that day in his head. Among the 'what ifs' that tormented him was the fact that he is a seventh-degree black belt in bujinkan budo taijutsu, a traditional Japanese martial art. He wondered if he could have disarmed Bishop, if he had attended the meeting. \"If only I could have been there, maybe to have done something,\" he says. \"But eventually I figured out that I just wasn't given that opportunity.\" Instead, Cseke continued on his way as Bishop fled from the floor. She later borrowed a phone and called her husband to pick her up. While waiting at the building's loading dock, she was arrested by police. On 23 March, Bishop appeared in a Huntsville courtroom, handcuffed and dressed in orange prison clothes. During a preliminary hearing, police investigator Charlie Gray testified that police searching the building on 12 February had found a bloodstained woman's jacket and a 9-milli-metre pistol in a rubbish bin in the second-floor toilets of the Shelby Center. Gray also testified that Bishop had told police that the shooting hadn't happened, that she had not been present at the meeting and that \"it wasn't her\". The presiding judge at the preliminary hearing, Ruth Ann Hall, ruled that Bishop should be kept in custody without bail, and that her case should be turned over to a grand jury, which will examine the possibility of indicting her. That process will take months. Hall also imposed a gagging order on attorneys in the case. Before that, Bishop's lawyer, Roy Miller, had told the media that he will be mounting an insanity defence, but she could plead in a different way if this case moves to trial. In Alabama, capital murder is punishable with life in prison without parole, or with the death penalty. In late April, a crowd of minority students gathered at a research conference in the Westin Hotel in Huntsville. Normally, the conference would have been overseen by Johnson. Instead, the registration material included a copy of a resolution by the Board of Trustees of the University of Alabama, expressing their deep sorrow at Johnson's loss. Many of the students are supported by 'Bridge to the Doctorate' awards from the National Science Foundation, the same type of grant that Johnson had helped Hobbs to land six years earlier. During a break from her duties at the conference, Hobbs says that she has been coping by \"keeping myself busy\". She has been visiting labs at HudsonAlpha and continuing to work with the diabetic rats in Johnson's lab. Now that the initial shock has worn off, a new species of desolation has set in. The once-collegial third floor of the Shelby Center, where she used to enjoy hanging out, has become a lonely place that she leaves as soon as she can. \"Every time you are in the building you are thinking about it,\" she says. \"On Fridays, when the clock strikes three or four, you are thinking about it.\" Ng is also at the Westin Hotel on this April morning, judging the poster competition and energetically quizzing a freshman nursing student about her work. Ng has been working with his colleagues to develop strategies for moving ahead. They have already posted job vacancies for three visiting professors to start in time for the new academic year in September, and they will soon send out the job announcement for a permanent chairman. The events of the past few months have refocused him. \"Your biggest problems all became minuscule. The things you worry about: getting a manuscript done, the grant proposal that didn't make it. All that stuff just became low priority.\" After months of avoiding the lab at night, he is starting to work late there again, \"accepting that if something happens, it will happen\". He has also lost his obsession with collecting news articles about the shootings. \"The adrenaline is gone,\" he says. But the sadness has moved in. \"You go into the building and you are really missing these people.\" Moriarity feels much the same. \"I told somebody a week ago that I felt worse than I have the whole time,\" she says. She also sees similar signs in her students. \"I have had a number of good students who are not doing well at all now. They come in to me and say, 'I just can't get my mind on it'. I send them all to counsellors.\" Moriarity talks to a lot of students. Like her colleagues, she has doubled her advising load and is now mentoring an additional 30 or so undergraduates who had been the charges of Podila, Johnson or Davis. She has also taken on Johnson's mantle as the special adviser for undergraduates bound for the health professions. Moriarity tries to stay focused on the positives. She and colleagues were buoyed when Monticciolo, the staff assistant, was discharged from a local hospital in March. And they all rejoiced when Joe Leahy returned to the department for a visit in late April. Leahy, who had spent more than two months in hospital, was wearing a helmet covering a spot where his skull had been shattered. Eventually a metal plate will be inserted over the hole. He also had an external jaw brace that was scheduled to be removed three days later. Bishop's last bullet had severed his right optic nerve, blinding his right eye. He had also had lost peripheral vision in his left eye. Still, says Moriarity, he was \"the same old Joe\". Students and faculty members clustered around him and he was eagerly planning his return to work. It was uplifting \"just to know that even though he had such a terrible injury, that it hasn't taken him away\", says Moriarity. \"I was so, so glad to see the amazing progress he has made.\" \n                     University of Alabama, Huntsville department of biology \n                   \n                     Joseph Leahy recovery blog \n                   \n                     Stephanie Monticiollo recovery blog \n                   \n                     HudsonAlpha Institute for Biotechnology \n                   Reprints and Permissions"},
{"file_id": "464344a", "url": "https://www.nature.com/articles/464344a", "year": 2010, "authors": [{"name": "Laura Spinney"}], "parsed_as_year": "2006_or_before", "body": "A single incriminating fingerprint can land someone in jail. But, Laura Spinney finds, there is little empirical basis for such decisions. The terrorist explosions that ripped through Madrid's crowded commuter trains on the morning of 11 March 2004 killed 191 people, wounded some 2,000 more and prompted an international manhunt for the perpetrators. Soon after, Spanish investigators searching the area near one of the blasts discovered an abandoned set of detonator caps inside a plastic bag that bore a single, incomplete fingerprint. They immediately shared the clue with law-enforcement colleagues around the world. And on 6 May 2004, the US Federal Bureau of Investigation (FBI) arrested Oregon lawyer Brandon Mayfield, proclaiming that his print was a match. Two and a half weeks later, a chagrined FBI was forced to release Mayfield after Spanish police arrested an Algerian national \u2014 one of several terrorists later charged with the crime \u2014 and found one of his fingerprints to be a much better match. The FBI eventually admitted that it had made multiple errors in its fingerprint analysis 1 . The Mayfield case is a textbook example of 'false positive' fingerprint identification, in which an innocent person is singled out erroneously. But the case is hardly unique. Psychologist Erin Morris, who works with the Los Angeles County Public Defender's Office, has compiled a list of 25 false positives, going back several decades, that is now being used to challenge fingerprint evidence in US courts. Those challenges, in turn, are being fed by a growing unease among fingerprint examiners and researchers alike. They are beginning to recognize that the century-old fingerprint-identification process rests on assumptions that have never been tested empirically, and that it does little to safeguard against unconscious biases of the examiners. That unease culminated last year in a stinging report by the US National Academy of Sciences (NAS) 2 , which acknowledged that fingerprints contain valuable information \u2014 but found that long-standing claims of zero error rates were \"not scientifically plausible\". Since then, fingerprint examiners have found themselves in an uncomfortable situation. \"How do you explain to the court that what you've been saying for 100 years was exaggerated, but you still have something meaningful to say?\" asks Simon Cole, a science historian at the University of California, Irvine. The only way out of the dilemma is data, says Cole: do the research that will put fingerprinting on solid ground. And that is what researchers are starting to do. In January, for example, the US Department of Justice's research branch, the National Institute of Justice, launched the first large-scale research programme to classify fingerprints according to their visual complexity \u2014 including incomplete and unclear prints \u2014 and to determine how likely examiners are to make errors in each class. \"The vast majority of fingerprints are not a problem,\" says Itiel Dror, a cognitive psychologist at University College London who is involved in the study. \"But even if only 1% are, that's thousands of potential errors each year.\"  \n                Leaving a mark \n              Even fingerprinting's harshest critics concede that the technique is probably more accurate than identification methods based on hair, blood type, ear prints or anything else except DNA. Granted, no one has ever tested its underlying premise, which is that every print on every finger is unique. But no one seriously doubts it, either. The ridges and furrows on any given fingertip develop in the womb, shaped by such a complex combination of genetic and environmental factors that not even identical twins share prints. Barring damage, moreover, the pattern is fixed for life. And thanks to the skin's natural oiliness, it will leave an impression on almost any surface the fingertip touches. The concerns start with what happens after a fingerprint, or 'mark', is found at a crime scene and sent to the examiners. The problem lies not so much with the individual examiners, most of whom have undergone several years of specialist training, but more with the ACE-V identification procedure they follow in most countries (see graphic). The acronym stands for the four sequential steps of analysis, comparison, evaluation and verification \u2014 the hyphen signifying that the last step is carried out by a different individual, who repeats the first three. \n               boxed-text \n             The analysis phase starts at the gross level, where there are three main patterns \u2014 loops, whorls and arches \u2014 that can be used to classify prints or to rapidly exclude suspects. Then comes a second level of analysis, which focuses on finer details, such as bifurcations and ridge endings (see graphic), which are highly discriminating between individuals. If necessary, the examiner can bore down to a third level of detail, related to the shape of ridge edges and the pattern of pores. Having analysed a mark and noted its distinctive features, the examiner then goes to the comparison step: checking for similarities or differences with a reference fingerprint, or 'exemplar', retrieved from law-enforcement files or taken from a suspect. This part of the process has become increasingly automated, first with the development of automatic fingerprint identification systems (AFIS) in the 1980s, then with the advent of digital print-capture technology in the 1990s. Today's AFIS technology can scan though the vast fingerprint databases compiled by the FBI and other agencies and automatically filter out all but a handful of candidate matches to present to the examiner. The examiner will then winnow the candidates down by eye. According to the ACE-V protocol, the third step, evaluation, can lead the examiner to one of three conclusions: 'identification', meaning that mark and exemplar came from the same finger; 'exclusion', meaning that they did not, as there is at least one significant difference that cannot be explained by factors such as smearing; and 'inconclusive', meaning that the mark is not clear enough for the examiner to be sure. \"The system as it's designed purposely produces false negatives,\" says legal scholar Jennifer Mnookin of the University of California, Los Angeles. Because the protocol makes it possible to have one difference and exclude a match, but a lot of similarities and still not be sure, it builds in a preference for missing the identification of a criminal rather than risking the conviction of an innocent person. Yet, as the Mayfield case illustrates, false positives can slip through the net. In Scotland, for example, an ongoing inquiry is trying to understand how a fingerprint found at a murder scene was wrongly attributed to police officer Shirley McKie, leading her to be falsely accused of perjury. Such errors may not come to light until some other, incontrovertible piece of evidence trumps the fingerprint, or until the prints are reanalysed in an internal review. But for examiners and researchers alike, the urgent question is why they happen at all. One of the problems with the ACE-V procedure lies in sloppy execution. For example, the protocol calls for the analysis and comparison steps to be separated, with a detailed description of the mark being made before an examiner ever sees an exemplar. This is to prevent circular reasoning, in which the presence of the exemplar inspires the 'discovery' of previously unnoticed features in the mark. But this separation doesn't always happen, says forensic-science consultant Lyn Haber, who together with her husband, psychologist Ralph Haber, co-authored the 2009 book  Challenges to Fingerprints . To save time, she says, many examiners do the analysis and comparison simultaneously. The FBI highlighted this as a factor contributing to the Mayfield error.  \n                Misprints \n              Another problem is that the ACE-V protocol itself is sloppy, at least by academic standards. For example, it calls for the final verification step to be independent of the initial analysis, but does not lay down strict guidelines for what that means. So in practice, the verifier often works in the same department as the first examiner and knows whose work he or she is checking \u2014 not a form of 'independence' with which many scientists would be comfortable. Nor is ACE-V especially strict about what examiners can and cannot know about the case on which they are working. This is especially worrying in light of a study 3  published in 2006 in which Dror and his colleagues showed that both experienced and novice fingerprint examiners can be swayed by contextual information. In one experiment, the researchers presented six examiners with marks that, unbeknown to them, they had analysed before. This time, the examiners were furnished with certain details about the case \u2014 that the suspect had confessed to the crime, for example, or that the suspect was in police custody at the time the crime was committed. In 17% of their examinations, they changed their decision in the direction suggested by the information. This point is emphasized by the conclusion in last year's NAS report that \"ACE-V does not guard against bias; is too broad to ensure repeatability and transparency; and does not guarantee that two analysts following it will obtain the same results.\" For many critics this is the central issue: fingerprint analysis is fundamentally subjective. Examiners often have to work with incomplete or distorted prints \u2014 where a finger slid across a surface, for example \u2014 and they have to select the relevant features from what is available. What is judged relevant therefore changes from case to case and examiner to examiner. Several research groups are now looking at this problem, with a view to understanding and improving the way that experts make a judgement. The FBI has an ongoing study looking at the quantity and quality of information needed to make a correct decision. Dror's group is doing a controlled study of the errors made by examiners in which they are given marks, told they have been taken from a crime scene \u2014 they were actually made by Dror \u2014 and asked to identify them.  \n                Expert testimony \n              Other critics have wondered whether any examiner truly qualifies as an expert. As the Habers point out in their book, examiners rarely find out whether their decision was correct, because the truth about a crime is often not known. As a result, they write, \"even years of experience may not improve [an examiner's] accuracy\". Some fingerprint examiners have simply rejected these criticisms. In 2007, for example, the chairman of Britain's Fingerprint Society, Martin Leadbetter, wrote in the society's magazine 4  that examiners who allow themselves to be swayed by outside information are either incompetent or so immature they \"should seek employment at Disneyland\". But others have taken the criticisms to heart. After hearing about Dror's research on bias, Kevin Kershaw, head of forensic identification services at Greater Manchester Police, one of Britain's largest police forces, decided to buffer his examiners from potentially biasing information by preventing investigating officers from coming on-site to wait for results, and potentially talking to the examiners about the case. This is made easier by the fact that in Manchester, as in many British police forces, the forensic division is separated from the others. In the United States, by contrast, most of the fingerprint work is done inside police departments \u2014 a situation that the NAS report recommended be changed. Kershaw also invited Dror to come and teach his examiners about the dangers of bias, and he changed his service so that the verifier no longer knows whose work he or she is checking. Finally, as Dror's research indicated that the decisions that are most susceptible to bias are those in which the mark is unclear or hard to interpret, Kershaw introduced blind arbitration in cases in which examiners disagree. Safeguards against bias are relatively easy to put in place, but another potential source of error might be harder to eliminate. It has to do with how faithfully the pattern on a finger is reproduced when it is inked or scanned to create an exemplar. No reproduction is perfect, notes Christophe Champod, an expert in forensic identification at the University of Lausanne in Switzerland. So a mark recovered from a crime scene could match exemplars from more than one person, or vice versa, he says. Exacerbating the problem is the continued growth in the exemplar databases that AFIS have to search. Champod thinks that the language of certainty that examiners are forced to use hides this uncertainty from the court. He proposes that fingerprint evidence be interpreted in probabilistic terms \u2014 bringing it in line with other forensic domains \u2014 and that examiners should be free to talk about probable or possible matches. In a criminal case, this would mean that an examiner could testify that there was, say, a 95% chance of a match if the defendant left the mark, but a one in a billion chance of a match if someone else left it. To be able to quote such odds, however, examiners would need to refer to surveys showing how fingerprint patterns vary across populations and how often various components or combinations of components crop up. For example, is a particular configuration of bifurcations, ridge endings and the like found in 40% of a given population or in 0.4%? Some research has been done on this issue, but not on a sufficiently large or systematic scale. Nevertheless, Champod is optimistic that a probabilistic system is within reach. Unlike with DNA, he says, which has strong subpopulation effects, fingerprint patterns vary little between populations, simplifying the task. A probabilistic approach would not replace the examiner or address bias, but it would render the decision-making process less opaque. \"Once certainty is quantified, it becomes transparent,\" says Champod. Ultimately, however, it is for the courts to decide how much weight they accord to fingerprint evidence. The fact that courts still routinely treat it as infallible \u2014 which means a single incriminating fingerprint can still send someone to jail \u2014 strikes Mnookin as \"distressing jurisprudence\". Champod, too, would like to see its importance downgraded. \"Fingerprint evidence should be expressed by fingerprint examiners only as corroborative evidence,\" he says. If other strands of evidence limit the pool of suspects, then a fingerprint is much less likely to be misattributed. To date, judges haven't shown much inclination to alter the status quo. But to be fair, says Barry Scheck, co-director of the Innocence Project \u2014 a group in New York that campaigns to overturn wrongful convictions \u2014 they haven't been given a viable alternative. The probabilistic approach is not yet ready for court. But that may be about to change if researchers can come up with ways to help fingerprinting profession re-establish itself on a more scientific footing. A cultural change will also be needed, within both the fingerprint community and the legal system. \"This is all about adding a culture of science to the forensic-science community,\" says Harry Edwards, a senior judge on the District of Columbia circuit and co-chair of the NAS committee that produced last year's report. \"From what I have seen, we still have a long way to go.\"\n   See Editorial,  \n                     page 325 \n                    ; Opinion,  \n                     page 351 \n                    ; and online at  \n                     www.nature.com/scienceincourt \n                   . Laura Spinney is a freelance writer based in Lausanne, Switzerland. \n                     The Innocence Project \n                   \n                     Itiel Dror's website \n                   \n                     The Fingerprint Inquiry \n                   Reprints and Permissions"},
{"file_id": "465412a", "url": "https://www.nature.com/articles/465412a", "year": 2010, "authors": [{"name": "Sharon Weinberger"}], "parsed_as_year": "2006_or_before", "body": "Can the science of deception detection help to catch terrorists? Sharon Weinberger takes a close look at the evidence for it. In August 2009, Nicholas George, a 22-year-old student at Pomona College in Claremont, California, was going through a checkpoint at Philadelphia International Airport when he was pulled aside for questioning. As the Transportation Security Administration (TSA) employees searched his hand luggage, they chatted with him about innocuous subjects, such as whether he'd watched a recent game. Inside George's bag, however, the screeners found flash cards with Arabic words \u2014 he was studying Arabic at Pomona \u2014 and a book they considered to be critical of US foreign policy. That led to more questioning, this time by a TSA supervisor, about George's views on the terrorist attacks on 11 September 2001. Eventually, and seemingly without cause, he was handcuffed by Philadelphia police, detained for four hours, and questioned by Federal Bureau of Investigation agents before being released without charge. George had been singled out by behaviour-detection officers: TSA screeners trained to pick out suspicious or anomalous behaviour in passengers. There are about 3,000 of these officers working at some 161 airports across the United States, all part of a four-year-old programme called Screening Passengers by Observation Technique (SPOT), which is designed to identify people who could pose a threat to airline passengers. It remains unclear what the officers found anomalous about George's behaviour, and why he was detained. The TSA's parent agency, the Department of Homeland Security (DHS), has declined to comment on his case because it is the subject of a federal lawsuit that was filed on George's behalf in February by the American Civil Liberties Union. But the incident has brought renewed attention to a burgeoning controversy: is it possible to know whether people are being deceptive, or planning hostile acts, just by observing them? Some people seem to think so. At London's Heathrow Airport, for example, the UK government is deploying behaviour-detection officers in a trial modelled in part on SPOT. And in the United States, the DHS is pursuing a programme that would use sensors to look at nonverbal behaviours, and thereby spot terrorists as they walk through a corridor. The US Department of Defense and intelligence agencies have expressed interest in similar ideas. Yet a growing number of researchers are dubious \u2014 not just about the projects themselves, but about the science on which they are based. \"Simply put, people (including professional lie-catchers with extensive experience of assessing veracity) would achieve similar hit rates if they flipped a coin,\" noted a 2007 report 1  from a committee of credibility-assessment experts who reviewed research on portal screening. \"No scientific evidence exists to support the detection or inference of future behaviour, including intent,\" declares a 2008 report prepared by the JASON defence advisory group. And the TSA had no business deploying SPOT across the nation's airports \"without first validating the scientific basis for identifying suspicious passengers in an airport environment\", stated a two-year review of the programme released on 20 May by the Government Accountability Office (GAO), the investigative arm of the US Congress. In response to such concerns, the TSA has commissioned an independent study that it hopes will produce evidence to show that SPOT works, and the DHS is promising rigorous peer review of its technology programme. For critics, however, this is too little, too late.  \n                The writing's on the face \n              Most credibility-assessment researchers agree that humans are demonstrably poor at face-to-face lie detection. SPOT traces its intellectual roots to the small group of researchers who disagree \u2014 perhaps the most notable being Paul Ekman, now an emeritus professor of psychology at the University of California Medical School in San Francisco. In the 1970s, Ekman co-developed the 'facial action coding system' for analysing human facial expressions, and has since turned it into a methodology for teaching people how to link those expressions to a variety of hidden emotions, including an intent to deceive. He puts particular emphasis on 'microfacial' expressions such as a tensing of the lips or the raising of the brow \u2014 movements that might last just a fraction of a second, but which might represent attempts to hide a subject's true feelings. Ekman claims that a properly trained observer using these facial cues alone can detect deception with 70% accuracy \u2014 and can raise that figure to almost 100% accuracy by also taking into account gestures and body movements. Ekman says he has taught about one thousand TSA screeners and continues to consult on the programme. Ekman's work has brought him cultural acclaim, ranging from a profile in bestselling book  Blink   \u2014 by Malcolm Gladwell, a staff writer for  The New Yorker   magazine \u2014 to a fictionalized TV show based on his work, called  Lie to Me . But scientists have generally given him a chillier reception. His critics argue that most of his peer-reviewed studies on microexpressions were published decades ago, and much of his more recent writing on the subject has not been peer reviewed. Ekman maintains that this publishing strategy is deliberate \u2014 that he no longer publishes all of the details of his work in the peer-reviewed literature because, he says, those papers are closely followed by scientists in countries such as Syria, Iran and China, which the United States views as a potential threat. The data that Ekman has made available have not persuaded Charles Honts, a psychologist at Boise State University in Idaho who is an expert in the polygraph or 'lie detector'. Although he was trained on Ekman's coding system in the 1980s, Honts says, he has been unable to replicate Ekman's results on facial coding. David Raskin, a professor emeritus of psychology at the University of Utah in Salt Lake City, says he has had similar problems replicating Ekman's findings. \"I have yet to see a comprehensive evaluation\" of Ekman's work, he says. Ekman counters that a big part of the replication problem is that polygraph experts, such as Honts and Raskin, don't follow the right protocol. \"One of the things I teach is never ask a question that can be answered yes or no,\" Ekman says. \"In a polygraph, that's the way you must ask questions.\" Raskin and Honts disagree with Ekman's criticism, saying that Ekman himself provided the materials and training in the facial-coding technique. Yet another objection to Ekman's theory of deception detection is his idea of people who are naturally gifted at reading facial expressions. These \"wizards\", Ekman argues 2 , 3 , are proof that humans have the capability to spot deception, and that by studying those abilities, others can be taught to look for the same cues. But in a critique 4  of Ekman's work, Charles Bond, a psychologist retired from Texas Christian University in Forth Worth, argues that Ekman's wizard theory has a number of flaws \u2014 perhaps the most crucial being that the most successful individuals were drawn out of a sample pool in the thousands. Rather than proving these people are human lie detectors, Bond maintains, the wizardry was merely due to random chance. \"If enough people play the lottery, someone wins,\" says Bond. Linking displays of emotion to deception is a leap of gargantuan dimensions. ,  Ekman says that Bond's criticism is a \"ridiculous quibble\" and that the statistics speak for themselves. The wizards' scores were based on three different tests, he says, making it impossible to assign their high success rate to chance. Bond replies that he took the three tests into account, and that doing so doesn't change his conclusion.  \n                Leap of logic \n              But there is yet another problem, says Honts. Ekman's findings are \"incongruent with all the rest of the data on detecting deception from observation\". The human face very obviously displays emotion, says Maria Hartwig, a psychology professor at the City University of New York's John Jay College of Criminal Justice. But linking those displays to deception is \"a leap of gargantuan dimensions not supported by scientific evidence\", she says. This point is disputed by one of Ekman's collaborators, Mark Frank, a psychologist at the University at Buffalo in New York. Although Frank acknowledges that many peer-reviewed studies seem to show that people are not better than chance when it comes to picking up signs of deception, he argues that much of the research is skewed because it disproportionately involves young college students as test subjects, as opposed to police officers and others who might be older, more motivated and more experienced in detecting lies. Moreover, he says, when law-enforcement officials are tested, the stakes are often too low, and thus don't mimic a real-world setting. \"I think a lot of the published material is still important, good work about human nature,\" says Frank. \"But if you want to look at the total literature, and say, let's go apply it to counter-terrorism, it's a huge mistake.\" A confounding problem is that the methodology used in SPOT, which is only partially based on Ekman's work, has never been subjected to controlled scientific tests. Nor is there much agreement as to what a fair test should entail. Controlled tests of deception detection typically involve people posing as would-be terrorists and attempting to make it through airport security. Yet Ekman calls this approach \"totally bogus\", because those playing the parts of 'terrorists' don't face the same stakes as a real terrorist \u2014 and so are unlikely to show the same emotions. \"I'm on the record opposed to that sort of testing,\" he says. But without such data, how is anyone supposed to evaluate SPOT \u2014 or its training programmes? Those programmes are \"not in the public scientific domain\", says Bella DePaulo, a social psychologist at the University of California, Santa Barbara. \"As a scientist, I want to see peer-reviewed journal articles, so I can look at procedures and data and know what the training procedures involve, and what the results do show.\" Carl Maccario, a TSA analyst who helped to create SPOT, defends the science of the programme, saying that the agency has drawn on a number of scientists who study behavioural cues. One he mentions is David Givens, director of the nonprofit Center for Nonverbal Studies in Spokane, Washington. Givens published a number of scholarly articles on nonverbal communications in the 1970s and 1980s, although by his own account he is no longer involved in academic research. His more recent publications include books such as  Your Body at Work: A Guide to Sight-Reading the Body Language of Business, Bosses, and Boardrooms   (2010). But Givens says that he has no idea which nonverbal indicators have been selected by the TSA for use in SPOT, nor has he ever been asked by the TSA to review their choices. In the absence of testing, Maccario points to anecdotal incidents, such as the 2008 case of Kevin Brown, a Jamaican national who was picked out by behaviour-detection officers at Orlando International Airport in Florida and arrested with what they took to be the makings of a pipe bomb. Witnesses said that Brown was rocking back and forth and acting strangely, so it is hard to say whether specialized training was needed to spot his unusual behaviour. In any case, Brown successfully claimed that the 'pipe bomb' materials were actually fuel bottles, pleaded guilty to bringing a flammable substance onto an aircraft, and was released on three years' probation.  \n                Arrest record \n              The TSA does track statistics. From the SPOT programme's first phase, from January 2006 through to November 2009, according to the agency, behaviour-detection officers referred more than 232,000 people for secondary screening, which involves closer inspection of bags and testing for explosives. The agency notes that the vast majority of those subjected to that extra inspection continued on their travels with no further delays. But 1,710 were arrested, which the TSA cites as evidence for the programme's effectiveness. Critics, however, note that these statistics mean that fewer than 1% of the referrals actually lead to an arrest, and those arrests are overwhelmingly for criminal activities, such as outstanding warrants, completely unrelated to terrorism. According to the GAO, TSA officials are unsure whether \"the SPOT program has ever resulted in the arrest of anyone who is a terrorist, or who was planning to engage in terrorist-related activity\". The TSA has hired an independent contractor to assess SPOT. Ekman says he has been apprised of the initial findings, and that they look promising. But the results aren't expected until next year. \"It'll be monumental either way,\" says Maccario. SPOT was in its first full year of operation when the DHS science and technology directorate began to look at ways to move people through the screening points faster. One was Future Attribute Screening Technology (FAST), which is now being funded at around US$10 million a year. The idea is to have passengers walk through a portal as sensors remotely monitor their vital signs for 'malintent': a neologism meaning the intent or desire to cause harm. FAST operates on much the same physical principle as the century-old polygraph, which seeks to reveal lies by measuring psychophysiological responses such as respiration, cardiac rate and electrical resistance of the skin while a subject is being asked a series of questions. The FAST portal would also look at visual signals such as blink rate and body movement \u2014 and would give up the polygraph's contact sensors in favour of stand-off sensors such as thermal cameras, which can measure subtle changes in facial temperature, and BioLIDAR, a laser radar that can measure heart rate and respiration. Most of the FAST work, particularly the sensors, is contracted out to the Charles Stark Draper Laboratory, an independent, not-for-profit, research centre in Cambridge, Massachusetts, which has the goal of producing a prototype portal next year. The project is then scheduled to enter a second phase that will remove the questioning process altogether and instead try to induce a response in the subjects by using various stimuli such as sounds or pictures, possibly of a known terrorist. \"In the laboratory now, we have a success detection rate [percentage] of malintent or not malintent, in the mid-70s,\" says Robert Burns, the DHS programme manager for FAST. \"That's significantly better than chance or what the trained people can do.\" Those results have not yet been published, but Burns says that the FAST programme sets great store on peer review and publication, and that three papers are currently in the process of review. But FAST's critics maintain that the malintent theory and FAST both suffer from some of the same scientific flaws as SPOT. Flying is stressful: people worry about missing flights, they fight with their spouses and they worry about terrorism. All of these stresses heighten the emotions that would be monitored by the FAST sensors, but may have nothing to do with deception, let alone malintent. \"To say that the observation is due to intent to do something wrong, illegal or cause harm, is leaping at the Moon,\" says Raskin. The malintent theory underlying FAST is the creation of Daniel Martin, who is the director of research for FAST, and his wife, Jennifer Martin. Both are psychologists, and Daniel Martin, who is on the faculty of Yale University in New Haven, Connecticut, has in the past focused primarily on the area of substance abuse. Daniel Martin says that at the time he and his wife developed the malintent theory, \"there was minimal published work available that specifically tested whether physiological, behavioural, and paralinguistic cues could detect malintent in a realistic applied research study\". He says that they have had to develop their own laboratory protocols to carry out those tests. Martin and his colleagues have just published what they say is the first peer-reviewed study to look specifically at the links between psychophysiological indicators and intent. The study 5  looks at 40 native Arabic-speaking men and finds a connection between intent to deceive and a heart-rate variation known as respiratory sinus arrhythmia. We are pursuing the answer, we're not sure yet. We have years yet to go. ,  \"I have not come out and said, 'We have found the answer',\" Martin adds. \"We are pursuing the answer, we're not sure yet. We have years yet to go.\" The lack of answers has not stopped aviation-security programmes from moving forwards with deception detection. Maccario points to the UK pilot scheme, now in its first year at Heathrow Airport. He says that the programme, like SPOT, uses specially trained behaviour-detection officers, and \"their initial results are very successful\". Earlier this year, the US Intelligence Advanced Research Projects Activity announced its own plans to study \"defining, understanding, and ultimately detecting valid, reliable signatures of trust in humans\". And about two years ago, the Pentagon asked JASON to look at the field. \"As we dug in, we found it was very hard to subject the research to the kinds of standard we're used to in the physical sciences,\" says JASON head Roy Schwitters, a physics professor at the University of Texas at Austin. In fact, the executive summary of the JASON report,  The Quest for Truth: Deception and Intent Detection , which was provided to  Nature   by the Pentagon, criticizes many of the allegedly successful results from deception-detection techniques as being post-hoc identifications. One problem, the study found, was that the reported success rates often included drug smugglers, warrant violators and other criminals, not covert combatants or suicide bombers who might not have the same motivations or emotional responses. Sallie Keller, dean of engineering at Rice University in Houston, Texas, and the head of the JASON study, said that it seemed that those involved in the field were trying to get their work peer reviewed. But doing research \u2014 even if it is properly peer reviewed \u2014 doesn't mean the technology is ready to be used in an airport. \"The scientific community thinks that it is extremely important to go through the process of scientific verification, before rolling something out as a practice that people trust,\" she says. Researchers involved in the field suggest a number of research avenues that could be more fruitful for counter-terrorism. Aldert Vrij, a social psychologist at the University of Portsmouth, UK, says that structured interviews may offer the best credibility-assessment research. Nonverbal cues might play a part in this process, he says, but you need to actively interview a person. For example, his work shows that subjects were able to give more reasons for supporting an opinion that they believed than if they were acting as a devil's advocate and feigning support 6 . He suggests that such an approach could have helped to determine the beliefs of the Jordanian suicide bomber who killed seven CIA employees in Afghanistan after being taken into their confidence. Although Israeli aviation security uses interview-intensive screening, it's not clear how practical such an interview method would be at busy airport checkpoints, which have to screen hundreds or thousands of passengers every hour. The guards would still need some way to choose who to interview, or no one would ever get on a plane. This is the seductive appeal of programmes such as SPOT and FAST. But, to Honts, the decade since the 11 September attacks has been one of lost opportunity. Calling SPOT an \"abject failure\", he says that the government would have done better to invest first in basic science, experimentally establishing how people with malintent think and respond during screenings. That work, in turn, could have laid a more solid foundation for effective detection methods. Granted, Honts says, that measured approach would have been slow, but it would have been a better investment than rushing to build hardware first, or implementing programmes before they have been tested. \"We spent all this time, and all this money,\" he says, \"and nothing has been accomplished.\" Sharon Weinberger is a freelance reporter based in Washington DC. \n                     DHS programme overview \n                   \n                     Privacy Impact Assessment for the FAST program \n                   \n                     TSA Behavior Detection Officer program \n                   \n                     Paul Ekman's personal website \n                   Reprints and Permissions"},
{"file_id": "464664a", "url": "https://www.nature.com/articles/464664a", "year": 2010, "authors": [{"name": "Erika Check Hayden"}], "parsed_as_year": "2006_or_before", "body": "The more biologists look, the more complexity there seems to be. Erika Check Hayden asks if there's a way to make life simpler. Not that long ago, biology was considered by many to be a simple science, a pursuit of expedition, observation and experimentation. At the dawn of the twentieth century, while Albert Einstein and Max Planck were writing mathematical equations that distilled the fundamental physics of the Universe, a biologist was winning the Nobel prize for describing how to make dogs drool on command. The molecular revolution that dawned with the discovery of the structure of DNA in 1953 changed all that, making biology more quantitative and respectable, and promising to unravel the mysteries behind everything from evolution to disease origins. The human genome sequence, drafted ten years ago, promised to go even further, helping scientists trace ancestry, decipher the marks of evolution and find the molecular underpinnings of disease, guiding the way to more accurate diagnosis and targeted, personalized treatments. The genome promised to lay bare the blueprint of human biology. That hasn't happened, of course, at least not yet. In some respects, sequencing has provided clarification. Before the Human Genome Project began, biologists guessed that the genome could contain as many as 100,000 genes that code for proteins. The true number, it turns out, is closer to 21,000, and biologists now know what many of those genes are. But at the same time, the genome sequence did what biological discoveries have done for decades. It opened the door to a vast labyrinth of new questions. Few predicted, for example, that sequencing the genome would undermine the primacy of genes by unveiling whole new classes of elements \u2014 sequences that make RNA or have a regulatory role without coding for proteins. Non-coding DNA is crucial to biology, yet knowing that it is there hasn't made it any easier to understand what it does. \"We fooled ourselves into thinking the genome was going to be a transparent blueprint, but it's not,\" says Mel Greaves, a cell biologist at the Institute of Cancer Research in Sutton, UK. Instead, as sequencing and other new technologies spew forth data, the complexity of biology has seemed to grow by orders of magnitude. Delving into it has been like zooming into a Mandelbrot set \u2014 a space that is determined by a simple equation, but that reveals ever more intricate patterns as one peers closer at its boundary. With the ability to access or assay almost any bit of information, biologists are now struggling with a very big question: can one ever truly know an organism \u2014 or even a cell, an organelle or a molecular pathway \u2014 down to the finest level of detail? Imagine a perfect knowledge of inputs, outputs and the myriad interacting variables, enabling a predictive model. How tantalizing this notion is depends somewhat on the scientist; some say it is enough to understand the basic principles that govern life, whereas others are compelled to reach for an answer to the next question, unfazed by the ever increasing intricacies. \"It seems like we're climbing a mountain that keeps getting higher and higher,\" says Jennifer Doudna, a biochemist at the University of California, Berkeley. \"The more we know, the more we realize there is to know.\"  \n                Web-like networks \n              Biologists have seen promises of simplicity before. The regulation of gene expression, for example, seemed more or less solved 50 years ago. In 1961, French biologists Fran\u00e7ois Jacob and Jacques Monod proposed the idea that 'regulator' proteins bind to DNA to control the expression of genes. Five years later, American biochemist Walter Gilbert confirmed this model by discovering the lac repressor protein, which binds to DNA to control lactose metabolism in  Escherichia coli   bacteria 1 . For the rest of the twentieth century, scientists expanded on the details of the model, but they were confident that they understood the basics. \"The crux of regulation,\" says the 1997 genetics textbook  Genes VI   (Oxford Univ. Press), \"is that a regulator gene codes for a regulator protein that controls transcription by binding to particular site(s) on DNA.\" Just one decade of post-genome biology has exploded that view. Biology's new glimpse at a universe of non-coding DNA \u2014 what used to be called 'junk' DNA \u2014 has been fascinating and befuddling. Researchers from an international collaborative project called the Encyclopedia of DNA Elements (ENCODE) showed that in a selected portion of the genome containing just a few per cent of protein-coding sequence, between 74% and 93% of DNA was transcribed into RNA 2 . Much non-coding DNA has a regulatory role; small RNAs of different varieties seem to control gene expression at the level of both DNA and RNA transcripts in ways that are still only beginning to become clear. \"Just the sheer existence of these exotic regulators suggests that our understanding about the most basic things \u2014 such as how a cell turns on and off \u2014 is incredibly naive,\" says Joshua Plotkin, a mathematical biologist at the University of Pennsylvania in Philadelphia. Even for a single molecule, vast swathes of messy complexity arise. The protein p53, for example, was first discovered in 1979, and despite initially being misjudged as a cancer promoter, it soon gained notoriety as a tumour suppressor \u2014 a 'guardian of the genome' that stifles cancer growth by condemning genetically damaged cells to death. Few proteins have been studied more than p53, and it even commands its own meetings. Yet the p53 story has turned out to be immensely more complex than it seemed at first. In 1990, several labs found that p53 binds directly to DNA to control transcription, supporting the traditional Jacob\u2013Monod model of gene regulation. But as researchers broadened their understanding of gene regulation, they found more facets to p53. Just last year, Japanese researchers reported 3  that p53 helps to process several varieties of small RNA that keep cell growth in check, revealing a mechanism by which the protein exerts its tumour-suppressing power. Even before that, it was clear that p53 sat at the centre of a dynamic network of protein, chemical and genetic interactions. Researchers now know that p53 binds to thousands of sites in DNA, and some of these sites are thousands of base pairs away from any genes. It influences cell growth, death and structure and DNA repair. It also binds to numerous other proteins, which can modify its activity, and these protein\u2013protein interactions can be tuned by the addition of chemical modifiers, such as phosphates and methyl groups. Through a process known as alternative splicing, p53 can take nine different forms, each of which has its own activities and chemical modifiers. Biologists are now realizing that p53 is also involved in processes beyond cancer, such as fertility and very early embryonic development. In fact, it seems wilfully ignorant to try to understand p53 on its own. Instead, biologists have shifted to studying the p53 network, as depicted in cartoons containing boxes, circles and arrows meant to symbolize its maze of interactions.  \n                Data deluge \n              The p53 story is just one example of how biologists' understanding has been reshaped, thanks to genomic-era technologies. Knowing the sequence of p53 allows computational biologists to search the genome for sequences where the protein might bind, or to predict positions where other proteins or chemical modifications might attach to the protein. That has expanded the universe of known protein interactions \u2014 and has dismantled old ideas about signalling 'pathways', in which proteins such as p53 would trigger a defined set of downstream consequences. \"When we started out, the idea was that signalling pathways were fairly simple and linear,\" says Tony Pawson, a cell biologist at the University of Toronto in Ontario. \"Now, we appreciate that the signalling information in cells is organized through networks of information rather than simple discrete pathways. It's infinitely more complex.\" The data deluge following the Human Genome Project is undoubtedly part of the problem. Knowing what any biological part is doing has become much more difficult, because modern, high-throughput technologies have granted tremendous power to collect data. Gone are the days when cloning and characterizing a gene would garner a paper in a high-impact journal. Now teams would have to sequence an entire human genome, or several, and compare them. Unfortunately, say some, such impressive feats don't always bring meaningful biological insights. \"In many cases you've got high-throughput projects going on, but much of the biology is still occurring on a small scale,\" says James Collins, a bioengineer at Boston University in Massachusetts. \"We've made the mistake of equating the gathering of information with a corresponding increase in insight and understanding.\" A new discipline \u2014 systems biology \u2014 was supposed to help scientists make sense of the complexity. The hope was that by cataloguing all the interactions in the p53 network, or in a cell, or between a group of cells, then plugging them into a computational model, biologists would glean insights about how biological systems behaved. In the heady post-genome years, systems biologists started a long list of projects built on this strategy, attempting to model pieces of biology such as the yeast cell,  E. coli , the liver and even the 'virtual human'. So far, all these attempts have run up against the same roadblock: there is no way to gather all the relevant data about each interaction included in the model.  \n                A bug in the system \n              In many cases, the models themselves quickly become so complex that they are unlikely to reveal insights about the system, degenerating instead into mazes of interactions that are simply exercises in cataloguing. In retrospect, it was probably unrealistic to expect that charting out the biological interactions at a systems level would reveal systems-level properties, when many of the mechanisms and principles governing inter-and intracellular behaviour are still a mystery, says Leonid Kruglyak, a geneticist at Princeton University in New Jersey. He draws a comparison to physics: imagine building a particle accelerator such as the Large Hadron Collider without knowing anything about the underlying theories of quantum mechanics, quantum chromodynamics or relativity. \"You would have all this stuff in your detector, and you would have no idea how to think about it, because it would involve processes that you didn't understand at all,\" says Kruglyak. \"There is a certain amount of naivety to the idea that for any process \u2014 be it biology or weather prediction or anything else \u2014 you can simply take very large amounts of data and run a data-mining program and understand what is going on in a generic way.\" This doesn't mean that biologists are stuck peering ever deeper into a Mandelbrot set without any way of making sense of it. Some biologists say that taking smarter systems approaches has empowered their fields, revealing overarching biological rules. \"Biology is entering a period where the science can be underlaid by explanatory and predictive principles, rather than little bits of causality swimming in a sea of phenomenology,\" says Eric Davidson, a developmental biologist at the California Institute of Technology in Pasadena. Such progress has not come from top\u2013down analyses \u2014 the sort that try to arrive at insights by dumping a list of parts into a model and hoping that clarity will emerge from chaos. Rather, insights have come when scientists systematically analyse the components of processes that are easily manipulated in the laboratory \u2014 largely in model organisms. They're still using a systems approach, but focusing it through a more traditional, bottom\u2013up lens. Davidson points to the example of how gene regulation works during development to specify the construction of the body. His group has spent almost a decade dissecting sea-urchin development by systematically knocking out the expression of each of the transcription factors \u2014 regulatory proteins that control the expression of genes \u2014 in the cells that develop into skeleton. By observing how the loss of each gene affects development, and measuring how each 'knockout' affects the expression of every other transcription factor, Davidson's group has constructed a map of how these transcription factors work together to build the animal's skeleton 4 . The map builds on the Jacob\u2013Monod principle that regulation depends on interactions between regulatory proteins and DNA. Yet it includes all of these regulatory interactions and then attempts to draw from them common guiding principles that can be applied to other developing organisms. For example, transcription factors encoded in the urchin embryo's genome are first activated by maternal proteins. These embryonic factors, which are active for only a short time, trigger downstream transcription factors that interact in a positive feedback circuit to switch each other on permanently. Like the sea urchin, other organisms from fruitflies to humans organize development into 'modules' of genes, the interactions of which are largely isolated from one another, allowing evolution to tweak each module without compromising the integrity of the whole process. Development, in other words, follows similar rules in different species. \"The fundamental idea that the genomic regulatory system underlies all the events of development of the body plan, and that changes in it probably underlie the evolution of body plans, is a basic principle of biology that we didn't have before,\" says Davidson. That's a big step forwards from 1963, when Davidson started his first lab. Back then, he says, most theories of development were \"manifestly useless\". Davidson calls his work \"a proof of principle that you can understand everything about the system that you want to understand if you get hold of its moving parts\". He credits the Human Genome Project with pushing individual biologists more in the direction of understanding systems, rather than staying stuck in the details, focused on a single gene, protein or other player in those systems. First, it enabled the sequencing of model-organism genomes, such as that of the sea urchin, and the identification of all the transcription factors active in development. And second, it brought new types of biologists, such as computational biologists, into science, he says.  \n                The eye of the beholder \n              So how is it that Davidson sees simplicity and order emerging where many other biologists see increasing disarray? Often, complexity seems to lie in the eye of the beholder. Researchers who work on model systems, for instance, can manipulate those systems in ways that are off-limits to those who study human biology, arriving at more definitive answers. And there are basic philosophical differences in the way scientists think about biology. \"It's people who complicate things,\" says Randy Schekman, a cell and molecular biologist at the University of California, Berkeley. \"I've seen enough scientists to know that some people are simplifiers and others are dividers.\" Although the former will glean big-picture principles from select examples, the latter will invariably get bogged down in the details of the examples themselves. Mark Johnston, a yeast geneticist at the University of Colorado School of Medicine in Denver, admits to being a generalizer. He used to make the tongue-in-cheek prediction that the budding yeast  Saccharomyces cerevisiae   would be \"solved\" by 2007 when every gene and every interaction has been characterized. He has since written more seriously that this feat will be accomplished within the next few decades 5 . Like Davidson, he points out that the many aspects of yeast life, such as the basics of DNA synthesis and repair, are essentially understood. Scientists already know what about two-thirds of the organism's 5,800 genes do, and the remaining genes will be characterized soon enough, Johnston says. He works on the glucose-sensing pathway, and says he will be satisfied that he understands it when he can quantitatively describe the interactions in the pathway \u2014 a difficult but not impossible task, he says. Not everyone agrees. James Haber, a molecular biologist at Brandeis University in Waltham, Massachusetts, says it is hard to argue that the understanding of fundamental processes will be enriched within 20\u201330 years. \"Whether this progress will result in these processes being 'solved' may be a matter of semantics,\" he says, \"but some questions \u2014 such as how chromosomes are arranged in the nucleus \u2014 are just beginning to be explored.\" Johnston argues that it is neither possible not necessary to arrive at the quantitative understanding that he hopes to achieve for the glucose-sensing pathway for every other system in yeast. \"You have to decide what level of understanding you're satisfied with, and some people respond that they're not satisfied at any level \u2014 that we have to keep going,\" he says. This gulf between simplifiers and dividers isn't just a matter of curiosity for armchair philosophers. It plays out every day as study sections and peer reviewers decide which approach to science is worth funding and publishing. And it bears on the ultimate question in biology: will we ever understand it all?  \n                The edge of the universe \n              Some, such as Hiroaki Kitano, a systems biologist at the Systems Biology Institute in Tokyo, point out that systems seem to grow more complex only because we continue to learn about them. \"Biology is a defined system,\" he says, \"and in time, we will have a fairly good understanding of what the system is about.\" Others demur, arguing that biologists will never know everything. And it may not matter terribly that they don't. Bert Vogelstein, a cancer-genomics researcher at Johns Hopkins University in Baltimore, Maryland, has watched first-hand as complexity dashed one of the biggest hopes of the genome era: that knowing the sequence of healthy and diseased genomes would allow researchers to find the genetic glitches that cause disease, paving the way for new treatments. Cancer, like other common diseases, is much more complicated than researchers hoped. By sequencing the genomes of cancer cells, for example, researchers now know that an individual patient's cancer has about 50 genetic mutations, but that they differ between individuals. So the search for drug targets that might help many patients has shifted away from individual genes and towards drugs that might interfere in networks common to many cancers. Even if we never understand biology completely, Vogelstein says, we can understand enough to interfere with the disease. \"Humans are really good at being able to take a bit of knowledge and use it to great advantage,\" Vogelstein adds. \"It's important not to wait until we understand everything, because that's going to be a long time away.\" Indeed, drugs that influence those bafflingly complex signal-transduction pathways are among the most promising classes of new medicines being used to treat cancer. And medicines targeting the still-mysterious small RNAs are already in clinical trials to treat viral infections, cancer and macular degeneration, the leading cause of untreatable blindness in wealthy nations. The complexity explosion, therefore, does not spell an end to progress. And that is a relief to many researchers who celebrate complexity rather than wring their hands over it. Mina Bissell, a cancer researcher at the Lawrence Berkeley National Laboratory in California, says that during the Human Genome Project, she was driven to despair by predictions that all the mysteries would be solved. \"Famous people would get up and say, 'We will understand everything after this',\" she says. \"Biology is complex, and that is part of its beauty.\" She need not worry, however; the beautiful patterns of biology's Mandelbrot-like intricacy show few signs of resolving. See Editorial,  \n                     page 649 \n                   , and human genome special at  \n                     http://www.nature.com/humangenome \n                   . \n                     Human Genome at Ten \n                   \n                     Personal Genomes web focus \n                   \n                     Big Data special \n                   \n                     Eric Davidson Laboratory \n                   Reprints and Permissions"},
{"file_id": "465284a", "url": "https://www.nature.com/articles/465284a", "year": 2010, "authors": [{"name": "Jane Qiu"}], "parsed_as_year": "2006_or_before", "body": "When a submarine volcano erupts, the results can be devastating \u2014 and fascinating. Jane Qiu finds new drama in underwater biogeography. Richard Lutz, a marine biologist at Rutgers University in New Brunswick, New Jersey, and his colleagues were 2,500 metres beneath the ocean's surface when they encountered the 'blizzard'. It was April 1991, and an underwater ridge, 900 kilometres off the coast of Acapulco, Mexico, was splitting open, introducing 1,200 \u00b0C molten rock to 2 \u00b0C water. The results were apocalyptic. While the researchers kept a safe distance from the action in their submersible, the 'snow' of microbial debris blowing around them signalled devastation at the site of the eruption. The bacteria had been thriving at the mouths of hydrothermal vents in the area, extracting energy from the hydrogen sulphide and other chemicals pouring out of the sea bed. In turn, the bacteria nourished a diverse ecosystem of organisms: clams, crabs, mussels and armies of tubeworms. The eruption had laid waste to nearly all of this. But nature wasn't permanently obliterating life at this site at latitude 9\u00b0 50\u2032 N on the East Pacific Rise \u2014 rather, it was hitting the reset button. Lutz and his colleagues now had an opportunity to see how life returns to the vents. Putting monitoring equipment in place and returning regularly, they documented the drama of life-after-death as it unfolded. And after 15 years of research, nature repeated the experiment: Nine North, as the researchers call the site, erupted again in 2006. \"It is the only deep-sea location where a full eruptive cycle has been observed,\" says Lauren Mullineaux, an ecologist at Woods Hole Oceanographic Institution (WHOI) in Massachusetts who has been working at the site. Nine North has allowed marine biologists to identify the first species to return to a vent and the parade of life that follows. And now, four years on from the latest eruption, researchers have begun to answer tougher questions: where do these species come from, how do they travel and how do their populations shift over time? But more than that, says Robert Vrijenhoek, a molecular ecologist at the Monterey Bay Aquarium Research Institute in Moss Landing, California, \"habitat turnover is a window into evolutionary pressures that contribute to speciation and genetic diversity\". By identifying more vent sites around the world, researchers are starting to untangle the intricate interconnectedness between these tiny islands of life scattered through the abyss and separated by vast distances and rugged topography. These sites share similar geological features, with mineral-spewing vents and daunting ridges and valleys. But they don't always share the same species. Researchers are now starting to understand why.  \n                Rapid succession \n              The first glimpses through the window provided by Nine North revealed a picture of a community rebuilding quickly, says Timothy Shank, a marine biologist at WHOI who collaborates with Lutz (see 'The cycle of life'). The microbial snowfall that piled around the volcano after the 1991 eruption first attracted grazers \u2014 shrimps, crabs and fish \u2014 both vent and non-vent in origin. Within a year of the eruption, the tubeworms returned. First came  Tevnia jerichonana , which thrives at high sulphide concentrations; then, as sulphide levels dwindled in the subsequent year, dense bushes of  Riftia pachyptila , the giant tubeworm, took over.  Click here for larger image Thousands of these creatures, with sturdy tubes up to two-metres long, created a rich environment for other organisms to inhabit. Snails, and more crabs and swarming shrimps arrived 1 . It was a never-before-told story of biological succession. But it was unclear whether the organisms taking over the vents were from local sources or came from far away. Nor was it clear whether species composition in the region had changed as a result of the eruption, says Shank, because the area had not been studied in great detail previously. This is where Mullineaux, a specialist in marine larvae, comes in. Although many vent inhabitants are fused to the sea floor or to other organisms as adults, most spend their early lives as free-swimming larvae that can ride the currents to new homes. She and her team have been studying larvae at Nine North ever since the 1991 eruption. They've collected them at different depths and at the sea floor to get a sense of abundance and species make-up. As Mullineaux and her colleagues describe in a paper published this year 2 , the species composition of the larvae at the hydrothermal vents changed markedly after the 2006 eruption. Larvae from species common at the site before the eruption were nowhere to be seen afterwards \u2014 even though there were potential sources of recolonization within a few kilometres. By contrast, other species that had been rare became abundant after the catastrophe. The Woods Hole team also discovered larvae of a species never seen before at Nine North, a rock-clinging snail called  Ctenopelta porifera . The nearest hydrothermal system known to host the species is more than 300 kilometres to the north. \"This has greatly exceeded our expectation of how far vent larvae could travel in the ocean,\" says Mullineaux. Mullineaux's group had in 2001 calculated that a larva with a lifespan of about a month could travel at most up to 100 kilometres from Nine North and that the majority stayed within 60 kilometres 3 . Even if larvae lived longer, they wouldn't be able to travel farther; the flow of the current reverses every few weeks along the ridge axis and so limits how far larvae could go. How  C. porifera   had made a 300-kilometre journey was a mystery.  \n                Complex flow \n              The researchers had based their calculation of flow on measurements of the current at a single location at the crest of the ridge, and assumed that the direction and speed of the currents were the same over the whole area. \"This is a reasonable assumption in many parts of the ocean,\" says Andreas Thurnherr, a physical oceanographer at Columbia University's Lamont-Doherty Earth Observatory in Palisades, New York. But the topography of Nine North disrupts current flow significantly, he says. To better estimate currents, Thurnherr, Mullineaux and their colleagues placed 15 current-measuring devices along the crest and both flanks of the ridge; they also deployed two mobile current meters that travelled between the sea floor and the top of the ridge, sampling as they went. The team found that currents near the crest of the ridge were faster than those farther away, and that currents on the eastern flank of the ridge flowed southwards, whereas those on the west side went northwards. Surprisingly, the study shows that currents closer to the sea floor are among the strongest, at about 10 centimetres per second 4 . This caught the researchers by surprise. \"Our observations are markedly different from the much weaker and wider currents we see in many areas of the deep ocean,\" says Thurnherr. It may also explain how  C. porifera   larvae could travel so far. Diane Adams, a former graduate student of Mullineaux's, uncovered another surprise. She had noted a drop in larval abundance at Nine North whenever surface eddies \u2014 loops of rotating currents resulting from differences in water density interacting with Earth's rotation \u2014 were passing by above. The researchers hadn't thought that surface eddies could reach down to the ocean floor, but they were able to pick up signals of rotating current loops a short time after surface eddies passed by. \"The surface eddies could, in effect, blast larvae off the ridge and have an important role in their dispersal,\" says Mullineaux. Such interplay between geology and ocean currents has fascinated researchers of chemosynthetic life for decades. \"It's important not only for recolonization after natural disasters but for the evolutionary connectivity of marine life,\" says Robert Cowen, a marine biologist at the University of Miami in Florida. Researchers want to know why pockets of life-enabling chemicals in different parts of the ocean host distinct yet overlapping assemblages of species. And where did these species originate? Did they arise first at the vents or perhaps in shallow-water cold seeps \u2014 areas where hydrogen sulphide and hydrocarbon sources such as methane leak out from Earth's interior? And how do organisms such as those living off the sulphide-laden oozes of dead whales contribute to the connectivity between chemosynthetic communities? The key to the divergence and convergence of these marine species during evolution lies in the ability of larvae to negotiate ocean currents, geological barriers and changes in sea-floor topology over millions of years. In this amount of time, the movement of only a small number of larvae is sufficient to allow genetic exchange between geographically separated populations. The scale of such connectivity \"can be astonishing\", says Charles Fisher, a marine biologist at Pennsylvania State University in University Park.  \n                Long-distance taxi \n              Fisher's team has found, for example, that some species of tubeworm and mussel living on cold seeps in the Gulf of Mexico are genetically related to their counterparts off the west coast of Nigeria, an indication of genetic exchange between two regions that are more than 10,000 kilometres apart 5 . This connectivity occurs over numerous generations through steps that are not yet clear. But researchers suspect it may be aided by the equatorial deep jets in the Atlantic, which alternate between easterly and westerly flow depending on depth and so could transport larvae both ways. In other instances, larval dispersal is blocked over quite short distances, leading to speciation. In the northeastern Pacific Ocean, the 450-kilometre-long Blanco transform fault has separated the Juan de Fuca and Gorda ridge systems, off the coast of Washington State and Oregon (see 'Curiosities of the deep'). Vrijenhoek's team has found that similar-looking snails at Juan de Fuca and Gorda are related, but quite different, species that diverged roughly 11 million years ago, consistent with the time of the fault's formation 6 . boxed-text But, says Vrijenhoek, \"the barrier doesn't affect all animals in the same way\". The tubeworms at the two ends of the fault, for example, are the same species, even though there are some slight genetic differences 7 . Gene flow has taken place from populations in the north to their southern counterparts, the same direction as the ocean currents in the region. \"Tubeworm larvae probably have a sufficiently long lifespan or stay at the right parts of the water column to allow them to make that jump,\" he says. Geological changes may have separated species in other locations as well. For example, the Logatchev hydrothermal vent, located just east of the Caribbean, is the only place in the Mid-Atlantic Ridge known to host vesicomyid clams, which are more typical of the Pacific. Some researchers suspect that the animals might have originated from the Pacific \u2014 arriving through an ancient seaway between North and South America before the rise of the isthmus of Panama 5 million years ago. Other marine biologists, including Cindy Van Dover, a deep-sea biologist at Duke University Marine Laboratory in Beaufort, North Carolina, say that the clam species are probably more common on the Mid-Atlantic Ridge than it looks at present and didn't necessarily originate in the Pacific. \"We simply don't have enough samples to know for sure.\" This highlights a limitation for this relatively young research field: scientists have only a rudimentary knowledge of the distribution of chemosynthetic life, let alone the underlying mechanisms of connectivity and speciation 8 . So far, only 200 or so hydrothermal vents and a few dozen cold seeps have been discovered around the world. \"Most parts of the ocean are unexplored,\" says Paul Tyler, a marine biologist at the National Oceanography Centre in Southampton, UK, and chair of the Biogeography of Deep-Water Chemosynthetic Ecosystems (ChEss) programme of the Census of Marine Life, a global initiative to document the biodiversity of the ocean. \"Some strategic locations are missing pieces of the puzzle of how things have evolved.\" These include the Arctic and the Antarctic, where thick ice and turbulent seas make exploration immensely challenging. Others include the Chile triple junction, where three tectonic plates meet, resulting in the subduction of the Chile rise, a mid-ocean ridge, under the South American plate. \"There you have the potential for vents and seeps in close proximity,\" says Tyler. \"This allows you to address the evolutionary relationship between vent and seep animals without the variable of geography.\" \n                The great unknown \n              To many, the Caribbean, especially the little-explored Mid-Cayman rise near the island of Grand Cayman, might hold the key to some of the most perplexing questions in deep-sea biology. At almost 5,000 metres deep, hydrothermal vents in the region should exist at unrecorded environmental extremes and may reveal new species. The location is also ideal for studying the role of the isthmus of Panama in the divergence and connectivity of vent species. Researchers wonder whether the fauna on the Mid-Cayman rise will be more closely related to that on the East Pacific Rise, on the other side of the Panama land bridge, or to the organisms on the Mid-Atlantic Ridge. Chris German, a marine geochemist at WHOI and co-chair of the ChEss programme, led an expedition in October 2009 that detected signals of hydrothermal plumes near the Mid-Cayman rise. A follow-up expedition by the UK National Oceanography Centre managed to find the vents at a depth of 5,000 metres, the deepest ever recorded. As to what they found there, the team would say little. Nevertheless, German warns to keep expecting surprises. \"A few years ago, we thought we knew everything about geological barriers and ocean currents to predict what we are going to find, but we have been wrong every time since.\" Jane Qiu writes for  Nature   from Beijing, China. \n                     Nature Geoscience \n                   \n                     Lauren Mullineaux's laboratory website \n                   \n                     Robert Vrijenhoek's website \n                   \n                     Biogeography of Deep-Water Chemosynthetic Ecosystems (CHESS) programme's website \n                   \n                     Census of Marine Life website \n                   \n                     WHOI website \n                   \n                     National Oceanography Centre website \n                   Reprints and Permissions"},
{"file_id": "465416a", "url": "https://www.nature.com/articles/465416a", "year": 2010, "authors": [{"name": "Corie Lok"}], "parsed_as_year": "2006_or_before", "body": "The US National Science Foundation's insistence that every research project addresses 'broader impacts' leaves many researchers baffled. Corie Lok takes a looks at the system. Research-funding agencies are forever trying to balance two opposing forces: scientists' desire to be left alone to do their research, and society's demand to see a return on its investment. The European Commission, for example, has tried to strike that balance over the past decade by considering social effects when reviewing proposals under its various Framework programmes for research. And the Higher Education Funding Council for England announced last year that, starting in 2013, research will be assessed partly on its demonstrable benefits to the economy, society or culture. But no agency has gone as far as the US National Science Foundation (NSF), which will not even consider a proposal unless it explicitly includes activities to demonstrate the project's 'broader impacts' on science or society at large. \"The criterion was established to get scientists out of their ivory towers and connect them to society,\" explains Arden Bement, director of the NSF in Arlington, Virginia. The criterion was established to get scientists out of their ivory towers and connect them to society. ,  Unfortunately, good intentions are not enough to guarantee success, says Diandra Leslie-Pelecky, a physicist at the University of Texas in Dallas who is active in popular science writing and other forms of outreach. Leslie-Pelecky remembers a pilot project she carried out in 2001, when she was at the University of Nebraska in Lincoln. In many ways, it was typical of the kinds of things that NSF-funded researchers do to fulfil their broader-impacts requirement. She took three female graduate students on weekly visits to local classrooms, where they spent 45-minutes leading nine- and ten-year-old children in practical activities designed to teach them about electricity and circuits. The visitors also talked about their lab work and careers. In addition, Leslie-Pelecky did something less typical of broader-impacts efforts: she brought along education researchers to study the effect of this interaction on the children's perception of scientists. Those assessments were startling, she says. After three months, most of the students said that they still weren't sure who these young 'teachers' were \u2013 except that they couldn't possibly be scientists. In their minds, scientists were unfriendly, grey-haired old men in white lab coats 1 . \"And that's what I worry about with broader impacts,\" says Leslie-Pelecky. \"There are a lot of people putting time and effort into [these sorts of activities] and they have no idea if they're making any difference or not.\" Many NSF-funded researchers find the foundation's definition of broader impacts to be, perhaps unsurprisingly, broad, and frustratingly vague. Among the examples of activities listed in the foundation's proposal guide are: developing educational materials for elementary, high-school and undergraduate students; involving these students in the research where appropriate; creating mentoring programmes; maintaining and operating shared research infrastructure; presenting research results to non-scientific audiences such as policy-makers; establishing international, industrial or government collaborations; developing exhibits in partnership with museums; forming start-up companies; and giving presentations to the public. Because it lacks conceptual clarity, the broader-impacts requirement often leaves researchers unsure about what to include in their proposals, and leads to inconsistencies in how reviewers evaluate applications. \"Broader impacts were designed to be open, but openness confuses a lot of people,\" says Luis Echegoyen, the division director for NSF chemistry. To make matters worse, the NSF has made little attempt to systematically track how its broader-impacts requirements are being met, or how much grant money is being spent in the process. Nor does it have a system in place to evaluate the effectiveness of the various projects. These problems with the broader-impacts requirement have been confirmed over the past decade in studies from the National Academy of Public Administration and elsewhere. In March, the NSF's oversight body, the National Science Board, launched a task force to examine how broader impacts can be improved. Chaired by Alan Leshner, chief executive of the American Association for the Advancement of Science in Washington DC, the task force is not expected to make its recommendations until 2011. In the meantime, a small number of academic institutions are already exploring ways to make broader-impacts efforts work better. After all, says Ralph Nuzzo, a chemist and materials scientist at the University of Illinois in Urbana\u2013Champaign, most US scientists have come to accept \u2014 even if grudgingly \u2014 that it is probably a good idea to demonstrate the wider implications of their work. \"People want to do the right thing,\" says Nuzzo. \"It's just hard to know what that is.\"  \n                Scientists get creative \n             Nonetheless, there have been some successes. At Rensselaer Polytechnic Institute in Troy, New York, for example, the NSF's Nanoscale Science and Engineering Center for Directed Assembly of Nanostructures sponsors the Molecularium project, which has produced teachers' materials on nanoscience and an animated three-dimensional IMAX film called  Molecules to the Max . At the University of Wisconsin\u2013Madison, microbial biochemist Douglas Weibel and his group have prepared a child-friendly, interactive display about microscopy that they exhibit every year at the university's one-day public science exposition. At Stanford University in California, chemical engineer Andrew Spakowitz spends two to three hours a week working with graduate and undergraduate students to provide workshops for patients at Stanford's Lucile Packard Children's Hospital, most of whom are unable to attend school. Spakowitz and his group create the workshops that cover topics such as pH and gravity, and lead the hands-on activities at the hospital. It makes scientists think more explicitly about how their work is connected to enhancing benefits to society. ,  Some say that the broader-impacts criterion has helped to catalyse a change in the research-focused culture of academic science. \"It makes scientists think more explicitly about how their work is connected to enhancing benefits to society,\" says Robert Mathieu, chair of the astronomy department at the University of Wisconsin\u2013Madison and director of the university's NSF-funded Center for the Integration of Research, Teaching, and Learning (CIRTL). As an example, Mathieu points to the NSF's prestigious CAREER award for junior faculty members, which requires that applicants propose educational activities, such as designing courses and carrying out public-outreach activities, that are integrated with the proposed research. He has sat on several review panels, and says that the education section of proposals has grown in length and sophistication over the years. But despite the NSF's efforts to educate scientists about broader impacts through websites, workshops and conference sessions, most still approach the criterion with confusion and dread. Researchers often end up repackaging what they're already doing. \"Overwhelmingly,\" says Echegoyen, \"the number one broader impact that most people in the chemistry division are using is 'training graduate students and postdocs.'\" One problem is that the kind of support network that researchers take for granted \u2014 working with collaborators, sharing ideas and advice, learning from published results, attending conferences \u2014 is still rudimentary when it comes to broader impacts. A useful model could be the network of technology-transfer offices that are found on many US campuses, which have been instrumental in helping researchers to maximize the commercial effect of their research. A preliminary network for broader impacts already exists. Stanford, for example, has an Office of Science Outreach, which helped Spakowitz to make the initial contacts to get his project started at the hospital. And Mathieu's centre at the University of Wisconsin\u2013Madison is part of a network of six CIRTLs located at research campuses such as Vanderbilt University in Nashville, Tennessee, and Texas A&M University in College Station. The Wisconsin centre runs workshops and conducts individual consultations with faculty members needing assistance with integrating broader-impacts activities into their grant proposals. The other CIRTLs are moving towards similar sorts of programmes. Mathieu and his group are putting together plans to expand this network to 20\u201325 universities. Their ultimate goal is for any US research university that wants its own CIRTL to have one, creating a community that shares best practices among its researchers and other professionals, and develops the expertise to effectively broaden impacts. Mathieu estimates that establishing CIRTLs at the nation's top research universities would cost roughly US$100 million over five years. Yet such ideas lead to a more fundamental question. Is having every principal investigator working individually on broader impacts \u2014 for which many are inexperienced and untrained \u2014 the most efficient way of achieving the maximum effect? Some scholars say no. In a paper published last year, Warren Burggren, a biologist and dean of the College of Arts and Sciences at the University of North Texas in Denton, writes that the job of implementing broader impacts should fall to the researcher's institution, not to the researcher him or herself 2 . The institution, be it college, department or centre, would pool a portion of the NSF grants obtained by its members and hire the professionals needed to broaden impacts effectively. Scientists should still be involved, but the coordination would happen at the institutional level. \"I think it will be more efficient, because you've got people doing what they're trained for,\" says Burggren. Another idea, suggested by Barry Bozeman, a science-policy expert at the University of Georgia in Athens, is for the NSF to create specific research programmes with strong broader-impact goals around areas in which the effects are important and obvious, such as climate change 3 . Bozeman says that the NSF is already following this strategy with awards that, for example, promote the recruitment and retention of women in academic science. The NSF's broader-impacts requirement took its current form in 1997, when the foundation simplified the criteria used by reviewers to evaluate proposals. Two of the four criteria \u2014 the intrinsic scientific merit of the project, and the soundness of the team's approach \u2014 were merged into one, known as 'intellectual merit'. And the other two \u2014 the utility or relevance of the project, and its effect on the infrastructure of science and engineering \u2014 were collapsed into 'broader impacts'. For the first few years, many proposers and reviewers ignored this second criterion, treating it with the same disregard that they had previously expressed towards 'utility or relevance'. It was only in 2002 that the NSF cracked down, announcing that any proposal that didn't separately address both the intellectual-merit and the broader-impacts criteria would be returned without review.  \n                The right track \n              Of the few small-scale efforts to track and assess the broader-impacts requirement, none has been conclusive. In 2008, for example, the NSF sent Congress a report on broader impacts, as mandated in the 2007 America COMPETES Act \u2014 but included little more than anecdotal descriptions of specific research projects. The chemistry division recently contracted with an outside company to assess the broader-impacts activities of a sample of its grantees, but the project fell into limbo when the company dissolved. And the geosciences directorate carried out informal surveys of broader-impacts activities in the ocean and Earth sciences, which yielded some results, but also showed that the research and the broader-impacts work were often so interwoven that it was difficult to tease them apart. Mostly, evaluation happens as a by-product of other NSF activities \u2014 routine reviews of grantees' annual reports, for example, or the regular review of programmes at each division carried out by a committee of external scientists. By not tracking broader-impacts activities, the NSF undervalues its true contribution to society. ,  \"By not tracking broader-impacts activities, the NSF undervalues its true contribution to society,\" says Melanie Roberts, an assistant director at the Colorado Initiative in Molecular Biotechnology at the University of Colorado in Boulder who has analysed the broader-impacts statements from recent grant abstracts. \"It is missing an opportunity to create a knowledge base of how to carry out broader-impacts activities effectively and reward those who do a good job.\" The confusion that persists despite the NSF's repeated attempts to clarify broader impacts perhaps reflects more fundamental issues about the relationship between science and society, says Britt Holbrook, a philosopher of science at the University of North Texas. Is the NSF 'passing the buck' by asking scientists to meet what is essentially a political goal: demonstrating the benefits of science? \"My hypothesis is that the NSF has passed some of that burden to the people getting funded,\" says Holbrook, who has a grant from the foundation to study how different funding agencies incorporate societal impacts into their review process. \"But when you do that, you get push back from the scientific and engineering community because it goes against the traditional idea of peer review,\" which is designed to assess work at a technical or scientific level. And how does the NSF show impact, given that the agency's specialty, basic research often doesn't have an immediate pay-off, or else has a pay-off that is difficult to quantify? It's a delicate balancing act, says Neal Lane, a physicist at Rice University in Houston, Texas, who was the NSF director from 1993 to 1998 when the broader-impacts criterion was first implemented. It's important to get scientists to think about how their work affects society, he says. \"But one has to be careful not to push it too far. If the NSF moves too far in the direction of doing things that have short-term benefits, then I think that is not consistent with the NSF's mission, and that would not be good for American science, engineering and technology.\" \n                     US National Science Foundation merit review criteria \n                   \n                     US National Science Foundation broader impacts criterion \n                   \n                     The broader impacts toolbox \n                   \n                     Molecules to the Max \n                   \n                     MicroExplorers: Science for children \n                   Reprints and Permissions"},
{"file_id": "464340a", "url": "https://www.nature.com/articles/464340a", "year": 2010, "authors": [{"name": "Virginia Hughes"}], "parsed_as_year": "2006_or_before", "body": "Last year, functional magnetic resonance imaging made its debut in court. Virginia Hughes asks whether the technique is ready to weigh in on the fate of murderers. Brian Dugan, dressed in an orange jumpsuit and shackles, shuffled to the door of Northwestern Memorial Hospital in downtown Chicago, accompanied by four sheriff deputies. It was the first time that Dugan, 52, had been anywhere near a city in 20 years. Serving two life sentences for a pair of murders he had committed in the 1980s, he was now facing the prospect of the death penalty for an earlier killing. Dugan was here on a Saturday this past September to meet one of the few people who might help him to avoid that fate: Kent Kiehl, a neuroscientist at the University of New Mexico in Albuquerque. Dugan, Kiehl and the rest of the entourage walked the length of the hospital, crossed a walkway to another building, and took the lift down to a basement-level facility where researchers would scan Dugan's brain using functional magnetic resonance imaging (fMRI). Todd Parrish, the imaging centre's director, offered plastic zip ties to replace the shackles \u2014 no metal is allowed in the same room as the scanner's powerful magnet \u2014 but the guards said they weren't necessary. Dugan entered the machine without restraints, and Parrish locked the door \u2014 as much to keep the guards and their weapons out as to keep Dugan in. Dugan lay still inside the scanner for about 90 minutes, performing a series of cognitive control, attention and moral decision-making tests. Afterwards he ate a hamburger, sat through an extensive psychiatric interview and rode back to DuPage county jail, about 50 kilometres west of Chicago. Kiehl has been amassing data on men such as Dugan for 16 years. Their crimes are often impulsive, violent, committed in cold blood and recalled without the slightest twinge of remorse. They are psychopaths, and they are estimated to make up as much as 1% of the adult male population, and 25% of male prisoners. To date, Kiehl has used fMRI to scan more than 1,000 inmates, many from a mobile scanner set up in the courtyard of a New Mexico prison. He says that the brains of psychopaths tend to show distinct defects in the paralimbic system, a network of brain regions important for memory and regulating emotion.  \n                Mitigating circumstances \n              The purpose of the work, Kiehl says, is to eliminate the stigma against psychopaths and find them treatments so they can stop committing crimes. But Dugan's lawyers saw another purpose. During sentencing for capital crimes, the defence may present just about anything as a mitigating factor, from accounts of the defendant being abused as a child to evidence of extreme emotional disturbance. Kiehl's research could offer a persuasive argument that Dugan is a psychopath and could not control his killer impulses. After reading about Kiehl's work in  The New Yorker , Dugan's lawyers asked Kiehl to testify and offered him the chance to scan the brain of a notorious criminal. Kiehl agreed and Dugan's case became what is thought to be the first in the world to admit fMRI as evidence. Kiehl's decision has put him at odds with many in his profession, and stirred debate among neuroscientists and lawyers. \"It is a dangerous distortion of science that sets dangerous precedents for the field,\" says Helen Mayberg, a neurologist at Emory University School of Medicine in Atlanta, Georgia. Mayberg, who uses brain imaging to study depression, has testified against the use of several kinds of brain scan in dozens of cases since 1992. Although other brain-imaging techniques have been used in court, it is especially hard to argue that fMRI should be, argue critics. The technique reveals changes in blood flow within the brain, thought to correlate with brain activity, and it has become popular in research. But most fMRI studies are small, unreplicated and compare differences in the average brain activity of groups, rather than individuals, making it difficult to interpret for single cases. It is rarely used in diagnosis. Moreover, a recent scan, say some critics, wouldn't necessarily indicate Dugan's mental state when he committed his crimes. In 1983, Dugan kidnapped 10-year-old Jeanine Nicarico, of Naperville, Illinois. He raped her in the back seat of his car and beat her to death. In 1984, he saw a 27-year-old nurse waiting at a stop light on a deserted road. He rammed into her car, raped her and drowned her in a quarry. A year later, he plucked a 7-year-old girl from her bicycle, raped her, killed her and left her body in a drainage ditch, weighed down with rocks.  \n                Plea bargaining \n              Police charged Dugan with the third murder three weeks after it happened and listed him as a suspect in the nurse's death. Through his lawyers, Dugan offered to confess to all three killings, but only if prosecutors took the death penalty off the table. Authorities, deeming some of Dugan's statements on the Nicarico murder to be unreliable, wouldn't negotiate.One reason for their reluctance was that two men had already been sent to death row for killing Nicarico. But more than ten years later the two convicted men were finally exonerated. With constituents demanding justice for Nicarico, local authorities used DNA evidence to link Dugan to the crime in 2002. In July last year, he formally pleaded guilty. It was a high-profile case for the area, and local media accounts depict a community haunted by the girl's death. The defence lawyers knew that they would have a tough time arguing for leniency. They were willing to try anything, including the latest that neuroscience could offer. Brain imaging has a long history in legal cases. Lawyers have often used scans as a way to tip the scale in the perpetual battle between opposing expert psychiatric witnesses. You can't control your brain waves, the theory goes, and scans are an objective measure of mental state. \"The psychiatric diagnosis is still soft data \u2014 it's behaviour,\" notes Ruben Gur, director of the Brain Behavior Center at the University of Pennsylvania in Philadelphia. \"The brain scan doesn't lie. If there is tissue missing from your brain, there is no way you could have manufactured it for the purpose of the trial.\" Brain imaging played into the 1982 trial of John Hinckley Jr, who had attempted to assassinate US President Ronald Reagan. Lawyers presented a computed tomography X-ray scan of his head, arguing that it showed slight brain shrinkage and abnormally large ventricles, indicating a mental defect. The prosecution's expert witnesses said the scans looked normal. Whether imaging influenced the verdict is not known, but Hinckley was found not guilty by reason of insanity. Over the next decade, lawyers gradually switched to positron emission tomography (PET), which can be used to give a measure of metabolic activity in the brain. Gur's research team has scanned dozens of patients with mental illness and hundreds of healthy volunteers using PET and structural MRI \u2014 a technique that looks at the static structure of the brain and is more established for diagnosis than fMRI. Through his research, he has developed algorithms that can predict whether a person has schizophrenia, for example, from structural MRI alone with about 80% accuracy 1 . Gur has testified in roughly 30 criminal cases on behalf of defendants alleged to have schizophrenic or brain-damage. \"We determine whether the values are normal or abnormal,\" Gur says. \"It's a challenge to explain that to a jury, but when they understand, basically all I'm telling them is that this is not someone who's operating with a full deck. And so, they may not be eligible for the harshest punishment possible.\" Gur gets so many requests to testify that he has a team of psychology residents and interns to vet them. Still, he doesn't think that fMRI is reliable enough for legal settings. \"If somebody asked me to debunk an fMRI testimony, it wouldn't be too hard,\" Gur says. That's mainly because fMRI studies deal in average differences between groups. For example, Kiehl's work has shown that when processing abstract words, psychopathic prisoners have lower activity in some brain regions than non-psychopathic prisoners and non-prisoners. But there's bound to be overlap. He has not shown, for example, that any one person showing a specific brain signature is guaranteed, with some per cent certainty, to be a psychopath or behave like one. For Kiehl, the scan is just part of the picture and he conducts extensive interviews to determine a diagnosis of psychopathy. \"It's just one bit of information that helps us understand brain function,\" he says.  \n                Taking the stand \n              On 29 October, Kiehl participated in a 'Frye hearing' for Dugan's case. Based on a 1923 ruling, the hearing determines whether scientific evidence is robust enough to be admitted. Joseph Birkett, the lead prosecutor in the Dugan case, argued that allowing the scans \u2014 the bright colours and statistical parameters of which are chosen by the researchers \u2014 might bias the jury. Some studies, prosecutors argued, have shown that neuroscientific explanations can be particularly seductive to the layperson 2 . The judge ultimately \"cut the baby in half\", says Birkett. He ruled that the jury would not be allowed to see Dugan's actual brain scans, but that Kiehl could describe them and how he interpreted them based on his research. On 5 November, Kiehl took the stand for about six hours. He described the findings of two, three-hour psychiatric interviews with Dugan. Dugan had scored 38 out of a possible 40 on the Hare Psychopathy Checklist, which evaluates 20 aspects of personality and behaviour through a semi-structured interview. (It was developed by Kiehl's graduate-school mentor, Robert Hare.) That puts him in the 99.5th percentile of all inmates, Kiehl says. Using PowerPoint slides of bar graphs and cartoon brains \u2014 but not the scans \u2014 Kiehl testified that Dugan's brain, like those of psychopaths in his other studies 3 , showed decreased levels of activity in specific areas. Prosecutors, Kiehl says, went to great lengths to sow confusion about the data. At one point during Kiehl's testimony, he showed a cartoon brain with Xs marked on various regions, to illustrate where Dugan's activity was aberrant. The prosecutor, Kiehl recalls, \"asked me on cross examination, 'Are there really Xs in this guy's brain?' It's the adversarial system \u2014 they were just out to make it look like nothing made sense.\" The next day, the prosecution brought a rebuttal witness: Jonathan Brodie, a psychiatrist at New York University. He refuted the imaging evidence on several grounds. First, there was timing: Kiehl scanned Dugan 26 years after he killed Nicarico. It was impossible to know what was going on in Dugan's brain while he was committing the act, and it was perhaps not surprising that his brain would look like a murderer's after committing murder. Second, Brodie said, there was the issue with average versus individual differences. If you look at professional basketball players, most of them are tall, he told the jury, but not everyone over six foot six is a basketball player. From a technical perspective, Kiehl's work is expertly done, says Brodie. \"I have no issue with his science. I have an issue with what he did with it. I think it was just a terrible leap.\" Even if fMRI could reliably diagnose psychopathy, it wouldn't necessarily reduce a defendant's culpability in the eyes of a judge or jury. Ultimately, the law is based on an individual's rational, intentional action, not brain anatomy or blood flow, says Stephen Morse, professor of law and psychiatry at the University of Pennsylvania. \"Brains don't kill people. People kill people,\" says Morse, who also co-directs the MacArthur Foundation's Law and Neuroscience Project, which brings together scientists, lawyers and judges to debate how brain technology should be used in legal settings.  \n                Change of heart \n              Dugan's sentencing proceedings ended four days after Brodie's testimony. The jury deliberated for less than an hour before coming back with a verdict: ten for the death penalty and two for life in prison \u2014 a death sentence requires a unanimous vote. But while waiting for the Nicarico family to return to the courtroom, one of the jurors asked for more time and the judge agreed. The jury asked for copies of several transcripts of testimony, including Kiehl's, and went back into deliberation. The next day, all 12 jurors voted to send Dugan to his death. Even with the unfavourable final verdict, Kiehl's testimony \"turned it from a slam dunk for the prosecution into a much tougher case\", says Steve Greenberg, Dugan's lawyer. The switched verdict will definitely be brought up in the appeal to the state supreme court, says Greenberg, as will the fact that the jurors weren't allowed to see Dugan's brain scans. Since November, Kiehl says he has been contacted by about a dozen lawyers asking for similar services. In February, he and Parrish scanned another man on trial using the same fMRI scanner at Northwestern Memorial Hospital. For Kiehl the motivations for scanning Dugan were twofold. \"It gave me an opportunity to study one of the classic psychopaths in American history,\" he says. That included privileged access to Dugan and his massive case file. Kiehl also says he feels a \"moral obligation\" to help educate the jury, and the public, about psychopathy, a disorder that is often maligned in the popular press. \"You can live in the ivory tower for your whole career, and never really have a practical influence. This was a chance to help,\" says Kiehl. Many of his peers are unimpressed. The witness stand, says Mayberg, is \"not a soapbox\". \"What you might believe to drive your own research is very different from being a spokesperson for science,\" she says, adding that people who take the stand should not use the platform as a way to meet their own scientific agenda. Others are less critical. When DuPage county prosecutors discovered that Dugan's defence team would be using brain imaging, they contacted Scott Grafton, director of the Brain Imaging Center at the University of California, Santa Barbara. Grafton has testified against imaging in many cases, but this time he declined, saying that even potentially shaky scientific evidence should be allowed during the sentencing phase of a trial. \"Kiehl got a lot of criticism, but I think what he did is perfectly reasonable,\" says Grafton. \"I've had judges tell me, 'Look, everything else everybody's bringing into a mitigation hearing is extremely unreliable, so why should we hold scans to a different standard?'\" Whatever standards fMRI scans should be held to, neuroscientists and the legal system are under pressure to work them out fast. Although a fledgling technology, fMRI has been knocking at the doors of the courts for a while. Since at least 2007, two US companies, Cephos in Tyngsboro, Massachusetts, and No Lie MRI in San Diego, California, have been selling 'deception detection' services based on fMRI scans. The technology has almost certainly been used in plea bargains (like its older cousin, the polygraph), and is expected to debut in court within a few years. Use of fMRI for diagnosing mental disorders, as was the case for Dugan, will improve and more scans will probably show up in court, says Gur. \"That's only a question of time.\" What matters, though, is how the jury perceives the evidence and the testimony \u2014 and in Dugan's case, it's not clear how much weight it carried. For Michael Euringer, a juror in the trial and a retired stockbroker, it didn't mean much at all. \"I don't think that they were able to present that idea that he had a brain defect. He was a psychopath, but he was not psychotic,\" he says. \"But it all depends on the jury.\"\n   See Editorial,  \n                     page 325 \n                   , News Features, pages  \n                     344 \n                    and  \n                     347 \n                   , Opinion,  \n                     page 351 \n                   , and online at  \n                     http://www.nature.com/scienceincourt \n                   . Virginia Hughes is a freelance writer in New York City. \n                     Kent Kiehl's website \n                   \n                     Ruben Gur's website \n                   \n                     MacArthur Foundation's Law and Neuroscience Project \n                   Reprints and Permissions"},
{"file_id": "465282a", "url": "https://www.nature.com/articles/465282a", "year": 2010, "authors": [{"name": "Alison Abbott"}], "parsed_as_year": "2006_or_before", "body": "Studying primates is the only way to understand human cognition \u2014 or so neuroscientists thought. But there may be much to learn from rats and mice, finds Alison Abbott. Anne Churchland had little time for rats. In the course of 13 years' work on decision-making in monkeys, she had never questioned that primate studies were the only way to understand the neurobiology of human cognition. Her work in the lab of Michael Shadlen at the University of Washington, Seattle, had monkeys watch moving dots flitting about on a screen until the animals indicated, with a flick of their eyes, the direction in which most of the dots were going. She recorded from single brain neurons as the monkeys slowly made sense of this 'fuzzy' information \u2014 the sort of sophisticated experiment that she did not think was possible in rodents. \"I didn't think rats would have the right sorts of brains to contemplate accumulating evidence,\" says Churchland. And with poor eyesight, and heads that bob around, \"I didn't imagine they would be able to convey to us any decision they might be silently making\". All that changed a year ago, when Churchland visited Cold Spring Harbor Laboratory in New York. Working with scientists there, she saw that rats could also learn to gather 'fuzzy' sensory information \u2014 in this case to decide whether the frequency of a rapid sequence of tones was mostly high or low. And they could convey their decision with a poke of the nose. Churchland was not alone in her earlier scepticism. Neurophysiological research into higher cognitive functions such as decision-making, attention, working memory \u2014 even risk-taking \u2014 have traditionally been carried out on non-human primates. That seemed an obvious choice, given the closeness of their brain anatomy to that of humans, the sophistication and breadth of their behaviour and their ability to reliably report to experimenters much of what is going on in their minds through eye, hand or other movements. But primate work comes with major downsides: the animals are so expensive, and their use so highly regulated, that a research paper typically relies on data from just a couple of precious animals, which have been used for multiple experiments over their lifetime. This raises concerns that observations could be unique to those animals, rather than a general property of the primate brain. Mice and rats, by contrast, can be studied in the tens or hundreds. But with brains a fraction of the size of those of humans or non-human primates \u2014 and no prefrontal cortex, the highly-evolved brain area where 'higher' cognition is thought to take place \u2014 neuroscientists assumed that rodents were simply incapable of learning complex behavioural paradigms. That scepticism is dissolving, thanks in large part to a 'rodent cognition movement' started by a small group of researchers at Cold Spring Harbor almost a decade ago and now spreading far beyond its grounds. Using carefully designed tasks, these researchers have shown that rodents can undertake some types of complex cognitive behaviour just like experimental primates, and just like humans. \"Primate used to be the only game in town,\" says Zachary Mainen, now at the Champalimaud Neuroscience Programme in Lisbon, Portugal, but one of the founders of the rodent movement when he was at Cold Spring Harbor. \"Now we are starting to appear as a small force in cognition meetings.\"  \n                Evolutionary similarities \n              Mainen joined Cold Spring Harbor Laboratory in 1999, the same year as his colleague Tony Zador. Both wanted to move beyond their backgrounds in computational neuroscience and cellular neurophysiology, and find out how electrical activity in neurons \u2014 such as that stimulated by sensory input \u2014 related to behaviours such as decision-making. They thought that these components of behaviour \"would likely be evolutionarily similar across mammals\", says Mainen. And rats, they thought, would move the field forwards faster than primates, particularly given the greater availability of tools for manipulating rodent genes. Zador sees the choice of primates for cognition experiments as a \"historical accident\", naturally evolving from research begun in the 1960s to understand how vision was processed in the brain. \"Using primates made complete sense, because vision is so highly specialized in primates for functions such as face recognition,\" he says. Then, in the 1980s, some primate-research groups went on to ask how visual information couples to motor output; having seen an object, what happens in an animal's brain as it decides whether to reach for it? The interesting questions were now about cognition. \"At this point primates offered no unique advantage, because the tasks that the researchers were asking monkeys to do were so simple,\" says Zador. To study these tasks in rats, Zador and Mainen had to decide what sensory system to use, and establish a behavioural read-out. Rodents primarily rely on senses other than vision, such as hearing and smell, to guide their behaviour. So Zador began to look at how rats processed auditory information; Mainen focused on odours. It took a few years, and a lot of trial and error. But by 2003 Mainen had published his first paper 1  showing that rats could be trained to reliably repeat behaviour, discriminating between similar smells after a single whiff. More to the point, he showed that they could indicate their detection of an odour by poking their noses through a 'port' in the cage wall. That paper was an eye-opener for Carlos Brody, a computational neuroscientist at Cold Spring Harbor. \"I had theories that I would have liked to test in a primate lab, but this paper showed that you could do equally rigorous work with rats,\" he says. Brody joined forces with Mainen and Zador and the three of them persuaded the laboratory to set up the Center for Neural Mechanisms of Cognition in 2006, devoted to rodent work. In 2008, Mainen published a second watershed paper, using electrophysiology to show how rats make everyday decisions on the basis of fuzzy evidence 2 . This time he trained rats to distinguish between two odours delivered through the central port of a row of three. If a mixture had more of odour one, the rat had to poke its nose through the left-hand port; if it had more of odour two, it had to select the right-hand one. The decision became very difficult when the mixtures contained nearly equal parts of the two odours, but if the rats decided correctly, and waited long enough at the correct port, they received a drink reward there. The more confident rats were about their decision, Mainen found, the longer they were prepared to wait. And when he took recordings from single neurons in the orbital frontal cortex, a brain area involved in decision-making, he found patterns of electrical activity that correlated with the rats' conviction. \"It hadn't been clear whether the rat brain was going to be up to the task of estimating confidence in decisions,\" says Mainen. \"But we showed it was, and at least in this sort of task, rats are as good as monkeys as subjects.\" In certain ways, rodents are better. In the past few years the development of 'optogenetic' tools has allowed rodent researchers to engineer particular neurons so that their activity can be switched on or off with flashes of laser light 3 , allowing the role of neurons in a behaviour circuit to be dissected. Right now these systems work best in mice. But because most behavioural studies have been carried out using rats, cognitive scientists have mostly chosen to start on rats in the hope that the techniques will be quickly transferred. Zador says it's still not clear whether the smaller-brained mouse is capable of the behaviours in which the field is interested.  \n                Rodent logic \n              Churchland is so taken with the experimental possibilities afforded by rodents that she is staking her career on them. This summer she is moving to Cold Spring Harbor, where she will establish her own lab to study rat decision-making. Looking back, she wonders why she doubted rats' cognitive abilities so much. \"They also have to make decisions in the wild in order to survive, and would obviously have to accumulate and sift evidence to do so.\" She wants to explore why some rats choose a strategy of decision-making that sacrifices accuracy for speed. \"With higher numbers, we can start to look at individual differences,\" she says. Other committed primate researchers, such as Daeyeol Lee, a neuroeconomist at Yale University School of Medicine in Connecticut, are also exploring the use of rodents. Lee has been working on primate decision-making for 15 years. \"The rat brain shares some of the most fundamental design principles with those of humans and other primates, such as connectivity between the cortex and some sub-cortical areas,\" he says. \"Rats may be able to teach us a lot.\" And behavioural researchers are working to see how far they can go with rodents, developing new paradigms in rats that might even mimic some classic human psychology tests, including a version of the Iowa gambling task, which probes the ability to make appropriate decisions in the face of stacked odds 4 . Researchers have also claimed that a paradigm based on the prisoner's dilemma, which explores why people might not cooperate even when it is in their best interests, shows that rats can understand the complex pay-offs that cooperation entails 5 . Some primate researchers, though, remain unconvinced. \"It is good to develop rodent models and see what they are capable of,\" says Shadlen. \"But it still isn't clear to me that rodents do any serious deliberating in decision-making.\" And Daniel Salzman at Columbia University in New York says that the differences, rather than the similarities, in brain anatomy and circuitry are going to be decisive, such as the smaller rodent frontal cortex. Rodent researchers \"are quickly going to run up against a wall,\" he predicts. Still, few on either side of the species divide care to be too categorical. Rodent-cognition researchers have presented enough new data at meetings to discourage dogmatism from primate loyalists. And rodent proponents emphasize that primates will always be required to reality-check the theories about cognition spawned by rodent research. \"Primates are going to be capable of some cognitive processes that rats are simply not capable of,\" says Brody, who thinks both types of research should run in parallel. \"But the jury is still very much out in terms of where the capability border lies, and we think it is worth finding out.\"   See Editorial,  \n                     page 267 \n                   . \n                     Antony Zador's homepage \n                   \n                     Zachary Mainen's homepage \n                   \n                     Carlos Brody's homepage \n                   \n                     Mike Shadlen's homepage \n                   \n                     Daniel Salzman's homepage \n                   \n                     Daeyeol Lee's homepage \n                   Reprints and Permissions"},
{"file_id": "463420a", "url": "https://www.nature.com/articles/463420a", "year": 2010, "authors": [{"name": "Katharine Sanderson"}], "parsed_as_year": "2006_or_before", "body": "The surprising discovery of methane in Mars's atmosphere could be a sign of life there. Researchers are now working out how to find its source, reports Katharine Sanderson. For the past decade, NASA's mantra for exploring Mars has been to 'follow the water'. The agency based its aqueous obsession on the idea that finding evidence of past or present water would yield clues to whether life once graced the planet \u2014 or still exists there. Now, some scientists are chanting a new slogan: 'follow the methane'. This small hydrocarbon is tantalizing because much of the methane in Earth's atmosphere is produced by microbes, within soils and inside the guts of cows and other mammals, including humans. So the fact that methane has been discovered on Mars could signal the presence of life somewhere on the red planet. Or not. Methane can also form through geological processes. In either case, the methane findings signal that unknown processes are happening. \"Methane is one molecule that really tells us that Mars is a coupled system of the interior, the surface and the atmosphere,\" says Sushil Atreya, director of the Planetary Science Laboratory at the University of Michigan in Ann Arbor. In November, fans of Martian methane gathered in Frascati, Italy, to puzzle over the new data. They hope to pinpoint where the methane is coming from and why it gets scrubbed from the atmosphere so quickly. Solutions to these conundrums will require better data, so researchers are working out how to sniff out the gas in future missions to Mars. The excitement over methane started to build in 2003 and 2004, when three groups 1 ,   2 ,   3  spotted methane in the atmosphere of Mars using spectroscopic measurements from telescopes on Earth and data from the European Space Agency's Mars Express spacecraft. The amounts of methane detected by the teams differed, but the planetary average was estimated to be about 10 parts per billion by volume 3 , an extremely low level. And the amount changed over time \u2014 suggesting that the gas is still being released, even though it might have been produced at some point in the past. In January 2009, the leader of one of the teams, Michael Mumma from NASA's Goddard Space Flight Center in Greenbelt, Maryland, published a paper 4  that reanalysed his 2003 and 2006 observations from the Keck telescope and the Infrared Telescope Facility in Hawaii. Mumma's team showed that three neighbouring areas on Mars, Nili Fossae, Terra Sabae and Syrtis Major, were 'hot spots' of methane production during 2003. But by 2006, methane levels had dropped at those sites, signalling that some active process was venting the gas. The quick drop also suggests that something is rapidly destroying the methane. \"For us atmospheric chemists, it is still very difficult to understand how methane can vary so rapidly in time and space,\" says Franck Lef\u00e8vre at the Laboratory of Atmospheres, Environments and Space Observations in Paris. Lef\u00e8vre calculated that the atmospheric lifetime of methane is less than 200 days 5  \u2014 hundreds of times shorter than prevailing models of Mars's atmosphere predict. \"If the measurements are correct, this means that we are missing something really important,\" says Lef\u00e8vre. Raina Gough, a PhD student at the University of Colorado at Boulder, tried to fill that gap by testing how methane reacts with samples made up to resemble Martian soil. But none of the soil samples removed methane that quickly, she reported at the meeting. Gough's results were among the most important presented there because they \"tend to rule out what was thought to be the most likely explanation for the observed methane variations on Mars\", Lef\u00e8vre says. \"The mystery deepens.\" Where the gas is going is not the only problem. \"The big question really is what is producing the methane,\" says Atreya. Is it biological or geological, past or present? Although conditions on the surface of Mars are extremely harsh, there may be spots below the surface where microbes could survive. In research in the high Arctic, microbiologist Lyle Whyte from McGill University in Montreal, Canada, has found methane-making microbes within extremely salty pools in the permafrost, the permanently frozen ground. Martian microbes, if they exist, could be making methane in conditions somewhat like this on Mars, he suggests. Alternatively, the methane could be forming geologically as a by-product of a process called serpentinization, which happens on Earth when water reacts with olivine, a mineral that is present on Mars 6 . Another debate that dominated the meeting revolved around the location of the methane sources. Vittorio Formisano and Anna Geminale from the Institute of Physics of Interplanetary Space in Rome have tried to pinpoint the seasonal bursts using the Planetary Fourier Spectrometer on the Mars Express spacecraft. Their findings suggest that trace amounts of methane are released from the northern polar cap when its surface melts during summer. The Keck data argue otherwise, say Mumma and Geronimo Villanueva, also at Goddard. The water released from the north pole in summer does not contain any methane, they say. And although the pair found their first three methane hot spots in the northern hemisphere, they have also identified methane releases in the southern part of the planet during that hemisphere's spring. The team is now using the Keck telescope to pinpoint the sources to regions as small as 80 kilometres, from the original best resolution of 500 kilometres. The reason for the different conclusions could stem from the methods the two teams used. The Mars Express data are averaged over many thousands of spectra, and over time, whereas the Keck maps are snapshots. The disagreement and the failure of atmospheric models to explain the methane measurements has meant that some researchers remain sceptical about the interpretation of the data. Todd Clancy at the Space Science Institute in Boulder is particularly troubled by the fluctuations in time and space that Mumma reports. \"These variations are unphysical for any plausible photochemical processes active in the Mars atmosphere,\" says Clancy. Yet, he says, if the observations are right, \"the existence of such methane in the current Mars atmosphere would be profound on so many levels as to constitute the highest priority for planetary mission studies\". \n               boxed-text \n             With so many questions swirling around the issue of methane, researchers agree that only new missions to Mars will provide the answers (see graphic).  \n                Chasing a gas \n              First up is NASA's Mars Science Laboratory, which is due to launch next year. This car-sized rover will carry a tunable laser spectrometer, which should be able to answer one of the burning questions about Martian methane: what is the isotopic make-up of the carbon? If the carbon is mostly the lightest stable isotope, carbon-12, this could hint at a biological origin. In 2016, NASA and the European Space Agency are scheduled to send an orbiter to monitor trace gases. At the Frascati meeting, Richard Zurek, chief scientist for the Mars programme at NASA's Jet Propulsion Laboratory in Pasadena, California, described this newly selected mission, which would be the first to look expressly for methane and other trace gases in the Martian atmosphere. The mission has methane enthusiasts salivating, and the call for instrument proposals has just gone out. For researchers with questions about the methane data, the 2016 mission should provide some firm answers. \"It will measure the level of methane on Mars unambiguously and determine whether it really is seasonally and latitudinally variable,\" says David Catling from the University of Washington in Seattle. But some researchers are looking beyond the planned missions and are drawing up proposals for other ways to study the gas. Mumma would like to find active vents and watch them closely to see how methane releases change over time. He has proposed the Mars Organics Observer, which would sit at Mars's first Lagrange point, a special orbit around the Sun that is locked to the movement of Mars by the gravity of both bodies. The telescope could monitor the red planet every day and locate methane bursts with a resolution of 10 kilometres. Others have placed drilling operations at the top of their wish lists. \"Most astrobiologists believe that the best hope for detecting microbial life will be in the subsurface,\" says Whyte. Researchers are also dreaming up other schemes for tracing methane sources. Atreya, and Paul Mahaffy from Goddard, propose using a balloon to measure gases in the Martian atmosphere. Joel Levine, at Nasa Langley Research Center in Hampton, Virginia, is developing a Mars plane that could soar over the planet and monitor methane releases from the surface. But those not directly involved in methane research caution about leaping on to the gas bandwagon. \"We know that methane can be made by processes that have nothing to do with biology,\" says David Stevenson from the California Institute of Technology in Pasadena. Water should remain the research priority, he says. Lef\u00e8vre, however, says he is happy that mission planners are hearing the wishes of methane enthusiasts. \"Atmospheric chemistry has rarely been the trendiest topic in Mars science, but this has completely changed since the discovery of methane,\" he says. \"We are now designing space missions entirely devoted to the detection of trace species, which was impossible to imagine a few years ago.\" Katharine Sanderson is a reporter for  Nature   in London. \n                     ESA Workshop on methane on Mars \n                   \n                     Mars Express \n                   \n                     Mars Science Laboratory \n                   \n                     ExoMars \n                   \n                     Mars Airplane \n                   Reprints and Permissions"},
{"file_id": "463416a", "url": "https://www.nature.com/articles/463416a", "year": 2010, "authors": [{"name": "Corie Lok"}], "parsed_as_year": "2006_or_before", "body": "Scientists are struggling to make sense of the expanding scientific literature. Corie Lok asks whether computational tools can do the hard work for them. In 2002, when he began to make the transition from basic cell biology to research into Alzheimer's disease, Virgil Muresan found himself all but overwhelmed by the sheer volume of literature on the disease. He and his wife, Zoia, both now at the University of Medicine and Dentistry of New Jersey in Newark, were hoping to test an idea that they had developed about the formation of the protein plaques in the brains of people with Alzheimer's disease. But, as newcomers to the field, they were finding it almost impossible to figure out whether their hypothesis was consistent with existing publications. \"It's really difficult to be up to date with so much being published,\" says Virgil Muresan. And it's a challenge that is increasingly facing researchers in every field. The 19 million citations and abstracts covered by the US National Library of Medicine's PubMed search engine include nearly 830,000 articles published in 2009, up from some 814,000 in 2008 and around 772,000 in 2007. That growth rate shows no signs of abating, especially as emerging countries such as China and Brazil continue to ratchet up their research. The Muresans, however, were able to make use of Semantic Web Applications in Neuromedicine (SWAN), one of a new generation of online tools designed to help researchers zero in on the papers most relevant to their interests, uncover connections and gaps that might not otherwise be obvious, and test and generate new hypotheses. \"If you think about how much effort and money we put into just Alzheimer's disease research, it is surprising that people don't put more effort into harvesting the published knowledge,\" says Elizabeth Wu, SWAN's project manager. SWAN attempts to help researchers harvest that knowledge by providing a curated, browseable online repository of hypotheses in Alzheimer's disease research. The hypothesis that the Muresans put into SWAN, for example, was that plaque formation begins when amyloid-\u03b2, the major component of brain plaques, forms seeds in the terminal regions of cells in the brainstem that then nucleate the plaques in the other parts of the brain into which the terminals reach. SWAN provides a visual, colour-coded display of the relationships between the hypotheses, as derived from the published literature, and shows where they may agree or conflict. The connections revealed by SWAN led the Muresans to new mouse-model experiments designed to strengthen their hypothesis. \"SWAN has advanced our research, and focused it in a certain direction but also broadened it to other directions,\" says Virgil Muresan. The use of computers to help researchers drink from the literature firehose dates back to the early 1960s and the first experiments with techniques such as keyword searching. More recent efforts include the striking 'maps of science' that cluster papers together on the basis of how often they cite one another, or by similarities in the frequencies of certain keywords. As fascinating as these maps can be, however, they don't get at the semantics of the papers \u2014 the fact that they are talking about specific entities such as genes and proteins, and making assertions about those entities (such as gene X regulates gene Y). The extraction of this kind of information is much harder to automate, because computers are notoriously poor at understanding what they are reading. Even so, informaticians and biologists are working together more and making considerable progress, says Maryann Martone, the chairwoman of the Society for Neuroscience's neuroinformatics committee. Recently, a number of companies and academic researchers have begun to create tools that are useful for scientists, using various mixtures of automated analysis and manual curation (see , 'Power tools').  \n                Deeper meaning \n              The goal of these tools is to help researchers analyse and integrate the literature more efficiently than they can do through their own reading, to hone in on the most fruitful experiments to do and to make new predictions of gene functions, say, or drug side effects. The first step towards that goal is for the text- or semantic-mining tool to recognize key terms, or entities, such as genes and proteins. For example, academic publisher Elsevier, headquartered in Amsterdam, has piloted Reflect in two recent online issues of its journal  Cell . The technology was developed at the European Molecular Biology Laboratory in Heidelberg, Germany, and won Elsevier's Grand Challenge 2009 competition for new tools that improve the communication and use of scientific information. Reflect automatically recognizes and highlights the names of genes, proteins and small molecules in the  Cell   articles. Users clicking on a highlighted term will see a pop-up box containing information related to that term, such as sequence data and molecular structures, along with links to the sources of the data. Reflect obtains this information from its dictionary of millions of proteins and small molecules. Such 'entity recognition' can be done fairly accurately by many mining tools today. But other tools take on the tougher challenge of recognizing relationships between the entities. Researchers from Leiden University and Erasmus University in Rotterdam, both in the Netherlands, have developed software called Peregrine, and used it to predict an undocumented interaction between two proteins: calpain 3, which when mutated causes a type of muscular dystrophy, and parvalbumin B, which is found mainly in skeletal muscle. Their analysis found that these proteins frequently co-occurred in the literature with other key terms. Experiments then validated that the two proteins do indeed physically interact ( H. H. van Haagen  et al. PLoS One    4,   e7894; 2009 ).  \n                Development role \n              At the University of Colorado in Denver, bioinformatician Lawrence Hunter and his research group have developed a tool called the Hanalyzer (short for 'high-throughput analyser'), and have used it to predict the role of four genes in mouse craniofacial development. They gathered gene-expression data from three facial tissues in developing mice and generated a 'data network' showing which genes were active together at what stage of development, and in which tissues. The team also mined relevant abstracts and molecular databases for information about those genes and used this to create a 'knowledge network'. Using both networks, the researchers homed in on a group of 20 genes that were upregulated at the same time, first in the mandible (lower jaw area) and then, about 36 hours later, in the maxilla (upper jaw). A closer look at the knowledge network suggested that these genes were involved in tongue development, because the tongue is the largest muscle group in the head and is in the mandible. Further analysis led them to four other genes that had not been previously linked to craniofacial muscle development but that were active in the same area at the same time. Subsequent experiments confirmed that these genes were also involved in tongue development ( S. M. Leach  et al. PLoS Comput. Biol.    5,   e1000215; 2009 ). Somebody staring at the data or using existing tools would never come up with this hypothesis. Lawrence Hunter ,  \"I don't see that there is any way that somebody staring at the data or using existing tools would have ever come up with this hypothesis,\" says Hunter. Although extracting entities and the relationships between them is a common approach for literature-mining tools, it is not enough to pull out the full meaning of research papers, says Anita de Waard, a researcher of disruptive technologies at Elsevier Labs in Amsterdam. Scientific articles typically lay out a set of core claims, together with the empirical evidence that supports them, and then use those claims to argue for a conclusion or hypothesis. \"Generally that's where the real, interesting science is,\" de Waard says. Capturing the higher-level argument is an even more difficult task for a computer, but a small number of groups, such as the SWAN group, are trying to do so. \n               boxed-text \n             The SWAN website, which opened to the public in May 2009, was developed by two Boston-based groups, the Massachusetts General Hospital and the Alzheimer Research Forum, a community and news website for Alzheimer's researchers. For each hypothesis in the system, SWAN shows the factual claims that support it, plus links to the papers supporting each claim. Because claims from the various hypotheses are linked together in a network, a user can browse from one to the next and see the connections between them. The visualization tool uses a red icon to show when two claims conflict and a green icon to show when they're consistent, allowing the user to see at a glance which hypotheses are controversial and which are well supported by the literature (see graphics, above). At the moment, this information is unlikely to surprise experts in Alzheimer's disease. In its current stage of development, SWAN may be more useful for newcomers trying to get up to speed on the subject. Beneficiaries could include more established scientists such as the Muresans who want to move into a different field, or researchers with a pharmaceutical or biotech company who have just been put on an Alzheimer's disease project.  \n                Building up \n              SWAN also has scalability issues. The vast majority of the hypotheses, claims and literature links in SWAN have been annotated and entered by the site's curator, Gwen Wong, with the help of authors. This curation is a painstaking process that has so far produced only 1,933 claims and 47 fully annotated hypotheses. But the intent is for these early hand-curation efforts to set a 'gold standard' for how the SWAN knowledge base should be built by the community as a whole. The SWAN developers plan to improve the user interface to encourage scientists to submit their own hypotheses, post comments and even do some of the curation themselves. The need for some level of manual curation is common to the various literature tools, and limits their scalability. The SWAN team is working to automate parts of the curation process, such as extracting gene names. Elsewhere, de Waard and other researchers are investigating ways of automatically recognizing hypotheses \u2014 for example, by looking for specific word patterns. For most of these tools, however, curation is unlikely to become fully automated. \"Literature mining is hard to do in a way that is both high scale and high accuracy,\" says John Wilbanks, director of Science Commons, a data-sharing initiative in Cambridge, Massachusetts. Developers say a more likely solution, at least in the short term, is that papers will have to be curated and annotated through some combination of automated tools, professional curators and the papers' authors, who might, for example, be prevailed on to write their abstracts in a more structured machine-readable form.  \n                The right people \n              Are authors willing to add to the already arduous task of writing an article? And are authors even the best people to do this job? The journal  FEBS Letters   experimented in 2009 with structured digital abstracts to see how authors would respond and perform in shaping their own machine-readable abstracts. The results were not encouraging. Authors presented their abstracts about protein\u2013protein interactions as structured paragraphs describing entities, the relationships between the entities and their methods using specific, simple vocabularies (for example, 'protein A interacts with protein B'). But the curators of a protein database didn't accept them, says de Waard. \"Authors are not the right people to validate their own claims,\" she says. The community \u2014 referees, editors, curators, readers at large \u2014 is still needed. This could be a business opportunity for the publishers, says Wilbanks: they could curate and mark up their publications for text and semantic mining and provide that as a value-added service. \"There's a lot of business out there for the publishers, but it's not the same business,\" says Allen Renear, associate dean for research at the Graduate School of Library and Information Science at the University of Illinois at Urbana-Champaign. \"If they keep making PDFs, that's not going to work for them. They have to get into more of the semantic side of this.\" Perhaps the largest challenge is getting scientists to use these tools. It will be up to the developers to demonstrate the benefits and make their wares easy to use. That's going to be difficult, says Hunter. Academic informaticians are rewarded more for coming up with new algorithms, and less for making their programs usable and widely adoptable by biomedical scientists, he says. Only a few tools are being developed by companies for more widespread use. Major issues that all technology developers will need to tackle are transparency, provenance and trust. Scientists won't trust what a computer is suggesting in terms of new connections or hypotheses if they don't know how the results were generated and what the primary sources were. \"We as informaticians are going to have to take on these more user-driven and less technology-driven problems,\" says Hunter. Even if researchers do start to trust the new tools, it's not clear how much of their reading they will delegate. \"As reading becomes more effective,\" says Renear, \"some people have speculated that we won't do as much because we'll get done what we need to do sooner.\" Or, he says, \"it may be that we'll do more reading because it's more valuable. Which one is true is actually an empirical question.\" Analysing articles in new ways leads to the larger question of whether the articles themselves should change in structure. If an article is to be boiled down into machine-readable bits, why bother writing whole articles in the first place? Why don't researchers just deal with statements and facts and distribute and mash them up to generate hypotheses and knowledge? \"Human commentary and insight are still extraordinarily valuable,\" says Martone. \"Those insights don't immediately fall out of data without human ingenuity. So you need to be able to communicate that and that generally means building an argument and a set of supporting claims. These things are not going to go away any time soon.\" \n                     Semantic Web Applications in Neuromedicine \n                   \n                     Reflect \n                   \n                     Map of science \n                   \n                     FEBS Letters Structured Digital Abstracts \n                   \n                     Concept Web \n                   \n                     Neuroscience Information Network \n                   \n                     Information Hyperlinked over Proteins (iHOP) \n                   \n                     MEDIE \n                   \n                     Textpresso \n                   \n                     Arrowsmith \n                   Reprints and Permissions"},
{"file_id": "463602a", "url": "https://www.nature.com/articles/463602a", "year": 2010, "authors": [{"name": "Katharine Gammon"}], "parsed_as_year": "2006_or_before", "body": "The Internet is struggling to keep up with the ever-increasing demands placed on it. Katharine Gammon looks at ways to fix it. The Internet is feeling the strain. Developed in the 1970s and 1980s for a community of a few thousand researchers, most of whom knew and trusted one another, the Internet has now become a crucial worldwide infrastructure that connects nearly two billion people, roughly a quarter of humanity. It offers up something like a trillion web pages, and transports roughly 10 billion gigabytes of data a month \u2014 a figure that is expected to quadruple by 2012. Moreover, those two billion users are exploiting the network in ways that its creators only dimly imagined, with applications ranging from e-commerce to cloud computing, streaming audio and video, ubiquitous mobile devices and the transport of massive scientific data sets from facilities such as the Large Hadron Collider, the world's highest-energy particle accelerator, based near Geneva, Switzerland. To some extent, this rapidly rising flood of information has been dealt with by updating the software and expanding the size of the data pipes \u2014 a development that most Internet users experience through the proliferation of 'broadband' services provided through cable television connections, digital subscriber lines and wireless hot spots. Yet users continue to be plagued by data congestion, slowdowns and outages, especially in wireless networks. And, as dramatized in January when search-engine giant Google publicly protested against digital assaults coming from somewhere in China, everyone on the Internet is vulnerable to cyberattack by increasingly sophisticated hackers who are almost impossible to trace \u2014 security having been an afterthought in the Internet's original design. The result has been a rising sense of urgency within the networking research community \u2014 a conviction that the decades-old Internet architecture is reaching the limits of its admittedly remarkable ability to adapt and needs a fundamental overhaul. Since 2006, for example, the Future Internet Design (FIND) programme run by the US National Science Foundation (NSF) has funded researchers trying to develop wholesale redesigns of the Internet. And since October 2008, the NSF has operated the Global Environment for Network Innovations (GENI): a dedicated, national fibre-optic network that researchers can use to test their creations in a realistic setting. Similar efforts are under way in Europe, where the Future Internet Research and Experimentation (FIRE) initiative is being funded through the European Union's Seventh Framework research programme; and in Japan, where in 2008 the National Institute of Information and Communications Technology launched JGN2plus, the latest iteration of its Japan Gigabit Network system. Buoyed by these funding initiatives, researchers have been testing out a plethora of ideas for reinventing the Internet. It is still too early to know which will pan out. But the following four case studies give a sense of both the possibilities and the challenges.  \n                Make the pipes adaptable \n              The problem with the bigger-and-bigger-data-pipe approach to dealing with the Internet's growth is that it perpetuates a certain dumbness in the system, says electrical engineer Keren Bergman of Columbia University in New York. Right now, there is no way for a user to say: \"This ultrahigh-resolution video conference I'm in is really important, so I need to send the data with the least delay and highest bandwidth possible\", or \"I'm just doing routine e-mail and web surfing at the moment, so feel free to prioritize other data\". The network treats every bit of data the same. There is also no way for the Internet to minimize redundancy. If 1,000 people are logged into a massively multiplayer role-playing game such as World of Warcraft, the network has to provide 1,000 individual data streams, even though most are close to identical. The result is a lot of wasted capacity, says Bergman, not to mention a lot of wasted money for users who have to pay extra for high-capacity data connections that they will need only occasionally. If the Internet could just adapt intelligently to what its users are trying to do, she says, it could run much more data though the pipes than it does now, thereby giving users much more capacity at a lower cost. This is easier said than done, however, because the dumbness is deliberate. In an effort to simplify the engineering, Bergman explains, the architecture of the Internet is carefully segregated into 'layers' that take one another for granted. This means that application programmers, for example, don't have to worry about physical data connections when they are developing new software for streaming video or online data processing; they can just assume that the bits will flow. Likewise, engineers working on the physical connections can ignore what the applications are doing. And neither has to worry about in-between layers such as TCP/IP (Transfer Control Protocol/Internet Protocol): the fundamental Internet software that governs how digital messages are broken up into 'packets', routed to their destination, then reassembled. But this clean separation also stops the layers from communicating with one another, says Bergman, which is exactly what they need to do if the data flow is to be managed intelligently. Working out how to create such a 'cross-layer' networking architecture is therefore one of the central goals of Bergman's Lightwave Research Laboratory at Columbia. The idea is to provide feedbacks between the physical data connection and the higher-level routing and applications layers, then to use those feedbacks to help the layers adjust to one another and optimize the network's performance. This kind of adaptability is not new in networking, says Bergman, but it has been difficult to implement for the fibre-optic cables that are carrying more and more of the Internet's traffic. Unlike standard silicon electronics, optical data circuits are not easily programmable. As a result, many of the dozen projects now under way in her lab aim to integrate optics with programmable electronic systems. Bergman's lab is also a key member of the NSF-funded Center for Integrated Access Networks, a nine-university consortium headquartered at the University of Arizona in Tucson. Her group's efforts have helped to drive many of the technology development projects at the centre, which hopes to ultimately deliver data to users at rates that approach 10 gigabits a second, roughly 1,000 times faster than the average household broadband connection today. \"The challenges are to deliver that information at a reasonable cost in terms of money and power,\" says Bergman.  \n                Control the congestion \n              Meanwhile, however, some researchers are taking issue with TCP itself. Any time a data packet fails to reach its destination, explains Steven Low, professor of computer science and electrical engineering at the California Institute of Technology (Caltech) in Pasadena, TCP assumes that the culprit is congestion at one of the router devices that switch packets from one data line to another. So it orders the source computer to slow down and let the backlog clear. And generally, says Low, TCP is right: whenever too many data try to crowd through such an intersection, the result is a digital traffic jam, followed by a sudden spike in the number of packets that get garbled, delayed or lost. Moreover, the tsunami of data now pouring through the Internet means that congestion can crop up anywhere, whether the routers are switching packets between high-capacity fibre-optic land lines carrying data across a continent, or funnelling them down a copper telephone wire to a user's house. But more and more often, says Low, simple congestion is not the reason for lost packets, especially when it comes to smart phones, laptop computers and other mobile devices. These devices rely on wireless signals, which are subject to interference from hills, buildings and the like, and have to transfer their connection from one wireless hub to the next as the devices move around. They offer many opportunities for things to go wrong in ways that won't be helped by slowing down the source \u2014 a practice that just bogs down the network unnecessarily. Researchers have been exploring several more-flexible ways to transmit data, says Low. One of these is FAST TCP, which he and his Caltech colleagues have been developing over the past decade, and which is now being deployed by the start-up company FastSoft in Pasadena. FAST TCP bases its decisions on the delay faced by packets as they go through the network. If the average delay is high, congestion is probably the cause, and FAST TCP reduces speed as usual. But if the delay is not high, FAST TCP assumes that something else is the problem and sends packets along even faster, helping to keep the network's overall transmission rate high. To test his FAST TCP algorithms, Low's lab teamed up with Caltech's high-energy physics community, which needed to transmit huge files to researchers in 30 countries on a daily basis. From 2003 to 2006, the team broke Internet world network speed records at the International Supercomputing Conference's annual Bandwidth Challenge, which is carried out on the ultrahigh-speed US research networks Internet2 and National LambdaRail. In the 2006 event, they demonstrated a sustained speed of 100 gigabits per second, and a peak transfer speed of 131 gigabits per second \u2014 records that have not been substantially bettered by subsequent winners of the challenge.  \n                Integrate social-networking concepts \n              What's great about the Internet, says computer scientist Felix Wu of the University of California, Davis, is that anyone with an address on the network can contact anyone else who has one. But that's also what's terrible about it. \"Global connectivity means you have no way to prevent large-scale attacks,\" he says, citing as an example recent digital assaults that have temporarily shut down popular sites such as Twitter. \"At the same time you are getting convenience, you are actually giving people the power to do damage.\" In 2008, for example, security software maker Symantec in Mountain View, California, detected 1.6 million new threats from computer viruses and other malicious software \u2014 more than double the 600,000 or so threats detected the previous year \u2014 and experts say that these attacks will only get more common and more sophisticated in the future. What particularly drew Wu's attention a few years ago was the problem of unsolicited junk e-mail, or 'spam', which accounts for an estimated 90\u201395% of all e-mails sent. What makes spam trivial to broadcast and hard to filter out, Wu reasoned, is the Internet's anonymity: the network has no social context for either the message or the sender. Compare that with ordinary life, where people generally know the individuals they are communicating with, or have some sort of connection through a friend. If the network could somehow be made aware of such social links, Wu thought, it might provide a new and powerful defence against spam and other cyberattacks. With funding from the NSF, Wu has created a test bed for such ideas, called Davis Social Links. The test bed has a messaging system that routes packets between users on the basis of the lists of friends that each person creates in social networking sites such as Facebook. This gives test-bed users the option of accepting only the messages that reach them through the paths or groups they trust, making it more difficult for them to be reached by spammers or attackers who lack the proper trusted paths. These social relationships in the system don't have to be restricted to people, Wu notes. Websites are fair game too. Users of Davis Social Links can build social relationships with YouTube, for example. A search engine based on this social-network idea might pick up two sites that claim to be YouTube, one that is real and one that is cloned to look like the video site. The system would try one and if it didn't have the expected connections to other trusted contacts, the path would be designated as untrustworthy and the site dropped. \"In today's routing you only give the IP address to the service provider, they do the rest,\" says Wu. \"In social routing I don't have a unique identity. I have a social identity that supports different routing.\" Davis Social Links is part of the GENI test bed and will soon start testing with up to 10 million network nodes. But even if this approach turns out not to be viable, says Wu, more types of social research need to be integrated into the future Internet. \"We need to mimic real human communication,\" he says.  \n                Break from reality \n              Computer scientist Jonathan Turner of Washington University in Saint Louis, Missouri, says that the basic packet-delivery service hasn't changed in more than 20 years not because no one has a better idea, but because new ideas can't get a foothold. \"It's increasingly difficult for the public Internet to make progress,\" he says. The network's infrastructure is fragmented among many thousands of network providers who are committed to the Internet as it is, and who have little motivation to cooperate on making fundamental improvements. This spectre of a rapidly ossifying Internet has made Turner a champion of data channels known as virtual networks. In such a network the bits of data flow through real, physical links. But software makes it seem as though they are flowing along totally different, fictitious pathways, guided by whatever rules the users desire. In present-day commercial virtual networks, those rules are just the standard network protocols, says Turner. But it is possible to create virtual networks that work according to totally new Internet protocols, he says, making them ideal laboratories for researchers to experiment with alternatives to the current standards. His group, for instance, is working on virtual networks that enable classes of applications that are not well-served by the current Internet. \"This includes person-to-person voice communication, person-to-person video, fast-paced multi-player games and high-quality virtual world applications,\" he says. \"In general, any application where the quality of the user experience is dependent on non-stop data delivery and there is low tolerance for delay.\" Moreover, Turner is just one of many researchers pursuing this approach. An academic\u2013industry\u2013government consortium known as PlanetLab has been providing experimental virtual networks on a worldwide collection of computers since 2002. The GENI test bed is essentially a collection of virtual networks, all of which run atop Internet2 and National LambdaRail. This allows the same physical infrastructure to handle multiple experiments simultaneously \u2014 including many of the experiments mentioned in this article. Looking farther down the road, says Turner, as the best of these non-standard, experimental protocols mature to the point of being ready for general use, virtual networks could become the mechanism for deploying them. They could simply be built on top of the Internet's existing, physical infrastructure, and users could start using the new functionality one by one. Different protocols could compete in the open marketplace, says Turner \u2014 and the era of the ossified Internet would give way to the era of the continuously reinvented Internet. Katharine Gammon is a freelance writer in Los Angeles. \n                     Nature Web Matters: Internet Computing and the Emerging Grid \n                   \n                     Keren Bergman \n                   \n                     GENI \n                   \n                     Steven Low \n                   \n                     Davis Social Links \n                   \n                     Jonathan Turner \n                   \n                     NSF Future Internet Network Design \n                   Reprints and Permissions"},
{"file_id": "463605a", "url": "https://www.nature.com/articles/463605a", "year": 2010, "authors": [{"name": "Ananyo Bhattacharya"}], "parsed_as_year": "2006_or_before", "body": "Researchers in France have switched on the world's most powerful nuclear magnetic resonance instrument. Ananyo Bhattacharya asks whether it will attract new life to NMR spectroscopy. The 12-tonne, 4.5-metre-tall machine does little to betray the fact that it is working. No flashing lights break its smooth, white cylindrical surface. It makes no noise, not even a hum. But an incautious step over the yellow-and-black striped tape on the floor 12 metres from the world's most powerful nuclear magnetic resonance (NMR) spectrometer will erase the data on one's credit cards. Individuals with pacemakers are warned to go no closer. It is the magnet that makes this spectrometer at the European Centre for High Field NMR (CRMN) in Lyon, France, unique and hugely powerful. In a magnetic field, atomic nuclei with an odd number of protons or neutrons will resonate when pulsed with the right radio-wave frequency. The frequency at which they resonate depends on the chemical environment of the atom \u2014 the type of atoms that surround it, and their proximity. By gauging the resonance frequencies for a compound, researchers can map the relative positions of atoms and the structure and identity of the molecule. It is a technique that researchers have used to great effect since the first commercial 30-megahertz (MHz) machines became available in the 1950s. The more powerful the magnet, the more spectroscopists can 'see' \u2014 picking up tiny traces of chemicals in complex mixtures of body fluids or environmental samples, for example. The magnet at the core of the Lyon spectrometer can generate a massive 23.5-tesla field. But they give it the designation 1 gigahertz (GHz) \u2014 the resonance frequency of a single hydrogen-1 nucleus in a field of this strength. As the most powerful of its kind, the machine represents a milestone. Lyndon Emsley, the CRMN's scientific director who worked for seven years with the federal and regional governments and the CNRS, France's basic-research agency, to buy the instrument for \u20ac11.7-million (US$16.3-million), says that the machine will attract Europe's best chemists and structural biologists. A huge swathe of science is set to benefit from the added field strength, he says, which will reveal structures of bigger, more difficult to analyse proteins, and enable a clearer look into complex mixtures of chemicals and metabolites. But the machine has some work to do to prove that this milestone is more than symbolic. An increase of 50 MHz over other high-field machines hardly matches the vast improvements in power made during NMR spectroscopy's heyday in the 1970s and 1980s, when superconducting magnet coils vastly enhanced field strength. Even Emsley's father, a chemist at the University of Southamphton, UK, who had long used NMR spectroscopy for his studies of liquid crystals, calls the step to 1 GHz \"incremental\". Nevertheless, Emsley the younger predicts that the behemoth will prove its mettle. One reason for his optimism is the way in which biologists, particularly structural biologists, have produced demand for increasingly powerful machines. Emsley suspects that as much as half the time on the new machine could go to cracking the atomic structures of proteins.  \n                Protein power \n              NMR spectroscopy provides advantages over its main rival, X-ray crystallography. Crystallography can resolve protein structures quickly \u2014 sometimes in hours or days, compared with months for NMR spectroscopy \u2014 but only after crystals of the protein have been grown, a trial-and-error process that can add months or years to a project. In addition, NMR spectroscopy can yield useful information about proteins without a well-defined structure, including those that are extremely difficult to crystallize because they assume a shape only when they bind to another protein. Charles Sanders, a biochemist at Vanderbilt University School of Medicine in Nashville, Tennessee, and his collaborators recently used an 800-MHz machine to determine the structure of diacylglycerol kinase, one of the largest membrane proteins solved by NMR spectroscopy to date ( W. D. Van Horn  et al .  Science    324,   1726\u20131729; 2009 ). But he says that the 1-GHz machine will appeal to those working on large proteins and membrane-bound proteins, which are extremely difficult to crystallize. For the latter, NMR-spectroscopy researchers can use lipid constructs called bicelles, which mimic cell membranes allowing bound proteins to assume a more natural shape. Some are even beginning to use nanodiscs, which corral lipid and protein together, maintaining them in their natural conformation (see image, right). A technique developed in the late 1990s called transverse relaxation optimized spectroscopy (TROSY), has allowed NMR spectroscopy to deduce the structures of ever larger molecules by labelling the protein with different isotopes ( K. Pervushin  et al .  Proc. Natl Acad. Sci. USA    94,   12366\u201312371; 1997 ). The physical properties that TROSY exploits are predicted to give the sharpest possible peaks at fields of up to about 1.2 GHz, so the new machine could allow spectroscopists to crack proteins that are currently outside their grasp. Sanders, who until recently only had access to an 800-MHz machine, says he has travelled to use a 900-MHz spectrometer. \"The same will be true for 1 GHz. People will travel for that.\" But Jeremy Nicholson of Imperial College London says that grant-giving agencies may be losing interest in protein NMR spectroscopy. \"It has been very successful in discovering the structures of a lot of very interesting, important proteins,\" he says. \"But very few of those discoveries have translated into anything that has made a big mark in medicine.\" Nicholson does foresee medical applications, however, particularly in metabolic profiling, which he helped to pioneer. Researchers generate complex chemical 'fingerprints' of body fluids such as urine, plasma or tissue \u2014 or even whole organisms such as nematode worms \u2014 which are analysed to tease out the effects of drugs, disease or environmental changes on the metabolism. Higher field strength helps to differentiate the thousands of different metabolites. Benjamin Blaise, a PhD student and trainee medic at the University of Lyon, is carrying out metabolic profiling studies using samples from children with urinary problems. Blaise was one of the first people to use the new spectrometer. \"It's amazing,\" he says. Formerly, he would run each of his samples about 100 times on the 900-MHz machine before getting a strong signal. Now, \"you just need two experiments and you have a beautiful signal\", he says.  \n                Watching reactions \n              The metabolic profiling work also signals a return to NMR spectroscopy's roots in chemistry. Analytical chemists will be able to do similar kinds of chemical profiles for things such as water and soil samples to measure industrial effects. And high-field NMR spectroscopy could be used to analyse chemical reactions as they take place on the surface of large particles, such as chemical catalysts, which are often attached to grains of silica. Here, the concentration of the active sample is often only around 1\u20132% of the total. The 1-GHz spectrometer \"opens up new horizons\", says chemist Christophe Cop\u00e9ret of the Lyon School of Chemistry Physics and Electronics. \"We can study reactions that were not amenable to NMR at lower field.\" There are still questions about whether the increase in field strength is worth the cost. Most NMR spectroscopy work doesn't require such large magnetic fields. Many protein structures can easily be deciphered at field strengths of less than 800 MHz, and 400 MHz is usually enough to check the purity and structure of compounds in chemistry during organic synthesis. The CRMN already had a 900-MHz spectrometer before the 1-GHz machine arrived and it cost half as much. Given that for many applications the detail visible in NMR spectra improves in step with field strength, the extra \u20ac6 million seems steep for an improvement of around 10%. Emsley sees the machine as a resource for the entire research community. With the European Union contributing towards the running costs of the facility, up to 200 days a year will be allocated to users from other labs \u2014 many from outside France. Even relatively well-equipped labs would be lucky to have a 600-MHz machine. An increase of 400 MHz is \"an astonishing difference\", Emsley says. He points out that as magnetic field strength has increased in the past, new techniques have cropped up to exploit them. For example, TROSY came along just as NMR spectrometers with fields higher than 700 MHz were becoming more widely available. Sanders agrees. \"In the past, every time there has been a big increase in the size of the magnet, NMR has leapt forward,\" he says. \"Based on history, one might guess that there will be some breakthrough that results from this.\" Kurt W\u00fcthrich, who won the 2002 Nobel Prize in Chemistry for the development of NMR spectroscopy to determine protein structures, cautions that the step from 950 MHz to 1 GHz is small compared with the big hikes in field strength that have taken place since the early 1960s. For instance, fields had increased from around 60 MHz to 600 MHz by the mid-1980s. \"There's now talk of 1.2- and 1.3-GHz machines,\" says W\u00fcthrich, who is based at the Swiss Federal Institute of Technology in Zurich and the Scripps Research Institute, in La Jolla, California. \"But in the past, relative increases were bigger.\" The inroads that NMR spectroscopy has made into previously intractable problems hasn't just been driven by the increases in field strength. Other improvements in technology have contributed \u2014 such as the probe heads used to pulse samples with radio waves and measure resonances, and the computers that crunch the data. There have been big advances in sample preparation too, such as stereo-array isotope labelling (SAIL). In SAIL, proteins are made from specially labelled amino acids and these proteins produce much better spectra in NMR experiments. Jim Emsley, Lyndon's father, says that the 1-GHz machine will provide a worthy challenge for his son. The technical advances of recent years and the funding secured by the Lyon centre are all signs that NMR spectroscopy remains a dynamic field. \"As long as the science keeps saying 'Yes, it is worthwhile', that impetus will continue,\" he says. \"So the onus now, obviously, is on Lyndon's laboratory.\"   Ananyo Bhattacharya is online news editor for    Nature . \n                     CRMN \n                   \n                     Lyndon Emsley \n                   \n                     Benjamin Blaise \n                   \n                     Christophe Coperet \n                   Reprints and Permissions"},
{"file_id": "463724a", "url": "https://www.nature.com/articles/463724a", "year": 2010, "authors": [{"name": "Rex Dalton"}], "parsed_as_year": "2006_or_before", "body": "Eske Willerslev combines Arctic escapades with meticulous lab work in his quest to pull ancient DNA from the ice. Rex Dalton talks to the adventurer about extracting the first ancient human genome. All Eske Willerslev wanted was a scrap of something human: some hair, a bone, a tooth. For this, the Danish evolutionary biologist flew to Greenland's most remote northern tundra in 2006. At Nord military station, he learned that the helicopter that was to take him and his team to an archaeology site had just crashed in a storm. Undeterred, he commandeered a plane, which had to land in the rocky outback. Then there were six frigid weeks of probing remnants of human camps \u2014 all without finding the specimen he sought. So two years later, Willerslev could laugh when he found the hair he was seeking a 10-minute bicycle ride away from his lab in Copenhagen, in the basement of the Natural History Museum of Denmark. From that tuft, which had been extracted from permafrost some 20 years earlier, Willerslev and his team have now secured the first complete ancient human genome 1  (see  page 757 ) \u2014 that of a palaeo-Eskimo some 4,000 years old, whose ancestors had trekked from Siberia around the Arctic circle in pursuit of game. The accomplishment comes less than a decade after the first genome from a living human was deciphered, and at a time when the ancient-DNA field still struggles to eliminate contamination from samples. With it, the 38-year-old has cemented a reputation for conjuring prehistoric genetic gems from frozen ground. \"I never quite fully appreciated Eske's willpower until this project,\" says Thomas Gilbert, an evolutionary biologist who works with him at the University of Copenhagen. \"Now, if he said he was going to walk on water, I think he might.\" \"It is a lot of fun having Eske around,\" adds Morten Meldgaard, director of the Natural History Museum. \"He takes things to another level.\" By comparing the ancient Eskimo sequence to modern human genomes from Arctic populations and elsewhere, Willerslev's team has deduced that the palaeo-Eskimo was a male member of the Arctic Saqqaq, the first known culture to settle in Greenland. The team shows that the Saqqaq are closest genetically to the Chukchis from Siberia, and that beginning about 5,500 years ago, this man's ancestors migrated from Siberia east through Alaska and Canada to Greenland, settling there about 1,500 years later. The genome settles a debate about Saqqaq origins, showing that they are descended from separate migrations from those that gave rise to today's Inuit Greenlanders and natives of North America. It also suggests that the man was dark skinned, brown-eyed and possibly going bald. Worldwide, there are just a handful of laboratories seeking to wrest ancient genomes out of bone, teeth or even petrified faeces. In the past five years, though, Willerslev and the Center for GeoGenetics he directs have become leaders of the pack. In 2003, Willerslev reported the recovery from permafrost cores of the oldest known DNA at the time \u2014 a collection of plant and animal sequences dating from 10,000 to 400,000 years ago and including mammoth and bison 2 . By 2007, his group had showed how DNA extracted from ice cores could describe ancient plants and animals from an estimated 450,000\u2013800,000 years ago, painting a scene of a Greenland once rich with forest 3 . The next year, he and collaborators reported DNA from fossilized human faeces at what is thought to be the oldest inhabited site in North America, Oregon's Paisley Caves, dating a human presence there to some 12,300 years ago 4 . Sequencing the roughly 3 billion base pairs of an ancient, fragmented human genome \u2014 and showing convincingly that it is not tainted with the near-identical DNA of modern humans \u2014 was the biggest challenge yet. What made this possible is Willerslev's exacting requirement for technical detail and his willingness to go to extremes in pursuit of pristine DNA. The best place to find it is entombed in ice, where it is preserved by the cold and protected from contamination. And Willerslev's background made him want to go looking for it.  \n                Frozen feats \n              As a boy, Willerslev frequently journeyed beyond the upmarket suburb of Copenhagen where he lived. His father, a history professor at the University of Copenhagen, took him and his identical twin brother, Rane, on his trips to study Swedish migrations of the 1800s, and the twins spent summers at the family's cabin in a Swedish forest. \"Our father was pretty tough \u2014 into icy waters and chopping wood,\" says Rane Willerslev, now an ethnologist at the Arrhus University in Denmark. \"We did everything together,\" he says, speaking of his brother. \"It was always competitive.\" In 1993, the twins embarked on a summer fossil-collecting trek in eastern Siberia that extended into a five-month winter sojourn with Russian trappers. Moving by cross-country skis, hauling gear in dog sleds, Eske Willerslev hunted moose, set traps and skinned mink and sable. \"Physically and mentally, I was done by Christmas,\" he recalled, \"but it was weeks later before I got out.\" The pair eventually escaped in a Russian tank-like vehicle. \"I didn't think my Siberian trapping experience would add to my scientific career, but it did,\" says Willerslev. \"It taught me how far to push the body and mind. I find I can work twice as long as some colleagues.\" Willerslev applied that stamina to his academic work at the University of Copenhagen, where as a master's and doctoral student he showed that it was possible to extract ancient DNA from debris in polar ice cores 5  and permafrost 2 . Captivated by the field, he moved to the University of Oxford, UK, in 2003 for a fellowship in the lab of Alan Cooper, a prominent member of the ancient-DNA field. In 2005, Willerslev returned to Copenhagen to establish Denmark's first ancient-DNA lab, which grew into the Center for GeoGenetics. A year ago, he won a 50-million-Danish-krone (US$9-million) excellence grant from the Danish National Research Foundation, along with a 25-million-krone government grant to build up and equip the centre. Willerslev has been ruthless about keeping contamination at bay. He prowled museums at home and abroad looking for usable ancient specimens. It was fear of contamination that prompted his team to risk the flight to northern Greenland \u2014 to a location so inhospitable that, they hoped, no modern archaeologists would have reached it. The hair clump that they eventually used for the ancient-human genome project had been discovered in 1986 on the western shore of Greenland's Disko Bay, on an archaeology dig directed by Meldgaard's father. It had been stored in a specimen case at room temperature and handled very little. Willerslev says that it is easier to capture pristine ancient DNA from hair than from bone and teeth. Hair doesn't as readily absorb contaminants, and its surface can be bleached clean.  \n                Well preserved \n              Shortly after securing the hair, Gilbert, Willerslev and their colleagues sequenced the roughly 16,000 base pairs of the mitochondrial genome, work that was published in 2008 (ref.  6 ). Usually, researchers have an easier time finding mitochondrial DNA in ancient specimens, because it is more abundant and survives more readily than that from the nucleus. But permafrost seemed to have maintained large amounts of nuclear DNA in the Saqqaq sample too. \"About three weeks after the mitochondrial genome publication, I recall waking in the middle of the night, thinking: We can do the whole genome,\" Willerslev says. To guard against contamination, a co-author of the genome study, Rasmus Nielsen of the University of California, Berkeley, tagged the millions of fragments of extracted DNA with a barcode-like sequence to distinguish them from stray modern human DNA. The genome sequencing was completed in two and a half months last year at a cost of about $500,000 by teaming up with Jun Wang, deputy director at the Beijing Genomics Institute (BGI) at Shenzen, China. Willerslev contacted the institute after learning it had 120 of the newest Illumina sequencing machines. The collaboration has since blossomed: late last year, the Sino-Danish Genomics Center was established at the BGI. By the end of the summer, the group had sequenced the ancient genome roughly ten times over on average, equivalent to the coverage achieved for the Human Genome Project. Then it was discovered that the data included artefacts from the sequencing process. \"A cheapskate might have been happy with ten times coverage,\" says Gilbert. \"Eske wanted higher quality.\" Willerslev decided to refine and re-sequence. The published genome has been sequenced 20 times on average over nearly 80% of its length. This level of rigour helps to ensure that any differences between the ancient and modern sequences are true differences, rather than sequencing errors, or degraded or contaminating DNA. To identify the origins of the ancient man, the team had to compare his sequence with some from living populations. They turned to researchers such as Richard Villems, a population geneticist at University of Tartu in Estonia, who studies migrations worldwide and whose freezers are stocked with human specimens. \"The ancestry really gives you a geographical context,\" says Villems. \"This is important for Eske and me. We both are field men.\" And the field is where Willerslev is heading next. This summer, he will return to Greenland, seeking DNA from more humans and hoping to map out broader migration patterns across the north. He is also ready to take on the next technical challenge: \"I hope to do a complete ancient genome from a non-frozen specimen,\" he says, with all the additional contamination problems this could entail. Few doubt he can do it. \"I think he's the best in his field,\" says Dennis Jenkins, an archaeologist at the University of Oregon in Eugene who sought Willerslev's expertise to date the human faeces in Oregon's caves. \"When you provide him with samples, you get results.\"   See News & Views,  \n                     page 739 \n                   , and Article,  \n                     page 757 \n                   . \n                     Eske Willerslev \n                   \n                     Beijing Genomics Institute \n                   \n                     Saqqaq Culture \n                   \n                     Chukchi Culture \n                   Reprints and Permissions"},
{"file_id": "463726a", "url": "https://www.nature.com/articles/463726a", "year": 2010, "authors": [{"name": "Michael Cherry"}], "parsed_as_year": "2006_or_before", "body": "The release of Nelson Mandela sent optimism coursing through South Africa's research community. Twenty years on, Michael Cherry finds that it is still struggling to get on its feet. When Nelson Mandela was released from prison on 11 February 1990, Carolyn Williamson watched the events on her television in the Canadian province of Newfoundland. Having completed her doctorate a year previously, she was seven months pregnant and being supported by her husband. She listened to the stirring speech \u2014 \"apartheid has no future\" \u2014 that Mandela delivered from the balcony of Cape Town's city hall. \"I was very sad not to be there,\" Williamson says, \"and that was the deciding factor in our decision to return home later that year.\" Williamson took up a temporary lectureship at the University of Cape Town and, 20 years on, she has no regrets. One of the country's foremost HIV researchers, she runs a thriving group of ten postdoctoral and graduate students that forms part of the university's Institute for Infectious Diseases and Molecular Medicine. \"Being back in South Africa has contributed to my success,\" she says: the country's soaring rates of HIV and tuberculosis have presented research opportunities. But Williamson also knows that she is one of the lucky ones. \"The only reason I have been able to survive in terms of research is because I have had international funding,\" she says. By contrast, a generation of younger researchers is finding it tough to keep labs alive. Botanist Bruce Anderson was a 15-year-old in 1990 and now has a tenured position at Stellenbosch University \u2014 but he is surviving on funding of 40,000 rands (US$5,400) a year from the country's National Research Foundation (NRF). \"All I can fund with this are the running costs for my own research and that of a master's student,\" he says. The mixed feelings among researchers now are rather different from the optimism they shared when Mandela was released. South Africa was already by far the biggest research base on the continent, with historical strengths in fields such as optical astronomy, geology, botany, zoology, clinical medicine and mining technology. Associated with this base is a set of universities, the best of which are far better than in any other country on the continent. But for many years research had been focused on big applied projects aimed at addressing the isolation of the apartheid era, such as extracting oil from coal, uranium enrichment, and building a national accelerator. Buoyed by political change, researchers who had felt isolated hoped that renewed contact with the outside world would be a stimulus to promote good-quality basic research, rather than projects associated with a state of siege. More than anything else, the reform of the country's education system was expected to unleash a pool of talented students who had been denied opportunities during apartheid. Lack of strong science leadership, a dearth of funds and a series of well-intentioned but poorly executed schemes have left most of those hopes unrealized. In 1994, the Mandela government established a ministry of science, technology, arts and culture; later, under Mandela's successor, Thabo Mbeki, the Department of Science and Technology (DST) split off as a separate ministry. But its spending priorities have been questioned, its efforts to boost student numbers have failed to live up to expectations, and beyond that, many scars of the racially segregated education system remain. The research community's hopes are now pinned on the new minister of science and technology, Naledi Pandor, who has been in office as part of Jacob Zuma's government for only nine months. A language specialist by training, Pandor has emerged as an enthusiastic advocate for science and technology, and has been refreshingly frank about her ministry not meeting its objectives. Moreover, she agrees with researchers that in the past too much focus has been placed on applied research \u2014 both under apartheid and by the new governments. \"There has been some shift in emphasis to applied as opposed to basic research which we need to correct,\" she says.  \n                Finding funding \n              Research funding in South Africa is a perennial problem. In 1990, the apartheid government was spending 0.61% of its gross domestic product (GDP) on research and development (R&D), but at least half of this was in the military sector. Spending started to climb as South Africa entered a boom period, and in 2003, at the inaugural meeting of the African ministerial council on science and technology, the Mbeki government agreed to a target of 1% of GDP by 2008. In Africa, only Egypt spends more than this, yet the target is still modest compared with, for example, the 27 European Union states, which spent 1.85% of total GDP in 2007. By 2006, South Africa's spending on R&D had grown to 0.95% (see graph). But it dropped slightly in 2007 and the 1% target seems likely to be missed \u2014 2008 figures will be released later this year. One major problem is that research has lacked a strong advocate in government. For 15 years \u2014 until Pandor was appointed last year \u2014 the DST was headed by politicians from minority partners in government with the ruling African National Congress, and they lacked the political clout to substantially increase sums allocated to research. Hand in hand with funding problems has been the difficulty of recruiting graduate students. According to the DST, the country awarded 23 PhDs per million population in 2003, which is low compared to other emerging economies (for example, Brazil's 42 or South Korea's 172). Five years ago, after the appointment of mathematician Mosibudi Mangena as minister for science and technology, the DST and the NRF \u2014 the main government agency that funds postgraduate training and university researchers in natural and social sciences \u2014 jointly launched two schemes to address this situation. One was a programme aimed at increasing the number of PhDs in the country (1,274 in 2007, see graph) to 6,000 by 2018, and ensuring that half of those remained in science and engineering. The other was the country's research-chair scheme, which is intended to release top researchers from teaching and grant writing by awarding them five-year support packages, including replacement posts in their departments so that they can concentrate on training graduate students. Heinrich Schwoerer arrived from Germany two years ago to take up a research chair in laser physics at Stellenbosch. Schwoerer, whose research uses very fast spectroscopy to investigate molecular reactions in real time, is delighted with the move. \"I have more freedom than I would have in my own country in terms of pursuing my research interests,\" he says. But after the first two rounds of appointments in 2006 and 2007, which created a total of 72 chairs, only another 10 have been appointed. The goal of 210 chairs by the end of this year seems unlikely to be met, as the DST has not yet allocated the necessary funds to the NRF. PhD recruitment also seems to have faltered. The number of PhD students supported by the NRF rose by 61% in 2005, from 1,360 to 2,186, after the DST made more funds available. But since then, the increase has stalled. Figures for 2009 are not yet available, but the number of PhDs awarded is expected to be significantly lower than in 2008: last year the DST cut its allocation to the NRF slightly. The department's explanation was that the global economic crisis forced it to slash budgets across the board. Robin Drennan, executive director of governance at the NRF, says that the agency's goal is still to increase the number of PhDs produced, to 6,000 per year by 2025. \"We realize that this is a stretch target,\" he says. He also points out that the value of graduate studentships was increased in 2008 for the first time in many years, and that this could affect the number of students that can be supported. To researchers, however, the real reason the department cannot sustain these programmes is no secret: South Africa's democratic government has yet to shed its apartheid-era-predecessor's appetite for 'big science' projects. Seven years ago, the country bid to build the powerful radio telescope called the Square Kilometre Array (SKA). After it was short-listed with Australia as one of two possible host sites, the DST allocated 1.9 billion rands to the project between 2009 and 2012, mostly towards the construction of a prototype telescope, MeerKAT. This sum, comprising 14% of the DST's annual budget, is almost three times the current annual allocation to the NRF (680 million rands). If the country wins the bid, the construction cost for the SKA is estimated at more than $2 billion. Saul Dubow, a historian at the University of Sussex, UK, who has studied the history of science and apartheid, says the ambition runs counter to the rhetoric of the Mbeki era about the 'Africanization' of science. The country's \"aspirations remain thoroughly first world in orientation, and the appeal of prestige projects remains virtually undimmed\", he says. Pandor says that \"it is important to have grand challenges and to succeed in addressing these\", and that, if South Africa wins the SKA bid, \"this would bring a great deal of opportunities to the country\". Not all researchers share her conviction, or believe that the country will win the telescope. Phil Charles, director of the South African Astronomical Observatory in Cape Town, says that the sites on offer in South Africa and Western Australia are technically equivalent and that \"South Africa's investment in a prototype is a gamble for the jackpot\". The gamble wasn't helped when Charles was suspended this month following allegations that he had leaked documents relating to the prototype telescope (see   Nature doi:10.1038/news.2010.52 ). The NRF itself has come under fire for the way that it allocates what funds it has. Over the past two years the agency has reverted to a system implemented in the apartheid era but abandoned in 1996, by which researchers, rather than their proposals, are rated and commensurate research funds are then awarded annually over a given funding cycle. A-rated researchers receive 100,000 rand each year; B-rated researchers, 80,000; and C-rated researchers, 40,000. This system is controversial: a report commissioned in 2007 by Higher Education South Africa, an organization that represents the universities, found \"a growing scepticism and even disillusionment\" with the rating system. The disillusionment comes partly from the fact that, compared with 20 years ago, the rating-based awards are a pittance. Bongani Mayosi, the University of Cape Town's chair of medicine and whose group studies cardiomyopathy, is feeling the sharp end of the cuts. Over the past three years he has received 1 million rands in studentships and running expenses from the foundation, but this year he will get only 80,000. \"This has meant that I have had to turn away promising doctoral students, something that is counter to the national imperative to increase production of PhD graduates,\" he says. As a clinical investigator, Mayosi is able to survive on other grants, including ones from the US National Institutes of Health and the South African Medical Research Council, which falls under a different ministry from the DST. But although grateful for the support \u2014 nearly 11% of South Africa's R&D in 2007 was financed from abroad \u2014 researchers say that dependence on foreign funds means they have to dovetail their research priorities with those set by other countries.  \n                Education legacy \n              Much of the difficulty in recruiting PhD students can be traced to the wider problems of addressing the legacy of a racially segregated education system. Under apartheid, whites (now only 9% of the population) attended better-funded state schools than the 9% of people of mixed race (usually called coloureds in South Africa), those of Indian extraction (2%) and indigenous black Africans, who comprise 80% of the population. Since 1994, the government has in theory been committed to ensuring that 'blacks' (a generic term it uses to mean all people not classified as white under apartheid) have had improved access to higher education and research, but this has turned out to be difficult to achieve, particularly in science. Stark racial differences in participation rates still exist across the board in higher education: in 2006, 59% of white and 42% of Indian 18\u201324-year-olds \u2014 but only 13% of coloureds and 12% of black Africans \u2014 were engaged in tertiary education. And although the NRF is justifiably proud that more than half the doctoral students it supported in 2008 were black, it declined to disclose how many of these were in science and engineering. Many black schools in the apartheid era did not offer mathematics and physical science as subjects, initially as a point of policy, and latterly on account of a teacher shortage. But the sad reality is that after almost 16 years of democracy, the proportion of black and white school leavers attaining good enough grades in these subjects to qualify for university courses in science has not risen significantly, in part because efforts to train and recruit schoolteachers in these subjects have failed. Most school leavers with good grades in maths and science have little time for research anyway \u2014 they are attracted to more lucrative fields such as medicine, engineering and commerce. Christopher McQuaid, a marine biologist who has a research chair at Rhodes University in Grahamstown, has been unable to fill all his studentships with South African candidates: half of his graduate students come either from other African countries or from overseas. \"Meritorious black candidates are particularly scarce, and then often drop out when they experience pressure from their families to get jobs,\" he says. In terms of staff transformation, the situation is even worse: universities struggle to retain talented black staff members, who leave for higher salaries in the public services or the private sector. Of the NRF's 1,922 'rated' researchers in all fields, for example, only 16% are black. As minister of education in the previous government, Pandor is aware that the problems faced by science in South Africa cannot be resolved without fundamental reform of the school system. She concedes that insufficient attention has been paid by government to funding the science councils (the DST has budgetary responsibility for the Human Sciences Research Council and the Council for Scientific and Industrial Research as well as the NRF). She is also adamant that the research-chair target has not been abandoned \u2014 \"Actually I believe that 210 research chairs is not enough,\" she says \u2014 and that she intends to continue to lobby the treasury for funds. How much weight she carries within her party should become clearer when the first budget of President Zuma's government is presented to parliament later this month. Mayosi, for one, remains optimistic, but cautions researchers not to be complacent: \"If we don't fight for resources, we will perish,\" he says. \"We must fight our corner \u2014 the future of research is too important to leave in the hands of government alone.\"   See Editorial,  \n                     page 709 \n                   , and Opinion,  \n                     page 733 \n                   . Michael Cherry is  Nature 's contributing correspondent in South Africa. \n                     South African Department of Science & Technology \n                   \n                     South African National Research Foundation \n                   Reprints and Permissions"},
{"file_id": "463868a", "url": "https://www.nature.com/articles/463868a", "year": 2010, "authors": [{"name": "Eric Hand"}], "parsed_as_year": "2006_or_before", "body": "As hundreds of US astronomers draft their latest decadal wish list of new projects,  Nature   took a short-cut by convening a small survey around a dinner table. Eric Hand listens in. In a Lebanese restaurant in Washington DC, seven astronomers are sitting down to dinner. It is a crisp evening in January, and the researchers are here at  Nature 's request to discuss the challenges faced by their discipline and the future directions it might take. Although the group is in town for the annual meeting of the American Astronomical Society, the dinner has been convened to consider the next 'decadal survey', which is set for release this September. Since the 1960s, astronomers have been drawing up these surveys to present funders with a prioritized wish list of new facilities they would like to see built over the coming decade, together with price estimates and a discussion of the science they would perform. The decadal surveys carry significant weight with Congress and funding agencies, which use them to guide their spending decisions. But they are a huge undertaking: by the time the 2010 survey is complete, it will have involved 218 scientists, at least 37 committee meetings and consumed some $4 million in funds from NASA, the National Science Foundation (NSF) and the Department of Energy. This is in stark contrast to the first survey in 1964, which consisted of just eight astronomers \u2014 a set-up more closely aligned with the dinner group convened by  Nature . The seven guests at the dinner table, representing a cross-section of their community (see  'Who's who' ), are not involved in the formal decadal survey. But they are well aware of the challenges it faces. They know that it would be hard to justify building, say, a billion-dollar, 30-metre-diameter telescope on the basis of it being newer and more sophisticated than the dozens of large ground-based telescopes that already exist. For any project, \"bigger and better is not going to sell it\", says Josh Grindlay. Matt Mountain agrees. \"It's got to be, 'Here is the science that we are going to do with this',\" he says. Even after the community agrees on what the key science goals are, explaining those goals to non-astronomers can be fraught with pitfalls. The most recent survey, in 2001, suffered because it lacked a cohesive 'narrative', the diners note. \"There was no story behind it,\" says Mountain. Or rather, there were too many stories and no overarching theme, unlike the way the 1991 survey framed its wish list around 'the decade of infrared astronomy.' \"I remember talking to a [congressional] staff member who said 'Why do you want all these things?'\" Mountain continues. \"'Why should the federal government fund astrophysics?' is the fundamental question that the decadal survey ultimately has to answer.\" Alycia Weinberger agrees: \"Part of the goal is to sell the project of astronomy to the American people or to Congress as their proxy,\" she says. In some ways that sale can be straightforward. \"If you sit on an airplane and say you're an astronomer, people talk to you,\" says Bruce Partridge. \"If you sit on the airplane and say you're a physicist, they read their book.\" Many aspects of astronomy can readily grab the public's imagination, says David Silva. \"When you talk about our connection to the general public \u2014 the people who are ultimately paying for what we do \u2014 the search for life in the Universe is one of the fundamental questions that we want to answer as a species: are we alone in the Universe?\" \"We walk out there and we say, 'We're really about origins',\" adds Garth Illingworth. \"That excites people and it's easy to communicate.\"  \n                A question of time \n              That search for habitable worlds is likely to figure prominently in the forthcoming survey. The hunt for exoplanets \u2014 worlds orbiting stars outside our Solar System \u2014 has \"exploded virtually out of nowhere\", says Silva. First detected in the mid-1990s, exoplanets were still a comparative rarity in 2001. Detection techniques have now improved to the extent that the list now contains more than 400 planets and is growing fast. \"Now we're looking at [spectral] signatures and atmospheres,\" says Silva. \"We're in an amazing, amazing era.\" Many astronomers are betting that the first Earth-sized exoplanet will be discovered within a few years \u2014 and then the race will be on see whether its atmosphere shows the spectral signatures of oxygen and water, possible signs of life. The rapid developments in the hunt for exoplanets highlight another problem faced by the decadal surveys. \"Science changes on time scales that are often shorter than the distance between two decadal surveys,\" says Silva. A similar case in point is dark energy, which is also likely to dominate the forthcoming wish list. So far, little is known about dark energy other than it pervades the Universe, comprises three-quarters of its mass and seems to be accelerating its expansion. \"At the last decadal survey, we were just absorbing the conclusion of an accelerating Universe and what that impact was,\" says Silva. \"Now [understanding the nature of] dark energy is the raison d'etre of a lot of projects.\" In addition to dark energy and exoplanets, the diners expect to see several other science horizons in the survey, such as gravitational waves \u2014 infinitesimal ripples in space-time caused by ultraviolent events such as the collision of two black holes. Upgrades to existing facilities such as the ground-based Laser Interferometer Gravitational-Wave Observatory ( LIGO ) are beginning to reach the required sensitivities to detect one of these waves. \"We may get it in five years,\" says Jay Lockman, who notes that radio telescopes are also aiming for a detection by measuring variations in the timing of pulsars. The proposed space-based Laser Interferometer Space Antenna ( LISA ) would go further, pinpointing the source and shape of the gravitational waves \u2014 if it gets the green light. Getting that approval from funders is likely to hinge on one major factor in the survey: believable cost estimates. The 2001 survey gave the community \"a black eye\", says Silva, for claiming that its entire slate of 7 major and 12 medium-sized projects could be built for $4.4 billion (about $5.3 billion today). In fact, that figure barely covers the survey's number one priority, the James Webb Space Telescope ( JWST ), which is now expected to cost $4.9 billion. \"I think many projects have recognized that,\" says Silva, \"and really tried to step up the credibility of their cost estimates this time.\" Escalating costs also mean that astronomers need to be realistic in their expectations, especially when it comes to the billion-dollar-plus 'flagship' missions. The survey should recommend \"two in space, two on the ground\", says Grindlay. \"And it should clearly prioritize \u2014 as it's unlikely that we probably will even do one in the current climate.\" Historically, notes Mountain, \"it's a flagship per decade\" \u2014 one in space and one on the ground. In the current budgetary climate, completing even that many projects before the end of the decade may be tough. NASA will probably not take on any major new mission until later in the decade, after the JWST launches in 2014. And the NSF is just as unlikely to undertake another major astronomy project until the Atacama Large Millimeter/Submillimeter Array ( ALMA ) in Chile is finished in 2012. This makes the ranking of the projects in the wish list that much more crucial. Putting a project first instead of second can have a huge influence on the direction of astronomy \u2014 and in the process, shape careers. Lockman recalls one colleague saying that: \"If this one project gets funded, he has a salary for three years; if another project gets funded, he doesn't have a salary for three years.\"  \n                High priorities \n             \n               Click here for larger image \n               To get a sense of the 2010 survey's likely priorities, the diners filled out ballot papers giving their personal rankings for the projects expected to figure in the real survey (see  'Astronomy priorities' ). The straw poll's number one pick is the Large Synoptic Survey Telescope ( LSST ), which would use its 3,200 megapixel camera and ultrawide field of view to scan the heavens continuously in search of things that don't stay fixed: from near-Earth asteroids, to the optical counterparts of \u03b3-ray bursts and X-ray flashes. The LSST should also contribute to the understanding of dark energy, by systematically measuring the gravitationally distorted shape of 3 billion galaxies. Number two in the poll \u2014 the same spot it held in the 2001 survey \u2014 is the Giant Segmented Mirror Telescope ( GSMT ): a workhorse observatory that would dwarf existing telescopes with a main mirror roughly 30 metres across. The NSF would ultimately need to choose between two competing designs, although it would not have to bear the entire cost: each has already attracted millions of dollars from private philanthropists. \"You can convince people to give money for a giant telescope on the ground,\" says Weinberger. \"There's hardly any other field where you can make an impact like that.\" A planet-hunter mission \u2014 the Terrestrial Planet Finder ( TPF ), which would seek to capture the spectrum of an Earth-like planet \u2014 was ranked as the most important space mission to pursue, and third overall. \"If you delivered the spectrum of a living planet, that would be the most memorable thing of the decade,\" says Mountain. The ranking is a big jump up from the last decadal survey, when the TPF was ranked sixth. However, the TPF of the 2001 report was an expensive flagship mission that would have taken the form of either many telescopes flying in formation, or a large telescope with an internal filter. Most of the diners agree that although the TPF is important, it should be realized in a far cheaper configuration. The straw poll is interesting not just for the leaders but also the laggards. For example, next to last in the poll was the Joint Dark Energy Mission ( JDEM ), a small space telescope optimized to pin down the nature of dark energy. As recently as 2007, the JDEM received the top-priority endorsement in a report from the National Academies on NASA's Beyond Einstein astrophysics programme. With the energy department committing a portion of the money \u2014 and with the European Space Agency also expressing interest in becoming a partner \u2014 the JDEM seemed to be on the fast track. But since then, bureaucratic squabbles and scientific disagreements on the telescope's design have left it in limbo (see   Nature  461, 1182\u20131183; 2009 ). The poll could reflect a growing belief that, for now, the mystery of dark energy is best attacked from the ground with instruments such as the LSST.  \n                Difficult decisions \n              What took an hour and seven pieces of paper in  Nature 's mock survey will have taken two years and some 600 white papers in the real thing. \"It's shocking to think about how many person hours of effort went into all of those white papers,\" says Weinberger. So the question is inevitable: has the whole process become too cumbersome? Or is the complexity necessary \u2014 a product of the scientific ambition, technological sophistication and staggering expense of modern astronomy? It's the latter, says Grindlay, who credits Roger Blandford, director of the 2010 survey, with trying to involve the whole field. \"It's an impossible organizational task,\" he says. \"I'm not sure whether it will work in the end, but he's made the right effort.\" And as Grindlay points out, the final priorities will be decided by a group only slightly larger than that gathered for  Nature 's dinner: the 23 members of the survey committee itself. \"They're going to make the hard decisions,\" he says. They may face an unenviable task and a difficult funding landscape, but that is no reason for the community to lower its sights. \"You should be ambitious,\" says Weinberger. After all, adds Mountain, citing Galileo and an associated 400 years of history, \"Astrophysicists have the power to change society.\" \n                 For the panel's views on data accessibility, career prospects and the rise of chemistry, see the full transcript of the round-table discussion at  \n                 \n                     http://go.nature.com/ITkwWr \n                   \n               \n                     NASA Astrophysics \n                   \n                     National Optical Astronomy Observatory \n                   \n                     National Radio Astronomy Observatory \n                   \n                     Thirty-Meter Telescope \n                   Reprints and Permissions"},
{"file_id": "463864a", "url": "https://www.nature.com/articles/463864a", "year": 2010, "authors": [{"name": "Tanguy Chouard"}], "parsed_as_year": "2006_or_before", "body": "Experiments have revealed how single mutations can have huge effects that drive evolution. But small steps pave the way, finds Tanguy Chouard. Since the origin of evolutionary science, biologists have insisted that adaptation is an achingly slow process. '  Natura non facit saltum  ' (nature does not take leaps) was a favourite incantation of Charles Darwin. As the combined power of genetic mutation and natural selection became better appreciated in the 1930s and 1940s, theorists solidified a gradualist doctrine: adaptation must rely on innumerable genetic changes, each with effects so small that any attempt to catch them experimentally was considered futile. Suggestions to the contrary were met with ridicule: geneticist Richard Goldschmidt, in 1940, envisioned subtle developmental mechanisms producing great leaps of adaptation, but his use of the phrase \"hopeful monsters\" was misrepresented as extreme saltationism (perfection in one jump), and equated with belief in miracles. But through fish in the murky depths of a British Columbia lake and through bacteria in the flasks of a Michigan lab, the monsters have returned. Experimental evidence has shown that individual genetic changes can have vast effects on an organism without dooming it to the evolutionary rubbish heap. Single-gene changes that confer a large adaptive value do happen: they are not rare, they are not doomed and, when competing with small-effect mutations, they tend to win. But small-effect mutations still matter \u2014 a lot. They provide essential fine-tuning and sometimes pave the way for explosive evolution to follow. As the molecular details unfold, theory badly needs to catch up. \"This is a very exciting age,\" says Joe Thornton, who studies protein evolution at the University of Oregon in Eugene: new molecular approaches are bringing mechanistic understanding to the field of 'evo-devo' (evolutionary developmental biology), ushering in what Thornton calls \"the functional synthesis\" 1 . The shift even promises to bridge microevolution and macroevolution, suggesting how, for example, genetic changes \u2014 large and small \u2014 might eventually lead to a new species.  \n                Marine gladiators \n              The promise of such insights was what drew developmental biologist David Kingsley from Stanford University in California and then-postdoc Katie Peichel, to the three-spined stickleback ( Gasterosteus aculeatus ). To move beyond evo-devo studies that compared gene expression differences between vertebrate species, they decided to identify the genetic changes that actually caused variation in body plans. Sticklebacks were chosen because populations in different environments can look very different but can produce fertile hybrids through  in vitro   fertilization (IVF). So, in 1998, Kingsley and Peichel flew to Vancouver, Canada, to start a collaboration with Dolph Schluter, an ecologist and evolutionary biologist at the University of British Columbia, and an expert in stickleback IVF. They used two stickleback populations: a heavily armoured one, from a patch of sea north of Vancouver, that sports a pair of sharp pelvic spines, and one that lives at the bottom of British Columbia's Lake Paxton and is relatively naked. With Schluter's help, one saltwater female and one freshwater male produced a slew of grandchildren with features anywhere between gladiator and nudist. In particular, they exhibited pelvic spines of all sizes. Such features that vary along a continuum are known as quantitative traits and can be linked back to large segments of genomic DNA \u2014 the quantitative trait loci (QTLs) that cause them. By analysing the DNA of hundreds of hybrid fish, the Kingsley group spotted one particular QTL in the genome responsible for two-thirds of all variation in pelvic spine length 2 . Such large-effect QTLs were not unheard of \u2014 many had been shown to vastly change the sizes of leaves or fruits in domesticated plants, for example \u2014 but the reigning gradualist dogma regarded these as artificially protected monstrosities that would never survive the harsh hand of natural selection. It remained unclear whether plant QTLs found in natural populations corresponded to individual genes. To map vertebrate QTLs at higher resolution, Kingsley needed greater statistical power. \"The combination of Dolph's large hatcheries with our dense physical maps of the stickleback genome allowed us to zero in on individual genes quite effectively,\" says Kingsley. In 2004 they found one gene 3 ,  Pitx1 , but it had a problem: it seemed to control the development of too many parts of the embryo. When disrupted, the gene, widely found in vertebrates, left mice with severe craniofacial abnormalities and pituitary defects. They died at birth. \"That didn't look too promising to evolve new traits,\" says Kingsley. Pleiotropy, the way in which a single gene can affect many factors, had been a major rationale behind gradualist dogma. In the 1930s, theorist Ronald Fisher formalized the intuition that mutations with large effects should be deleterious 4 . He conceived of an organism's quantitative traits as the axes of a multi-dimensional space with optimal fitness at the centre. In this model (see figure, right), an organism maladapted to its environment, such as a spiny stickleback in freshwater, lies some distance away from the optimum. A mutation may either be beneficial, nudging that organism closer to the optimum, or deleterious, pushing it further away. According to Fisher's model, pleiotropic mutations, such as the deletion of  Pitx1 , reduce the spine size, but push other traits fatally away from optimal fitness. How could a mutation in such a crucial gene result in anything but a hopeless monster? Further inspection of the freshwater stickleback  Pitx1   gene revealed a DNA lesion far more subtle than the gene knockout in mice: it removed some 500 base pairs of regulatory sequence that enhances Pitx1 protein production only in the pelvis 5 . With expression of the Pitx1 protein preserved in all other vital structures, freshwater sticklebacks could lose their pelvic spines without dire repercussions elsewhere. \"This made so much sense,\" says Kingsley. Thanks to the way regulatory DNA sequences are organized \u2014 as independent modules \u2014 evolution had managed a surgical strike on  Pitx1 , resulting in a beast hopeful and not so monstrous after all. This event is far from rare: freshwater sticklebacks with reduced pelvic structures fished from various sites across the world have independent deletions in the same DNA region, all more or less eliminating the marine pelvic enhancer. But how could such an improbable surgical strike happen over and over again just by chance? Recently, Kingsley's team showed that four of the ten stickleback DNA segments predicted to be most susceptible to deletion are in  Pitx1 , one right next to the pelvic enhancer 5 . The work, building on plant studies, showed that single-gene lesions could produce large morphological effects and yet be viable in natural situations. Such physical effects also seemed advantageous, but Kingsley's team could not prove this. This is because QTL studies map genotype (the DNA sequence) to phenotype (observable traits) but do not measure the resulting changes in fitness \u2014 actual reproductive success. To measure fitness, researchers need to watch mutants as they compete in real time. And the easiest place to watch this race as it happens is in the lab.  \n                Catching evolution red-handed \n              More than 20 years ago, Rich Lenski, then at University of California, Irvine, started a simple experiment. On a Wednesday morning, the 24 February 1988, he started the parallel evolution of 12 populations of  Escherichia coli , all clonally derived from a single bacterium and competing for limited sugar in Erlenmeyer flasks. Each day, roughly half a billion new copies of the  E. coli   genome are made in each flask as the bacteria multiply, along with about a million mistakes, meaning that in the span of a few days, virtually all conceivable mutations in the bacteria's five million base pair genome will have been attempted. Most of these mutations make no difference or are deleterious, but a few make the bacteria grow a little bit faster \u2014 providing up to a 10% growth advantage over their predecessors. The fastest ones are extremely rare and the population must 'wait' a couple of days for them to show up. Every night, the bacteria run out of the sugar glucose and go dormant. The following day around noon, a researcher plunges a pipette in and sucks up 1% of the culture to inoculate a fresh flask. Those faster at gobbling up glucose will send more of their descendants to the following day's pipette and, after a few weeks, descendants of the fastest one will be the only ones transferred as the mutation 'sweeps' to fixation. \n               boxed-text \n             To draw a parallel with an Aesop's fable, a small-benefit mutation, with its slow sweep but short wait, would be the tortoise of this race, whereas a large-benefit mutation, with its fast sweep but long wait, would be the hare. But in real life, whether wait or sweep matters more will depend on many factors, such as mutation rates (which vary from gene to gene), genome size, population size and composition (which both vary over time) and environmental conditions. Lenski's conditions seemed to favour the hare, at least when the researchers first looked. Just a few months in, Lenski and his students observed two striking things: the evolving bacteria's relative fitness \u2014 their growth advantage over the ancestor \u2014 increased in abrupt jumps. And these jumps became smaller in later generations (see graph, above). The jumps seemed to correspond to individual mutations taking over the population, one after the other: large-benefit mutations seemed to respond to the initial, abrupt environmental change, followed by smaller ones for 'fine-tuning', if allowed enough time and stability 6 . This fits well with Fisher's model. With each step towards the centre, the sphere representing an organism's adaptive space shrinks, limiting the potential pay-off from the next mutation 4 . There are exceptions to this 'large-early, small-late' rule more recently gathered by the Lenski team. First, some of the earliest mutations provided minuscule fitness gains. Second, some of the largest-effect mutations took tens of thousands of generations to become fixed. One of the earliest mutations to get fixed (less than 2,000 generations), and in all 12 populations, was the deletion of the 'ribose operon', a cluster of genes used in breaking down the sugar ribose. Getting rid of it provides a sluggish (1\u20132%) selective advantage against the ancestor on a glucose-only diet 7 . But there's a reason it gets fixed so early. The ribose operon is a favourite destination for transposons (bits of DNA that cut, copy and paste themselves throughout the genome), and one in every 50 mutations popping up wipes it out. Even though such mutations provide only a tiny advantage, they will almost certainly sweep to fixation. It would take a very fast hare to catch up with an army of early tortoises such as these.  \n                The rise of  \n               Escherichia erlenmeyeri  \n              Another type of rule break has proved to be more surprising. One morning, at the turn of generation 33,127 according to the lab's log book, a massive increase in turbidity was recorded on the vial labelled Ara-3. The sugar-starved bacteria had suddenly 'discovered' a vast new source of carbon by importing citrate, a pH buffer that had been in the growth media all along, and it sent the population size through the roof. This Cit+ phenotype happened in only one of the twelve populations and took more than a decade to show up 8 . It has been compared to the evolution of a new species, jokingly christened  Escherichia erlenmeyeri . \"It must have been like the colonization of land by proto-tetrapods,\" says Lenski, now at Michigan State University in East Lansing. 'Land' (citrate) had been there for quite a while, but it was only after that first breakthrough that new horizons were opened and further changes could evolve. In a Fisher model, this would amount to teleportation to a new adaptive sphere. To find out the mechanism behind the exceptional and extremely slow evolution of the Cit+ innovation, Lenski and his colleagues went back to their frozen fossil record:  E. coli   samples they had been periodically storing in a \u221280 \u00b0C freezer. They then attempted to replay the evolutionary tape. They revived hundreds of specimens, made hundreds of replicate cultures of each and pushed them through thousands more generations. They ended up testing close to 50 trillion individual cells for the ability to grow on citrate without glucose, but cornered only a handful of 're-evolved' Cit+ mutants. All were derived from recent samples. The capacity to exploit citrate could not evolve from the ancestral  E. coli   genome in one step. Instead it required a minimum of three mutation events 8 . Smaller mutations had to set the stage for the later, more dramatic event. A given mutation often seems to depend on modifier mutations \u2014 a phenomenon known as epistasis \u2014 and thus on the organism's history before it can have a large effect. It remains to be seen whether such elementary mechanisms of adaptation, often referred to as microevolution, can instruct the higher processes that constitute macroevolution, such as speciation and the emergence of biodiversity or complex organs. In 1996, Lenski and his colleagues noted that the adaptive jumps in his bacterium's cell size were reminiscent of the abrupt morphological changes punctuating long periods of stasis in the palaeontological record 9  publicized 24 years earlier by palaeontologists Stephen Jay Gould and Niles Eldredge. Lenski meant to show how such seemingly complex punctuated equilibria could arise by the simplest \u2014 and indeed most orthodox \u2014 mechanisms in Darwinian population genetics: mutation and natural selection (wait and sweep). At that time, some evolutionary biologists dismissed the case as irrelevant to Gould and Eldredge's much-disputed theory because, they said, there was neither speciation nor species selection in Lenski's studies. But that was years before the 'new land' of citrate was discovered in vial Ara-3. Ara-3 now essentially had biodiversity. As the new Cit+ clique gorged itself with citrate, it became less interested in glucose, leaving space for a 1% minority of Cit\u2212 glucose experts, which now discretely coexist in the same vial. This is remarkable, Lenski says, because the total absence of sexual reproduction, or any form of horizontal DNA transfer, in his experiment normally leads to a no-prisoners-taken mode of evolution, in which the best genome of the moment forces all others to extinction \u2014 a phenomenon called clonal interference. From the moment they stop competing for the same resources, however, species can coexist, a principle thought to force the tree of life to fan out 10 . \"From so simple a beginning,\" as Darwin envisaged, a single-cell ancestor and its descendants gave rise to their own, microbial version of his beloved \"entangled bank\". Jerry Coyne, an evolutionary biologist at the University of Chicago, Illinois, urges caution. \"Nobody agrees about what speciation means in bacteria,\" he says, and although the concept of adaptation is similar in bacteria and sexual organisms, speciation is a different ball game. Lenski argues that the emergence of the Cit+ variety is about as close as it gets to speciation particularly as the inability to use citrate in the presence of oxygen is a defining feature of natural varieties of  E. coli . But he also emphasizes at least two properties that make it difficult to generalize the dynamics of his bacterium's evolution to more complex organisms: bacteria have relatively high DNA mutation rates and the particular strain he uses is fully asexual.  \n                Of form, function and fitness \n              More complex organisms such as sticklebacks present extremely low rates of new mutation \u2014 with the remarkable exception of the mutation-susceptible  Pitx1   gene \u2014 and so must rely on other mechanisms to produce genetic variation. Some of Kingsley's work illustrates this. Besides the shrinking of pelvic spines, his group has identified individual mutations that have large effects in two more morphological traits that get reduced in freshwater fish: the number of armour plates is chiefly controlled by ectodysplasin (EDA) and skin pigmentation by kit-ligand (KITLG), both key developmental signalling proteins. In each case, the researchers found that a copy of the freshwater 'version', or allele, of the gene is carried by a small percentage of marine sticklebacks. Those carrying it often have an intermediate phenotype. This standing variation is probably maintained by regular mixing of marine sticklebacks with those in coastal streams 11 ,   12 . Each time a population moves between sea and lake, the selective pressure shifts and the allele frequencies respond, with major morphological consequences. This, the researchers found, has happened independently many times, and the very same alleles have been seen to swap throughout the Northern Hemisphere. As marine sticklebacks populated lakes at the end of the last ice age, 10,000\u201320,000 years ago, Kingsley considers this part of a very fast speciation event. But changes at a single QTL don't have to be a single mutation. For both the  Eda   and  Kitlg   genes, the freshwater and marine alleles differ at hundreds of scattered positions. The researchers don't yet know which ones control phenotype. They do know, however, that most of the DNA changes accumulated over the millions of years since sticklebacks first migrated between salt and fresh water. Individual QTLs that may provide large and immediate benefits in the short term when environmental conditions change, could thus have evolved previously \u2014 through the accumulation of myriad mutations of small effects over immense periods of time. To study those mutations, Kingsley is collaborating with scientists at the Broad Institute in Cambridge, Massachusetts, to sequence full stickleback genomes from ten marine and ten freshwater individuals collected around the world. Large effect or small, evolution begins to look like an endless list of special cases, each a new challenge to Fisherian models. One reason is the general lack of knowledge about how changes in genes contribute to function and how this affects fitness. \"Current population-genetics models consider the molecular details of how mutations control phenotype as statistical noise,\" says Paul Rainey, an evolutionary microbiologist at Massey University in Auckland, New Zealand. This is what Thornton hopes will be corrected by a \"functional synthesis\", marrying evolutionary biology, molecular genetics and structural biology. Thornton and his colleagues have begun to look at the evolutionary constraints placed on proteins. They have studied a regulatory protein \u2014 the glucocorticoid receptor \u2014 as the protein evolved from one species of fish to another over millions of years. They combined X-ray crystallography and biochemistry to dissect how the protein's structure constrains its stability and function, namely which hormones it can bind. They found that key function-switching mutations (which swap amino acids directly in contact with the hormone) depend on smaller-effect mutations at distant positions, to determine which route adaptation can actually follow under natural selection \u2014 a mechanism they call an 'epistatic ratchet' 13 . Many researchers have welcomed the return to favour of large-effect mutations and have resurrected Goldschmidt's long reviled idea of the hopeful monster. But they can't ignore the small-effect mutations. \"We need much more data before the issue of large versus small can be settled\", says Coyne. Kingsley, like Coyne favours a middle-ground view, in which neither large- nor small-effect mutations are ruled out. \"Our work has too often been portrayed as saying that Darwin was wrong\" about big leaps in adaptation, he says. But in fact, none of the traits his group has studied is completely due to the effects of a single gene.  Pitx1   accounts for only two-thirds of the variation in pelvic-spine length. Moreover, Kingsley says, \"We've got more than 150 other QTLs to study.\" The difference between large and small effect mutations can be sliced many ways, and it greatly depends on context. A mutation may affect phenotype but not change fitness much. It may have a large effect in the context of a given genome, or in a given environment, but may have a smaller effect later in an organism's history. As researchers drill down to the molecular mechanisms driving adaptation, theory may catch up and dogmas may recede. Ultimately, says Kingsley, \"we should let the bacteria, the fish and other organisms speak\". Tanguy Chouard is a senior editor for  Nature . \n                     Darwin 200 News Special \n                   \n                     Evo\u2013devo: extending the evolutionary synthesis \n                   \n                     The Kingsley Lab \n                   \n                     The Peichel Lab \n                   \n                     The Lenski Lab \n                   \n                     The Thornton Lab \n                   Reprints and Permissions"},
{"file_id": "4631014a", "url": "https://www.nature.com/articles/4631014a", "year": 2010, "authors": [{"name": "Olive Heffernan"}], "parsed_as_year": "2006_or_before", "body": "A new generation of sophisticated Earth models is gearing up for its first major test. But added complexity may lead to greater uncertainty about the future climate, finds Olive Heffernan. The government building in the south of England looks open and airy with its three-storey glass facade. But security measures such as guards stationed at the front serve as a reminder that this Ministry of Defence property is carrying out sensitive work important to the nation's future. Deep in the building's basement, two adjoining rooms house 27 big black boxes that churn through a million lines of computer code every hour to gaze into the future of Earth and its 7 billion inhabitants. This massive supercomputer at the UK Met Office in Exeter is home to what is possibly the world's most sophisticated climate model. Developed by researchers at the Hadley Centre, the Met Office's climate-change branch, the newly finished model will be put to its first big test over the coming months. It will run a series of climate simulations out to the year 2100 for the next report of the Intergovernmental Panel on Climate Change (IPCC), on the physical-science basis of climate change, which is due out in 2013. Four years in the making, the model is known as HadGEM2-ES, short for the Hadley Centre Global Environmental Model, version two, with an added Earth-system component. It is one of a dozen Earth-system models under development worldwide that reach far beyond their distant forebears, which represented just the physical elements of the climate, such as air, sunlight and water. The new generation includes all that and much more: forests that can shrink or spread as conditions change; marine food webs that react as the oceans grow more acidic with carbon dioxide; aerosol particles in the atmosphere that interact with greenhouse gases, enhancing or sapping their warming power. The Hadley Centre is at the forefront of efforts around the world to develop such complex climate models. \"It's really pushing the envelope\", says Andrew Weaver, a climate modeller at the University of Victoria in British Columbia, Canada. Researchers hope that the added complexity will lead to more realistic predictions and help them to derive new insights about how elements of the climate interact with each other. But it is a bit of a gamble whether this monumental effort will actually help political leaders and researchers to plan for the future. Because models are simulating more components and each one is subject to variation, the extra complexity could make error bars expand greatly in forecasts of how temperatures or precipitation will change with time. \"It's very likely that the generation of models that will be assessed for the next IPCC report will have a wider spread of possible climate outcomes as we move into the future,\" says Jim Hurrell, who is heading development of the Earth-system model at the National Center for Atmospheric Research (NCAR) in Boulder, Colorado. At its core, HadGEM2-ES is much like any climate model. It is a series of equations describing atmospheric circulation and thermodynamics, solved for a number of points that form a three-dimensional grid over the surface of Earth. The grid for HadGEM2-ES has 192 evenly spaced points running east\u2013west and 144 points in the north\u2013south direction. Each box represents an area of roughly one degree latitude by two degrees longitude, which is about 100 kilometres by 200 kilometres at the equator. The atmosphere has 38 unequally divided levels, about 20 metres deep near the land and ocean surface but getting progressively thicker with height.  \n                From slime to leaf \n              As with all modern global climate models, HadGEM2-ES couples the atmospheric model with an ocean model. In the 1980s and early 1990s, the oceans in the earliest of the Hadley coupled models were called swamps and were little more than large, shallow puddles connecting continents. They gradually evolved into 'slab oceans', which were rigid but could absorb and release heat (see  'Model evolution' ). The global ocean in HadGEM2-ES, by contrast, flows with currents and eddies. Its 40 layers reach down to 5,000 metres, with the top layer measuring just 10 metres thick, an important advance that allows models to simulate more realistically how the ocean takes up and retains carbon dioxide at different depths. Amid the stark white walls and uniform desks of the Hadley Centre, researchers are wrestling with how to mimic the clouds and currents, trees and tundra and the myriad other aspects of the planet that can amplify or diminish global warming. Yet the only signs of the work that is done here are the lines of computer code scrolling across the computer monitors. The coding that makes up HadGEM2-ES includes several other components of the Earth-system, such as a closed carbon cycle, which in theory can account for all of the carbon on Earth that might affect climate over the next few centuries. The model does this with detailed atmospheric chemistry, dynamic vegetation that can grow and die depending on the climate, and ocean biological and geochemical systems that can respond independently to changing greenhouse-gas levels. On land, vegetation is split into nine categories from shrubs to needle-leaf trees \u2014 a major advance in how models represent the biosphere. The first attempts to include biology in a climate model gave rise to the 'green scum' model, which treated all of life as a uniform green layer from equator to pole. HadGEM1, the immediate forerunner of HadGEM2-ES, included the same types of vegetation as HadGEM2-ES but as a static layer. Now, the vegetation types can shift, and foliage can change with the seasonal cycle. Merging these aspects together in a single model slows it considerably. Simulating one month takes an hour of computing time. \"That's the price you pay,\" says Chris Jones, a carbon-cycle specialist who has been instrumental in developing HadGEM2-ES. HadGEM1 runs three times faster than the newer model. Even with the fine resolution of HadGEM2-ES, there are climatically important processes that are simply too small for the model to simulate directly, such as clouds. To get around this, climate modellers use a technique called parameterization. Rather than trying to simulate individual clouds within a grid box, for example, the generalized effect of clouds on climate is represented by a series of equations that describe the conditions under which clouds form and decay, and how they absorb or reflect radiation. These are then averaged for each grid box. Parameterizations are a necessary evil that can introduce error into models. Perhaps more bothersome, though, are the random errors. Just one incorrectly entered bit of data can send the whole system haywire. So too can an imperfect representation of a seemingly simple aspect of the real world, as Hadley Centre scientists discovered when they put too little plant life in the world's arid regions, leaving insufficient vegetation to hold the soil in place. Soon, dust levels in the model atmosphere surged out of control, reaching three times higher than average, which in turn fertilized the ocean, causing phytoplankton to flourish.  \n                Identifying culprits \n              Model developers are only too familiar with the effects of errors. \"That's not only common, that's the evolution that all of us go through,\" says Hurrell. To identify these quirks, each model goes through a rigorous testing phase. This involves putting the model through a series of 'control runs' in which its skill is tested by simulating a stable climate. The model is also commonly 'hindcast' to determine its ability to match historical changes. That's what HadGEM2-ES is currently doing and will finish up next month. \"Sometimes, you can get completely unrealistic results,\" says Jones. When the results don't correlate with reality, modellers use their knowledge of the climate system to try to identify the likely culprits within the model and then correct them. And occasionally what starts as a problem can lead to insights. The unrealistic dust storms in HadGEM2-ES, for example, revealed that even a small amount of plant life in the world's deserts can influence the whole climate. The testing phase usually gets the model to a point at which it can simulate past and current climate well, but often certain aspects continue to cause problems. In fact, every model has a weak spot. When the Hadley Centre scientists developed HadGEM1, the model had difficulty simulating the El Ni\u00f1o Southern Oscillation, a phenomenon that drives much of the climate variability across the Pacific Ocean. \"This led many to harp on that the Hadley Centre had lost its footing at the lead of the pack,\" says Bill Collins, the scientist responsible for the HadGEM2-ES model development. But the model used by NCAR scientists had an equally bad problem with El Ni\u00f1o. And Ronald Stouffer, a climate researcher at the Geophysical Fluid Dynamics Laboratory (GDFL) in Princeton, New Jersey, says the GDFL model was \"the worst in the world\" for simulating North Pacific sea ice in the last IPCC assessment. But when it came to reproducing past and current climates on a global scale, these models were on top of their game. \"That's one of the joys of this business,\" Stouffer says with a laugh. \"You can make your model better overall but at any one point it might be worse.\" Whether or not HadGEM2-ES has a blind spot should become apparent after running the simulations for the IPCC over the next few months. Whatever flaws there are in HadGEM2-ES may be compensated for by its peers \u2014 the other Earth-system models that will be running simulations for the IPCC forecasts. These models hail from the research groups at the GFDL, NCAR and from centres in Australia, Canada, China, France, Germany, Japan and Norway. \"It is important that there's a good diversity of models,\" says Weaver, because, when it comes to ways of modelling, \"there's no one right answer\". The advantage of having numerous groups developing models simultaneously is that they each approach the task differently. Other institutes have not put as much work into their Earth-system models as the Hadley Centre, so most of them are not relying entirely on the new models. \"Every one of these simulations is a challenge and we don't know until the last moment whether everything will work out as we would like,\" says Christian Reick, who is leading vegetation modelling for the Max Planck Institute for Meteorology in Hamburg, Germany. Many groups will run a hierarchy of simpler models as a way of hedging their bets. If the Earth-system models produce forecasts of, say, temperature change that have an unreasonably large uncertainty range, the teams can fall back on more familiar models for a simpler evaluation of what the future holds. That concern has led some to call for caution in increasing the complexity of models. \"Once in a while you have to stop and think if this complexity is really warranted in light of our ignorance,\" says Syukuro Manabe, one of the founders of modern climate models and a researcher at the GFDL. \"You don't want to oversimplify,\" says Manabe, adding that the key is to look at whether \"the detail in the model is balanced with our knowledge of the process\".  \n                Practical constraints \n              Although the new models are better from an academic perspective, they do not necessarily produce results that are more useful for policy-makers wrestling with how to plan for the future. Jones acknowledges that models are \"only useful if we understand them to the point where we have confidence in them\". But, he says, \"we're going into a future that we can't constrain with observations, and climate models are the only tool we have for making projections\". During the report-writing process, the IPCC authors will analyse all the model projections to develop their best estimates for the climate of the future. The estimates will include how quickly temperatures will rise, where rainfall will increase or decrease and how vegetation patterns will shift. But even as they run through the IPCC simulations with the current crop of models (see  'The range of future climates' ), Jones and his peers are thinking about how to improve them by adding more components. \"We're keen that we don't stand still,\" he says. One addition they hope to make is the thawing of permafrost in the Arctic tundra that could accelerate warming by releasing methane, a potent greenhouse gas. HadGEM2-ES has factored in methane release from wetlands, but doesn't account for permafrost. Also in the pipeline for the Hadley Centre's next model is the nitrogen cycle and how it influences plant growth. But the point of all of this \"is not to develop a perfect model\", says Hurrell. \"We develop new representations of aspects of the system based on our best understanding of the system, and when we are confident that we can represent that process well, we include it in the model.\" For climatologists, models are not just tools that can give a glimpse of what the future holds; they are also an experimental playground \u2014 a replica world on which they can test their knowledge of the climate system. Without the ability to conduct global-scale experiments in the lab or in the field, models are the only tool they have. At the Hadley Centre, some staff joke that modellers will eventually try to include panda bears in their simulations. The bit of hyperbole no doubt riles some of the modellers, perhaps because they are only too keenly aware that their creations can never fully represent the real thing. \"Part of our intellectual challenge is to find out what's important to include,\" says Jones. \"There'll always be some level of detail or complexity that we can't get in there.\"  Olive Heffernan is the editor of  Nature Reports Climate Change . \n                     Nature reports climate change \n                   \n                     Nature Geoscience \n                   \n                     Hadley Centre \n                   \n                     Geophysical Fluid Dynamics Laboratory \n                   \n                     National Center for Atmospheric Research \n                   Reprints and Permissions"},
{"file_id": "464026a", "url": "https://www.nature.com/articles/464026a", "year": 2010, "authors": [{"name": "Sharon Weinberger"}], "parsed_as_year": "2006_or_before", "body": "Georgia's borders are guarded by some of the best radiation detectors available \u2014 so why are nuclear smugglers still slipping through? Sharon Weinberger reports. The radiation alarm started wailing and flashing its red strobe light on an otherwise ordinary day last August as a car tried to pass through the Sadakhlo border crossing. The alarm itself wasn't unusual. This remote outpost linking the former Soviet republics of Georgia and Armenia scans hundreds of cars, buses and trucks every day, and the radiation monitors, housed in their distinctive white casings, are regularly triggered by the natural radioactivity in ceramic tiles, quarry stones \u2014 even bananas. But the levels of \u03b3 radiation in this car, and on its Armenian driver, were so high that the checkpoint police were compelled to call in the Georgian government's special nuclear investigators in the capital city of Tbilisi, about two hours drive away. Once there, the specialists determined that the car was heavily contaminated with caesium-137, a radioisotope often used for cancer therapy and food irradiation \u2014 and often cited as an ideal source for a radiological dispersal device, better known as a 'dirty bomb'. But they couldn't find any caesium-137, or proof that the driver was deliberately transporting anything dangerous. So the officials found themselves at an impasse: what should they do? Such conundrums are also not unusual in this part of the world. From a technological perspective, the Sadakhlo crossing is about as advanced as it gets \u2014 a showcase for international efforts to stop the illegal trafficking of nuclear materials. The Georgian patrol police who staff the station can check passports against a centralized database, capture video recordings of every person crossing the border and rely on the radiation monitors to watch for telltale \u03b3-rays and neutrons. Sadakhlo is just one of the many such modern border crossing facilities that can be found in Georgia and much of the rest of eastern Europe, as well as in an increasing number of ports worldwide. The hardware is provided through a US anti-proliferation programme known as the Second Line of Defense \u2014 the first line being the global effort to secure nuclear materials at their source. Run by the US Department of Energy's National Nuclear Security Administration, the programme's long-term goal is to protect more than 650 sites in 32 countries. And US President Barack Obama, who has made non-proliferation a cornerstone of his foreign policy, submitted a 2011 budget request that would continue this focus, doubling the annual funding for Second Line of Defense to more than half a billion dollars by 2015. Given the scale of that effort, as well as funding from the European Union for training, it is sobering to realize how porous the defence is. The radiation monitors have had some success at intercepting radiological sources, as in the August incident at Sadakhlo. But they have had just one claimed success at intercepting fissionable material suitable for use in a nuclear weapon \u2014 again, at Sadakhlo. At the time, the crossing wasn't as high-tech and the detectors were an earlier model than the ones now in place. But on 26 June 2003, the border guards closed in on an Armenian citizen named Garik Dadayan, who was found to be carrying 170 grams of highly enriched uranium (HEU). Although this is far too little for a nuclear weapon \u2014 that would require more than 10 kilograms \u2014 the case did confirm the existence of a black market in fissile materials. It also, according to US officials, validated the use of the detectors. Others are not so sure. \"Border policemen said the detector worked; other police officers say [the uranium] was not detected,\" says Alexandre Kukhianidze, who directs the Caucasus Office of the Transnational Crime and Corruption Center in Tbilisi. But one thing is clear: technological solutions such as radiation monitors are far from a panacea for nuclear smuggling. And Georgia is an excellent case study for why that is.  \n                Security crackdown \n              Smuggling and bribery were once a way of life in Georgia, which lies on a key transit route between Russia and Turkey. And this was especially true in the post-Soviet era, when the chaotic regime of former Georgian President Eduard Shevardnadze left the police chronically underfunded and susceptible to bribery. But after Shevardnadze was ousted in the Rose Revolution of November 2003, the government that replaced him instituted sweeping reforms, strengthened Georgia's already close ties to the West, and expanded its cooperation with Western nuclear security efforts. Today, Georgia is widely considered a model citizen in the non-proliferation world. Certainly the officials there swear by their radiation detectors. Trying to track down non-weapons-grade radiological materials without them would be like operating \"without hands or legs\", says Alexander Okitashvili, deputy director of the Georgian patrol police, who watches videos of the borders in real time from his desk in Tbilisi. And both US and Georgian officials agree that radiological materials are by far the biggest nuclear smuggling threat, accounting for 321 of the 336 confirmed cases of illicit nuclear trafficking reported to the International Atomic Energy Agency in Vienna between January 1993 and December 2008. Only 15 involved weapons-grade HEU or plutonium. Still, the monitors have many pitfalls. Their propensity for false alarms is one. And another, paradoxically, is that the potentially deadliest materials \u2014 fissile HEU and plutonium \u2014 are not strongly radioactive. One obvious upgrade was to add the ability to detect neutrons produced by the spontaneous fission reactions that occur in any bomb-grade material. The original devices, which were installed by the US State Department in Georgia and other parts of the world during the 1990s, could detect only \u03b3-rays. The upgraded portal monitors were installed last year and can now be found at every border crossing, port and airport in Georgia. Another refinement, currently being pursued by physicists at the Pacific Northwest National Laboratory in Richland, Washington, is to equip the detectors with 'energy windowing' algorithms that would give an approximate measure of the energy of the \u03b3 rays. Each element has a unique \u03b3-ray spectrum, so this would help the detectors to distinguish between real threats and naturally occurring radioactive materials. Researchers also are trying to improve neutron detection by looking at how neutrons are emitted by materials exposed to cosmic rays. However, none of these technological efforts gets at the much more serious, non-technical problems with relying on the detectors, such as the fact that they are installed only at official checkpoints. Georgia hasn't installed detectors at crossings into the Russian-backed breakaway regions of Abkhazia and South Ossetia (see map), because to do so would be a tacit recognition of the regions' independence. Nor can Georgia control what passes across the borders between Russia and these regions, which have long been regarded as smuggling havens. Given those realities, critics charge that detectors are effective only against nuclear smugglers who are too stupid or lazy to go around the checkpoints, or too incompetent to shield the material they take through. \"You will be able to catch the Richard Reids, but not the Mohamed Attas,\" says nuclear physicist Thomas Cochran of the Natural Resources Defense Council in New York, comparing the 'shoe bomber' with the lead hijacker in the 11 September 2001 attacks.  \n                Ambitious amateurs \n              But there are plenty of rank amateurs in this game, says Archil Pavlenishvili, Georgia's chief nuclear investigator. A law-enforcement official with a chemical-engineering degree, Pavlenishvili heads a small government team created in 2004 to investigate nuclear incidents. He says that would-be smugglers are forever trying to scam one another by hawking 'red mercury', a fictional substance supposedly used for nuclear weapons. \"In 2006, we had a case when a Turkish citizen tried to smuggle real caesium-137,\" he says. \"He placed the caesium inside red liquid and tried to sell it as red mercury; it has very strong radiation.\" But for every bumbler there are unknown numbers of smart operatives whom the radiation detectors never see, says Pavlenishvili, who says that his team captures two to three smuggling rings every year. For them, he says, the answer is solid, low-tech police work. In 2005, for example, a tip from an informant led him and his team to mount a sting operation. They caught Oleg Khintsagov, a Russian citizen, in a suburb of Tbilisi trying to sell 100 grams of HEU to a man he believed was a Turkish go-between, but who was in fact a Georgian operative. When arrested, Khintsagov claimed that the blue-grey substance was printer powder. In fact it was military weapons-grade uranium enriched to 89% of the fissile isotope, uranium-235. Even more worrisome for nuclear security specialists is another unknown, especially when it comes to radiological smuggling. \"There's a market and something is driving that market,\" said a US law enforcement official involved in the issue, who agreed to speak only on background. \"We need to know why people are so interested in obtaining the material.\" All the detectors in the world won't answer that question. Nor will captured smugglers, as a rule, who typically deal with intermediates, and may not even know the final customer. Dadayan, for example, provided little useful information, and was soon turned over to Armenian authorities. And Khintsagov is still in a Georgian prison, refusing to talk. Not even the Armenian citizen caught last August yielded any definitive answers. \"The driver told an unbelievable story,\" says Pavlenishvili. \"He worked in a radio station, and said maybe because he was dealing with specific electronic equipment, his car and clothes were contaminated. It's not true, of course.\" But unable to find the source in the car, says Pavlenishvili, we had \"no legal possibility to arrest him\". Instead, the driver was simply let go. Sharon Weinberger is a freelance reporter in Washington, DC. Her trip to Georgia was funded by the International Reporting Project in Washington DC. \n                     Nature Nuclear Proliferation Special \n                   \n                     Second Line of Defense programme \n                   \n                     International Atomic Energy Agency's Illicit Trafficking Database \n                   Reprints and Permissions"},
{"file_id": "464158a", "url": "https://www.nature.com/articles/464158a", "year": 2010, "authors": [{"name": "Katharine Sanderson"}], "parsed_as_year": "2006_or_before", "body": "Chemists looking to create complex self-assembling nanostructures are turning to DNA. Katharine Sanderson looks at the science beneath the fold. DNA is the kind of polymer that chemists dream about. Because its complementary sequences can bind to one another, individual molecules of the right sequence will assemble all by themselves into intricate shapes and structures at the nanoscale. DNA can weave together and bind other molecules, allowing it to serve as a scaffold for complex nanomachinery. DNA nanoengineering is dreamy, but difficult. Researchers have been putting together carefully chosen segments of DNA to form sheets, tubes, even simple machines such as tweezers since the early 1980s. But back then, designing these structures could take months to years. And because researchers were focused on designing them from scratch, they could use only the short segments, no more than 150-base-pairs long, that DNA synthesizers could manufacture. This in turn constrained the size and complexity of the designs. \"The problem is that we don't just want to make small stuff, we want to make complicated small stuff, cheaply and easily,\" says Paul Rothemund, a computational bioengineer at the California Institute of Technology in Pasadena. Rothemund wondered whether he could create the complicated stuff using a longer, naturally occurring piece of DNA, such as the genome of a virus, and folding it over on itself. So in 2004 and 2005 he spent months, he says, programming in his underpants, trying to work out a way to bend a 7,000-base-pair viral genome to his will. In his design he visualized how the genome could be folded into a predetermined, two-dimensional shape. Knowing the sequence of the virus at every twist and turn, he was able to write complementary DNA sequences, about 16-base-pairs long, that would essentially staple the folds in place. He ordered the 'staples' from a DNA-synthesis company, mixed them with his virus in a buffer that stabilized the DNA and then heated and cooled the mixture, allowing the single stranded viral DNA to bind with the staples (see ' Stapling a smiley '). The result, viewed using atomic-force microscopy, was the smiley face and several other shapes, created by what he called DNA origami 1 . The ease of DNA origami was a breakthrough, dispensing with the intricacies of precise DNA engineering and other metamaterials development. \"It's like being able to bake a cake and not pay attention to the ingredient ratios,\" says Rothemund. But with the right ingredients complex structures can be built with the kind of precision that many people have been looking for. Origami scaffolds, sheets or bricks of folded DNA, are packed with known sequences that could be used to position DNA-binding molecules just a few nanometres apart. And the new, larger structures can contain upwards of 200 sites for affixing such molecules, compared with only a handful on pre-origami structures. This type of precision engineering could be a boon to nanoengineers wanting to position components on nanoelectronic circuits or for bioengineers looking to place proteins in close, accurate proximity to one another. Now the challenge is to go beyond the novelty of Rothemund's smileys and a dozen or so other demonstration patterns and build structures with a practical purpose. Here's what several researchers are dreaming of doing.  \n                Make a ruler \n              Rothemund's technique was a door opener for Friedrich Simmel, a biophysicist at the Technical University of Munich in Germany. Suddenly, Simmel says, he was able to have even \"rather sloppy\" physics students making DNA structures with ease. Simmel has used DNA origami to make a ruler to measure distances between single molecules and calibrate super-high resolution microscopes. Simmel designed a DNA origami rectangle measuring 100 nm by 70 nm and included some staple strands labelled with fluorescent dye molecules. When the DNA folds, two labelled staples sit at opposite ends of the rectangle in precise locations. This ruler can be used to calibrate high-tech microscopes that can resolve objects smaller than the diffraction limit of light \u2014 roughly 200 nm (ref.  2 ). The kinds of molecules generally used for calibrating such scopes, such as loose pieces of DNA or filamentous proteins, are not ideal because they are flexible and their dimensions can change. Simmel wants to use the ruler and related structures to help track the movement of molecular motors. Because fluorescent tags allow them to use light microscopy, as opposed to atomic-force microscopy or electron microscopy, researchers will be able to view molecular processes as they happen. That's important, Rothemund says, once scientists begin coupling proteins to DNA origami. Rulers like Simmel's will be useful to watch how those proteins behave.  \n                Build an artificial leaf \n              DNA origami could allow researchers to put biomolecules together according to specification. A grail of sorts for many engineers working at the nanoscale is photosystem II, a complex of more than 20 protein subunits and accessories that helps to split water into hydrogen ions and oxygen during photosynthesis. Attempts to recreate photosystem II, or even some of the catalysts involved in electron transport, have been disheartening. DNA origami could provide the scaffold to hold proteins in place, says Hao Yan, a biochemist at Arizona State University in Tempe. As part of a collaboration funded by the US Department of Energy at the university's Center for Bio-Inspired Solar Fuel Production, he has been looking to use DNA origami as the basis for an artificial leaf that makes hydrogen fuel from water. Yan plans to use a cage-like DNA structure to position the proteins and to bind manganese, a crucial component of the water-oxidation process. The goal is to better direct electron flow in an artificial system that requires precise placement of components. \"If we can really control all the electron-transfer sites then we can improve the efficiency,\" says Yan.  \n                Put a drug in a box \n              Last year, Kurt Gothelf, director of the Centre for DNA Nanotechnology at Aarhus University in Denmark, and his colleagues, reported that they had made a box from DNA origami 3 . One strand of DNA holds the lid shut; a separate DNA 'key' springs it open. The invention prompted many to wonder what could be put into the box and subsequently released. William Shih, a DNA nanotechnologist at Harvard Medical School in Boston, Massachusetts, is keen to exploit this kind of vessel for drug delivery, but the challenges are significant. Getting the box to pass through a cell membrane is going to be difficult, he says. He proposes covering the box with a membrane similar to those sported by some enveloped viruses. These viruses also have special proteins to facilitate entry. Shih says that he can take viral proteins or related proteins and fix them to the outside of his DNA cages, but he faces a long list of challenges. \"There's a lot of stuff,\" Shih says. \"At this point it's just a concept.\"  \n                Go for gold \n              Many have talked about using DNA origami as a substrate for nanoelectronic circuitry, such as in plasmonic devices. Plasmonic devices couple light waves with charges on a metal surface and offer the speed of information transfer that light provides, but at sizes smaller than those to which technologies such as fibre optics are limited. Current lithographic techniques run into trouble when trying to arrange metallic materials such as gold into patterns with features smaller than about 100 nm in size. Rothemund says that gold spheres might be positioned using DNA origami to make structures with better optical qualities. He has already taken steps, in concert with IBM Almaden in San Jose, California, towards arranging metal nanoparticles. Last year they managed to arrange DNA triangles on a lithographically patterned surface 4 . Jennifer Cha from IBM and her colleagues subsequently showed that they could place gold nanoparticles smaller than 10 nm onto each origami structure by affixing strands of DNA to the gold that were complementary to loose ends on the DNA triangles 5 . The work is still quite crude, says Rothemund. They haven't got an active device yet. \"Much of the next five years will be spent perfecting techniques to place DNA origami on surfaces where we want them and making this technique widely available to other scientists,\" he says. Importantly, they've tacked down the shapes rather than having them \"bobbing around like jellyfish in solution\", says Rothemund. \"I didn't think anyone would solve that problem in ten years. Now we've got DNA origami lined up like little ducks in a row.\"  \n                Pushing past smileys \n              The grand schemes go on. Researchers imagine an artificial ribosome capable of building custom enzymes, a matrix for supporting artificial organs, or a DNA origami network designed to support a neuronal network connected to electrical circuitry. Researchers are attempting all of these, but they will face some serious hurdles. Buffer solutions must be finely tuned, otherwise structures fall apart. And large, complicated structures can take up to a week to fold completely. Change is needed to the basic, almost slapdash synthesis approach of using unpurified DNA that isn't adequately sequenced, says Shih, \"just because we got away with it so far doesn't mean we want to get away with it forever\". But as long as this growing group of researchers keeps trying to make different shapes, other applications will appear. Rothemund can't predict quite how, though. \"The problem is that you can do all these little cool things but they in no way form a whole system. It's not exactly clear what kinds of systems we're going to be able to build.\" He's working hard to find out. \"In the past six months, I've mostly been back at home, coding in my underwear again, which hopefully means good things are happening.\" \n                     Nature Nanotechnology \n                   \n                     Paul Rothemund \n                   \n                     William Shih \n                   \n                     Center for Bio-Inspired Solar Fuel Production \n                   Reprints and Permissions"},
{"file_id": "464347a", "url": "https://www.nature.com/articles/464347a", "year": 2010, "authors": [{"name": "Natasha Gilbert"}], "parsed_as_year": "2006_or_before", "body": "It may be the gold standard of forensic science, but questions are now being raised about DNA identification from ever-smaller human traces. Natasha Gilbert asks how low can you go? When Peter Hoe was found stabbed to death in his home in North Yorkshire, UK, in the afternoon of 13 October 2006, investigators were able to connect the murder to brothers Terence and David Reed on the basis of a small amount of DNA lifted from shards of plastic found near the body. The men were convicted the next year. But an appeal to the ruling heard in 2009 raised questions about the reliability and interpretation of DNA profiles drawn from very small amounts of genetic material, a technique known as low-copy-number analysis. In the appeal, the Reeds' lawyers argued that Valerie Tomlinson, an officer involved in the analysis at the Forensic Science Service (FSS) based in Birmingham, UK, had overstepped her bounds by speculating how the men's DNA came to be on the pieces of plastic \u2014 thought to have broken off two knife handles. The appeal failed last December, but a larger question looms about how suspects can be fingered from such a small amount of DNA. The case is one of the most recent public airings of a highly charged debate in the science and law-enforcement communities about low-copy-number analysis. Some argue that profiles are not reproducible, are prone to contamination and lack a scientifically validated means for deciding on their accuracy. Moreover, the methods and procedures employed are shrouded in secrecy \u2014 leading some forensics researchers to demand that the interpretation of such profiles, if not the practice itself, be re-evaluated. Low-copy-number analysis is accepted in just a handful of countries including Britain and New Zealand, but it is being applied in many cases. The FSS says that it has used the technique in more than 21,000 serious criminal cases since its development in the late 1990s. Recent appeals affirming the validity of low-copy-number typing suggest that law enforcement may increasingly embrace the technique. Some suggest that its wider acceptance could threaten DNA profiling's 20-year reputation as the gold standard of forensic science. Contrary to the public's perception, \"DNA evidence is not flawless\", says Peter Neufeld, co-director of the Innocence Project, an advocacy group in New York that has campaigned for broader access to DNA evidence to overturn wrongful convictions. British geneticist Alec Jeffreys developed DNA profiling in the 1980s. Profiles are drawn from short, repeating sequences of DNA scattered throughout the genome, called short tandem repeats (STRs). Because the number of repeats varies widely from person to person, the length of these STRs is also highly variable, meaning that by measuring several STRs \u2014 between 10 and 17 \u2014 forensic scientists can declare with calculable probability whether DNA left at a crime scene belongs to a suspect.  \n                A soup\u00e7on of cells \n              Scientists employ the polymerase chain reaction (PCR) to amplify the STRs to detectable levels. The copies are then separated according to size by electrophoresis. The different-sized STRs show up as a pattern of peaks on an electropherogram that can be compared against a database or an individual. Standard DNA analysis requires about 200 picograms of DNA, 33 cells' worth, or twice that for haploid sperm cells. Technicians can generally get more than enough DNA from visible samples such as blood or semen. But to produce profiles from just a few cells, scientists have developed ways of increasing the sensitivity of the analysis, including running more PCR cycles to copy more DNA or purifying the sample after PCR to remove unused reagents. In many cases the amount of starting material for low-copy-number analysis is unclear. The quantitation methods, also based on PCR, can suggest that no DNA is present, but the technicians will still be able to produce at least a partial profile. This sensitivity, although remarkable, has a downside. During analysis of any sample, large or small, random fluctuations occur that distort the results. STRs present in the original sample might not show up \u2014 a 'drop-out' effect. In addition, profiles can show STRs that are not present in samples. This 'drop-in' effect can be caused by contamination. These stochastic effects are easily spotted and ruled out from standard analysis because there is enough DNA to run multiple controls, and generally the strong signals from true STRs can be distinguished from the faint peaks of drop-ins. But when pushing the signal as far as it will go, even faint, artefact peaks are amplified, making them look more acceptable. Bruce Budowle, a forensic and investigative geneticist at the University of North Texas in Fort Worth and a former scientist at the Federal Bureau of Investigation (FBI), gave evidence criticizing low-copy-number analysis at the Reeds' appeal trial. He argues that the methods for determining whether a signal is true in low-copy-number analysis haven't been validated. In low-copy-number profiling, forensic scientists generally split their limited amount of DNA into two or three samples and run analyses on two of them. The third, if available, is reserved for the defence. The results of analyses aren't completely reproducible, profiles often won't match and the scientists generally accept as true those STR signals that show up in both runs. The practice is worrying, says Budowle: \"The logic of this approach makes some sense, but the confidence in it has not been assessed.\"  \n                Inherent bias? \n              Common forensics practices can also lead to biasing errors, says Dan Krane, a molecular evolutionist at Wright State University in Dayton, Ohio. \"Forensic scientists tell me that it is easier for them to distinguish between noise and what is really coming from the DNA if they have a reference sample to work with,\" he says. Low copy number or not, that reference sample is often the suspect's DNA \u2014 and faint peaks in a crime-scene sample might seem more convincing when viewed side by side with a strong peak from the suspect. Only a couple of labs in the United States, he says, require blind testing in their protocols. Critics' fears are confounded by an unwillingness of the labs that use the technique to reveal their guidelines for interpreting results. Labs should be forced to disclose details, says Budowle. Given the technique's reproducibility problems, he argues it is imperative that these protocols are robust and reliable. But \"none of the labs disclose what they do. They say it is proprietary information,\" he says. The FSS did not respond to several interview requests, but Peter Gill, a forensic scientist at the University of Strathclyde in Glasgow, UK, who developed low-copy-number typing in a former post at the FSS 1 , says that the quality of the science \"is not in question\". Although drop-in and drop-out effects are more pronounced than in standard analysis because of the sensitivity of the technique, operating in clean conditions and monitoring negative controls should avoid serious issues. The effects of drop-in and drop-out can also be accommodated statistically, reducing the reliance on human judgement, he says. But the probability theory that deals with these stochastic effects is not yet in use in forensic circles. He says that statisticians have not had the funding to adapt the existing theory to forensic use. \"It is a disappointment that tools to enable better interpretation have not kept pace with developments in forensic science over the past ten years,\" he says. All these concerns have led some countries to approach the use of low-copy-number analysis with caution. In 2001, the FBI recommended that it be used only in certain circumstances in the United States, such as for identifying the bones of someone suspected of being a missing person. Its use has been controversial in Britain, too. Sean Hoey was accused of killing 29 people in an explosion that destroyed the town centre of Omagh in Northern Ireland in 1998. Central to the case against him was a low-copy-number analysis of DNA left on bomb parts connected to the attack. Hoey was freed on appeal in 2007. The judge presiding over the case expressed \"concern about the present state of the validation of the science and methodology\", and its use was suspended in British courts. But a UK review of the technique published in 2008 concluded that the method is \"robust\" and \"fit for purpose\", and its use was reinstated 2 . In March 2009, a judge in California ruled that, due to a lack of scientific acceptance of the procedures and statistics used to interpret low-copy-number results, such evidence was inadmissible. More recently, however, a judge in New York's supreme court denied a motion to exclude low-copy-number DNA evidence in a murder trial, ruling that the technique is reliable. The courtroom is not the best place for the science of low-copy-number analysis to be debated, says Allan Jamieson, director of the Forensic Institute in Glasgow, which provides forensic-science services to UK police forces. Jamieson, who gave evidence in the Reed brothers' and the Omagh bombing trials, says that the forensics community must validate procedures and further investigate the issues that low-copy-number profiling has brought to light, before scientists and courtrooms can have confidence in the results. \"The public does not understand that just because your DNA is on an object it does not mean you have touched it,\" he says. A point of contention in the Reeds' case was whether the Reeds had ever come in direct contact with the plastic knife handles, or whether they might have transferred DNA indirectly through someone else's touch, or, say, by sneezing. As Budowle says, \"with [low-copy-number typing] you can't say what the tissue source is\". Scientists, he says, \"overstep the line\" when interpreting the results of low-copy-number typing in court cases. But Gill disagrees. \"Scientists are there to explain what the evidence means,\" he says. \"It is their responsibility to explain all the possibilities of how the DNA got there and then it is up to the court to decide.\" The arguments are likely to continue.\n   See Editorial,  \n                     page 325; \n                    News Features, pages  \n                     340 \n                    and  \n                     344; \n                    Opinion,  \n                     page 351; \n                    and online at  \n                     http://www.nature.com/scienceincourt \n                   . \n                     Appeal in the case of David and Terence Reed \n                   \n                     The Forensic Science Service \n                   \n                     FBI guidelines for DNA testing labs \n                   Reprints and Permissions"},
{"file_id": "464482a", "url": "https://www.nature.com/articles/464482a", "year": 2010, "authors": [{"name": "Zeeya Merali"}], "parsed_as_year": "2006_or_before", "body": "Social scientists have embedded themselves at CERN to study the world's biggest research collaboration. Zeeya Merali reports on a 10,000-person physics project. \"I am here to watch you.\" So began anthropologist Arpita Roy when introducing herself in 2007 to a roomful of particle physicists. At the time, those scientists were racing to finish work on the world's biggest machine, the Large Hadron Collider (LHC) at CERN, Europe's high-energy physics laboratory near Geneva, Switzerland. The LHC carries the hopes of generations of physicists, who have designed it to reach energies never before achieved in a collider and \u2014 possibly \u2014 to produce a zoo of particles new to science. But the LHC is also a huge human experiment, bringing together an unprecedented number of scientists. So in recent years, sociologists, anthropologists, historians and philosophers have been visiting CERN to see just how these densely packed physicists collide, ricochet and sometimes explode. \"The LHC allows a unique sociological study of how an experiment develops in real time: how scientists form opinions, make technical decisions and circulate knowledge in such a big project,\" says Arianna Borrelli, a particle physicist and philosopher of physics at the University of Wuppertal in Germany. Sergio Bertolucci, CERN's research director, is acutely aware of the importance of cohesive collaboration. \"This is an incredible social experiment,\" he says, noting that roughly 10,000 physicists around the world are taking part in the LHC experiments and 2,250 of them are employed at CERN. Just reflecting on the size of the collaboration he co-manages makes Bertolucci's head ache. \"Imagine the organization needed when 3,000 people all want to know in advance if they can go home for Christmas,\" he says. Managers at CERN have endured a series of headaches since the LHC powered up in September 2008. A little more than a week after the collider came online, a faulty electrical coupling caused an explosion that brought the project to a halt for 14 months. That setback demoralized the scientists at CERN, particularly the graduate students, who worried about the fate of their degrees, says Roy. A graduate student herself, from the University of California, Berkeley, Roy has been camped out at CERN on and off for three years to observe the \"language, taboos and rituals of this exotic community\". The collider restarted in November 2009 and should gather two years of data before it shuts down for a year of scheduled upgrades in 2012. Next month, the LHC is expected to achieve record energies of 7 teraelectronvolts. The collider will reach such an extreme by accelerating two beams of protons to nearly the speed of light and then sending them in opposite directions around a 27-kilometre underground track. The beams cross each other at four spots along the ring, and it is here that the real science happens, within giant detectors surrounding each collision zone. The two biggest particle detectors, A Toroidal LHC Apparatus (ATLAS) and the Compact Muon Solenoid (CMS) experiment, are the size of apartment buildings and each boasts a team of nearly 3,000 people.  \n               Population explosion \n              Each generation of collider has brought a jump in the size of the experimental collaborations (see graph, below), a trend that provides ample opportunities for researchers interested in human interactions. Karin Knorr Cetina, a sociologist at the University of Constance in Germany, is one of the few social scientists to have witnessed this growth directly over multiple generations. She has been studying CERN's collaborations for almost 30 years. \n               Click here for larger image \n               When Knorr Cetina first arrived, physicists there were working on a smaller collider and their detector teams were less than one-tenth the size of today's. \"In those days 100 people in a team was considered huge,\" she says. Knorr Cetina says she was met with friendly bemusement by particle physicists, who were helpful, but thought of a sociologist \"as a poor cousin of real scientists\". That attitude continues today, says Roy. \"What can you say? Physicists are professionally contemptuous,\" she says. Social scientists say they earn the trust of the physicists at CERN by immersing themselves in the culture, just as they would with any other population. Knorr Cetina used this approach to unravel the politics of peacekeeping among the thousands of scientists at the lab. When she first started, she says, \"I expected the same lines of command we know from other complex organizations \u2014 industry or government\". But she didn't find that hierarchy at CERN. Although there are spokespeople who hold positions of authority in the collaboration, there is no top-down decision-making because there are so many highly specialized teams working on different parts of the detector. Knorr Cetina says that at CERN, \"the industrial model cannot work. One human simply cannot make technical decisions on such a large scale. CERN's unconventional structure stems in part from its history and philosophy. The lab was established on the Swiss\u2013Franco border in 1954 to unite a Europe that had been fractured by war. \"It's a place for global collaboration, where science exists beyond the politics of nationality,\" says Bertolucci. But within the lab, the idealism runs into the tensions of conducting actual research. \"The paradox is that science is not democratic; we don't determine who is right by a vote or the majority decision.\" If not an industry or a democracy, what is the structure? Knorr Cetina says that CERN functions as a commune, where particle physicists gladly leave their homes and give up their individuality to work for the greater whole. The communal lifestyle is encouraged by the fact that the laboratory stands on its own international territory. \"Even the Swiss police cannot come in and grab us,\" says Bertolucci. It has its own restaurants, post office, bank and other facilities. \"You can live forever within CERN, without ever needing to visit nearby Geneva,\" says Knorr Cetina. \"It's a cognitive bubble that you can't escape \u2014 that you don't want to escape.\" Bertolucci says that this immersion is essential to CERN's success as a global enterprise. \"People coming here from around the world don't feel like they are visiting someone else's country, they feel they are coming home.\" \"The laboratory does feel like a commune with so many people coming from around the world to work towards a collective goal,\" says Kevin Black, a postdoc with the ATLAS collaboration.  \n                Sacrificing identity \n              Around the CERN campus, the atmosphere is welcoming, and its two restaurants live up to their reputation for offering some of the best food of any physics canteen in the world. But it takes more than comfort and the promise of discovering new particles to persuade thousands of physicists to join the commune. Knorr Cetina points to the organizational structure of the collaborations as a factor that leads physicists to sacrifice their identity to the LHC. As a window into that structure, she describes the evolution of the LHC's largest experimental collaboration, the ATLAS team, which she has studied since its formation in the late 1980s from the remnants of older groups at CERN. ATLAS will be looking for, among other things, the elusive Higgs particle, hypothesized to give other elementary particles their mass. During the 'birth stage' of the ATLAS collaboration, LHC management had to choose between various proposals for detector designs offered by rival groups at different universities and institutes. It might seem that the most obvious and efficient strategy would be for a committee of experts to a make a decision about which technology to use. However, the ATLAS group did not take that path, says Knorr Cetina. Instead, the birth stage was a laborious process in which competing groups were repeatedly sent back to retest their designs, until they all agreed on a single plan. In this way, they avoided alienating groups and losing the manpower needed to build the detector. \"It's an interesting strategy to get groups to accept losing out, yet remain committed to the collaboration,\" says Knorr Cetina. This prolonged process inevitably delayed construction. Physicists at the lab laugh that there are still brochures at CERN that advertise \"the start-up of the LHC coming in 2000\" \u2014 a deadline that was missed by almost a decade. Such delays were no doubt frustrating for physicists but, at least in some cases, they were the necessary cost of keeping the collaboration together, says Knorr Cetina. Albert de Roeck, deputy spokesman for the CMS experiment, notes other practical reasons behind the strategy. \"Spokespersons are considered to be the 'bosses' of the experiment, but we actually have no means of enforcing tyrannical decisions,\" he says. In industry, if people don't agree with you and refuse to carry out their tasks, they can be fired, but the same is not true in the LHC collaborations, he says. \"On our experiments, physicists are often employed by universities, not by us.\" John Krige, a historian at the Georgia Institute of Technology in Atlanta who studied the collaboration structure at CERN before the formation of ATLAS, agrees that there is no simple top-down decision-making at the laboratory. However, he notes that the word \"commune\" implies that there is little rivalry between the members of the collaboration. By contrast, he says, the collaboration thrives on healthy \"organized competition\" between subgroups working to build different components for the detector quickly and effectively. Now that the collider is running, there are other policies within the collaborations that reinforce the communal over the individual, essentially divorcing physicists from ownership of their research, says Knorr Cetina. All papers containing experimental results must list the name of every member of the thousands-strong collaborations alphabetically by country, giving little hint of the real originators of the work. \"This could never happen in biology, where the most intense disputes are about publishing, and reputations are established by your publications,\" says Knorr Cetina, who has also studied the lab life of molecular biologists. \"So much of the narrative of science is about the genius of the individual \u2014 even the Nobel can only be shared by three people,\" says Maria Ong, a sociologist at TERC, an education research collaborative in Cambridge, Massachusetts. \"The LHC is an amazing anti-example of that.\"  \n                Who can review? \n              Collective authorship opens up questions about the construction of knowledge in particle physics, says Peter Galison, a historian at Harvard University in Cambridge, Massachusetts. In February, the CMS collaboration published its first paper based on an analysis of LHC data that showed that a larger than expected number of exotic particles, known as mesons, were produced during the first collisions (CMS Collaboration  J. High Energy Phys.    doi:10.1007/JHEP02(2010)041;  2010). The paper includes 15 pages of author names, totalling between 2,200 and 2,300 people (the collaboration leaders are unsure of the exact number). \"Can it be said that any one person truly understands all the knowledge that it contains?\" says Galison. And who, he asks, can externally review the papers produced? \"You reach a stage where the only people qualified to truly review the work are within the collaboration,\" Galison says. De Roeck says that the size of particle-physics collaborations does inevitably affect peer review. The CMS paper went through months of rigorous checks and revisions during its internal review process; by contrast, it passed through external peer review by the  Journal of High Energy Physics   in just four days. \"External peer review for publication in journals is becoming less important because it is far less stringent than our internal peer-review process,\" he says. Although the collaboration's strength comes from stressing the communal good, recent developments may strain the system. A rising number of particle physicists are turning to the individualistic pursuit of blogging. Although most posts are not controversial, the Fermi National Accelerator Laboratory (Fermilab) in Batavia, Illinois, has had to deal with cases in which physicists broke ranks and leaked information before their collaborations were ready to release it. James Gillies, CERN spokesman, says that the European laboratory has no desire to censor blogs, but it does provide strict guidelines about when it is appropriate to discuss results. Even with these guidelines in place, the blogging phenomenon at CERN \u2014 and its possible tension with official lines of communication \u2014 is something that will be closely followed by Borrelli as part of a team of more than 20 historians, philosophers and sociologists \u2014 \"a huge collaboration in the humanities,\" Borrelli jokes \u2014 that will begin investigating the LHC this year, with funding by the German Research Foundation (DFG). \"This will be a real-time study of how knowledge circulates in such a big project,\" says Borrelli. She is particularly interested in the immediacy of publication in the physics community via the online repository  arXiv.org , where a hundred or so non-peer-reviewed high-energy physics preprints are deposited every day and are openly accessible. \"How do physicists select papers and orient themselves given this onslaught of information?\" she asks. Social scientists are looking beyond the professional lives of the scientists to assess how the collective collaborations affect the physicists on a personal level. Knorr Cetina says that many particle physicists are plagued by nightmares in which their actions cause the project to fail. \"These are the nightmares of those who perceive themselves as a link in a chain, not as an individual,\" she says.  \n                Abnormal stress \n              Knorr Cetina argues that the anxiety displayed by particle physicists is heightened beyond the usual career stress because they strongly identify with the detector ( K. Knorr Cetina  Interdiscipl. Sci. Rev.    32,   361\u2013375; 2007 ). \"This is an object that they built with their hands, but they describe it as a friend,\" says Knorr Cetina. When reporting their nightmares, physicists described being shaken by the imagined loss of the detector as though it was the death of a beloved family member \u2014 something not routinely seen in other experimental scientists. De Roeck agrees that physicists at the LHC are under extreme pressure. Each experiment is made up of subgroups who oversee different components of the experiment and no subgroup wants to be the weakest link that lets thousands of other people down. But he cautions against making too much of the relationship between experimenter and detector. \"I wouldn't go so far as to say that physicists have some psychological problem where they start mistaking the detector for a friend and talking to it,\" he says with a laugh. Ultimately, it is the ever-growing detectors that are to blame for the increasing size of collaborations in particle physics. The invention of the bubble chamber for tracking the path of particles in the 1950s required groups of 10 physicists \u2014 at the time thought to be a large collaboration. Physicists are already making plans for the next generation of particle accelerators. But these may provide little new territory for social scientists to explore. \"There won't be another step up in collaboration size to 25,000 physicists,\" says Galison. \"We're hitting the limits of people in high-energy physics.\" \n                 Zeeya Merali is a freelance writer in London.  \n               \n                     Nature Jobs: Collision Course \n                   \n                     Large Hadron Collider \n                   \n                     Karin Knorr Cetina's web site \n                   \n                     Peter Galison's web site \n                   \n                     John Krige's web site \n                   \n                     Albert de Roeck \n                   \n                     Tom LeCompte \n                   Reprints and Permissions"},
{"file_id": "464022a", "url": "https://www.nature.com/articles/464022a", "year": 2010, "authors": [{"name": "David Cyranoski"}], "parsed_as_year": "2006_or_before", "body": "The bold ambitions of one institute could make China the world leader in genome sequencing. David Cyranoski asks if its science will survive the industrial ramp-up. In 2006, Li Yingrui left Peking University for the BGI, China's premier genome-sequencing institute. Now, freckled and fresh-faced at 23 years old, he baulks at the way a senior BGI colleague characterized his college career \u2014 saying Li was wasting time playing video games and sleeping during class. \"I didn't sleep in lectures,\" Li says. \"I just didn't go.\" He runs a team of 130 bioinformaticians, most no older than himself. His love of games has served him well when deciphering the flood of data spilling out of the BGI's sequencers every day. But \"science is more satisfying\" than video games, he says. \"There's more passion.\" The people at the BGI \u2014 which stopped officially using the name Beijing Genomics Institute in 2007 after moving its headquarters to Shenzhen \u2014 brim with passion, and an ambition so naked that it unsettles some. In the past few years the institute has leapt to the forefront of genome sequencing with a bevy of papers in top-tier journals. Some recent achievements include the genomes of the cucumber 1 , the giant panda 2 , the first complete sequence of an ancient human 3  and, in this issue of  Nature 4 , the genomes of more than 1,000 species of gut bacteria, compiled from 577 billion base pairs of sequence data. The mission, BGI staff say with an almost rehearsed uniformity, is to prove that genomics matters to ordinary people. \"The whole institute feels this huge responsibility,\" says Wang Jun, executive director of the BGI and a professor at the University of Copenhagen. The strategy is to sequence \u2014 well, pretty much anything that the BGI or its expanding list of collaborators wants to sequence (see ' Mass production '). It has launched projects to tackle 10,000 microbial genomes and those of 1,000 plants and animals as part of an effort to create a genomic tree of life covering the major evolutionary branches. Important species, such as rice, will be sequenced 100 times over, and for humans there seems no limit to the number the institute would like sequenced. To fulfil that mission, the BGI is transforming itself into a genomics factory, producing cheap, high-quality sequence with an army of young bioinformaticians and a growing arsenal of expensive equipment. In January, the BGI announced the purchase of 128 of the world's newest, fastest sequencers, the HiSeq 2000 from Illumina, each of which can produce 25 billion base pairs of sequence in a day. When all are running at full tilt, the BGI could theoretically sequence more than 10,000 human genomes in a year. This puts it on track to surpass the entire sequencing output of the United States, says David Wheeler, director of the Molecular Biology Computational Resource at Baylor College of Medicine in Houston, Texas. \"It is clear there is a new map of the genomics world,\" he says. The charge that the BGI has reduced science to brute mechanization does little to ruffle feathers in Shenzhen. Wang himself quips that the BGI brings little intellectual capital into projects: \"We are the muscle, we have no brain.\" But such comments belie a quiet confidence, in everyone from the BGI's seasoned management to its youngest recruits, that they can make an impact not just to the balance of sequencing power but also in biology, medicine and agriculture. This will be a challenge given the significant loans taken out to expand capacity. Torn between scientific and financial goals, even its founder can't seem to decide whether the BGI is a business or a non-profit research institute. Genome scientists around the world are watching to see how it will strike a balance. Edison Liu, director of the Genome Institute of Singapore and head of the Human Genome Organization warns: \"If they are just a sequence-for-money operation, they will not be remembered.\"  \n                Getting far from the emperor \n              China was late to the genomics frenzy of the 1990s that led to the sequencing of the human genome. The fact that the country didn't miss out altogether is thanks largely to the BGI's determined, charismatic and sometimes abrasive leader Yang Huanming ('Henry'). As the human genome project was nearing completion, Yang and a small group of sequencing advocates tried to get China involved. They found support from the Chinese Academy of Sciences (CAS), which secured a building and a start-up fund of 1 million renminbi (US$150,000). Wang announced the establishment of the BGI on 9 September 1999, at nine seconds past the ninth minute of the ninth hour. Few moments portend more longevity in Chinese numerology, says Wang. That November, the government issued a grant of 3 million renminbi, of which the BGI got the lion's share, to support the sequencing of 1% of the human genome. China was the only developing nation involved in the international project, and it finished its 30 million bases in less than a year. Like those at other major sequencing centres at the time, Yang acquired a taste for big genome projects. While completing a somewhat underfunded 'scan' of the swine genome for Danish agencies in 2000, Yang says he decided to do something \"more significant\". The BGI launched a project to sequence the rice genome in 2001, using a grant of 60 million renminbi from the Hanzhou municipal government to buy 36 state-of-the-art sequencers. The BGI published the genome of the indica variety of rice in  Science   in 2002 (ref.  5 ), months before an international consortium published that of the japonica variety. The BGI moved on to sequence the chicken 6  and silkworm 7  genomes. In 2003, it sequenced the corona virus 8  that caused severe acute respiratory syndrome (SARS) and released a diagnostic kit that impressed Chinese President Hu Jintao. The BGI's reward was to be made part of the CAS, an honour that came with extra funding, but the academy turned out to have stipulations that didn't fit the BGI. CAS institutes are not supposed to have more than 150 scientists; the BGI had twice that and was looking to expand. Yang had to make some of his workforce official CAS staff and make special arrangements for others, stretching the CAS budget to the extreme. \"No one was happy,\" he says. The move to Shenzhen provided a release valve, luring the BGI in 2006 with 10 million renminbi in start-up fees and 20 million renminbi in annual grants. The city is a driving force in southern China's 'factory of the world', with many of its 12 million people producing the cheap clothing and electronics that helped to usher in China's economic miracle. The BGI is at home in Shenzhen. Yang wants to sequence genomes at twice the speed and half the price of anyone else. And he was eager to slip away from some of the oversight in Beijing. Although he doesn't like talking about those with whom he's clashed, Yang likes to say that in Shenzhen, \"the mountains are high and the emperor is far away\".  \n                The BGI gets big \n              With this breathing room, the BGI has grown to employ 1,500 people nationwide, more than two-thirds of them in Shenzhen, and this is expected to jump to 3,500 by the end of the year. With the investment in new sequencers, provided by a 10-billion-renminbi loan from the China Development Bank, the BGI's capacity will grow, but so will costs. Staff at the BGI won't say how much they paid for the new sequencers, but the list price is about 3.4 million renminbi each. The purchase, which was announced on the same day the model launched, raised hackles among competing genomics centres. They accuse Illumina of making a secretive deal with the BGI while only granting others access to older models. Illumina denies such allegations, and says it has a trade-in programme for those who want to upgrade. Of the machines, 100 will be installed in a new Hong Kong lab to facilitate international collaborations. But staff in Hong Kong cost more than the BGI is used to paying, and will be kept to a minimum (40\u201350 researchers). Reagents cost about 1 billion renminbi per year, and electricity for computers and cooling systems consumes another 9 million renminbi. Yang emphasizes that the loan will be paid back. But as the commodification of sequencing continues to push prices down, how the BGI will do this is an open question. A BGI monopoly in providing sequencing services is far from assured. Aside from existing academic competitors, private ones using newer technology are starting up. Complete Genomics, based in Mountain View, California, which specializes in human genomes, expects to sequence 5,000 human genomes in 2010, starting in April. It has already logged more than 500 orders. The BGI's solvency depends in part on scientists elsewhere paying to have microbe, plant and human genomes sequenced and resequenced faster and better than they could themselves. But like many sequencing centres, the BGI is looking to be more than a service provider. Maynard Olson, a genomics researcher at the University of Washington in Seattle who trained Wang and has close ties to the BGI, says it needs to be. \"Outsourcing only works well when there is some scientific relationship between the parties. There are too many trade-offs during both the laboratory procedures and the low-level data analysis to commodify sequence data entirely.\" Yang says he hopes that collaborators will pay half of the estimated costs of the genomes they want sequenced and then publish jointly, but for interesting projects he will cover 70% or even all of the cost if the collaborators lack funding. For Eske Willerslev at the University of Copenhagen, it made sense. He collaborated with the BGI on the genome of a 4,000-year-old frozen Greenlander dubbed Inuk. Although his lab had the capacity to sequence up to about 50 billion bases in a week, he went for the BGI's technical expertise. \"I have a lot of respect for people like the folks at the BGI that really can run second-generation sequencing platforms to their perfection,\" he says. The ancient human genome was sequenced in two and a half months for roughly $500,000, split evenly between Willerslev's funders and the BGI. Willerslev says the BGI was integral not just to the sequencing but to the science. \"The whole project was started because of an important scientific question agreed on by both Wang Jun and myself,\" he says.  \n                For science or service \n              Proving its science goes beyond brute-force sequencing could be a challenge. The BGI's Luo Ruibang, also a student at the South China University of Technology in Guangzhou, turned 21 while at his last scientific meeting. He says he's had trouble convincing other scientists that, lacking doctoral training, he can do top-notch science. \"A lot of the foreigners wonder if I'm really capable,\" he says. Luo and Li were co-first authors on a paper 9  describing the discovery of large DNA segments in the Asian and African genomes that are absent in the Caucasian genome. Li and his bosses are confident that this youth brigade can piece together and verify sequences. \"It is a new field,\" says Wang. \"There is not much experience anyway.\" But interpreting data and designing experiments are two different things, and BGI staff admit a dearth of knowledge in the latter. \"We don't know much about biology,\" Li says. Liu says the BGI needs to overcome its biological blindspot, but he is supportive of its mission. \"They are primarily sequencers, but smart ones with big guns,\" he says. The panda genome was, in part, a way to show off those guns. As few biologists work on the animal, its genome is unlikely to lead to basic or applied breakthroughs. But it gave the BGI a chance to show the power of what many call next-generation sequencing, which produces shorter individual DNA reads \u2014 100 base pairs or less, compared with the 1,000 base-pair fragments with previous technology \u2014 but at unprecedented speed. This increases the sequencing output thousands of times, but for an unfamiliar genome, it's difficult to assemble the finished product, says Wang. \"Some thought we wouldn't be able to do it.\" There was also public interest and local government support because Jing Jing, the panda whose genome was sequenced, was the model for Beijing's 2008 Olympic mascot. \"And pandas are cute,\" says Wang. The BGI is also powering more biologically relevant projects coordinated by collaborators. For example, sequences of 40 silkworm strains, published last October, uncovered some 300 genes showing the history of breeding and domestication 10 . Sequencing was recently completed on the Tibetan antelope, whose ability to gallop at more than 4,500 metres above sea level might offer clues to adapting to high altitudes. Ge Ri-Li, a specialist in high-altitude medicine at Qinghai University in Xining plans to study antelope genes related to energy transport. Next he will sequence the genomes of Tibetan Chinese people, who suffer far less mountain sickness than the majority Han Chinese. \"We want to know why,\" says Ge. \"The final goal is to make humans more capable of adapting to high altitudes.\" Research alone is not going to pay back the 10-billion renminbi bank loan. The BGI makes some income from collaborations, which account for 40% of the sequencing workload. Outsourced sequencing services for universities, breeding companies or pharmaceutical companies bring in higher margins and account for another 55% of the workload (the final 5% is the BGI's own projects). In 2009, the BGI pulled in 300 million renminbi in revenue. That is not enough, says BGI marketing director Hongsheng Liang. In 2010, Liang hopes to pull in 1.2 billion renminbi. New income could come from proprietary rights to agricultural applications. The BGI, which owns more than 200 patents, has been attempting to do genomics-based breeding with foxtail millet in Hebei and has other agricultural projects in Laos. More cash could come from expansion of services overseas. Within three years, the institute plans to open offices in Copenhagen and San Francisco. The BGI may also charge for access to its Yanhuang database, a project launched in 2008 to sequence the genomes of 100 Chinese; BGI scientists say they would like to expand this number into the thousands. Although according to Yang, it would be charging \"at cost\" \u2014 to cover computational expenses and maintenance, not for the data. Wheeler says that the BGI will need constant innovation to keep up, and that means maintaining its high rate of collaboration. The United States has three large national centres that can test a broader range of technology, he says. \"They constantly challenge one another to improve. They work cooperatively in large national sequencing projects, and critique one another to improve production and analytical methods.\" Single institutions that bank on a single technology may lose their edge when the technology goes out of date. Yang recognizes the unpredictability of technological advances. Asked why he didn't stagger his investment in sequencers to take advantage of new technology as it appears, he says he'll just replace what he has when the time comes. He admits, however, that if that time comes too soon, he will be out of luck. \"There are plenty of risks, but I admire that,\" says Olson. By aggressively seeking collaborations and new technologies, the BGI's ambitious approach will no doubt continue to turn heads. \"The bottom line is that the BGI is doing something exciting. It is a Chinese solution to the challenge of developing stronger science. Time will tell how well it works.\"   See Editorial,  \n                     page 7 \n                   . \n                     China Special \n                   \n                     Web Focus: Personal Genomes \n                   \n                     BGI \n                   Reprints and Permissions"},
{"file_id": "464156a", "url": "https://www.nature.com/articles/464156a", "year": 2010, "authors": [{"name": "Geoff Brumfiel"}], "parsed_as_year": "2006_or_before", "body": "With the launch of a powerful laser facility, Britain's most secretive lab is opening up to academics. Geoff Brumfiel secures a preview. Aldermaston is a picturesque English village with red-brick houses, tidy gardens and an inviting local pub. But just to the south lies a much more forbidding set of buildings. Behind double-fencing and guarded gates is the sprawling campus of the Atomic Weapons Establishment (AWE), Britain's most hush-hush research laboratory. The 304-hectare facility, a converted Second World War airfield, is home to the roughly 4,000 scientists, engineers and technicians who are responsible for maintaining Britain's nuclear warheads. The Ministry of Defence, which oversees the AWE, is notoriously tight-lipped about the lab's activities, and scientists who work there generally try to keep a low profile. But within the next few months, the AWE's most ambitious and expensive scientific project is due to be completed, and it is prompting the lab to open up its doors \u2014 at least a chink. The new Orion facility will be home to 12 high-powered laser beams capable of heating and compressing material to millions of degrees Celsius in less than a nanosecond. Housed in a gleaming building the size of a soccer pitch, the laser system will provide physicists at Aldermaston with crucial data about how components of their ageing nuclear weapons behave. Under current plans, around 15% of Orion's time will be offered to academics wanting to study conditions on stars or inside giant planets. And in that open spirit, researchers there invited a  Nature   reporter in for a look around. In most respects, Orion is the smaller cousin of the US National Ignition Facility (NIF) in Livermore, California, which is already running academic experiments. When operating at full steam, the NIF will use 192 lasers to create around 4 million joules of energy, some 100 times more powerful than Orion. What makes the AWE's laser notable is the exquisite precision that it will give researchers in controlling the heat and compression exerted on the materials placed in its target chamber \u2014 and the fact that the AWE is sharing it at all. The motivations for collaboration are not entirely selfless. The defence establishment wants to provide scientists inside the security cordon with the sort of intellectual stimulation needed to keep them on their toes; it also has an incentive to nurture the wider community of physicists from which it draws its staff. \"We want to demonstrate that we maintain high standards for our science,\" says Daryl Landeg, the AWE's chief scientist. And academics far beyond Aldermaston are keen to cross the fence. At present, Europe has only a handful of comparable laser facilities, says Fran\u00e7ois Amiranoff, director of the Laboratory for the Use of Intense Lasers at the \u00c9cole Polytechnique near Paris. \"A facility like Orion is very, very interesting for the scientific community,\" he says.  \n                Secrecy rules \n              Britain established its atomic-weapons programme at the current AWE site in 1950, after close involvement in the Manhattan Project. The campus, which also houses facilities for manufacturing and storing sensitive nuclear materials, is shielded by the nation's strict secrecy laws, and has historically shunned visitors. Things have started to change over the past decade, says Steven Rose, a physicist at Imperial College London who headed the AWE's plasma-physics division from 2001 to 2004. \"The old argument that secrecy was paramount I think perhaps holds less sway these days,\" he says. \"They've realized that you can do some things in collaboration with the outside world.\" Enter Orion, the \u00a3183-million (US$274-million) laser on which construction began in 2005. The machine has ten conventional lasers (see  graphic ), each of which can deliver 500 joules of energy in about a nanosecond (a billionth of a second). It also has two short-pulse lasers, which deliver the same amount of energy in just half a picosecond (a trillionth of a second). \n               Click here for larger image \n               Inside Orion's main building, workers in Teflon suits and hairnets are busily scrambling around gigantic white scaffolding. The structure will soon hold the mirrors, amplifiers and lenses needed to boost and focus the 12 beams onto their target, which lies in a separate chamber behind 1.5 metres of solid concrete, a shield necessary to contain the radiation generated when the laser beams hit. The exceptional cleanliness in the laser halls and target area (and expected of visitors) is about more than aesthetics: a stray hair in the path of the intense beam could cause irregularities and crack an expensive mirror or grating. Orion's main mission, like that of the NIF, is to explore how nuclear weapons work, particularly as they get older. In 1998, Britain ratified the Comprehensive Nuclear-Test-Ban Treaty, an international agreement prohibiting tests of nuclear weapons. Scientists in Britain and worldwide have therefore been busy developing computer models to simulate nuclear warheads and work out whether the weapons will still detonate after decades in storage, and what type of detonation will result. What is missing, however, are actual data. US scientists hope that the more powerful NIF will contribute some of those data, by generating temperatures and pressures so high that they will spark nuclear fusion in small quantities of two hydrogen isotopes, deuterium and tritium. This fusion process would resemble conditions inside the most powerful stage of a modern thermonuclear weapon. If the NIF is a thermonuclear hammer, then Orion is a scalpel. The smaller facility will never achieve full-scale fusion, but it will be able to carefully control conditions inside test materials such as uranium. Pressure and temperature usually go hand-in-hand, explains Peter Roberts, head of the AWE's plasma-physics department. \"You pump up a tyre with a bicycle pump and it gets hot,\" he says. But Orion can get around this. It can compress a material with its long, nanosecond pulses then suddenly heat it with its very short, half-picosecond pulses. The result is 'isochoric heating', an unusual condition in which a material is heated so quickly that it doesn't have time to expand. This capability allows Orion to probe materials at wide-ranging combinations of temperatures and pressures. In particular, researchers will use Orion to explore two key parameters for materials used in nuclear weapons: their opacity and their equation of state. The first describes how radiation travels through a material \u2014 in this case, the two stages that make up a weapon. The first stage, or primary, is a few kilograms of plutonium that are compressed by conventional explosives until they begin a runaway nuclear reaction. The radiation from that reaction is then focused onto the 'secondary', the stage in which hydrogen isotopes create a much larger blast using nuclear fusion. Researchers want to know what the opacity is and how it changes with age so that they can model radiation's flow from the primary to the secondary and verify whether the warheads will still work. The other parameter \u2014 the equation of state \u2014 describes how a material behaves at enormous pressures and temperatures. By generating data on these and other crucial parameters, Orion will give nuclear-weapons scientists the information they need to ensure that their models are correct. \"You can't look this stuff up,\" Rose says. The researchers running the NIF often emphasize the giant laser's applications in energy production and fundamental science over its military role; it could, for example, lead to new reactors that produce electricity using tiny fusion implosions. Orion's scientists are much less circumspect. \"We're working on weapons physics fundamentally,\" Roberts says. Nevertheless, he and others at the lab are eager to give civilian scientists an opportunity to use the laser \u2014 and the academics are eager to try out its capabilities. \"It will have significant characteristics that no other laser in the United Kingdom or indeed Europe has,\" says Mike Dunne, director of the Central Laser Facility at the Rutherford Appleton Laboratory near Didcot, UK. Dunne says that Orion should relieve the strain on the facility's Vulcan laser, Orion's massively oversubscribed civilian counterpart.  \n                Extreme conditions \n              Physicists studying astronomical objects find themselves in a situation not dissimilar to that of nuclear-weapons scientists: unable to recreate the extreme conditions inside a star, for example, and largely dependent on complex computer models to show how they work. Orion will not reach stellar temperatures and pressures, but it will be able to create flows of hot ionized gas with the sorts of high magnetic fields and temperatures that mimic parts of stars and other astronomical objects. By scaling up the data from Orion, astrophysicists should be able to improve their models, Dunne says. The laser facility can also inform more abstruse, fundamental work in atomic physics. Current theory does well at describing normal matter and extremely hot matter that has been stripped of electrons. But it cannot describe situations in which atoms are subjected to high levels of radiation without losing all their electrons. By heating a test material, then probing it with its short-pulse beams, Orion can provide data that can extend existing theories into this middle region, says Amiranoff. This warm, dense matter may exist at the heart of gas giants such as Jupiter, or even within Earth's core. Scientific collaborations are not slated to begin on Orion until the second half of 2012, and proposals to use the facility will be submitted through the system being used for Vulcan and other UK lasers, rather than being determined by weapons scientists. This open peer-review of proposals \"is absolutely critical to gaining the confidence of the community\", Dunne says. \"If some committee inside of the wire assessed relative merits, there would always be the suspicion that they were picking topics that helped their programme rather than just because they were good science.\" Back on the AWE's campus, the weapons scientists are eagerly preparing for their new guests. Tom Bett, who helps to manage construction, shows off a data-analysis room purpose-built for unclassified visitors and lined with sleek computer terminals. (The weapons work will be done in a separate room, he explains, and the data will be stored on servers locked inside vaults.) On the ground level, floor-to-ceiling windows illuminate a bright reception area \u2014 the first thing that visiting researchers will see as they are welcomed to the new laser facility. \"Those windows,\" Bett adds proudly, \"are all bullet-proof.\" \n                     Atomic Weapons Establishment \n                   \n                     National Ignition Facility \n                   Reprints and Permissions"},
{"file_id": "463871a", "url": "https://www.nature.com/articles/463871a", "year": 2010, "authors": [{"name": "Richard Van Noorden"}], "parsed_as_year": "2006_or_before", "body": "Protesters saying \"no to CO 2 \" are just one roadblock facing carbon sequestration \u2014 a strategy that could help prevent dangerous climate change. Richard Van Noorden investigates. The idea of injecting 400,000 tonnes of carbon dioxide under a shopping mall was always going to be a tough sell. And so it proved when the Dutch minister of economic affairs, Maria van der Hoeven, came to the small town of Barendrecht in December to explain why the government supported the proposal, made by the petroleum company Shell. At a public meeting in a packed theatre, attendees hurled jeers and threats at van der Hoeven and her colleagues, who were trying to convince the residents that the injection project was safe and environmentally beneficial. The conflict has escalated since then. Last month, the Dutch parliament voted to continue with the project, prompting Barendrecht's deputy mayor, Simon Zuurbier, to threaten legal action against Shell. \"It is foolish to experiment in a residential area,\" he says. John Brosens, who chairs the citizens' 'No to CO 2 ' society, says: \"We are against any underground storage of CO 2  \u2014 wherever.\" Public opposition is just one of several obstacles blocking the strategy known as carbon capture and storage (CCS). The idea behind CCS is to strip CO 2  from the exhaust gases of factories and power plants, then inject it as a compressed liquid into secure geological formations, such as depleted oil and gas deposits. Politicians and scientists have for years touted CCS as a way to help the world cut its carbon emissions. And the idea of injecting CO 2  underground is not a pipe dream: the petroleum industry has been doing it for nearly 40 years to aid the extraction of crude oil. But efforts to expand CCS to save the climate have largely stalled. Public opposition is disrupting some early pilot schemes, the process is too expensive in most cases, regulations covering its use are not yet fully in place and investors are uncertain about its viability at large scales. \"There is lots of research and lots of talking \u2014 lots of recycling of information \u2014 but little real progress,\" says Heleen de Coninck, who works on climate policy at the Energy research Centre of the Netherlands in Petten. That slow pace is especially harmful for CCS because the strategy has a limited lifetime. It is viewed as a bridge technology \u2014 one that would allow coal-rich nations such as the United States and China to burn fossil fuels guilt-free for a generation or two, until cleaner forms of energy become economically competitive. But to serve this temporary function, CCS must expand soon. The International Energy Agency (IEA) projects that the cheapest way to halve expected carbon emissions by 2050 would be to use CCS to contribute almost 20% of the necessary cuts. On that schedule, the technology must quickly grow to the scale of an oil industry: by mid-century, the volume of liquid CO 2  that must be injected underground for permanent storage each year would be three times the current amount of petroleum used every year. \n               Click here for larger image \n               In 2008, the G8 declared that 20 large-scale plants should be launched by the end of 2010. But development is way off track (see  map ): there are only seven large projects and most of these use the gas to loosen trapped oil \u2014 a process not designed to store CO 2 .  \n                The price isn't right \n              The main sticking point with CCS is its expense, which comes mostly from capturing the gas. Existing factories and power plants must install a bulky capture unit, which uses an amine-based solvent to strip CO 2  from exhaust gases. New power plants can use a more compact strategy in which coal or other fossil fuels are transformed into CO 2  and hydrogen, and the hydrogen is then burned. A third, relatively new concept involves burning fuel in pure oxygen, creating a waste stream of water and CO 2 . The chemical transformations required in all these technologies suck up energy. That means that power plants need to burn more fuel to produce the same amount of electricity, roughly doubling the cost of that energy. From an environmental point of view, however, the technology is relatively cheap. Studies suggest that, once the technology matures, commercial carbon-capture coal plants would spend US$50\u201380 for every tonne of CO 2  they avoid emitting. According to estimates by Bloomberg New Energy Finance in London, the costs of electricity produced by mature CCS plants would overlap with those for other low-carbon electricity options, such as solar, wind and nuclear power (see graph). \"Of course CCS will raise energy prices, but if you look at reaching certain targets, such as keeping below 2 \u00b0C of warming, then it is a cost-effective system,\" says Howard Herzog, who works on carbon sequestration at the Massachusetts Institute of Technology in Cambridge. In theory, these technologies would be more competitive if some sort of cost were associated with emitting CO 2 , but few countries have taken that step. The main exception is the European Union (EU), which has a carbon emissions trading scheme that effectively sets a price on CO 2  pollution from large emitters. But at a current rate of around \u20ac13 (US$18) per tonne of CO 2 , the EU price is well below the level that would provide an incentive for coal plants to capture their emissions.  \n                Government's greenish light \n              Without any economic force driving companies to start capturing and storing CO 2 , the CCS industry is waiting for government incentives to kick-start demonstration projects, says Tom Kerr, an IEA analyst. The agency calculates that to keep to its ambitious schedule, governments in developed nations will need to invest US$3.5 billion to $4 billion in demonstration projects each year from 2010 to 2020. Europe, Canada, the United States, Australia and China have so far publicly allocated just $7.3 billion, although they have promised a total of around $20 billion. Private investors also complain that delays in establishing government regulations for CCS are preventing companies from making meaningful commitments. In the United States, for example, federal rules are unclear on who, if anyone, owns the pore space in the rock that will be used to store CO 2 . And in Europe, national governments are only slowly incorporating existing EU laws to govern carbon storage. The absence of such laws in Germany has stymied plans by Vattenfall, a Swedish energy company, to store carbon emissions from its Schwarze Pumpe plant in Germany. Similarly, regulators are only beginning to consider what monitoring requirements should be placed on sequestration projects to ensure that the carbon stays underground. A second major unresolved issue involves long-term liability over leakage, and how long developers will remain legally responsible for any damages after the injection is stopped. \"If people want to make CCS work, we can have regulations that promote a sensible system and allow it to operate,\" says Herzog. \"But if people want to kill it, easy \u2014 we can say we need more and more regulations, and just strangle it.\"  \n                Inner space \n             While the regulatory uncertainty drags on, geoscientists are gaining valuable experience at several large-scale projects. At the In Salah natural-gas plant in Algeria and the Sleipner West gas field in the North Sea, for example, unwanted CO 2  present in the methane deposits is being removed and injected into aquifers covered by impermeable rock layers. \"For us, carbon storage is a ho-hum activity, but to the outside world a lot of what we do is rocket science,\" says Iain Wright, who manages the CO 2  storage project at In Salah for BP, one of the companies that runs the plant. But geoscientists have scant experience with CO 2  storage elsewhere. When CCS moves into settings that are less well studied, \"the uncertainty levels rise\", says Travis McLing, a geologist involved with the US Department of Energy's Big Sky Carbon Sequestration Partnership in Bozeman, Montana. Experts remain unclear as to how much capacity there will be to sequester carbon when the process is scaled up to global proportions. Preliminary estimates of capacity in the United States, China and Europe suggest that the world has ample space to store CO 2 , but only a tiny proportion of that is likely to be usable, says Bert van der Meer, a reservoir engineer for the non-profit research organization TNO in Delft, the Netherlands. Engineers might find that less CO 2  can be forced into deposits than hoped because water already occupies the available rock pores. Or large projects might get bogged down if CO 2  does not spread quickly through underground deposits or if the rate at which it can be injected is slower than desired. \"The ultimate test will be to try to do injections and see what push-back you get from nature,\" says geoscientist Stuart Haszeldine from the University of Edinburgh, UK \u2014 although he doesn't see the issue as a deal-breaker for CCS. Jens Birkholzer, an Earth scientist at Lawrence Berkeley National Laboratory in California, calculates that an industrial-scale CO 2  sequestration plant would increase pressure within rock pores for some 100 kilometres around the injection site. He modelled the effects of 20 CO 2  storage sites across the Mount Simon sandstone formation underneath Illinois and neighbouring states; the pressure build-up eventually affected the entire 241,000-square kilometre basin. This could push brackish water into freshwater stores via existing faults, Birkholzer says. He adds, however, that previous practices, such as pumping out groundwater in Northern Illinois, drew saline waters towards withdrawal wells and dwarfed the environmental consequences of his CO 2  storage scenario. In practice, such problems may never materialize. Unlike many investors, most geologists think that the unanswered questions simply reinforce the need to build large demonstration projects to study what happens.  \n                Mixed reception \n              That try-it-and-see approach has not won over the NUMBY (not under my back yard) crowd. Aside from Barendrecht, protests have arisen at several sites for which CCS is planned or being discussed, and not just in Europe. In August last year, despite funding from the US Department of Energy, a consortium scrapped a $92.8-million project to bury CO 2  from an ethanol plant in Greenville, Ohio. Battelle, the non-profit research organization that ran the project, said that it pulled out for \"business considerations\" \u2014 but local media called it a victory for protesters. \"People ask: 'why should I be the guinea pig? What's in it for me? Will this affect the value of my property? Just how dangerous is it?',\" says Staffan G\u00f6rtz, a spokesman for Vattenfall. Such worries have prompted many to suggest that it would be preferable, although more expensive, to store the carbon off-shore. But geoscientists say that worries about leakage are unfounded. Seismic studies and monitoring techniques show that nothing has yet leaked from the few large-scale carbon sequestration projects already in operation. And natural deposits of CO 2  show that the gas can be trapped for millions of years ( J. Lu, M. Wilkinson, R. S. Haszeldine and A. E. Fallick  Geology    37,   35\u201338; 2009 ). If the CO 2  is stored correctly, the chance of a problem is near zero, says Susan Hovorka, a geochemist at the University of Texas at Austin. Right now, society is leaking almost all of the CO 2  it produces. \"With CCS,\" she says, \"we could do better than that with some degree of confidence.\" Not everyone is opposed to sequestration. Sensing economic benefits, one landowner in Mississippi didn't want Hovorka's team to conduct a geological survey of his land because he was afraid they would find something that would disqualify his land as a potential injection spot. Near L'Aquila, Italy, people live comfortably above leaky stores of natural CO 2 . \"There are flats built here, and CO 2  seeps into people's cellars, but they just ventilate it out again,\" says Tore Torp, a CO 2  storage specialist in Trondheim, Norway, with Statoil, the company that runs the Sleipner sequestration project. Although they may not know it, many more people in the United States and Europe have, for close to a century, lived above deposits of a much more dangerous gas, methane, which is injected underground every summer to store for use in the winter. David Keith, an Earth scientist at the University of Calgary in Alberta, Canada, thinks that protests about leakage are often a tactic. \"Where people have quite legitimate reasons for thinking CCS shouldn't be done, the way they get it stopped is to push on the safety issue,\" he says. Some environmental groups, for example, oppose CCS because they worry that industry is using it as an excuse to build more coal plants or to make extra profit from enhanced oil recovery without truly sequestering the CO 2 . But these concerns \u2014 which reflect CCS's origins in the petroleum industry \u2014 also point to a fact that many see as the technology's greatest asset: it can get polluting industries and countries talking about carbon cuts. That was one of the reasons why van der Hoeven followed up her chilly visit in Barendrecht with a trip to Saudi Arabia last month. There, she enjoyed a much warmer reception discussing plans for Saudi Arabia to collaborate on CCS with the Netherlands, Norway and the United Kingdom. With decades of oil under the Arabian sands and more than a century of coal left in the ground worldwide, many energy researchers view CCS as a foregone conclusion. \"Nobody likes it \u2014 it's a mess. But you've got to have it,\" says Jon Gibbins, an energy expert at the University of Edinburgh. \"If the world is going to cut carbon emissions, how is it going to do it without CCS?\" \n                     Nature Reports Climate Change \n                   \n                     Scottish Centre for Carbon Storage \n                   \n                     CCS World Map \n                   \n                     US Department of Energy carbon sequestration programme \n                   \n                     International Energy Agency's CCS programme \n                   \n                     Global Carbon Capture and Storage Institute \n                   \n                     Barendrecht's \u201cNo to CO2\u201d society \n                   Reprints and Permissions"},
{"file_id": "463422a", "url": "https://www.nature.com/articles/463422a", "year": 2010, "authors": [{"name": "Daniel Cressey"}], "parsed_as_year": "2006_or_before", "body": "Alan Ashworth took a cancer drug from Petri dish to patients in near record speed. Daniel Cressey meets a biologist who is evangelical about translational research. A short, laminated article has fallen off the wall of press cuttings outside Alan Ashworth's London office. It is an editorial published in 2000 by Britain's widely read and notoriously opinionated tabloid newspaper,  The Sun   \u2014 and it is praising geneticists who study cancer. \"They are, without doubt, the most important people alive,\" it crows. \"People like Professor Alan Ashworth and his team at the Institute of Cancer Research are dedicated to the cause.\" Last year, Ashworth's work was lauded, somewhat more quietly, in an editorial in the  New England Journal of Medicine   ( NEJM ) 1 . \"Readers may be surprised by the editors' decision to publish a small early-stage trial,\" the journal wrote of a study largely based on Ashworth's discoveries, \"but this trial not only reports important results \u2014 it also points to a new direction in the development of anticancer drugs\". That Ashworth has won praise from these diverse camps is a testament to his record in 'translational' medicine \u2014 moving developments in basic science into the doctor's surgery. In just under 15 years, he has gone from early work on the major cancer-risk gene  BRCA2 , to involvement in the development of a promising cancer drug based on knowledge about the gene's biology. His work \"is really the shining example of successful translation of a basic biology idea into successful clinical application\", says Julian Downward, from Cancer Research UK's London Research Institute. Ashworth \u2014 who prefers the term 'integration' to translation \u2014 plans to make his approach a central tenet of the Centre for Molecular Pathology, the London facility he will lead, which is scheduled to break ground early this year and should be completed in 2012 at a cost of \u00a320 million (US$33 million). He has even become something of an integration evangelist, chastising those who do basic cancer research without considering the work's future application. \"This integrated approach is something I demand of everybody,\" he says. Yet what Ashworth makes look easy, others find extraordinarily hard. He says that the key to his success has been a thorough understanding of basic biology and a commitment to seeing it through to application, plus a sprinkle of serendipity. But others say that Ashworth is also set apart by his drive, his charm and his ability to win over others to build the networks of expertise needed for integration. \"One of his big achievements,\" Downward says, \"has been to actually hold together this grouping of people who aren't usually very good at talking to each other.\" This integrated approach is something I demand of everybody. Alan Ashworth ,  In the 1980s, when Ashworth started a PhD in biochemistry at University College London, molecular biology was in the \"really early phase of gene cloning\", he says. It was his flair in this field that led him in the 1990s to Mike Stratton, then at the Institute for Cancer Research in Sutton, UK, who had recently shown 2  that  BRCA2   \u2014 which, along with  BRCA1 , was known to be important for determining breast cancer risk \u2014 was localized to a small region of chromosome 13. Ashworth says that he was brought in \"as kind of a hired gun\" to help clone the gene. And he was confident that he would. Over a bawdy meal in April 1995, Ashworth predicted that the gene would be in hand within a year and scrawled his prediction on a napkin, witnessed by several colleagues. The group didn't need the whole year. In December 1995,  Nature   reported 3  the cloning of  BRCA2 . Ashworth recalls when the first woman was tested for  BRCA2   mutations; she had been scheduled for prophylactic mastectomy because of her family's cancer history. The tests were negative, and the surgery was called off. \"It was only then that I realized I could apply what I was good at to patient benefits,\" he says. By the end of the 1990s, Ashworth was directing a team at the Institute of Cancer Research in London and had developed a mouse with a mutated  Brca2   gene that was highly susceptible to cancer 4 . Work by several groups showed that the gene is involved in repairing DNA damage by a process called homologous recombination (see graphic). When it is mutated, DNA breaks start to accumulate, increasing the risk that a cell will turn cancerous. \"We had to retool essentially and start to understand DNA repair,\" says Ashworth. In 1999, he was chosen to lead the Breakthrough Breast Cancer Research Centre at the Institute of Cancer Research, where he came into close contact with patients and physicians.  \n                Double jeopardy \n              Ashworth became interested in the idea that a mutated  BRCA   gene could not only render cells susceptible to cancer \u2014 it could also be exploited to target them. The team turned to a concept in genetics called 'synthetic lethality', in which mutations are harmless on their own but together will kill a cell. They theorized that cancer cells bearing a mutated  BRCA2   were now reliant on another leg of their DNA repair machinery. Taking out a second repair pathway should bring them to the floor. \n               boxed-text \n             The opportunity to test the theory presented itself when, as Ashworth puts it: \"I met a bloke in a pub and he offered me some drugs.\" That bloke was Steve Jackson, a DNA-repair researcher from the University of Cambridge, UK. The company he had started, KuDOS Pharmaceuticals, had developed the drug olaparib, which inhibits an enzyme vital for the repair of DNA breaks: poly(ADP-ribose) polymerase, or PARP. Over a drink, Jackson and Ashworth hit on the idea of testing whether Jackson's 'PARP inhibitor' would take out the second leg in  BRCA2   mutant cells (see graphic). \"It wasn't months; it was days or weeks after the compounds went down to his lab that they told us about these absolutely stonking results,\" says Jackson. The team showed in 2005 that  BRCA2   mutant cells died when they were hit by Jackson's drug 5 , and a back-to-back report from Thomas Helleday, then at the University of Sheffield, UK, echoed the finding 6 . He holds together people who aren't usually very good at talking to each other. Julian Downward ,  When it came to testing the drug in people, Ashworth had a head start: olaparib had already passed many of the early safety and regulatory hurdles required for a new drug. But human trials bring other challenges: intellectual property, financing and mountains of regulatory bureaucracy can be enough to smother anyone's translational ambitions. Ashworth, who had access to clinical expertise via the cancer centre and nearby hospital, is sanguine, and says that the key has been to listen to those from different disciplines \"so I can understand what they do\". Part of Ashworth's success may lie in his air of being a regular, amiable scientist who can seem slightly uncomfortable in a suit. \"It may be because he was slightly outside the normal operation of all those things,\" says Downward. \"He's not a clinician, he's not a pharma person.\"  \n                Killer instinct \n              Of the 60 participants recruited into the eventual phase I clinical trial, 23 had mutations in one of the  BRCA   genes; and after treatment, 12 of these people showed a 'clinical benefit' 7 , such as no progression of their disease for 4 months or longer. The synthetic lethality approach \u2014 the new direction in anticancer drugs referred to in the  NEJM   editorial \u2014 looked highly promising, and the cancer community was abuzz. AstraZeneca, which acquired KuDOS in 2006, is now developing the drug and the results of a phase II trial, presented at the American Society of Clinical Oncology meeting in 2009, showed that more than a third of patients taking the maximum dose showed some improvement in their tumours 8 . \"I've no doubt this approach will work,\" says Ashworth. The drug might also work against cancers with other DNA-repair defects. Last year, his team showed that PARP inhibitors were also lethal in cancer cells with mutations in  PTEN , one of the genes most commonly disrupted in cancers 9 . Ashworth cites another example of the integration he seeks between basic and applied research. His team knew that  BRCA - mutant tumours develop resistance to the platinum-based drugs such as cisplatin that are a mainstay of treatment; the group went back to the lab to work out whether the PARP inhibitors would run up against the same problem. The resulting paper in  Nature 10 , along with one from another group 11 , showed that drug resistance arises when the mutant  BRCA2   undergoes a deletion, restoring DNA repair. This means that PARP inhibitors might sometimes need to be used with other therapies. . Ashworth's evangelism is persuasive. \"It is a fantastic feeling to think that the work you've done in the lab can actually have an output in patients,\" he says, \"and I think many other people want to feel like that.\" He imagines a future in which all tumours are targeted according to their precise genetic characteristics, a vision that many researchers are now working towards. At the new Centre for Molecular Pathology, he plans to collect genetic and molecular profiles of patients as they enter trials to work out which people are most likely to benefit from the therapies being tested. He is also heavily involved in running Breakthrough Generations, a study into the genetic and environmental causes of breast cancer that has recruited 100,000 British women and has received \u00a312-million in start-up funding from the Institute of Cancer Research and the Breakthrough Breast Cancer charity. The plan is to collect detailed health information over the next 40 years to improve understanding of the causes and prevention of cancer. Today, he finds some escape in the basic biology. \"It's a relief to look at data rather than at higher-level political things,\" he says. \"What really drives me is looking at experiments. I still get very, very excited \u2014 some would say too excited \u2014 when there's a hint of a good result.\" Daniel Cressey is a reporter based in  Nature 's London office. \n                     Translational research news special \n                   \n                     Breakthrough Breast Cancer Research Centre \n                   Reprints and Permissions"},
{"file_id": "463284a", "url": "https://www.nature.com/articles/463284a", "year": 2010, "authors": [{"name": "Quirin Schiermeier"}], "parsed_as_year": "2006_or_before", "body": "Like any other field, research on climate change has some fundamental gaps, although not the ones typically claimed by sceptics. Quirin Schiermeier takes a hard look at some of the biggest problem areas. The e-mails leaked from the University of East Anglia's Climatic Research Unit (CRU) in November presented an early Christmas present to climate-change denialists. Amid the more than 1,000 messages were several controversial comments that \u2014 taken out of context \u2014 seemingly indicate that climate scientists have been hiding a mound of dirty laundry from the public. A fuller reading of the e-mails from CRU in Norwich, UK, does show a sobering amount of rude behaviour and verbal faux pas, but nothing that challenges the scientific consensus of climate change. Still, the incident provides a good opportunity to point out that \u2014 as in any active field of inquiry \u2014 there are some major gaps in the understanding of climate science. In its most recent report in 2007, the Intergovernmental Panel on Climate Change (IPCC) highlighted 54 'key uncertainties' that complicate climate science. Such a declaration of unresolved problems could hardly be called 'hidden'. And some of these \u2014 such as uncertainties in measurements of past temperatures \u2014 have received considerable discussion in the media. But other gaps in the science are less well known beyond the field's circle of specialists. Such holes do not undermine the fundamental conclusion that humans are warming the climate, which is based on the extreme rate of the twentieth-century temperature changes and the inability of climate models to simulate such warming without including the role of greenhouse-gas pollution. The uncertainties do, however, hamper efforts to plan for the future. And unlike the myths regularly trotted out by climate-change denialists (see  'Enduring climate myths' ), some of the outstanding problems may mean that future changes could be worse than currently projected. Researchers say it is difficult to talk openly about holes in understanding. \"Of course there are gaps in our knowledge about Earth's climate system and its components, and yes, nothing has been made clear enough to the public,\" says Gavin Schmidt, a climate modeller at NASA's Goddard Institute for Space Studies in New York and one of the moderators and contributors to the influential RealClimate blog. \"But this climate of suspicion we're working in is insane. It's really drowning our ability to soberly communicate gaps in our science when some people cry 'fraud' and 'misconduct' for the slightest reasons.\"   Nature   has singled out four areas \u2014 regional climate forecasts, precipitation forecasts, aerosols and palaeoclimate data \u2014 that some say deserve greater open discussion, both within scientific circles and in the public sphere.  \n                Regional climate prediction \n              The sad truth of climate science is that the most crucial information is the least reliable. To plan for the future, people need to know how their local conditions will change, not how the average global temperature will climb. Yet researchers are still struggling to develop tools to accurately forecast climate changes for the twenty-first century at the local and regional level. The basic tools used to simulate Earth's climate are general circulation models (GCMs), which represent physical processes in the global atmosphere, oceans, ice sheets and on the land's surface. Such models generally have a resolution of about 1\u20133\u00b0 in latitude and longitude \u2014 too coarse to offer much guidance to people. So climate scientists simulate regional changes by zooming in on global models \u2014 using the same equations, but solving them for a much larger number of grid points in particular locations. However, increasing the resolution in this way can lead to problems. Zooming in from GCMs bears the risk of blowing up any inherent weakness of the 'mother' model. If the model does a poor job of simulating certain atmospheric patterns, those errors will be compounded at the regional level. Most experts are therefore cautious when asked to make regional predictions. \"Our current climate models are just not up to informed decision-making at the resolution of most countries,\" says Leonard Smith, a statistician and climate analyst at the London School of Economics and Political Science. \"You need to be very circumspect about the added value of downscaling to regional impacts,\" agrees Hans von Storch, a climate modeller at the GKSS Institute for Coastal Research in Geesthacht, Germany, who has recently contributed to a regional climate assessment of the Hamburg metropolitan region. If the simulations project future changes in line with the trends already observed, von Storch has more confidence in them. But if researchers run the same model, or an ensemble of models, multiple times and the results diverge from each other or from the observed trends, he cautions, \"planners should handle them with kid gloves. Whenever possible, they'd rather wait with spending big money on adaptation projects until there is more certainty about the things to come.\" Downscaled climate models face particular uncertainty problems dealing in regions with complex topography, such as where mountains form a wall between two climatically different plains. Another potential source of error comes from projections concerning future greenhouse-gas emissions, which vary depending on assumptions about economic developments. All the problems, however, do not make regional simulations worthless, as long as their limitations are understood. They are already being used by planners at the local and national levels (see graphs, right). Simulations remain an important tool for understanding processes, such as changes in river flow, that global models just cannot resolve, says Jonathan Overpeck, a climate researcher at the University of Arizona in Tucson. Overpeck is part of a research team that is using statistical techniques to narrow down divergent model projections of how much average water flow in the Colorado River will decrease by 2050. Researchers hope that by improving how they simulate climate variables such as cloud coverage and sea surface temperatures, they will further reduce the uncertainties in regional forecasts, making them even more useful for policy-makers.  \n                Precipitation \n              Rising global temperatures over the next few decades are likely to increase evaporation and accelerate the global hydrological cycle \u2014 a change that will dry subtropical areas and increase precipitation at higher latitudes. These trends are already being observed and almost all climate models used to simulate global warming show a continuation of this general pattern 1 . Unfortunately, when it comes to precipitation, that is about all the models agree on. The different simulations used by the IPCC in its 2007 assessment offer wildly diverging pictures of snow and rainfall in the future (see graphic, right). The situation is particularly bad for winter precipitation, generally the most important in replenishing water supplies. The IPCC simulations failed to provide any robust projection of how winter precipitation will change at the end of the current century for large parts of all continents 2 . Even worse, climate models seemingly underestimate how much precipitation has changed already \u2014 further reducing confidence in their ability to project future changes. A 2007 study 3 , published too late to be included into the last IPCC report, found that precipitation changes in the twentieth century bore the clear imprint of human influence, including drying in the Northern Hemisphere tropics and subtropics. But the actual changes were larger than estimated from models \u2014 a finding that concerns researchers. \"If the models do systematically underestimate precipitation changes that would be bad news\", because the existing forecasts would already cause substantial problems, says Gabriele Hegerl, a climate-system scientist at the University of Edinburgh, UK, and a co-author on the paper. \"This is, alas, a very significant uncertainty,\" she says. Climate scientists think that a main weakness of their models is their limited ability to simulate vertical air movement, such as convection in the tropics that lifts humid air into the atmosphere. The same problem can trip up the models for areas near steep mountain ranges. The simulations may also lose accuracy because scientists do not completely understand how natural and anthropogenic aerosol particles in the atmosphere influence clouds. Data on past precipitation patterns around the globe could help modellers to solve some of these issues, but such measurements are scant in many areas. \"We really don't know natural variability that well, particularly in the tropics,\" says Hegerl. The uncertainties about future precipitation make it difficult for decision-makers to plan, particularly in arid regions such as the Sahel in Africa and southwestern North America. 'Mega-droughts' lasting several decades have struck these areas in the past and are expected to happen again. But the models in use today do a poor job of simulating such long-lasting droughts. \"That's pretty worrying,\" says Overpeck. Increasing the resolution of models will not be enough to resolve the convective processes that lead to precipitation. To forecast precipitation more accurately, researchers are trying, among other things, to improve the simulation of key climate variables such as the formation and dynamics of clouds. Furthermore, high-resolution satellite observations are increasingly being used to validate and improve model realism.  \n                Aerosols \n              Atmospheric aerosols \u2014 airborne liquid or solid particles \u2014 are a source of great uncertainty in climate science. Despite decades of intense research, scientists must still resort to using huge error bars when assessing how particles such as sulphates, black carbon, sea salt and dust affect temperature and rainfall. Overall, it is thought that aerosols cool climate by blocking sunlight, but the estimates of this effect vary by an order of magnitude, with the top end exceeding the warming power of all the carbon dioxide added to the atmosphere by humans. One of the biggest problems is lack of data. \"We don't know what's in the air,\" says Schmidt. \"This means a major uncertainty over key processes driving past and future climate.\" To measure aerosols in the sky, satellite and ground-based sensors detect the scattering and absorption of solar radiation. But researchers lack enough of this kind of data to complete a picture of aerosols across the globe. And a complex set of coordinated experiments is required to determine how aerosols alter climate processes. Some aerosols, such as black carbon, absorb sunlight and produce a warming effect that might also inhibit rainfall. Other particles such as sulphates exert a cooling influence by reflecting sunlight. The net effect of aerosol pollution on global temperature is not well established. And various studies have produced conflicting conclusions over whether global aerosol pollution is increasing or decreasing. The relationship between aerosols and clouds adds another layer of complication. Before a cloud can produce rain or snow, rain drops or ice particles must form and aerosols often serve as the nuclei for condensation. But although some aerosols enhance cloudiness, others seem to reduce it. Aerosols could also have a tremendous impact on temperatures by altering the formation and lifetime of low-level clouds, which reflect sunlight and cool the planet's surface. Scientists have yet to untangle the interplay between pollution, clouds, precipitation and temperature. However, NASA's Glory satellite, an aerosol and solar-irradiance monitoring mission scheduled for launch in October, will provide some greatly anticipated data. Still, atmospheric researchers say that ground-based sensors capable of determining the abundance and composition of aerosols in the atmosphere are needed just as much.  \n                The tree-ring controversy \n              Many of the e-mails leaked from the CRU computers came from a particular group of climate researchers who work on reconstructing temperature variations over time. The e-mails revealed them discussing some of the uncertainties in centuries worth of climate information gleaned from tree rings and other sources. Records of thermometer measurements over the past 150 years show a sharp temperature rise during recent decades that cannot be explained by any natural pattern. It is most likely to have been caused by anthropogenic greenhouse-gas emissions. But reliable thermometer records from before 1850 are scarce and researchers must find other ways to reveal earlier temperature trends. Palaeoclimatology relies on records culled from sources such as tree rings, coral reefs, lake sediments, stalagmites, glacial movements and historical accounts. As trees grow, for example, they develop annual rings whose thickness reflects temperature and rainfall. Proxies such as these provide most knowledge of past climate fluctuations, such as the Medieval Warm Period from about 800 to 1300 and the Little Ice Age, centred on the year 1700. \n               boxed-text \n             When proxy records for the Northern Hemisphere are stitched together, they show a pattern resembling a hockey stick, with temperatures rising substantially during the late twentieth century above the long-term mean conditions. This type of work was pioneered in 1998 by Michael Mann, a climate researcher then at the University of Virginia in Charlottesville, and his co-authors 4 . In a subsequent publication 5 , they concluded that the decade of the 1990s was probably the warmest decade, and 1998 the warmest year, in at least a millennium. That work figured prominently in the 2001 assessment by the IPCC. But the use and interpretation of such proxy records has generated considerable controversy. One notable critic, Stephen McIntyre, a retired Canadian mining consultant and editor of the Climate Audit blog, has spent much of the past decade challenging the work of Mann and other scientists whose e-mails were leaked. McIntyre has doggedly attacked the proxy records 6 , particularly the statistics used to analyse tree-ring data. Many scientists are tired of the criticisms, and the IPCC concluded that it is \"likely\" that the second half of the twentieth century was the warmest 50-year period in the Northern Hemisphere during the past 1,300 years. But legitimate questions remain about paleoclimate proxies, according to the IPCC 7 . Climate scientists are worried in particular about tree-ring data from a few northern sites. By examining temperature measurements from nearby, researchers know that tree growth at these locations tracked atmospheric temperatures for much of the twentieth century and then diverged from the actual temperatures during recent decades. It may be that when temperatures exceed a certain threshold, tree growth responds differently. The 'divergence' issue also made an appearance in the CRU affair. In the most frequently quoted of the CRU e-mails, the former director of the centre, Phil Jones, mentioned a 'trick' \u2014 namely using actual observations of late-twentieth-century temperatures instead of tree ring data \u2014 to 'hide the decline' in the response of trees to the warming temperatures.\" On the surface, Jones's phrasing seems damning. Indeed, a graph of Northern Hemisphere temperature produced for the World Meteorological Organization in 2000 with Jones's help fails to make clear that instrumental records from the nineteenth and twentieth centuries were spliced onto proxy data for the past millennium because of the divergence issue. The figure did, however, contain clear references to papers that discussed the divergence issue. \"They show what was, at the time, the best estimate of how temperatures evolved over time,\" says Hegerl. \"However, with hindsight, they could have been a bit clearer how this was done, given the high profile that figures like this can have.\" Aside from the issue of clarity, the decision to exclude the tree-ring records that diverge from the instrumental data makes sense, says Thomas Stocker, co-chair of the IPCC's working group on the physical basis of climate change. The tree ring divergence problem is restricted to a few high-latitude regions in the Northern Hemisphere and is not ubiquitous even there, he says. Still, the divergence issue remains a source of debate within the scientific community. \"I'm worried about what causes the divergence,\" says Hegerl. \"As long as we don't understand why they diverge, we can't be sure that they accurately represent the past.\" So improving the usefulness of proxies will require a better understanding of how different species of trees grow and respond to climate change. Another outstanding problem in proxy research is the large range of uncertainty for temperatures from before about 1500. Studies published in 2004 (ref.  8 ) and 2005 (ref.  9 ), based on a combination of proxies of different resolution, suggest that fluctuations in global temperature during the past millennium may have been larger than initially thought. However, these studies still show late twentieth century warming to be unprecedented, says von Storch. And the most recent decade was warmer still. Even with ongoing questions about the proxy data, the IPCC's key statement \u2014 that most of the warming since the mid-twentieth century is \"very likely\" to be due to human-caused increases in greenhouse-gas concentration \u2014 remains solid because it rests on multiple lines of evidence from different teams examining many aspects of the climate system, says Susan Solomon, the former co-chair of the IPCC team that produced the 2007 physical science report and a climate researcher with the US National Oceanic and Atmospheric Administration in Boulder, Colorado. \"The IPCC's team of scientists,\" she says, \"would not have said that warming is unequivocal based on a single line of evidence \u2014 even if came from Moses himself.\"   See Editorial,  \n                     page 269 \n                   . \n                     NASA Glory Mission \n                   \n                     The Intergovernmental Panel on Climate Change \n                   Reprints and Permissions"},
{"file_id": "464480a", "url": "https://www.nature.com/articles/464480a", "year": 2010, "authors": [{"name": "Heidi Ledford"}], "parsed_as_year": "2006_or_before", "body": "Questions about a laboratory assay are making Sirtris, a high-profile biotechnology company, the talking point of the ageing field. Heidi Ledford investigates. Konrad Howitz wasn't looking for a fountain of youth. As director of biochemistry at BIOMOL International in Plymouth Meeting, Pennsylvania, Howitz wanted to add new molecular assays to the company's catalogue. A protein called a sirtuin had recently been shown to lengthen lifespan in yeast 1 , and Howitz was developing methods to measure the activity of one of its mammalian forms, called SIRT1. But when Howitz and his team stumbled on the discovery that a compound called resveratrol seemed to activate SIRT1, they quickly realized the implications. Resveratrol was rumoured to be the ingredient in red wine that kept the French healthy. In 2003, Howitz shared his findings with David Sinclair, a molecular biologist at Harvard Medical School in Boston, Massachusetts, who had studied sirtuins extensively. Sinclair immediately saw commercial potential for resveratrol as a human anti-ageing drug. Fearing corporate espionage, Sinclair code-named the compound 'R'. A year later, 'R' became the centrepiece of Sirtris, a company Sinclair co-founded in Cambridge, Massachusetts. Four years after that, Sirtris became a biotechnology success story when London-based pharmaceutical giant GlaxoSmithKline (GSK) purchased it for US$720 million. Yet behind the scenes, researchers have been voicing concerns about some of the research that forms the basis for the company. Most recently, questions have resurfaced 2  over whether the interpretation of Howitz's original SIRT1 assay 3  was flawed. It is a technical debate with big implications, not just for Sirtris but also for the rapidly expanding research community that is now studying sirtuins. Some researchers are questioning whether resveratrol, and other compounds like it that Sirtris is now testing in clinical trials, really does activate sirtuins. Until the mechanism is clear, they are cautious about pursuing drugs that might have unanticipated biological targets and effects. \"It is an exciting time in the ageing field,\" says Brian Kennedy, a molecular biologist at the University of Washington in Seattle. \"But this issue has had a polarizing effect. It needs to be resolved.\" Helped in no small part by Sirtris's public-relations efforts, sirtuins have been trumpeted in the press as the key to boosting human lifespan. The compounds have a compelling biological narrative: Sinclair and others have proposed that resveratrol and other SIRT1 activators imitate the effects of 'caloric restriction', a drastic reduction in calories that lengthens lifespan in some animals. Compounds that activate sirtuins might therefore have all the benefits of caloric restriction without the starvation.  \n                Seeing the light \n              Those were the exciting implications when, in 2003, Howitz, Sinclair and their colleagues showed that sirtuin activators could extend yeast lifespan by 70%. Their paper, published in  Nature 3 , also showed results from Howitz's assay. The sirtuins are members of a class of enzyme called deacetylases, which strip acetyl groups from proteins. To measure this activity, Howitz designed an acetylated protein fragment bearing a chemical tag. When activated, SIRT1 deacetylated this peptide substrate and the tag started to fluoresce. The greater the activity of SIRT1, the greater the fluorescence \u2014 and in the paper, resveratrol and other chemicals thought to activate SIRT1 generated a mighty glow. Doubts about the assay first surfaced publicly two years later. Two papers 4 , 5  showed that resveratrol boosted the activity of SIRT1 only when its peptide substrate contained the fluorescent tag: no tag, no activity (see  graph ). The papers caused a stir in the field, but failed to register among investors or the public. \"The sirtuin stuff has just sort of been a runaway train,\" says Kennedy, who was lead author on one of the papers 4 . \"We might have caused the train to wobble a little, but it kept barrelling down the tracks.\"  Click here for larger image The assay controversy wasn't the only stone on the tracks. Some labs have been unable to consistently reproduce Sinclair's life-extending results in model organisms such as fruitflies and nematodes 6 , and debate has simmered over whether SIRT1 activation truly mimics caloric restriction. For Sirtris, however, the arguments have little practical bearing \u2014 the company hopes to market its drugs as a way to stave off diabetes and other diseases associated with ageing, rather than a way to extend lives. And by 2006, Sinclair and his colleagues had shown that resveratrol improved the health of mice fed extremely high-fat diets 7 . Then, in a high-profile  Nature   paper published at the end of 2007, Sirtris unveiled a set of compounds that its researchers said were 1,000 times more potent than resveratrol 8 . In April the following year, GSK announced that it planned to purchase the tiny biotech for $22.50 per share, nearly double its $12 trading price. In the paper, the team used mass spectrometry instead of the fluorescence assay to show that resveratrol and the more potent newcomers activated SIRT1, allaying some researchers' concerns. \"A lot of people read 'mass spec', considered that a different assay, and moved on,\" says Joseph Baur, a molecular biologist at the University of Pennsylvania in Philadelphia and a former member of Sinclair's lab. Yet closer inspection showed that the peptides used in the mass spec experiments still carried a fluorescent tag, hinting that this assay, too, worked only when the tag was present. \"That should have thrown up a red flag to one of the reviewers, but apparently it did not,\" says Ronen Marmorstein, a structural biologist at the Wistar Institute in Philadelphia, Pennsylvania. Sinclair says that they used peptides originally prepared with a fluorescence screen in mind. When they switched to mass spectrometry, they simply used the substrates they had on hand. The debate blew up again this year when a team led by Pfizer researchers in Groton, Connecticut, published a paper in the  Journal of Biological Chemistry 2 . The group claimed that several of the new Sirtris compounds did not activate SIRT1 without the fluorescent tag. Worse, they presented evidence that the compounds were inhibiting a slew of other proteins \u2014 and some of the mice taking high doses of the drugs died. The paper concluded that the Sirtris compounds and resveratrol were pharmacological dead-ends owing to \"their highly promiscuous profiles\". Biotech blogs have been abuzz about Sirtris ever since. What did GSK know about these problems before it bought the company? Do the drugs work as claimed? Do they confer metabolic benefits at all? Most researchers seem to agree with Sirtris that the SIRT1-activating compounds do have beneficial effects in mice. Rafael de Cabo at the National Institute on Aging in Baltimore, Maryland, and a co-author on the 2006  Nature   paper, says that he has safely tested one of the compounds, SRT1720, in more than 1,000 mice, some of which received the compound for two years. At least three other groups have published data showing that SRT1720 is beneficial and non-toxic in mice 9 , 10 , 11 . But the debate about how the compounds act rages on. Some interpret the Pfizer results to mean that the assay had yielded an artefact, and the compounds, resveratrol included, may not act on SIRT1 at all. \"If this drug is just binding to this fluorigenic group, then why would it really talk to SIRT1  in vivo ?\" asks Kennedy. Instead, the compounds might be acting directly on the tag, and they could be bestowing beneficial health effects by acting on other, unknown, targets. Sinclair, Howitz and their colleagues at Sirtris acknowledge that the assay does not work with some substrates in the absence of a fluorescent tag. But they stand by their argument that the drugs are acting on SIRT1, saying that when the peptide is attached to the fluorophore, it mimics a natural SIRT1 substrate found in the cell. \"To my mind, these results are structural clues,\" says Howitz, who says that the biochemical data in the Pfizer group's paper are solid. Howitz, whose company is now called ENZO Life Sciences, says that the assay was still worthwhile because it yielded drug candidates that are looking promising in follow-up work. If the assay was useless, Sinclair points out, then it would have been highly unlikely to have repeatedly identified compounds that all identically improve the health of mice. \"What's going on? These compounds just happen to hit random targets in the cell and most targets just happen to lower blood sugar, increase endurance and boost mitochondrial function?\" asks Sinclair. \"I wish finding drugs was that easy.\" In fact, several companies, including GSK, dropped their pursuit of SIRT1 activators after the results of their high-throughput fluorescence screens failed to stand up to closer scrutiny. A former GSK employee says that the firm's internal hunt for SIRT1 activators was killed well before the Sirtris deal. The same fate met putative SIRT1 activators fished out of a screen at Elixir Pharmaceuticals, a company founded two years before Sirtris, says chief scientific officer Peter DiStefano. Whereas Sirtris's fortunes soared, Elixir is now down to five employees at its office in Cambridge, Massachusetts. The same fate might have met Sirtris's programme, says Baur, but for the compelling animal data from Sinclair's lab and from the company itself. Ad Rawcliffe, head of worldwide business development at GSK, said in a statement that the company was aware of the controversies surrounding the fluorescence-based assays before it purchased Sirtris. GSK fully evaluated those risks, he says, and remained \"confident that the consistency of the activity in cell-based and animal studies is driven through a SIRT1-dependent mechanism\". Not everyone shares that confidence, but few are willing to dismiss sirtuin activators until they see the outcomes of Sirtris's clinical trials. These include phase IIa trials against type 2 diabetes, inflammation and cardiovascular disease. Meanwhile, the field is intent on working out exactly how resveratrol and these compounds act, something that is likely to be important for drug development, says Marmorstein. \"If something goes wrong with the compound, you can't modify it based on an understanding of sirtuin function if you don't know that is the target.\" Several academic labs hope to tackle the assay question head on. Anthony Sauve, an enzymologist at Weill Cornell Medical School in New York and a member of Sirtris's scientific advisory board, is considering treating cells with SIRT1 activators to look for 'native' substrates that are deacetylated by SIRT1. And biologist Leonard Guarente from the Massachusetts Institute of Technology in Cambridge, whom Sinclair calls the \"grandfather\" of the sirtuin field and who is co-chair of Sirtris's scientific advisory board, says he is so anxious to have the matter resolved that his lab may begin a hunt for other proteins that may be required to activate SIRT1 alongside resveratrol. At Sirtris, the labs are just as intent on clarifying how the drugs work, says company president George Vlasuk. \"I care what the mechanism is,\" he says. \"I think everybody cares what the mechanism is.\"   See Editorial,  \n                     page 465 \n                   , and Ageing Insight,  \n                     page 503 \n                   . \n                     Web Focus: Determining Lifespan \n                   \n                     Sirtris Pharmaceuticals \n                   \n                     National Institute on Ageing \n                   Reprints and Permissions"},
{"file_id": "463288a", "url": "https://www.nature.com/articles/463288a", "year": 2010, "authors": [{"name": "Roberta Kwok"}], "parsed_as_year": "2006_or_before", "body": "Can engineering approaches tame the complexity of living systems? Roberta Kwok explores five challenges for the field and how they might be resolved. To read some accounts of synthetic biology, the ability to manipulate life seems restricted only by the imagination. Researchers might soon program cells to produce vast quantities of biofuel from renewable sources, or to sense the presence of toxins, or to release precise quantities of insulin as a body needs it \u2014 all visions inspired by the idea that biologists can extend genetic engineering to be more like the engineering of any hardware. The formula: characterize the genetic sequences that perform needed functions, the 'parts', combine the parts into devices to achieve more complex functions, then insert the devices into cells. As all life is based on roughly the same genetic code, synthetic biology could provide a toolbox of reusable genetic components \u2014 biological versions of transistors and switches \u2014 to be plugged into circuits at will. Such analogies don't capture the daunting knowledge gap when it comes to how life works, however. \"There are very few molecular operations that you understand in the way that you understand a wrench or a screwdriver or a transistor,\" says Rob Carlson, a principal at the engineering, consulting and design company Biodesic in Seattle, Washington. And the difficulties multiply as the networks get larger, limiting the ability to design more complex systems. A 2009 review 1  showed that although the number of published synthetic biological circuits has risen over the past few years, the complexity of those circuits \u2014 or the number of regulatory parts they use \u2014 has begun to flatten out. Challenges loom at every step in the process, from the characterization of parts to the design and construction of systems. \"There's a lot of biology that gets in the way of the engineering,\" says Christina Agapakis, a graduate student doing synthetic-biology research at Harvard Medical School in Boston, Massachusetts. But difficult biology is not enough to deter the field's practitioners, who are already addressing the five key challenges.  \n                Many of the parts are undefined \n              A biological part can be anything from a DNA sequence that encodes a specific protein to a promoter, a sequence that facilitates the expression of a gene. The problem is that many parts have not been characterized well. They haven't always been tested to show what they do, and even when they have, their performance can change with different cell types or under different laboratory conditions. The Registry of Standard Biological Parts, which is housed at the Massachusetts Institute of Technology in Cambridge, for example, has more than 5,000 parts available to order, but does not guarantee their quality, says director Randy Rettberg. Most have been sent in by undergraduates participating in the International Genetically Engineered Machine (iGEM) competition, an annual event that started in 2004. In it, students use parts from a 'kit' or develop new ones to design a synthetic biological system. But many competitors do not have the time to characterize the parts thoroughly. While trying to optimize lactose fermentation in microbes, an iGEM team from the University of Pavia in Italy tested several promoters from the registry by placing them in  Escherichia coli , a standard laboratory bacterium. Most of the promoters tested by the team worked, but some had little documentation, and one showed no activity. About 1,500 registry parts have been confirmed as working by someone other than the person who deposited them and 50 have reportedly failed, says Rettberg. 'Issues' have been reported for roughly another 200 parts, and it is unclear how many of the remaining parts have been tested. The registry has been stepping up efforts to improve the quality by curating the collection, encouraging contributors to include documentation on part function and performance, and sequencing the DNA of samples of parts to make sure they match their descriptions, says Rettberg. Meanwhile, synthetic biologists Adam Arkin and Jay Keasling at the University of California, Berkeley, and Drew Endy at Stanford University in Stanford, California are launching a new effort, tentatively called BIOFAB, to professionally develop and characterize new and existing parts. Late last year, the team was awarded US$1.4 million by the National Science Foundation and is hiring staff, says Arkin. Endy, moreover, has proposed methods to reduce some of the variability in measurements from different labs. By measuring promoter activity relative to a reference promoter, rather than looking at absolute activity, Endy's team found that it could eliminate half the variation arising from experimental conditions and instruments 2 . Measurements are tricky to standardize, however. In mammalian cells, for example, genes introduced into a cell integrate unpredictably into the cell's genome, and neighbouring regions often affect expression, says Martin Fussenegger, a synthetic biologist at the Swiss Federal Institute of Technology (ETH) Zurich. \"This is the type of complexity that is very difficult to capture by standardized characterization,\" he says.  \n                The circuitry is unpredictable \n              Even if the function of each part is known, the parts may not work as expected when put together, says Keasling. Synthetic biologists are often caught in a laborious process of trial-and-error, unlike the more predictable design procedures found in other modern engineering disciplines. \"We are still like the Wright Brothers, putting pieces of wood and paper together,\" says Luis Serrano, a systems biologist at the Centre for Genomic Regulation in Barcelona, Spain. \"You fly one thing and it crashes. You try another thing and maybe it flies a bit better.\" Bioengineer Jim Collins and his colleagues at Boston University in Massachusetts crashed a lot when implementing a system called a toggle switch in yeast. His lab built one roughly ten years ago in  E. coli 3 : the team wanted to make cells express one gene \u2014 call it gene A \u2014 and then prompt them with a chemical signal to turn off A and express another gene, B. But the cells refused to express B continuously; they always shifted back to expressing A. The problem, says Collins, was that the promoters controlling the two genes were not balanced, so A overpowered B. It took about three years of tweaking the system to make it work, he says. Computer modelling could help reduce this guesswork. In a 2009 study 4 , Collins and his colleagues created several slightly different versions of two promoters. They used one version of each to create a genetic timer, a system that would cause cells to switch from expressing one gene to another after a certain lag time. They then tested the timer, fed the results back into a computational model and predicted how timers built from other versions would behave. Using such modelling techniques, researchers could optimize computationally rather than test every version of a network, says Collins. But designs might not have to work perfectly: imperfect ones can be refined using a process called directed evolution, says Frances Arnold, a chemical engineer at the California Institute of Technology in Pasadena. Directed evolution involves mutating DNA sequences, screening their performance, selecting the best candidates and repeating the process until the system is optimized. Arnold's lab, for instance, is using the technique to evolve enzymes involved in biofuel production.  \n                The complexity is unwieldy \n              As circuits get larger, the process of constructing and testing them becomes more daunting. A system developed by Keasling's team 5 , which uses about a dozen genes to produce a precursor of the antimalarial compound artemisinin in microbes, is perhaps the field's most cited success story. Keasling estimates that it has taken roughly 150 person-years of work including uncovering genes involved in the pathway and developing or refining parts to control their expression. For example, the researchers had to test many part variants before they found a configuration that sufficiently increased production of an enzyme needed to consume a toxic intermediate molecule. \"People don't even think about tackling those projects because it takes too much time and money,\" says Reshma Shetty, co-founder of the start-up firm Ginkgo BioWorks in Boston, Massachusetts. To relieve similar bottlenecks, Ginkgo is developing an automated process to combine genetic parts. The parts have pre-defined flanking sequences, dictated by a set of rules called the BioBrick standard, and can be assembled by robots. At Berkeley, synthetic biologist J. Christopher Anderson and his colleagues are developing a system that lets bacteria do the work. Engineered  E. coli   cells, called 'assembler' cells, are being equipped with enzymes that can cut and stitch together DNA parts. Other  E. coli   cells, engineered to act as 'selection' cells, will sort out the completed products from the leftover parts. The team plans to use virus-like particles called phagemids to ferry the DNA from the assembler to the selection cells. Anderson says that the system could shorten the time needed for one BioBrick assembly stage from two days to three hours.  \n                Many parts are incompatible \n              Once constructed and placed into cells, synthetic genetic circuits can have unintended effects on their host. Chris Voigt, a synthetic biologist at the University of California, San Francisco, ran into this problem while he was a postdoc at Berkeley in 2003. Voigt had assembled genetic parts, mainly from the bacterium  Bacillus subtilis , into a switch system that was supposed to turn on expression of certain genes in response to a chemical stimulus. He wanted to study the system independently of  B. subtilis ' other genetic networks, so he put the circuit into  E. coli   \u2014 but it didn't work. \"You looked under the microscope and the cells were sick,\" says Voigt. \"One day it would do one thing, and another day it would do another thing.\" He eventually saw in the literature that one of the circuit's parts dramatically disrupted  E. coli 's natural gene expression. \"There was nothing wrong with the design of the circuit,\" he says. \"It was just that one part was not compatible.\" Synthetic biologist Lingchong You at Duke University in Durham, North Carolina, and his colleagues found that even a simple circuit, comprising a foreign gene that promoted its own expression, could trigger complex behaviour in host cells 6 . When activated in  E. coli , the circuit slowed down the cells' growth, which in turn slowed dilution of the gene's protein product. This led to a phenomenon called bistability: some cells expressed the gene, whereas others did not. To lessen unexpected interactions, researchers are developing 'orthogonal' systems that operate independently of the cell's natural machinery. Synthetic biologist Jason Chin of the Medical Research Council Laboratory of Molecular Biology in Cambridge, UK, and his colleagues have created a protein-production system in  E. coli   that is separate from the cell's built-in system 7 . To transcribe DNA into RNA, the team uses a polymerase enzyme that recognizes genes only if they have a specific promoter sequence that is not present in the cell's natural genes. Similarly, the system's orthogonal 'O-ribosomes', which translate RNA into protein, can read only 'O-mRNA' that contains a specific sequence, and O-mRNA is unreadable by natural ribosomes. A parallel system gives biologists the freedom to tweak components without disrupting the machinery needed for the cell to survive, says Chin. For example, his team has stripped down the DNA sequence encoding part of the O-ribosome to speed up production. This allows the cell to boot up protein manufacture more quickly, he says. Another solution is to physically isolate the synthetic network from the rest of the cell. Wendell Lim, a synthetic biologist at the University of California, San Francisco, is experimenting with the creation of membrane-bound compartments that would insulate the genetic circuits. Lim's team is working in yeast, but similar principles could be applied to bacterial cells, he says.  \n                Variability crashes the system \n              Synthetic biologists must also ensure that circuits function reliably. Molecular activities inside cells are prone to random fluctuations, or noise. Variation in growth conditions can also affect behaviour. And over the long term, randomly arising genetic mutations can kill a circuit's function altogether. Michael Elowitz, a synthetic biologist at the California Institute of Technology in Pasadena, observed the cell's capacity for randomness about ten years ago when his team built a genetic oscillator 8 . The system contained three genes whose interactions caused the production of a fluorescent protein to go up and down, making cells blink on and off. However, not all cells responded the same way. Some were brighter, and some were dimmer; some blinked faster, others slower; and some cells skipped a cycle altogether. Elowitz says that the differences might have arisen for multiple reasons. A cell can express genes in bursts rather than a steady stream. Cells also may contain varying amounts of mRNA and protein-production machinery, such as polymerase enzymes and ribosomes. Furthermore, the number of copies of the genetic circuit in a cell can fluctuate over time. Jeff Hasty, a synthetic biologist at the University of California, San Diego, and his colleagues described an oscillator with more consistent behaviour 9  in 2008. Using a different circuit design and microfluidic devices that allowed fine control of growth conditions, the team made nearly every monitored cell blink at the same rate \u2014 though not in sync. And in this issue of  Nature   (see  page 326 ) 10 , Hasty's team reports the ability to synchronize the blinking by relying on cell\u2013cell communication. But Hasty says that rather than trying to eliminate noise, researchers could use it to their advantage. He notes that in physics, noise can sometimes make a signal easier to detect. \"I don't think you can beat it, so I think you ought to try to use it,\" says Hasty. For example, noise could allow some cells to respond differently to the environment from others, enabling the population to hedge its bets, says Elowitz. Meanwhile, geneticist George Church at Harvard Medical School in Boston, Massachusetts, is exploring ways to make a bacterial strain more stable. Church says that this might be achieved by introducing more accurate DNA-replication machinery, changing genome sites to make them less prone to mutation and putting extra copies of the genome into cells. Although stability may not be a serious issue for simple systems, it will become important as more components are assembled, he says.  \n                Time to deliver? \n              Despite the challenges, synthetic biologists have made progress. Researchers have recently developed devices that allow  E. coli   to count events such as the number of times they have divided and to detect light and dark edges. And some systems have advanced from bacteria to more complex cells. The field is also gaining legitimacy, with a new synthetic-biology centre at Imperial College London and a programme at Harvard University's recently launched Wyss Institute for Biologically Inspired Engineering in Boston. The time has come for synthetic biologists to develop more real-world applications, says Fussenegger. \"The field has had its hype phase,\" he says. \"Now it needs to deliver.\" Keasling's artemisinin precursor system is approaching commercial reality, with Paris-based pharmaceutical company Sanofi-Aventis aiming to have the product available at an industrial scale by 2012. And several companies are pursuing biofuel production via engineered microbes. But most applications will take time. As the cost of DNA synthesis continues to drop and more people begin to tinker with biological parts, the field could progress faster, says Carlson. \"It's a question of whether the complexity of biology yields to that kind of an effort.\"   See Editorial,  \n                     page 269 \n                   . Roberta Kwok is a freelance writer in the San Francisco Bay Area. \n                     biotechnology@nature.com \n                   \n                     Registry of Standard Biological Parts \n                   \n                     International Genetically Engineered Machine competition \n                   \n                     Ginkgo BioWorks \n                   Reprints and Permissions"},
{"file_id": "463150a", "url": "https://www.nature.com/articles/463150a", "year": 2010, "authors": [{"name": "Brendan Maher"}], "parsed_as_year": "2006_or_before", "body": "Richard Besser led the United States' top public-health agency as swine flu broke out on its doorstep. And his communication shaped the early days of a pandemic, finds Brendan Maher. Swine flu officially became a national emergency in the United States on a Friday in late October, when US President Barack Obama signed an order giving health facilities extra power to implement crisis operations. The next morning, the daily news programme  Good Morning America   turned for analysis to its newest health editor, Richard Besser, whose two-metre frame was folded uncomfortably into a swivel chair in the studio's New York newsroom. \"This is an extremely challenging communication issue for the government,\" Besser was saying, explaining why the White House was calling an emergency, but trying not to sound too alarmist. \"How do you convey the fact that this is serious \u2026 but it's not a 1918 pandemic?\" Few people are as qualified to comment on this challenge as Besser. Nine months earlier he had been appointed acting director of the US Centers for Disease Control and Prevention (CDC) in Atlanta, Georgia, widely considered the world's pre-eminent public-health agency but one beleaguered with morale issues and a strained budget. It and the broader international health community had been gearing up for a potentially apocalyptic flu pandemic, one that would require a rapid global health response and candid communication to engender trust, not panic. In April, when reports started rolling in that a new influenza virus was infecting people in Mexico and the United States, that pandemic seemed to be emerging on the CDC's doorstep and on Besser's watch, even though he had been put in charge largely as a stop-gap in case of emergency. Many public-health experts now say that they are glad Besser was there when the emergency struck. He has received little attention for his role, which lasted just a few weeks into the pandemic. Yet experts credit him \u2014 and particularly his decision-making and calm and upfront communication \u2014 with helping the agency through the first perilous days of the outbreak and setting the tone for a rapid and transparent response from the international public-health community. \"I was pretty impressed with his style and the way he communicated uncertainty,\" says Jon Andrus, deputy director for the Pan American Health Organization, a regional office of the World Health Organization. He left you with a sense \"that this guy's honest\", Andrus says. The CDC has not been faultless: it stumbled on delivering guidance for school closures, and since Besser's departure in June, it has been criticized for over-promising and under-delivering on a vaccine. But Besser did so well with the media that the media invited him to join its ranks. He started at ABC News, which produces the  Good Morning America   programme, in September, and says he hopes to use his roles as expert and reporter to promote public-health issues. It is hard to measure the effect that the early response and communication in the pandemic had on people's health. Besser can't take responsibility for the mildness of the pandemic thus far, which is due to the nature of the H1N1 virus. But experts, who are already looking to the next wave of this pandemic and any signs of the next pandemic, say that the episode has tested their preparedness plans and identified the spots to patch up. And at least some evidence suggests that communication is a vital part of those plans. Besser points to a poll commissioned by the CDC in late April showing that most of the US public had a fairly clear understanding of what swine flu was and how to avoid catching it. \"That struck me,\" he says. \"that communication can be a very powerful tool.\" The CDC first acknowledged the existence of swine flu in its  Morbidity and Mortality Weekly Report   on Tuesday 21 April. The dispatch described how, earlier that month, the CDC had received samples from two children in California who had come down with illness from a flu subtype the local doctors could not identify. CDC researchers determined that it was a virus of swine origin. Its substantial difference from seasonal H1N1 and the fact that neither child had had recent contact with pigs led the authors to suggest that this might be a new virus with the ability to spread from person to person ( M. Ginsberg  et al. MMWR Morb. Mortal. Wkly Rep.    58,   1\u20133; 2009 ). By the next day, the CDC had learned of similar cases in California and Texas \u2014 and, more alarmingly, farther afield. \"For about two weeks at that time we'd been hearing about unexplained respiratory outbreaks in Mexico,\" says Lyn Finelli who, as the CDC's chief of influenza surveillance, presented some of the results at the agency's weekly pandemic briefing, which Besser attended. The virus in Mexico seemed to be killing a large number of people who had been hospitalized, and it was clear that the CDC had to find out quickly whether the two countries' cases were related. The agency already had a plan for dealing with a potential pandemic, and Besser decided immediately to activate its Emergency Operations Center at its lowest level \u2014 level three \u2014 to help facilitate communication between the two countries.  \n                Powers of persuasion \n              On Thursday, he asked Anne Schuchat, director of the National Center for Immunization and Respiratory Diseases at the CDC, and Nancy Cox, the head of the influenza division, to deliver a press briefing on what were now seven confirmed cases in the United States, but no deaths. \"So far this is not looking like very, very severe influenza,\" Schuchat said at the briefing. But by now, most signs were pointing to a connection between the cases in the United States and Mexico. Besser pushed the Emergency Operations Center up to level one, its highest. He also e-mailed Dora Hughes, his contact at the Department of Health and Human Services, which oversees the CDC. He needed to get the message, through her, to the top layers of government. \"We have a situation that I'm very concerned about,\" he wrote. It took some convincing, Besser says, but that evening he was on a call with Hughes and Laura Petrou, the department's chief of staff. From the CDC, Schuchat and Phil Navin, director of the emergency-operations division, were on the line. As a thunderstorm raged in Atlanta, they broke down the information available. Besser says he knew he had to be clear that this was scary. When Petrou asked him to tell her how concerned he was, Besser remembers saying, \"eight\". \"She asked, 'Eight?' I said, 'Yeah. Eight.'\" After the call was over, Navin commented: \"I would have said six!\" Navin now concedes that Besser was probably right. Even as they were on the call, Cox was leaving a message on Schuchat's phone. The lab results were in, confirming that the cases in Mexico were caused by the same swine flu virus as the US ones. The next morning, Friday 24 April, the World Health Organization activated its emergency response room and Besser prepared to deliver a briefing from the CDC. Besser has had experience in briefing people in emergencies. After receiving his medical degree from the University of Pennsylvania in 1986, he trained as a paediatrician, and honed his skill at reassuring fraught parents. An interest in public health brought him into the CDC's Epidemic Intelligence Service in 1991 and, except for a brief period doing paediatrics at the University of California, San Diego, he has been at the CDC in various roles ever since. In 2005, he took over as head of the Coordinating Office for Terrorism Preparedness & Emergency Response. It was a time bookended by excitement. He arrived in the office the day after Hurricane Katrina started pounding the southeast coast of the United States. \"He had a truly unique leadership style,\" says Navin, who worked closely with him during the response operation. \"People had confidence in him and he had confidence in them.\" And in January 2009, Besser was in the midst of a ground war in Gaza doing crisis leadership training when the Obama transition team called asking if he would consider running the CDC. He was told that he would not be considered for the permanent job; the administration wanted someone from outside the agency. Even so, Besser agreed. \"I love a leadership challenge,\" he says. Part of the test was to boost morale in the agency after the unpopular restructuring instituted by Besser's predecessor, Julie Gerberding. \"It was hard to describe \u2014 almost a sea change \u2014 when Rich became acting director,\" says David Sencer, who headed the CDC from 1966 to 1977. \"People were much more open and willing to question decisions, which had not been the case before.\"  \n                The circumstances change \n              In his office at ABC News in New York, Besser talks about the principles he looked to when talking about the H1N1 pandemic. He refers to a CDC pamphlet on crisis and emergency risk communication with the subtitle: 'Be First, Be Right, Be Credible'. Credibility was provided in large measure by Besser's relaxed and telegenic persona. But, he says, \"there's an inherent challenge between being first and being right\". The agency was working with incomplete information: in particular, data on the severity of the virus were changing rapidly. When the virus spread, would it kill tens \u2014 or millions? In his first press briefing that Friday morning in April, Besser tackled the problem head on, admitting that much was unknown. \"At the early stages of an outbreak, there's much uncertainty, and probably more than everyone would like.\" He explained that the agency's guidelines would probably change, and said: \"It's very likely that this will be more of a marathon than a sprint.\" \"That statement stands out as stunningly good,\" says Peter Sandman, a risk-communications consultant and formerly a social scientist at Rutgers University in New Jersey. Although authorities are often tempted to over-reassure the public, Sandman says, Besser gave a clear statement of concern and an indication that the agency would be searching for answers. It was a message he repeated at almost all his briefings for the next two weeks. Some public-health experts contrast the CDC's recent response with that during the last swine-flu outbreak, which started in early 1976 at Fort Dix, a military base in New Jersey. Under Sencer's direction, the CDC launched a vaccination campaign that inoculated more than 40 million Americans, yet the virus never took hold, the vaccine was blamed for hundreds of cases of the neurological condition Guillain\u2013Barre syndrome and the public-health establishment was demoralized. Harvey Fineberg, president of the Institute of Medicine and co-author of a book about the episode, says that it was a lesson in how to deal with uncertainty. \"A fundamental strategic lesson was not to pre-decide what you will be able to decide with more information later,\" he says. \"The second big problem was a failure to ask, 'What information could we learn that would lead us to change course?'\" The third lesson, he says, was to work with the media, to maintain a consistent, honest message. Besser and the CDC weren't the only ones practising good communication. The day after Besser's briefing, the World Health Organization declared a public-health emergency, and gave the first of what were to become daily briefings on the virus. Most health officials also heap credit on Mexico for reporting its first cases early, despite the economic hit it took from closed businesses and lost tourism. \"It was a level of transparency and communication that was exemplary,\" says Cox. Julio Frenk, dean of the Harvard School of Public Health in Boston, Massachusetts, and former health minister for Mexico, credits decades of investment in pandemic surveillance and communication internationally. \"You don't start building communication lines and trust once the outbreak occurs,\" he says. By 27 April more than 60 flu cases had been confirmed in the United States and Mexico. By that point Besser had been to Washington DC to brief the White House and appeared on numerous television news programmes. \"I was doing all the morning shows, a press conference at noon and then a fair number of the evening shows.\" The goal, he says, was to \"tell everything we knew, everything we didn't know and what we were doing to get the answers\", Besser says.  \n                One step at a time \n              Not every step was sure-footed. On 28 April Besser approved CDC recommendations that schools \u2014 hubs of flu transmission \u2014 should close if one student or staff member came down with confirmed flu, and stay closed for 14 days. \"Well, that's science. You can shed virus for a week. If you really want to be certain, two weeks,\" he says. Although the costs and benefits of school closures had been actively debated, Besser wanted to take an aggressive approach. But he didn't fully appreciate the political ramifications. People at the education department, for example, weren't happy that they hadn't been consulted. On 1 May the issue was discussed when Besser was called into White House Chief of Staff Rahm Emanuel's office with others including Kathleen Sebelius, just confirmed as the head of the Department of Health and Human Services. Emanuel wanted to redraft the guidelines; Besser was uncomfortable with changing what should be science-based recommendations. In the end, Besser was handed guidelines with a modest revision \u2014 closure for one week followed by reassessment \u2014 and asked if the science supported them. From what they knew, it did. \"It was extremely gratifying to see that they wanted to ensure that science was supporting the policy,\" Besser says. Over the next week, as it became apparent the flu was milder than expected, the recommendations were revised, and local health officials struggled to keep up with what they should do. (Epidemiologists have yet to determine whether the guidelines affected the virus's spread.) Observers also cite communication missteps around the vaccine. Health officials promised some 160 million doses in the summer, but only a fraction of those had materialized by October. The reason was that the H1N1 virus grew slowly in the chicken eggs required to produce the vaccine \u2014 exactly the type of uncertainties that Besser and others had warned about. Nevertheless, Mark Nichter, a medical anthropologist at the University of Arizona in Tucson who looks at responses to pandemics, says that the problem dented public trust in the agency and made further advice hard to swallow. \"What are these elderly people thinking, or mothers with kids in daycare, with the CDC telling us how important vaccines are, then saying that they're not available?\" As fears about the virus abated, the frantic sprint of the first few days stretched out into the marathon that Besser had predicted. Case counts climbed; contracts for vaccine production were signed; and attention turned to the Southern Hemisphere's winter flu season. Everything seemed to be going as well as could be expected, at least according to pandemic plans. Many people were shocked when it was announced on 15 May that Tom Frieden, then New York City health commissioner, would replace Besser the following month. \"It was something like a punch in the stomach,\" says Navin, who had worked with Besser for more than four years. Besser, who had known it was coming, started casting about for other jobs. The long shots, he says, were the two television networks who had contacted him during the outbreak, impressed by his performance on screen. He started at ABC just as the number of autumn swine-flu cases was increasing in the Northern Hemisphere. Besser's audience is now smaller:  Good Morning America   reaches five million \u2014 during CDC press conferences he was playing on every news broadcast in the country. But the atmosphere suits him. \"I like working on a setting of crisis,\" he says. \"The news business is a constant crisis.\" As for flu, Besser still projects uncertainty. At that Saturday morning broadcast in October, he warned that the worst may be yet to come. \"With a pandemic they come in waves,\" he said, adding that \"there likely will be further waves, maybe this spring, maybe into next year\". But by that point in his two-minute-long slot, Besser had already delivered his message. Get the vaccine when it arrives, he said. \"Do those things to protect your health that you can. But this is not a flu like 1918. This is \u2014 the flu.\"   See Editorial,  \n                     page 135 \n                   . \n                     Swine Flu Special \n                   \n                     CDC Press Briefings since the outbreak \n                   \n                     ABC News Swine Flu Center \n                   Reprints and Permissions"},
{"file_id": "463154a", "url": "https://www.nature.com/articles/463154a", "year": 2010, "authors": [{"name": "Erika Check Hayden"}], "parsed_as_year": "2006_or_before", "body": "An increase in premature births means that more babies are at risk of neurological damage. Erika Check Hayden talks with researchers who are developing ways to help these children. Julia lies with her eyes closed in an incubator, twitching her tiny limbs in a quiet, sedated sleep. Like the other babies in this intensive-care unit, she is surrounded by a phalanx of machinery. But unlike her nursery mates, this baby isn't wrapped in a warm blanket. Her doctors at the Children's Hospital of the University of California, San Francisco (UCSF) have left most of her days-old body bare and placed her on a blue mat that is cooling her to a hypothermic 33.5 \u00b0C. Gauze wrapping holds a network of electrodes against her skull. The electrodes are sending a stream of signals to a nearby monitor, which is being watched carefully by David Rowitch. Like a growing number of babies in the United States, Julia (not her real name) is at risk of permanent brain damage. The trend is driven by increasing rates of preterm births coupled with medical advances that allow the survival of very premature babies \u2014 and also full-term babies such as Julia, whose births were not straightforward. Although these advances have focused on babies' hearts and lungs, they have largely ignored the newborn brain. Rowitch, a neonatologist, is one of a handful of doctors around the world who hope to reverse this trend by using advances in basic neuroscience to develop treatments for injured newborn brains. He and Donna Ferriero, chief of paediatric neurology at the hospital, founded the university's Newborn Brain Research Institute in 2006. Two years later they created one of the nation's first neurointensive-care nurseries, where Julia is now sleeping. They and other scientists are pushing to get treatments into clinics as soon as possible to make up for what they call years of inadequate funding for research into brain damage in babies. \"It is frustrating to see the lack of progress that has been made over the course of the past 20 years,\" Rowitch says. \"There has really been very little in the way of any meaningful therapy.\" A potential first step is the experimental cooling treatment that Julia is undergoing. Doctors throughout the San Francisco Bay area, and some even further away, identify possible candidates for this intervention as they check newborns for seizures or other signs of lack of oxygen around the time of birth. Within hours, the child is transferred to UCSF's neurological nursery and nestled on a refrigerated mat. For the next three days, nurses and doctors monitor the patient's brain activity on an electroencephalogram, watching for signs of seizures. A video camera records the baby's fitful sleep. Then, the newborn is slowly warmed up and wheeled to a magnetic resonance imaging machine in a specially designed incubator, where a technician will conduct brain scans. After being released, the child must return periodically to UCSF for many years for check-ups.  \n                Cool care \n              If all goes well, the hypothermic treatment will avert further brain damage beyond the initial injury. A study of 325 babies published last October 1  found that the cooling intervention cuts the risk of brain damage. Of the children that received the hypothermic treatment, 44% survived without a neurological abnormality compared with 28% in the control group who received regular intensive care. The treatment may work by lowering the demand for oxygen among stressed neurons, allowing them to recover from damage. UCSF began offering the hypothermic treatment in July 2008 and has now treated more than 80 newborn babies. It is one of a growing number of hospitals in North America and around the world that offer such hypothermic treatment to newborns at risk of brain damage. A 2007 study, for instance, found that 28% of the neonatal units in the United Kingdom offer this kind of treatment 2 , and it is spreading across other European countries as well as less wealthy nations such as South Africa. But cooling a baby's entire body to spare its brain is a blunt instrument. Rowitch envisions a future of more precise treatments targeted at specific molecules that orchestrate neural development. Recent advances provide some optimism on this front: in his own research, Rowitch has discovered hints that the injured brains of babies might be more capable of recovering than doctors previously thought. With collaborators at UCSF, Rowitch is building a translational-research programme aimed at understanding more about the healthy and injured developing human brain, and converting those findings into therapies. \"That would really be the vindication of this programme,\" he says.  \n                Formative years \n              Rowitch took an interest in brain injuries in newborns during his medical training and later, when he set up his first lab at the Dana-Farber Cancer Institute in Boston, Massachusetts, in the 1990s. He began a research programme that would soon lead to high-profile discoveries in the molecular basis of brain cancer. His collaborator, Charles Stiles of Dana-Farber, says that Rowitch was often happiest spending his time with sick babies at the neonatal intensive-care unit at Children's Hospital Boston. \"Most of the physician-scientists I know would dread that one month of the year when they had to fulfil their clinical obligation, but David really looked forward to it,\" says Stiles. \"He loved putting on the scrubs and spending all night in the premature critical-care ward. He loves the machines and he loves little babies.\" There, Rowitch witnessed the sweeping changes in medical care that are allowing increasingly premature neonates to survive, even those born as early as 24 weeks. At the same time, assisted-reproduction techniques, which often lead to multiple births, have helped to drive up the rates of prematurity by 36% in the United States since the early 1980s. As the number of extremely premature births climbs, more children face the risk of brain injury because their developing organs can not deliver enough oxygen to the brain; 20% of babies born before six and a half months of pregnancy are at risk of stroke and neurological complications such as cerebral palsy, a movement disorder caused by brain damage. About half will have learning deficits and other cognitive problems. The financial burden is enormous: it costs nearly US$1 million to treat and care for each patient with cerebral palsy in the United States over that person's lifetime. But the understanding of the neural complications of prematurity has not kept pace with this surge in very young babies. Scientists know little about early brain development, in part because parents who lose children are reluctant to allow research on them. That makes it difficult for scientists to conduct structural studies of human brain development using the kinds of detailed molecular analyses that are now common in research on non-human brains. Rowitch is trying to move the field forwards by setting up a paediatric neuropathology lab to collect and study donated brains from deceased babies and children. He is also collaborating with Arturo Alvarez-Buylla, a neuroscientist at UCSF, to perform the first detailed molecular analyses of autopsied brains of children who died from various causes between birth and the age of 18. By staining molecules to identify cells at different stages of development, Rowitch and Alvarez-Buylla have succeeded in tracing some neural growth patterns. Preliminary data from 45 brains, for example, suggest that one area of neural growth \u2014 the subventricular zone \u2014 generates fewer new brain cells by 18 months of age or may completely stop by that point. Rowitch says this type of data is crucial for understanding exactly what goes wrong when the brains of newborns are damaged. For instance, it is important to know whether stroke wipes out sources of new cells in the still-developing brain, or whether it prevents repairs to damaged cells, or causes damage through both mechanisms. Rowitch is also approaching this question from a different direction, by studying the brains of newborns who have died after suffering brain injuries. This work builds on his earlier discovery of genes that direct developing brain cells to grow into oligodendrocytes, a type of glial cell 3 . Such cells protect neurons by wrapping their long signalling projections, called axons, in a protective myelin sheath. Rowitch's gene discovery led him to study diseases involving malfunctioning oligodendrocytes, such as periventricular leukomalacia (PVL), a form of brain injury in fetuses and newborns that can result from stroke, and which can progress into cerebral palsy. The conventional wisdom is that PVL occurs when a stroke kills off oligodendrocytes in a certain region of the brain, leading to the death of neurons. When Rowitch and his collaborators studied the brains of 18 children who had died after being diagnosed with PVL, however, they found plenty of oligodendrocytes in the injured regions. It looked as if new oligodendrocyte precursors were migrating to the site of the injury to replace cells that had been destroyed by the stroke 4 . But something was stopping the precursors from developing into oligodendrocytes that produce myelin. The finding has important implications, Rowitch says, because repairing damage is a much more practical treatment strategy than trying to prevent damage. It may be difficult to predict which babies are at risk of injury, and even harder to access the baby  in utero . \"This profoundly shifts the kind of questions you can ask about PVL,\" Rowitch says. His lab is now trying to better understand the signals that block repair, so that they may be reversed \u2014 a strategy that might also work for patients with multiple sclerosis, which is also caused by a loss of myelination.  \n                Forced separation \n              Such basic-science studies will take years, but in the meantime, the UCSF team is working to improve current treatments. For instance, a team of doctors and nurses monitors Julia around the clock. No one, including her parents, can move her from her cooling bed, which means that she won't be picked up or cuddled for the first few days of her life. She seems quiet and comfortable, with the help of a low dose of sedatives, and doesn't cry. Still, Rowitch says his team knows it is difficult for parents to be separated from their babies so soon after birth. So the researchers are trying to determine whether some babies might benefit from shorter doses of cooling. And the clinic has begun testing new candidate therapies to prevent brain damage. Ferriero has performed extensive tests of the hormone erythropoietin in rats, and has found that this red-blood-cell boosting compound prevents long-term cognitive damage in rat models of neonatal stroke 5 . Erythropoietin is already used to treat premature babies with immature circulatory systems, and UCSF will begin a clinical trial to test it on newborns at risk of brain damage this month. Other candidates for clinical trials include magnesium, which is already used in adult stroke patients. The intervention seems to decrease the rate of cerebral palsy in children surviving very premature births 6  and lowers the rate of neurological problems, at least in the short term, of full-term newborns who suffer from oxygen deprivation 7 . Even now, Ferriero says, doctors can make a big difference by adjusting the way they treat vulnerable babies. For example, studies have revealed that over the past decade, the pattern of brain injury seen most often in premature babies is shifting away from PVL to a slightly less severe form of damage, apparently because of changes in hospital procedures 8 . To Ferriero, this underscores the importance of monitoring the brains of vulnerable babies, which is not currently done in most intensive-care nurseries. \"They monitor pulse, they monitor temperature, they monitor heart rate. But the brain, which runs it all, is being totally ignored, until something really bad happens, and then it's too late,\" Ferriero says. She points out that many infant seizures go undetected, because there are no outward signs of a problem. But these seizures can be detected by an electroencephalogram. If doctors can catch the seizures early enough, they can intervene to stave off further damage. So for now, the most important result of UCSF's programme may be convincing other doctors to start paying attention to the neonatal brain. Ferriero and Rowitch have invited doctors from around the country to visit the Neurointensive Care Nursery, and some, such as the Phoenix Children's Hospital in Arizona, have started setting up similar nurseries. \"That's the wave of the future,\" says Cristina Carballo, medical director of Phoenix's Neuro-Neonatal Intensive Care Unit. \"It means we can diagnose and treat neurological issues faster to improve neurodevelopmental outcomes.\" But the movement to pay more attention to brain injuries in babies, and to use neuroscience to treat them, is still young; even its proponents counsel caution. \"We're really still taking baby steps,\" says Huda Zoghbi, the director of the Jan and Dan Duncan Neurological Research Institute at Texas Children's Hospital in Houston. For now, even those small steps are helping patients such as Julia. As she sleeps through her hypothermic treatment, her anxious father can only watch from the bedside. It's hardly the welcome to the world he had hoped for his daughter, but it will give her a good shot at thriving when he finally takes her home. \n                 Watch a video on this topic at  \n                 \n                     http://go.nature.com/VGGayN \n                   \n               \n                     NIH resources on premature babies \n                   \n                     UCSF Newborn Brain Research Institute \n                   Reprints and Permissions"},
{"file_id": "463018a", "url": "https://www.nature.com/articles/463018a", "year": 2010, "authors": [{"name": "David Lindley"}], "parsed_as_year": "2006_or_before", "body": "Renewable energy is not a viable option unless energy can be stored on a large scale. David Lindley looks at five ways to do that. In February 2008, during a sudden cold snap, the normally relentless winds of west Texas fell silent and the thousands of wind turbines that dot that part of the state slowed to a halt. Local utility operators, unable to make up the shortfall with power from elsewhere in the grid, were forced to cut service to some users for up to an hour and a half before the winds picked up again. That windless interval would have been a non-event if the utility companies had had a few hundred megawatt hours of energy stored away that they could draw on in emergencies. But they didn't. Ever ephemeral, electrical energy is difficult and expensive to store in large quantities. The lack of good storage options has plagued utility operators for generations. Obligated to provide a steady supply of electricity to meet constantly varying demand, they have conventionally resorted to the costly and inefficient method of adjusting the output of a coal-fired plant, say, or by turning on a gas-powered 'peaker' plant during periods of high demand. But that supply-based strategy is becoming less viable with the increased use of renewable energy sources with unpredictable output, notably solar arrays and wind farms. As the Texan example indicates, the power produced by these technologies is dependent on nature's whim, not human demand. \"If we want to have a significant part of our energy come from renewable sources, storage is a must,\" says Ali Nourai, manager of energy storage at American Electric Power, a utility company in Columbus, Ohio, and chairman of the Electricity Storage Association, a trade association in Washington DC. A number of technologies for energy storage already exist, including some that have been around for decades. The challenge is to make them robust, reliable and economically competitive \u2014 while matching the most suitable technology to each energy source or location. \"Each technology has unique features,\" says Jillis Raadschelders, of energy consulting firm KEMA in Arnhem, the Netherlands. \"There will never be a winning technology.\" Choosing the right technology means looking at each one in some detail. \n                Pushing water uphill \n              The need for storage is particularly acute in densely populated northern Europe, where many countries are building offshore turbines to harness the winds blowing across the North Sea. Denmark already gets about 20% of its electricity from land- and sea-based wind farms, and it is aiming to increase that figure to 50% by 2025. Because the North Sea winds can drop to low levels for days at a time, however, countries such as Denmark and the Netherlands are increasing their grid connectivity to Norway, which gets the vast majority of its power from hydroelectric plants. Norway's mountain reservoirs provide back-up power capacity, and also offer substantial amounts of pumped storage hydroelectricity, in which water is pumped uphill to a reservoir using surplus electricity, and released downhill again to turn a generator when power is needed.  Pumped hydroelectricity has a storage efficiency of 70\u201385%, and it is the most mature and widespread technology being used for large-scale electricity storage. China, Japan and the United States, for example, have numerous installations with generating capacities ranging from tens of megawatts (MW) to several gigawatts (GW). Pumped storage hydroelectricity is a particularly good match for wind power because water pumped into an upper reservoir will stay there for a long time, making up for potentially large gaps in wind generation. But in its conventional form, pumped storage hydroelectricity requires mountains, so opportunities are limited by geography. Building such storage also tends to be expensive and environmentally destructive, and installing high-voltage transmission lines to connect remote storage sites to grids often triggers opposition on environmental grounds. If the capacity of pumped storage hydroelectricity is to grow significantly, it will have to leave the mountains. One innovative concept by KEMA would put wind turbines and pumped hydro in the same place: an 'energy island' in a shallow part of the North Sea. An area of perhaps 60 square kilometres would be ringed off by a dyke or levee to create an artificial lake. Wind turbines would stand on the encircling dyke, and any excess power would be used to pump water out of the lake and into the surrounding sea. Letting sea water flow back in would regenerate the stored electricity. In the absence of wind, KEMA estimates that the energy island could supply an average of 1,500 MW for as long as 12 hours. \n                Squeezing air underground \n              In the farmlands near Huntorf, Germany, about 100 kilometres southwest of Hamburg, an ordinary-looking industrial installation performs an unusual task: when demand for electricity in the local grid is low, the plant uses excess power to compress air and pump it into two underground salt caverns with a combined volume of more than 300,000 cubic metres. Then, at times of high demand, the compressed air is allowed to expand through turbines on the surface to regenerate the electricity. The Huntorf plant, which has been working since 1978, can supply almost 300 MW of reserve power for up to three hours, and comes into operation about 100 times a year. But it has not exactly spawned a legion of imitators. A similar but smaller plant in McIntosh, Alabama, came online in 1991, and efforts to build another such system in Iowa, begun in 2002, are only now at the point of acquiring land for test drilling. The problem is that these compressed-air energy storage (CAES) facilities are considerably more complex in practice than they are in principle. Gas heats up when it is compressed, which limits how much air can be pumped underground before it becomes too hot to be stored safely. Moreover, the longer that hot air is left in place, the more of its heat \u2014 which represents a substantial fraction of the input energy \u2014 is lost into the walls of the surrounding cavern. And then when it is released again, the expanding air cools down. In the Huntorf and McIntosh facilities, in fact, the released air is fed into a standard natural gas turbine, boosting its efficiency. So the net effect of the air-compression system is to boost the efficiency of a more or less conventional natural-gas-fuelled power plant. In the near term this kind of hybrid system \"makes a great deal of sense\", says Haresh Kamath, a researcher at the Electric Power Research Institute (EPRI) in Palo Alto, California, especially as more electricity from renewable sources is available to recharge the system at night. Looking further down the road, however, the EPRI and others are also researching improvements that would turn CAES into a true energy storage system, requiring no fossil fuel. Such an 'advanced adiabatic' system would capture and store the heat of compression and then use it to reheat the released air, which would spin a turbine directly without any additional fuel. Metal foundries and blast furnaces have for years captured waste heat in stacks of refractory bricks or similar materials, says Christoph Jakiel, a researcher at MAN Turbo in Oberhausen, Germany. So applying the technique to compressed-air storage should be straightforward. He estimates that the efficiency of such a system to be something under 80%, comparable with pumped storage hydroelectric systems. The overall costs of construction and operation would also be about the same. Suitable locations should not be hard to find in most regions of the world, says Jakiel. Salt caverns are not uncommon, and the proposed Iowa Stored Energy Park, should it ever be built, will pump compressed air into an aquifer. \n                Electricity in a box \n              Large-scale battery storage would be a solved problem already if utility companies could use the ubiquitous lead-acid technology that has been the basis of car batteries for nearly a century. Unfortunately, lead-acid batteries have a low energy density \u2014 they are bulky and heavy for the amount of energy they store \u2014 and they do not stand up well to repeated charge\u2013discharge cycles. A better solution is the sodium\u2013sulphur (NaS) battery, which stores energy by chemically dissociating sodium polysulphide into sodium and sulphur. The energy can then be released by allowing the two elements to react again. NaS batteries have a high energy density and can last through thousands of charge\u2013discharge cycles. Their chief drawback is that the sodium and sulphur have to be kept in separate reservoirs in the molten state, at about 300 \u00b0C. Also, the batteries suffer irreparable damage if they discharge completely and grow cold. The resulting need for a robust container, along with other technical requirements, means that NaS batteries cost about US$3,000 per kilowatt (kW) of available power. That compares unfavourably with standard gas-powered plants, which cost about $1,000/kW. Nonetheless, NaS batteries have been developed commercially by NGK Insulators in Nagoya, Japan. Japan now has an installed capacity able to supply its grid with about 300 MW when extra power is needed, for up to six hours at a stretch. Other countries are also picking up the pace. The United States, for example, has about 10 MW of NaS capacity in place and a similar amount on the way, led by companies such as American Electric Power and Xcel Energy in Minneapolis, Minnesota. In the future, large-scale NaS storage could face a challenge from lithium-ion technology. Already in widespread use for mobile phones and laptops, and under development for electric cars, lithium-ion batteries have a high energy density and efficiencies of more than 90%. Their big drawback is cost, which is in part driven by safety considerations: the batteries use a lithium salt in an organic solution, which is flammable, necessitating robust construction to minimize fire hazards. Lithium-ion batteries made for consumer electronics currently cost a few hundred dollars per stored kW hour. But for widespread vehicle applications, that cost must come down closer to $100 per kW hour, and for grid applications it needs to be lower still. Yet Nourai, for one, remains optimistic. Safety issues are more easily and cheaply met for batteries in secure, fixed installations than in hand-held devices, he says. And in Asia, especially, there is strong support for lithium-ion technology and keen competition between manufacturers, which he hopes will lead to dramatic cost reductions. In China, he recently saw a cargo-container-sized lithium-ion installation, and expects to see capacities of a megawatt or more in the coming years. At the Massachusetts Institute of Technology in Cambridge, materials chemist Donald Sadoway is trying a more radical approach to reducing the cost. \"I want a battery that's dirt cheap,\" says Sadoway, \"and the way to do it is to build it from dirt\" \u2014 that is, from the most abundant elements in Earth's crust. Although there is little new to discover about the electrochemistry of these elements \u2014 such as silicon, iron and aluminium \u2014 a battery involves two reactions, one at each electrode, along with an electrolyte that supports the appropriate ion transfer. That makes for a huge and largely untested combination of possible compounds and reactions to search through. The quest is made feasible, says Sadoway, by supercomputers that can quickly assess proposed battery chemistries, freeing researchers from the need to synthesize and test actual materials. In the coming decade, he says, \"I'm optimistic that the rate of discovery will accelerate.\" \n                Taking electricity for a spin \n              Conceptually, at least, one of the most straightforward ways to store energy is in a spinning flywheel: electrical energy gets converted into the kinetic energy of rotation by running it through a motor, which accelerates the flywheel. And the kinetic energy is extracted when it is needed by coupling the flywheel to a generator, which slows the wheel down and produces electricity. Again, however, the reality is more complex \u2014 the flywheel has to spin very fast yet be strong enough to keep from flying apart. Flywheel storage systems are commercially available as uninterruptible power supplies that can deliver modest amounts of power for seconds or minutes, but they are not competitive for the longer storage times needed by the electric utility companies. One big advantage of flywheels is that they can absorb the energy within seconds or minutes, and give it back just as quickly. This is exactly what is needed for regulating the frequency of a power grid, which is supposed to be maintained at an even 50 or 60 cycles per second, depending on the country, but which tends to drop whenever short-term increases in the load cause the turbines to slow down. Keeping it stable is a challenge for utility companies everywhere. With that in mind, Beacon Power of Tyngsboro, Massachusetts, has spent the past decade developing a high-tech flywheel that is optimized for frequency regulation. Measuring about 2 metres tall and 1 metre in diameter, the flywheel consists of a cylindrical aluminium core, which houses the motor and generator, and a carbon-fibre composite rim. It is suspended on magnetic bearings inside a vacuum-sealed chamber, where it can spin at up to 16,000 revolutions per minute. The devices are designed to run for 20 years or more with no maintenance, says Matthew Lazarewicz, Beacon's chief technical officer. They can store energy with an efficiency of 85%, he says, and can spin up and down for perhaps millions of cycles during their working life, making them far more durable than batteries. The challenge now is to bring the cost down, which Beacon hopes to do thanks to a project it has recently undertaken with loan guarantees from the US Department of Energy. In Stephentown, New York, Beacon has begun construction of a $70-million, 20-MW, 200-wheel flywheel farm that will help to regulate frequency in the regional power grid. The budget includes a number of one-time costs related to establishing its qualifications for the federal loan guarantees. The company estimates that future plants of this size will cost less than $50 million \u2014 a price it hopes to bring down to about $30 million. In late November, the energy department awarded Beacon $24 million for half the cost of a new 20-MW plant to be built outside Chicago, Illinois. \n                Integrating with a smart grid \n              There are a number of even more exotic technologies that could become candidates for large-scale energy storage \u2014 assuming that researchers can eventually get the cost down to a competitive level. Examples include 'ultracapacitors' that can store huge amounts of electrical charge in atoms-thick layers next to the electrodes, and coils of superconducting wire able to store large amounts of circulating current indefinitely. But by far the most cost-effective approach to large-scale electric energy storage is to minimize the need for it. That is one of the goals set out earlier this year in the US stimulus bill, which allocated $4.3 billion to research and development in renewable-energy generation, energy efficiency and, most especially, a 'smart grid'. Instead of simply adjusting the supply of electricity in response to the vagaries of unpredictable demand, a smart grid would constantly adjust demand as well. When demand hits a peak, for example, the grid might start cutting power for household refrigerators, office air-conditioning systems and other non-urgent uses \u2014 just for a moment in each case, and nothing that anyone would notice, but enough to smooth out variations in the overall load.  In that kind of system, says Nourai, storage and smart-grid technologies would work together, evening out the usual peaks and troughs in grid load to a greater extent than either could achieve alone. \"Variation is never going to go away but with storage it can be much flatter,\" he says. He sees a future in which even fairly small communities could be \"net zero\", meaning that on average they make as much electricity as they need, and maintain a reliable supply by exchanging modest amounts of power back and forth with neighbouring communities. Local interconnections would be low-voltage lines, and long-distance high-voltage lines would be needed only to connect wind farms or solar arrays in remote areas with populated regions. That transformation, Nourai says, \"will change the way we think about, run and plan\" the storage of electricity. \n                 David Lindley is a freelance writer in Alexandria, Virginia.  \n               \n                     Electricity Storage Association \n                   \n                     KEMA: International Energy Consulting Co \n                   \n                     Electric Power Research Institute \n                   \n                     Beacon Power \n                   Reprints and Permissions"},
{"file_id": "463022a", "url": "https://www.nature.com/articles/463022a", "year": 2010, "authors": [{"name": "Mark Schrope"}], "parsed_as_year": "2006_or_before", "body": "An annual excursion to an exclusive Caribbean island has yielded an impressive body of ecological fieldwork. Just don't call it a holiday, says Mark Schrope. It's supper time on Guana Island in the British Virgin Islands and a dozen diners are relaxing on a candlelit veranda atop a cliff overlooking the Atlantic Ocean. They're enjoying a four-course meal and several bottles of wine. The privilege of staying at this very private resort \u2014 the only sign of civilization on the 340-hectare island \u2014 typically costs guests between US$700 and $8,000 per night. But these aren't typical guests. They're biologists, and if the constant talk of taxonomy isn't enough to prove it, they have physical evidence. After the meal, zoologist James 'Skip' Lazell (pictured above), produces from his shirt pocket a small bag containing a live snake,  Liophis exigua . It had been captured that evening and would soon be measured and returned to where it was found. Lazell, president of the non-profit organization the Conservation Agency in Jamestown, Rhode Island, has been coming to Guana every year for nearly three decades to lead a study of this distinctly unspoiled island's flora and fauna. He and a rotating crew of collaborators have produced what is arguably the most comprehensive, long-term record of the natural history of an island in the Caribbean, where high-volume tourism and frequent catastrophic weather shape the ecosystem. They have revealed a remarkably diverse ecological cast: including some 12 species of reptiles, 100 birds, 160 fungi, 330 plants and hundreds of insects, several of them new. \"I can't really think of another place where exactly these kinds of data for a whole bunch of species all at the same time have been gathered year after year,\" says Daniel Simberloff, an ecologist at the University of Tennessee Knoxville. That record has lent itself to increasingly sophisticated ecological modelling approaches that may ultimately help to predict the regional effects of global climate change. Moreover, the work challenges the prevailing theory on what drives island biodiversity. Development on the island has been minimal, making the place attractive not only to the 30 or so guests the island can house at any given time, but also to scientists: the island offers a relatively unspoiled baseline for healthy Caribbean plant and animal life. \"It is in the best ecological condition and the least screwed up, certainly, of all the islands on the Greater Puerto Rico Bank,\" Lazell says. Most other islands in the region have undergone significant development and human settlement for at least some portion of their history.  \n                Welcome to the island \n              But on Guana, before the resort there was only a short-lived Quaker plantation in the 1700s and a native American settlement centuries before that. Gloria Jarecki, who with her husband, Henry, bought Guana in 1975, says the plan had always been to maintain as small a footprint as possible, \"and try to pass [the island] on in the condition we found it in or better\". So they were receptive when Lazell first proposed a long-term study of the island in the early 1980s. A foundation run by the Jareckis covers the cost of the team's accommodation in October, during tourism's low season for the area. Scientists typically only have to pay for travel. (The author of this article was similarly accommodated.) One can be forgiven for assuming, as some of their colleagues have, that the whole enterprise is a tropical junket. In addition to the dinners, the team meets most afternoons on a palm-lined, white-sand beach with a well-stocked bar. \"It's about as cushy as you can get on the domestic end,\" says Clint Boal, an ornithologist with the US Geological Survey based at Texas Tech University in Lubbock, and long-time Guana team member. But the days on Guana are anything but pampered. The vast majority of the island is accessible only by rugged footpaths, some of which descend precipitously down boulder-strewn gulches. The scientists typically slog kilometres every day with their gear, conducting general surveys or working at regular study plots. \"It is a very difficult place to work in many ways,\" Boal says. For the first few years, the goal was mainly to catalogue the plant and animal species. In parallel with the survey work, Lazell also worked with the owners to preserve the ecosystem and restore it as much as possible to its pre-human condition, recommending efforts such as limiting the population of wild sheep and removing cats, dogs and invasive plant species such as Australian pines. He also established a programme to replace animals thought lost, based on historic and archaeological records, including flamingos, red-legged tortoises, and critically endangered stout iguanas. The team also watches island populations. For birds and reptiles, the researchers measure survival rates and demographics \u2014 data that could reveal the factors most important to maintaining a healthy Caribbean island. To monitor the birds, Boal and his colleagues spread lightweight mist nets across bird thoroughfares. After weighing captured birds and taking other measurements, researchers mark them with leg tags. That doesn't work for reptiles, however, as many have legs that grow substantially and some don't have any. So, the team began implanting radio-frequency identification devices in 2001. Brent Bibles, a population biologist at Utah State University in Vernal, points to an adult stout iguana ( Cyclura pinguis ) on a trail near the resort. These reintroduced animals now number in the hundreds, from the eight that were brought to the island in 1984 (J. D. Lazell  Island: Fact and Theory in Nature   Univ. California Press, 2005). The team hopes soon to begin recapturing adults. Bibles, who has bite marks all over his right hand from an encounter with a metre-long Puerto Rican racer snake ( Alsophis portoricensis ), is excited by the prospect. But at up to 30 kilograms, the iguanas, he says, are, \"kind of nasty to handle\". Barry Valentine, a retired entomologist from Ohio State University in Columbus, has been collecting insects since he was ten. In 1946 he joined a group of adventurous undergraduates at the University of Alabama in Tuscaloosa. In his book  Naturalist   (Island Press, 1994), Edward O. Wilson describes chasing down specimens while hanging off the hood of Valentine's car. Still, it's difficult to imagine Valentine any more exuberant in his work than he remains today at 85. On a clear night, Valentine flips on a black light at his cottage and darts after interesting visitors.  \n                Tropical futures \n              In recent years Valentine has found four new beetle species, but also some puzzling shifts in the number and types of various insect groups on the island. Most years he collects about 40 ground beetles, for instance; this year he found just six. He is working with others in the Guana team to see whether they can spot corresponding trends between weather patterns and animal populations. \"There are always questions being raised,\" says Valentine, \"Which is one of the things that makes it so much fun.\" Some species of birds show troubling patterns with fewer young and lower body mass. Because the Lazell team's data set stretches back so far and covers so many groups, they can look for explanations for shifts in populations. \"That's exactly the kind of data you need to demonstrate the impacts of climate change,\" says Simberloff. Boal, Bibles and others are looking at rainfall data from the National Oceanic and Atmospheric Administration to see if they can spot correlations between drought years and population declines, for example. Others are looking at water retention in the reptiles on the island for clues to how a changing climate could affect animals. To Lazell, though, the most important application of the Guana data set, along with his related, but less extensive, studies on other islands around the world, is to inform the ongoing debate over whether mathematical formulae can reliably predict the diversity on a given island. One of the most prominent theories, proposed in the 1960s by Wilson and Robert MacArthur (R. H. MacArthur & E. O. Wilson  The Theory of Island Biogeography   Princeton Univ. Press, 1967), correlates the number of species on an island with its area. But the number of species on Guana far exceeds the theory's predictions, even ignoring reintroduced species. \"We took the MacArthur\u2013Wilson species\u2013area notion and blew it out of the water,\" says Lazell, who is not known for mincing his words. Lazell argues that human impact, although difficult to quantify, may be the main controling factor of diversity. MacArthur and Wilson \"were looking at islands that were just wrecked\", he says, and that skewed their results. Overall, Lazell says that ecology may be too complex for broadly applicable formulas. Gad Perry, a herpetologist and conservation biologist at Texas Tech who helps to coordinate the Guana programme, has similar suspicions. \"There are people that say this is not rocket science, but rocket science is so much simpler,\" he says. \"Our systems have a lot more variables and that's what makes them interesting.\" Simberloff, who was a student of Wilson's, agrees that a universal theory of biodiversity will probably remain elusive. \"I don't think anyone would say that Guana resolutely rejects any prominent theories,\" he says, \"but it could be that in a few years people reviewing literature on some particular theory might use Guana as one of several examples to say, 'Look, it doesn't hold up as strongly as we hoped'.\" For his part, Lazell says that more studies like those conducted at Guana may be needed. But, he says, \"there's not much more I can do in one lifetime\". Although he's crisscrossed the island more times than he can count, his health now confines him mainly to the resort, where he spends most of his time facilitating the work of others, and reigning as the self-proclaimed \"Curmudgeon in Chief\". As for Guana itself, with its trails through virgin forest and its collegial, candlelit dinners, his conclusions are difficult to argue. \"This is my favourite place to do work,\" he says, \"You can't beat it.\" Mark Schrope is a freelance writer based in Florida. \n                     The Conservation Agency \n                   \n                     Guana Island Resort \n                   Reprints and Permissions"},
{"file_id": "465682a", "url": "https://www.nature.com/articles/465682a", "year": 2010, "authors": [{"name": "Colin Macilwain"}], "parsed_as_year": "2006_or_before", "body": "Spending on science is one of the best ways to generate jobs and economic growth, say research advocates. But as Colin Macilwain reports, the evidence behind such claims is patchy. President Barack Obama says it. Francis Collins, director of the US National Institutes of Health (NIH), says it. University and research leaders elsewhere are saying it, too. The number one current rationale for extra research investment is that it will generate badly needed economic growth. \"Science is more essential for our prosperity, our health, our environment and our quality of life than it has ever been before,\" said Obama, addressing the National Academy of Sciences in Washington DC last year. Getting down to the details, Collins has recently cited a report by Families USA, a Washington DC-based health-advocacy group, which found that every US$1 spent by the NIH typically generates $2.21 in additional economic output within 12 months. \"Biomedical research has generally been looked at for its health benefits, but the case for it generating economic growth is pretty compelling,\" says Collins. In Britain, senior scientists have called on the government to support science as a means of helping the economy out of recession. Heeding such arguments, governments in Germany, Sweden, Canada and Australia, as well as the United States, have increased research spending as part of stimulus packages designed to aid their struggling economies. Beneath the rhetoric, however, there is considerable unease that the economic benefits of science spending are being oversold. The Families USA study used a model developed by the Bureau of Economic Analysis at the US Department of Commerce to deduce the likely benefits of NIH spending in each state. Collins says he has been advised that the approach is \"standard and considered reliable\". But some economists question the basic assumption behind such models \u2014 that a certain amount of research input will generate corresponding economic outputs \u2014 or that those outputs can be quantified.  \n                Costs or benefits \n              The problem, economists say, is that the numbers attached to widely quoted economic benefits of research have been extrapolated from a small number of studies, many of which were undertaken with the explicit aim of building support for research investment, rather than being objective assessments. The economics of health research, on which much analysis of costs and benefits has been focused, \"has had very little money invested in it\", says Martin Buxton, director of the Health Economics Research Group at Brunel University, UK. \"And too much of what has been done, has been done as a process of advocacy.\" Research leaders acknowledge that they need better tools. Collins says that the NIH held a workshop with economists in May to see whether it should invest some of its funds into economic outcomes. \"We're very interested in tightening up the evidence base,\" he says. Some of that evidence is already being collected. Under the programme STAR METRICS (Science and Technology in America's Reinvestment \u2014 Measuring the Effects of Research on Innovation, Competitiveness and Science), implemented after the US stimulus package was introduced, the Obama administration is seeking to trace the effect of federal research grants and contracts on outcomes such as employment, publications and economic activity (see   Nature  464, 488\u2013489; 2010 ). The programme's supporters say it will provide justification for the stimulus money \u2014 exactly what research agencies need as they come under pressure to show what recent investments have produced. Economic arguments have always been used to make the case for science spending, particularly when times are tough. In the United States, these were strengthened by the publication of  Rising Above the Gathering Storm , an influential 2006 report from the US National Academies, which called for the sharp expansion of publicly funded research and development to stave off competition from China and elsewhere. The 600-page report, written by a panel chaired by Norman Augustine, former chairman of Lockheed Martin, was put together by a large panel of senior scientists in a matter of weeks, to meet a tight congressional deadline. In a section titled \"Why are science and technology critical to America's prosperity in the 21st century?\" the report reviews the literature that estimates return on investment (ROI) from research (see ). This is illustrated by various graphs, including one showing steep declines in US death rates from heart disease between 1950 and 2000, inferring that this drop can be partly attributed to biomedical research.  \n                Innovation drive \n               Gathering Storm   recommended that federal investment in basic research should increase by 10% every year for seven years, and led Congress to consider spending increases of that order, mainly in the physical sciences and engineering. When the newly elected President Obama was hurriedly preparing a February 2009 bill aimed at stimulating economic growth, parts of these increases were thrown in, together with extra spending for the NIH. In the end, the American Recovery and Reinvestment Act included a further $21 billion of research spending, all justified by its supporters on the grounds that it would yield speedy economic returns. Yet Stephen Merrill, executive director of the Board on Science, Technology and Economic Policy at the National Academies but who was not involved in the report, concedes that  Gathering Storm   doesn't, in itself, make a detailed case for the economic benefits of investing in research. For that, one has to look further back in the literature. Economists have agreed for decades that a large component of modern economic growth has to be driven by 'innovation' \u2014 that is, the arrival of new ideas and technologies. \"We have very good evidence that 50\u201370% of productivity growth arises from innovation,\" says Iain Gillespie, head of the Science and Technology Policy Division at the Organisation for Economic Co-operation and Development in Paris. Greater difficulty arises in determining what drives the innovation, though. Is it basic research, often publicly funded, as the science advocates contend? Or are other factors, such as the demands of consumers who buy, say, mobile phones or computer games, also involved? And even if scientific research does drive innovation, will more investment in science necessarily speed up the process? Unfortunately, economists concede, no one really knows. In one of the bedrock papers in this field, Edwin Mansfield, the late University of Pennsylvania economist, estimated that academic research delivered an annual rate of return of 28% ( E. Mansfield  Research Policy    20,   1\u201312; 1991 ). The figure has been widely quoted ever since. But Mansfield reached this estimate by interviewing chief executives, asking them what proportion of their companies' innovation was derived from university research and, in effect, demanding that they come up with a number. \"He was asking an impossible question,\" says Ben Martin, a former director of the Science and Technology Policy Research Unit at the University of Sussex, UK. \"Methodologically, this was a dubious thing to do.\" Whatever method economists have used since, measuring the ROI from research has proved tough, and has produced a wide range of values (see ). Some look at the 'micro' level, asking things such as: what contribution did a dozen neuroscience grants received by the University of Cambridge in 1972 eventually make to drug development? Such efforts are complicated, however, by the difficulties of attributing credit for any given drug to the numerous research teams involved over time. Policy-makers are more interested in the 'macro' question, measuring the effect of combined research activities on a country's economic growth. According to Merrill, repeated efforts to pin down firm numbers here have also failed. \"It is fair to say that this is an analytical dead end,\" he told attendees at the American Association for the Advancement of Science annual meeting in February.  \n                Exceptional returns? \n              Martin says that for much of the literature, \"there is some PR, rather than rigorous research involved\". This influence derives in part from the activities of US medical research lobbyists. An example is the 2000 report  Exceptional Returns: The Economic Value of America's Investment in Medical Research   by Funding First, an initiative of the Mary Woodard Lasker Charitable Trust that advocated biomedical research spending. Pointing to work by various economists, the document estimated that the steep decline in cardiovascular deaths in the United States between 1970 and 1990 has an economic value of $1.5 trillion annually, and deduced that one-third of this \u2014 $500 billion a year \u2014 could be attributed to medical research that led to new procedures and drugs, a finding that was echoed in the  Gathering Storm   report. A plethora of studies in the United States and Australia followed through with similar claims. Funding First has been disbanded, but Robert Topel, who studies labour economics at the University of Chicago and whose work was cited in the report, distances himself from some of its claims. \"Probably only a little of the fall in the cardiovascular death rate has to do with surgery and beta-blockers,\" he says. \"It is very hard to take changes in public health and attribute their cause.\" Topel also questions the report's implication that publicly funded biomedical research will create thousands of jobs in the pharmaceutical and biotechnology industries. Topel says that Mark Hatfield, the former Republican senator for Oregon who wrote the report's introduction, was constantly fishing for job numbers. \"We kept telling Hatfield that jobs are a cost, not a benefit.\" \n                The price of research \n              A key problem, says Topel, has been economists' inability to measure the costs of research as well as the benefits. These costs include the added expense of caring for elderly patients kept alive by new treatments, the costs of talented people doing research instead of something economically productive (such as running a technology company or an ice-cream van), and the cost of wayward outcomes, such as nuclear clean-up \u2014 a long-term 'outcome' of the research and development of nuclear energy and weaponry. Research agencies have no interest in assessing the costs of research fairly, says Barry Bozeman, a science-policy specialist at Georgia Institute of Technology in Atlanta. \"Honest clients are in short supply\" for research in this field, he says. \"Most of them think they already have the answers, and want someone to find the numbers to prove them right.\" The flaws in the 'exceptional returns' literature were thrown into sharp relief in a November 2008 study called  \n                   Medical Research: What's it Worth? \n                  by the London-based Wellcome Trust and the UK Medical Research Council. In it, some UK health economists attempted to make rigorous estimates of the economic benefits of publicly and charitably funded medical research in Britain. They estimated that every pound invested in cardiovascular disease and mental-health research brought about, through improved health, economic returns of 9% and 7%, respectively. Work in both fields generated an extra return of 30% through 'spillover' effects from research to the broader economy, such as training and industrial activity. But the report said that these findings were \"at best tentative\", and spelled out a long list of knowledge gaps. Little is known about how long the economic benefits of research take to accrue; nor the extent to which the benefits of research done in one country or region are specific to that area, which is a central question for policy-makers. \"Three-quarters of the benefits are in spillover, and that's where the evidence is weakest,\" says Jonathan Grant, president of RAND Europe in Cambridge, UK, and one of the study's main authors. Grant also questions the way that data on ROI gathered in one sector, such as agricultural research, have sometimes been applied to others. \"Most of the empirical evidence in this area is (a) historical, (b) American and (c) from agriculture. How transferable is that? It's a big question in my mind,\" he says. Efforts to strengthen the evidence are increasing. An $8-million-a-year grants programme at the National Science Foundation (NSF), for example, is supporting investigations by science-policy specialists and economists into various aspects of research economics, including several approaches to measuring the impact of Obama's stimulus package. The programme grew out of an initiative to build \"a scientifically rigorous, quantitative basis\" for research policy, launched in 2005 by John Marburger, then science adviser to President George W. Bush. As Marburger explains, \"We need disinterested people \u2014 as opposed to the current situation, where everyone involved has an interest in the outcome.\" Julia Lane, head of the NSF project, is also directing the related STAR METRICS programme. The first aim of the programme is to build a 'clean' database of all federally funded researchers in the United States \u2014 current records are confused, with conflicting information on names and affiliations \u2014 and estimate the number of people that they keep in employment. Later on, the plan is to track patents, citations and other metrics of the research's impact. Lane, like Marburger, suggests that researchers' use of the Internet to communicate and publish will enable STAR METRICS to track the creation and transfer of knowledge properly for the first time. \"In the past, we haven't had the data infrastructure to do a full analysis,\" she says. Tobin Smith, vice-president for policy at the Association of American Universities in Washington DC, is confident that the first STAR METRICS results in summer 2011 will help to show doubters that the stimulus-bill money has been wisely spent. \"It will certainly help me,\" he says, \"by telling our campuses how many people they are keeping in work \u2014 something universities have never been able to do.\" Like Smith, most research leaders and advocates seem assured that new data will reveal the healthy return on investment they have been touting all along. Not that this guarantees that the economic growth argument will continue to persuade. There are signs that a backlash against further research spending is already emerging. In May, the US House of Representatives decisively rejected a bill that would have authorized increased research funding for physical sciences agencies, and in Britain, research spending cuts by the newly elected government are widely anticipated. The pressure is building to show what earlier investments have produced. As one former congressional staffer, who didn't want to be named, puts it: \"If it turns out that all the stimulus has done is hire a load of foreign postdocs, there's going to be trouble.\"   See Editorial,  \n                     page 665 \n                   . \n                     Recession Watch News Special \n                   \n                     US science of science policy programme \n                   \n                     Medical Research: What's it worth? \n                   \n                     Rising above the Gathering Storm \n                   Reprints and Permissions"},
{"file_id": "465544a", "url": "https://www.nature.com/articles/465544a", "year": 2010, "authors": [{"name": "Katharine Sanderson"}], "parsed_as_year": "2006_or_before", "body": "The Icelandic eruption has given researchers the opportunity of a lifetime. Katharine Sanderson talks to scientists working around the clock to study the volcano and its effects. On the evening of 14 April, Gelsomina Pappalardo sent a rushed message to her colleagues across Europe, asking them to switch on their lasers and point them skywards. The volcano Eyjafjallaj\u00f6kull in Iceland had just erupted, and an unusual wind pattern was sending the ash cloud into Europe's crowded airspace. Pappalardo, who works at the Institute of Methodologies for Environmental Analysis outside Potenza, Italy, wanted to use the network of laser-based instruments, known as lidars, to measure the ash as it spread over the continent. It would be a chance for the lidar collaboration to shine. Pappalardo, who coordinates the European Aerosol Research Lidar Network (EARLINET), wasn't the only scientist on high alert. As Eyjafjallaj\u00f6kull sullied the skies, dozens of researchers across Europe started scrambling to gather data. Atmospheric modellers, remote-sensing researchers, air-sampling experts, geologists and volcanologists are all taking advantage of this rare opportunity to measure a volcano in their own backyard. Researchers in Iceland are probing the volcano's plumbing beneath the surface, whereas scientists farther afield are trying to assess the evolution of the ash cloud. All are hoping to fulfil their own scientific curiosity about a rare and dramatic phenomenon, but they must first answer to national governments and civil-aviation authorities, who are demanding data on the eruption as soon as possible to forecast potential problems. That means putting aside normal practices. Working virtually non-stop since the eruption, researchers are quickly gathering data and releasing them, instead of keeping the information to themselves until it is ready for publication. \"It's hard work for us,\" says Pappalardo, who has relied on the goodwill of her team to put in the extra hours for free to address government requests. \"Since the 15th of April, I've been sleeping two or three hours a night, maximum.\" Pappalardo is faced with a problem: her data are highly valued by aviation authorities and atmospheric modellers, who want them instantly. But the measurements are not useful until they are analysed, and that takes time. The data are normally used in long-term studies of clouds, Saharan dust and other atmospheric components that affect the climate. \"This is not a simple technique,\" she says. Pappalardo wants to ensure that the information she gives to others is accurate, so that forecasts made with those data are correct. So she stays up late, collating and analysing data from the 26 lidar stations across Europe, and then making the results available to aviation authorities. In the future, she says she would like governments to support the infrastructure for real-time monitoring and automated analysis. Faster access to such data would help atmospheric modellers improve their projections of ash concentrations at various altitudes. Aviation officials need that information when deciding whether to close airspaces (see   Nature  464, 1253; 2010 ).  \n                An eye on Eyjafjallaj\u00f6kull \n              Icelandic scientists are at the centre of the action and have been the busiest of all. For them, Eyjafjallaj\u00f6kull is more than just a recent air-traffic nuisance. \"I've been keeping a close watch on this volcano for 18 years,\" says Freysteinn Sigmundsson, a volcanologist from the Institute of Earth Sciences at the University of Iceland in Reykjav\u00edk, who is interested in how Earth's crust deforms during seismic events. The volcano has been seismically jumpy for the entire time that Sigmundsson has studied it. But the earthquake activity intensified in December 2009, and came to a head with a lava-spewing eruption on the volcano's flank on 20 March. The spike in seismic unrest led authorities to evacuate people from the volcano's immediate vicinity before its summit started shooting ash on 14 April. This eruption seems to be similar to the volcano's previous one in 1821, which pulsed on and off for more than a year. Although the nineteenth-century pattern might indicate that the present eruption will be prolonged, the magmatic plumbing under Eyjafjallaj\u00f6kull is complicated says Sigmundsson, making predictions difficult. \"There is no way of knowing how long this will last,\" he says. Sigmundsson and his colleagues want to trace the magma conduits that feed the volcano. To do this, they track ground movement using Global Positioning System receivers around Iceland and satellite-borne radar interferometers, while seismic recordings provide information about underground structures. Such data are useful for the Icelandic Meteorological Office and the government, which must assess whether the volcanic activity is likely to increase. Magn\u00fas Tumi Gudmundsson, head of the faculty of Earth sciences at the University of Iceland, is watching the eruption itself using a variety of means, including radar instruments that can see through the ash cloud into the crater. His research group is studying the composition and amount of ash and rock discharged from the volcano, as well as water generated from the melting of glacial ice on top of Eyjafjallaj\u00f6kull. These data can help to predict whether the volcano will produce ash or lava, as well as whether floods are likely.  \n                Fire and water \n              The meltwater produced by the initial stages of the eruption had far-reaching consequences. Researchers suspect that the interaction between water and magma produced exceptionally fine-grained ash. Between 50% and 70% of the ash grains were less than 100 micrometres in diameter, and some were smaller than 10 micrometres, says Gudmundsson. Such minute particles stay airborne longer than heavy particles, and they can get into jet engines and then melt, causing damage. Because the eruption quickly melted all ice near the summit, the ash grew coarser after the first four days. As they gather their data, Gudmundsson and his colleagues upload much of it to the university's website and send it to the Icelandic Meteorological Office. Gudmundsson says that 20 people from his institute are working \"flat out\" to monitor the volcano \u2014 putting aside regular research projects to do so. Eventually, Gudmundsson hopes to make geological sense of the data, and use them to investigate the magma chambers beneath the volcano and what controls the changes in magma behaviour, particularly when it interacts with water. At the moment all ideas are \"highly speculative\", he says. \"I haven't had a chance to get a handle on changes that are happening.\" While Pappalardo, Sigmundsson and Gudmunsson's teams rushed to measure the eruption, Urs Baltensperger just had to bide his time. Baltensperger runs an atmospheric chemistry lab at the Paul Scherrer Institute in Villigen, Switzerland, which has instruments atop Jungfraujoch, a 3,500-metre-high Swiss mountain. Once Baltensperger realized that the ash cloud was on its way, he knew that this was a data-collection opportunity too good to miss. \"We just had nothing to do but sit back, relax and wait for the plume to arrive,\" he says. Baltensperger's team, and his colleagues at Empa, the Swiss federal laboratories for materials science and technology, used filters and other instruments to determine the ash's mass concentration \u2014 the mass per volume of air. His group also measured how much light the ash cloud was blocking. From those two values, the researchers could calculate a parameter needed to turn lidar information into measures of how much ash is in the sky, of what type, and at what altitude. Such data complement the information provided to aviation authorities by Pappalardo's lidar network. Baltensperger was happy to pass his results on to the Swiss aviation authority. But, like Pappalardo, he says that to monitor future eruptions properly, real-time analysis and an automated data-transmission system are required. The eruption and the crisis it created in Europe are uniting researchers in unusual ways. Thor Thordarsson, a volcanologist from the University of Edinburgh, UK, who was in Iceland when the eruption began, says that because the ash blew over northern Europe, with its dense research network, it presents a unique opportunity to reconstruct the entire eruption and work out what happened within the volcano. But, as he told a group of UK-based scientists in Oxford at the end of April, \"It can only work if everyone works together.\" Sigmundsson has taken a strategic approach to the release of data. His group posts updates on the seismicity, meltwater and the nature of the plume, such as its height, colour and heading. But it doesn't put up raw information. \"We are expecting other scientists to allow us time to fully analyse those data for scientific publications,\" he says. Pappalardo says she hopes that her team's efforts to get lidar data out to the people who need them will bring something in return. The EARLINET lidar network has been running for a decade, at times on a completely voluntary basis. In a brief break from her work, she says the eruption might help to secure long-term funding for the network. \"Now everybody wants to have these data.\" \n                     Nature Geoscience \n                   \n                     EARLINET \n                   \n                     Iceland Institute of Earth Sciences \n                   Reprints and Permissions"},
{"file_id": "466812a", "url": "https://www.nature.com/articles/466812a", "year": 2010, "authors": [{"name": "Virginia Gewin"}], "parsed_as_year": "2006_or_before", "body": "Every summer for the past nine years, water with lethally low concentrations of oxygen has appeared off the Oregon coast. The hypoxia may be a sign of things to come elsewhere, finds Virginia Gewin. The dead fish were one of the first signs. In July 2002, scientists with the  Oregon Department of Fish and Wildlife  found unusual numbers of bottom-feeding sculpin lying lifeless on the ocean floor, which would normally be teeming with life. Crabs were also dying, and they washed up onto some beaches in large numbers. Officials at the government agency asked Francis Chan, a biogeochemist at  Oregon State University  in Corvallis, for help in discovering the cause of the disturbance as quickly as possible. Chan was about to set off on a scheduled research cruise along the Oregon coast, so he grabbed all the extra equipment he could think of, including a brand-new oxygen sensor. Ocean surface waters normally contain 5\u20138 millilitres of oxygen per litre of water, a number that declines rapidly with depth. But on his first day out, Chan found that at a depth of 50 metres the inner coastal waters off Oregon were hypoxic \u2014 oxygen levels there were lower than 1.43 millilitres per litre, so low that fish can't survive 1 . Many regions of the world have hypoxic coastal waters, usually caused by agricultural fertilizers leaking into the ocean. The excess nutrients fuel plankton blooms, which consume oxygen. But Chan knew that the hypoxia off the Oregon shoreline must have a different cause, because that part of the coast does not have enough farming to explain it. And when his colleague Jack Barth, an oceanographer from Oregon State University, found similarly low oxygen levels farther offshore, the researchers knew that something unprecedented was happening. \"I remember thinking at first, 'Lucky us, we saw this once in a lifetime event',\" says Chan. But it wasn't an anomaly. Every summer since then, much of the oxygen has disappeared from a large patch of inner continental-shelf waters. In the most extreme case, in 2006, the waters lost all detectable oxygen for four weeks. Starfish and mussels died, and rockfish and other mobile fish fled the hypoxic zone, which grew to 3,000 square kilometres (ref.  2 ). The phenomenon has worried the fishing industry in Oregon, which brings in hundreds of millions of dollars each year and now watches each spring and summer for the return of the deadly waters. At the same time, scientists have puzzled over the cause of the hypoxia and whether it might be linked to global warming or to natural variations in climate. Oceanographers, including Chan, are scrambling to investigate the hypoxia, which is expected to reach this year's peak in a few weeks. By tracing the origins of the oxygen-deprived waters, they hope to produce forecasts that can help fisheries managers to minimize the harm to marine life in the region. The changes in Oregon may be related to a broader pattern around the globe, in which subsurface patches of permanent hypoxia seem to be growing in size and losing yet more oxygen, for unknown reasons. And whether or not global warming is responsible for the changes to date, ocean models forecast that in the coming decades increasing water temperatures and changes in circulation will drive oxygen concentrations down even further. \"What we have been experiencing is a perfect storm \u2014 where weather, climate and currents can come together to crash an ecosystem,\" says Chan.  \n                Deep trouble \n              In mid-June this year, Chan and his crew of eight ploughed through 3-metre swells to collect water samples along a track leading from the open ocean to the inner continental shelf. Many of the researchers wore anti-nausea patches; others turned green during the 12-hour trip as they hoisted the heavy sampling equipment onto the deck of the  RV  Elakha . The scientists were searching for signs that the oxygen levels were starting to decline, a sign of an impending hypoxic summer. Storms had repeatedly delayed Chan's cruises during the spring, and he wondered whether the weather was also delaying the oxygen's vanishing act by keeping the winds in a favourable, generally southerly, pattern. Usually in the spring, occasional periods of northerly wind blow surface waters offshore, allowing cool waters, rich in nutrients but poor in oxygen, to upwell from deeper, offshore layers. That upwelling is what makes Oregon's fisheries so productive. But upwelling can turn deadly for creatures near the sea floor if the winds are unrelenting. Normally, the winds that promote upwelling slacken at various times during spring and summer, enough to mix the waters on the continental shelf, refreshing them with oxygen. But some recent years have brought strong and steady spring winds that prevent oxygen from reaching subsurface waters. At the same time, the constant upwelling spurs blooms of phytoplankton, which die and then decompose, using up oxygen in the nearshore waters (see 'Danger below'). \n               Click here for larger image \n               Chan and Barth have identified a second, more distant, factor that predisposes Oregon's waters to become hypoxic. Farther out to sea, beyond the continental shelf, water at a depth of roughly 600\u20131,200 metres is permanently oxygen deprived, at roughly 0.5 millilitres per litre. Called an oxygen minimum zone (OMZ), the layer is a normal feature in many parts of the ocean; it is too far down to mix with the well oxygenated surface waters. The waters above an OMZ are expected to have slightly lowered amounts of dissolved oxygen. But Chan and Barth have discovered that the water above the OMZ off the Oregon coast is steadily losing oxygen. The researchers pulled together 30 years of offshore recordings and, in an unpublished preliminary analysis, found that concentrations of the gas have dropped by 0.5 millilitres per litre and are now about 2 millilitres per litre \u2014 dangerously close to hypoxic conditions. These are the very waters that well up onto the continental shelf in the spring and summer. Chan estimates that this oxygen decline above the OMZ has pushed the chances of seeing hypoxia in the nearshore waters from 10% to roughly 60% each year. Around the globe, many OMZs are also losing oxygen and expanding horizontally and vertically. OMZs currently cover about 30 million square kilometres, or 8% of the ocean area. These regions have mostly attracted little attention from researchers, but the best known OMZs sit off the coasts of Namibia, Chile and Peru \u2014 also areas of strong upwelling. Lothar Stramma, an oceanographer at the University of Kiel in Germany, cobbled together the sparse existing records and found that oxygen concentrations declined by between 0.1 and 0.4 millilitres per litre over the past 50 years within the OMZs in most regions of the equatorial Pacific, Atlantic and Indian Oceans, where the OMZs naturally reach much shallower depths 3 . And the area with oxygen levels intolerable for fish survival had grown by 4.5 million square kilometres (ref.  4 ), about double the size of Argentina. Many of the OMZs are adjacent to continental shelves, and their expansion makes it easier for water with low oxygen concentrations to lap up onto those shelves. Chan and other researchers are trying to determine whether the warming climate is to blame for expanding OMZs and the recent upwelling along the Oregon coast. A connection is possible, in theory. Climate models suggest that ocean oxygen concentrations will decline in the future, mainly because increasing temperatures of the surface waters impede mixing with deeper layers, and the warming also reduces the solubility of oxygen in the water. On top of that, heating of the polar regions may slow down the giant currents that carry oxygen from cold regions to warmer ones. Various model runs forecast that the global oxygen content of oceans will decline by 1\u20137% over the next century 5 . At this point, though, the Oregon researchers are reticent to make global warming the scapegoat. \"We're being rigorous in how we approach it, and are careful to not cry wolf,\" says Alan Mix, a marine geochemist at Oregon State University. The main problem with investigating the link between climate change and hypoxia is that researchers lack long-term measurements of oxygen concentration, temperature and many other variables for most of the ocean. \"We need more evidence to conclusively fingerprint global warming \u2014 and that requires a full-time presence in the ocean,\" says Barth. Four years ago, he began deploying two automated gliders to patrol the continental shelf. The gliders, which are equipped with sensors for temperature, salinity, oxygen and the chlorophyll in plankton, have logged 36,400 kilometres to date during 1,664 days at sea. Shaped like torpedoes, the gliders pop to the surface every six hours and send the data back to Barth by satellite phone. Barth is also spearheading the northwestern coastal portion of the  Ocean Observatories Initiative . This $385-million, US-wide programme, funded by the National Science Foundation, will pay for an array of offshore instruments on the ocean floor and in the water above, as well as a further four gliders patrolling the waters off the coasts of Oregon and Washington, starting in 2012. Barth wants to locate the source of the low-oxygen water that wells up onto the shelf and feeds the hypoxic events during the summer. Some of the samples collected by Chan's crew will help to pinpoint the origin of those unwelcome waters. Mix will use the isotopic signature of the oxygen in the water molecules to track whether it came from northern or southern regions of the ocean. This will help to determine whether expanding OMZs in distant parts of the Pacific have contributed to the hypoxia events on the continental shelf of Oregon. If so, the fish deaths there may serve as a warning to other coastal zones in the ocean, which may soon start to see similar hypoxia.  \n                Fishing for answers \n              Some researchers in southern California are already on the alert. They have yet to see any effects on fisheries, but deeper waters off the continental margin have experienced a 20\u201330% decline in oxygen, according to Lisa Levin, a marine biologist at the Scripps Institution of Oceanography in La Jolla, California. \"Right now, it's still out of sight, out of mind,\" says Levin of the low-oxygen water. But the hypoxic conditions are creeping up the water column, rising by as much as 90 metres between 1984 and 2006 off the coast of Santa Barbara, California 6 . \"My view, which is only an educated guess, as we don't have all the information, is that the expansion of OMZs that we've seen recently is driven by natural cycles,\" says Francisco Chavez, a biological oceanographer at the  Monterey Bay Aquarium Research Institute  in Moss Landing, California. But if ocean circulation slows over the coming decades, as it is expected to do, many OMZs will expand, including the world's largest, which spans the southwestern coast of North America and the northwestern coast of South America. The Oregon State University team's working hypothesis is that the growing OMZs will come within reach of many coastal regions, raising the risk that upwelling currents will carry oxygen-starved waters to areas that have thus far remained free from hypoxia. Chan says that his most immediate concern is how to advise fisheries managers, who are already facing waters that are growing warmer and more acidic because of climate change and increasing levels of dissolved carbon dioxide. \"We have no idea whether marine organisms will be able to adapt to such fast changes,\" he says. \"This is evolution in real time.\" Bottom dwellers, such as crabs and starfish, are most vulnerable to the hypoxia and die first. Humboldt squid, which are tolerant of low-oxygen conditions, move in to feast on the remains. These 1.75-metre cephalopods have become a mascot of low-oxygen studies; the arrival of the squid and changes in the population of microbes are considered signs of a system in trouble (see  'Natural born sensors' ). Unlike bottom dwellers, fish can swim away from the hypoxic waters, and fishermen have learned to take advantage of that behaviour. \"As OMZs expand and move closer to shore, they push the intolerant species upwards and narrow the amount of the water column they have to live in,\" says Levin. That makes them easier to catch, so even stocks of fish that have escaped the hypoxia are vulnerable to its impacts. \"We're going to have to manage fisheries differently,\" says Frank Whitney, an oceanographer with Fisheries and Oceans Canada, based in Victoria, British Columbia. In the past, he says, there was enough oxygen in the ocean, so fish could find refuges when upwelling caused local hypoxia. But that may be changing. If oxygen levels continue to decline, he says, managers may have to think about setting aside marine reserves \u2014 areas closed to fishing \u2014 in both deep and shallow waters to give fish ample refuges during hypoxic events. Off the coast of Oregon, the situation seemed promising this spring, because the storms kept the coastal waters well oxygenated. Chan and other researchers there had hoped that the weather might break the recent cycle of hypoxia. But when he went out on the RV  Elakha   in June, Chan found that oxygen values had started to drop just as they have every year since 2002. He returned to sea late last month and saw a that there has been a steady decline in oxygen in the nearshore waters. Summertime hypoxia, it seems, has become the norm in this part of the world. Virginia Gewin is a freelance writer based in Portland, Oregon. \n                     PISCO Hypoxia webpage \n                   \n                     Lothar Stramma \n                   \n                     Osvaldo Ulloa \n                   \n                     Jack Barth \n                   \n                     Ocean Observatories Initiative \n                   \n                     Microbial Initiative in low oxygen areas off Concepci\u00f3n and Oregon \n                   Reprints and Permissions"},
{"file_id": "468886a", "url": "https://www.nature.com/articles/468886a", "year": 2010, "authors": [{"name": "Alison Abbott"}], "parsed_as_year": "2006_or_before", "body": "To learn the chemical language of plants, Ian Baldwin has built up a German research empire that engineers seeds \u2014 and a field station in the Utah wilderness to grow them. In late spring 1988, Ian Baldwin was driving through the dessicating heat of the Utah desert in his rickety old VW microbus. The young researcher, from the State University of New York (SUNY), Buffalo, was searching for a native species of the tobacco plant as well as a place to sleep for the night. When he pulled up at the Desert Inn Ranch, he encountered a different form of wildlife. A posse of ferocious dogs flew out of the gate, puncturing his car tyres with their teeth. Behind them was rancher Herb Fletcher, cradling a submachine gun. Baldwin was terrified. But when Fletcher called the dogs off, Baldwin slipped, very cautiously, out of the bus. Fletcher smiled \u2014 \"He had a wonderful smile,\" recalls Baldwin \u2014 and invited him in. The scientist and the old rancher quickly bonded over their shared interest in natural history. It was the start of a firm friendship \u2014 and the opening of a new era in Baldwin's research life, one that has helped propel him into a dominant position in the burgeoning field of chemical ecology, the study of the chemical signals between plants and other organisms in the environment. Rooted and unable to flee, plants have evolved many ingenious ways of repulsing their enemies, from generating noxious chemicals in their leaves to emitting complex, volatile bouquets to attract predators that will pick off the plant's attackers 1 . It is a highly sophisticated chemical language undetectable by the human nose and largely undeciphered by science. But if and when it can be understood, it might open the way to modifying plants' signals to give them stronger protection, or to developing environmentally friendly mimics of natural signals as alternatives to herbicides. In his efforts to understand this language, Baldwin has embarked on a project unique in its ambition and scale, carried out along what he calls \"the longest lab corridor in the world\". Working in Jena, Germany, where he is a director of the Max Planck Institute for Chemical Ecology, he and his team develop powerful genetic tools to systematically knock out, or knock down, genes involved in making the chemical signals. Then they observe the effects by growing the modified plants in the wild \u2014 8,844 kilometres away, next to the Utah ranch. The fastest journey from Jena to the field station takes 27 hours. The researchers have little choice, however. In Germany, with its populist aversion to anything genetically modified, such trials cannot proceed.  \n                Cocktails and craziness \n              High-profile papers roll out of Baldwin's institute with regularity. One published in  Science   in August 2  showed that the plants, when nibbled by herbivorous insects, can change the ratio of isomers of some of their signalling molecules specifically to attract predators of the leaf-eaters. And although Baldwin deliberately keeps his distance from applications, the agricultural industry studies his results attentively. It wants to learn how plants, or mixtures of plants, might be persuaded to produce the best cocktails of volatile emissions for their own defences. \"Ian Baldwin is like a madman,\" says Ted Turlings, a plant scientist at the University of Neuch\u00e2tel in Switzerland, with some awe. \"He doesn't stop working, day and night, and he lets nothing get in his way. He sets up a field station in an area where no one would think of going, builds himself a complete molecular tool set for his tobacco species, and sets out with purpose to get permission to use transgenic plants.\" For Baldwin, though, the approach is the only way to learn how a particular plant has evolved to survive in the real, stressful world of harsh weather and hungry insects. \"It seems to me it would be madder not to do it this way,\" he says. With his humorous and low-key manner, and customary jeans, plaid shirt and baseball cap, Baldwin, 52, shows no sign of frenzy. The son of academic historians in Baltimore, Maryland, he decided early on that the ivory tower was not for him. \"My parents are medievalists and live in the eleventh century,\" he says. \"I wanted to know what the rest of the world did for a living.\" During high school and college he worked in his spare time as a fish cleaner, landscaper, truck driver, an auto and tractor mechanic, a logger and tree climber, and even a maple-sugar producer. And some of these skills became useful in unexpected ways. As an undergraduate majoring in chemistry and biology at Dartmouth College in Hanover, New Hampshire, he became an informal assistant to Jack Schultz, one of the earliest pioneers of chemical ecology. Schultz, who was studying forest canopies but couldn't stand heights himself, was as attracted to Baldwin's tree-climbing skills as to his precocious ability in the chemistry lab. In a joint experiment published in  Science   in 1983 3 , the pair claimed that chemicals from leaves that had been ripped to mimic insect damage could travel through the air to neighbouring plants and change their biochemistry in a way that wards off further insect attack. Their 'talking trees' notion was dismissed by many plant scientists as a fanciful over-interpretation of results. Burned by the reaction, Baldwin decided to play it safe for his PhD studies at Cornell University in Ithaca, New York, and instead researched the more mainstream internal signalling pathways within plants. The favoured plant model in the Cornell lab was  Nicotiana sylvestris , a species of tobacco native to Peru. But when it came to extending this work into the field, Baldwin, by now on the tenure track at SUNY, decided to switch to a similar species native to the United States. His hunt for  Nicotiana attenuata   in Utah led to the fateful encounter with Fletcher. Baldwin and his family spent the next seven summers with Fletcher, who regaled them with stories about the area's violent recent history. Fletcher had been brandishing a weapon at his first meeting with Baldwin because a neighbouring family of polygamists had been attacking him and his land. A mobster friend eventually put a stop to it. If, during those summers, Baldwin learnt new things about how humans defend themselves, he learnt even more about plant defences. Fletcher would drive Baldwin around the 1,300-square-kilometre property to point out small clumps of  N. attenuata   he had spotted while out ranching, and Baldwin would use them in experiments to find out, for example, how the plants activated chemical defences against herbivores.  \n                The hunt for hotspots \n              After a brush fire in 1992, Baldwin discovered that the seeds of  N. attenuata   germinate only when activated by components of wood smoke penetrating the soil around seeds. Then they suddenly flourish in the temporarily nutrient-rich, herbivore-free, post-fire environment. He learnt to locate natural populations of  N. attenuata   more efficiently by chasing lightning strikes \u2014 \"or simply phoning the fire department and asking them where they spent money\". One of his earliest series of experiments was designed to understand the costs and benefits to the plant of one particular defence mechanism \u2014 producing nicotine in its roots and then pumping the toxin up into its leaves \u2014 in the field. Plant biology had been transformed in 1990 with the discovery that the hormone jasmonic acid could induce volatile signalling. Baldwin immediately set out to see if it could also induce nicotine production, and found that it could. So he synthesized an artificial version of the hormone that he could inject into the soil around plant roots. His complex set of experiments involved four populations of  N. attenuata , each with more than 1,000 plants, spread over a 100-kilometre loop. He found that among plants pretreated with jasmonate to artificially induce a level of nicotine defence, those that were not attacked produced less seed than those that subsequently were attacked, but not ravaged, by herbivores 4 . With this work, he proved a point about plant defence that had previously only been assumed \u2014 that using defences only when needed is an evolutionary advantage, because it maximizes benefits and minimizes cost. More than a decade after his ill-received 'talking trees'  Science   paper, Baldwin was itching to return to the theme. The work had been vindicated by the early 1990s when several scientific groups working on crop plants had established that such plant volatiles not only exist, but stimulate responses from other species \u2014 pathogens, herbivores, herbivore-eating carnivores, and perhaps other plants as well. But most of the work had been done under laboratory conditions. Baldwin wanted to understand plant biology in the real world. He particularly wanted to explore the volatile chemicals that plants exude, using genetic manipulation to take apart the machinery involved in making them. He was convinced that the desert-dwelling  N. attenuata   would make an ideal model because it has evolved such an array of mechanisms to survive severe environmental stresses including fire, herbivores and drought. What wasn't yet available for  N. attenuata , however, was the toolkit necessary for genetic engineering. One was already being assembled for  Arabidopsis   \u2014 the favoured study subject of lab-based plant biologists, but one that Baldwin regarded as \"a boring weed of no use to ecological evolution research\". He wanted to have the same for his  N. attenuata   \u2014 but he knew it would be expensive and take many years. That didn't discourage the Max Planck Society in Munich, which recruited him in 1995. Following the reunification of Germany in 1990, the society was obliged to extend its network of research institutes into former East Germany, and it took the opportunity to add exciting areas of research and recruit more foreign directors. The society wholly bought into Baldwin's vision, and made him a founding director of the Max Planck Institute for Chemical Ecology in Jena. All Max Planck directors are given generous, guaranteed funding and time to develop long-term projects without having to apply for grants, making Baldwin's dream possible. Baldwin now has a formidable institute employing more than 50 researchers, students and technicians to develop the chemical, as well as genetic tools to mimic or block signalling pathways. Not even the might of the Max Planck Society could help him realize one part of his original vision \u2014 to stage field trials of genetically modified plants native to Germany. \"Even if you do get approval, the German government now requires the GPS positioning of all field trials with transformed plants to be posted on the web \u2014 so every single trial is destroyed by activists,\" he says. In the United States, the work is arduous but doable. The team in Jena engineers seeds and sends them to the US Animal and Plant Health Inspection Service in Rockville, Maryland, where they are inspected and sent on to the field station in Utah. This season, Baldwin's team planted out 4,000 seedlings for around 36 studies on topics ranging from plant\u2013pollinator interactions to how plants allow their roots to be colonised by microbes. \"This is really a lot of backbreaking work,\" says Baldwin. Life at the field station is tough in other ways. Brush fires spread \"faster than you can run\", says Baldwin. In 2005, a fire spotted on the horizon sent a dozen or so scientists running for their lives. Aware that their station has an explosive tank of propane fuel, they tore away in vans. (The fire shifted direction before reaching the station.) Baldwin admits to being \"tyrannical\" about safety. In a region shared with deadly animals such as the sidewinder rattlesnake, he forbids iPods and sunglasses \u2014 \"people have to be able to hear and see snakes\" \u2014 and no one is allowed to wander around the desert alone. It is an hour's rough drive to the nearest hospital. Everyone must learn how to change a tyre on the vans. \n                Life in the wild \n              Nature can add to their troubles in other ways. This year, for example, an elaborate series of experiments designed to study how the empoasca leaf hopper, a herbivore, recognizes and interacts with its host plant went to waste because the leaf hopper didn't show up. But results continue to flow. One of Baldwin's most colourful papers, published in February this year 5 , probes the dilemma of plants that need to attract pollinators while remaining inconspicuous to herbivores.  Nicotiana attenuata   normally flowers at night, emitting the volatile benzyl acetone to attract hawk-moths. Unfortunately, hawk-moth larvae are also herbivores, and the moths often leave their eggs on the leaves as they pollinate. When the plants become infested, the team found, they shut down production of benzyl acetone and open their flowers at dawn when the moths are gone. They are then pollinated by hummingbirds. Using a series of genetically modified strains, Baldwin's team showed how the oral secretions from the munching hawk-moth larvae trigger the dramatic switch in flowering time. Baldwin's research has inspired attempts to develop new crop strains that could be practical for farmers in poorer countries who can't afford lots of pesticides and herbicides, says John Pickett, director of Rothamsted Research in Harpenden, UK, a historic agricultural research centre. The centre is working with some agricultural companies on field trials in the United States and United Kingdom of crops genetically modified to amplify chemical signals that plants make when they are under attack. Details are currently confidential, he says, but \"we'll have big announcements in the next year or two\". Back in Jena, at the end of the 2010 season, Baldwin is planning next year's experiments with genetically altered strains designed to have elevated or suppressed emissions of volatile signals, which he labels as the 'screamers' or the 'mute'. He will investigate how single screamers planted among a colony of mutes, for example, might affect herbivore or predator behaviour. Right now, Baldwin is 8,844 kilometres away from the next experiment at his barren, hostile field site. Human relations in the region are now a lot tamer. Relations between plants and insects, though, are as wild as ever. Alison Abbott is Nature's senior European correspondent. \n                     Special: Can science feed the world? \n                   \n                     Paging Dr Baldwin \n                   \n                     Ian Baldwin \n                   Reprints and Permissions"},
{"file_id": "465540a", "url": "https://www.nature.com/articles/465540a", "year": 2010, "authors": [{"name": "Mark Schrope"}], "parsed_as_year": "2006_or_before", "body": "New England fishermen have mixed feelings about a programme designed to allow overfished species to recover. Mark Schrope reports on how catch shares have scientists fishing for answers. Dave Marciano first started working on commercial fishing boats when he was ten. Unlike many from his hometown in Gloucester, Massachusetts, Marciano didn't have deep generational ties to the sea. His father sold insurance. He nevertheless saved up the money to buy his first boat in 1993 \u2014 the  Angelica Joseph , an 11-metre, Maine-built lobster rig. But in May, things changed dramatically for Marciano, 45, when regional fisheries managers instituted a new system to regulate the harvesting of 'groundfish' \u2014 cod, haddock and other predators that live on or near the sea floor. Convinced that he could no longer survive financially, he sold his fishing permit. He doubts he'll find a buyer for his current boat. The changes in New England \u2014 the region containing Massachusetts, as well as Connecticut, Maine, New Hampshire, Rhode Island and Vermont \u2014 portend what is likely to be the main thrust of US fisheries management for years, if not decades, to come. Under pressure from stiff new legal mandates, managers have had to slash the size of allowable catches for overfished species. To do this, they've chosen to implement catch shares, a radical departure from past management. Where previous systems enforced seasonal, area-based and other limits to reduce catches, catch shares give set quotas to individual fishermen, who are then free to decide how and when to fill them. They can also lease quotas to others or, as in Marciano's case, sell them off. In theory, catch shares have economic and ecological benefits, making for steadier income streams for fishermen and improving incentives for environmentally friendly practices. Many researchers, conservationists and even some fishermen have grown increasingly enamoured of catch shares and hail an emerging policy at the National Oceanic and Atmospheric Administration (NOAA) encouraging their widespread use as part of the solution for ailing oceans. These supporters are not insensitive to the social and economic costs associated with major management shifts, but they say the long-term benefits will be worth it. Still, some fishermen, legislators and scientists question whether the policy will work to replenish stocks and whether it is fair to fishermen. Scientists are struggling with the fact that catch shares have had different effects in different places. \"Catch shares are an effective tool for addressing a lot of economic problems,\" says Roy Crabtree, southeast regional administrator for NOAA's Fisheries Service, \"But I think the benefits from a biological perspective really have to be looked at case by case.\" Catch-share programmes have been used to a limited degree in the United States, but the New England groundfish programme, because it is particularly challenging, will provide a unique test case for NOAA's new catch-shares policy, with lessons for other US fisheries and beyond. In the early seventeenth century, the English privateer Bartholomew Gosnold named the crooked finger of land extending from the Massachusetts coast Cape Cod, after the fish then so abundant in its surrounding waters that they \"vexed\" his crew. But as early as the 1800s, it was the lack of cod that became vexing, with fishermen travelling ever farther offshore for their quarry 1 . Eventually, cod became an infamous cautionary tale of ocean exploitation, and a key target of restoration. The situation varies within New England waters, but one of the most troubled cod stocks, on the Georges Bank, a 29,000-square-kilometre shallow located 100 kilometres off the Cape (see map), stands at just 12% of restoration goals, which in turn are probably a fraction of historic levels 2 .  Click here for larger image For decades, fisheries managers have tried increasingly restrictive methods to rebuild fish populations. In the 1990s, the New England Fisheries Management Council settled on the Days at Sea system. This is a complex series of fishing curbs including limitations on the number of days a fisherman can fish and how much fish can be caught per day, along with various fishing-ground closures. It essentially mandated inefficiency to limit catches and was restrictive enough to slash the number of active fishing boats from about 1,200 in 2001 to roughly 600 by 2009.  \n                Disaster management \n              Some fish populations increased under the Days at Sea policy. However, most people agree that the system didn't work economically. \"Days at Sea has been a disaster,\" says Robert Steneck, a marine ecologist at the University of Maine in Walpole. Jackie Odell, executive director of the Northeast Seafood Coalition, an industry group based in Gloucester, agrees. \"People were very frustrated,\" she says \u2014 fishermen and scientists alike. An even greater incentive to seek a new management system came in 2007, with the reauthorization and revision of the Magnuson\u2013Stevens Fishery Conservation and Management Act, wide-ranging national legislation passed in 1976 to regulate fisheries. New provisions included two ambitious mandates. First, managers had to eliminate overfishing \u2014 the practice of taking more than a sustainable level of fish \u2014 by 2010, by setting a total allowable catch for each stressed population each year. Second, if a stock is officially overfished \u2014 a distinct designation meaning the population is too low to be stable \u2014 total allowable catches need to be low enough to allow stocks to rebuild within a decade. Catch shares began looking more attractive than traditional management structures. One group of fishermen in 2004 got the management council to approve such a programme for cod caught on hook and line on the Georges Bank. In 2006, the pilot programme expanded to include net fishing in the area. In June 2009, the council decided to adopt the scheme as its primary management tool. Some countries have been adopting and modifying catch-share programmes since the 1970s. Considered a market-based solution, the idea is to minimize the competition for a limited resource by giving individual fishers the right to catch a certain amount of fish. Among the potential benefits, quotas can stabilize fishermen's income and allow them to fill their quotas whenever they like, spreading out fishing efforts. Doing away with season restrictions reduces 'derby' conditions, in which fishermen race out, even in dangerous weather, to catch as much as possible. It also eliminates seasonal market gluts, potentially increasing the prices fishermen can command for their catch. On the ecological side, catch shares can be designed to limit the catch of non-target fish, increase populations of regulated fish and possibly encourage better resource stewardship. Although many catch-share systems are based on giving fishermen individual quotas, the New England council opted for collective sectors in which groups of fishing operations are allotted a quota and can determine what portion of it goes to each sector member. For the quota system in New England, a major contention is not so much the catch shares themselves, but how the quotas are set. And here, say many, the science is lacking. The Magnuson\u2013Stevens reauthorization dictates that NOAA scientists conduct annual assessments for each managed population. In some cases, assessments were being done about once every five years. And it's doubtful that NOAA currently has the resources to scale up the effort. \"That's big science,\" says Steve Cadrin, a biologist with NOAA's National Marine Fisheries Service based in Woods Hole, Massachusetts and chair of the scientific committee that advises the New England management council. Cadrin says that the agency has proposed relying on multiyear surveys that would set catch limits for a similar amount of time. \n                Stock growth \n              Some have also questioned the accuracy of certain current surveys. In New England, the best available data show that overfishing is going on for 11 species or regionally distinct populations. Even more populations are already low enough to qualify as officially overfished. The New England Fishery Management Council is therefore required by law to drastically lower catch rates in 2010. Marciano says that the measures being taken don't align with his experience on the water. \"If you set the technical definitions of overfishing aside for a minute and look at the stocks overall, you'll see nothing but upward trajectory for the past 15 to 20 years,\" he says. \"There are more fish now than I've seen my entire life.\" But fishermen have a different perspective from scientists, says Cadrin. \"They're out there every day and they know their fishing grounds very well, but they probably don't have as broad a view of all the different areas and different species.\" For instance, they may not appreciate the importance of rebuilding a variety of stocks if at least some stocks are doing very well. Recent research even suggests that genetic variety within a species is necessary to ensure a secure fishery 3  (see  page 609 ). Other scientists, however, find fault with the targets. Brian Rothschild, who oversaw implementation of the original Magnuson\u2013Stevens act for NOAA is now a professor at the University of Massachusetts in Dartmouth. He says that the restoration targets for some troubled species are too high and that the ecosystem isn't equipped to handle maximum levels of all species all the time. \"The carrying capacities of ecosystems don't really work that way,\" he says. Another question is whether catch shares are the best way to achieve mandated population restorations and increased biodiversity. A study 4  published in 2008 analysed a database of more than 11,000 fisheries, 121 of which were managed using some form of catch shares. The study focused mainly on landings data, and looked for drops in catch as a sign of fishery collapse. The authors concluded that those under catch shares were less prone to collapse and that catch shares can halt or even reverse collapse. Other studies, using different methods, have suggested a more complicated picture. Timothy Essington, a fisheries ecologist at the University of Washington in Seattle, for instance, looked in more detail at 15 catch-share programmes in North America, analysing their effect on fish numbers, the use of habitat-damaging gear and commercial landings 5 . He found that results varied widely between systems, with continued population declines for some fisheries even under catch share management. \"It totally depends on the baseline you're looking at,\" says Essington.  \n                Rise of the crustaceans \n              Steneck, who was part of a task force that examined the catch-share concept for NOAA, says that the scheme could address the loss of diversity in New England's fisheries, which he has studied for more than 20 years. Initially focused on lobsters, he noticed early on that fish such as cod that feed on lobster young were suspiciously scarce. His work has convinced him that the ecosystem may well have shifted into a new state dominated by crustaceans. Even efforts to substantially limit fishing for cod on Georges Bank failed to revive the stocks. Although he has some reservations, Steneck says that catch shares could be a step in the right direction for fisheries management. \"We really have to work on breathing more life into the biodiversity of the marine ecosystem for the good and stability of the populations and also for economic diversity,\" he says. One way catch shares may do this is by reducing by-catch \u2014 the non-target fish caught and often discarded. Under the Days at Sea system, and most traditional management schemes, the incentive is to catch a lot of fish quickly and there is no individual penalty for by-catch. Under the catch-share system, fishermen get very small annual quotas for the most troubled species \u2014 choke species \u2014 and throwing fish overboard becomes illegal. Once the choke species quotas are met, all fishing by the sector has to end, providing a strong incentive for avoiding them, for instance by working with other fishers to identify and avoid areas where choke species are most plentiful. In theory, overexploited species will have a better chance of recovery. Another oft-touted benefit of catch shares, and one more difficult to prove, is that the system inspires better resource stewardship, including of the habitat on which a fishery depends as nursery and spawning ground. Fishermen might, for example, call for or acquiesce to new protected areas or fishing-gear limitations if they think it will improve the resources they share. \"I've seen fishermen argue for closed areas and no-trawl zones because of the longer-term perspective that catch shares allow,\" says Rod Fujita, a senior scientist with the Environmental Defense Fund, a non-profit organization, based in San Francisco, California. But Les Watling, a deep-sea ecologist at the University of Hawaii at Manoa, says that anecdote isn't enough. \"I haven't seen anything that demonstrates that the industry will protect bottom habitat as a result of catch shares,\" he says. Beyond such disputes lies the question of how the catch shares should be distributed. In New England's catch-share system, like most, quotas are determined on the basis of fishing history, but this can cause fairness issues. \"The allocation issue really is the tough nut,\" says Essington. \"Chances are there will be winners and losers.\" Marciano definitely considers himself a loser. Last year he landed almost 36 tonnes of cod, but his quota allocation for this year under catch shares was only 16.8 tonnes. He couldn't afford to buy quotas from others. There are no solid estimates of how many fishermen like Marciano will go out of business, but catch-share opponents say it could be 50% or more, a number many managers and scientists question.  \n                Have to be in it to win it \n             Discontent with catch shares isn't universal among independent fishermen. \"If you're in it, you think it's great. If you don't have any quota, you think it's terrible,\" says Larry Huntley, a commercial fisherman based in Pensacola, Florida, where several fledgling catch-share systems are in place. Tom Dempsey, policy coordinator for the Cape Cod Commercial Hook Fishermen's Association in North Chatham, Massachusetts, which has helped to organize some catch-share sectors, says that because of past problems with Days at Sea, many fishermen he works with are optimistic. \"The thinking in town largely is that this is going to help them be more efficient and profitable,\" he says. He adds, however, that his group still has problems with some of the underlying economics of the system, including a lack of measures to protect against consolidation and monopolies. His group has already started a trust fund to purchase quotas that can then be leased to fishermen who might not otherwise be able to stay in the system. Many fishermen and scientists are hoping that the catch-share system will be changed to correct perceived economic inequities and other problems. But politicians aren't always willing to wait. US Representative Barney Frank (Democrat, Massachusetts) is proposing legislation to ease the requirements and timetables of the Magnuson\u2013Stevens reauthorization. Huntley says he fears that such whims may be the greatest threat to the long-term success of rebuilding efforts. In the Gulf of Mexico, for example, in addition to commercial pressure, managers also have to address the concerns of powerful recreational fishing lobbies that have been generally displeased with their allocations under catch-share systems. \"What worries me is that politics may outplay the science,\" says Huntley, \"and I don't want to see that.\" Mark Schrope is a freelance writer in Melbourne, Florida. \n                     NOAA website \n                   \n                     Cape Cod Commercial Hook Fishermen's Association \n                   \n                     New England Fisheries Management Council \n                   \n                     Magnuson-Stevens Fishery Conservation and Management Act \n                   Reprints and Permissions"},
{"file_id": "468748a", "url": "https://www.nature.com/articles/468748a", "year": 2010, "authors": [{"name": "Eugenie Samuel Reich"}], "parsed_as_year": "2006_or_before", "body": "John Reppy has come out of retirement to question the high-profile discovery of a new kind of quantum matter. The fourth time he is asked what the dental floss is for, John Reppy seems to hear. He picks up a pair of scissors, and starts snipping away at the plastic strands wound round the shiny beryllium\u2013copper components of his torsional-oscillator experiment. \"I want to make a change to it anyway,\" he says. As he snips, pieces of wire and piping begin to pop out of the neat cylindrical column he has built, making it completely clear what the floss is for: to hold everything down. The pieces of this experiment, in a basement lab at Cornell University's Clark Hall in Ithaca, New York, span more than half of Reppy's 50-year career studying the behaviour of helium cooled to ultra-low temperatures. Near the top of the metre-long column is a 30-year-old refrigeration unit that Reppy found among the bric-a-brac in his lab a few years after he signed up for retirement. Below that is a torsional oscillator of the type he invented in the 1970s \u2014 a cylindrical vessel, just a few centimetres across, that is free to twist back and forth around a rod running down the centre of the cylinder. When the vessel is filled with the isotope helium-4 via pipes wrapped around the column, and when its temperature is gradually lowered, changes in the oscillation frequency reveal changes in the physical properties of the helium. At two-tenths of a degree above absolute zero, for example, the helium-4 condenses into a solid crystal \u2014 and may even turn into a 'supersolid', a strange quantum state in which some of the atoms seem to pass through others without friction. Or it may not. In recent months, the results from his apparatus have led the 79-year-old, semi-retired Reppy to become a vocal critic of a 2004 claim by physicists Moses Chan and Eun-Seong Kim that they had formed a supersolid in Chan's laboratory at Pennsylvania State University in University Park. The stakes are high: other such macroscopic-scale quantum effects, such as superconductivity and superfluidity, have won their discoverers Nobel prizes. And Reppy knows his criticisms are raising hackles in the field. Others have replicated Chan and Kim's results, yet no one has replicated Reppy's contradictory finding. \"My result is a bolt out of the blue,\" he admits. Nonetheless, as the inventor of the modern torsional-oscillator apparatus, and as Chan's former supervisor, Reppy has a professional stature that makes his views impossible to ignore. \"He's come up with a lot of inventive, clever experimental techniques, and always manages to pick out the experiment that reveals what's really going on,\" says David Lee at Texas A&M University in College Station, one of the winners of the 1996 physics Nobel prize for the discovery of superfluidity in another isotope of helium, helium-3, work done while he was at Cornell.  \n                Heart of the matter \n              The roots of the supersolid controversy go back to 1969, when Russian physicists predicted a state of solid matter in which gaps, or vacancies, in a crystal structure could move together as a single quantum wave \u2014 a collective motion reminiscent of the frictionless flow of a superfluid. In 2004, Chan and Kim reported the first experimental evidence consistent with this 'supersolid' behaviour 1 , 2 . They found that the back-and-forth swings of a torsional oscillator filled with solid helium-4 sped up as the temperature was lowered to below two-tenths of a degree above absolute zero \u2014 just as would be expected if a supersolid were forming inside. The idea is that the zero-friction quantum effects predicted by the Russians effectively decouple some of the atoms in the solid and prevent them from oscillating along with the rest of the atoms. This makes the inertia of the oscillator smaller than the total quantity of helium would suggest, which leads to the faster oscillations (see  graphic ). Chan and Kim's claim prompted enormous excitement, and about a dozen researchers began building torsional oscillators in a bid to replicate the observation. Reppy was one of them. When Chan and Kim first reported their results, Reppy was approaching the end of a five-year Cornell programme intended to ease older faculty members into retirement by steadily reducing their hours, teaching responsibilities and lab space. He was looking forward to a retirement spent rock-climbing \u2014 a field in which his reputation looms as large as it does in physics. A world-class climber since his student days, Reppy is famous for his invention and promotion of clean-climbing techniques, in which the nuts that hold the rope are wedged into existing, natural cracks in rock faces rather than banged in like pitons. He says he likes the technique not just because it is environmentally friendly, but because it is easy. And safe: when he talks about climbing, he doesn't emphasize the obvious excitement it gives him, so much as his caution. \"You always climb with a partner,\" he says. But with supersolidity promising a different kind of adventure, Reppy decided to make a comeback from retirement. When he heard that a fellow faculty member at Cornell had taken on a graduate student, Sophie Rittner, to replicate Chan and Kim's experiment, Reppy suggested that she work with him instead: just as in climbing, he needed a partner. Rittner did not immediately jump at the idea of signing on with a retired professor with a lab full of junk and no active research group. But it was obvious that Reppy would be a good supervisor for her work, as he had had a long career developing experimental tricks for studying superfluidity in helium-3. \"I came to appreciate the fact that I had an adviser with a huge amount of time. He was super hands-on,\" says Rittner. Together they ordered the parts for a torsional oscillator, which Rittner constructed. Then every morning, Reppy came into the lab at about 7 a.m. and started the experiment going. He would stay until about 4 p.m.; Rittner came in later in the morning and stayed into the evening. The pair soon saw the increased oscillation that Chan and Kim had reported. But in February 2006, they tried something new and got a surprise. After one run of recording data, Reppy and Rittner let the frozen helium warm up to just above 1 kelvin, and then lowered the temperature again to repeat the run. The second time around, the speed-up was markedly diminished. Heating and then recooling a crystal, a process called annealing, is in general expected to remove defects in the crystal structure. To Reppy and Rittner, the implications of their observations were clear: the supersolid signal was not due to an intrinsic quantum behaviour of a pure crystal, but was somehow caused by disorder in the structure, which is why it went away when the defects did.  \n                In a spin \n              When Rittner presented the results 3  at the March 2006 American Physical Society meeting, there was something of an uproar, she says. \"People were saying 'what is this?'\" The findings threatened to make supersolidity substantially less interesting, because effects caused by imperfections and impurities often turn out to be impossible for theoretical physicists to calculate exactly. Even now, six years after Chan and Kim's experiment was published, there is still no comprehensive theory of supersolidity. And when Rittner gained her PhD, she decided to move on to a different research field. \n               Click here for larger image \n               Left on his own, Reppy \u2014 an inveterate tinkerer \u2014 was soon trying to improve his apparatus. Picking up a box containing many of his historical torsional oscillators, he gives it a rattle, selects one and points out an interesting ridge. He loves to shape the metal pieces himself, he says. And with no more administrative duties to distract him, he adds happily: \"I can spend all my time in the machine shop.\" One of Reppy's first moves after Rittner left was to make a new oscillator vessel, which, instead of holding the helium in a ring-shaped channel circling the cylinder's rim, also included a channel across the middle. He filled the oscillator with helium-4, ran the apparatus and verified that he saw the faster oscillations attributed to a supersolid. Then he blocked one side of the ring, so that the putative supersolid could flow only through the central channel and the other side of the ring, and found that the signal decreased \u2014 just as the supersolid theory would predict. But then he also blocked the central channel to try to stop the supersolid flowing at all, which should have made the signal go away entirely. But it didn't. Thinking that there must be a leak, Reppy tried several variations \u2014 including just watching to see if the helium-4 escaped like air out of a balloon. It didn't. Reppy has never understood this observation, and hasn't published it. But the unexpected behaviour planted a seed of doubt in his mind: was the formation of a supersolid the true explanation for the effects that everyone had seen? There were other discordant findings too. For example, liquid helium-4 ought to be able to flow through a solid helium-4 barrier if that solid contains some supersolid. But neither Reppy and Rittner, nor a group led by John Beamish at the University of Alberta in Edmonton, were able to observe this. By late 2009, Reppy had tinkered with his apparatus yet again, adding a diaphragm on top that allowed him to deform a sample during a measurement run. Following up on the possibility that disorder was involved in supersolidity, he wanted to see if he could increase the amount of supersolidity in a given sample by using the deformation to introduce more defects. The results of this experiment were totally unexpected: Reppy found no evidence of a supersolid signal at all \u2014 at least, not at ultra-low temperatures. Instead, the deformation produced a decrease in the oscillation frequency at higher temperatures \u2014 so high that the jiggling of atoms would be expected to destroy any quantum effect such as supersolidity.  \n                Solid ground? \n              The publication of these results 4  in June 2010 caused another stir, including a news article in  Science   claiming that the evidence for a supersolid was \"slipping away\" 5 . In a commentary in  Physics 6 , Beamish suggested that Reppy had discovered a kind of \"quantum plasticity\", an effect in which solid helium-4 radically increases its softness as its temperature is raised, then stiffens again as the temperature is lowered. That stiffening would cause the frequency of a torsional oscillator to increase and mimic the supersolid signal (see  graphic ). Reppy has embraced that idea and interpreted his results as a repudiation of supersolidity \u2014 an indication that he, Chan, Kim and everyone else had in fact been seeing the disappearance of the previously undetected quantum plasticity at low temperatures. He insists that he takes no joy in that conclusion. \"I'm disappointed that this is turning out to be something other than a supersolid,\" he says. But others in the community are not so convinced. Is the evidence for supersolidity really slipping away, or does Reppy just have anomalous equipment? Sebastien Balibar, an expert on helium-4 at the \u00c9cole Normale Sup\u00e9rieure in Paris, says he believes two novel effects have been discovered \u2014 supersolidity and a radical change in elasticity, something akin to Beamish's quantum plasticity. Without meaning to, Balibar says, Reppy configured his vessel to be especially sensitive to changes in the elastic properties of the material \u2014 perhaps because the stiffer helium-4 at lower temperatures was effectively gluing together parts of his experimental chamber, and producing a heavier oscillator. Asked about that possibility, Reppy whisks the visitor out of his lab to a blackboard in a breezy corridor, where he chalks out a calculation showing why he feels the gluing hypothesis is extremely unlikely. Even so, Reppy is having difficulty getting others in the field to share his doubts about the supersolid. Kim and his colleagues have just published additional results 7  showing what they say is conclusive evidence for formation of a supersolid. Reppy has seen Kim's work, and says that he feels the problem presented by his own results hasn't yet been addressed properly. But the field of supersolid helium-4 is too small and collaborative for Reppy's result to be ignored. Chan \u2014 for one \u2014 naturally bridles at the suggestion that the supersolid interpretation is in trouble, but he has also asked Reppy to collaborate, to gain a better understanding of his equipment. Chan points out that even when Reppy and Rittner replicated the 2004 experiment, they were reporting supersolid fractions of 20% \u2014 20 times greater than the 1\u20132% measured by other groups. He takes that as evidence that there's a secondary effect at work in Reppy's apparatus that is swamping the supersolid signal. Chan hopes that could be understood by testing the vessel's response when filled with better-studied superfluid helium-3. Meanwhile, Reppy's latest, unpublished, results are giving him new cause for doubt. These data were taken with a secondary oscillator added to the bottom of his experiment, which allows him to vary the frequency of the vessel's oscillation. His preliminary finding is that the response of the helium-4 sample depends on that frequency \u2014 which would not be the case if the helium-4 was a supersolid. But, Reppy wonders, doodling with chalk on a cartoon sketch of his vessel, could this be a way to turn the critiques of his experiment into a bonus? He starts drawing an alternative configuration of the apparatus, in which he could produce the first measurement of the elasticity of helium-4. Asked what light that would shed on the formation \u2014 or otherwise \u2014 of a supersolid, he shrugs. \"I don't know,\" he says. Chan says that in a similar situation, with an experiment giving very surprising results, he probably wouldn't have published anything. But researchers in this field are having to feel their way experimentally because of the absence of a guiding theory. And, as tends to happen with a quintessential experimentalist, Reppy's caution inevitably gives way to dogged determination once he is confident that each result is real. \"That 20% \u2014 he knows it's unusual, but he felt compelled to publish it,\" Chan says. \"Whatever way it turns out, I think respect for him will grow.\" \n                     John Reppy \n                   \n                     Moses Chan \n                   Reprints and Permissions"},
{"file_id": "468884a", "url": "https://www.nature.com/articles/468884a", "year": 2010, "authors": [{"name": "Brendan Maher"}], "parsed_as_year": "2006_or_before", "body": "Cancer epidemics in Turkey could hold the secret to staving off a public health disaster in North Dakota. They became known as the cancer villages \u2014 tiny hamlets in Cappadocia, Turkey, that for generations have been haunted by an extremely rare lung condition. Mesothelioma, responsible for up to half of the deaths in these towns, is almost always associated with exposure to asbestos. But here, researchers found a different cause: a mineral called erionite, which is built into the very fabric of the villages. It is on the roads, in the fields and in the stone used to construct the houses. Now, decades of research in Turkey may help to save lives 9,500 kilometres away, in a rural corner of North Dakota. The Killdeer mountains in the western part of the state are rich in erionite, and they serve as the only nearby source of stone for surrounding Dunn County. When Ed Murphy, the state geologist, heard about the Turkish cancer villages five years ago, he grew concerned and launched an investigation that found erionite in gravel covering hundreds of kilometres of roads. It also turned up in driveways, car parks and even a playing field used by children. The North Dakota study eventually grew into a global collaboration including cancer biologists, geologists, epidemiologists, environmental scientists and physicians. And this week, the team is reporting some worrying results: that levels of exposure to erionite in North Dakota are the same as in some of the Turkish villages ravaged by mesothelioma.  The cancer hasn't yet shown itself in North Dakota, but mounting evidence suggests that large-scale clean-up efforts should commence immediately, says Michele Carbone, a mesothelioma expert at the University of Hawaii in Honolulu who has worked in both North Dakota and the Turkish villages. \"The reason that I find it exciting is that here we have a chance to do something,\" he says.  Carbone has been visiting Cappadocia since 1996, when he was invited by \u0130zzettin Bari\u015f, a physician who at the time specialized in studying and treating mesothelioma at Hacettepe University in Ankara. Starting in the 1960s, Bari\u015f travelled the country, uncovering small clusters of the rare cancer \u2014 generally alongside local sources of asbestos. The microscopic, needle-like fibres of asbestos penetrate the lungs and get stuck in the surrounding mesothelial tissues, causing inflammation, scarring and, eventually, cancer. Villagers, says Bari\u015f, had been unknowingly mixing the durable white mineral into the stucco on their buildings.  \n                Deadly fibres \n              In 1975, Bari\u015f was called to help with a crisis in the village of Karain. The previous year, the town of just 604 adults had lost 11 to mesothelioma, a disease that usually affects only a few people per million. The epidemic had given the town an infamous reputation. Bari\u015f wrote in 1978: \"The saying goes that 'The peasant of Karain falls ill with pain in the chest and belly, the shoulder drops, and he dies'.\" 1  But unlike everywhere else that Bari\u015f had investigated, there was no asbestos to be found in Karain, nor in the nearby towns of Tuzk\u00f6y and Sar\u0131h\u0131d\u0131r, which also had high rates of death due to mesothelioma. Bari\u015f searched for a culprit, and in the early 1980s he found one, with the help of Frederick Pooley, a geologist at University College, Cardiff, UK. On the streets of the cancer villages and in the lungs of the meso\u00adthelioma patients, Bari\u015f, Pooley and their collaborators found microscopic bits of erionite, which has fibrous qualities like those of absestos 2 .  Bari\u015f's work gave the villages a somewhat unwelcome celebrity status among cancer researchers. In 1987, his studies and others led the International Agency for Research on Cancer in Lyon, France, part of the World Health Organization, to classify erionite as a group 1 carcinogen \u2014 of clear danger to humans. Some research suggests that the mineral might have even greater potential to cause cancer than asbestos.  Murphy had no idea of erionite's dangers in the 1980s, when it turned up in rocks that he collected during a survey of the Killdeer mountains. Twenty years later, he heard about the mineral's connection to cancer in Cappadocia and alerted state health officials. Together, they collected samples from the roads in Dunn County and found erionite in the gravel and dust. They notified the Environmental Protection Agency (EPA). In October 2006, a team led by the EPA began to assess residents' exposure to erionite by collecting air samples while simulating everyday activities. The team raked car parks and followed vehicles that kicked up dust along the gravel roads. \"Lo and behold, even though it was windy and rainy and snowing, we had exposures that were quite impressive,\" says Aubrey Miller, then a senior medical officer for the EPA in Denver, who led the team. The levels were below the regulatory limits for occupational asbestos exposure, but were high enough to raise concerns about community exposure, says Miller. The data left many questions. The EPA has not established any guidelines for safe exposure to erionite and it was unclear whether the levels of exposure in North Dakota were the same as those that cause problems in Cappadocia, or even whether the erionite itself was the same. The group needed data from Turkey, so Miller contacted Carbone, who by this time was known as an expert on the cancer villages. \"I convinced the EPA to come to Cappadocia and bring their expensive equipment and team,\" says Carbone.  \n                Serious exposure \n              Geologists, epidemiologists and environmental scientists travelled to Turkey to sample the air and compare the erionite with samples from North Dakota. The authors presented the results of these and other studies on 9 December at the Chicago Multi\u00addisciplinary Symposium in Thoracic Oncology in Illinois, and their findings paint a bleak picture for Dunn County and its 3,000 inhabitants. Erionite exposure levels in North Dakota are not quite as high as those in Tuzk\u00f6y, Karain and Sar\u0131h\u0131d\u0131r, but they equal or exceed those in neighbouring Boyal\u0131, where 6.2% of all deaths are caused by mesothelioma. The structure and composition of the erionite from both regions is practically identical, and samples from each place cause mesothelioma-associated changes and inflammation in cell culture and mouse lungs. Still, the human data are harder to interpret. Mesothelioma can appear 30 years or more after exposure to asbestos. Many of the roads in Dunn County have been around at least that long, but cancer records for the state have been spotty. North Dakota doesn't seem to have a higher incidence of the disease than is normal nationwide, so the scientists looked to see whether residents were exhibiting early warning signs. Researchers at the University of Cincinnati in Ohio collaborated with the North Dakota department of health to round up more than 30 individuals with high occupational exposure to erionite dust, including some who had worked in the quarries and on the crews that paved the roads. Murphy was also a subject.  The researchers studied the subjects' lungs using X-ray and high-resolution computed tomography scans. According to their report, released in October 3 , the subjects showed unusual changes in their lung tissue at roughly eight times the rate seen in a study of similarly aged transportation workers, although these could have had many causes. More tellingly, two people had 'pleural plaques', essentially scarring in the lining of the chest cavity, consistent with exposure to mineral and man-made fibres.  For Miller, now a senior medical adviser at the National Institute for Environmental Health Services in Bethesda, Maryland, the similarities between erionite and asbestos are too great to ignore in Dunn County. \"Does it look like a duck? Does it quack like a duck? Does it have exposure?\" he says.  The search is on to find out whether there are other areas in which people might be exposed to erionite, something that Carbone thinks is likely. Deposits of the mineral are known throughout western North America, Japan, New Zealand and parts of Europe. If researchers can identify populations at risk, they could help to reduce further exposure to erionite and monitor for signs of disease, says Carbone, who has been working with geologists to survey parts of Arizona, Nevada, Oregon, California and Idaho for the mineral.  In Turkey, his team is investigating whether genetic factors predispose some people more than others to developing mesothelioma after exposure to erionite or asbestos. The researchers are also looking for metabolites in the blood that might predict the onset of disease. Large-scale studies are set to begin next year, and Carbone says that such research could help in developing treatments and early-detection strategies. After decades of study, Turkish officials are now building new housing in some of the villages of Cappadocia, and residents are being moved \u2014 some of them with great reluctance. Meanwhile, officials in North Dakota are also making changes. Gravel mining in the area has tailed off, and most road work is done with stone from other areas. The playing field has been cleaned up, and talks are in progress about whether the roads can be paved over. But gravel containing erionite is still in use in some places, and all the scientists can do is advise. The EPA has no authority to regulate erionite, says Miller.  Given the lack of mesothelioma clusters in North Dakota, many residents there are not concerned about erionite, says Murphy. He frequently hears people say, \"We've lived here for years. We're third generation or so, and no one has had a problem that we know of.\" In terms of his own health, Murphy found nothing too troubling in his recent scans. But with his record of exposure to erionite, he knows that he needs to keep monitoring his lungs. \"I try not to dwell on it,\" he says. \n                     Perspective: an epidemic in Cappadocia \n                   \n                     Erionite information from the North Dakota Department of Health \n                   \n                     North Dakota Geological Survey \n                   \n                     Environmental Protection Agency \n                   Reprints and Permissions"}
]