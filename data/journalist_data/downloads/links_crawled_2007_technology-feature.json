[
{"file_id": "4501119a", "url": "https://www.nature.com/articles/4501119a", "year": 2007, "authors": [], "parsed_as_year": "2006_or_before", "body": "\u201cEach day you see major, significant publications on the roles of microRNAs in regulating pathways of genes involved in disease aetiology,\u201d says John Maraganore of Alnylam Pharmaceuticals in Cambridge, Massachusetts. Alongside the research into the mechanisms and roles of microRNA (miRNA) in disease, researchers are now starting to look at the potential of miRNA-based therapeutics. Regulus Therapeutics in Carlsbad, California, is one of the first companies to be founded entirely for the development of miRNA-based therapeutics. It was formed in September as a joint venture between Alnylam and Isis Pharmaceuticals in Carlsbad. \u201cWe recognized that miRNAs were becoming a potential therapeutic opportunity,\u201d says Frank Bennett, senior vice-president of research at Isis, \u201cbut we also recognized that it was difficult for both companies to input the resources that were warranted.\u201d After much discussion, the two firms agreed to supply miRNA assets and core technologies to Regulus.  Regulus is taking two approaches to the development of miRNA-based therapeutics. \u201cThe most advanced approach is inhibiting the function of an endogenous miRNA in cells,\u201d says Bennett. This uses a synthetic oligonucleotide to target the miRNA for silencing. Although this is different from siRNA, as the miRNA is the target not the therapeutic agent, Bennett thinks that much of the technology developed for targeting mRNAs with siRNA is directly translatable to the targeting of miRNAs with oligonucleotides. And he notes that using oligonucleotides will also minimize any 'off-target' effects. \u201cThese oligonucleotides are very specific \u2014 even a single base mismatch will cause the oligo to lose activity,\u201d he says. The second approach involves replacing miRNAs in or delivering them to cells, which Bennett says is similar to the current approach for siRNAs. The first applications here might be replacing miRNAs that are missing from disease-associated cells but present in a normal cells, which happens in some cancers, or augmenting naturally occurring miRNAs. Although experience of siRNAs will help this nascent field, there is one unique issue in miRNA biology that researchers will have to address: miRNAs can inhibit tens to hundreds of genes at a time. Scores of researchers, both inside and outside academia, are now working to understand exactly how this regulation by miRNAs occurs. \u201cYou can use specific chemical modifications to the miRNA to limit that potential,\u201d says Bennett, \u201cbut it is still present.\u201d Many researchers and companies now think miRNA-based therapeutics hold great promise for the future. And although some companies are waiting in the wings for more information on their basic mechanisms of action, others including Merck, Santaris Pharma of H\u00f8rsholm, Denmark, Rosetta Genomics in Rehovot, Israel, and Actigenics of Maurens-Scopont, France, are actively involved in miRNA research and development. \n               N.B. \n             Reprints and Permissions"},
{"file_id": "4501117b", "url": "https://www.nature.com/articles/4501117b", "year": 2007, "authors": [], "parsed_as_year": "2006_or_before", "body": "Many companies are realizing that the development of new delivery vehicles for therapeutics based on RNA interference (RNAi) will require collaborative efforts. \u201cWe view the delivery of small RNAs as one of the most important biomedical endeavours in modern biological science,\u201d says John Maraganore, chief executive of Alnylam Pharmaceuticals in Cambridge, Massachusetts. \u201cAnd when you have such a broad-scale challenge and opportunity, you can't do everything internally \u2014 you have to work with the best groups outside as well.\u201d This spring, Alnylam initiated a collaboration with Daniel Anderson and Robert Langer at the Massachusetts Institute of Technology (MIT) in Cambridge. Under the agreement, Alnylam will provide funding for ten postdocs at MIT to work on issues directly related to small-RNA delivery while having the exclusive option to any intellectual property that comes out of the effort. Anderson says that at its core this is an academic programme, so the goal is to get postdocs to work on the delivery issue while being trained as future leaders in the field.  \u201cOne nice feature is that we have collaborations with a company that is a leader in this field, so it allows us to accelerate our efforts tremendously,\u201d Anderson says. For example, when the MIT researchers develop any new delivery technologies, scientists at Alnylam can provide animal models and other methods to test and evaluate these new vehicles, providing quick feedback on whether or not the research is on the right track. Langer's lab has a long history of translational research. It has developed principles that have led to some 40 products that are now in clinical trials or have been approved by the US Food and Drug Administration. \u201cThe hope is that we can do that here,\u201d says Langer. \u201cWe can do the kind of basic research to help solve the RNAi delivery problem and then work closely with Alnylam to take the basic research into the clinic where it can be used to treat different diseases.\u201d In addition to the MIT collaboration, Alnylam also funds R&D and manufacturing activities in Vancouver, Canada, to further the development of cationic lipids for delivery, and has some 25 feasibility agreements in place in which Alnylam is testing and evaluating technology that has been introduced from either companies or academic groups. \n               N.B. \n             Reprints and Permissions"},
{"file_id": "4501118a", "url": "https://www.nature.com/articles/4501118a", "year": 2007, "authors": [], "parsed_as_year": "2006_or_before", "body": "How to deliver short-interfering RNAs (siRNAs) to specific tissues is only part of the problem facing researchers. They also need to find out whether the RNA has reached its intended target. Anna Moore, a radiologist at Harvard Medical School in Boston, Massachusetts, is well aware of the issue. \u201cThere was no way to use a clinical imaging modality to see the delivery of siRNA,\u201d she says. When researchers want to see whether an siRNA has been reached a particular tissue, they usually perform histological analysis followed by reverse transcription PCR to see whether the target gene was silenced. \u201cYou can do this with mice, but when you move on to humans it becomes impractical,\u201d Moore says.  Researchers can track siRNAs  in vivo  using bioluminescence imaging or by tracking green fluorescent proteins. But bioluminescence imaging is not a clinical modality. So Moore and her colleagues decided to try magnetic resonance imaging (MRI). The first step was to design an siRNA delivery vehicle that could be imaged by MRI. Moore and her team used a nanoparticle containing an iron oxide core. They coated it with dextran, which could have various targeting features added to it relatively easily. Although iron oxide can be imaged using MRI, the group also attached a fluorescent dye, Cy 5.5, to the dextran coat for optical imaging. \u201cWe wanted to correlate the imaging data with microscopic findings,\u201d says Moore. The iron oxide nanoparticle generates a bright spot on the MRI image. The exact target of the nanoparticle can then be confirmed by the fluorescent dye and by doing microscopy for histological analysis. Two further attachments were then made to the nanoparticle via the dextran coating: a membrane translocation peptide that can cross cell membranes and an siRNA. With this, Moore and her colleagues thought they had a particle that could target and image delivery to tumour cells  in vivo . But the imaging showed that the nanoparticle went to the liver and kidneys, and was present in other organs as well 1 . Moore and her team plan to continue with the nanoparticles, trying to make them more efficient in terms of delivery and target uptake. But the real value of these nanoparticles might be their versatility. As different siRNAs or targeting peptides can be attached to the dextran coat, a large range of therapeutic siRNAs and peptides can be tested. \u201cMy lab is really interested in imaging other pathologies such as diabetes, which is far from cancer but the imaging approaches are very similar,\u201d says Moore. \u201cAnd that is the beauty of this technology \u2014 you can apply it to different pathologies.\u201d \n               N.B. \n             Reprints and Permissions"},
{"file_id": "446941a", "url": "https://www.nature.com/articles/446941a", "year": 2007, "authors": [], "parsed_as_year": "2006_or_before", "body": "Reprints and Permissions"},
{"file_id": "446937b", "url": "https://www.nature.com/articles/446937b", "year": 2007, "authors": [], "parsed_as_year": "2006_or_before", "body": "Taking the idea of nanoscale design for cell culture further, some cell biologists are investigating cellular responses to more complex three-dimensional structures. A collaboration in this field between cell biologists and electronic engineers at the University of Glasgow's Centre for Cell Engineering is proving fruitful. The group has the advantage of a state-of-the-art electron-beam nanolithography system (E Beam) manufactured by Leica, which creates nanoscale features on a surface under the control of customized software (see also  'Down to the letter' ). \u201cWe're quite unique in the United Kingdom in that we have access to such an incredible machine with which you can define structures so deliberately,\u201d says Mathis Riehle, who directs research at the centre. \u201cWith the E Beam you can be very specific.\u201d The E Beam is used to define or \u201cwrite\u201d a pattern at nanometric resolution into an electron-sensing polymer. The pattern is developed and used as a template for etching or depositing material to form the desired nanotopographic features. This technique gives a greater degree of control than 'natural' lithography, which relies on self-assembly of colloidal particles in regular arrays to produce the pattern. Riehle wants to explore the limits of the machine. \u201cWe've made tubes and we want to make more complicated three-dimensional structures \u2014 maybe structures that will look like cell 'car parks'. But to do that we would need something like origami, because at the moment the E Beam can only really write on a flat plane. We cannot write on a shaped, undulating surface.\u201d The team will have to write its own software to create these complex structures. As one of a consortium of institutions working on emerging nanopatterning methods, the Glasgow centre is also developing alternative methods of engineering 3D structures with nanoscale features for use as cell-culture substrates. Kris Seunarine and Osian Meredith have fabricated a 'swiss roll' from e-poly-caprolactone, a biodegradable thermoplastic shaped by hot nano- and micro- embossing. This is the 'sponge' of the swiss roll, with the cells as the 'jam'. Two levels of microfeatures are sculpted into the polymer (see photos). One aligns fibroblasts and smooth-muscle cells, keeping them from being squashed. The inward-facing surface is patterned at the nanoscale with a regular array of pits (100 nm diameter and 80 nm deep) spaced 300 nm apart, which define locations for endothelial cell adhesion. These cell scaffolds could be useful in developing methods for vascular and urogenital reconstruction. \n               H.M.B. \n             Reprints and Permissions"},
{"file_id": "447741a", "url": "https://www.nature.com/articles/447741a", "year": 2007, "authors": [{"name": "Nathan Blow"}], "parsed_as_year": "2006_or_before", "body": "Proteomics is hungry for well-validated antibodies. Nathan Blow looks at the options and sees how researchers are redefining the way to generate an antibody. The products of more than 22,700 genes make up the human proteome. But researchers hoping to unpick the mysteries of the proteome are restricted by the fact that antibodies against only a small percentage of these proteins are available. Although commercial production of antibodies is well established, the market has so far been driven by the popularity of particular antibody targets. More than 1,000 antibodies against the tumour suppressor p53 are available, for example, but the less sought-after targets frequently have none. With the rise of proteomics, the need for antibodies has become global and is no longer limited to a small number of targets central to most hypothesis-driven research projects. \u201cOne of the hurdles in proteomics is a lack of high-quality, well characterized affinity reagents,\u201d says Henry Rodriguez, director of the clinical proteomic technologies initiative for cancer at the US National Cancer Institute (NCI) in Bethesda, Maryland. Not only do researchers need access to more antibodies, they also need to know how these antibodies have been characterized to determine whether they will work in the assay they are using. \u201cLarge numbers of antibodies are already available, but an investigator has to navigate through a complex system to find out which target antibodies are going to be appropriate for his or her particular assays,\u201d says Adam Clark, who works on the NCI's proteomic technologies initiative. \n               Standard issue \n             At present, there is no universal validation: an antibody that works wonders for a Western blot may perform poorly in immunohistochemistry. This growing need for faster antibody production and stronger validation data is leading many groups to explore high-throughput methodologies for creating and validating affinity reagents. A group led by Mathias Uhl\u00e9n at the Royal Institute of Technology in Stockholm, Sweden, is spearheading an initiative to assemble the Human Protein Atlas ( http://www.proteinatlas.org ). The project aims to explore the entire human proteome using antibodies. Uhl\u00e9n's group is producing polyclonal antibodies directed against each human protein, and then characterizing the antibodies using Western blots, protein microarrays and immunohistochemistry. The project generates around ten new polyclonal antibodies and more than 10,000 immunohistochemistry images every day \u2014 an achievement that relies heavily on high-throughput methodology. Although most steps in the process are amenable to automation, some, such as annotating tissue immunohistochemistry images, are proving to be a significant challenge. Indeed, analysing these images still involves ten pathologists who have to annotate them manually. \u201cAll the annotation is Internet based. The pathologists view and evaluate 600 tissue images per antibody via a web-based tool on their personal computers, and the results are stored in our database,\u201d says Uhl\u00e9n. In addition to producing and testing their own antibodies, Uhl\u00e9n and his team will put antibodies from commercial sources through their standardized quality-control pipeline. Uhl\u00e9n says he was surprised that only about 35% of commercial antibodies seemed to work \u2014 although he notes that this could be a result of the way his group analyses them. \u201cWe decided to use a very standardized way of validating antibodies: if they don't work, we don't try other ways of doing it,\u201d he says. The success rate may be low, but such a standardized quality-control process offers researchers a rare level of confidence in the antibodies that pass the test. Currently, the atlas consists of more than 1,500 antibodies, half of which have been provided by commercial sources. In October 2007, a new release will be made available bringing the total number of antibodies to slightly less than 3,000. Confocal microscopy images of tissues stained with the antibodies will also be released in October to complement immunohistochemistry images. \u201cWe are committed to showing the public all the primary validation of the antibodies,\u201d says Uhl\u00e9n. Although polyclonal antibodies are not renewable, a limited number of aliquots of each antibody generated by the atlas will be made available through Atlas Antibodies of Stockholm. Other initiatives are focusing on monoclonal, rather than polyclonal, antibodies against proteins on a large scale. In 2002, for example, the Wellcome Trust Sanger Institute near Cambridge, UK, launched the Atlas of Gene Expression. Although a change in focus at the institute means that this is closing down, the atlas was set up to generate high-quality monoclonal antibodies that were well characterized for a variety of assays. Much as for the Human Protein Atlas, a well-standardized characterization was key. \u201cHistorically, the validation of antibodies has been ad hoc with different people generating antibodies that have been assessed in different ways,\u201d says project leader John McCafferty. The project involved four key groups. One generated the proteins of interest and did quality control. This team used the Gateway cloning system made by Invitrogen of Carlsbad, California, to move a variety of open reading frames between different expression vectors and so improve yields for troublesome proteins. A second group used phage display for high-throughput screening of a single-chain antibody library generated at the Sanger Institute and containing more than 10 10  phage clones. The other two groups were dedicated to immunohistochemistry and the informatics infrastructure necessary to deal with the large volume of data. To deal with image acquisition issues, the institute collaborated with Applied Imaging of San Jose, California, to develop an automated high-throughput image-analysis system suitable for tissue microarray applications. So far, the Sanger project has generated more than 4,000 monoclonal antibodies to 290 antigens, which are available to buy from Geneservice in Cambridge, UK. Although the project is being discontinued, McCafferty says that much has been learned about the bottlenecks of high-throughput generation of antibodies and how these can be overcome. \u201cSurprisingly, the generation of the antibodies was not the major issue,\u201d he says. \u201cThe bottlenecks were generating good quality protein product to do selection, and how to deal with the large amounts of image data a project such as this produces.\u201d \n               Finding affinity \n             Even as the Sanger project comes to a close, other initiatives are beginning to gather steam \u2014 although these have been hampered somewhat by a lack of funding. \u201cThere seems to be a reluctance from the funding agencies to put money into large-scale antibody initiatives,\u201d says Andrew Bradbury of the biosciences division at Los Alamos National Laboratory in New Mexico. The NCI's five-year, $104-million clinical proteomic technologies initiative that is now getting off the ground may be the start of a change. In 2005, the NCI held a workshop to discuss affinity capture. It found that the scientific community wanted renewable resources that were well characterized for performance data, says Clark. The meeting also revealed that the community was concerned by the lack of characterization data for most available antibodies. Following this lead, the NCI proteomics reagent core, one of the centres in the clinical proteomic technology initiative, is embarking on the production of affinity reagents. To focus its efforts, it has identified a list of protein targets: all cancer-related proteins for which no commercial antibodies are yet available. The core will develop monoclonal antibodies that will be characterized by Western blots, enzyme-linked immunosorbent assay (ELISA), immunohistochemistry and immunoprecipitation followed by mass spectrometry. \u201cAll the raw data on how the antibodies perform on a variety of assays will be provided and an investigator will be able to acquire these antibodies through a website organized by the NCI,\u201d says Clark. In Europe, another group of investigators plans to generate affinity reagents against the human proteome. The group, called ProteomeBinders, consists of 26 European Union and two US institutional partners. \u201cThe goal or the hope is to get funding from the European Union to put a project together next year or the year after,\u201d says Bradbury, one the US participants. Although the antibody remains the affinity reagent of choice, the exploration of alternative binders by large groups, such as ProteomeBinders, shows how far these non-traditional reagents have come in a relatively short time. \n               Gold standard \n             A quick glance through the catalogues from commercial vendors and researchers reveals thousands of antibodies not only to proteins, but also to specific protein changes such as post-translational modifications. Still other companies offer to produce antibodies to an investigator's antigen of interest. Monoclonal antibodies produced by animal immunization remain the 'gold standard' of affinity reagents. They are relatively renewable, can usually be made with high specificity and affinity for their target and can be used in common biochemical assays such as Western blotting, ELISA and immunochemistry. But the traditional monoclonal antibody has its drawbacks. Its production can be challenging, time-consuming and costly. So there is a lot of interest in identifying novel affinity reagents that would be less expensive and quicker to produce. \u201cThe future is with alternative binders,\u201d says Bradbury, who works with single-chain antibodies (see  'Antibodies in the fast lane' , page 743). \n               Optimistic expression \n             The structural characteristics of antibodies make it difficult to produce recombinant versions in bacteria and restricts their use in some high-throughput screening methods. But George Georgiou and his colleagues at the University of Texas at Austin have come up with a method to produce and screen full-length immunoglobulin G (IgG) antibodies expressed in  Escherichia coli  (Y. Mazor  et al .  Nature Biotechnol.   25,  563\u2013565; 2007). The technology produces full-length antibodies that are initially tethered to the inner membrane of the bacterium. When the bacteria are treated with EDTA and lysozyme, the resulting spheroplasts with exposed antibodies can be selected in a high-throughput manner by using fluorescently labelled antigens and flow cytometry. \u201cWe can isolate several bacterial clones expressing full-length IgG antibodies that can bind to the antigen with the requisite affinity,\u201d says Georgiou. \u201cThe advantage of the technology is that the expression of the antibodies is directly in bacteria and we can use the bacteria to produce the antibody without going through the steps of reformatting the antibody and then expressing in a mammalian system.\u201d But the antibodies are not glycosylated, which limits some of the therapeutic applications. The more traditional way to screen and obtain IgG antibodies rapidly is the generation of recombinant antibody fragments. Single-chain variable (scFv) antibody fragments are created by the fusion of the variable regions of the heavy and light chains of immunoglobulins using a short peptide linker. This allows scFv fragments to be expressed from a single open reading frame and screened by phage display or other high-throughput approaches. Although larger than scFv fragments and still composed of two independent polypeptide chains, Fab antibody fragments are also being used and are usually favoured for their high stability and compatibility with existing antibody-based assays. The Fab antibody region is the antigen-binding region of the immunoglobulin. Fab fragments consist of one constant and one variable domain from each of the heavy and light chains. Both scFv- and Fab-based technologies are developing rapidly, with several companies now supplying either scFv or Fab libraries and screening systems to consumers. One such company is BioInvent of Lund, Sweden, which provides the n-CoDeR human-antibody library based on both the scFv and Fab formats. Other companies, including Cambridge Antibody Technology in Cambridge, UK, and MorphoSys in Martinsried, Germany, have developed human-derived phage-display libraries using either the scFv or Fab format to identify binding regions for development of therapeutic monoclonal antibodies. Although rapidly generated and effective for many  in vitro  applications, scFv and Fab fragments are less effective for therapeutic applications because they have short half-lives. \u201cSingle-chain fragments can't really be used in animals because they are cleared very rapidly \u2014 the half-life of a single chain Sv is about 10 minutes, whereas full-length antibodies can persist for several days,\u201d says Georgiou. \n               Breaking with tradition \n             Described for the first time in 1997 by Uhl\u00e9n and his colleagues, affibodies were among the first non-immunoglobulin-based affinity reagents. These small molecules are based on a bacterial receptor ( Staphylococcus aureus  protein A), and use combinatorial protein engineering to introduce random mutations in the affinity region. Affibody of Bromma in Sweden, which was co-founded by Uhl\u00e9n, currently produces affibody-based reagents for basic research laboratories and commercial partners. Another non-immunoglobulin-based affinity reagent that is becoming more widely used is the aptamer. Made of DNA, RNA or modified nucleic acids and typically 15\u201340 bases in length, aptamers have a stable tertiary structure that permits protein binding through van der Waals forces, hydrogen bonding and electrostatic interactions. Early studies showed that aptamers can be highly specific for target proteins, with the ability to distinguish between related members of a protein family (S. D. Seiwart  et al .  Chem. Biol.   7,  833\u2013843; 2000). Unlike the scFv and Fab fragments, both aptamers and affibodies are useful for  in vivo  applications because they have longer half-lives. In addition, both function well in the reducing environment of the cell cytosol, which is a problem for larger monoclonal antibodies. Currently, Affibody is testing an HER2-binding affibody as an alternative to herceptin for treatment of HER-2-positive breast cancer with a clinical proof-of-principle microdosing study to occur this year. Archemix in Cambridge, Massachusetts, has three aptamer-based therapeutics in phase I clinical trials. Two aptamers target coagulation processes and the third targets nucleolin, a protein that is involved in the development of some cancers. Overall, there is much optimism regarding the future of alternative affinity reagents. But several problems have to be overcome before they are adopted more widely by the scientific community. One of the most pressing issues is the inability to produce these reagents at a truly high-throughput scale. Overcoming this obstacle would make these alternative binders not only cheaper to produce than traditional antibodies, but would also require significantly less time. \u201cI would love to see a major technology breakthrough where someone shows that you can actually produce these in a high-throughput manner, but so far I don't think anyone has been able to do that,\u201d says Uhl\u00e9n. Reprints and Permissions"},
{"file_id": "446940a", "url": "https://www.nature.com/articles/446940a", "year": 2007, "authors": [], "parsed_as_year": "2006_or_before", "body": "Nanoprobes come in all shapes and sizes. In the latest advance in probe engineering, chemists, physicists and engineers at the University of California, Los Angeles, are pooling their resources to perfect a method for mass-producing novel fluorescent microparticles. The nature of these particles can be so precisely controlled that researchers have been experimenting by creating entire alphabets that can be manipulated with optical tweezers, raising the intriguing possibility of playing nano-scrabble. These so-called LithoParticles are sculpted by electron-beam lithography, directed by the same computer-aided design (CAD) software used by architects. \u201cE-beam writing is a serial process,\u201d says Thomas Mason, who leads the group. \u201cEach letter is written one at a time, so it's not very good for mass production. However, once the mask is made, it can be used over and over again in a special optical-projection printer. We use a mask made by E-beam lithography to expose resist-coated wafers to patterned ultraviolet light. A different projection-printing device \u2014 an optical lithography system known as a stepper \u2014 is used to mass-produce many particles in parallel.\u201d The Ultratech XLS stepper has a lens weighing over 90 kilograms and its own heating and air-conditioning systems to control thermal expansion. The same technology could be used to mass-produce particles with feature sizes as small as 30 nm. The potential implications for cell biology are huge. Such accuracy of design, coupled with high fidelity on a mass scale, means researchers could soon be supplied with solutions of probes tailored to their specific needs, as neatly demonstrated by Mason's 'alphabet soup'. Nanoprobes are being increasingly used in the emerging field of bio-microrheology, which examines transport processes within living cells, and in investigating the mechanical properties of cellular components. Nanoparticles introduced by ballistic injection have revealed how the cytoplasm of human umbilical vein endothelial cells undergoes elastic changes in response to growth factors. But the approach could be expanded to investigate the cell's response to all manner of different shapes. \u201cTracking how differently shaped particles move and rotate inside cells may provide a wealth of information about life cycles and internal cytoplasmic transport in different cell types,\u201d says Mason. \u201cYou could also use these probes to study how cells respond to various external stimuli. For instance, particles that have many long 'arms' may behave very differently to the compact spheres and quantum dots that are currently available.\u201d UCLA is currently applying to patent their technology and are involved in discussions with commercial partners. Mason is already speculating about building functional nanomachines \u2014 including motors, pumps and entire engines \u2014 which could be sent to probe even further into the workings of the cell. H.M.B. Reprints and Permissions"},
{"file_id": "447745a", "url": "https://www.nature.com/articles/447745a", "year": 2007, "authors": [], "parsed_as_year": "2006_or_before", "body": "\n               Table 1 \n               Reprints and Permissions"},
{"file_id": "447743a", "url": "https://www.nature.com/articles/447743a", "year": 2007, "authors": [], "parsed_as_year": "2006_or_before", "body": "In making recombinant antibodies, the resulting antibody is only as good as the combinatorial library and the screening assay. The trick is to find the molecule of highest affinity and specificity for the target among a library of millions of clones. Traditionally, recombinant antibody libraries have been phage-based and the screening relied on enzyme-linked immunosorbent assay (ELISA), not a high-throughput method. Changes in both phage display and screening methods are now moving recombinant antibody production into a high-throughput world. Flow cytometry has become the assay of choice for rapid screening of clones from recombinant antibody libraries. \u201cWe looked at different ways of screening. Flow cytometry was the only one that seemed to meet our throughput requirements,\u201d says Andrew Bradbury of Los Alamos National Laboratory in New Mexico. Bradbury's group has developed a flow-cytometry assay to screen its single-chain antibody-fragment phage libraries using a mixture of beads coated with specific and non-specific antigens. The method rapidly identifies antibodies that have good affinity for the protein of interest while discarding those that show low specificity. Phage-display screening methods are also used to identify antibodies that target post-translational modifications (PTMs) such as phosphorylation or acetylation. As PTMs have a role in many processes \u2014 from gene regulation to apoptosis \u2014 they are of growing interest for the biological community. Companies have responded by developing antibodies targeting proteins in a specific state of modifications. \u201cAntibodies to PTMs are gaining in importance with customers,\u201d says Kumar Bala, director of antibody technologies for Millipore in the company's lab in Temecula, California. But obtaining antibodies directed against PTMs is not a trivial task. Rockland Immunochemicals of Gilbertsville, Pennsylvania, has put in a lot of effort to develop antibodies for looking at phosphorylated and non-phosphorylated forms of various proteins in a sequence independent context, says Daniel O'Shannessy, the company's vice-president of corporate development. Antibodies that recognize PTMs independently of the protein site on which the modification occurs are useful \u2014 particularly for enriching, for example, all phosphorylated proteins from a cell. But such antibodies are hard to make by animal immunization as the PTM itself is not immunogenic. So scientists are turning to recombinant molecules and  in vitro  screening such as phage display to isolate 'pan-PTM' affinity reagents. Although making steps in the right direction, many more antibodies and further improvements in affinity reagent technology will be needed to understand and characterize the full range of PTMs found in nature. Still, Bala argues that the \u201cbest tools for purifying, identifying, differentiating and characterizing PTMs are antibodies\u201d. \n               Nathan Blow \n             Reprints and Permissions"},
{"file_id": "448959a", "url": "https://www.nature.com/articles/448959a", "year": 2007, "authors": [{"name": "Nathan Blow"}], "parsed_as_year": "2006_or_before", "body": "Millions of tissue samples have been collected and archived, but researchers wanting to explore them at the molecular level have found it tough going. Nathan Blow investigates the issues. According to experts, there are more than a billion tissue samples archived in hospitals and tissue banks around the world, most of them formalin-fixed and paraffin-embedded (FFPE). Today, these samples present both an incredible opportunity and a huge challenge to researchers. FFPE tissue samples have been extensively annotated and well preserved, allowing detailed study of the progression of diseases such as cancer. But due to the method of preservation, obtaining biomolecules from these samples is proving difficult, to say the least. FFPE was first described more than 100 years ago, and most hospitals still use this method today. But there has never been a set of standardized guidelines for processing FFPE tissue samples taken from patients to preserve tissue histology, let alone biomolecules. And although it works well for histology, the lack of standardized guidelines seems to have hampered the use of FFPE samples in molecular analyses. This may soon change, as pathologists are working towards standardizing FFPE sample preparation, and companies and researchers are developing the technology needed to isolate biomolecules and tap into the vast treasure chest of archived samples. Although FFPE tissue preparation is simple in theory, many problems associated with downstream molecular applications \u2014 such as PCR or microarray analysis \u2014 can arise. \u201cThis is all about the fact that there has been no attention paid to uniformity of preparation,\u201d says David Rimm, a pathologist at Yale University. Between hospitals the time to tissue fixation and even the method of fixation can vary dramatically. \u201cI would say that the biggest issue is time from ligation of circulation to fixation,\u201d says Rimm. During this period of ischaemia, molecular changes occur that cause problems in obtaining biomolecules. \u201cPhosphorylation is very sensitive to ischaemic times. There seems to be promiscuous phosphotases in the cell that knock phosphates off tyrosines during this period,\u201d says Rimm. DNA and RNA can also suffer damage before fixation, with enzymes degrading and modifying both. It seems obvious that rapid fixation is the answer, but no simple solution is in sight. Rimm says that for a researcher interested in only DNA or RNA, rapid fixation using quick freezing methods, instead of chemical fixation, is probably best. But if you are interested in proteins, freezing is problematic as the subsequent thawing process tends to break up and denature proteins and, like ischaemia, can lead to modifications. Then there are pathologists who think that standardizing the time to fixation will prove to be a difficult task. \u201cStandardization would be great, but I don't know how realistic it is across institutions or even within institutions,\u201d says Christine Iacobuzio-Donahue, a pathologist at Johns Hopkins University School of Medicine in Baltimore, Maryland. She believes that the best place to tackle the problems caused by ischaemia is actually downstream of fixation, after the biomolecules have been isolated. \u201cOnce you have the samples and have extracted the biomolecules you are interested in, then you can perform quality control to determine if all your samples are similar,\u201d she says. Even if the optimal time to fixation were found and standardized, this is only part of the problem; researchers still need to identify the best fixative to preserve both histology and biomolecules. Formalin, the most widely used fixative, provides good preservation of tissue histology but can cause problems for researchers interested in downstream molecular applications. \u201cFormalin tends to crosslink the tissue to such an extent that it is hard to get fragments of DNA that are longer than, maybe, a hundred or two hundred base pairs,\u201d says Rimm. Formalin fixation causes the crosslinking of DNA as well as RNA and proteins, although proteins seem to fair better following formalin treatment (see  'Frozen in time' ). \n               Preservation society \n             Although tissue preservation has its problems, the good news is that these are now being dealt with. Governmental agencies, such as the US Office of Biorepositories and Biospecimen Research at the National Cancer Institute (NCI) in Bethesda, Maryland, are starting to tackle the difficult issue of biospecimen standardization. In June, the office released a guide to the NCI's best practices for biospecimen resources, detailing technical guidelines for NCI-supported biospecimen collection and storage. And researchers and companies are now creating methods to use the degraded and modified biomolecules obtained from FFPE samples for molecular analysis. Isolation of RNA or DNA from FFPE tissue samples can be accomplished using a number of methods or kits. The main problem is that, almost without fail, the RNA or DNA isolated is degraded and chemically modified. But because researchers want to tap into the vast archives of FFPE tissues for global expression analysis and biomarker discovery, this is spurring companies to address the issue. Microarray analysis has proven a valuable tool for understanding global gene-expression patterns. But using the degraded messenger RNA obtained from FFPE tissues for such analysis is problematic. \u201cThe results on standard microarrays are currently unsatisfactory,\u201d says Shawn Baker, scientific product manager for gene expression at Illumina in San Diego, California. For this reason, Illumina offers a gene-expression application that can study RNA extracted from FFPE tissue samples. Called DASL, short for cDNA-mediated annealing, selection, extension and ligation assay, this system amplifies the mRNAs from FFPE-extracted samples, but unlike other amplification systems it is not 3\u2032 biased, says Baker. \u201cWe use a combination of random and oligo dT primers to generate the complementary DNA, which means that even with degraded RNA it still amplifies quite well and produces good, consistent profiles.\u201d Following the first amplification, the DASL system uses two gene-specific probes to amplify the cDNA. The resulting cDNA can be hybridized to a DASL-specific array. \u201cThe DASL assay is multiplexed up to 1,536 genes. So you get a tremendous boost in the overall throughput,\u201d says Baker. Although unable to survey as many genes as standard microarrays, Baker says that using the DASL system, researchers have been able to profile FFPE tissue samples that are up to 30 years old \u2014 demonstrating the potential to examine the numerous collections of archived FFPE samples. Illumina is now working to increase the number of genes analysed for each DASL assay. NuGEN Technologies in San Carlos, California, is another company developing methods to use RNA extracted from FFPE tissue samples for gene-expression analysis. NuGEN specializes in working with very small amounts of RNA or difficult-to-use RNA, such as RNA extracted from whole blood or very degraded sources. This is commonly seen with clinical samples, but is most significant in the case of FFPE, says Gianfranco de Feo, senior director of customer solutions at NuGEN. Although NuGEN did not start off looking at RNA from FFPE tissue samples, it was the next logical step. \u201cWe have had a product on the market for over a year now that allows users to work with very degraded RNA in very limited amounts, down to 500 picograms. We built on that technology to create kits for the much more degraded RNA that comes from FFPE samples,\u201d says de Feo. At the core of NuGEN's technology is its amplification and labelling system, which has been optimized to work with Affymetrix 3\u2032 microarrays. The system relies on a combination of random hexamers, similar to that of Illumina, augmented with oligo (dT) primers to convert mRNA into cDNA in a linear amplification process. The inclusion of oligo (dT) primers was essential because the Affymetrix arrays probe the 3\u2032 ends of transcripts. But NuGEN hopes to have labelling and hybridization protocols and products for other microarray platforms available before the end of the year. To determine how well these degraded RNA samples from FFPE tissue will work on microarrays, NuGEN developed a tool using quantitative real-time PCR (qPCR) assay. It turns out that the results from this assay correlate very well with the overall results of microarray analysis, says de Feo, allowing researchers to decide whether the data that could be obtained from the array will be of sufficient quality to continue. And this is critical information, as in some cases less than 50% of the transcripts on the array may hybridize with the amplified RNA. \n               Think small \n             Asuragen in Austin, Texas, is a new company on the commercial block, working to understand and characterize the biological role of small RNAs. Although founded only a year ago, Asuragen's RNA roots go much deeper. Asuragen is a spin-off of Ambion, a company that worked in the field of molecular biology with a focus on RNA for nearly 17 years. \u201cAt Ambion, we developed the first kits and technologies for characterizing small RNAs,\u201d says Gary Latham, associate director of technology development at Asuragen. \u201cAnd when microRNAs emerged as a new class of regulatory RNAs in humans, we were sitting in an excellent position to explore this area of 'biological dark matter'.\u201d In March 2006, Ambion was sold to Applied Biosystems for US$273 million and a portion of those proceeds were used to fund Asuragen. Asuragen has concentrated its efforts on the diagnostic and therapeutic opportunities of microRNAs (miRNAs). \u201cAs they are smaller, miRNAs tend to survive the more tortuous conditions of FFPE tissue processing better than mRNAs,\u201d says Latham, which makes degradation less of an issue. This means that downstream applications, such as microarrays, are better suited to miRNAs isolated from FFPE samples than mRNAs, says Latham. \u201cWe have found that there is tremendous diagnostic potential for microRNAs as biomarkers of disease states,\u201d he adds. This shows how companies are moving from traditional RNA analysis to non-traditional methods that use FFPE tissue samples. Several other companies also offer technologies to explore RNA isolated from FFPE tissue samples. Panomics in Fremont, California, has developed a direct hybridization method using branched DNA technology. The system, called QuantiGene FFPE, is unique because it does not involve the linear amplification of RNA, as do many other systems. Instead, the use of branched DNA technology permits the direct measurement of RNA from the sample source. The Paradise System, developed by Arcturus Biosciences and now supplied by Molecular Devices of Sunnyvale, California, has been optimized for RNA extraction from FFPE samples, followed by a linear amplification step prior to use in qPCR or microarray applications. Even though much headway has been made in the molecular analysis of FFPE tissue samples over the past few years, high-throughput solutions to examine the billions of archived FFPE tissue samples are still needed. But advances in technologies, including tissue microarrays (TMAs) and laser-capture microdissection (see  'The cutting edge' ), are signalling that high-throughput analysis might be around the corner. In 1998, Juha Kononen and his colleagues described a tissue sampling method that produced regular-sized spots that could be densely packed on a microarray slide (J. Kononen  et al .  Nature Med.   4,  844\u2013847; 1998). Using this methodology, archival FFPE tissues can be sampled onto TMAs, allowing researchers to examine numerous samples by techniques such as fluorescence  in situ  hybridization and immunohistochemistry on a single microscope slide. Nucleic acids and proteins can even be extracted from archival FFPE tissue TMAs. Best of all, the technology for constructing TMAs is readily available from commercial suppliers. \n               Spot the tissue \n             Beecher Instruments of Sun Prairie, Wisconsin, produces both manual and automated tissue microarrayers. With the Manual Tissue Arrayer II, a block can be directly attached to a microtome for sectioning of arrays, whereas the automated arrayer, ATA-27, accepts nearly all tissue cassettes and can be adapted for either large arrays or variously shaped archival samples. The ATA-27 can also accommodate a full range of tissue spot sizes from 0.6 to 3 millimetres. One concern regarding TMAs has been the representation of a whole tissue section by a single spot. \u201cAs the throughput is so much higher, I think that trumps the fact that it is not a whole section,\u201d says Rimm. But he cautions that care must be taken when constructing tissue microarrays. The optimal region to be sampled for the microarray is usually identified by a pathologist, but only a section or core of this region can be placed into the array. Problems can arise because the region that cores are taken from can vary. A core taken from the leading edge of a tumour might provide different results from a core acquired from the middle. So design of the slide is crucial; spots should be separated and standards (spots of known tissues) used. In the past, pathologists have evaluated and analysed TMAs following immunohistochemistry or other histological analyses. But that is now changing, and some companies are offering software specifically for automated TMA analysis. \n               Image bank \n             Bacus Laboratories in Lombard, Illinois, and recently acquired by Olympus, is one such company. It has focused its efforts on virtual microscopy \u2014 the digital imaging of microscope slides \u2014 which involves scanning an entire slide at very high resolution to acquire a large number of images for each slide. The images are stored in a database and the virtual slide can be reconstructed using software that puts the single images back together to form a whole. It was virtual microscopy that led Bacus into the world of tissue microarrays. \u201cSeveral years ago, there were no good scanning methods for tissue microarrays,\u201d says James Bacus, the company's president, \u201cbut we had the ability to scan entire slides.\u201d Using virtual microscopy as a starting point, Bacus has developed several programs for quantitative, automated TMA analysis including TMAscore and IHCscore. Several other companies also provide automated TMA analysis solutions. Beecher Instruments has the TMAx software package that relies on morphometric processing algorithms and classification rules to automatically interpret TMAs. And HistoRX in New Haven, Connecticut, offers the AQUA automated quantitative pathology system for biomarker discovery and validation from tissue samples. With TMAs moving FFPE analysis onto the fast track of high-throughput analysis, all the pieces are falling into place for researchers to begin exploring archival FFPE tissue samples. The coming years could prove very interesting indeed as the molecular secrets of these FFPE samples are finally unlocked. Reprints and Permissions"},
{"file_id": "449631a", "url": "https://www.nature.com/articles/449631a", "year": 2007, "authors": [], "parsed_as_year": "2006_or_before", "body": "\n               Table 1 \n               Reprints and Permissions"},
{"file_id": "448959b", "url": "https://www.nature.com/articles/448959b", "year": 2007, "authors": [], "parsed_as_year": "2006_or_before", "body": "Analysing proteins from tissue samples that have been formalin-fixed and paraffin-embedded (FFPE) can provide critical information about how cells function before fixation. \u201cWhen you formalin-fix the protein, you fix it in time; it is not going anywhere,\u201d says Peter Tunon, vice-president of sales and marketing at Expression Pathology in Gaithersburg, Maryland. Researchers are taking advantage of this fact to explore the protein world in FFPE samples. Expression Pathology was founded in 2001 by researchers from the US National Cancer Institute and the company Life Technologies (now Invitrogen) who had experience in studying gene expression in tissue and histology. \u201cThe company was founded on the fact that examining protein expression is crucial to understanding what is happening in cells,\u201d says Tunon. To this end, Expression Pathology has worked on ways of extracting and isolating proteins from FFPE tissue samples for analysis by mass spectrometry or reverse-phase dot blot arrays. The company developed a technology that integrates extraction of total proteins from FFPE samples with tryptic digestion so that finished samples are ready for mass spectrometry. To simplify the system further, all reagents are completely compatible with mass-spectrometry instrumentation, says Tunon, making it a good starting point for broad-based screening of proteins in FFPE samples. Other companies also think that protein isolation from FFPE tissue samples will provide valuable information. QIAGEN and EMD Chemicals, both based in San Diego, California, now offer systems that chemically reverse formalin crosslinking to isolate full-length proteins for applications such as western blotting and protein arrays. Surprisingly, post-translational modifications (PTMs) such as phosphorylation and acetylation, can also be observed when examining proteins isolated from FFPE samples. PTMs can be critical to the role of a protein in the cell, changing the function or localization. \u201cWe do see post-translational modifications that are preserved on the peptides, and they seem to be present in ratios that are similar to those we get when using fresh frozen tissues,\u201d Tunon says of work done by Expression Pathology. Even much older archival samples of FFPE tissues do not seem to pose a problem for protein extraction. \u201cWe have worked with samples that are more than 15 years old and found no difference in the profiles compared to samples that were just a few weeks old,\u201d says Tunon. He is quick to add that he believes even older samples could be examined, but Expression Pathology has yet to test this idea. \n               N.B. \n             Reprints and Permissions"},
{"file_id": "448963a", "url": "https://www.nature.com/articles/448963a", "year": 2007, "authors": [], "parsed_as_year": "2006_or_before", "body": "\n               Table 1 \n               Reprints and Permissions"},
{"file_id": "448960a", "url": "https://www.nature.com/articles/448960a", "year": 2007, "authors": [], "parsed_as_year": "2006_or_before", "body": "Researchers have the opportunity to play surgeon \u2014 slicing and dissecting out specific sections of tissue or even cell populations \u2014 with laser-capture microdissection (LCM). This level of analysis might seem to be difficult when applied to formalin-fixed and paraffin-embedded (FFPE) tissue samples, but many companies are now offering easy and quick LCM solutions. Leica Microsystems of Wetzlar, Germany, offers the LMD6000 LCM system, which uses an upright microscope for dissection and capture. Christoph Horlemann, the company's product manager for the LMD6000, says this is possible because the transport mechanism for capture is based on gravity, unlike other LCM systems on the market. 'Transport mechanism' refers to the method for delivering a dissected tissue sample from a slide to a collection vessel. Other approaches to this process include that of PALM Microlaser Technologies in Bernried, Germany, which uses a 'pressure catapult' to send sections into tubes from the LCM instrument, and the CapSure system from Molecular Devices in Sunnyvale, California, which uses the laser to extend a polymer onto the tissue sample for capture. Leica Microsystems' transport mechanism uses slides with a foil covering. The tissue is attached to the foil covering but not the slide, so researchers cut the tissue and the foil together and the sample simply falls by gravity into the collection tube below, explains Horlemann. But even this foil coating might not be needed in the future. Both Leica and PALM are working with Expression Pathology of Gaithersburg, Maryland, on what may be the next generation of LCM slides. Called Director, these glass slides are based on laser-induced forward transfer (LIFT), a non-contact microdissection technique that uses a thin energy-transfer coating that replaces plastic films or adhesives. The technology was co-developed by researchers at the US Naval Research Laboratory in Washington DC and scientists from Expression Pathology. Laser energy is transferred to the transfer layer and the layer is vaporized. The laser energy is then converted into kinetic energy, and the selected feature is shot instantly into a collection tube. LIFT works equally well whether the tissue is collected up (PALM) or down (Leica) as the energy is sufficient to propel the tissue into the collection tube either way. As the transfer layer completely absorbs the laser energy, the biomolecules in the sample are not affected. The use of glass with this coating has other advantages as well. \u201cGlass slides are quite useful as you can also perform fluorescence and contrast applications without any interfering foil,\u201d says Horlemann. Molecular Devices, a division of MDS Analytical Technologies, acquired Arcturus in 2006 and is now providing both the Veritas and the Arcturus XT LCM systems. Whereas most LCM systems use an ultraviolet laser for cutting samples, the systems offered by Molecular Devices can have either ultraviolet or the standard infrared laser options. \u201cThe infrared laser is ideal for small areas where a user is looking to pick up only a cell or two,\u201d says Steven Blakely, product manager for the Arcturus LCM systems. \u201cInfrared allows for a gentle collection of cells.\u201d The use of infrared laser light is critical to the CapSure system because the dye found in the CapSure polymer is activated and becomes adherent with infrared light. Blakely says the CapSure system is particularly useful for FFPE samples that are already mounted on glass slides because the transport mechanism involves only the polymer adhering to the tissue and so can work with any glass slide. Cutting tissue samples with ultraviolet light offers advantages too, such as dissecting thicker samples. Leica has worked to optimize objectives for its LCM systems with high-energy ultraviolet-light transmission, allowing more power to come in direct contact with the tissue sample. Horlemann says this provides faster cutting of the thicker tissue samples while using less power. The future of LCM might just lie in automation and increases in throughput. \u201cNow, researchers want to do faster, automated microdissection for proteomics,\u201d says Horlemann. But before anything else, some hardware and software issues needed to be resolved. Any automated software package has to control all microdissection steps including focusing the microscope, recognizing cells of interest, focusing the laser and defining the area to cut \u2014 definitely not a simple task, but one that companies have worked on and made tremendous strides in recent years. Horlemann says that new advances are making automation easier every day. He points to the Director slides, which can allow for contrasting methods and fluorescence, making it easier when attempting to automatically define cells of interest as a step in the right direction. Although simpler now, further advances will be required before LCM becomes as easy as pushing a button and walking away. \n               N.B. \n             Reprints and Permissions"},
{"file_id": "449627b", "url": "https://www.nature.com/articles/449627b", "year": 2007, "authors": [], "parsed_as_year": "2006_or_before", "body": " George Church, of Harvard Medical School in Boston, Massachusetts, is working on a project that he thinks could change the landscapes of both genomics and medicine. His Personal Genome Project aims to integrate data for genomics, environment and phenotype in more than 100,000 volunteers. Perhaps the biggest issue for the project is how to acquire informed consent from so many participants, who will have their data become publicly available. This is something that Church hopes the Personal Genome Project will be able to address. He comments that some researchers think that this level of scale-up is not compatible with the current rules of informed consent. From the start, participants will need to know what they are volunteering to do. \u201cWe are trying to emphasize to participants that rich holistic genetic and trait data are going to be obtained,\u201d says Church. Therefore, he says, participants must be aware of the risks and benefits of modern genetics and of the fact that modern digital information has various ways to get into the public domain. To make certain that everyone involved clearly understands these points, the project will establish an online educational system targeted to these topics. All participants will need to pass a test to see how well they understand them. After this education and evaluation, the project will allocate participants 'set points', which will determine how much information should be released to them. Additionally, Church says, participants will have access to data only for validated genes for which we know something about their action or disease risks or for which treatment is possible. Although that list is short now, he expects that it will grow quickly. Still, some researchers are concerned about the risks of providing patients with genetic data. \u201cI think there is a danger with a lot of the new and best technologies that it is tempting to provide sequence information to patients before the biological implications of those data are known,\u201d warns Bert Vogelstein of Johns Hopkins University in Baltimore, Maryland. In August 2005 the project received approval from Harvard Medical School's institutional review board to start enrolling its first ten participants. The criteria were stringent: participants had to have at least a master's-level education in genetics or an equivalent understanding of genetics research. Church and others involved in the field were among the first to volunteer because they were thought to be the best informed to give consent. Church thinks that now is the time to resolve the issues of informed consent because the technology has arrived to make personal genome sequencing a reality. He points out that companies such as 23andMe in Mountain View, California, and DNAdirect in San Francisco, either already offer personal genetic testing services or plan to in the future. \u201cWe have to get this in place before everything just goes crazy,\u201d says Church. \n               N.B. \n             Reprints and Permissions"},
{"file_id": "449627a", "url": "https://www.nature.com/articles/449627a", "year": 2007, "authors": [{"name": "Nathan Blow"}], "parsed_as_year": "2006_or_before", "body": "Innovations in DNA sequencing and genotyping are opening doors for personal genomics. Nathan Blow explores these technological advances and their implications. The era of personal genomics is upon us, with advances in technologies such as DNA sequencing and genotyping fuelling the fires. Personal genomics is a story of researchers looking for genetic clues to our most common diseases, of dazzling advances in genetic analysis technology and of lingering questions about how the public will view and use the information. DNA sequencing is clearly driving much of this revolution in personal genomics. In late May 2007, 454 Life Sciences in Branford, Connecticut, and the Human Genome Sequencing Center at the Baylor College of Medicine in Houston, Texas, made headlines around the world with the announcement that they had sequenced James Watson's entire genome using 454 Life Sciences' next-generation sequencing technology. And just four months later researchers at the J. Craig Venter Institute in Rockville, Maryland, along with collaborators at The Hospital for Sick Children in Toronto, Canada, and the University of California, San Diego, published the first full genome sequence of a single individual \u2014 Craig Venter 1 . This analysis, though, relied on the traditional approach of Sanger sequencing.  Now, some groups are looking to take DNA sequencing and personal genomics to even higher levels. \u201cWe want to look at 100,000 genomes and rather than just look at the genomics, in which you get an idea of variation like with the HapMap, we want to actually look at the trait connected with the variation and the environment,\u201d says George Church of Harvard Medical School in Boston, Massachusetts, and founder of the Personal Genome Project (see  'Being well informed' ). When first conceived in 2003, the Personal Genome Project faced numerous challenges, not least that the technology required to meet its goals was not even available. But technology is catching up with ambition, and advances in DNA sequencing are making it possible to decode individual genomes much faster, making endeavours such as the Personal Genome Project more feasible. \n               Sequencing's new wild west \n             A new generation of faster DNA-sequencing systems has exploded onto the genetic-analysis scene, with at least five companies offering or preparing to offer sequencers that boast amazing output. Several sequencers produce upwards of a billion bases of raw data per run \u2014 the equivalent of one-third of the human genome. But Chad Nusbaum, co-director of the genome-biology programme at the Broad Institute in Cambridge, Massachusetts, is quick to point out that these new systems are in fact quite different from one another and at various stages of maturity. \u201cThe principle that we use when applying these new technologies is that there is a lot of expensive sequencing that we do with Applied Biosystem's 3730xl system and anything that we can move over to the new technologies, as long as it is effective, is bound to be cheaper.\u201d The systems available now from Roche, Illumina and Applied Biosystems do seem to be effective, as the Broad Institute and other organizations are using them for various sequencing-based applications. \n               Assembling the future \n             By the end of last year, 454 Life Sciences, which was founded in 2000 and was recently acquired by Roche, had more than 60 of its sequencing systems placed around the world. \u201cOur technology is in all major US genome centres and some of the international centres,\u201d says Michael Egholm, vice-president of research and development at 454 Life Sciences. The technology developed by 454 Life Sciences is based on two fundamental principles: emulsion PCR and pyrosequencing. Emulsion PCR side-steps the conventional process of bacterial cloning by attaching fragments of DNA 300 to 500 base pairs long to beads  in vitro , then amplifying them with PCR to make millions of identical copies. Pyrosequencing allows for a massive parallel reaction format done in 1.6 million wells on a PicoTiterPlate. \u201cRight now, day in and day out, we can perform 400,000 reads of 250 bases each with an accuracy of 99.5% or better,\u201d says Egholm. Although the 454 Life Sciences system is not as accurate as conventional Sanger sequencing, Egholm notes that it is an order of magnitude more productive (see  'Truth and accuracy' ).  This upgraded Genome Sequencer FLX System allows more sequencing cycles and therefore longer reads than the previous Genome Sequencer 20 System. Longer reads help in whole-genome sequencing and assembly applications. \u201cWe believe that shortly there will be many more  de novo  assembled genomes due to our technology,\u201d says Egholm. He notes that the genomes of several microorganisms have been assembled from scratch by use of 454 sequencing, and the technology has also been used to supplement Sanger sequencing on a few projects involving larger genomes. At the Broad Institute, where researchers use two FLX systems and one Genome Sequencer 20, Nusbaum appreciates the ease of the 454 sequencing process. \u201cIt is nicer than Sanger sequencing because it is a faster and simpler process.\u201d He points out that at the Broad Institute, sequencing a bacterium can take a month with Sanger methodology, whereas with 454 technology it can be done in a week and without the high degree of clone tracking associated with Sanger sequencing. Still, for  de novo  sequencing and to assemble larger genomes, such as those of mammals, longer paired reads \u2014 that is, two reads that are a known distance apart \u2014 will be necessary \u2014 an issue that Roche and 454 are trying to address. \u201cWhether [454 sequencing] will work with a mammalian genome is a good question, and it is a little way off,\u201d says Nusbaum. But he optimistically notes that 454 Life Sciences has exceeded his expectations in surmounting several other technical hurdles. Egholm, however, is much more direct in his vision for the future of 454 sequencing. \u201cMy goal is simple, I want to displace Sanger sequencing for  de novo  sequencing.\u201d \n               Counting games \n              Like the Genome Sequencer FLX system, both Illumina and Applied Biosystems have used emulsion PCR as a starting point for their next-generation sequencing systems. But from there the methods of sequencing are quite different from each other. In January this year, Illumina, located in San Diego, California, acquired the Hayward-based firm Solexa. Solexa's key technology, previously called the Solexa 1G and now named the Genome Analyzer, is a next-generation sequencing system that can sequence the equivalent of a third of the entire human genome in a single run. The Broad Institute now uses 16 Genome Analyzers for various projects. \u201cAny application that is counting-related is a very good one to perform using Illumina's system,\u201d says Nusbaum. Nusbaum and his colleagues, along with other groups, have already demonstrated the usefulness of the Genome Analyzer in looking at patterns of chromatin structure by using chromatin immunoprecipitation 2 , 3 , 4 , 5 . \u201cIt is an incredibly powerful application of the technology,\u201d says Nusbaum. By pulling down DNA bound to histones carrying specific modifications, sequencing it and mapping it back to the genome, they could map the status of chromatin across the genome and throughout development. Nusbaum adds that it is also an incredibly easy application of the technology and anticipates that Illumina instruments will be used for other applications such as transcriptional profiling or microRNA and small RNA discovery. \u201cIt is also a great way to identify polymorphisms in genomes that are not extremely different.\u201d  Applied Biosystems in Foster City, California, is rolling out its new sequencing by oligonucleotide ligation and detection, or SOLiD, system in October 2007. The target is to cover a whole human genome in one run, says Kevin McKernan, senior director of scientific operations at Applied Biosystems. McKernan says that in-house, the SOLiD system has been obtaining around a gigabase more data than their target, achieving 4 gigabases of sequence per run that aligns to the target genome, and 8 gigabases overall. McKernan thinks that with advances in the PCR process over time, this will turn into 8 gigabases of sequence that aligns to the target genome. The SOLiD system differs from other next-generation sequencing systems by performing sequencing by ligation rather than by synthesis, as conventional systems do. \u201cTraditionally, people probably don't think of ligases as being more accurate than polymerases,\u201d says McKernan. But he claims that the SOLiD system achieves its accuracy by the new way in which the probes are encoded. By doing successive rounds of ligation and looking at particular probe colour, the SOLiD sequencer obtains information not from just one base, but also from adjacent bases. So after ligation every colour has more than one base of information, which permits multiple colour calls for each base location. By using this redundant information, McKernan says a tremendous amount of error correction can be performed when making base-call decisions. \u201cWhat we have been seeing is that this is giving us a ten to twenty times improvement over polymerase-based systems in terms of raw accuracy,\u201d says McKernan. Applied Biosystems views the SOLiD system as particularly well suited for the cancer-research community and for applications in personal genomics. \u201cFolks in the cancer community are going to gravitate towards it because they are looking for low-frequency mutations and the higher accuracy the system delivers through our error-correction scheme will be beneficial,\u201d says McKernan. He notes that cancer genomes tend to be very complex, with copy-number changes or copy-number neutral changes, such as translocations. Using sequencing systems such as SOLiD, these changes can be visualized by watching when pair reads, or 'mate pairs', break, quickly providing information about translocations. These data are currently analysed with copy-number chip assays (See  'Chipping out our differences' ). Both the Genome Analyzer and the SOLiD system have some limitations because of the short read lengths of around 35 base pairs per run. \u201cFor sequencing larger genomes, 35 bases or fewer per run just won't cut it. We tried with 100 bases for a long time and had problems,\u201d says Egholm. McKernan says that the way Applied Biosystems is attempting to resolve this issue is by using mate pairs. \u201cWhenever there is a single read in a repetitive region you do not know where to place it,\u201d says McKernan. But he notes that with a mate pair you know that that read is linked to something 3\u20134 kilobases away, so those reads can be placed accurately. And this placement accuracy can be very important, as the repeat content of the human genome is high, estimated to be upwards of 50%. \n               Pulling in the 'exome' \n              Church is excited by the fact that the advanced sequencing technology now in place has drastically lowered the cost of sequencing. \u201cWhat has happened in the past year is that the price has plummeted by a factor of 100,\u201d he says, making a project of the scope of the Personal Genome Project realistic from a financial perspective. The tricky part now is getting the most useful information from the human genome in a similarly cost-effective manner. For the Personal Genome Project and other groups, the challenge is to obtain just exons, or the 'exome', from the human genome. Technically, the simplest method, performing in excess of 200,000 individual PCR reactions, would also be the most labour intensive and costly. But groups of researchers and companies are now working on ways to selectively amplify or capture different parts of the genome. \u201cSoon you will be able to cherry-pick all the way along and identify the parts of the genome that are most likely to yield the maximum information,\u201d says Church. And these methods will be very welcome additions to the genomics world because, as Church notes, not all pieces of DNA are created equal. Although he points out that no DNA is 'junk', he contends that by examining only 1% of the genome you can get about 98% of the information about positions that cause changes in traits. The technology necessary for a personal-genomics revolution is here on the scene. Most people say that the major concern for personal-genomics projects is how to deal with the data from participants. And even on that front there seems to be a lot of optimism within the genomics community. Nusbaum is encouraged that people such as Church have taken up such initiatives as the Personal Genome Project. \u201cI am glad that someone like George is taking this on because he has charisma and clout, so that even if people don't want to hear what he is saying, they have to listen.\u201d Reprints and Permissions"},
{"file_id": "449629a", "url": "https://www.nature.com/articles/449629a", "year": 2007, "authors": [], "parsed_as_year": "2006_or_before", "body": " Single nucleotide polymorphism (SNP) genotyping is a method for determining genetic variation. As more and more SNPs have been identified from the genome in recent years, the power of this technique has steadily increased. Affymetrix, located in Santa Clara, California, is one company that is taking advantage of SNP-discovery projects, such as the International Hap Map effort, to generate SNP arrays for whole-genome association studies. In May, Affymetrix launched its next-generation array, called the Genome-wide Human SNP Array 6.0, or SNP6.0. \u201cThis chip allows us to look simultaneously at more than 1.8 million markers of genetic variation,\u201d says Keith Jones, vice-president of assay and application product development at the company. The SNP6.0 not only offers genome-wide SNP coverage, but also contains more than 900,000 probes that target copy-number variants (CNVs) in the genome. \u201cWhen walking down the path in designing the SNP6.0, we took the biochemistry that we used to generate targets to hybridize to the arrays and empirically identified probes that responded in a dose-dependent manner to changes in copy number,\u201d says Jones. Other companies have also placed CNV probes onto their genotyping chips. Illumina, based in San Diego, California, now offers the Human 1M BeadChip, which boasts more than one million SNP and CNV probes, for whole-genome genotyping applications. Nimblegen, located in Madison, Wisconsin, and recently acquired by Roche Applied Sciences, also offers several whole-genome and custom-tiling array comparative genome hybridization products for examining copy-number variation across the entire genome. These arrays contain more than 385,000 probes at a median spacing of 6,000 base pairs. Agilent Technologies, located in Santa Clara, California, provides several array comparative genome hybridization products for analysis of copy-number variation in humans, mice and rats. Illumina also offers several more-focused SNP arrays, including a cancer panel and one that targets the major histocompatibility complex, an area that Affymetrix also seems to be moving into. \u201cI think it is also fair to say that you will see more application-specific SNP panels in the future,\u201d says Jones. \n               N.B. \n             Reprints and Permissions"},
{"file_id": "449628a", "url": "https://www.nature.com/articles/449628a", "year": 2007, "authors": [], "parsed_as_year": "2006_or_before", "body": " Mitch Sogin, director of the Josephine Bay Paul Center for Comparative Molecular Biology and Evolution at Woods Hole Marine Biological Laboratory in Massachusetts, performs environmental sampling of nucleic-acid sequences. \u201cEvery sequence has the potential to tell us an important story,\u201d says Sogin, so highly accurate analysis techniques are needed. But when Sogin's lab switched over from traditional Sanger-based sequencing to the next-generation sequencing system of 454 Life Sciences in Branford, Connecticut, to study these environmental samples, something strange happened. \u201cThe diversity was between ten- and a hundred-fold more divergent than we expected,\u201d recalls Sogin. So Sogin and his colleagues needed to determine whether the unexpected findings represented true biological diversity, or just errors caused by the new sequencing technology. \u201cWe had to explore just how good the sequencing technology actually was,\u201d he says. Sogin and his colleagues did a straightforward experiment in which they resequenced more than 50 templates and cloned sequences on a 454 Life Sciences Genome Sequencer 20 that they had sequenced previously with the Sanger methodology. The work showed that the 454 system was 98% accurate if no culling was used to remove bad bases or reads 6 . However, by using a very simple set of rules, which caused somewhere between 10% and 20% of the data to be discarded, the accuracy could be pushed up to 99.75%. And discarding up to 20% of the data for this level of accuracy is a trade-off that is fine with Sogin because the latest 454 system \u2014 the Genome Sequencer FLX \u2014 can produce up to 400,000 reads per run. Others agree that for some applications the large amount of data generated by the next-generation sequencing systems could trump the accuracy produced through Sanger sequencing. \u201cFor applications such as CHIP-sequencing you can use the 454 or Solexa 1G data even though they have lower base accuracy because you do not need it. What you need is the volume for the experiment,\u201d says Chad Nusbaum of the Broad Institute in Cambridge, Massachusetts. Sogin now thinks that traditional sequencing methods had been underestimating the biological diversity of the environmental nucleic-acid samples. \u201cTurns out that the diversity is coming from low-abundance nucleic-acid populations that you are not likely to encounter if you sequence only a few hundred molecules. You see these low-abundance molecules only if you sequence many tens of thousands of molecules,\u201d he says. \n               N.B. \n             Reprints and Permissions"},
{"file_id": "446219a", "url": "https://www.nature.com/articles/446219a", "year": 2007, "authors": [{"name": "Caitlin Smith"}], "parsed_as_year": "2006_or_before", "body": "Even though the pace of drug discovery is hotting up, many candidate drugs fail late in development. Caitlin Smith looks at some of the tools used early in drug discovery that could help improve the situation. The failure of hitherto promising drug candidates when they go into animal tests and clinical trials occurs far too often for the pharmaceutical industry's comfort. It represents costly failures for the companies involved and pushes up the overall cost of drug development. Improving pipeline productivity is \u201cclearly the biggest challenge\u201d, according to Kevin Hrusovsky, president and chief executive of microfluidics specialists Caliper Life Sciences in Hopkinton, Massachusetts. Others agree. \u201cThe attrition rate of candidate drugs in later clinical development is still way too high,\u201d says Gary Franklin, industrial sector specialist in marketing communications at molecular-interactions company Biacore, based in Uppsala, Sweden, and recently acquired by GE Healthcare. As well as interacting with its molecular target, a successful drug must also behave properly in the human body. It must be easily absorbed, not be broken down too quickly, and have no side effects that make it too toxic to use. Known as ADMET or ADMETox (for absorption, distribution, metabolism, excretion and toxicity), these tests are key to efficient drug discovery. ADMETox testing is initially carried out in cell-based and  in vitro  assays to determine properties such as a compound's solubility, its ability to cross cell membranes, and its cellular toxicity. But these tests are still far from perfect predictors, and improving them is a priority area in drug research. Undesirable ADMETox properties account for some 50\u201360% of drugs that fail at the preclinical stage. Franklin believes that the biggest challenge is to obtain more comprehensive data much earlier in the drug-development process. He points to the Critical Path Initiative, published in 2005 by the US Food and Drug Administration, which aims to stimulate a national effort to develop improved evaluation methods that provide better information and maximize the chances of clinical success. To address this pressing need, researchers will need faster and more precise instruments that make measurements with greater information content than ever before. This is a big demand, but there are some new tools of the trade that may make a difference. \n               Automating ADMETox \n             Equipment companies have their sights set on improving ADMETox testing, including the tests that all new drugs must undergo for their possible adverse effects on the cardiac potassium channel (see  'Ion channels get automated' ). Another routine ADMETox test predicts cellular uptake and efflux of a compound. The absorption of a compound across the epithelial lining of the human gut is conventionally predicted by assaying its transport into and out of Caco-2 cells. Caco-2 is a human colonic adenocarcinoma cell line that will differentiate in culture to express characteristic features of mature intestinal cells. The cells must be grown for 21 days in strictly controlled culture conditions to form a monolayer of differentiated cells before the test can be done. This tedious preparation process can be automated using a liquid-handling workstation such as the Biomek 3000, from Beckman Coulter of Fullerton, California, to perform the culture manipulations. \u201cWe have demonstrated intact and functional Caco-2 monolayers after 21 days of culture,\u201d says Keith Roby, product manager for strategic marketing automation at Beckman Coulter. \u201cThe workstation can also automate the compound permeability and efflux testing and sample collection.\u201d The new Biomek FXP automated platform from Beckman Coulter can be used as a complete ADMETox workstation, and offers a faster  in vitro  assay for drug absorption than the Caco-2 assay. The new assay is based on parallel artificial membrane permeability analysis (PAMPA) and \u00b5SOL technologies for measurement of  in vitro  drug permeability and solubility, respectively, developed by pION of Woburn, Massachusetts. In PAMPA, passive absorption through an artificial phospholipid membrane is intended to mimic the movement of a compound across the gut lining. \u201cThe PAMPA method provides significant time savings compared with the Caco-2 cell-based method, which takes more than three weeks to complete,\u201d says Roby. \u201cWith PAMPA, data can be collected in less than two hours,\u201d he says. All aspects of the permeability assay, including all liquid-handling steps and analysis, are controlled by PAMPA Evolution 96 software. The Biomek FXP also supports metabolism assays, such as cytochrome P450 inhibition studies and microsomal stability assays. The toxicology assays it supports include ones based on cell proliferation, cytotoxicity and apoptosis. The small-footprint Biomek NX Assay Workstation can also be used to automate cell-based screens for apoptosis. The system will schedule a six-hour method to process ten assay plates from the initial induction of the apoptosis pathway to detection of activity of the effector enzyme caspase 3/7, without the intervention of an operator. Mark Collins, associate director of strategic marketing at Thermo Fisher Scientific, based in Waltham, Massachusetts, also considers late-stage failure of drug candidates a big problem for the pharmaceutical industry. \u201cThe attrition rate is one of the biggest issues; too many compounds fail at preclinical or phase I clinical tests,\u201d he says. Collins thinks that high-content screening (HCS) could be one of the answers. \u201cADMETox testing is starting to use HCS for automated genotoxicity assays as well as for predictive assays to rank compounds for animal testing. This research may pave the way for HCS assays to be a better predictor of certain types of toxicity than animals,\u201d he says. On the software front, both Inpharmatica of London, UK, (soon to be part of Belgian drug-discovery company Galapagos) and ChemSilico of Tewksbury, Massachusetts, have software tools to predict ADME properties. \n               Intelligent cell screening \n             \u201cThe continued growth of cell-based assays is driving a need for automated systems to prepare and run these assays,\u201d says Marc Feiglin, chief technology officer in life-science research at Tecan in M\u00e4nnedorf, Switzerland. \u201cAutomating this in a cost-effective manner requires a single system that can pipette both large volumes of media and small volumes of compounds.\u201d The Freedom EVO platform from Tecan offers versatile robotic liquid-handling. It is available in four sizes, with a variety of robotic arms and other liquid-handling accessories. According to Feiglin, it offers the widest dynamic range of pipetting volumes currently available on one platform \u2014 from less than 1 \u00b5l to hundreds of millilitres. The Cellomics family of automated imaging instruments from Thermo Fisher Scientific are designed for HCS and high-content analysis. They enable researchers to analyse the responses of cells  in vitro  to stimulation by small molecules or the knockdown of gene expression by RNA interference (RNAi). \u201cHCS provides temporal and spatial information about such cellular responses as well as phenotypic profiles,\u201d says Collins. The core of a typical Cellomics HCS system is an imager such as their ArrayScan VTI. This is packaged with quantitative image-analysis software modules that address the experimental area of interest and data-management software, necessary because of the huge quantities of imaging data that HCS generates. A feature of the Cellomics HCS system is intelligentAcQuisition (iQ), which allows images to be analysed on-the-fly in real time, and measurements from the analysis fed back to allow the image-acquisition process to adapt to the biology being measured. \u201cIQ provides significant productivity gains compared with HCS systems that acquire images and then do the analysis offline, as it performs in parallel all the steps of the HCS process during the time the plate is being scanned,\u201d says Collins. He likens Cellomics iQ to a digital camera, which allows you to see immediate results. Without it you're more like someone taking pictures with a film camera: \u201cYou don't know how good the picture is until you have the film developed, but by then it might be too late, or you have too many pictures of the same thing,\u201d he says. \n               Picking up the signal \n             One application of Cellomics HCS systems is to study intracellular signalling pathways in oncology, as they enable a pathway to be 'walked' using multiplexing. Another company pursuing ways to streamline signalling research is DiscoveRx of Fremont, California, which received a 2006 Frost & Sullivan award for technology innovation for its PathHunter assay platform, which is used to study signalling pathways in intact cells. A recent addition to the range is a \u03b2-arrestin assay to study the activation of G-protein-coupled receptors (GPCRs), a common drug target. An attractive feature of PathHunter assays is that they are relatively simple: they are adapted to microtitre plates, involve only one or two additions of reagents, and do not require cell washing or fixation. The assays are compatible with many different cell lines, especially those most commonly used in high-throughput screening, such as CHO and HEK cells. PathHunter assays detect receptor activation by a technique called enzyme fragment complementation. Two modified \u03b2-galacto-side fragments, fused to the receptor and the \u03b2-arrestin molecule, respectively, are brought together when \u03b2-arrestin binds to the activated receptor and form an active enzyme that generates a chemiluminescent signal. DiscoveRx claims that PathHunter is the first chemiluminescence assay technology that can measure protein trafficking directly inside the cell \u2014 as such, it is especially suited to evaluating how cellular physiology is perturbed by drug candidates. It can be used to study translocation, degradation, secretion, protein\u2013protein interactions and membrane trafficking. The chemiluminescent signal generated does not need dedicated imaging technology and can be read in 96-, 384- or 1,536-well microplates with a standard luminometer. Tecan's new Infinite series of microplate readers are compatible with its Freedom EVO liquid-handling workstation and are available as either filter- or monochrometer-based systems. According to Feiglin, the sensitivity of the luminescence readers makes them a popular choice for assays of GPCR activity. Using the 1,536-well optics options on the Infinite F500 filter-based microplate reader, a 1,536-well plate can be read in less than 30 seconds. Also new from Tecan is the HydroFlex automated microplate washing and vacuum filtration system, which can be integrated into the Freedom EVO platform. CyBio of Jena, Germany, made its reputation as one of the first companies to offer multipipettors. It recently unveiled a new robotic liquid-handling platform, CyBi-RoboSpense, which will work with almost any type of sample tube or plate, and can even transport plates to other instruments such as other pipettors, microplate readers or liquid chromatography or mass spectroscopy equipment. The CyBi-RoboSpense can also be used for complicated sample preparation and purification routines such as tryptic digests and magnetic bead separation. \n               Microfluidics applications \n             More companies are incorporating the principles of microfluidics into their instruments, making the movement of microlitre \u2014 or even nanolitre \u2014 volumes of solutions do what used to take hundreds of millilitres, not to mention a saving in time and labour. An innovator in this area is Gyros of Uppsala, Sweden, whose Gyrolab Bioaffy microlabs for protein purification and quantification are based on affinity-capture techniques. The Gyrolab Bioaffy 200 is ideal for high-sensitivity assays to detect low concentrations of proteins. It can be used, for example, to quantify protein markers of disease present in the nanograms per millilitre range. The Gyrolab Bioaffy 20HC is designed for bio-pharmaceutical applications such as process optimization and the quantification of monoclonal antibody drugs intended for therapeutic use, especially for measurements on the milligrams per millilitre scale, which means that samples can be run without dilution. The CD-sized microlabs run on Gyrolab workstations; the Gyrolab Workstation LIF can run up to five CDs per batch, and includes a laser-induced fluorescence (LIF) detector. \u201cWhat made this system particularly interesting to the customer,\u201d says Christina Burtsoff Asp, vice-president of marketing at Gyros, \u201cwas that it provided a measurement range from micrograms to grams per litre, which allowed dilution-free analysis in concentration ranges relevant for the entire purification process, from cell supernatant to purified product \u2014 all on a single system.\u201d Another new product using microfluidic technology is Caliper's Desktop Profiler, a bench-top kinase-profiling system that is pre-loaded with microplates with protein kinases, substrates and reagents. \u201cKinase profiling enables the identification of potential side effects in therapeutic candidates, thus helping researchers to better understand how a compound will react in the human body,\u201d explains Hrusovsky. The Desktop Profiler uses Caliper's LabChip microfluidics-based screening technology, and is designed for researchers seeking to qualify drug candidates \u2014 that is, to select the more promising drug candidates from those that might be too toxic or ineffectual. Fluidigm in San Francisco, California, recently introduced a new digital array nanofluidic chip for a type of quantitative PCR that the company calls digital PCR, which runs on its BioMark system. Digital PCR is based on limiting-dilution analysis, and estimates the absolute number of copies of, for example, a target sequence in a sample by successively diluting the sample and partitioning it into a large number of chambers, such that each chamber contains either one or no target molecules. Nanofluidics makes this application of limiting-dilution analysis practicable, in that after the sample is loaded, it is automatically distributed into about 1,000 nanolitre-volume reaction chambers. Amplification of the target sequence by PCR in chambers that contain a molecule can be detected and the number of original molecules in the sample calculated. The new technology is already proving useful: one group of researchers has used it to correlate specific copies of genes with the regulatory states of single stem cells; another study used digital arrays to identify individual bacterial cells carrying marker genes. This technology promises to compete with the microwell plates currently used for high-throughput quantitative PCR. Digital arrays offer higher throughput, use smaller amounts of reaction solutions, and avoid the thousands of pipetting steps that would be needed for a similar experiment in microwell plates. \n               Going  \n               in vivo \n             However sophisticated cell-based and  in vitro  tests become, promising candidate drugs have eventually to be tested in living organisms. Hrusovsky points to the problem of the poor correlation of  in vitro  experiments with what happens  in vivo . \u201cPipeline productivity can be improved by preventing the drug attrition that happens when a candidate compound moves from  in vitro  to  in vivo  studies,\u201d he says. Caliper's IVIS Spectrum  in vivo  optical-imaging system uses tomography to give three-dimensional images of fluorescent and bioluminescent reporters in whole organisms. It is designed to help researchers \u201cbetter visualize, track and understand how a therapeutic compound reacts, at a molecular level,  in vivo , thus bridging  in vitro  and  in vivo  experimentation\u201d, says Hrusovsky. Spectral unmixing capabilities allow multiple fluorescent reporters to be used. Illumination can be switched from above the specimen to below it, enabling the tomographic reconstruction of, for example, both shallow and deep tumours. \n               Getting more selective \n             Several general trends may speed up the drug-discovery process and reduce the rate of attrition of drug candidates. Franklin notes a shift towards structure-based techniques for drug discovery and the identification of new druggable targets (see  ' In silico  screening with chemical informatics' ), using dramatically smaller compound libraries compared with high-throughput screening. \u201cThis has the potential to significantly shorten drug-discovery timelines and, hopefully, to increase overall productivity in the industry,\u201d he says. \u201cThe trend towards fragment-based screening in particular shows great promise.\u201d \u201cMore rational screening \u2014 that is, for higher information content \u2014 of smaller libraries would help,\u201d agrees Collins. \u201cToo many targets from molecular biology and genomics have little or no clinical relevance; high-content screening and other technologies such as RNAi can help here.\u201d Collins also sees pressures on the drug-discovery industry to produce 'personalized medicine', which would provide a drug that best fits the individual patient. Traditionally, the drug industry \u201chas sought the one-size-fits-all blockbuster, so to respond to the idea of personalized medicine, technological, economic, and ethical changes will be needed\u201d, he says. \u201cSome believe that the industry is poised to see the benefits of the investments made in new technologies over the past few years, and that state-of-the-art informatics is finally capable of supporting the complexity of life-science discovery,\u201d says Kathleen Mensler, vice-president of marketing and corporate development at modelling software company Tripos in St Louis, Missouri. \u201cWith advances in understanding biological pathways and the unprecedented amount of valuable data available, breakthroughs are inevitable. The question is when.\u201d Reprints and Permissions"},
{"file_id": "4501121a", "url": "https://www.nature.com/articles/4501121a", "year": 2007, "authors": [], "parsed_as_year": "2006_or_before", "body": "\n               Table 1 \n               Reprints and Permissions"},
{"file_id": "446223a", "url": "https://www.nature.com/articles/446223a", "year": 2007, "authors": [], "parsed_as_year": "2006_or_before", "body": "\n               Table 1 \n               Reprints and Permissions"},
{"file_id": "4501117a", "url": "https://www.nature.com/articles/4501117a", "year": 2007, "authors": [{"name": "Nathan Blow"}], "parsed_as_year": "2006_or_before", "body": "Drugs to treat diseases from cancer to AIDS could soon rely on short strands of RNA for their effects. But scientists must first work out how to navigate these fragments around the body. Nathan Blow reports. The remarkable ability of short sequences of synthetic RNA to interfere with messenger RNA and thereby silence the activity of specific genes has proved incredibly helpful to geneticists wrestling with genetic function. And the push to harness this RNA interference (RNAi) for therapeutic use is now beginning to make headway. In the six years since the first paper reporting RNAi gene silencing in mammals was published 1 , at least six therapeutic programmes based on the concept have moved into clinical trials. \u201cProgress in the field of RNAi therapeutics has occurred remarkably fast,\u201d says John Maraganore, president and chief executive of Alnylam Pharmaceuticals in Cambridge, Massachusetts. But delivering the sequences remains a problem. Initial clinical trials relied on 'local delivery', directly introducing short interfering RNAs (siRNAs) into the specific tissue they were to treat. But for true therapeutic value, the siRNAs need to be introduced systemically.  \u201cSystemic delivery is the major issue right now,\u201d says Alan Sachs, vice-president for RNA therapeutics based at Sirna Therapeutics, a wholly owned subsidiary of Merck in San Francisco. Getting a small RNA to interfere with the right messenger RNA in the correct tissue and cell type at a safe, therapeutic level by systemic administration requires an exquisite degree of control \u2014 creating the need for different delivery vehicles and potentially even specialized targeting strategies. Animal studies 2  have shown that it is possible for siRNAs delivered systemically to silence target genes. \u201cWhat we have learned over the past couple of years is that systemic delivery of RNAi can be achieved, and there are a variety of methods that can be used to achieve it,\u201d says Maraganore. But he is also quick to note that there is no simple solution. \u201cIf you inject naked siRNA into the blood, under normal pressure, it doesn't work,\u201d says Daniel Anderson of the Center for Cancer Research at the Massachusetts Institute of Technology (MIT) in Cambridge. Yet naked siRNA delivered directly into the lungs to treat respiratory syncytial virus (RSV) can profoundly reduce viral replication. This contrast highlights the chasm between local and systemic delivery of synthetic siRNAs. \n               Local delivery \n             Alnylam focused on local delivery when it began to develop RNA-based therapeutics. Its treatment for RSV in the lungs uses an inhaled synthetic RNA that triggers the destruction of a protein essential for the virus's replication. Inhalation allows high local concentrations of the RNA to be achieved, says Maraganore, while also taking advantage of naturally occurring mechanisms, such as pinocytosis, for uptake into the target cells.  Work by researchers at the company has also helped shed light on systemic delivery. In a 2004 study 2 , they affirmed for the first time the potential 'drug-like' properties of siRNAs when delivered systemically. The team used a synthetic RNA conjugated to cholesterol and stabilized with a partial phosphorothioate backbone and 2\u2032- O -methyl sugar modifications on both the sense and antisense strands of the RNA. Since this study, both lipid- and polymer-based vehicles for systemic delivery of siRNAs have been developed and tested. At Polyplus-transfection in Illkirch, France, researchers have taken advantage of the difference between cationic polymers and cationic lipids for systemic delivery to different organs. \u201cWe are interested in delivery to the lung and have used systemic administration of the cationic polymer polyethylenimine for delivery,\u201d says Patrick Erbacher, the company's chief scientific officer. \u201cBut for tumour injections, we use either a cationic polymer or a cationic lipid formulation.\u201d With siRNAs conjugated to lipids or encapsulated in liposomes or lipid nanoparticles, several companies have achieved stable and efficient systemic delivery to organs including the liver, pancreas, kidneys and even to some types of tumour. And polymers that can complex with siRNA can deliver the short sequences to organs such as the lungs, spleen and kidneys.  Researchers at Altogen Biosystems based in Las Vegas, Nevada, are exploring cationic lipids and biodegradable polymers for  in vivo  delivery, but have not found it easy. \u201cThere is no perfect method for delivery,\u201d says Andreas Kim, the company's vice-president of research and development. \u201cNothing really works amazingly well. All methods have their advantages and disadvantages.\u201d In mice, he notes, lipid-based delivery of siRNA is very efficient but tends to induce an inflammatory response to the lipid formulation. Delivery vehicles based on biodegradable polymers, on the other hand, don't cause inflammatory responses but are not delivered as efficiently and the effects seem to be more transient than their lipid-based counterparts. Alnylam is using liposomes to deliver siRNAs to the liver. \u201cIt is easier to target things in the liver with liposomes because about 95% of the injected dose for liposomal formulations goes to the liver,\u201d Maraganore says. Liposomes are synthetic analogues of the cell membrane and are made up of hydrophilic and hydrophobic regions that form spherical 'packages' in aqueous conditions. Alnylam is using this approach to target two genes in the liver \u2014 one involved in regulating levels of low-density lipoprotein in the blood and the other involved in liver cancer (targeting both vascular endothelial growth factor and kinesin spindle protein). The ability to use lipid-based delivery vehicles to target the liver has made the organ a popular starting point for many companies. Merck, for example, is using lipid nanoparticles that, like liposomes, take advantage of endogenous mechanisms for uptake, but have no specific ligand attached for targeting. Although lipid nanoparticles are its main focus, Merck is also exploring other delivery vehicles \u2014 in some cases through external collaborations. \u201cWe are working aggressively in the licensing arena \u2014 inviting people to work with us in an evaluation phase,\u201d says Sachs. In October, the company finalized a licensing agreement with Protiva Biotherapeutics of Burnaby, British Columbia in Canada, for its stable nucleic acid lipid particles (SNALPs). These are specialized lipid nanoparticles that encapsulate siRNA and were the first non-viral siRNA delivery vehicles that showed activity in non-human primates 3 . Alnylam has also launched a number of academic and industrial delivery collaborations (see  'The vehicle laboratory' ).  The rapid advancement of RNAi-based therapeutics is leading Polyplus-transfections to explore the manufacturing aspects of delivery vehicles. The company develops and markets DNA, RNA and protein transfection and delivery reagents for both  in vitro  and  in vivo  applications. And with RNAi therapeutics on the cusp of entering the clinic, the company sees the need to produce delivery vehicles in bulk under governmental quality specifications or 'current good manufacturing practices' (cGMP) standards. \u201cIf you want to be able to get these into the clinics, you have to have cGMP-qualified delivery systems,\u201d says Erbacher. \n               Beyond the liver \n             Despite this progress in general delivery, there is still some way to go for full therapeutic application. \u201cI do not think that we are at the point now where we can name a tissue, specifically deliver and get good knockdown,\u201d says Anderson. Although he notes that there is good evidence to suggest that targeted systemic delivery should be possible. Kim agrees that targeted delivery looks promising, but he cautions that the development of the delivery vehicles will be complicated. \u201cYou need a number of additional steps in the formulation process,\u201d he says. A promising approach to targeted delivery involves incorporating targeting elements such as antibody fragments, ligands and small chemical groups with the synthetic RNA. At the same time, techniques are being designed to image targeted nanoparticle delivery and biodistribution (see  'The inside track' ). Some of the new approaches might overcome issues associated with lipid-based vehicles. For example, RNA aptamers attached to siRNAs, which have been used by researchers to target prostate cancer cells, might not cause the inflammatory responses seen in some cases with lipids or antibodies. Another approach that might avoid inflammatory responses is being developed by Calando Pharmaceuticals in Pasadena, California, using polymers that contain cyclodextrin. When the polymers are mixed with siRNAs, they bind to the RNA backbone and assemble into nanoparticles. Targeting ligands or even stabilizing agents can then be attached to the cyclodextrin in the polymer to improve delivery. \n               The evolutionary advantage \n             Another approach to targeted delivery takes advantage of nature. \u201cViruses have learned how to get into cells \u2014 cross the membrane into the cytoplasm then get into the nucleus,\u201d says Inder Verma, a geneticist at the Salk Institute for Biological Studies in La Jolla, California. \u201cAll other approaches have to figure out how to do this.\u201d This gives viruses an \u201cevolutionary advantage\u201d as delivery vehicles for small RNAs, he adds. By exploiting the natural diversity of viruses, researchers have developed a suite of viral vectors that can be used to deliver therapeutic sequences into a wide variety of cell types. Viral vectors have been around for some time and scientists are currently using them to deliver short hairpin RNA (shRNA) that can be processed by cells to yield siRNAs and silence genes. Viral vectors from the DNA-based adenovirus and adeno-associated virus (AAV) have been used by researchers and companies, such as ArmaGen Technologies in Santa Monica, California, for shRNA delivery. AAV vectors have, however, found more widespread use in RNAi applications as these viruses can transduce both dividing and non-dividing cell types and result in stable, site-specific integration.  Lentivirus-based vectors are another option. \u201cThe utility of lentiviral vectors for RNAi research is their ability to transduce non-dividing cells and to efficiently transduce difficult-to-transfect cells,\u201d says Boro Dropulic, founder and chief executive of Lentigen in Baltimore, Maryland. But any viral vector will still have to overcome the issue that plagues synthetic RNAs: targeting. \u201cTargeting is a big problem. There are several groups working on that issue now but the jury is still out,\u201d says Dropulic. What makes targeting viral vectors such a challenge is the difficulty in altering the viral structure with targeting elements while maintaining the proper function of viral particles. For adenovirus and AAV vectors, for example, the viruses require a specific number of proteins to make their viral shell. Adding more proteins, such as an antibody fragment for targeting purposes, could be problematic. \u201cImagine the viral shell is like the dome of a sports centre,\u201d says Verma. \u201cYou can only put so much information in there, after which the dome will break.\u201d To overcome this problem, Verma thinks that researchers will need to find a way to modify the entire shell of the virus. Lentiviral vectors are less problematic \u2014 the virus particles are surrounded by a lipid envelope in which proteins can be inserted without disrupting the viral structure. Even so, efficient and specific targeting remains an issue.  \u201cI think that it will come down to two things: a more extensive learning of the modification of the viral structure proteins, and identification of specific target receptors so that you can have the viruses interact with cells in a very specific manner,\u201d says Verma. He points to recent work from David Baltimore's lab at the California Institute of Technology in Pasadena, in which a small monoclonal antibody was linked to a lentiviral envelope protein for targeting to specific cell types 4 , and to other groups trying to use small ligands and signature sequences from cell-surface receptors as promising developments for the targeting of viral vectors. As researchers wrestle with the targeting issue, some companies have already begun to use lentiviral vectors to deliver therapeutic RNAi. Dropulic sees the  ex vivo  use of lentiviral transduction systems as being a critical first step. \u201cI think using the transduced cells as the vehicles of widespread dissemination rather than thinking about direct injection of lentiviral vectors will be the way to go,\u201d he says. Lentigen is using this approach on T cells and stem cells for cancer therapies as well as for treatment of infectious diseases. \n               The silent future \n             Most experts agree that therapeutic RNAi will probably not rely on a single delivery vehicle or administration approach for all diseases. \u201cSome diseases might require lower doses than others or could be semi-local in nature,\u201d says Robert Langer, a bioengineer at MIT, \u201cso I think a lot may depend on the disease and treatment modality you are thinking about.\u201d One likely beneficiary of the work on delivery mechanisms are siRNA's recently uncovered siblings: microRNAs (miRNAs), which are naturally occurring siRNAs found in plants and animals. \u201cWith miRNA therapeutics, you are generally focusing on a new class of non-coding RNAs where the biology is still being discovered,\u201d says Maraganore. It is early days for this technology, but companies are now being established to work exclusively on miRNA-based therapies (see  'Thinking small' ). When it comes to siRNA, Maraganore is encouraged by the early approaches to delivery being used in clinical studies. He expects the development of new delivery vehicles to continue for sometime to come. \u201cI suspect that in 35 years scientists will continue to work on optimizing delivery approaches,\u201d he says. And Verma thinks that the discovery of RNAi has even breathed fresh life into the still struggling field of gene therapy. \u201cI think having RNAi technology available now gives a new impetus because it is such an effective technology \u2014 that is, if only we could deliver it.\u201d Reprints and Permissions"},
{"file_id": "446219b", "url": "https://www.nature.com/articles/446219b", "year": 2007, "authors": [], "parsed_as_year": "2006_or_before", "body": "Ion channels currently represent a less well charted territory of druggable targets than cell-surface receptors and enzymes, but this is changing. The electrophysiological techniques traditionally used to study ion-channel activity (one-cell, one-pipette patch clamping) are too slow for screening drug candidates, but some companies are finding ways to adapt and automate these techniques. One factor driving this development is the requirement by the regulatory authorities that all drugs must be tested for possible effects on a cardiac potassium channel (the hERG channel), which can cause cardiac arrhythmia. Compounds with adverse effects need to be weeded out as early as possible in the drug-discovery process. The Dynaflow Pro II system from Cellectricon in G\u00f6teborg, Sweden, is based on a combination of patch clamping and microfluidics. \u201cDynaflow enables sequential rather than parallel testing of compounds on ion channels by high-speed translational scanning of a single patch-clamped cell across a laminar stream of solution environments,\u201d says Mattias Karlsson, vice-president of research and development at Cellectricon. Use of patch clamping retains the high quality of data characteristic of the technique, which is crucial in regulatory testing for the hERG channel, but the microfluidics component increases throughput compared with traditional lab methods. For the highest-throughput automated patch-clamping, turn to Sophion Bioscience in Ballerup, Denmark. Sophion's QPatch HT is a 48-channel gigaseal patch-clamp system that can be used to study both voltage-gated and ligand-gated ion channels. Each channel, which patches one cell, is controlled individually, so the system consists of 48 individual low-noise patch-clamp amplifiers and pressure controllers (for individual gigaseal formation). The liquid-handling robot controls eight pipettes. The recording chambers are contained within Sophion's QPlates, which house 48 glass-coated microfluidic channels that hold about 5 \u00b5l. With the small recording volumes and a liquid-exchange time of about 100 milliseconds, it is possible to screen multiple compounds on the same cell, or perform cumulative dose\u2013response experiments. Npi Electronic in Tamm, Germany, has incorporated a Tecan liquid-handling system into its ScreeningTool instrument for automated, fast and precise screening of ion-channel activity using two-electrode voltage clamping of  Xenopus  oocytes. Drugs are delivered by a rapid (millisecond resolution) automated system into the 15-ml bath of a miniature recording chamber. Npi plans to extend ScreeningTool to other cell types. The ICR range of spectrometers from Aurora Biomed of Vancouver, British Columbia, provides a non-electrophysiological screen for ion-channel activity. The machines use atomic absorption spectroscopy and flux assays to detect activity and can be used to study both ligand- and voltage-gated channels. The new ICR 12000 is designed for ultra-high-throughput screening of compound libraries against ion-channel targets. \n               C.S. \n             Reprints and Permissions"},
{"file_id": "446221a", "url": "https://www.nature.com/articles/446221a", "year": 2007, "authors": [], "parsed_as_year": "2006_or_before", "body": "Imagine evaluating the efficacy and safety of new drugs without having to lift a pipette. Advances in  in silico  drug development are beginning to make this possible. The SYBYL range of computational chemistry software from Tripos of St Louis, Missouri, will design compound libraries, select the best binding partners and screen potential drug molecules for safety and efficacy on the basis of known protein target and/or ligand structures \u2014 all  in silico . A new advanced protein-modelling module predicts the structure of proteins of unknown structure using related proteins of known structure as the models. \u201cIt rapidly and accurately provides a valuable protein structure to be used for  in silico  docking experiments,\u201d says Kathleen Mensler, vice-president of marketing and corporate development at Tripos. Another new product, Surflex-Sim, helps find new classes of compounds that are structurally similar to already known, suboptimal, lead compounds, but that may be more active or safer. Modelling software from Cresset BioMolecular Discovery in Letchworth, UK, uses surface properties or 'molecular fields' of proteins and other molecules to predict intermolecular interactions. \u201cWe know that structurally diverse compounds can bind to the same protein site and elicit the same response, but this has never been fully explained by molecular structure,\u201d says Sally Rose, Cresset's director of business development. \u201cOur view is that molecular fields provide vital information for understanding and predicting activity,\u201d she says. The fields are derived by calculating the interaction energy of a 'probe' atom carrying a positive, negative or neutral charge with the compound. \u201cOur fields summarize the key binding information, and we locate the most important regions around a ligand where the fields are strongest and binding is expected,\u201d says Rose. The original FieldScreen software for virtual screening has been joined by FieldTemplater, designed to find three-dimensional bioactive conformations, and FieldAlign, a molecular-alignment tool for comparing the three-dimensional structures of two molecules. A pioneer of  in silico  modelling, De Novo Pharmaceuticals in Cambridge, UK, recently announced a new focus for its drug-discovery programme \u2014 finding drug targets in various metabolic diseases. De Novo has upgraded SkelGen, its proprietary  in silico  drug-design platform, with the addition of Reflex, which takes into account the rotational flexibility of amino acids in the target's active site. This better evaluates the shape of the active site and its interaction with small-molecule ligands, so that the modelling more closely resembles the dynamics of real protein\u2013ligand interactions. \n               C.S. \n             Reprints and Permissions"},
{"file_id": "446937a", "url": "https://www.nature.com/articles/446937a", "year": 2007, "authors": [{"name": "Hayley M. Birch"}, {"name": "Julie Clayton"}], "parsed_as_year": "2006_or_before", "body": "To define the workings of cellular structures and molecules requires cutting-edge technology not only in biology and biochemistry, but also now in nanotechnology. Hayley M. Birch and Julie Clayton report. The dawn of the nano-era has placed a whole new array of tools in the hands of cell biologists who are keen to go deeper into the intricacies of how cells work. Forever pushing the boundaries, cell biologists are shifting focus from the micro- towards the nano- and even sub-nano level. To do so means having to find ever more creative ways of using not just biology, but also elements of physics, materials science and engineering. Biophysicists, nano-technologists, nanofabricators and electrical engineers are working side by side to probe further into the cell. \n               Nanotopography \n             Knowing what makes a cell tick is as much about understanding its outside environment as its internal workings. The availability of sophisticated nanopatterning and nanotopography techniques \u2014 in which surfaces can be etched and coated with a variety of substances at the nanoscale \u2014 is enabling cell biologists to gain a more precise understanding of, and control over, cellular responses compared with cell culture in conventional glass or polystyrene culture plates A range of substrate structures designed specifically for exploring cell behaviours such as spreading, migration and adhesion are under development (see  \u201cCell culture in three dimensions\u201d  and  \u201cDown to the letter\u201d ,  page 940 ). The field is so new that most research teams are creating their own nanopatterned surfaces on which the cells are cultured. At Columbia University, New York, Michael Sheetz is developing nanopatterning methods with a team of cell biologists, systems biologists and nanofabricators. \u201cWe put down arrays of different spacings of molecules and use those to measure cellular responses,\u201d he explains. \u201cWe have found that the spacing really matters.\u201d Sheetz and colleagues are currently using patterning to examine how the spacing of protein ligands on a substrate surface is important in the binding of dimeric proteins such as talin. While Sheetz and many others are currently using home-made devices, commercial developers are beginning to sense an opportunity. BioForce Nanoscience in Ames, Iowa, has developed the Nano eNabler, a device that can deposit molecules on surfaces at defined locations at nanometre resolution, and which, among its various options, can be used to apply extracellular matrix proteins onto cell-culture surfaces in predefined patterns. A key feature of the machine is that the drops deposited can be as large as 100 \u00b5m or as small as 2 \u00b5m across \u2014 smaller than a single cell. \u201cThe cells can interact with many spots at once, which enables the investigator to construct more complex questions, such as how well a cell will respond given a choice of two different proteins,\u201d says BioForce's product manager Michael Lynch. The fine control is achieved through a microfluidic cantilever, which dispenses liquid onto a surface that is etched into defined regions. The Nano eNabler was originally developed for the biosensor market, to look at interactions between proteins, DNA and RNA without the need for labelling. But interest has grown in the past year from researchers wishing to use the device for nanopattern-based cell culture. Around a dozen research groups are currently exploring its potential for this kind of work, including the study of neurons. Lynch anticipates that in future cell biologists will prefer to use the equipment to design their own nanopatterns rather than buy nanopatterns off-the-shelf. \u201cThere are so many different questions being asked that it's hard to narrow down to one pattern, chip or substrate for cell culture. It's not as standard as the microarray field, which has turned into a service.\u201d \n               Two worlds collide \n             Until recently, cell biologists using confocal microscopy and those using electron microscopy (EM) inhabited two completely separate worlds: confocal microscopists made movies in glorious technicolour, whereas EM practitioners produced black-and-white stills. Both caught cells on camera, but neither got all the results they really wanted. For confocal film-makers the problem is the lack of resolution, as this has to be sacrificed in order to capture live-action cell behaviour. EM experts, on the other hand, have all the resolution they require, but only a slim chance of snapping a cell in the process of doing something interesting. But at last these two worlds are slowly converging to give correlative light and electron microscopy through a new and surprisingly simple technology. Ian Lams-wood, marketing manager for Leica Microsystems in Vienna, Austria, explains how Leica was approached by Paul Verkade and his colleagues from the Max Planck Institute for Cell Biology in Dresden, Germany. \u201cThey wanted to look at the same cell in the EM as they had in the confocal; basically to freeze-frame it and look at the same proteins being synthesized at the same moment in time.\u201d Verkade, an electron microscopist now at the University of Bristol, UK, recalls, \u201cI saw a paper in  Nature Cell Biology Reviews  that said it would be great to have something that could combine fluorescence and electron microscopy. It was a really funny coincidence because we were working on it, but of course, I couldn't say.\u201d His invention, currently marketed by Leica as the Rapid Transfer System (RTS), takes a sample from under the confocal microscope to a high-pressure freezer within a few seconds, in preparation for electron microscopy. The RTS has been designed as an attachment to Leica's EM PACT high-pressure freezer. It consists of a rapid loader, which holds the live sample beneath the confocal, then transfers it quickly to the high-pressure freezer, creating a specimen that can be used for EM. Confocal microscopy allows the user to pinpoint a potentially interesting cell event, while the high-pressure freezing preserves the fine ultrastructure at the chosen moment. This provides the researcher with a snapshot of a known cell activity. Using the RTS system, the time for transfer between light microscopy and the fixation of a cell for EM is reduced from around a minute to less than 5 seconds. The state of the cell viewed in the EM is then the same as that last seen by the confocal microscope, but at a much higher resolution. \u201cThis machine has the added advantage that it is really good for standard fixation for EM,\u201d notes Verkade. For other investigators, the most appealing new development is the combination of atomic force microscopy (AFM) with fluorescence microscopy. Veeco, of Woodbury, New York, has combined AFM with an inverted optical or confocal microscope in the BioScope II, which enables the high-resolution images of AFM to be complemented by fluorescence microscopy. The BioScope II is an advance over previous Veeco models, with new software for greater flexibility and control by the investigator. Dennis Discher and his team at the University of Pennsylvania in Philadelphia are using the BioScope II for investigating the differentiation of stem cells on different types of substrates. In particular, they want to assess the rigidity of the cytoskeleton and mechanical changes in its microenvironment at the various stages of differentiation. AFM, like scanning EM, gives highly precise topographical information \u2014 what Discher calls \u201dblobology\u201d \u2014 such as height, length and shape. But in contrast to EM, AFM can be performed on live cells. Combining this with a fluorescence microscope allows investigators at the same time to identify and track the various structures observed by AFM, for example, in cells expressing proteins tagged with green fluorescent protein. \u201cYou can do mechanical interrogations on cells, and on isolated molecules and complexes, that you can't do with EM \u2014 such as make force measurements \u2014 and as well you can treat with drugs, and then push and poke to characterize remodelling, and this can all be done before, during and after imaging by fluorescence,\u201d says Discher. JPK Instruments in Berlin, Germany, also markets a new AFM instrument \u2014 the NanoWizardII BioAFM \u2014 which can be mounted either on a confocal laser-scanning microscope (such as the LSM 510, from Carl Zeiss in Jena, Germany) or on an inverted light microscope (such as the Zeiss Axiovert 200). This allows the user to identify the location of cells of interest across a wide field of view before using AFM to scan a particular region at high resolution and/or perform fluorescence imaging. JPK Instruments also supply a cell-culture vessel (the JPK BioCell) in which cells can be maintained in buffer or cell-culture medium at 37 \u00b0C for the duration of the observation. The AFM tip can be coated with ligands or used to prod cells mechanically, and the ensuing biochemical reaction observed by fluorescence. JPK's DirectOverlay software includes calibration that enables the user to integrate the various types of images and precisely superimpose a fluorescence image onto a 3D projection of a cell. These techniques are particularly well suited to studying the role of the cytoskele-ton in processes such as cell spreading and attachment. Another company improving the performance of atomic-force microscopy by integrating it with confocal microscopy is Asylum Research of Santa Barbara, California, while nAmbition in Dresden, Germany, offers instruments for automated force spectroscopy. \n               Nanomanipulations \n             Going beyond observation to direct cell manipulation, Iva Tolic-N\u00f8rrelykke and her colleagues at the Max Planck Institute for Cell Biology in Dresden have brought optics giant Olympus in Hamburg and optoelectronics company PicoQuant, based in Berlin, together to market a combination of confocal microscopy and a picosecond (10 \u221212  s) pulsed diode laser cutter, to enable manipulation of subcellular structures at the same time as viewing the results. \u201cPeople have done laser ablation before but mainly with an ultraviolet laser on other microscopes. The new picosecond laser is simple and cheap, and induces less damage than UV because of its longer wavelength and short pulses,\u201d Tolic-N\u00f8rrelykke explains. She is using the device to examine the effects of laser cutting on the ability of microtubules to maintain the position of cellular organelles. And taking the idea of combinations a step further, Tolic-N\u00f8rrelykke is also exploring, so far in the lab rather than commercially, putting together a two-photon microscope with a femtosecond (10 \u221215  s) pulsed laser \u2014 to do both fluorescence imaging and laser cutting \u2014 and adding another, continuous, laser. This laser will act as optical tweezers, which can trap and displace microscopic objects. (Two-photon microscopes are a type of fluorescence microscope that enables imaging of living tissue down to a depth of 1 millimetre with low bleaching and photodamage.) This combination has enabled her team, for example, to specifically destroy small portions of the cytoskeleton and then use optical tweezers to displace the nucleus and follow how the cytoskeleton works to restore the correct position of the nucleus in the centre of the cell. The Max Planck team has put together its system from standard commercial products from companies such as Zeiss, Hamamatsu and Thorlabs. \u201cThese techniques have existed for 10 to 20 years but we have optimized and combined them in one set-up,\u201d says Tolic-N\u00f8rrelykke. Looking ahead, optical tweezers are likely to become an even more sophisticated tool for manipulating either whole cells or subcellular structures. This is being made possible through the use of holograms to split the optical tweezer laser into multiple beams, each one capable of directing the movement of a separate structure. By projecting a sequence of holograms, each positioned slightly differently to the one before, a sequence of light patterns occurs that can be made to move multiple objects around in three dimensions, in a kind of animated dance, according to physicist David Grier at New York University. Grier created the spin-off company Arryx in Chicago, Illinois, based on this concept, the result being BioRyx 200, a device that has a variety of applications, including sorting cells according to size, shape and composition, differentiating between cancerous and non-cancerous cells, and distinguishing bacteria from viruses. Further developments in software and photonics means that holographically controlled optical traps will become an important technology for manipulating structures inside cells in the next 10\u201320 years, predict biologist Daniel Robert and physicist Mervyn Miles at the University of Bristol's new nanoscience centre. Robert and Miles collaborate with Miles Padgett at the University of Glasgow to explore the use of 'haptics' \u2014 computer-tracked hand movements \u2014 to manipulate objects through the movements of optical traps (see R. Webb,  Nature   444 , 1017; 2006). These developments will help cell biologists gain a different perspective. \u201cThe mechanical aspects of biology at the micro- and nanoscale have not been explored to their full potential. There are now lots of new tools that can enable us to do that,\u201d says Robert. \n               Nanoprobes \n             Nanotechnology is also proving useful to biologists wishing to focus on single molecules. Nanoprobes such as quantum dots, semiconducting crystals that can fluoresce in a wide variety of colours, work as cell spies, shadowing the movements of their molecular quarries and allowing the investigator to follow in-cell events on camera with an astonishing level of detail. Such close camera work requires sophisticated imaging solutions. In confocal microscopy, the majority of photons emitted from a fluorescent light source are rejected, so as few as two out of every 100 falling onto a pixel are lost. Charge-coupled device (CCD) image detectors improve on this low level 'quantum efficiency' by capturing more photons and converting them into electrons, but what is really needed is a way of amplifying the signal above the level of background noise. Colin Coates, market development manager for Andor in Belfast, UK, explains how electron multiplying CCD (EMCCD) improves on standard CCD, \u201cIt's like trying to find something hidden in long grass,\u201d says Coates, \u201cWith EMCCD we amplify to such a high level that those photons aren't lost any more.\u201d Having pioneered the technology, Andor now markets a range of EMCCD cameras for studying single-molecule dynamics. Stefan Diez, at the Max Planck Institute of Cell Biology and Genetics in Dresden, uses an Andor iXon EMCCD in combination with quantum-dot probes 10\u201320 nm in diameter to watch motor proteins walk on microtubules. Not content with just observing, Diez is harnessing the power of motor proteins to perform highly specific manipulations, including stretching single DNA molecules as well as networks of DNA. \u201cIt's like body surfing,\u201d says Diez. \u201cThe microtubules are gliding across the surface of the kinesin motors. It's the force of the motors that moves the DNA attached to the microtubules.\u201d These kinds of manipulations are highly effective because they employ the cell's own machinery, he explains. \u201cThis is actually a good example, not of where nanotechnology helps us to improve biology, but of where biology has been used to improve nanotechnology.\u201d These manipulations could reveal more about DNA mechanics and DNA\u2013enzyme interactions, and also show the potential of biological motors for use in the molecular manufacturing of nanoelectronic circuits. Biochemists generally have to break open and extract the contents of cells to study reactions of interest, often after having to synchronize the cells' activity to get a high enough concentration of the molecules of interest in the same state. But now, thanks to a handful of pioneering research groups, it is becoming possible to study individual protein, DNA or RNA molecules in live cells, in real time, and to follow their interactions and kinetics with nanometre-scale spatial precision and millisecond time resolution. \n               One at a time \n             Chemical biologist Sunney Xie and his team at Harvard University in Cambridge, Massachusetts, have developed methods for observing the expression of individual protein molecules. After initially testing them in bacterial cells, the group is now extending the techniques to mammalian cells. This approach is useful for detecting proteins expressed at low copy number, at just a few copies per gene, such as transcription factors, which would be undetectable using conventional proteomics methods. The great advantage of the single-molecule method is that individual proteins are detected as they are synthesized in an individual cell. One of Xie's techniques detects the expression of proteins genetically tagged with the fast-maturing yellow fluorescent protein Venus. The appearance of the protein, observed as a series of fluorescent bursts that can be quantified, indicates that transcription and translation are taking place. Rapid strobing illumination enables the observation of fast binding and unbinding of fluorescently tagged proteins to immobile components in the cell and of interactions between fluorescent proteins in the cytoplasm. \u201cLike photographic images of a bullet going through an apple, we can detect fluorescent proteins in the cytoplasm. We can play with the pulse width to determine the dynamics and where the proteins are,\u201d says Xie. This technology could be extended to a high-throughput system, according to Xie's postdoc Nir Friedman. \u201cYou could put cells on a large chip with many chambers and do these kinds of measurements on a large scale.\u201d The fluorescent protein fusion could be done with a DNA library in order to detect the expression of proteins of unknown function. \u201cAlthough you need to target specifically, the advantage is that you can look at live cells in real time and with single-molecule sensitivity,\u201d Friedman adds. Nanotechnology seems destined to leave a lasting legacy for cell biology with a host of innovative new technologies. With continuing efforts to combine existing technologies in novel ways, and to create new ones, the possibilities for gaining new insights through nanoscale cell manipulation are increasing rapidly. Nanopatterning and nanotopography are techniques that are, as yet, practised by only a handful of specialists, but the equipment and software are fast becoming available commercially. This trend towards the increasing use of nanotechnology is pushing the very boundaries of cell biology. Reprints and Permissions"}
]